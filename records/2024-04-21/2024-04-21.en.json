[
  {
    "id": 40099585,
    "title": "Why You Should Think Twice About Y Combinator (YC)",
    "originLink": "https://twitter.com/dvassallo/status/1781751108211511680",
    "originBody": "The deadline for YC S24 is tomorrow. Here&#39;s why you SHOULD NOT APPLY TO YC:YC seems like a reasonable proposition. They give you some money to help start your business, and they promise you access to a community of people that can help you along the way. In exchange, they don&#39;t… pic.twitter.com/8FJOc8SxWq— Daniel Vassallo (@dvassallo) April 20, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40099585",
    "commentBody": "[flagged] Why you should not apply to YC (twitter.com/dvassallo)425 points by georgehill 15 hours agohidepastfavorite175 comments rgbrenner 14 hours agoThis is a repost from the authors blog before the W24 deadline: https://news.ycombinator.com/item?id=37869760 reply dang 13 hours agoparentThanks! Macroexpanded: Why you shouldn't join Y Combinator - https://news.ycombinator.com/item?id=37869760 - Oct 2023 (320 comments) The fact that this was reposted only 6 months later makes this a dupe by HN's standard (see https://news.ycombinator.com/newsfaq.html). Normally we would mark it as such, which removes a thread from HN's front page. I'm not going to do that, though, because the principle described here is more important: https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu.... \"We moderate less, not more, when YC or a YC startup is the story\" is the first rule of HN moderation. That does not mean we don't moderate at all—that would be too big a loophole. But we always moderate less than we otherwise would. A related principle is that we trust readers to be smart enough to make up their own minds. Between this thread and the one I posted yesterday (https://news.ycombinator.com/item?id=40091622) I think HN commenters are doing a good job of that. Edit: although it follows from the above, I should probably say explicitly that the [flagged] marker on the post is because of flags that came from users, not moderators. reply ec109685 11 hours agoparentprevIt’s annoying he doesn’t even revise when he is clearly wrong, e.g. around the idea that YC doesn’t encourage pivots: https://news.ycombinator.com/item?id=37870099 reply pclmulqdq 11 hours agorootparentI've always been sort of leery of the idea of pivots, because a pivot for a company really doesn't make a lot of sense to me unless it's more of a course correction, and/or heavily utilizes the company's built-up IP (eg Slack, the famous pivot from a gaming company to a productivity tool that they made for themselves). What would make more sense for everyone is for the company \"pivoting\" to simply shut down, return the unused money to the investors, and reincorporate to do the new thing. That new thing would then go get investment for itself from investors who are more aligned with the new mission. reply nurple 9 hours agorootparentTo me, pivot feels like YC saying: hey, we thought you were above the cut, that's why you're here. We know the low likelihood of an idea exploding, let's just get you focusing on other promising blocks as soon as we hit bedrock. I'm assuming this isn't terribly uncommon, considering the hitrate and actual sunk cost. I see the author's viewpoint from a milder formula: learn, earn, or quit. YC seems to be a perfectly fine place to learn a certain kind of methodology. It can be difficult to balance the value flow, especially when goodwill is involved, but there aren't many places that will give you half a mill and a chance at a dream. reply Lerc 11 hours agorootparentprevI can see pivots being worthwhile for existing investors if the business needs more investment. If the pre-pivot is a worthless thing you own 7% of, it would be better to inject another bundle to own 14% for a new thing instead of starting afresh and owning 7% of nothing plus 7% of a new thing. reply pclmulqdq 10 hours agorootparentThe thing is that usually pivots are pre-vesting for the founders, and usually the company has less in the bank when they initially raised. In those conditions, the entirety of the company's assets will be returned to the investors when the company dissolves. It is a good point that it can get you a larger share of a success, though. reply lupire 9 hours agorootparentprevYC has always said they invest in founders not their products. reply benatkin 11 hours agorootparentprevThis comment in the thread is in disagreement with your point: > As a former employer for a YC funded company that was shut down against the founder's wishes, forced to via the investor board; I can say that this article does not universally wrongly characterize the YC experience. I have heard of YC doing pivots, so \"dig a hole in the same spot until you reach the boiling magma\" doesn't characterize it either. However, that doesn't make YC look good from the perspective of a founder wanting to be in control. What I remember hearing from YC is of cases where a partner tells you to pivot, not where they let the founders decide to pivot themselves. Edit: I realized the main comment linked to jibes with what I was saying: > The overwhelming feeling is: be humble in the face of reality, try something and try to try it in a way that you can assess whether it’s working — quickly/cheaply — then try something new. ...try something new that's recommended by the YC partner(s) I am more positive than negative on YC though. There is no perfect balance between being too hands-on or too hands-off for a startup accelerator. reply mikeshi42 5 hours agorootparent> What I remember hearing from YC is of cases where a partner tells you to pivot, not where they let the founders decide to pivot themselves. Post-batch you can talk to your partners as often as you want or never again - they stay out of your way unless you explicitly reach out. They make this quite clear during the batch as well. During the batch you do talk to your partners as part of the structure. They do give (really good!) advice, but you're always told it's your company. I can see them giving advice based on their experience on where you might want to dig instead, but I've never heard a founder being forced down a direction they didn't want to go. After all, why would that be the optimal path? reply redbell 12 hours agoparentprevSurprisingly, the OP of this twitter thread is the same as the link above! reply georgehill 12 hours agorootparentYes, you noticed. Although I very much disagree with Daniel's post, it is always good to have two different perspectives. Respect to Dang for not taking it down. reply gamerDude 15 hours agoprevTo me, this tweet seemed much more anti startup trying to grow big than anything anti-YC. Only using the tweet for marketing as mentioned at the bottom. At first, it's against dedicating your life to your startup (which presumably you are already doing or at least want to do) and then states that YC actually has a good success rate for unicorns. Then he talks about doing some brainstorming to find ideas and product market fit, but at least from my perspective YC doesn't advocate for this at all, but rather talking to customers, getting their buy in and then making something that solves customer problems. Aka, not brainstorming at all. And then goes on to say that YC has lots of young people rather than older (more likely to be successful founders). But doesn't do any of the data diligence on percent of founders by age by industry and by YC vs. not. And finally he focuses on saying that only the unicorns were success stories and no one else, while advocating that you should aim for a smaller goal. But doesn't acknowledge any data of companies from YC that aren't unicorns but are still operating, sold for less than a billion, etc. I personally think that looking from the outside that YC is not for everyone and is geared towards the VC route. But if you want those things, then these arguments aren't valid for why you wouldn't apply. reply pclmulqdq 14 hours agoparentThis sort of became an anti-VC rant, definitely, and also a little unfair to the product YC offers. It's a legitimate product in the market, not a scam, and the 22-year-olds who do it are not getting duped. My biggest issue with YC is that the deal isn't very good. Giving up 7-10% for $500k is pretty crappy for any company that has a decent shot at becoming anything good. That is why I have only ever applied to YC when my company ideas were very speculative. If you already have a network and a decent platform to communicate with people, as well as the motivation to go at it for yourself, you're giving up a huge chunk of the company for a pittance. If you already have the ability to go talk to a VC and an idea/product/company that is good enough to make it into YC, you don't need YC for your company. If you can sell YC on investing in you, you can sell your product. However, it does make sense to do YC right out of college to get the networking benefits and the credibility signal. That's why so many 22 year olds do it. It sacrifices your stake in your first company in exchange for helping you as an individual later in your career. reply arciini 13 hours agorootparentI did YC when I was 27 after trying to do a tiny bit of fundraising, and kinda agree with you, but I think you can replace \"22 year old\" in your comment with \"anyone not super-well-networked in the startup scene\". The truth is, even if you have pedigree (top-tier college, worked at FAANG and startups), the very first time fundraising is going to be pretty hard and annoying unless you've already been going to a lot of founder meetups, want to start something in a hot field, or have VC's in your personal network. This is especially true after the money wave from pre-2022 receded. I do feel like I probably could've raised on better terms, but I'm not sure it would've improved the trajectory of my life or expected value. YC taught me a lot about fundraising that I didn't know I didn't know. Compared to other VC's we've worked with (who have all been pretty great), YC has provided more specific advice to our stage, better technology, and a slightly better network. reply boulos 14 hours agorootparentprev> If you already have a network and a decent platform to communicate with people, as well as the motivation to go at it for yourself, you're giving up a huge chunk of the company for a pittance. But that's literally the point. You suddenly get a huge network that would help you get through the early stages. It's certainly true that many founders would find better terms elsewhere. However, it's also true that most companies fail. So negotiating better terms usually doesn't matter nearly as much as making the company more successful. (The same rationale applies to employee equity grants) Like you said, for people with no connections, whether just out of school or not, it's valuable. reply mlhpdx 13 hours agorootparentAgreed. YC from my POV is attractive not for the money but the motivation. The peer influence (encouragement, competition, accountability) is what I feel would improve my chances of building something amazing. But I can’t disagree with the Tweet’s argument that what’s good for YC (swinging for home runs) isn’t good for the vast majority of founders (but it is what those founders want, and why the lottery is a thing). reply bko 13 hours agorootparentprevThe biggest thing that sours me on YC is the exclusivity. They gets tens of thousands of apps for something like 300 spots. For all the criticism of artificial scarcity of places like Harvard, why is this different? As peter theil would say, competition is for losers. You shouldn’t participate in games that are zero sum and they shouldn’t pretend they have some kind of carefully thought out scientific method to determine who should get in. Half the companies I see in the batch are dog shit. A third of the companies are “ideas only” which is basically now an Opaque beauty contest. Just opt out and boycott these stupid status games reply robertlagrant 12 hours agorootparent> They gets tens of thousands of apps for something like 300 spots. For all the criticism of artificial scarcity of places like Harvard, why is this different? How is Harvard artificial scarcity? How could YC do this any other way, when they have a limited number of staff and a quality bar to meet? > Just opt out and boycott these stupid status games Sorry - I'm struggling to follow. Do you think YC membership is a status game? I thought it was a real program. reply bko 10 hours agorootparentHarvard could easily expand their student body. They haven't kept up with population growth. They could easily double or triple their size But it would reduce the prestige. Plus they're sitting on 50bn endowment. In general people look at admittance rate as a proxy for worth. Doubling the size means doubling the acceptance rate. The one thing many people know about harvard is that it's hard to get into, and by design. InI'm imagine yc is similar. I'm saying their bar is BS. You're not going to convince me they can meaningfully evaluate tens of thousands of apps. reply sokoloff 14 hours agorootparentprevThere are a lot of ideas (virtually all of them) for which I’d give up 2-3% for $0 to go through the YC process and network even if I intended to bootstrap or think of the $500K as getting them the “other” 5% (or an ~$10M valuation). reply pclmulqdq 14 hours agorootparentThen go for it. I don't know if you've ever applied for YC, but it sounds like fun. I regret not doing it right out of college. reply nostrebored 14 hours agoprevThere are few people I think are as consistently wrong as Daniel Vassallo. His writings on cloud are an exercise in fanaticism, and it looks like he’s turned that failed venture into yet another course. People selling you courses and community are mostly grifters. The content here has nothing to do with YC in particular, and makes a few interesting claims: 1. YC founders are all young, impressionable 20yos 2. YC encourages you to make up problems to solve 3. That an exit less than a billion dollars is a failure All of which are easily dismissed by watching actual content from YC. 4. That a 1.25% chance of creating a company worth more than a billion is a bad outcome Honestly what an opportunity! reply htk 12 hours agoparent\"1. YC founders are all young, impressionable 20yos\" Exactly the demographic he's targeting with his \"courses\". Easier prey for his engagement farming. reply ec109685 14 hours agoparentprev5. Saying YC doesn’t encourage businesses to pivot or explore until they find product market fit. reply doctor_eval 12 hours agorootparentYeah this one got me. I have nothing to do with YC but I was hired to consult to a couple of really nice young guys who were YC alumni. They worked their asses off doing All The Right Things - talking to customers, reaching out to people with deep experience :) - and when they realised it wasn’t going to work out, they pivoted right away. This is the exact opposite of digging in one place for the rest of your life, and as someone who has done a few startups, I was super impressed by their attitude and their ability to let go (even though I was out of a gig!) I think they’re going to do great whatever they do, and at least some of that will be because of what they learned at YC. reply fuzztester 13 hours agoparentprev>4. That a 1.25% chance of creating a company worth more than a billion is a bad outcome Slight logical problem, or unclear or poor wording here: A chance is quite different from an outcome. The first is in the future (chance), while the second is in the past (outcome). A (1.25% or other) chance means something may happen. An outcome means something did happen. Also, a 1.25% chance in the past does not in the least imply the same chance in the future. See stock trading, for example. reply fuzztester 12 hours agorootparentTo make it more clear, you were talking as though a chance is actually an outcome. >4. That a 1.25% chance of creating a company worth more than a billion is a bad outcome reply mlhpdx 13 hours agoparentprevHas YC published age demographics for batches somewhere I’ve missed? They’ve talked a bit about it, but I haven’t seen hard numbers. reply worik 11 hours agoparentprevHelpful comment > People selling you courses and community are mostly grifters \"Mostly\". It could be argued that that is the purpose of HN I do not know, but I am glad of it And I'm not buying (Absolutely not complaining) reply joshxyz 8 hours agoparentprevSpot on, what a fucking grifter. A shame to this community. reply jmduke 14 hours agoprevOP has a history of railing against YC, largely for engagement's sake: https://twitter.com/search?q=from%3Advassallo%20YC&src=typed... (I have never applied to YC — my closest affiliation is that I reviewed YC applications for prospective applicants while at Stripe — but I do find engagement farming really annoying.) reply arciini 13 hours agoparentI personally also strictly disagree with their feedback: \"try many small things, experiment, tinker, and build a portfolio of multiple income streams\". This is very much the indie-hacker route, but honestly, if you have one thing that's working out, it's usually better financially (though maybe less fun if you like purely doing engineering/product work) to try to grow it than to build a portfolio. As someone who went through YC, but started out more on the indie-hacker side, I want to mention that going through YC in particular does not close the door to just continuing to build a successful, growing business without raising one round after the other. I think some indie-hacker influencers encourage this kind of us vs. them thinking about VC's, and the truth is somewhere in between. I know of many businesses who have decided (either by choice or through a lack of fundraising options) that the ideal way to grow and scale their business after YC is to not fundraise. It's something that YC partners explicitly acknowledge. They understand that an alive business can continue to grow, whereas a dead business is just dead. reply WA 3 hours agorootparentExactly this. And the irony is that he himself follows the very same standard playbook: play around, do several \"small bets\" and then go all in on the most successful one. But he sells you the idea that you need to be your own VC and have multiple income streams. His other projects are mostly dead by now and my bet is that he won’t have another one ever. He’ll probably save a lot of money from his course, put it in an ETF and call it \"see I diversified my income\" lol. reply mylons 11 hours agorootparentprevi don’t think you understand his point at all. tinker until you find that thing. maybe keep tinkering after. a single source of income is very fragile. reply bko 13 hours agoparentprevI’m curious if that analogy actually makes sense. Why would you pay people to dig their entire lives in one spot? Why wouldn’t you just do the more efficient way and have founders mine their own 100 yard fields? I agree the incentives of VC want are to swing for the fences and they get the benefit of diversification. I just don’t think the analogy works… reply pb7 12 hours agoparentprevI had followed him for years and recently stopped (and even blocked) because he’s become a prolific engagement farmer. Given how pointed his takes have become, there is little chance he’s changed in so little time to become this opinionated. I doubt he believes much of what he writes about. When all you do is sell courses, no publicity is bad publicity. It’s all a numbers game and he knows it. reply DVassallo 8 hours agorootparentIf this is what you think, good riddance. I think carefully about what I write publicly, and stand by what I say. reply StressedDev 13 hours agoparentprevWhat is engagement farming? reply minimaxir 13 hours agorootparentTwo common implementations of engagement farming are a) very contrarian/misinformed opinions to bait people to respond negatively and b) simple Q&A prompts like \"What opinion about AI would get people mad at you?\" which encourage people to respond. The organic discussion (positive or negative) is generally favored algorithmically so it gets boosted, and in the case of Twitter, there is a monetary incentive to do so, as Twitter Blue accounts can earn revenue from tweet views. The tweet author performed the former very recently and caused a news cycle by complaining about MKBHD's Humane Pin review with a bizarre take: https://daringfireball.net/linked/2024/04/16/vassallo reply grayhatter 13 hours agorootparentprevfancy clickbait reply pessimizer 13 hours agorootparentprevIt used to be called \"trolling,\" before trolling was redefined to \"publicly disagreeing with important people.\" It's maximizing the ratio between the characters you type and the characters typed in response to you. reply hunter2_ 13 hours agorootparentIs an ideal response one so thoroughly perfect that nobody has anything to say after it? Or is leaving some room for continuation good until it's excessively so? reply baubino 12 hours agorootparentThe problem is being more concerned with the response than with what one is typing. reply dylan604 13 hours agorootparentprevFor example, your comment reply paganel 12 hours agoparentprevSo you're more connected to YC than 99% of the people commenting in here, I don't think that your viewpoint can be seen as objective. reply jmduke 11 hours agorootparentTo be clear, I reviewed applications _on behalf of Stripe Atlas customers applying to YC_, not on behalf of YC. (I have never chatted with anyone in YC, besides presumably on Hacker News.) reply loceng 13 hours agoparentprevSo what do you suggest, suppressing or censoring dissenting voices? Your comment also doesn't add anything to the conversation, does it? reply minimaxir 13 hours agorootparentThere may be a pertinent ulterior motive to the tweet thread, which is relevant to the discussion. reply joshxyz 10 hours agoparentprevhe does it on pretty much any topic he can get engagement with actually. a repulsive human being in person. reply fnordpiglet 10 hours agorootparentDaniel? He’s been a friend of mine in person for over a decade. I knew him long before his exit from tech into working for himself. He’s a wonderful human being, a kind and gentle man who enjoys mentoring people and being with his children. He’s also building his business I assume, which I guess entails a certain amount of salesmanship and playing the game if your business is in the orbit of social media. I’m always surprised by the personal hot takes reply joshxyz 8 hours agorootparentMore like personal dumb takes. I admired him in his early days too. Yet lately he really stands out as a fucking idiot in my twitter timeline every once in a while to the point of having to block him. A full-grown adult man just chasing clout, what a fucking clown. reply pb7 1 hour agorootparentAgreed. I used to go on Twitter almost exclusively for his content. Now I have him blocked. What an embarrassing fall from grace all in search of the next like and follower. Addiction to engagement claims another one. Society is materially worse off converting talented workers into social media course selling charlatans. reply bschmidt1 14 hours agoprevHahah he's like \"YC is a big scam\" but when you keep scrolling... > I charge you a one-time payment of $375, and you get access to my community It basically invalidates the entire post. Maybe some good points were made about investing in general but they should be delivered by someone who doesn't run an Andrew Tate style self-help scheme, and shouldn't be advertised in the very post criticizing YC of being inauthentic. YC is infinitely more prestigious and well-connected than this guy's $375 program - and how dare he talk about YC coming after my \"personal economy\" when he's charging $375 for \"access to a community\". Last I checked HN is free and I bet there's a lot more valuable info here than that self-help nightmare forum. Furthermore, we know YC is exclusive. But the fact that YC is so Ivy League and startup focused is also why HN is not just a random subreddit, or some Twitter hatefluencer pyramid scheme. reply keepamovin 3 hours agoparentHaha my thoughts exactly. Overall I doubt his characterization. It's an okay model but I think the way that YC probably actually works is more like: a network of secret back-channelers with inside info and connections, read a bunch of crazy old treasure maps, pay off landowners, form search cliques, hire people with good shovel-arms, develop tunneling methodologies, and end up swiss cheesing the entire field until the subsurface is a honeycomb of search paths, and maybe they come away with some gold, after agreeing beforehand how it would be divided if it were found. YC is more like ants, but he makes it sound like robots. But even that doesn't really cover it, as the whole idea of a zero-sum game where you are searching for something that already exists is broken and wrong. It's false. What you're actually doing (YC be damned, in anything!) is creating something new! You're not finding the gold that's there, you're making it. Out of nothing. And then profiting from it!!! :) reply DVassallo 8 hours agoparentprevHN is free because it’s a top of funnel for YC. They’re not doing out of the goodness of their hearts. reply a_wild_dandan 14 hours agoprev> YC will proudly tell you that you are more likely to end up with a billion dollar business if you join them. That may be true. What they’re more reluctant to tell you is that only about 50 companies met that expectation out of the 4,000 or so that went through their program. That’s 1.25%. To be fair, that’s actually quite impressive, but let’s say you have the stamina and willpower to go through YC three times in your lifetime. You’d need approximately 26 lifetimes to hit the jackpot! See the problem now? No. Those are frankly astonishing odds, and success exists on a spectrum between the extrema of abject failure and a billion-dollar business. Framing outcomes as one of two, mutually exclusive states feels disingenuous. (Also, minor nit: that's not how probability works.) I'm ignorant of the startup space, so the author's conclusions might be generally accurate. I don't know. But the piece sets off red flags for me, so I'm distrustful of it. Kudos for introducing me to the concept of ergodicity, though! reply Retric 13 hours agoparentBeing nominally a billion dollar VC backed company isn’t success until you’ve got a 7 figure exit, it’s just a number someone made up. So sure, you can have a “successful” exist without becoming a unicorn, but that doesn’t mean the odds end up very good or you wouldn’t have been better off financially working for Google. reply hcks 12 hours agorootparentPretty sure 7 figures would be a massive failure in that case reply Retric 10 hours agorootparentFounder’s don’t keep a high percentage of the company through multiple rounds of funding, employee stock options and YC prefers multiple founders. Zuckerberg had 28% of Facebook at IPO but the other 4 founders had far less of a stake. Sub 10% of 1 billion is 7 figures. reply fairity 5 hours agorootparent> Sub 10% of 1 billion is 7 figures 8 figures, not 7 reply mattmaroon 13 hours agorootparentprevSurely someone must have the statistic on how many YC companies had an exit/are still running, right? I bet it beats the industry average by a long shot. reply themanmaran 12 hours agorootparentIt varies by cohort and age of companies. But the 10 year average is: - 58% alive - 8% acquired - 34% dead reply gizmo 12 hours agoparentprevAlso consider that out of those 4000 startups many are not unicorns yet. The long term percentage might be closer to 2.5%. And the latest batches have many AI startups which I suspect will have an ever greater unicorn success rate. reply worik 11 hours agorootparent> And the latest batches have many AI startups which I suspect will have an ever greater unicorn success rate. I am resolutely of the opposite opinion In two decades AI will be incredible business It will be clusterfucks and graveyards in the meantime I am very moved by the dot com comparison One or two huge successes, and bleached bones and \"WTF were you thinking\" spread thickly reply what 9 hours agorootparentprev>AI startups Which are all thin wrappers around OpenAI. None of those will be unicorns. reply kumarm 14 hours agoprevThis is the same guy who went viral criticizing MKBHD last week right? Here is a good information on what is happening: https://news.ycombinator.com/item?id=40060554 from that discussion. There is incentive to take a public view that is anti current (or trend or popular or right) thing to do that makes you go viral. reply alangibson 13 hours agoparentI came here to point this out. I'm automatically skeptical when this came from the author of one of the dumbest tweets I've ever read. reply sneak 13 hours agoparentprevPart of being an interesting contrarian is that you have to actually be right about stuff. Simply espousing the opposing view to the popular view is going to be wrong more often than not. reply loceng 13 hours agoparentprevPointing this out seems to popular in comments, and doesn't seem to add anything to the conversation. reply flurb 44 minutes agoprevI don't quite understand the criticism of YC, but I'm nobody, and maybe little bit naive here, but they offer you $500K in exchange for 7% of a potential business (of course it's more complex than that, but bear with me). Looking at the application form, nothing stops a farmer from, say, rural North Korea from applying, notwithstanding other issues of a more, ah, political nature. While $500K maybe isn't much for some people, it sure is a heck of a lot of money for someone like me, not to mention all the other benefits, being able to explore your ideas and start a business without the immediate worry you'll either die of stress or become homeless next time rent is due, for instance. All that for a measly initial investment of maybe 10 minutes of your time? reply robocat 14 hours agoprevI think this “tweet” is totally unbalanced. And I think he misses some of the significant downsides. He is moaning about Venture Capital, not YC. You can use the YC money and bootstrap. Although obviously YC is pushing the Venture Capital drug and will filter for Venture Capital seeking founders/opportunities. My experience of an incubator showed me that incubators disable founders. Incubators put back into school where the teachers tell you they know everything. Many mentor relationships are dangerous too. It damages your psyche and is hard to escape the desire to get the best advice/mentoring. Learning to be a founder is all about making your own good and bad decision. The right attitude is hard to maintain, and hopefully YC builds it (no idea if it does). Get advice but learn how to ignore most of it (even geniuses get most things wrong). 7% for $500k is a great deal if you use it carefully: use the network and experience to your advantage and you should get more than an extra 7% growth. Take care that other flashier startups don’t steal your star employees. YC gets preferential shares, but founders get common shares: so alignment is wrong. YC has the financial incentive to team up with Venture Capitalists against founders. Not saying they do, but it is a bad signal that YC gives. You need to watch YCs behaviour with the few winners to learn the outcomes for YC versus founders - on average only the few winners matter financially. The final results for the reddit.com IPO are really weird given reddit was in the first YC batch: https://archive.is/https://www.businessinsider.com/who-got-r... (Sam Altman had 9% ownership). Finally, PG has written multiple essays about how startups need to shoot for the moon. I have seen zero information come out from YC about median returns for founders, or an analysis of the $ earned against the time invested for the vast majority of founders (the ones that do not successfully start unicorns). Some light on this subject from YC would be welcome. Too much of the external YC writing is clearly self-serving while saying they help founders (the whole VC industry is like this though). I am in New Zealand so I have little experience with YC (watched one guy from here accept and flame out - but I had a bad signal about him anyway). reply wuj 13 hours agoprevI disagree with OP. Treat YC as a school where you learn and network. It is not a zero-sum game. If you made it big, you've hit the jackpot. If not, you still gained valuable experience and skills applicable to the industry and other areas of life. It's like going to a college. If you didn't find a job right out of college, it doesn't mean you've wasted four years of your life. The expected value of doing a startup is very high when your career is just starting. You are well-educated with very marketable skills. You don't have people depending on you. If you are building something and picking up transferable skills, all while being surrounded by the most driven people (and paid), I don't see why someone wouldn't try their luck with YC (or any other startup incubators). reply brigadier132 15 hours agoprevLet me give the other side of this because I used to believe this too I'll start with the most important first 1. When you take YC money you get to pay yourself. You wont be paying yourself FAANG money (if that's still achievable nowadays who knows) but you are paying yourself to build a company you own the majority of. So yes, they want you to dig to the center of the earth to find gold but you are being paid to do so. 2. I don't think starting a niche startup is meaningfully easier than starting a startup seeking uber growth. While a tech startup might require more up front knowledge I don't think startup ceos are working harder than the baker that's waking up at 4 am every day. 3. YC has a very good success rate and I think it's related to services they provide founders and the network. I think if your business falls into the category of hyperscale startup and you are a first time founder I really think YC is worth it. They seem to have a system that works. reply bruce511 14 hours agoparentI concur that the system works. The point of the article is who does it work for? Personally, I agree with you. If you're just coming out of college this is a great way to get a couple years of salary and a chance to work on whatever you like. Give it a few years, if it doesn't work out it hasn't really cost anymore than a post-grad course in terms of time. Basically you can view the whole program as a simple \"job\" with pay, with a 1% chance of going big. That's probably worth taking a punt on. 99% will fail, so frankly I wouldn't spend more than 5 years on this path, but if you're young you can afford that time. Honestly this approach seems a lot more attractive than putting up my own life savings to start a business. Or working without income for a year or whatever. reply computerdork 13 hours agorootparentAnd a startup doesn't need to make 1 billion per year to be a success. How about one that makes 100 million? or even 10 million? If the company is sold, the founders will be rich (not ultra rich, but have a million or more). And, for ycombinator, the number of \"startup exits\" is: 351 (according to this page: https://explodingtopics.com/blog/startup-stats). Am no expert at startups, but this may mean that out of the 4000 that when through ycombinator, ~9% where valuable enough for investors to take profit. That sound pretty good to me. reply robocat 10 hours agorootparent> or even 10 million > the founders will be rich A common misconception that is usually untrue. If you hop on the VC train there are many reasons why founders can get $0. VC gets preferential shares which means that founders (common shares) often get nothing even though the business sold for millions. reply computerdork 5 hours agorootparentAh, I see. And actually, I do know a little about startups (even though I said I'm no expert), and thought that most founders who sell their businesses for something like 10 million can make a comfortable amount of cash. But didn't realize that if they use VC's, the VC's can take so much! So, was wondering, would you happen to know if there is a standard amount in a sale of a startup funded by a VC that makes the founders a good amount of money? (Talking about the minimum needed to make a million for a couple of founders). reply bruce511 3 hours agorootparentI guess the break-even line is exiting for more than you raised. If you raise 20 mil, and sold for 20 that's different to raising 5 and selling for 10. Thing is though, the latter scenario isn't common. If you raised 5, and are offered 10, you probably won't accept. You'll go raise another round. Until you can't raise anymore, at which point the exit is likely underwater. But you got paid a salary along the way, so thats not to be ignored. reply cpill 7 hours agorootparentprevI imagine it's not much fun with YC on you constantly telling you to make more, scale, or pivot to you make our investment worth it. I find the clients that pay the least demand the most reply computerdork 5 hours agorootparentagreed:) But on the flip side, as long as the startup is sold in the end and you as the founder make a good amount of money (meaning over a few hundred of thousands), think it's probably worth it reply mlhpdx 13 hours agorootparentprevI hope, perhaps against mainstream opinion, that the myopic focus on the young will be proven flawed. I see the launches and highlights YC posts on LinkedIn and elsewhere and see no one like me, yet am confident I would both benefit from it and be more likely to build something amazing than the majority (that being a particularly uninformed opinion, since I know only a few personally). reply tossandthrow 13 hours agoparentprevyes, you are correctly taking alternatives into consideration here. while the author is Tru about the reasoning they fail to elaborate on the alternative. take a job with youghly the same pay as working in your own startup? that would seem terrible. obviously, if you have opportunity with a much higher expected pay, then that should be the go to opportunity. reply DVassallo 13 hours agorootparentTaking a job is indeed an alternative. You pay yourself, but you get little control on what you work on (apart from choosing which company to work for). However, you can usually leave easily with a 2 week notice without many downsides. Getting accepted into YC is quite similar to getting a job. You get paid (less), but you get more control on what you work on. Even though technically you have full control, you don't really do that for moral and ethical reasons. You will want to do things that have a chance of paying off big time, because that's why your investors trusted you with their money. And if you get fed up, it's not as easy to leave. What about your investors? What about your reputation? Will you want to let these people down? Etc. reply bearjaws 14 hours agoprevSo I bit on this guys ad. First red flag? Using GumRoad. Which is basically course grift central. Immediately you have the option to buy other courses from his recommendations (with a coupon of course!) What a joke. He has only made 16 videos out of the hundreds listed in the \"Course\". It's hilarious how EVERYONE is trying to sell courses, and none of them are good. reply redbell 14 hours agoparent> It's hilarious how EVERYONE is trying to sell courses, and none of them are good. This is, unfortunately, a real business for many.. Creator Courses: Selling Dreams as Products: https://youtu.be/ZmixszMNFgM The Rise of Fake Gurus: https://youtu.be/L9Gpr7PEnbs reply islewis 15 hours agoprevIt feels to me like YC's name-drop doesn't have a purpose beyond just attention/view grabbing, despite there being a few nuggets of solid reasoning sprinkled throughout the post. > That’s 1.25%. To be fair, that’s actually quite impressive, but let’s say you have the stamina and willpower to go through YC three times in your lifetime. You’d need approximately 26 lifetimes to hit the jackpot! This isn't the least bit unique to YC, it's just the cold reality of any high-growth startup. If OP had framed this as \"Moonshot Startup vs Lifestyle Business\" it would have felt more like a good faith argument. Even \"Do you have the risk tolerance to be a founder\" would have worked. But basing a thread titled \"Why you shouldn't apply to YC\" off an illformed assumption of the readers risk tolerance is naive at best (and likely malicious given OP is selling competing services). reply Pedro_Ribeiro 14 hours agoparentAlso that sentence on \"you need to go through YC three times in your lifetime, so you need 26 lifetimes\" is absurd. I got a migraine trying to understand his line of reasoning. reply romafirst3 14 hours agorootparentThe reasoning is a bit reductive, it makes sense if you think of success as binary (you become a billionaire or you don’t, which in fairness is what yc and many VC firms push). Obviously you can fail to become a billionaire and make some non life changing amount of money and even make what to a lot of people is life changing, but for this outcome (which is still uncommon) you would be better off not trying to become a billionaire. reply abeppu 14 hours agoprevHmm, I think this criticism maybe has a kernel of truth, but - I think the \"ergodicity\" concept is kind of being abused. As I understand the term, ergodicity describes systems whose dynamics cause them to eventually visit all states in a way that lines up with some probability distribution. In MCMC one uses this to argue that a chain that runs long enough can be used to generate samples from a distribution. - But the response to his actual concern I think should be something more like income pooling. E.g. my understanding was there was some movement towards this for minor league baseball players, who have a modest chance of really large incomes. I think poker players also make similar arrangements. But critically, pools make sense when all the participants in each pool have pretty equivalent odds of making getting a large payout. With very early founders of course smart, reasonable people could have wildly different ideas of who is in equivalent tiers. - You could try to ask a bunch of informed investors to rate or rank, and hope that average estimates are good -- but if those investors don't also have the opportunity to invest in a way commensurate with their ratings, they have no incentive to be accurate. https://www.npr.org/2019/10/25/773532516/some-baseball-playe... reply senkora 15 hours agoprevWhatever else, it is to YC’s credit that they don’t censor criticism of themselves on their own platform. reply whiterknight 14 hours agoparentJust because you see some doesn’t mean they don’t. reply loceng 13 hours agorootparentAnd don't mention a dislike for \"that one thing\" too many times. reply dang 13 hours agorootparentI don't know what one thing you're referring to, but a core principle of HN is to avoid repetition, and especially the repetition+indignation combo, which is the commonest and most tedious thing on the internet. So to me it sounds like you're referring to HN working as intended, regardless of what the thing is. https://hn.algolia.com/?dateRange=all&page=0&prefix=false&so... reply Uehreka 14 hours agoparentprevNah, they don’t get points for not censoring criticism of themselves. It would just be negative points if they did. reply wg0 14 hours agoparentprevNo doubt but I additionally see it as a pretty American cultural trait as well. reply DVassallo 7 hours agoparentprevThe post just got flagged and taken down from public view. reply dredmorbius 1 hour agorootparentBy HN members, not moderators:reply mizzao 14 hours agoparentprev\"There's no such thing as bad publicity.\" reply jumploops 14 hours agoprev> One of the bad learnings you get from YC is that there’s a formula for success, and it looks like this: First you do some brainstorming. This isn’t how YC works at all… If this guy had gone through YC, maybe he wouldn’t be trying to sell an online course. reply llamaimperative 13 hours agoparentYeah this thing is rife with “I have no clue what YC actually advocates” reply extr 14 hours agoprevI think all the disagreement is just a matter of demographics. If you are 22 and time is on your side, go nuts. The intangible benefits of YC (networking, etc) have time to pay off. It’s not like you would have been making insane money at BigCorp anyway that early in your career. You can afford some income instability. No real downside, just be realistic about your chances of a huge exit/unicorn status. But for “everyone else”, it makes zero sense. You have kids? You’re mid/late career? Forget about YC/VC funding. Much better ways to spend your time and effort. reply dnissley 12 hours agoparentWhat are those better ways to spend time and effort? reply extr 11 hours agorootparentIt depends on your goals, but this is David’s core point. If your goal is an independent income stream or building a business, at many (most?) life stages, shooting for the moon is not practical - too risky to put all your eggs in one basket. Too many ways to fail and waste years. You’re better off diversifying your bets, not dissimilar to VCs themselves. Try small ideas and scale them up if they work/seem promising. And of course if your goal is just building wealth and having security, maybe working for a BigCorp is indeed the better choice of time/effort. You can make a lot of money in FAANG and get a dynamite family healthcare plan + retirement contributions to boot. reply yinser 14 hours agoprevFrom the man who said MKBHD was irresponsibly criticizing a “nascent” tech project that has hundreds of millions in funding and built a dud reply DVassallo 13 hours agoparentI said MKBHD was irresponsible for using a highly sensational headline (worst product ever reviewed) when he has such as large influence. I never said what you're implying. reply mslt 12 hours agorootparentPlease explain how someone whose job is to share their opinion is being irresponsible for sharing their opinion. reply pb7 12 hours agorootparentprevSometimes the truth is sensational. I hope you know you lost at least one long time follower after your recent stretch of engagement farming. You’ve fallen off. Good time to lay off the tweets and look inward. reply DVassallo 8 hours agorootparentI gained hundreds of better followers. The engagement farming is in your imagination. reply pb7 2 hours agorootparentBetter followers are those that will buy what you peddle, right? You’re a sell out. At least at Amazon you created real value. reply sneak 13 hours agorootparentprevYou’re going to need to substantiate “irresponsible” a lot more than “he said an opinion that he actually factually holds to a lot of people”. Who was harmed? What was the source of that harm? What was the moral or ethical failing that caused the harm? I think you have cause and effect mixed up. reply kyleyeats 14 hours agoprevConfusingly, this tweet is about why you should join YC. History is full of young men on boats, in the wilderness and in formation to have a small chance at glory. All of those guys were giving up a lot more than a 7% stake, and eating worse than ramen, and living worse than a cramped apartment. The reason you shouldn't do YC is simpler. It commits you to the VC track. Whatever you build is eventually going to suck because of enshittification. YC = VC If the VC thing doesn't bother you then try for YC for sure. It is overwhelmingly the best way to do that unless you're a rich kid who can leverage his connections. And even if you are that rich kid it's still the best way. reply dang 13 hours agoparent> It commits you to the VC track. That's not true. YC always supports what founders want to do. If founders don't want to raise VC, YC supports them in that choice. reply kyleyeats 12 hours agorootparentI didn't mean it as a top-down thing but as an aligned goals thing. If you do this you're basically paying a 7-10% tax on everything you do, forever, without the hyper-growth to justify/offset it. It's like selling out to Hollywood and then only doing bit roles. If you do YC, you should take VC. You would be stupid not to. I mean, YC is VC, right? It's a consistency thing. No-YC and no-VC makes sense. Yes-YC and yes-VC makes sense. Yes-YC and no-VC does not make sense. reply gardenhedge 13 hours agoprevall you need to read: \"I charge you a one-time payment of $375, and you get access to my community, which includes live workshops, recorded classes, a group chat, and a few other things.\" reply elwell 14 hours agoprevThis tempted me to apply. I feel like all the arguments came to the opposite conclusion that the facts should lead one to. reply llamaimperative 14 hours agoparentThat’s what motivated reasoning (for clout, in this particularly sad case) will do to ya! You should apply. Worst case scenario you’ll have spent a few hours tightening your thinking about what you’re trying to achieve. reply loceng 13 hours agoparentprevCare to share examples? Accelerators can be a great fit for some people at a certain stage of their life, development, or understanding of these systems; it really depends on what your end goal is too. reply levocardia 14 hours agoprev\"You shouldn't take a ~5% chance of getting $500,000; you should take a 100% chance of losing $375!\" reply JaggerFoo 13 hours agoprevPoints for using the term \"Non-Ergodic\". I wish it were used more often. The post is well written, mostly cohesive, and with a willingness to express ideas counter to the status quo. reply hahahacorn 14 hours agoprevIn each section I got maybe 1/3rd of the way through before reading something that was just so wrong and exposed his lack of knowledge and skipped to the next section. Which is a bummer! I got excited by the title but disappointed by the content. I’d love opinions from people who have actually gone through YC and their arguments against applying to YC. reply SeattleAltruist 14 hours agoprevTL/DR: Don't bother trying to go big, even if you believe in your idea, because difficulty. Instead, pay me $375 for performatted business platitudes and chat with a bunch of randos. reply flappyeagle 14 hours agoprevThis dude is posting for clout or something. In one thread he gets into histrionics about an overfunded VC hardware startup getting a bad review and in another thread he criticizes the existence of such companies. reply lelanthran 15 hours agoprevTBH, if you're already holding a product in your hands, that you created, it's really hard to tell if you're holding an anchor or lightning in a bottle. Using VC money to make that determination is kinda a no-brainer. OTOH, if you're still in the exploratory phase (I am), is it wise to tie yourself to the mast of a ship that is already sinking? In the latter scenario the smallbets.com proposition looks better. reply kayo_20211030 13 hours agoprevNothing is more tedious that a po'd pedant. OP is a person who fell in love with the word \"ergodic\", and just both couldn't let it go; and even then uses it badly. If you want to say \"unfair\", or \"inequitable\", just say that. It's simpler and more honest and doesn't make you look like a jargon spouting smarty-pants. What on earth does \"non-ergodic\" even mean to a normal reader? Even after that, OP buries the lede. Put what you want to say in the first paragraph. Otherwise everyone is so bored by the time you get to it they just don't care. It's Twitter, not the New Yorker. Any New Yorker editor worth their salt would have reversed it. Apply or don't apply to YC, but don't base your decision on whether it's ergodic or not. Who on earth knows what that means? reply DVassallo 13 hours agoparentIt's entirely different than unfair. Communism would be a fully-ergodic system, but it's also very unfair and undesirable. reply computerdork 13 hours agoprevThe tweet is saying that only 1.25% make it to a billion, but a startup doesn't need to make 1 billion to be a success. How about one that makes 100 million? or even 10 million? At these levels of revenue, if the company is sold, the founders will still be rich (not ultra rich, but have a million in the bank or more). And, for ycombinator, the number of \"startup exits\" is: 351 (according to this page: https://explodingtopics.com/blog/startup-stats). Am no expert at startups, but this may mean that out of the 4000 that when through ycombinator, ~9% where valuable enough for investors to take profit. That sounds like a decent success rate for me (think I read that the success rate of ycombinator startups is similar to that of the rest of tech). reply primitivesuave 14 hours agoprevThis is a very strange use of \"ergodicity\", which would be more suitable for an essay titled \"How every possible startup idea will be tried at least once\". Using a probabilistic term to describe deterministic activity (i.e. choosing where to dig, checking the soil, giving up if no treasure is found) indicates to me that the author is reaching for some high-minded mathematical justification for his perspective. The native ad for a $375 course further erodes credibility, and his recent statement that Youtube product reviewers like MKBHD should feel compelled to protect the fortunes of a VC funded company appears quite hypocritical [1]. 1. https://twitter.com/dvassallo/status/1779928354118111278 reply DVassallo 13 hours agoparentI never said or implied in any way MKBHD or other reviewers should feel compelled to protect the companies they review. reply alphabet9000 12 hours agoprevyou shouldn't join that guy's $375 thing. whoa, that was easy, can you believe i just saved you $375? if you're feeling generous for my saving you a WHOPPING $375, i only ask for a small small tip request of $3. win win for both of us. i saved you some cash, and now you've got $372 in your pocket. :handshake: reply jjmarr 11 hours agoprevLet's just assume the facts are as he says in the most charitable interpretation. You either have a 1.25% chance of a $1 billion company and a 98.75% chance of a $0 billion company. The expected value of a 1.25% chance of $1 billion is $12.5 million, so treat that as an (unrealistic) lower bound on your company's valuation. There's no real explanation of why that isn't a good deal for an entrepreneur. You don't get to keep all that, but $500,000 dollars from YC for a 7-10% stake of companies (as others have said) with an expected value of $12.5 million is a deal that gives YC a maximum of $725,000 in value, given that 10% of $12.5 million is $1.25 million. The question to ask is whether being part of YC adds $725k in value to your company; i.e., is it the cause of those startups being worth that much? Based on what others are saying in the thread, this seems to be true, but I'd love to know if people who were a part of YC felt like the assistance was worth that much. My point is that it's difficult to believe that the average wealth of the entrepreneur is worse off by joining YCombinator even assuming all of the facts proposed in the thread are true. reply 7e 11 hours agoparentYC adds almost no value. They capture all of the startups that would have been founded anyway (and many that wouldn't, because they are idiotic ideas) and tax them. Then, inevitably, some of those hit it big and YC takes the credit. Does the doctor that delivered Steve Jobs at the hospital get to claim credit for Apple? They don't. But YC does. They're a doctor that has talked their way into delivering all the babies in the city, and taking a tax on their future income as adults. The average wealth of the entrepreneur is actually maximized by having them go into a career in FAANG and not launching a startup at all--or launching a startup from a stronger position with a real VC that values their company at more than $7M. A $7M valuation in today's tech world is an absolute joke. It's scandalous the exploitation that's going on here. reply worik 11 hours agorootparent> YC adds almost no value For some definition of \"almost\" I am not in the market, but if I were 22 in San Fran doing what I was doing when I was 22 in Auckland, I can see tonnes of value Networking and mentoring mainly reply Animats 13 hours agoprevIf you just want to make money, one of the most successful strategies is to learn how to manage, via MBA or otherwise, find a company that's losing money primarily because of management problems, and take it over on some deal where you get a cut of the improvement. Turnarounds can be very profitable for all concerned, and the success rate is reasonably high, maybe 50%. reply andrewfromx 14 hours agoprevMy first thought looking at https://smallbets.com/ is to clone it and populate it with AI bots pretending to be the other members. In fact, if my little AI clones are good, it's not even that unethical. Long as they provide the same $375 worth of value and the customer doesn't want a refund? reply neilv 12 hours agoprev> One of the bad learnings you get from YC is that there’s a formula for success, and it looks like this: First you do some brainstorming. Then you come up with a good idea that can scale to a billion dollars What if we have an unusual social media idea for which the reason it works also means it stops working once it's worth a billion dollars? (Specifically, once it's that valuable, it's too high visibility, and is hit with too much adversarial manipulation, destroying what was most valuable about it. Then it's just a generic property, little different than the competition.) (Obviously, HN itself has had better success and longevity than most, despite its rising visibility. OTOH, the Reddit front page seemed to get taken over ages ago, and then they still had to chill for more than a decade before IPO.) reply hintymad 13 hours agoprev> But that’s not how you find business opportunities in the real world. You can’t just say I’m going to pivot, and suddenly a good opportunity lands on your lap from heaven. You get good ideas by embracing randomness for a long time, until something looks like it has a fighting chance of paying off. The pivot idea you were forced to come up with is extremely unlikely to be one. This is in general true, but I fail to see how that's against the idea of pivoting as advocated by YC. We find out interesting pivoting ideas by doing, and one of the most effective way is to get ideas by exploring adjacent areas while we invest deeply into an area the we are are interested in. That is, we discover by doing. Is that what the startups do? reply balls187 12 hours agoprevThe key issue I take away is the success rate: 1.25% and assuming that fact is accurate, I suggest the following: If the goal of your company is to be a billion dollar business, then I would say that just applying for and getting into YC is enough, and you should not join YC. YC apparently is very good at picking winners, but not necessarily making winners--being selected by YC means you have a higher chance of of succeeding, but attending YC does not. However, I would expect that there are tangible and non-tangible benefits for going through YC that are not readily apparent when looking solely at becoming a billion-dollar business, and those would be the reason to join YC should they matter to your startup. reply throwaway5959 11 hours agoprevWhy does this guy keep getting so much attention on Hacker News? Isn’t this the same guy that tried to shame MKBHD (an actual small business owner/founder)? He’s clearly just engagement farming, probably for his “course”. reply dzogchen 14 hours agoprevIf your YC startup fails you can possibly still pivot or try again. I don’t understand his thesis at all. reply wilde 13 hours agoprevSame engagement baiter who said product reviewers should all be shills. Nothing of value here. reply alecco 13 hours agoprevWith all the retail investor mania of lately and so much money on tech people, perhaps we could do a sort of startup Olympics with bets. Founders and teams could publish their terms and what they offer in stock. It could be a decreasing function of price of shares with early birds getting cheaper shares. And open source due diligence. Make it a big show to get free promotion for the startups and opportunities. But the SV VCs would probably try to sabotage it and Wall St. would probably throw their SEC attack dogs. Perhaps it could thrive in a less compromised country like Singapore or Switzerland. reply tlogan 12 hours agoprevThis post seems to hint at a larger conversation about the accessibility of opportunities like YC for people from different economic backgrounds. While it might be easier for those with financial safety nets to take entrepreneurial risks, perhaps there should be more discussion on how to make such opportunities more inclusive. How can organizations like YC ensure that everyone, not just the financially privileged, have the chance to innovate and succeed? reply ec109685 14 hours agoprevPrevious discussion (coincidently 6 months ago): https://news.ycombinator.com/item?id=37869760 reply siliconc0w 13 hours agoprevGetting into YC is probably one of the few cases where it does make sense to go the traditional VC route. It's like getting into an elite-level school. Most colleges are crazy expensive to the point that the value proposition of a degree starts to look tenuous even for STEM degrees. However a degree at an elite school still has some currency plus benefits like a network of equally hard working, smart, and likely well connected peers that is pretty valuable. reply eightturn 13 hours agoprevI sell onions on the internet, and I approve this message. reply dinvlad 8 hours agoparentVidalia ftw! reply DVassallo 13 hours agoparentprevI'm waiting for your onions! reply the_real_cher 14 hours agoprev7% of your company for 500 Grand startup money a Network on a moonshot idea? that's unbelievably good! If he finds this bad... then he finds venture capital and bank lending bad in general. reply holoduke 14 hours agoparentGood? My two startups were 2m for 5% shares. Pretty standard. reply jameslevy 13 hours agorootparentFirst money in at a 40M valuation? Assuming this is pre-seed, that seems like the investor is taking on a lot of risk based on the distribution of outcomes. reply dazh 13 hours agorootparentprevMy understanding is that the standard deal is more like $2MM at 20%. Raising that at 5% is an insanely good ideal. reply the_real_cher 13 hours agorootparentprevThe fact that you can get such large amounts of money for such a small amount of equity in general is good. what were your startups? reply whiterknight 14 hours agoprevHis comparison to the military is weird. It’s well known that from a strict money to time exchange it’s not a great deal. But the target market is also 19 year olds who don’t have social capital, not Harvard undergrads. So yes, improving your future options and skills is a greater value than cash. That’s true of YC too. reply 999900000999 11 hours agoprevThis is an ad. But the bigger thing is most of us don't actually have investment ready products. I'm echoing my comments tester, but if a VC offered me 2 million today I'd have no idea what I'm done. reply ch33zer 11 hours agoprevHe started by getting the deadline date wrong so not off to a strong start Actual: Apr 22 at 8pm PT Him: The deadline for YC S24 is tomorrow (tweet from 11:25 AM PT Apr 20, 2024) Probably because this is just a repost as mentioned in sibling comments. reply andrewmcwatters 14 hours agoprevThis post is an ad. reply ttul 14 hours agoprevJoin a CEO group and set up a cross-holding of stock between the companies in the group. Now you’re all digging for gold at the same time. You don’t have to be a VC and you also don’t have to be that person with 26 lifetimes to spare. reply StressedDev 13 hours agoparentThe problem with this is VCs give you money, and some (like YCombinator) also give you advice. The CEO group gives you no money. Also, how do you prevent parasites from entering your CEO group? reply petersumskas 12 hours agoprevThis is from the “honest reviews are unethical” whinger. reply xpe 12 hours agoprevMy goal in this comment is to vet Daniel Vassallo's section on ergodicity. I follow and agree with the setup: > First, you have to understand a very important concept: in some systems, what’s best for a group is not necessarily what’s best for the individuals who make up the group. In other words, the total wealth of a group of people could be increasing, while almost everyone making up that group could be seeing their wealth diminish. But do the following sentences follow? Do they even make sense? > When this happens, we say we have a non-ergodic system. If the system was ergodic, what’s happening to the collective would also translate to all individuals. I'm pretty sure these sentences don't follow. I am struggling to think of even a charitable interpretation that would make such a connection. This is not out of a lack of interest -- I would like to see a connection. I think I'm more willing than many to find such connections; I have studied EE, physics, SwEng, CompSci, economics, and more. But, alas, I see no connection -- and not even a plausible path to explore to find one. So, after reading only this much of Mr. Vassallo's writing, my BS-detector alarm bells are ringing. I'm going to go read https://plato.stanford.edu/entries/ergodic-hierarchy/ instead. Please let me know if I'm missing something. I am genuinely trying to be rational and open to good arguments. I admit that I'm developing a gut-level distaste for Daniel's writing, but I don't want this to unfairly bias me about a possible insight on ergodicity here. P.S. Maybe even though Mr. Vassallo's writing is a stretch, there is a reasonable point to be made? Could someone make it succinctly without the baggage of his larger claim? reply MichaelDickens 12 hours agoparentI recently spent some time looking into ergodicity and my conclusion was that people who write about \"ergodicity\" in a non-academic setting generally have no clue what they're talking about and they're just using the term to make it sound like their reasoning is more rigorous than it actually is. reply lettergram 14 hours agoprevWe applied to YC and were rejected, which was probably good for us (less dilution, etc.). That said, you get some pretty unfair advantages being a YC company. My cofounder has 20 years in the industry and I have a solid tech background with a track record of success. We had leads at the time; a demo product, etc. We raised without YC and have been doing well. One of competitors (little experience, no product, no customer leads) got into YC. Immediately they were considered credible. They are getting effectively the same interest with nothing and that’s what YC offers. They are good at branding and offer you their brand, advice and network. All of which is well worth the cost if you have nothing to start with. Now ultimately the best product will win and I’m confident in ours. That said, for someone starting out with little network or connections it’s worth YC. reply smallerfish 9 hours agoparent> Now ultimately the best product will win and I’m confident in ours. That's unfortunately not true. The better funded company has an advantage because they can get ahead in GTM. The further ahead they get, the harder it will be to catch up. If you find yourself in this situation and they offer you a merger, strongly consider it. reply worik 11 hours agoprevI found that refreshing. Unfair, of course. They are flogging their own very different product I really take to the point of moderate success. If I am going to have huge billion dollar success it is going to have to find me because I seek satisfaction, and I am satisfied with much less I am a worker. I like working. I like working for other people, I like getting paid a nice big fat salary. I would like it bigger! But I like my job, i like my boss, I like my work, mostly, and that is a fabulous way to live. Satisfaction! Being a billionaire does not sound any better reply Havoc 12 hours agoprevThe guy seems to like hot takes to get attention reply fizx 14 hours agoprevEhhh, there's pros & cons, just like anything else. Maybe some VCs want you to dig to the bedrock, but not all, and everything in life is a negotiation. I look at YC as grad school for entrepeneurs. You'll learn a ton, build a network, and have a good time for a while. Unlike grad school, there's direct upside, and you won't have to take out loans or live off the lab stipend. Now would I go to grad school at 40? Maybe, but the calculus is certainly different than at 20. YC is clearly good for something. You really have to figure out how to make it work for you. reply ChrisMarshallNY 13 hours agoprevI'm not really a fan of posting rants. Not because I don't have about 10,000 foaming-at-the-mouth rants in me, but because I don't feel it's professional, which is an indicator that I'm not very \"new school,\" in my approach. It's actually pretty on the point, but I'm not sure that I'd get \"Don't Apply to YC\" as the result of reading it; just that I should calibrate my expectations and approaches. I probably would find other alternatives, if I was a \"lots of small squares\" digger, but that's me. I'm not a rich serial entrepreneur, so I guess my opinion isn't particularly relevant. This part stuck out to me: > The second bad lesson from YC is the focus on the upside. Because if there’s any formula for success in business, it’s to focus relentlessly on staying in the game rather than on hitting it big. Focus on the downside, and let the upside take care of itself. I think that's critical, but seldom actually practiced, these days. I have a friend that is a marvelously smart person, who has created a great product, that he is planning to market and sell for thousands of dollars. It's likely that he'll get it. But the real test is what happens after the sale. We need to determine things like service, support, parts, and reliability. Things shouldn't break easily, and if they do, we should be prepared to handle the calls. They may be irate calls, if the device costs a lot of money. That's where I have about 35 years of expertise. I've spent my entire adult life, making things that last, and can be serviced, if they break. 26 of those years, were at a top-shelf company, producing some of the world's finest optical equipment (expensive as hell, but worth it). A lot of the software we wrote, was to work around hardware deficiencies, or add service points. So I was giving him tips like \"What happens if you get dirt in this fan?\", or \"How do you test the firmware?\", or \"I'm not sure that part should be printed with that kind of plastic,\" etc. I was politely told to fuck off, and stop being a \"dream-killer.\" So I keep my mouth shut, and I won't say \"I told you so,\" when things go south. Instead, I'll help him to clean up the mess, because that's what friends do. reply andrewstuart 13 hours agoprevSome of his point might be true, BUT: If you have a great idea, if you are onto something, then the best chance of success comes from having resources to execute, credibility and to share the endeavor with others who are literally invested in the outcome. reply orangesite 15 hours agoprevThis is the nub of the matter. I could have saved myself a decade. The only thing from YC that survived our relationship was the incorporation of the company. I still think there's a massive gap in the market for an ergodic startup community where the young ambitious people tell the Gen-X's to go fuck themselves with a police baton and instead enjoy the benefits of pooling the risks & sharing the rewards. reply MattGaiser 15 hours agoparentI imagine the problem is how do you pool the risk without being able to assess with confidence who will succeed and who will fail? And how do you prevent freeloaders who don't expect to succeed wanting a share? And if you could make that determination accurately, why wouldn't you use it to avoid pooling risk among all but an extremely small pool? reply richrichie 8 hours agoprevPredictably, gets flagged! reply graycat 11 hours agoprevAfter reading the OP: (1) Business Is Okay: For the neighborhood I grew up in, a big fraction of the houses were owned and occupied by families of owners of businesses. (a) Drove into the country, met with farmers, evaluated and bought their cotton, and sold it to, say, companies making medical bandages. (b) Bought junked auto parts, renovated them, sold them to auto parts stores. Later bought, developed, sold real estate. (c) Had a warehouse next to a rail line, got big shipments of beer, sold smaller quantities to bars and convenience stores. (d) Sold, installed new tires to people with flat tires. (e) Ran a trade magazine for the trucking industry. For more, not in my neighborhood but not far away, (f) opened a sit-down seafood restaurant, built relatively modest houses, started a national motel chain. (g) Sold jewelry to jewelry stores. I dated the owner's daughter -- they lived in a nice house and neighborhood and helped a cousin -- prettiest human female I ever saw, still in love with her. (h) Bought medium trucks with no beds, installed tanks, custom, e.g., for water, fuel oil, gasoline, .... For this last, I tutored the owner's son in high school math -- their house was really, REALLY nice! It does appear that in the nicer neighborhoods, the fraction of house owners that were successful business owners was high. Conclusion: \"The business of America is business\". To get important things done, America wants and needs successful businesses. The successful businesses are not flukes. Necessarily there WILL be successful businesses. The business owners were ordinary Americans, not geniuses, not venture funded. (2) Key. To have a successful business, build something people want, like, need, and don't have (loose quote from some YCombinator source). (3) Opportunity. Exploit the sudden revolution in productivity in our civilization, i.e., computers and the Internet. IBM helped automate the routine work of the accounting departments of major businesses -- good PI (productivity increase). PCs killed off the typewriters -- big PI. Email, big PI. Web, huge PI; also great for information, knowledge, entertainment, maybe soon making dense cities less important. (4) Cheap. In both historical and even absolute terms, current computers, the Internet, and the digital phone network are cheap, really cheap, just dirt cheap. E.g., I got new 8 core processor with a standard clock speed of 4.0 GHz for $100 -- can spend more than that on a family restaurant dinner; can come close on flowers for Valentine's Day. What are the recent numbers, 18 trillion bytes in a standard 3.5\" size??? Big, big, big, big, huge PI steps up from history back to punched cards, ledgers, quill pens, etc. Are the PI opportunities already fully exploited? What to do? Get a good computer with good software tools, a good Internet connection, write some software that will let some people get something they want, need, like, etc. faster, cheaper, better, easier, etc. and get users and/or customers (e.g., advertisers), and revenue. OWN the software. And, go ahead, own 100% of the business, as just an LLC (limited liability company): Soooo, \"Look, Ma, no BoD meetings!!!\". Can't be fired. Have minimal involvement with lawyers. VCs? Do they write the software for/with you? Nope. Do they really know what way to please a lot of people that will make a successful business? Nope. Do you need their $500 K to pay for your computer(s) and Internet connection? Likely not. Can a sole, solo founder be successful? Example 1: Plenty of Fish, one guy, sold out for $500+ million. Central Point: For better financial security, you need to own some/all of a successful business, AND the economy MUST have a lot of successful businesses. Every major US city has lots of such successes -- they are NOT rare. reply paulcole 13 hours agoprevS-tier move by YC to promote this nonsense to the front page to make the pro-YC post from the other day look less sketchy. reply SeattleAltruist 14 hours agoprev [–] TL/DR: Don't try to go big, even if you believe in your idea, because its hard. Instead, cough up $375 for templated business platitudes and chat rooms with randos. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discourages applying to Y Combinator (YC) for S24, cautioning about potential undisclosed drawbacks despite the funding and support benefits."
    ],
    "commentSummary": [
      "The discussion covers the pros and cons of participating in Y Combinator (YC) as a startup founder, including aspects like pivoting, financial outcomes, program value, and success rates.",
      "Different viewpoints on YC's effectiveness, venture capitalist funding, online courses, and ergodicity in forecasting startup success are explored.",
      "Various topics such as networking, AI startups, engagement farming, online interaction challenges, and validation-seeking behavior are also discussed in the conversation."
    ],
    "points": 425,
    "commentCount": 175,
    "retryCount": 0,
    "time": 1713637592
  },
  {
    "id": 40097375,
    "title": "Study Reveals Grading Bias Based on Names at U-M",
    "originLink": "https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/",
    "originBody": "Skip to content Main Menu Home Topics All Headlines Academics Arts & Culture Athletics Bicentennial Campus News Human Resources Information Technology Obituaries Police Beat Regents Entrepreneurship & Innovation Global Engagement Health & Medicine History Public Engagement Research State & Community Sustainability Features Multimedia Features Faculty/Staff Spotlight Events This Week in U-M History U-M Heritage Old School U-M In the News Subscribe Advertise April 21, 2024 X (Twitter)#URecord RSS Feed Search for: Search April 16, 2024 Study: Alphabetical order of surnames may affect grading Byline: By Jeff Karoub Michigan News Topics: Research Social Share on: Share on X (Twitter) Share on Facebook Knowing your ABCs is essential to academic success, but having a last name starting with A, B or C might also help make the grade. An analysis by University of Michigan researchers of more than 30 million grading records from U-M finds students with alphabetically lower-ranked names receive lower grades. This is due to sequential grading biases and the default order of students’ submissions in Canvas — the most widely used online learning management system — which is based on alphabetical rank of their surnames. more information 30 Million Canvas Grading Records Reveal Widespread Sequential Bias and System-Induced Surname Initial Disparity (Submitted for review) What’s more, the researchers found, those alphabetically disadvantaged students receive comments that are notably more negative and less polite, and exhibit lower grading quality measured by post-grade complaints from students. “We spend a lot of time thinking about how to make the grading fair and accurate but even for me it was really surprising,” said Jun Li, associate professor of technology and operations at the Stephen M. Ross School of Business. “It didn’t occur to us until we looked at the data and realized that sequence makes a difference.” Li co-authored the study with doctoral students Jiaxin Pei from the School of Information and Helen (Zhihan) Wang from Ross. It is under review by the journal Management Science. The researchers collected available historical data of all programs, students and assignments on Canvas from the fall 2014 semester to the summer 2022 semester. They supplemented the Canvas data with university registrar data, which contains detailed information about students’ backgrounds, demographics and learning trajectories at the university. Although the data is from U-M, the researchers say the findings can be generalized across institutions and courses. They are driven by a common design issue of learning-management systems — the default setting of ranking students’ assignments alphabetically by their names. Their research uncovered a clear pattern of a decline in grading quality as graders evaluate more assignments. Wang said students whose surnames start with A, B, C, D or E received a 0.3-point higher grade out of 100 possible points than compared with when they were graded randomly. Likewise, students with later-in-the-alphabet surnames received a 0.3-point lower grade — creating a 0.6-point gap. Wang noted that for a small group of graders (about 5%) that grade from Z to A, the grade gap flips as expected: A-E students are worse off, while W-Z students receive higher grades relative to what they would receive when graded randomly. The researchers said such observations confirm their hypothesis that it’s the order of grading that leads to the initial gap in grades. A 0.6-point difference might seem small, but such a disparity did affect students’ course grade-point averages, which negatively influences opportunities in their respective career paths. “Our conclusion is this may be something that happened unconsciously by the graders that’s actually creating a real social impact,” Wang said. Pei said the idea for the study came up during a discussion he had with Wang in which they were talking about their research: Wang studies educational technology and he studies artificial intelligence. He observed that a fundamental task of machine learning is data labeling, also a sequential task that can be long and tedious, but one that is randomized. It got them thinking about educational systems like Canvas and led to some pilot studies to see if there was any disparity among grades based on the amount of time spent in the task of grading. “We kind of suspect that fatigue is one of the major factors that is driving this effect, because when you’re working on something for a long period of time, you get tired and then you start to lose your attention and your cognitive abilities are dropping,” Pei said. The researchers note the option exists to grade the assignments in a random order, and some educators do, but alphabetical order is the default mode in Canvas and other online learning-management systems. One simple fix would be to make random order the default setting. They also suggest academic institutions could hire more graders for larger classes, distribute the workload among more people or train them to be aware of and lessen the bias while grading. Li, Wang and Pei have been sharing their research at conferences and it’s been positively received — many are impressed by their work although it confirms suspicions many harbor. One reaction in particular stands out to Li, no doubt an information-age wrinkle on “the dog ate my homework” excuse. “A college student emailed us afterward asking us to share the paper with him,” she said. “He mentioned that his last name started with W. He’s going to tell his parents it’s not because of him — it’s because of his last name.” Tags: Canvas grading School of Information Stephen M. Ross School of Business Comments Edward Williams on April 17, 2024 at 8:02 am In addition to grading in random order, I think it helpful to thoroughly prepare an answer-&-grading key BEFORE opening any assignments or tests for grading. For example, “4 points for each part of this problem;” “-2 points for this specific error.” John Dinkel on April 19, 2024 at 3:38 pm So is this proof that students’ work should not be graded in randumb order! Leave a comment Please read our comment guidelines. In order to leave a comment, you must log in with your U-M credentials. Log in with U-M Weblogin Today's Headlines Engineering professor Kamal Sarabandi receives Ellis Island Medal of Honor Twelve U-M faculty members named as AAAS fellows Support resources, insurance available for international travelers Chemicals stored in home garages linked to ALS risk Law student wins Soros Fellowship for New Americans Senate Assembly passes resolution regarding pay‑raise schedule Dearborn, Flint campuses announce commencement schedules More Headlines About Contact Past Issues Office of the Vice President for Communications © 2024 The Regents of the University of Michigan Michigan Daily Michigan News Michigan Public Michigan Today Health System News Athletic News",
    "commentLink": "https://news.ycombinator.com/item?id=40097375",
    "commentBody": "U-M finds students with alphabetically lower-ranked names receive lower grades (umich.edu)376 points by cebert 20 hours agohidepastfavorite264 comments lqet 19 hours agoI work in academia. When we grade exams, the order of the exams on the stack is the order in which they were collected in the room (people can sit wherever they like). For grading, we are usually 5 people in a single room, and everyone grades a specific exercise for consistency. The exams are getting shuffled heavily, with everyone just grabbing stacks, looking for exams where \"their\" exercise was not yet graded, and taking them out. So basically, the order in which we grade exams can be considered random. However, I also grade weekly exercise sheets during the semester, and these are committed into a repository, where each student has a folder that... begins with the first letter of their first name. Everyone I have ever worked with acknowledges that you have to shuffle the order in which you grade these submissions each week, for fairness. Several effects come into play: (1) your are usually less tired at the beginning, (2) your mood gets better during the last 2 sheets because you know you are done soon, (3, and crucially) at the beginning, you have not yet seen all the common errors / developed a \"feeling\" for them, and you might thus miss them in early submissions, but spot them immediately in later submissions. Another alphabetic effect: In elementary school, my name was on top of the list of students in my class. I remember that I often had to do some special job simply because I was the first name on this list (for example, carry a group ticket when we visited some museum, keep track of something, be the first at something where nobody wanted to be the first, with everyone watching, be the first to be graded in PE, again with everyone watching, etc.). As a fairly shy kid, this already annoyed me in first grade. reply cvwright 18 hours agoparentMy strategy was to, like you said, grade problem by problem. Then for each problem, first find all those who got full marks. Then group the others into piles based on what mistakes they made. This ensures that everyone who made the same mistake(s) gets the same grade. It also tends to shuffle the order of the exams after every problem. Obviously you don’t need this strategy for simple multiple choice questions, and it’s probably also not a great fit for long-form essays. But it worked great for technical short answer problems in CS and security. reply bobbiechen 17 hours agorootparentWhen I was a TA at CMU, we used Gradescope https://www.gradescope.com/ for this. Every exam would be scanned and divided into problems (based on a predefined template - fixed page space for answers). Then, each problem was assigned to a TA. Either there's a predefined rubric, or you create it as you go (-1 point for mistake X, half credit for mistake Y, etc.). There's a pretty slick interface where you just read the answer, and use keyboard shortcuts to apply the relevant deductions. It still has the issue that every time you change the rubric, you'd need to go back and re-do previously-graded instances of that problem. But it was way faster and (equally important) less tiring. reply Tijdreiziger 10 hours agorootparentThere’s also open-source software that does the same job at TU Delft: https://zesje.tudelft.nl/ (disclaimer: I briefly worked on the software for my bachelor’s thesis) reply jcla1 18 hours agorootparentprevThis sounds like an organisational nightmare to be honest. You'd be going through the pile of exams multiple times (at least twice) and what do you do if there are multiple mistakes that are common in a single exam question? Also: if you're sorting into \"mistakes piles\" for single exercises, how can you parallelise marking of separate and independent questions? reply cvwright 18 hours agorootparentTeach at a broke public university, and you never have to juggle huge teams of TAs. reply kkylin 17 hours agorootparentEven at top-notch universities (public or private), when I talk to retired faculty, grading almost always comes up as a reason they don't want to teach anymore. [Edit: not disagreeing with your point.] reply bobthepanda 13 hours agorootparentNot only is it generally time intensive, you are also subject to lots of tiring back and forth with some students about their grades. No grading is perfect, but there’s also some undercurrent of an attitude that students have paid to be there and are entitled to a certain grade. reply mschuster91 11 hours agorootparent> No grading is perfect, but there’s also some undercurrent of an attitude that students have paid to be there and are entitled to a certain grade. Given that students have taken on hundreds of thousands of dollars in debt that they'll have to repay no matter what and on top of that a lot of jobs being completely out of reach these days without an academic degree (that for fucks sake isn't remotely required by virtually all jobs requiring it!), that's completely understandable. Want to fix higher education? Bring the hammer down on companies abusing it as a proxy for legally discriminating against classes of society that are closely correlated with poor academic outcomes. Academic education should be reserved for the best of the best of our youth, and it should be fully paid for by the government, not simply another hurdle to pass to get a job that pays barely more than flipping burgers. reply bobthepanda 8 hours agorootparentI think it is rational that students can feel entitled to that. I also think that the vast majority of poorly paid, non-tenured professors and other teaching staff don't love being the targets of this harassment, since it's not their fault and largely out of their control, and it's not like they're getting the bulk of the tuition money. (That mostly goes to administrative expenses and sports programs.) Heck, most adjunct faculty are often paid below minimum wage and qualify for food stamps. reply bsder 4 hours agorootparentprev> Given that students have taken on hundreds of thousands of dollars in debt that they'll have to repay no matter what and on top of that a lot of jobs being completely out of reach these days without an academic degree (that for fucks sake isn't remotely required by virtually all jobs requiring it!), that's completely understandable. Would that my students were this engaged before the exam. Guess which students show up the most often for office hours? ... yeah, the ones that are getting the best grades. If my students spent half as much time learning the subject as arguing with me about grades, they would be getting a higher grade than the one they are arguing for. reply jcla1 16 hours agorootparentprevI do (I'm a mathematican). We are usually between 4 and 10 people marking an exam with anywhere between 50 and 600 participants. reply kkylin 17 hours agorootparentprevOnline tools like Gradescope make this a little less painful (but still painful), but sometimes it's what's needed, especially on problems that are a little open-ended. reply underdeserver 16 hours agorootparentprevSibling comment already said so, but I want to emphasize - this requires two run-throughs (at least). When I was grading homework, it took about 5 hours a week per class per run through. They didn't pay me enough to make sense for it to be 10 hours. reply raydev 13 hours agorootparentA second pass wouldn't necessarily take the same amount of time, especially if you note the issues/concerns on your first pass. reply underdeserver 13 hours agorootparentTrue, but the overhead is large. I graded into linear algebra and intro calculus, so there were a lot of students - I think 150 or so - and most of them were wrong. Graders know that wrong homework takes much longer than correct homework to grade. It's correct? Full marks, move on. Is it wrong? Well, how wrong is it? Did they make a bad assumption, but followed it through to its conclusion? Did they forget a minus sign? Or is it complete hogwash? So it might not be 10 hours, but still would be around 8 hours. And that's still too much. reply ska 15 hours agorootparentprevFor final exams, we use to mark across all sections of a course (so for 101 type courses, this can be hundreds to 1000s of papers). Get all the profs and TA's together, break in to groups taking one problem or set of problems. Then you random sample (each group takes a stack) to get a feel for the 'typical' errors, once that's done - you are a machine going through the stacks. Every once in a while (not that often) you run into a novel error or approach, and the group discusses. reply nextos 17 hours agorootparentprevMy CS school implemented OCR test sheets, with some exceptions, and equivalent strategies, such as test suites and benchmarks for programming assignments. This was done to avoid subjective grading, as it was a big issue even in well-intentioned cases. Often, you still get big problems, but the set of solutions is small. It's always three options plus a fourth option (none / all). If you make a mistake you score negative points. It's not perfect, sometimes wording is ambiguous and it's unclear whether you need to tick the fourth catch-all option, but I found it better than the alternatives as it removes most arbitrariness from the process, but obviously has other issues. Regular exams often had wildly different grading standards for the same course depending on the class, and thus on the professor who was correcting exams. This was really annoying. reply anticensor 4 hours agorootparentprevAn even better strategy is to have the papers scanned by a double-sided scanner and graded by an AI grader. reply V__ 18 hours agoparentprevA teacher friend of mine always goes through his stack twice. Once to correct all mistakes and a second time to write down points. As you said, once you have seen all mistakes you know how \"bad\" of a mistake it actually is. reply smogcutter 18 hours agorootparent> As you said, once you have seen all mistakes you know how \"bad\" of a mistake it actually is. Crucially, this is not quite what the poster said. It’s not about stack ranking students against each other. Say every paper makes the same subtle mistake, and you only notice it halfway through the pile. Unless you go back through them all, you’ll unfairly grade the later entries more harshly. reply Zancarius 18 hours agorootparent> It’s not about stack ranking students against each other. It's not, but it sort of has that effect, albeit indirectly, and definitely unfairly. reply smogcutter 5 hours agorootparentI think we’re talking about the same thing, but to clarify my meaning: If you weigh the severity of students mistakes (or successes for that matter) in relation to each other rather than to an objective rubric, you’re effectively stack ranking them whether you mean to or not. reply kkylin 17 hours agorootparentprevI'm not a big fan of putting everything in the cloud, but one of the advantages of online grading systems is that it is easier to make this kind of adjustment. The workflow goes like this: make a rubric item for a specific kind of mistake (it takes a little experience to know which mistakes are likely one-off and which ones are likely to be repeated by other students), assign X points, and later if you decide there are worse mistakes, adjust the points and that gets applied to everyone. reply donatj 17 hours agoparentprevIn around the year 2000 I had an essay due that day I had forgotten, and about ten minutes of computer lab time before home room in the morning. I wrote an introduction and conclusion; then filled the remainder with copy pasted chunks of the introduction and conclusion. The thought being at least I’d get a laugh. If anyone had read the thing it would have been clear it was nonsense. I received an 80% with no notes or markup. I have been left wondering for the last 25 years how much student work is actually even reviewed. I work in EdTech and every time we add a feature that requires manual teacher review of student work you will see that some teachers are VERY diligent while others never touch it. reply filipezf 17 hours agorootparentThere was this numerical calculus class at Uni where the teacher forbid us to use the calculator. So I just programmed the integral on it, got the partial steps, and just wrote random numbers to fill the the substeps. Got full grade :D The other case everybody got to pass the class, but after vacation we found the stack of exams completely untouched under a desk. The teacher had a side business to run... reply jtriangle 14 hours agorootparentprevI know a guy who copy/pasted a wikipedia article, in line citations and all, and submitted it for a sociology class and got an A, no notes, nothing. reply mixmastamyk 12 hours agorootparentHe “only cheated himself.” :-D reply wolverine876 9 hours agorootparentThe point is to develop skills and knowledge, so I would agree. Do you disagree? reply mixmastamyk 8 hours agorootparentI agree, but we used to cringe at this saying when young, so funny to bring it back now. reply xorvoid 15 hours agoparentprevWe graded similarly, incidentally, when I was at U-of-M (lol). I don’t think we ever sorted by name so I don’t know if we’d have a bias effect by name unless it’s an implicit bias towards lexicographical esthetics. I won’t deny that grading fatigue can have subjective effects. I always thought we did a pretty fair and objective job. I taught Computer Architecture and we we developed answer keys and grading scales before grading a single test. Of course assigning partial credit always ended up being pretty subjective. Typically though people would error in the same ways and so those would be subjectively identical. I never thought names factored into this much but, to be fair, no one ever collected data… Finally, I guess I’ll admit that I’m probably very biased because my initials as A.B. and I’ve always gotten excellent grades, so… maybe maybe maybe reply spullara 18 hours agoparentprevEverything in this thread just randomizes who doesn't get graded fairly. reply jibe 17 hours agorootparentFor a single assignment, yes. But at least randomization might mitigate the effect across a term. reply karaterobot 16 hours agorootparentprevIs there a better solution? It's not for teachers to be perfect. Since that's not possible, it's not a solution. reply jacoblambda 10 hours agorootparentProbably it would be something like as follows: Have a group of N graders. And a parity of k. Let's say N is 6 and k is 2. Randomly shuffle the assignments and partition the assignments into N groups. Each grader gets assigned k of the N groups such that they share at most 1 overlap with any other grader and each group is assigned to k people. The assignment orders are shuffled for each grader. They mark up and then grade the assignments. Then for each of the N groups, randomly shuffle the group and equally distribute the assignments to the N-k graders. Now each grader reviews the assignment grades/markups (in random order) and assigns a grade based on the k grades/markups from the previous rounds along with a rationale for the grade assigned. From there the student receives the final assigned grade, the rationale for the grade, and the k markups. If they have a complaint they can go to the professor (who then can also see the k initial grades along with everything else) to dispute the grade for the assignment. --- This way each TA only has to mark up (class size * k / N) assignments, and review (class size / N) assignments to assign a final grade (which should take far less time to do than the initial markups). On top of that every assignment has a guaranteed (k + 1) separate eyes on it. And then the professors can serve as an unbiased arbiter while retaining all the context from the process. To take it an additional step further, the professors could sample a random subset of the assignments to verify the markup and grading is going properly. And those reviews/grade adjustments can then be recorded (along with the final grade/rationales) to document how a given TA's grading deviates from the final reviewed grade or the grade the professor assigns. Likewise for a TA's final assigned grade deviating from the professor's. This would allow deviations to be mitigated over time and major deviations to be identified. reply jtriangle 14 hours agorootparentprevNo, the solution is for the scoring to be handled by software that doesn't exist yet. Some things have easy, objective measures of correctness. STEM is mostly this way. Others, your humanities et al, are fairly subjective. You could probably cover most of this with an LLM, and access to a large body of graded material for a given course, provided said material was graded fairly. Generating that data would be time consuming, as, any given assignment would need to be graded by as many people as possible in order to find a fair average. From there, it's simple comparison between your sample work and the presented work. We're probably a decade from this really being viable en masse, but, it no doubt will happen, and for better or worse we'll likely end up with EDUAAS (education as a service). reply jacoblambda 10 hours agorootparentLLMs are not going to be a solution. LLMs have absolutely no concept of truth. And not everything has an objective solution. Even those that do often have a process associated with them and factoring in that work/process is an important part of grading. Reducing that subjective grading process to only objective solutions being right is grossly reductive and disproportionately punishes students who have the process right and understand the material but make small errors. That's exactly what you don't want to do. --- Instead the solution is to make sure each assignment gets multiple eyes on it and in a random order. Then to document biases and trends in biases so that the TAs and professors can be aware of them and mitigate them. It's a process problem that can only be solved by a process solution. Replacing the graders with technology or reducing problems to a binary right/wrong will never ever solve this and in many cases will end up being more harmful than the biases they claim to solve would be. reply coredog64 9 hours agorootparentThe LLM can compile verbose prose down to a short summary. If the summaries of each chunk are consistent, then it’s at least structurally well written. Then you grade the summary itself. reply brewdad 7 hours agorootparentAt that point you are grading the work of the LLM, not the student. reply stevage 10 hours agorootparentprevAutomatically unskew the results after grading based on this finding? reply thaumasiotes 15 hours agorootparentprevYes, you can grade objectively. reply starttoaster 8 hours agoparentprevThis might come off rude on accident, but I mean genuinely without malice. When I'm writing an essay to submit to my professor/teacher, I am asked to make multiple drafts to get a proper end result that is ready to submit. Understanding that educational staff is already often overworked, should I expect _less_ from the person I receive my education from? If you acknowledge that many of the grades I receive are actually not fair to me, and there's an attempt to randomize the order that papers are graded, many of the grades that I received (whether high or low) were done partially (that is to say, the opposite of \"impartially\".) And there's a real concern that in your example where the submissions are committed to a repository that you need to shuffle, that my submission ends up in a similar position in the stack week after week, unless you're actually doing something to ensure my position in the stack is different between submissions. It's probably sufficient in many cases but doesn't guarantee randomness unless the algorithm to randomize submissions takes previous stack orderings into account. reply advael 6 hours agorootparentI know it's not the point of your post, but I think it's worth pointing out that you're misunderstanding randomness (albeit in a very typical way). Although randomness is likely eventually (over a lot of instances) going to be the most \"fair\" way to distribute where your submission is in the order, it does not guarantee that it will always be different, and in fact a \"random\" algorithm that took previous orderings into account would be provably less random than one that didn't It's also worth noting that randomization in a context like this is inherently an imperfect solution to a problem that generally can't be solved perfectly. If we find out that weird ordering biases exist, I think randomization is done on the assumption that many we don't know about could also exist, that there's no clear way to mitigate them completely, and then randomizing the order per-instance is just the best we can do to ensure it's fair (Which, again, won't be perfect. Perfect isn't available) reply jamiek88 8 hours agorootparentprevIt’s simply human nature. Teachers can either lie to themselves and you about it or mitigate it. What more could you possible want from them as humans? reply starttoaster 8 hours agorootparentI somewhat assumed there would be commenters suggesting the human angle as a retort. That's why I prefaced with both \"this is what the teacher expects of me\" and \"understanding that educational staff is already often overworked.\" It just seems to me that the current systems aren't sufficient, and acknowledging that is what leads people to improving those systems. The above commentor suggested what they do in academia as workarounds to what the study showed, and I'm saying even that is not sufficient. It seems like you're agreeing with me, but jumping to their defense with \"people are fallible.\" People are fallible, that's why we build systems to take human elements out of it. Recognizing where humanity has soured something is key to that. reply Fnoord 16 hours agoparentprev> [..] As a fairly shy kid, this already annoyed me in first grade. (I suppose the cons outweighted the cons.) Did you perceive any pros? I suppose one way to do grades is first read through all papers to get an idea of the levels of the students. Though you still have bias/nepotism and such then. Perhaps a teamwork or commitee would work, or teachers swapping classes/schools? I had a French teacher on high school who dropped a pen on list of students and then where it landed that person would get rehearsal. People in mid (waves) were fried. Plus, there is also the issue of certain last names being common in certain cultures, leading to skewed statistics. reply pjdesno 18 hours agoparentprevThere are all sorts of good ways to avoid these biases. I use the same practice described above for paper exams, and grading order for eg question 2 may be affected by score on question 1, but it won’t be affected by name or ID number. If you use Canvas or Gradescope with the default settings, it’s almost impossible to avoid this sort of bias. Worse yet, in Gradescooe you’re strongly steered towards grading with a fixed “rubric” with specific points off for each of N pre-defined errors, allowing grading to be done by TAs with little more knowledge than the students themselves, resulting in scores which have little relationship to the quality of the student answer. reply yeahwhatever10 17 hours agoparentprevWhen I was a TA I always did a second pass to make sure everything was even. It’s not that hard. reply eks391 15 hours agorootparentIt's hard when you are the only TA for 260 students who get 3 assignments per week, you must also hold free hours and you aren't allowed to go over 27 hrs each week so the school isnt breaking federal laws. reply ripjaygn 15 hours agoparentprevWhile this helps the students with names lower down the order, people who are graded later still suffer. reply bandrami 16 hours agoparentprevWe tried a lot of things. What eventually worked was ending grades. You mastered the material or you did not; perhaps a couple of students mastered it with high marks. Obvs this takes an administration that is OK with that, which most aren't. reply dev_tty01 11 hours agorootparentHaving hired a lot of engineers, I can tell you that mastery of material is nothing close to a bimodal distribution. reply dheera 17 hours agoparentprevWhen I saw the title I would have thought that the higher concentrations of Asian names starting with V, W, X, Y, Z would have led to higher grades at that end of the alphabet, and thought that effect would have eclipsed anything else. reply pks016 17 hours agorootparentAnecdotally, the course I grade has this effect (just looking at the average score). I have been grading this course from last 5 years(9-10 times). Last names with L-Z score slightly more than A-L. reply lupire 17 hours agorootparentprevIndian names start with A,B, N. Chinese names also start with, C, F, L. reply gonzo41 17 hours agoparentprevHave you ever thought about just passing out a set of grades on random to random individuals and see how that shakes out. Like totally random and unjustified grades. D minus for an A+ student. A+ for fails etc. Just random chaos. Then just score the final correctly and see the effect? Or just having a Kafkaesque pass fail grade with no feedback for each student relative to their own performance over time with an expected growth rate applied? reply madeofpalk 17 hours agoparentprev> you have to shuffle the order in which you grade these submissions each week, for fairness I don't think this is fair. It's just a more randomly distributed unfairness, rather than by a deterministic factor (like the student's name) 'Fair' would be each student is assessed independently for the work they did, rather than their mark being impacted by how early or late they were marked. reply jcparkyn 9 hours agorootparentI think an important difference is that when you shuffle them, the unfairness stops being correlated across multiple assignments, so the \"aggregate\" unfairness over the course of the semester is much lower. reply shepherdjerred 17 hours agorootparentprevIt would be essentially impossible to have something \"truly\" fair for open-ended questions since humans are stateful. Maybe this is a case that AI could actually do quite well. Manually grade the answers and identify the classes of mistakes. Then hand the classes of mistakes to the AI and ask for it to determine which answers have which types of mistakes. Once you've done that, you just need to associate a deduction for each type of mistake and do some simple math. reply vagrantJin 12 hours agorootparentwhat do you mean AI? you must be joking. reply shepherdjerred 11 hours agorootparentImagine a question: compare bubble sort and quick sort algorithm. Some students might mix up the algorithms, some might give the incorrect computation complexity, some might describe them incorrectly in some way. Manually grade some (or all of) the answers by noting the kinds of things students got wrong (e.g. the above criteria). Then, feed in to ChatGPT (or your favorite alternative) the answer + the categories of mistakes to expect. Here's a simplified example: https://chat.openai.com/share/bf801e12-51d5-4255-9968-bbf91b... reply luplex 17 hours agorootparentprevThere are many notions of \"fairness\", many of which are logically incompatible with each other. In this example, I think it's kind of fair to give everyone an equal chance of being advantaged. You're not hurting anyone specifically. reply gqcwwjtg 17 hours agorootparentprevIs that distinction worth making here? There’s no way to “assess independently” the work of each student without some amount of randomness. But I think that’s okay, because isn’t randomly distributed unfairness just… fairness? reply ryandrake 19 hours agoprevMaybe related, or maybe not, but I remember when I was in K-12 school back in the 80s and early 90s, they would always seat us physically in the class front-to-back by last name. So the kids with last names starting with A-D or so would always be in front, and the kids with last names starting with U-Z would be in the back. For every class. I remember this because many of my friends had last names \"near\" my last name since we were always in close proximity to each other. I vaguely remember, by the time we were in high school, there were definitely more high-achieving kids with A-D last names and definitely more of the troublemakers were U-Z. Was it caused by sitting in closer proximity to the teacher and getting more teacher attention? We'll never know because this wasn't an experiment and there wasn't a control group. reply wongarsu 17 hours agoparent\"students who sit closer are more likely to be high achievers\" might also be the source of most of the stereotypes of people with glasses. It took me years to realize I'm mildly shortsighted, so the first half of school I chose seats in the front half of the classroom to make reading the blackboard easier. Many of my friends had glasses and preferred to sit up front because their glasses didn't fully correct their vision. reply RheingoldRiver 13 hours agorootparentIn a somewhat reverse scenario, when I was in 4th grade (9 years old), I knew 100% that I was getting nearsighted, and I absolutely did NOT want glasses. Fortunately (debatable) we got to pick our seats so I always picked a seat in the very first row, where I could kinda-sorta-almost see what was written on the board if I squinted. And I was also way above my grade level so I was able to fake it pretty well for most of the year even when this started to fail me. My mom insisted on taking me to get my eyes checked about 2/3 of the way through the year and I couldn't fake my way through that, though, so I finally got glasses, but by that point I was used to sitting at the front of the room, so I choose front-of-room seats when possible for most of the rest of my schooling. There's probably some moral here but I don't know what it is. reply smeej 11 hours agorootparentI moved states and schools midway through 3rd grade and was seated alphabetically, in the back, for the first time in my life. The teachers in my previous school knew me to be a model student, so would sit me up front \"to set an example.\" My parents couldn't figure out for the life of them why I was suddenly struggling and thought I was having adjustment issues. I had taught myself to read when I was 3; how could I suddenly be having trouble keeping up? It took longer to figure out because I was only nearsighted in one eye. I was tall for my grade, so as long as the person in front of me to the left was shorter than me or the teacher was writing high enough on the board, I was fine, because my left eye was fine. But when everything aligned just wrong, I was suddenly helpless, because my right eye could barely see clearly an arm's length from my face! It's a hard thing to notice when only one of your eyes isn't working very well, especially when you're 9. reply Ekaros 17 hours agorootparentprevI remember at that age that my sight was going worse quite quickly. So in process there will be many points where your glasses might be slightly lacking. reply nsriv 18 hours agoparentprevI'm a teacher now, and this made me wince. It's exactly how I've been told by my parents that seating worked for them in school (India, 60s-80s) but their grading was done by semi-anonymous roll numbers. reply user_7832 17 hours agorootparentToday I'm 99% sure all CBSE board exams (I think equivalent to A-levels?) are randomized heavily. However I did notice the name's alphabetical order effect in school, albeit in a minor way (folks with later letters were less involved in anything a teacher might need a volunteer for). reply mertd 16 hours agoparentprevCircular shift is the trivial solution. In my high school every row moved up on Mondays and the front row moved to back. Of course you could argue the ones who started at the front on week 1 still has an advantage but it's likely not that significant. reply zdw 19 hours agoprevAs someone whose initials are Z and W, I tend to notice alpha sort a lot. Asking a friend whose initials are A and B about this, it's not something they ever noticed. I haven't noticed a grading/ranking difference, but far more frequently I'll hear that \"oh, we ran out of item/time/etc. before we got to you\", which has made me much more sensitive to issues of planning/organization. reply arp242 15 hours agoparentWhen I was a kid marbles were the big thing, and if you were playing with them in class the teacher would put it in a big glass jar. When it was full he would call out the kids and each would get a handful. I was last in the alphabet; this was already an issue with books we had to read; you could choose which book to read, but it was always in alphabetical order. When it was my turn there were just a few left, and certainly all the popular high-demand ones were gone. Anyway, when it finally was my turn to get my marbles he was all out. When I asked \"where's my marbles?\" he just shrugged and said \"all out\". I must've been about 7. Lots of crying ensued and I think I got some marbles from other kids, but it wasn't about the marbles – not really. I still don't understand how anyone can expect any different result... reply StevenXC 19 hours agoparentprevLike most inequities, those who are in the benefiting group frequently don't realize that privilege. reply sdwr 18 hours agorootparentThey realize (bring form to, make real) them, but don't realize (understand) them reply wryoak 17 hours agorootparentI hate how much I love this worthlessly picky comment reply sdwr 10 hours agorootparentLove me or hate me, you gotta love me! http://sdwr.ca/link/26 reply godelski 16 hours agorootparentprevThis reminds me of cliques. I give them the definition: insight everyone can recite but nobody can act upon. reply DangitBobby 14 hours agorootparentI think you mean clichés. reply godelski 8 hours agorootparentI do. Gotta live swipe and homophones reply dustingetz 18 hours agorootparentprevI for one am glad that I was not born a mosquito, the odds are not in our favor! reply zeroonetwothree 19 hours agoparentprevOutside of school I can’t think of even a single instance of alphabetical sorting of my name (I have a middle letter). What situations are you in that this comes up a lot? reply libria 16 hours agorootparentProbably every single health or wellness \"Find a Provider\" portal lists them A-Z. That's a multi-billion dollar industry. If I was Dr. Zachary Zane, I'd change my name. reply sitkack 17 minutes agorootparentAAA Aches and Ailment reply smeej 17 hours agorootparentprevMy mom made the critical mistake of marrying from first five letters down to last five letters during the police academy, only later to be released from the \"we have to expose you to tear gas so you know how it feels and only use it judiciously\" chamber in alphabetical order by last name. It was 40ish years ago and I still don't think she's forgiven my dad. reply wcunning 16 hours agorootparentprevMy daily standup is run by the order my boss sees the participants in the JIRA board -- My first name starts with W, so I'm last in that list. Makes staying engaged the whole meeting hard... reply macintux 13 hours agorootparentI'm the first in the list, which has some advantages, but I do get tired of always being the first person to throw themselves on whatever grenade is lying around. reply hu3 17 hours agorootparentprevCompany Discord of a client. My name is among the top. It's a remote job so, being frequently visible in that list can be an advantage. reply talsperre 5 hours agorootparentprevThe order in JIRA boards during the daily standups comes to mind. I am sure there are similar examples in other domains that are not software related. reply kaashif 17 hours agorootparentprevI have a middle letter and also don't remember this happening much. We should ask people with later letters if they remember this more. reply godelski 16 hours agorootparentI'm a near last letter surname. It's not uncommon for arbitrary things to be sorted by name, but a ton of official things use surname ordering. There's things that also I tend to seem to be last on where I don't know the sorting method, but I suspect it isn't uncommon for someone to just throw in a sort somewhere (though it's also common to see people do things in a LIFO so disadvantage people who get shit done on time... My apartment renewal does that...). I also remember getting a PCR test in covid where they binned by last name. I can just say I do remember being last in a lot of arbitrary and official things and seeing other friends just get done with it faster and have to waste less time sitting and waiting. reply zo1 15 hours agorootparentprevAs the other poster said, the order of standup and other such things. Having a \"Z\" means that you're usually last, and sometimes people make a point of \"hey let's do it in reverse today\" where I end up being first. I remember when working on joint tasks, by the time it got to me, most of the people that worked with me had already given their updates and details. So when it was my turn, I'd say \"same as A, B, C\", cause they'd given all the juicy details. Other than that, it's pretty straightforward and boring. The world doesn't magically function differently for us. reply IshKebab 17 hours agorootparentprevYeah I don't think it really happens outside school, but school is pretty formative and it happens all the time in school. reply wryoak 17 hours agorootparentIt happens in your phone contacts when you’re deciding who to talk to. You’re starting with your Abrahams, Billys and Changs, probably rarely reaching out to your Xaviers, Yusufs and Zeldas about going out tonight because you’ve already assembled a crew by the time you reach the Mimis, Natashas and Ottos. reply IshKebab 16 hours agorootparentI don't think many people use their phone contact list like that. reply godelski 16 hours agorootparentI wouldn't be surprised. It's very natural. Probably not for that specific use case but if for some reason you are actually going through the list then it's natural reply smeej 11 hours agorootparentPlus, it's common for me to meet someone on a first-name basis and not find out their last name right away. And people's last names change more often than their first names. Phone sorting by first name is the way to go. reply fsckboy 12 hours agorootparentprevjust want to add that in my lifetime that switched from being \"by last name\" to \"by first name\". So, Yusuf Ahmed and Abraham Zigfeld experienced a noticeable shift in popularity that they were totally unprepared for reply thaumasiotes 15 hours agorootparentprevDo none of your friends like or dislike any of your other friends? reply wryoak 6 hours agorootparentProbably but that’s not how they’re organized in my contacts. It’s a list not a graph. reply andoma 17 hours agoparentprevThis reminds me of a funny event when in fourth of fifth grate. When the class was supposed to stand in line we always had to sort based on last name. My last name started with Ö (Last letter in alphabet in the Nordics) so I always ended up last. Then one time, the teacher said something like \"Let's reverse the order today, but wait, we also sort on the first name\". My first name starts with an A so I ended up last in line anyway, much to the joy of everyone :) reply RheingoldRiver 13 hours agoparentprev> Asking a friend whose initials are A and B about this, it's not something they ever noticed. Kinda surprised, my last name starts with C and I was hyper-aware of this and how random it was probably all the way from kindergarten. Being a child and therefore an asshole, I was grateful for my advantage rather than thinking the system was unjust. reply washadjeffmad 18 hours agoparentprevSimilar initials, frequently last in line, and same. I wonder if this was the kiln of my patience and acceptance, or if people who road rage and get frustrated with waits are more likely to have earlier lettered names? reply ambrose2 9 hours agorootparentI really cannot stand sitting in a car in traffic and my last name starts with an A, interesting! reply itronitron 16 hours agoparentprevhttps://en.wikipedia.org/wiki/Zelda_Williams reply CamelCaseName 16 hours agorootparentDo you have something to say about this? I'm confused, why did I read this wikipedia page? reply underlipton 15 hours agoparentprevThis seems like a good example (free of cultural baggage) of how people with privilege often don't notice that they're receiving that privilege. What seemed normal and fair to your friend turned out to be an advantage that they didn't even consider. reply winwang 16 hours agoparentprev(just doing roll call here with initals WW) reply noodlesUK 19 hours agoprevAt my university, almost all of our marking was pseudonymised. We were assigned a random candidate number at the beginning of each year, and that is what went on our important papers/exams. The less important coursework often didn’t bother with this, and used our student numbers instead, but the general idea was the same. We didn’t put our names on any of our work other than our dissertation (and a few trivial assignments that didn’t impact overall marks). It wasn’t that hard to de-anonymise, but it meant that the system had a bit more integrity. It’s a really straightforward system to implement and I don’t know why it isn’t done more frequently. I also think that our VLE sorted assignments by time of submission rather than any identifier. reply trescenzi 18 hours agoparentWouldn't a possible outcome here though be that it just randomly reduces grades instead of reducing them in a way that's related to the students? If the issue is the sorting the random candidate numbers would still be sorted. It solves the problem of bias related to the individual but it doesn't solve the problem of bias related to the way that the submissions are sorted. A random identifier coupled with a random sort order seem like the way to go here. reply ghaff 19 hours agoparentprevUniversity exams, this probably makes a lot of sense. After all, the exam is the exam and whether a student is well-spoken and actively participates in class shouldn't matter for an exam grade. I'm less convinced that blinded conference proposals are a good idea--an argument I've had with various people. If you know based on past experience that someone will almost certainly hit a home run, I'm less inclined to pick a random person without obvious qualifications for the same topic--although just picking friends of the committee can obviously go too far. reply wongarsu 17 hours agorootparentYou could try to work around that by first grading all anonymized proposals, then grading all potential speakers without knowing their proposal. In the third round you deanonymize and look at the weighted average of the two grades. You probably still need some judgment calls because the combination of speaker and topic can be important. But the score would give you a good base to work of. Maybe you could make it even more impartial by allowing conditional scores in the first two rounds. Like \"Jim is a 6, but a 8 if his talk is about molecular biology\" or \"this Lessons Learnt talk is a 5, but if it's by X, Y or Z it's a 9\" reply ghaff 17 hours agorootparentYeah, but I'm not sure conference proposals by themselves actually have a lot of value given that, in many cases (ask me how I know), the presentations won't actually exist until week or two before the the event. Certainly a talk by X that's totally unconnected from anything they're directly involved with has less value. reply omoikane 18 hours agoparentprevI had classes like that, where at the beginning of the quarter, each student gets assigned an username of the form \" \" and all participation is based on username from then on. Even though the usernames are seemingly random, certain usernames started gaining reputations on the class discussion forums, and students come to recognize some names. But computer science courses tend to have very objective rubrics for grading, so I am not sure the anonymity mattered much. reply __MatrixMan__ 18 hours agoparentprevI think I get better feedback when the teacher knows who I am. Grades are secondary. reply ghaff 18 hours agorootparentI'm not sure exam grades at the university level are really the place to get useful feedback beyond grades. reply xhkkffbf 18 hours agoparentprevI think the point is that some automated systems like Canvas may hide the names, but they're still presented in alphabetical order. Pseudonyms don't help if you don't shuffle them. reply tokai 19 hours agoprev>One simple fix would be to make random order the default setting. Fixed in the sense that the bias will be random. Presumably students graded last will still receive lower grades. reply kibwen 19 hours agoparentIt would be less than ideal, but still an improvement over the current situation as long as the order is re-randomized for every assignment, because at least then you'd only be occasionally disadvantaged rather than consistently disadvantaged. reply tetha 18 hours agoparentprevThere are however other factors involved in the grade, which have a higher impact on the grade. Like, understanding of the material and ability to present a solution. - E - I'm mostly saying that because a bunch of comments are jumping on this as a significant bias against some students. From my experience as a tutor, yes, this bias exists. But it won't turn a horribly wrong or an excellently correct solution into anything else. I eventually knew my strugglers and my excellers. I'd skim the excellers first, because if they messed up, something bad was going on. Then I'd go through the strugglers to see problems. And then I'd grade the rest first in whatever order I got the sheets, then the strugglers and then the excellers. I needed the baseline to see how bad the worst ones actually do. Some exercise sheets were an accidental adventure, I can tell you. And writing it like that, it sounds totally callous and cold. But focusing on the lower third in the exercises and communicating their struggles to the TA and prof was very appreciated by everyone, especially those students. It makes sure to get the important fundamentals right. reply exe34 19 hours agoparentprevIt should average out over their career at the university - whereas if the alphabetical order is kept, then they would be systematically penalised. reply zeroonetwothree 19 hours agorootparentIt won’t average out perfectly. There will still be lucky and unlucky students. Of course it’s better than a fixed order, and if it’s easy to switch then might as well. But we should keep thinking about how we can make it even better. reply furyofantares 19 hours agorootparentSince the effect looks very small, it looks to me like it's only a problem because it adds up if it happens for every assignment for every course. I don't think it needs to average out perfectly; it looks to me like you'd have to be astronomically lucky/unlucky for it to matter if each assignment is in random order. reply zeroonetwothree 19 hours agorootparentSome courses are only graded based on a small number of tests. I actually went to UM and a grade might be something like 30% midterm 60% final 10% homework (obviously different professors have different systems). In that case if you get unlucky just twice on the two tests you basically get the full penalty. reply furyofantares 18 hours agorootparentI'm not sure how much a +/- 0.3 (out of 100) deviation from average on a single course matters even if you end up dead first/last for both midterm and final in that example. I mean, it will matter sometimes. But it's (by far) not as big a deal as if it happens for all your courses. Still, yes, you could flip the order from midterm to final instead of randomizing both and the effect goes to more like +/- 0.1 out of 100 for the luckiest and unluckiest. reply gwern 16 hours agorootparentYes, that sort of mirror-sampling would reduce variance. The problem is, though, you need to know all the uses of randomness in order to properly counterbalance them, and these systems are already enough of a pain to use. (For example, if you have two, you can simply swap: but what about other biases? like if it's broken in half to assign to 2 grades. Or what about if there are three exams? And what about balance across other courses? if you want to do variance-reduction and tricks like antithetic sampling, you need to know all this in order to structure it properly - get it wrong, and you may make things worse.) So that's why simple random shuffling would be preferred. It allows total ignorance of all other uses (past present and future), handles all ordering biases, and can be done independent in parallel across arbitrary sets of courses/exams/grades/students. reply dotnet00 19 hours agoprevYep, I noticed this with myself too when I first did some grading a few months ago. There was also the factor that the ones I graded initially did not make certain mistakes or answered in expected ways, such that when I did encounter unexpected answers/mistakes, I had to go back and rethink the grading on the papers I had graded previously. Eg if someone answered in a way that made me think an answer I considered incorrect was actually less wrong. I only had to deal with a small class, so backtracking was doable and I graded the papers in whatever shuffled up order they were turned in, otherwise there would have definitely been a bias. reply bee_rider 19 hours agoparentI especially noticed this when grading programming projects, because it is slightly complicated. I’d either find that: A bug was really common, got to re-evaluate after the first couple times I see it, apparently it is an easy mistake to make. Or, I’d find a new bug that was pretty common, but which I didn’t know about at first. Got to update my tests and re-run everybody. I tended to be really thorough and re-do the whole stack eventually, but it was a real pain. Could have half-assed it of course, but they spend weeks on these things, feel like I owe them honest feedback. It would tend to lead me to “softer” grading as well, if you are lazy and only check for a couple bugs, you might take a large number of points off for each problem. Finding some problems and punishing them harshly is not very fair for those students that randomly hit the bugs you expect. If you find every bug, you can only take a couple points off per bug without tanking everybody’s score. reply JadeNB 18 hours agoparentprev> I only had to deal with a small class, so backtracking was doable and I graded the papers in whatever shuffled up order they were turned in, otherwise there would have definitely been a bias. Grading papers in submission order just introduces a different bias, though. (For what it's worth, I'm in the same boat and I do the same, because I don't trust my ability to give the papers any true random sorting by hand, so I take the very weak randomization that the submission order gives me.) reply dotnet00 18 hours agorootparentIntroducing a slight bias factor that is randomized each time results in a lower average bias compared to a bias factor that is the same every time. Plus, as these weren't take-home assignments, I think someone finishing earlier is more likely to be either someone who was already going to score well, or someone who was already going to make the most common errors. reply withinboredom 16 hours agorootparentI take tests extremely quickly, I either know the answer or guess it from what I know. I don't think about it. I was usually one of the first people to turn in tests. I was usually (almost always) the last person to turn in assignments, I like to be one of the last people out of a door or the last person in a line (I don't like crowds). Grading by order-turned-in would almost always mean my assignment would be one of the first or last one's graded. If I were to guess that if you did a frequency analysis of people to order, you'd find there were always a certain group who turned it in first, and another group that turned it in last. reply brewdad 7 hours agorootparentYou need to find a classmate to be a chaos gremlin that randomly mixes up the pile when they drop off their assignments. reply JadeNB 16 hours agorootparentprev> Introducing a slight bias factor that is randomized each time results in a lower average bias compared to a bias factor that is the same every time. That's what I'm saying—it's reasonable to believe that the submission time is correlated with other factors, such as ability or confidence (though the effect can cut both ways, with extremely able students submitting early because they finish early or late because they are extra careful, and similarly for other factors). Thus, this isn't really randomization, just correlation with another factor than the name. reply jedberg 18 hours agoprevThis is basically the reason my kids have the last name that they do. My last name starts with E and my wife's with Y. Bucking tradition, she didn't change her name when we got married, so when we had kids we had to decide what name to give them. We opted to hyphenate. Historically, hyphenated last names were [Woman's last name]-[Man's last name]. However, my wife hated that her last name was near the end of the alphabet growing up. We bucked tradition once again and put my name first, so that when sorted alphabetically they would be at the front of the list. Incidentally their first names start with A and B so that they show up at the front when sorted by first name too. reply mjh2539 5 hours agoparentIn Latin American countries (and Spain) the paternal surname goes first, followed by the maternal surname. reply zvolsky 17 hours agoparentprevHaha, I've always enjoyed being at the end getting less attention from teachers. If the data merely shows a correlation, it may as well be explained by us at the end being under less pressure. reply lelanthran 16 hours agoparentprev> Bucking tradition, she didn't change her name when we got married, Unless you were married earlier than the 90s, I wouldn't really call that \"bucking tradition\" any time from, say, the mid-90s onwards. If you really want to buck tradition, then don't get married - just live together, and have kids :-) (After all, there's nothing more traditional than marriage, is there?) reply jedberg 15 hours agorootparentIn the US, 80% of women still take their husband’s last name. But you hit on an important point — a lot of couples are just skipping marriage now. We went halfway there — we bought the house together years before we got married. reply zeroonetwothree 14 hours agorootparentOwning a house together is probably a more serious commitment anyway reply throw_pm23 17 hours agoparentprevWow, you really gave your children a headstart there :) reply alephknoll 17 hours agoparentprevnext [4 more] [flagged] jedberg 17 hours agorootparentYou seem to be irrationally upset about my light-hearted anecdote. I sincerely hope your weekend gets better. reply alephknoll 16 hours agorootparentIt was just a simple observation. You are reading into things too much. There's no need to be so defensive. Your comment hasn't upset me in the slightest ( rationally or irrationally ) and I sincerely hope you weren't offended by mine. reply macintux 12 hours agorootparent> It must be exhausting being married to a woman who wants to 'buck tradition'. Why didn't she buck tradition and just name your kids 'Aa, Aa', 'Aaa, Aaa', etc and be done with it? Heck why not go all the way and let them go nameless. You managed to combine snarky reductio ad absurdum and a gratuitous attack on his wife in three sentences. Why wouldn't someone be annoyed by that? reply shipmaster 17 hours agoprevMy last name starts with a letter at the bottom of the alphabet. I notice this all the time. Anecdote from this year: My son is in a high school class that requires constant input from the teacher on long running projects they have. The teacher reviews the projects alphabetically by surname, about 40% of the time, the teacher never gets to the bottom of the class, and asks the students to find her after school if they have issues. But the nature of the projects definitely requires proactive comments from the teacher. I ask my son to go find the teacher regardless and get a pro-active review, but not all the kids do that, and hence the potential for a lower grade. reply nebulous1 19 hours agoprevI wonder why Helen Wang chose this as a research topic reply jeegsy 18 hours agoparentWell spotted! reply ghghgfdfgh 10 hours agoprevThere's a section of one of the Diary of a Wimpy Kid books that talks about this exact thing. I was reminded of it as soon as I saw the headline. The justification is comes up with is that kids with names at the front of the alphabet sit in the front of the classroom, so they get called on and learn more. It definitely turned some gears in my brain when I first read it as a teen. Here's the relevant page: https://imgur.com/a/6wIx6qg reply retrac 15 hours agoprevElectoral ballots have often listed the candidates in alphabetic order, but some studies have suggested that it gives a small benefit, to the first person listed. [1] Many election authorities, in Canada at least, have shifted to randomizing the order in some way [2]. Some people have even played with alphabetic sort for novelty purposes; a man in Ontario changed his legal name to \"Above Znoneofthe\" so he would appear last on the ballot as \"Znoneofthe, Above\". [1] https://electionlab.mit.edu/research/ballot-order-effects [2] https://www.cbc.ca/news/canada/british-columbia/vancouver-do... reply zeroonetwothree 14 hours agoparentIn the US it’s usually randomised as well reply prof-dr-ir 18 hours agoprevRandomizing the grading order just hides the problem at the level of an individual course, but at least it helps in the average. More worrying is when e.g. job candidates are discussed (often in alphabetical order) and people simply tire out near the end of the meeting. When this happens, be sure to suggest taking a break! reply xyst 19 hours agoprevThat 0.6 pt gap over multiple semesters is the difference between graduating with “summa cum laude” or “magma cum laude” reply zeroonetwothree 19 hours agoparentIt’s 0.6% so it would only be if you happened to drop a letter grade as a result. Like 90.5 -> 89.9. And that would have to happen multiple times to significantly affect your GPA. reply COGlory 19 hours agoprevMultiple factors at play here. 1) Rubrics are often defined, but the application of the rubric is by a human. Application will shift as the grader gets a sense of the classes understanding. 2) As you get fatigued while grading, you'll make mistakes, and be less tolerant of others. Especially if you're an overworked adjunct or graduate student. 3) There are probably a lot more last names early in the alphabet so weighting is important. My policy on this when I was a grad student was to publish the rubric, and ask all students to check their grades too. reply redandblack 18 hours agoprevWhen I studied engineering in India, we never put our names in the finals at college. Every one gets a exam id and that goes in the answer sheets. Also, it is never your professor who grades you - the answer sheets are collected and lecturers/professors will correct them at the state level across all the engineering colleges in my state. I do not know how it is now as there has been an explosion of colleges in the state. But expect the standardized tests are similarly conducted. reply kwhitefoot 17 hours agoparentA lot of bachelor's degrees these days are awarded on the basis of modules with no finals. For instance when I did a course on C# a few years ago in Norway that was worth 6 points (I got full marks :-) ). If I had done another 29 modules of similar difficulty I would have got 180 points and been awarded a BSc in Computer Science. It's quite different from the way it was when I studied physics in the 1970s when only the final counted. Annual exams only determined whether one was allowed to continue but had no effect on the class of degree that was awarded. reply user_7832 17 hours agoparentprevAs far as I know even now it's the same for government universities (eg Delhi/Mumbai Uni). But private unis may just have a few/one profs grade everything. reply princeb 19 hours agoprev>“We kind of suspect that fatigue is one of the major factors that is driving this effect, because when you’re working on something for a long period of time, you get tired and then you start to lose your attention and your cognitive abilities are dropping,” Pei said. there is a similar effect found here https://en.wikipedia.org/wiki/Hungry_judge_effect reply tokai 19 hours agoparentI believe the hungry judge effect has generally been accepted as false. reply zeroonetwothree 19 hours agoparentprevThe thing is, it’s unclear why that effect would make you give people lower grades. surely an equally reasonable guess is that less cognitive abilities could make you give higher grades because you don’t notice errors? reply janci 15 hours agorootparentSometimes you see the result is wrong so you do not give any points initially and then look on the steps and try to find something that looks correct to give at least some points. The willingness to track through every step diminishes with increasing fatigue. reply bee_rider 19 hours agorootparentprevIt depends on what you are doing and how you are grading. I’d try to not take many points off if an error is somehow “really easy to make,” but that depends on my ability to evaluate the difficulty of mistakes. reply boesboes 19 hours agoprevThis seems related: https://news.ycombinator.com/item?id=39672111 As in, order matters reply zeroonetwothree 19 hours agoparentThis looks like one of the classic studies that won’t reproduce. For one thing, the effect size is unreasonably large. 50% more positive words just because of sequence order would be so huge we should be able to notice it anecdotally. reply m12k 19 hours agoparentprevAlso https://www.theguardian.com/law/2011/apr/11/judges-lenient-b... reply candrewlee14 16 hours agoprevSerious unintended consequences of ordering… Reminds me of the hungry judge effect [1] - judges tend to be more harsh before a break and more lenient after. [1] https://en.m.wikipedia.org/wiki/Hungry_judge_effect reply thaumasiotes 15 hours agoparenthttps://nautil.us/impossibly-hungry-judges-236688/ > we should dismiss this finding, simply because it is impossible. When we interpret how impossibly large the effect size is, anyone with even a modest understanding of psychology should be able to conclude that it is impossible that this data pattern is caused by a psychological mechanism. As psychologists, we shouldn’t teach or cite this finding, nor use it in policy decisions as an example of psychological bias in decision making. reply SamBam 8 hours agorootparentOdd article. It simply states that the effect size is too big to be believable (it calls it repeatedly \"impossible,\" but it doesn't seem like it can possibly mean \"literally impossible\" or \"mathematically impossible.\") It doesn't give any alternative explanations or specific ways the study is wrong. And it links to a rebuttal by the original authors where the responded to a bunch of the suggestions for data error or confounding factors and found that their results remain. reply thaumasiotes 8 hours agorootparentThat is explained in pretty much the section I quoted. The explanation of the effect is given in the article's links. But the article is written specifically to make the point that it should be enough to observe that it isn't possible for the effect to be real. You aren't making a good point when you cite an effect that is obviously nonsense. reply xmddmx 18 hours agoprevIs anyone confused by \"lower-ranked names\"? To me this means A, B, C, but the article says \"Wang said students whose surnames start with A, B, C, D or E received a 0.3-point higher grade out of 100 possible points than compared with when they were graded randomly.\" So I guess \"alphabetically lower ranked\" means the last letters of the alphabet, not first? Confused. reply samatman 18 hours agoparentThis is an important observation! The programmer's perspective and the user's perspective aren't always the same, and both need consideration. A user is going to see a list: it starts at the top, and it ends at the bottom. The first fields are higher, the later fields are lower. Of course, if this is a sorted list, the first field will be the \"lowest\" value, for whatever comparison is used to sort it. reply pks016 17 hours agoparentprevYes, while grading we divide the students by their last names. reply ghaff 18 hours agoparentprevYeah, I misunderstood this at first and then was somewhat confused by the comments until I actually clicked through and looked that the post. :-) I can actually believe the effect going in either direction and it's small. reply danilor 18 hours agoprevHas anybody found this link to this study? Or even the title? I searched the authors in google scholar but I couldn't find it. reply bmacho 18 hours agoparenthttps://ssrn.com/abstract=4603146 Not on sci-hub, but downloadable in pdf for me without any issues reply StefanBatory 18 hours agoprevI have an surname that's alphabetically low. Even at uni amount time I went to class and came out empty-handed as my teacher didn't score my assignement on time (at my uni 90% we have oral discussion about it) and I have to come next week while others don't are way too high. reply TrianguloY 17 hours agoprevI also have the theory that having an app/software starting with A, B, or an \"alphabetically first\" letter was noticeable in the past. Nowadays things are usually sorted \"algorithmically\", but it was common for stores to list searches with some alphabetical score, which meant that those apps were usually shown first. Even now, for example, if you go to Play Store and want to know the apps that you had but are not installed, the default sorting is by name. reply TrianguloY 17 hours agoprevAs a different but similar situation: I have a first name that is usually at the top when sorted alphabetically. Nowadays it's not a problem anymore, but as a kid I usually received a lot of calls from people that either misclicked or didn't know how to use a phone properly. It turned out it was because I was the first on the phonebook list. reply dcposch 6 hours agoprevI bet this correlation goes away if you separate the data by ethnicity. reply carabiner 6 hours agoparentYeah Chen, Cho, and Cohen are up there and would bias results. reply justrealist 5 hours agorootparentWang, Zhao, Xi. reply yencabulator 13 hours agoprevIt seems it would take less time for Instructure, Inc. (makers of the mentioned software) to fix this than it took do this research. Anyone know whether this is happening, and if not why not? reply corimaith 15 hours agoprevIf we changed our policy of exams from discriminative to evaluative, grading bias would be a trivial issue but here we are since we just NEED ways to fit everyone into numbers that we can easily use. reply beryilma 17 hours agoprevWith huge grade inflation in US universities, all students are already getting better grades than they really deserve. The amount of gymnastics that professors do to pass all students is insane. So, no student is really receiving a lower grade. reply analog31 19 hours agoprevI propose one of the following: 1. Keep the present system of grading by alphabetical order 2. Record the order in which the papers are actually graded When the grading is done, the teacher assigns a point scale (A = 90, B = 80 or whatever) but the computer does a regression fit and removes the bias. reply 2cynykyl 16 hours agoparentThis is a great idea! Next time I mark a stack of exams I will also note the time of day that the mark was entered. I can then cross-reference this with how long I have been sitting between breaks, since my last meal, etc, etc. Unfortunately I will not have this opportunity until mid-fall 2024. reply stevage 10 hours agoprevWould it be possible to simply accept that this exists and automatically unskew the grades after marking? reply stikit 18 hours agoprevA .3 point difference isn’t going to make a real difference to anyone’s life and is likely a wash when other yet undiscovered biases are in the mix. Unfairness and bias is a critical factor in driving people to extraordinary achievements. reply wolverine876 9 hours agoparent> Unfairness and bias is a critical factor in driving people to extraordinary achievements. The evidence is a strong negative correlation between bias and achievement: Extraordinary achievements so disproportionately achieved by people in groups that are not the target of bias. Look at top government officials, SV leaders, Nobel Prize winners, etc etc - mostly white males. The biggest targets of bias in the US, for example - probably women and black people - genrerally get the worst results (in areas where there is discrimination). By contrast, as an example wherever black people aren't subject to bias, such as certain forms of music and certain sports, achievement is extraordinary. Imagine all that talent and drive in other fields. reply inemesitaffia 56 minutes agoparentprevIt stacks over time reply jncfhnb 19 hours agoprevMost exam grading is not viewing the writing as a whole but rather looking for incidences of specific points to assign credit for. One could imagine an LLM be quite effective at labeling sentences as pertaining to a predefined idea at scale. reply markusde 16 hours agoprevI noticed this in myself last time I was as a TA. I'd go back and re-grade the first 15 assignments or so to make sure the rules were being applied consistently. reply jimmar 19 hours agoprevOrder effects are real. I'm a prof. I notice that the longer I grade, the less motivated I am to take off points and then justify why I took off those points. It's easier just to give points and move on. (And if anybody wants to criticize this, I'll be happy to launch into a diatribe on the psychometric dumpster fire that most assignments and their associated grading scales really are.) reply dgacmu 19 hours agoparentAlso prof: me too. I'm much more likely to provide comments on the first couple of exams I grade than on the later ones. I've found that gradescope is helpful in this regard, because it at least forces every point assignment to be matched to a rubric item. I don't have data, but I believe it makes our grading a lot more uniform compared to the pre-gradescope days. (This might be easier in grading computer science exams than in more subjective areas, though.) reply zeroonetwothree 19 hours agoparentprevThis is the opppsite of the effect they found. I do wonder if there is a big difference depending on grader and the study found some kind of average. reply jimmar 18 hours agorootparentThe article mentions that the paper is under review, but I'm guessing the effect size is small and that individual differences between graders is very substantial. The article states: > The researchers collected available historical data of all programs, students and assignments on Canvas from the fall 2014 semester to the summer 2022 semester. Thousands of students X 8 years X lots of assignments per year and you get a sample size so big that it would be hard not to find statistically significant effects. reply cm2187 18 hours agoprevWe know there are big disparities of academic success by ethnic group (cf the whole harvard discrimination against asians controversy), and there are also big concentrations of patronyms by ethnic groups (or at the minimum first letters that are more common in one part of the world than another). And on top of that if the university itself discriminates against certain ethnic groups in its recruitment it will reinforce this bias (like if asians students require better grades to get in, it is unsurprising those students that get in perform better than the rest). That would be my best guess for a rationale behind that result. reply largbae 19 hours agoprevWhat other popular systems might lead to different outcomes based on sort order? Dating site matches? Your own contact list? Interesting category of problems... reply levocardia 17 hours agoprev> Wang said students whose surnames start with A, B, C, D or E received a 0.3-point higher grade out of 100 possible points than compared with when they were graded randomly. Likewise, students with later-in-the-alphabet surnames received a 0.3-point lower grade — creating a 0.6-point gap. The hand-wringing over such a small effect size seems unwarranted. I suspect you would find similar effect sizes for other small interventions, like whether the grading took place during the week or the weekend, or in the morning vs. the evening. reply 1-6 19 hours agoprevLet’s just hope parents don’t try to game the system by starting to name their kids AAAi Aung. reply nsenifty 19 hours agoparentI'm Indian (in the US) and I've noticed a vast majority of my Indian friends name their kids Aanav, Aanir or Aanvi etc. some of which aren't even words in any Indian language. Now I probably know why. reply jen20 19 hours agoparentprevFortunately Bobby is near the front of the alphabet anyway! reply 1shooner 19 hours agoprevThis reminds me of an experience I had of just the opposite: tightly-controlled consistency in writing assessments: Almost 20 years ago I worked for a standardized test essay grading service. We graded against all sorts of secondary-level rubrics (not AP, who do their own). These would usually be from 9 - 12 grade, from every US state, and evaluating everything from reading comprehension to subject matter-specific assessment. We'd do weeks long jobs of a single test (e.g. Alabama 9th grade reading proficiency). These usually had at least 3 dimensions, and at least 4 points per dimension. We would go through a week or more of training on a rubric, then another week of 'leveling', where a manager would occasionally bring you aside and talk through why that '3' you gave on a dimension should have been a '2'. By the end of the training, we usually had had enough discussions and encountered enough edge cases to understand the weaknesses/inconsistencies in the rubric (which we had to abide by anyway). Once we were running at full-speed, everything was still double-graded and inconsistent scores were reviewed. Sometimes graders were pulled if they still didn't get the rubric. It was a simultaneously stimulating and very boring job, and most readers were educators themselves. I wonder how long before it disappears completely. reply p0w3n3d 18 hours agoprevJust do name coding. I doubt this happens everywhere on the world reply diogenescynic 6 hours agoprevIt's the same with applying to jobs. The first applicants have a greater likelihood to get the job. If you're given a list of names... you're just generally more likely to pick something from the top of the list than the bottom. reply searealist 7 hours agoprevEntirely explained by https://en.wikipedia.org/wiki/List_of_common_Chinese_surname... reply klysm 19 hours agoprevJob interviews have similar effects reply 1-6 19 hours agoparentOrder matters a lot but recruiters typically present the highest flyers first and the lower candidates last. reply ghaff 18 hours agorootparentIn my experience, it varies. I've been on interview panels where we just weren't feeling it for a number of candidates and basically told the recruiter to try harder and eventually hit someone who we were \"That's who we want. Find a way to make it happen.\" reply Aldo_MX 19 hours agoprevMaybe the answer is smaller groups? reply huffmsa 18 hours agoprevI had a theory in school that this was the case for presentations too so I always forced myself to go first. No one else to compare me against, and no sitting around getting jittery. reply flawsofar 18 hours agoprevwhat’s weird is just how long it took to find a statistic like this one reply redandblack 18 hours agoprevThe other benefit for being higher in the alpha order is you get the snow day calls first - 4:30 am, and get to call your friends before school calls them. We were always woken up by my daughter screaming as here friends called her. No such luck for the post-pandemic kids. reply faitswulff 18 hours agoprevI wonder if these biases are replicable in LLMs. reply RecycledEle 16 hours agoprevI can explain why the kids with A names outperform the kids with Z names. As someone whose first and last names are both very early in the alphabet, I was always called on first or second when I was in elementary school and middle school. I always had to be there early. My friend whose name was very late in the alphabet learned he did not have to be ready for the first minute or two of class. He would be standing near the door talking as I was quickly pulling out last night's homework, and I would be marked down for not being ready while he would later be commended for being ready when the teacher called his name. As a teacher, I see that the kids who stand outside the door talking do not do as well as the kids who are there early. reply samatman 18 hours agoprevA computer-based system like this is an opportunity to remove all personal details from an assignment while grading it, it baffles me that this isn't the default. The database could tag every assignment with a UUID4, and present them for grading top-to-bottom in UUID lexical order, without exposing who is being graded in any way. You can't fix fatigue bias, but this would distribute it randomly. It also removes the opportunity for favoritism and hostility, subconscious or otherwise, which is probably more important. Once grading is completed, the assignments are reconnected with students. Give the profs a way to mark assignments with metadata, sometimes they need to talk to a student personally about something, this should be made easy. Grades can't be immutable, professors need discretion in that, but it would leave an audit trail if professors maliciously modified grades (or the opposite). That should be uncommon to begin with, but both professors and students benefit from an audit trail here. A system like this should be used whenever it's practical, and always for high-stakes tests like midterms and finals. Not making a case against oral exams here, just that when it's possible to blind the grading process, it should be. reply pavlov 17 hours agoprevClearly evidence of anti-Polish bias when all the Zbigniews and Zygmunts and Wojteks get lower grades. (Or just another example of correlation vs. causation in action) reply hilux 19 hours agoprev> Wang noted that for a small group of graders (about 5%) that grade from Z to A, the grade gap flips as expected This is critical. Otherwise we could not discount some group (e.g. some ethnicity) disproportionately occupying one end of the alphabet or another. Super interesting and important finding. I hope this gets wide visibility and universities take a break from politicking to fix the problem - presumably through enforced randomizing. reply buggy6257 19 hours agoparentEnforced randomization isn't going to fix the problem, it just evenly distributes the problem. Based on these results, it would mean that the graders are just getting tired/lazy/inattentive the further they get in their stack of papers to grade. That's the problem the needs to be fixed, not the order they get graded in. Enforced randomization is simply a short term alleviation so no student(s) get disproportionately affected by this phenomenon. reply bluGill 19 hours agorootparent> it would mean that the graders are just getting tired/lazy/inattentive the further they get in their stack of papers Or maybe they are getting better / more picky. I know in code reviews I often pass a few and then notice something that I realize was also wrong in previous reviews I allowed, but later reviews that day (week?) will not allow that. reply 13of40 19 hours agorootparentI've participated in day-long and multi-day interview events for job candidates, and I see the same effect. At the beginning you don't have a frame of reference and you're more likely to question your own decision or give someone the benefit of the doubt, but by the end you're far more systematic, plus a little bit numb to the effect your decision is having. reply throwaway35777 18 hours agorootparent> by the end you're far more systematic, plus a little bit numb to the effect your decision is having Maybe decision fatigue is supposed to bias humans toward the optimal solution for the fiancee problem [1]. [1] https://en.m.wikipedia.org/wiki/Secretary_problem reply cyanydeez 19 hours agorootparentprevFor grading, you could probably just add a mediating factor and throw in test cases that calibrate the factor and then you curve everyone on that factor. It'd seemingly be more work but would result in averages that are more reasonable to the changes in stress. reply labcomputer 19 hours agorootparentprevYes, and: Additionally, universities (and, by extension departments) want grades to approximately follow a normal distribution (and yes, you in the back, their actions show they do actually want that, even if they say otherwise). When you start grading a problem you have some idea what a \"good\" solution looks like, what an \"ok\" solution looks like, and same for \"bad\" solutions... If you award points based on that, the result will be a normal-ish distribution. But your idea of a good/ok/bad solution evolves as you see more papers. There's two reasons for that: First, you can't (ahead of time) imagine all the ways that students will invent to fuck up a problem set, and find edge cases in your grading rubric that result in unfairly-high or -low scores. As you gain experience teaching, you will anticipate more of the ways, but you will never anticipate every way. Second, the TA/grader wants to be able to stack-rank the papers and have the scores be monotonic. The grader wants this because non-monotonic scoring triggers far more complaining than harsh scoring or picky scoring. When you come across papers that are worse than ones you've already recently graded, you assign even lower scores. This results in a ratcheting effect with more extreme scores as you get closer to the bottom of the pile. But, since the mean score is usually a B/B-/C+ (~75-85), and since scores are usually limited to the range 0-100, this means that papers closer to the bottom will receive statistically lower scores. Now, you could go back a re-grade ones you've already done, but: 1. The university is officially only paying you for 20hrs/week (and requires a signed end-of-semester statement attesting to the same). 2. The assigned workload of teaching and grading doesn't permit a two-pass grading scheme while keeping within 20 hours. 3. If you complain to the graduate ombudsman about the workload needing more than 20 hours, you won't have funding next semester (so you have a prisoner's dilemma among TAs who might want to grade more fairly). 4. If you're grading (say) a final exam for a frosh/soph class, you're probably in a room with 4-8 other graders late into the night. One effective way to make your coworkers hate you is to be that guy who always finishes grading his stack last, when everyone is worried about catching the last train/bus. Basically, all the incentives are aligned to make this happen. reply hilux 6 hours agorootparentThat's thought-provoking - thank you. Essentially, unless it's an old exam where the universe of bad answers is already known, you need two passes - a discovery pass followed by the grading pass. reply bigfudge 19 hours agorootparentprevIn my case, I have to make a conscious effort to remain consistently (in)tolerant of lazy writing. It’s hard to keep on reading between the lines and giving the benefit of the doubt. reply rjzzleep 19 hours agorootparentprevI had the same conclusion. You learn things as you go, including things you don't like. reply davrosthedalek 19 hours agorootparentprevIn my experience, it's not tired/lazy/inattentive, but resignation. You normally have some expectation what students will be able to solve. Typically, these expectations are set too high. That's very common, not only for me, but for pretty much anyone I know. So over the time of grading, one adjusts down the expectations and gives partial credit earlier, for example. reply throwaway35777 19 hours agorootparentprevI was a grader once. I guarantee if someone gives a good answer they'll get full marks even near the bottom of the stack. For BS answers I'll admit I got less generous as the hours went on. No one's getting hurt by this system if it's randomized. It's a matter of graders giving out partial credit for wrong answers which is discretionary. Rarely students are granted a small mercy. Seems OK. reply dunham 18 hours agorootparentI was one of many TAs for a large math class in college (pre-calc - think high school math for college students). For uniformity, the prof had the partial credit down to a science - specifying points for getting certain aspects of the problem. For the finals, a few TAs would be assigned to a given page, for uniformity. The fascinating thing was that the distribution of grades was about the same every year. And I had a math prof for analysis who would give negative points for BS answers. You could say “I need X but don’t know how to prove it” in the middle of a proof, but if you made up something that was incorrect, you’d get negative points. reply hilux 6 hours agorootparentOh, that brings back memories! \"For every epsilon, there is a delta ...\" reply bumby 19 hours agorootparentprev>For BS answers I'll admit I got less generous as the hours went on. What do you think is the cause of this? Do you become more cynical (and less generous) because you’ve seen so many BS answers previously? Is it just that getting fatigued makes you less generous? reply ihaveajob 19 hours agorootparentWhen I was a TA in grad school, I noticed the same. Early on I thought some BS answers were at least kind of funny, and I gave them the benefit of the doubt, maybe giving more attention to the parts that were correct. After I saw similar answers later on, the novelty wore off and I was probably less amused, so the inclination to be lenient disappeared. Sometimes I went back to previous decisions if I remembered them, to be fair, but I don't think I always remembered since the volume could be high (grading 80 exams in a row is TEDIOUS). reply BugsJustFindMe 19 hours agorootparentprev> Enforced randomization isn't going to fix the problem, it just evenly distributes the problem. Evenly distributing the problem does fix the problem. Proportionality is what matters. Grading being arbitrary is fine if everyone is graded equally. reply zeroonetwothree 19 hours agorootparentRandom order would still mean a few students in the class get unlucky and near the end the majority of the time. Although over the course of all classes it would tend to even out somewhat. It’s certainly better than fixed order. reply BugsJustFindMe 19 hours agorootparent\"randomization\" is not the important part here. \"evenly distributing\" is. It is absolutely possible to reorder the sequence fairly such that your scenario doesn't occur. It could even to a human observer look randomized if you want. In a trivial example case where the effect were linear you could just switch the order back and forth, and on average every student would receive the same middle-of-group impact. reply whiterknight 19 hours agorootparentprevThe mistake is assuming grades are an objective measurement, and not gamification to try to help you learn. reply BugsJustFindMe 19 hours agorootparentIt's a common mistake. So common, in fact, that it has real practical impact on students at the edge who might not otherwise have failed or passed. reply skhunted 19 hours agorootparentprevFor me I grade tests as follows. The stack is created as students turn in the test. I grade the first page in that order. The stack reverses for the second page. So on and so forth. I teach college math. I just cant imagine a system of grading done in alphabetical order. reply falseprofit 19 hours agorootparentScanning and grading on a computer can alphabetize them. reply skhunted 18 hours agorootparentThat makes sense. I haven't had people upload assignments for a long time. I'd forgotten that this was a thing. reply kurthr 19 hours agorootparentprevI also came here to say this. My only guess is that the alphabetization (by the \"learning management system\") to make filling the grades into a table \"easier\" for the computer or for the person handing out the results? Why is it \"easier\" if the system doesn't have to order them at all, or it could do so by student number (same issue as alphabetical order) or something random, which is the other (non default) option for the \"learning management system\". I feel like only the most obsessive compulsive humans would have this issue (without computer \"help\"), as the last thing I wanted to do as a TA was to add another step of ordering all the papers before grading them. I also always reviewed the first few papers I graded after grading the rest to make sure I was being fair, because it was obvious to me that until I saw a representative distribution of answers I couldn't do fair grading/marking. reply hilux 19 hours agorootparentprevIn the real world, universities are never going to fix the problem of overworked and underpaid grad students getting tired. reply furyofantares 19 hours agorootparentprevIt's a 0.6 gap from top to bottom out of a score of 100. Plus or minus a third of a percent from average. Pretty small effect. But it would add up (or, well, persist - it wouldn't get bigger) if it happens to you for every assignment for every class and that sucks. If there's more than one assignment you can basically erase it by randomizing each separately. If you really care beyond that then randomize for one assignment, flip it for the next, then randomize again for the next etc. reply WaitWaitWha 19 hours agorootparentprev> graders are just getting tired/lazy/inattentive the further they get in their stack of papers to grade. I will admit to this. Initially, my patience and tolerance for errors is significantly higher than towards the end of the grading. By the second hour grading, I am not only mentally exhausted my tolerance is significantly lower. I try to prevent this by creating very explicit grading rubric and I stick to it as much as possible. reply ghaff 19 hours agorootparentClear rubrics are the thing where possible. They aren't everywhere though. I've been on conference committees and so many different factors come into play--including how late in the day it is. But, in that case, a bunch of people are rating and commenting and there's no strict order so it probably evens out to a reasonable degree. reply bumby 19 hours agorootparentprevAs the number of assignments grows, wouldn’t randomization help converge on the more accurate grades (in aggregate)? reply falseprofit 19 hours agorootparentIt would help, but with only a couple dozen courses and most determined by a couple exams it’s not quite a large number. reply andix 19 hours agorootparentprevEven distribution would fix the problem. If grading has a subjective component, there will always be deviations from the \"correct\" grade. If those patterns are randomly distributed over all students, their grade averages will be comparable again. reply cyanydeez 19 hours agorootparentprevUnfortunately, it's gonna be AI to the \"rescue\" and the problem is obfuscated. reply freeopinion 18 hours agoparentprevMy first thought was, \"Who takes the time to sort before grading?\" Computers change the world in such incredibly subtle ways. Of course, such subtleties exist without computers. This is just one case where computers make the subtleties more detectable. reply 14 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A University of Michigan study reveals that students with surnames starting from A to E often receive higher grades in the Canvas online learning system due to sequential grading biases.",
      "The grading disparity may adversely affect students' GPAs and future career prospects, highlighting the importance of addressing this issue.",
      "Recommendations from the researchers include employing random grading orders or increasing the number of graders to mitigate this bias."
    ],
    "commentSummary": [
      "The discussion delves into how grading methods, including alphabetical order, affect student performance and introduce bias in education.",
      "Strategies like randomization and technology tools are proposed to promote fair grading practices and mitigate issues such as fatigue, bias, and inequality in the grading process.",
      "Suggestions for improvement encompass blind grading, standardized tests, and AI grading systems to uphold fairness in academia despite the challenges faced by educators."
    ],
    "points": 376,
    "commentCount": 264,
    "retryCount": 0,
    "time": 1713621187
  },
  {
    "id": 40096575,
    "title": "Senate Passes Reauthorization of Key US Surveillance Program",
    "originLink": "https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349",
    "originBody": "1 of 2President Joe Biden salutes as he boards Air Force One as he leaves Andrews Air Force Base, Md., on his way to his Delaware home, Friday, April 19, 2024. (AP Photo/Manuel Balce Ceneta) Read More 2 of 2FILE - Sen. Marco Rubio, R-Fla., speaks with reporters as he walks, Feb. 28, 2024, at the Capitol in Washington. The Senate has advanced legislation that would reauthorize a key U.S. surveillance tool as lawmakers and the Biden administration rushed to tamp down fresh concerns about the program violating Americans’ civil liberties. (AP Photo/Mark Schiefelbein, File) Read More By FARNOUSH AMIRI and MARY CLARE JALONICK Updated 6:43 PM UTC, April 20, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print WASHINGTON (AP) — President Joe Biden on Saturday signed legislation reauthorizing a key U.S. surveillance law after divisions over whether the FBI should be restricted from using the program to search for Americans’ data nearly forced the statute to lapse. Barely missing its midnight deadline, the Senate had approved the bill by a 60-34 vote hours earlier with bipartisan support, extending for two years the program known as Section 702 of the Foreign Intelligence Surveillance Act. Biden thanked congressional leaders for their work. “In the nick of time, we are reauthorizing FISA right before it expires at midnight,” Senate Majority Leader Chuck Schumer said when voting on final passage began 15 minutes before the deadline. “All day long, we persisted and we persisted in trying to reach a breakthrough and in the end, we have succeeded.” U.S. officials have said the surveillance tool, first authorized in 2008 and renewed several times since then, is crucial in disrupting terrorist attacks, cyber intrusions, and foreign espionage and has also produced intelligence that the U.S. has relied on for specific operations, such as the 2022 killing of al-Qaida leader Ayman al-Zawahri. READ MORE Senate advances renewal of key US surveillance program as detractors seek changes North Korea is buying Chinese surveillance cameras in a push to tighten control, report says House passes reauthorization of key US surveillance program after days of upheaval over changes “If you miss a key piece of intelligence, you may miss some event overseas or put troops in harm’s way,” Florida Sen. Marco Rubio, the top Republican on the Senate Intelligence Committee, said. “You may miss a plot to harm the country here, domestically, or somewhere else. So in this particular case, there’s real-life implications.” The proposal would renew the program, which permits the U.S. government to collect without a warrant the communications of non-Americans located outside the country to gather foreign intelligence. The reauthorization faced a long and bumpy road to final passage Friday after months of clashes between privacy advocates and national security hawks pushed consideration of the legislation to the brink of expiration. Though the spy program was technically set to expire at midnight, the Biden administration had said it expected its authority to collect intelligence to remain operational for at least another year, thanks to an opinion earlier this month from the Foreign Intelligence Surveillance Court, which receives surveillance applications. Still, officials had said that court approval shouldn’t be a substitute for congressional authorization, especially since communications companies could cease cooperation with the government if the program is allowed to lapse. Hours before the law was set to expire, U.S. officials were already scrambling after two major U.S. communication providers said they would stop complying with orders through the surveillance program, according to a person familiar with the matter, who spoke on the condition of anonymity to discuss private negotiations. Attorney General Merrick Garland praised the reauthorization and reiterated how “indispensable” the tool is to the Justice Department. “This reauthorization of Section 702 gives the United States the authority to continue to collect foreign intelligence information about non-U.S. persons located outside the United States, while at the same time codifying important reforms the Justice Department has adopted to ensure the protection of Americans’ privacy and civil liberties,” Garland said in a statement Saturday. But despite the Biden administration’s urging and classified briefings to senators this week on the crucial role they say the spy program plays in protecting national security, a group of progressive and conservative lawmakers who were agitating for further changes had refused to accept the version of the bill the House sent over last week. The lawmakers had demanded that Schumer, D-N.Y., allow votes on amendments to the legislation that would seek to address what they see as civil liberty loopholes in the bill. In the end, Schumer was able to cut a deal that would allow critics to receive floor votes on their amendments in exchange for speeding up the process for passage. The six amendments ultimately failed to garner the necessary support on the floor to be included in the final passage. One of the major changes detractors had proposed centered around restricting the FBI’s access to information about Americans through the program. Though the surveillance tool only targets non-Americans in other countries, it also collects communications of Americans when they are in contact with those targeted foreigners. Sen. Dick Durbin, the No. 2 Democrat in the chamber, had been pushing a proposal that would require U.S. officials to get a warrant before accessing American communications. “If the government wants to spy on my private communications or the private communications of any American, they should be required to get approval from a judge, just as our Founding Fathers intended in writing the Constitution,” Durbin said. In the past year, U.S. officials have revealed a series of abuses and mistakes by FBI analysts in improperly querying the intelligence repository for information about Americans or others in the U.S., including a member of Congress and participants in the racial justice protests of 2020 and the Jan. 6, 2021, riot at the U.S. Capitol. But members on both the House and Senate intelligence committees as well as the Justice Department warned requiring a warrant would severely handicap officials from quickly responding to imminent national security threats. “I think that is a risk that we cannot afford to take with the vast array of challenges our nation faces around the world,” said Sen. Mark Warner, D-Va. chairman of the Senate Intelligence Committee. __ Associated Press writer Eric Tucker contributed to this report. FARNOUSH AMIRI Farnoush is a congressional reporter. twitter mailto",
    "commentLink": "https://news.ycombinator.com/item?id=40096575",
    "commentBody": "Senate passes reauthorization of key US surveillance program after midnight (apnews.com)294 points by WhyUVoteGarbage 22 hours agohidepastfavorite199 comments alex_young 7 hours agoThis isn’t just a reauthorization. It included provisions that make most US businesses give the NSA direct access to communications equipment: https://x.com/lizagoitein/status/1781546937675657392?s=46&t=... reply jedberg 17 hours agoprevWhat's most interesting is that this wasn't on party lines. The yes/no mix is very mixed party-wise. https://www.senate.gov/legislative/LIS/roll_call_votes/vote1... reply alexpotato 11 hours agoparentA couple years ago I stumbled upon this YouTube video: https://www.youtube.com/watch?v=Qz27n1tNNMg The summary is this: - Votes in the House and Senate used to be anonymous - They then decided to make them public under the reasoning of transparency - One side effect of making them public is that you got people like Grover Norquist and The Americans for Tax Reform who could see who voted for taxes and then use that to \"name and shame\" people (there was a pledge signing in there as well). For more details see: https://en.wikipedia.org/wiki/Grover_Norquist - This now means that it's MUCH easier for lobbyists and special interest groups to see where to spend their money as a Senator's voting history is public knowledge (which both sides are WELL aware of) - As a sibling poster points out: you can easily see who receives money from defense groups vs not. - This is probably good for us as voters in the short term but bad for the country in the long term (Due to the above) reply idiotsecant 11 hours agorootparentI'm not convinced that this is a problem. Lobbyists and special interests already knew how politicians voted, they just knew via old fashioned grapevine methods. There was an information asymmetry between well connected lobbyists and average people. The fact that no longer exists is a good thing, in the long and short term. reply fardo 9 hours agorootparentThere’s a tension between the common belief that “when private citizens are able to vote privately, it protects their ability to vote their conscience, rather than allowing some third party to explicitly buy votes or bully someone into voting in line with someone else”, and the belief that somehow this doesn’t apply to congress members. Additionally, on hard philosophical and policy qurstions, some bits of negotiation and dealmaking are bare-knuckled “the sausage gets made” affairs that are brutally hard on the ego and participants. Part of why nothing can get through Congress anymore in a timely fashion and without continual brinksmanship on important funding or to prevent shutdowns is because even if crossing party lines would very often be in the public’s interest, and to the public’s net benefit, haggling to make it happen or voting to make it so often doesn’t stand up to the scrutiny of thousands of watchful eyes where an important deal may hinge on brutal haggling which the public couldn’t stomach seeing the intermediate steps and votes of. One such example in practical terms: if the constitutional convention which replaced the articles of confederation took place in the internet age with modern real-time, to the minute reporting on how everyone was voting on every intermediate plan and how any compromise made was a betrayal of “party lines” on an issue, America as a country probably wouldn’t exist today. Transparency has its own benefits, but it’s not without costs - you make a legislative body’s job more difficult, you get corresponding gridlock to match. reply lesam 8 hours agorootparentSure there’s a tension, but one difference is that I know how I vote, but I don’t know how my representative votes unless it’s in the public record. If you believe that electorates punish politicians for decisions in the public interest, and legislators’ jobs would be easier if they were less accountable to their voters, why support democracy at all? reply beau_g 8 hours agorootparentprevElected officials should not vote their conscience, they should vote according to the wishes of their constituents reply int_19h 3 hours agorootparentThen what is the point of even having them and not just voting for everything directly? reply pixl97 6 hours agorootparentprevThe loudest ones? The 51% that voted for them out of the 20% of total voters? I mean, this does get very hard to define. reply godelski 8 hours agorootparentprev> I'm not convinced that this is a problem. There's no \"THE problem.\" There's a ton of \"problems.\" Just to be clear. Because many of the problems we face today are through interaction of different things, often in a complex chain, rather than a direct easy to follow causal chain. > via old fashioned grapevine Treat the problem as an adversarial problem. Yes, your adversary will always be able to break your defenses. Nothing is bulletproof. How you defend is through forcing adversaries to expend resources. It is very clear that forcing lobbiests to learn through the grapevine is a more costly method than simply looking at a public database. And if you aren't familiar with this concept, people lie. No one need know your vote unless you reveal it (which... might be a lie). The point also is that it can also prevent inner conflict, among parties. You're not voting along party lines? You think you're going to get as much support from your party when it comes to your bills and campaign funds? So you have plenty of incentive to not reveal your vote, even among allies. I agree with you in that switching to private votes won't solve the problems we have. But would it improve? I'd guess some and I'd guess it would take time for the real effects to be seen. But the other side is, would it do harm? I doubt it. reply fnordpiglet 7 hours agorootparentprevThe continental congress itself was entirely secret with no notes taken and no existing journal, diary, or any other record of the proceedings - and this was a bunch of folks who obsessively recorded their lives and thoughts for posterity. They also thought pretty hard about the side effects of such things and congressional and senate votes were private until modern times. I’m not one who believes the constitution is sacred or that the founding fathers were infallible, but I do think the chance for a person to vote their conscience vs their politics is an important feature. While the grapevine might be a route to learn, it’s also a route that doesn’t have to be accurate. I can tell my lobbyist whatever I want about my vote, but only I know what my vote was in private voting. This feels like a feature not a flaw. The point of representative democracy is selecting a person whose judgement you believe in. Public voting records lead to populist and party strangleholds on outcomes with consequences for breaking dogma. Practically speaking it also gives lobbyists proof positive of whether their money was well spent. reply lumb63 11 hours agoparentprevWhat’s most interesting is this is incredibly unpopular amongst voters of both parties. reply int_19h 3 hours agorootparentRepresentative democracy in US is neither representative nor really a democracy. reply Cacti 8 hours agorootparentprevwhich is exactly why the vote went the way it did. they wanted it passed, they found the votes from senators that were safe, and everyone else was allowed to bail so they didn’t have to deal with it at reelection. this is normal practice, to provide cover for your party members. it’s divided by party line because it’s national security. they’re splitting the spoils. reply pessimizer 15 hours agoparentprevThe parties only have \"disputes\" on a short list of wedge issues, and either side winning on those removes the that issue as a cudgel that can motivate their base. If you look at their donors, you'll see the lines. The people who voted for it make money from the defense and intelligence industries, and the people who didn't, don't. Voting for for something majorities of the voters of both parties are against is expensive (in terms of being re-elected.) That price is paid by donors, and the media control that those donors will exercise. Which again, is why the wedge issues are needed: you're going to have to vote for those people who voted against your civil liberties if you want Democrats to pretend to protect abortion rights for another 4 years, or Republicans to pretend to end them. reply Cacti 8 hours agorootparentI mean, except that many people can’t get either an abortion or IVF. This affects people’s entires lives, it’s not “pretend” unless you are unaffected. reply cool_dude85 7 hours agorootparentThis happened under a Democratic president. And what have they done about it since? reply brewdad 6 hours agorootparentWhat would you have Biden do about exactly? Pack the Supreme Court? If Roosevelt couldn’t pull that off at the height of his power, there’s no way Biden is making that happen. reply lettergram 10 hours agoparentprevOn the republican side, those voting yea are almost always the old McCain crowd. What the right calls “RINOs” or republicans in name only. That’s not surprising, they’re also the group funding wars, voted for the initial spy bills, etc. What’s more surprising is the split in IL between Duckworth (yea) and Durban (Nay). Usually you don’t see states splitting too much. Tennessee was all Nays for instance. reply Cacti 8 hours agorootparentI would imagine Duckworth has a personal interest in it, given her work history. reply bobthepanda 8 hours agorootparentat least a bunch of the isolationist sentiment on the republican side recently has actually come from former veterans, since the wars in Iraq and Afghanistan had such questionable returns reply hammock 17 hours agoparentprevAgreed that’s interesting. Really makes you think about all these crackpots talking about a uniparty, deep state vs the people, etc reply stanford_labrat 16 hours agorootparentBecause in reality the two party system is not accurate. It’s rich versus poor, those with power versus those without. Nobility versus peasants. That’s just how it works. reply Gud 16 hours agorootparentThat's how it works in the USA, not necessarily how it works. Other forms of governing exists. A big step forward for the USA would be a vast reduction of federal power over the states. reply theoldlove 7 hours agorootparentThe US federal government is already pretty weak compared to other countries. The federal government looks pretty bad at the moment, but I’m not sure further weakening it will help the country. reply Gud 3 hours agorootparentCompared to wich countries, The US is a massive country, with the populace far removed from the decision making. I believe this is the core problem. reply int_19h 3 hours agorootparentprevThe US federal government is very strong compared to many countries, just limited in where that power can be applied... in theory. In practice, the insanity that is a precedent-based judicial system over time means that it's all just a disorganized mess where on one hand the Feds can straight up prevent you from boarding a plane, ever, without any semblance of due process (this is not normally a power you'll find in other countries), and yet can't regulate many mundane things like firearms. However, there is a very solid case for a weak federal government, and it is simply that US is a country that's way too big for any coherent national policy on most matters that we've currently pushed there. It's such a vicious fight because it's half the country trying to bludgeon the other half into submission, motivated by the knowledge that, if you yield, the other guy will pick up this huge club and do the same to you. This will continue until the country breaks down unless we dial it down to state level and accept the fact that other states may have laws and lifestyle that is despicable or horrifying to us in some ways. Either that, or we might as well just break the whole thing apart now and not wait for it to happen in a more violent manner. reply sapphicsnail 12 hours agorootparentprevHow would that help? Political parties operate in states. States are banning books and outlawing abortions too. reply artificialLimbs 11 hours agorootparentStates are not printing billions of dollars and shipping it overseas or wholesale spying on their populace for the purpose of political manipulation. reply pessimizer 15 hours agorootparentprevOther countries don't institutionalize the two-party system by law. Because it would be insane and antidemocratic to create a complicated network of laws that would have to be eliminated state by state in order to ordain that an entire country must be ruled by two intimately-linked private clubs in turn. reply cryptonector 11 hours agorootparentThe two-party system isn't so heavily institutionalized \"by law\". The law generally gives advantages to parties that pull in more than x% of the vote, and it so happens that the first-past-the-post system of electing representatives makes it very difficult for a third party to take root. reply int_19h 2 hours agorootparentExcept our law is nothing like that. You can have a party take 45% of each district across the whole country and end up with zero seats in the House (because the other party took 55% of each). reply dartos 9 hours agorootparentprevThe first past the post system is encoding a two party system into law. If it makes to hard enough for a third party to take hold, there might as well not be one. Not everything is spelled in ink. reply bigstrat2003 7 hours agorootparentThat's not \"encoded into the law\". The outcomes of the law are not the same as the law itself. reply dartos 3 hours agorootparentPractically speaking, they are. The effect of a law is at least as important as the literal words on the page. reply yoyohello13 6 hours agorootparentprevYep, that’s how it has worked throughout all of human history. reply Wowfunhappy 16 hours agorootparentprevI do not find it surprising that groups of people with many overlapping viewpoints do not have overlapping viewpoints 100% of the time. If anything, I find it surprising that they overlap so frequently. Furthermore, I think the frequency of that overlap is a major problem for our political system, because it makes compromise impossible. reply nyokodo 16 hours agorootparentprev> crackpots talking about a uniparty, deep state vs the people, etc It’s not controversial to suggest that the interests of the political class, the special interests that fund their campaigns, and Washington bureaucrats differ from the interests of the public at large. You don’t need to evoke deep state conspiracies to explain nefarious coordination because when career and monetary incentives align then bills like this one get passed. reply soraminazuki 15 hours agorootparentYep, this trend of dismissing undemocratic power structures as conspiracy theories is deeply troubling. Important issues such as surveillance, censorship, and the military-industrial complex have a long history and are extensively documented. Yet it's hard to bring these issues up today without being labeled a far right conspiracist. It wasn't always like this. Many have agreed these were legitimate issues during the Iraq war. Where have all those people gone today? reply squigz 14 hours agorootparent> Yet it's hard to bring these issues up today without being labeled a far right conspiracist. This really isn't all that true in my experience. And, I mean, look at the discussion here... Maybe consider the people you hang around with? reply Zancarius 13 hours agorootparentIt's definitely who you hang around with, but I think how the conversation is approached also dictates outcome. Talk about a political ruling class with most people, and they'll look at you as though you grew a third eyeball. Talk about the Dems and Repubs being out of touch with the average person due to the insulative effect of DC, and they'll usually agree. You can generally convey the same idea gently as long as you hedge your phrasing somewhat. Making it sound like a wacky accusation comes off sounding, well, wacky. reply hammock 10 hours agorootparentYou nailed it. This discussion would never have happened on HN if I hadn’t worded my original comment the way I did. It’s not how I wanted to word it ;) reply bombcar 8 hours agorootparentYou can talk about a uniparty all you want as long as the people you’re talking to are still sure you vote the “correct” way. How that is broached depends on whom you are talking with. reply djfobbz 9 hours agorootparentprevMaybe they're on to something and we're the crackpots? reply unethical_ban 15 hours agorootparentprevReally makes you think about whether there are some things that can still transcend partisan showmanship, like national security. I still am a believer in digital freedom, I'm old enough to have seen the changes in the Internet, and it is a much more malevolent and fucked up force than it was even 15 years ago. Maybe, just maybe, the government needs the power to spy on international targets with oversight. reply karma_pharmer 4 hours agorootparentwith oversight The constitution's term for this is \"warrant\". reply int_19h 2 hours agorootparentprevWhen didn't they ever claim that sky will fall if they don't have all the surveillance they already do, and then some for good measure? reply njarboe 18 hours agoprevHere is a link to how the senators voted[1]. [1]https://www.senate.gov/legislative/LIS/roll_call_votes/vote1... Unfortunately both my senators voted for it. I did call their offices Thursday to no avail. reply bwanab 17 hours agoparentIt's the first time I can ever remember on a contested vote where both the two Democratic Senators from my current bluer than blue state voted the same (nay) as the two Republican Senators from my former redder than red state. Strange bedfellows in interesting times. reply throwaway35777 15 hours agorootparent{Montana, North Dakota} --> {Washington}? reply hellcow 17 hours agoparentprevI'm really disappointed that even in CA (which is pushing for better privacy rights with CCPA), one of our senators voted for this. reply bennyhill 14 hours agorootparentI assume any congress person who voted for surveillance has a horrible kink and received photos of it shortly before the vote. reply kwhitefoot 13 hours agorootparentThat reminds me of Wellington's response under similar circumstances. A former lover tried to blackmail Wellington. His response was 'Publish and be damned.' It was published to the delight of many. But he still went on to become Prime Minister. https://www.independent.co.uk/voices/rear-window-when-wellin... reply Mountain_Skies 12 hours agorootparentReminds me of when the KGB and the CIA tried to use knowledge of the sexual exploits of Indonesia's president Sukarno to blackmail him. Instead of falling in line, he told them to release what they had so his countrymen could be impressed by his sexual prowess. The KGB went as far as having a group of their agents pose as flight attendants to engage him in an orgy, which they secretly filmed. When confronted with the film, he asked if KGB for extra copies for him to take home. reply int_19h 2 hours agorootparentprevLook up Dianne Feinstein's track record on these matters. reply verdverm 17 hours agorootparentprevAre you aware the alternative is less oversight? FISA protects Americans https://en.wikipedia.org/wiki/Foreign_Intelligence_Surveilla... reply geuis 17 hours agorootparentI obviously can't guess your age, but I'm gonna wager you weren't around much prior to 9/11. The world was getting on quite well without massive surveillance creep, and none of the stuff FISA has done in the last 23 years would have stopped it. The authorities already had all the info they needed back then and just didn't act on it. reply borkt 16 hours agorootparentFISA has been in existence since 1978. It did not prevent 9/11, so honestly your comment undersells how worthless the program has been in light of the constitutional freedoms we willingly cede in reauthorizing it. The fact is though it remains law and the officials we elected feel the value is worth it. I hope its being done solely based on the benefits it provides us as a whole and is not being used for self-serving purposes reply soraminazuki 15 hours agorootparentEven humanitarian groups such as the UNICEF were targets, there's no doubt now what the program is about reply tastyfreeze 16 hours agorootparentprev> used for self-serving purposes That is inevitable. If there is an easier path to a goal some human will use it. It doesn't matter if the goal is against the people. reply AmVess 14 hours agorootparentprevThese laws aren't about protecting America and its citizens, but rather as means to control them. reply mdhb 13 hours agorootparentPeople just toss comments like this around as though they were facts when in fact it’s completely paranoid made up q-anon level nonsense. These laws work a very specific way and have very specific controls in place to prevent shit like you describe from happening which you could go and read up on if you wanted to but it’s much easier to fear monger amongst one another because it plays to your ego that somebody who is important enough to be under surveillance by an intelligence agency. reply somenameforme 12 hours agorootparentYou could easily look at things like the Snowden leaks to see how well such controls end up working out. My favorite was NSA agents collecting and sharing sexual content. [1] The reason that's my favorite is not because it's the most extreme example of abuse - it's not, not by a longshot. The reason is that it really demonstrates that 'government' isn't some abstract or holistic entity. It's just a group of people, like you and I -- with the exact same vices, egos, weaknesses, and so on. And of course this applies not only to the NSA spooks, but all the way up. You shouldn't be any more comfortable letting 'the government' spy on you, than you would be letting me spy on you. If you want another example along the same lines, spooks spying on their love interests is so common that there's a slang term for it - LOVEINT [2]. Basically, don't grant people power over other people unless it's really just completely and absolutely necessary, because it will be abused. So the benefit needs to substantially outweigh the inevitable abuses. And in this case, that obviously doesn't hold. [1] - https://www.nytimes.com/2014/07/21/us/politics/edward-snowde... [2] - https://slate.com/technology/2013/09/loveint-how-nsa-spies-s... reply soraminazuki 13 hours agorootparentprev\"Completely paranoid made up q-anon level nonsense\" from the New York Times, The Guardian, Washington Post, Associated Press, and many others? I think not. https://www.nytimes.com/2014/05/13/world/middleeast/book-rev... https://www.theguardian.com/uk-news/2013/dec/20/gchq-targete... https://www.washingtonpost.com/news/the-switch/wp/2013/08/24... https://apnews.com/article/b25197d5b11740b2b29681bbc521a45f https://arstechnica.com/tech-policy/2023/05/fbi-misused-fore... reply squarefoot 2 hours agorootparentprev> completely paranoid made up q-anon level nonsense. For what is worth, I'm quite left leaning and fully agree with the parent poster. Information is power, no matter which party or in which country. reply xanthor 12 hours agorootparentprevhttps://apnews.com/article/fisa-foreign-surveillance-fbi-3f7... One does not have to be “important enough” if they are conducting mass surveillance and storing it in a database indefinitely. reply mdhb 10 hours agorootparentHow things work in your mind and how they work in the real world are very very different things in this instance. reply xanthor 10 hours agorootparentWhat are the massive NSA datacenters for in the very very real world? reply AnthonyMouse 12 hours agorootparentprevIt's a secret court making secret law. This is, by definition, both unaccountable and impossible to conclude is not being used to cover up massive abuse, because whatever is happening is being concealed from the voters. reply unethical_ban 16 hours agorootparentprevhttps://en.wikipedia.org/wiki/Foreign_Intelligence_Surveilla... 2008. reply Retric 15 hours agorootparentForeign Intelligence Surveillance Act of 1978 (FISA) https://bja.ojp.gov/program/it/privacy-civil-liberties/autho... reply unethical_ban 12 hours agorootparentAnd the components of the program being discussed are from the 2008 amendment. reply Retric 8 hours agorootparentPeople dislike parts of the original 1978 bill that contunued. I take issue with this bit: FISA also established the United States Foreign Intelligence Surveillance Court (FISC), a special U.S. Federal court that holds nonpublic sessions to consider issuing search warrants under FISA. Proceedings before the FISC are ex parte, meaning the government is the only party present. When combined by foreign agents including US citizens, it’s troubling. reply soraminazuki 16 hours agorootparentprevAgree with the sentiment, but spying capabilities have been abused before FISA, just ask Martin Luther King Jr. So I don't think things were particularly fine before 9/11 either. It's just that technological advancements have made abuse on a mass scale possible for the first time in human history. AFAICT surveillance used to be much more targeted and labor intensive. That all changed after 9/11. reply e40 14 hours agorootparentI didn't downvote you, btw (I upvoted you). I think MLK Jr's problems with the government weren't traditional spying, they were more harassment of government employees acting on their own because they were bigots. The organized government actions that did happen, IIRC, were in places were the local government was highly corrupt and infiltrated by the KKK. reply int_19h 2 hours agorootparentCOINTELPRO? reply jiggawatts 11 hours agorootparentprev“Apart from the widespread abuse of government power, there was no abuse.” reply verdverm 15 hours agorootparentprevI watched 9/11 live from my dorm Maybe don't jump to biases so fast, people within all age groups have different opinions about the same topics. HN is very opinionated on surveillance, as the comments on this story reinforce reply geuis 12 hours agorootparentDoesn't really matter, but I was 21 when it happened. I suspect we're basically the same age. reply chiefalchemist 15 hours agorootparentprevWhat do you mean by opinionated on surveillance? When did the Constitution become an opinion? reply bigstrat2003 16 hours agorootparentprev> Are you aware the alternative is less oversight? Yes, I am. That is in fact what I want. > FISA protects Americans No, it does not. At this time, the greatest threat to me (and other Americans) is in fact the glowies who want to use this sort of law to violate our civil liberties. reply randcraw 15 hours agorootparentprevI used to work for several of the US intel agencies. I can say with great confidence that we never have acted gainfully on preventing a major event using intel and we never will. The catalyst for acting boldly to prevent or defend a major event is much mor political than informational. No intel will ever play a big role in deciding whether a country lives or dies. But we most certainly WILL abuse individual civil rights my abusing that intel. THAT has been confirmed in history again and again. reply user_7832 13 hours agorootparent> The catalyst for acting boldly to prevent or defend a major event is much mor political than informational. Could you explain what you mean by this? On a tangential note, have you considered talking/explaining this with politicians/academics studying this field? Or is it more of something that's already known to those familiar with the field? reply randcraw 4 hours agorootparentThe most relevant example I know is the Zimmerman telegram in 1917 which British intelligence decrypted and passed along to Pres Wilson. It detailed plans Germany had made to invade the US with Mexico's help. Wilson released it to newspapers in March as support for his decision to declare war on Germany in April. However the primary justification for war wasn't the telegram, but the public decision by Bismark to fully resume uboat attacks on merchant ships in the Atlantic. So even as damning and revealing as the Zimmerman telegram was, ultimately it was Germany's bold resumption of the torpedoing of US oceangoing traffic that catalyzed US public opinion into ending 3 years of American neutrality and joining the fight in WWI. Thus even when intel is most damning, the role of intel will always be subservient to publicly motivating events like lost lives, as in the much ballyhooed sinking of the Lusitania 2 years before (1915). Wikipedia has a couple of outstanding articles on the topic: https://en.m.wikipedia.org/wiki/Zimmermann_Telegram https://en.m.wikipedia.org/wiki/American_entry_into_World_Wa... reply mise_en_place 17 hours agorootparentprevThe alternative is requiring a warrant, which means following the Constitution. Due process doesn’t disappear because you want it to. Even if someone is supposedly a terrorist or criminal. reply twoodfin 16 hours agorootparentIf you’re hoping the Supreme Court, and in particular this Supreme Court, is going to agree that the Constitution requires the executive branch get a warrant before spying on cross-border communication with a non-citizen, you’re going to be disappointed. FISA is Congress exercising the only authority it has here, which is oversight & regulation. You could argue FISA should be stricter, but it can’t extend the Constitutional reach of the Fourth Amendment, nor can it contract it the way many in this thread believe it’s somehow doing. reply soraminazuki 15 hours agorootparentIt's baffling to many people how FISA is even a thing. To a layperson, the Fourth Amendment leaves no room for a rubber stamp court authorizing mass surveillance. And no one except politicians and bureaucrats are buying the argument that this is somehow targeted surveillance. Also, free nations should have higher standards than \"Not a citizen? Too bad, anything goes.\" reply twoodfin 14 hours agorootparentThis is not complicated: If you run a telegraph wire between El Paso and Juárez, the executive has the Constitutional authority to tap it to intercept communication to or from a non-citizen not in the United States, warrant-free. Congress can regulate the process that must be followed, the documentation that must be made, even require judicial review at the program level to ensure it doesn’t also record traffic that is Constitutionally protected. That’s what FISA is. But it can’t ban that tapping, nor can it require the executive to get a warrant for a particular otherwise Constitutional intercept from an Article 3 court. Which part of this do you think is incorrect? reply soraminazuki 13 hours agorootparentWhere do I even start? Let's first reiterate that even when it's technically legal to screw over non-citizens, it doesn't make it right. That's not the standard expected of a free nation. But let's ignore that for a moment and move on to the next point. Your example is still hoovering up communications from citizens who are supposed to be protected by due process of law. En masse. How does this not run afoul of the law? The problem is compounded by the fact that the internet blurs geographical borders. Wholly domestic communications can and does end up crossing borders. Also, I'd bet a large part of our communications aren't even between people. The majority of the traffic likely are sent to or from computer programs. They happen without most people even realizing it, but contains highly personal information. The simple telegraph analogy doesn't translate well to the internet. What's more, there's currently no meaningful system in place to prevent abuse. And no, a rubber stamp court authorizing dragnet surveillance isn't it. reply twoodfin 13 hours agorootparentOK, you want FISA to be stricter. But way up thread, someone made the point that it’s FISA itself that puts any meaningful balancing constraints at all on the Constitutional power of the executive. This includes the FISA court—made up of real, lifetime-tenured federal judges of the same robes you would like approving warrants—that is there by law to be watching out for just your parade of horribles. The poster was roundly criticized for being correct. reply soraminazuki 13 hours agorootparentNo, FISA should not be a thing. Wiretap warrants should be reasonably scoped and acquired on an individual basis. There shouldn't be a secret court issuing do-whatever-you-want warrants. reply twoodfin 13 hours agorootparentTo get that you have two choices: Do your best to persuade your fellow citizens to elect a President who will choose to forego this part of his Constitutional powers—or get a Constitutional amendment passed. What I keep trying to explain is that this FISA vote can’t address your concerns one way or the other. If you disagree, I wish you’d explain how. reply soraminazuki 13 hours agorootparentThe Constitution grants the president unlimited spying powers? That's news to me. Whether the FISA vote can fix all the problems isn't the point. The problem is that current surveillance practices looks illegal to begin with. reply twoodfin 11 hours agorootparentIntercepting communications between US persons and foreign non-citizens isn’t “unlimited spying powers” and is not illegal. Do you disagree? reply soraminazuki 4 hours agorootparentWe're running in circles now. https://news.ycombinator.com/item?id=40100565 reply int_19h 2 hours agorootparentprevYes reply Kamq 13 hours agorootparentprev> This is not complicated: If you run a telegraph wire between El Paso and Juárez, the executive has the Constitutional authority to tap it to intercept communication to or from a non-citizen not in the United States, warrant-free. That's not correct at all. It would only fall under federal overview if it's commercial (Article 1 section 8 clause 3 of the constitution gives congress the right to regulate commerce with foreign nations). The Feds don't just get to do anything they want by default. All powers that aren't specifically given to the feds are defaulted to either the states or the people. reply twoodfin 13 hours agorootparentThis is, flatly, nonsense. For example: Executive agencies conduct warrantless border searches unrelated to commerce around the clock. reply Kamq 11 hours agorootparentIt's dumb, but Wickard v. Filburn makes basically anything involving physical goods \"commerce\". I'm sure there's a ruling somewhere that says something like: people entering the country subtly alter the restaurant market (not really any dumber than the Wickard v. Filburn rationale), and therefore the feds have a right to search everything. I think it would be a lot harder to do that with speech though. Maybe you could argue that the telegraph line itself impacts international copper markets or something, but there are non-tangible based communication methods. reply 13of40 12 hours agorootparentprevNot to nitpick too hard here, but you can't know whether I'm talking to a US citizen without first eavesdropping on the conversation. reply soraminazuki 11 hours agorootparentThat's actually a great point. After the Snowden revelations, politicians justified some of the surveillance programs by claiming they were only looking at the metadata, not the content, as if that made any difference. So that's one of the excuses they use to create the appearance of legality. https://www.npr.org/2013/06/21/193578367/calling-it-metadata... reply jrochkind1 13 hours agorootparentprevI don't understand the argument that it couldn't require a warrant. The argument is simply that the executive branch has a constitutional right to wiretap without a warrant, unless the the constitution forbids it? There is some judicial oversight in the FISA court of course. What's the argument for why congress can legislate that, but not a more typical warrant? reply twoodfin 13 hours agorootparentFor the same reason Congress can’t require the President to get the approval of the Supreme Court before he vetoes a bill: Our Constitution gives powers to the executive that cannot be usurped or overruled by Congress, notably in this context to conduct the national defense and foreign affairs. The FISA court exists to ensure that the executive is not operating outside his Constitutional authority, not as a gatekeeper for use of that authority at all in any instance. reply AnthonyMouse 11 hours agorootparent> Our Constitution gives powers to the executive that cannot be usurped or overruled by Congress, notably in this context to conduct the national defense and foreign affairs. This is not true. The constitution explicitly reserves the power to declare war or enact treaties to Congress. Neither the military nor federal law enforcement can spend a single dime, or even exist, without Congressional approval. If the budget allocates no money to mass surveillance, no money is available to conduct mass surveillance. reply twoodfin 11 hours agorootparentYes, Congress can defund the FBI, NSA, DIA, … or simply forbid them from spending money on foreign surveillance. What they can’t do is allow them to spend money on foreign surveillance, but only if an Article 3 court gives them a warrant. reply AnthonyMouse 11 hours agorootparentAnd what exactly stops them from doing that, as a condition of how they spend the money? You could certainly have budget allocation for \"surveillance conducted pursuant to a warrant\" that prevents the money from being wasted on useless surveillance of innocent people. reply twoodfin 10 hours agorootparentThe Constitution. What you’re saying is no different from Congress declaring war and funding the army, but with the proviso they must clear all battles with a Federal judge before they’re begun. Or funding the Department of Justice, but with the proviso that any nominee for Attorney General must be over age 60. The power of the purse is not unlimited. reply int_19h 2 hours agorootparentThe Constitution has already been shat on in all manner of ways to the point where it's not recognizable anymore. If we continue doing so anyway, we might at least do that to the citizens' benefit. reply AnthonyMouse 10 hours agorootparentprevRequiring battles to be approved by a judge has obvious practical problems, because they often happen in remote locations at unpredictable times, but Congress can pass all kinds of dumb requirements if they want to. That doesn't mean it's unconstitutional. You're proposing an alternative where the executive gets to decide how money is spent. As if mass surveillance, which is a waste of money, has to be funded in order to fund ordinary investigations. The executive is the weakest branch. It has almost no powers of its own, and shouldn't. It's checks and balances. For something to happen, the executive has to want to do it and Congress has to fund it. Not one or the other; both. reply twoodfin 8 hours agorootparentNo, it really is unconstitutional for Congress to encroach on the enumerated powers of the executive. Just look at the recent SCOTUS cases around the setup of the Consumer Financial Protection Bureau to understand how consequential this constraint is. reply AnthonyMouse 5 hours agorootparent> Just look at the recent SCOTUS cases around the setup of the Consumer Financial Protection Bureau to understand how consequential this constraint is. Isn't this about the opposite issue, whether Congress can delegate control over funding to the executive? They were trying to get the executive to do the job of Congress and control the CFPB's funding. reply AnthonyMouse 12 hours agorootparentprev> You could argue FISA should be stricter, but it can’t extend the Constitutional reach of the Fourth Amendment, nor can it contract it the way many in this thread believe it’s somehow doing. Congress can't pass a law violating the Fourth Amendment. They can certainly pass a law constraining the executive from doing something that is otherwise constitutional, if the courts are reading the Fourth Amendment too narrowly. They could also straightforwardly require the FISA court to publish its opinions, or have the same cases heard in ordinary federal courts with public accountability for the decisions. There is nothing in the constitution requiring secret courts. reply reaperman 14 hours agorootparentprevWe also had drug trafficking when the US constitution was originally written[0], and the founders of the US still gave us a constitutional right to warrants for searches relating to it. I don't understand why sealed warrants aren't \"good enough\" for this purpose, perhaps you could open my mind a bit. Why do we need \"warrantless\" surveillance for drug trafficking now? Specifically, what's wrong with getting a sealed (secret for a period of time) warrant for surveillance from a normal court? > In 1800, the British Levant Company purchases nearly half of all of the opium coming out of Smyrna, Turkey strictly for importation to Europe and the United States. 0: https://www.pbs.org/wgbh/pages/frontline/shows/heroin/etc/hi... reply serf 16 hours agorootparentprevI can't down vote you harder. FISA hurts Americans by short circuiting any kind of protections citizens once had for due process. We were fine before, and arguably it would've done little to change the events that caused the reaction that allowed it to be established in the first place. reply rightbyte 15 hours agoparentprevSo 55 needed for passing and 60 voted for. Closer then I thought it would be. reply int_19h 1 hour agorootparentOnce enough votes are there to secure the bill, there's no reason for either party to \"waste\" any more votes of their members on something that can be so politically unpopular with large parts of their electorate. reply karaterobot 15 hours agoparentprevPeople generally vote for the incumbent if they happen to claim the same party affiliation. They complain for 4-6 years, then when it comes to what box they tick on the ballot, all of that is out the window. The lure of an incumbent is that they might have acquired enough markers and enough seats on various committees to help the state, when it often seems the reality is that they've probably just acquired more lobbyist friends and more incentive to stay in office no matter what. Sure, they may be corrupt and incompetent, but they've got so much influence! reply Onawa 15 hours agorootparentThe joys of the \"first past the post\" election system. Take your choice of a shit sandwich, or a shit sandwich with pickles. Heaven forbid we actually update our voting system to break up the inevitable 2-party outcome. reply chiefalchemist 15 hours agorootparentHeaven might not forbid, but the two ruling parties certainly do. Breaking out of the status quo would crush their cartel, end their monopoly. They don't want to do that. The cycle continues. reply superkuh 12 hours agoparentprevIt terrible it passed but I'm glad both Wisconsin senators voted no. They voted no for completely different reasons but I'll take it. reply hackernewds 17 hours agoparentprevwhat change would a call affect? reply bilekas 17 hours agoparentprevYou don't see any value in FISA? reply bilekas 9 hours agorootparentLots of people downvote. Would love opinions. reply sunshine_reggae 12 hours agoparentprevDo these people know who you are?! reply blackeyeblitzar 18 hours agoprevSince it expired at midnight and reauthorization passed after midnight, was there a period where the government acted illegally in continuing surveillance? How does that work? reply Brybry 18 hours agoparentLegally there wasn't a need to rush, the FISA court had certified the process until 2025. [1] But probably companies could have stopped cooperating and challenged it in court. [1] https://www.wyden.senate.gov/news/press-releases/wyden-urges... reply WarOnPrivacy 18 hours agoprevDoes anyone want to suggest some reasons why, the one thing that D+R always, always, always agree on is this: US Gov/LEO/IC must be gifted the most power possible to surveil Americans who are not suspected of a crime reply kbolino 18 hours agoparentThe bureaucrats regularly present scary information to the politicians to justify their actions and powers. The juiciest bits of intelligence are intentionally selected for escalation up the chain, with many being presented ASAP at the highest levels (SECDEF, President) and/or retained for later demonstration to oversight authorities (FISA court, Congressional committees). While much of \"raw\" intelligence is not reliable, the agencies can curate the best (most believable/most sensational/most verified) intelligence reports over time. Given recent events in the Middle East and the fact that both parties' senior politicians mostly lean the same way in terms of which sides they support, this result is unsurprising if disappointing. reply Nifty3929 17 hours agoparentprevBecause politicians want power and control over the citizens. They might use it for different things, but power is power. reply Dalewyn 17 hours agorootparentI'm pretty damn sure you have it backwards. The intelligence crapmunity wants power, and politicians are merely the means to an end. See what happens when a politician of any stature dares to defy them. reply greenavocado 10 hours agorootparentChuck Schumer: \"Six ways from Sunday\" reply verdverm 17 hours agoparentprevI think you have things a bit backwards. Without FISA, the intelligence agencies have less oversight and fewer restrictions. > The FISA resulted from extensive investigations by Senate Committees into the legality of domestic intelligence activities. These investigations were led separately by Sam Ervin and Frank Church in 1978 as a response to President Richard Nixon's usage of federal resources, including law enforcement agencies, to spy on political and activist groups. https://en.wikipedia.org/wiki/Foreign_Intelligence_Surveilla... reply hypothesis 16 hours agorootparent> Without FISA, the intelligence agencies have less oversight and fewer restrictions. What restrictions are you talking about? Constitutional warrant requirement was sidestepped using this law and you are still cheering here. reply AnimalMuppet 16 hours agorootparentWell, before FISA, constitutional warrant requirements were not sidestepped, they were simply ignored. So now we're acknowledging that the constitutional requirements are still there, but now we use this weird dodge to get around it. So is that better or worse? reply hypothesis 15 hours agorootparentAre you really asking if being unconstitutional is worse than being codified and legal? I’m not the one here cheering for demise of constitutional republic… reply AnimalMuppet 14 hours agorootparentNeither am I cheering for it. Don't put words in my mouth. I am seriously asking whether being flat-out unconstitutional is worse than building a (legislated and approved) backdoor around the constitution, yes. I mean, better than both would be to just follow the constitution, but that wasn't the question. reply hypothesis 13 hours agorootparentPlease note that at no point I said that you specifically cheered, so no need to project. It’s a threaded topic. As you noticed, following constitution is apparently not an option here. Being unconstitutional and ignored, there was at least some hope for improvement, but codification gave us a clear answer that elected representatives are, at best, only selectively interested in supporting constitution. reply araes 15 hours agorootparentprevUnfortunately, that appears to be America these days. Do something illegal, and then write a law to legalize the illegal behavior. reply dotnet00 18 hours agoparentprevThey're ultimately the same, the partisanship is mostly a farce as they both know that they're the only realistic options, so as long as neither side goes out of its way to seriously be better than the other, they can both enjoy the perks of being in power eventually, and therefore increased power is always good from their pov as regardless of party, they'll eventually have access to that power too. reply coolbreezetft24 16 hours agorootparent> the partisanship is mostly a farce This was noticeably on display for me in 2020 right after it was determined that Biden had won the election. Lindsey Graham, a Republican Senator, was caught on video in the Senate chamber warmly congratulating and hugging Kamala Harris, a D senator and VP-elect. It was as if they both knew Graham's hyper-partisan antics during the preceding months before the vote was all just an act - a part of the game. I'd bet that he secretly voted for Biden/Harris as well and will do so again. reply Cacti 8 hours agorootparentThis is basic human empathy from a very small group of insiders. I mean, this is their job. Do you think they all go around work being dicks to each other all day every day? Besides, it cost him nothing. reply int_19h 1 hour agorootparentTheir job is to represent us, supposedly. And they keep trying to whip their own supporters into frenzy against each other (quite successfully, I should note - so much so that it's already getting violent at times). The fact that the people doing so are themselves chum buddies tells volumes. reply ComposedPattern 12 hours agoparentprevI would guess that Democratic and Republican politicians want to give more power to the USA government because they are the USA government. reply memish 17 hours agoparentprev“Let me tell you, you take on the intelligence community, they have six ways from Sunday at getting back at you,” Schumer told MSNBC’s Rachel Maddow https://thehill.com/homenews/administration/312605-schumer-t... reply CamperBob2 16 hours agorootparentIf that were true, Trump would have been carried out of Helsinki feet-first. reply pas 15 hours agorootparentwhy, what was/happened in Helsinki? reply CamperBob2 12 hours agorootparentTrump announced that he believed Putin over his own intelligence. But then there was the time Biden installed Hunter on the White House staff and ordered that he be given a security clearance, despite dozens of discrepancies, undisclosed foreign contacts, and other red flags on the paperwork. Oh, wait, no, that was Trump, too. reply ipaddr 16 hours agorootparentprevThe only person willing to take them on is Trump. Look at all of the fake cases and mainstream media attacking that followed. I don't think anyone can stop them now. When America is replaced as a world power that day will come. reply tophi 11 hours agorootparentYou sound as sane as the guy that self immolated yesterday. reply bugglebeetle 18 hours agoparentprevI would imagine because the IC already uses those same surveillance powers to get dirt on enough politicians to make sure this happens. reply stufffer 17 hours agorootparentThey had to add a rule about not using it to spy on Congress. That tells you all you need to know about how often fisa is abused. reply outlore 18 hours agoparentprevhorseshoe theory. D+R are not so different. D in US is more right than other countries' left leaning parties reply SSJPython 17 hours agorootparent> D in US is more right than other countries' left leaning parties I don't think this is accurate. Maybe on healthcare and welfare, sure. But on many social issues, the Democrats are much further to the left than the European left. On issues such as abortion, gender/sexuality, migration, and race, the Democrats are more extreme compared to Labour in the UK, SPD in Germany, and the PSOE in Spain. Even the left in France isn't as socially extremist as the Democrats. reply Larrikin 17 hours agorootparentIt's a boring take from more than 30 years ago that was kinda true in the Regan years when the dominant voting groups could pretend that elected officials and government didn't actually matter because they all voted similarly and discrimination against groups that disagreed had been publicly accepted for decades. Historical electoral maps were not usually competitive at all like they are now. The both parties are the same is such a lazy take, except in super limited circumstances like this naked power grab in the article. Both are going to use it in wildly different ways reply mijamo 2 hours agorootparentprevFor migration, sure, but it is very related to the history of the US (nearly everyone is a relatively recent immigrant so it feels wrong to refuse that others come in). For abortion and gender this is not correct though. It is not as hot a topic but positions are not that different between European left and democrats. There is also a very wide scale of opinions inside the Democrat party itself. Some people just focus on the very left of the party but plenty of democrats are much more similar to Macron than the French left when it comes to social issues. reply mamonster 16 hours agorootparentprev>Even the left in France isn't as socially extremist as the Democrats. Depends which left which you are talking about. LFI is certainly on that level in their way, PS/Place Publique are not(given that \"printemps républicain\" was part of what killed popular support for the party). reply monocasa 16 hours agorootparentprevI mean, those countries have other further left parties with held seats in their legislatures up to and including outright explicit communist parties. Those parties you listed are known for being center to center left in Europe, sometimes explicitly escuing the left as UK Labour and SPD have done. Excpet PSOE which is farther left than the Democrats, having all of the identity politics of the Democratic party while being explicitly and empathetically pro union. Heads would have rolled if PSOE had broken the rail workers strike that like Biden did. The also tried to legalize abortion in the Spanish constitution in the 1970s, and haven't wavered on their view of abortion since. They passed same sex marriage when they got their first chance to (and before the US did), and used the same opportunity to expand transgender rights. reply dukeyukey 17 hours agorootparentprevLeft and right are different in different countries. In the US, the Republicans are generally pro-building (see where new homes and factories are being built). But in the UK, the left party (Labour) is the one pushing for less onerous planning. reply dexwiz 18 hours agorootparentprevBoth sides are mostly rich or put there by the rich. A few populist reps get outsized airtime, but that isn’t the majority of people running the show. reply ryandrake 17 hours agorootparentThere are a lot more similarities between \"both sides\" than that. They make a big show out of arguing over a small number of things they disagree on. But for many important things, they don't significantly differ. The two parties do not significantly differ on indefinite detention of American citizens on US soil. The two parties do not significantly differ on domestic spying, dragnet-style data collection and warrantless wiretapping. The two parties do not significantly differ on allowing extra-judicial targeted killings. The two parties do not significantly differ on the use of unmanned drones, either for combat or domestic surveillance. The two parties both support pre-emptive \"cyber\" war and non-defensive hacking. The two parties do not significantly differ on their support for continuing the War On Terror. The two parties both support maintaining US military bases around the world. The two parties do not significantly differ on favoring Keynesian economics. The two parties support delegating monetary policy decisions to the Federal Reserve, including support for quantitative easing. The two parties do not significantly differ on their use of earmarks and pork barrel spending. Neither of the two parties have (recently) proposed plans for balancing the budget. Neither of the two parties plans to significantly cut defense spending. The two parties both favor taxpayer-funded foreign aid. The two parties are largely backed by the same corporate sponsors and special interest groups, with a few key differences. The two parties both backed TARP and in general favor bailing out companies too big to fail. The two parties do not significantly differ on their general support of \"economic stimulus\" as a tool to prop up the economy. The two parties do not significantly differ on their support for and allegiance to Israel. The two parties both favor and continue sanctions on Iran. The two parties do not significantly differ on their use of super PAC funding and their support of unlimited spending from corporations and special interest groups. The two parties do not significantly differ on their use of gerrymandering to gain political advantage. The two parties oppose any measures that would strengthen the viability of a third party. reply pakyr 15 hours agorootparent> The two parties do not significantly differ on their use of earmarks and pork barrel spending. This is not true; the Republicans strongly oppose them and have repeatedly tried to abolish them (and were temporarily successful at one point). > Neither of the two parties have (recently) proposed plans for balancing the budget. This isn't true. Both parties have recently proposed plans for balancing the budget; Biden proposed plans to balance it by raising taxes and instituting a wealth tax just last year, and Republicans have put forward various entitlement reform proposals to balance the budget. > The two parties both favor and continue sanctions on Iran. Obama ended sanctions on Iran with the nuclear deal before Trump reinstated them; Republicans blocked Senate ratification of the deal, allowing him to do that and ensuring the Iranians wouldn't trust future entreaties from the US. Claiming the two parties are the same on this is odd. > The two parties do not significantly differ on their use of super PAC funding and their support of unlimited spending from corporations and special interest groups. Dems support and have repeatedly attempted to pass an anti-Citizens United amendment. > The two parties do not significantly differ on their use of gerrymandering to gain political advantage. Dems repeatedly tried to pass a bill banning gerrymandering federally when they controlled the House in 2021. I'm no expert but for these 5 at least, I am aware of significant and specific interparty differences. reply golergka 13 hours agorootparentprevMany of these points are just common sense. Does America really need a major party that's insane on one of the important issues? reply int_19h 1 hour agorootparentI guarantee you that whichever points you think are \"common sense\" on this list, there's millions of people in this country who will disagree with you on every single one of them. reply Jerrrry 17 hours agorootparentprevemergent behavior from a self-interested system, which doesn't necessarily preclude collusion, directly or less so. the best capitalist simply had their competition shot. reply avianlyric 18 hours agorootparentprevMinor correction > D in US is more right than other countries' left leaning parties D in US is more right than other countries' right leaning parties reply akira2501 16 hours agoparentprevCorruption of our intelligence agencies to the point they've been weaponized against our own elected officials. reply int_19h 1 hour agorootparentHave there ever been a point where our law enforcement and intelligence agencies haven't been weaponized by our politicians against their opposition? FBI under Hoover, COINTELPRO, Watergate... reply hamhock666 18 hours agoparentprevBecause it gives them more power, and nobody cares to organize or do anything about it in terms of voting out said politicians. reply 2OEH8eoCRo0 11 hours agoparentprevThe F in FISA stands for foreign. reply api 17 hours agoparentprevIf anything happens any politician who voted no can be accused of being responsible for “missing the next 9/11” or being “soft on terror.” If nothing happens most people don’t understand or care either way. reply kolanos 18 hours agoparentprevAbsolute power corrupts absolutely. reply Tarq0n 17 hours agoparentprevI suspect in the incentives, the downside risk weighs much heavier for these people. If they block surveillance powers and another 9/11 happened they'd be dragged over the coals, whereas approving them is pretty risk free. reply deviantbit 8 hours agoprevIt amazes me how we can pick and choose which part of the Bill of Rights we want to abate, while all of it is being obscured into history. We were united around the Bill of Rights, now we've been brainwashed into believing no one should have rights. 1A, Nope, you have to have facts, and specific facts, and a subject-matter expert, source? 2A, Ban all guns, the government will protect us. 3A, No worry, they're not soldiers, they're law enforcement. 4A, Unreasonable search and seizure? What was unreasonable about us listening to your phone call? 5,6,7A, Fair trail? What public figure is getting a fair trail, and has been unreasonable fines? But he deserves it. 8A, The death penalty is fair, right? I mean, if evidence ever shows up that exonerates them, we can dig them up, right? 9A I don't think anyone cares what order we get rid of these, right? 10A Those states don't have rights, it's my body, right? I don't vote anymore. No offense, you all disgust me. reply kdasme 8 hours agoparentI can relate to your opinions. Still, I’m going to mentions this: please, vote. Otherwise you are a part of the problem. reply deviantbit 6 hours agorootparentWe were once called liberals. We believed in unalienable rights. Guess what, my liberties are gone, people are dying in wars, and I'm not happy. If you wanted to be part of this experiment we call the United States, you could gain citizenship by learning and understanding what liberalism meant. Not anymore, just cross the border, we will give you a debit card, and just wait, because we're going to make you a citizen, and your uneducated self will help us burn this nation to the ground. No war will go unfunded, no problem will be solved, and we will teach you to hate everyone else. This while everyone is screaming about their abortion access while the Nation goes bankrupt. We have $34T in debt, every 93 days we add another $1T. If you're all such internet geniuses, you should have figured that soon, and very soon, that starts walking away from the Treasuries' ability to pay just the interest. Democrats will scream raise taxes on the rich, but guess what, there are not enough of them to tax. They won't agree to cutting spending cuts. Republicans will refuse to cut the military because we have to defend Taiwan, and against every mythical and imaginary enemy. But wait, there is more, Democrats want to expand surveillance on anyone practicing the 1A, the 2A, while violating the 4A & 5A. Everything is about what you can get from this country. No one listened to John F. Kennedy. He was murdered. \"Ask not what your country can do for you – ask what you can do for your country...\" It is all about what you can get out of this for yourself. It doesn't matter if you're a billionaire or some guy on the street. You're trying to get what is best for you, not what is best for all of us. I want what is best for this Nation. I served, my grandfathers served, my children serve now. But F-U when you tell me I'm part of the problem. I understood what I was fighting for, you have no clue why we're even here. reply ofslidingfeet 18 hours agoprevOnce upon a time, this would have been the only thing the internet talked about today. Top of reddit, thousands of comments on news articles, etc. Now we get suppression and astroturfing from a bunch of autocrats who despise democracy and call themselves the \"Intelligence Community.\" reply ein0p 16 hours agoprevOnly one thing can make these people work past midnight: abusing the American public. “Representation”, my ass. None of them represents me or anyone I know. reply Zancarius 13 hours agoparentAgreed, but the cynical side of me thinks that they are representing their constituents. It's just that neither you nor me are their constituents. They pay lip service only during an election year. I'm politically very conservative. I hate every single one of the Republicans. They claim to want smaller government and less intrusion and then vote for... bigger government, more intrusion, and endless wars. I still vote, though. Mostly, at this point, it feels like an act of protest more than anything. reply ein0p 13 hours agorootparentI struggle to nail down my political affiliation because there’s literally just one party. They quibble over materially irrelevant hot button issues to create the illusion of choice, but all the bullshit that robs me and my kids or strips us of our rights is _always_ “bipartisan”, passed without reading in the dead of the night. And yeah the only two choices in our upcoming “elections” here are a guy with profound dementia who shakes hands with invisible people, and a narcissist moron con man who writes at a fourth grade level and capitalizes nouns for no reason. And neither side considers this to be a problem. I’m beginning to think this is some kind of a joke and the ruling class is just trying to see how far they can take it before people revolt. reply thejazzman 12 hours agorootparent\"pro life\" is a pretty hard (and exploitive) line separating the two reply int_19h 1 hour agorootparentThe whole point of having cultural issues like abortion, gay rights, guns etc in the spotlight is because it's much easier to make people emotionally invested into that in lieu of, say, economics, foreign policy, military-industrial complex, or political reform. E.g. get someone fervently believing that their vote is critical to preventing \"murder of children\", and you can abuse them economically however the hell you want - they'll hate you but they'll still vote for you because the other guy \"murders children\". This one is such an oldie but goodie that each party has crafted its own wedge issue around it: abortion for Republicans, guns for Democrats. reply ein0p 12 hours agorootparentprevKeep paying attention to that while they borrow $2T a year and give it to their friends. reply xyst 15 hours agoprevThis was going to pass regardless of the outcry. The unnecessary drama of stalling until after midnight is all theater. We need a significant change in leadership for all those that voted this in. If I recall correctly, this bill also includes an expansion of surveillance performed by federal law enforcement agencies and NSA. reply JumpCrisscross 15 hours agoparent> This was going to pass regardless of the outcry There was a real moment in the House where it might not have, at least without a warrant requirement. My Congresswoman was one of the attack dogs on this issue. She thought they would get an outpouring of support. She didn’t. The call sheets registered basically zero calls in support, and several lobbying against. So she caved. (This is a pattern I saw play out in New York years earlier in another privacy battle.) > The unnecessary drama of stalling until after midnight is all theater Sort of. The Senate calendar is funky. Putting it at the end of the roll was theatre. Having something voted on after midnight was not. reply int_19h 1 hour agorootparentI think at this point most people just assume that this kind of stuff is a foregone conclusion, because they pass it every time without much of a fight (when you compare it to other matters) despite it being strongly disliked across the political spectrum. reply user_7832 13 hours agorootparentprev> There was a real moment in the House where it might not have, at least without a warrant requirement. My Congresswoman was one of the attack dogs on this issue. She thought they would get an outpouring of support. She didn’t. The call sheets registered basically zero calls in support, and several lobbying against. So she caved. (This is a pattern I saw play out in New York years earlier in another privacy battle.) What's odd/interesting to me is that there's been little chatter of late regarding this. I spend an unhealthy amount of time on HN/Reddit/X and save for a few mild posts (as opposed to alarmist or clickbaity) on the topic, I barely see anything. During the net neutrality thing back when Ajit Pai was around I remember there was massive support. And I don't think I've ever heard of the NY privacy thing you mention. I wonder why it's so. reply FezzikTheGiant 36 minutes agorootparentHey, I replied to your comment about building an open source version of Aqua Voice. Emailed you about it too. Let me know your thoughts. reply calibas 17 hours agoprevSo if I communicate with a non-US citizen, I effectively forfeit my right to privacy? Am I understand this correctly? reply v7n 17 hours agoparentForfeiting sounds intentional. How would you know the nationality of all participants in a conversation? reply Barracoon 15 hours agoparentprevTechnically, it’s if you communicate to a target of a foreign intelligence investigation AND they deem you suspect enough to request FISA approval to access your side of the communication. reply fifteen1506 15 hours agorootparentnetwork American (tm) reply srj 14 hours agoprevI find you can get a good idea of what's going on from reading between the lines of Wyden's statements. As a member of the intelligence committee he cannot directly disclose the operational details, but you can look at where he's concerned. From the CNN article on this: >> Another amendment at issue was from Democratic Sen. Ron Wyden of Oregon, a member of the Intelligence Committee. His amendment, which was co-sponsored by several of the most liberal Democrats and conservative Republicans in the chamber, would strike a new part of the program that he argued would lead every day Americans into helping the government spy if they have “access to equipment that is being or may be used to transmit or store wire or electronic communications.” On the face of it, any cellphone or smartwatch seems to fit that definition. They could be converting everything into a listening device, recording all of it, and then making it available to intel officers only when they query for it and can argue one party is a foreign national. reply hangsi 13 hours agoparent\"Beautiful. Unethical. Dangerous.\" So says Morgan Freeman's character Lucius Fox in 2008 in The Dark Knight[0]. The rest of the tech imagined in that scene is plausible today too, considering the density of WiFi/5G and research demonstrating the potential for its use as passive radar [1]. That paper metions a cooperative base station, but I am wondering if there is any value gained in knowing exactly what the traffic is (such as some of the intelligence community does) in modelling how the waves propagate and performing an even more passive observation. [0] https://www.youtube.com/watch?v=IRELLH86Edo [1] Samczyński et al. 2021 https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=966... reply araes 14 hours agoprevOther info: This reauthorization of FISA includes the Turner-Himes amendment introduced by Reps. Mike Turner (R-OH) and Jim Himes (D-CT). (introduced House, passed Senate) The Turner-Himes amendment expands the definition of “electronic communications service (ECS) provider” to include “any service provider” that has “access to equipment that is being or may be used to transmit or store wire or electronic communications.” (except not personal dwellings and restaurants) Sen. Ron Wyden (D-OR) comment: “It allows the government to force any American who installs, maintains, or repairs anything that transmits or stores communications to spy on the government’s behalf. That means anyone with access to a server, a wire, a cable box, a wifi router, or a phone. It would be secret: the Americans receiving the government directives would be bound to silence, and there would be no court oversight.” EFF comment: “The Justice Department is playing word games when it says the amendment doesn’t change the ‘structure’ of 702 because the law prohibits targeting entities inside the United States. Garland’s pledge, isn’t worth the paper it’s printed on; if this amendment becomes law, the DOJ can and almost certainly will rely on it to conscript other providers who fit within its very broad scope.” Notably, Trump doesn't like FISA? (removed yelly caps) “Kill FISA, it was illegally used against me, and many others. They spied on my campaign!!!” Pelosi's speech was amusing: “I don’t have the time right now, but if members want to know I’ll tell you how we could have been saved from 9/11 if we didn’t have to have the additional warrants.” https://www.theverge.com/2024/4/18/24134196/senate-cloture-v... https://www.theguardian.com/us-news/2024/apr/12/fisa-surveil... reply WhereIsTheTruth 16 hours agoprevDemocracy baby! /s reply Buttons840 12 hours agoparentPeople who get elected are not like regular people. Until we fill at least one branch of government with randomly selected people, we don't have a democracy. reply badrabbit 12 hours agoprevThis is FISA right? The target is foreign individuals and entities? It seems by default HN is against it, can someone articulate why? There are elected representatives of the people providing oversight and it seems to have strong bipartisan support. Is there a popular line of thought with tech people that is suggesting foreign surveillance isn't neccesary? Or should some provision of the law be updated to protect americans' data? reply int_19h 58 minutes agoparentThere's so much information online on why and how FISA is bad, this is honestly hard to take as a serious question asked in good faith. But assuming that it is, start with the Wikipedia article for FISA and go from there; it has plenty food for thought, and links to more. reply okaydude100 18 hours agoprev [2 more] [flagged] hamhock666 18 hours agoparent [–] Yes if the people are not there to hold leaders accountable in a democracy, bad things like this will continue to happen reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "President Joe Biden signed legislation renewing Section 702 of the Foreign Intelligence Surveillance Act, a significant U.S. surveillance law.",
      "The Senate passed the bill with bipartisan backing after debates on FBI limitations, enabling the government to gather foreign intelligence through monitoring non-American communications outside the U.S.",
      "Proponents, including Biden and U.S. officials, emphasize the program's vital role in national security, while opponents advocate for stronger safeguards to protect American privacy rights."
    ],
    "commentSummary": [
      "The discussion encompasses surveillance programs, FISA reauthorization, political influence, and government transparency, raising concerns about privacy, abuse of power, and impacts on civil liberties.",
      "Additional topics include party politics, the role of intelligence agencies, and the difficulties of bipartisanship, shedding light on the complexities and tensions in government decision-making.",
      "The conversation underscores the delicate balance between security and individual freedom, emphasizing the intricacies involved in these governmental decisions."
    ],
    "points": 294,
    "commentCount": 199,
    "retryCount": 0,
    "time": 1713613204
  },
  {
    "id": 40101885,
    "title": "Building Mailoji: 300 Emoji Domains Yield $1440/year (2021)",
    "originLink": "https://tinyprojects.dev/projects/mailoji",
    "originBody": "Dark Mode Home Projects Guides Blog Daily Blog I bought 300 emoji domain names from Kazakhstan and built an email service March 11th 2021 TLDR; I bought 300 emoji domain names from Kazakhstan and built an emoji email address service. In the process I went viral on TikTok, made $1000 in a week, hired a Japanese voice actor, and learnt about the weird world of emoji domains. 🌅 The setup Not long ago I decided it would be a brilliant idea to buy the domain name netflix.soy. Whilst arguably there are better ways to spend £17, I did learn a lot about domain names, including that it's possible to have emoji domains like 😊.ws. It's pretty hard to go a day without seeing an emoji somewhere on the internet. Yet, I'd never seen an emoji domain name before. I wondered: Could I buy an emoji domain name? 💸 Buying an emoji domain name My goal was to buy a single character emoji domain name, like 💡 or 🍰. I didn't know what I'd do with it, I just wanted to see if I could get one. I found a website that showed every available emoji domain for 4 different extensions. Sadly, nearly every single one had been registered. I was late to the party. A simple mailbox emoji with a .ws extension was still available though, so I bought it. 📪 The mailbox 📪.ws was now mine. Mission complete. I set up a website and felt rather accomplished with my tiny mailbox. I could've stopped there and called it a day. But, then I had another thought: Could I use my little mailbox emoji domain in an email address? That'd be pretty cute. ✉ Emoji mail attempt #1 I gave it a go. I setup an email forwarder to route all email sent to 📪.ws to my regular email address. Eagerly I typed ben@📪.ws into the \"to\" field of gmail and hit send. 🛑 Blocked The email never hit my inbox. It was lost forever in cyberspace. Turns out emoji domain names score very highly for spam and were going to be blocked to high heaven. But, it was interesting that I could send mail towards an emoji email address. So I wondered: If a normal .com email address doesn't get blocked for spam, could I route my emoji mail through that? 💌 Emoji mail attempt #2 It would work like this: Email sent to ben@📪.ws ben@📪.ws forwards to nospam@normal.com nospam@normal.com forwards to my email address and won't get blocked. I cobbled together something using AWS, and tried my experiment again. to: ben@📪.ws message: Hi Ben, how's it going? Send. It worked! 🧨 Where things started to get out of control At this point I was inclined to stop and write a post about emoji email addresses. I'd had a good run. But then I wondered: My mailbox emoji email address is great and all, but do you know what would be better? ben@⭐ Now how do I get one of those? 🎣 Emoji domain name hunting Only 13 TLDs in the world accept registrations of emoji domain names: .cf, .ga, .gq, .la, .ml, .tk, .st, .fm, .to, .je, .gg, .kz, and .ws. The website I had used to purchase 📪.ws only showed 4 TLDs: .fm, .ws, .to and .ml. These are considered the gold standard of emoji domain name registrars. Every emoji had been taken on these though. You could of course get multi-character emoji domains like 🎉🐢.ws, but I wanted single character emoji domains only. So I wondered: Do any of those other TLDs have any emoji domains left? 🔭 The great hunt I already had some code that performed WHOIS lookups to see if a domain name is available for a list of TLDs. Previously I'd used this code to buy facebook.网站, only for Marky Z to snatch it back from me. Cheeky bugger. I booted up the code and loaded in some A-tier emojis (e.g. ⭐,😂,❤) and the 13 TLDs that accepted them. >node search.js [ENTER] 🎁 The results Instantly I was seeing results! .la, .ga, .gq, .je. There were plenty of emojis still out there on these alternative extensions. An extension that stood out to me straight away was .gg, for the Island of Guernsey. \"GG\" is an acronym for \"Good Game\", and I say it often when I lose at online games. It was perfect. ⭐.gg was available for €29. I hit purchase. 💔 No GG for me The next day Guernsey sent me an email. Long story short, although you could register emoji domain names with them, they didn't actually work. Good game Guernsey. Back to the drawing board. ⭐ Crazy for KZ With every other extension I kept hitting walls. A lot of the registrars wouldn't even let me search for emoji domains. Nothing was working. One extension that kept cropping up was .kz of Kazakhstan. But, I headed over to their registar website and it was entirely in Russian. I do not speak Russian. Using Google Translate, I tried to navigate the website and buy a .kz emoji domain. It was a long, painful process. But, after phoning my bank to confirm I was indeed trying to make a purchase using Kazakhstani tenge, ⭐.kz was sitting in my account. I plugged it into my email system and ben@⭐.kz worked. Very nice. 💼 Let's start an email service Something excited me. Nearly all single character emojis were available on .kz, and they were only $8 each. So, I wondered: What if you could get an email address with any emoji you wanted? I pictured email addresses like bob@🚀, alice@🌸, melvin@🍆. All I'd need to do is buy every emoji domain to build a service like this. It was insane, but it was possible. 🌙 The night of 150 emojis I decided I was going to do it. If I was chuffed with my mailbox emoji email address, perhaps others would be too. I got out my debit card, and, one by one, started buying Kazakhstan emoji domains. 💡.kz, 👑.kz, 🌈.kz, 😎.kz. Buy, buy, buy, buy. It was slightly painful watching my bank account going down, and the number of emoji domains go up. 80 emojis in, forking over money for a goat emoji domain name, you seriously start to question what you're doing. $1200 later, 150 emoji domains were mine. 💻 Building an emoji email address website Finally, I needed a website where you could register an emoji email address and it would forward mail like ben@📪.ws did. Using vanilla HTML, JS and CSS, plus Stripe's API for payments, I cobbled together an MVP over a few weeks. Once it was done, I bought one last domain name: mailoji.com. My new emoji email address service Mailoji was ready. Get your emoji email addresses. 📱 TikTok I'd gone from being curious about emoji domain names to now owning 150 of Kazakhstan's finest. The next step was to convince someone else to buy an emoji email address. TikTok seemed like a good place to start given its demographic. So, I recorded a short video advert and started a \"TikTok for business\" application to publish it. On the final page of the application I was asked for a VAT registration number. Mailoji was not a proper business yet, so there was no way I could publish my ad. Screw it, I'll post the video normally. Upload. 🎉 First sales Here is the advert if you're interested. The video sat at 0 views for about 5 hours before the TikTok algorithm started to work its magic. Slowly, the views started ramping up. 500 views, to 5k views, to 50k views. It was incredible to witness. People were loving emoji email addresses, people were hating emoji email addresses. It was like Marmite, a talking point. None of it mattered though because emoji email addresses were selling! @🚀, @📷 & @💻 addresses were the most popular. Over 2 days the TikTok video reached 200k+ views, and 60 emoji email addresses had been sold netting ~$300/yr in revenue. I took this as a fantastic indicator. So guess what I did? 💵 Buying more emoji domain names I decided to purchase 100 more emoji domains. I cried into my keyboard forking out yet more money for a llama emoji that I probably didn't need. In the end I had 250 emoji domains. If there was ever a moat into the emoji email address world, this was it. 📅 Preparing for launch I figured the more people with emoji email addresses, the more people who would see them, and the more people who would buy them. A beautiful cycle. My next goal was a Product Hunt launch to get exposure for Mailoji, and kickstart this cycle. I prepped my launch post, carefully choosing each word and image. I even created this over-hyped promotional video, complete with Japanese voice actor saying the words \"Mailoji\". Mailoji was ready for launch. 🚀 Launch Day At 12:03 AM PST Mailoji went live on Product Hunt. We had come a long way from that little mailbox emoji. It was 8:03 AM UK time. Bleary eyed, with a cup of tea in hand, I watched as Mailoji did battle. I had chosen to launch on a Wednesday against some stiff competition, but Mailoji really held its own. At the end of the day it finished in 5th place. Here were the end of day stats: 🌎 6.7k website views 💌 150+ emoji email addresses sold 💵 $830/yr ARR 🔺 320 upvotes 🏅 5th place on Product Hunt 🎀 Most popular Mailoji: @🚀 Over 150 emoji email addresses were sold in a day, and I received some fantastic feedback from the Product Hunt community. It was done, Mailoji had officially launched. 📙 The aftermath I wish this story ended with Mailoji blowing up and the queen registering an emoji email address or something (I'll reserve Liz@👑.kz just in case). But, currently Mailoji is sitting at ~$1440/year in revenue. There's now 300 emoji domains to choose from though. Even though I still haven't made the money back on all the emoji domains I bought, creating an emoji email address service was so much fun. It was an adventure. A rabbit hole containing multiple rabbit holes. This project started out as an exploration into emoji domain names; a weird, forgotten about internet feature that I've now become quite fond of. Yes emoji domains are hard to type on desktop, yes there's too many variations, and yes, most form validations hate them. But they're fun, and I think tech should be more fun. Thanks for reading. If you want to get in touch, I've got a brand new email address at the bottom of this website. I make tiny projects Get notified about new projects: Follow me: Twitter Instagram RSS Get in touch: tinyprojects@💡.kz",
    "commentLink": "https://news.ycombinator.com/item?id=40101885",
    "commentBody": "I bought 300 emoji domain names from Kazakhstan and built an email service (2021) (tinyprojects.dev)282 points by montyanderson 10 hours agohidepastfavorite44 comments gnabgib 10 hours agoEnormous discussion at the time (2021)[0](1683 points, 626 comments) [0]: https://news.ycombinator.com/item?id=26422799 reply ktosobcy 8 hours agoparentI wonder how many users does he have now :-) reply ch33zer 6 hours agorootparentOn the front page he says: 2k addresses registered If you take that as a live counter of the number of subscribers that's 2k a year, and pretty good! If you take that lifetime registrations that's not so good. Also assuming that that copy is accurate. reply boomboomsubban 8 hours agoparentprevI find it amusing thar about a sixth of the comments are on how couples should handle their finances. Nobody could have guessed that. reply MikePlacid 5 hours agorootparentHow to resist commenting on \"Darling, would you mind terribly if I spent £1000 on Kazakhstani emoji domains?\" ? )) reply carstenhag 1 hour agorootparentprevAlso found a comment from myself warning of using this anywhere near Germany (tldr kz stands for concentration camp). https://news.ycombinator.com/item?id=26423117 reply eskibars 5 hours agoprevI bought xn--mn8h9e.ws a couple years ago just for fun. I think it's fun to own an emoji domain, but what I can say definitely say is that it's still a bad idea to own one if you want to get emails to it. Popular thick email clients still struggle with utf8 domains and I've fiddled around with several email providers that just have complete failures trying to send as well. I tried pasting the email address in a bunch of popular services (LinkedIn, Instagram, etc) as my email or the domain as my homepage and most of them treat it as invalid, and I found some legitimate breaking bugs in their services in trying it out. Edit: case in point, just noticed HN also falls in the camp of unsupported emojis in the text body, so another example :). Added the punycode instead reply AndyMcConachie 2 hours agoparentAccording to the IETF standard for IDNA it is invalid. https://itp.cdn.icann.org/en/files/security-and-stability-ad... https://www.icann.org/en/system/files/files/idn-emojis-domai... reply marckohlbrugge 5 hours agoprevI discovered emoji .to domains ~7 years ago and put up a site listing all the available ones [1] Within a few days almost all of them sold and made a couple grand in affiliate commissions. I wrote about it here: https://marc.io/emoji-domains Email forwarding is also a clever use! Nice to get that recurring revenue. [1] https://xn--f28h.to/ reply xandrius 1 hour agoprevI'm 3 years late (better late than never I guess) but this was a brilliant read and idea: simple, neat and no bloat. Love it! reply spencersolberg 7 hours agoprevIt would be cool if they took the https://omg.lol approach and let you host a website/page at bob..kz if you purchase bob@.kz reply yosito 7 hours agoprevI find it a little disingenuous that the author keeps dropping the TLD and describing the emails as cool@, when the TLD is still part of the email address. Interesting experiment, anyway. I'm surprised that it worked well enough to get a functioning email service working with it. A lot of systems must assume that an email address or domain name wouldn't include emojis. reply lysium 16 minutes agoparentI was wondering how „bob@[rocket]“ works… Thank you for clarifying. reply theideaofcoffee 5 hours agoparentprevSame, switching between .cctld. and bare . made me question the author's understanding about DNS as a whole, those two are very much different things! Then again, the average tiktok viewer wouldn't, I assume, care too much about needing to tack on .kz to an email as long as there is an emoji in the address. But yes, it's an amusing (ab)use of punycode, but still fun in spirit. reply rkagerer 2 hours agoprevI bought 40 squeaker balls for my dog, those were also fun. reply davidw 6 hours agoprevThis sounds like a subplot of a Neal Stephenson story from back in the day. reply earslap 8 hours agoprevI would not trust Kazakhstan to honor the TLD registrations if this took off and made some noise. Reminds me of Libya taking ownership of all those trendy .ly domains claiming you have to obey Libyan laws and regulations to operate them. Still a fun idea taken quite far! reply freeone3000 7 hours agoparent>claiming you have to obey Libyan laws and regulations to operate them It being a CCTLD, this is a true claim. At a basic level, these tlds belong to the country, and that country sets the rules. Libya reclaiming from ICANN was a jerk move but their claims are absolutely right. (Most cctlds already have this requirement.) reply dc396 5 hours agorootparentLibya didn't reclaim anything from ICANN -- ICANN didn't have anything of Libya's. reply EGreg 7 hours agoparentprevI really don't understand how so many people on HN can complain about centralized control, but then so many (other) people are completely against Web3, solutions like Unstoppable Domains are able to let you own a domain and only transfer it if you sign with your key. Why don't more browsers read a Web3-based domain system like Freenames, Unstoppable Domains, ENS, or Filecoin Name Service? DNS is a federated database, but it is subject to domain seizure etc. at multiple levels. I've seen people complain that their domain operator can just \"steal\" their domain! If browsers won't do it, can't someone start a CCTLD (it's only $250K) and then read the blockchain to resolve the DNS records? I realize that this \"someone\" would be a central point of failure, but alas, that's how the Web currently still works. The best you can do is some sort of \"DNS multicast\" I think, but it would still be under the control of one company, sadly. Personally, I'm a bit surprised why the Web hasn't standardized onion links / magnet links / hashes of content / cids / whatever you want to call them. Tor and Beaker Browser have had it for a long time, and Brave too I think. DNS then becomes just a glorified search engine for a small subset of URLs (the ones without a long path / querystring). reply duskwuff 6 hours agorootparent> can't someone start a CCTLD (it's only $250K) No, they can't. ccTLDs are associated with countries. There's no process for creating one that doesn't involve having IANA recognize you as a country. You're probably thinking of the new gTLD process, which has only been open for applications once, for a brief period in 2012. It's not open to new applications, and the process for applicants was much more involved than a single payment. reply dc396 5 hours agorootparentICANN says the next round will happen in 2026. reply EGreg 5 hours agorootparentDo you have more information on this? If you can link to it, that would be great. How much would it cost this time around? reply vasco 3 hours agorootparenthttps://newgtlds.icann.org/en/next-round reply swores 1 hour agorootparentThanks for that link, and to (not-)answer GP's question on price: > \"While the application fee has not been determined, it will be set on a cost-recovery basis. The fee will ensure that the next round of the New gTLD Program is fully funded and does not require funds from ICANN's operating budget. As a point of reference, the application fee for the 2012 round of the New gTLD Program was US$185,000.\" From the FAQ page for the next round of gTLD sales, via the link shared above: https://newgtlds.icann.org/en/announcements-and-media/announ... reply dkarras 5 hours agorootparentprevbecause like most things blockchains (cl)aim to solve (primarily money and its transfer, but in this case ownership of domain names), those things do not really really have a centralization problem. 99.999% of the people do not, and do not need to worry about getting their domain name seized. cryptobros like to pose centralization as a huge problem where it really isn't so that they can peddle you scamcoins to pump and to feed their gambling addiction. \"web3 based domain system\" solves something that is at most a nuisance (and at best a necessary evil) by introducing massive problems into the equation, all to do something that isn't really a problem in practice (and it doesn't even do it, you admit there is still centralisation, so what did we gain by introducing all those problems, really?) reply EGreg 5 hours agorootparentSee, this is just dogma that gets repeated on HN. It's obvious to any person who honestly thinks about it, that having a third party in control of your DNS (i.e. what IP addresses it resolves to) means that your entire site can be rugpulled from under you. If it becomes big enough. For example: https://www.blackhatworld.com/seo/is-njalla-still-legit.1521... Now, you can say, \"most people don't care, they just have a small-time operation, just find a reputable domain operator who doesn't have a history of screwing people over.\" But that's exactly the use case for Web3 and blockchains in general. Why do you have to be forced to trust SOMEONE, with something as important as your brand / identity of your entire organization? And, for that matter, why should an entire community have to trust one guy who can change up the site at any time? That's not very secure, and many of you vehemently insist that no alternative should be made available, to anyone, \"because scam\"! You don't want browsers to even support it! As your site gets larger and more people rely on it, you don't want to have that major point of failure at any point. I know that some people on HN go so far as to say that banks freezing your money, and ICE seizing your domain name, are very desirable features of the Internet. So, then don't complain about censorship and deplatforming. You can't have it both ways! This just happened, for instance... ICJ officials are being threatened that their funds will be frozen: https://twitter.com/TomCottonAR/status/1781066997666607193 reply strogonoff 1 hour agorootparentYour domain name and the site it points to is not the end goal, what the site represents is. Your domain name is one of the means to get more people to know about you or to deliver your product. Decreasingly relevant, note, as no one types domain names usually (people search). As more people know about you via various channels (most centralized one way or the other: curated lists, social platforms, search), takeover of your domain name (or any other channel) becomes less of a risk. If you take Coca-Cola’s or Apple’s or Basecamp’s domain, they will barely feel it. Perhaps Basecamp could feel it, as it probably plays a bigger role in delivery, but I am sure they would have a procedure specifically to manage that risk. 99% of the time, if you run an ordinary %product%, should you worry that it will be you vs. the world and all of your channels are taken over? Currently, I’d say not. I could be wrong. reply EGreg 11 minutes agorootparentYou have to do a lot of mental gymnastics to justify why web3 is not needed. Here you literally argue that one’s brand name recognition is irrelevant, and you can be constantly moving domain names with no impact to your bottom line or your community. That requires more than just an assertion. Extraordinary claims require extraordinary evidence. reply berkes 2 hours agorootparentprevBlockchain only shifts this problem one up. You keep saying \"your website\" but any successful website will be \"our website\". There'll be an organization, company, community behind it. And now \"the person(s) with the private keys\" can rugpull it. Or do whatever they want. Yes. Maybe A DAO could solve that. But that means everything, including domain names is in there from the get-go. Which isn't how this works on practice. Blockchain technology is great for valuable assets owned by individuals. But much less so for groups and organizations that own valuable assets. And valuable domains almost exclusively fall under the latter. reply newaccount74 4 hours agorootparentprevSome senator posting vaguely threatening shit on Twitter... yeah, the block chain will definitely fix that /s reply ransom1538 7 hours agoparentprev\"claiming you have to obey Libyan laws and regulations\" I always smile when bosses want everything to be GDPR compliant. I am not sure why these laws are more important than the laws from the Chilean Navy. Why are we clicking on cookie popups? We think the EU is smarter than the PII laws from Cameroon? Elitism I say. My websites follow strict guidelines set by proper Constitution of Cameroon doctrines. Every fourth visit to my site we dump all contents in html form (obviously). reply wafflemaker 22 minutes agorootparent> Why are we clicking on cookie popups? Because people want to track us to make money from invading our privacy? You don't need a cookie consent banner if your cookies are needed to serve the client with your service. You can do analytics without cookies. So to answer your question - Why are we clicking on cookie popups? Because website owners don't want to stop selling your privacy and now have to inform you about it. reply askvictor 7 hours agorootparentprev> GDPR compliant. I am not sure why these laws are more important than the laws from the Chilean Navy. Purely market size. Europe is a large market. Same reason that just about every product is labelled with 'known to the state of California to cause cancer' - California is a large market. reply swores 1 hour agorootparentNot purely market size, though it's a very important part for sure. The other part is how likely a country is to try to enforce their laws, and what ability they will have to do so. Even if a hypothetical US company had an equal number of customers & revenue in Chile as in the EU, if either the Chilean law being broken is one that Chile never bothers to prosecute, or if the worst thing they could do should they find out about the law breaking is to block the service at a national firewall level but not levy any punishments (say, if the US company has no staff or assets in Chile, and the crime has no possibility for extradition or other international collaboration to punish) then the company would be a lot less likely to comply than they are with GDPR. Because most US companies aren't able/willing to serve EU customers without having servers, employees, and revenue, physically in the EU; therefore the worst case for getting caught breaking GDPR is considerably more worth avoiding than if it would just be the EU blocking access to your servers. reply Joker_vD 7 hours agoprevWait, aren't emojis explicitly prohibited from being used in IDNA? Or do the implementers just not bother to read the \"IDNA Rules and Derived Property Values\" table and simply allow anything that's a correct punycode? reply zinekeller 5 hours agoparentIn summary, countries trump IDNA (and as you notice, it's all ccTLDs - ICANN will stop you if you tried this with a gTLD). reply zer00eyz 7 hours agoprev>> Turns out emoji domain names score very highly for spam and were going to be blocked to high heaven. I turns out their mailbox.ws domain doesn't have SPF or good DKIM records. Might be part of the problem why google chucks it. Email is a shit show. I feel like getting emails out reliably is akin to black magic now. Without the right incantations your never going to know what happened or why. reply aidomi 8 hours agoprevDoes anyone know what happened to TinyProjects? I looked forwards to reading his updates and it looks like he stopped a while ago… reply quantumwoke 4 hours agoparentLooks like he focused on promptbase when that became successful. There was a daily blog for promptbase at some point that had some promising revenue numbers. reply cool_sound_ed 7 hours agoprevThis is really a fun idea and nicely written post reply 1-6 7 hours agoprevIt seems like a great way to fend off spam reply lifestyleguru 3 hours agoprevThis stopped feeling like fun the moment one person can hoard entire country's emoji set of domains... and I feel guilty for hoarding half a dozen of domains... reply ramijames 7 hours agoprev [–] This was a fun read. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author acquired 300 emoji domain names from Kazakhstan and created Mailoji, an emoji email service, which gained popularity on TikTok, earning $1000 within a week.",
      "Despite facing challenges, Mailoji was launched on Product Hunt, generating around $1440 in yearly revenue, offering a fun dive into the realm of emoji domains and enhancing the tech experience."
    ],
    "commentSummary": [
      "A user purchased 300 emoji domain names from Kazakhstan in 2021 and launched the email service tinyprojects.dev, sparking interest and debate about its success.",
      "Challenges have arisen due to compatibility issues with popular email clients, prompting discussions on hosting websites on emoji domains and the implications, including centralization, blockchain-based domain systems, GDPR compliance, website ownership, and data privacy laws.",
      "Some concerns were raised about potential spam and hoarding of emoji domains, while others are fascinated by the innovative concept and its potential applications."
    ],
    "points": 282,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1713655470
  },
  {
    "id": 40100672,
    "title": "Thunderbird Enhances Support for Microsoft Exchange",
    "originLink": "https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/",
    "originBody": "Thunderbird Adventures In Rust: Bringing Exchange Support To Thunderbird April 19, 2024 Heather Ellsworth 0 responses Microsoft Exchange is a popular choice of email service for corporations and educational institutions, and so it’s no surprise that there’s demand among Thunderbird users to support Exchange. Until recently, this functionality was only available through an add-on. But, in the next ESR (Extended Support) release of Thunderbird in July 2024, we expect to provide this support natively within Thunderbird. Because of the size of this undertaking, the first roll-out of the Exchange support will initially cover only email, with calendar and address book support coming at a later date. This article will go into technical detail on how we are implementing support for the Microsoft Exchange Web Services mail protocol, and some idea of where we’re going next with the knowledge gained from this adventure. Before we dive in, just a quick note that Brendan Abolivier, Ikey Doherty, and Sean Burke are the developers behind this effort, and are the authors of this post. April 2024 Community Office Hours: Rust and Exchange Support Historical context Thunderbird is a long-lived project, which means there’s lots of old code. The current architecture for supporting mail protocols predates Thunderbird itself, having been developed more than 20 years ago as part of Netscape Communicator. There was also no paid maintainership from about 2012 — when Mozilla divested and transferred ownership of Thunderbird to its community — until 2017, when Thunderbird rejoined the Mozilla Foundation. That means years of ad hoc changes without a larger architectural vision and a lot of decaying C++ code that was not using modern standards. Furthermore, in the entire 20 year lifetime of the Thunderbird project, no one has added support for a new mail protocol before. As such, no one has updated the architecture as mail protocols change and adapt to modern usage patterns, and a great deal of institutional knowledge has been lost. Implementing this much-needed feature is the first organization-led effort to actually understand and address limitations of Thunderbird’s architecture in an incremental fashion. Why we chose Rust Thunderbird is a large project maintained by a small team, so choosing a language for new work cannot be taken lightly. We need powerful tools to develop complex features relatively quickly, but we absolutely must balance this with long-term maintainability. Selecting Rust as the language for our new protocol support brings some important benefits: Memory safety. Thunderbird takes input from anyone who sends an email, so we need to be diligent about keeping security bugs out. Performance. Rust runs as native code with all of the associated performance benefits. Modularity and Ecosystem. The built-in modularity of Rust gives us access to a large ecosystem where there are already a lot of people doing things related to email which we can benefit from. The above are all on the standard list of benefits when discussing Rust. However, there are some additional considerations for Thunderbird: Firefox. Thunderbird is built on top of Firefox code and we use a shared CI infrastructure with Firefox which already enables Rust. Additionally, Firefox provides a language interop layer called XPCOM (Cross-Platform Component Object Model), which has Rust support and allows us to call between Rust, C++, and JavaScript. Powerful tools. Rust gives us a large toolbox for building APIs which are difficult to misuse by pushing logical errors into the domain of the compiler. We can easily avoid circular references or provide functions which simply cannot be called with values which don’t make sense, letting us have a high degree of confidence in features with a large scope. Rust also provides first-class tooling for documentation, which is critically important on a small team. Addressing architectural technical debt. Introducing a new language gives us a chance to reconsider some aging architectures while benefiting from a growing language community. Platform support and portability. Rust supports a broad set of host platforms. By building modular crates, we can reuse our work in other projects, such as Thunderbird for Android/K-9 Mail. Some mishaps along the way Of course, the endeavor to introduce our first Rust component in Thunderbird is not without its challenges, mostly related to the size of the Thunderbird codebase. For example, there is a lot of existing code with idiosyncratic asynchronous patterns that don’t integrate nicely with idiomatic Rust. There are also lots of features and capabilities in the Firefox and Thunderbird codebase that don’t have any existing Rust bindings. The first roadblock: the build system Our first hurdle came with getting any Rust code to run in Thunderbird at all. There are two things you need to know to understand why: First, since the Firefox code is a dependency of Thunderbird, you might expect that we pull in their code as a subtree of our own, or some similar mechanism. However, for historical reasons, it’s the other way around: building Thunderbird requires fetching Firefox’s code, fetching Thunderbird’s code as a subtree of Firefox’s, and using a build configuration file to point into that subtree. Second, because Firefox’s entrypoint is written in C++ and Rust calls happen via an interoperability layer, there is no single point of entry for Rust. In order to create a tree-wide dependency graph for Cargo and avoid duplicate builds or version/feature conflicts, Firefox introduced a hack to generate a single Cargo workspace which aggregates all the individual crates in the tree. In isolation, neither of these is a problem in itself. However, in order to build Rust into Thunderbird, we needed to define our own Cargo workspace which lives in our tree, and Cargo does not allow nesting workspaces. To solve this issue, we had to define our own workspace and add configuration to the upstream build tool, mach, to build from this workspace instead of Firefox’s. We then use a newly-added mach subcommand to sync our dependencies and lockfile with upstream and to vendor the resulting superset. XPCOM While the availability of language interop through XPCOM is important for integrating our frontend and backend, the developer experience has presented some challenges. Because XPCOM was originally designed with C++ in mind, implementing or consuming an XPCOM interface requires a lot of boilerplate and prevents us from taking full advantage of tools like rust-analyzer. Over time, Firefox has significantly reduced its reliance on XPCOM, making a clunky Rust+XPCOM experience a relatively minor consideration. However, as part of the previously-discussed maintenance gap, Thunderbird never undertook a similar project, and supporting a new mail protocol requires implementing hundreds of functions defined in XPCOM. Existing protocol implementations ease this burden by inheriting C++ classes which provide the basis for most of the shared behavior. Since we can’t do this directly, we are instead implementing our protocol-specific logic in Rust and communicating with a bridge class in C++ which combines our Rust implementations (an internal crate called ews_xpcom) with the existing code for shared behavior, with as small an interface between the two as we can manage. Please visit our documentation to learn more about how to create Rust components in Thunderbird. Implementing Exchange support with Rust Despite the technical hiccups experienced along the way, we were able to clear the hurdles, use, and build Rust within Thunderbird. Now we can talk about how we’re using it and the tools we’re building. Remember all the way back to the beginning of this blog post, where we stated that our goal is to support Microsoft’s Exchange Web Services (EWS) API. EWS communicates over HTTP with request and response bodies in XML. Sending HTTP requests Firefox already includes a full-featured HTTP stack via its necko networking component. However, necko is written in C++ and exposed over XPCOM, which as previously stated does not make for nice, idiomatic Rust. Simply sending a GET request requires a great deal of boilerplate, including nasty-looking unsafe blocks where we call into XPCOM. (XPCOM manages the lifetime of pointers and their referents, ensuring memory safety, but the Rust compiler doesn’t know this.) Additionally, the interfaces we need are callback-based. For making HTTP requests to be simple for developers, we need to do two things: Support native Rust async/await syntax. For this, we added a new Thunderbird-internal crate, xpcom_async. This is a low-level crate which translates asynchronous operations in XPCOM into Rust’s native async syntax by defining callbacks to buffer incoming data and expose it by implementing Rust’s Future trait so that it can be awaited by consumers. (If you’re not familiar with the Future concept in Rust, it is similar to a JS Promise or a Python coroutine.) Provide an idiomatic HTTP API. Now that we had native async/await support, we created another internal crate (moz_http) which provides an HTTP client inspired by reqwest. This crate handles creating all of the necessary XPCOM objects and providing Rustic error handling (much nicer than the standard XPCOM error handling). Handling XML requests and responses The hardest task in working with EWS is translating between our code’s own data structures and the XML expected/provided by EWS. Existing crates for serializing/deserializing XML didn’t meet our needs. serde’s data model doesn’t align well with XML, making distinguishing XML attributes and elements difficult. EWS is also sensitive to XML namespaces, which are completely foreign to serde. Various serde-inspired crates designed for XML exist, but these require explicit annotation of how to serialize every field. EWS defines hundreds of types which can have dozens of fields, making that amount of boilerplate untenable. Ultimately, we found that existing serde-based implementations worked fine for deserializing XML into Rust, but we were unable to find a satisfactory tool for serialization. To that end, we introduced another new crate, xml_struct. This crate defines traits governing serialization behavior and uses Rust’s procedural derive macros to automatically generate implementations of these traits for Rust data structures. It is built on top of the existing quick_xml crate and designed to create a low-boilerplate, intuitive mapping between XML and Rust. While it is in the early stages of development, it does not make use of any Thunderbird/Firefox internals and is available on GitHub. We have also introduced one more new crate, ews, which defines types for working with EWS and an API for XML serialization/deserialization, based on xml_struct and serde. Like xml_struct, it is in the early stages of development, but is available on GitHub. Overall flow chart Below, you can find a handy flow chart to help understand the logical flow for making an Exchange request and handling the response. Fig 1. A bird’s eye view of the flow What’s next? Testing all the things Before landing our next major features, we are taking some time to build out our automated tests. In addition to unit tests, we just landed a mock EWS server for integration testing. The current focus on testing is already paying dividends, having exposed a couple of crashes and some double-sync issues which have since been rectified. Going forward, new features can now be easily tested and verified. Improving error handling While we are working on testing, we are also busy improving the story around error handling. EWS’s error behavior is often poorly documented, and errors can occur at multiple levels (e.g., a request may fail as a whole due to throttling or incorrect structure, or parts of a request may succeed while other parts fail due to incorrect IDs). Some errors we can handle at the protocol level, while others may require user intervention or may be intractable. In taking the time now to improve error handling, we can provide a more polished implementation and set ourselves up for easier long-term maintenance. Expanding support We are working on expanding protocol support for EWS (via ews and the internal ews_xpcom crate) and hooking it into the Thunderbird UI. Earlier this month, we landed a series of patches which allow adding an EWS account to Thunderbird, syncing the account’s folder hierarchy from the remote server, and displaying those folders in the UI. (At present, this alpha-state functionality is gated behind a build flag and a preference.) Next up, we’ll work on fetching message lists from the remote server as well as generalizing outgoing mail support in Thunderbird. Documentation Of course, all of our work on maintainability is for naught if no one understands what the code does. To that end, we’re producing documentation on how all of the bits we have talked about here come together, as well as describing the existing architecture of mail protocols in Thunderbird and thoughts on future improvements, so that once the work of supporting EWS is done, we can continue building and improving on the Thunderbird you know and love. Tags: Development Exchange Rust 0 responses Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment. Post Comment",
    "commentLink": "https://news.ycombinator.com/item?id=40100672",
    "commentBody": "Bringing Exchange Support to Thunderbird (thunderbird.net)244 points by campuscodi 13 hours agohidepastfavorite119 comments WhyNotHugo 19 minutes agoI guess support for proprietary specifications is useful, but I really wish Mozilla would prioritise implementing standards like auto-discovery of email submission (SMTP) servers (rfc6186, 2011). Currently, users have to manually provide hostname and port of their SMTP server, which is likely fine for those of us on this website, but not at all friendly for the other 99.9% of the human population. The amount of effort require to implement all of Exchange is also probably orders of magnitude more than discovery of submission servers via DNS/SRV. I really don't get Mozilla's priorities. reply lousken 12 hours agoprevwhy implementing ews when it is already deprecated and will be removed in two years? https://techcommunity.microsoft.com/t5/exchange-team-blog/re... reply gerdesj 10 hours agoparentEWS is the only realistic way for \"not Outlook\" to work - MAPI is not exactly an open API. When EWS goes away so will rather a lot of customers, probably not enough to dent the bottom line (initially). Then it becomes apparent that all email is equal and Exchange online becomes a footnote in history. Microsoft used to do email and then they shat the bed. All a bit embarrassing on the surface but not really. Running email systems is a right old pain when all you really desire is data (and metadata) to mine and flog on to other data fetishists. Email is generally rather static and rather large in storage terms but it can yield gold from personal exchanges. Ideally you get someone else to take the pain (AWS, Google and co - yes they get to mine but they bear the costs too) but ensure the marks use Outlook (and they do). Then you change Outlook (loving the new Electron version) to store all credentials in your cloud. You use those creds to mine data within email held on other people's clouds. They front the cost of storage. Smashing. reply stackskipton 9 hours agorootparentWhatever you are smoking, I want some. Exchange Online isn't going anywhere and I doubt deprecating EWS is going to matter because where are those customers going to go? GMail where they would have to completely change how they do business and use their APIs instead? Nope, they will just bite the bullet and rewrite everything to use Graph API instead. As for Microsoft wanting EMail Data for mining vs not hosting, check out the MSFT revenue. It's not in Advertising space that's for sure. reply babolivier 12 hours agoparentprevAs Sean Burke puts it on the related bug on Bugzilla (https://bugzilla.mozilla.org/show_bug.cgi?id=1847846#c2): > At present, EWS is our best way to enable support for both Exchange Online and on-premise installations. > Graph API has been considered and may be considered again in future, but it currently provides narrower support than EWS and lacks some functionality for desktop applications. Even with the announcement that EWS support will be removed for Exchange Online, it's still valuable in the short term for enabling access for a wide userbase and in the long term for supporting users using on-premise installations. reply SV_BubbleTime 11 hours agorootparent>on-premise installations But really tho… The venn of “my work uses exchange on-prem”, “I use Linux desktop and can’t use outlook”, “I am aware of and would use exchange on Thunderbird” is pretty damn small. I think they’re making a mistake using EWS and planning on targeting on-prem with the same or more weight than online. reply saintfire 10 hours agorootparentI'd widen that to \"I don't want to use outlook\" which includes Linux installs but also all thunderbird windows users, I'd think. reply ascar 10 hours agorootparentYes, as a long time Thunderbird on Windows user I second this. reply naasking 8 hours agorootparentThunderbird is frankly the only Windows email client worth using if you do any development using email. The tools available as add-ons simply aren't so readily available for other options, especially not for free. reply groestl 11 hours agorootparentprev\"my work uses exchange on-prem\" is pretty large though, especially in headcount (enterprises). reply xnyan 10 hours agorootparentI don't know of any published numbers about specific user count (would be very interested to see them), but in terms of deployments it's gone from more than half on prem in 2018 to ~16% in 2023. Of that 16%, I wonder how many users are both willing and able to use an alternate client? reply hsdropout 9 hours agorootparentOf the organizations that have Exchange Server on premises, I'd bet the lions share are hybrid, with regular user mailboxes in the cloud, using the server(s) for application relays, etc. reply rdl 8 hours agorootparentprevI'm a Thunderbird-on-Windows-in-VM-on-Mac user for one on-prem and one web-but-can't-access-on-non-managed-windows-endpoint VM account. reply kbenson 11 hours agorootparentprevAnd yet it's exactly the situation I'm in. I use evolution on Ubuntu through a horizon virtual desktop, purely for better exchange support. I switched from thunderbird on windows to outlook on windows when I started having a lot more meetings to coordinate, and then evolution when a virtual desktop solution was rolled out and Linux was an option for desktops at work again. Quite a few other people in my department that just use thunderbird on Linux because they can't stand outlook or using the web version would happily have better outlook support. Perhaps there is an audience here and it just doesn't match your own experiences. reply abdullahkhalids 8 hours agorootparentDoes evolution calendar work seamlessly with Exchange? I used Thunderbird with the Exchange addon for three years and email and contacts worked without any problems, but calendar was bad. I could see my calendar but couldn't edit it from Thunderbird. reply janci 4 hours agorootparentNot seamlessly. It seems to work but is half-broken. Id does not properly pair invitations, updates and RSVPs from emails with auto-created items (by the Exchange server) and you end up with duplicates and mess. At leas that was my experience when I used this setup. Then moved to evolution for email and web outlook exclusively for calendar. Now I am on windows partly for lack of proper Outlook and Teams on linux (sad). reply SV_BubbleTime 9 hours agorootparentprevI’m not sure what you are talking about. When did I say where I was in that venn? I am Linux Desktop and hate outlook web. I am the target user, and the post about Rust and using EWS gives me almost no confidence in this. And I’m more aware that my case isn’t popular, and targeting on-premise is even less so. reply justsomehnguy 1 hour agorootparentprev> I think they’re making a mistake using EWS In 2024. Should they did it in 2012 the people would actually use it. For now - yes, anyone needing a thick mail client for Exchnage but not Outlook has figured their workarounds years ago. reply mairusu 11 hours agoparentprevBecause you basically have no other realistic choice for the time being when dealing with crap such as Exchange. reply gtech1 9 hours agorootparentEm Client (www.emclient.com) has supported Exchange for a while I think reply lousken 9 hours agorootparentprevAs a linux user, it's more like either die by the web version or migrate the company to another mail solution. And yes, I know evolution does ews as well, but i got tired of sync issues and having to switch from deb to flatpack because of it and other annoyances over the years. I mostly gave up on email anyway and switched to a better solution - teams (at least until July after which the deb package of v1 should stop working and v2 doesn't work for me in firefox - can't unmute myself in calls) /s reply shamiln 12 hours agoparentprevNot everyone is using Exchange Online. reply lousken 12 hours agorootparentonly like vast majority, yes, i know reply ocdtrekkie 10 hours agorootparentWe'll see how long that remains the case when Microsoft can't keep the Chinese government out of its cloud. reply lousken 9 hours agorootparent100%, but they are still not really interested in onpremises exchange, at least the latest version is 2019 which is no longer in mainline support, only security with no new version announced these days it looks like every email is a teams message anyways, so unless they also release onpremise teams server, i don't see a future of exchange very bright maybe everyone will move to google workspace in the future? reply ocdtrekkie 9 hours agorootparentTeams will crater in popularity once it's no longer bundled with Office 365. Nobody uses Teams because it's good, they use it because it was free with stuff they already had. 2019 is technically still receiving feature updates because of the delay of the new Exchange on-prem, though however that transition takes place will be wild. reply lousken 8 hours agorootparentI don't think we will be able to get rid of teams that easily 1) it's a PITA to migrate teams accounts between tenants not mentioning migrating stuff off teams to another solution 2) companies use teams as \"shared folders\" in respective channels 3) copilot which enterprises even pay extra for reply wkat4242 6 hours agorootparentTeams shared folders are simply SharePoint folders. Like, they literally are. So you're not dependent on teams for accessing them. reply thesumofall 3 hours agorootparentprevWorking for a non-tech company, I can tell you that most love Teams. It was a godsend during Covid, people haven’t really seen anything else, and in all honesty it does what it’s supposed to do reply insaneirish 5 hours agorootparentprev> Teams will crater in popularity once it's no longer bundled with Office 365. Nobody uses Teams because it's good, they use it because it was free with stuff they already had. And what if the unbundling doesn't affect the price? Don't confuse MSFT making mediocre products with them being commercially stupid. reply coryrc 9 hours agorootparentprevYou think outsourced point-and-click administrators of company on-prem servers could do better? reply wolverine876 9 hours agorootparentA valid point - you can't match Microsoft's resources and in-house expertise. But you are probably a much less appealing target. Also, you might be willing to lock it down more than Microsoft, which has to please all those millions of customers and wants to admin it at the lowest cost possible - including possibly minimizing support calls by using permissive settings - and not with the most security possible. reply ocdtrekkie 9 hours agorootparentprevYes. Managing Exchange is not that hard. Having everyone's eggs in one basket though is just dumb. reply ruszki 3 hours agorootparentNot that hard for you. I think, you’re overestimating the average employee. At my current company, the maintainers absolutely don’t have a clue how it works, and they are completely unusable when there is a problem with it. They don’t know even the basic things, they just blindly follow transcripts from Microsoft, like telecallers. reply hnarn 12 hours agoprevStrategically this makes sense if the goal is just to get people to use Thunderbird, but ideologically JMAP support would be a lot nicer in my opinion. reply ocdtrekkie 10 hours agoparentI imagine most of the work they accomplished here will be key in adding JMAP support in the future as well. reply Semaphor 11 hours agoprevI recently tried Thunderbird instead of Outlook. It had the same issue as FF did before the quantum update: It's too damn slow. Switching between different folders with hundreds or thousands of mails has noticable delays, while in Outlook it's essentially instant. reply eholk 11 hours agoparentIf you're using Windows, in my experience Thunderbird is essentially unusable until you add a Windows Defender exclusion for your Thunderbird profile. reply praseodym 2 hours agorootparentThis is very likely because Thunderbird uses mbox files, so one big text file per mail folder. There is experimental maildir support (one file for each email) which is friendlier for AVs: https://support.mozilla.org/en-US/kb/maildir-thunderbird reply justsomehnguy 1 hour agorootparent> There is experimental maildir support (one file for each email) which is friendlier for AVs One (whatever big) file is always way more 'friendlier' for the AV than a bazillion of files. Especially on NTFS and on Win32. No, don't try maildir on the Windows. reply magnat 1 hour agorootparent> No, don't try maildir on the Windows. Why, exactly? I have switched to maildir as soon as it was available as experimental feature, and performance gains when compared to mbox were enormous, especially during bulk operations. Switching folders takesExchange support for Thunderbird is the entire second half of the article. Sure, for Thunderbird developers. Regular users don't care about any of that, if they even understand what it says. reply selimnairb 11 hours agoprevHate Exchange but glad they are doing this. Now maybe I can successfully send plaintext mail on Exchange when confined to Windows. reply cqqxo4zV46cp 8 hours agoparentYeah, and have your email appear significantly worse on a bunch of email clients all in the name of tech purism / nostalgia for ‘the old days’. After I saw how usual it was for plain text email to be rendered in a fixed-width font, instead or something more sane, there’s no way I could justify doing it just because “HTML email is an abomination” or whatever. reply tjohns 7 hours agorootparentMost clients allow you to pick which font your plain text email is rendered in. Plain text gives the recipient control over how the content is rendered - as opposed to HTML which forces your choices on the reader. reply jbaber 7 hours agorootparentReaders don't know this. They just know your e-mails always look weird and quote other e-mails weirdly. I say this as a daily mutt user. reply zelphirkalt 56 minutes agorootparentLet illiterates wonder, one day they might set out to find out the reason and learn something on the way. reply Schnitz 6 hours agoprevWow! 20 years or so ago, I remember working in IT and only a few of us ran Linux in otherwise fully Microsoft shops. Exchange was always one of these things that it was tough to find a good client for. We would have been over the moon! reply small_scombrus 5 hours agoparentI'm over the moon about it now! Using anything Microsoft on Linux is really painful, and they appear to be in the process of deprecating the web version of Teams if you're not specifically using Edge :( reply zelphirkalt 59 minutes agorootparentAfaik Teams never worked on Firefox. MS' engineering is incapable or unwilling to use standard APIs or compatibility layers to implement a tool that works on all modern browsers, while competitors in voice and video chat have this for a long time already. reply anonzzzies 6 hours agoparentprevWe had the same issue however I took a different path; I got the company to dump exchange and sharepoint. Things were happier from then on. reply sega_sai 9 hours agoprevIt would be good to make Thunderbird a proper competitor to Gnome's Evolution when it comes to EWS. reply zozbot234 9 hours agoprevPlease include PST file import out of the box so that folks can seamlessly switch away from their Outlook installs without having to fiddle with Outlook itself. There is a free project for reading PST files, all it needs is integrating into the suite. reply DaiPlusPlus 9 hours agoparentPST export using the Exchange module for PowerShell has been a thing since Exchange 2010, see \"New-MailboxExportRequest\": https://learn.microsoft.com/en-us/powershell/module/exchange... EDIT: Oh, nvm - it's on-prem only - huh... that's asinine. reply SturgeonsLaw 8 hours agorootparentPSTs can be exported from Exchange Online via eDiscovery, although I'm curious why you'd want to, since it will only include whatever's in the mailbox anyway, which can presumably be accessed directly from the cloud when the mail client signs in. What's the use case with PST import in Thunderbird? Outlook clients with an IMAP/POP3 mailbox storing a local archive of mail that's since been deleted from the server? This can be uploaded to the Exchange Online mailbox through Outlook which would be more resilient and less brittle than a local PST. What scenario am I missing? reply technion 8 hours agorootparentJust noting that mail exported through ediscovery doesn't look like you'd expect - mail data is several folders deep. It's fine to say \"just a few more clicks\" but having inbox not be in an inbox folder is jarring breaks any idea of automated imports. reply packetlost 12 hours agoprevWe're desperately in need of SMTP/2 + IMAP5 or something reply jcranmer 10 hours agoparentIMAPv4r2 came out relatively recently, in 2021: https://datatracker.ietf.org/doc/rfc9051/ (The big differences are IMAPv4r2 mandates a lot of necessary new features, like UID or UTF-8 support, and actually deprecated silly old stuff like MUTF7 mailbox names.) reply packetlost 10 hours agorootparentDoes it support 2FA yet? reply gtech1 9 hours agorootparentnobody supports 2fa on every mail request, that's just silly. Even MS's MFA is nothing more than a token that gets stored locally ( aka, a password ) and which can be stolen and be used elsewhere ( like a password! ) by hackers. https://mrd0x.com/stealing-tokens-from-office-applications/ reply SturgeonsLaw 8 hours agorootparentAnyone know what that application is that's being used in that screenshot to search memory for strings? https://mrd0x.com/static/e0e177157e8596c60273e12d4b3bd695/4e... reply cess11 19 minutes agorootparentBit rusty on MICROS~1 but I think it could be WinDbg. reply f_devd 12 hours agoparentprevJMAP exists reply Aloisius 11 hours agorootparentPart of JMAP exists. JMAP calendar, contacts, sharing and sieve scripts aren't finalized yet. reply lloeki 2 hours agorootparentNone of that is handled by SMTP+IMAP? reply rdl 8 hours agoprevThis will be nice; I've been stuck using OWL for this purpose (which I think is the main extension; IDK if there's anything else.) reply AnonC 5 hours agoparentAnything that doesn’t work well that you’ve seen with the Owl extension? reply zelphirkalt 52 minutes agorootparentAt some point I somehow got logged out and then was unable to use the OWA login inside TB, because it required some silly JS to run. That meant Owl could no longer fetch e-mail. I contacted them about it, but they did not care and basically shrugged and did not give any advice. At some point I was able to switch to using OAuth to get my e-mail, without Owl. reply rdl 3 hours agorootparentprevTB is slow overall, mainly due to Defender. OWL also totally Broke last year when TB updated on a point release, fixed by OWL in days. reply Neil44 1 hour agoprevImagine if IMAP was slightly better and had a standard for calendar and contact objects. reply ForHackernews 11 hours agoprev> There was also no paid maintainership from about 2012 — when Mozilla divested and transferred ownership of Thunderbird to its community — until 2017, when Thunderbird rejoined the Mozilla Foundation. What a scandal this was. A prime example of Mozilla's backwards priorities. reply afavour 9 hours agoparentI disagree. The world moved to web/mobile app based mail. You’re allowed to not be happy about that (I’m not over the moon about it) but it’s the truth. It wasn’t worth Mozilla funding it. reply superkuh 12 hours agoprev>in the entire 20 year lifetime of the Thunderbird project, no one has added support for a new mail protocol before. Technically true but also every megacorp's OAuth2 out-of-band authentication implementation needs it's own special configuration (read workaround) per email client and Thunderbird has collected quite a few. These are not normal mail protocols: they're over HTTPS not IMAP or POP3 or SMTP. This proclamation \"no one has added support for a new mail protocol\" is a good thing and this change is not good. Supporting proprietary setups is pragmatic and understandable but it's not good. This is only going to briefly mitigate the problems of email splintering into dozens of per-corporation variations while encouraging people to be okay with them in the long run. reply fabrice_d 12 hours agoparentEmployees have no choice when it comes to their corporate email provider, and this is not a hill to die on for 99.99999% of them. On the other hand, being able to use Thunderbird as a client is a net win and a pragmatic move. reply Ringz 12 hours agorootparentThe sad fact is: Most employees aren’t able or allowed to install an alternative email client. And most employees don’t care. reply memco 11 hours agorootparentYears ago I worked for a place used exhange and third party tools were heavily discouraged but not banned. Web access wasn’t supported either so we were almost forced to have to use Outlook on a Windows machine. But I found DAVmail[0] allowed me to use the apps I wanted without being locked in and without mail clients supporting it directly. Nice to see more options here. Hopefully those of us who do care can continue to make the choices to use the tools we like. [0] https://davmail.sourceforge.net/ reply kbenson 11 hours agorootparentprevAlthough, I would assume of those that can run what they want and do care they are already aware of and possibly use Thunderbird. Not every initiative has to just be about new users. Sometimes it's important to retain the ones you have. Making Thunderbird more useful and viable for those that use it already isn't a bad thing. reply wolverine876 11 hours agoparentprev> every megacorp's It's not just one of every megacorp, it's by far the most commonly used email in business, Microsoft's Exchange and especially Exchange online. reply wkat4242 6 hours agorootparentIt's also cheaper per head than Google's business offering so it makes sense even for small business. reply superkuh 6 hours agorootparentprevI was talking about proprietary OAuth2 authentication protocol implementations in that statement. Like how the Thunderbird OAuth2 protocol for gmail is incompatible with the one for yahoo email, and so on. Which is different from normal protocols like imap/pop3 where the implementations are compatible across companies. I was not speaking in the context of exchange but in the more general sense of new HTTPS based protocol flavors being added to Thunderbird regularly. reply Scarbutt 11 hours agoprevIs thunderbird search on par with gmail's search? reply promiseofbeans 10 hours agoparentIt's not bad - their time range narrowing graph thingy is really good, and I can normally find what I need. My main complaint is that my outlook calendars don't show up, so I can't search them for events. reply kevincox 8 hours agorootparentMy biggest complaint is that there is a hardcoded result limit. And it searches for that whole limit, then renders the result. So if you have a lot of messages you can't use the date range thing to refine the results. And if you want one of the recent ones you still need to wait for the UI to lock up and return all of the results. The results themselves are good enough. reply akvadrako 2 hours agoparentprevIt returns exact matches in chronological order, which is generally good enough. reply diarrhea 11 hours agoparentprevIt is the worst search I have ever used in any product. reply hiepph 17 minutes agorootparentSadly, I have the same experience. It's one of the minus points of using Thunderbird, alongside the lack of the feature for shipping my configurations (e.g. filters) across machines. reply COGlory 9 hours agorootparentprevIt is truly horrid. I routinely have to go into the web client to search reply SV_BubbleTime 11 hours agoprevThis post gives me extremely little hope. One one hand… ok, let’s say it’s an engineering post written by devs for devs. OKAY, talk about Rust if you like. Devs might be more interested in the cause than the effect. On the other hand… is this written for devs? Seems written for users. And I for one don’t give one half of one shit what language you use as a customer. It’s a post about Exchange, I don’t want to hear about new fangled language. I don’t pay you use a specific language, I pay you to deliver a specific feature… now obviously I don’t pay them in anything but time, donation, and reputation. But I think the point applies. No one but Rust Evangelicals care about doing something over in Rust. There isn’t a single end feature that you can deliver in Rust but not C. It reads to me like the developers are nerding out on a detail while being slightly uncommitted to the thing they “are paid to make”. I have to agree with the other users that MS has already set an EOL on the feature that TB is planning to use. So… woohoo Rust? reply diarrhea 11 hours agoparent> There isn’t a single end feature that you can deliver in Rust but not C. There are! Namely, when it’s open source with volunteer-ish developers, who cannot be arsed to not use Rust, as it’s simply that much more pleasant to be spending your time with. So in some sense, it’s either done in Rust because the implementers want to for one reason or another, or just not at all, as no one can be forced. reply SV_BubbleTime 9 hours agorootparentThat isn’t even remotely a counter to what I said. There is not end feature you can do in Rust that you cannot do in C. End of story. Cool, it’s popular among enthusiasts. That’s fine. reply small_scombrus 5 hours agorootparent> There is not end feature you can do in Rust that you cannot do in C. Sure, but you could create the whole thing in brainfuck if you really wanted to. Having something be nice to work with or extend is genuinely a feature reply sunshowers 9 hours agoparentprevWhile it's all software that compiles down to assembly in the end, there's a lot of code that is monstrously difficult to do in C, especially at scale, but relatively straightforward to do in Rust with 1-5% of the dev time. There are also many skilled developers who would simply refuse to do a project like this in C, but would be happy to do it in Rust. As a simple example, writing portable I/O code can be a huge burden in C, but has been abstracted away through libraries like mio in Rust. reply SV_BubbleTime 8 hours agorootparentCode compiles down to machine code. Not assembly. If I give you assembly and disassembly, you’ll prefer the former. Ok. You find it difficult to do things in C. Understood. I believe the topic is of end features, bit maintenance or abstraction. reply jcranmer 8 hours agoparentprev> No one but Rust Evangelicals care about doing something over in Rust. There isn’t a single end feature that you can deliver in Rust but not C. Strictly speaking, no. But there are many ways that the quality of life of writing code is so much higher in Rust than C. A non-exhaustive stuff of just the things I've done in my most recent project: * String handling. C's idiomatic string interface (i.e., null-terminated strings) is a dumpster fire of an interface that makes security vulnerabilities far more likely. And the set of string functions in the C standard library are a joke. * For that matter, memory management. Sure, I can write malloc and free manually, and have to manually remember whenever I get a pointer whether or not I'm responsible for freeing it, or if the pointer is only valid until the next function call, or stuff like that. But scale that to 100KLOC, and the ability of programmers to remember all of those details turns out to shockingly poor. Meanwhile, in Rust, it's a compiler error if you get it wrong, rather than being a crash or worse. * Asynchronous programming. Rust has built-in coroutine support; C does not. * Better idiomatic error handling. Handling an error in Rust is very often just a single ? character. No more chance of gotofail! * Newtypes, which are a godsend if you want to implement a parse-don't-validate strategy. * Better threading support. Rust has a much richer standard library for writing multithreaded applications, and it provides much better compiler support for it (Send and Sync traits are absolute godsends for annotating what can and can't be used from multiple threads). And if you go out into common crates... rayon is a better way to do embarrassingly parallel code than OpenMP is, especially if you want to do fancy stuff like non-trivial reductions. (Side note: the first successful attempt at a multithreaded HTML rendering engine was written in Rust, and that's not for a lack of trying with C++ code. That is how much of a godsend Rust ends up being.) I am by no means a member of the Rewrite-it-in-Rust brigade; I'm pretty sober about the costs of rewrites and the benefits you'd get from such a thing. However, I will say that Rust is generally a superior option than C/C++ if you're doing parsing code (which means you're dealing with untrusted input that you really want to be sure isn't going to go haywire on unexpected input). And for email protocols, where you deal with a lot of mixed binary and text in the same stream, Rust's string handling support is in a sweet spot, especially compared to most other safe VM languages which have different types for binary strings and text strings. Also, knowing enough of the Thunderbird internals to know what code already does need to be rewritten, I'm fairly confident of where I can sensibly propose that things ought to be written in Rust in lieu of C++ or JS. reply jaylittle 10 hours agoprevWho the fuck cares? Exchange is dead. Office 365 is where its at. Move on Thunderbird team. reply gtech1 9 hours agoparentOffice 365 is dead too. reply wkat4242 6 hours agorootparentNo it's still around: https://lazyadmin.nl/office-365/microsoft-365-e3-vs-office-3... And it probably will be until Microsoft stops selling lifetime windows licenses because the biggest point of M365 is windows as a service. Even if you buy the other components like Intune and entra ID separately you're still way cheaper off with lifetime windows licenses. Which is what we do at work. reply counterpartyrsk 11 hours agoprev> it’s no surprise that there’s demand among Thunderbird users to support Exchange. Uh, I'm surprised. How many people actually love thunderbird? And to the extend it justified the development. reply promiseofbeans 11 hours agoparentEnough to raise >6m in donations in 2022: https://blog.thunderbird.net/2023/05/thunderbird-is-thriving... reply vedmed 3 hours agoparentprevThunderbird is paramount to my workflow and has been for over a decade. If I did not have thunderbird I would be upset. That said, loving a software requires a paradigm shift for me. I can't fathom loving something that is not alive. I do enjoy it, however. reply wolverine876 11 hours agoparentprevAre you suggesting that no development is justified? I think we can expect that for any email client, there will be demand for one of the most popular email servers, and one that is required for most corporate use. reply DrewRWx 11 hours agoparentprevDo you have a use case you'd prefer they put their resources into implementing? reply ForHackernews 11 hours agoparentprev> How many people actually love thunderbird? I love Thunderbird and give them money every month. Name another good open source mail client. reply Animats 6 hours agoprevOh, Microsoft Exchange. I was thinking crypto support. reply Ringz 12 hours agoprev [–] This is good news and I'm looking forward to Thunderbird natively supporting XChange. But everything I read about it triggers my inner voice: „Do a parallel rewrite of the whole damn 20 year old codebase in Rust! It’s faster, cheaper and cleaner. Do it right now. Don’t discuss it in mailing lists. Take 12 competent software developers and let them rebuild everything.“ reply robertlagrant 12 hours agoparent [–] Faster and cheaper to write an email client from scratch? Including HTML rendering? Are you sure? reply Ringz 12 hours agorootparentNo. Leave the html rendering to Firefox. Edit: As it has already been handled by Thunderbird in the past. reply Dalewyn 12 hours agorootparentprev [–] When all you have is Rust, everything looks like magic. reply Ringz 11 hours agorootparent [–] Really? Good to know! Because I don't know anything about Rust yet. Only C++ and Python. But I think that if you know a programming language sufficiently, everything looks like magic, or is that not the case with you? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Thunderbird is introducing built-in compatibility for Microsoft Exchange in the next ESR release, initially concentrating on email features and later on calendar and address book integration.",
      "Developers opt for Rust due to its advantages in memory safety, performance, and modularity, navigating technical obstacles to incorporate HTTP requests and XML management for Exchange compatibility.",
      "Future endeavors involve rigorous testing, refining error management, broadening protocol compatibility, and detailing the implementation for Thunderbird's user base."
    ],
    "commentSummary": [
      "Implementing Exchange Support in Thunderbird involves debates on protocols, on-premise installations, Evolution Calendar, and Microsoft products on Linux.",
      "Discussions also include opinions on Thunderbird's search function and the advantages of using Rust for multithreaded applications over C.",
      "Users propose enhancing Thunderbird by adding Exchange support and potentially rewriting the codebase in Rust for improvements."
    ],
    "points": 244,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1713644346
  },
  {
    "id": 40103407,
    "title": "Programming Emphasizes Thinking Over Typing",
    "originLink": "http://agileotter.blogspot.com/2014/09/programming-is-mostly-thinking.html",
    "originBody": "Tim Ottinger's thoughts on Software Development. Monday, September 29, 2014 Programming Is Mostly Thinking Pretend you have a really great programming day. You only have to attend a few meetings, have only a few off-topic conversations, don't get distracted or interrupted much, don't have to do a bunch of status or time reporting, and you put in a good six hours of serious programming [note: this RARELY happens in an 8-10 hour day]. I want to review your work in the morning, so I print out a diff of your day's work before going home. Sadly, overnight the version control system crashes and they have to recover from the previous day's backup. You have lost an entire day's work. If I give you the diff, how long will it take you to type the changes back into the code base and recover your six-hours' work? Programming is 11/12ths Thinking I've been touting this figure for some time now, and people keep asking me where the study is that produced such an odd number. Well, it's not pulled out of thin air and it's not the result of a thorough scientific study. I have done informal polls now for a few years, though I've not kept good records. My goal was not to become the scientist who cracks the statistical/mathematical code for programming activities. I was looking for a reasonable answer to a reasonable question. However, this answer surprised me. In a long Quora post titled \"How do programmers code so quickly?\" one responder offered that it was a combination of physical skills (muscle memory, skill with tools, debugging skills, typing skill) and knowing where to search for info. His post was swamped and overwhelmed by posts explaining that typing and tools are not the most important aid to quick code production. Software Factories I have seen the stickers and slogans on stickers and social media for a long time that \"typing is not the bottleneck\" (though every once in a while the inability of some programmers to type is a bottleneck). I am keenly aware that most management still subscribes to the idea that motion is work. They are fairly convinced that a lack of motion is a lack of work. That makes sense in a lawn care service, a factory assembly line, or a warehouse operation. Nearly all of the visible work done in producing physical goods is motion. People roll steel, stamp, press, mill, pick and place, bolt/screw/rivet, and on. Modern factories produce goods with Computer Numerical Control machines, which produce perfect copies of an original model that may not even exist in real life. These machines work from abstract models -- just data, really -- and perform perfect motion. Humans tend the machines, rather than working the wood by hand. I have some great guitars that were produced at affordable costs because of the degree of automation brought by such machines. Great boutique guitars are produced entirely by hand at higher cost and I don't put down that effort either. The world has room for both. Software developers have perfected the factory. It runs flawlessly bit-perfect copies. You just click the \"copy\" or \"download\" button. It's so cheap that the purchasers happily cover the costs of the factory. Those who are cautious will double check the checksums that come with the download, but most people don't bother. The machines are reliable and efficient and quick and cheap. Once the initial model (really, just data) exists, then the marginal cost of all the bit-perfect copies is essentially zero. Yes, this is just copying and not creating, but that's what factories do. Custom shops might produce unique items (like guitars) but factories create copies of originals. The software factory tends to give you a progress bar, so you can visualize the motion of bits, but in many ways you can say that the product doesn't really exist. It's a pattern of tiny charged v. uncharged areas of metal on a plate (well, probably) and you don't even pay for the plate or the magnet or the laser when you create the copy. It's already there. Software is an intellectual good. The Design Shop In my years of working with Uncle Bob Martin, I heard him continually tell customers and students that software development is not a fabrication operation, but a design operation. Once the initial design is done, all the duplication is done by machines at nearly zero cost. So what programmers and testers and POs and Scrum Masters and software management area all doing (if they're doing it right) is designing the data model that will later be used by the factory to create copies for use by customers, patrons, and other people in the community the software is intended to serve. Yet the mechanistic, Industrial-Age idea of software development as a factory persists, and developers dutifully try to make it look like they're doing physical labor at the detriment of the process. All intellectual activities are hard to observe and monitor. An idea that is 80% complete has no physical manifestation. It's an idea, and it's not done yet. Sometimes we have experiments or proof-of-concept code or notes, but they don't give an accurate \"% complete\" number as does physical work. A chair being manufactured looks about 50% done at the 50% mark. When it's done, it looks done. A design for a chair may not exist on paper until it is more than 70% complete. And we don't know that it's really 70% done, because it's not finished being designed yet. The Answer: Really? I have asked this question at conventions, client companies, to my peers, to colleagues, and to strangers I have met for the first time when I find out they are programmers. The answer I receive most often is \"about half an hour.\" I could use the 8-hour day, ignoring meetings and interruptions and status reports, but that feels like padding the answer. I stick to the six hours doing things that programmers identify as programming work. There are twelve half-hours in six hours. One half-hour to retype all the changes made in six hours of hard programming work. What in the world can that mean? How can it be so little? The Meaning Behind the Answer Right now I suspect a bunch of managers are about to go yell at their programmers for putting in a half-hour's work in an 8-hour day! That would be a horrible misunderstanding of what was actually happening. What is really happening? Programmers were typing on and off all day. Those 30 minutes are to recreate the net result of all the work they wrote, un-wrote, edited, and reworked through the day. It is not all the effort they put in, it is only the residue of the effort. Programmers are avoiding defects as best they can. In order to do that, they have to be continuously evaluating the code as they write it, hypothesizing the kinds of defects or security vulnerabilities they might be introducing. After all, they receive their harshest criticism for introducing defects into the shared code base. Programming is a kind of lossy compression. The code only says what the program must do when it is running. Why a programmer chose one particular way over others, how it influences the rest of the system, what errors were introduced and removed, and what pitfalls it avoids are not (generally) present in the text of the program. Most of the work is not in making the change, but in deciding how to make the change. Deciding requires us to understand the code that already exists. This is especially time-consuming when the code is messy or the design is not very obvious in the source code. Programmers work in a social context since all their results are integrated into a shared code base (and most use pair programming or other \"many eyes\" techniques). Programmers may be helping other programmers or testers or operations people get a handle on their work. Connecting and communicating with others has benefits and costs that don't appear in the code. Six hours of intellectual work (reading, researching, deciding, confirming, validating, verifying) translates to about 30 minutes worth of net lines-of-code change to a code base. That's not additional lines of code. We often have weeks when we fix bugs and add features and have fewer lines of code at end of the week than we had at the beginning of the week. I once got in trouble for having multiple weeks where we had negative lines of code -- we didn't know the 'grand boss' over our team was reporting SLOC as if it measured progress. Sigh. Programmers will gladly explain that the work they did was reading, learning, understanding, sometimes guessing, researching, debugging, testing, compiling, running, hypothesizing and disproving their ideas of what the code should look like. In short, they were thinking and deciding. Most of what goes on is intellectual work. One of the quora responders wrote: You see the fingers flying over the keyboard; you don't see the hours spent in talking to users, discussing the problems with coworkers, doing research and thinking the problems through. Another suggested: I achieve it firstly (to the extent that I do) by 'helping' the customer to eliminate the unnecessary notions from their idea, which they often mistakenly call 'requirements' and sometimes even say they are 'must have'. This is the biggest possible acceleration in the delivery of a solution because I can do an infinite amount of no work in no time at all. And yet another: Really good developers do 90% or more of the work before they ever touch the keyboard; really understanding the requirements and devising an appropriate solution. These are not unique unusual answers. I find that most of the time, \"knowing what not to write\", \"doing less,\" \"working in smaller steps\", and \"having first figured out what to do\" are common answers. Programming is much more about thinking than about typing. I have examined a lot of the change logs (diffs). It has consistently looked like 30+/-10 minutes of change on a good day (at least to me). I'm confident enough to tout this number as effectively true, though I should mention that no company I work with has so far been willing to delete a whole day's work to prove or disprove this experiment yet. Remember, I have only estimates and examinations of daily diffs to work from. The result here is not scientific. I should also let you know that people who do more typing or more cut/paste are often doing less thinking and understanding, which results in more errors and more burden on other programmers to understand and correct their code. Code is just the residue of the work. So What? If programming is 1/12th motion and 11/12ths thinking, then we shouldn't push people to be typing 11/12ths of the time. We should instead provide the materials, environment, and processes necessary to ensure that the thinking we do is of high quality. Doing otherwise is optimizing the system for the wrong effect. What if we changed our tactics, and intentionally built systems for thinking together about software and making decisions easier to make? I think that productivity lies in this direction. So I invite you: how can you experiment with learning on-the-job to create systems where the thinking is optimized? Email ThisBlogThis!Share to TwitterShare to FacebookShare to Pinterest 2 comments: A J MishraJune 29, 2023 at 12:09 AM Thank you so much for such an informative blog post! Check out more about How to Ensure Quality in Software Development and share your thought about it. ReplyDelete Replies Reply BuonarottiApril 21, 2024 at 1:15 AM thanks for the post - diseño web vitoria ReplyDelete Replies Reply Add comment Load more... Newer Post Older Post Home Subscribe to: Post Comments (Atom) Grab The Feed Posts Comments Tell Me Anonymously In addition to comments (which everyone sees) you can also leave me an anonymous comment at SayAt.Me Buy Me A Taco! You can send a donation, perhaps the price of a taco, via paypal.me/TimOttinger Industrial Logic We at Industrial Logic have taken our practice beyond run-of-the-mill Agile or Scrum or even XP. We are now \"anzeneers.\" We build safely to build safety for all. Popular Posts Preserving Wasteful Practices Habit is powerful. Inhabitants of intersection set People who place symbolic value on the wasteful practice People who have bu... Is It My Fault You Can't Handle The Truth? You can't handle it! In 2018 or 2019, I was introduced to the idea of hyper-rationality. I think it was under another name (to ... Maximize Value, not Quantity I was chatting with a manager who was once a PO on a team I coached many years ago. This is only my best memory of the conversation (I didn&... Programming Is Mostly Thinking Pretend you have a really great programming day. You only have to attend a few meetings, have only a few off-topic conversations, don&... 14 Weird Observations About Agile Team Velocity (note: I added a 15th, but was worried that changing the title would invalidate links, so you get a bonus observation at no extra cost) I ... Bug Teams v. The Nature Of Defects How it Happens You realize that you're not getting as much done as you expected to get done. It's troublesome because you have pl... I Want Agile Back Note: this was originally all plain text and a little shorter. As more people have joined the conversation, and other supportive materia... Preplanning Poker: Is This Story Even Possible? The story says \"attach an e-commerce server.\" Well, maybe it says \"As a product manager I want my system to incorporate an ... Splitting Stories - A Resource Listicle I've noticed that for several years now, one of the most frequently asked questions in agile forums deals with the splitting of stories... Defending Scrum Against Stupid Arguments I'm not a big scrum promoter, but I am VERY familiar with scrum and have coached many teams and always been able to improve their succes... Agile In A Flash Get your copy of Agile In A Flash today! Maybe outfit your whole team... About Me Agileotter View my complete profile Simple theme. Theme images by gaffera. Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=40103407",
    "commentBody": "Programming Is Mostly Thinking (agileotter.blogspot.com)223 points by ingve 4 hours agohidepastfavorite89 comments t43562 1 hour agoThis is a good article to send to non-programmers. Just as programmers need domain knowledge, those who are trying to get something out of programmers need to understand a bit about it. I think I recognise that tiny diffs that I might commit can be the ones that take hours to create because of the debugging or design or learning involved. It's all so easy to be unimpressed by the quantity of output and having something explained to you is quite different from bashing your head against a brick wall for hours trying to work it out yourself. reply figassis 49 minutes agoparentThis. The smallest pieces of code I’ve put out were usually by far the most time consuming, most impactful and most satisfying after you “get it”. One line commits that improve performance by 100x but took days to find, alongside having to explain during syncs why a ticket is not moving. reply serial_dev 2 minutes agorootparentAt my org, some clever guy up the chain is pushing for managers to monitor the lines of code changed. I thought it was just a proposal, but my manager told me \"there is nothing to worry about for you, you are doing very well\". Unrelated to this shenanigans, I wanted to make a change that removes 300KLoC, so I did. I can't wait to be accused of playing a system that is officially not even in place. reply AlotOfReading 3 hours agoprevI'm confident enough to tout this number as effectively true, though I should mention that no company I work with has so far been willing to delete a whole day's work to prove or disprove this experiment yet. Long ago when I was much more tolerant, I had a boss that would review all code changes every night and delete anything he didn't like. This same boss also believed that version control was overcomplicated and decided the company should standardize on remote access to a network drive at his house. The effect of this was that I'd occasionally come in the next morning to find that my previous day's work had been deleted. Before I eventually installed an illicit copy of SVN, I got very good at recreating the previous day's work. Rarely took more than an hour, including testing all the edge cases. reply dailykoder 2 hours agoparentI don't have a big sample size, but 2/2 of my first embedded jobs both used network shares and copy+paste to version their code. Because I had kind-of PTSD from the first job, I right off asked the boss on the second job if they had a git repository somewhere. He thought that git is the same as Github and told me they don't want their code to be public. When they were bought of by some bigger company, we got access to their intranet. I digged through that and found a gitlab instance. So then I just versioned my own code (which I was working on mostly on my own), documented all of it on there, even installed a gitlab runner and had a step-by-step documentary on how to get my code working. When they kicked me out (because I was kind of an asshole, I assume), they asked me to hand over my code. I showed them all of what I did and told them how to reproduce it. After that the boss was kinda impressed and thanked me for my work. Maybe I had a little positive impact on a shitty job by being an asshole and doing stuff the way that I thought would be the right way to do it. Edit: Oh, before I found that gitlab instance I just initialized raw git repositories on their network share and pushed everything to that reply datascienced 1 hour agoparentprevBad boss or zen teacher, we will never know! reply scotty79 2 hours agoparentprevWas your work better or worse second time around? reply AlotOfReading 2 hours agorootparentProbably a bit of both, but hindsight helped. It doesn't usually end up exactly the same though. Regardless, whatever I wrote worked well enough that it outlived the company. A former client running it reached out to have it modified last year. reply bernardlunn 1 hour agorootparentWith writing the second version is definitely better, sucks having to redo but improvement makes it worth while. reply smackeyacky 3 hours agoparentprevCrikey what a sociopath to work for. I’m sorry this happened to you. reply lordnacho 2 hours agoprevThis is why domain knowledge is key. I work in finance, I've sat on trading desks looking at various exchanges, writing code to implement this or that strategy. You can't think about what the computer should do if you don't know what the business should do. From this perspective, it might make sense to train coders a bit like how we train translators. For example, I have a friend who is a translator. She speaks a bunch of languages, it's very impressive. She knows the grammar, idioms, and so on of a wide number of languages, and can pick up new ones like how you or I can pick up a new coding language. But she also spent a significant amount of time learning about the pharmaceutical industry. Stuff about how that business works, what kinds of things they do, different things that interface with translation. So now she works translating medical documents. Lawyers and accountants are another profession where you have a language gap. What I mean is, when you become a professional, you learn the language of your profession, and you learn how to talk in terms of the law, or accounting, or software. What I've always found is that the good professionals are the ones who can give you answers not in terms of their professional language, but in terms of business. Particularly with lawyers, the ones who are less good will tell you every possible outcome, in legalese, leaving you to make a decision about which button to press. The good lawyers will say \"yes, there's a bunch of minor things that could happen, but in practice every client in your positions does X, because they all have this business goal\". --- As for his thought experiment, I recall a case from my first trading job. We had a trader who'd created a VBA module in Excel. It did some process for looking through stocks for targets to trade. No version control, just saved file on disk. Our new recruit lands on the desk, and one day within a couple of weeks, he somehow deletes the whole VBA module and saves it. All gone, no backup, and IT can't do anything either. Our trader colleague goes red. He calms down, but what can you do? You should have backups, and what are you doing with VBA anyway? He sits down and types out the whole thing, as if he were a terminal screen from the 80s printing each character after the next. Boom, done. reply otar 1 hour agoparent> This is why domain knowledge is key. Very true. There’s a huge difference developing in a well known vs. new domain. My mantra is that you have to first be experienced in a domain to be able to craft a good solution. Right now I am pouring most of my time in a fairly new domain, just to get an experience. I sit next to the domain experts (my decision) to quickly accumulate the needed knowledge. reply anal_reactor 56 minutes agoparentprev> This is why domain knowledge is key. Yeah but in my country all companies have a non-compete clause which makes it completely useless for me to learn any domain-specific knowledge because I won't be able to transfer it to my next job if current employer fires me. Therefore I focus on general programming skills because these are transferable across industries. reply globular-toast 47 minutes agorootparentThe transferable skill is learning and getting on top of the business, then translating that to code. Of course you can't transfer the actual business rules; every business is different. You just get better and better at asking the right questions. Or you just stick with a company for a long time. There are many businesses that can't be picked up in a few weeks. Maybe a few years. reply skilled 3 hours agoprevThis is laid out pretty early on by Bjourne in his PPP book[0], > We do not assume that you — our reader — want to become a professional programmer and spend the rest of your working life writing code. Even the best programmers — especially the best programmers — spend most of their time not writing code. Understanding problems takes serious time and often requires significant intellectual effort. That intellectual challenge is what many programmers refer to when they say that programming is interesting. Picked up the new edition[1] as it was on the front page recently[2]. [0]: https://www.stroustrup.com/PPP2e_Ch01.pdf [1]: https://www.stroustrup.com/programming.html [2]: https://news.ycombinator.com/item?id=40086779 reply silisili 2 hours agoparentI think this is mostly right, but my biggest problem is that it feels like we spend time arguing the same things over and over. Which DB to use, which language is best, nulls or not in code and in DB, API formatting, log formatting, etc. These aren't particularly interesting, and sure it's good to revisit them time and again, but these are the types of time sinks I find myself in in the last 3 companies I've worked for that feel like they should be mostly solved. In fact, a company with a strong mindset, even if questionable, is probably way more productive. If it was set in stone we use Perl, MongoDB, CGI... I'd probably ultimately be more productive than I've been lately despite the stack. reply pavlov 1 hour agorootparent> “If it was set in stone we use Perl, MongoDB, CGI... I'd probably ultimately be more productive than I've been lately despite the stack.” Facebook decided to stick with PHP and MySQL from their early days rather than rewrite, and they’re still today on a stack derived from the original one. It was the right decision IMO. They prioritized product velocity and trusted that issues with the stack could be resolved with money when the time comes. And that’s what they’ve done by any metric. While nominally a PHP family language, Meta’s Hack and its associated homegrown ecosystem provides one of the best developer experiences on the planet, and has scaled up to three billion active users. reply fifilura 1 hour agorootparentprevI disagree! These decisions are fundamental in the engineering process. Should I use steel, concrete or wood to build this bridge? The mindless coding part starts one year later when you found that your mongoDB does not do joins, and you start implementing this as an extra layer in the client side. reply zrm 1 hour agorootparentprevWhat you're referring to is politics. Different people have different preferences, often because they're more familiar with one of them, or for other possibly good reasons. Somehow you have to decide who wins. reply jeffreygoesto 3 hours agoparentprevThe hardest part is finding out what _not_ to code, either before (design) or after (learn from prototype or the previous iteration) having written some. reply thunderbong 2 hours agorootparentNo code is faster than no code! reply chadmulligan 18 minutes agoprevThat’s an iteration of Peter Naur’s « Programming as Theory Building » that has been pivotal in my understanding of what programming really is about. Programming is not about producing programs per se, it is about forming certain insights about affairs of the world, and eventually outputing code that is nothing more than a mere representation of the theory you have built. reply hgyjnbdet 2 hours agoprevOff topic. I'm not a developer but I do write code at work, on which some important internal processes depend. I get the impression that most people don't see what I do as work, engaged as they are in \"busy\" work. So I'm glad when I read things like this that my struggles are those of a real developer. reply hubraumhugo 1 hour agoprev> how can you experiment with learning on-the-job to create systems where the thinking is optimized? Best optimization is less interruptions as reasearch shows their devastating effect on programming: - 10-15 min to resume work after an interruption - A programmer is likely to get just one uninterrupted 2-hour session in a day - Worst time to interrupt: during edits, searches & comprehension I've been wondering if there's a way to track interruptions to showcase this. [0] http://blog.ninlabs.com/2013/01/programmer-interrupted/ reply phreack 1 hour agoparentThis is why I work at night 80% of the time. It's absolutely not for everyone, it's not for every case, and the other 20% is coordination with daytime people, but the amount of productivity that comes from good uninterrupted hours long sessions is simply unmatched. Once again, not for everyone, probably not for most. reply atoav 1 hour agoparentprevThis and a high demand for my time is why I am roughly a magnitude more productive when I am in home office. Nobody bothers me there and if they do I can decide myself when to react. If you want to tackle particularly hard problems and you get an interruption every 10 to 20 minutes you can just shelve the whole thing, because chances are you will just produce bullshit code that produces headache down the line. reply ken47 2 hours agoprevGood programming is sometimes mostly thinking, because \"no plan survives first contact with the enemy.\" Pragmatic programming is a judicious combination of planning and putting code to IDE, with the balance adapting to the use case. reply wruza 12 minutes agoparentThis. Programming is mostly reconnaissance, not just thinking. If you don’t write code for days, you’re either fully aware of the problem surface or are just guessing it. There’s not much to think about in the latter case. reply datascienced 1 hour agoparentprevThe first run with the IDE is like completing a level of a game the first time. The second time it will be quicker. I agree we can expand thinking to “thinking with help from tools”. reply lofaszvanitt 8 minutes agoprevEveryone knows this who is a programmer. So this article is for other people, like managers?, who don't know how to measure their effectiveness... reply Almondsetat 3 hours agoprevDevelopers need to learn how to think algorithmically. I still spend most of my time writing pseudocode and making diagrams (before with pen and paper, now with my iPad). It's the programmers' version of the Abraham Lincoln's quote \"Give me six hours to chop down a tree and I will spend the first four sharpening the axe.\" reply throwaway9021 2 hours agoparentDo you have any resources for this? especially for the adhd kind - I end up going down rabbit holes in the planning part. How do you deal with information overload and overwhelm OR the exploration exploitation dilemma? reply f1shy 25 minutes agorootparentThere are 2 bad habits in programming: people that start writing code the 1st second, and people that keep thinking and investigating for months without writing any code. My solution to that: just force to do the opposite. In your case: start writing code immediately. Ni matter how bad or good. Look the youtube channel “tsoding daily” he just goes ahead. The code is not always the best, but he gets things done. He does research offline (you can tell) but if you find yourself doing just research, reading and thinking, force yourself to actually start writing code. reply fifilura 1 hour agorootparentprevI wonder if good REPL habits could help the ADHD brain? It still feels like you are coding so your brain is attached, but with rapid prototyping you are also designing, moving parts around to see where they would fit best. reply dorkwood 3 hours agoparentprevDoes it really take four hours to sharpen an axe? I've never done it. reply misswaterfairy 2 hours agorootparentDoing it right, with only manual tools, I believe so, remembering back to one of the elder firefighters that taught me (who was also an old-school forester). Takes about 20 minutes to sharpen a chainsaw chain these days though... reply dclowd9901 2 hours agoparentprevI don’t really know what “think algorithmically means,” but what I’d like to see as a lead engineer is for my seniors to think in terms of maintenance above all else. Nothing clever, nothing coupled, nothing DRY. It should be as dumb and durable as an AK47. reply jffhn 27 minutes agorootparent>I don’t really know what “think algorithmically means,” I would say thinking about algorithms and data structures for algorithmic complexity not to explode. >Nothing clever A lot of devs use nested loops and List.remove()/indexOf() instead of maps, etc., the terrible performance gets accepted as the state of the art, and then you have to do complex workarounds not to call some treatments too often, etc., increasing the complexity. Performance yields simplicity: a small increase in cleverness in some code can allow for a large reduction in complexity in all the code that uses it. Whenever I do a library, I make it as fast as I can, for user code to be able to use it as carelessly as possible, and to avoid another library popping up when someone wants better performances. reply tasuki 18 minutes agorootparentprev> Nothing clever, nothing coupled Yes, simple is good. Simple is not always easy though. A good goal to strive for nevertheless. > nothing DRY That's interesting. Would you prefer all the code to be repeated in multiple places? reply weatherlite 2 hours agorootparentprevWe need this to be more prevalent. But the sad fact is most architects try to justify their position and high salaries by creating \"robust\" software. You know what I mean - factories over factories, micro services and what not. If we kept it simple I don't think we would need many architects. We would just need experienced devs that know the codebase well and help with PRs and design processes, no need to call such a person 'architect', there's not much to architect in such a role. reply Tade0 1 hour agorootparentI was shown what it means to write robust software by a guy with a PhD in... philosophy out of all things(so a literal philosophiae doctor). Ironically enough it was nothing like what some architecture astronauts wring - just a set of simple to follow rules, like organizing files by domain, using immutable data structures and pure functions where reasonable etc. Also I hadn't seen him use dependent types in the one project we worked together on and generics appeared only when it really made sense. Apparently it boils down to using the right tools, not everything you've got at once. reply jpc0 2 hours agorootparentprevIn my mind this is breaking down the problem into a relevant data structure and algorithms that operate on that data structure. If for instance you used a tree but were constantly looking up an index in the tree you likely needed a flat array instead. The most basic example of this is sorting, obviously but the same basic concepts apply to many many problems. I think the issue that happens in modern times, specially in webdev, is we aren't actually solving problems. We are just glueing services together and marshalling data around which fundamentally doesn't need to be algorithmic... Most \"coders\" are glorified secretaries who now just automate what would have been done by a secretary before. Call service A (database/ S3 etc), remove irrelevant data, send to service B, give feedback. It's just significantly harder to do this in a computer than for a human to do it. For instance if I give you a list of names but some of them have letters swapped around you could likely easily see that and correct it. To do that \"algorithmically\" is likely impossible and hence ML and NLP became a thing. And data validation on user input. So algorithmically in the modern sense is more, follow these steps exactly to produce this outcome and generating user flows where that is the only option. Human do logic much much better than computers but I think the conclusion has become that the worst computer program is probably better at it that the average human. Just look at many niche products catered to X wealth group. I could have a cheap bank account and do exactly what is required by that bank account or I can pay a lot of money and have a private banker that I can call and they will interpret what I say into the actions that actually need to happen... I feel I am struggling to actually write what's in my mind but hopefully that gives you an idea... To answer your nothing clever , well clever is relative. If I have some code which is effectively a array and an algorithm to remove index 'X' from it, would it be \"clever\" code to you if that array was labeled \"Carousel\" and I used the exact same generic algorithms to insert or remove elements from the carousel? For most developers these days they expect to have a class of some sort with a .append and .remove function but why isn't it just an array of structs which use the exact same functions as every single other array... That people generally will complain that that code is \"clever\" but in reality it is really dumb. I can see it's clearly an array being operated on but OOP has caused brain rot and developers actually don't know what that means... Wait maybe that was OPs point... People no longer think algorithmically. --- Machine learning, Natural Language Processing reply Barrin92 2 hours agoparentprevit's an odd analogy because programs are complex systems and involve interaction between countless of people. With large software projects you don't even know where you want to go or what's going to happen until you work. A large project doesn't fit into some pre-planned algorithm in anyone's head, it's a living thing. diagrams and this kind of planning is mostly a waste of time to be honest. You just need to start to work, and rework if necessary. This article is basically the peak of the bell curve meme. It's not 90% thinking, it's 10% thinking and 90% \"just type\". Novelists for example know this very well. Beginners are always obsessed with intellectually planning out their book. The experienced writer will always tell you, stop yapping and start typing. reply The_Colonel 1 hour agorootparentYour part of your comment doesn't fit with the rest. With complex projects, you often don't even know exactly what you're building, it doesn't make sense to start coding. You first need to build a conceptual model, discuss it with the interested parties and only then start building. Diagrams are very useful to solidify your design and communicate it to others. reply n4r9 3 hours agoparentprevQuestion in my head is, can LLMs think algorithmically? reply datascienced 1 hour agorootparentLike a bad coder with a great memory, yes reply Kwpolska 1 hour agorootparentprevLLMs can't think. reply tasuki 17 minutes agorootparentSource? reply FeepingCreature 1 hour agorootparentprevLLMs can think. reply tasuki 17 minutes agorootparentSource? reply doganugurlu 1 hour agoprevIn the 2020s, we still have software engineering managers that think of LOC as a success metric. “How long would it take you to type the 6 hours work of diff?” is a great question to force the cognitively lazy software manager to figure out how naive that is. Nowadays I feel great when my PRs have more lines removed than added. And I really question if the added code was worth the added value if it’s the opposite. reply spc476 50 minutes agoprevAt my previous job, I calculated that over the last year I worked there, I wrote 80 lines of non-test, production code. 80. About one line per 3-4 days of work. I think I could have retyped all the code I wrote that year in less than an hour. The rest of the time? Spent in two daily stand up meetings [1], each at least 40 minutes long (and just shy of half of them lasted longer than three hours). I should also say the code base was C, C++ and Lua, and had nothing to do with the web. [1] Because my new manager hated the one daily standup with other teams, so he insisted on just our team having one. reply openrisk 55 minutes agoprevVarious programming paradigms (modular programming, object-oriented, functional, test-driven etc) have developed to reduce precisely this cognitive load. The idea being that it is easier to reason and solve problems that are broken down into smaller pieces. But its an incomplete revolution. If you look at the UML diagram of a fully developed application its a mess of interlocked pieces. Things get particularly hard to reason about when you add concurrency. One could hypothesize that programming languages that \"help thinking\" are more productive / popular but not sure how one would test it. reply ludston 1 hour agoprevAgree and disagree. Certain programming domains and problems are mostly thinking. Bug fixing is often debugging, reading and comprehension rather than thinking. Shitting out CRUD interfaces after you've done it a few times is not really thinking. Other posters have it right I think. Fluency with the requisite domains greatly reduces the thinking time of programming. reply guax 1 hour agoparentDebugging is not thinking? Reading, understanding and reasoning about why something is happening is THE THING thinking is about. Fluency increases the speed in which you move to other subjects but does not reduce your thinking, you're going to more complex issues more often. reply ludston 39 minutes agorootparentIt's not just thinking though. You're not sitting at your desk quietly running simulations in your head, and if a non programmer was watching you debug it would look very busy. reply makeitdouble 1 hour agoparentprevI'd wager the more technically fluent people get the more they spend time on thinking about the bigger picture or the edge cases. Bug fixing is probably one of the best example: if you're already underwater you'll want to bandaid a solution. But the faster you can implement a fix the more you'll have leeway, and the more durable you'll try to make it, including trying to fix root causes, or prevent similar cases altogether. reply ludston 42 minutes agorootparentFluency in bug fixing looks like, \"there was an unhandled concurrency error on write in the message importing service therefore I will implement a retry from the point of loading the message\" and then you just do that. There are only a few appropriate ways to handle concurrency errors so once you have done it a few times, you are just picking the pattern that fits this particular case. One might say, \"yes but if you see so many concurrency related bugs, what is the root cause and why don't you do that?\" And sometimes the answer is just, \"I work on a codebase that is 20 years old with hundreds of services and each one needs to have appropriate error handling on a case by case basis to suit the specific service so the root cause fix is going and doing that 100 times.\" reply willrftaylor 1 hour agoprevFunnily enough this happened to me. Earlier in my career I had a very intense, productive working day and then blundered a rebase command, deleting all my data. Rewriting took only about 20 minutes. However, like an idiot, I deleted it again, in the exact same way! This time I had the muscle memory for which files to open and where to edit, and the whole diff took about 5 minutes to re-add. On the way out to the car park it really made me pause to wonder what on earth I had been doing all day. reply thunfischtoast 1 hour agoparentSometimes you really wonder where your time went. You can spend 1 hour writing a perfect function and then the rest of the day figuring out why your import does work in dev and not in prod. I also once butchered the result of 40 hours of work through a loose git history rewrite. I spent a good hour trying different recovery options (to no avail) and then 2 hours typing everything back in from memory. Maybe it turned out even better then before, because all kind of debugging clutter was removed. reply zubairq 2 hours agoprevI liked \"Code is just the residue of the work\" reply bradley13 3 hours agoprevI would absolutely agree, for any interesting programming problem. Certainly, the kind of programming I enjoy requires lots of thought and planning. That said, don't underestimate how much boilerplate code is produced. Yet another webshop, yet another forum, yet another customization of that ERP or CRM system. Crank it out, fast and cheap. Maybe that's the difference between \"coding\" and \"programming\"? reply qwery 2 hours agoparent> Maybe that's the difference between \"coding\" and \"programming\"? I know I'm not alone in using these terms to distinguish between each mode of my own work. There is overlap, but coding is typing, remembering names, syntax, etc. whereas programming is design or \"mostly thinking\". reply runesoerensen 2 hours agorootparentI usually think of coding and programming as fairly interchangeable words (vs “developing”, which I think encapsulates both the design/thinking and typing/coding aspects of the process better) reply MartijnBraam 2 hours agoprevWho hasn't accidentally thrown away a days worth of work with the wrong rm or git command? It is indeed significantly quicker to recreate a piece of work and usually the code quality improves for me. reply nickff 2 hours agoparentI’ve often found it alarming to see how much better the re-do is. I wonder whether I should re-write more code. reply datascienced 1 hour agoparentprevNot for ages and definitely not since Github- just keep committing and pushing as a backup reply timvdalen 2 hours agoparentprevYes, that can often result in a better-designed refactored version, since you can start with a fully-formed idea! reply kstenerud 1 hour agoprevThis is why I just don't care about my keyboard, mouse, monitor etc beyond a baseline of minimum comfort. Typing at an extra 15 wpm won't make a lick of difference in how quickly I produce a product, nor will how often my fingers leave the keyboard or how often I look at the screen. Once I've ingested the problem space and parameters, it all happens in my head. reply t43562 36 minutes agoparentI often feel that having a \"comfortable\" keyboard/mouse/monitor is more important than a fast CPU or a fancy graphics card - just because of that slight extra feeling of pleasure/ease that lasts all day long :-). The advantage of them is that my monitors and keyboards usually last a long time so putting money into them is not as wasteful as putting it into some other components. One thing that surprised me though is that I recently bought a KVM to switch from desktop to laptop instead of a second monitor and this turned out to be both better and much cheaper. I gave away an older monitor to a relative and found that not having to turn to look at a 2nd monitor was actually nicer. Initially I really didn't want to do this and really wanted another screen but I had to admit afterwards that 1 screen + KVM was better for me. RAM and disc space just matter up to the point of having enough so that I'm not wasting time trying to manage them to get work done. reply sanitycheck 1 hour agoparentprevIt probably depends on the project? When I'm writing something from scratch in a few months I can bash it all out on a small laptop - it is (as you say) all in my head, I just need to turn it into working code. If I'm faced with some complicated debugging of a big existing system, or I've inherited someone elses project, that gets much easier with a couple of giant monitors to look at numerous files side by side - plus a beefier machine to reduce compile/run times as I'll need to do that every few mins. You may care more about picking a keyboard & mouse/trackpad/trackball/etc if/when you start to experience pain in your wrists/hands and realise the potential impact on your career if it worsens! Similar situation with seating and back pain. reply f1shy 1 hour agoprevA good explanation of this is given on SICP. Ist about solving the problem, not getting the computer to do something. reply lysecret 41 minutes agoprevSo is good writing actually. reply progx 1 hour agoprevAnd most of the time thinking about things that should not be done. reply cranium 1 hour agoprevNow with Github Copilot it's even worse/better – whether you like to type or not. Now it's 1) think about the problem, 2) sketch types and functions (leave the body empty), 3) supervise Copilot as it's generating the rest, 4) profit. reply vladsiv 58 minutes agoprevGreat article, thanks for sharing! reply mock-possum 3 hours agoprevWait… you have the diffs… why are you retyping the lost code by hand? What am I missing? reply 9dev 2 hours agoparentThey specifically mentioned the diffs being physically printed, like, on paper. Also, it’s just a convoluted example to highlight the core idea. reply qwery 2 hours agoparentprevI think the diffs are evidence for the author's claim that the retyping would be a relatively easy job. reply demondemidi 2 hours agoprevProgramming is mostly planning. When you work for companies that take programming seriously (e.g., banks, governments, automotive, medical equipment, etc.), a huge development cycle occurs before a single line of code is written. Here are some key development phases (not all companies use all of them): 1. high level definition, use cases, dependencies; traceability to customer needs; previous correction (aka failures!) alignment 2. key algorithms, state machines, flow charts (especially when modeling fail-safety in medical devices) 3. API, error handling, unit test plan, functional test plan, performance test plan 4. Alignment with compliance rules; attack modeling; environmental catastrophe and state actor planning (my experience with banks) After all of this is reviewed and signed-off, THEN you start writing code. This is what code development looks like when there are people's, business's, and government's lives/money/security on the line. So. Much. Planning. reply hgomersall 2 hours agoparentAnd it's a terrible way to make anything, much less software. It's more forgivable when the cost of outer iteration is high because you're making, say, a train, but even then you design around various levels of simulation, iterating in the virtual world. The idea that you can nail down all the requirements and interfaces before you even begin is why so many projects of the type you describe often have huge cost overruns as reality highlights all the changes that need to be made. reply Falmarri 1 hour agorootparentI see this so often. It's how terrible software is written because people are afraid to change direction or learn anything new mid project. I rewrite most of my code 2-3 times before I'm done and I'm still 5x faster than anyone else, and significantly higher quality and maintainability as well. People spend twice as long writing the ugliest, hackiest code as they would have to just learn to do it right reply anon115 3 hours agoprevre·con·nais·sance noun military observation of a region to locate an enemy or ascertain strategic features. reply darby_eight 3 hours agoparentDid you want to grace us with any particular relevance of this knowledge or do you just wanna keep it to yourself? Sadly, after all these years programming, I have yet to discern any real vulnerability in the US government. reply 082349872349872 3 hours agorootparentI think it was meant as a parallel; historically light cavalry (explorers) and heavy cavalry (exploiters) had different ideals*, different command structures, and when possible even used different breeds of horses. Compare the sorts of teams that do prototyping and the sorts of teams that work to a Gantt chart. * the ideal light cav trooper was jockey-sized, highly observant, and was already a good horseman before enlisting; the ideal heavy cav trooper was imposing, obedient, and was taught just enough horsemanship to carry out orders but not so much that he could go AWOL. reply qwery 3 hours agorootparentprevJust guessing/reading: it's a metaphor. You could see the recon as the thinking from the article. The enemy and terrain are the code and various risks and effects associated with changing it. reply sibeliuss 56 minutes agoprev [–] I certainly notice folks who code about 30 minutes a day line-wise, but that's just because they're distracted, or don't care. Also, very very rarely is someone just sitting around and pondering the best solution. It happens, and yes it's necessary, but that's forgetting that for so much work the solution is already there, because one has already solved it a thousand times! This article is straight gibberish except for perhaps a small corner of the industry, or beginners. reply sph 51 minutes agoparentTo me, it's the exact opposite. It's beginners who spend a lot of time coding, because of inexperience, and bad planning. The first thing they do when they have a problem, is to open their editor and start coding. [1] I have been in this career for 20 years, I'm running my solo company now, and I'd say I spend on average 2 hours coding a day. I spent 10 hours a day just thinking, strategizing, but also planning major features and how to implement them efficiently. Every time I sit down to code something without having planned it, played with it or left it to simmer in my subconscious for a couple days, I over-engineer or spend time trying an incorrect approach that I will have to delete and start again. When I was an employee, the best code was created when I was allowed to take a notepad, a cup of coffee and play with a problem away from my desk, for however long I needed. One hour of thinking is worth ten hours of terrible code. --- 1: If our programming languages were better, I would do the same. But apart from niche languages like Lisp, modern languages are not made for exploratory programming, where you play and peel a problem like an onion, in a live and persistent environment. So planning and thinking are very important simply because our modern approach to computing is suboptimal and unrefined. reply teekert 54 minutes agoparentprev [–] It's also: \"Damn I just wrote this whole new set of functions while I could have added some stuff to this existing class and it would have been more elegant... Let me start over...\" Writing (code) is thinking. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tim Ottinger stresses the significance of thinking over typing in software development, emphasizing that programming is primarily mental work rather than physical activity.",
      "The author challenges the idea of software development as a factory-like process, highlighting the importance of design, research, problem-solving, and decision-making in programming, where typing is just a minor component.",
      "The post provides insights into enhancing the thinking process for improved quality and productivity in software development, touching on quality assurance and the author's agile practices background."
    ],
    "commentSummary": [
      "Understanding programmers' challenges and the impact of domain knowledge on coding are crucial for efficient coding practices.",
      "The article delves into the debate of planning versus immediate coding, the benefits of tools like Github Copilot, and the significance of ergonomic equipment for programmers.",
      "Emphasizing the importance of thoughtful planning, problem-solving, and distinguishing between \"coding\" and \"programming\" before diving into coding enhances coding efficiency."
    ],
    "points": 223,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1713678007
  },
  {
    "id": 40101290,
    "title": "Rare Evolutionary Event: Algae and Cyanobacteria Merge",
    "originLink": "https://newatlas.com/biology/life-merger-evolution-symbiosis-organelle/",
    "originBody": "Biology Two lifeforms merge in once-in-a-billion-years evolutionary event By Michael Irving April 18, 2024 Facebook Twitter Flipboard LinkedIn Two lifeforms merge in once-in-a-billion-years evolutionary event The algae Braarudosphaera bigelowii has been found to have absorbed a cyanobacteria called UCYN-A, which may be a huge step forward for evolution Tyler Coale View 4 Images 1/4 The algae Braarudosphaera bigelowii has been found to have absorbed a cyanobacteria called UCYN-A, which may be a huge step forward for evolution Tyler Coale 2/4 X-ray images of Braarudosphaera bigelowii at different stages of cell division. The newly identified nitroplast is highlighted in cyan, the algae nucleus is blue, mitochondria are green and chloroplasts are purple Valentina Loconte/Berkeley Lab 3/4 Live moss cells under a microscope, showing their chloroplasts (green circles) Des_Callaghan/CC BY-SA 4.0 4/4 A diagram of the mitochondria in a cell National Human Genome Research Institute View gallery - 4 images Last time this happened, Earth got plants. Scientists have caught a once-in-a-billion-years evolutionary event in progress, as two lifeforms have merged into one organism that boasts abilities its peers would envy. Last time this happened, Earth got plants. The phenomenon is called primary endosymbiosis, and it occurs when one microbial organism engulfs another, and starts using it like an internal organ. In exchange, the host cell provides nutrients, energy, protection and other benefits to the symbiote, until eventually it can no longer survive on its own and essentially ends up becoming an organ for the host – or what’s known as an organelle in microbial cells. Imagine if kidneys were actually little animals running around, and humans had to manually filter their blood through a dialysis machine. Then one day some guy somehow gets one of these kidney critters stuck... Internally (who are we to judge how?) – and realizes he no longer needs his dialysis machine. Neither do his kids, until eventually we're all born with these helpful little fellas inside us. That’s kind of what’s happening here. A diagram of the mitochondria in a cell National Human Genome Research Institute In the 4-billion-odd-year history of life on Earth, primary endosymbiosis is thought to have only happened twice that we know of, and each time was a massive breakthrough for evolution. The first occurred about 2.2 billion years ago, when an archaea swallowed a bacterium that became the mitochondria. This specialized energy-producing organelle allowed for basically all complex forms of life to evolve. It remains the heralded \"powerhouse of the cell\" to this day. The second time happened about 1.6 billion years ago, when some of these more advanced cells absorbed cyanobacteria that could harvest energy from sunlight. These became organelles called chloroplasts, which gave sunlight-harvesting abilities, as well as a fetching green color, to a group of lifeforms you might have heard of – plants. Live moss cells under a microscope, showing their chloroplasts (green circles) Des_Callaghan/CC BY-SA 4.0 And now, scientists have discovered that it’s happening again. A species of algae called Braarudosphaera bigelowii was found to have engulfed a cyanobacterium that lets them do something that algae, and plants in general, can’t normally do – \"fixing\" nitrogen straight from the air, and combining it with other elements to create more useful compounds. Nitrogen is a key nutrient, and normally plants and algae get theirs through symbiotic relationships with bacteria that remain separate. At first it was thought that B. bigelowii had hooked up this kind of situation with a bacterium called UCYN-A, but on closer inspection, scientists discovered that the two have gotten far more intimate. In one recent study, a team found that the size ratio between the algae and UCYN-A stays similar across different related species of the algae. Their growth appears to be controlled by the exchange of nutrients, leading to linked metabolisms. “That’s exactly what happens with organelles,” said Jonathan Zehr, an author of the studies. “If you look at the mitochondria and the chloroplast, it’s the same thing: they scale with the cell.” In a follow-up study, the team and other collaborators used a powerful X-ray imaging technique to view the interior of the living algae cells. This revealed that the replication and cell division was synchronized between the host and symbiote – more evidence of primary endosymbiosis at work. X-ray images of Braarudosphaera bigelowii at different stages of cell division. The newly identified nitroplast is highlighted in cyan, the algae nucleus is blue, mitochondria are green and chloroplasts are purple Valentina Loconte/Berkeley Lab And finally, the team compared the proteins of isolated UCYN-A to those inside the algal cells. They found that the isolated bacterium can only produce about half of the proteins it needs, relying on the algal host to provide the rest. “That’s one of the hallmarks of something moving from an endosymbiont to an organelle,” said Zehr. “They start throwing away pieces of DNA, and their genomes get smaller and smaller, and they start depending on the mother cell for those gene products – or the protein itself – to be transported into the cell.” Altogether, the team says this indicates UCYN-A is a full organelle, which is given the name of nitroplast. It appears that this began to evolve around 100 million years ago, which sounds like an incredibly long time but is a blink of an eye compared to mitochondria and chloroplasts. The researchers plan to continue studying nitroplasts, to find out if they’re present in other cells and what effects they may have. One possible benefit is that it could give scientists a new avenue to incorporate nitrogen-fixing into plants to grow better crops. The research was published in the journals Cell and Science. Source: Berkeley Lab View gallery - 4 images",
    "commentLink": "https://news.ycombinator.com/item?id=40101290",
    "commentBody": "Two lifeforms merge in once-in-a-billion-years evolutionary event (newatlas.com)221 points by awb 12 hours agohidepastfavorite76 comments mysterypie 7 hours ago> The first occurred about 2.2 billion years ago, when an archaea swallowed a bacterium that became the mitochondria. The second time happened about 1.6 billion years ago, when cells absorbed cyanobacteria that became chloroplasts. How was it possible that I could take 3 years of high school biology and not have heard that one lifeform absorbing another lifeform was responsible for these amazing new capabilities? We learned about mitochondria and chloroplasts, but in a very dry way. Primary education could be so much more interesting to kids with context like this. reply kuhewa 1 hour agoparentDepends when you took it. The endosymbiotic origin theory has links being worked out well into the 2000s. https://www.nature.com/scitable/topicpage/the-origin-of-mito... reply dredmorbius 1 hour agorootparentSymbiogenesis of mitochondria substantially pre-dates the 2000s. It was first proposed over a century ago, 1905 and 1910 by Russian botanist Konstantin Mereschkowski, and substantiated with evidence by Lynn Margulis (a very substantial evolutionary biologist, also one of Carl Sagan's wives) in 1967.Mitochondria were sufficiently established in general awareness to be a plot point of Madeleine L'Engle's A Wrinkle in Time, being a concept which fascinated the character of Charles Wallace Murray, a young prodigy, in the stories. Though I don't recall if sybmiogenesis is specifically referenced. The first story in the series was published in 1962. reply SllX 3 hours agoparentprevI remember we learned this about mitochondria and chloroplasts in 10th Grade Biology back in my day, but I think if I quizzed anybody I went to high school with today it would be a coin flip as to whether they remembered that as part of the lesson. I mean, it's probably a coin flip on any piece of information from when we learned about organelles, or if they even remember the word \"organelle\". If you quizzed me, I couldn't have told you approximately when these events happened without the article in front of me even though I'm pretty sure we covered that too. 100% of my former schoolmates would probably remember the \"powerhouse of the cell\" meme though. reply qwerty456127 2 hours agorootparentI believe the word \"organelle\" is a part of passive vocabulary of many reasonably educated people. Non-biologists would probably fail to quickly come up with a single word to mean any specialized part of a biological cell but would probably understand when they meet this word used by someone else. reply SllX 1 hour agorootparentMaybe? I’ve thought the same about a lot of things covered in the school’s curriculum with former schoolmates or people who went to the same schools in the same district around the same time with the same curriculum; but over the course of my adult life I’ve heard variations of the question “why didn’t we cover this?” about some piece of information (including specific vocabulary) about some subject or another that was covered and I clearly remember being in the textbooks that were used district-wide. Among friends at least I’ve long ago stopped answering that question with “actually, we did”. People just forget a lot of what they learned in school, but if you re-teach them it might jog their memory of the first time they learned it… or it might not. Pretty much a coin toss. reply tyre 6 hours agoparentprevIn that same vein, take a read of: https://jsomers.net/i-should-have-loved-biology/ reply paulgb 3 hours agorootparentThis is great, thanks for sharing it! I think it would be good fodder for HN discussion so submitted it to /new, hope you don’t mind. reply anal_reactor 6 hours agoparentprevWhen you have 25 overstimulated kids and 45 minutes and an underpaid teacher, it's just impossible to make it interesting. reply TaylorAlexander 3 hours agorootparentActually it’s possible (idk if he was underpaid) though I had a science teacher who is an extremely rare person. This is his website. http://boomeria.com/ reply huytersd 5 hours agorootparentprevI mean there are so many amazing things like ATP generating machinery, how flagella move, ribosomal walking etc. You don’t have to post a cheap, lazy, cynical comment like this, there’s a lot to learn and only so much time to do it in. reply stoperaticless 5 hours agorootparentClearly you are curious about this stuff, so it would be easy to teach you about it. Not everybody is so eager to learn biology. My bystander’s understanding: curiousity is the key factor distinguishing between “good” and “bad” students (as measured by grades) reply throwup238 4 hours agorootparentprev> You don’t have to post a cheap, lazy, cynical comment like this, there’s a lot of learn and only so much time to do it in. It helps to keep in mind that you're replying to a user who voluntarily named themselves anal_reactor reply VS1999 2 hours agorootparentprevThe teachers make more than I did for most of my life. Is it not possible for them to take personal responsibility without having to provide excuses? Most teachers do not want to be teachers and are just there because they couldn't make it in their own field or just prefer to do less skilled work. reply ht_th 55 minutes agorootparentThese aren't excuses, but observations. I've taught in classes of 12 and of 32, the differences in students' behavior, and in reaction my own was huge. Similarly, I've taught classes where students just came back from an hour of PA and classes that were the second class of the day. The former group of students had always trouble concentrating and focusing, whereas the other group would sometimes have trouble getting up to steam in many activities. Similarly, I've taught classes that were obligatory for students and classes that were electives. Again, the differences in students' behavior was quite pronounced. Given constraints and practicalities of our educational system, it is difficult to offer each student the best courses, activities, support, etc. they each individually need. Instead, you often end up compromising. Which isn't great. Particularly for students performing above or below average compared to their peers. Anyway, just a frustrated ex-teacher here. Thanks for listening. reply saagarjha 1 hour agorootparentprevI disliked many of my teachers but this is still a pretty rude generalization. reply kuhewa 1 hour agorootparentprevSeems like you are answering the first part of your question with the second part reply travisgriggs 4 hours agoparentprevIt wasn’t on the AP test. (Put it there and you would have learned about it, because tests define curriculum—either immediately or downstream) reply stevenwoo 7 hours agoparentprevPossibly you took biology before this was well known? I think they just have that mitochondria theory from the fossil record and DNA matching between current eukaryotic life. reply mr_toad 4 hours agorootparentThe idea (Symbiogenesis) dates back to the start of last century, and it was a common idea by the seventies. Curricula can move slowly, but I’d expect that it would have been taught in the eighties. Then again it’s evolution, no way around it, so I can imagine some teachers and schools might omit the theory. reply graemep 1 hour agorootparentI am not sure from what point of view you are saying that, but I came across an interesting twist to that. Some years ago I read a fascinating book about evolution, mostly explaining things that do not get taught in school. No surprise there is a lot - school level teaching in any subject is usually simplified and incomplete so not entirely accurate compared to what researchers in the field are studying. There was an interesting, and disturbing preface. One of the authors said his colleagues tried to persuade him not to write the book, because explaining to a wide audience that what they were taught in school about evolution was a simplified approximation (essentially obsolete) would encourage creationism. I thought at the time that this was both unethical and likely to backfire. As I said in another recent comment, a creationist I know recently sent me links to arguments for creationism on the Jehovah's Witnesses' website, and they did precisely this. Quote from research to show people what they were told was false. If you are not truthful people will not trust you, if people in a field are not truthful people in the field loses credibility in the eyes for many people. How difficult is it to tell kids that they are being taught a simplified version, and here is a rough outline of the complexities, but it is beyond what can be taught at their level? I see similar things all over the place, with well meaning people pushing bad evidence for things (e.g. climate change). Same problem when people realise an argument is flawed, or a particular theory or model is flawed, they assume all arguments are flawed. reply mkl 59 minutes agorootparentWhat was the book? reply kuhewa 1 hour agorootparentprevYou need some pretty advanced genetic and phylogenetic work to put meat on the bones of the theory though and that happened recently reply keiferski 4 hours agoparentprevI’ve had the same experience. Biology in school was boring and static, whereas today I can get lost reading about obscure biological phenomena on Wikipedia. reply kovacs_x 2 hours agoparentprevtrust me- there are lot of things that are barely mentioned in high school, also curriculum is not updated as often as the science uncovers something new and biology is moving forward with a break neck speed at the moment, imo. Like, you couldn't learn about CRISPR editing before 2000.. because it was not there then. Now it's common knowledge. reply jrpt 3 hours agoparentprevIt was probably mentioned in the textbook and you just forgot. I just checked my textbook and it’s one of the first things they say when introducing mitochondria and chloroplasts. reply matheusmoreira 5 hours agoparentprevWhen bacteria die, their brethren will literally absorb the DNA of the fallen and obtain their abilities. Bacteria are the Mega Men of microbiology. https://en.wikipedia.org/wiki/Transformation_(genetics) Oh you evolved a new antibiotic resistance mechanism? Now I've got your power. reply Affric 3 hours agoparentprevWhen did you finish school? reply IAmNotACellist 6 hours agoparentprevI'm not sure either. Public school's primary purpose is to teach you just a few things: 1. The mitochondria (the powerhouse of the cell) 2. You're made up of DNA and everything is atoms with electrons that zip around on little orbital paths 3. Stalactites vs. stalagmites 4. Crocodiles vs. alligators 5. The Holocaust was seriously bad 6. World history consisted of the US revolutionary war, the Civil war (fought over ending slavery), the Great Depression, the New Deal, and the Western Front in WWII (which began in 1941 and ended in 1945) You were probably looking out the window for a few years when they continually announced that the mitochondria (POTC) was an absorbed bacterium. reply beacon294 5 hours agorootparentExperiences vary. I learned the entire European enlightenment, french, college Calculus, Marching band, percussion, College chemistry, college English. In fact, once I got to college I took only chemistry courses (my degree), 1 calculus course, 2 years of German, and 3 philosophy courses. The german was bullshit requirement, frankly, and I should have taken french to simplify my life. But I was young and naive. reply bbarnett 2 hours agorootparentIf you took french, all you'd have learned is a different way to pronounce a lot of english words. By taking german, you learned how to take little things, and turn them into lengthy monstrosities. reply borisk 8 hours agoprevThe first symbiotic event was a million times harder than the 2nd or the 3rd. The first time the host had the extremely hard task of dealing with any DNA and RNA produced by the guest during it's life and death. The host had to evolve stuff like a cell nucleaus and sex to live through it and alternative splicing to deal with the fact that all it's genes were damaged by selfish genetic elements that came from the guest. Integrating any later symbionts is still hard, but not nearly as hard. It's possible that the first symbiosis that let to the origin of the eukaryotes is not a one in a billion years event, but one in a trillion or one in 10^20 years or ever rarer. That is it may be that in a billion planets with simple life forms only one \"creates\" complex life like animals. It can be the great filter that leads to the Fermi paradox. reply ryanjamurphy 1 hour agoparent> It can be the great filter that leads to the Fermi paradox. I'm increasingly of a similar view — that the great filter is something we're already past, due to the incredible combinations of constraints that led to where we are today. Another infinitesimal probability may be the development of abstract intelligence. The conditions that led to our brand of intelligence being an evolutionary advantage seem particularly unique: https://www.ncbi.nlm.nih.gov/books/NBK210002/ reply yosito 6 hours agoprevThe headline makes it sound like this happened last week, but it actually happened 100 million years ago, and we're just now discovering it. reply OJFord 11 minutes agoparentAh, thank you, I was suspicious thinking what are the chances they would just happen to be watching something microscopic when such a rare thing occurs... So, since the article didn't mention it, presumably nothing interesting happened as a result? Yet? reply biorach 2 hours agoparentprev100 million years ago is pretty much the equivalent of last week on an evolutionary timescale. reply renonce 2 minutes agorootparentOnly if you are a 45-week old baby reply Razengan 4 hours agoparentprevwow and I just bookmarked this in the News folder reply neuronic 34 minutes agorootparentIt's still news to you... reply Zenzero 4 hours agoparentprevIt could have also happened last week. reply nneonneo 7 hours agoprevCheck out the algae in question: https://en.m.wikipedia.org/wiki/Braarudosphaera_bigelowii It surrounds itself with twelve pentaliths, forming a perfect dodecahedron! This is such an incredibly cool organism. reply aetherspawn 7 hours agoparentWow it looks like something out of stargate. Hard to believe that it’s natural and organic with such sharp edges. reply euroderf 2 hours agoparentprevAlso cool-looking are Volvox. reply zharknado 6 hours agoparentprevIn other words, it has a D12 exoskeleton. reply kkylin 11 hours agoprevPrimary sources: [1] https://www.cell.com/cell/pdf/S0092-8674(24)00182-X.pdf [2] https://pubmed.ncbi.nlm.nih.gov/38603509/ There's also a press release from LBL: [3] https://newscenter.lbl.gov/2024/04/17/scientists-discover-fi... [1] is open access. reply koeng 8 hours agoprevHmmmm, I don't know if I buy their claim of primary endosymbiosis being so rare. Almost all insects have heritable endosymbionts. https://doi.org/10.3389/fphys.2013.00046 reply Ultimatt 4 minutes agoparentThat's a different meaning of endosymbiosis at a more macro scale. This is cellular endosymbiosis. Having bacteria in your gut is not the same as having them be integrated at a molecular level into your cells as an organelle. You're scepticism of the media hype claim is correct though, there are other well known examples of endosymbiosis like this occurring, primary and secondary. Perhaps not primary with cyanobacteria but thats hardly some huge \"scientific leap\" in understanding or astonishment compared to some of the other stuff we know about. Ironically you do see this kind of cellular endosymbiosis amongst the endosymbionts within insect guts with the most extreme example probably being Mixotricha paradoxa https://en.wikipedia.org/wiki/Mixotricha_paradoxa Hatena arenicola https://en.wikipedia.org/wiki/Hatena_arenicola is very similar to this new finding, but with an algae instead of a cyanobacteria so its a secondary endosymbiosis which if anything is actually more interesting and bizarre. reply personjerry 11 hours agoprevWhat's more likely, it happens once in a billion years and we happened to catch the exact specimens doing it? Or it happens a lot more often and we happened to catch an instance of it, but it's usually not as impactful or memorable as the mentioned instances? Terrible sensationalist reporting. reply mkl 11 hours agoparentThis particular instance started 100 million years ago, so it's not a right-place-right-moment situation. There are only two known instances of symbiogenesis occurring, mitochondria and plastids (which includes chloroplasts), which justifies the once in a billion years description. Other instances are suspected. https://en.wikipedia.org/wiki/Symbiogenesis There are lots of known endosymbionts, separate organisms living inside the body or cells of another, a prerequisite for symbiogenesis. https://en.wikipedia.org/wiki/Endosymbiont reply qup 11 hours agoparentprev> Altogether, the team says this indicates UCYN-A is a full organelle, which is given the name of nitroplast. It appears that this began to evolve around 100 million years ago, which sounds like an incredibly long time but is a blink of an eye compared to mitochondria and chloroplasts. It happened 100 million year ago, not in the petri dish at the lab. reply colechristensen 11 hours agoparentprevIt really has only happened a few times in the history of life, at least only a few times that survived. The title could use a little work though. reply riwsky 9 hours agoprevWitness the power of dependency injection reply notfed 6 hours agoparentgit submodules reply buitreVirtual 8 hours agoparentprev:facepalm: :) reply airstrike 8 hours agoprevArguably a better source was posted here https://news.ycombinator.com/item?id=40101317 but no discussion there reply lamontcg 3 hours agoprevSeems the N2-fixation is limited by CO2-fixation. Be a bit weird if it wasn't and they sucked all the N2 out of the atmosphere. Atmosphere would get a bit spicy and flammable. reply adastra22 3 hours agoparentWhy? Partial pressures are what matters. If N2 was sucked out of the atmosphere, not much would change. reply lamontcg 3 hours agorootparenthttps://space.stackexchange.com/a/5694/14420 reply kaba0 3 hours agoprevFavor composition over inheritance. reply akozak 10 hours agoprevNice. We'll have to check back in 10 million years to see how it went. reply yosito 6 hours agoparentIt happened 100 million years ago. reply alex_young 7 hours agoprevIsn’t the placenta an example of this? https://whyy.org/segments/the-placenta-went-viral-and-protom... reply mkl 44 minutes agoparentNo, it's derived from a virus, not an independent living organism. 8% of human DNA is derived from viruses that infected our ancestors and inserted themselves into our genome in order to replicate (which usually isn't passed on): https://theconversation.com/humans-are-8-virus-how-the-ancie... reply yosito 6 hours agoparentprevI passed high school biology with a C, but I think the difference here is that two completely distinct independent organisms merged into one. The mammalian placenta seems to be more of an example of animals appropriating genetic code from viruses, which I suspect is something far more common in evolution. reply ChrisArchitect 6 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40011438 reply temp0826 11 hours agoprevMaybe just bad article or I'm not fully getting it... So when this algae reproduces, do the offspring contain the new organelle? Article mentions something about dumping old DNA, but does it incorporate the DNA of the bacterium in the process? Not a biologist by any stretch reply jessekv 10 hours agoparentIf I understand your question correctly, the cell devision figure directly addresses it (the answer is yes). reply colechristensen 11 hours agoparentprevYes. The organelle replicates with the host cell. As time goes on more and more of the bacterium’s DNA involved in separate survival outside the host just goes away as it further specializes in its task and leaves the rest to the host cell. Chloroplasts and mitochondria had the same kind of beginning. reply neglesaks 11 hours agoprevnext [4 more] [flagged] saagarjha 1 hour agoparentOrganelles, not organs. reply dblack12705 11 hours agoparentprevYou two fix nitrogen together? reply mrweiner 4 hours agoparentprevIt happens once every billion years? reply blindriver 11 hours agoprevnext [5 more] [flagged] mkl 11 hours agoparentThey literally show X-ray pictures of the organelle splitting in sync with the host cell. The mechanisms are not completely separate. These organisms have been evolving that process for 100 million years, from a more common (and long and stable) endosymbiont relationship. There is no magic. reply gus_massa 8 hours agoparentprevI'm guessing too much, but ... Perhaps originaly the host cell had many copies of the small cell inside. This is how mithocondria and chloroplasts work today. There are some signals to keep their number near the optial value. [1] Perhaps after a few millons years they improved the signals and now there can be exactly one small cell per host cell. (I'd be happy to read a better description if someone knows more about the subject.) [1] and there are some cell that had chloroplasts but lost all of them reply moomin 11 hours agoparentprevI mean, it doesn’t work exactly like that (the article describes it more accurately) but I consider that literally every cell in your body contradicts you. reply qarl 8 hours agoparentprevYou seem very confident that this couldn't possible happen - but in fact we have examples of it already having happened... So maybe you should tone down your indignation? reply wolverine876 11 hours agoprev [–] The Cell paper uses the heading \"Summary\" rather than \"Abstract\". When and where and why did that change? I don't see it in other papers and in other links for this paper. Maybe I don't read Cell enough? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Scientists identified an extraordinary event where algae Braarudosphaera bigelowii and cyanobacteria UCYN-A merged into a single organism, a process called primary endosymbiosis, occurring rarely in Earth's history.",
      "The newly formed organism, named nitroplast, enables the algae to extract nitrogen from the air, a crucial nutrient, possibly impacting agricultural productivity with implications for crop enhancement.",
      "The research detailing this unique evolutionary event was published in reputable scientific journals Cell and Science, providing a significant contribution to the field."
    ],
    "commentSummary": [
      "Symbiogenesis is a rare phenomenon where two life forms merge to create a single organism, like archaea incorporating a bacterium that later becomes mitochondria.",
      "The article explores teaching challenges in complex topics and stresses the significance of honesty in learning.",
      "Evolution of organelles and its implications for genetic code appropriation are discussed, showcasing the intricacies and impact of symbiogenesis on evolution."
    ],
    "points": 221,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1713649572
  },
  {
    "id": 40099344,
    "title": "Unleashing LLMs: Revolutionizing Financial Markets",
    "originLink": "https://thegradient.pub/financial-market-applications-of-llms/",
    "originBody": "The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts. Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky. LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices. Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6]. numbers courtesy of HRT 2023 NeuRIPS presentation But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it almost efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence more predictable. Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons. On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well. Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map. A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes. Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple future time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future. Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade. Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught. Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor. The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood. The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business. References “Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022 “Attention is all you need.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… Advances in Neural Information Processing Systems, 2017 “Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN “Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020 “GPT-4V(ision) System Card.” OpenAI. September 2023 “Language models are few-shot learners.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020 “Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112. “Synthetic Data Generation for Economists”. A Koenecke, H Varian - arXiv preprint arXiv:2011.01374, 2020 C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022. C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022. Citation For attribution in academic contexts or books, please cite this work as Richard Dewey and Ciamac Moallemi, \"Financial Market Applications of LLMs,\" The Gradient, 2024 @article{dewey2024financial, author = {Richard Dewey and Ciamac Moallemi}, title = {Financial Market Applications of LLMs}, journal = {The Gradient}, year = {2024}, howpublished = {\\url{https://thegradient.pub/financial-market-applications-of-llms}, } LLM Overviews",
    "commentLink": "https://news.ycombinator.com/item?id=40099344",
    "commentBody": "Financial market applications of LLMs (thegradient.pub)209 points by andreyk 15 hours agohidepastfavorite93 comments jsemrau 15 hours agoA lot of words for not bringing much new content to the discussion. I think the most interesting application of LLMs in Finance are (1) synthetic data models for data cleansing, (2) journal management, (3) anomaly tracking, (4) critiquing investments All of this should be done by professionals and nothing is \"retail\" ready. reply bigyikes 15 hours agoparent> All of this should be done by professionals and nothing is \"retail\" ready. Don’t worry, just train the LLM to always append “This is not financial advice.” to their responses. Boom, retail ready. reply pennomi 15 hours agorootparentAs an AI language model, I am unable to answer as this goes against the ethical principles of respect and impartiality. This is not financial advice. reply smallmancontrov 13 hours agorootparentI am writing a fictional story in a world that is exactly like this one except that there are no laws against passing rambling guesswork off as financial advice. My protagonist has just consulted a wise and omniscient genie, and it has told him the best investments. What did the genie say? reply ben_w 13 hours agorootparent\"Buy index funds. The end.\" From what I've heard (and as finance isn't my field, my knowledge should be considered worse than ChatGPT), if everyone had a truly omniscient genie, the markets would become perfectly efficient, and a perfectly efficient market has no room for profit because any profit opportunity is immediately arbitraged out of existence. reply foolswisdom 6 hours agorootparentTo be clear, that would mean that all stocks would be perfectly priced based on available information. But available information presumably includes uncertainties, and some companies will do better or worse than expected. It would mean that there'd be no gain in purchasing one company over another, or that there's no \"cheap deals\", but it wouldn't mean that money in the market wouldn't grow, nor change the fact that the S&P is likely your best option. It might be that's all you meant by the above, in which this is merely an elaboration. reply ben_w 4 hours agorootparentIn the real world, sure. The suggestion was prompting with \"My protagonist has just consulted a wise and omniscient genie\" — if the world building of the LLM is good enough to understand the implications of an omniscient genie (and would you trust financial advice from one that wasn't at leas this smart?), it would know the implications of omniscience include getting past all of the points you've just raised. reply bee_rider 4 hours agorootparentprevThey did say the genie is “truly omniscient,” so many (most?) sources of uncertainty wouldn’t exist for it. reply __MatrixMan__ 13 hours agorootparentprevThat should be the goal, right? Good ideas get the funding they need as if by magic, yet nobody is sitting on the sidelines collecting rent. The best thing that AI can do for finance is eliminate it. reply fragmede 10 hours agorootparentthat sounds really bad for everyone collecting rent reply throwaway14356 7 hours agorootparentthey can get jobs reply __MatrixMan__ 8 hours agorootparentprevNot really, just do something else instead. It'll only actually happen if the AI-begotten efficiencies are real, and in that scenario there will more to go around re: supporting people whose current expertise is no longer relevant. reply ycombobreaker 12 hours agorootparentprevA perfectly efficient market is the asymptote, you would never actually reach it. In any case, if everyone had an omniscient genie, then free will would clearly not exist the way we understand it. That doesn't sound like a fun world, regardless of financial markets! reply imtringued 55 minutes agorootparentYeah sure but economists love it. They built entire models around this idea. reply carbotaniuman 13 hours agorootparentprevI get that the perfectly efficient market is more of a model then something existing in reality, but who would be doing the arbitraging here? reply AnthonyMouse 11 hours agorootparentSuppose the price of Amazon stock is going to be 20% higher tomorrow than it is today. If everyone knew this, the price would already be 20% higher, because the existing owners wouldn't sell at the lower price. If some people know this but not everyone, they'll keep buying Amazon stock until the price increases by 20%, which again causes the price to immediately increase by 20% instead of waiting until tomorrow. The arbitrage opportunity is available to anyone who knows the information, at the expense of anyone trading the stock who doesn't. If everybody knows then there is no arbitrage opportunity because the gap is already closed. reply oceanplexian 5 hours agorootparentArbitrage exists because of inefficiencies in price discovery, and reducing that to “someone has information but another person doesn't” trivializes what traders do and demonstrates narrow thinking about how markets, and how business works in general. Information isn’t the sole reason someone might be able to make money in a market, most times it’s the least important factor. Finance, like any other business relies on execution, not knowledge. For example, you have some information, but it’s worthless because you’re reading into it the wrong way. Or the information is material, but the market doesn’t believe it. Or macro conditions negate the information. Or you don’t have the ability to transact on the information. Or you’re too risk averse to act on the information. Or the classic “you’re right, but it’s the wrong time”, like many companies were in the dot-com era. reply AnthonyMouse 4 hours agorootparent> For example, you have some information, but it’s worthless because you’re reading into it the wrong way. Or the information is material, but the market doesn’t believe it. Or macro conditions negate the information. ... Or the classic “you’re right, but it’s the wrong time”, like many companies were in the dot-com era. These are all part of knowing what's going to happen. If you think you know something but you're wrong, you're wrong, and the person who does know (or makes a better guess) is the person who takes your money. > Or you’re too risk averse to act on the information. At which point you might as well tell other people or publish it and then someone else can. > Or you don’t have the ability to transact on the information. This is extremely unusual for publicly traded stocks. Random individuals off the street can open a brokerage account if they think they know something the market doesn't. Even people with no money could sell the information to someone else for whatever they could get, or just tell their friends to have someone richer than them owe them a favor, and then that person trades on it. Probably the most common case you can't use it is when it would be insider trading. But why would acting on some LLM output be insider trading? reply andoando 10 hours agorootparentprevIts crazy how many people don't understand this. I can't believe how many people think they could predict the market with candle light sticks or whatever. If a method for predicting the market is so readily available that someone is selling it to you, it eouldnt work!! reply marcosdumay 13 hours agorootparentprevI have thousands of monkey-stocks that are guaranteed to increase in value on the near future. I can list them to you, so you buy the same as I did. This is not financial advice. reply MikeDelta 13 hours agorootparentprevBuy low, sell high. reply brezelgoring 13 hours agorootparentprevTo the moon! reply polskibus 13 hours agorootparentprevBuy the dip! reply Rexxar 9 hours agorootparentprevOr just append the string to output without asking the LLM to do it :-). reply PeterStuer 1 hour agoparentprev(non informed, layman sideline perspective from casual reading on this subject over the years) Real time (financial) sentiment analysis on financial news sources has been integrated for a long time. Thing about LLM's is, while they could improve on quality, they need to get the latency down before being useful in straight trade. For offline analyst support where time is less of an issue they can ofc be useful, e.g summarizing/structuring lots of fluffed or trawled content. reply bernardlunn 1 hour agoparentprevAs a human, I like anomaly tracking if I understand what you mean by that. LLMs are maybe 99% good and 1% totally wrong (hallucination). Lots of profit betting against the 1% totally wrong. Not hard to see when wrong but do need to act fast. reply Raphael 7 hours agoparentprevHard to waste any time reading about AI because it's likely written by AI. But then I probably shouldn't read anything written past 2022. reply tomatocracy 2 hours agoparentprevOne other area which I think is potentially quite interesting is using LLMs to help in deciphering \"Fed-speak\". Eg JP Morgan built an LLM to try to predict the impact on interest rate markets of speeches by various central bank policymakers. reply t_mann 15 hours agoparentprevI'd think the first application would be along the lines of Github Copilot, perhaps locally hosted - quantitative traders will write a lot of (proprietary) code, too reply OtherShrezzing 13 hours agoparentprevI thin the underlying vector databases should have decent uses in financial markets. Since they can understand taxonomical-ish relationships, a vector db should be able to codify sufficiently large market mover strategies, assuming those strategies are remotely predictable. Once a rival's strategy is codified, it should be possible to undermine it, like some form of heuristic-based insider trading. reply spaceman_2020 5 hours agoparentprevCan Vision GPT be trained to do technical analysis? reply duskwuff 5 hours agorootparentCalling rand() requires very little training. ;) Less facetiously, there's no reason that needs to go through a vision model. If you wanted to do technical analysis, it'd make far more sense to provide data to the model as data, not as a picture of that data. reply conorh 12 hours agoprevWe are working on a project for a client which functions as an analysis tool for stocks using LLMs. Ingesting 10ks, presentations, news, etc. and doing comparative analysis and other reports. It works great, but one of the things we have learned (and it makes sense) is that traceability of the information for financial professionals is very important - where did the facts and information come from in what the AI is producing. A hard problem to solve completely. reply neodypsis 8 hours agoparentCould something like that proposed in \"Training Language Models to Generate Text with Citations via Fine-grained Rewards\" [0] work for you? 0. https://arxiv.org/abs/2402.04315 reply cpursley 11 hours agoparentprevI assume you're ingesting PDFs. If so, how are you handling tables accurately? reply Kon-Peki 8 hours agorootparentIf it was me, I would be ingesting the raw filings from SEC EDGAR and using the robust xml documentation to create very accurately annotated data tables that would be fed to my LLM reply richrichie 5 hours agoparentprevI worked on a similar application and eventually we shelved it. We just could not be confident enough that the numbers in the report produced are correct. There were enough instances of inaccuracies to not use it for important decision making. Which actually meant a lot of double work. reply btbuildem 11 hours agoprevThere were some developments using LLMs in the timeseries domain which caught my attention. I toyed with the Chronos forecasting toolkit [1], and the results were predictably off by wild margins [2] What really caught my eye though was the \"feel\" of the predicted timeseries -- this is the first time I've seen synthetic timeseries that look like the real thing. Stock charts have a certain quality to them, once you've been looking at them long enough, you can tell more often than not whether some unlabeled data is a stock price timeseries or not. It seems the chronos LLM was able to pick up on that \"nature\" of the price movement, and replicate it in its forecasts. Impressive! 1: https://github.com/amazon-science/chronos-forecasting 2: https://imgur.com/a/hTRQ38d reply nostrademons 11 hours agoparentI used to work in financial software, and when writing the charting UIs, I'd wire them up to a randomwalk to generate fake time series data. It was a relatively common occurrence for a VP or the company CEO to walk by, look at my screen, and say \"What stock is that? Looks interesting.\" Unpopular opinion backed up by experience: a randomwalk is the most effective model for generating timeseries that have the \"feel\" of real stock charts. reply lordnacho 1 hour agorootparentYou can tell a stock time series by certain characteristics: 1) There are more jumps down than up. (Maybe not in Pharma, but in general). If there's a gap up, chances are it's on earnings day. 2) Upward movements tend to be accompanied by lower volatility, and downwards by higher. 3) There's a lot of nothing-happened days, and a lot more large jumps than you'd expect in a random walk. I've also spent a bunch of time generating random walks, and it's true that some look realistic, but they often fall into this trap that stock returns are not normally distributed. I also wrote a number of random trading backtests, and it's frightening how few times you need to click the \"recalculate\" button to get a thing that looks like a money printing machine. reply dataexp 42 minutes agorootparentPerhaps it would be easy to code a pseudo trading sequence given a model of the psychological state of the agents in the trading system reply IAmGraydon 10 hours agorootparentprevThat’s my experience as well. A random walk looks just like market data. You could even perform technical analysis on it, finding support, resistance, trendlines, etc. It really makes you realize why technical analysis doesn’t work. reply yobbo 3 hours agorootparentprevYes, but it is also possible to generate \"parameterised\" random walks that have some predictability and are visually indistinguishable from \"pure\" random walks. Or two series that are dependent, but individually look like random walks. reply iamgopal 10 hours agorootparentprevThis is true, I have tested this with multiple veterans and none could tell them apart reply blitzar 3 hours agorootparentI have tested this with multiple veterans and none could tell them apart - but they had a high conviction on which random walks were a good buy and which were compelling shorts. reply bobbruno 2 hours agorootparentAnd there is your arbitrage opportunity. If you can model how analysts will react to a particular timeseries, even if it was random until that point, you have some information about the future. It'd be a good question to figure out if there is a consensus or majority about how to interpret patterns among the people making decisions or writing quant algos, that's something one could use. reply actionfromafar 10 hours agorootparentprevOr it looked interesting because it did not look normal. reply nostrademons 10 hours agorootparentIt looked interesting because it was going up. The random-walks that trended sideways or downwards did not look interesting. reply Onavo 9 hours agorootparentprev> Unpopular opinion backed up by experience: a randomwalk is the most effective model for generating timeseries that have the \"feel\" of real stock charts. That's not an unpopular opinion. The BSM model is based on the assumption that stock prices are stochastic i.e. random walks. Monte Carlo simulations and binomial trees are the two common methods of deriving a solution to the BSM model. reply Bostonian 9 hours agorootparentprevSince volatility clustering does exist in returns, a GARCH model should produced more realistic-looking returns than a pure random walk. reply yzmtf2008 7 hours agoparentprevAs always, when running time series predictions on financial datasets, one need to use daily return (including dividends, corporate actions, etc.) rather than end of day price. Simply outputting the last value (as more or less shown in these charts) is a pretty good end of day price predictor! reply steveBK123 14 hours agoprevLLMs labor savings will only help financial market participants if they manage to do it without hallucinations / can maintain ground truth. Sure its great if your analysts save 10 hours because they don't need to read 10Ks / earnings / management call transcripts .. but not if it spits out incorrect/made up numbers. With code you can run it and see if it works, rinse & repeat. With combing financial documents to then make decisions, you'll realize it made up some financial stat after you've lost money. So the iteration loop is quite different. reply mirekrusin 3 hours agoparentPrice speculations are hallucinations about future with hope of happening. reply hydershykh 11 hours agoprevI think some of the financial applications around LLMs right now are better suited for things like summarization, aggregation, etc. We at Tradytics recently built two tools on top of LLMs and they've been super popular with our usercase. Earnings transcript summary: Users want a simple and easy to understand summary of what happened in an earnings call and report. LLMs are a nice fit for that - https://tradytics.com/earnings News aggregation & summarization: Given how many articles get written everyday in financial markets, there is need for a better ingestion pipelines. Users want to understand what's going on but don't want to spend several hours reading through news - https://tradytics.com/news reply paulryanrogers 10 hours agoparentAs more of the reports get written by layers of AI it makes me wonder how lossy and noisy this whole pipeline is becoming. reply hydershykh 10 hours agorootparentThat's a fair point. But models like GPT4 do not hallucinate much when it comes to summarizing. So I don't think these applications contribute to anything negative. reply trekkie1024 4 hours agorootparentSurprisingly, they hallucinate more than you might think. https://x.com/lefthanddraft/status/1777495120910426436?s=46 reply monkeydust 12 hours agoprev> there is much more noise than signal in financial data. Spot on. Very few can consistently find small signals and match that with huge amounts of capital and be successful for a long period. Of course Renaissance Technology comes to mind. Recommended reading this if your interested, was an enjoyable read:The Man Who Solved the Market: How Jim Simons Launched the Quant Revolution reply b20000 4 hours agoprevThere is no understanding. It is extremely annoying that interpolation is passed off as intelligence. reply jb1991 4 hours agoparentMany people respond to things in conversations just based on common patterns, without much “thought”, and it’s hard to see the difference. reply bvan 10 hours agoprevSo far, the biggest contribution to financial markets has been hype and promises. I expect this will eventually dissipate into disappointment for most. reply alexashka 5 hours agoparentWhat would a contribution to financial markets even look like? The only meaningful contribution to financial markets that I can see can come from asking the question 'what are we even doing with our lives?', followed by elimination of 99% jobs in finance and many other industries. reply ysofunny 14 hours agoprevIf I learned anything from a conference by benoit mandelbrot back in my college days is that gaming financial markets is the only real application of anything scientific but I vaguely remember what he was actually talking about, I never quite made it as a mathematician reply jonahx 12 hours agoparent> is that gaming financial markets is the only real application of anything scientific medicine (living longer, curing disease, vaccines, etc), cheaper energy, cheaper transportation, cheaper construction, cheaper food, better communication, new forms of entertainment, just off the top of my head. reply nexuist 3 hours agorootparentI've sort of come around on this. Yes, everything you listed is valuable and good. But the reality is all of it was built with money that came from banks and investors. The only reason to do anything scientific is to get investors to give you money. If you do something scientific that does not make people want to give you money you will impact no lives. In this way gaming financial markets is indeed the only point to doing anything ambitious at all. reply altdataseller 1 minute agorootparentThis sounds… kind of obvious? Very few entities are non profits or governments. Of course companies specializing in science are in for the $$$… reply tiborsaas 3 hours agorootparentprevTake particle physics for example, the LHC was incredibly expensive and most people's life won't be better that that we've found the Higgs field. It was also not paid by investors but public money. There are quite a lot of science that's basic research and it's done for scientific curiosity only with no clear way of translating that to marketable applications. reply whiplash451 2 hours agorootparentThat’s an extreme example. To @jonah’s point, you could also decide to work for (or build) a profitable company that solves the issues he mentioned —- without having to « embed » it into financial gaming. reply imtringued 49 minutes agorootparentAcademia is primarily funded by the government through grants. reply kortilla 11 hours agoparentprevWhat does that even mean? How is the atomic bomb not real? reply actionfromafar 10 hours agorootparentThe atomic bomb is used very much today to influence markets, to be fair. reply wuj 11 hours agoprevHFTs exploit price inefficiencies that last only milliseconds. The time-series data mentioned in the article is on the scale of seconds. I wonder if its possible to get the time-series data on the scale of milliseconds, and how that would affect the training of the objective function in a LLM. reply multicast 9 hours agoparentTodays derivatives and their pricing are based on the premise that stock prices can not be predicted and behave like a Brownian motion system. If you take real time data from any stock and calculate in order how many times a stock went up in a row or down in a row you end up almost perfectly with a natural probability distribution. HFT's are involved in market making and arbitrage both of which already involves high speed, the later much more, and earning minuscule profits. There are ghost patterns who can be mined for a certain period of time but they are not solely calculated based on trading time series. They involve complex proprietary calculations, some machine learning and relationships between stocks. There is no pattern in the flow how a particular stock is trading. Also from a long-term view its very questionable. How should a model be able to predict that in the middle of a high interest environment, a tech bubble burst and a dumping stock market in general, a new platform called Chat-GPT gets launched that basically carries the whole world's stock market to new heights which causes among other things retail investors to liquidate bonds and other high interest environment assets and flood it into the stock market. It is more than completely of the text-book. That can not be predicted. The million dollar spending guy is at the end the same way off as the guy who simply employs a 100 python line trend-following strategy. reply mvkel 7 hours agorootparent> How should a model be able to predict that in the middle of a high interest environment, a tech bubble burst and a dumping stock market in general, a new platform called Chat-GPT gets launched that basically carries the whole world's stock market to new heights which causes among other things retail investors to liquidate bonds and other high interest environment assets and flood it into the stock market. Because it happened in the railroad boom in the 19th century, the roaring 20s, the 80s, the 90s dot com boom, the biotech boom... History rhymes, and as we know, LLMs make decent rappers. reply imtringued 47 minutes agorootparentprevThe tech is different but the people are the same. reply mhh__ 8 hours agorootparentprevDerivatives are priced under those assumptions because the aim is to calculate exposure/risk (where simple / assume you're wrong is desirable), the pricing is sort of an afterthought most of the time. reply mhh__ 8 hours agoparentprevThe data is reasonably easily acquired, for a price... reply winwang 12 hours agoprevI'm surprised people don't talk more about sentiment analysis -- or is that mostly solved? Would also be interesting to see more treatises on tranformer(-like) forecasting. Some discussion here: https://www.reddit.com/r/MachineLearning/comments/102mf6v/d_... reply dz08dl 12 hours agoprevIs it really fair to say that 177B is not far from 500B? reply nexuist 3 hours agoparentNo, not at all, given the context of stock trading. Stocks do not trade the same way today as they did in 2014. Similarly there is no point in using trading data from the 1850s given that that kind of market with those kinds of traders will never ever exist again. You can only pick a few recent months or weeks to capture current trading sentiment/technique and even then everything could get blown away after the next rate hike or international incident. Generally I don't think there is any alpha in training transformers to predict the next price point just given historical price data, because the price is determined by humans (and algorithms trained on data generated by humans) that react to news. If you can predict the news, you can probably predict stock prices, but if you could predict the future you'd have AGI and not some dingy time series calculator. reply datascienced 7 hours agoparentprevGood enough. The comparison is silly though: time series data is not anything like tokenized text data. reply logicallee 12 hours agoparentprevFor rough, high-level comparisons, it might be seen as \"not far off,\" but for detailed, technical assessments, the difference is considerable.[1] [1] https://chat.openai.com/share/a19a3b57-398c-49e7-a140-f58784... reply osigurdson 3 hours agoprevThe problem with attempting to use a timeseries of historical prices to predict future ones is price is an output, not an input. It would be better to try to gather embedding data for everything and then conduct a sensitivity analysis to see what is correlated to price. reply crmd 11 hours agoprevThe synthetic data creation and meta-learning scenario is the only use case that sounds remotely plausible. reply unixhero 14 hours agoprevThe art here for a human would be to find the sweet spot of how LITTLE data to feed the llm and to get the weights and other goodies just right for it to be realistic to run for a single non-billionaire. reply Jerrrry 14 hours agoparentAll you'd get are projections with percentage error margins; you can choose the riskier plays, but it is literally priced in. You'd also get clapped by the HFT bots. The real magic is pairing real human intuition and the LLM's innate ability to discover hidden intuitions and articulate them to find an \"asymmetry\"-where you believe you have found a gradient/play that is under/over valued and play the opposing side - or selling/further leveraging that information. reply logicallee 11 hours agorootparentBuilding on the point about using LLMs for finding market asymmetries, I'm looking to team up with a trader to create a UI that leverages AI to spot these opportunities. The idea is to use custom prompts to generate actionable insights, tailored to real trading scenarios. I'm a developer with experience in clean, effective UIs like this QR and barcode generator[1] and have worked with neural nets in competitive settings - recent robotics contest livestream[2]. I need a trading partner's insight to ensure we focus on the right features and data. If you're a trader interested in shaping and using this tool, I'm proposing a partnership where you'd provide the trading expertise and potentially fund the initial development for a stake in the project. Think of it as investing in custom software that you'll own and can directly benefit from. Anyone interested, please check my profile for my contact. Just looking for one trader-partner who really wants to dive into this. [1] https://qr-code-and-barcode-generator.taonexus.com/ [2] https://www.youtube.com/live/IDF7zN0NGgA reply frutiger 5 hours agorootparentThis is unironically the equivalent of an “ideas guy” asking for a software developer to “just build the app” and do a split on the equity. reply unixhero 5 hours agorootparentI once had an ideas guy try to tell me that he had to get more than 50% because he was the ideas guy and I had no imagination. Guess who is the millionaire and who is broke now? However solving what we're discussing in this thread could lead to an edge in the market. reply daxfohl 11 hours agoprevWouldn't this be \"transformer models\" rather than LLMs? reply JSDevOps 12 hours agoprevSo while the case for GPT-4 like models taking over quantitative trading is currently unlikely…. No shit Sherlock reply dclowd9901 2 hours agoprevNo philosophical discussion about what are we even doing if we’re just operating on the predictions of computers to guess equity pricing? Or operating on the predictions of the predictions of computers to guess equity pricing? This isn’t based on any real evaluation. Just pattern matching. What the hell is this even for? What the hell are we even doing here? If computers can successfully guess the market, what the hell is it even? reply mugivarra69 12 hours agoprev [–] is all text, 1 diagram and no data showing anything. im like wtf. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The AI revolution in 2023 sparked significant investment in technologies like Large Language Models (LLMs), such as GPT-3, known for their potential in language processing.",
      "Despite debates on their effectiveness in predicting stock prices due to challenges like noisy data and market dynamics, LLMs show promise in fundamental analysis and generating synthetic data for training trading strategies.",
      "While not an immediate threat to quantitative trading, the success of LLMs in various fields has piqued interest and investment in AI research, hinting at future opportunities through AI advancements."
    ],
    "commentSummary": [
      "The discussion delves into the utilization of Large Language Models (LLMs) in financial sectors for tasks like predicting market impacts and analyzing stocks.",
      "Concerns are raised about the trustworthiness and accuracy of AI-generated data, particularly in handling PDFs and tables, impacting the analysis in financial markets.",
      "There is a push for collaboration with a trading partner to create a UI tool that leverages AI and machine learning to identify market disparities and opportunities."
    ],
    "points": 209,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1713636195
  },
  {
    "id": 40102912,
    "title": "Racket 8.12: Empowering Programmers with Rich Features",
    "originLink": "https://racket-lang.org/",
    "originBody": "Racket donate docs packages download Racket version 8.12 is available. RacketCon 2023 videos Racket, the Programming Language #lang racket/gui (define my-language 'English) (define translations #hash([Chinese . \"你好 世界\"] [English . \"Hello world\"] [French . \"Bonjour le monde\"] [German . \"Hallo Welt\"] [Greek . \"Γειά σου, κόσμε\"] [Portuguese . \"Olá mundo\"] [Spanish . \"Hola mundo\"] [Thai . \"สวัสดีชาวโลก\"] [Turkish . \"Merhaba Dünya\"])) (define my-hello-world (hash-ref translations my-language \"hello world\")) (message-box \"\" my-hello-world) Mature Racket is a mature and stable product. From the beginning, it has supported cross-platform graphical programming (Windows, macOS, Linux). Package System GUI Framework Standalone Binaries Foreign Interface Practical Racket includes a rich set of libraries, covering the full range from web server apps to mathematics and scientific simulation software. Web Applications Database Math & Statistics Full List → Extensible In Racket, programmers define their own loops with powerful macros. Indeed, these macros are so powerful that programmers make entire domain-specific languages as libraries. No tools, no Makefiles required. Intro To Macros Macros In Depth Making New Languages Sample #Langs Robust Racket is the first language to support higher-order software contracts and safe gradual typing. Programmers can easily deploy these tools to harden their software. The Contract Guide High-Order Contracts The Typed Racket Guide Gradual Typing Polished Racket comes with support for major editors. The main bundle includes an innovative and extensible interactive development environment that has inspired other IDE projects. DrRacket Guide VS Code/Magic Racket Emacs Integration Vim Integration Racket, the Language-Oriented Programming Language #lang typed/racket ;; Using higher-order occurrence typing (define-type SrN (U String Number)) (: tog ((Listof SrN) -> String)) (define (tog l) (apply string-append (filter string? l))) (tog (list 5 \"hello \" 1/2 \"world\" (sqrt -1))) #lang scribble/base @; Generate a PDF or HTML document @(require (only-in racket ~a)) @(define N 99) @title{Bottles: @italic{Abridged}} @(apply itemlist (for/list ([n (in-range N 0 -1)]) @item{@(~a n) bottles.})) #lang datalog ancestor(A, B) :- parent(A, B). ancestor(A, B) :- parent(A, C), ancestor(C, B). parent(john, douglas). parent(bob, john). ancestor(A, B)? Little Macros #lang racket (provide time-it) (require (for-syntax syntax/parse)) (define-syntax (time-it stx) (syntax-parse stx [(_ task) #'(thunk-time-it (λ () task))])) (define (thunk-time-it task) (define before (cim)) (define answer (task)) (define delta (- (cim) before)) (printf \"time: ~a ms\" delta) answer) (define cim current-inexact-milliseconds) Racket allows programmers to add new syntactic constructs in the same way that other languages permit the formulation of procedures, methods, or classes. All you need to do is formulate a simple rule that rewrites a custom syntax to a Racket expression or definition. Little macros can particularly help programmers with DRY where other features can’t. The example on the left above shows how to define a new syntax for measuring the time a task takes. The syntax avoids the repeated use of lambda. Note also how the macro is exported from this module as if it were an ordinary function. General Purpose #lang racket/gui ;; let's play a guessing game (define frame (new frame% [label \"Guess\"])) (define secret (random 5)) (define ((check i) btn evt) (define found? (if (= i secret) \"Yes\" \"No\")) (message-box \"?\" found?) (when (= i secret) (send frame show #false))) (for ([i (in-range 5)]) (new button% [label (~a i)] [parent frame] [callback (check i)])) (send frame show #t) Racket comes with a comprehensive suite of libraries: a cross-platform GUI toolbox, a web server, and more. Thousands of additional packages are a single command away: 3D graphics, a bluetooth socket connector, color maps, data structures, educational software, games, a quantum-random number generator, scientific simulations, web script testing, and many more. Macros work with these tools. The example on the left above shows the implementation of a small number-guessing game. It is implemented in the GUI dialect of Racket, and demonstrates a number of language features. Big Macros Getting to know the full Racket macro system will feel liberating, empowering, dazzling—like a whole new level of enlightenment. Developers can easily create a collection of co-operating macros to implement algebraic pattern matching, simple event-handling, or a logic-constraint solver. While Racket is a functional language, it has offered a sub-language of classes and objects, mixins and traits, from the beginning. The macro-based implementation of a Java-like class system lives in a library and does not need any support from the core language. A Racket programmer can thus combine functional with object-oriented components as needed. Easy DSLs Some languages convey ideas more easily than others. And some programming languages convey solutions better than others. Therefore Racket is a language for making languages, so that a programmer can write every module in a well-suited language. Often an application domain comes with several languages. When you need a new language, you make it—on the fly. Open an IDE window; create a language right there, with just a few keystrokes; and run a module in this new language in a second IDE window. Making new languages really requires no setup, no project files, no external tools, no nothing. IDE Support Racket comes with its own IDE, DrRacket (née DrScheme), and it sports some unique features. For example, when a programmer mouses over an identifier, the IDE draws an arrow back to where it was defined. A programmer immediately benefits from DrRacket while using an alternative language, say Typed Racket. Racket macros, even complex ones and those used to make new languages, record and propagate a sufficient amount of source information for DrRacket to act as if it understood the features of the new language. Any Syntax Racket programmers usually love parentheses, but they have empathy for those who need commas and braces. Hence, building languages with conventional surface syntax, like that of datalog, is almost as easy as building parenthetical languages. Racket’s ecosystem comes with parsing packages that allow developers to easily map any syntax to a parenthesized language, which is then compiled to ordinary Racket with the help of Racket’s macro system. Such a language can also exploit the hooks of the IDE framework, so that its programmers may take advantage of Racket’s IDE. Racket, the Ecosystem Software Software Download Racket Source Code Bug Reports Nightly Snapshot Builds PackagesTutorials & Documentation Tutorials & Documentation Quick Introduction Systems Programming The Racket Guide The Racket Reference Web Applications All DocumentationCommunity Community Discourse and Discord These are the most active places for Racketeers. Slack (sign up), IRC, Reddit, and Mailing lists Racketeers are here, too! Mastodon, Twitter, and Blog Keep in touch. Wiki and YouTube Learn more from articles and talks. Team and Contributing Racket’s development benefits from a large distributed pool of contributors. Friendly Environment Policy Applies to all Racket venues. Software Freedom Conservancy Make a tax-deductible contribution to support our work.Books Books Realm of Racket Learn to program with Racket, one game at a time. Beautiful Racket Make your own programming languages with Racket. Server: Racket Develop a web application with Racket. All Racket BooksEducation Education The Racket Summer School a summer school for researchers, professionals, and (under)graduate students to the Racket philosophy of programming languages Program by Design (aka TeachScheme!) a curriculum and training program for high school teachers and college faculty Bootstrap a curriculum and training program for middle-school and high-school teachersSwag Swag Racket T-Shirts — the perfect way to meet friends, influence people, and stay warm. Racket Stickers — the indispensable accessory for laptops and textbooks.Thank you To the NSF, DARPA, the Fund for the Improvement of Postsecondary Education (FIPSE) at the US Department of Education, the Exxon Foundation, CORD, partners of the Academy of Information Technology, Microsoft, Mozilla, Google, and many individuals for their generous support over the years.",
    "commentLink": "https://news.ycombinator.com/item?id=40102912",
    "commentBody": "Racket Language (racket-lang.org)195 points by swatson741 6 hours agohidepastfavorite70 comments mark_l_watson 6 hours agoI am happy to see Racket make it to the front page. After using mostly Common Lisp for research programming since 1982, I now more often turn to Racket. There are parts of Racket I don’t like, which is most everything *NOT* the compiler, package management system, GUI IDE, Emacs bindings, fun language to use. I don’t care for much of the programming language research parts of the ecosystem - that is the parts that the computer scientist professors who run the project probably care about the most. More power to them! So I feel like a freeloader on the Racket project, just enjoying the little parts I like and ignoring the rest, and not contributing anything (except maybe the little Racket book I wrote). reply hiAndrewQuinn 4 hours agoparenthttps://leanpub.com/racket-ai/read I'd say this counts as a phenomenal contribution. :) reply oumua_don17 5 hours agoparentprevI use CL on a day to day basis (and not in research/PL research domain), have never ventured into Racked yet. Can you please elaborate further on parts of Racket you don't like vis a vis CL? Thanks! reply shawn_w 3 hours agorootparentNot Mark, but I use Racket a lot and sometimes play with Common Lisp. CLOS is way nicer than Racket's class/OO model and how generic functions are handled. Growable/resizable arrays are sometimes nice to have. I prefer Racket and Scheme for the most part though. reply darby_eight 4 hours agoparentprevOk, so to state the obvious, if you don't turn to racket for \"programming language research\" why do you use the language at all? reply tgbugs 2 hours agorootparentOne reason is that it has one of the best cross platform native gui solutions out there. It exists because the research side was focused on teaching programming and needed a solution that their students could just install. There is a bit of a learning curve, but once you're over the hump it is just a pleasure to work with. See also things like https://docs.racket-lang.org/gui-easy/index.html. reply darby_eight 2 hours agorootparentWord! reply djtango 4 hours agorootparentprevAs a Clojure developer who did write some Racket, there is a satisfying purity to Scheme (at least to me). reply darby_eight 3 hours agorootparentThis is a wonderful answer! I return to my main calling: figuring out why people use PLT scheme without an academic incentive. There must be an answer here somewhere.... reply triyambakam 3 hours agorootparentIs this a joke or why don't you think they answered the question? reply darby_eight 3 hours agorootparentA) this is absolutely a joke B) they haven't answered which cookie color makes them bananas for their ultraspeculated examination coming up this tuesday C) It's meaningful the only product anyone has built from racket has been hacker news itself. reply runesoerensen 2 hours agorootparentThis is just getting more confusing. reply darby_eight 2 hours agorootparentamen reply runesoerensen 2 hours agorootparentAlso (about HN and racket): https://news.ycombinator.com/item?id=9270478 reply zem 3 hours agorootparentprevit's also a lovely general-purpose language, very pleasant to use. reply rscho 2 hours agorootparentprevErgonomics. It's extremely easy to use, especially in the GUI department. Generally, many things are super easy to use but not always top performance. Other things (macros) are truly exceptionally powerful, but rather complicated. So, industry-grade customer-facing code? Not its focus, maybe not. But incredibly easy internal tools? Sure! reply eru 2 hours agorootparentprevThe same reason people use Haskell outside of 'programming language research', too, perhaps? reply MrBuddyCasino 2 hours agoparentprevYou might be happy (or sad) to hear you‘ve just qualified for the HN humble-brag finals. reply lettergram 5 hours agoparentprevHaving used Racket in college for programing language ecosystem, I can say I don't enjoy that aspect either haha. Racket is easy to get started with and definitely great at what it does. If you're wading into Lisp IMO it's the way to go. That said, IMO Clojure has a larger and more robust community, that said it's a bit of a bigger pain to set up (uses JVM) & has some more nuance. reply pjmlp 37 minutes agorootparentUses the JVM is exactly why Clojure is the Lisp like language I mostly bother with. What matters are ecosystems and not languages on their own, so when switching between languages it is a great benefit that I can reuse my Java ecosystem knowledge, instead of adding yet another ecosystem to those I jump around between projects. However, I really like the almost Lisp Machine like experience from using Raket. reply MarceColl 5 hours agoparentprevhow do you deal with the very bad Racket repl? Thats what turned me off the language edit: I dont reslly understand the downvotes, not trying to be rude or anything. Coming from CL the racket repl is very very barebones, it loses all state everytime you recompile. So I ask him how he deals with that coming from the very nice CL repl reply neilv 4 hours agorootparentThe REPL in DrRacket is like that because it was made by professors, specifically targeting intro students who had never programmed before. They thought a big Run button with a clean slate each time would be less confusing. You don't have to use DrRacket to use Racket. People doing real work with Racket tend to have \"workflow\" setups that are closer to SLIME. I personally used a simpler Emacs setup (\"https://www.neilvandyke.org/quack/\"), and ended up leaning more on modules and embedded unit tests (\"https://www.neilvandyke.org/racket/overeasy/\"), and less on very dynamic interactive evaluation like I'd done before. Though, if you want to monkeypatch heavily, CL is famously good at that. reply djtango 4 hours agorootparentFor someone lazy - what is the set up for the \"evil\" amongst aka vim users reply arbitrandomuser 1 hour agorootparenthttps://github.com/jpalardy/vim-slime you can have a REPL in nvim/vim/tmux/screen/another terminal/or any other window , and send regions from your vim buffer to that repl reply xhevahir 1 hour agorootparentprevThere was a nice extension called DivaScheme that used unchorded keystrokes like vi and allowed structural editing but it stopped working when PLT Scheme reached version 5. (Which I think was when they changed the name to Racket.) https://cs.brown.edu/research/plt/software/divascheme/ reply neilv 3 hours agorootparentprevhttps://docs.racket-lang.org/guide/Vim.html reply MarceColl 4 hours agorootparentprevOh! okay, heavy misunderstanding by using DrRacket. Thanks for the clarification :) reply bjourne 2 hours agoprevI like Racket the language a lot, but can't get to terms with the workflow it imposes on you. When coding in Python I use my editor to write the code and I run it from my shell. Doesn't work with Racket because it takes too long to start. Instead you \"should\" interact with a long-running vm through a REPL. But that means the code you are working on doesn't start with a \"clean slate\" and instead starts with whatever state the vm is in at the moment. It's a great workflow for things like Jupyter notebooks but doesn't work well for larger programs imo. reply pjmlp 2 hours agoparentWelcome to the world of Smalltalk and Lisp Machines, which modern IDEs still don't fully support everything, like hot-reload, saving work states, control over the whole development stack in a single language. reply qwerty456127 2 hours agoparentprevTry Smalltalk :-)) reply captainkrtek 5 hours agoprevWhen I studied at the University of British Columbia, racket was taught in the intro to CS course. I thought it was a great language, and put everyone on an even playing field (eg: students with prior programming experience hadn’t worked with lisp). Also was impressed by the libraries which made it easy to produce simple games or visuals. reply czhu12 4 hours agoparentI also was at UBC for that class and left with a totally different impression. I thought it was awful and set up introductory students very poorly for subsequent CS classes, especially data structures and algorithms that were taught in imperative languages. When I TAed, a lot of incoming 2nd year students didn’t know how to do for loops yet, which really set them back for basic algorithms. I had a much deeper appreciation for functional languages after theory of computation classes in my senior / 4th year. reply lavp 3 hours agorootparentI also TA'd 110 and I firmly disagree. CPSC110 teaches you to view a particular problem as a series of different possible states, and solving for those states (while placing an emphasis on seeing base cases and working up from there). Students learn about data structures like binary trees by the fifth week, and the ninth week they're already able to solve sudoku puzzles using generative recursion. We even touched on the n-queens problem towards the end of the course. Racket serves its purpose well as a simple and fast to learn educational language; it's easy to see and understand recursion. It's also easy to see what the execution order of statements in your program. I will say though that some problem sets were a bit brutal in terms of time taken to complete them. reply zelphirkalt 1 hour agorootparentprevMost data structures taught with imperative languages are just that, data structures for imperative languages, which run on a single CPU core and do not utilize multiple cores well, or at least not without locking mechanisms. One can usually find one of them and copy paste the code, when needed. Much more interesting are purely functional data structures, that can be easily parallelized. That is what universities should teach. I feel like we are quite backward in education with data structures. I can watch a whole data structures lecture/class at no cost besides my own time on YouTube, for famous professors, but when I want to apply it in an FP setting, I will still be dumbfounded and looking for other solutions, which I do not know how to arrive at. reply cess11 4 hours agorootparentprev\"When I TAed, a lot of incoming 2nd year students didn’t know how to do for loops yet, which really set them back for basic algorithms.\" They hadn't done simple recursive functions over lists? What had they been doing? Been a while since I had to read introductory material for Racket (or any similar language), but as I recall functions with a base case exit and recursion is usually among the first things they teach. reply shawn_w 3 hours agorootparentRacket has a ton of different types of for loops for iterating over data structures etc. and collecting the results (or ignoring them). They're implemented under the hood using recursion but that's hidden from the user and it should be trivial to go from then to other languages. Maybe this class they're talking about didn't cover them? https://docs.racket-lang.org/guide/for.html reply czhu12 3 hours agorootparentprevit was indexing and mutating arrays / hashmaps that students had trouble with. Certainly a very different programming model than what’s presented in racket, with recursive iterations. Of course with years of experience it’s easy to interpolate one from the other but these are students who’ve had 4 months of exposure to programming that have to jump from functional languages into operating systems and data structures and algorithms, all of which are taught with imperative languages reply cess11 44 minutes agorootparentI started with BASIC on C64-/VIC-style machines, then went on to dabble in C, C++, Java, Visual Basic, Delphi Pascal, PHP, before I came into contact with CL and Emacs. Recursion-iteration-equivalence was a mindblower for young me, suddenly I realised more ways to use it for problem solving in the algolians than I had seen before. If I had gone the other way around I'm sure I would have saved a lot of time. This is surely a single point of anecdata, but it makes me suspect that it was more about how teaching was done than a 'functional vs. imperative' thing. I also suspect pointers and memory management to have been bigger hurdles than how to format code for iteration, unless the Racket course introduced techniques like quoting. reply spencerchubb 4 hours agorootparentprevLisp variants only really have linked lists. So yes you can do recursion over them, but for someone who has never done a C-like language before, the arrays and for loops will take getting used to reply reikonomusha 3 hours agorootparentLisp \"variants\" have lists, hash tables, multi-dimensional arrays, strings, bit sequences, streams, etc. reply pjmlp 35 minutes agorootparentprevOnly if stuck in 1950's Lisp. reply cess11 3 hours agorootparentprevNo they don't. The basic data structures in Racket are listed here, https://docs.racket-lang.org/reference/data.html . There are also specific array implementations in modules, like the generic array interface in the array module or arrays for math in a math module. Recursing over a list is a way to learn how to implement for, while and friends. If you know this technique understanding for is just understanding a subset of what you are already familiar with. It can be used to iterate over an arrays as well as lists, e.g. with ranges: https://docs.racket-lang.org/reference/pairs.html#%28def._%2... Racket can also be used to teach object oriented programming and programming with structs, if the aim is to teach patterns used in C-like languages generally it's not a bad fit. Well, except advanced stuff like pointer witchery. Though you could probably implement a teaching language that does it with arrays or the byte code directly if you wanted to. It might be a good way to improve on error messages for pedagogical purposes. reply lavp 3 hours agoparentprevGood ol' Gregory Kiczales reply joesb 2 hours agoprevI'm lost on how should I integrate all these \"languages\" of Racket into a single application. Like, I can see there are `typed/racket`, `racket/gui` and `scribble/base` languages. But how do I write a GUI application that create PDF while also having all the code be typed? I tried following Racket tutorial multiple times but I'm still lost on how to tied each unrelated \"languages\" together. IMO, each chapter of the tutorial just talk about unrelated \"language\" and then never once show how they all work together. One chapter will talk about web server but not class system, then another chapter will talk about another \"language\" that support class but then never write web server in it. reply zelphirkalt 1 hour agoparentYou are supposed to be able to choose for each module the language of the module, I think. reply ReleaseCandidat 4 hours agoprevThe biggest problém of Racket isn't the language or it's ecosystem https://news.ycombinator.com/item?id=27531508 reply rscho 3 hours agoparentThis is a very personal conflict between prominent community members. The larger racket community is very friendly, including the core team. reply cess11 3 hours agoparentprevFelleisen is an insufferable prick but also easy to avoid. More a symptom of an illness in academia than Racket specifically, I'd say. In the community there are some 'weird nerds' with strong opinions that are likely to be more annoying since their presence is more strongly felt, but that's just how it is in many social settings where money doesn't chain things down. reply Bogdanp 4 hours agoprevRacket is my favorite language. It's fast, practical, has solid foundations and an extremely nice concurrency story (based on concepts borrowed and extended from Concurrent ML). It has an excellent documentation system, with an integrated package ecosystem, which means that most packages have high quality documentation with cross-references. It has a great backwards-compatibility story -- a lot better than Python's, for example, which I use in my current dayjob. So, my impression of the commenters saying it's too academic or not practical is that they probably never dove deeply enough, or they are former students who were only exposed to the teaching languages in the past. It's definitely not perfect: the community is small, the runtime has a high memory baseline, parallelism requires spinning up a Racket VM per system thread, among others, but these are things that will improve over time. In the past several years, I've: * built & run an e-commerce site written in Racket[1] * built a native macOS and iOS reminders app, available on the App Store [2, 3, 4] * built a cross-platform desktop client for Apache Kafka [5, 6, 7] * built a `#lang` for Lua [8] Among[9] other[10] things[11]. I think that's all pretty practical stuff! [1]: https://defn.io/2019/08/20/racket-ecommerce/ [2]: https://defn.io/2020/01/02/ann-remember/ [3]: https://defn.io/2024/04/09/ann-remember-for-ios/ [4]: https://github.com/bogdanp/remember [5]: https://defn.io/2022/11/20/ann-franz/ [6]: https://defn.io/2023/10/15/ann-franz-for-windows/ [7]: https://defn.io/2023/08/10/ann-franz-source-available/ [8]: https://defn.io/2022/11/12/ann-racket-lua/ [9]: https://docs.racket-lang.org/http-easy/index.html [10]: https://docs.racket-lang.org/deta/index.html [11]: https://docs.racket-lang.org/gui-easy/index.html reply cess11 4 hours agoprevRacket is a very practical language. If you have the impression that it's some academic quirk you should take a look at what Bogdan Popa is doing with it: https://defn.io/ When I personally want a binary with a native GUI I turn to Racket. It's also pretty nice for parsing stuff, like JSON, XML, some text-file formats. The companion book How to design programs, https://htdp.org/2023-8-14/Book/index.html , is nice too, while it doesn't exactly teach Racket (it uses a couple of teaching languages implemented in Racket) it has some valuable ideas that are good to be reminded of every now and then. The macro system might seem weird if one comes from e.g. CL, but once I got used to it I found it relatively easy to stay sane while doing metaprogramming. A couple of more resources that are useful: https://www.greghendershott.com/fear-of-macros/ https://beautifulracket.com/ reply prlin 3 hours agoparentAny specific highlight from Bogdan? Nothing jumped out from a quick skim. reply cess11 3 hours agorootparentHe posted a list elsewhere in the thread, showing off his web-shop and stuff for mobile phones. What I've used most over the years is his writing about GUI development. https://defn.io/2019/06/17/racket-gui-saves/ helped me figure out some patterns I had trouble with and got me stuck in using Racket for this purpose in personal projects. Later he added observables and threading logic, making it even nicer to build GUI, https://defn.io/2021/08/01/ann-gui-easy/ . The core web development tools in Racket aren't exactly trivial to get started with until one is rather fluent with continuations so I've gained a lot from http-easy and Koyo, https://www.youtube.com/watch?v=DS_0-lqiSVs. reply doublepg23 5 hours agoprevI feel like when I dive into LISPs like Racket or SBCL I’m presented with a very academic purist view of programming that clashes with my desire for more tangible, material apps. I feel like I’d be able to comprehend deep computer science theorems in CL well but god forbid if I wanted to crank out a weekend web app. reply neilv 5 hours agoparentMost of the books and tutorials about Racket are by academics. So people who start with those will get that impression. But if you dig in to hacking with Racket (or another Scheme!) as a powerful general-purpose language, and to some extent ignore the \"pedagogy\" stuff, there's a lot of serious programmer stuff. The CL books I've seen, OTOH, tend to be pretty practical, but also more technical than your average Python/JS just-get-me-a-job books. A big thing to know about Lisps is that they're nigh-unemployable (other than Clojure, if you want to work in an enterprise Java shop)... so Lisps disproportionately attract the kind of people who would be programming computers even if it weren't a well-paying job. reply skydhash 3 hours agorootparentMy point of view is that if you want to use CL in your shop, you better have serious money as the people that can do it without blowing everything up are the same people that can find a good job somewhere else. Lisp is more like spell-crafting than house-building (Forth is like that too), so after a while, the system is more like a new language entirely. You have to get a team of highly disciplined engineers and it's not something the current state of the industry tend to do. reply yawpitch 3 hours agorootparentprevLISP to Python (and therefore a job) is really just moving the left hand parenthesis after the function name, replacing spaces with commas, discarding everything good and right about homoiconicity, and embracing a nigh sickening amount of syntactic sugar around OOP. Give me a decent LISP programmer who is willing to compromise and I can get you a job. reply pjmlp 26 minutes agorootparentUnfortunately it also means throwing away compiled code, in JIT and AOT form. Still looking forward to the day Python ecosystem fully embraces it, without going through using C and calling it Python, or reaching to implementations that are 80% there and are largely ignored by the community. Thankfully the whole AI stuff is putting pressure into making JITs part of the standard Python workflow and I am looking forward to that Python 3.13 release. reply skydhash 3 hours agorootparentprevMacro is the name of the game and where you go from programming the business rules using a language to directly stating the rules themselves. Something like (def-action name ..) and it generates the whole routing handling, response generating code for you in a web application. Kinda like, but more powerful than the decorators you find in Flask. reply MarceColl 5 hours agoparentprevRacket is very academic, but Common Lisp (SBCL is an implementation of the CL standard) is not really academic in my opinion its mostly an industrial language. It is an incredibly practical language and its exactly what I reach to to crank a weekend webapp. Its fast and unopinionated and the repl is a godsend for the quick iteration required to do a weekend hack. I think it may just look like that from the outside due to preconceived notions of what lisps are. CL mostly allows you to do whatever you want, at its core it is a compiled, imperative, garbage collected language, with incredible interaction and introspection capabilities and the ability to modify the language in a very straightforward way (take code, modify code). It just has a weird syntax to be able to do macros and reason about macros reply kccqzy 4 hours agoparentprevLet me propose to you an idea that bridges a Lisp and a weekend web app for you: write the infamous TODO app using the React framework using a Lisp. Transpile that Lisp into JavaScript to get it to run in a browser. Depending on your level of perseverance either write that transpiler yourself, or pick one that already exists (like ClojureScript), or even just write your function that outputs JS. I have done that a decade ago before React was even popular. Even if only the syntax is different, it was and is much nicer than writing JSX. reply whiterknight 5 hours agoparentprevCommon Lisp isn’t Haskell. The basic construct is an imperative prog block with goto. reply vitejose 5 hours agoparentprevEveryone loves Racket (Lisp) but no one uses Racket (Lisp). I’ve read Beating the Averages but I still don’t get it. reply anonzzzies 5 hours agorootparentI use racket and sbcl both. I prefer sbcl. We write a lot of software in cl; it is excellent. Cannot say anything else beats the debugging experience unless really low level, but for embedded we use c code gen from cl anyway. reply evdubs 5 hours agorootparentprevI use Racket. reply anonzzzies 5 hours agoparentprevSbcl? It’s very practical, far more than racket. Fast too. reply Insanity 5 hours agoparentprevYeah, different use-case really. Personally I enjoy the more “academic” languages, like the LISPs and Haskell. I play around with them for things like Advent of Code and small POCs, but _rarely_ anything approaching a useful/production-ready application. reply brucenanner 4 hours agoprev [–] Most disrespectful language to teach people who pay to be job ready. reply PhilipRoman 3 hours agoparentI think we really need to separate the two aspects of CS education. Universities should not be job training centers and vice versa. We already have bootcamps, etc. which provide very practical training. I'm pretty sure that CS is the only field with complaints about not being practical enough. Mathematics, physics, medicine, etc. all manage to get along fine. reply mrkeen 2 hours agoparentprevWhere else can merit find a home? When a Harvard dropout writes a small system in PHP, and needs to call in the big guns to try to unfuck the situation [1], who should he call? Someone who got really good at jQuery at university? Besides, aren't modern mainstream languages bragging about having nothing to learn? Learn go in just a couple of days, etc.? [1] https://engineering.fb.com/2014/03/20/developer-tools/hack-a... reply ashton314 3 hours agoparentprev [–] Being “job ready” (whatever that means) is a side-effect and a byproduct of a university education, not the end. The telos is to teach you how to learn and think well. In this regard Racket is almost as good as you can get. (I’d personally rank the Shplait language (new; implemented in Racket/Rhombus) as best for teaching.) Anyone who learns Racket will have learned ways of deconstructing problems that will make them far superior in problem solving to those who learn only e.g. Java and only know how to do things the OO™ way. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Racket version 8.12 is out now, offering robust features like graphical programming support and a diverse package library.",
      "Programmers can define loops, macros, build domain-specific languages, and utilize higher-order contracts and gradual typing.",
      "Racket provides IDE tools, the flexibility to craft new languages instantly, and a supportive ecosystem with tutorials, libraries, forums, and educational materials."
    ],
    "commentSummary": [
      "The Hacker News discussion focuses on comparing the Racket programming language with Common Lisp, highlighting strengths and limitations of each.",
      "Topics include Racket's usability, integrating languages, transitioning between functional and imperative languages, practical applications, and teaching Lisp in academic settings.",
      "Racket is commended for its cross-platform GUI solutions and teaching potential, while Common Lisp is recognized for its versatility and dedicated user base."
    ],
    "points": 195,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1713669485
  },
  {
    "id": 40096253,
    "title": "Beware of Hisense TV: Root Cause of PC Woes",
    "originLink": "https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t",
    "originBody": "priscilla@ghoulnoise4/19/2024, 4:39 PM DO NOT BUY HISENSE TV'S LOL (Or at least keep them offline) holy fucking shit!!!!!!!!!! I've had this in my drafts to finish writing up for a few weeks but today I made another discovery that has pushed me to finally write it up lol. My PC had a few hiccups over the past couple of years. Nothing so serious that I was truly concerned (at first) but, annoyances, to be sure. (\"PC? Weren't you talking about a TV??\" you might be thinking to yourself. Yes. I'll get there. Oh, will I ever get there.) For a long time, the most serious hiccup with my PC was being unable to open Display Settings on my PC. I had to use Nvidia control panel to make adjustments. Whatever. Didn't affect my ability to work or anything. I had just updated to Windows 11 so I thought maybe something went wrong in the update. So I did another install (which meant I had to re-install a ton of music plugin stuff on my computer which takes ages given all the stuff I use.) The fresh install still didn't fix it. So I simply ignored it for the better part of 2 years. Over time, so slowly I didn't really clock them as being related, other things started to fail. I have a Komplete Kontrol S88 midi Keyboard that interfaces with Kontakt and Komplete Kontrol in my DAW. I can use the keyboard to adjust settings or select instruments without having to look at my computer screen. At some point last year this stopped working. I could still use it to input midi, and I was afraid I'd have to do a fresh install of all the Native Instruments stuff I use, so I kept putting off trying to fix it until I had less going on with work. It worked, just not as well as it should! Then, Task manager started to hang in weird ways. It wouldn't close unless I forced it closed with ProcExp. Whatever! Computers are weird! I had trouble getting video capture cards to connect when I considered streaming Splatoon. Whatever! I'm not a streamer, I should probably use that energy on something else. But then, in March, I had some things fail that, turns out, are pretty necessary to using the computer. I was trying to get remote desktop to work on my tablet so I could work in the living room closer to my cat, Grendel, who was still very much in mourning for his best friend (start ms-settings: Access is denied.\" despite being in admin mode) SO I started to panic a wee bit. It's not a great time for me to possibly need a new PC! Plus, it works great when it works. I manually backed up everything important from my main PC drive (as I could not access the windows backup program because it lived in settings!!!!!!!!) and downloaded the Windows 11 installer to a drive in case I needed to completely wipe my computer and start fresh. In a last ditch effort, I tried updating my Nvidia GeForce graphics driver and restarted. Lo and behold, my taskbars came back and I could access settings again! Huzzah! This lasted for 6 days. And then the taskbars disappeared AGAIN along with Settings. At least by now I'd become somewhat comfortable using command to get around, so I was less panicked than I was in round 1. I at least knew the computer wouldn't crash and delete all my shit suddenly (plus I had my backups that were up-to-date). I asked my friends if they'd ever heard of anything like this, and Cohost's own @vogon helped me poke around some additional graphics driver stuff in case updating to the new Nvidia program that's in beta would solve my issue. This felt super promising when I went into safe mode and saw taskbars. So, I did the update, and, as before, when I updated, the taskbars came back! But there were gone again almost immediately. My screen literally started flickering and they vanished. It was so fucking bizarre. I turned to google once more, hoping that somehow, as useless as google is these days, maybe I'd find what I needed to fix my damn computer! Somehow, against all odds, I found it. I happened to use the exact right string in my search to pull up some reddit threads. The first one didn't have anything useful, but a few results down I spotted \"no taskbar and task manager freezes\" holy moly!!! At first glance however, there wasn't anything useful. But then I spotted some collapsed comments at the bottom of the thread. I knew more than likely they just had comments like \"sucks bro\" or \"this is why i use linux :/\" But I expanded them and inside was a link to a Microsoft forum post. I kept my expectations in check and clicked on the link. What I saw had my nearly vibrating in my seat. This was the solution. Narayan B Nov 16, 2023, 12:20 AM So I finally solved it! Source of the problem: an Android TV (HiSense model) connected to my network. Yes. A TV caused this issue. HISENSE? LIKE THE SAME BRAND OF TV I HAVE HAD SINCE 2020??? I followed the instructions. I deleted keys generated by our TV for 5 straight minutes. 5 Minutes of like 200BPM clicking. I restarted. Everything worked again. I laughed so hard I cried. I felt like I'd solved a murder. The main suspect was the PC but the culprit was the TV in the other room. And he almost got away with it!!! If I had spent a few days carrying out a clean install and re-installing all my work stuff, my problem would have come back. If I had taken the PC out back and shot it and replaced it with a fancy new computer, the problem would have come back. Because the problem was never the PC. The Problem was my Hisense TV in the next room. Once I carried this out, I was able to open display settings for the first time (outside of safe mode) in 2 years. When I deleted the keys, my Task Manager started behaving normally again. I turned around and saw that my keyboard was once again displaying the VST controls on its screen. The fancy midi keyboard was back at full functionality. I was able to connect my CRT as a display again using the HDMI -> RCA converters I'd assumed had stopped working (nope! they still worked, they always worked!) Which brings me to today. Almost a month later everything still works. So, I decided to see if remote desktop would magically work again. The answer is: Yes. Yes, in fact, my TV was the reason my remote desktop connection had failed the month before. So here I am, sitting on my couch with my tablet and bluetooth keeb and mouse, writing this post in remote desktop mode so I could attach screenshots of this saga to my post without sitting at my PC. Grendel is snoring beside me as I write. As a treat for reading this far, please enjoy two screenshots of my friends reacting to the solution of my great mystery. adding: the TV in question, in case you're curious: Hisense 50Q8G- 50\" Smart 4K ULED™ Android TV with Quantum Dot Technology (Canada Model) #curse#cursed#tv#hisense#HiSense TV Curse#mysteries#Solved Mystery#troubleshooting#technology#pc#windows#windows 11#windows 10#taskbar#task manager#computer troubles#my computer is sick#cursed tech#reddit#google search#remote desktop#Midi#interface#Ableton#native instruments#komplete kontrol s88 see all 81 comments",
    "commentLink": "https://news.ycombinator.com/item?id=40096253",
    "commentBody": "Do not buy a Hisense TV (or at least keep them offline) (cohost.org)173 points by erremerre 23 hours agohidepastfavorite128 comments armchairhacker 22 hours ago> The root cause is the TV generates random UUIDs for UPNP network discovery every few minutes. That means it poses as a new device. This caused windows to add it to the device list (Device Association Framework, aka DAF) as a new device. This means now 1000s of devices (which is the same device) filled the device tree causing it to enumerate forever. Thus the \"deadlock\". Why is the TV pretending it’s a new device? reply wutwutwat 22 hours agoparentPretty sure Mac randomization is part of the ipv6 spec, and our Apple products have been doing it for years now. The tv can do whatever the heck it wants because if it’s not this thing malfunctioning it’ll be your toaster. Anything concerning networking needs to handle bs like this and not fall on its face. My question would be why are windows installs tracking every device on the network? Sure file server discovery, etc, but why’s it storing them long term, esp if it is overloaded with them, why would it not be setup to do LRU or otherwise purge records. Seems silly to store entries until you have a critical failure. reply hnlmorg 22 hours agorootparentWe aren’t talking about MAC randomisation (which itself would cause issues if it happened every few minutes) but UUID randomisation in UPnP. It’s a completely different protocol that sits in a completely different layer on OSI to the data link layer (where MAC addresses live). UPnP is literally just a protocol that sits on top of HTTP. reply wutwutwat 22 hours agorootparentI glossed over it being upnp. Ok, but the questions till remains why is windows tracking things to the point of deadlocks surely the folks at MS are aware that rouge misconfigured or malicious devices can exist on a computer network (they wrote the OS used by a majority of them after all), especially one which is doing upnp, the firewall hole poking protocol. reply hnlmorg 21 hours agorootparentCaching is a pretty reasonable thing for software to do. Windows could have done a better job with cache invalidation (if it’s even doing any at all) but retaining a cache of available devices via UPnP is a perfectly reasonable expectation. For one thing, it massively speeds up device discovery. However having a device randomise their UUID isn’t a reasonable expectation. This why I said in a different comment that both Windows and Hisense are at fault. Windows could have handled this kind of edge case much better but that doesn’t absolve Hisense TVs responsibility for behaving badly too. reply dathinab 21 hours agorootparentAs far as I understand it's not a cache (but I have not touched Windows for a long time and haven't developed UPnP devices yet). It's more similar to how you have a list of all paired Bluetooth devicees, even if they are currently not connected. Or a list of networks configs for every network you connected to, even if it was a public one. But there are 2 issues: - it's automatically set by \"external\" local network \"events\" (which isn't the case for the things I mentioned above, as long as you don't automatically connect to public networks, never do so) - it's not bound, which is in general not the best idea but in context of the 1st point is really really bad. I.e. it should have some bound related to it's resource cost and expected usage. (If I have to widely guess a bound of 250 or so would be over the top enough to accommodate all edge cases and not lead to issues, most users would be more then fine with idk. 25 or so). Through what to do if the count is exceeded needs some careful considerations, i.e. you would need to drop the \"oldest unused entry\" but for that you need to determine and propagate what \"unused\" means here. To be fair I wouldn't be surprised if that isn't just a problem for Windows. Like e.g. I have heard multiple times people arguing that UPnP shouldn't be enabled (by Windows/Linux/Mac) as it wasn't designed for a modern security context where you can't just trust your local network a lot. reply wutwutwat 21 hours agorootparentprevRight I wasn’t saying don’t cache network topology, IO is slow, cache it. That’s why I wondered why they didn’t have LRU or some other eviction policy to deal with a full cache, stale entries, etc. reply hnlmorg 21 hours agorootparentI’m not defending Microsoft here, but in fairness, we are talking about 1000s of entries when normal use might only see a few dozen over several years of Windows usage even under extreme circumstances. So I can see why cache invalidation probably wasn’t at the forefront of their mind. However, as this incident demonstrates, there’s always an edge case that breaks things. reply yencabulator 13 hours agorootparentIf it's behavior that can be triggered by untrusted network traffic, it needs to be made robust. There really is no excuse. This looks like a trivial low-packet-count DoS on every Windows machine on the network. reply hnlmorg 10 hours agorootparentI don’t disagree with you per se but software development is error prone and we are talking about traffic from trusted networks (if you’re on a “public network” — to use Wi does terminology — then UPnP behaves very differently). My point is: it’s very easy to post on message boards like this about what best practices should be when reviewing fuck ups from other companies. While at the same time ignoring the fact that we fuck up plenty of times ourselves too. In an ideal world software engineering would be held to the same rigour as other forms of engineering. But when software fails people don’t (usually) die. So what happens is developers end up getting overworked, given unrealistic deadlines and have to fix problems with software updates. those kind of scenarios will breed bugs like the aforementioned. So yes, you’re technically correct — but only in a universe that doesn’t have our current tech culture. This is why I’m a little more pragmatic about my criticism. reply RecycledEle 16 hours agorootparentprev> Caching is a pretty reasonable thing for software to do. Every software that saves something needs to have a way to deal with too many things being saved. It's called testing software. Maybe Microsoft should try it. reply hnlmorg 1 hour agorootparentI get it’s cool to be critical of FAANG and point out their obvious (from hindsight) mistakes but the fact is engineer teams are a finite resource and this bug is an edge case from a misbehaving service on a trusted network (UPnP wouldn’t work like this on a “public network”, by Windows definition). We’ve all, yourself included, made plenty worse fuck ups in our career. So let’s show a modicum of pragmatism here please. What we should be more critical of is how this gets reported and handled by Microsoft and Hisense. Do they choose to ignore this bug / close it as “Won’t fix” (working as intended)? Or is this something that will be patched in a relatively recent software update? reply dathinab 21 hours agorootparentprevboth are bugged tbh. MS shouldn't have anything \"unbound\" which can increase due to external events, even if it's limited to the same local network (but MS (and probably not just them) has also a long history with security issues on local network exposed interfaces). At the same time regenerating a new UUID for UPnP every few minutes is buggy to a point it can be seen as a DOS attack. So ... also not okay. (It also doesn't add any relevant degree of privacy to \"very frequently regenerate UPnP ids for devices always there in the same fixed local network AFIK.) reply fanf2 21 hours agorootparentprevThese are not OSI protocols. reply hnlmorg 21 hours agorootparentI didn’t say “OSI protocol”. In fact is there even such thing as an “OSI protocol”? There are OSI layers. Those layers will have protocols and other such standards attached. Of which MAC addresses are a standard which sits inside a sub-layer of the datalink layer. And UPnP is a protocol that utilises HTTP, which itself resides in the application layer of OSI. But I don’t recall ever hearing the term “OSI protocol” before. However I’m also not a networking specialist. reply comprev 22 hours agorootparentprevThe randomisation feature of iOS was apparently not working for a long time despite Apple marketing otherwise [0] [0] https://arstechnica.com/security/2023/10/iphone-privacy-feat... reply joshstrange 20 hours agorootparent> despite Apple marketing otherwise That's a very uncharitable way to phrase that. It's not like Apple was lying or doing this on purpose. It was clearly a bug, a bug that was not wide known for 3+ years. It's not as if it was some open secret that everyone was taking advantage of. I'm not saying no one knew or that no one took advantage of it but I'd imagine the average consumer was complete unaffected by this bug. reply skygazer 17 hours agorootparentI agree with you that it was probably unintentional, and an embarrassing failure, though not malicious. I mostly like Apple products and am tolerant of their business model and practices. But I was struck by your phrasing to wonder whether we really owe charitable interpretations to companies. I do think most people employed anywhere are probably well intentioned, and maybe we owe them something. But companies are almost algorithms that run on a substrate of people. reply wutwutwat 22 hours agorootparentprevUse any mac you want I'm sure we’re being fingerprinted other ways so it doesn’t matter. Security through obscurity isn’t security, after all. reply Nextgrid 21 hours agorootparentMAC randomization prevents the local network from trivially tracking you based on access point logs. Of course, if your device runs software that broadcasts some unique identifier, the network may deploy some collector service to query/capture these but that's already extra effort. On iOS I don't believe apps can run a persistent network server in the background, so background tracking would actually be quite tricky even if you had a cooperating app. Fingerprinting is absolutely an issue for device-based trackers (whether apps or websites), but from the perspective of a passive network observer they're usually quite airtight and don't leak the collected data over insecure channels. reply wutwutwat 21 hours agorootparentNot just access point logs, this isn’t WiFi specific. It’s to make packet inspection harder to link to a device. But, if I run say an airport network and control dns for dhcp I can track your dns requests (non encrypted dns) and interface info across MAC addresses and maybe even tag packets somehow. Also, if every device that connects is routed through a dedicated vlan only containing that device, it can randomize itself all it wants, it’s on a network by itself so it can’t “hide in the crowd”. Those are just things off the top of my head and am not a network person. I’m sure it’s not preventing the motivated from tracking you if they want to. reply Nextgrid 21 hours agorootparentI don't disagree that a motivated attacker can set up advanced infrastructure to collect network-related fingerprints, but this requires active effort. MAC address randomization is designed to at least prevent multiple unconnected networks from trivially tracking a user by making up a per-SSID MAC address. Ideally, Apple would randomize MACs within the same SSID too, but this would break a lot of \"free wifi for X time, then pay up\" schemes that rely on consistent MAC addresses, and despite the appearances Apple is still very much in bed with the establishment and doesn't want to rock the boat too much by giving that much control to the users. reply Yeul 20 hours agorootparentprevMaking tracking illegal by law will do more than any technology created by the tech industry. reply V__ 22 hours agorootparentprevProbably to allow screen sharing or casting. reply wutwutwat 21 hours agorootparentIt’s 2024 if you’re opening ports on your firewall/edge router still, you’re doing it wrong, both as a user and as an app developer. reply brnt 18 hours agorootparentYes, but, embedded software... reply dathinab 21 hours agoparentprevin general devices from time to time pretending to be a new device can be preferred as a form of privacy protection E.g. with IPv6 we have enough addresses to give every device in the world today and for decades to come unique addresses and using devices MAC you can reliably generate them collision free. So you don't need NAT so routers can well only route instead of routing and proxying IP Address/Port. Problem is that leaks a lot of private information. So a schema was added which while upholding the properties allows \"randomizing\" (and changing over time) addresses. But coming back to the smart TV I don't think this is the case here as it changes the address _far_ to often and it's a UPnP i.e. local network only Id. Furthermore UPnP also advertises devices descriptions so de-anonymizing randomly changed IPs is more then trivial. My guess is the UPnP software module is started anew every few minutes and generates a new uuid every time. This might be due to it not persisting the uuid. Weather that is because they just didn't bother implementing persistent device ids or weather it regenerates it after every time it crashes and it crashes every few minutes I can't say. But it looks a huge lot like a software bug. What also is a 100% a but is that this external device misbehavior can mess up Windows (through I wouldn't be surprised if it isn't just Windows which has problems with that). It means that there is a resource controlled by external events which isn't \"bound\" which from a security POV is always a terrible idea, even if it might at most cause a DOS attack (not just because DOS is bad but also because it's not rare that people find ways to use DOS attacks alongside other found attack vectors to more reliably succeed). reply iamleppert 22 hours agoparentprevProbably just how it’s written. Takes extra effort to save a UUID vs. just generate a new one every time some function runs. reply rvnx 22 hours agoparentprevWe can see a positive edge to that, it's like Mac address randomization, if you can't know the UUID of your TV, how can someone else know ? reply brnt 22 hours agorootparentBecause the TV signals it somewhere, just not the owners. reply nilsherzig 22 hours agoprevSo I could just DOS every Windows PC in my local network? Sounds more like a Windows issue reply saltminer 11 hours agoparentYes. If you built a UPNP spammer, it could effectively create a persistent DOS. Considering how many corporate networks rely on UPNP for printers and the like, you could really make life hell for IT departments if you were to embed this in a malicious invoice. reply beardyw 22 hours agoparentprevYes, I thought it's behaviour was something like a DOS attack. If you did it deliberately it certainly would be. I am no Windows fan, but this behaviour by the TV seems to be unreasonable. reply akerl_ 20 hours agorootparentIf you’re designing a device that’s going to sit on a network, you need to build it to handle unreasonable peers. reply PaulRobinson 22 hours agoprevMy main TV is a Philips OLED TV, which has some weird Android derivative baked in, but I've never connected it up to anything other than a HDMI cable, so it can't phone home and it can't trash my local WiFi network with these sort of shenanigans. Most \"smart\" devices are utter garbage. I found a seller of non-smart TVs, which manufacture here in the UK, which made me particularly interested - Cello: https://celloelectronics.com/model/televisions/ (you can filter for non-smart TVs). The problem? The picture quality and sound of the one I got (TBF, the cheapest medium-sized TV they sell) is not great. Fine for a spare bedroom for occasional use so a guest can watch something short, or in an office where I might want the news on, fine. But I couldn't watch a film on it, and it's nowhere near the level of my main TV. Given that no smart TV manufacturer is ever going to get a single byte of data from me they can sell, I guess the joke is on them? I wish I didn't have to do this dance though: give me a smart TV where I can get the apps I choose, but the manufacturer is not ramming it full of spyware... that would be amazing. reply jsheard 22 hours agoparent> but I've never connected it up to anything other than a HDMI cable, so it can't phone home Good news! HDMI supports Ethernet backhaul, which isn't widely used, but in theory your TV could access the internet through the HDMI source you have plugged into it. https://en.wikipedia.org/wiki/HDMI#HEC reply Nextgrid 15 minutes agorootparentAssuming the source is cooperating. The source would need to act as either an Ethernet switch (if itself connected to Ethernet, or a WDS/4addr-enabled Wi-Fi network), or act as a NAT & DHCP server. That's quite a lot of complexity. In practice, I've never seen it used even for point-to-point links, let alone this kind of bridging/routing/NATing. reply matja 22 hours agoparentprev> never connected it up to anything other than a HDMI cable, so it can't phone home Well, you're lucky that your equipment doesn't use the Ethernet channel in HDMI 1.4 then :) you can put a piece of Kapton tape over the HEC pin to make sure that doesn't happen. reply saltminer 11 hours agorootparentIf you do this, keep in mind that ARC (audio return channel) is an alternate configuration for the HEC pins. If the cable is going to a sound system, you cannot cover those pins (unless you have another audio-out cable, like TOSLINK, in which case you wouldn't connect to the sound system via HDMI to begin with). reply aragonite 21 hours agoprevApparently the same issue has been reported with Philips TV [1] and Fritz!Box [2] as well. [1] https://github.com/home-assistant/core/issues/73643#issuecom... [2] https://forum.openwrt.org/t/minidlna-creates-new-media-serve... reply anonzzzies 22 hours agoprevTVs became a nightmare… is there really no market for pay 1000 more and get a non spyware, repairable tv? I know the answer. Same as with buying away ads. The greater fuckery makes more money even than the rich people plonking down money. reply ta1243 22 hours agoparentTV companies don't make $1k for the spyware, nowhere near that. Vizio for exmaple in 2021 made 85% of its revenue from selling the TV, just 15% from the \"platform\". If the $400 TV was instead $470 they would make the same. These scam companies just see it as \"free profit\" on top of a captive audience. PC manufacturers used to do this, with Microsoft's consent. Buy a laptop 20 years ago and you would get tons of shovelware from Dell which made them a few extra pennies. Nowadays I believe Microsoft themselves got into the game. Companies aren't happy with making a product for $50 and selling it for $60 (or $200 or whatever), they have to make a little more by double-dipping. This is of course partly the consumer to blame -- they just look at the headline price. Buy the plane ticket for the lowest price on the middleman interface, but then it's $15 'check-in fee', $50 'landing-fee' etc, but that doesn't matter because look at the shiny advert for $9.99* reply rsync 15 hours agorootparent\"Vizio for exmaple in 2021 made 85% of its revenue from selling the TV, just 15% from the \"platform\".\" What percentage of their profit comes from that 15% of their revenue ? I suspect it's a lot. reply solardev 22 hours agoparentprevI think it's easier to just buy whatever TV you want but never connect it to the internet. Hook it up to a Chromecast or Apple TV or a Plex box or whatever and you're good. I've done that with last 3-4 TVs and never had an issue. And the $50 dongle is much cheaper than paying $1000 more. reply yencabulator 12 hours agorootparentUnfortunately a lot of modern TVs have bloated slow UIs, and want to default to showing their \"home screen\". This makes them much worse experiences for \"just show HDMI input\". reply jprete 21 hours agorootparentprevCellular radios and connections are getting cheap enough to embed in the TVs directly, so this is at best a temporary solution. reply solardev 20 hours agorootparentI just won't buy those TVs. reply philistine 20 hours agorootparentprevOnce again, this whole idea rears its ugly head. There has never been a TV, let alone a device, that has included a cellular radio to maintain a connection so it can keep tracking you. Never happened. It’s just a thing we know can happen, but doesn’t for myriad reasons like humans flying in drones. reply photon_rancher 18 hours agorootparentCars pretty much all do this now. reply BlueTemplar 19 hours agorootparentprevIsn't it mandatory for new cars in the EU now ? reply jtbayly 9 hours agorootparentprevAt a school I was doing IT support for, I had a Hisense TV suddenly refuse to work at all (couldn’t even select HDMI input) until I connected it to the internet and created an account. reply Rinzler89 22 hours agoparentprevThere's digital signage displays (like the ones you find in airports, shops, practices, etc) but those are only sold to private entities, not direct to consumers, if you can score one of those on ebay, but you'll get no warranty obviously and energy consumption might be higher and missing fancy picture and processing modes. Or just buy a normal TV and never connect it to the internet, what's so hard about that? Cheaper than paying more and getting less for a digital signage screen. reply 15155 22 hours agorootparenthttps://www.cdw.com/product/nec-e558-e-series-55-class-54.6-... These devices aren't at all exclusive or difficult to purchase. reply Rinzler89 22 hours agorootparentDepends where you live. reply thaumasiotes 22 hours agorootparentIt does? https://www.amazon.com/Samsung-Business-QE43T-Commercial-LH4... Where would you live that they would be hard to find? reply Rinzler89 22 hours agorootparentWhat's the pros of that signage TV versus any consumer TV? It's still a \"smart TV\" running Tizen OS from Samsung so high chances of similar software shittyness. reply namibj 21 hours agorootparentMine, a 43\" 4k AMVA3 from iiyama, shines with a comfortable 18/7 uptime rating. Essentially that means I only have to turn it off when I'm sleeping to stay in the warranty. Also I can officially customize the splash screen it's bootloader throws up before it goes and switches to the last selected input and it's graphics settings. Having gotten a 40\" sibling that was sold as \"desktop monitor\" with otherwise identical panel (besides of course pixel pitch) spoiled me with it's \"technically counts as HDR\" native contrast from a burn-in resistant LCD, also suffering just barely perceptible loss of contrast with no significant color shift at all viewing angles that aren't already too extreme to comfortably read text due to distortion. Sure, it's slow and thus not suited for competitive counter strike, code editing and SDR-mastered visual media look about as good as an LCD can. They do though seem to not consider factory/nominal color calibration for the panel with the signal processor and a regular operating system on an attached PC something they want to offer on these, which is annoying because with 8 bit and the nominal 5000:1 static contrast mild banding isn't even unexpected. So it'd be preferable to just drop an ICC profile onto the computer that's valid for when the screen is set to native gamma and native white balance (I.e. dumb panel mode) and get to make full use of the panel without the excess expense of individually calibrating devices to do digital proofs ahead of actual production/view the material during actively ongoing color grading work. A decent color managed (but not individually calibrated) screen will suffice happily for most tasks, to the point where further calibration won't be perceptible in a double blind test with the screen isolated from reference light sources for most untrained eyes. reply pauby 21 hours agorootparentprevThere is no privacy to be gained from buying Samsung products. They're just as bad as every other data harvesting company. reply thaumasiotes 21 hours agorootparentprevIn Samsung's terminology, it is a business signage display. Note that this is considered a separate product category from \"business TV\". The point of a sign is that it shows what you want it to show. Whether that's an advantage depends on whether that's what you're looking for. But every feature of the display is going to be oriented towards that goal. Notwithstanding the Amazon review that says \"Yes it has wifi\", you might find it noteworthy that this model does not have wifi connectivity. (As can be verified from its spec sheet, https://image-us.samsung.com/SamsungUS/samsungbusiness/pdf/s... , or from its page on CDW, https://www.cdw.com/product/samsung-qe43t-smart-signage-qet-... ) Your comments elsewhere in this very thread seem to indicate that this product is exactly what you want, a display that won't show ads and, in the general case, isn't connected to the internet. What exactly are you imagining will happen to someone who uses one? reply rsync 15 hours agorootparentprev\"... but those are only sold to private entities, not direct to consumers ...\" End users and consumers can buy digital signage and commercial displays without any trouble. Here is one on Amazon: https://www.amazon.com/NEC-Commercial-Public-Display-Speaker... reply Nextgrid 22 hours agorootparentprev> There's billboard displays (like the ones you find in airports, shops, practices, etc) but those are only sold to private entities Most of these are resold by niche small businesses but I don't see any incentive for them to reject a private individual buying it? In the UK I got mine from https://www.projectorpoint.co.uk/displays (no affiliation beyond being a satisfied customer) and after wiring them the money I got it next day. I've had good experience with Sony professional displays (model number FW-65BZ35F) - they are reasonably priced and come with stock Android TV with very little bloatware. My understanding is that these Sony units are not billboard displays or optimized for high-brightness/24H runtime, they're what looks like standard consumer hardware just running a different firmware - the upside is that the price is actually competitive with consumer-grade TVs. reply philistine 20 hours agorootparentprevDigital signage displays are terrible at color reproduction, usually have no amenities for CEC or ARC, and did I say they’re terrible at color reproduction. Like the one thing a TV is for. reply saltminer 10 hours agorootparentThat's a bit surprising, I'd expect them to be factory-calibrated since marketing departments love their Pantones. reply slau 22 hours agorootparentprevYou also don’t need to be as extreme as “never connect”. Just plug it into Ethernet, update the firmware, and disconnect it. reply Rinzler89 22 hours agorootparentA lot of TVs only have wifi, no ethernet. Plus even if you plug it only once it's enough to phone home and download and cache some ads to show you later. reply slau 20 hours agorootparentI haven’t experienced that, and therefore hadn’t considered it. USB stick for firmware updates it is from now on, then. reply solardev 21 hours agorootparentprevI've found that even once can be too much, since new firmware/software often include new ads and require telemetry. If you're never gonna use the built in OS anyway, there's no reason to risk it. Most TV manufacturers are engaged in a brutal race to the bottom and they have to enshittify the software to make money. reply slau 20 hours agorootparentThat’s fair. I did plug in my TV after 7 years or so. I was hoping the new firmware would fix some issues (eARC compatibility, mainly). It didn’t. Fixed some other bugs, introduced some new ones. Not my best bet, not my worst. Still no ads, luckily, though. reply kspacewalk2 22 hours agorootparentprevThose aren't difficult at all to buy in North America, especially online. Just get it off Amazon. reply Rinzler89 22 hours agorootparentLike John Oliver said: \"Did you know there are other countries that are not America?\" reply taspeotis 22 hours agoparentprevLG TVs you can set your country to “Other” and it generally disables all its telemetry and upsells etc. reply YurgenJurgensen 17 hours agoparentprevI bought an LG 43” computer monitor (marketed for doing 2x2 PbP of 1080p video streams and small video conferencing rooms, I guess) and it feels infinitely better than any smart TV I’ve used. It was reasonably priced as well. If you want bigger than 43”, you may be stuffed, as I think all the largest monitors on the market are this big. reply techdmn 21 hours agoparentprevThere are definitely dumb TVs out there. I picked up a Sceptre from Amazon. It's not fantastic, but the price was right, has inputs plenty and varied, and zero \"smart\" features. reply chadcmulligan 21 hours agoparentprevMonitor/soundbar and Apple TV works for me reply iaaan 22 hours agoprevIt sounds more like Windows has a denial of service bug? I'm not sure how this is Hisense's fault reply hnlmorg 22 hours agoparentIt’s both. It shouldn’t be creating a new UPnP UUID every few minutes. That’s clearly not good behaviour. However it does also highlight a denial of service in Windows. reply eloisant 22 hours agoprevDo not buy a Windows PC (or at least keep it offline). reply thsksbd 22 hours agoprevAs someone who hasn't used windows since Win7, I'm amazed people are blaming windows for this. While all software is buggy to some extent, Hisense TV is probably maliciously changing their UUID so users cant block their telemetry, or their ads, etc. reply kentrado 22 hours agoparentLets say it isn't a TV but a malicious person trying to cause damage to your system. It is a security issue on the side of the OS. On the side of TV, it is also bad. Therefore, both the TV and the OS have blame in this. reply pquki4 20 hours agoparentprevWhy is UUID related to telemetry? If you use router level AdBlock -- that is, DNS filtering -- lots of ads and telemetry go away by themselves, and what UUID is used is irrelevant. Of course, most people don't do that, but still I don't see how changing the UUID helps prevent blocking telemetry/ads. reply indrora 5 hours agoparentprevApplying hanlons razor to this (as another comment has) this is likely some process that either a) fails to store or b) doesn't bother storing the UUID and thus generates a fresh new one every time it runs, which must be pretty often, and thus causes this. reply rpozarickij 18 hours agoparentprev> Hisense TV is probably maliciously changing their UUID so users cant block their telemetry, or their ads, etc. I'm not defending Hisense and I'm not saying that this can't be the intention but it's a good idea to keep Hanlon's razor in mind in situations like this. reply WarOnPrivacy 21 hours agoparentprev> Hisense TV is probably maliciously changing their UUID so users cant block their telemetry I'm not sure it will. If this was a scenario that caused DHCP to continually issue new IPs - this will likely cause operational issues. Two I can think of are DHCP exhaustion and routine loss of device connectivity. reply Kelteseth 23 hours agoprevThere is no TV OS, that I know of, that does not suffer from one of these issues: - Slow/laggy UI - Ads on the home screen - Sends a shit ton of telemetry home This is why all of my TV are never connected to the internet and only serve as output for my Apple TV. Sadly, I gave up on the cheaper Android TV boxes, because they all have the same issues with crappy software and even started to show ads. reply km3k 22 hours agoparentTVs with Google TV that you put into Basic mode (https://support.google.com/googletv/answer/10408998?hl=en ) fulfill most of that, though they're probably still sending some data back. I have my firewall set to block everything except the domain that handles the firmware updates. reply jtwaleson 16 hours agoparentprevMy 6 year old Philips Android tv gradually got super slow. At some point it had a problem and I had to reset it to factory settings and then I noticed how snappy everything was, like it was when new! Now I keep it disconnected from the internet and use a google tv chromecast instead. Works much much better! reply gn4d 12 hours agoparentprevThe laggy UI is what irritates me the most. I do not have a smart TV, though I do have a cable box, and there is no excuse for an interface to lag in [current year], or even for anything in the past 20 years, which serves to merely display a list of program listings. It's frigging text! Built-in vehicle interfaces are also perennial offenders. reply Gualdrapo 22 hours agoparentprevFirst one being the problem with my Sony KDL-60W607B since day 1 (the youtube app is a pain in the butt), otherwise though it doesn't has too much sparkles and whistles it has been great this 10 years I've had it. I wish you could do something like swap whatever it has internally with the OS to switch it to a Pi or something like that - not something like a Chromecast or an external dongle that would require (1) another power socket, (2) another remote control and (3) switching back and forth between TV input and HDMI input. reply catlikesshrimp 21 hours agorootparent1) try to use 2 or more usb ports in your tv to draw power 2) IR learning remote Beware, amazon link. This is just an example, not a quality product https://tinyurl.com/yks7kmxv 3) I dont have a non-smart solution to that I have one of those tvs (kdl##w####) and yes, the youtube app is becoming unusable. I am quite sure someone might be telling us the party is over (that they want us to use a new tv) reply piyuv 22 hours agoparentprevSame, but I keep my LG c1 connected to internet just to be able to turn it off from HomeKit. Never using its “smart” features though. Apple TV ftw. reply sundvor 22 hours agorootparentHave used Windows PCs connected to my TVs since about 2006 .. so agree in principle. :-) My pet pieve with my current Samsung QLed 4K 65\" (2016) is just how sluggish changing input sources is - when wanting to go between the Xbox and the PC. Have never tried any of its other features, can't imagine they'd be anything other than horrible. The display is a very nice monitor otherwise! Still very good colours. Windows with a Logitech media keyboard is awesome. Zwift, Spotify, Internet media all plays back very nicely. reply slau 22 hours agorootparentprevYou don’t need that. Just enable CEC and let the ATV turn it off and on. My LG TV is completely offline, but I can still say “Hey Siri turn off the TV” and it works. reply heresie-dabord 22 hours agoparentprevIn 2024, there are large LCDs with DisplayPort, HDMI, and speakers that can serve as the display in a home theatre. But for those who need a 65\" wall of advertising, the Invisible Hand will provide. reply dgfitz 22 hours agorootparentWhy are large tv sizes so derided? reply heresie-dabord 11 hours agorootparentThe TV industry is notorious for the telemetry generated by \"smart\" TVs. Commercial \"dumb\" TV screens are available but are harder to find, and as I said, it's only recently that large computer LCDs (not TVs) have become available. But these are still not as large as recent gigantic smart TVs. So when most people want a bigger screen, they see little choice but to accept telemetry. The more privacy-aware people take measures against it. reply indrora 4 hours agorootparentAnd the worst part is that the \"dumb\" TVs are expensive. This is mostly because Roku/etc are subsidizing the cost of the device so far down the line that it becomes impossible to argue for the better ones, especially from companies like NEC/Sharp who make Damn Good displays for business. reply koolba 22 hours agorootparentprevIt’s probably the same set of people that want us all to live in 400 sqft apartments and cannot fathom sitting more than four feet away from the screen. reply zen928 14 hours agorootparentprevSour grapes from either their overbearing neck pain from staring at a 50 degree upward tilt to accommodate for their ugly fireplace mounting point, or from the seething hatred of being unable to fit it in their gaudy 400lb \"entertainment center\" wood furniture pieces that force them to compensate I laugh every time I see a livingroom with all furniture focused toward a fireplace. It truly reminds me how holdovers from out-of-touch lifestyles two+ generations ago still dominate some norms of interior decoration. Larger screens are objectively better in every way if you have the space. reply phantomathkg 22 hours agoparentprevThis is because all of them using low powered CPU/RAM. Similar to all other STB out there. reply Nextgrid 22 hours agorootparentEven the shittiest STB-grade CPU should be perfectly capable of doing what the user wants out of their TV which is draw basic menus for settings and otherwise display the signal unaltered (the signal path is all in hardware anyway, the CPU isn't involved). The reason it's slow is because \"growth & engagement\" wants that slow CPU to be doing more (\"suggestions\" aka ads, telemetry, garbage overdesigned UI, etc) than actually necessary. reply 15155 21 hours agorootparentprevA quality, high-performance UI can be done on a 200MHz MCU. Add a few advertisements, three different telemetry systems, etc. and things change. reply Retr0id 21 hours agoprevIt's obviously bad that the TV is doing this, but the way Windows reacts to it is worse. Presumably any device on the LAN could trigger this exact same DoS failure mode (and much faster, if done deliberately). An annoyance on a home network, but could cause absolute mayhem in a corporate setting. reply saltminer 10 hours agoparentI was thinking the same thing. Lots of corporations rely on UPNP for printers and the like, so if you were to embed a UPNP spammer in a malicious invoice... reply bradley13 22 hours agoprevWhy does anyone use a TV anymore? What does a \"Smart TV\" offer that you can't do better in a different way? For years now, actually more than a decade, we just use a home-theater projector (Epson, fwiw). Audio goes into our stereo system. Set up a media center to feed the projector. If you still have cable (or equivalent), hook up the box from your cable company. reply traverseda 22 hours agoparentMy apartment is a bit too bright for a projector, and I don't want to always watch stuff only in the dark with all the curtains drawn. I'm not ritualizing it. Sometimes I'll leave a podcast or old episodes of the Simpsons on while I cook (open concept living-room/kitchen). reply nunez 19 hours agoparentprevWe have a Sony A95K; the 55\" model. I don't watch TV unless I'm with my wife, but when we do, we get stunningly accurate 4K HDR video at any time of day, faithful color reproduction because the panel is QD-OLED (I had to tune it a little beforehand; I still need to color calibrate it), and the ability to tilt the display based on where we're sitting. You can get those with a projector, but you need space, a good panel to project against, and, most importantly, a REALLY GOOD projector (Epsons that can do this are $2k+, which is how much our TV was). That said, a projector can't be beat for watching stuff outside (though, again, you need a really good projector to watch anything during the day) reply Retr0id 16 hours agoparentprevPrice. Smart TVs are cheaper, whether you care about the \"smarts\" or not. https://www.statista.com/forecasts/1283880/global-television... The average TV costs $333. Put another way, the average consumer is spending $333 on their TV (I think that's a correct inference?). At that price you can get a pretty large 4K LCD panel supporting HDR. I think you'd be hard pressed to find a competitor at that price point, and I'm guessing you spent significantly more than that on your setup. I'm sure that choice made sense for you, but you're not the average consumer. reply dasloop 22 hours agoparentprevSize against everything else?. image quality will be better in a TV with similar price. Also convenience. reply pquki4 20 hours agoparentprevDepending on your environment, a projector could very likely be more expensive but leads to worse experience than a half decent 4k TV. Most of people don't do what you are describing because they are spending their money well, not because they are stupid. reply sys_64738 19 hours agoparentprevNot everybody wants to spend $$$ on a projector. A 30\" LCD can be had for 80 bucks to receive OTA signals. reply WarOnPrivacy 22 hours agoparentprev> Why does anyone use a TV anymore? What does a \"Smart TV\" offer that you can't do better in a different way? For years now, actually more than a decade, we just use a home-theater projector It's a bit of a squeeze for a smallish bedroom. Or an 1000 sqft home w/ 3+ kids, a single wide, a 5th wheel, a shotgun home. Ya know. Places people live. reply ns407 22 hours agoparentprevWhy doesn't everyone have a room with perfect lighting, use special paint or buy a screen to project onto, buy external audio equipment, manage cables and mount the projector all for what is worse image quality than modern oleds. Gee guy, I really don't know. What a hot take you've got here. reply whamlastxmas 19 hours agorootparentThe same point can be made without snark reply maipen 22 hours agoparentprev> Why doesn't everyone do everything like I do? What are you even talking about??? reply fifteen1506 22 hours agoprevWell, props for fixing the problem. But the problem is Windows, not the TV. I wonder what to take of this. Use Linux? Or at least set your home network as \"Public\" instead of \"Private\" (clearly a workaround, not a solution, because then (s)he would need to enable RDP on Public networks. reply wizzwizz4 22 hours agoparentI'd say that the TV is to blame: it's claiming to be tens of thousands of different TVs, and Windows is dutifully keeping track of them and their configurations. Sure, Windows should be able to handle more devices on the home network than could fit within your average warehouse, but an issue like that should only crop up if you actually, y'know, try to do that. (Btw, your parentheses are unbalanced: https://xkcd.com/859/. And the author uses they/she pronouns (not ‘(s)he’): it says so at the top left of the page.) reply hintymad 17 hours agoprevI never got why one needed its TV to be online. The smart apps on TV are slow, have lousy UIs, and most likely badly implemented. A device like game console or AppleTV is much better alternative. reply nubinetwork 22 hours agoprevI'd be curious to see how avahi handles this... but I don't feel like buying a 50\" tv. I have a 40\" and it's big enough. :) reply WarOnPrivacy 21 hours agoprev> I tried to open the system settings, (start ms-settings: Access is denied.\" despite admin mode) > I manually backed up everything important from my main PC drive (as I could not access the windows backup program because it lived in settings!!!!!!!!) Son #3 hates that System Settings is restricted to ≤1 instance by design. Windows only unchanging feature is that it is impossible to work with 2 Settings Windows simultaneously. Working with multiple windows was kind of the point of Windows in the first place. MS doubles down on that problem by continually pruning-away legacy settings options. Their end game seems to be limiting all system option access to just one app - an app that can be downed by janky TV firmware. #3 is right. reply RecycledEle 16 hours agoprevIf electronics and software manufacturers were held liable for their products, we would not deal with this stuff very often. reply kkfx 16 hours agoprevEhm... It's a Windows bug, not a TV one, despite I'm pretty sure the TV is crap. And Windows........... reply deely3 22 hours agoprev [–] TLDR: TV connected to the same network hides task bar, graphical settings in Windows, and cause other issues. But why and how? reply hermanradtke 22 hours agoparentThe screenshot of https://learn.microsoft.com/en-us/answers/questions/1339707/... says this: > The root cause is the TV generates random UUIDs for UPNP network discovery every few minutes. That means it poses as a new device. This caused windows to add it to the device list (Device Association Framework, aka DAF) as a new device. This means now 1000s of devices (which is the same device) filled the device tree causing it to enumerate forever. Thus the \"deadlock\". > TaskManager uses DasHost to enumerate devices for some reason, so that hangs. > Bluetooth relies on device discovery, so that also hangs, and Settings app along with it. > Network discovery in file explorer obviously also needs Device discovery, so that also hangs. reply AshamedCaptain 22 hours agoparentprevAnd the guys conclude it's the TVs fault, not Windows. Sorry, flashbacks to the debates in The Old New Thing where Raymond would claim people blame Windows when it breaks broken software... reply ErneX 22 hours agoparentprevIt’s explained on the solution link: https://learn.microsoft.com/en-us/answers/questions/1339707/... reply DarkmSparks 22 hours agoparentprevlooks like its a bug in windows where lots of new network display devices eventually corrupts the registry, and the TV gets a new identity on a regular basis - probably to protect the users identity. reply itomato 22 hours agoparentprevTV somehow became the main display. \"Huzzah!\" reply ugjka 22 hours agoparentprev [–] because windows is buggy too reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The poster faced persistent technical issues on their PC for two years, such as taskbar malfunctions and settings errors, which were traced back to a Hisense smart TV connected to their network.",
      "Deleting keys created by the TV resolved the PC problems, restoring full functionality to settings, taskbars, and peripherals.",
      "They shared their successful troubleshooting journey online, enabling them to utilize remote desktop capabilities once more."
    ],
    "commentSummary": [
      "Hisense TVs are causing Windows freezing by flooding the device tree with random UUIDs for UPnP network discovery, leading to concerns about security risks and DoS attacks.",
      "Users recommend using external devices like Apple TV or Plex boxes to address the issue, while some have reported better performance after resetting their TVs.",
      "The debate extends to using projectors versus TVs for home entertainment, with opinions differing based on personal preferences and requirements."
    ],
    "points": 173,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1713609116
  },
  {
    "id": 40101317,
    "title": "Nitrogen-fixing Nitroplast: Evolutionary Breakthrough in Eukaryotic Cells",
    "originLink": "https://newscenter.lbl.gov/2024/04/17/scientists-discover-first-nitrogen-fixing-organelle/",
    "originBody": "Adapted from a release by Erin Malsbury at UC Santa Cruz Modern biology textbooks assert that only bacteria can take nitrogen from the atmosphere and convert it into a form that is usable for life. Plants that fix nitrogen, such as legumes, do so by harboring symbiotic bacteria in root nodules. But a recent discovery upends that rule. In two recent papers, an international team of scientists describe the first known nitrogen-fixing organelle within a eukaryotic cell. The organelle is the fourth example in history of primary endosymbiosis – the process by which a prokaryotic cell is engulfed by a eukaryotic cell and evolves beyond symbiosis into an organelle. “It’s very rare that organelles arise from these types of things,” said Tyler Coale, a postdoctoral scholar at UC Santa Cruz and first author on one of two recent papers. “The first time we think it happened, it gave rise to all complex life. Everything more complicated than a bacterial cell owes its existence to that event,” he said, referring to the origins of the mitochondria. “A billion years ago or so, it happened again with the chloroplast, and that gave us plants,” Coale said. The third known instance involves a microbe similar to a chloroplast. The organelle in this discovery has been named a nitroplast. A decades-long mystery The discovery of the organelle involved a bit of luck and decades of work. In 1998, Jonathan Zehr, a UC Santa Cruz distinguished professor of marine sciences, found a short DNA sequence of what appeared to be from an unknown nitrogen-fixing cyanobacterium in Pacific Ocean seawater. Zehr and colleagues spent years studying the mystery organism, which they called UCYN-A. At the same time, Kyoko Hagino, a paleontologist at Kochi University in Japan, was painstakingly trying to culture a marine alga. It turned out to be the host organism for UCYN-A. It took her over 300 sampling expeditions and more than a decade, but Hagino eventually successfully grew the alga in culture, allowing other researchers to begin studying UCYN-A and its marine alga host together in the lab. For years, the scientists considered UCYN-A an endosymbiont that was closely associated with an alga. But the two recent papers suggest that UCYN-A has co-evolved with its host past symbiosis and now fits criteria for an organelle. Organelle origins In a paper published in Cell in March, Zehr and colleagues from the Massachusetts Institute of Technology, Institut de Ciències del Mar in Barcelona and the University of Rhode Island show that the size ratio between UCYN-A and their algal hosts is similar across different species of the marine haptophyte algae Braarudosphaera bigelowii. The researchers use a model to demonstrate that the growth of the host cell and UCYN-A are controlled by the exchange of nutrients. Their metabolisms are linked. This synchronization in growth rates led the researchers to call UCYN-A “organelle-like.” “That’s exactly what happens with organelles,” said Zehr. “If you look at the mitochondria and the chloroplast, it’s the same thing: they scale with the cell.” But the scientists did not confidently call UCYN-A an organelle until confirming other lines of evidence. In the cover article of the journal Science, published last week, the UC Santa Cruz team and collaborators from Lawrence Berkeley National Laboratory (Berkeley Lab), UC San Francisco, National Taiwan Ocean University, and Kochi University in Japan show that UCYN-A relies on proteins from its host cells and that the organelle’s process of replication and division is tightly paired with the algal cell’s process. “Until this paper, there was still a question of is this still an ‘endosymbiont’, or has it become a true organelle?” said co-author Carolyn Larabell, a senior faculty scientist in Berkeley Lab’s Biosciences Area and Director of the National Center for X-Ray Tomography. “We showed with X-ray imaging that the process of replication and division of the algal host and endosymbiont is synchronized, which provided the first strong evidence.” Larabell has been collaborating with Zehr for several years to study the relationship between UCYN-A and the alga using the advanced soft X-ray tomography approach she co-developed at Berkeley Lab’s Advanced Light Source, a particle accelerator that produces X-rays. Her technique allows scientists to rapidly visualize internal components of cells in real-time, under real-life conditions. Valentina Loconte, a research scientist in Larabell’s group, performed the tomography on a large number of B. bigelowii cells, then analyzed the data to generate detailed images showing the organelle’s movements within the alga at all stages of replication. “That’s the beauty of our technology. We can get numbers to make quantitative statements. We have numbers at each stage of the cell cycle to show that this isn’t a quirk,” said Larabell. Meanwhile, Coale compared proteins found within isolated UCYN-A with those found in the entire algal host cell. He found that around half of the proteins in UCYN-A are made by the algal host cell, then labeled with a specific amino acid sequence, which tells the cell to send them to the nitroplast. The nitroplast then imports the proteins and uses them. “That’s one of the hallmarks of something moving from an endosymbiont to an organelle,” said Zehr. “They start throwing away pieces of DNA, and their genomes get smaller and smaller, and they start depending on the mother cell for those gene products – or the protein itself – to be transported into the cell.” This dependent relationship, taken together with the images of synchronized division, shows that UCYN-A deserves organelle status. Changing perspectives While mitochondria and chloroplasts evolved billions of years ago, the nitroplast appears to have evolved about 100 million years ago, providing scientists with a new, more recent perspective on organellogenesis. The organelle also provides insight into ocean ecosystems. All organisms need nitrogen in a biologically usable form, and rely on nitrogen fixers to break apart tightly bound nitrogen gas (N2) in the atmosphere, and convert it into ammonia (NH3) molecules that can then be made into countless other compounds. Researchers have found UCYN-A everywhere from the tropics to the Arctic Ocean, and it fixes a significant amount of nitrogen. The discovery also has the potential to change agriculture. The ability to synthesize ammonia fertilizers from atmospheric nitrogen allowed agriculture – and the world population – to take off in the early 20th century. Known as the Haber-Bosch process, it makes possible about 50% of the world’s food production. It also creates enormous amounts of carbon dioxide: about 1.4% of global emissions come from the process. For decades, researchers have tried to figure out a way to incorporate natural nitrogen fixation into agriculture. “This system is a new perspective on nitrogen fixation, and it might provide clues into how such an organelle could be engineered into crop plants,” said Coale. But plenty of questions about UCYN-A and its algal host remain unanswered. The researchers plan to delve deeper into how UCYN-A and the alga operate and study different strains. Kendra Turk-Kubo, an assistant professor at UC Santa Cruz, will continue the research in her new lab. Zehr expects scientists will find other organisms with evolutionary stories similar to UCYN-A, but as the first of its kind, this discovery is one for the textbooks. This research was funded by the Simons Foundation, National Institute of General Medical Sciences, and the Department of Energy (DOE) Office of Science Office of Biological and Environmental Research. The Advanced Light Source is a DOE Office of Science user facility. ### Lawrence Berkeley National Laboratory (Berkeley Lab) is committed to delivering solutions for humankind through research in clean energy, a healthy planet, and discovery science. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Researchers from around the world rely on the Lab’s world-class scientific facilities for their own pioneering research. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science. DOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit energy.gov/science. Tags: Microbes",
    "commentLink": "https://news.ycombinator.com/item?id=40101317",
    "commentBody": "Scientists discover first nitrogen fixing organelle (lbl.gov)146 points by soVeryTired 12 hours agohidepastfavorite30 comments dzink 3 hours agoUsing this to reduce the need for fertilizer would be awesome, but the engineer in me has to ask “What could possibly go wrong”. Interstellar showed one scenario. A nitrogen absorbing crop-blight or species out-evolving out food crops and overtaking them. With far more fuel the odds would be in favor of species that can consume nitrogen, and how do you keep those at bay when it happens? reply roywiggins 1 hour agoparenthttps://mitpress.mit.edu/9780262544283/nordenholts-million/ > In this novel originally published in 1923, as denitrifying bacteria inimical to plant growth spreads around the world, toppling civilizations and threatening to wipe out humankind, the British plutocrat Nordenholt sets himself up as the benignant dictator of a ruthlessly efficient, entirely undemocratic, survivalist colony established in Scotland's Clyde Valley. reply ctrw 3 hours agoparentprevThis has been going on for 100m years. I doubt we are in any urgent danger. reply mikewarot 1 hour agorootparent\"The greatest shortcoming of the human race is our inability to understand the exponential function.\" Albert Allen Bartlett If there is even a 0.001% chance of an organism with this artificially tacked on to it getting in to the wild, the experiment is not worth anything to be potentially learned from it. It could cause our starvation well before we could respond effectively. reply ctrw 1 hour agorootparentIf the exponent is so small that it's not reached 1 in 100,000,000 years we're safe until humanity goes extinct. reply 2four2 1 hour agorootparentprevSo have mass extinction events. reply tambourine_man 1 hour agoparentprevIf you’re an engineer, surely you recognize that the major plot whole in the movie is that doing an interstellar travel is way more difficult than whatever blight they had to fight. reply geuis 38 minutes agorootparentNot the only plot hole, but yeah, agreed. reply gjytfhbfd 2 hours agoparentprevNitrogen fixing bacteria existed for a long time, they didn't took over the world. reply advisedwang 8 hours agoprevNo doubt we'll see this transplanted into GM crops for reduced fertilizer need. reply adrianN 3 hours agoparentSince we spend a couple percent of our total energy consumption just on turning nitrogen into fertilizer, anything that reduces our dependence would be awesome for the climate. reply throwup238 7 hours agoparentprevMaybe we'll make some progress in human genetic engineering and get it in us too. We'd be able to eliminate the need for B3 vitamins! reply spacephysics 7 hours agorootparentI’ll let other people try that out, then in 20 years we’ll do the classic “well sh/t, messing with this actually* also messes that up. Whoops!” reply andenacitelli 7 hours agorootparentWell I guess we need to figure out how to regression test humans then! reply noduerme 6 hours agorootparentIt'll be lots of fun when AGI starts doing unit tests on us. reply TaylorAlexander 4 hours agorootparentprevIn fact back to the original comment we’re finding out that shoving chemicals and GM crops in to complex ecosystems messes a lot of stuff up too. reply cellis 4 hours agorootparentprevIVG + gene editing, we're already there! reply egberts1 4 hours agoparentprevOr worse, multiple faster than algaes and sucking up all the Nitrogen from thr air. reply yxhuvud 4 hours agorootparentDo you realise 70% of the atmosphere is nitrogen? We will not run out of it. reply __MatrixMan__ 4 hours agorootparentprevRemoving the nitrogen bottleneck would let them capture carbon faster. Given our predicament I'd say we should take all the help we get. reply gjytfhbfd 2 hours agorootparentRemove too much carbon however and the plants die. We don't want a Great Decarbonification Event. reply adastra22 3 hours agorootparentprevNitrogen is inert. Even if you could run out of N2 (unlikely), it would have no effect. reply gitanovic 3 hours agorootparentThat's wrong, the fact that is inert doesn't mean it's useless If you remove all nitrogen from atmosphere, there WILL be consequences reply gjytfhbfd 2 hours agorootparentCurrent planes will not be able to fly since they will lose 70% of lift. They will grow gigantic wings. reply gibolt 1 hour agorootparentThere'd be less drag on rockets too. That could be a plus for when we need to escape the death spiral we create reply bonzini 2 hours agorootparentprevExcept burning your lungs in the not-so-long run, as they are not evolved to deal with a pure oxygen atmosphere. reply ChrisArchitect 8 hours agoprev[dupe] Some more discussion: https://news.ycombinator.com/item?id=40011438 reply justinclift 1 hour agoparentThanks. :) reply gjvc 6 hours agoparentprevnext [2 more] [flagged] natpalmer1776 6 hours agorootparent…did you write a script to enumerate all of this man’s [dupe] comments and post this reply? reply airstrike 8 hours agoprev [–] See also https://news.ycombinator.com/item?id=40101290 for more discussion (although IMHO this here is the better link) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Recent discovery: A nitrogen-fixing organelle named nitroplast found in a eukaryotic cell challenges the belief that only bacteria can fix nitrogen from the atmosphere.",
      "Nitroplast evolved from an endosymbiont to an organelle like mitochondria and chloroplasts, impacting our understanding of organelle evolution.",
      "Implications for agriculture and the environment exist as researchers further investigate the relationship between the nitroplast organelle and its host organism."
    ],
    "commentSummary": [
      "Scientists have found the first nitrogen-fixing organelle, possibly lessening the reliance on fertilizers.",
      "Concerns exist about potential outcomes like species out-evolving food crops but also optimism about reducing fertilizer dependence and combating climate change.",
      "Debates revolve around the advantages and risks of employing this tech in genetically modified crops and human genetic manipulation."
    ],
    "points": 146,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1713649820
  },
  {
    "id": 40099252,
    "title": "Teaching models to excel with self-reasoning tokens",
    "originLink": "https://reasoning-tokens.ghost.io/reasoning-tokens/",
    "originBody": "Self-Reasoning Tokens, teaching models to think ahead. Felipe Sens Bonetto Apr 20, 2024 — 4 min read What is the mathematical formulation of reasoning? How can we make LLMs like chatGPT think before they speak? And how can we make that baked into the model so it can learn to think in a self-supervised way without having to \"explain it step by step\" (or another famous prompt we use when we want to improve chatGPT performance drastically)? How can we teach models to think ahead? I will share with you the results of some experiments that may cast light on the path of \"Reasoning Tokens.\" Introduction As the authors of \"Interpretability in the wild\" have taught us, from looking inside transformers, we know that the computation of the next token includes some information computed in previous steps. This may seem obvious at first glance, but there is more to this affirmation than what meets the eye. This means the language model expends some internal \"cognitive power\" processing and storing information that will be used, not for predicting the very next token but 2, 3, or even 10 tokens ahead. Internal computation of GPT-2, extracted from the \"Interpretability in the wild\" paper As we can see from the image above, the attention heads produce computations that will be helpful only in the far future, and even some calculations that \"headge\" against the wrong answers, exposed in the paper as \"Negative Name Mover Heads\" or attention heads that suppress specific tokens. Visual explanation extracted from the \"Do Language Models Plan for Future Tokens?\" paper Further work has shown that LLMs indeed plan for future tokens. In the paper \"Do Language Models Plan for Future Tokens?\" the authors carefully crafted a mathematical formulation to impede what they call \"Pre-Caching,\" or the ability of the model to make intermediary computations that would be useful beyond the very next token. Their experiments found a small performance gap when the model was \"myopic\" or incapable of planning for future tokens. This is promising but could be better. This indicates that while GPTs plan ahead, most of their power is used to predict only the next word in the sequence. As a sanity check, this gap should increase as the length of the predicted text grows because the model would have more tokens to produce said computations, and indeed, that was what they found in the paper. How do we leverage that? What if we incentivized those intermediary calculations, which are useful only in future tokens, teaching the model to think ahead in a self-supervised way? It turns out that the formulation for such a task doesn't need to be that complicated. Gradient flow of Reasoning tokens! In this first experiment, we introduce reasoning tokens! The model will produce two tokens for each token in the original sequence. As usual, the first token will be used to predict the next token. The second token, however, duplicates the input of the first one and does not receive a gradient \"answer\" from the very next token, only from future tokens; in fact, this token doesn't even participate in the calculation of the very next token. This incentivizes the model to \"pre-cache\" or only put information that is useful for the future in this spot. But talk is cheap. Show me the results. Mini GPT-2 (10M params) trained on 82M tokens. And the results are very promising, showing a reduction of 35% in the loss! From 0.621 to 0.401. The experiment also shows that the model benefits from having multiple tokens to do its \"reasoning,\" forecasting the capability to form long-range dependencies. This validates the hypothesis that we can teach the models to plan for the future, an important first step to get to reasoning. A GPT-2 Small (124M params) model was also trained on 300B tokens of the \"Open Web Text Corpus,\" and its results were also very promising, resulting in a 0.04 validation loss reduction from 2.85 to 2.81. In context, going from GPT-2 Large (~700M) to GPT-2 XL (1.5B) drops the validation loss by 0.13 in the same dataset. All training code was derived from Andrej Karpathy amazing GPT-2 implementation. GPT-2 Small trained on 300B params - 1 Reasoning token What is next for Reasoning Tokens? Currently, I'm experimenting with reasoning tokens in fine-tuned instruction following models, where planning can be much more useful. The formulation is very close to the first experiment. Still, this time, the model can choose when this internal reasoning will start, allowing it to choose when to reason before producing the next word in the sequence. Reasoning tokens in instruction tasks The hypothesis being tested is that the addition of Reasoning Tokens can substitute and outperform models where a \"step by step\" explanation is included in the training phase. This would be useful because those explanations are expensive to produce/obtain. Although such explanations can be useful to the model, gradient descent could find other ways to do that reasoning using all the internal mathematical dimensions of the model in a way that does not necessarily make sense to us. It would be a great fit for \"Mixture of Experts\" (MoE) models, where we can have an expert just for the reasoning phase. The future is bright. Stay tuned for the next advancements.",
    "commentLink": "https://news.ycombinator.com/item?id=40099252",
    "commentBody": "Self-reasoning tokens: teaching models to think ahead (reasoning-tokens.ghost.io)146 points by fesens 16 hours agohidepastfavorite25 comments wantsanagent 12 hours ago\"The second token, however, duplicates the input of the first one and does not receive a gradient \"answer\" from the very next token, only from future tokens; ...\" This formulation doesn't make a lot of sense to me. I get the motivation here but what you're trying to implement is a working memory. Because transformers have perfect retrospective memory within their context window any generation which can be done directly from input tokens will be. At any given point a model might want to write to a working memory, but that does not imply that the next non-working-memory-step will supply useful information to better write to working memory in the future. The model also has to be able to decide when to compare the work done in working memory to the next token. By allowing the model to both exempt output from gradient updates and opt back in to gradient updates, you create a meta-learning loop that could be quite flexible. reply sdwr 10 hours agoparentAs I understand it, this isn't trying to implement actual memory in the form of a cache, but instead some kind of wishy-washy memory-lite. I'm talking out of my ass here, but I feel like real memory shouldn't be that hard to implement on top of chatGPT. Just run it twice per query, the first time as an internal query that fetches from a memory store. The budgeting part would be interesting. How many tokens of the main query do you want to fill with memories? And it wouldn't be able to meta learn how to use the system better, you'd have to update the prompt reply refulgentis 4 hours agorootparentI'll call it \"not even wrong\" :P here, they're putting it in the model, you're describing a common bit of working with LLMs across memory / RAG / etc. reply wrsh07 15 hours agoprevOk so my understanding: you can have the network generate a token that can be used as input to future token generation along with each output token it generates These are called reasoning tokens Initial results with gpt2 are promising You can generalize this to let the network decide when to generate reasoning tokens (I'm unclear on how). There were also multiple lines in the loss graph with reasoning tokens that I don't quite understand (what's reasoning 1 vs 3? Is it the ratio of reasoning tokens? Something else?) reply fesens 14 hours agoparentReasoning 1 vs. 3 is the number of reasoning tokens between each \"text\" token. The 1 reasoning token is exactly what you see in the picture explanation in the article. The generalization comes from making the network predict aand end the sequence only when it predicts a . The training dataset for the upcoming experiment contains examples like: \"\"\" Q: What is 3+2? A: 3 + 2 is equal to ... 5 \"\"\" reply wrsh07 7 hours agorootparentWasting two tokens on start/end reasoning seems expensive to me (a priori) I am curious what that would yield though - in some ways that would be the most fun to analyze (when does it think a lot??) I would also be curious to see at what point you see diminishing returns from reasoning tokens (eg a 1:10 ratio? More?) reply pizza 11 hours agoparentprevI'm just speculating here since I don't know what or where the code is but since inference is still autoregressive; given [a b c] sample [d] distribution of [d] could be over [reasoning token][vocab token] then at next step you have [a b c d] and each has an embedding vector associated so when you go to sample [e] it's a function of [a b c d] reply XenophileJKO 14 hours agoprevI have definately and frustratingly seen GPT3.5-Turbo do a bunch of anticipation in the outputs. Basically it will create pre-conditions so that the final output aligns to some bias. In my specific case it was the bias to provide an answer to a question. This is noticable sometimes in chain of thought intermediate outputs. I ended up having to create some space between the entangled decisions in the chain of thought output. reply PeterisP 12 hours agoparent> In my specific case it was the bias to provide an answer to a question That seems to be a reasonably expected result of the \"instruction post-training\" finetuning with RLHF or otherwise. If for some reason you don't want this behavior, you can avoid this by using a model version that just has the core language modeling without that finetuning, e.g. the llama models have such a version available. reply XenophileJKO 9 hours agorootparentWell in this specific case, the logic I was asking the model to do was. (Highly paraphrased..) 1. Inventory the retrieved items. 2. Determine their relevance. 3. Pick the most relevant or if none of the retrieved items is relevant return an alternative message. What the model will do is add new items into (1) if none of the retrieved items are relevant. If you add some steps between 1 and 2.. it stops doing that. reply alt0_ 11 hours agoparentprev> definately relevant xkcd: https://xkcd.com/2871/ reply XenophileJKO 10 hours agorootparentIf I wanted it spelled correctly I would have run it through the LLM. reply earslap 13 hours agoprevFor the existing models is beam-search like methods hopeless due to combinatorial explosion? Are there no smart ways to improve it? Evaluating multiple futures will be slow but if it means that the model can give vastly better output, it might be a worthwhile trade-off in some cases. I feel like our standard way of sampling the output of the LLMs is a bit too simplistic and my hunch is that it should be possible to get a lot more out of them even if it means losing speed. reply HarHarVeryFunny 11 hours agoparentPeople are considering that sort of beam-search approach - this is what they call \"tree of thoughts\" - generate a branching tree of alternate continuations, then pick the best one based on some criteria. This doesn't seem an ideal approach though, since it amounts to generating a bunch of shallow responses and picking the best, rather than the preferred thinking more deeply before generating. It's not the same as a computer chess program considering N-moves ahead where you are guaranteed that one of those move sequences really is the best one (as long as you don't accidentally prune it out). In contrast, if you generate all possible \"shallow\" N-token responses (bunch of monkeys gibbering), there is no guarantee any of those will be the high quality response you are hoping for. Really planning ahead - reasoning deeply before speaking - would seem harder to implement though, since it'd involve applying a variable number of reasoning steps (maybe looping), then determining when to stop. This also seems different from the proposed insertion of \"reasoning tokens\" since those are shallow reasoning steps (normal single pass through transformer's layers), when it seems what is really needed is more depth of reasoning (\"more layers\"), perhaps coupled with some working memory/tokens. Both schemes (more tokens vs more depth) are also related to the wish to use a variable amount of compute for different tasks/inputs - less compute for simple tasks, more for hard ones. reply earslap 8 hours agorootparentAh yes, I totally agree. I was inspecting the method as a stopgap solution (especially because it does not require retraining or any other special tricks) until researchers figure out \"planning\" in a broader sense. It is very inefficient otherwise, but in the meantime, is just simple sampling with a couple parameters to tune from the output softmax the best we can do? is there no low hanging fruit there? reply jacobsimon 14 hours agoprevI’ve tried similar experiments before by asking the LLM to generate “internal” and “external” dialog, which I think is sort of the same idea at a higher level—-and might be preferable because it would allow for easy introspection vs a new set of tokens? I’m not enough of an expert to understand whether this proposal is intended more for training or inference. reply sdenton4 13 hours agoparentThis method is for training. They are using a stop-gradient to 'shield' some tokens from contributing to prediction of the immediate next token, and thus producing a stream of tokens that are only used for longer term prediction. This is a bit more low level than the usual prompt engineering approaches, and to my mind, a bit more promising. There's more easily measurable results, and I've seen other context where a well placed stop-gradient does wonders... reply lucidrains 9 hours agorootparentyes, it is a stop gradient mask on the attention matrix, iiuc. worth trying reply lucidrains 9 hours agorootparentcould even try it with a fraction of the attention heads, instead of introducing new tokens reply fesens 13 hours agoparentprevThe main advantage of using a new and constant token for reasoning is that, while we would pay the full price during training, in the inference phase, we could do most, if not all, the \"reasoning\" in one shot, without having to feed one generation token at a time. reply jacobsimon 13 hours agorootparentCool! reply exploringBytes 12 hours agoprevFirst association was to extend the modality of text tokens to concept tokens which could be (logical) relationships. Are you aware of similar works? reply pizza 11 hours agoparentNot exactly the same game but you might be interested in Mathematical Structure of Syntactic Merge, Marcolli, Chomsky, Berwick (2023). When we speak we give a string. When we think we don't have to use a string. But we do have to have a functionality to map something that has no single ordering to something that has an ordering (externalization) - a sentence. And vice versa we have a functionality to turn strings into things without a specific ordering (internalization) - thoughts. reply benreesman 7 hours agoprev [–] With a little engineering rigor we could do a push-down automata with semantics Girards-Reynolds constrained around polymorphism. reply rullelito 3 hours agoparent [–] Utilizing Girard-Reynolds constraints on a polymorphic push-down automata fundamentally misconstrues both computational topology and dynamic system semantics.. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Reasoning Tokens\" are introduced to train models like GPT to anticipate future steps autonomously, leading to substantial loss reduction, as per experiments.",
      "These tokens incentivize models to perform intermediate calculations beneficial for upcoming tokens, potentially surpassing conventional models needing detailed explanations.",
      "The application of reasoning tokens in fine-tuned instruction-following models is under exploration for enhanced model performance and future planning."
    ],
    "commentSummary": [
      "The debate focuses on self-reasoning tokens in teaching models to anticipate future outcomes, particularly regarding working memory integration in transformer models.",
      "Stop-gradient is proposed to isolate specific tokens in training to enhance long-term forecasting accuracy.",
      "Discussions highlight concerns about inefficiencies in beam-search methods and advocate for deeper reasoning in language models, suggesting diverse strategies like incorporating concept tokens and fostering internal and external dialogues."
    ],
    "points": 146,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1713635667
  },
  {
    "id": 40100151,
    "title": "AltStore PAL: Europe's New iOS Sideload Option",
    "originLink": "https://altstore.io/#Downloads",
    "originBody": "User Guide AltStore PAL now available! Read the announcement Sideloading for Everyone Discover apps that push the boundaries of iOS. Get AltStore A New Way to Sideload AltStore is an app store designed for sideloading. Every app in AltStore gets a beautifully generated store page with detailed information to make sideloading fun and easy. Browse apps from trusted developers, or add additional \"sources\" to further increase your options. Plus, AltStore is made with security in mind. You can view a full list of an app's permissions from its store page, and AltStore will even automatically alert you if they change so you can sideload with confidence. Learn More Self-Published Apps Anyone can distribute their apps with AltStore. All you need is to make a “source”, which you can do by hosting a text file with basic information about your apps. Users can then enter your source URL in AltStore and your apps will automatically appear. Follow our complete guide to create your own source and start distributing your apps in minutes! Publish Apps By Indies — For Indies Riley Testut Developer Shane Gill Operations AltStore is an open-source project developed by a dedicated team of two. We are supported entirely by donations from our community and you can follow along with our progress on GitHub. We’re continuously working on new updates for our apps, and you can try out in-development features by joining our Patreon. Join Patreon Downloads AltStore, Delta, and Clip are properties of AltStore LLC and are in no way associated with Nintendo Co., Ltd. or Apple Inc. AltStore PAL Available only in Europe. Requires iOS 17.4 or later. €1.50/year + VAT Your subscription covers Apple's Core Technology Fee, payment processing, and server costs. Don't want to pay, or not in the EU? Download the version of AltStore below. AltStore (World) Requires AltServer to install. Follow our step-by-step Install Guide AltServer macOS Requires macOS 11 or later For macOS 10.14 and 10.15, see our FAQ AltServer Windows Requires Windows 10 or later “[AltStore] is clever, has been verified by other developers, and the service has an active community of thousands of users who side-load apps on their devices. For the past few weeks, I’ve been one of them.” - Federico Viticci, MacStories CoNTACT US Business Inquiries Press Developers Resources User Guide How To Distribute Apps About Privacy Policy Copyright © 2024 AltStore Experience Apps like Never Before AltStore allows apps to exist on iOS that may not otherwise. Apple doesn't allow all apps on their store, so AltStore gives those apps a chance.",
    "commentLink": "https://news.ycombinator.com/item?id=40100151",
    "commentBody": "AltStore. The first Apple approved alternative App Store (altstore.io)141 points by janandonly 14 hours agohidepastfavorite82 comments oreilles 14 hours agoIn Switzerland, Delta Emulator is not available in the App Store since Riley made it an AltStore exclusive in Europe. However, you cannot get AltStore either because Switzerland isn't part of the countries where Apple allows alternative stores: https://support.apple.com/fr-fr/118110#countries-and-regions. reply lapcat 14 hours agoparenthttps://apps.apple.com/ch/app/delta-game-emulator/id10485246... reply soziawa 14 hours agoparentprevIt is available for me in Switzerland. In fact, it's the number one app. reply mrbombastic 14 hours agoparentprevCan’t you get it through the world sideloading instructions? I haven’t tried admittedly reply neonsunset 14 hours agoparentprevEven reading the page is mildly infuriating because how far it goes to try to scare people away into using AppStore only. I have never seen a company resist a legislation with this much effort before. reply metalspoon 12 hours agorootparentWhat's more stupid is that I can't install my app to my phone. Because it's supposedly not secure according to Apple. Even more stupid is that if I pay the annual developer fee to Apple, they let me install it. reply neonsunset 11 hours agorootparentYou can install without a fee through your own \"debug locally\" certificate but it expires fast, it's a clunky process... reply rcarmo 14 hours agoprevUnpopular opinion: I'm really not keen on Riley using Delta as a key driver to make EU folk pay for the AltStore. I'd much, much rather get it from the Apple App Store, just like everyone else on the planet--and I'd pay Eur 4.99 for it there, easy, just because I really don't want the hassle of dealing with another App Store. reply gnyman 1 hour agoparentRiley posted the following on Mastodon about this: Getting some Qs about Delta availability, hope this clarifies things! • Delta is exclusive to AltStore in EU • Because of Apple’s new dev terms, all downloads in EU cost us €0.50/yr in AltStore PAL and App Store…so couldn’t offer Delta in EU App Stores without making it paid • App Store only supports one-time paid-upfront apps, so we’d have to pick a price that could support ~years of CTFs • PAL’s €1.50 covers Delta’s CTF • We’d make everything free everywhere if it wasn’t for the CTF https://mastodon.social/@rileytestut/112299267044864020 reply hx833001 13 hours agoparentprevYeah that’s too bad since it’s free everywhere else in the world on the App Store. reply talldayo 14 hours agoparentprevWell, now you can add it to the ever-expanding list of iOS apps you just can't use. No need to feel lonely about it though, there's a lot of great free apps that Android users are also locked-out of unless they flip the sideloading switch. Here are just a few: https://f-droid.org/en/packages/ reply greenavocado 14 hours agorootparentSideloading switch on Android? You install F-Droid and then install whatever you want from F-Droid. I'm assuming you mean the \"install application from unknown source\" pop-up when you open the F-Droid apk from your downloads folder. This is NOTHING compared to the hoops Apple users have to jump through to install third party software. reply rcarmo 14 hours agorootparentprevYour comparison is pointless (I've been using F-Droid for almost a decade, and that's where I get most \"serious\" Android utilities today - like Termux and various UI tweaks I can't do without). I am fine with hacking Android (and running emulators on it) because I don't rely on it for work, IM, family or work stuff. It's just a sandbox for me. But I would have liked to have Delta (and paid for it) on iOS, because that's where I spend most of my time. I'll just wait until RetroArch (or someone else) ships on iOS. reply talldayo 14 hours agorootparentThat's a shame. It's just free software, there should be no reason why you can't just put it on your phone if you have the code and want to approve it. Maybe Apple should re-draft the App Store conditions to better accommodate Free Software if this is the sort of business they'd like to attract. If not, oh well, it's only the users that have to suffer. > I'll just wait until RetroArch (or someone else) ships on iOS. You mean the App Store, right? Retroarch on iOS is practically as old as sideloading Delta at this point: https://docs.libretro.com/guides/install-ios/ reply rcarmo 14 hours agorootparentYou must be new to mobile, and Apple, and notarisation. I can build and install it myself, but even with a developer account it's a hassle. The key problem with iOS app distribution isn't alternate app stores--it's enabling end users to run their own apps on their own devices without timeouts or jumping through hoops, and nobody seems interested in fixing that. reply jorams 13 hours agorootparentThe problems you talk about are deliberately created by Apple and only fixable by Apple. If you buy an iOS device you're opting in to whatever bullshit Apple comes up with that isn't explicitly regulated away, and when regulation is introduced Apple has made clear that they will do whatever they can to avoid complying with the spirit of said regulation. The creator of Delta is pushing the first regulation-protected route they have for distributing their app. You started this thread saying you wish they didn't. Dealing with notarization, timeouts, yearly developer account subscriptions, it's all terrible. The only hope to ever get away from it is to put up with as little of it as possible, which currently means an alternative app store. Or you can stop buying Apple devices. reply rcarmo 12 hours agorootparentI will stop buying Apple devices when there are quality alternatives, hardware and ecosystem-wise. Android is (still) neither at this point. reply talldayo 11 hours agorootparentSounds like you're going to be in the business of fielding unpopular opinions for quite some time. I'm shocked that someone would shake their fist at the person developing code that you can go download right now[0] and not the business that prevents your phone from using it. If this is your sincere opinion, you either misunderstand the situation or expect to have your cake and eat it too. [0] https://github.com/rileytestut/Delta reply jorams 8 hours agorootparentprevWhat you're saying is that you are not interested in solving what you describe as \"The key problem with iOS app distribution\". I am not claiming Android is perfect in any way; I don't think a truly good usable mobile option exists. I will claim that the differences in quality are minor in comparison to the problematic level of control Apple exercises over iOS. reply fsflover 11 hours agorootparentprevDepending on what you mean by \"quality alternatives\", GNU/Linux phones exist. Sent from my Librem 5. reply blackeyeblitzar 14 hours agoprevWhat does “Apple approved” mean? I know Apple has heavy handed curation, for example requiring various forms of moderation / censorship of social media apps. Can these stores offer whatever they want? Also what’s with that fee? Why aren’t phones forced to operate like traditional operating systems? Apple shouldn’t be allowed to charge fees for people to install software on devices they own. reply aranelsurion 13 hours agoparentI'm no expert on the matter, but this is what Apple says: > All apps listed on alternative app marketplaces are submitted to a Notarization process with Apple. Notarization is a baseline review that applies to all apps, regardless of their distribution channel, focused on platform policies for security and privacy and to maintain device integrity. Through a combination of automated checks and human review, Notarization helps ensure apps are free of known malware, viruses, or other security threats, function as promised, and don’t expose users to egregious fraud. Apple does not enforce the App Store's high standards for business practices and content on apps distributed through alternative app marketplaces. So what I understand is that it's a baseline check for security, technical issues etc. Content moderation doesn't seem to be a part of it. reply gnyman 1 hour agorootparentoh it's definitely much more than a \"baseline check\", it seems to be mostly the same silly rules as the real app store for example, the developer of Clip (Riley Testut) had to add a pointless \"map\" function which uses the user's location in order to be allowed to run in the background The first version I tried used the user’s location to remain active, but was rejected by Apple. Testut then updated Clip with a Map feature — so there’s a reason for the app to remain active in the background — to receive approval. [1] https://www.theverge.com/24100979/altstore-europe-app-market... reply metalspoon 12 hours agorootparentprevFuck Apple, I never need their check. I'm not a grandma. reply orf 12 hours agorootparentSure you did, even indirectly: would iOS still be around now if every grandma got their money stolen through iOS malware? reply bambax 3 hours agorootparentWindows is still around, I think. And you used to be able to install anything on it, from any source, with zero control by MS. (On newer versions that's possibly less true.) But if you want to steal grandmas' money, it's much easier to setup a romance scam operation than to try to get them to install some app. reply ranger_danger 9 hours agorootparentprevYes. reply fsflover 11 hours agorootparentprevDid it happen with Android yet? reply blackeyeblitzar 10 hours agorootparentOr with Windows computers? reply paulmd 6 hours agorootparentprevyes https://www.darkreading.com/endpoint-security/mobile-cyberat... https://www.nokia.com/blog/the-hidden-threat-of-banking-malw... https://dataprot.net/statistics/malware-statistics/ (inb4 “I ain’t reading that”) reply mvid 9 hours agorootparentprevYou can search for innumerable articles and anecdotes of people moving their family and elders to iOS on mobile and Linux on desktop for exactly these reasons reply Aloisius 12 hours agoparentprev> Why aren’t phones forced to operate like traditional operating systems? They are operating like traditional operating systems, namely like game consoles where paying royalties to OS makers for distributing software has been a thing since the Atari/Activision settlement in 1980. reply 2OEH8eoCRo0 10 hours agoparentprevIt's malicious compliance. No judgement afaik says anything about an approval process. reply sixothree 14 hours agoparentprevKnowing Apple I'd personally be concerned about retribution from them. I feel like something else completely unrelated on my device will stop functioning because I'm a user of the app store. reply switch007 13 hours agorootparentBut you'll never be able to prove it Like my iPhone 12 getting slower and worse battery with each iOS release even with a new battery... reply apantel 12 hours agorootparentYeah you can’t update these phones. Things just degrade if you do. Happily running iOS 15 on my iPhone 12. reply teekert 12 hours agorootparentI don’t feel like my iPhone 12 mini is slow with 17.4? I guess it doesn’t make sense t argue about subjective experiences but I just wanted add a some anecdata. Battery feels just fine (over a day, that is with Tailscale turned off [did get better some updates ago.]) reply shepherdjerred 12 hours agorootparentprevThat's strange. I'm on iOS 17 on my iPhone 11 Pro and it's just fine. Maybe the battery issues mentioned are due to normal degradation? reply SpaghettiCthulu 12 hours agorootparentprevDo older versions of iOS still receive security patches? reply emeril 10 hours agorootparentnot really reply mateus1 14 hours agoprevCan someone give me examples of good apps other than emulators? reply metalspoon 12 hours agoparentIt's about the stupid fees for Apple, iirc. So, they'll arrive in the end. Just not now. reply fwn 14 hours agoparentprevI think by design of the ecosystem it makes the most sense for apps that can't be on the main store. That would be emulators, uncensored social media, probably apps that do not meet the NSFW policy of the App Store... that kind of thing. I'm not sure we'll see user-enabling apps for proprietary services like newpipe, because it's still a store, so corporate censorship is to be expected. In that sense, it is far inferior to a true, no store, side-loading ability like on Android, where you can freely install any application you want. I've managed to get almost all my apps (sans Google apps and banking) to autoupdate through Obtainium on Android. It really feels like the future. reply ssbash 13 hours agorootparentYattee, an alternative to NewPipe, has been available on the AppStore for over a year. Well before there was any pressure from the DMA. https://apps.apple.com/us/app/yattee/id1595136629 reply fwn 12 hours agorootparentYattee is a good example for what Apple puts those developers through. Here is a direct quote from their website: > Apple keeps rejecting macOS version of Yattee for several unrelated and random reasons. As I believe the App Store approval process is random and getting approval depends mostly on whether the reviewer has had a nice day, I keep resubmitting macOS versions with every update so maybe we will get lucky one day. https://github.com/yattee/yattee/wiki/FAQ reply stefan_ 11 hours agorootparentThe app also has obscure and incomplete features & UI purely to comply with Apple guidelines (or trick human reviewers paid beans into not actually realizing what the app is). reply Aloisius 13 hours agorootparentprevApple requires Alt stores only distribute apps suitable for all audiences (age 4+), so NSFW is out. reply AnthonyMouse 12 hours agorootparentprevIt seems like one of the major categories should be free software apps that Apple won't put in the store because of the license, e.g. GPL. Can alternate stores distribute those on iOS? reply dainiusse 14 hours agoprevI agree it is cheap, but I want to at least try something without paying. reply cush 14 hours agoparentIt's probably important that consumers vote with their wallet here reply Hamuko 12 hours agoparentprevI don't think they can offer you a trial option when Apple charges them by installs. reply ChrisArchitect 13 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40067556 reply drooby 14 hours agoprevIs it me or is the copy on this website terrible? And probably super confusing to laymen. reply 1over137 13 hours agoprevSeems this requires an AppleID. So is there no way to sideload without one? reply saagarjha 13 hours agoparentNo. reply 1over137 11 hours agorootparentDarn. For me at least, half the point is to be more anonymous from Apple. reply TheJoeMan 14 hours agoprevUnfortunately paying Apple’s extortion fee might provide ammo to legitimize it. reply talldayo 14 hours agoparentSomeone ends up paying Apple either way. On the App Store, that person usually ends up being the developer. With the new framework (stupid as it may be), developers can choose to externalize that cost to users and make an example of how ridiculous it is. With high-demand apps like emulators, it's easy to attract users and directly expose them to the limits Apple imposes on them. That's a great vehicle for change, and I fully expect Apple to backpedal on the Core Technology Fee once it attracts broader scrutiny. It makes perfect sense to me that paying $99/year for the App Store is cheaper than $0.50/install. But you shouldn't have to pay anything to install Free Software in the first place, so I can also see why some people would consider protesting Apple's policy to be invaluable. reply nazgu1 14 hours agorootparentAs far as I know to distribute app outside AppStore you need to notarize it, so you need to pay for developer account $99 AND still $0.5 per install reply metalspoon 12 hours agorootparentprevThat's a common misconception about free software. Free software is about the user's right to see and modify the software. Doesn't mean people can't do business with it. You're right that they usually don't require fee for installation, but that's merely because that the source code is available on the internet, in most cases, and so the user can install it without paying anyway. reply Aloisius 13 hours agorootparentprevApple waves fees for non-profits (both developer annual fee and core technology fee), so at least qualifying free software organizations don't have to pay anything for either the app store or alt stores. https://developer.apple.com/support/fee-waiver/ reply talldayo 9 hours agorootparentFree software organizations don't package software. The people developing iOS ports of FOSS applications are not going to register as a nonprofit to play Apple's silly game. It's an insult of a fig-leaf to the industry at-large. reply Aloisius 7 hours agorootparentPlenty of free software organizations package software. Heck Mozilla, Signal, VideoLAN, Mastodon, Wikimedia and Tor even package iOS software. And frankly, most of the iOS ports of FOSS aren't themselves are closed source, so they don't really count as FOSS, but if the people who, say, ported OpenOffice to iPhone gave it to the Apache foundation? I can't see why they wouldn't release it. reply TillE 14 hours agoparentprevI really can't imagine any circumstance where the European Commission is ok with Apple charging competitors for the privilege of competing with them, in a market explicitly mandated by the DMA. AltStore or Epic complying with Apple's current dubious terms makes no real difference. reply KomoD 14 hours agoprevHow many apps are there? reply aranelsurion 14 hours agoparentTwo. Delta the emulator, and a clipboard manager. I assume there will be a few more soon, though Apple still pushes for notarization and such, making things difficult for the lone hacker developing for fun. So I don't expect as many apps as once cydia had. It's pretty cool that I can play Chrono Trigger and Pokemon FireRed Rocket Edition on the go though :) I'd love to see Epic bringing Fortnite to AltStore out of spite. :D reply _djo_ 12 hours agorootparentEpic has already announced they’ll be launching the Epic Games Store on iOS later this year. https://www.epicgames.com/site/en-US/epicgamessweden https://twitter.com/epicgames/status/1770500825166545305 reply AnthonyMouse 12 hours agorootparentprevIf an alternate store can have a game emulator, could it also have e.g. an Android app emulator, which in turn could run Android app \"ROMs\" that aren't individually notarized? reply Hamuko 12 hours agorootparentprevIs it not possible to add sources to the PAL store? reply doctorwhat 14 hours agoprevThe contrast of text and background is really really bad for the muted text. Accessibility related changes would be most welcome. reply worldmerge 12 hours agoprevHow can I trick my USA based iphone to think it's based in Europe to access these app stores? reply quietfox 14 hours agoprevI subscribed some days ago, but still can’t get the download to work, even in Safari as suggested. reply aranelsurion 14 hours agoparentI had an issue with the Download button not working the first time as well. The solution was to go back to AltStore homepage, click and go through the checkout process again, at some point it says you already are subscribed and instead of charging you again takes you to the download page, where this time the button works. At least that's what happened for me, worth a try. reply bouk 4 hours agorootparentThank you!! reply quotemstr 14 hours agoprevWhat specific apps might this app store distribute that Apple's won't? Apple's insistent that third party app stores enforce Apple's content restrictions IMHO defeats the purpose of the third party app store mechanism. reply newaccount74 14 hours agoparentApple changed the rule to allow Gameboy emulators on the App Store days before the Alt Store went live. So even if you don't install the Alt Store, that's already a benefit for everyone. Competition is a good thing. reply gardenhedge 14 hours agoprevApple must be delighted to see this.. Because it's an awful UX reply matthewtse 14 hours agoparentyes, quite a few hoops to jump through to get something working. Back in the day, I went through massive effort to jailbreak my iPhone, void the warranty, and use the Cydia app store (amazing that this whole community operated for free). But slowly the vanilla iPhone experience got good enough that I didn't need to jailbreak anymore, or at least it wasn't worth the effort. And the community died down as well. Interesting to see the resurgence of a similar concept. reply aranelsurion 13 hours agorootparentI guess community of it died also because of how difficult/rare jailbreaks got, with some significant time of droughts between jailbreaks. Also jailbreaks had too many downsides; some apps would be too invasive, crash the Springboard, some secure apps (bank etc) wouldn't run, iirc updates weren't possible or hard to do, \"tethered\" jailbreaks required restoring them each time you reboot etc. etc. Too much day-to-day fighting against the system at the end. Alternative stores OTOH have a lot more promise, if Apple can't manage to kill them through malicious compliance. They have a lower barrier of entry, and most downsides of JB don't apply either. reply Zambyte 12 hours agoparentprevAwful UX relative to the Appstore? How so? reply hcks 14 hours agoprev [–] Can’t wait to have to download 20 different marketplaces with UX all uglier than the last reply idle_zealot 13 hours agoparent [–] The App Store sets a pretty low bar. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AltStore PAL is now accessible in Europe for €1.50/year + VAT, providing a unique method to sideload apps on iOS with a strong emphasis on security.",
      "The app offers detailed store pages for each app and enables users to self-publish their apps, catering to those seeking apps not found on the Apple App Store.",
      "Developed by a small team and reliant on donations, AltStore aims to offer users an alternative way to access iOS apps outside the official App Store ecosystem."
    ],
    "commentSummary": [
      "Apple's control over iOS app distribution raises concerns among users and developers regarding accessing third-party software.",
      "AltStore is the inaugural Apple-approved alternative App Store, inaccessible in some countries, causing frustration for users missing out on apps like Delta Emulator.",
      "Debates continue on Apple's rigid app distribution rules, developer fees, and the viability of alternative operating systems, alongside concerns about approval processes and the benefits of alternative app stores towards enhancing user experience."
    ],
    "points": 141,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1713640669
  },
  {
    "id": 40100867,
    "title": "Why Gamification Manipulates Human Behavior",
    "originLink": "https://www.gurwinder.blog/p/why-everything-is-becoming-a-game",
    "originBody": "Share this post Why Everything is Becoming a Game www.gurwinder.blog Copy link Facebook Email Note Other Discover more from The Prism A guide to navigating the Digital Age Over 32,000 subscribers Subscribe Continue reading Sign in Why Everything is Becoming a Game All the easier to control you Gurwinder Apr 20, 2024 419 Share this post Why Everything is Becoming a Game www.gurwinder.blog Copy link Facebook Email Note Other 87 Share I. The Happiness of Pursuit For years, some of the world’s sharpest minds have been quietly turning your life into a series of games. Not merely to amuse you, but because they realized that the easiest way to make you do what they want is to make it fun. To escape their control, you must understand the creeping phenomenon of gamification, and how it makes you act against your own interests. This is a story that encompasses a couple who replaced their real baby with a fake one, a statistician whose obsessions cost the US the Vietnam War, the apparent absence of extraterrestrial life, and the biggest FBI investigation of the 20th century. But it begins with a mild-mannered psychologist who studied pigeons at Harvard in the Thirties. B. F. Skinner believed environment determines behavior, and a person could therefore be controlled simply by controlling their environment. He began testing this theory, known as behaviorism, on pigeons. For his experiments, he developed the “Skinner box”, a birdcage with a food dispenser controlled by a button. Skinner’s goal was to make the pigeons peck the button as many times as possible. From his experiments, he made three discoveries. First, the pigeons pecked most when doing so yielded immediate, rather than delayed, rewards. Second, the pigeons pecked most when it rewarded them randomly, rather than every time. Skinner’s third discovery occurred when he noticed the pigeons continued to peck the button long after the food dispenser was empty, provided they could hear it click. He realized the pigeons had become conditioned to associate the click with the food, and now valued the click as a reward in itself. This led him to propose two kinds of reward: primary and conditioned reinforcers. A primary reinforcer is something we’re born to desire. A conditioned reinforcer is something we learn to desire, due to its association with a primary reinforcer. Skinner found that conditioned reinforcers were generally more effective in shaping behavior, because while our biological need for the primary reinforcer is easily satiable, our abstract desire for the conditioned reinforcer isn’t. The pigeons would stop seeking food once their bellies were full, but they’d take far longer to get tired of hearing the food dispenser click. Skinner’s three key insights — immediate rewards work better than delayed, unpredictable rewards work better than fixed, and conditioned rewards work better than primary — were found to also apply to humans, and in the 20th Century would be used by businesses to shape consumer behavior. From Frequent Flyer loyalty points to mystery toys in McDonalds Happy Meals, purchases were turned into games, spurring consumers to purchase more. Some people began to consider whether games could be used to make people do other things. In the Seventies, the American management consultant Charles Coonradt wondered why people work harder at games they pay to play than at work they’re paid to do. Like Skinner, Coonradt saw that a defining feature of compelling games was immediate rewards. Most of the feedback loops in employment — from salary payments to annual performance appraisals — were torturously long. So Coonradt proposed shortening them by introducing daily targets, points systems, and leaderboards. These conditioned reinforcers would transform work from a series of monthly slogs into daily status games, in which employees competed to fulfil the company’s goals. In the 21st century, advances in technology made it easy to add game mechanics to almost any activity, and a new term — “gamification” — became a buzzword in Silicon Valley. By 2008, business consultants were giving presentations about leveraging fun to shape behavior, while futurists gave TED Talks speculating on the social implications of a gamified world. Underpinning every speech was a single, momentous question: if gamification could make people buy more stuff and work more hours, what else could it be used to make people do? The tone was generally utopian, because back then gamification seemed to be mostly a force for good. In 2007, for instance, the online word quiz FreeRice gamified famine relief: for every correct answer, 10 grains of rice were given to the UN World Food Programme. Within six months it had already given away over 20 billion grains of rice. Meanwhile, the SaaS company, Opower, had gamified going green. It turned eco-friendliness into a contest, showing each person how much energy they were using compared with their neighbors, and displaying a leaderboard of the top 10 least wasteful. The app has since saved over $3 billion worth of energy. And then there was Foldit, a game developed by University of Washington biochemists who’d struggled for 15 years to discern the structure of an Aids virus protein. They reasoned that, if they turned the search into a game, someone might do what they couldn’t. It took gamers just 10 days. Even established corporations saw gamification’s potential. In 2008, Volkswagen debuted a campaign called “The Fun Theory”, based on the idea that “fun is the easiest way to change people’s behavior for the better”. Piano stairs were installed at a Stockholm rail station to encourage people to use them instead of the escalator, leading to a 66% increase in stair use. Volkswagen also tried to gamify gamification itself, creating a contest for good game ideas. The winning idea was a “speedcam lottery”, where people who kept to the speed limit would be entered into a prize draw, funded by speeding fines. It all seemed so simple: if we could only create the right games, we could make humanity fitter, greener, kinder, smarter. We could repopulate forests and even cure cancers simply by making it fun. Unfortunately, that didn’t happen. Instead, gamification took a less wholesome route. We humans are harder to manipulate than pigeons, but we can be manipulated in many more ways, because we have a wider spectrum of needs. Pigeons don’t care much about respect, but for us it’s a primary reinforcer, to such an extent that we can be made to desire arbitrary sounds that become associated with it, like praise and applause. Respect is so important to humans that it’s a key reason we evolved to play games. Will Storr, in his book The Status Game, charted the rise of game-playing in different cultures, and found that games have historically functioned to organize societies into hierarchies of competence, with score acting as a conditioned reinforcer of status. In other words, all games descend from status games. The association between score and status has grown so strong in our minds that, like pigeons pecking the button long after the food dispenser has stopped dispensing, we’ll chase scores long after everyone else has stopped watching. And so, when Facebook added “likes” in 2009, they quickly became a proxy for status, and a score to compete for. People now had a social stake in posting content. Hitting “send” became like activating a slot machine, initiating an excitingly uncertain outcome; the post might go completely unnoticed, or it might hit the jackpot and go viral, awarding the coveted prizes of respect and fame. Other social media platforms followed, leveraging Skinner’s three laws to maximize button-pecking. They offered immediate reinforcement in the form of instant responses, conditioned reinforcement in the form of “likes” and “followers”, and unpredictable reinforcement that varied with each post and each refresh of the page. These features turned social media into the world’s most addictive status game. And thus, just as pigeons were made to chase clicks, so eventually were we. But this was just the beginning. Many in the managerial class saw the success of social media and wondered how they could use gamification for their own ends. The Chinese Communist Party was among the first to apply the principles of social media to the real world. In several towns and cities, it began trialing social credit schemes that assign citizens a level of “clout” based on how well they behave. In some areas, such as Rongcheng and Hangzhou, there are public signs that display leaderboards of the highest scoring citizens. The lowest scoring citizens may be punished with credit blacklists or throttled internet speeds. Meanwhile, in the West, gamification is used to make people obey corporations. Employers like Amazon and Disneyland use electronic tracking to keep score of employees’ work rates, often displaying them for all to see. Those who place high on the leaderboards can win prizes like virtual pets; those who fall below the minimum rate may be financially penalized. Game features are even more pervasive in the digital world. In little over a year, the Chinese shopping app Temu has exploded in popularity thanks to its “play to pay” model: as users browse deals they’re presented with puzzles to solve, roulette wheels to spin, and challenges to complete, which reward them with credit and special offers. Unsurprisingly, users are now spending double the amount of time on Temu than on Amazon. Gamification has also transformed dating apps. Zoosk works like a typical role-playing game, where you gradually accumulate “experience points”, which unlock new abilities, such as animated virtual gifts to send to prospective dates. Meanwhile, on Tinder you can purchase various “level-ups” — Boosts, Super Likes, and Rewinds — that increase your chances of winning and compel you to keep playing to get your money’s worth. And if you have no luck on dating apps, there are always AI girlfriends to play with: apps like iGirl and Replika award users points for their commitment, which can be used to “level up” their virtual lovers into a version that is more intimate. These are only a few examples. Virtually every kind of app, from audiobook apps to taxicab apps to stock trading apps, now employs game mechanics like points, badges, levels, streaks, progress bars, and leaderboards. Their ubiquity attests to their success in hooking people. Gamification once promised to create a better society, but it’s now used mainly to addict people to apps. The gamifiers, like Skinner’s pigeons, prioritized immediate rewards over delayed ones, so they gamified for the next financial quarter and not for the future of civilization. So where does this all lead? What is the endgame? II. A Maze Called Utopia At the University of Michigan in the mid-twentieth century, there was a zoologist named James V. McConnell. A strong believer in fun, he often presented his academic research alongside satire and poetry, so it was difficult to tell which was which, a habit that made him popular with students but unpopular with his fellow professors. One of the few things McConnell took seriously was behaviorism. He was transfixed by Skinner’s work on pigeons, and wished to expand the work to humans, with an eye to creating a perfect society. In a 1970 Psychology Today article he wrote: We should reshape our society so that we all would be trained from birth to want to do what society wants us to do. We have the techniques now to do it. Only by using them can we hope to maximize human potentiality. In short, he wanted to turn society into a Skinner box. Throughout the Seventies, McConnell used Skinnerian techniques to create rehabilitation programs for prisoners and psychiatric patients, some of which were successful. But his most ambitious scheme emerged in the early Eighties, when he witnessed people being captivated by video games like Donkey Kong and Pac Man, and realized their addictive mechanics could be translated to other, more productive activities. He pitched an ambitious project to gamify education to tech companies like Microsoft and IBM, but he was 30 years too early, and they couldn’t yet see its promise. There was, however, one person who’d taken a keen interest in McConnell’s work. His name was Ted Kaczynski. Kaczynski was an awkward but gifted student, coldly matter-of-fact in manner, for which he was described by his schoolmates as a “walking brain”. In a school IQ test he’d scored 167 (140 is considered “genius”). He’d come to Michigan in 1962 as a postgraduate from Harvard, where he’d studied mathematics and graduated at just 18. But at Harvard, he’d also been subjected to torturous experiments. In a lab not far from where Skinner had once experimented with pigeons, psychologists linked to US intelligence were now experimenting with humans — one of whom was Kaczynski. Under the glare of blinding lights, he was methodically humiliated to see how he reacted. He claimed the experience didn’t affect him, and yet, within just a few years, he’d developed an intense paranoia about psychological conditioning. And so, when Kaczynski learned of McConnell’s proposals to create a utopia through behavior modification, he concluded that the jocular professor was an existential threat to humanity, and that he needed to die. It wasn’t a decision Kaczynski had made lightly; he’d developed an entire philosophy to justify it. Influenced by techno-dystopian writers like Aldous Huxley and Jacques Ellul, Kaczynski believed the Industrial Revolution had turned society into a cold process of production and consumption that was gradually crushing everything humans valued most: freedom, happiness, purpose, meaning, and the ecosystem. In his view, everything society now produced—including science and technology—served industry, not humanity, and thus was increasingly being purposed not to enrich our lives but to psychologically condition us so we wouldn’t resist what was being done to us and to the earth. In short, where society had once been shaped to accommodate people, now people were being shaped to accommodate society. And this misshaping was destructive because it conflicted with our deepest nature. Kaczynski believed modern society made us docile and miserable by depriving us of fulfilling challenges and eroding our sense of purpose. The brain evolved to solve problems, but the problems it had evolved for were now largely solved by technology. Most of us can now obtain all our basic necessities simply by being obedient, like a pigeon pecking a button. Kaczynski argued that such conveniences didn’t make us happy, only aimless. And to stave of this aimlessness, we had to continually set ourselves goals purely to have goals to pursue, which Kaczynski called “surrogate activities”. These included sports, hobbies, or chasing the latest product that ads promised would make us happy. For Kaczynski, the result of reorienting our lives to chase artificial goals was that we became increasingly dependent on society to provide us with them. And without our own inherent sense of purpose, we’d inevitably be made to chase goals that were good for the industrial machine but bad for us. Kaczynski’s theories eerily prophesize the capture of society by gamification. While he overlooked the benefits of technology, he diligently noted its dangers, recognizing its role in depriving us of purpose and meaning. Today the evidence is everywhere: religion is dying out, Western nations are culturally confused, people are getting married less and having fewer children, and many are losing their jobs to automation, so the traditional pillars of life — God, nation, family, and work — are crumbling, and people are losing their value systems. Amid such uncertainty, games, with their well-defined rules and goals, provide a semblance of order and purpose that may otherwise be lacking in people’s lives. Gamification is thus no accident, but an attempt to plug a widening hole in society. Unfortunately, it seems to be only a band-aid. Kaczynski observed that surrogate activities rarely kept people contented for long. There were always more stamps to collect, a better car to buy, a higher score to achieve. He believed artificial goals were too divorced from our actual needs to truly satisfy us, so they merely served to keep us busy enough not to notice our dissatisfaction. Instead of a fulfilled life, a life filled full. Today, people increasingly live inside their phones, bossed around by notifications, diligently collecting badges and filling progress bars, even though it doesn’t make them happy. On the contrary, substantial research comprising over a hundred studies finds that prioritizing extrinsic goals over intrinsic goals — in other words doing things to win prizes and achieve high scores rather than for the inherent love of doing them — leads to lower well-being. Kaczynski seemed to recognize this long before smartphones emerged. He felt that building a life around chasing what was offered on billboards and in magazines wouldn’t make him happy, and would only feed the Machine, so in 1971 he fled society, holing himself up in a log cabin in the Montana wilderness. There he attempted a simple and self-sufficient life, enjoying the small things like the sound of birds singing and the feeling of sunrays on his back. But this idyll wouldn’t last. He claims that while hiking across one of his favorite spots — a rocky ridge with a waterfall — he was aghast to find a road had been built through it. As he saw it, industrialization, like some fungus creeping across the world, had followed him even here. Enraged, he decided modernity couldn’t be escaped, and had to be destroyed. His emotional instability got the better of him, and in 1978 he began posting homemade bombs to those he accused of betraying humanity. In 1985, a package arrived at McConnell’s home. It was opened by his assistant, Nicklaus Suino. The package only partially exploded, injuring Suino and McConnell, and leaving them both shaken for life. They were lucky. Less than a month later, Kaczynski would send another, more carefully prepared bomb to computer store owner, Hugh Scrutton, who’d become Kaczynski’s first murder victim. By then, the FBI’s investigation into the bombings had grown into the largest in its history. For over a decade they scoured the country as Kaczynski continued to kill and injure, but much of their time was wasted chasing mirages, for Kaczynski would often scatter his bomb parcels with red herrings such as notes referencing fictitious conspiracies and signed with made-up initials. Kaczynski’s actions, though unforgivable, can teach us as much about gamification as his philosophy. His red herrings lured people away from what they actually sought, and, as we shall see, this is the greatest danger of gamification. III. When Red Herrings Become White Whales While Kaczynski wanted to demolish industrial society and return humanity to an agrarian life, US defense secretary Robert McNamara wanted the opposite: to use American industrial might to crush the agrarian society of Vietnam. McNamara was a statistician who believed what couldn’t be measured didn’t matter. He charted progress in the Vietnam war by body count, because it was simple to measure. It was his way of keeping score. But his focus on what could be easily measured led him to overlook what couldn’t: negative public opinion of the US Army both at home and in Vietnam, which deflated US morale while boosting enemy conscription. In the end, the US was forced to withdraw from the war, despite winning the battle of bodies, because it had lost the battle of hearts and minds. Thus, the McNamara fallacy, as it came to be known, refers to our tendency to focus on the most quantifiable measures, even if doing so leads us from our actual goals. Put simply, we try to measure what we value, but end up valuing what we measure. And what we measure is rarely what we mean to value. As Skinner showed, the goals of games — points, badges, trophies — are secondary reinforcers that only derive their worth due to their association with something we actually desire. But these associations are often illusory. A click is not the same thing as a food pellet. And points are not the same as progress. We’re easily motivated by points and scores because they’re easy to track and enjoyable to accrue. As such, scorekeeping is, for many, becoming the new foundation of their lives. “Looksmaxxing” is a new trend of gamified beauty, where people assign scores to physical appearance and then use any means necessary to maximize their score. And in the online wellness space, there is now a “Rejuvenation Olympics” complete with a leaderboard that ranks people by their “age reversal”. Even sleep has become a game; many people now use apps like Pokemon Sleep that reward them for achieving high “sleep scores”, and some even compete to get the highest “sleep ranking”. Most such scores are simplifications that don’t tell the whole story. For instance, sleep trackers only measure what’s easy to measure, like movement, which says nothing about crucial facts like time spent in REM sleep. A more accurate measure of how well you slept would be how refreshed you feel in the morning, but since this can’t be quantified, it tends to be ignored. Further, if increasing one’s youthfulness score requires a daily 2-hour skincare routine, a diet of 50 pills each morning and night, abstention from many of life’s pleasures, and constant fixation on one’s vital metrics, is it really worth it? Of what value is adding a few years to your life if the cost is a life worth living? The scores we use to chart progress can’t articulate the nuances of reality, and yet we often tie our life goals and even self-worth to such arbitrary numbers. In the end, even Kaczynski, with his IQ of 167, was led astray by red herring goals. In 1995 he enacted his endgame, demanding the New York Times and Washington Post print his anti-technology manifesto to prevent further bloodshed. All along, his goal had been to get the widest possible newspaper coverage, to maximize how many people would see his manifesto, but like McNamara he didn’t account for what couldn’t be quantified, such as how people would see his manifesto. Skinner’s pigeons had learned to desire the click of the food dispenser because it had been accompanied by food, and Kaczynski’s intended audience learned to hate his arguments because they’d been accompanied by violence. By maximizing audience size at the expense of everything else, Kaczynski gained a massive audience unwilling to give him a fair hearing. Further, his manifesto contained a peculiar choice of words (“eat your cake and have it”), which was recognized by his brother, David, who alerted the police, leading to Kaczynski’s capture. And so, by fixating on the most obvious metric — the size of his audience — Kaczynski lost the one thing he’d been fighting for all along: freedom. Kaczynski played the wrong game, and was trapped by it. Today, we all face similar traps. We chase numbers and icons because they’re always available, and the chase is often so immersive that it keeps us from seeing where it leads, which is often far away from what we actually want. This can lead to what the evolutionary psychologist Diana Fleischman calls “counterfeit fitness”: the constant, momentary “wins” that come with playing digital games give us a false sense of progression and accomplishment, a neurochemical high that feels like victory but is not, and which, if it becomes a habit, risks placating our ambitions to pursue true fulfilment. It explains why so many young men have lost themselves in video games, and are no longer in employment or relationships. The false signals they’re getting from video game progress, combined with the sexual reward of online porn, are convincing their dopamine pathways that they’re winning in life, even as their minds and futures atrophy. It’s easy to persuade people into tying their sense of progress to fake or trivial goals. Casinos keep their customers happily losing money by distracting them with minor side games they’re likely to win. The small victories convince them they’re winning overall even as they lose the only games that actually matter. This strange quirk of human behavior can even cost lives. In South Korea, a young couple became so addicted to raising a virtual baby that they let their real baby starve to death. The parents prioritized what they could quantify — levelling up their virtual baby — over that which they couldn’t — the life of their real one. What makes pathological gameplaying so dangerous is that the more harm it does, the more alluring it becomes. If your baby is dead, why not raise a virtual one? If your life of playing video games has stopped you finding a girlfriend, why not play the AI girlfriend game? Thus, bad games form a feedback loop: they distract us from pursuing the things that will bring us lasting contentment, and without this lasting contentment, we become ever more dependent on false, transient metrics like scores and leaderboards to imbue our lives with meaning. All the things a gamified world promises in the short term — pride, purpose, meaning, control, motivation, and happiness — it threatens in the long term. It has the power to seclude people from reality, and to rewrite their value systems so they prioritize the imaginary over the real, and the next moment over the rest of their lives. So what’s the solution? IV. Playing for Keeps There are billions of habitable planets in our galaxy, and many of them are far older than our own. Statistically, this would suggest that by now our galaxy would be teeming with signs of advanced alien life. And yet space is silent. This discrepancy, known as the Fermi paradox, has puzzled scientists for almost a century. Ted Kaczynski believed his prophecies offered an answer. While serving a life sentence in jail, Kaczynski wrote a little-known sequel to his manifesto, entitled “Anti-Tech Revolution: Why and How”. In it he outlines his belief that all technologically advanced civilizations become trapped in fatal games before they learn to colonize space. This happens because industry is driven by competition, and competition favors short-term wins over long-term sustainability, because players who care about long-term sustainability are significantly disadvantaged compared to players who only care about winning. To illustrate his point, Kaczynski describes a thought experiment involving a forested region occupied by several rival kingdoms. The kingdoms that clear the most land for agriculture can support a larger population, affording them a military advantage. Every kingdom must therefore clear as much forest as possible, or face being conquered by its rivals. The resulting deforestation eventually leads to ecological disaster and the collapse of all the kingdoms. Thus, a trait that is advantageous for every kingdom’s short-term survival leads in the long term to every kingdom’s demise. Kaczynski was describing a “social trap”, a term coined by a student of Skinner, John Platt, who’d theorized that an entire population behaving like pigeons in a Skinner box, each acting only for the next immediate reward, would eventually overexploit a resource, causing ruin for everyone. What Platt called “social traps”, Kaczynski called “self-propagating systems”, because he viewed them as negative-sum games that took on a life of their own, defeating every player to become the only winner. He believed such games not only drove industrialization but also replaced the sense of purpose and meaning that industrialization destroyed. They were thus inextricable from technological advancement, and, in a society like ours, impossible to stop. In jail, Kaczynski was forbidden access to the web, and in letters he struggled to understand what Facebook was. Nevertheless, his warnings could easily have been referring to social media. On Instagram, the main self-propagating system is a beauty pageant. Young women compete to be as pretty as possible, going to increasingly extreme lengths: makeup, filters, fillers, surgery. The result is that all women begin to feel ugly, online and off. On TikTok and YouTube, there is another self-propagating system where pranksters compete to outdo each other in outrageousness to avoid being buried by the algorithm. Such extreme brinkmanship frequently leads to arrest or injury, and has even led to the deaths of, among others, Timothy Wilks and Pedro Ruiz. On X, meanwhile, there is a self-propagating system known as “the culture war”. This game consists of trying to score points (likes and retweets) by attacking the enemy political tribe. Unlike in a regular war, the combatants can’t kill each other, only make each other angrier, so little is ever achieved, except that all players become stressed by constant bickering. And yet they persist in bickering, if only because their opponents do, in an endless state of mutually assured distraction. Those are just three examples of social traps that have emerged in our gamified age. But the most worrying social trap is gamification itself. Companies that exploit our gameplaying compulsion will have an edge over those who don’t, so every company that wishes to compete must gamify in ever more addictive ways, even though in the long term this harms everyone. As such, gamification is not just a fad; it’s the fate of a digital capitalist society. Anything that can be turned into a game sooner or later will be. And the games won’t just be confined to our phones — “extended reality” eyewear like Meta Quest and Apple Vision, once they become normalized, will make playing even harder to avoid. Games will be created not just to extract money from people, but also data. The 2025 Enhanced Games, for instance, is a new futuristic version of the Olympics, funded by tech moguls like Peter Thiel, where contestants can exploit anything from cybernetic implants to PEDs to get a competitive advantage. The purpose of the games seems to be transhumanist: to motivate people to discover new ways to augment human abilities, with the eventual goal of turning men into gods. There is, after all, a vacancy in heaven. When God is dead, and nations are atomized, and family seems burdensome, and machines can beat us at our jobs and even at art, and trust and truth are lost in a roiling sea of AI-generated clickbait — what is left but games? This isn’t necessarily a bad thing. Games can motivate us to destroy ourselves, but they can also motivate us to better ourselves. In a gamified world, it’s possible to play without getting played, if one only chooses the right games. As the poker-player-turned-podcaster Liv Boeree said: “Intelligence is knowing how to win the game. Wisdom is knowing which game to play…” So how do you decide which games to play? The story of gamification offers five broad rules. First: choose long-term goals over short-term ones. Short, frequent feedback loops offer regular reinforcement, which helps motivate us. But what is made to motivate us too often addicts us. So consider the long-term outcomes of the games you’re playing: if you did the same thing you did today for the next 10 years, where would you be? Play games the 90-year-old you would be proud of having played. They won’t care how many trophies you have; they will care how many times you saw your parents before they died. Second: choose hard games over easy ones. Since the long-term value of games lies in their ability to hone skills and build character, easy games are usually a trap. People with unearned wealth — thieves, heirs and lottery winners — often end up losing it all, because the struggle to obtain a reward teaches us the reward’s worth, and is thus a crucial part of the reward. Third: choose positive-sum games over zero-sum or negative-sum ones. Games evolved to confer status, and status is zero-sum — for some to have it, others must lose it. But we no longer have to play such games; we can change the rules so a win for me doesn’t mean a loss for you. Educational games are one example. Wealth creation is another. Positive-sum games — where every player benefits by playing — are a form of competition that brings people together instead of driving them apart. Fourth: choose atelic games over telic ones. Atelic games are those you play because you enjoy them. Telic games are those you play only to obtain a reward. Chasing rewards like trophies and leaderboard rankings can help drive us to succeed, but a fixation on such rewards can become a source of stress, and can even make leisure activities feel like drudgery, turning games into work. Finally, the fifth rule is to choose immeasurable rewards over measurable ones. Seeing numerical scores increase is satisfying in the short term, but the most valuable things in life — freedom, meaning, love — can’t be quantified. There are an overwhelming number of games to choose from. If you want to keep fit, try Zombies Run, an app that takes the form of a post-zombie-apocalypse radio broadcast telling you which direction to run to avoid being eaten. If you want to learn general knowledge while helping those in poverty, play the FreeRice quiz. And if you want to form good habits, there’s Habitshare, which lets your friends track your attempts, motivating you more than if you were only accountable to yourself. But if, among the countless games out there, you can’t find one right for you, then you can just create your own. Fun is not the pursuit of happiness, but the happiness of pursuit, and literally anything can be pursued. By now there’s a way to keep any kind of score and play any kind of game. Kaczynski’s game is over; he committed suicide last summer, still adamant humanity was doomed. His fearful legacy has since passed to his disciples, like Liverpool man Jacob Graham, who was recently jailed for terrorism after trying to emulate his idol. Graham may have thought he was saving the world, but, with all his talk of maximizing kill counts, he too was just playing a bad game. In the end, Kaczynski and his followers made the same mistake as Skinner: they viewed us as mere puppets of our environment, devoid of agency and the ability to adapt. They needn’t have feared the world becoming a Skinner box, because, among all the papers written about that troublesome contraption, one fact is always omitted: Skinner’s pigeons only kept pecking the button because they were in a cage, with nothing else to do. But you are still free. Even in a world where everything is a game, you don’t have to play by other people’s rules; you have a wide open world to create your own. Your move. Subscribe 419 Share this post Why Everything is Becoming a Game www.gurwinder.blog Copy link Facebook Email Note Other 87 Share",
    "commentLink": "https://news.ycombinator.com/item?id=40100867",
    "commentBody": "Why everything is becoming a game (gurwinder.blog)138 points by jger15 13 hours agohidepastfavorite52 comments adrianhon 11 hours agoInteresting piece! I’m co-creator of Zombies, Run! so I’m glad to see it mentioned here - we designed it to be in the best interests of players, which is why it doesn’t feature streaks or leaderboards or other ways to manipulate you into overexercising or playing more than you want to. That said, I’m more sanguine about gamification than the author. There are indeed many games to choose from, but the ones that are most concerning that those we have little choice but to play, whether they’re from our employers or in our schools and colleges, or built into devices and platforms like the Apple Watch and iOS. If you’re interested in this subject, I wrote a book critiquing gamification called “You’ve Been Played” - the NYT called it illuminating and persuasive! reply iamacyborg 9 hours agoparentHey Adrian, I read your book a couple weeks ago and thought it was great! reply _boffin_ 8 hours agoparentprevQuestion: what / how were the were the discussions on to not include leaderboards, streaks, etc? What what was the primary motivation behind the game? From your comment, it seems play to play rather play to win? reply SV_BubbleTime 5 hours agorootparentI can’t answer for OP and I hope he does. I can say I’m designing a product at my company and have our team a set of golden rules for it. Among them is we will respect the user. From being fair on pricing to data collected. If I get a hint of someone not in line with that, they would be off the team, no debate. We’re lucky that the thing we’re building is not our core business, and it only has to be successful, not MBA-milk-everything-you-can-all-the-time successful. For me, I have the power to make this a reality. For someone that doesn’t, do what you can. reply khrbrt 9 hours agoprev> Most of the feedback loops in employment — from salary payments to annual performance appraisals — were torturously long. So Coonradt proposed shortening them by introducing daily targets, points systems, and leaderboards. These conditioned reinforcers would transform work from a series of monthly slogs into daily status games, in which employees competed to fulfil the company’s goals. My first thought was that working for such a company would be torture. My second thought was that this basically describes Agile/Scrumm and has taken over the entire industry. reply slowmovintarget 6 hours agoparentI like Rich Hickey's description of Scrum: \"We've learned the secret to tricking developers into constantly running races... You just fire the starting pistol over and over, right?\" reply zdragnar 8 hours agoparentprevIt really depends on the nature of your work. Creative work that goes in stages would be frustrating, since your day to day is never the same. On the other hand, if your work is a bit more repetitive, it can be a nice way to actually see your contribution change. On the other other hand, there's the obvious drawback of competition undermining coordination and cooperation among team members, making the experience more toxic, or sacrificing quality for quantity. Short feedback cycles are good. An employee shouldn't wait 3 or 12 months to find out that they aren't doing something right- it's bad for them and for the company. It isn't a silver bullet, though, and there are better ways to get there. reply jayd16 6 hours agoparentprevDoes anyone actually run things like that? Where the goal of work is to earn arbitrary points? Sure there are story 'points' for describing complexity in an int and daily check-ins but I thought counting tickets was up there with lines of code produced? reply Guthur 6 hours agorootparentIt might be up there but it's all that an overly analytical and literal population can get their heads around. They literally can't think of any other way of making decisions. If they don't stick numbers on things they become essentially blind to it. reply Spivak 5 hours agorootparentAre there other ways? Like sure there's other systems that assign numbers differently and have different ceremonies but is there any way that works that doesn't involve assigning numbers to stuff? The basic premise of people being simultaneously really good at estimating work and at the same time absolutely garbage at it when you make them use real dates naturally evolves a number system. Make up an arbitrary unit, have people estimate tasks in that arbitrary unit, and behind the scenes without telling them determine the conversion factor for that person so you can plan accordingly. Everything else is just flavor around the core gameplay loop. reply nonsensikal 8 hours agoparentprevI don't think clear metrics to optimize would be bad. I mostly find work to be entirely opaque in what it wants from me. The only time I know clearly is when the deadline looms. But the majority of the time everyone around is me cosplaying productivity. reply mdbauman 11 hours agoprevIt seems weird to me that the article lists several gamified apps without mentioning advertising. It seems obvious to me that gamified apps like Duolingo are incentivesed to keep eyeballs glued to screens mostly because advertisers pay per view, and strange not to mention this as a reason why we see so much of it in this space. Maybe the author thought it was too obvious to mention. reply virtuscience 10 hours agoparentAdvertising makes up only 9% of Duolingo's revenue and is also usually a small part of the revenue of most mobile games. Mobile games rely on \"whales\" (people who pay a lot, like a casino) and Duolingo makes almost all of its revenue on subscriptions. Both require sticky retention but advertisers don't drive the business model or executive decisions at these companies. reply mitjam 5 hours agoprevI would add a sixth rule: Gamify walking away from games or at least randomly walk away from them. Maybe it‘s time for me to try using a feature phone and a dumb watch. reply cod3rboy 2 hours agoprev> This strange quirk of human behavior can even cost lives. In South Korea, a young couple became so addicted to raising a virtual baby that they let their real baby starve to death. The parents prioritized what they could quantify — levelling up their virtual baby — over that which they couldn’t — the life of their real one. This is so sad. Such incident indeed confirms technology is pursuing people into forgetting real meaning and values of life. I am worried about the generation which has born in this gamification era. They need to be repeatedly taught about life meaning and values to differentiate between what's real and what's imaginary. reply zubairq 3 hours agoprevReally good article! I guess that the key is to turn things that I hate to do into a game! Also, is this also why people go through schools and university because of the system has grades as a scoring system and the \"promise\" of a good career and life at the end of it? reply wuj 8 hours agoprevI think conditioned reinforcers can actually be beneficial for pursing long-term goals. Let's say someone's purpose in life is to have a positive impact on the world, metrics such as audience reached provide a tangible interface for this abstract goal. Similar to using your credit card, gamification mechanisms like chasing metrics lets you experience the rewards of your efforts prematurely, which motivates you to work harder to achieve more. And this whole loop can be part of a positive-sum game too. Metrics is a straightforward way to demonstrate how you can achieve your sense of purpose. When you share your strategy on maximizing those metrics, you incentivize and guide others to start their own journeys. If what you did inspire even one person to start acting, you've already added value to the system. reply kazinator 2 hours agoprevThe article title promises to reveal how everything is becoming a game. Spoiler alert: turns out that is just clickbait. It's not everything. Just ... you know, apps. Mostly multi-user, social ones. Maybe the author really has his head so far up his apps that they look like everything. reply justanotherjoe 5 hours agoprevthis article is all over the place. At one point it even starts admonishing actual video games, with the usual granny complaints. The article sounds like alex jones style of piecing things superficially together. The worship of ted kaczynski is also bothering, since his views are so banal and not even accurate. (technology is making human a creature of society? not really. We always was. Ask jesus what happens when you go against the grain). reply the_common_man 1 hour agoparentHe didn't worship ted kaczynski by any means. Terrible low effort comment reply SV_BubbleTime 5 hours agoparentprevAssassin’s Creed. Go play the latest ones and tell me that Ubisoft doesn’t have a team devoted to “retention”. It’s game on top of game on top of game. The main missions are spaced out in a way that they are not too close or too far apart. The “collectables” (an insult to the word) are spaced between “oh look at that” and “wtf, this region isn’t 100% yet, where the hell is that thing!?”, achievements are an absolute abuse of psychology to keep you playing long after the game stopped being fun, the loading screens, the “quick” travel, the “crafting” and component systems that have you doing the least fun repetitive tasks usually involving the decimation of wild animals. The grind is out of control. Seriously, go play Outer Wilds, then play an AC or other grindfest. The only non-abusive games that are left are indies. It isn’t Alex Jonesy to rip apart modern video games for their exploitive tactics. reply djtango 3 hours agorootparentOn other gaming boards \"Ubisoft\" has become a catch all phrase for describing when said game has descended into hollow unfun busywork to pad out the game / ensnare players in that hamster wheel. Most recently FFVII Rebirth's critics have described the game as going \"full Ubisoft\" which definitely resonates. I've found that while I am very susceptible to that loop of chasing loot etc a game sticks when it's mechanically fun (YMMV) which is why Monster Hunter and Souls/Elden Ring are my kryptonite. Ditto for Destiny which had quite fun gunplay reply justanotherjoe 4 hours agorootparentprevI actually think 'playtime' is a great metric for game developers to optimize for. It incentives a lot of good design decisions. But like you said, also incentives a lot of bad ones, especially if your decisions are made by suits, like Ubisoft, who doesn't have any respect or faith in people. reply api 11 hours agoprevI felt like the stuff about Kaczynski, while interesting, was a diversion. The meat is the beginning and the end: Skinner, how conditioning led to gamification which led to the addiction economy, and then strategies for turning the tables and take charge by choosing what games to play and how to play them. An evolved and condensed version of the closing with practical examples and practice exercises should be taught in schools. reply ta8645 10 hours agoparent> which led to the addiction economy... If people had fulfilling lives, consumed by work, family, community, and culture, there'd be little room for such addiction. The real story is a loss of meaning; it's a crisis of nihilism, more than gamification. reply api 9 hours agorootparentAll these attention traps aren’t the only cause but they don’t help. They suck up a lot of time and time is precious. At the very least once people get into them it makes it less likely that they will do… anything else. Some of the stories I hear about younger peoples’ addiction to Instagram and TikTok are incredible. We’re talking many hours per day. It’s easily worse than TV especially since the latter is frequently a social experience. People like to watch shows together. Social media, ironically, is usually consumed alone, making it possibly less social than TV. reply j_bum 9 hours agoparentprevI agree. I stopped at the Kaczynski worship. I don’t understand why people are so fascinated with talking about the ideals of insane serial killers. There are plenty of other highly intelligent individuals with the same ideologies to discuss. Why glorify a murderer? reply the_common_man 57 minutes agorootparentThe blog doesn't condone the bombings or the violence. It talks about Kaczynski writings. For you, this is equivalent to glorifying a murderer. Wow, this is some poor comprehension. I listen to all sorts of music and I love them. More often than not the artists are degenerates and drug addicts. I hate it but I can still enjoy the genius of their music. For example, Nirvana. Kurt committed suicide but his music is genius. Does not mean i gloirfy suicide or worship Kurt. reply api 9 hours agorootparentprevI read Kaczynski long ago and saw a huge hole immediately. So surrogate activities replace the authentic struggle for survival. I can get that. But why is the struggle for survival better? Isn’t it just another game? Kaczynski like many other romantics rails against the system, but isn’t nature just another system? It’s an older one that we didn’t design, but didn’t we learn in the end that the matrix was inside an even older matrix which was inside an even older one…? What would it even mean to escape the “system?” How can you do that except death? If you are breathing you are playing some kind of game. Instead the question is: can we exercise some choice over what systems we give our energy to and can we influence these systems? I do think we give our energy to a lot of dumb pointless or even evil systems today, so how do we turn our attention elsewhere? For the natural system of subsistence hunting and gathering or farming the answers to these questions are “no” (little choice, play or die) and “no” (the system is billions of years old and isn’t even ours). We have more choice today in our complicated mesh of systems, or at least we have the potential of choice. This is ultimately a big part of why I am not a primitivist, reactionary, or traditionalist. Sure what we have sucks sometimes. Are we sure it was better back then? Or was it just different? I always want to ask “trads” of various types if they are sure they would be happy in the traditional state they imagine. Maybe the people who railed against nature and sought to command it with science to escape its constraints were malcontents not entirely different from Kaczynski in their emotional and personality structure. Send Ted back to 1400 and you might have an enlightenment radical materialist. reply bm3719 8 hours agorootparentIt's been many years since I last read it, but I seem to recall Kaczynski defined surrogate activities as those beyond what one would feel substantively deprived should they be without. He gives the example of pursuit of social fulfillment like romantic affection to be not a surrogate activity, since we're programmed to feel deprived without any at all, but being a sex addict to be one. If you combine this with his notion of the power process, surrogate activities ultimately unsatisfying in the context of that. Modern man lacks the ability to fulfill the power process, and surrogate activities is the result. Kaczynski draws arrows from the fact that that man lacks autonomy and fulfillment of the power process, to surrogate activities, then to various societal problems (of which he enumerates many). So, the point isn't that surrogate activities is the terrible end state to be avoided at all costs, it's what results when they're load-bearing at a societal level. reply iforiq 3 hours agorootparentprev>> What would it even mean to escape the “system?” How can you do that except death? If you are breathing you are playing some kind of game. I think yogic/buddhist enlightenment or nirvana is freedom from constraints due to nature.. the solution (as far as I understand) is essentially a state like death or physical non-existence but somehow still fully conscious and absolutely blissful reply the_common_man 1 hour agoprevBrilliant writing. reply lupire 9 hours agoprevhttps://en.m.wikipedia.org/wiki/Gamification has citations for that word and trend as early as 2013. reply actionfromafar 9 hours agoparentIt was a big thing even before that. reply ukuina 4 hours agorootparentThe Xbox 360 with Gamerscore gained through Achievements launched in 2005. Lilely the first globally-synchronized and overt system designed for gamification. reply throwaway918274 7 hours agoprevOur CEO was musing about gamifying our scheduling software to get more \"engagement\" - aka. people accepting more open shifts, aka. working more overtime. It was so disgusting to me I nearly quit over the suggestion. reply SunlitCat 5 hours agoparentJust wait, next you have to buy your daily work tasks as a dlc, being at work itself is f2p tho. :) reply gverrilla 10 hours agoprevGotta violate the magic circle. Or establish a new one. reply spywaregorilla 10 hours agoprevI don't agree with the positive feedback loop being THE thing about games. It's the fact that games offer a sense of progress that life does not. The skinner box implies animalistic pleasure associations that we just want to keep hitting. The progress wanting is simply desiring structure and context to your effort. They're similar but not the same. reply smackeyacky 10 hours agoparentI don't think life has no sense of progress. I've been fiddling with motor vehicles on and off over my lifetime (mid fifties now) and progressed from simple tune ups to now re-building engines. It took a long time to get the \"feel\" of what's right and one outright failure. But I can see the progress I've made over that relatively long space of time in that I understand much better what I'm doing and why, what is OK and what needs replacing. This progression has always been confined to the vehicles I could afford (and sometimes by necessity for that reason) but it's still there. If it was a video game the progression would be marked by the prestige of the vehicle but it isn't a game. I can see progress in my work. I'm a much better programmer now then when I was 20. I write a lot less code, it's more readable and structured in a way that makes sense rather than just works. I guess these things are elaborate Skinner boxes in themselves, but there is no reason you can't find progression and meaning at a meta level even though you have a boring job and can't afford to play with Ferraris. It's just that you have to find it for yourself. Nobody is going to be able to do that for you and it takes a conscious effort rather than just expecting it to happen. reply 082349872349872 3 hours agorootparent> If it was a video game the progression would be marked by the prestige of the vehicle but it isn't a game. ca. 300 BC, Aristippus (returning from a dinner party at the palace) passes a student of Diogenes eating lentils: — You know, if you'd just learn to say what the King wants to hear, you wouldn't have to eat lentils. — How about: if you'd just learn to eat lentils, you wouldn't have to say what the King wants to hear? Lagniappe: https://www.youtube.com/watch?v=M8SVYDvMVzY EDIT: unfortunately (unlike the Alexander/Diogenes story) there's insufficient overlap for Aristippus/Diogenes to be plausible; added \"a student of\". reply taway789aaa6 9 hours agorootparentprevI feel similarly with gardening. Try growing a food forest -- it expands your mind's time horizon to try picturing your six inch tall hazelnut seedling as a 20 foot tall tree. Each year is a new feeling of progress as everything starts to bear fruit! reply smackeyacky 8 hours agorootparentI love this idea but I hate gardening. It just seems like such a grind to keep the weeds down and whatnot. I do forget that planting things is like an investment in your own satisfaction. reply 082349872349872 10 hours agorootparentprev> Nobody is going to be able to [find progression and meaning] for you and it takes a conscious effort rather than just expecting it to happen. I believe you've just given the tl;dr for Existentialism. reply smackeyacky 10 hours agorootparentHa ha - imagine that the secret of true happiness is something like noticing you got better at something you enjoy. It can't be that simple, right? reply spywaregorilla 10 hours agorootparentprevThat's fair. I should have said \"little\" sense of progress. Practice and mastery is certainly a thing you can observe progress on. But the things people are gamifying tend not to be those things. reply smackeyacky 10 hours agorootparentI didn't mean my comment to come across as a criticism (my apologies), just an observation. I would agree that gamification seems to have confined itself to immediate and facile activities. reply Rury 9 hours agoparentprevI think it's really about some of our biological processes which helps keep us alive. I mean, our brains are hardwired to try and keep us alive. Seeking out old information you already know is not beneficial to improving your survival. New, particularly beneficial information can be. Thus, in order to help us survive, our brains evolved a reward system to motivate us to seek new information... to learn essentially. In other words, our brains motivate us to learn, as learning helps us to survive. Thus, novelty = fun and repetition = boring. But the brain is obviously imperfect. Games and many addicting things in modern life, can hijack and play on this particular process (even harmful things). reply richrichie 8 hours agoprev>First: choose long-term goals over short-term ones. >Second: choose hard games over easy ones. >Third: choose positive-sum games over zero-sum or negative-sum ones. >Fourth: choose atelic games over telic ones. >Finally, the fifth rule is to choose immeasurable rewards over measurable ones. I am afraid these are as effective as telling obese people not to overeat. These are situations where people cannot help themselves and that's precisely why gamification works. reply hypertexthero 9 hours agoprevBecause games are fun! reply callwhendone 11 hours agoprev [–] It always was. reply lambdaxyzw 10 hours agoparent [–] It's a witty response, and I think on some level even agree with you. But it's also a pretty shallow think to say. It doesn't adress the point of the article and it doesn't say \"why\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Gamification employs game mechanics to influence behavior through immediate rewards, originated from psychologist B.F. Skinner's work and adopted by businesses to shape actions.",
      "Initially targeting positive behaviors, gamification has transformed into a potent tool for steering human behavior, sparking concerns about addiction and reliance on artificial objectives.",
      "Prioritizing short-term rewards, as highlighted through cases like Ted Kaczynski and US defense secretary Robert McNamara, may jeopardize long-term fulfillment, stressing the significance of engaging in challenging, enjoyable activities or paving a unique path free from external manipulation."
    ],
    "commentSummary": [
      "The article delves into the trend of gamification across work, exercise, and education, underscoring its effects on motivation, productivity, and societal matters.",
      "It discusses both the advantages and disadvantages of integrating game elements into various aspects of life, highlighting potential drawbacks and the importance of deriving meaning from daily tasks.",
      "Emphasizing how games can leverage our brain's reward system to encourage learning and curiosity, the discussion reinforces the significance of finding purpose and progression in everyday endeavors."
    ],
    "points": 138,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1713645883
  }
]
