[
  {
    "id": 40310896,
    "title": "Revisiting TCP_NODELAY in Modern Distributed Systems",
    "originLink": "https://brooker.co.za/blog/2024/05/09/nagle.html",
    "originBody": "Marc's Blog About Me My name is Marc Brooker. I've been writing code, reading code, and living vicariously through computers for as long as I can remember. I like to build things that work. I also dabble in machining, welding, cooking and skiing. I'm currently an engineer at Amazon Web Services (AWS) in Seattle, where I work on databases, serverless, and serverless databases. Before that, I worked on EC2 and EBS. All opinions are my own. Links My Publications and Videos @marcbrooker on Mastodon @MarcJBrooker on Twitter It’s always TCP_NODELAY. Every damn time. It's not the 1980s anymore, thankfully. The first thing I check when debugging latency issues in distributed systems is whether TCP_NODELAY is enabled. And it’s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded. First, let’s be clear about what we’re talking about. There’s no better source than John Nagle’s RFC896 from 19841. First, the problem statement: There is a special problem associated with small packets. When TCP is used for the transmission of single-character messages originating at a keyboard, the typical result is that 41 byte packets (one byte of data, 40 bytes of header) are transmitted for each byte of useful data. This 4000% overhead is annoying but tolerable on lightly loaded networks. In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many write calls. Nagle’s proposal for fixing this was simple and smart: A simple and elegant solution has been discovered. The solution is to inhibit the sending of new TCP segments when new outgoing data arrives from the user if any previously transmitted data on the connection remains unacknowledged. When many people talk about Nagle’s algorithm, they talk about timers, but RFC896 doesn’t use any kind of timer other than the round-trip time on the network. Nagle’s Algorithm and Delayed Acks Nagle’s nice, clean, proposal interacted poorly with another TCP feature: delayed ACK. The idea behind delayed ACK is to delay sending the acknowledgement of a packet at least until there’s some data to send back (e.g. a telnet session echoing back the user’s typing), or until a timer expires. RFC813 from 1982 is that first that seems to propose delaying ACKs: The receiver of data will refrain from sending an acknowledgement under certain circumstances, in which case it must set a timer which will cause the acknowledgement to be sent later. However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer interrupt. which is then formalized further in RFC1122 from 1989. The interaction between these two features causes a problem: Nagle’s algorithm is blocking sending more data until an ACK is received, but delayed ack is delaying that ack until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications. This is a point Nagle has made himself several times. For example in this Hacker News comment: That still irks me. The real problem is not tinygram prevention. It’s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful. As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard. Is Nagle blameless? Unfortunately, it’s not just delayed ACK2. Even without delayed ack and that stupid fixed timer, the behavior of Nagle’s algorithm probably isn’t what we want in distributed systems. A single in-datacenter RTT is typically around 500μs, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn’t clearly a win. To make a clearer case, let’s turn back to the justification behind Nagle’s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems don’t. Partially that’s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say. The core concern of not sending tiny messages is still a very real one, but we’ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isn’t going to be very efficient, no matter what Nagle’s algorithm does. Is Nagle needed? First, the uncontroversial take: if you’re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable TCP_NODELAY (disable Nagle’s algorithm) without worries. You don’t need to feel bad. It’s not a sin. It’s OK. Just go ahead. More controversially, I suspect that Nagle’s algorithm just isn’t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, TCP_NODELAY should be the default. That’s going to make some “write every byte” code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency. Footnotes I won’t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: “This condition is stable. Once the saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.” As this has gone around the internet, a number of folks have asked about TCP_QUICKACK. I don’t tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read the man page). The bigger problem is that TCP_QUICKACK doesn’t fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I say write(), I mean write(). Other Posts « Back to the blog index 25 Apr 2024 » MemoryDB: Speed, Durability, and Composition. 17 Apr 2024 » Formal Methods: Just Good Engineering Practice? 25 Mar 2024 » Finding Needles in a Haystack with Best-of-K Marc Brooker The opinions on this site are my own. They do not necessarily represent those of my employer. marcbrooker@gmail.com RSS Atom This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=40310896",
    "commentBody": "It's always TCP_NODELAY (brooker.co.za)663 points by todsacerdoti 16 hours agohidepastfavorite215 comments ironman1478 14 hours agoI've fixed multiple latency issues due to nagle's multiple times in my career. It's the first thing I jump to. I feel like the logic behind it is sound, but it just doesn't work for some workloads. It should be something that an engineer needs to be forced to set while creating a socket, instead of letting the OS choose a default. I think that's the main issue. Not that it's a good / bad option but that there is a setting that people might not know about that manipulates how data is sent over the wire so aggressively. reply nh2 10 hours agoparentSame here. I have a hobby that on any RPC framework I encounter, I file a Github issue \"did you think of TCP_NODELAY or can this framework do only 20 calls per second?\". So far, it's found a bug every single time. Some examples: https://cloud-haskell.atlassian.net/browse/DP-108 or https://github.com/agentm/curryer/issues/3 I disagree on the \"not a good / bad option\" though. It's a kernel-side heuristic for \"magically fixing\" badly behaved applications. As the article states, no sensible application does 1-byte network write() syscalls. Software that does that should be fixed. It makes sense only in the case when you are the kernel sysadmin and somehow cannot fix the software that runs on the machine, maybe for team-political reasons. I claim that's pretty rare. For all other cases, it makes sane software extra complicated: You need to explicitly opt-out of odd magic that makes poorly-written software have slightly more throughput, and that makes correctly-written software have huge, surprising latency. John Nagle says here and in linked threads that Delayed Acks are even worse. I agree. But the Send/Send/Receive receive pattern that Nagle's Algorithm degrades is a totally valid and common use case, including anything that does pipelined RPC over TCP. Both Delayed Acks and Nagle's Algorithm should be opt-in, in my opinion. It should be called TCP_DELAY, which you can opt-into if you can't be asked to implement basic userspace buffering. People shouldn't /need/ to know about these. Make the default case be the unsurprising one. reply pzs 5 hours agorootparent\"As the article states, no sensible application does 1-byte network write() syscalls.\" - the problem that this flag was meant to solve was that when a user was typing at a remote terminal, which used to be a pretty common use case in the 80's (think telnet), there was one byte available to send at a time over a network with a bandwidth (and latency) severely limited compared to today's networks. The user was happy to see that the typed character arrived to the other side. This problem is no longer significant, and the world has changed so that this flag has become a common issue in many current use cases. Was terminal software poorly written? I don't feel comfortable to make such judgement. It was designed for a constrained environment with different priorities. Anyway, I agree with the rest of your comment. reply SoftTalker 5 hours agorootparent> when a user was typing at a remote terminal, which used to be a pretty common use case in the 80's Still is for some. I’m probably working in a terminal on an ssh connection to a remote system for 80% of my work day. reply dgoldstein0 4 hours agorootparentsure, but we do so with much better networks than in the 80s. The extra overhead is not going to matter when even a bad network nowadays is measured in megabits per second per user. The 80s had no such luxury. reply underdeserver 3 hours agorootparentprevIf you're working on a distributed system, most of the traffic is not going to be your SSH session though. reply klabb3 8 hours agorootparentprev> As the article states, no sensible application does 1-byte network write() syscalls. Software that does that should be fixed. Yes! And worse, those that do are not gonna be “fixed” by delays either. In this day and age with fast internets, a syscall per byte will bottleneck the CPU way before it’ll saturate the network path. The cpu limit when I’ve been tuning buffers have been somewhere in the 4k-32k range for 10Gbps ish. > Both Delayed Acks and Nagle's Algorithm should be opt-in, in my opinion. Agreed, it causes more problems than it solves and is very outdated. Now, the challenge is rolling out such a change as smoothly as possible, which requires coordination and a lot of trivia knowledge of legacy systems. Migrations are never trivial. reply oefrha 5 hours agorootparentI doubt the libc default in established systems can change now, but newer languages and libraries can learn the lesson and do the right thing. For instance, Go sets TCP_NODELAY by default: https://news.ycombinator.com/item?id=34181846 reply jandrese 8 hours agorootparentprevThe problem with making it opt in is that the point of the protocol was to fix apps that, while they perform fine for the developer on his LAN, would be hell on internet routers. So the people who benefit are the ones who don't know what they are doing and only use the defaults. reply a_t48 9 hours agorootparentprevThanks for the reminder to set this on the new framework I’m working on. :) reply hgomersall 3 hours agorootparentprevWould one not also get clobbered by all the sys calls for doing many small packets? It feels like coalescing in userspace is a much better strategy all round if that's desired, but I'm not super experienced. reply carterschonwald 9 hours agorootparentprevOh hey! It’s been a while how’re you?! reply Sebb767 11 hours agoparentprev> It should be something that an engineer needs to be forced to set while creating a socket, instead of letting the OS choose a default. If the intention is mostly to fix applications with bad `write`-behavior, this would make setting TCP_DELAY a pretty exotic option - you would need a software engineer to be both smart enough to know to set this option, but not smart enough to distribute their write-calls well and/or not go for writing their own (probably better fitted) application-specific version of Nagles. reply pjc50 38 minutes agoparentprev> I feel like the logic behind it is sound, but it just doesn't work for some workloads. The logic is only sound for interactive plaintext typing workloads. It should have been turned off by default 20 years ago, let alone now. reply Bluecobra 13 hours agoparentprevI agree, it has been fairly well known to disable Nagle's Algorithm in HFT/low latency trading circles for quite some time now (like > 15 years). It's one of the first things I look for. reply Scubabear68 13 hours agorootparentI was setting TCP_NODELAY at Bear Stearns for custom networking code circa 1994 or so. reply kristjansson 5 hours agorootparentThis is why I love this place reply Reason077 6 hours agorootparentprevSurely serious HFT systems bypass TCP altogether now days. In that world, every millisecond of latency can potentially cost a lot of money. These are the guys that use microwave links to connect to exchanges because fibre-optics have too much latency. reply mcoliver 12 hours agorootparentprevSame in M&E / vfx reply hinkley 13 hours agoparentprevWhat you really want is for the delay to be n microseconds, but there’s no good way to do that except putting your own user space buffering in front of the system calls (user space works better, unless you have something like io_uring amortizing system call times) reply mjevans 7 hours agorootparentIt'd probably be amazing how many poorly coded games would work better if something like... TCP_60FPSBUFFER Would wait for ~16mS after the first packet is queued and batch the data stream up. reply dishsoap 6 hours agorootparentMost games use UDP. reply Chaosvex 2 hours agorootparentprevAdding delay to multiplayer games? That's worse. reply bobmcnamara 9 hours agorootparentprevI'd rather have portable TCP_CORK reply nsguy 12 hours agoparentprevThe logic is really for things like Telnet sessions. IIRC that was the whole motivation. reply bobmcnamara 9 hours agorootparentAnd for block writes! The Nagler turns a series of 4KB pages over TCP into a stream of MTU sized packets, rather than a short packet aligned to the end of each page. reply ww520 7 hours agoparentprevSame here. My first job out of college was at a database company. Queries at the client side of the client-server based database were slow. It was thought the database server was slow as hardware back then was pretty pathetic. I traced it down to the network driver and found out the default setting of TCP_NODELAY was off. I looked like a hero when turning on that option and the db benchmarks jumped up. reply inopinatus 8 hours agoparentprevWith some vendors you have to solve it like a policy problem, via a LD_PRELOAD shim. reply nailer 12 hours agoparentprevYou’re right re: making delay explicit, but also crappy use the space networking tools don’t show whether no_delay is enabled on sockets. Last time I had to do some Linux stuff, maybe 10 years ago you had to write a systemtap program. I guess it’s EBNF now. But I bet the userspace tools still suck. reply nailer 8 hours agorootparent> use the space Userspace. Sorry, was using voice dictation. reply 0xbadcafebee 10 hours agoprevThe takeaway is odd. Clearly Nagle's Algorithm was an attempt at batched writes. It doesn't matter what your hardware or network or application or use-case or anything is; in some cases, batched writes are better. Lots of computing today uses batched writes. Network applications benefit from it too. Newer higher-level protocols like QUIC do batching of writes, effectively moving all of TCP's independent connection and error handling into userspace, so the protocol can move as much data into the application as fast as it can, and let the application (rather than a host tcp/ip stack, router, etc) worry about the connection and error handling of individual streams. Once our networks become saturated the way they were in the old days, Nagle's algorithm will return in the form of a QUIC modification, probably deeper in the application code, to wait to send a QUIC packet until some criteria is reached. Everything in technology is re-invented once either hardware or software reaches a bottleneck (and they always will as their capabilities don't grow at the same rate). (the other case besides bandwidth where Nagle's algorithm is useful is if you're saturating Packets Per Second (PPS) from tiny packets) reply Spivak 6 hours agoparentYes but it seems this particular implementation is using a heuristic for how to batch that made some assumptions that didn't pan out. reply resonious 2 hours agoprev~15 years ago I played an MMO that was very real-time, and yet all of the communication was TCP. Literally you'd click a button, and you would not even see your action play out until a response packet came back. All of the kids playing this game (me included) eventually figured out you could turn on TCP_NODELAY to make the game buttery smooth - especially for those in California close to the game servers. reply somat 13 hours agoprevWhat about the opposite, disable delayed acks. The problem is the pathological behavior when tinygram prevention interacts with delayed acks. There is an exposed option to turn off tinygram prevention(TCP_NODELAY), how would you tun off delayed acks instead? Say if you wanted to benchmark all four combinations and see what works best. doing a little research I found: linux has the TCP_QUICKACK socket option but you have to set it every time you receive. there is also /proc/sys/net/ipv4/tcp_delack_min and /proc/sys/net/ipv4/tcp_ato_min freebsd has net.inet.tcp.delayed_ack and net.inet.tcp.delacktime reply mjb 12 hours agoparentTCP_QUICKACK does fix the worst version of the problem, but doesn't fix the entire problem. Nagles algorithm will still wait for up to one round-trip time before sending data (at least as specified in the RFC), which is extra latency with nearly no added value. reply Culonavirus 2 hours agoparentprevApparently you have time to \"do a little research\" but not to read the entire article you're reacting to? It specifically mentions TCP_QUICKACK. reply Animats 13 hours agoparentprev> linux has the TCP_QUICKACK socket option but you have to set it every time you receive Right. What were they thinking? Why would you want it off only some of the time? reply batmanthehorse 13 hours agoparentprevIn CentOS/RedHat you can add `quickack 1` to the end of a route to tell it to disable delayed acks for that route. reply rbjorklin 8 hours agorootparentAnd with systemd >= 253 you can set it as part of the network config to have it be applied automatically. https://github.com/systemd/systemd/issues/25906 reply pclmulqdq 14 hours agoprevIn a world where bandwidth was limited, and the packet size minimum was 64 bytes plus an inter-frame gap (it still is for most Ethernet networks), sending a TCP packet for literally every byte wasted a huge amount of bandwidth. The same goes for sending empty acks. On the other hand, my general position is: it's not TCP_NODELAY, it's TCP. reply metadaemon 14 hours agoparentI'd just love a protocol that has a built in mechanism for realizing the other side of the pipe disconnected for any reason. reply toast0 14 hours agorootparentThat's possible in circuit switched networking with various types of supervision, but packet switched networking has taken over because it's much less expensive to implement. Attempts to add connection monitoring usually make things worse --- if you need to reroute a cable, and one or both ends of the cable will detect a cable disconnection and close user sockets, that's not great, now you do a quick change with a small period of data loss but otherwise minor interruption; all of the established connections will be dropped. reply 01HNNWZ0MV43FF 11 hours agorootparentprevTo re-word everyone else's comments - \"Disconnected\" is not well-defined in any network. reply dataflow 9 hours agorootparent> To re-word everyone else's comments - \"Disconnected\" is not well-defined in any network. Parent said disconnected pipe, not network. It's sufficiently well-definable there. reply pjc50 33 minutes agorootparentSee https://news.ycombinator.com/item?id=40316922 : \"pipe\" is L3, the network links are L2. reply Spivak 6 hours agorootparentprevI think it's a distinction without a difference in this case. You can't know if the reason your water stopped is because the water is shut off, the pipe broke, or it's just slow. When all you have to go on is \"I stopped getting packets\" the best you can do is give up after a bit. TCP keepsalives do kinda suck and are full of interesting choices that don't seem to have passed the test of time. But they are there and if you control both sides of the connection you can be sure they work. reply dataflow 6 hours agorootparentThere's a crucial difference in fact, which is that the peer you're defining connectedness to is a single well-defined peer that is directly connected to you, which \"The Network\" is not. As for the analogy, uh, this ain't water. Monitor the line voltage or the fiber brightness or something, it'll tell you very quickly if the other endpoint is disconnected. It's up to the physical layer to provide a mechanism to detect disconnection, but it's not somehow impossible or rocket science... reply umanwizard 5 hours agorootparentWell, isn't that already how it works? If I physically unplug my ethernet cable, won't TCP-related syscalls start failing immediately? reply pjc50 29 minutes agorootparentLast time I looked the behavior differed; some OSs will immediately reset TCP connections which were using an interface when it goes away, others will wait until a packet is attempted. reply dataflow 4 hours agorootparentprevProbably, but I don't know how the physical layers work underneath. But regardless, it's trivial to just monitor something constantly to ensure the connection is still there, you just need the hardware and protocol support. reply pjc50 30 minutes agorootparentModern Ethernet has what it calls \"fast link pulses\"; every 16ms there's some traffic to check. It's telephones that use voltage for hook detection. However, that only applies to the two ends of that cable, not between you and the datacentre on the other side of the world. reply jallmann 11 hours agorootparentprevThese types of keepalives are usually best handled at the application protocol layer where you can design in more knobs and respond in different ways. Otherwise you may see unexpected interactions between different keepalive mechanisms in different parts of the protocol stack. reply pclmulqdq 11 hours agorootparentprevSeveral of the \"reliable UDP\" protocols I have worked on in the past have had a heartbeat mechanism that is specifically for detecting this. If you haven't sent a packet down the wire in 10-100 milliseconds, you will send an extra packet just to say you're still there. It's very useful to do this in intra-datacenter protocols. reply koverstreet 14 hours agorootparentprevLike TCP keepalives? reply mort96 13 hours agorootparentIf the feature already technically exists in TCP, it's either broken or disabled by default, which is pretty much the same as not having it. reply hi-v-rocknroll 4 hours agorootparentYou're conflating all optional TCP features of all operating systems, network devices, and RFCs together. This lack of nuance fails to appreciate that different applications have different needs for how they use TCP: ( serverclient ) x ( one waychatty bidirectionalidle tinygrammixed ). If a feature needs to be used on a particular connection, then use it. ;) reply voxic11 13 hours agorootparentprevkeepalives are an optional TCP feature so they are not necessarily supported by all TCP implementations and therefor default to off even when supported. reply dilyevsky 10 hours agorootparentWhere is it off? Most linux distros have it on it’s just the default kickoff timer is ridiculously long (like 2 hours iirc). Besides, TCP keepalives won't help with the issue at hand and were put in for totally different purpose (gc'ing idle connections). Most of the time you don't even need them because the other side will send RST packet if it already closed the socket. reply halter73 8 hours agorootparentAFAIK, all Linux distros plus Windows and macOS have TCP keepalives off by default as mandated by the RFC 1122. Even when they are optionally turned on using SO_KEEPALIVE, the interval defaults to two hours because that is the minimum default interval allowed by spec. That can then be optionally reduced with something like /proc/sys/net/ipv4/tcp_keepalive_time (system wide) or TCP_KEEPIDLE (per socket). By default, completely idle TCP connections will stay alive indefinitely from the perspective of both peers even if their physical connection is severed. Implementors MAY include \"keep-alives\" in their TCP implementations, although this practice is not universally accepted. If keep-alives are included, the application MUST be able to turn them on or off for each TCP connection, and they MUST default to off. Keep-alive packets MUST only be sent when no data or acknowledgement packets have been received for the connection within an interval. This interval MUST be configurable and MUST default to no less than two hours. [0]: https://datatracker.ietf.org/doc/html/rfc1122#page-101 reply dilyevsky 8 hours agorootparentOK you're right - it's coming back to me now. I've been spoiled by software that enables keep-alive on sockets. reply mort96 2 hours agorootparentprevSo we need a protocol with some kind of non-optional default-enabled keepalive. reply josefx 2 hours agorootparentNow your connections start to randomly fail in production because the implementation defaults to 20ms and your local tests never caught that. reply mort96 1 hour agorootparentI'm sure there's some middle ground between \"never time out\" and \"time out after 20ms\" that works reasonably well for most use cases reply the8472 14 hours agorootparentprevIf a socket is closed properly there'll be a FIN and the other side can learn about it by polling the socket. If the network connection is lost due to external circumstances (say your modem crashes) then how would that information propagate from the point of failure to the remote end on an idle connection? Either you actively probe (keepalives) and risk false positives or you wait until you hear again from the other side, risking false negatives. reply dataflow 9 hours agorootparent> If the network connection is lost due to external circumstances (say your modem crashes) then how would that information propagate from the point of failure to the remote end on an idle connection? Observe the line voltage? If it gets cut then you have a problem... > Either you actively probe (keepalives) and risk false positives What false positives? Are you thinking there's an adversary on the other side? reply pjc50 34 minutes agorootparentThis is a L2 vs L3 thing. Most network links absolutely will detect that the link has gone away; the little LED will turn off and the OS will be informed on both ends of that link. But one of the link ends is a router, and these are (except for NAT) stateless. The router does not know what TCP connections are currently running through it, so it cannot notify them - until a packet for that link arrives, at which point it can send back an ICMP packet. A TCP link with no traffic on it does not exist on the intermediate routers. (Direct contrast to the old telecom ATM protocol, which was circuit switched and required \"reservation\" of a full set of end-to-end links). reply sophacles 13 hours agorootparentprevIt gets even worse - routing changes causing traffic to blackhole would still be undetectable without a timeout mechanism, since probes and responses would be lost. reply noselasd 13 hours agorootparentprevSCTP has hearbeats to detect that. reply sophacles 13 hours agorootparentprevThat's really really hard. For a full, guaranteed way to do this we'd need circuit switching (or circuit switching emulation). It's pretty expensive to do in packet networks - each flow would need to be tracked by each middle box, so a lot more RAM at every hop, and probably a lot more processing power. If we go with circuit establishment, its also kind of expensive and breaks the whole \"distributed, decentralized, self-healing network\" property of the Internet. It's possible to do better than TCP these days, bandwidth is much much less constrained than it was when TCP was designed, but it's still a hard problem to do detection of pipe disconnected for any reason other than timeouts (which we already have). reply niutech 12 hours agoparentprevShouldn't QUIC (https://en.wikipedia.org/wiki/QUIC) solve the TCP issues like latency? reply klabb3 10 hours agorootparentAs someone who needed high throughput and looked to QUIC because of control of buffers, I recommend against it at this time. It’s got tons of performance problems depending on impl and the API is different. I don’t think QUIC is bad, or even overengineered, really. It delivers useful features, in theory, that are quite well designed for the modern web centric world. Instead I got a much larger appreciation for TCP, and how well it works everywhere: on commodity hardware, middleboxes, autotuning, NIC offloading etc etc. Never underestimate battletested tech. In that sense, the lack of TCP_NODELAY is an exception to the rule that TCP performs well out of the box (golang is already doing this by default). As such, I think it’s time to change the default. Not using buffers correctly is a programming error, imo, and can be patched. reply supriyo-biswas 3 hours agorootparentWas this ever implemented though? I found [1] but it was frozen due to age and was never worked on, it seems. (Edit: doing some more reading, it seems TCP_NODELAY was always the default in Golang. Enable TCP_NODELAY => \"disable Nagle's algorithm\") [1] https://github.com/golang/go/issues/57530 reply bboreham 2 hours agorootparentYes. That issue is confusingly titled, but consists solely of a quote from the author of the code talking about what they were thinking at the time they did it. reply jallmann 10 hours agorootparentprevThe specific issues that this article discusses (eg Nagle's algorithm) will be present in most packet-switched transport protocols, especially ones that rely on acknowledgements for reliability. The QUIC RFC mentions this: https://datatracker.ietf.org/doc/html/rfc9000#section-13 Packet overhead, ack frequency, etc are the tip of the iceberg though. QUIC addresses some of the biggest issues with TCP such as head-of-line blocking but still shares the more finicky issues, such as different flow and congestion control algorithms interacting poorly. reply djha-skin 10 hours agorootparentprevQuic is mostly used between client and data center, but not between two datacenter computers. TCP is the better choice once inside the datacenter. Reasons: Security Updates Phones run old kernels and new apps. So it makes a lot of sense to put something that needs updated a lot like the network stack into user space, and quic does well here. Data center computers run older apps on newer kernels, so it makes sense to put the network stack into the kernel where updates and operational tweaks can happen independent of the app release cycle. Encryption Overhead The overhead of TLS is not always needed inside a data center, where it is always needed on a phone. Head of Line Blocking Super important on a throttled or bad phone connection, not a big deal when all of your datacenter servers have 10G connections to everything else. In my opinion TCP is a battle hardened technology that just works even when things go bad. That it contains a setting with perhaps a poor default is a small thing in comparison to its good record for stability in most situations. It's also comforting to know I can tweak kernel parameters if I need something special for my particular use case. reply mjb 10 hours agorootparentMany performance-sensitive in-datacenter applications have moved away from TCP to reliable datagram protocols. Here's what that looks like at AWS: https://ieeexplore.ieee.org/document/9167399 reply theamk 15 hours agoprevI don't by the reasoning for never needing Nagle anymore. Sure, telnet isn't a thing today, but I bet there are still plenty of apps which do equivalent of: write(fd, \"Host: \") write(fd, hostname) write(fd, \"\\r\") write(fd, \"Content-type: \") etc... this may not be 40x overhead, but it'd still 5x or so. reply temac 15 hours agoparentFix the apps. Nobody expect magical perf if you do that when writing to files, even though the OS also has its own buffers. There is no reason to expect otherwise when writing to a socket and actually nagle already doesn't save you from syscall overhead. reply citrin_ru 1 hour agorootparentI agree that such code should be fixed but having hard time persuading developers to fix their code. Many of them don't know what is a syscall, how making a syscall triggers sending of an IP packet, how a library call translates to a syscall e. t. c. Worse they don't want to know this, they write say Java code (or some other high level language) and argue that libraries/JDK/kernel should handle all 'low level' stuff. To get optimal performance for request-response protocols like HTTP one should send a full request which includes a request line, all headers and a POST body using a single write syscall (unless POST body is large and it make sense to write it in chunks). Unfortunately not all HTTP libraries work this way and a library user cannot fix this problem without switching a library which is: 1. not always easy 2. it is not widely known which libraries are efficient and which are not. Even if you have an own HTTP library it's not always trivial to fix: e. g. in Java a way to fix this problem while keeping code readable and idiomatic is too wrap socket into BufferedOutputStream which adds one more memory-to-memory copy for all data you are sending on top of at least one memory-to-memory copy you already have without a buffered stream; so it's not an obvious performance win for an application which already saturates memory bandwidth. reply toast0 15 hours agorootparentprevNagle doesn't save the derpy side from syscall overhead, but it would save the other side. It's not just apps doing this stuff, it also lives in system libraries. I'm still mad at the Android HTTPS library for sending chunked uploads as so many tinygrams. I don't remember exactly, but I think it's reasonable packetization for the data chunk (if it picked a reasonable size anyway), then one packet for \\r, one for the size, and another for another \\r. There's no reason for that, but it doesn't hurt the client enough that I can convince them to avoid the system library so they can fix it and the server can manage more throughput. Ugh. (It might be that it's just the TLS packetization that was this bogus and the TCP packetization was fine, it's been a while) If you take a pcap for some specific issue, there's always so many of these other terrible things in there.reply bjourne 10 hours agorootparentprev> Fix the apps. Nobody expect magical perf if you do that when writing to files, We write to files line-by-line or even character-by-character and expect the library or OS to \"magically\" buffer it into fast file writes. Same with memory. We expect multiple small mallocs to be smartly coalesced by the platform. reply PaulDavisThe1st 4 hours agorootparentIf you expect a POSIX-y OS to buffer write(2) calls, you're sadly misguided. Whether or not that happens depends on nature of the device file you're writing to. OTOH, if you're using fwrite(3), as you likely should be actual file I/O, then your expectation is entirely reasonable. Similarly with memory. If you expect brk(2) to handle multiple small allocations \"sensibly\" you're going to be disappointed. If you use malloc(3) then your expectation is entirely reasonable. reply _carbyau_ 7 hours agorootparentprevTrue to a degree. But that is a singular platform wholly controlled by the OS. Once you put packets out into the world you're in a shared space. I assume every conceivable variation of argument has been made both for and against Nagles at this point but it essentially revolves around a shared networking resource and what policy is in place for fair use. Nagles fixes a particular case but interferes overall. If you fix the \"particular case app\" the issue goes away. reply eru 9 hours agorootparentprevYes, your libraries should fix that. The OS (as in the kernel) should not try to do any abstraction. Alas, kernels really like to offer abstractions. reply jeroenhd 8 hours agorootparentprevEverybody expects magical perf if you do that when writing files. We have RAM buffers and write caches for a reason, even on fast SSDs. We expect it so much that macOS doesn't flush to disk even when you call fsync() (files get flushed to the disk's write buffer instead). There's some overhead to calling write() in a loop, but it's certainly not as bad as when a call to write() would actually make the data traverse whatever output stream you call it on. reply meinersbur 14 hours agorootparentprevThose are the apps are quickly written and do not care if they unnecessarily congest the network. The ones that do get properly maintained can set TCP_NODELAY. Seems like a reasonable default to me. reply blahgeek 8 hours agorootparentprevWe actually have the similar behavior when writing to files: contents are buffered in page cache and are written to disk later in batch, unless user explicitly call \"sync\". reply ale42 13 hours agorootparentprevApps can always misbehave, you never know what people implement, and you don't always have source code to patch. I don't think the role of the OS is to let the apps do whatever they wish, but it should give the possibility of doing it if it's needed. So I'd rather say, if you know you're properly doing things and you're latency sensitive, just TCP_NODELAY on all your sockets and you're fine, and nobody will blame you about doing it. reply rwmj 15 hours agoparentprevThe comment about telnet had me wondering what openssh does, and it sets TCP_NODELAY on every connection, even for interactive sessions. (Confirmed by both reading the code and observing behaviour in 'strace'). reply c0l0 15 hours agorootparentEspecially for interactive sessions, it absolutely should! :) reply syncsynchalt 12 hours agorootparentIronic since Nagle's Algorithm (which TCP_NODELAY disables) was invented for interactive sessions. It's hard to imagine interactive sessions making more than the tiniest of blips on a modern network. reply eru 9 hours agorootparentIsn't video calling an interactive session? reply semi 6 hours agorootparentI think that's more two independent byte streams. You want low latency but what is transfered doesnt really impact the other side, you just constantly want to push the next frame reply eru 6 hours agorootparentThanks, that makes sense! It's interesting that it's very much an interactive experience for the end-user. But for the logic of the computer, it's not interactive at all. You can make the contrast even stronger: if both video streams are transmitted over UDP, you don't even need to sent ACKs etc. To be truly one-directional from a technical point of view. Then compare that to transferring a file via TCP. For the user this is as one-directional and non-interactive as it gets, but the computers constantly talk back and forth. reply asveikau 14 hours agoparentprevI don't think that's actually super common anymore when you consider that doing asynchronous I/O, the only sane way to do that is put it into a buffer rather than blocking at every small write(2). Then you consider that asynchronous I/O is usually necessary both on server (otherwise you don't scale well) and client (because blocking on network calls is terrible experience, especially in today's world of frequent network changes, falling out of network range, etc.) reply jabl 51 minutes agoparentprevEven if you do nothing 'fancy' like Nagle, corking, or userspace building up the complete buffer before writing etc., at the very least the above should be using a vectored write (writev() ). reply Too 32 minutes agoparentprevShouldn’t that go through some buffer? Unless you fflush() between each write? reply silisili 15 hours agoparentprevWe shouldn't penalize the internet at large because some developers write terrible code. reply littlestymaar 14 hours agorootparentIsn't it how SMTP is working though? reply leni536 12 hours agorootparentNo? reply grishka 15 hours agoparentprevAnd they really shouldn't do this. Even disregarding the network aspect of it, this is still bad for performance because syscalls are kinda expensive. reply otterley 15 hours agoparentprevMarc addresses that: “That’s going to make some “write every byte” code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.” reply jrockway 15 hours agoparentprevDoes this matter? Yes, there's a lot of waste. But you also have a 1Gbps link. Every second that you don't use the full 1Gbps is also waste, right? reply tedunangst 14 hours agorootparentThis is why I always pad out the end of my html files with a megabyte of. A half empty pipe is a half wasted pipe. reply arp242 12 hours agorootparentI am finally starting to understand some of these OpenOffice/LibreOffice commit messages like https://github.com/LibreOffice/core/commit/a0b6744d3d77 reply dessimus 12 hours agorootparentprevJust be sure HTTP Compression is off though, or you're still half-wasting the pipe. Better to just dump randomized uncompressible data into html comments. reply Arnt 15 hours agoparentprevThose aren't the ones you debug, so they won't be seen by OP. Those are the ones you don't need to debug because Nagle saves you. reply eatonphil 15 hours agoparentprevI imagine the write calls show up pretty easily as a bottleneck in a flamegraph. reply wbl 15 hours agorootparentThey don't. Maybe if you're really good you notice the higher overhead but you expect to be spending time writing to the network. The actual impact shows up when the bandwidth consumption is way up on packet and TCP headers which won't show on a flamegraph that easily. reply sophacles 13 hours agoparentprevTCP_CORK handles this better than nagle tho. reply tptacek 14 hours agoparentprevThe discussion here mostly seems to miss the point. The argument is to change the default, not to eliminate the behavior altogether. reply the8472 14 hours agoparentprevshouldn't autocorking help with even without nagle? reply loopdoend 14 hours agoparentprevAh yeah I fixed this exact bug in net-http in Ruby core a decade ago. reply batmanthehorse 15 hours agoprevDoes anyone know of a good way to enable TCP_NODELAY on sockets when you don't have access to the source for that application? I can't find any kernel settings to make it permanent, or commands to change it after the fact. I've been able to disable delayed acks using `quickack 1` in the routing table, but it seems particularly hard to enable TCP_NODELAY from outside the application. I've been having exactly the problem described here lately, when communicating between an application I own and a closed source application it interacts with. reply jdadj 12 hours agoparentDepending on the specifics, you might be able to add socat in the middle. Instead of: your_app —> server you’d have: your_app -> localhost_socat -> server socat has command line options for setting tcp_nodelay. You’d need to convince your closed source app to connect to localhost, though. But if it’s doing a dns lookup, you could probably convince it to connect to localhost with an /etc/hosts entry Since your app would be talking to socat over a local socket, the app’s tcp_nodelay wouldn’t have any effect. reply coldpie 14 hours agoparentprevWould some kind of LD_PRELOAD interception for socket(2) work? Call the real function, then do setsockopt or whatever, and return the modified socket. reply cesarb 14 hours agorootparent> Would some kind of LD_PRELOAD interception for socket(2) work? That would only work if the call goes through libc, and it's not statically linked. However, it's becoming more and more common to do system calls directly, bypassing libc; the Go language is infamous for doing that, but there's also things like the rustix crate for Rust (https://crates.io/crates/rustix), which does direct system calls by default. reply zbowling 14 hours agorootparentAnd go is wrong for doing that, at least on Linux. It bypasses optimizations in the vDSO in some cases. On Fuchsia, we made direct syscalls not through the vDSO illegal and it was funny the hacks to go that required. The system ABI of Linux really isn't the syscall interface, its the system libc. That's because the C ABI (and the behaviors of the triple it was compiled for) and its isms for that platform are the linga franca of that system. Going around that to call syscalls directly, at least for the 90% of useful syscalls on the system that are wrapped by libc, is asinine and creates odd bugs, makes crash reporters heuristical unwinders, debuggers, etc all more painful to write. It also prevents the system vendor from implementing user mode optimizations that avoid mode and context switches when necessary. We tried to solve these issues in Fuchsia, but for Linux, Darwin, and hell, even Windows, if you are making direct syscalls and it's not for something really special and bespoke, you are just flat-out wrong. reply JoshTriplett 14 hours agorootparent> The system ABI of Linux really isn't the syscall interface, its the system libc. You might have reasons to prefer to use libc; some software has reason to not use libc. Those preferences are in conflict, but one of them is not automatically right and the other wrong in all circumstances. Many UNIX systems did follow the premise that you must use libc and the syscall interface is unstable. Linux pointedly did not, and decided to have a stable syscall ABI instead. This means it's possible to have multiple C libraries, as well as other libraries, which have different needs or goals and interface with the system differently. That's a useful property of Linux. There are a couple of established mechanism on Linux for intercepting syscalls: ptrace, and BPF. If you want to intercept all uses of a syscall, intercept the syscall. If you want to intercept a particular glibc function in programs using glibc, or for that matter a musl function in a program using musl, go ahead and use LD_PRELOAD. But the Linux syscall interface is a valid and stable interface to the system, and that's why LD_PRELOAD is not a complete solution. reply zbowling 13 hours agorootparentIt's true that Linux has a stable-ish syscall table. What is funny is that this caused a whole series of Samsung Android phones to reboot randomly with some apps because Samsung added a syscall at the same position someone else did in upstream linux and folks staticly linking their own libc to avoid boionc libc were rebooting phones when calling certain functions because the Samsung syscall causing kernel panics when called wrong. Goes back to it being a bad idea to subvert your system libc. Now, distro vendors do give out multiple versions of a libc that all work with your kernel. This generally works. When we had to fix ABI issues this happened a few times. But I wouldn't trust building our libc and assuming that libc is portable to any linux machine to copy it to. reply cesarb 13 hours agorootparent> It's true that Linux has a stable-ish syscall table. It's not \"stable-ish\", it's fully stable. Once a syscall is added to the syscall table on a released version of the official Linux kernel, it might later be replaced by a \"not implemented\" stub (which always returns -ENOSYS), but it will never be reused for anything else. There's even reserved space on some architectures for the STREAMS syscalls, which were AFAIK never on any released version of the Linux kernel. The exception is when creating a new architecture; for instance, the syscall table for 32-bit x86 and 64-bit x86 has a completely different order. reply withinboredom 11 hours agorootparentI think what they meant (judging by the example you ignored) is that the table changes (even if append-only) and you don't know which version you actually have when you statically compile your own version. Thus, your syscalls might be using a newer version of the table but it a) not actually be implemented, or b) implemented with something bespoke. reply cesarb 6 hours agorootparent> Thus, your syscalls might be using a newer version of the table but it a) not actually be implemented, That's the same case as when a syscall is later removed: it returns -ENOSYS. The correct way is to do the call normally as if it were implemented, and if it returns -ENOSYS, you know that this syscall does not exist in the currently running kernel, and you should try something else. That is the same no matter whether it's compiled statically or dynamically; even a dynamic glibc has fallback paths for some missing syscalls (glibc has a minimum required kernel version, so it does not need to have fallback paths for features introduced a long time ago). > or b) implemented with something bespoke. There's nothing you can do to protect against a modified kernel which does something different from the upstream Linux kernel. Even going through libc doesn't help, since whoever modified the Linux kernel to do something unexpected could also have modified the C library to do something unexpected, or libc could trip over the unexpected kernel changes. One example of this happening is with seccomp filters. They can be used to make a syscall fail with an unexpected error code, and this can confuse the C library. More specifically, a seccomp filter which forces the clone3 syscall to always return -EPERM breaks newer libc versions which try the clone3 syscall first, and then fallback to the older clone syscall if clone3 returned -ENOSYS (which indicates an older kernel that does not have the clone3 syscall); this breaks for instance running newer Linux distributions within older Docker versions. reply withinboredom 2 hours agorootparentEvery kernel I’ve ever used has been different from an upstream kernel, with custom patches applied. It’s literally open source, anyone can do anything to it that they want. If you are using libc, you’d have a reasonable expectation not to need to know the details of those changes. If you call the kernel directly via syscall, then yeah, there is nothing you can do about someone making modifications to open source software. reply tedunangst 13 hours agorootparentprevThe complication with the linux syscall interface is that it turns the worse is better up to 11. Like setuid works on a per thread basis, which is seriously not what you want, so every program/runtime must do this fun little thread stop and start and thunk dance. reply JoshTriplett 13 hours agorootparentYeah, agreed. One of the items on my long TODO list is adding `setuid_process` and `setgid_process` and similar, so that perhaps a decade later when new runtimes can count on the presence of those syscalls, they can stop duplicating that mechanism in userspace. reply pie_flavor 11 hours agorootparentprevYou seem to be saying 'it was incorrect on Fuchsia, so it's incorrect on Linux'. No, it's correct on Linux, and incorrect on every other platform, as each platform's documentation is very clear on. Go did it incorrectly on FreeBSD, but that's Go being Go; they did it in the first place because it's a Linux-first system and it's correct on Linux. And glibc does not have any special privilege, the vdso optimizations it takes advantage of are just as easily taken advantage of by the Go compiler. There's no reason to bucket Linux with Windows on the subject of syscalls when the Linux manpages are very clear that syscalls are there to be used and exhaustively documents them, while MSDN is very clear that the system interface is kernel32.dll and ntdll.dll, and shuffles the syscall numbers every so often so you don't get any funny ideas. reply toast0 13 hours agorootparentprev> The system ABI of Linux really isn't the syscall interface, its the system libc. Which one? The Linux Kernel doesn't provide a libc. What if you're a static executable? Even on Operating Systems with a libc provided by the kernel, it's almost always allowed to upgrade the kernel without upgrading the userland (including libc); that works because the interface between userland and kernel is syscalls. That certainly ties something that makes syscalls to a narrow range of kernel versions, but it's not as if dynamically linking libc means your program will be compatible forever either. reply jimmaswell 13 hours agorootparent> That certainly ties something that makes syscalls to a narrow range of kernel versions I don't think that's right, wouldn't it be the earliest kernel supporting that call and onwards? The Linux ABI intentionally never breaks userland. reply toast0 12 hours agorootparentIn the case where you're running an Operating System that provides a libc and is OK with removing older syscalls, there's a beginning and an end to support. Looking at FreeBSD under /usr/include/sys/syscall.h, there's a good number of retired syscalls. On Linux under /usr/include/x86_64-linux-gnu/asm/unistd_32.h I see a fair number of missing numbers --- not sure what those are about, but 222, 223, 251, 285, and 387-392 are missing. (on Debian 12.1 with linux-image-6.1.0-12-amd64 version 6.1.52-1, if it matters) reply LegionMammal978 10 hours agorootparentprev> And go is wrong for doing that, at least on Linux. It bypasses optimizations in the vDSO in some cases. Go's runtime does go through the vDSO for syscalls that support it, though (e.g., [0]). Of course, it won't magically adapt to new functions added in later kernel versions, but neither will a statically-linked libc. And it's not like it's a regular occurrence for Linux to new functions to the vDSO, in any case. [0] https://github.com/golang/go/blob/master/src/runtime/time_li... reply asveikau 11 hours agorootparentprevLinux doesn't even have consensus on what libc to use, and ABI breakage between glibc and musl is not unheard of. (Probably not for syscalls but for other things.) reply assassinator42 13 hours agorootparentprevThe proliferation of Docker containers seems to go against that. Those really only work well since the kernel has a stable syscall ABI. So much so that you see Microsoft switching to a stable syscall ABI with Windows 11. reply leni536 12 hours agorootparentprevIt should be possible to use vDSO without libc, although probably a lot of work. reply LegionMammal978 10 hours agorootparentIt's not that much work; after all, every libc needs to have its own implementation. The kernel maps the vDSO into memory for you, and gives you the base address as an entry in the auxiliary vector. But using it does require some basic knowledge of the ELF format on the current platform, in order to parse the symbol table. (Alongside knowledge of which functions are available in the first place.) reply intelVISA 10 hours agorootparentIt's hard work to NOT have the damn vDSO invade your address space. Only kludge part of Linux, well, apart from Nagle's, dlopen, and that weird zero copy kernel patch that mmap'd -each- socket recv(!) for a while. reply Thaxll 13 hours agorootparentprevThose are very strong words... reply sophacles 13 hours agorootparentprevLinux is also weird because there are syscalls not supported in most (any?) libc - things like io_uring, and netlink fall into this. reply gpderetta 13 hours agorootparentFutex for a very long time was only accessible via syscall. reply Too 27 minutes agoparentprevIs it possible to set it as a global OS setting, inside a container? reply praptak 14 hours agoparentprevAttach debugger (ptrace), call setsockopt? reply the8472 14 hours agoparentprevopening `/proc//fd/` and setting the socket option may work (not tested) reply tedunangst 15 hours agoparentprevLD_PRELOAD. reply batmanthehorse 14 hours agorootparentThank you, found this: https://github.com/sschroe/libnodelay reply tuetuopay 12 hours agoparentprevyou could try ebpf and hook on the socket syscall. might be harder than LD_PRELOAD as suggested by other commenters though reply zengid 14 hours agoprevRelevant Oxide and Friends podcast episode https://www.youtube.com/watch?v=mqvVmYhclAg reply matthavener 13 hours agoparentThis was a great episode and the really drove home the importance of visualization. reply rsc 14 hours agoprevNot if you use a modern language that enables TCP_NODELAY by default, like Go. :-) reply andrewfromx 14 hours agoparenthttps://news.ycombinator.com/item?id=34179426 https://github.com/golang/go/issues/57530 huh, TIL. reply eru 9 hours agoparentprevWhy do you need a whole language for that? Couldn't you just use a 'modern' networking library? reply rsc 8 hours agorootparentSure, like the one in https://9fans.github.io/plan9port/. :-) reply silverwind 9 hours agoparentprevNode.js also does this since at least 2020. reply Ono-Sendai 5 hours agoprevFrom my blog > 10 years ago but sadly still relevant: \"Sockets should have a flushHint() API call.\": https://forwardscattering.org/post/3 reply obelos 15 hours agoprevNot every time. Sometimes it's DNS. reply p_l 14 hours agoparentOnce it was a failing line card in router zeroing last bit in IPv4 addresses, resulting in ticket about \"only even IPv4 addresses are accessible\" ... reply jcgrillo 14 hours agorootparentFor some reason this reminded me of the \"500mi email\" bug [1], maybe a similar level of initial apparent absurdity? [1] https://www.ibiblio.org/harris/500milemail.html reply chuckadams 14 hours agorootparentThe most absurd thing to me about the 500 mile email situation is that sendmail just happily started up and soldiered on after being given a completely alien config file. Could be read as another example of \"be liberal in what you accept\" going awry, but sendmail's wretched config format is really a volume of war stories all its own... reply rincebrain 10 hours agorootparentMy favorite example of that was a while ago, \"vixie-cron will read a cron stanza from a core dump written to /etc/cron.d\" when you could convince it to write a core dump there. The other crons wouldn't touch that, but vixie-cron happily chomped through the core dump for \"* * * * * root chmod u+s /tmp/uhoh\" etc. reply jcgrillo 13 hours agorootparentprevConfiguration changes are one of those areas where having some kind of \"are you sure? (y/n)\" check can really pay off. It wouldn't have helped in this case, because there wasn't really any change management process to speak of, but we haven't fully learned the lesson yet. reply unconed 12 hours agorootparentConfirmations are mostly useless unless you explicitly spell out the implications of the change. They are also inferior to being able to undo changes. That's a lesson many don't know. reply lanstin 4 hours agorootparentYour time from commit to live is proportional to your rollback to a known good state. Maybe to a power of the rollback time. reply p_l 1 hour agorootparentprevI can definitely confirm our initial reaction was \"WTF\" followed with idea that the dev team is making fun of us... but we went in and run traceroutes and there it was :O Was fixed in incredible coincidence manner, too - the CTO of the network link provider was in their offices (in the same building as me) and felt bored. Apparently having went through all the levels from hauling cables in datacenter up to CTO level, after short look at traceroutes he just picked a phone, called NOC, and ordered a line card replacement on the router :D reply skunkworker 15 hours agoparentprevDon’t forget BGP or running out of disk space without an alert. reply marcosdumay 14 hours agoparentprevWhen it fails, it's DNS. When it just stops moving, it's either TCP_NODELAY or stream buffering. Really complex systems (the Web) also fail because of caching. reply Sohcahtoa82 13 hours agoparentprevI chuckle whenever I see this meme, because in my experience, the issue is usually DHCP. reply anilakar 2 hours agorootparentBut it's usually DHCP that sets the wrong DNS servers. It's funny that some folks claim DNS outage is a legitimate issue in systems whose both ends they control. I get it; reimplementing functionality is rarely a good sign, but since you already know your own addresses in the first place, you should also have an internal mechanism for sharing them. reply jeffrallen 15 hours agoparentprevOnce every 50 years and 2 billion kilometers, it's a failing memory chip. But you can usually just patch around them, so no big deal. reply rickydroll 14 hours agoparentprevNot every time. Sometimes, the power cord is only connected at one end. reply sophacles 13 hours agoparentprevOne time for me it was: the glass was dirty. Some router near a construction site had dust settle into the gap between the laser and the fiber, and it attenuated the signal enough to see 40-50% packet loss. We figured out where the loss was and had our NOC email the relevant transit provider. A day later we got an email back from the tech they dispatched with the story. reply drivers99 14 hours agoparentprevOr SELinux reply jedberg 12 hours agoprevThis is an interesting thing that points out why abstraction layers can be bad without proper message passing mechanisms. This could be fixed if there was a way for the application at L7 to tell the TCP stack at L4 \"hey, I'm an interactive shell so I expect to have a lot of tiny packets, you should leave TCP_NODELAY on for these packets\" so that it can be off by default but on for that application to reduce overhead. Of course nowadays it's probably an unnecessary optimization anyway, but back in '84 it would have been super handy. reply Dylan16807 10 hours agoparent\"I'm an interactive shell so I expect to have a lot of tiny packets\" is what the delay is for. If you want to turn it off for those, you should turn it off for everything. (If you're worried about programs that buffer badly, then you could compensate with a 1ms delay. But not this round trip stuff.) reply eru 9 hours agoparentprevThe take-away I get is that abstraction layers (in the kernel) can be bad. Operating system kernels should enable secure multiplexing of resources. Abstraction and portability should be done via libraries. See https://en.wikipedia.org/wiki/Exokernel reply JoshTriplett 14 hours agoprevI do wish that TCP_NODELAY was the default, and there was a TCP_DELAY option instead. That'd be a world in which people who want the batch-style behavior (optimizing for throughput and fewer packets at the expense of latency) could still opt into it. reply mzs 13 hours agoparentSo do I, but I with there was a new one TCP_RTTDELAY. It would take a byte that would be what 128th of RTT you want to use for Nagle instead of one RTT or full* buffer. 0 would be the default, behaving as you and I prefer. * \"Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn’t clearly a win.\" I don't think that's such an issue anymore either, given that the server produces so much data it fills the output buffer quickly anyway, the data is then immediately sent before the delay runs its course. reply evanelias 14 hours agoprevJohn Nagle has posted insightful comments about the historical background for this many times, for example https://news.ycombinator.com/item?id=9048947 referenced in the article. He's a prolific HN commenter (#11 on the leaderboard) so it can be hard to find everything, but some more comments searchable via https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... or https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply Animats 13 hours agoparentThe sending pattern matters. Send/Receive/Send/Receive won't trigger the problem, because the request will go out immediately and the reply will provide an ACK and allow another request. Bulk transfers won't cause the problem, because if you fill the outgoing block size, there's no delay. But Send/Send/Receive will. This comes up a lot in game systems, where most of the traffic is small events going one way. reply pipe01 3 hours agorootparentI would imagine that games that require exotic sending patterns would use UDP, giving them more control over the protocol reply EvanAnderson 11 hours agoparentprevI love it when Nagle's algorithm comes up on HN. Inevitably someone, not knowing \"Animats\" is John Nagle, responds a comment from Animats with a \"knowing better\" tone. >smile datagrams) reply chuckadams 14 hours agoparent> We used to call them \"packlets.\" setsockopt(fd, IPPROTO_TCP, TCP_MAKE_IT_GO, &go, sizeof(go)); reply mirekrusin 14 hours agoprevCan't it have \"if payload is 1 byte (or less than X) then wait, otherwise don't\" condition? reply chuckadams 14 hours agoparentSome network stacks like those in Solaris and HP/UX let you tune the \"Nagle limit\" in just such a fashion, up to disabling it entirely by setting it to 1. I'm not aware of it being tunable on Linux, though you can manually control the buffering using TCP_CORK. https://baus.net/on-tcp_cork/ has some nice details. reply deathanatos 13 hours agoparentprevHow is what you're describing not just Nagle's algorithm? If you mean TCP_NODELAY, you should use it with TCP_CORK, which prevents partial frames. TCP_CORK the socket, do your writes to the kernel via send, and then once you have an application level \"message\" ready to send out — i.e., once you're at the point where you're going to go to sleep and wait for the other end to respond, unset TCP_CORK & then go back to your event loop & sleep. The \"uncork\" at the end + nodelay sends the final partial frame, if there is one. reply fweimer 14 hours agoparentprevThere is a socket option, SO_SNDLOWAT. It's not implement Linux according to the manual page. The description in UNIX Network Programming and TCP Illustrated conflict, too. So it's probably not useful. reply the8472 14 hours agoparentprevYou can buffer in userspace. Don't do small writes to the socket and no bytes will be sent. Don't do two consecutive small writes and nagle won't kick in. reply astrange 14 hours agoparentprevFreeBSD has accept filters, which let you do something like wait for a complete HTTP header (inaccurate from memory summary.) Not sure about the sending side. reply projectileboy 9 hours agoprevThe real issue in modern data centers is TCP. Of course at present, we need to know about these little annoyances at the application layer, but what we really need is innovation in the data center at level 4. And yes I know that many people are looking into this and have been for years, but the economic motivation clearly has not yet been strong enough. But that may change if the public's appetite for LLM-based tooling causes data centers to increase 10x (which seems likely). reply benreesman 13 hours agoprevNagle and no delay are like like 90+% of the latency bugs I’ve dealt with. Two reasonable ideas that mix terribly in practice. reply ryjo 10 hours agoprevI just ran into this this week implementing a socket library in CLIPS. I used Berkley sockets, and before that I had only worked with higher-level languages/frameworks that abstracts a lot of these concerns away. I was quite confused when Firefox would show a \"connection reset by peer.\" It didn't occur to me it could be an issue \"lower\" in the stack. `tcpdump` helped me to observe the port and I saw that the server never sent anything before my application closed the connection. reply ramblemonkey 7 hours agoprevWhat if we changed the kernel or tcp stack to hold on to the packet for only a short time before sending it out. This could allow you to balance the latency against the network cost of many small packets. The tcp stack could even do it dynamically if needed. reply tucnak 2 hours agoparentGenius reply kaoD 10 hours agoprevAs a counterpoint, here's the story of how for me it wasn't TCP_NODELAY: for some reason my Nodejs TCP service was talking a few seconds to reply to my requests in localhost (Windows machine). After the connection was established everything was pretty normal but it consistently took a few seconds to establish the connection. I even downloaded netcat for Windows to go as bare ones as possible... and the exact same thing happened. I rewrote a POC service in Rust and... oh wow, the same thing happens. It took me a very long time of not finding anything on the internet (and getting yelled at in Stack Overflow, or rather one of its sister sites) and painstakingly debugging (including writing my own tiny client with tons of debug statements) until I realized \"localhost\" was resolving first to IPv6 loopback in Windows and, only after quietly timing out there (because I was only listening on IPv4 loopback), it did try and instantly connect through IPv4. reply littlestymaar 2 hours agoparentI've seen this too, but luckily someone one the internet gave me a pointer to the exact problem so I didn't have to go deep to figure out. reply epolanski 11 hours agoprevI was curious whether I had to change anything in my applications after reading that so did a bit of research. Both Node.js and Curl use TCP_NODELAY by default from a long time. reply maple3142 8 hours agoprevNot sure if this is a bit off topic or not, but I recently encountered a problem where my program are continuously calling write to a socket in a loop that loops N times, with each of them sending about a few hundred bytes of data representing an application-level message. The loop can be understanded as some \"batched messages\" to server. After that, the program will try to receive data from server and do some processing. The problem is that if N is above certain limit (e.g. 4), the server will resulting in some error saying that the data is truncated somehow. I want to make N larger because the round-trip latency is already high enough, so being blocked by this is pretty annoying. Eventually, I found an answer on stackoverflow saying that setting TCP_NODELAY can fix this, and it actually magically enable be to increase N to a larger number like 64 or 128 without causing issues. Still not sure why TCP_NODELAY can fix this issue and why this problem happens in the first place. reply gizmo686 8 hours agoparentMy guess would be that the server assumes that every call to recv() terminates on a message boundary. With TCP_NODELAY and small messages, this works out fine. Every message is contained in a single packet, and the userspace buffer being read into is large enough to contain it. As such, whenever the kernel has any data to give to userspace, it has an integer number of messages to give. Nothing requires the kernel to respect that, but it will not go out of its way to break it. In contrast, without TCP_NODELAY, messages get concatenated and then fragmented based on where packet boundaries occur. Now, the natural end point for a call to recv() is not the message boundary, but the packet boundary. The server is supposed to see that it is in the middle of a message, and make another call to recv() to get the rest of it; but clearly it does not do that. reply caf 7 hours agorootparentOtherwise known as the \"TCP is a stream-based abstraction, not a packet-based abstraction\" bug. A related one is failing to process the second of two complete commands that happen to arrive in the same recv() call. reply lanstin 3 hours agorootparentI find these bugs to be a sign that the app is not using a good wrapper but just mostly gets lucky that the packet isn’t split randomly on the way. reply blahgeek 8 hours agoparentprev> The problem is that if N is above certain limit (e.g. 4), the server will resulting in some error saying that the data is truncated somehow. Maybe your server expects full application-level messages from single \"recv\" call? This is is not correct. A message may be spited across multiple recv buffers. reply meisel 9 hours agoprevIs this something I should also adjust on my personal Ubuntu machine for better network performance? reply elhosots 14 hours agoprevThis sounds like the root of my vncviewer / server interaction bugs i experience with some vnc viewer/server combo’s between ubuntu linux and freebsd… (tight/tiger) reply gafferongames 11 hours agoprevIt's always TCP_NODELAY. Except when it's head of line blocking, then it's not. reply tempaskhn 12 hours agoprevWow, never would have thought of that. reply pandemicsyn 10 hours agoprevI was gonna say its always lro offload but my experience is dated. reply hi-v-rocknroll 5 hours agoprevApropos repost from 2015: > That still irks me. The real problem is not tinygram prevention. It's ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful. Unfortunately by the time I found about delayed ACKs, I had changed jobs, was out of networking, and doing a product for Autodesk on non-networked PCs. > Delayed ACKs are a win only in certain circumstances - mostly character echo for Telnet. (When Berkeley installed delayed ACKs, they were doing a lot of Telnet from terminal concentrators in student terminal rooms to host VAX machines doing the work. For that particular situation, it made sense.) The delayed ACK timer is scaled to expected human response time. A delayed ACK is a bet that the other end will reply to what you just sent almost immediately. Except for some RPC protocols, this is unlikely. So the ACK delay mechanism loses the bet, over and over, delaying the ACK, waiting for a packet on which the ACK can be piggybacked, not getting it, and then sending the ACK, delayed. There's nothing in TCP to automatically turn this off. However, Linux (and I think Windows) now have a TCP_QUICKACK socket option. Turn that on unless you have a very unusual application. > Turning on TCP_NODELAY has similar effects, but can make throughput worse for small writes. If you write a loop which sends just a few bytes (worst case, one byte) to a socket with \"write()\", and the Nagle algorithm is disabled with TCP_NODELAY, each write becomes one IP packet. This increases traffic by a factor of 40, with IP and TCP headers for each payload. Tinygram prevention won't let you send a second packet if you have one in flight, unless you have enough data to fill the maximum sized packet. It accumulates bytes for one round trip time, then sends everything in the queue. That's almost always what you > want. If you have TCP_NODELAY set, you need to be much more aware of buffering and flushing issues. > None of this matters for bulk one-way transfers, which is most HTTP today. (I've never looked at the impact of this on the SSL handshake, where it might matter.) > Short version: set TCP_QUICKACK. If you find a case where that makes things worse, let me know. > John Nagle (2015) https://news.ycombinator.com/item?id=10608356 --- Support platform survey: TCP_QUICKACK: Linux (must set again every every recv()) TCP_NODELAY: Linux, Apple, Windows, Solaris, FreeBSD, OpenBSD, and NetBSD References: https://www.man7.org/linux/man-pages/man7/tcp.7.html https://opensource.apple.com/source/xnu/xnu-1504.9.17/bsd/ne... https://learn.microsoft.com/en-us/windows/win32/winsock/ippr... https://docs.oracle.com/cd/E88353_01/html/E37851/esc-tcp-4p.... https://man.freebsd.org/cgi/man.cgi?query=tcp https://man.openbsd.org/tcp https://man.netbsd.org/NetBSD-8.0/tcp.4 reply stonemetal12 13 hours agoprev>To make a clearer case, let’s turn back to the justification behind Nagle’s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? That is a bit of a strawman there. While he uses single byte packets as the worst case example, the issue as stated is any not full packet. reply landswipe 12 hours agoprevUse UDP ;) reply hi-v-rocknroll 4 hours agoparentToo many applications end up reinventing TCP or SCTP in user-space. Also, network-level QoS applied to unrecognized UDP protocols typically means it gets throttled before TCP. Use UDP when nothing else will work, when the use-case doesn't need a persistent connection, and when no other messaging or transport library is suitable. reply gafferongames 11 hours agoparentprevBingo reply AtNightWeCode 10 hours agoprev [–] Agreed. Another thing along the same path is expect. Needs to be disabled in many cloud services. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Marc Brooker, an engineer at Amazon Web Services, focuses on databases and serverless tech, debates the TCP_NODELAY socket option and Nagle's algorithm in distributed systems.",
      "Brooker suggests TCP_NODELAY should be the default setting, stating Nagle's algorithm might not be essential in contemporary systems.",
      "He elaborates on the history and consequences of these network protocols."
    ],
    "commentSummary": [
      "The debate centers around enhancing network performance by adjusting settings like TCP_NODELAY, TCP_QUICKACK, and TCP_CORK, with a focus on latency impact in network applications.",
      "Participants exchange experiences and insights on optimizing network protocols such as TCP and UDP for better performance, stressing the significance of efficient protocol utilization and code optimization in data center scenarios.",
      "Recommendations are provided for tweaking settings to boost data transmission effectiveness and avert network-related challenges."
    ],
    "points": 663,
    "commentCount": 215,
    "retryCount": 0,
    "time": 1715277289
  },
  {
    "id": 40307138,
    "title": "World First Gene Therapy Trial Restores Hearing in Deaf Child",
    "originLink": "https://www.independent.co.uk/news/health/deaf-cure-girl-gene-therapy-b2541735.html",
    "originBody": "NewsHealth Deaf girl is cured in world first gene therapy trial Opal Sandy, aged 18 months, was born completely deaf due to condition auditory neuropathy Jane Kirby 1 day ago Comments FILE: Deaf baby hears mum for first time Sign up for our free Health Check email to receive exclusive analysis on the week in health SIGN UP I would like to be emailed about offers, events and updates from The Independent. Read our privacy policy A British girl has had her hearing restored after becoming the first in the world to take part in a groundbreaking new gene therapy trial. Opal Sandy, aged 18 months, was born completely deaf due to the condition auditory neuropathy, which is caused by the disruption of nerve impulses travelling from the inner ear to the brain. Now, thanks to a “one and done” gene therapy being trialled in the UK and worldwide, Opal’s hearing is almost normal – and could even improve further. The little girl from Oxfordshire, who has a genetic form of auditory neuropathy, was treated at Addenbrooke’s Hospital, which is part of Cambridge University Hospitals NHS Foundation Trust. Professor Manohar Bance, an ear surgeon at the trust and chief investigator for the trial, told the PA news agency the results were “better than I hoped or expected” and may point to a cure for patients with this type of deafness. He said: “We have results from [Opal] which are very spectacular – so close to normal hearing restoration. So we do hope it could be a potential cure.” Opal (centre) with her mother Jo, father James and sister Nora (left) at their home in Eynsham, Oxfordshire (Andrew Matthews/PA Wire) Auditory neuropathy can be caused by a fault in the OTOF gene, which is responsible for making a protein called otoferlin. This enables cells in the ear to communicate with the hearing nerve. To overcome the fault, the “new era” gene therapy – from biotech firm Regeneron – delivers a working copy of the gene to the ear. In Opal’s case, she received an infusion containing the working gene in her right ear during surgery last September. Her parents Jo and James, both 33, noticed improvements to her hearing within four weeks when Opal turned her head to loud clapping. “When she first turned, I couldn’t believe it,” Mrs Sandy told PA. “I thought it was a fluke or like a change in light or something that had caught her eye, but I repeated it a few times. “I picked my phone up and texted James, and said, ‘I think it’s working’. I was absolutely gobsmacked. I thought it was a fluke.” But even more impressive results were on the horizon. Some 24 weeks after surgery, in February this year, tests in Cambridge showed Opal could also hear soft sounds such as a whisper. “The audiologist played back some of the sounds that she was responding to and they were ridiculously quiet sort of sounds that in the real world wouldn’t catch your attention during a conversation,” Mrs Sandy said. “Certainly since February, we’ve noticed her sister [Nora] waking her up in the morning because she’s running around on the landing, or someone rings on the door so her nap’s cut short. “She’s definitely responding more to sort of what we would call functional sounds rather than just sounds that we use to test her. “We were told she had near-normal hearing last time – I think they got responses at sort of 25 to 30 decibels. “I think normal hearing is classed at 20 decibels, so she’s not far off. Before, she had no hearing whatsoever.” Prof Bance said Opal’s hearing is now “close to normal”, adding: “We hope she’ll get back to normal by the next testing.” He added that the treatment is “a one-and-done therapy, so hopefully you have your treatment and then you go back to your life”. A second child has also received the gene therapy treatment at Cambridge University Hospitals, with positive results seen recently, six weeks after surgery. The overall phase 1/2 Chord trial consists of three parts, with three deaf children, including Opal, receiving a low dose of gene therapy in one ear only. A different set of three children will get a high dose on one side. Then, if that is shown to be safe, more children will receive a dose in both ears at the same time. Up to 18 youngsters from the UK, Spain and the US are being recruited to the trial and will be followed up for five years. Prof Bance said: “My entire life, gene therapy has been ‘five years away’, and I’ve been in practice about 30 years. “So, for me, it was almost unreal that this moment had arrived. “It was just the fact that we’ve been hearing about this for so long, and there’s been so much work, decades of work … to finally see something that actually worked in humans … It was quite spectacular and a bit awe-inspiring really. “It felt very special.” At the moment, the gold standard treatment for auditory neuropathy is cochlear implants. Opal had one fitted to her left ear at the same time as she underwent gene therapy in her right ear, to ensure she got hearing as soon as possible. The youngster is the first patient globally to receive the Regeneron therapy and “she’s the youngest globally that’s been done to date as far as we know,” Prof Bance said. China has also been working on targeting the same gene, with positive results, though Prof Bance said theirs uses a different technology and slightly different mode of delivery. Medics in Philadelphia have also reported a good outcome with a type of gene therapy on an 11-year-old boy, who was operated on after Opal. Prof Bance said he believes the trial is “just the beginning of gene therapies”, adding: “It marks a new era in the treatment for deafness. “It also supports the development of other gene therapies that may prove to make a difference in other genetic-related hearing conditions, many of which are more common than auditory neuropathy.” He said it could take a while for more children to benefit from gene therapy. The treatment was currently not available on the NHS. “What’s really helped though is that the NHS does pay for genetic testing now for hearing loss,” he said. Opal Sandy can now hear unaided for the first time (Andrew Matthews/PA Wire) Opal’s surgery, which was carried out under general anaesthetic, was very similar to fitting a cochlear implant, Prof Bance continued. “So basically, we find the inner ear and we open the inner ear and infuse the treatment, in this particular case using a catheter, over 16 minutes,” he said. “We have to make a release hole in another part of the ear to let the treatment out because it has to go all the way through the ear. “And then we just repair and close up, so it’s actually a very similar approach to a cochlear implant, except we don’t put the implant in.” Martin McLean, senior policy adviser at the National Deaf Children’s Society, welcomed the study, saying it would lead to learning regarding gene therapies for deafness with a specific genetic cause. “We would like to emphasise that, with the right support from the start, deafness should never be a barrier to happiness or fulfilment,” he said. “As a charity, we support families to make informed choices about medical technologies, so that they can give their deaf child the best possible start in life.” More aboutBritishEarGene TherapyBrain Most Popular Popular videos Sponsored Features",
    "commentLink": "https://news.ycombinator.com/item?id=40307138",
    "commentBody": "Deaf girl is cured in world first gene therapy trial (independent.co.uk)621 points by belter 22 hours agohidepastfavorite284 comments world2vec 17 hours agoMy Mom was born with less than half of her hearing in left ear and barely anything at all in right ear. She had surgery as an adult and it slightly improved, especially on the right side. Pretty sure this little girl and my Mom don't share the same disability at all but she saw these news today and texted me so excited because future kids won't have to endure the same. Brings tears to my eyes, I'm so grateful for modern medicine and its stupendous advances. reply dhosek 16 hours agoparentIt might be the condition I have, otosclerosis, where the bones of the middle ear fuse. The surgery, a stapedectomy, involves removing the bones and replacing them with a metallic prosthesis. Unfortunately, in my case, the calcification is also happening in my cochlea which means that at some indeterminate time in the future, I will end up losing all hearing. Not to mention that even with the prosthetic, my hearing isn’t 100% so I have a difficult time understanding speech. There might be some gene therapy that can remedy things, but I think what I may be hoping to see is the ability to grow a new inner and middle ear from stem cells and transplant them into my head, but I suspect that at age 55, I won’t see this happen in time for me. reply jfengel 15 hours agorootparentI had no idea that was possible. If you'd asked me, I would have guessed that it wasn't. Astonishing. Sorry it's not a permanent fix for you, but it's impressive as hell that they could do anything. reply izend 12 hours agorootparentprevI was recently diagnosed with otosclerosis, have you found a hearing aid that works best for otosclerosis, my Doctor mentioned that most hearing aids don't work well for low frequency loss. reply randlet 10 hours agorootparentI have moderate to severe hearing loss mostly in the low frequency region from otosclerosis...I use Widex Moment 440s BTE with custom ear moulds. They sound fantastic. My only complaint is that require a necklace pendant to get Bluetooth connectivity but other than that I love them. They're expensive but worth every penny IMO. reply tunesmith 7 hours agorootparentprevThose torps are pretty cool but they can get dislodged. I got one in college but I think the benefits for me didn't last more than a year or two. I'll try again someday because my nerve hearing is excellent, it's only my conductive that is poor. reply Geee 12 hours agorootparentprevAnother viable future tech for this might be neural implants similar to Neuralink. Not sure how viable it would be. reply SJC_Hacker 5 hours agorootparentThey already have those - called cochlear implants (CI user here) reply throwaway2037 13 hours agorootparentprevThank you to share about your personal experience. Will a cochlear implant help for your condition? reply dhosek 13 hours agorootparentIt will, but with the caveat that hearing with a cochlear implant is (at least at current technology levels), inferior to hearing with an actual cochlea. So there’s a balancing act where they want to continue having me hear with my cochlea as long as possible. The other problem is that with a CI, I have no hearing at all without the receiver which means that I would be completely deaf while swimming, showering, etc. while I have at least some hearing without my hearing aids right now (although I had an ear infection in December which left me completely deaf for a week. It was a bit startling how much people were unwilling to engage in the smallest adaptations for me—I found how to set up live transcriptions on my phone and I remember the cashier at the grocery store being unwilling to use that so I could see what she was saying). reply dhosek 13 hours agorootparent(With the live transcription feature, I was actually able to engage in normal-ish telephone conversations, maybe even a little more effective than I can with using the audio.) reply shirleyquirk 13 hours agorootparentprevDid your sense of taste change after your surgery? My sister is considering a similar procedure and is concerned that everything could start tasting like hot garbage. reply dhosek 7 hours agorootparentNo, I had a bit of dizziness for the first day or so, but otherwise there were no side effects. Absolutely no impact on my sense of taste. reply sandworm101 13 hours agoparentprev>> so excited because future kids won't have to endure the same. When cochlear implants became routine there was a brief protest by the deaf and hard-of-hearing community. The line between people with a disability and a person who is simply different is a longstanding debate. How and when medicine should intervene is a hot button issue. I had a relative born with a malformed ear canal. The doctors rushed to get her the surgery needed so that she could hear equally in both ears, before her developing brain started ignoring the \"bad\" ear. A few years in and her hearing is now better than mine. reply squigz 10 hours agorootparentThe deaf community is incredibly proud. I have a lot of respect for that. But at the same time, I don't understand it. I'm extremely visually impaired, and I've never seen the same \"It's not a disability!\" sentiment mirrored in the blind community. Does anyone have any insight why this might be? reply wl 9 hours agorootparentThe Deaf (capital D) community has its own special language. There's a lot of culture that comes out of that. On the other hand, the blind community has the same languages as the rest of us. reply squigz 9 hours agorootparentI do suppose that makes a lot of sense. I imagine it's twofold, too: having your own language probably instills a sense of community, which is heightened by the isolation one feels from being so disconnected from so many other communities (including society at large) reply jhbadger 9 hours agorootparentprevProbably because in humans sight is the primary sense and hearing secondary. Deaf people face a lot of challenges, but not nearly as many as blind people (amd I say this as someone with -10 prescriptions, which is close to legal blindness without my glasses) reply mercurywells 8 hours agorootparentprevBrief? Many are still strongly against it, especially doing this very invasive surgery on babies. reply squigz 6 hours agorootparentIt's a fair enough point, it being invasive, but at what point do the benefits outweigh the negatives? reply codeulike 12 hours agoprevHow it actually works (from https://www.fiercebiotech.com/biotech/little-protein-factory... ) DB-OTO is a cell-selective AAV gene therapy for children with hearing loss stemming from a mutation to the otoferlin gene. The otoferlin protein is expressed in the sensory hair cells of the ear, which have tiny cilia that move as vibrations come into the ear. These cells help signal between the auditory nerve and the hair cells, passing information from the ear to the brain. Children born with this type of genetic hearing loss have the hair cells and can detect the signal coming into the ear. “But they can't get that message from the ear to the brain, basically, because otoferlin is critical to enable that communication,” Whitton explained. That’s where DB-OTO comes in. The adeno-associated viral vector delivers the gene therapy to the ear to provide a payload of cDNA that expresses the protein in the hair cells that are missing it. The hypothesis was that if they provided the gene, patients could eventually begin to hear on their own. Regeneron does not yet know how long the effect will last, but “rigorous” preclinical tests were done to get a sense of durability, according to Whitton. Since those hair cells targeted by the gene therapy do not turn over during a person’s lifetime, they believe the effect should be persist once restored. “The ones that you're born with are the ones you will have the rest of your life, so if we can create a little protein factory in those cells, make the protein that's missing, there's reason to believe that you could have long-term benefit,” Whitton said. Note this is not CRISPR, its more like just adding little chunks of DNA into the cell (which I think is called upregulation). 'Gene Therapy' can mean many other things apart from Crispr, there's a whole complicated pipeline of processes that can be targeted. reply hnick 9 hours agoparentSorry just wanted to clear this up - do you mean it delivers an extra set of DNA that does its thing alongside what is already there, rather than editing the main sequence like CRISPR wants to? reply pas 1 hour agorootparentThe usual term for this is \"nonintegrating\" meaning the virus (or whatever delivery mechanism) does not insert itself (or anything) into the cell's genome. (Some viruses do this to help them proliferate - and they do it crudely which increases cancer risk, - but some don't because it's added complexity to have this \"feature\".) https://pubmed.ncbi.nlm.nih.gov/28895845/ reply raindear 7 hours agorootparentprevYes, cDNA is a DNA fragment that does not interact with the patient's DNA in this treatment. Wikipedia: \"Once amplified, the sequence can be cut at each end with nucleases and inserted into one of many small circular DNA sequences known as expression vectors. Such vectors allow for self-replication, inside the cells, and potentially integration in the host DNA. They typically also contain a strong promoter to drive transcription of the target cDNA into mRNA, which is then translated into protein.\" reply alsetmusic 16 hours agoprevMedical science is about the only thing in modern life that consistently delivers hope. While all the other terrible things are going on, something like this comes along and I’m just extremely happy to be alive right now. reply contingencies 11 hours agoparentSerious suggestion: try gardening. To plant a garden is to believe in tomorrow. - Audrey Hepburn reply kosmozaut 3 hours agorootparentThanks for this beautiful quote! So simple yet powerful reply jajko 13 hours agoparentprevAstronomy is cool too, nothing anchors you back in reality like realizing how absolutely microscopic and insignificant our entire mankind's existence was and is. Solid perspective and understanding of bigger picture is important, then nothing can surprise you much. reply harimau777 11 hours agorootparentPersonally, I find astronomy quite depressing. Learning that humanity is stuck on this rock; at least for my lifetime if not forever. As a kid I dreamed of exploring space. As an adult I learned that is impossible. reply selcuka 10 hours agorootparentThe term \"observable universe\" is depressing enough for me. reply ssl-3 7 hours agorootparent\"Welp, that's enough of looking at tens of thousands of places I'll never see up close.\" smashes telescope, grabs another beer, and buys some tickets to visit Spain before going to bed with a little bit less defeat and despair than usual reply downWidOutaFite 11 hours agoparentprevFunny because I feel the opposite. Health care is so broken with all the politics, insurance, bureaucracy, financialization, etc that the vast majority of people are not able to reap the rewards of progress. Even the science is perverted because of bad incentives and only potentially lucrative research is funded. reply aprilthird2021 4 hours agorootparentI was about to say we pay for all this super modern progress by making healthcare so expensive it's out of reach of the vast majority of people in the world (and in the US). reply antegamisou 12 hours agoparentprevYet shitty LLM grift research proposals receive 100x the funding compared to endeavors similar to this. reply raindear 7 hours agorootparentMolecular Biology research is much more funded than computer science. At least in Academia. reply highwayman47 15 hours agoparentprevIt would be nice if people at these companies were rewarded the same way as food delivery app employees. reply Petersipoi 14 hours agorootparentYou want the people at these companies to get a 3 dollar tip each time they restore someone's hearing? reply oxguy3 13 hours agorootparentprev???? I would imagine the average surgeon is much happier with their compensation than the average DoorDasher. It's a reliable fixed-rate salary vs wondering if you'll get enough $3 tips to make rent this month. reply lizardking 13 hours agorootparentMaybe gp was referring to the coders who work at said companies. I don't even think the individual dashers are employees. reply zaphod12 11 hours agorootparentprevI can assure you that regeneron pays very well, including significant equity and has absolutely fantastic benefits. Software developers probably get more at door dash (it isn't bad by any means, but they aren't \"the talent\" at regeneron after all), but folks involved in the science and trials are doing very well. reply HaZeust 14 hours agorootparentprev... Food delivery app employees get shitty rewards in proportion to their work as well? reply weakfish 14 hours agorootparentprevhuh? reply lucianbr 14 hours agorootparentMaybe they meant \"food delivery app developers\"? reply sh4rks 10 hours agoparentprevWhat's the end goal of modern medicine though? Immortality? You can perhaps reduce suffering to nil, but you can't cheat death. reply Brian_K_White 8 hours agorootparentWhat's the end goal of this remark? What's the end goal of learning to farm and produce food even when the weather and pest gods don't bless you this year? What's the end goal of learning to heat your hut instead of dying every winter? reply sircastor 9 hours agorootparentprevWouldn't reducing suffering to nil be a worthy goal? Is there any reason we shouldn't pursue this to its end? reply icehawk 5 hours agorootparentprevWhy does there have to be an end goal? Why cant things just do the thing they do? reply Panzer04 10 hours agorootparentprevWhy can't you cheat death? Excluding accidents and the like. reply kennedywm 10 hours agorootparentprevEven if you solve for immortality, that wouldn’t mean you will have solved all of the medical ills of humanity. We might be able to “cheat death” within our lifetimes. reply fastball 8 hours agorootparentprevWhat is the purpose of this question? \"Yeah sure, we're fixing people's problems that cause pain, suffering, loss of productivity, death, etc, but what is the ultimate goal?\" Sometimes the ends are the means. reply Taylor_OD 9 hours agorootparentprevI mean... Let's cure cancer and then worry about the end goal. reply arjie 14 hours agoprevMy wife and I are selecting which embryos to implant because we have a related condition (this one is a mutation in OTOF, we have one in GJB-2). Being able to implant the embryos affected with this condition (carrying two copies of the gene - mine and my wife) would increase the number of viable embryos we have by 50%. It won't be in time for us to actually implement unless we're down to our last few embryos through failure. We were able to detect ours because we used Orchid Health to scan the embryo genome for monogenic conditions. But it's exciting to think that novel gene therapies might be accessible if we somehow fail to implant with the other embryos we have. All this stuff is very futuristic and it's definitely rescued us since my wife and I started dating and got married within the last year, by which time we were quite old. Very cool stuff from Regeneron. EDIT: I actually looked this up and got a reading list which I went through over lunch. There's quite a lot of work on this front. Regeneron/Decibel Therapeutics have DB-OTO and are developing a GJB-2 (the one we have) gene therapy too https://www.decibeltx.com/pipeline/ Akouos has AK-OTO https://akouos.com/our-focus/ There's a Chinese group that claims they had success in older patients (search for AAV1-hOTOF) The April issue of Molecular Therapy has a few of these. For a quick read look at the Oral Abstracts from the Presidential Symposium. Molecular Therapy Vol 32 No 4S1, April 2024 reply 2dvisio 11 hours agoparentYou are lucky to know ahead of time about your respective mutations. We didn’t (no family history and both asymptomatic carriers) and my son was born bilateral profoundly deaf (connexin 26). We have chosen the path of cochlear implant (which is an amazing technology). We are both looking forward to seeing advancements in therapies for GJB2 mutations sparing many parents (and children) what we have been through. However, corrective therapy for GJB2 might require much earlier discovery of the issue and earlier administration I believe, as usually those variants affect actual physical growth of hair cells in the cochlea. reply arjie 8 hours agorootparentYes, we are fortunate to live in a time where the technology is advanced enough to do this. My wife and I got ourselves sequenced before we decided to have a child (that's just data nerd nonsense) and when we discovered this we went through the whole shebang. I believe it's the OTOF variant that affects the cilia growth. Without intending to give any hope (because I am a software engineer in the end, not a geneticist), perhaps the GJB2 variants can be fixed because they regulate electrical function. Who can tell. Decibel's AAV.103 will go into clinical trial in the next couple of years. https://www.decibeltx.com/pipeline/ The cochlear implant is wonderful tech. Before we had enough embryos, it was something that we discussed with our genetic counselor. Is your son still very young? Some of the OTOF treatments were in older children (in China). Perhaps the GJB2 treatments could be too. I am hopeful that even if your children and mine can't access this treatment, we will make this condition repairable shortly after birth from our generation onwards. reply sircastor 9 hours agoparentprevI don't have anything to offer about your comment, but good luck! We did in-vitro and it was a long, stressful, expensive process. I hope you're successful and find parenthood to be fulfilling and joyful. (Ours is in Kindergarten :) ) reply arjie 9 hours agorootparentThank you very much and congratulations! We have 4 unaffected embryos and 2 affected ones, so I'm quite hopeful. reply dekhn 14 hours agoprevGene therapy has been an extremely slow game. Part of that is ethics, and part of that is technology. A single death in a single trial slowed down gene therapy for decades https://en.wikipedia.org/wiki/Jesse_Gelsinger I dedicated much of my biology training to learning how to carry out gene therapy only to conclude that it will continue to be the underdog in disease therapy for a long time. I do not expect to see large-scale germline modification for non-disease purposes in my lifetime (another 20-30 years). reply codeulike 40 minutes agoparentI do not expect to see large-scale germline modification for non-disease purposes in my lifetime (another 20-30 years). I had to read that twice. Non-disease purposes? The term \"Gene-therapy\" to me means therapy as in treatment of a disease or condition. If we're talking about non-disease purposes like enhancements then you're damn right thats still a long way off. I would have put it further off than that, its going to be a minefield. (Although with 200 different countries in the world it seems like someone will just do it anyway. Maybe someone is doing it now?) reply CSSer 11 hours agoparentprevI expected to read that and find it was a fluke accident. Instead, it reads like the scientists shrugged before the injection and said, “What’s the worst that could happen?” That’s unreal. It killed monkeys under similar circumstances and the kid wasn’t at his healthiest at the time of treatment either. Did they not care or was there some reason to believe that outcome was virtually improbable? reply naasking 5 hours agorootparentBioethics is a lot stricter than you think. They did not simply shrug and say that. reply dekhn 11 hours agorootparentprevI honestly don't know what they were thinking. BTW, most gene vectors today are based on lentiviruses... specifically, from HIV. It took a few tries before they found a method to reliably remove all the bad bits from the vector. See https://www.nature.com/articles/s41375-018-0106-0 and other articles on the evolution of HIV as a gene therapy vector. reply Salgat 10 hours agoparentprevOne good thing about this specific case is that it only persists in the cells exposed to it. New cells and reproduction are unaffected, which helps with the ethics surrounding gene therapy. reply DarkNova6 11 hours agoparentprev> I do not expect to see large-scale germline modification for non-disease purposes in my lifetime (another 20-30 years). Could you please share what makes you think so? reply dekhn 10 hours agorootparentTwo lines of argument: social opposition (people are scared by the technology and many religions strongly oppose human GMO) and lack of adequate technology (we couldn't actually make a permanent change that does what we want without side effects). Note, germline modification means making changes to future children, and all their cells would permanently be modified, and they would pass the changes on to their children. Look at what happened with Jianku He- who ignored all the advice to do this (amazingly, they eventually let him back in the lab). reply jasonfarnon 10 hours agorootparentSo you're only pessimistic about the deployment of germline modifications rather than therapies like this? reply dekhn 10 hours agorootparentYou missed the part where I'm describing germline modifications for non medical uses. Like, intelligence augmentation for your children. Very controversial. reply surfingdino 3 hours agorootparentPublic acceptance of those modifications could increase if such treatments are marketed as new or alternative approaches to what the cosmetic surgery and erectile disfunction treatment industries are making money off today. reply financetechbro 13 hours agoparentprevDo not underestimate the entrepreneurial spirit when there is a profit to be made reply thehappypm 17 hours agoprevI’m optimistic that at some point, I’ll be able to get a treatment to take care of my red green color blindness. I’m well aware that it’s very much a first world problem, and rightfully should be very low on the totem pole of medical issues to cure. But goddamnit, I want to see red. reply dekhn 14 hours agoparentI am really curious what the subjective experience of a color-blind person undergoing gene therapy gradually being able to perceive the difference between two colors that previously appeared the same. See also https://en.wikipedia.org/wiki/What_Is_It_Like_to_Be_a_Bat%3F and https://en.wikipedia.org/wiki/Knowledge_argument reply nozzlegear 8 hours agorootparentSmall tangent, but I first learned of the \"What is it like to be a bat?\" question via the book The Mountain in the Sea by Ray Nayler. Really good book that deals with the ethical problems of emerging sentience in a species of octopus, while set to the backdrop of climate change, sentient AI and overfished oceans. reply eternauta3k 14 hours agorootparentprevI guess the person would have no associations with the pure color red. Non-colorblind people have a life full of memories where red is associated with blood, lipstick, flushed cheeks, race cars, etc. plus the associated feelings. I'm guessing they only really get the full effect of the therapy after they've lived without colorblindness for a while and made those associations. Do you think there's a fundamental difference between getting a new color receptor and training yourself to recognize/differentiate more of the colors you already have the receptors to distinguish? reply interloxia 14 hours agorootparentRed is a great. I hope that it would be like the reaction videos of kids having their Cochlear implant turned on for the first time. reply dekhn 14 hours agorootparentprevI'm not attempting to speculate, I'm just saying I wonder what the subjective experience would be like. I stopped trying to think what it would like to be a bat a while ago. reply bongodongobob 14 hours agorootparentprevI think most people misunderstand color blindness in that most red-green colorblind can see red, it's just muted. I know what red looks like, but it needs to be plain old bright red. reply dekhn 13 hours agorootparentprotanopia: can't see red. protanomly: some red is visible. The guy I worked with who was RG CB couldn't wire up stepper motors (typically use 4 wires, red green blue black), he couldn't tell the difference and had to use a multimeter. reply silverquiet 17 hours agoparentprevI don't really know enough of the medical details - I thought people with color blindness lacked the proper receptors in their eyes, so it's not like in the article where a connection is restored; you'd have to grow new infrastructure in the eye which seems a bit more complicated. Nevertheless, would be very cool if possible - I know lots of men with color blindness. reply ggreer 16 hours agorootparentIn color blindness, the cone cells produce an incorrect opsin protein that isn't sensitive to red (or is sensitive to a frequency close to green). Gene therapy would fix that, though it'd probably take a while for the brain to understand the new information. The main reason this therapy isn't available is because the FDA has decided that the risks of gene therapy aren't worth the benefit of curing color blindness. I agree with that evaluation, but I also think mavericks should be allowed to try curing their own colorblindness (assuming they had informed consent and paid for the therapy themselves). reply scottlamb 15 hours agorootparent> The main reason this therapy isn't available is because the FDA has decided that the risks of gene therapy aren't worth the benefit of curing color blindness. I agree with that evaluation, but I also think mavericks should be allowed to try curing their own colorblindness (assuming they had informed consent and paid for the therapy themselves). In what way do you agree with that evaluation then? The FDA certainly doesn't force anyone to undergo a treatment. And my understanding was the FDA doesn't decide who pays for a given treatment. In particular I don't think they define what medical insurance companies (or Medicare/Medicaid) have to pay for, do they? I think they just say whether the treatment is ethical to be performed at all. So when you say that mavericks should be allowed to try it, I think you're simply disagreeing with them? reply trebligdivad 16 hours agorootparentprevThe 'though it'd probably take a while for the brain to understand the new information' is an interesting question; on one hand people say you need to fix stuff early in development while the brain is still very plastic (so great for this kid); but then again there is that demo of people getting used to vertically flipped vision with weird glasses; so perhaps something is flexible? reply wizzwizz4 15 hours agorootparentThe brain is good at re-mapping things where it can make a distinction, but it seems to only learn distinctions in so-called \"critical periods\". See doi:10.1038/228477a0 for a famous 1970s experiment on the subject. reply lupire 13 hours agorootparentThere are lots of examples. Phonemes for language and are a very common example of things that can't be learned easily or at all later in life. But huge changes like a major color might overcome that. Humans learn to recognize new major objects they've never seen before. reply wizzwizz4 9 hours agorootparentHumans can learn to recognise new sensory modalities, too (see paragraph 4 of doi:10.3389/fnhum.2019.00443's introduction) – but only if there's hardware that responds to them. The question is whether the visual cortex is capable of making the distinctions. If colourblindness glasses (e.g. EnChroma®) work for you, then I suppose the answer is trivially \"yes, because it already does\" – and we should expect gene therapy to be useful. If there are people for whom those glasses don't do anything at all, I'd be more cautious. reply adgjlsfhk1 16 hours agorootparentprevyeah I'd be pretty surprised if your brain didn't figure it out in a week. reply Ballu 13 hours agorootparentprevCould you provide the ref/details of this particular gene therapy and FDA views? I can understand why the FDA wants to move ahead slowly unless the condition is life-threatening or the impact is life-changing. reply derefr 15 hours agorootparentprev> I agree with that evaluation, but I also think mavericks should be allowed to try curing their own colorblindness (assuming they had informed consent and paid for the therapy themselves). I mean, what's stopping you? This guy cured his own lactose intolerance: https://www.youtube.com/watch?v=J3FcbFqSoQY But I'm guessing — in part because you say \"paid for the therapy\" — that you don't really mean \"should be allowed to cure their own colorblindness\"; but rather \"should be allowed to buy a colorblindness treatment from a vendor of such\" or maybe \"should be allowed to request a prescription of a colorblindness treatment from a doctor who acquires it from a vendor of such.\" And you're already also allowed to do that — in theory. The FDA doesn't care about people buying medical treatments. It cares about companies selling them. Because companies are profit-focused, and the quickest way to money in medicine is to lie. For you to be able to get what you want, there'd have to be an intermediate category that stops just shy of the current \"FDA-approved medical therapy\" category. For a treatment to qualify for this category, it would still have to pass both safety and efficacy studies. The only thing that would be different, is that the FDA wouldn't weigh the safety vs. the efficacy of the treatment at the end; but instead just put an absolute minimum threshold on both safety and efficacy, and leave the prescriber to determine whether the safety risk outweighs the efficacy. But think about that word \"prescriber.\" This would certainly allow people motivated to do this to go to their doctors and ask for such treatments specifically. But it would also enable doctors to choose such therapies for their patients, without the patient being previously aware of the treatment option. And doctors can be bought. (See: doctors prescribing on-patent drugs when equally-effective generics are available; doctors adding on unnecessary adjunct therapies because a sales rep convinced them they should \"give it a try\"; etc.) Do you want to live in a world where there's no ability for regulatory bodies to set a safety:efficacy-ratio threshold on a doctor's ability to prescribe drugs to patients? A world where a (bad) doctor might prescribe a drug that has a huge risk for very little chance of success, and all they have to do is put a consent form in front of the patient for the huge risk — without even having to explain the low-efficacy part? (Honest question. Some people might very well prefer such a world.) reply devilbunny 14 hours agorootparentFWIW, doctors usually specify brands over generics when it actually matters if you have the same supplier every time. Bioavailability of generics is +/- 20% compared to the original branded drug by law in the US. So if you have a legal, generic drug here, it might be a 120% one, and your next refill might be an 80% one - you just got a 1/3 reduction in dose without warning. And if you have seizures? That’s a bad thing. My wife is a neurologist. She deals with this. I’m an anesthesiologist; I just give more as needed because my patients don’t take their own drugs. Almost everything I give is generic, and the exceptions are when those fail. You can make a very good living as a physician in the US without being a whore. Not worth my moral sense to make a few more dollars that won’t go to me directly anyway. Sure as hell not worth it to her; she’s a hospital employee. reply lupire 13 hours agorootparentTrue but it's not ±20%, it's \"90% confidence interval of the mean is within +25%/-40%\" which puts the dose much closer to 100% in practice. https://www.pharmacytimes.com/view/debunking-a-common-pharma... Still, these measured %ages should be much more explicit in the actual delivered drugs and in the range of values that are determined safe. There are plenty og drugs where double dose is fine and the therapeutic dose is \"let's experiment on every patient evey day to see what works\". reply devilbunny 11 hours agorootparentThanks, that's good to know. Antiepileptics are a very special category of drugs in that respect - they really do need to be right on target every time. I know a neurologist with epilepsy - not my wife - and he takes branded Zonegran. Thyroid hormone supplements are similar although the consequences are much lower (you're not going to accidentally kill anyone in a car wreck if your thyroid hormone is low). reply kelnos 13 hours agorootparentprevThat's a really tough one. I just watched the lactose intolerance fix video you linked. Certainly that fabrication process is beyond my ability. But, after more of my own research, maybe I'd decide I want to pay someone else to produce the therapy for me. (I'm not anywhere near as lactose intolerant as the guy in the video, but it's bad enough that I'd be happy to rid myself if this problem.) But should I be able to do that? Am I capable of evaluating the risks? Am I capable of even enumerating the various risk factors? This is not my field at all. Maybe the person making this for me is doing it in an unsafe manner, in a way that could result in dangerous contamination. How would I know? reply RockRobotRock 9 hours agorootparentI was blown away when I first saw the video, but he's completely reckless. This Reddit thread has some interesting insight into just how dangerous it could have been: https://www.reddit.com/r/videos/comments/7x8x3q/dude_uses_ho... reply kelnos 4 hours agorootparentYep, and there it is -- I'd have no idea watching that video the risk mentioned by the top commenter there about the possibility of getting cancer for doing something like that. This is why I'm leaning toward a \"no\" here... I probably shouldn't be legally allowed to pay someone else to make me something like this, regardless of what sort of informed consent form I sign. reply thehappypm 16 hours agorootparentprevIt's the cones in the eye -- they have a defect and make them less receptive to red wavelengths. If you gene-edited them, you could have new cells without that defect. reply coolspot 17 hours agorootparentprevInstead of modifying existing eyes, they should grow additional one/two. reply tejtm 16 hours agoparentprevAnd I want to see UV like our proto progenitors, enough of these little brown birds we can't tell apart. reply jfengel 15 hours agorootparentThe problem there isn't your receptors, which are UV sensitive. The problem is your lens, which filters it out. Remove the lens and you can see UV just fine. Until you get eyeball cancer, that is. UV is mutagenic. That's why you filter it out. People with cataracts get replacement lenses, and some exist that don't filter the UV. I think they're hard to get, what with the whole \"eyeball cancer\" thing. But if I get to my 70s and need replacement lenses (I do have a family history), I'm going to campaign to see if I can spend a couple of decades looking at the pretty birdies. reply capitainenemo 14 hours agorootparentIt's not just cancer right? UV light will damage those sensitive photoreceptor cells, so you would end up seeing less and less of anything over those couple of decades. Even non-UV blue light is hard for the eyes to deal with. Personally I'd just prefer wearing glasses that shift the UV to some other part of the spectrum. reply eternauta3k 14 hours agorootparent> Personally I'd just prefer wearing glasses that shift the UV to some other part of the spectrum. Does this exist? I thought those kinds of non-linear effects only happen at extremely high fields (e.g. with a very fast pulsed laser to concentrate energy in time) reply bongodongobob 14 hours agorootparentNo, at least not without active electronics. reply capitainenemo 12 hours agorootparentFluorescence? reply foota 15 hours agorootparentprevAre you eyes more sensitive to UV than the rest of your body? I guess melanin absorbs UV in the skin, so this serves a similar purpose in the eye? reply Analemma_ 14 hours agorootparentI'm not sure about more sensitive, but I know the immune system (which cleans up a lot of pre-cancerous cells before they become a problem) is much less active in the eyeball, so it probably pays to have extra defense against cell damage. reply lupire 13 hours agorootparentprev\"Sensitivity to light\" is the primary function of the eye. reply delinom 17 hours agoparentprevHave you tried color blindness glasses (e.g. EnChroma)? I am not familiar with them but some reaction videos were pretty heartwarming. reply thehappypm 16 hours agorootparentI have, yes. I can describe it sort of like this: Imagine you're watching a black-and-white movie on an old TV, and it looks kind of washed out. You fiddle with the contrast, and suddenly the movie looks much crisper and with better contrast. You're not actually seeing more colors -- its just grayscale -- but you can optimize it to give you more depth of perception. That's what EnChroma does. It doesn't actually make you see red, but it heightens the contrast to make it stand out more. reply itslennysfault 14 hours agorootparentSo, does it just look like different green? Like still green, but you can tell it apart from \"true\" green? Like wearing them could you pass a colorblind test even though you still can't see red? reply 93po 12 hours agorootparenti can see reds and greens, but very similar shades next to each other, i can't tell. i also can't see either color very well when it's a really small sample size (for example, the green under my power switch on my wireless mouse to show that it's on). the way i think about it is that people who aren't colorblind have a maximum green value that is much higher than mine. the enchroma glasses, which i've tried, effectively make greens more green. i feel like it makes things inaccurately colored, but exaggerates colors enough to be able to better differentiate. i still can't see past my \"maximum green\" value though, which is why those marketing videos of people crying are total bullshit. it just looks like a very saturated instagram filter. it doesn't make me see colors i haven't seen before. i didn't try a color blindness test with them but i should have. the best memory i have of using it is that it was fall time and the leaves on a bush in my yard were turning red, but i had no idea until i put the glasses on, and i could distinctly tell which leaves were turning and which weren't. i liked them and wanted to keep them, but i couldnt justify the $220 price or whatever it was, so i returned them. i want to get another pair some day reply delinom 16 hours agorootparentprevThat's what I imagined it to be, a slight improvement but obviously no miracle. reply tantalor 16 hours agorootparentprevThose videos are fake the glasses don't do very much. https://www.youtube.com/watch?v=RY-NF_7R-pk&t=559 reply yakovsi 16 hours agorootparentColorblind person here. This debunking video is piling on more BS, like oh, colorblind people learned to be amazing at detecting slight color changes! No, we are not. We learned what names are attached to what color, but we see them differently, much poorer (save for blues). Enchroma apparently helps with separation, as parent comment suggests. That being said, the fake dramatic videos are definitely shameful scam, agreed. reply jerf 14 hours agorootparentWhat boggles my mind a bit is that true colorblindness glasses don't seem that difficult to me. Standard red-blue 3d glasses almost do it, it's just you need red-green differentiating glasses. I don't think this would Open the World of Color!, but it would with a bit of practice probably allow you to at least perceive a difference. But the glasses need to be visibly-differently (to a non-color-blind person) tinted. If they look the same, they're not going to work. Just like a \"blue-reducing\" pair of glasses needs to look visibly yellow, or it clearly (in all senses of the term) isn't doing anything. A truly optimal pair would take some sciencing but bashing something prototype-quality with something like https://www.amazon.com/dp/B0928YT83C would be a matter of holding up the cyan-ist of the films up to one eye and the magenta-ist of the films to the other, and looking at some red and green things, concentrating on which eye the object is bright in. reply lupire 13 hours agorootparentRB 3GD glasses remove color to create a different image. I don't see how that would generalize to add color. reply drdeca 10 hours agorootparentIf one removes red while the other removes green, then if something looks dark through the lens that blocks green, and looks bright in the one that doesn't block green (but blocks red), then one could tell that it is green. (and visa versa for something red) Not that this would give the same subjective experience of a person w/o colorblindness seeing red vs seeing green, but I wouldn't be surprised if it allows one to pass r/g colorblindness tests fairly well? reply jerf 9 hours agorootparentprevIt wouldn't. It would subtract, but differently for each eye. Differential subtraction between two eyes is not addition, but it has certain characteristics in common with it. reply achenatx 14 hours agorootparentprevIm red green colorblind. When wearing my enchroma, some things I thought were brown are actually very red. On the other hand green lights lose their green. On a day to day basis I can functionally see red and green, but sometimes when a red or a green is next to a brown I cant distinguish them. And versions of red that regular people might have a hard time distinguishing would be impossible for me to distinguish. reply swozey 16 hours agorootparentprevWell that was eye opening reply yieldcrv 14 hours agorootparentprev> but some reaction videos were pretty heartwarming shameful marketing campaign reply bongodongobob 13 hours agorootparentprevI've tried them, they kind of make reds pop a little more. But imagine this. You put on a pair of pink tinted glasses to fix your vision. Ok great, but everything is tinted now. I don't find it pleasing at all. My normal is my normal, putting tinted glasses on me doesn't make things look better, it looks wrong. reply moduspol 17 hours agoparentprevIf it makes you feel better, red is overrated. reply noman-land 17 hours agorootparentI think it's rated pretty appropriately. reply santoshalper 12 hours agorootparentI think whatever it is rated, by definition, is the appropriate rating. reply AuryGlenz 17 hours agorootparentprevSure. Green though? It’s the best. You know, I’ve never thought of this: does anyone with a type of color blindness consider their “favorite color” to be the ones they can’t distinguish? reply unsupp0rted 16 hours agoprevEvery time I hear about a new medical discovery or treatment, I think \"yeah right, maybe in 15 years\". Well, here we finally are. Our ancestors, and I mean great-grandparents and grandparents, would call us gods. reply qkeast 13 hours agoparentI have profound hearing loss (sensorineural) in both ears, and I've been following gene therapy in all of its fits and starts for almost two decades. I was lucky enough to have _not horrible_ hearing at my youngest, and then lost it progressively up until I rely entirely on lip reading for conversation. I was eligible for cochlear implants probably more than 15 years ago, but I was getting by, and I heard that getting implants would almost certainly preclude any future therapy like this. I'm ecstatic to finally hear an actual \"it worked, in a human\" outcome, and can't wait for something that works for my hearing to reach maturity. reply dotnet00 14 hours agoparentprevIt is interesting to wonder if they'd call us gods in a good way though. Perhaps they might see it as 'playing god' instead. HN being a techie crowd tends to skew this a bit, but it's worth remembering that genetic engineering is still pretty controversial to the average person: https://www.pewresearch.org/internet/2022/03/17/americans-ar... reply dekhn 14 hours agorootparentWhen I was 18 (early 90s) and applying for university, the admissions office asked me what I wanted to do and I said \"play god with DNA\". Sure it sounds arrogant, but I knew that gene therapy would become a thing and if it worked, eventually people would apply it recreationally (by which I mean, to change your phenotype, or your future children's phenotype, for non-disease purposes). I went into that field and spent a couple decades learning how to do it. My only conclusion at the end was that it would only be considered societally acceptable for diseases, not recreation, for the foreseeable future, and even things that seem \"easy\" (like fixing retinitis pigmentosa) are in fact fractally challenging. As much as I would like to have chromatophore tattoos, we're just not in a place where we can justify this (even self-experimentation) because it's really hard to know if your intervention had the effect you desired, and no other effects. reply devilbunny 13 hours agorootparentI’m about your age, but holy hell, if I had been an admissions officer, that would have sold me on you. I’m an anesthesiologist, though, so “likes to play god” sort of goes with the territory. It’s not common, but there are surgeries where you go on heart-lung bypass, chill the patient down to 30 C, and then stop the pumps. No blood flow. The surgeon does the critical part, you turn the pumps back on and warm them up, and then the body takes over again. It is absolutely magic. reply jasonfarnon 10 hours agorootparentprev\"My only conclusion at the end was that it would only be considered societally acceptable for diseases, not recreation\" I strongly doubt this. If technology is there, the path from disease to recreation would be very short once the market is involved. Except at the extremes, whether a condition is a \"disease\" or \"impairment\" can be pretty subjective. reply unsupp0rted 13 hours agorootparentprevThose who suddenly got eyesight or hearing would call us gods in a good way. Everybody has a problem with people \"playing god\" until they get paralyzed or a serious illness. Then \"playing god\" is all they wish for. reply devilbunny 10 hours agorootparentI'm an anesthesiologist, which curiously enough is a lacuna in pretty much every religion that has opinions about medical treatment. Oh, it's not okay to take this drug or that one, but you're totally fine with being in a medically induced coma? Whatevs, bro. reply jimnotgym 14 hours agoparentprevCaveman brains, mediaeval institutions, and godlike technology. What could possibly go wrong. That is a quote I heard somewhere and paraphrased. reply gherkinnn 17 hours agoprevWhat always interests me with stories of people hearing or seeing for the first time in their life, what is it like to gain a new sense? One would think that growing up with a sense is integral to its processing (and filtering) and the sudden introduction of a sense later in life would come with all sorts of side effects. reply burnished 15 hours agoparentI started smoking as a teenager and almost entirely lost my sense of smell. I later spent years working in cafes and bars and did a lot of food and beverage tastings where I used my limited sense of smell to the best of my ability and got pretty good at it. Eventually I met my favorite person, who didn't smoke, so I quit and over the course of the following weeks it felt like I was regaining my sense of smell with owed interest. I found out my city smells like shit and piss, flowers can have powerful aromas you smell from some distance, and some smells had become physically distressing to me. At one point I went to my favorite person's house, commented that it smelled like old grapes when they opened the door, and found out they had burnt a scented candle called champagne dream hours ago. At the height of it I thought I would eventually learn to smell the future. I find I filter more now but I still find a lot of stronger smells to be deeply unpleasant and I don't recall being sensitive like this as a child. I learned to identify some smells and relate the experience to others but my notes tended to be very basic, especially compared to a friend who was working on his level two sommelier test (who was ruthlessly empirical in his epicureanism so I believe wasn't bullshiting the wine snobbery). reply tomaskafka 14 hours agorootparentWow! There is a great gag in latest Star Trek where Spock reveals that Vulcans need to medically suppress their smell near humans because we just stink to their sensitive noses. reply TecoAndJix 14 hours agorootparentHere is another reference from Enterprise with T'pol: https://www.youtube.com/watch?v=tIYsVBaYNSU reply Izkata 13 hours agorootparentIt's a recurring thing in Enterprise, Archer's pet dog Porthos also bothers T'Pol at one point. I'm not sure but I think Enterprise was the one that introduced this aspect of Vulcans. reply lupire 13 hours agorootparentprevBabylon 5 had that with telepaths who sing songs to block out the noise of human thoughts. reply claytonwramsey 14 hours agorootparentprev> At the height of it I thought I would eventually learn to smell the future. Oddly enough, Salman Rushdie’s Midnight’s Children covers this exact feeling almost in the same way as you describe. If you haven’t read it, I definitely recommend it - it’s a long read but also very enojoyable. reply squigz 8 hours agoparentprevI have extreme vision loss, and the idea of gaining full vision has always scared me a little. It seems like such a huge change in my perception of the world would be... jarring. Of course, I still would. It just makes me a bit nervous. reply dclowd9901 3 hours agorootparentThis must be a similar feeling to the feeling of being a parent and the thought of hearing your kid talk for the first time. For some reason, with my kids, it was partly excitement and partly dread. I can’t explain why. It’s something like… it’s such an overwhelming thing to imagine that it’s frightening. reply stronglikedan 17 hours agoparentprevIf I could just finally achieve a true kundalini awakening, then I could tell you. reply toast0 16 hours agoparentprevI had a chronic stuffed nose into like the 2nd grade. So when I started taking steroids for my nose, I was able to really smell for the first time, and started noticing cooking smells. Overall, kind of meh, missing out on smell is mostly not a big deal. Smell is useful to detect odorants added to dangerous gasses and to know when the cooking is done if your smoke detector is too far from the kitchen, and it can help detect spoiled food (but visual inspection usually works too), but like I don't feel like my life was worse before I could smell, and there definitely wasn't a big step change improvement. reply rantymcrant 4 hours agorootparentI think someone going from normal smell ability to no smell ability might feel differently. One of the top joys in my life is eating and a large portion of that experience is smell. Take it away and I'd expect to lose a lot of what I enjoy. Not just as I eat it but even before, the smells of prep. Smelling garlic being fried or the stink of shrimp paste knowing delicious things are arriving soon. The smell of baking bread that makes my mouth water. Etc.... Another example which might seem the opposite for most. I don't personally understand the appeal of stinky tofu (yet). (all over China but most of my experience with it were Taiwan). To me it tastes like a savory tofu dish where someone set a bowl of steaming hot feces on plate next to you while I you eat. In other words, to me, nothing is added by the smell. Take it away and it would be a standard savory tofu dish. But, all of my Taiwanese friends crave this stuff and I think the smell is part of it. reply 1970-01-01 15 hours agoparentprevNYT has an excellent summary on it. Life becomes very hard when they try to use their new sense in any meaningful way: https://www.nytimes.com/2021/06/29/books/review/comnig-to-ou... reply iLoveOncall 14 hours agorootparentThis is not at all a summary, it's a book review and it is based on 4 hand-picked examples. I wouldn't read anything into it. reply 1970-01-01 13 hours agorootparenthttps://en.wikipedia.org/wiki/Molyneux%27s_problem#Current_r... reply iLoveOncall 11 hours agorootparentNot sure what's that supposed to show? n = 5 and the results just 5 days after the first survey were extremely positive. reply noman-land 17 hours agoparentprevThere is an entire class of YouTube video of the first moments that people see color, or hear, and it is absolutely, without question, the most joyous and wonderful class of video out there. Just so many happy tears. Little kids hearing their mother's voice for the first time. Friends seeing color for the first time after being given corrective glasses as a gift. Highly recommended watching. reply delecti 16 hours agorootparentUnfortunately most of those videos about the glasses are basically staged. Some of those companies have gotten in trouble for making false claims. reply HumblyTossed 15 hours agorootparentIt truly is sad if that is the case that those videos are staged, but, anecdotally, I worked in the eye glass business (lab manager) to pay for college back in the day and I assure you, I have personally seen babies see their parents for the first time clearly and there was plenty of tears to go around. reply delecti 14 hours agorootparentYeah, to be clear I was specifically referring to videos of people reacting to the glasses that supposedly let colorblind people see color. I think EnChroma is the big name in that space. I didn't specify \"color correcting\", because that was the only kind of glasses mentioned in the GP comment. Normal glasses that correct refractive vision issues are obviously real, and great, I'm certainly a big fan of mine. And while I've never seen it in person, videos of babies seeing through refractive correction glasses are adorable every time. reply noman-land 9 hours agorootparentprevWow I had no idea. I didn't realize it was possible to become even more cynical about the bullshit people will get up to for a couple bucks. reply jeffwass 16 hours agorootparentprevAs the father of a deaf girl who had cochlear implant surgery a few years ago, I despise these videos for misleadingly representing the reality of the situation for most patients. Those videos you mention are designed to invoke “happy-feelies” to maximise engagement and monetisation, nothing more. The doctors warned us not to expect any reaction in our daughter like in those videos. Even so, I wasn’t prepared for just how difficult my daughter’s post-implantation journey was. It took many months before the implants provided even remotely similar benefit as the tiny amount of benefit her older hearing aids could give. She had to learn to hear all over again from scratch. This affected her self-confidence, her friendships, her schooling, and was a very difficult time for her. Those turn-on videos you mention are overly saccharine to make you feel good while having nothing to do with the reality of the situation for almost all cases. reply kiwijamo 5 hours agorootparentAs someone who has had cochlear implant surgery myself, that is 100% true. Luckily as a member of the Deaf community I knew full well what to expect so I didn't expect to hear straight away and just looked forward to taking my time learning how to hear. Even now I still can't hear 100% (and will never be able to) but that is fine as I still use our national sign language which will always be the language I can communicate best in. It took a bit of adjustment for the hearing members of my family. Even though I kept explaining I won't be able to hear perfectly from day 1, it was still a shock for some who realised the switch on was only the start of an incredibly long journey which I am still on. Ironically, only those who had deep involvement in the Deaf/sign language community understood exactly the reality of cochlear implants -- showing that even with medical advancements, the Deaf community is still an important part of any Deaf person's life and sign language will remain a useful toolkit for all Deaf people. reply noman-land 9 hours agorootparentprevWow I truly had no idea about this. Thanks for sharing your personal experience and setting the record straight. I wish you and your family nothing but positivity and good luck in life. Every day I discover a new way that humanity has disappointed me. reply tombert 17 hours agoprevI hope CRISP and gene therapy are going to be the watershed moment in medicine that they appear to be. I heard a Radiolab about CRISPR like 8 years ago, and I remember thinking \"if this is even half as cool as it sounds, then this is utterly amazing\". It feels like a whole slew of disease will just stop being problems, and I look forward to the results of it. Of course, I'm not a biologist, and I don't really understand any aspects of this stuff, so I have an extremely lay-person understanding of all this, but it seems insanely cool. reply julianlam 16 hours agoprevI'm excited to see possible parallel applications of this therapy for other hearing-loss related gene mutations such as GJB2. From what I can tell from a press release from Regeneron themselves, they're also working on GJB2 and STRC therapies. reply janeerie 14 hours agoparentCan you point me towards more info on GJB2 therapies? This is the cause of my son's hearing loss. reply xyst 12 hours agoprevAs a person that’s not deaf. It’s great to hear scientific breakthroughs can help people experience life with this important sense. Although I think there’s a train of thought that think being deaf is not a genetic deficiency. Would deaf parents raising a child that is not deaf struggle? Would assimilation into the family unit be difficult? Personally, I think the level of isolation as a child from their parents would have a certain degree of impact on mental health. reply lapetitejort 11 hours agoparentThere's some good movies based around deafness: https://en.wikipedia.org/wiki/CODA_(2021_film) https://en.wikipedia.org/wiki/Sound_of_Metal And a bad video game: https://en.wikipedia.org/wiki/The_Quiet_Man_(video_game) reply tnorthcutt 11 hours agorootparentI strongly endorse both of the mentioned movies. They're both excellent. reply jasonfarnon 10 hours agoparentprevWhat isolation? The children just become fluent signing with their parents, just as any child will spontaneously pick up their parents' language. reply squigz 8 hours agorootparentDo you think you'd feel as connected to your family if you could only communicate with them by writing, while you can communicate with everyone else by talking? Yes, there would be a level of isolation. It needn't be extreme, but I can't imagine there would be none reply jpk2f2 6 hours agorootparentIt's really no different from immigrant families where the parents don't speak English, but the children speak both languages. I've known plenty of people and families like that, it was never an issue. reply rantymcrant 4 hours agorootparentI've known plenty of people like that where is was an issue. The parents can't rely on neighbors because they can't communicate. Don't have as many friends. Have trouble at local stores, doctors, agencies, etc... reply SJC_Hacker 5 hours agorootparentprev>Do you think you'd feel as connected to your family if you could only communicate with them by writing, while you can communicate with everyone else by talking? Maybe more connected? If you are using a language that few others know, seems like it would enhance the \"specialness\" reply _whiteCaps_ 11 hours agoparentprevAnecdotally, I know of one hearing person raised by deaf parents, and he's fluent in sign language. There's no isolation between them. reply maratc 17 hours agoprevYuval Harari makes an interesting point in one of his books: that many kinds of these inventions always start as a way to treat people with some kind of a medical condition and bring them into the \"median\" spectrum, but there's no stopping there and the same inventions are then used to bring \"median\" people into the \"super-human\" spectrum. What happens next is that the \"regular\" humans have to compete with these \"super-humans\", for example on a job market, or in Olympics. reply TeMPOraL 15 hours agoparentI can't think of a medically relevant example except maybe stimulants and painkillers, but outside of medicine, this definitely is the case with technology: many inventions have a ratchet effect - they start as gimmicks, but when they reach a critical mass of users, they confer so much competitive benefits that they rapidly spread everywhere, and eventually become required to function. Examples range from agriculture to the Internet; currently, smartphones seem to be hitting this threshold, as more private and public services become designed primarily with smartphones in mind. I found it enlightening to ponder the history of clocks. There was a time nobody needed one; it wasn't actually useful for anything[0], because nothing in agricultural societies happened fast enough to require hour or minute accuracy. Some people eventually found use for more accurate and precise time tracking, then more, then those people realized they're able to coordinate better when they have synced clocks, which made new things possible, and few centuries later, our entire civilization runs on clocks, and it's near-impossible to live without minding what time is it. -- [0] - Use in sea navigation notwithstanding. reply davidgay 7 hours agorootparent> I found it enlightening to ponder the history of clocks. There was a time nobody needed one; it wasn't actually useful for anything[0], because nothing in agricultural societies happened fast enough to require hour or minute accuracy. This seems a rather dubious claim, for something invented and used for more than 3000 years (see sundials and water clocks - https://en.m.wikipedia.org/wiki/Clock). Uses mentioned incidentally in that wikipedia article seem to include religious observances, astronomy, astrology, watches (as in “night watch”). Also, as I understand it, the navigation use wasn’t really practical until precise clocks were developed in the 18th century. reply maratc 14 hours agorootparentprev> those people realized they're able to coordinate better when they have synced clocks, which made new things possible The development of clocks is related to industrialization and opening of plants that need all the workers to be there at e.g. 8 am for the shift to start. If you have to be at the factory at 8 am you'd better know what time it is. It's not that the development of clocks has brought us factories; rather, the development of factories has brought us clocks. reply TeMPOraL 14 hours agorootparentIt's a feedback loop; manufacturing couldn't scale without clocks, and neither transportation - think e.g. train time tables. Once it became possible to keep people in sync down to minutes, it immediately became popular, and those who avoided it were increasingly left behind. FWIW, I'm not saying it's a good development. I'm torn on this. Outside of direct progress, we seem to be running in Red Queen's races quite a lot, as improvements turn into baseline. reply kiba 13 hours agorootparentprevThe relationship is likely bidirectional as opposed to a one way street. reply dotnet00 16 hours agoparentprevYou say that as if this has happened before (\"many kinds of these inventions always start as...\"), but what conditions have actually had this happen? I can't think of anything that started as something to help bring people up to a median level, to then end up bringing median people into a super-human level. Everything we've done so far have all been ultimately limited by the human body, so things that bring people up to the median level ultimately have diminishing returns such that they don't benefit median people so much. reply maratc 16 hours agorootparentThink of e.g. the development of optics that brought us the eyeglasses first, and the telescopic sight on a sniper rifle second. Now imagine an army of median humans that need to fight the army of genetically-modified humans that can see in the dark. reply cush 16 hours agorootparentprevsteroids reply dotnet00 15 hours agorootparentMost, even in the presented example of the olympics aren't having to compete against those on steroids though? reply johnmaguire 15 hours agorootparentIt's hard to know how many people are getting away with cheating. I'm more familiar with it in the context of cycling, where performance-enhancing drugs have long been a thorny issue. > In 2004, 4.6% of the anti-doping samples tested were positive, and that is taking into account that there were many dopers who never tested positive. https://lanternerouge.com/2023/03/26/how-clean-is-cycling-an... reply Octokiddie 16 hours agorootparentprevBreast augmentation could be one case. reply dotnet00 15 hours agorootparentHaha I guess that might technically fit. reply PebblesRox 12 hours agorootparentYou could say the same for plastic surgery in general. Techniques developed to restore injured faces (e.g. of soldiers) are now used for cosmetic purposes. reply johnmaguire 15 hours agorootparentprevImplants, prosthetics, and exo-skeletons? reply dotnet00 15 hours agorootparentWhich implants and prosthetics currently exist that are such that a median person would want them and as a result would become super-human? reply johnmaguire 15 hours agorootparentAs was mentioned in another comment, breast augmentation and other forms of body modification in the context of influencers, broadcasters, actors, models, etc. Prosthetics may not quite be there yet, but do you seriously think they won't get there? Military is actively investing in exoskeleton R&D. Whether it's currently accessible to the median person is sort of besides the point. reply dotnet00 15 hours agorootparentRe-read my comment, I haven't said anything about the future. My point was just that it seems weird to talk about it as if it has already happened. It might happen in the future, but it's worth considering that a powerful prosthetic limb or exoskeleton that is superior to the biological version in every way makes both the disabled person and the perfectly healthy person equally superhuman. reply johnmaguire 15 hours agorootparentI re-read your comment, but I still disagree that the evidence provided by myself and others in response, as well as evident near-term future advancements, are irrelevant to the comment you responded to. > a powerful prosthetic limb or exoskeleton that is superior to the biological version in every way makes both the disabled person and the perfectly healthy person equally superhuman. I think I'm not following, or else failing to see the relevance to this comment. This is essentially what the GP was getting at: these advancements can/will lead to anatomically \"correct\" humans failing to compete. reply dotnet00 15 hours agorootparent>I think I'm not following, or else failing to see the relevance to this comment. This is essentially what the GP was getting at: these advancements can/will lead to anatomically \"correct\" humans failing to compete. I suppose this is just down to interpretation/point of focus. I was focusing on the 'levels' being referenced. A 'perfect' prosthetic allows all people to be able to reach the same level, while the GP was talking about things that bring below median people to median level only, and previously median people to super-human level. If we interpret this to refer to competition against people who aren't using any augmentation, it covers even just the act of practicing. Which in my opinion, kind of defeats their point by making it excessively broad. reply maratc 14 hours agorootparent> it covers even just the act of practicing Imagine that a gene modification is discovered that allows a significant breakthrough in human memory. First, people with Alzheimer would undergo that treatment, to the cheers and applauds of virtually everyone (including me and you). Then the people with a risk of Alzheimer would undergo it, then it would be cheap enough that anybody rich enough could do it. Then imagine that e.g. I undergo that treatment and am now able to memorize all of Wikipedia, GitHub, and Stack Overflow. Then imagine competing against me for a job opening. You can practice all you want but never able to reach that level, because you're limited by genes while I've got rid of that limitation. The Google guys are extremely well-invested in the bio-tech startups, and Larry the Oracle Guy puts all his money into an \"institute for prolonging of human life\". No bonus points will be awarded for the correct answer to the question \"whose life exactly that institute is working to prolong?\" reply jonasdegendt 15 hours agorootparentprevSomeone mentioned steroids, but Epogen is another example. reply amelius 17 hours agoparentprevYou can only switch a gene \"on\" once. So I'd like to see some examples. Or do you mean more people with all their genes switched \"on\" so to speak? reply tonetegeatinst 16 hours agorootparentEpigenetics would like a word. A gene can be expressed but from what I rember from my college classes....the epigenetics can determine if the gene actually works correctly and how effective it is. Think of it as the ability to lower the overall expression of the gene reply qp11 16 hours agoparentprevThe Theory of Bounded Rationality tells us super humans are over rated. There will always be problems you throw in the lap of a super human, that they wont be able to solve cause of lack of time, or cash, or personality or missing info, or conflicting needs, or their belief system etc etc. reply rowanG077 16 hours agoparentprevIsn't that already the case? Genetically the average Olympian is definitely super human compared to the population. Unless we start adding novel genetic data to humans I wouldn't be too worried. reply SJC_Hacker 4 hours agorootparentI wouldn't say superhuman - just better than the \"average\" athlete who trains by maybe 5-10%. But when the difference between finishing first and last in a race is that 5-10%, thats huge. reply vunderba 14 hours agoparentprevI mean this isn't really a novel point, this is true of everything not just biological enhancements, but technological enhancements as well. The privileged will always have first dibs. But historically the wealthy/first adopters help fuel the continued development of said inventions, which helps drive down the price, eventually allowing the larger public to have access to it. reply mparnisari 5 hours agoprevI'm 80% deaf on my left ear due to measles carried by my mom before my birth. I couldn't gather from the article whether this would help my case, but I'm hopeful regardless. reply lifeisjoy 8 hours agoprevAs a hearing person who grew up with deaf family members (my mom & aunt are deaf), I think many deaf people will actively oppose gene therapy for deafness. In the deaf community, there’s a large subsection that actively oppose things like cochlear implants or any other attempts to make deaf people hearing. There’s a few reasons why: First, many deaf people feel an immense pressure to fit into hearing society. I know in my mom’s case her hearing parents refused to teach her sign language because they wanted her to fit in with hearing people. This is a pretty common experience for deaf people and one that many of them hate (my mom included). My suspicion is that to the deaf community things like gene therapy might feel like another way to force them to fit in instead of being themselves. No idea if this is actually how deaf people feel, but it is an educated guess based on what I’ve seen in my family. Second, many in the deaf community don’t view deafness as a disability. I think it’s viewed as a gift of sorts. There’s even a term for it called “deaf gain” (a play on the phrase “hearing loss”). Basically, many deaf people think being deaf is a positive and not something that needs to be cured. So, the idea of curing deafness with things like gene therapy is very controversial in the deaf community. Third, the deaf community has its own culture. For instance, there’s different social norms that are unique to deaf people (e.g. if you want to get someone’s attention you flick a light switch on and off). One of the fears of the deaf community is that if deafness is eliminated then the culture disappears. So, in many deaf circles, things like gene therapy are viewed as an existential crisis. I’ve even heard things like this likened to cultural genocide (which is a bit extreme) because of its threat to their culture. So due to these reasons, I can’t see this treatment actually being popular with the deaf community. There’s already a huge emphasis within deaf families to not use cochlear implants on their kids due to it being viewed as a bad thing. As a result, I think it’s unlikely those same families will opt for gene therapy. I think most of the cases you’ll see of gene therapy being used are deaf kids whose hearing parents decided they need it or with the subset of deaf adults who don’t share the views of others in the deaf community. reply vouaobrasil 7 hours agoparentThis is a truly important perspective. And I wonder if often treatments to make people more \"normal\" are sometimes more pushed by society because one of the prime pressures of modern society is to have everyone homogenized as cogs to advance technology rather than just be a person. I would not be surprised if ADHD and other things are next. One of the prime aims of technology is in fact to erode independent communities because they are one of the few that can speak out against technological development (e.g. the Amish). reply jfoster 7 hours agoprev> “The audiologist played back some of the sounds that she was responding to and they were ridiculously quiet sort of sounds that in the real world wouldn’t catch your attention during a conversation,” Mrs Sandy said. That could be problematic, couldn't it? reply ForrestN 15 hours agoprevAs a disabled person, I admit to being made slightly uncomfortable by the uncritical framing of genetically modified people as \"therapy\" that all people should want. Where is the line between \"gene therapy\" to eliminate differences (such as deafness) and eugenics? If we have statistics that taller people have better outcomes in life, should we do gene therapy to make sure everyone is taller than 6'? How much diversity of human experience is too much? Obviously, there are easy cases: this kind of technique to prevent conditions leading to abject suffering, for example. But, knowing and admiring deaf people makes me unsure about the idea of \"curing\" deafness, for example, as a goal of medicine. reply dotnet00 14 hours agoparentAt the same level, it's worth considering the effect on society. Western society (and increasingly global society) has grown a lot more accommodating for certain disabilities because it is understood that the condition was not the person's choice and cannot be fixed. But if the condition is capable of being cured/managed with no serious side effects, and the cures are easily accessible, what is the right amount of effort society should put into being accommodating? These are all difficult questions, but it feels like we're going to eventually have to put aside our well founded fears over eugenics and confront these serious questions properly. For instance, many places offer the option to test fetuses/parents for markers of serious genetic disease and offer the option to terminate the pregnancy with the argument that the child would either not be viable or would have a horrible quality of life. On one hand this sounds reasonable, on the other hand it's pretty much a level of eugenics. reply legohead 15 hours agoparentprevDesigner babies are fine with me. If I could make a handsome, strong as an ape, genius, healthy baby, I'd do it! I'd do the same for me. If there's a moral sticking point, for me it would be about the cost and privilege it assumes. We still have a very long ways to go before that is figured out...but if we have genius level babies, maybe they can do it for us. reply jasonfarnon 10 hours agorootparentI imagine if people selected for what society wants at a given time you'd have disasterous population level effects. Aren't animals bred to taste pretty messed up in a million other ways? reply spondylosaurus 15 hours agoparentprevAlso disabled, and this is a topic I've been chewing on a lot lately—I started writing up a longer comment but deleted it, lol. What I really want to say is that I have a few problems I would 100% cure in a heartbeat, and a few that I'm less sure about, so I get it. Some disabilities only have one true cure: fix the part of your body that's bad at its job. No amount of accomodation or acceptance is going to mitigate the worst parts of, say, liver disease. But other disabilities have two paths forward: cure the body, or create a world that's more accommodating to people with that disability. Deafness seems like it falls in that category, which is tricky, because both paths have salient points but are also at odds with each other. reply silverquiet 14 hours agorootparentI'm actually in this position a bit. I'm still young(ish) with a serious hip condition that causes me some disability. There are options for replacement that could get me to near full function, but there are drawbacks and the shear fear of surgery and replacing part of my body with metal and plastic. If I was wheelchair-bound, I don't think it would be a hard choice, but I am able to essentially do most of the things I need to do at least as I am. And so I put it off and put it off. reply spondylosaurus 12 hours agorootparentHip problems are brutal, been there before :( Mine (mostly) resolved when I addressed some other underlying issues but I was also seriously considering joint replacement for a bit! The surgery and downtime are no joke, but everyone I've talked to who went through with a replacement was glad they did. I even know at least one guy who now works on his feet all day. Not saying to just take the plunge now, but if you ever do, the outcomes seem pretty damn good. reply silverquiet 10 hours agorootparentYes - it’s a bit ironic that I have an extremely rare condition (Perthes disease) that ultimately may be resolved by a very common surgery. And it is known as one of the best as far as outcomes - the Lancet called it the surgery of the century, so I do at least have some hope there. Good to hear you were able to mostly resolve your issue. Given your name I’m assuming you have ankylosing spondylitis - I certainly see that in joint replacement forums from time to time, often people younger than I at this point who are facing more joint issues than myself. reply spondylosaurus 2 hours agorootparentDamn, I recently learned about Perthes and it does not sound fun. Glad you're managing for now but I hope you get some relief in the future, through surgery or otherwise. And yep, spondylitis with a side of hip bursitis, so Perthes hits close to home. Eventually I figured out I have colitis, started to treat that, and the arthritis got 90% better. Also I learned that some people with really aggressive colitis who go on steroids for years wind up with osteoporosis so bad that they need... hip replacement surgery, lol. It all comes full circle! reply jamiek88 11 hours agorootparentprevDo you have significant pain? reply silverquiet 10 hours agorootparentPain is the main disability; most days I walk well enough that no one can tell I have issues, but the pain can make it hard to concentrate on my work. That said, some days I have little pain and so it is hard to commit to a surgery when I have a string of good days. reply Loocid 9 hours agoparentprev>Where is the line between \"gene therapy\" to eliminate differences (such as deafness) and eugenics? If we have statistics that taller people have better outcomes in life, should we do gene therapy to make sure everyone is taller than 6'? There may be benefits of being 6' over 5' but I dont think that's comparable to deafness. 6' vs 5' is the difference between great hearing and good hearing. The deafness we are talking about is the difference between having legs and not having legs. reply foxyv 14 hours agoparentprev> Where is the line between \"gene therapy\" to eliminate differences (such as deafness) and eugenics? The difference is usually a matter of informed consent. Eugenics tends to be non-consensual. Sterilization or forced birth control for unwanted individuals. Murder of unwanted individuals. Involuntary genetic modification of unwanted individuals will probably pop up eventually. Typically gene therapies are on living, consenting people with all the information to make a choice. It also doesn't usually result in germline modification. The sticky part is when you get to babies and fetuses. Can a mother consent for her fetus? What about germline modification? In-vitro gene therapy? Then you are getting into Brave New World territory. reply skybrian 14 hours agoparentprevHere is a framework for thinking about it: raising population-level concerns and using them to justify laws restricting what children parents can have (or not have) seems like the pro-eugenics side. The reproductive freedom side is to take a laissez-faire attitude on how the human population changes. Let parents choose the children they want to have and it will probably work out. That doesn't make the issues easy. There are some forms of state coercion that people are sympathetic to. For example, in India, there is unfortunately a strong preference for male children, and there are laws to prevent sex selection. This is obviously reducing people's reproductive freedom because there's a state interest in a balanced sex ratio. Another example of state coercion that people are unsympathetic to is China, where the state had an interest in reducing population growth and imposed a one-child policy. Seems like that's eugenics? It's imposing personal hardship for a population-level concern. Along these lines, I'm wary of population-level concerns like \"will deaf people die out.\" What could the state do about it? At the individual parent level, nobody should have to raise a deaf child if they don't want to, when it's unnecessary. But a tough case for the reproductive freedom side is: can deaf parents use prenatal testing to select for deaf children, if that's what they want? That's not a population-level concern, it's personal: specific parents want a deaf child. A lot of people have trouble with that kind of reproductive freedom when they wouldn't have an issue with wanting a boy or girl, because deliberately causing deafness sure seems bad for that child. reply ImJamal 14 hours agoparentprevI think there is an easy line. We know what should occur with certain parts of the body. Ears should be able to hear so when they don't we know there is a problem. There isn't a height in order to function properly or something like that. If somebody is 5 feet or 6 feet they are still capable of having their whole body function. Yes, they may have issues due to their height but their body still works correctly. (Extreme heights, both tall and short, may cause issues and there could be conversations around that, but within the normal range there isn't any sort of function of the body that doesn't work) reply vsuperpower2020 13 hours agorootparentIt's crazy to me that we even need to explain the difference between variation in height and a non-functional organ. I don't know if people are just so open minded that their brains fell out, or if it's some new idea where everyone gets their own personal perception of reality and nothing is real, maaan. reply eklavya 15 hours agoparentprevAm I reading it wrong or are you admiring deafness which shouldn't be cured? Or the deaf people who are admirable? Why should curing deafness not be a goal of medicine? reply sertgert 14 hours agorootparentI'm reading it like the hedonist's treadmill. Why be happy with a 6 figure salary when there's people with 7 figure salaries? Are you content with your current situation, or are you missing a part of what it means to be human by not having better eyesight, better teeth, better hearing? I think what OP was referring to was how rich the lives of the deaf can be, and how discouraging it might be to hear \"y'know, you're not /really/ experiencing life until you can hear\" reply santoshalper 12 hours agoparentprevHere's the problem with slippery slope arguments. You could substitute all medicine and your point would still stand. Do people with a limp need to be \"fixed\" or does it add character? You know who is the right person to decide that? The person receiving the therapy, or if they are a minor, their parents. Nobody else is well equipped to make the decision for them. I suppose I could be wrong, and this could be the start of Gattica, but I highly doubt it. I think far more likely is that over the next few decades, millions of people will be able to hear who otherwise would not have. reply dandanua 15 hours agoparentprevIn the current state of the world ethics is faked. Survival is everything. AI will be used for weapons and for power grab by politicians and billionaires (e.g. through mass manipulation). Gene \"fixing\" will be used by those who can afford it, 100%. Today everyone wants to be better, stronger and smarter. Otherwise you and your offspring (if any) are doomed to stay in lower castes of society for ages. Be sure the top castes will arrange that. reply virissimo 15 hours agoparentprevPerhaps we should have different words for voluntary (choosing to regain your hearing, etc...) and coercive (forced sterilization, etc...) \"eugenics\", since almost all of the negative connotations of the word are (rightly, IMO) attributed only to the latter. reply commandlinefan 15 hours agorootparentThat was my first though, BUT... to play devil's advocate, people choose gender reassignment surgery (including young children) and that choice continues to be very very controversial. reply xXx_uwu_xXx 10 hours agorootparentWhere are young children getting surgery? reply ImJamal 8 hours agorootparentIn the US there are plenty of places where children can get surgeries. This is a small sampling of the locations: The Stanford Children's Hospital [1] will provide surgeries to both minors and adults. It doesn't specify how young the children can be. The University of Illinois [2] will do surgery on people under 18, but doesn't specify how young the children can be. The University of Rochester [3] will do some surgeries for people under 18. It doesn't specify the age, but it is under the \"Adolescent Medicine\" section. Seattle Children's Hospital [4] will provide referrals for gender-affirming surgery to children as young as 9 so long as they have started puberty. [1] https://www.stanfordchildrens.org/en/services/gender/what-to... [2] https://hospital.uillinois.edu/primary-and-specialty-care/su... [3] https://www.urmc.rochester.edu/childrens-hospital/adolescent... [4] https://www.seattlechildrens.org/clinics/gender-clinic/ reply ImJamal 14 hours agorootparentprevThe controversy is due to cutting off perfectly good and functioning body parts. Correcting a body part that is not working correctly isn't all that controversial. reply xXx_uwu_xXx 10 hours agorootparentAre you implying that there'd be no controversy if people were able to swap parts of their bodies with others'? reply jacobgkau 8 hours agorootparentNo, he's implying that someone with a non-functioning body part getting a functioning donor (presumably from someone who just died and has no chance of ever needing or wanting theirs again) would not be controversial. To answer farther into your line of questioning, though, I'd personally consider replacement reproductive systems \"fully functional\" only if they allow having children with one's own DNA. If we were to reach the level where someone could get a sex change and then perform the reproductive role of their new sex with their own DNA (except maybe the sex chromosome itself)-- and especially if they could then swap back in the other direction later-- I actually do think that would eliminate a lot of the implicit concerns that make it controversial. reply ImJamal 8 hours agorootparentprevI'm not sure how you came to that conclusion? If you had a functioning body part and could swap with another person that would still be cutting off a functioning body part. Just because you got another functional body part doesn't change anything. If it becomes possible in the future to just swap body parts with no issues, it would probably be less controversial, but I don't think it would really be accepted. The problem is the current sex change operations are no where close to that. You can't just flip back and forth and have all your parts remain fully functional. There is, of course an exception and that is with organ donation. That however only happens when the person who is removing the functioning part does not need theirs which can happen because they are dead, there is an extra one (kidney), or it will grow back (liver). This, of course, is not the same type of thing as we are talking about. reply ImJamal 14 hours agorootparentprevParents are going to choose these things for their kids though? reply bfrog 10 hours agoprevThis is the kind of amazing news I come here for. And the kind of hard science real life tech that makes lives truly better. Love it. Thanks hacker news! reply poopsmithe 9 hours agoprevCan this treatment be used to treat other diseases? Say for example, people without a functional immune system? reply Aeolun 9 hours agoprevThis is one of those things that make me feel like I’m living in the future. reply tyingq 17 hours agoprevIs \"auditory neuropathy\" something where there isn't any other existing treatment that directly addresses the issue versus compensating for it (like cochlear implants)? reply dotnet00 17 hours agoparentThe 'neuropathy' part is meant to imply that the issue isn't with the hearing mechanism itself, but rather with the part that allows the mechanism to communicate with the brain. In this case, the expression of a specific gene causes the creation of a specific protein which plays a role in the transmission of audio to the brain. This gene was either not present or malfunctioning, so they added the proper gene manually. reply sneilan1 17 hours agoprevIs this an application of CRISPR? reply bglazer 17 hours agoparentNo, this is an adeno-associated virus (AAV) gene insertion. An engineered version of a naturally occurring virus inserts a working copy of the OTOH gene into the genome of cells in the inner ear. EDIT: I was incorrect. It seems that the virus doesn't actually does not insert itself into the genome. Instead it forms a circular DNA loop (episome) in the cell that is like an extra chromosome. The normal DNA reading machinery can make RNA from the circular DNA loop just like it can from a normal chromosome, so you get working proteins. The benefit is that you don't risk inserting the viral DNA into a normal chromosome at a spot in the DNA sequence that would break some other protein. This is the main difference from CRISPR. CRISPR is designed to directly alter genomic DNA. reply m1n1 14 hours agorootparentAnd regular cell-division will include this episome the way it does the regular chromosomes? reply causality0 11 hours agoprev“As a charity, we support families to make informed choices about medical technologies, so that they can give their deaf child the best possible start in life.” A pleasantly measured response. For those in the know, has the deaf community calmed down in recent years? I remember a time when they were radically opposed to anything that smelled like a \"cure\" for something they considered more of an identity than a disability, though I haven't read anything on it in a long time. reply throwaway9911d 16 hours agoprevI'm hopeful we will be able to cure retinitis pigmentosa with CRISPR one day. reply _qua 8 hours agoprevHow are we going to afford all these incredible therapies as a society? They’re life changing and worth it but we’re going to need some cost breakthroughs. reply janniehater 10 hours agoprevgene therapy to fix sequelae from the first gene therapy reply verisimi 14 hours agoprevI'm glad this little girl can hear. But I don't understand how this can possibly work. We can read: > “So basically, we find the inner ear and we open the inner ear and infuse the treatment, in this particular case using a catheter, over 16 minutes,” he said. Genes are in cells, right? It seems like what is being said is that this therapy goes in and alters all the cells. The altered cells then keep the alteration. Whatever it is that creates these cells, also knows to create this new type of cell - cos this is not an ongoing treatment. Anyway explaining whatever technical thing is going on here with the word 'infuse' is a bit simplistic. Can we not have some more detailed information? reply bglazer 12 hours agoparentIt's a virus that adds a bit of genetic code to cells in the ear. The new genetic code is the instructions for the protein that is broken in the the little girl (and other similar patients). The cell can then use the newly introduced DNA to make a protein. This protein restores the link between the auditory sensory cells and the auditory nerve cells. Regarding the word \"infused\", the viral particles are carried in an aqueous solution into the inner ear. The viral particles then do their work on the cells that the solution flows over. So it is literally infused, although that sort of elides the bit about the virus actually delivering the active bit of the treatment: the new gene. reply verisimi 5 hours agorootparentThanks for your answer. Yes, I get that bit you describe, more or less. But also cells die - all cells are new in the human body after 7 years, right? (Or are we saying ear cells don't?) New cells are produced. Whatever creates new ear cells, needs to know to produce them with the update. I know that marrow creates blood cells.... and I'm not sure how replacement ear cells are created, but if it were something like marrow, this creating piece too that makes new cells also needs to be updated, right? I have other questions too - eg does this viral deployment really impact every cell? You'd think that the ones nearest would get done repeatedly, whereas those that are less local would never be updated. Also, if this infusion were to be near reproductive cells, would it also update those cells, so that cells used is reproduction would be updated, and the line would be permanently edited? reply codeulike 45 minutes agorootparentThese particular hair cells dont die, so they think the treatment will persist, see https://news.ycombinator.com/item?id=40313256 reply foobarian 17 hours agoprev [–] This is a pretty insensitive title considering that the deaf community does not consider the lack of hearing a disease. \"Deaf girl's hearing restored...\" perhaps better. reply screye 17 hours agoparentScience doesn't (and shouldn't) care for how people socialize around a disability. I recognize the power and a reclaimed self worth that comes with reframing narratives around deafness. The lack of hearing is a disease. Because diseases are nominatively about being at dis-ease. As long as it's difficult for deaf people to socialize with those who outside the community, it dis-eases them. reply anthony__j 17 hours agorootparentlike someone mentioned in a separate comment, you are conflating disease with disability here. when people become too old to walk without assistance, you wouldn't say that they have a walking disease. reply s_dev 16 minutes agorootparentYes but the arthritis or whatnot that causes the impairment is a disease. reply TeMPOraL 15 hours agorootparentprev> when people become too old to walk without assistance, you wouldn't say that they have a walking disease. Why not? We have some peculiar cultural memes related to accepting the inevitable end of life, which we perhaps should revisit. I see no reason to not consider aging itself as a slow-burn disease, one we'll hopefully cure at some point too. reply xhevahir 13 hours agorootparent\"Peculiar cultural memes\" is an odd way of describing a piece of advice that one encounters so frequently in cultures the world over. reply TeMPOraL 12 hours agorootparentWell, peculiar now, given the understanding of biology we gained over the last 70-ish years. Individually, it still makes sense, but at social level, it stops us from working on fixing the problem. reply Aurornis 17 hours agorootparentprev> like someone mentioned in a separate comment, you are conflating disease with disability here. when people become too old to walk without assistance, you wouldn't say that they have a walking disease. This girl's disease was a genetic auditory neuropathy. They cured her genetic auditory neuropathy. Pedantic arguments about what it's called are missing the point. The person had a specific disease. It was cured. reply anthony__j 17 hours agorootparentto you, this is a pedantic argument. but to millions of Deaf people in the US alone, this is a very important political point. for example, lots of Deaf people who prefer to live without cochlear implants face lots of pressure from people who consider deafness to be a \"disease\" to be \"cured\", when in fact, they feel their most authentic way of living to be something different. in this way, language is significant reply macksd 16 hours agorootparentAnd people who do prefer to live with cochlear implants face pressure from the deaf community itself. You can't win. This was an achievement. A girl who probably saw this as a cure, saw success. Why can't we just be happy for her instead of detracting because others wouldn't make the same choice? reply wizzwizz4 14 hours agorootparent18-month-olds don't usually have opinions on what does or doesn't count as a 'cure': that's a bit too abstract for most of them. Their parents can. reply dotnet00 17 hours agoparentprevHer condition is a genetic disorder, which is a kind of disease. The deaf 'community' consists of many kinds of deafness, some are not diseases, others are. Some of those diseases cannot be cured, others are trivial to cure. In the same way there are many different kinds of cough, some due to diseases, others not, some curable, others not, yet no one pretends it's offensive to talk about curing a cough. This weird excessive sensitivity to language accomplishes nothing. reply anthony__j 16 hours agorootparentim being a little extra in replying here, but only because i used to think like many people in this thread until i met more people living with deaf and blindness, and i realized that i had a big (pun unintended) blind spot in my cultural understanding about disability. being deaf means that you live in a wholly separate world compared to a hearing person, and it changes the way you understand things- its a whole separate culture, as rich and complicated as any other. it does not compare to a cough, and many would find such a comparison very insensitive. for an example, i would check out the movie \"The Sound of Metal\" reply dotnet00 16 hours agorootparentI was expecting this sort of comment, but everyone lives in their own world with their own culture and perspective, especially given the large immigrant populations in the West. Yet, the vast majority of people get by not being offended over phrasings that th",
    "originSummary": [
      "A groundbreaking gene therapy trial in the UK has restored the hearing of an 18-month-old deaf girl, Opal Sandy, due to auditory neuropathy, marking a world-first achievement.",
      "The innovative one-time gene therapy treatment at Addenbrooke's Hospital significantly enhanced Opal's hearing, potentially paving the way for a cure for individuals with similar deafness.",
      "The approach involves introducing a functional gene copy to the ear, enabling communication between ear cells and the hearing nerve, with rapid noticeable improvements reported in Opal's case."
    ],
    "commentSummary": [
      "A gene therapy trial successfully restores hearing in a deaf girl, offering hope for future treatments in genetic hearing loss.",
      "Patients share experiences with surgeries, hearing aids, cochlear implants, and neural implants, sparking a debate on the controversial use of implants within the deaf community.",
      "Discussions revolve around medical advancements, gene editing, societal attitudes towards disabilities, ethics of curing deafness, reproductive freedom, genetic manipulation, and societal impacts of genetic technology advancements."
    ],
    "points": 621,
    "commentCount": 284,
    "retryCount": 0,
    "time": 1715254694
  },
  {
    "id": 40307519,
    "title": "Mapping Hacker News Data: Sentiment Analysis and Trends",
    "originLink": "https://blog.wilsonl.in/hackerverse/",
    "originBody": "Exploring Hacker News by mapping and analyzing 40 million posts and comments for fun The above is a map of all Hacker News posts since its founding, laid semantically i.e. where there should be some relationship between positions and distances. I've been building it and some other interesting stuff over the past few weeks, to play around with text embeddings. Given that HN has a lot of interesting, curated content and exposes all its content programatically, I thought it'd be a fun place to start. A quick primer of embeddings: they are a powerful and cool way to represent something (in this case, text) as a point in a high-dimensional space, which in practical terms just means an array of floats, one for its coordinate in that dimension. The absolute position doesn't really mean much, but their relativity to each other is where much of their usefulness comes in, because \"similar\" things should be nearby, while dissimilar things are far apart. Text embeddings these days often come from language models, given their SOTA understanding of the meaning of text, and it's pretty trivial to generate them given the high-quality open source models and libraries, that are freely accessible to anyone with a CPU or GPU. Going in, my theories for what I could do if I had the embeddings were: Powerful search: given HN's curated high bar of content, I knew there were lots of interesting insights and things I've missed over the years. It'd be cool if I could query something like \"how to communicate well\" and instantly surface the best advice over the years for communicating. Recommendations: it might be possible to build a personalized discovery engine by navigating the latent space of HN content biased towards/away from dis/interest areas. Analysis: there are a lot of opinions on HN. It should be possible to calculate the sentiment and popularity of various topics within the community, find opposing viewpoints, etc. These sounded pretty interesting, so I decided to dive right in. In this blog post, I'll lay out my journey starting from no data and no code, to interactive search, analysis, and spatial visualization tools leveraging millions of HN content, dive into all the interesting diverse problems and solutions that came up along the way, and hopefully give you some indication (and hopefully motivation) of the power and applicability of embeddings in many areas. You may also have better ideas of using the data or the demo that I came up with. I'm also opening up all the data and source code that I built as part of this journey, and invite you to use them to play around, suggest and refine ideas, or kick off your own creative projects and learning journeys. Over 30 million comments and 4 million posts are available to download here, which include metadata (IDs, scores, authors, etc.), embeddings, and texts (including crawled web pages). The code is also completely open source; feel free to fork, open PRs, or raise issues. If you do end up using the code or data, I'd appreciate a reference back to this project and blog post. If you want to jump right to the demo, click here. Otherwise, let's dive in! Fetching items from HN HN has a very simple public API: GET /v0/item/$ITEM_ID.json Host: hacker-news.firebaseio.com Everything is an item, and the response always has the same JSON object structure: { \"by\": \"dhouston\", \"descendants\": 71, \"id\": 8863, \"score\": 111, \"time\": 1175714200, \"title\": \"My YC app: Dropbox - Throw away your USB drive\", \"type\": \"story\", \"url\": \"http://www.getdropbox.com/u/2/screencast.html\" } There's also a maxitem.json API, which gives the largest ID. As of this writing, the max item ID is over 40 million. Even with a very nice and low 10 ms mean response time, this would take over 4 days to crawl, so we need some parallelism. I decided to write a quick service in Node.js to do this. An initial approach with a semaphore and then queueing up the fetch Promises, despite being simple and async, ended up being too slow, with most CPU time being spent in userspace JS code. It's a good reminder that Node.js can handle async I/O pretty well, but it's still fundamentally a single-threaded dynamic language, and those few parts running JS code can still drag down performance. I moved to using the worker threads API and distributed the fetches across all CPUs, which ended up saturating all cores on my machine, mostly spent in kernel space (a good sign). The final code ended up looking something like: new WorkerPool(__filename, cpus().length) .workerTask(\"process\", async (id: number) => { const item = await fetchItem(id); await processItem(item); }) .leader(async (pool) => { let nextId = await getNextIdToResumefrom(); const maxId = await fetchHnMaxId(); let nextIdToCommit = nextId; const idsPendingCommit = new Set(); let flushing = false; const maybeFlushId = async () => { if (flushing) { return; } flushing = true; let didChange = false; while (idsPendingCommit.has(nextIdToCommit)) { idsPendingCommit.delete(nextIdToCommit); nextIdToCommit++; didChange = true; } if (didChange) { await recordNextIdToResumeFrom(nextIdToCommit); } flushing = false; }; const CONCURRENCY = cpus().length * 16; await Promise.all( Array.from({ length: CONCURRENCY }, async () => { while (nextId{ // Run a few times to avoid potential cold start biases. for (let i = 0; i = min_threshold] df_scores = df_comments_relevant.groupby(\"author\").agg({\"sim\": \"sum\"}).reset_index().sort_by(\"sim\", ascending=False)[:20] One important realisation was that pre-filtering is really slow and usually unnecessary compared to post-filtering. By pre-filtering, I mean removing rows before doing similarity matching. This is because you end up needing to also remove those corresponding rows from the embedding matrix, and that can mean having to reconstruct (read: gigabytes of memory copying) the entire matrix or use much slower partially-vectorized computations. It's usually better to just filter the rows after finding the most similar rows i.e. post-filter. Note that a minimum threshold is important, because \"dissimilarity\" can be as high as 0.6, which makes the set of non-relevant items (in this case, users) have high scores due to the size of such sets. In this example, most comments aren't talking about Cloudflare, so any user that has a lot of comments would otherwise dominate this leaderboard just by sheer volume of comments as 100,000 * 0.6 is still higher than 500 * 0.999. Analyzing the entire dataset What can we do with the 30 million comments? Two things I wanted to try to analyze at scale were popularity and sentiment. Could I see how HN feels about something over time, and the impact that major events has on the sentiment? Can I track the growth and fall of various interests and topics, and how they compare against their competition? I don't have sentiment data, but there are lots of high-quality open source sentiment classification models, on HuggingFace, using Transformers. I decided to use the TweetEval model as it was similarly trained on social media content. Tweets are short, and I didn't know how well it'd work on contextualized comments (which I added when generating embeddings), so to keep it aligned with the model I only used the comments themselves without adding any context. A queue was created, the comments were pushed, a GPU cluster spun up to process the jobs, and the results were stored. The model was much smaller, so increasing the batch size was a good idea to get more bang-for-buck from the GPUs. Increasing the batch size uses more VRAM, but decreases the amount of host-GPU memory transfer (which can be the bottleneck given how fast GPUs are) and possibly increases parallelism. It's finicky though, because, at least for transformer-based models, it can sometimes cause memory spikes and OOM errors. This is because the input matrix has to be a \"rectangle\", so all inputs (which are converted into tokens) must be padded to the longest input length to maintain this constraint. If you have a batch where 4 texts are length 1, then the input size and internal state is relatively small. But if you instead have a batch size of 5 and the next text has length 1024, then all those sizes suddenly jump by thousands. I added some basic code to conservatively guess the optimal batch size given the VRAM, but I'm curious if this problem has already been tackled more dynamically, given its implications for efficiency. Once the data was ready, it was time for some number crunching. Let's check out the sentiment of Rust over time: Values below 0 represent the count of negative comments (where confidence of negative sentiment > 0.5) and above 0 represent positive (where confidence of positive sentiment > 0.5); I probably need to polish and clear it up a bit more. Nonetheless, we can see that there's generally a lot of positive sentiment about Rust (which isn't surprising if you've been around on HN). There was a spike in positivity around the 1.0 announcement, which makes sense, and the more negative posts correlated with a lot of negative comments (according to the model). This is similar to how bots measure sentiment on social media and predict the price of stocks; using powerful semantic embeddings would probably beat any keyword- or bag-of-words-based algorithm. I will say, assuming the model is accurate and I did a reasonable job, there seems to be a lot of negative sentiment on HN in general. We can also estimate the popularity of Rust compared to other languages by weighing the score and similarity. Unfortunately, HN does not expose comment scores, so we can't use them. It seems like Rust is doing great, but not as popular as the other languages. Some of the similarity thresholds may need tuning, so I may be wrong here; have a play with it yourself and try various queries and thresholds. Share anything interesting you find! These were very basic demos and analyses of the data available, and I'm sure there are infinitely more ways to slice and dice the data in interesting, insightful, useful, sophisticated ways. I have many more ideas myself, but wanted to open up the code and data sooner so you can build on top of this, either with more ideas and suggestions, or to play with your own research and visualization projects. Big data number crunching with a GPU One last thing before I wrap this long post up. The analysis queries were taking a while (10-30 seconds) to number-crunch for each one, which was annoying when playing around with it. This was on a 32-core machine, so it was not for a lack of horsepower. I was thinking of various ways to index, preprocess, or prepare the data, when it occurred to me that there already exists a powerful device for lots of vectorized number-crunching, and it's why we run all our AI models on it, but it doesn't have to be restricted to those. Fortunately, libraries like CuPy and cuDF exist, which basically have the same API as NumPy and pandas (respectively) but run everything on the GPU, so it was pretty trivial to port over. Now, queries run in hundreds of milliseconds, and life is great. It's so fast I didn't even bother using a built ANN graph. The only tricky thing was loading the data on the GPU. Given how large the matrix of embeddings was (30M x 512), it was critical to manage memory effectively, because it wasn't actually possible to fit anything more than 1x the matrix in either system or video memory. Some key points: Loading in batches can cause a lot of allocations, which can fragment memory, so in reality you may not be able to load in chunks and then concatenate at the end. (Concatenation also requires contiguous memory, which usually means copying into a separate memory location.) If you read the bytes from disk, load into a NumPy array, convert into a CuPy array, and then copy over to the GPU, that's 4 copies, 3 of which are in memory. CuPy seems to need to have the entire matrix in system memory first before it can copy over to the GPU. For example, cupy.asarray(np_matrix) actually creates a copy of np_matrix in system memory first. Ultimately I ended up memory-mapping the matrix on disk, preallocating an uninitialized matrix on the GPU of the same size, then copying over in chunks. This had the benefit of avoiding reading from disk into Python memory first, and using exactly 1x the system RAM and VRAM. Demo You can find the app at hn.wilsonl.in. The main page is the map and search, but you can find the other tools (communities and analysis) by clicking the button in the top right. If you find an interesting community, analysis, etc., feel free to share the URL with others; the query is always stored in the URL. Note that the demo dataset is cut off at around 10 April 2024, so it contains recent but not live posts and comments. What's next There is much more I wanted to explore, learn, build, but I did not get the time. Some ideas I'm thinking of going into: Live data that is continuously kept up to date. Deep learning powered recommendations system, a StumbleUpon over the curated HN web. Improving search results by training a reranker. Interesting \"paths\" and \"journeys\" along the map. Analyzing users more: who are the most similar/opposite to each other? Who is the most expert in various niches? … However, I'm more interested in hearing from the community. What do you want to see? How useful were these tools? What were shortcomings or things I overlooked? What other cool ideas do you have? Share any thoughts, feedback, interesting findings, complaints—there's likely a lot more potential with these data and tools, and I'm hoping that, by opening it up, there's some interested people who will push this further than I can alone. If there's any interest in diving deeper or clarifying any aspect of this project, let me know, I'd be happy to. Once again, you can find all the data and code on GitHub. If you managed to make it all the way here, thanks for reading!",
    "commentLink": "https://news.ycombinator.com/item?id=40307519",
    "commentBody": "Exploring HN by mapping and analyzing 40M posts and comments for fun (blog.wilsonl.in)431 points by wilsonzlin 21 hours agohidepastfavorite133 comments abe94 18 hours agoThis is impressive work, especially for a one man show! One thing that stood out to me was the graph of the sentiment analysis over time, I hadn't seen something like that before and it was interesting to see it for Rust. What were the most positive topics over time? And were there topics that saw very sudden drops? I also found this sentence interesting, as it rings true to me about social media \"there seems to be a lot of negative sentiment on HN in general.\" It would be cool to see a comparison of sentiment across social media platforms and across time! reply wilsonzlin 18 hours agoparentThanks! Yeah I'd like to dive deeper into the sentiment aspect. As you say it'd be interesting to see some overview, instead of specific queries. The negative sentiment stood out to me mostly because I was expecting a more \"clear-cut\" sentiment graph: largely neutral-positive, with spikes in the positive direction around positive posts and negative around negative posts. However, for almost all my queries, the sentiment was almost always negative. Even positive posts apparently attracted a lot of negativity (according to the model and my approach, both of which could be wrong). It's something I'd like to dive deeper into, perhaps in a future blog post. reply dylan604 13 hours agorootparentThe sentiment issue is a curious one to me. For example, a lot of humans I interact with that are not devs take my direct questioning or critical responses to be \"negative\" when there is no negative intent at all. Pointing out something doesn't work or anything that the dev community encounters on a daily basis isn't an immediate negative sentiment but just pointing out the issues. Is it a meme-like helicopter parent constantly doling out praise positive so that anything differing shows negativity? Not every piece of art needs to be hung on the fridge door, and providing constructive criticism for improvement is oh so often framed as negative. That does the world no favors. Essentially, I'm not familiar with HuggingFace or any models in this regard. But if they are trained from the socials, then it seems skewed from the start to me. Also, fully aware that this comment will probably be viewed as negative based on stated assumptions. edit: reading further down the comments, clearly I'm not the first with these sentiments. reply uyzstvqs 35 minutes agorootparentSpeaking from experience, debate is easily misread as negative arguing by outsiders, even though all involved parties are enjoying challenging each other's ideas. reply wilsonzlin 7 hours agorootparentprevYou may be right, a more tailored classifier for HN comments specifically may be more accurate. It'd be interesting to consider the classes: would it still be simply positive/negative? Perhaps constructive/unconstructive? Usefulness? Something more along the lines of HN guidelines? reply flawsofar 10 hours agorootparentprevEvery helicopter gets a trophy reply dylan604 10 hours agorootparentwait, the parents get a trophy? reply luke-stanley 17 hours agorootparentprevI did something related for my ChillTranslator project for translating spicy HN comments to calm variations which has a GGUF model that runs easily and quickly but it's early days. I did it with a much smaller set of data, using LLM's to make calm variations and an algo to pick the closest least spicy one to make the synthetic training data then used Phi 2. I used Detoxify then OpenAI's sentiment analysis is free, I use that to verify Detoxify has correctly identified spicy comments then generate a calm pair. I do worry that HN could implode / degrade if there is not able to be a good balance for the comments and posts that people come here for. Maybe I can use your sentiment data to mine faster and generate more pairs. I've only done an initial end-to-end test so far (which works!). The model, so far is not as high quality as I'd like but I've not used Phi 3 on it yet and I've only used a very small fine-tune dataset so far. File is here though: https://huggingface.co/lukestanley/ChillTranslator I've had no feedback from anyone on it though I did have a 404 in my Show HN post! reply abakker 14 hours agorootparentprevits so interesting that in Likert scale surveys, I tend to see huge positivity bias/agreement bias, but comments tend to be critical/negative. I think there is something related to the format of feedback that skews the graph in general. On HN, my theory is that positivity is the upvotes, and negativity/criticality is the discussion. Personally, my contribution to your effort is that I would love to see a tool that could do this analysis for me over a dataset/corpus of my choosing. The code is nice, but it is a bit beyond me to follow in your footsteps. reply al_hag 6 hours agorootparentprevIt will be a deep dive into the most essential of HN staples, the nitpick reply walterbell 17 hours agorootparentprevGreat work! Would you consider adding support for search-via-url, e.g. https://hn.wilsonl.in/?q=sentiment+analysis. It would enable sharing and bookmarks of stable queries. reply wilsonzlin 17 hours agorootparentThanks for the suggestion, I've just added the feature: https://hn.wilsonl.in/s/sentiment%20analysis reply deadbabe 17 hours agorootparentprevAnecdotally, I think anyone who reads HN for a while will realize it to be a negative, cynical place. Posts written in sweet syrupy tones wouldn’t do well here, and jokes are in short supply or outright banned. Most people here also seem to be men. There’s always someone shooting you down. And after a while, you start to shoot back. reply xanderlewis 16 hours agorootparent(Without wanting to sound negative or cynical) I don’t think it is, but maybe I haven’t been here long enough to notice. It skews towards technical and science and technology-minded people, which makes it automatically a bit ‘cynical’, but I feel like 95% of commenters are doing so at least in good faith. The same cannot be said of many comparable discussion forums or social media websites. Jokes are also not banned; I see plenty on here. Low-effort ones and chains of unfunny wordplay or banter seem to be frowned upon though. And that makes it cleaner. reply sethammons 15 hours agorootparentI've been here a hot minute and I agree with you. Lots of good faith. Lots of personal anecdotes presumably anchored in experience. Some jokes are really funny, just not reddit-style. Similarly, no slashdot quips generally, such as \"first post\" or \"i, for one, welcome our new HN sentiment mapping robot overlords.\" Sometimes things get downvoted that shouldn't, but most of the flags I see are well deserved, and I vouch for ones that I think are not flag-worthy reply goles 14 hours agorootparentI wonder how much of a persons impression of this is formed by their browsing habits. As a parent comment mentions big threads can be a bit of a mess but usually only for the first couple of hours. Comments made in the spirit of HN tend to bubble up and off-topic, rude comments and bad jokes tend to percolate down over the course of hours. Also a number of threads that tend to spiral get manually detached which takes time to go clean up. Someone who isn't somewhat familiar with how HN works that is consistently early to stories that attract a lot of comments is reading an almost entirely different site than someone who just catches up at the end of the day. reply fragmede 12 hours agorootparentsome of the more negative threads will get flagged and detached and by the end of the day a casual browse through the comments isn't even going to come across them. eg something about the situation in the middle east is going to attract a lot of attention. reply flir 12 hours agorootparentprevI think it's the engineering mindset. You're always trying to figure out what's wrong with an idea, because you might be the poor bastard that ends up having to build it. Less costly all round if you can identify the flaw now, not halfway through sprint 7. After a while it bleeds into everything you do. reply darby_eight 13 hours agorootparentprev> Anecdotally, I think anyone who reads HN for a while will realize it to be a negative, cynical place. I don't think this is particularly unique to HN. Anonymous forums tend to attract contrarian assholes. Perhaps this place is more, erm, poorly socially-adapted to the general population, but I don't see it as very far outside the norm outside of the average wealth of the posters. reply chiefalchemist 8 hours agorootparentprev> Anecdotally, I think anyone who reads HN for a while will realize it to be a negative, cynical place. Sure, sometimes. But usually it's Truth seeking > group thinking There's a fine line between critical and cynical. Sometimes that line gets crossed. Sometimes the ambiguity of text-only comms clouds the water. reply holoduke 12 hours agorootparentprevReally? Mmm i think hn is a place with on avarage above intelligent people. People who understand that their opinion is not the only one. I rarely have issues with people here. Might be also because we are all in the same bubble here. reply beeboobaa3 16 hours agorootparentprevnext [8 more] [flagged] Karrot_Kream 15 hours agorootparentLol what a typical comment for today's HN. Condescending (\"just plain wrong\") with a jab (\"this isn't a hugbox\") placed in just to remind you that not only are you perceived to be wrong but you've provoked anger. No proof to provoke the jab, no feedback to help fix what you perceive as wrong sentiment analysis. Just thoughtless condescension and anger. Why is the sentiment wrong? Is this a data analysis trap the OP fell into? Nah let's insult the OP instead. In my experience having run a bunch of different sentiment models on HN comments, HN comments tend to place around neutral to slightly negative as a whole, even when I perceive the thread to be okay. However I've noticed a huge bump in negative sentiment on large HN threads. I generally find that absolute sentiment doesn't work in most corpuses because the model reflects its training set's sentiment labels. I generally find relative sentiment to be a lot more useful. I have yet to do a temporal sentiment analysis on HN but I have a suspicion that it's gotten more negative over time. I agree with another poster that I think HN needs to be careful to not become so negative that it just becomes an anger echo. Relative sentiment on this site between topics is something I've done and the obvious results show. Crypto threads are by-and-large negative, most political and news related threads are also highly negative. reply the_sleaze_ 15 hours agorootparentCynicism is perceived as more intelligent [0]. I personally find the HN brand of discussion to be difficult to bs my way into. But no matter your level of competency you can always find something to criticize and feel you've contributed. I wonder if academia or even \"more intelligent\" discussion in general would be counted as more negative. https://journals.sagepub.com/doi/pdf/10.1177/014616721878319... reply importantbrian 15 hours agorootparentAs someone who is not an academic myself, but likes to listen to podcasts where academics discuss issues with each other, I often find that the conversations feel contentious, and sometimes they are, but the vast majority of the time the academics themselves feel like they're having a perfectly cordial and productive conversation. So I do think there is something to the idea that academic discussion comes across as being negative. reply sdwr 14 hours agorootparentprevHN definitely has a negative valence. Sure, there's the 20% of comments that are outright rude, or tie everything back to their pet grievance (job satisfaction, government surveillance, the existence of JS). But beyond that, the technical conversation has a negative, critical edge. A lot of comments come from the angle \"You did something wrong by...\", or only reply to correct. There are still golden comments, and most personal anecdotes are treated respectfully, but it makes for an intimidating environment. reply beeboobaa3 14 hours agorootparentprevWhoosh, I was making a point by styling my comment in a way that would be perceived as negative by sentiment analysis. Good job doing a whole psychoanalysis based on what's basically a joke, though. reply Karrot_Kream 13 hours agorootparentHeh did I miss the joke? That was a whoops indeed! Sentiment is hard on the internet ;) > Good job doing a whole psychoanalysis based on what's basically a joke, though. Guess there's still some work to be done on that positive sentiment replying eh? :) reply beeboobaa3 13 hours agorootparentThat one was intentional ;) reply walterbell 17 hours agoparentprev> sentiment across social media platforms and across time! Also time zones and weekday/weekend. reply kcorbitt 13 hours agoparentprevI actually did a blog post a few months ago where I analyzed HN commenter sentiment across AI, blockchain, remote work and Rust. The final graph at the very end of the post is the relevant one on this topic! https://openpipe.ai/blog/hn-ai-crypto reply abe94 3 hours agorootparentthanks, the sentiment in these graphs seem more positive in comparison. Did you run the sentiment on the whole corpus? What did that look like? reply moneywoes 5 hours agoparentprevCrypto i imagine is in that bucket reply gieksosz 7 hours agoparentprevHN is a pretty toxic place indeed. reply taco-hands 7 hours agorootparentPerhaps... it can be toxic if you dip into the comments sometimes... Otherwise the content and links are the stuff of gold! reply swatcoder 6 hours agorootparentprevHow did you get from negative sentiment to toxicity? Are those the same to you? It may be a cultural thing, but I think many people see negative sentiment as a constructive tool and a demonstration of trust and respect among people who recognize each others as robust and capable peers. Avoiding it is something you do with people who you believe need special delicacy: whether because they've told you so, because they intimidate you, or because you sense something pitiable and fragile about them. If you can trust that it's given in good faith, and by the guidelines of HN you are asked to, negative sentiment should be seen as an expression that someone thinks you're a fully capable adult and peer. Personally, I deeply appreciate that it's generally so comfortably shared and received here and would never include \"toxicity\" in one of my critiques of HN. It's a surprising thing to read someone say! (Unless you're thinking of the nastiness that can surface on flamewar topics, but there are numerous means by which those get downranked and displaced, and they're otherwise sparse and easy to avoid.) reply Swizec 7 hours agorootparentprev> HN is a pretty toxic place indeed This may be a personal style difference, but I find HN to be the least toxic of all social media I’ve tried. LinkedIn would be my example of ultra toxicity – the aggressive positivity there is unbearable. At least on HN people tell you what they think and even use a constructive decently argumented approach to doing so. HN to me feels like a good technical discussion where people tear apart ideas instead of each other. But yeah if you put a lot of ego into your ideas, HN must be an awful place to visit. reply rossant 2 hours agorootparentI agree, HN is much less toxic than about any other place on the internet. reply necovek 13 hours agoparentprevIt's really unfortunate the HN API does not provide votes on comments: I wonder if and how sentiment analysis would change if they were weighted by votes/downvotes? My unsupported take is that engineers are mostly critical, but will +1 positive feedback instead of repeating it, as they might for critism :) reply CuriouslyC 19 hours agoprevGood example of data engineering/MLops for people who aren't familiar. I'd suggest using HDBScan to generate hierarchical clusters for the points, then use a model to generate names for interior clusters. That'll make it easy to explore topics out to the leaves, as you can just pop up refinements based on the connectivity to the current node using the summary names. The groups need more distinct coloring, which I think having clusters could help with. The individual article text size should depend on how important or relevant the article is, either in general or based on the current search. If you had more interior cluster summaries that'd also help cut down on some of the text clutter, as you could replace multiple posts with a group summary until more zoomed in. reply zetazzed 5 hours agoparentFor folks with GPUs, note that HDBscan is very optimized in cuML (https://docs.rapids.ai/api/cuml/stable/api/#clustering / https://developer.nvidia.com/blog/faster-hdbscan-soft-cluste...). reply jszymborski 5 hours agorootparentOoo thanks for this reply wilsonzlin 18 hours agoparentprevThanks for the great pointers! I didn't get the time to look into hierarchical clustering unfortunately but it's on my TODO list. Your comment about making the map clearer is great and something I think there's a lot of low-hanging approaches for improving. Another thing for the TODO list :) reply rantymcrant 6 hours agoprevI'd like to see an analysis of the rise of self promotion on HN. I define self promotion on HN as a \"I ...\" post vs \"Something ...\" Examples from the top 100 right now * \"Exploring HN by mapping and analyzing 40M posts and comments for fun\" * \"Browser-based knitting (pattern) software\" These are not self promotional titles. The subjects are the exploration and the software respectively. * \"I built a non-linear UI for ChatGPT\" * \"I created 3,800+ Open Source React Icons\" These are self promotional titles. The subject of each is \"I\" My own simple check just via algolia search results checking for titles that start with \"I\" gave these results for years starting April 1st. Graphed divided by the total number of results for that year 2023 **************************************** 2022 *********************************** 2021 *************************** 2020 ************************************** 2019 ************************* 2018 ************* 2017 ******* 2016 ********** 2015 ******** 2014 ************ 2013 ********************* 2012 ***************** 2011 ********* 2010 *** I feel like maybe I grew up in a time when generally, self promotion was considered a bad character trait. Your actions are supposed to be what promotes you, calling attention to them is not but I feel that culture is changing. I wonder if the rise in self promotion (assuming there is a rise) has to do with social media etc... I perceive a similar rise on Youtube but I have no data, just a feeling from the number of youtube recommendations for videos of \"I.....\" reply Thorrez 5 hours agoparentYour definition of self promotion is a bit different from what I usually think. I usually consider self promotion to be someone promoting something that that same person did. Both of your non-self-promotion examples would be self promotion under my definition. So what you consider to be self promotion vs non-self-promotion, I consider to be self promotion with a title that very clearly indicates that vs self promotion with a title that less clearly indicates that. However, the \"Show HN\" phrase is only used for self promotion I think, so even without the \"I\", anyone familiar with the convention will know it's self promotion. reply rantymcrant 5 hours agorootparent> However, the \"Show HN\" phrase is only used for self promotion I think, so even without the \"I\", anyone familiar with the convention will know it's self promotion. I think that's an extremely cynical view though a common one. I've never thought of \"Show HN\" as self promotion if it doesn't include \"I\" unless I go through to the actual product/library/post and find it full of self promotion. I agree with you that a post that doesn't include \"I\" can be self promotion but I don't think it always is even if the person made/worked on it. \"XYZ and LLM library in rust\" to me is informational. It's point is, more often than not, to inform people of something they might get use out of. I know that's true when I've posted something like that. It's meaning is \"here's a useful resource that was just created\". Sure I get pleasure from knowing I helped people with something but I'm not trying to promote myself, I'm trying to promote the library/post/info. \"I made an LLM Library in rust\" to me is self promotional. It might be useful to others but it's intent was clearly self promotion given the subject is \"I\", not the library/post/product. reply Thorrez 4 hours agorootparent>Show HN is for sharing your personal work and has special rules. https://news.ycombinator.com/newsfaq.html reply satvikpendem 4 hours agorootparentprevShow HN is defined in the rules (as the sibling comment quotes) as something someone made to be shared, ie self promotion, regardless of whether they used \"I\" in the title. Your definition seems more arbitrary than what Hacker News itself intends. reply wodenokoto 4 hours agoparentprevAll show HN has to be created by the author, so I’m not sure what is self promoting about making the implicit explicit. They are all “look, I made something cool, what do you think?” reply amitlevy49 5 hours agoparentprevThis is talked about a lot in Einstein's Walter Isaacson biography, so people have been observing this trend for a long time (e.g the Germans accusing Einstein of doing self promotion, the US having celebrity culture in contrast), maybe it's cyclical reply ComputerGuru 6 hours agoprevAmazing work, I'm impressed by the scope of your project! I must say though, is it jina or bge-3/flag - the embeddings (and tokenizer?) do not do a good job on tech topics. It's fine for natural words, but searching for tech concepts like \"xaml\", \"simd\", etc cause it fall back to tokenizing the inputs and tries to grab similar sounding words. Also, just some constructive feedback, if there were some way to stop it from showing the same \"hn leaderboard\" of results when there are no results because a topic is too niche would be nice. I get a lot of \"Stephen Hawking has died\" when searching for words the embeddings aren't familiar with. Edit: I'm not so sure how well the sentiment analysis is working. I had the feeling that there was too much negative sentiment that didn't match up to reality, so I tried looking up things HN would feel overwhelmingly positive about like \"Mr Rogers\", I mean, who could feel negatively about him? The results show some serious negative spikes. Look up \"Carter\" and there's a massive negative peak associated with the passing of Rosalynn Carter. It was an HN submission talking about all the wonderful things the Carters did. Also, I think the \"popularity over time\" needs to be scaled by the median number of votes a story got that month/year, because the trend lines just go up and up if you plot strictly the number of posts. Look at the popularity of \"diesel\" and you'll see what I mean - this is a term that peaked ten years ago! Or perhaps it should be some sort of keyword incidence rate or number of items with a cosine similarity index of less than x from the query rather than post score, maybe? Edit2: The dynamic \"click a post to remove and recalculate similarity threshold\" is awesome. reply oersted 19 hours agoprevHere's a great tool that does almost exactly the same thing for any dataset: https://github.com/enjalot/latent-scope Obviously the scale of OP's project adds a lot of interesting complexity, this tool cannot handle that, but it's great for medium-sized datasets. reply replete 17 hours agoprevI think this is easily the coolest post I've seen on HN this year reply seanlinehan 20 hours agoprevIt was not obvious at first glance to me, but the actual app is here: https://hn.wilsonl.in/ reply uncertainrhymes 18 hours agoparentI'm curious if the link to the landing page was intentionally near the end. Only the people who actually read it would go to the site. (That's not a dig, I think it's a good idea.) reply bravura 16 hours agoparentprev1) it doesn’t appear search links are shareable or have the query terms are in it 2) are you embedding the search phrases word by word? And using the same model as the documents used? Because I searched for „lead generation“ which any decent non-unigram embedding should understand, but I got results for lead poisoning. reply oschvr 15 hours agoparentprevI found me and my post there ! Nice reply celltalk 3 hours agoprevIt would be cool to see yearly changes of UMAP, by different years or the overall evolution in pseudotime on the embedding. Such a cool side project! reply oersted 19 hours agoprevThis is a surprisingly big endeavour for what looks like an exploratory hobby project. Not to minimize the achievement, very cool, I'm just surprised by how much was invested into it. They used 150 GPUs and developed two custom systems (db-rpc and queued) for inter-server communication, and this was just to compute the embeddings, there's a lot of other work and computation surrounding it. I'm curious about the context of the project, and how someone gets this kind of funding and time for such research. PS: Having done a lot of similar work professionally (mapping academic paper and patent landscapes), I'm not sure if 150 GPUs were really needed. If you end up just projecting to 2D and clustering, I think that traditional methods like bag-of-words and/or topic modelling would be much easier and cheaper, and the difference in quality would be unnoticeable. You can also use author and comment-thread graphs for similar results. reply wilsonzlin 19 hours agoparentHey, thanks for the kind words. I wasn't able to mention the costs in the post (might follow up in the future) but it was in the hundreds of dollars, so was reasonably accessible as a hobby project. The GPUs were surprisingly cheap, and was only scaled up mostly because I was impatient :) --- the entire cluster only ran for a few hours. Do you have any links to your work? They sound interesting and I'd like to read more about them. reply oersted 19 hours agorootparent\"Hundreds of dollars\" sounds a bit painful as an EU engineer and entrepreneur :), but I guess it's all relative. We would think twice about investing this much manpower and compute for such an exploratory project even in a commercial setting if it was not directly funded by a client. But your technical skill is obvious and very impressive. If you want to read more, my old bachelor's thesis is somewhat related, from when we only had word embeddings and document embeddings were quite experimental still: https://ad-publications.cs.uni-freiburg.de/theses/Bachelor_J... I've done a lot follow-up work in my startup Scitodate, which includes large-scale graph and embedding analysis, but we haven't published most of it for now. reply wilsonzlin 18 hours agorootparentThanks for sharing, I'll have a read, looks very relevant and interesting! reply b800h 11 hours agorootparentprevAs an EU-based engineer, you wouldn't do this, it's a massive GDPR violation (failure to notify data subjects of data processing), which does actually have extraterritoriality, although I somehow doubt that the information commissioners are going to be coming after OP. reply PaulHoule 19 hours agoparentprev(1) Definitely you could use a cheaper embedding and still get pretty good results (2) I apply classical ML (say probability calibrated SVM) to embeddings like that and get good results for classification and clustering at speeds over 100x fine-tuning an LLM. reply Karrot_Kream 9 hours agorootparentI didn't think the OP used LLMs? They did use a BERT based sentiment classifier but that's not an LLM. My HN recommender works fine just using decision trees and XGBoost FWIW. I'll bet SVM would work great too. reply alchemist1e9 19 hours agoparentprevThe author is definitely very skilled. I find it interesting they submit posts on HN but haven’t commented since 2018! And then embarked on this project. As far as funding/time, one possibility is they are between endeavors/employment and it’s self funded as they have had a successful career or business financially. They were very efficient at the GPU utilization so it probably didn’t cost that much. reply wilsonzlin 19 hours agorootparentThanks! Haha yeah I'm trying to get into the habit of writing about and sharing the random projects I do more often. And yeah the cost was surprisingly low (in the hundreds of dollars), so it was pretty accessible as a hobby project. reply jxy 14 hours agoprev> We can see that in this case, where perhaps the X axis represents \"more cat\" and Y axis \"more dog\", using the euclidean distance (i.e. physical distance length), a pitbull is somehow more similar to a Siamese cat than a \"dog\", whereas intuitively we'd expect the opposite. The fact that a pitbull is \"very dog\" somehow makes it closer to a \"very cat\". Instead, if we take the angle distance between lines (i.e. cosine distance, or 1 minus angle), the world makes sense again. Typically the vectors are normalized, instead of what's shown in this demonstration. When using normalized vectors, the euclidean distance measures the distance between the two end points of the respective vectors. While the cosine distance measures the length of one vector projected onto the other. reply GeneralMayhem 12 hours agoparentThe issue with normalization is that you lose a degree of freedom - which when you're visualizing, effectively means losing a dimension. Normalized 2d vectors are really just 1d vectors; if you want to show a 2d relationship, now you have to use 3d vectors (so that you have 2 degrees of freedom again). reply minimaxir 17 hours agoprevA modern recommendation for UMAP is Parametric UMAP (https://umap-learn.readthedocs.io/en/latest/parametric_umap....), which instead trains a small Keras MLP to perform the dimensionality reduction down to 2D by minimizing the UMAP loss. The advantage is that this model is small and can be saved and reused to predict on unknown new data (a traditionally trained UMAP model is large), and training is theoetically much faster because GPUs are GPUs. The downside is that the implementation in the Python UMAP package isn't great and creates/pushes the whole expanded node/edge dataset to the GPU, which means you can only train it on about 100k embeddings before going OOM. The UMAP -> HDBSCAN -> AI cluster labeling pipeline that's all unsupervised is so useful that I'm tempted to figure out a more scalable implementation of Parametric UMAP. reply bravura 17 hours agoparentFrom a quick glance, it appears that it's because the implementation pushes the entire graph (all edges) to the GPU. Sampling of edges during training could alleviate this. reply minimaxir 16 hours agorootparentIndeed, TensorFlow likes pushing everything to the GPU by default whereas many PyTorch DL implementations encourage feeding data from the CPU to the GPU as needed with a DataLoader. There have been attempts at a PyTorch port of Parametric UMAP (https://github.com/lmcinnes/umap/issues/580) but nothing as good. reply bravura 16 hours agorootparentLooks like there is a little motion on this topic: https://github.com/lmcinnes/umap/pull/1103 reply Der_Einzige 17 hours agoparentprevIt exists in cuML with a fast GPU implementation. Not sure why cuMl is so poorly known though… reply minimaxir 16 hours agorootparentI'll give that a look: the feature set of GPU-accelerated ops seems just up my alley for this pipeline: https://github.com/rapidsai/cuml EDIT: looking through the docs it's just GPU-acceletated UMAP, not a parametric UMAP which trains a NN model. That's easy to work around though by training a new NN model to predict the reduced dimensionality values and minimizing rMSE. reply minimaxir 13 hours agorootparentTested it out and the UMAP implementation with this library is very very fast compared to Parametric UMAP: running it on 100k embeddings took about 7 seconds when the same pipeline on the same GPU took about a half-hour. I will definitely be playing around with it more. reply lmeyerov 11 hours agorootparentYeah we advise Graphistry users to keep GPU umap training sets toThere's also a maxitem.json API, which gives the largest ID. As of this writing, the max item ID is over 40 million. Even with a very nice and low 10 ms mean response time, this would take over 4 days to crawl, so we need some parallelism. > I've exported the HN crawler [1] (in TypeScript) to its own project, if you're ever in need to fetch HN items. Fetching and parsing linked URLs' HTML for metadata and text > For text posts and comments, the answer is simple. However, for the vast majority of link posts, this would mean crawling those pages being linked to. So I wrote up a quick Rust service [2] to fetch the URLs linked to and parse the HTML for metadata (title, picture, author, etc.) and text. This was CPU-intensive so an initial Node.js-based version was 10x slower and a Rust rewrite was worthwhile. Fortunately, other than that, it was mostly smooth and painless, likely because HN links are pretty good (responsive servers, non-pathological HTML, etc.). Recovering missing/dead links > A lot of content even on Hacker News suffers from the well-known link rot: around 200K resulted in a 404, DNS lookup failure, or connection timeout, which is a sizable \"hole\" in the dataset that would be nice to mend. Fortunately, the Internet Archive has an API that we can use to use to programmatically fetch archived copies of these pages. So, as a final push for a more \"complete\" dataset, I used the Wayback API to fetch the last few thousands of articles, some dating back years, which was very annoying because IA has very, very low rate limits (around 5 per minute). Finding a cost-effective cloud provider for GPUs > Fortunately, I discovered RunPod, a provider of machines with GPUs that you can deploy your containers onto, at a cost far cheaper than major cloud providers. They also have more cost-effective GPUs like RTX 4090, while still running in datacenters with fast Internet connections. This made scaling up a price-accessible option to mitigate the inference time required. This is the type of content that makes HN stands out from the crowd. _____________________________ 1. https://github.com/wilsonzlin/crawler-toolkit-hn/ 2. https://github.com/wilsonzlin/hackerverse/tree/master/crawle... reply chatman 9 hours agoprevWorth trying Cagra (Raft)/CuVS and Lucene-CuVS for the vector search. (https://github.com/SearchScale/lucene-cuvs) reply ed_db 20 hours agoprevThis is amazing, the amount of skill and knowledge involved is very impressive. reply wilsonzlin 18 hours agoparentThank you for the kind words! reply datguyfromAT 15 hours agoprevWhat a great read! Thats for taking the time and effort to provide the inside into your process reply thyrox 20 hours agoprevVery nice. Since Hn data spawns so many such fun projects, there should be a monthly or weekly updates zip file or torrent with this data, which hackers can just download instead of writing a scraper and starting from scratch all the time. reply zX41ZdbW 17 hours agoparentIt is very easy to get this dataset directly from HN API. Let me just post it here: Table definition: CREATE TABLE hackernews_history ( update_time DateTime DEFAULT now(), id UInt32, deleted UInt8, type Enum('story' = 1, 'comment' = 2, 'poll' = 3, 'pollopt' = 4, 'job' = 5), by LowCardinality(String), time DateTime, text String, dead UInt8, parent UInt32, poll UInt32, kids Array(UInt32), url String, score Int32, title String, parts Array(UInt32), descendants Int32 ) ENGINE = MergeTree(update_time) ORDER BY id; A shell script: BATCH_SIZE=1000 TWEAKS=\"--optimize_trivial_insert_select 0 --http_skip_not_found_url_for_globs 1 --http_make_head_request 0 --engine_url_skip_empty_files 1 --http_max_tries 10 --max_download_threads 1 --max_threads $BATCH_SIZE\" rm -f maxitem.json wget --no-verbose https://hacker-news.firebaseio.com/v0/maxitem.json clickhouse-local --query \" SELECT arrayStringConcat(groupArray(number), ',') FROM numbers(1, $(cat maxitem.json)) GROUP BY number DIV ${BATCH_SIZE} ORDER BY any(number) DESC\"while read ITEMS do echo $ITEMS clickhouse-client $TWEAKS --query \" INSERT INTO hackernews_history SELECT * FROM url('https://hacker-news.firebaseio.com/v0/item/{$ITEMS}.json')\" done It takes a few hours to download the data and fill the table. reply zX41ZdbW 17 hours agorootparentAlso, a proof that it is updated in real-time: https://play.clickhouse.com/play?user=play#U0VMRUNUICogRlJPT... reply minimaxir 17 hours agoparentprevThere is a public dataset of Hacker News posts on BigQuery, but it unfortunately has only been updated up to November 2022: https://news.ycombinator.com/item?id=19304326 reply remram 7 hours agoparentprevAs a starting point, that project has Apache Arrow files. I don't know if they'll update them though. https://github.com/wilsonzlin/hackerverse/releases/tag/datas... The comments text table is 13 GB, to give you an idea. Can definitely be processed on a laptop. reply pfarrell 18 hours agoparentprevI have a daily updated dataset that has the HN data split out by months. I've published it on my web page, but it’s served from my home server so I don’t want to link to it directly. Each month is about 30mb of compressed csv. I’ve wanted to torrent it, but don’t know how to get enough seeders since each month will produce a new torrent file (unless I’m mistaken). If you’re interested, send me a message. My email is mrpatfarrell. Use gmail for the domain. reply noman-land 19 hours agoparentprevI very much support this idea. Put them on ipfs and/or torrents. Put them on HuggingFace. reply pfarrell 18 hours agorootparentI’ve had this same thought but was unsure what the licensing for the data would be. reply average_r_user 20 hours agoparentprevthat's a nice idea reply chossenger 18 hours agoprevAwesome visualisation, and great write-up. On mobile (in portrait), a lot of longer titles get culled as their origin scrolls off, with half of it still off the other side of the screen - wonder if it'd be worth keeping on rendering them until the entire text field is off screen (especially since you've already got a bounding box for them). I stumbled upon [1] using it that reflects your comments on comment sentiment. This also reminded me of [2] (for which the site itself had rotted away, incidentally) - analysing HN users' similarity by writing style. [1] https://minimaxir.com/2014/10/hn-comments-about-comments/ [2] https://news.ycombinator.com/item?id=33755016 reply wilsonzlin 18 hours agoparentThanks for the kind words, and raising that problem --- I've added it as an issue to fix. Thanks for sharing that article, it was an interesting read. It was cool how deep the analysis went with a few simple statistical methods. reply coolspot 13 hours agoprevAbsolutely wonderful project and even more so the writeup! Feedback: on my iOS phone, once you select a dot on the map, there is no way to unselect it. Preview card of some articles takes full screen, so I can’t even click to another dot. Maybe add a “cross” icon for the preview card or make that when you tap outside of a card, it hides whole card strip? reply NeroVanbierv 19 hours agoprevReally love the island map! But the automatic zooming on the map doesn't seem very relevant. E.g. try typing \"openai\" - I can't see anything related to that query in that part of the map reply oersted 19 hours agoparentIndeed I've long been intreagued by the idea of rendering such clustering maps more like geographic maps for better readability. It would be cool to have analogous continents, countries, sub-regions, roads, different-sized settlements, and significant landmarks... This version looks great at the highest zoom level, but rapidly becomes hard to interpret as you zoom in, same as most similar large embedding or graph visualizations. reply NeroVanbierv 19 hours agoparentprevOk I just noticed there is a region \"OpenAI\" in the north-west, but for some reason it zooms in somewhere close to \"Apple\" (middle of the island) when I type the query reply wilsonzlin 19 hours agoparentprevThanks! Yeah sometimes there are one or two \"far\" away results which make the auto zoom seem strange. It's something I'd like to tune, perhaps zooming to where most but not all results are. reply luke-stanley 17 hours agorootparentOften embeddings are not so good for comparing similarity of text. A cross-encoder might be a good alternative, perhaps as a second-pass, since you already have the embeddings. https://www.sbert.net/docs/pretrained_cross-encoders.html Pairwise, this can be quite slow, but as a second pass, it might be much higher quality. Obviously this gets into LLM's territory, but the language models for this can be small and more reliable than cosine on embeddings. reply Lerc 16 hours agoprevA suggestion for analysis: Compare topics/sentiment etc. by number of users and by number of posts. Are some topics dominated by a few prolific posters? Positively or negatively. Also, How does one seperate negative/positive sentiment to criticism/advocacy? How hard is it to detect positive criticism, or enthusiastic endorsement of an acknowledged bad thing? reply aeonik 9 hours agoprevI couldn't help but notice that Hy is on the map but Clojure isn't. Am I out of touch? https://hylang.org reply graiz 20 hours agoprevWould be cool to see member similarity. Finding like-minded commentors/posters may help discover content that would be of interest. reply naveen99 17 hours agoparentWe implemented member similarity in our hacker read app: https://apps.apple.com/in/app/hacker-read/id6479697844 Once you register on ios, you can also login through webapp: https://hn.garglet.com probably not ready for a hacker news hug of death yet, but you can try. reply vsnf 19 hours agoparentprevReminds me of a similar project a few months ago whose purpose was to unmask alt accounts. It wasn’t well received as I recall. reply noman-land 19 hours agoparentprevAccidental dating app. reply internetter 18 hours agorootparent> Accidental dating app. Possibly the greatest indicator of social startup success. reply paddycap 19 hours agoprevAdding a subscribe feature to get an email with the most recent posts in a topic/community would be really cool. One of my favorite parts of HN is the weekly digest I get in my inbox; it would be awesome if that were tailored to me. What you've built is really impressive. I'm excited to see where this goes! reply wilsonzlin 18 hours agoparentThanks! Yeah if there's enough interested users I'd love to turn this into a live service. Would an email subscription to a set of communities you pick be something you'd be interested in? reply xnx 19 hours agoprevAs a novice, is there a benefit to using custom Node as the downloader? When I did my download of the 40 million Hacker News api items I used \"curl --parallel\". What I would like to figure out is the easiest way to go from the API straight into a parquet file. reply wilsonzlin 18 hours agoparentI think your curl approach would work just as fine if not better. My instinct was to reach for Node.js out of familiarity, but curl is fast and, given the IDs are sequential, something like `parallel curl ::: $(seq 0 $max_id)` would be pretty simple and fast. I did end up needing more logic though so Node.js did ultimately come in handy. As for the Arrow file, I'm not sure unfortunately. I imagine there are some difficulties because the format is columnar, so it probably wants a batch of rows (when writing) instead of one item at a time. reply sourcepluck 12 hours agoprevWhere is lisp?! I thought it was a verifiable (urban) legend around these parts that this forum is obssessed with lisp..? reply pinkmuffinere 11 hours agoparentMaybe lisp is so niche that even a rather small interest makes HN relatively lispy? reply gaauch 18 hours agoprevA long term side project of mine is to try to build a recommendation algorithm trained on HN data. I trained a model to predict if a given post will reach the front page, get flagged etc, I collected over a 1000 RSS feeds and rank the RSS entries with my ranking models. I submit the high ranking entries on HN to test out my models and I can reach the front page consistently sometimes having multiple entries on the front page at a given time. I also experiment with user->content recommendation, for that I use comment data for modeling interactions between users and entries, which seems to work fine. Only problem I have is that I get a lot of 'out of distribution' content in my RSS feeds which causes my ranking models to get 'confused' for this I trained models to predict if a given entry belongs HN or not. On top of that I have some tagging models trained on data I scraped from lobste.rs and hand annotated. I had been working on this on and off for the last 2 years or so, this account is not my main, and just one I created for testing. AMA reply saganus 18 hours agoparentdid you find if submitted entries are more likely to reach the frontpage depending on the title or the content? i.e. do HN users upvote more based on the title of the article or on actually reading them? reply gaauch 17 hours agorootparentI tried making an LLM generate different titles for a given article and compared their ranking scores. There seems to be a lot of variation in the ranking scores based on the way the title is worded. Titles that are more likely to generate 'outrage' seems to be getting ranked higher, but at the same time that increases is_hn_flagged score which tries to predict if a entry will get flagged. reply gitgud 11 hours agoprevVery cool! I was hoping to be able to navigate to the HN post from the map though? Is that possible? reply gsuuon 15 hours agoprevThis is super cool! Both the writeup and the app. It'd be great if the search results linked to the HN story so we can check out the comments. reply ashu1461 19 hours agoprevThis is pretty great. Feature request : Is it possible to show in the graph how famous the topic / sub topic / article is ? So that we can do an educated exploration in the graph around what was upvoted and what was not ? reply wilsonzlin 18 hours agoparentThanks! Do you mean within the sentiment/popularity analysis graph? Or the points and topics within the map? reply ashu1461 8 hours agorootparentPoints and topics within the map. reply fancy_pantser 18 hours agoprevHN submissions and comments are very different on weekends (and US holidays). Your data could explore and quantify this in some very interesting ways! reply dfworks 16 hours agoprevIf anybody found this interesting and would like some further reading, the paper below employed a similar strategy to analyse inauthentic content/disinformation on Twitter. https://files.casmconsulting.co.uk/message-based-community-d... If you would like to read about my largely unsuccessful recreation of the paper, you can do so here - https://dfworks.xyz/blog/partygate/ reply freediver 19 hours agoprevIf you have a blog, add an RSS feed :) reply breck 17 hours agoparentI tried to fetch his RSS too! :) Turns out, there's only 1 post so far on his blog. Hoping for more! This one is great. reply swozey 18 hours agoprevI'm.. shocked there's been 40 million posts. Wow. Really neat work edit: Also had no idea HN went back to 2006. https://news.ycombinator.com/item?id=1 edit2: PG wrote this? https://news.ycombinator.com/item?id=487171 reply c17r 8 hours agoparentAn HN \"item\" is not just posts but everything: posts, comments, the parts of a poll, etc. Still an impressive number reply Igor_Wiwi 13 hours agoprevhow much you paid to generate those embeddings? reply cyclecount 13 hours agoprevI can’t tell from the documentation on GitHub: does the API expose the flagged/dead posts? It would be interesting to see statistics on what’s been censored lately. reply nojvek 15 hours agoprevI'm impressed with the map component in canvas. It's very smooth, dynamic zoom and google-maps like. Gonna dig more into it. Exemplary Show HN! We need more of this. reply dangoodmanUT 10 hours agoprevexcellent work reply password4321 19 hours agoprevRelated a month ago: A Peek inside HN: Analyzing ~40M stories and comments https://news.ycombinator.com/item?id=39910600 reply callalex 17 hours agoprev [–] “Cloud Computing” “us-east-1 down” This gave me a belly laugh. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author utilized text embeddings to analyze 40 million posts and comments from Hacker News, developing tools for interactive search, recommendations, and sentiment analysis.",
      "The analysis covers data fetching optimization, sentiment trends like positive sentiments on Rust comments, and challenges in managing memory on GPUs.",
      "Collaboration and feedback are welcomed for enhancing the app, with data and code available for exploration on GitHub."
    ],
    "commentSummary": [
      "The post delves into sentiment analysis on Hacker News comments, emphasizing the high frequency of negativity and the necessity for a sophisticated sentiment classifier.",
      "Users engage in discussions about various topics such as platform atmosphere, self-promotion, data analysis projects, GPU optimization, and UMAP integration.",
      "The thread also touches on technical subjects like dimensionality reduction, visualization tools, topic prevalence, alt account identification, content recommendation algorithms, and predictive modeling using RSS feed content, as well as proposing enhancements for the platform and sharing resources for in-depth analysis."
    ],
    "points": 431,
    "commentCount": 133,
    "retryCount": 0,
    "time": 1715257864
  },
  {
    "id": 40307098,
    "title": "Datatype99: Safe Algebraic Data Types for C99",
    "originLink": "https://github.com/Hirrolot/datatype99",
    "originBody": "Datatype99 Safe, intuitive algebraic data types with exhaustive pattern matching & compile-time introspection facilities. No external tools required, pure C99. Highlights Type-safe. Such things as improperly typed variants, non-exhaustive pattern matching, and invalid field access are caught at compile-time. Portable. Everything you need is a standard-conforming C99 compiler; neither the standard library, nor compiler/platform-specific functionality or VLA are required. Predictable. Datatype99 comes with formal code generation semantics, meaning that the generated data layout is guaranteed to always be the same. Comprehensible errors. Datatype99 is resilient to bad code. Battle-tested. Datatype99 is used at OpenIPC to develop real-time streaming software for IP cameras; this includes an RTSP 1.0 implementation along with ~50k lines of private code. Installation Datatype99 consists of one header file datatype99.h and one dependency Metalang99. To use it in your project, you need to: Add datatype99 and metalang99/include to your include directories. Specify -ftrack-macro-expansion=0 (GCC) or -fmacro-backtrace-limit=1 (Clang) to avoid useless macro expansion errors. If you use CMake, the recommended way is FetchContent: include(FetchContent) FetchContent_Declare( datatype99 URL https://github.com/Hirrolot/datatype99/archive/refs/tags/v1.2.3.tar.gz # v1.2.3 ) FetchContent_MakeAvailable(datatype99) target_link_libraries(MyProject datatype99) # Disable full macro expansion backtraces for Metalang99. if(CMAKE_C_COMPILER_ID STREQUAL \"Clang\") target_compile_options(MyProject PRIVATE -fmacro-backtrace-limit=1) elseif(CMAKE_C_COMPILER_ID STREQUAL \"GNU\") target_compile_options(MyProject PRIVATE -ftrack-macro-expansion=0) endif() (By default, datatype99/CMakeLists.txt downloads Metalang99 v1.13.2 from the GitHub releases; if you want to override this behaviour, you can do so by invoking FetchContent_Declare earlier.) Optionally, you can precompile headers in your project that rely on Datatype99. This will decrease compilation time, because the headers will not be compiled each time they are included. Happy hacking! Usage Put simply, Datatype99 is just a syntax sugar over tagged unions; the only difference is that it is more safe and concise. For example, to represent a binary tree, you would normally write something like this: typedef struct { struct BinaryTree *lhs; int x; struct BinaryTree *rhs; } BinaryTreeNode; typedef struct { enum { Leaf, Node } tag; union { int leaf; BinaryTreeNode node; } data; } BinaryTree; To avoid this boilerplate, you can use Datatype99: datatype( BinaryTree, (Leaf, int), (Node, BinaryTree *, int, BinaryTree *) ); Say you want to sum all nodes and leafs in your binary tree. Then you may write something like this: int sum(const BinaryTree *tree) { switch (tree->tag) { case Leaf: return tree->data.leaf; case Node: return sum(tree->data.node.lhs) + tree->data.node.x + sum(tree->data.node.rhs); } // Invalid input (no such variant). return -1; } ... but what if you accidentally access tree->data.node after case Leaf:? Your compiler would not warn you, thus resulting in a business logic bug. With Datatype99, you can rewrite sum as follows, using a technique called pattern matching: int sum(const BinaryTree *tree) { match(*tree) { of(Leaf, x) return *x; of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs); } // Invalid input (no such variant). return -1; } of gives you variables called bindings: x, lhs, or rhs. This design has a few neat aspects: Compile-time safety. The bindings of Node are invisible after of(Leaf, x) and vice versa, so compilation will fail to proceed if you access them inappropriately. Flexibility. Bindings have pointer types so that you can mutate them, thereby mutating the whole tree; in order to obtain a value, you can dereference them, as shown in the example: return *x;. The last thing unmentioned is how you construct variants. Internally, Datatype99 generates inline static functions called value constructors; you can use them as follows: BinaryTree leaf5 = Leaf(5); BinaryTree leaf7 = Leaf(7); BinaryTree node = Node(&leaf5, 123, &leaf7); Finally, just a few brief notes about pattern matching: To match the default case, write otherwise { ... } at the end of match. To ignore a binding, write _: of(Foo, a, b, _, d). Please, do not use top-level break/continue inside statements provided to of and ifLet; use goto labels instead. Congratulations, this is all you need to know to write most of the stuff! If you feel fancy, you can also introspect your types at compile-time; see examples/derive/ for the examples. Syntax and semantics Having a well-defined semantics of the macros, you can write an FFI which is quite common in C. EBNF syntax::= \"datatype(\" [\",\" ]{ \",\"}+ \")\" ;::= \"record(\" [\",\" ]{ \",\"}* \")\" ;::=;::=;::= \"(\"{ \",\"}* \")\" ;::= \"(\"\",\"\")\" ;::=;::=;::= \"derive(\"{ \",\"}* \")\" ;::=;::= \"match(\"\") {\" {}* [] \"}\" ;::= \"MATCHES(\"\",\"\")\" ;::= \"ifLet(\"\",\"\",\"{ \",\"}* \")\";::= \"of(\"{ \",\"}* \")\";::= \"otherwise\"; Note: shortened vs. postfixed versions Semantics (It might be helpful to look at the generated data layout of examples/binary_tree.c.) datatype Before everything, the following type definition is generated: typedef struct; For each non-empty variant, the following type definition is generated (the metavariableranges over a corresponding variant's types): typedef struct{ 0 _0; ... N _N; } ; For each non-empty variant, the following type definitions to types of each field ofare generated: typedef 0 _0; ... typedef N _N; For each variant, the following type definition to a corresponding sum type is generated: typedef structSumT; For each sum type, the following tagged union is generated (inside the union, only fields to structures of non-empty variants are generated): typedef enum Tag { 0Tag, ..., NTag } Tag; typedef union Variants { char dummy; 0 0; ... N N; } Variants; struct{ Tag tag; Variants data; }; Note on char dummy; For each variant, the following function called a value constructor is generated: inline static(/* ... */) { /* ... */ } If the variant has no parameters, this function will take void and initialise .data.dummy to '\\0'; otherwise, it will take the corresponding variant parameters and initialise the result value as expected. Now, when a sum type is fully generated, the derivation process takes place. Each deriver taken from derive(...) is invoked sequentially, from left to right, as ML99_call(DATATYPE99_DERIVE_##I, v(), variants...) where I corresponds to a Metalang99-compliant macro of the form #define DATATYPE99_DERIVE_##I_IMPL(name, variants) /* ... */. variants... is a list of variants represented as two-place tuples: (, types...), where types... is a list of types of the corresponding variant. Put simply, a deriver is meant to generate something global for a sum type, like interface implementations or almost any other stuff. In terms of Rust, you can think of it as of the derive attribute. record record represents a record type: it is simply a struct for which the derivation process is defined. The following structure is generated: typedef struct{ // Only ifhas no fields: char dummy; 0 0; ... N N; } ; Note on char dummy; Each deriver taken from derive(...) is invoked sequentially, from left to right, as ML99_call(DATATYPE99_RECORD_DERIVE_##I, v(), fields...) where I corresponds to a Metalang99-compliant macro of the form #define DATATYPE99_RECORD_DERIVE_##I_IMPL(name, fields) /* ... */. fields... is a list of fields represented as two-place tuples: (, ). If a record contains no fields, the list would consist only of (char, dummy). match match has the expected semantics: it sequentially tries to match the given instance of a sum type against the given variants, and, if a match has succeeded, it executes the corresponding statement and moves down to the next instruction (match(val) { ... } next-instruction;). If all the matches have failed, it executes the statement after otherwise and moves down to the next instruction. A complete match construct results in a single C statement. of of accepts a matched variant name as a first argument and the rest of arguments comprise a comma-separated list of bindings. A binding equal to _ is ignored. A binding not equal to _ stands for a pointer to a corresponding data of the variant (e.g., let there be (Foo, T1, T2) and of(Foo, x, y), then x has the type T1 * and y is T2 *). There can be more than one _ binding, however, non-_ bindings must be distinct. To match an empty variant, write of(Bar). MATCHES MATCHES just tests an instance of a sum type for a given variant. If the given instance corresponds to the given variant, it expands to truthfulness, otherwise it expands to falsehood. matches DEPRECATED: use MATCHES instead. ifLet ifLet tries to match the given instance of a sum type against the given variant, and, if a match has succeeded, it executes the corresponding statement. Think of ifLet(, , vars...) { /* ... */ } as of an abbreviation of match() { of(, vars...) { /* ... */ } otherwise {} } A complete ifLet construct results in a single C statement. Unit type The unit type UnitT99 represents the type of a single value, unit_v99 (it should not be assigned to anything else). These are defined as follows: typedef char UnitT99; static const UnitT99 unit_v99 = '\\0'; If DATATYPE99_NO_ALIASES remains undefined prior to #include , UnitT99 and unit_v99 are also accessible through object-like macros UnitT & unit_v. Derive helper attributes You can pass named arguments to a deriver; these are called derive helper attributes. They must be specified as object-like macros of the form: #define __ attr(/* attribute value */) whereis either / or / for datatype/record-specific and variant/field-specific attributes, respectively. To manipulate derive helper attributes, there are a few predefined macros: DATATYPE99_attrIsPresent/DATATYPE99_ATTR_IS_PRESENT Accepts an attribute name and checks if it is present or not. It can be used to check the presence of an optional attribute. DATATYPE99_attrValue/DATATYPE99_ATTR_VALUE Accepts an attribute name extracts its value. A provided attribute must be present. DATATYPE99_assertAttrIsPresent Accepts an attribute name and emits a fatal error if the attribute is not present, otherwise results in emptiness. It can be used for mandatory attributes. (The naming convention here is the same as of Metalang99.) Miscellaneous The macros DATATYPE99_MAJOR, DATATYPE99_MINOR, DATATYPE99_PATCH, DATATYPE99_VERSION_COMPATIBLE(x, y, z), and DATATYPE99_VERSION_EQ(x, y, z) have the same semantics as of Metalang99. For each macro using ML99_EVAL, Datatype99 provides its Metalang99-compliant counterpart which can be used inside derivers and other Metalang99-compliant macros: Macro Metalang99-compliant counterpart datatype DATATYPE99_datatype record DATATYPE99_record of DATATYPE99_of ifLet DATATYPE99_ifLet (An arity specifier and desugaring macro are provided for each of the above macros.) There is a built-in deriver dummy which generates nothing. It is defined both for record and sum types. Guidelines Clang-Format issues If you use Clang-Format, cancel formatting for a datatype definition using // clang-format off & // clang-format on to make it look prettier, as in the examples. #undef derive helper attributes Always #undef derive helper attributes after a corresponding datatype definition not to pollute your namespace. Descriptive names If the meaning of variant parameters is not clear from the context, give them descriptive names. This can be achieved in several ways: // 1. Define type aliases to variant parameters. typedef double XCoordinate; typedef double YCoordinate; typedef double Width; typedef double Height; datatype( Shape, (Point, XCoordinate, YCoordinate), (Rectangle, Width, Height) ); // 2. Define separate structures. typedef struct { double x, y; } Point; typedef struct { double width, height; } Rectangle; datatype( Shape, (MkPoint, Point), (MkRectangle, Rectangle) ); Comparison: The former option has more concise syntax: MkPoint(x, y) instead of MkPoint((Point){x, y}). The latter option is more appropriate when the structures are to be used separately from the containing sum type. The latter option allows for more graduate control over the data layout: you can accompain the structures with compiler-specific attributes, alignment properties like __attribute__ ((__packed__)), etc. Pitfalls Top-level break/continue Do not use break/continue inside a statement provided to of/ifLet but outside of any for/while loops in that statement. For example, this code is fine: match(x) { of(Foo, a, b, c) { for (int i = 0; iand you are ready to go. On the other hand, other languages force you to separate native C files from their sources, which is clearly less convenient. In some environments, developers strick to pure C for historical reasons (e.g., embedded devices, Linux and other operating systems). C has a stable ABI which is vital for some projects (e.g., plugin systems such as MetaCall). C is a mature language with a complete specification and a plenitude of libraries. Rust has no complete specification, and Zig is not yet production-ready. I know a few stories when these two languages were rejected for new projects, and I can understand this decision. Historically, C has been targeting nearly all platforms. This is not the case with Rust, which depends on LLVM as for now. Your company obligates you to use C. Etc. See also: \"Rust is not a good C replacement\" by Drew DeVault. Overall, if you can afford a more modern/high-level language, I encourage you to do so instead of using old C. However, many people do not have this possibility (or it would be too costly). Q: Why not third-party code generators? A: See Metalang99's README >>. Q: How does it work? A: In short, datatype expands to a tagged union with value constructors; match expands to a switch statement. To generate all this stuff, Metalang99 is used, a preprocessor metaprogramming library. More on it in \"Compiling Algebraic Data Types in Pure C99\". Q: Does it work on C++? A: Yes, C++11 and onwards is supported. Q: What is the difference between Datatype99 and Metalang99? A: Metalang99 is a functional language for metaprogramming, whereas Datatype99 is an implementation of algebraic data types written in this language. Q: What about compile-time errors? A: Some kinds of syntactic errors are detected by the library itself: Error: Bar(int) instead of (Bar, int) [playground.c] datatype(A, (Foo, int), Bar(int)); [/bin/sh] $ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0 playground.c:3:1: error: static assertion failed: \"ML99_assertIsTuple: Bar(int) must be (x1, ..., xN)\" 3datatype(A, (Foo, int), Bar(int));^~~~~~~~ Error: Missing comma [playground.c] datatype(A, (Foo, int) (Bar, int)); [/bin/sh] $ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0 playground.c:3:1: error: static assertion failed: \"ML99_assertIsTuple: (Foo, int) (Bar, int) must be (x1, ..., xN), did you miss a comma?\" 3datatype(A, (Foo, int) (Bar, int));^~~~~~~~ Error: Trailing comma is prohibited [playground.c] datatype(A, (Foo, int), (Bar, int), /* trailing comma is prohibited */); [/bin/sh] $ gcc playground.c -Imetalang99/include -Idatatype99 -ftrack-macro-expansion=0 playground.c:3:1: error: static assertion failed: \"ML99_assertIsTuple: must be (x1, ..., xN)\" 3datatype(A, (Foo, int), (Bar, int), /* trailing comma is prohibited */);^~~~~~~~ (For better diagnostics, use the latest Metalang99.) The others are understandable as well: Error: unknown type name specified in datatype [playground.c] datatype(Foo, (FooA, NonExistingType)); [/bin/sh] playground.c:3:1: error: unknown type name ‘NonExistingType’ 3datatype(^~~~~~~~ playground.c:3:1: error: unknown type name ‘NonExistingType’ playground.c:3:1: error: unknown type name ‘NonExistingType’ Error: non-exhaustive match [playground.c] match(*tree) { of(Leaf, x) return *x; // of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs); } [/bin/sh] playground.c: In function ‘sum’: playground.c:6:5: warning: enumeration value ‘NodeTag’ not handled in switch [-Wswitch] 6match(*tree) {^~~~~ Error: excess binders in of [playground.c] match(*tree) { of(Leaf, x, excess) return *x; of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs); } [/bin/sh] playground.c: In function ‘sum’: playground.c:15:9: error: unknown type name ‘Leaf_1’; did you mean ‘Leaf_0’? 15of(Leaf, x, excess) return *x;^~Leaf_0 playground.c:15:9: error: ‘BinaryTreeLeaf’ has no member named ‘_1’; did you mean ‘_0’? 15of(Leaf, x, excess) return *x;^~_0 Error: improperly typed variant arguments [playground.c] BinaryTree tree = Leaf(\"hello world\"); [/bin/sh] playground.c: In function ‘main’: playground.c:18:28: warning: passing argument 1 of ‘Leaf’ makes integer from pointer without a cast [-Wint-conversion] 18BinaryTree tree = Leaf(\"hello world\");^~~~~~~~~~~~~|char * playground.c:6:1: note: expected ‘int’ but argument is of type ‘char *’ 6datatype(^~~~~~~~ Error: an undereferenced binder [playground.c] int sum(const BinaryTree *tree) { match(*tree) { of(Leaf, x) return x; // x is int * of(Node, lhs, x, rhs) return sum(*lhs) + *x + sum(*rhs); } } [/bin/sh] playground.c: In function ‘sum’: playground.c:17:28: warning: returning ‘Leaf_0 *’ {aka ‘int *’} from a function with return type ‘int’ makes integer from pointer without a cast [-Wint-conversion] 17of(Leaf, x) return x; // x is int *^ From my experience, nearly 95% of errors make sense. If an error is not comprehensible at all, try to look at generated code (-E). Hopefully, the code generation semantics is formally defined so normally you will not see something unexpected. Q: What about IDE support? A: VS Code automatically enables suggestions of generated types but, of course, it does not support macro syntax highlighting. Q: Which compilers are tested? A: Datatype99 is known to work on these compilers: GCC Clang MSVC TCC Troubleshooting warning: control reaches end of non-void function [-Wreturn-type] This warning happens when you try to return control from within a match statement, and your compiler thinks that not all hypothetical variants are handled. For example: datatype(MyType, (Foo), (Bar)); int handle(MyType val) { match(val) { of(Foo) return 5; of(Bar) return 7; } } The above code may seem perfect at first glance, but in fact, it is not. The reason is this: match(val) boils down to switch(val.tag) under the hood, with val.tag being an ordinary C enumeration consisting of the variants Foo and Bar. But what if a caller provides us with neither Foo nor Bar, but with something like 42 (not a valid variant)? Since enum is merely another way to give integers names, a compiler would not complain on the caller site. However, on the callee site, we would have the warning: test.c: In function ‘handle’: test.c:10:1: warning: control reaches end of non-void function [-Wreturn-type] 10}^ The solution is to either panic or return some error-signaling code, like this: int handle(MyType val) { match(val) { of(Foo) return 5; of(Bar) return 7; } // Invalid input (no such variant). return -1; } See issue #9.",
    "commentLink": "https://news.ycombinator.com/item?id=40307098",
    "commentBody": "Algebraic Data Types for C99 (github.com/hirrolot)347 points by bondant 22 hours agohidepastfavorite191 comments klysm 9 hours agoIf I ever implement a product from scratch again, discriminated unions with compiler enforced exhaustive pattern matching is a hard requirement. It’s too powerful to not have. reply nmfisher 31 minutes agoparentUnion types are my biggest wish for Dart, but unfortunately it doesn't look like they'll be added any time soon. They've recently added support for compiler-enforced pattern matching over sealed classes, which I suppose does get you halfway there though. reply tombert 20 hours agoprevInteresting. Algebraic Data Types are almost always one of the things I miss when I use imperative languages. I have to do Java at work, and while I've kind of come around on Java and I don't think it's quite as bad as I have accused it of being, there's been several dozen instances of \"man I wish Java had F#'s discriminated unions\". Obviously I'm aware that you can spoof it with a variety of techniques, and often enums are enough for what you need, but most of those techniques lack the flexibility and terseness of proper ADTs; if nothing else those techniques don't have the sexy pattern matching that you get with a functional language. This C extension looks pretty sweet since it appears to have the pattern matching I want; I'll see if I can use it for my Arduino projects. reply estebank 20 hours agoparentEveryone who hasn't used ADTs and pattern matching doesn't get what the big deal is all about. Everyone who is used to ADTs and pattern matching doesn't get what the big deal is all about, until they have to work in a language that doesn't have them. And everyone who just found out about them can't shut up about them being the best thing since sliced bread. :) reply acchow 11 hours agorootparentI’m in the latter camp (from Ocaml) and now using Go. Go feels clunky and awkward. reply packetlost 10 hours agorootparentThat's because Go is intentionally clunky and awkward in the name of \"simplicity\". IMO it's charming to some degree, but it's far from perfect and I think you'd need some pretty serious threats to get me to describe it as \"elegant\" in any way. Rust somehow has more elegance than Go, if only in small parts. Nothing compares to Scheme in the elegance category IMO :) reply im3w1l 19 hours agorootparentprevI have mainly used them in Rust. They are nice I suppose, but nothing mindblowing. To me it feels very similar to an interface (trait) implemented by a bunch of classes (structs). I have multiple times wondered which of those two approaches would be better in a given situation, often wanting some aspects of both. Being able to exhaustively pattern match is nice. But being able to define my classes in different places is also nice. And being able to define methods on the classes is nice. And defining a function that will only accept particular variant is nice. From my perspective a discriminant vs a vtable pointer is a boring implementation detail the compiler should just figure out for me based on what would be more optimal in a given situation. reply naasking 17 hours agorootparent> I have multiple times wondered which of those two approaches would be better in a given situation, often wanting some aspects of both. ADTs are closed to extension with new cases but open to extension with new functions, eg. anytime you want to add new cases, you have to update all functions that depend on the ADT, but you can add as many functions for that ADT as you like with no issues. Traits are open to extension with new cases but closed to extension with new functions, eg. you can add as many impl as you like with no issues (new cases), but if you want to add a new function to the trait you have to update all impl to support it. They are logical duals, and the problem of designing systems that are open to extension in both cases and functions is known as the expression problem: https://en.wikipedia.org/wiki/Expression_problem reply amluto 11 hours agorootparentI suppose this is a genuine dichotomy, but I feel like it’s missing a more critical difference: ADTs cleanly represent data, even when nothing can be, or needs to be, extended from outside. For example, a result is a success value or an error. A stock order is a market order or a limit order, and nothing else, at least until someone updates the spec and recompiles the code. Situations like this happen all the time. I don’t want to extend a result to include gizmos in addition to success value or errors, nor do I generally want to extend the set of functions that operate on a certain sort of result. But I very, very frequently want to represent values with a specific, simple schema, and ADTs fit the bill. A bunch of structs/classes, interfaces/traits and getters/setters can do this, but the result would look like the worst stereotypes of enterprise Java code to accomplish what a language with nice ADTs can do with basically no boilerplate. reply naasking 10 hours agorootparent> For example, a result is a success value or an error. A stock order is a market order or a limit order, and nothing else, at least until someone updates the spec and recompiles the code. But that's just it, specs are rarely complete because reality is fluid. For example, a result is a success or an error, until maybe you want an errors to prompt the user to correct something and then the computation can be resumed (see resumable exceptions). Should you even have to recompile your code to handle new cases? Why can't you just add the new case, and define new handlers for the functions that depend on your ADT without recompiling that code? That's the expression problem. reply im3w1l 16 hours agorootparentprevI want more syntax sugar for my ADTs that mirror what traits have. I don't need that kind of double extensibility. reply bPspGiJT8Y 4 hours agorootparentprev> And defining a function that will only accept particular variant is nice This is possible to achieve (or hack your way through, if you will) by parameterizing the type and using a nullary type (a type which is impossible to have) to exclude specific cases of a sum type. In Haskell this would look like this: data Weather a b c = Sunny aRainy bSnowy c -- can't snow in the summer! onlySummerWeather :: forall a b. Weather a b Void -> String onlySummerWeather weather = case weather of Sunny _ -> \"Got sunny weather\" Rainy _ -> \"Got rainy weather\" Snowy v -> absurd v where `absurd :: forall a. Void -> a` \"if you give me something you can't ever have, I will give you anything in return\". reply beltsazar 16 hours agorootparentprev> To me it feels very similar to an interface (trait) implemented by a bunch of classes (structs) Then you might not fully grok sum types yet. > From my perspective a discriminant vs a vtable pointer is a boring implementation detail the compiler should just figure out for me based on what would be more optimal in a given situation. Disagree. It's a design choice that should be decided by the programmers. There's a tradeoff—choosing which should be easier: adding a new variant or adding a new function/method. It's called the Expression Problem: https://wiki.c2.com/?ExpressionProblem reply bmoxb 17 hours agorootparentprevYou might be interested in taking a look at OCaml's extensible sum types which may straddle the line in the way you're describing. reply estebank 18 hours agorootparentprevEnums are closed sets and trait objects are open sets. They are conceptually related concepts, but the language puts syntactic distance between the two, and I don't think it should. There are lots of open design questions for every feature you propose, but all of them have been discussed and have higher or lower chance of making it into the language. reply im3w1l 18 hours agorootparentThat's kind of greek to me, but shouldn't promising the compiler that my set is closed unlock more features instead of taking features away? reply estebank 17 hours agorootparentI'm not sure I understand your point, but I'll elaborate on ways that we could \"homogenize\" the two features: --- We could add implicit enums to impl Trait, so that you could return different types from a function: fn foo() -> enum impl Display { if rand() > 0.5 { \"str\" } else { 42 } } which would let you get around the problem of returning a type erased object for a Trait that isn't object safe: trait Trait { const C: i32 = 0; } impl Trait for i32 {} impl Trait for &'static str {} fn foo() -> Box { if true { Box::new(\"\") } else { Box::new(42) } } error[E0038]: the trait `Trait` cannot be made into an object --> f500.rs:6:176fn foo() -> Box {^^^^^^^^^ `Trait` cannot be made into an objectnote: for a trait to be \"object safe\" it needs to allow building a vtable to allow the call to be resolvable dynamically; for more information visit--> f500.rs:2:111trait Trait {----- this trait cannot be made into an object... 2const C: i32 = 0;^ ...because it contains this associated `const` = help: consider moving `C` to another trait = help: the following types implement the trait, consider defining an enum where each variant holds one of these types, implementing `Trait` for this new enum and using it instead: &'static str i32 --- Relax object safety rules, like making all assoc consts implicitly `where Self: Sized`. --- We could make enum variants types on their own right, allowing you to write fn foo() -> Result::Ok { Ok(42) } let Ok(val) = foo(); There's some work on this, under the umbrella of \"patterns in types\". For now the only supported part of it is specifying a value range for integers, but will likely grow to support arbitrary patterns. --- Having a way to express `impl Trait for Enum {}` when every `Enum` variant already implement `Trait` without having to write the whole `impl`. --- Anonymous enums: fn foo() -> FooBarBaz --- Being able to match on Box or anonymous enums match foo() { x: Foo => ..., x: Bar => ..., _ => ..., } --- Stop needing to create a new struct type in order to box a single variant enum Foo { Bar(Box), } --- These are of the top of my head, there are many things that you can do to make trait objects and enums feel closer than they do today, to make changing the way your code works a \"gradient\" instead of a \"jump\". My go-to example for this is: if you have a type where every field is Debug, you can derive it. As soon as you add one field that isn't Debug, you have to implement the whole impl for your type. That's a \"jump\". If we had default values for structs you could still use the derive by specifying a default value in the definition. That makes the syntactic change \"distance\" be as far as the conceptual change \"distance\". reply im3w1l 17 hours agorootparentA trait is a collection of variants that may or may not have unknown members. An enum is a collection of variants that may not have unknown implementations. So enums are in some sense a subset of traits. Hence every property of traits is also a property of enums. Does that make sense? Those suggestion of your look interesting, but I haven't thought them through enough to have an opinion. reply AlecBG 20 hours agoparentprevSealed interfaces in java 21 allow pattern matching reply tombert 20 hours agorootparentYeah I know, we just don't use Java 21 at work yet. I'm super excited for that update, and it actually looks like we will be transitioning to that by the end of the year, but I haven't had a chance to play with it just yet. I do find it a little annoying that it's taken so long for Java to get a feature that, in my opinion, was so clearly useful; it feels like they were about a decade later on this than they should have been, but I'll take whatever victories I can get. reply brabel 16 hours agorootparentIf you can enable preview features, you can use pattern matching since Java 17 (though the final syntax in Java 21 was slightly changed - still you may want to use preview features, it's mostly fine in Java as they tend to change very little, and when you do upgrade, the compiler will tell you where you need to update your code). reply lupire 20 hours agoparentprevKotlin is JVM compatible and has ADTs. Java has https://github.com/functionaljava/functionaljava which is unsupported but stable. reply tombert 20 hours agorootparentSure, and Scala has had ADTs since its inception as well I think, and that's also JVM. It's not ADTs, but Clojure does have some level of pattern matching/destructuring as well. It wasn't that I though that the JVM was incapable of doing something like an ADT, just that vanilla Java didn't support it. While it's easy to say that \"companies should just use Kotlin\", that's a bit of a big ordeal if you already have a 15 year old codebase that's written in Java. I've heard of but never used the Functional Java library, though it'd be a tough sell to get my work to let me import a library that hasn't been updated in two years. reply Tainnor 15 hours agorootparent> that's a bit of a big ordeal if you already have a 15 year old codebase that's written in Java. JetBrains has prioritised compatibility with Java and it shows. Of course, there are some gotchas (such as nullability or checked exceptions which don't exist in Kotlin), but you can really mix Kotlin and Java code relatively freely. reply brabel 16 hours agorootparentprevJava 21's pattern matching (you don't need functionaljava, and shouldn't really use that unless you're really into FP) is kind of nicer than Kotlin's, because you can automatically \"destruct\" records in your matches. For Java, see https://www.baeldung.com/java-lts-21-new-features Kotlin's: https://www.baeldung.com/kotlin/when Make up your own mind. reply ackfoobar 5 hours agorootparent> kind of nicer than Kotlin's, because you can automatically \"destruct\" records in your matches. I find positional destructuring of records a bad idea. https://news.ycombinator.com/item?id=31399737 reply drycabinet 9 hours agoprevWikipedia has something interesting on this (how unions can be implemented using \"class hierarchy in object-oriented programming\"): https://en.wikipedia.org/wiki/Tagged_union#Class_hierarchies... There is a lengthy blog post about the same stuff, except that the author doesn't seem to have come across the said wiki section yet: https://nandakumar.org/blog/2023/12/paradigms-in-disguise.ht... Kudos to the dev of datatype99 for showing the problem with such ad-hoc methods in the readme right away. reply naasking 21 hours agoprevDefinitely looks nicer and probably works better than my older attempt [1], but uses 8x more code and depends on the awesome but kinda scary Metalang9 macro toolkit. I think libsum is a good intro if you want to see how algebraic data types work underneath. [1] https://github.com/naasking/libsum reply Hirrolot 18 hours agoparentI have a star on your repository, so it seems I was looking into it while designing Datatype99 :) reply cryptonector 15 hours agorootparentGH stars kinda function as a bookmark system, except I never go looking at what all I've starred, so it's more of an optimistic bookmark system. I only sometimes use it as a \"I would recommend this repo\" -- how can one do that anyways, given that the repo could morph into something one would no longer recommend? reply linkdd 21 hours agoprevThis is the work of a wizard. I've known C for almost 20 years, and never would I have thought the macro system was powerful enough to allow such black magic. This is awesome! reply cl91 18 hours agoparent> I've known C for almost 20 years The author is only 19 years old. I feel really dumb now. reply clnhlzmn 20 hours agoparentprevYou might also be interested in metalang99 by the same author. reply jacoblambda 20 hours agoparentprevYeah xmacros (the style of macro use) are pretty fancy. \"Classically\" they are used for creating and accessing type safe generics or for reducing boilerplate for hardware register and interrupt definitions. They are kind of cursed but at their core they are actually incredibly simple and a reliable tool for reducing cognitive complexity and boilerplate in C based projects. reply lupire 20 hours agoparentprevADTs are mostly string replacement on generic structs and unions, plus tagging on the union. It's not a complicated use of macros. reply mingodad 15 hours agoprevThere is also https://melt.cs.umn.edu/ that has an extension that add templates and algebraic data types to C : https://github.com/melt-umn/ableC-template-algebraic-data-ty... reply modeless 13 hours agoprev> PLEASE, do not use top-level break/continue inside statements provided to of and ifLet; use goto labels instead. Seems like a pretty big footgun. But otherwise, very cool. reply zzo38computer 12 hours agoparentUsing goto instead isn't a problem, but knowing not to use break/continue inside of such blocks is something that you will have to be aware of. I had written a immediate mode UI out of macros, and this reminded me of that although in my case it is not a problem, although some blocks are ones that you can use \"break\". For example, you can use \"break\" to exit out of a win_form block (\"goto\" also works), while a win_command block does not capture \"break\" so using break (or goto) inside of a win_command block will break out of whatever block the win_command is in (probably a win_form block; for example, this would commonly be used in the case of a \"Cancel\" button). reply 392 8 hours agoparentprevWhat's neat about Rust is that in its macro land, writing the code that checked for this condition would be not only possible, but doable, imaginable, and aided by easily installable OSS libraries. So it's not just about being slightly better in some ways, but smoothing over so many paper cuts that it can be hard to see how they have added up overtime across ecosystems, like CPython and co having so many of its own vocab types, or HPC libs. For example, the problem with this macro that causes this wouldn't even be problems in a well written Rust macro. They're artifacts of smart people trying to work around C's limitations. But then the macro wouldn't have been written anyway because this is a port of a native Rust feature (which means it gets taken advantage of in community software). reply linkdd 12 hours agoparentprevgoto is a footgun only if you use it to move from function to function, which btw was what \"goto considered harmful\" was about. That practice has disappeared, and now goto, within a function, is pretty harmless and quite identical to break/continue in fact. reply modeless 11 hours agorootparentGoto isn't the footgun. The footgun is if you use break/continue by accident then some unspecified bad thing will happen, silently I'm guessing. reply otikik 17 hours agoprevWhat a madlad. Kudos for implementing this. reply WhereIsTheTruth 14 hours agoprevTagged Union is a must have in a programming language reply KerrAvon 16 hours agoprevAnyone considering using this should be strongly looking at using Swift or Rust instead. You can build almost any given language idea using the C macro preprocessor, but that doesn't mean it's a good idea to ship production code using it. The worst codebases to inherit as a maintenance programmer are the ones where people got clever with the C preprocessor. Impossible to debug and impossible to maintain. reply endgame 4 hours agoparentC99 is a stable target for writing bootstrappable software: there are multiple mature compiler implementations, at least one of which is bootstrappable down to hex0, and the bootstrap chain is not too long. reply 392 8 hours agoparentprevI find that most abuses of the preprocessor are by folks unwilling/unable to simplify their design into a form that's (a) native to the C language/runtime or (b) not repetitive to type. This library on the other hand addresses a nasty papercut whose presence usually stops folks with modern language experience from choosing C when it might otherwise be valid. Plus you can't beat C's long-term stability. Though I agree that 90+% who _think_ they still need C should probably move on to making Rust work for them, instead. reply samatman 19 hours agoprevLet's say you have a C program to write, and you really want exhaustive pattern matching on the tags of unions (which is what Datatype99 provides: \"Put simply, Datatype99 is just a syntax sugar over tagged unions\"). Let's say further that you already know Rust exists, and aren't going to use it for reasons that anyone writing a C program already knows. At least consider Zig. Here's a little something I wrote in Zig two days ago: /// Adjust a label-bearing OpCode by `l`. No-op if no label. pub fn adjust(self: *OpCode, l: i16) void { switch (self.*) { inline else => |*op| { const PayType = @TypeOf(op.*); if (PayType != void and @hasField(PayType, \"l\")) { op.*.l += l; } }, } } This uses comptime (inline else) to generate all branches of a switch statement over a tagged union, and add an offset to members of that union which have an \"l\" field. You can vary the nature of the branches on any comptime-available type info, which is a lot, and all the conditions are compile-time, each branch of the switch has only the logic needed to handle that variant. \"But my program is already in C, I just need it for one file\" right. Try Zig. You might like it. reply pajko 17 hours agoparentSeems like Nim can be useful too, plus it compiles to C. https://gist.github.com/unclechu/eb37cc81e80afbbb5e74990b62e... reply brabel 16 hours agorootparentIn Nim, ADTs are painful still (as your example clearly shows), but they are working on adding proper ADTs to the language (I can't find where I read that, but I am sure I did!). reply j-james 8 hours agorootparenthttps://github.com/nim-lang/RFCs/issues/548 reply samatman 14 hours agorootparentprevI've been a Nim respecter for many years, it's slept on in general as a language. The difference here is that Nim compiles to C and you can turn the garbage collector off: Zig compiles C and there's no garbage collector. That means the entire standard library is available when generating object code. It's also trivial to opt-in to the C ABI on a fine-grained basis, by defining a function or struct with the extern keyword. I believe this is still fairly current about the difficulties of building Nim dylibs for C programs: https://peterme.net/dynamic-libraries-in-nim.html I expect Nim will stabilize about where D has: it will have a dialect of the language which, with relatively painless accommodations, is able to produce object code which speaks C ABI. Zig is different. The language is relentlessly focused on providing a better alternative to C while occupying the same niche, and a lot of design time has been spent on making it practical to take an existing C program and start writing the new parts of it in Zig. It's a good language, Nim, and getting better. I'd recommend it for someone who is considering Go, for example. reply keybored 3 hours agorootparentLet's say further that you already know Zig exists, and aren't going to use it for reasons that anyone writing a C/Rust/D/C++ program already knows. At least give Nim a try. reply j-james 8 hours agorootparentprevI think this is all true. Though with regard to your earlier example, it should be noted that Nim, too, has an extraordinarily powerful compile-time programming system: but it takes the form of typed macros and generics (as opposed to Zig's dislike for such abstractions). reply jackling 20 hours agoprevCould you not get most of the benefits of ADTs using structs + unions + enums? I've used the pattern where I had a union of several types and an enum to differentiate which one to pick. Something like std::variant seems to work a bit like a sum type. The only issue is you can't do a clean switch statement that matches on the specific value of a field, but nested switch statements aren't that messy. reply acuozzo 19 hours agoparentYes, and you can also get many of the benefits of OOP with convention and discipline, but doing so requires you to frequently get down in the weeds since, e.g., vtables must be dealt with manually. The trouble with this approach is that there's a lot of mental overhead in dotting all of your i's and crossing all of your t's. It's draining, so you start to, e.g., shoehorn additional functionality into existing classes instead of making new ones. You eventually wind up perceiving the abstraction as costly which lessons your use of it at the expense of producing a more elegant solution to the problem(s) you're solving. tl,dr? The ability to just state \"Darmok and Jalad at Tanagra\" is transformative when the alternative is telling an entire story every time you want to reference a complex idea. reply cryptonector 13 hours agoparentprevThe absolutely critical thing is to have checked every alternative when dealing with a sum type value. This is hard to do with macros, though not impossible (basically you'd need a macro to start a matching context and which introduces a variable in which to keep track of all the alternatives checked, then you need to arrange for the end of the matching context to check that all alternatives were checked). reply naasking 17 hours agoparentprev> Could you not get most of the benefits of ADTs using structs + unions + enums? The modelling aspects can be simulated, yes, but that's barely half of the benefits of ADTs. Pattern matching is a big ergonomic benefit. reply cryptonector 13 hours agorootparentTFA gets pattern matching. The critical thing is that the compiler (or macro system) needs to check that you've checked all the alternatives. reply naasking 10 hours agorootparentYes TFA is pretty close, but note that it's not just \"structs + unions + enums\" getting you \"most of the benefits\", which is what I was responding to. There's a buttload of macros hiding allocation and switch statements. reply mattgreenrocks 18 hours agoparentprevI generally don't mind C++ for most code when it's absolutely necessary, but I'm not a huge fan of std::variant. Using std::visit to exhaustively match all cases feels hacky. It really would benefit from being a first-class language feature. It's more impactful to a lot of day-to-day code than other things they've worked on, such as coroutines. reply TheBicPen 12 hours agorootparentThe 4th example at https://en.cppreference.com/w/cpp/utility/variant/visit that uses a class template makes the feature a bit nicer, but still not as ergonomic as something like Rust. reply different_base 21 hours agoprev [–] One of the crimes of modern imperative programming languages is not having ADTs (except maybe Rust) built-in. It is such a basic mental model of how humans think and solve problems. But instead we got inheritance and enums which are practically very primitive. reply adrian_b 20 hours agoparentMoreover, they have already been proposed by John McCarthy in October 1964, 60 years ago, for inclusion in the successor of ALGOL 60, which makes even more weird the lack of widespread support. (And in fact Algol 68 had a better implementation than most later languages, but Algol 68 was missing completely any documentation suitable for newbies, like tutorials and programming examples, while not being promoted by any hardware vendor, like IBM or DEC, so it was doomed.) reply floxy 18 hours agorootparent>The more I ponder the principles of language design, and the techniques which put them into practice, the more is my amazement and admiration of ALGOL 60. Here is a language so far ahead of its time, that it was not only an improvement on its predecessors, but also on nearly all its successors. https://web.eecs.umich.edu/~bchandra/courses/papers/Hoare_Hi... reply jjice 20 hours agoparentprevI've never written Swift, but it seems like they have it too https://docs.swift.org/swift-book/documentation/the-swift-pr... I also would love a future where ADTs are more common in imperative languages reply odyssey7 20 hours agorootparentSwift “enumerations” are very nice. reply skywal_l 20 hours agoparentprevZig is a modern imperative programming language with ADTs: https://ziglang.org/documentation/master/#Tagged-union reply chongli 18 hours agorootparentWhile this is a step up from C, it is still a long way from the full power and generality of algebraic data types. The key word here is algebraic. In a language with ADTs, such as Haskell, you can pattern match on an arbitrarily complex types, not just the outermost tag. A contrived example (from [1]): contrived :: ([a], Char, (Int, Float), String, Bool) -> Bool contrived ([], 'b', (1, 2.0), \"hi\", True) = False To achieve a result like this using Zig's switch syntax would seem to involve a huge amount of boilerplate code and nested switch statements. [1] https://www.haskell.org/tutorial/patterns.html reply Hirrolot 18 hours agorootparentThis is more of syntax sugar than power and generality, since nested pattern matching can be mechanically translated into \"top-level\" matching (e.g., see [1] and [2]). [1] L. Augustsson. Compiling Pattern Matching. In Functional Programming Languages and Computer Architecture, pages 368– 381, 1985. [2] P. Wadler. Efficient Compilation of Pattern Matching. In S.L. Peyton Jones, editor, The Implementation of Functional Programming Languages, pages 78–103. Prentice Hall, 1987. reply bunderbunder 15 hours agorootparentThis is where I like to cite Dijstra's \"Go To Statement Considered Harmful\". What a lot of people miss about that paper is that he wasn't just talking about goto statements. He was also making a more general observation about how more powerful and general programming language features are not necessarily desirable, because they tend to adversely impact developer productivity. The reason I, as a user, prefer structured control flow statements over goto is not that I believe they are powerful. It's precisely because they are less powerful. The resulting constraints on how the program can be structured make it easier for me to read and reason about existing code. That makes maintaining code easier. It also makes optimization and static analysis easier. And it makes writing tests easier, too. I have similar feelings about ADTs. The reason I prefer them to other ways of doing composite data types is not that I think they're more powerful. It's that they create constraints that tend to reduce the semantic complexity of the domain models people create in programming languages that use them. And that makes my job easier. The corollary to that, though, is that I'm not actually all that hype about adding ADTs to existing languages. For reasons that are similar to how the mere availability of structured, reentrant function calls is small consolation in a codebase that's already riddled with goto statements. The real win doesn't come from using ADTs, it comes from not having to worry about all those other confusing overpowered things that aren't ADTs. reply ajross 15 hours agorootparentThat's exactly where I am. Pattern matched sum types \"feel great\" to code in to an expert because they are a concise and reasonably tight way to express the otherwise boring \"enumerate over possibilities\" code. But they're hard to read for anyone who isn't an expert on not just the language but the type in question (c.f. Rust's Option() idioms all looks like line noise to newbies, etc...). And that's a bad trade. In essence, this stuff is just Perl all over again. It's a language feature that prioritizes concision over comprehension. And I say that as someone who really likes coding in perl. But \"people\" don't like perl, and the community moved on, and the reasons are... the same reason that uptake in ADTs is lagging where the experts want it to be. reply bunderbunder 14 hours agorootparentYou've got to distinguish syntax from semantics, though. I agree, it's easy to turn a big semantic win into a net readability loss if you choose to represent it with an overly terse syntax that promotes code golf. reply ajross 11 hours agorootparentCompilers are really smart. It's easy enough in the modern world to demand that a \"type\" field be checked against all enumerants, preserving the \"semantic win\". A plain old C switch statement with gcc -Wall will do this today, in fact. reply jerf 14 hours agorootparentprevPattern matching on the \"top level branch\" of the ADT, whatever you call it, is pretty darned useful. Pattern matching on the next level down is a power tool to be used with care. Having used some pattern matching languages for quite some time, I find anything much deeper than that is a code smell at best and pathological at worst. Pattern matching creates coupling proportional to the depth/complexity/precision of the pattern match. The top-level coupling is often unavoidable; if you're pattern matching at all, you certainly care which \"branch\" you are on and there is likely no refactoring that away. But the danger rises rapidly the deeper in you go. It's just so easy to pattern match on a third-level part of the complex object when you really ought to be wrapping that behind a function somewhere, possibly itself emitting a sum type value. ... but if all you really need is that \"top level\" match, a lot of pattern matching syntax and features are not really necessary (if not positively dangerous). reply ajross 13 hours agorootparent> Pattern matching on the \"top level branch\" of the ADT, whatever you call it, is pretty darned useful. Pattern matching on the next level down is a power tool to be used with care. Which is exactly how Perl apologia arguments went. reply chongli 18 hours agorootparentprevThis argument is the most common fallacy I see in programming language discussions. I might as well give it a name right here: \"Turing equivalence fallacy\" or perhaps \"syntax sugar fallacy.\" All Turing Complete programming languages are Turing equivalent to one another. Programs written in one language can be mechanically transformed into those written in another. This is irrelevant to the discussion of programming languages. The whole point of creating different programming languages is to explore different ways to express the same program! reply Hirrolot 17 hours agorootparentIn programming language design, we tend to distinguish between global and local analysis. While type checking and elaboration is an example of global analysis, desugaring is inherently local to some piece of code. Therefore, \"power\" or \"expressiveness\" usually mean that something cannot be syntactically \"expanded\"; e.g., while type classes elaborate into explicit dictionaries, they still require information from the type checker, and therefore considered a \"real\" feature of a programming language. On the other hand, nested pattern matching can be formulated as local syntax transformation, and therefore it doesn't bring anything fundamentally new to the type system or dynamic semantics. There's also a great talk on the matter [1], if somebody is interested in formalities. [1] https://www.youtube.com/watch?v=43XaZEn2aLc reply mitt_romney_12 17 hours agorootparentYou're approaching this from a PL design standpoint where the distinction is important, but from a user perspective it doesn't matter if it's just \"syntax sugar\" or if it's a super complicated to implement all that matters is whether the feature is available or not. reply Hirrolot 16 hours agorootparentTyping features affect the way we design APIs. Libraries written in languages with type classes and without them can have completely different designs. If nested pattern matching is not available, this will not affect the APIs, only the function bodies -- because desugaring is local by definition. reply chongli 16 hours agorootparentThat doesn't matter in practice. If two programming languages have the same underlying feature but one has syntactic sugar to make it very easy to use and the other does not (so is quite cumbersome to use) then you'll find that the library ecosystem for the former language will see the feature in widespread use whereas the ecosystem of the latter will tend to shun the feature. This is one of the social factors of programming language design and it's one of the main reasons successful programming languages work so hard to establish a coherent philosophy and a set of best practices or idioms within the language. For similar reasons, I believe this is why \"anything goes\" languages such as LISP have struggled to gain widespread adoption: with no philosophy every programmer becomes an island unto themselves. reply lispm 2 hours agorootparent> \"anything goes\" languages such as LISP have struggled to gain widespread adoption: with no philosophy every programmer becomes an island unto themselves. There are already two misconceptions. First: \"Lisp has no programming philosophies\" and styles. Not every program starts by zero. Since Lisp exists since the end 1950s, it has seen quite a lot in programming styles over the years and it may contain traces of several. Generally it may support more than one programming paradigm. For example during the Common Lisp standardization there was a wish to have a standardized object system. So instead of the multiple possible approaches (actors, message passing, prototype-based, ...), Common Lisp has just one: CLOS, the Common Lisp Object System. So, much of the object-oriented code written in CL is implemented in one particular object system: CLOS. Object Lisp, Flavors, LOOPs, Common Objects, and a bunch of other once had thus been replaced by one standard. CLOS also defines a bunch of user-level macros: DEFCLASS, DEFMETHOD, DEFGENERIC, ... Everyone using CL & CLOS will use those macros. Second: \"every programmer becomes an island unto themselves\". If we look at the way CLOS was designed: there was a core group of six people from three companies. Around that there was a mailing-list based communication with a large group of interested people. Early on a prototype was implemented as a portable implementation of CLOS. This was widely distributed among interested parties: implementors, companies, research groups, ... Then reports about the language extension and its layers were published, books were published, application & library code was published. One of famous books coming out of this effort: \"The Art of the Meta-Object Protocol\". It contained also a toy implementation of CLOS in Common Lisp. Book and the implementation of CLOS (both the larger prototype and the toy implementation) showed in excellent quality how to write object-oriented Lisp code. https://mitpress.mit.edu/9780262610742/the-art-of-the-metaob... So, there are communities, which share code and coding styles. Not every programmer is alone and starts from zero. reply medo-bear 1 hour agorootparentprev> I believe this is why \"anything goes\" languages such as LISP Why do you think that Lisp is an \"anything goes\" language? What's your baseline? I think that C is no less an \"anything goes\" language, but with a much less pleasant UI. > with no philosophy every programmer becomes an island unto themselves Some people actually think that Lispers tend to be too philosophical reply Hirrolot 16 hours agorootparentprevYes, features that are easy to use will be more often used, while inconvenient features will be less used. I don't quite see any controversy with my comment. reply chongli 13 hours agorootparentThe point is that in both cases the underlying feature is present, so APIs will be compatible. However, the lack of syntactic sugar in the one case will make any API that uses the feature cumbersome to use in that language, so in practice it will be avoided. reply tialaramex 16 hours agorootparentprevAbstractly this is true, but software development is a human practice, so it matters not what's technically possible but what people actually do. That's why the most important difference between C++ and Rust isn't some technicality even though the technical differences are huge, it's cultural. Rust has a Safety Culture and everything else is subservient to that difference. Sugar matters, Rust's familiar looking loops are just sugar, it only \"really\" has a single way to do loops, the loop construct, an infinite loop you can break out of. But despite that, people deliberately write the other loops - and the linter strongly recommends that they write them, because the programs aren't just for machines to compile, they're for other humans to read, and a while let loop is an intuitive thing to read for example, so is the traditional for-each style iterator loop. reply OskarS 16 hours agorootparentprevOf course the syntax sugar is a good thing if it makes it easier to write the code, but if the question is about \"expressive power of the type system\", it's not really relevant: Zig's type system can properly express a sum type. In addition: pattern matching is orthogonal to ADT, you can have pattern matching in both languages with and without algebraic types. Neither one implies the other. reply anon-3988 16 hours agorootparent> Zig's type system can properly express a sum type. Surely any Turing complete PL can express a sum type? I can't imagine a language that can support products but not sums. reply Tainnor 15 hours agorootparentTuring completeness has nothing to do with static type checking. Dynamically typed PLs can't express any type (except Any) yet are still Turing complete. reply gpderetta 17 hours agorootparentprev> Turing equivalence fallacy Better known as the Turing Tar Pit. reply xdavidliu 18 hours agorootparentprevsyntax sugar IS power. also if the mechanical transformation is nontrivial, then so is the power of the sugar reply thesz 15 hours agorootparentprevI once explored the Epigram dependently typed programming language. It used to preclude many types of free form pattern matches, due to heavy dependence on structured editor (you speciy a type, it generates pattern matching, you cannot change the structure), so it was almost completely unuseable for many, many tasks. So, while you are formally right, the need of shortcuts in pattern matching is undeniable to me. reply madeofpalk 19 hours agorootparentprevAlso Typescript https://www.typescriptlang.org/docs/handbook/2/everyday-type... reply fanf2 18 hours agorootparentUnion types are not the same as sum types. reply dtech 18 hours agorootparentTS narrows union types cases based on conditionals like \"if\" (called discriminated unions in the docs in the past), and supports exhaustiveness checks. How do they differ in functionality from sum types? reply nyssos 17 hours agorootparentSum types are disjoint unions. This `T` has three cases L = { tag: \"a\", payload: string }{ tag: \"b\", payload: number } R = { tag: \"b\", payload: number }{ tag: \"c\", payload: boolean } T = LR whereas a proper sum type `L + R` would have four. reply brabel 16 hours agorootparentIsn't that a completely useless distinction? For all purposes and intents, the \"b\" type in L and R should be treated the same, no? What do you gain by not doing that?? reply hexane360 16 hours agorootparentThis often comes up when writing a function which returns a wrapper over a generic type (like Option). If your Option type is Tnull, then there's no way to distinguish between a null returned by the function or a null that is part of T. As a concrete example, consider a map with a method get(key: K) -> Option. How do you tell the difference between a missing key and a key which contains `null` as a value? reply epolanski 14 hours agorootparent`Tnull` is equivalent to T. You can assign null to `T`> It's like saying `string\"foo\"` it is simply `string` due to subtyping. reply brabel 15 hours agorootparentprevThis is trivial to model by making your type `TnullMissing`. reply epolanski 14 hours agorootparentOr just using Option since you would have Some or None in that case. reply efnx 14 hours agorootparentprevMaybe trivial to “work around” but there is a difference, ay? With this type you would have to check/match an extra case! The type you use there also takes more memory than Option or Maybe. So it has some other downsides. reply Twisol 14 hours agorootparentprevNo, it isn't \"completely useless\". If you have a function that will normally return a string, but can sometimes fail due to reasons, you may wish to yield an error message in the latter case. So you're going to be returning a string, or a string. It's not what the content of the data is; it's how you're supposed to interpret it. You have two cases, success and failure, and control will flow differently depending on that, not based strictly on the type of data at hand. We just model those cases in a type. reply brabel 3 hours agorootparentNo disrespect, but that still sounds entirely useless to me. I would never model something as `StringString` as that makes zero sense. You should use a `Result` or `Either` type for that like everyone does. reply akavi 14 hours agorootparentprevAs you've demonstrated, you can always construct sum types in typescript with the use of explicit discriminants: T = {tag: \"L\", payload: L}{tag: \"R\", payload: R} The real issue is typescript doesn't have pattern-matching, which make operating on these sum types inelegant reply mpawelski 18 hours agorootparentprevSupports exhaustiveness checks only if you explicitly opt-in it (by coding to pattern where you use helper function that accepts `never` type). \"Dicriminated Unions Type\"/\"Sum Types\" feels very hacky there, at least syntax-wise, because it is constraint by being \"JS + types\" language. It's remarkable what Typescript can do, but having native Discriminated Unions in JS (hence in TS too) would be much more ergonomic and powerful. reply chem83 18 hours agorootparentprevF# too. And Elm. But I get your point. reply CraigJPerry 18 hours agoparentprev>> not having ADTs (except maybe Rust) built-in Most of the common languages today have product types. Java[1], Rust, Haskell, etc. have sum types. I think it gets a bit more escoteric beyond that though - i don't doubt that there's probably some haskell extension for quotient types[2] or some other category theory high-jinx. Most languages have ADTs built in. [1] https://blogs.oracle.com/javamagazine/post/inside-the-langua... [2] https://en.wikipedia.org/wiki/Quotient_type reply nextaccountic 17 hours agorootparentDoes Java sealed classes enable something like an exhaustive pattern matching? (A form of pattern matching that will fail at compile time if you add a new class that extends the sealed class) reply brabel 16 hours agorootparentYes, since Java 21. Example: sealed interface BinaryTree { record Leaf(int value) implements BinaryTree {} record Node(BinaryTree lhs, BinaryTree rhs, int value) implements BinaryTree {} } public class Hello { static int sum(BinaryTree tree) { return switch (tree) { case BinaryTree.Leaf(var value) -> value; case BinaryTree.Node(var lhs, var rhs, var value) -> sum(lhs) + value + sum(rhs); }; } public static void main(String... args) { var tree = new BinaryTree.Node(new BinaryTree.Leaf(1),new BinaryTree.Node(new BinaryTree.Leaf(2),new BinaryTree.Leaf(3),4),5); System.out.println(tree); System.out.println(\"Sum: \" + sum(tree)); } } If you added a new subtype to BinaryTree you would need to fix the switch. EDIT: I didn't handle the `null` case above... so it would be a NullPointerException if someone passed null... apparently, Java decided to make handling `null` optional. More information: https://www.baeldung.com/java-lts-21-new-features reply nextaccountic 16 hours agorootparentOkay that's better than I expected! reply davidalayachew 16 hours agorootparentprevIt absolutely does. Here is a (modified) snippet of my Java code from yesterday. final boolean hasUncollectedSecret = switch (each) { case Wall() -> false; case Goal() -> false; case Player p -> false; case BasicCell(Underneath(_, var collectible), _) -> switch (collectible) {case NONE, KEY -> false; case SECRET -> true;}; case Lock() -> false; }; reply thewakalix 17 hours agorootparentprev> The intent is to introduce a more-advanced construction called pattern matching in a later release. reply brabel 16 hours agorootparentYou read that in a blog post from 2019. Java has had comprehensive pattern matching since Java 21, like one year ago (current Java version is 22). I posted an answer to the same parent comment with the C example written in Java... You can read more about it here: https://www.baeldung.com/java-lts-21-new-features reply brabel 16 hours agorootparentprevPlease don't forget Dart! https://medium.com/dartlang/dart-3-1-a-retrospective-on-func... reply LegionMammal978 17 hours agorootparentprevJava's sealed classes are still somewhat more limited than Rust's or Haskell's sum types, in that each instance of the superclass holds a fixed variant (i.e., subclass), so you can't change the variant without creating a new instance. Clearly, this limitation is necessary for references to stay intact, but I've personally ran into this issue when trying to represent a sum type in an ORM. reply munificent 16 hours agorootparent> so you can't change the variant without creating a new instance. Isn't that true of ADTs in all languages? I can't think of a single language with ADTs that lets you change the tag/variant of an existing value. reply LegionMammal978 15 hours agorootparentIn Rust [0]: #[derive(Debug)] pub enum Example { Foo(i32), Bar(&'static str), } let mut ex: Example = Example::Foo(42); println!(\"{ex:?}\"); // Foo(42) let ex_ref: &mut Example = &mut ex; *ex_ref = Example::Bar(\"hello\"); println!(\"{ex:?}\"); // Bar(\"hello\") Given a mutable reference to a value of enum type, you can replace it with another variant. Or you can swap it out with any other value of the same type, even if the variants are different. This is most commonly used for Option, where you can insert or remove the contained value as long as you have a reference. The limitation here is that for as long as the mutable reference is live, no other code can access the value. So when you do change the variant, you can't have any other references sitting around that point to the removed subfields. [0] https://play.rust-lang.org/?version=stable&mode=debug&editio... reply munificent 13 hours agorootparentThis doesn't have anything to do with ADTs. It's because Rust has both in-place variables and references. You aren't changing the variant of an existing ADT, you're replacing the entire ADT value with a new one. That replacement is visible in multiple places because the language allows you to take references to variables (the `&mut ex` expression). You can accomplish the same thing in C and C++ because they also have in-place value semantics and allow you to take the address of any variable. You can't do that in Java only because Java doesn't let you take references to variables. reply LegionMammal978 10 hours agorootparent> You aren't changing the variant of an existing ADT, you're replacing the entire ADT value with a new one. 'Values' in Rust have no identity, except for their address. They're just a bunch of bytes in a row. What could it mean for a value to exist, except for it to be present at a set place in memory? If I have an instance of a Java class, and I change all the fields, I'd hardly say the instance has been replaced with a new instance. If you insist, I'd say \"Java's sealed classes are more limited in that when you have a bunch of references to the same thing, you can change the values in the fields of that thing (and have it be reflected in other references), but you can't change which variant it is.\" Call that thing a 'value' or a 'variable', it doesn't change the visible outcome compared to enums in Rust, or discriminated unions in C/C++. reply brabel 16 hours agorootparentprevI don't really think it's useful to do that though?? Can you give an example? By the way, I would claim Java's sum types are less limited than Rust because in Rust, variants don't have their own type. The consequence is that you can't have functions that only accept some variant, as far as I know (I remember having this problem once), or add \"methods\" only to one variant... while in Java, because variants are just normal types, you can do both, and doing that is pretty damn useful. reply LegionMammal978 15 hours agorootparent> I don't really think it's useful to do that though?? Can you give an example? For an exercise, I had to write a system with Admins and Customers, with the ability to upgrade a Customer into an Admin, or vice versa. My thought was to put them as two subclasses under a User superclass, so that I could put them under a single Users table, and not have to link and unlink things over the conversion. Hibernate ORM supports storing subclasses by adding an implicit discriminator field. However, its object model specifies that a single row always corresponds to a particular instance, so it has no support for changing the subclass of a row. Ultimately, I ended up with a hacky solution of creating a new record with the primary key copied over. > By the way, I would claim Java's sum types are less limited than Rust because in Rust, variants don't have their own type. The consequence is that you can't have functions that only accept some variant, as far as I know (I remember having this problem once), or add \"methods\" only to one variant... while in Java, because variants are just normal types, you can do both, and doing that is pretty damn useful. At least in Rust, you can simulate this pretty trivially by having each variant store a struct value with all the data and methods you want. See proc_macro::TokenTree [0] for an example of this. Of course, it's not ideal in how verbose it is (though not much worse than Java!), but it can be workable on the consumer's side if you add some extra From impls. [0] https://doc.rust-lang.org/proc_macro/enum.TokenTree.html reply thewakalix 17 hours agorootparentprevJava doesn't have pattern-matching yet. Haskell is not an imperative language. reply speed_spread 18 hours agorootparentprevJava sum types work but still need a bit of syntax sugar on the declaration side, IMHO. reply j2kun 17 hours agorootparentprevYour examples, on the TIOBE index, are #4, #18, and #28. https://www.tiobe.com/tiobe-index/ reply CraigJPerry 12 hours agorootparentProduct types are one kind of algebraic data type, only 5 languages from that TIOBE page don’t have them so most common langs have ADTs reply dannymi 11 hours agorootparentWhen you have natural numbers and a multiplication operation (product) would you say that those form an algebra? (ADT means algebraic data type) reply CraigJPerry 3 hours agorootparentYou’ve got both a carrier (the set of natural numbers) and a morphism (product operation) so yeah you have an algebra. reply Verdex 19 hours agoparentprevNAND is a universal circuit primitive because it can be used to create all of the other circuit primitives. But if you think about it, this is more of an argument of manufacturing than it is in comprehensibility. Only needing to manufacture NAND is easy, but if you could only create your circuit this way, then you would have an unmaintainable mess. You can do the same thing with boolean logic and just have not-and, but thankfully we have and, or, not, xor. Similarly, you don't need greater-than-or-equal because you can just write 'x > y || x == y'. Comprehension is linked to how closely you can express the idea of what you're doing in the object language that you have to look at. It might be convenient to compile everything down to SK combinators so that your optimizer and evaluator can be simpler, but people should never look at that level (at least not until you suspect a compiler defect). So we get to object oriented programming. Where our data expression has an AND property (a class has an INT field AND a STRING field), an existential property (interfaces: there exists some object with these methods), and inheritance (a truly bizarre feature where we duck tape subtyping to a method and field grouping mechanism with a bunch of hooks). With interfaces and inheritance you can simulate both a universal property (generic) and an OR property. But because it's not a direct expression, we leave this giant gap for what people intended to happen to diverge from what actually happens. Especially after time passes, defects are found, and requirements change. [For example, when using interfaces to simulate an OR property, there really isn't any mechanism to let everyone know that this construct is closed. So if something erroneously gets added, you won't know to check the entire code base. And if requirement change and you need to add a new case, then you have to check the entire code base. Completeness checking of ADTs give you this for free in your pattern matches.] Too many non-trivial architectural messes that I've encountered in my career have been due to either someone trying to solve all of their problems with interfaces or the same with inheritance* when a simple OR data structure would have made everything simple, clear, and correct. [*] - Inheritance being more problematic when someone tries to create a non-trivially sized category hierarchy, which ruins the day when requirements change and suddenly the tree needs to be reorganized but doing so would invalidate entire swaths of the code base already accepting types with a different assumed (and undocumented) hierarchal tree structure. Thankfully most people have gotten the memo and switched to interfaces. reply debo_ 20 hours agoparentprevADT feels like an unfortunately acronym-collision with \"Abstract data types.\" reply keybored 3 hours agorootparentADT feels like an unfortunately acronym-collision with \"algebraic data types.\" They were both introduced in the same decade. reply jghn 19 hours agorootparentprevMore often than not, when I say ADT to someone outside of the FP world, they assume I mean abstract data type. reply bee_rider 18 hours agorootparentOr the company that sells the security stickers, for houses. reply pjmlp 20 hours agorootparentprevYep, people that eventually buy a Modula-2 ADT book, when hunting old stuff, are in for a surprise. :) reply debo_ 19 hours agorootparentIt's a term that is commonly used in computer science education to refer to any data type independent of its concrete implementation (so, basically, its interface.) I don't think it's just restricted to Modula-2? reply pjmlp 19 hours agorootparentIndeed, however CLU and Modula-2 were the ones used mostly for practical teaching purposes, until ML became widespread enough for ADT to gain yet another meaning. reply thesz 15 hours agoparentprev\"Haskell is the best imperative language,\" (C) various software engineers. Also, algebraic data types can be seen as hierarchy consisting of abstract base class and several final children classes. So it is an inheritance model, just restricted one. reply tombert 20 hours agoparentprevYeah, when I first learned Haskell a million years ago, and Erlang slightly less than a million years ago, the pattern matching was so plainly obviously the \"correct\" way to do things; it just felt like it was exactly how I thought about problems, and all the constructs with if/switch/enums had been an attempt to force my brain thinking into something that executes. It honestly does annoy me that a lot of mainstream languages still haven't really adopted ADTs; when Java 8 added a lot of (well-needed) new syntax, it felt like that was an ideal opportunity to add ADTs and pattern matching (though I'm sure that was easier said than done). reply kaashif 20 hours agorootparent> when Java 8 added a lot of (well-needed) new syntax, it felt like that was an ideal opportunity to add ADTs and pattern matching Well at least Java does now (as of Java 21) have pattern matching (including nested record destructuring) and sealed classes, which let you have decent sum types. The one issue is that everything is nullable, but that's a wider Java issue. reply tombert 19 hours agorootparentYeah, but the annoying part of Java is that people stick with old versions for a long time. Java 21 looks pretty great but most companies are still using Java 17, or even Java 11 still. reply cess11 18 hours agorootparentSome have even older Java versions in 'prod'. 6 is still alive in some places out there, because management refuses to pay for either upgrade, replacement or dismantling. I have a mid-sized application I built on 17 that's used to deliver a particular project, really looking forward to finish the project so I get to move to 21 and refactor it with these new features and make it more general. reply tombert 17 hours agorootparentOof, I didn't know that anyone still used Java 6 anywhere. I have to think that it's a potential security nightmare at this point isn't it? reply Tainnor 15 hours agorootparentIt is. We run an old version of our application on Java 6. Apparently multiple people have tried to upgrade it in the past but it proved too difficult because it's using a bunch of completely obsolete technology with little available documentation that seems to randomly break when you upgrade even minor things. The plan has been to gradually replace it with the new version of the software (which runs on Java 17), unfortunately this plan has been ongoing for 10 years and it's not gonna be done anytime soon. Such are the sad realities of working on legacy code sometimes. reply cess11 13 hours agorootparentprevAbsolutely, so you try to seal it in hermetically, only talk to it over a special message bus or something. Sometimes the upgrade path from 6 onwards isn't as nice as it usually is from 8 up, especially if you built with some old undead libraries that require an heroic effort to understand well enough to reimplement. Takes a very special organisation to divert some person-years to pull it off, and as some middle or other manager it's highly unlikely to catch you a new, better job even if it goes well. reply tombert 13 hours agorootparentIt makes me sad, but I totally understand why someone would stay with Java 6, for the same reason that there's still COBOL stuff hanging around: the stuff \"works\" as is, upgrading is expensive, and some business person decided that the cost of the upgrade isn't worth it for the value they'd receive. I do worry that there's \"Y2K-esque\" bugs hiding in Java 6 programs somewhere. I don't think java.util.date uses 32 bit integers for time so the 2038 problem probably won't be an issue, but I do wonder if there's other stuff hiding. reply cess11 13 hours agorootparentThere might be some limit or bug like that in Java 6, but I think time stuff has been backported, so it'd likely be something else. I don't know enough about the JVM internals to speculate on whether the old ones were simpler and less likely to be defective, or not. By now most types of issues ought to have popped up though, since it was so widely used for such a long time. reply taeric 19 hours agoparentprevI'm curious on supporting evidence for it being a basic mental model of how humans think? That sounds like a fairly strong claim. reply Verdex 18 hours agorootparentI'm a huge proponent of ADTs being a more comprehensible way to write code than some of the alternatives. But I do have to agree with you that there isn't really evidence that this is a basic mental model. However What we do see is a bunch of mathematical disciplines that end up creating properties like: AND, OR, Universal, Existential, Implication, (and a few others). They end up in places like: set theory, type theory, category theory, various logics, lattice theory, etc. Now, maybe they're only copying one another and this is more of a memetic phenomena. Or maybe they've hit upon something that's important for human comprehensibility. That would be the 'evidence' of the positive effect of ADTs (scare quotes because it might just be math memes and not fundamental). But we can also think about what I feel is legit evidence for the negative effect of lacking ADTs. Consider what happens if instead of having the standard boolean logic operators and, or, not, xor, we only have the universal not-and operator. Now a straightforward statement like: A && B || C becomes (((A !& B) !& (A !& B)) !& ((A !& B) !& (A !& B))) !& (B !& B) [I think...]. It's more complicated to tell what's actually supposed to be going on AND the '&&' simulation can get intertwined with the '||' simulation. The result being that requirements changes or defect fixes end up modifying the object level expression in a way where there is no longer any mapping back to standard boolean logic. Comprehensibility approaches zero. And we've seen this happen with interfaces and inheritance being used to implement what would otherwise be a relatively simple OR property (with the added benefit that pattern matching ADTs often comes with totality checking; not something you can do with interfaces which can always have another instance even up to and including objects loaded at runtime). reply taeric 17 hours agorootparentAppearing in symbolic reasoning tools we have invented doesn't really support them being how brains work, though? This is akin to saying that gears are how nature works because gears are everywhere in how we build things. I could maybe buy that with \"friction\" being a fundamental thing, but feels like a stretch for the other. Now, I should add that I did not mean my question to be a criticism of them! I'm genuinely curious on evidence that they are a basic building block. Feels save to say they are a good building block, and those aren't the same thing. As an easy example for them not being basic building blocks, I can't remember ever seeing anything like them in any assembly instructions for things. Put together a batting net for the kids. Lots of instructions, but nothing algebraic, in this sense. Looking at recipes for food. Nothing algebraic, really? Maybe I can squint and see some, but it would be hard. Exercise plans? Music lessons? Playbooks for a sport? Again, though, I /do not/ intend this as a criticism of them. Genuinely curious on any investigation into them. reply humzashahid98 14 hours agorootparentI think you're right to point out that it's too strong a claim to say that sum types are a basic building block of thought, although I believe they are very useful in coding regardless of that claim. There is the still the ongoing debate about how much human perception and human reason are shaped by cultural forces vs. universal forces (where the latter asserts humans reason in the same/similar ways). There's evidence that certain optical illusions don't work across cultures for example (I seem to remember those in Western countries have a tendency to mentally group things in rectangular boxes). The exact balance between cultural and universal forces isn't known and I doubt we could say anything about sum types in that regard. reply naasking 18 hours agorootparentprevVerdex's explanation is detailed but too long IMO. The short version is that ADTs/sum types formally correspond to the OR-logical connective, and records/product types formally correspond to AND-logical connective. I think you'd be hard-pressed to argue that people don't think in terms of \"X AND Y OR Z\". These are core primitives of any kind of reasoning. reply taeric 17 hours agorootparentI can easily argue that people don't think in terms of boolean logic. For one, the evidence seems as strong that people generally think backwards from the answer far more than they do forwards from the ingredients. This is often why new things are so long to be discovered. It isn't that people couldn't have gotten there, but they didn't know to go for it. For two, addition is a wildly disparate thing everywhere we use it. We like to joke that computers made that hard, but literally half of intro chemistry is learning how to get thing to add together in a meaningful way, no? Balancing a chemical equation is a thing. reply naasking 17 hours agorootparent> For one, the evidence seems as strong that people generally think backwards from the answer far more than they do forwards from the ingredients. Logic doesn't really have a direction, it works backwards or forwards. Even if you're solving a system \"backwards\", whatever that means, you still have to satisfy all of the necessary AND and OR constraints for a solution to be valid, so you're effectively still building ADTs or records just using a different evaluation order. reply taeric 13 hours agorootparentAnd this logic is how folks convince themselves that ball players are doing trigonometry when playing. It is just wrong. You can /model/ it that way. But you are making a symbolic model to justify how a solution is reached. Now, it can be frustrating to consider that this model could produce an agent that is better at the ball game than the players. But it is silly to think that means you have mirrored them. reply naasking 10 hours agorootparent> And this logic is how folks convince themselves that ball players are doing trigonometry when playing. It is just wrong. You're attempting a sleight of hand here by saying \"they're\" not \"doing trig\". Clearly they are not doing anything like that consciously, but equally clearly some part of their brain is triangulating objects and predicting trajectories based on gradients, meaning that part is \"doing trig and calculus\" subconsciously. What else does it mean to \"do something\" if not \"process X is isomorphic to process Y\"? > You can /model/ it that way. But you are making a symbolic model to justify how a solution is reached. I really don't understand what you think people are doing when they're compiling a grocery list. They're clearly thinking, \"I need x AND y OR I can substitute z\". Or if they're planning to paint their fence, they're thinking, \"I need paint AND brushes AND I have to start before lunch OR I won't finish before dinner\". reply taeric 7 hours agorootparentNo. You are confusing the model for the reality. Again, our model may lead to a more impressive reality, but ball players are not doing trig. They are almost certainly simulating things, but that does not require trig. Indeed, it only loosely requires math. Models can be very informal with loose approximations. You seem to think I have to prove your model false to show others don't do that. But I am specifically not claiming your model is false. I'm saying folks don't think that way, necessarily. For example, many build lists for shopping that they were taught. Not that they reasoned. reply naasking 5 hours agorootparentThe ability to build lists requires reasoning. It requires enumeration, inclusion/exclusion conditions, and stop conditions, which necessarily requires logic. This argument is pointless, you go on believing that some people can't use basic logical connectives, I'm sure next you'll say they can't even count. reply samatman 9 hours agorootparentprev> Logic doesn't really have a direction, it works backwards or forwards. Implication is one of the primitives in logic, and gives us several of the classic logical fallacies: affirming the consequent, denying the antecedent, fallacy of the converse, and fallacy of the inverse. All of which are examples of trying to work logic as though it doesn't have a direction. reply naasking 5 hours agorootparentImplication is not primitive, \"x->y\" is reducible to \"not(x) or y\". None of those fallacies derive from a lack of direction, but from not being invalid logical deductions. reply Tainnor 15 hours agoparentprev> except maybe Rust Swift, Kotlin and Scala all have had ADTs for a while, even Java has it now. reply zozbot234 20 hours agoparentprevPascal has had variant records since the 1970s. reply adrian_b 20 hours agorootparentBut Pascal's variant records (1970-11) had very ugly design errors in comparison with the unions of Algol 68 (1968-12), which made them either useless or annoying for most applicatons. Niklaus Wirth is well known as a critic of Algol 68 (before the design of Algol 68 was finalized), but in the case of his variant records he has completely failed to create something competitive. reply mzs 11 hours agorootparentDoesn't look too bad particually the match tagged version last: MyType= (Scalar4, Real4, NullTerminatedStringC); MyUntaggedRecType= RECORD CASE MyType OF Scalar4: (longC: ARRAY[1..150] OF longint); Real4: (floatC: ARRAY[1..150] OF real); END; MyTaggedRecType= RECORD CASE tag: MyType OF Scalar4: (longC: ARRAY[1..150] OF longint); Real4: (floatC: ARRAY[1..150] OF real); END; ... { set all to 0.0 without running through the MC68881 } FOR j := 1 TO 150 DO longC[j]:= 0; ... CASE tag OF Scalar4: percentReal = longC[1]; floatC: percentReal = floatC[1]*100; ELSE percentReal = 0.0/0.0; edit: don't have a pascal compiler handy, but that's the idea reply pjmlp 20 hours agorootparentprevGiven where Algol 68 ended up, I would say Wirth was quite right. reply adrian_b 20 hours agorootparentAlgol 68 was a failure mainly due to its inappropriate documentation, not due to the quality of the language. It included many innovations that appeared again in other programming languages only decades later. Niklaus Wirth was a good teacher and writer and the success of his languages is due mostly to his books and due to his languages being used for teaching in many universities, not due to their technical qualities. reply cryptonector 15 hours agorootparentAlgol 68 may have been a failure because it was ahead of its time, which meant it was hard to write a compiler for it. For example, Algol 68 had closures (apparently Knuth snuck them in). reply pjmlp 19 hours agorootparentprevI beg to differ, after Modula-2 and Object Pascal (Apple designed it with Wirth's feedback). Most of his languages ended up losing, because they weren't being shipped with a free beer OS, coupled with a free beer compiler. reply peoplefromibiza 19 hours agorootparentprev> not due to their technical qualities AFAIK Pascal is C and Algol 68 is C++ people used Pascal because the compiler was blazing fast, it was easier to implement and learn and the features it lacked against Algol did not really matter most of the time (at the time) More features doesn't automatically means \"better\" Also Pascal had quite strong technical qualities, not very common among other contemporary languages edit: can I ask the reason for the downvote? I would really like to hear an opinion on what Pascal did wrong, having used it extensively in the late 80s until the end of the 90s and why my comment was awarded with a negative score. reply adrian_b 17 hours agorootparentExtended variants of Pascal, like Turbo Pascal, should not be confused with the Pascal language as designed by Niklaus Wirth. Wirth's Pascal was a language designed for the purpose of teaching programming and it was adequate for that, but it was completely inappropriate for any serious work. It had no means for writing big programs that must be divided into multiple source files and it had a lot of design errors, like including the size of an array in its data type (which made impossible the writing of any linear algebra library) or the way of handling the variant records (which was insecure and verbose). The well known paper \"Why Pascal Is Not My Favorite Programming Language\" by Brian W. Kernighan contains valid arguments against Pascal (against Wirth's Pascal, there have been many extended Pascals, especially following Turbo Pascal, which have corrected some of the most egregious defects of the original Pascal). reply pjmlp 17 hours agorootparentPeople keep playing this tired song, while clearly ignoring that until ISO C89, even C was full of dialects, a mix of K&R C and whatever the compiler writer felt like, filled with tons of Assembly. Small-C, RatC,.... Additionally forgetting that Modula-2 came out in 1978, as means the sort out all those issues, a language designed for systems programming, instead of the \"Python\" from 1970, designed for teaching. With features that C is yet to have, 50 years later, while HN is hypping for having them in a C like syntax delivered in a Zig package. reply peoplefromibiza 16 hours agorootparentprevI know that paper and personally I think some of the critiques are actually qualities of pascal, making it, while certainly not a completely refined language, a more modern language than C - no escape (AKA no casting): good! - no default clause in case: good idea, not so good implementation (undefined behaviour) - no break outside for loops: inconvenient, but that's how FP works. it is still debated today if breaking loops is considered a good or a bad practice - no separated compilation: I will quote Kernighan on this Theoretically, there is no need for separate compilation - if one's compiler is very fast Pascal compiler was fast, maybe not very fast, but speed was one of the primary goals for Wirth. many other issues were similar in other languages and in C Pascal had obviously its faults, but every language back then had some Pascal was simple enough to make it easy to compile and implement. That's what Wirth thaught, he's the author of Compiler Construction after all, it wasn't like learning Python today as a data scientist make the language more complex (and more useful/interesting) and you're stuck with a very slow or very buggy compiler that very few people would know how to implement I think there's a reason why we had Turbo Pascal and not Turbo Algol 68 reply mgaunard 19 hours agoparentprevC has always had them, it's called union. In practice you need to couple it with an enum, and your visitation mechanism is a switch statement. But C doesn't impose that on you and lets you do it as you see fit. reply duped 18 hours agorootparentYou're confusing semantics for implementation. The point of union and discriminated union types (not what C calls union) is to enable compiler checked pattern matching, which tagged enums in C plus a switch statement do not get you. reply coldtea 18 hours agorootparentprev>C has always had them, it's called union It also has all the features of Haskell, since you can implement a Haskell compiler in C. reply estebank 18 hours agorootparentprevTagged unions + pattern matching is what gp wants. You can always encode whatever model you want using any programming language, but language features/ergonomics matter. reply naasking 18 hours agorootparentprevThat you can sort of simulate the skeleton of algebraic data types does not mean that C has algebraic data types. The whole point of the algebra part is that the syntax has a compositional semantics which is completely absent in C, unless you go to great lengths as with this macro header. reply anon-3988 16 hours agorootparentprevlol this is like saying C doesn't need structs, you can just declare the variables with a common prefix separately! See ma, product types! reply mgaunard 48 minutes agorootparentThat doesn't work since you cannot pass that as a single argument to a function. reply anon-3988 24 minutes agorootparentSo? make the function accept multiple arguments? reply grumpyprole 16 hours agorootparentprevYep, or C doesn't need arrays just use pointers! And C doesn't need strings, just use a pointer to the first byte and assume it's ASCII! reply bmoxb 17 hours agorootparentprevThat is not a proper alternative to real pattern matching. reply Alifatisk 16 hours agoparentprevIsn’t ADT abbreviation for Abstract Data Type? Or does it depend in context nowadays? reply Jaxan 16 hours agorootparentYou answered your own question: it depends and the context and is confusing imo. Both are very common in compsci reply Alifatisk 45 minutes agorootparentNo I didn’t, I provided two options and asked which case it was. I could’ve assumed OP had used the wrong abbreviation. reply rowanG077 16 hours agorootparentprevContext. It means algebraic data type here. reply drycabinet 8 hours agorootparentprevWait until you switch to unions in rust and ask yourself whether it is a union or a struct. reply Alifatisk 44 minutes agorootparentOh dear reply epolanski 14 hours agoparentprevMay not be built in but many mainstream languages such as typescript have libraries or the tools to easily implement them. reply ajross 17 hours agoparentprev [–] > [Algebraic Data Types are] such a basic mental model of how humans think and solve problems I think that's actually wrong for \"Sum types\". Product types, sure. The idea of storing a bunch of fields in a single thing matches the way we've been organizing information since we started writing things down. But I genuinely don't think I've seen an attempt at a sum/union/enumerant/whatever syntax in a programming language that wasn't horrifyingly confusing. Where by extension: class-based inheritance is actually pretty simple to understand. The classic \"IS-A\" relationship isn't as simple as \"fields in a struct\", but it's not hard to understand (c.f. all the animal analogies), and the syntax for expressing it is pretty clean in most languages. Is it the \"best\" way to solve a problem? Maybe not. Neither are ADT sum types. But I think there's a major baby-in-the-bathwater problem with trying to be different. I really don't think, for the case of typical coders writing typical code, that ADTs are bringing as much to the table as the experts think. reply throwawaymaths 8 hours agorootparent [–] > class-based inheritance is actually pretty simple to understand Simple to understand, a nightmare to debug, as you'll be chasing where your data and data contracts across a ton of files. reply ajross 6 hours agorootparent [–] That's a bit much. Type inheritance has been a core abstraction in software development since before most working developers were born. We as a society know how to do this. The idea that one oddball new idea is a revolution that turns a \"nightmare\" into sunshine is way too hyperbolized. Sum typing might be better! But frankly the jury is still out, and the impact is clearly going to be smaller than what you're imagining. reply throwawaymaths 6 hours agorootparent [–] The new lowest level pls (zig, rust) have ditched class based inheritance. Higher level PLs are going more functional, to include JS, where entire frameworks are encouraging functional (not to mention how everyone complains about the opacity of trying to use inheritance in place of declarative i.e. Amazon CDK) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Datatype99 is a secure C99 library for algebraic data types, ensuring type safety, portability, predictability, and clear error messages in real-time streaming software.",
      "It offers syntax sugar for tagged unions, safe pattern matching, and guidelines for code organization, error handling, and data type optimization in C programming.",
      "The library supports Metalang99 macros, simplifying data manipulation and guaranteeing compiler-time safety."
    ],
    "commentSummary": [
      "The discussion on Github delves into the advantages and constraints of Algebraic Data Types (ADTs) and pattern matching in different programming languages.",
      "Users value ADTs for their flexibility and concise code, expressing dissatisfaction with languages lacking these features, such as Go.",
      "The conversation also explores unions in object-oriented programming, macros in languages like C, Rust, and Swift, and the introduction of pattern matching in Java, underlining the importance of syntax sugar and the benefits of utilizing Zig and Nim as C alternatives."
    ],
    "points": 347,
    "commentCount": 191,
    "retryCount": 0,
    "time": 1715254287
  },
  {
    "id": 40310228,
    "title": "OpenAI's Partnership Pitch to News Publishers",
    "originLink": "https://www.adweek.com/media/openai-preferred-publisher-program-deck/",
    "originBody": "Presented by NEWFRONTS Leaked Deck Reveals How OpenAI Is Pitching Publisher Partnerships OpenAI's Preferred Publisher Program offers media companies licensing deals OpenAI has been courting publishers with select benefits through its Preferred Publisher Program.Hasan Mrad/UCG/Universal Images Group via Getty Images By Mark Stenberg Mark your calendar for Mediaweek, October 29-30 in New York City. We’ll unpack the biggest shifts shaping the future of media—from tv to retail media to tech—and how marketers can prep to stay ahead. Register with early-bird rates before sale ends! The generative artificial intelligence firm OpenAI has been pitching partnership opportunities to news publishers through an initiative called the Preferred Publishers Program, according to a deck obtained by ADWEEK and interviews with four industry executives. OpenAI has been courting premium publishers dating back to July 2023, when it struck a licensing agreement with the Associated Press. It has since inked public partnerships with Axel Springer, The Financial Times, Le Monde, Prisa and Dotdash Meredith, although it has declined to share the specifics of any of its deals. A representative for OpenAI disputed the accuracy of the information in the deck, which is more than three months old. The gen AI firm also negotiates deals on a per-publisher basis, rather than structuring all of its deals uniformly, the representative said. “We are engaging in productive conversations and partnerships with many news publishers around the world,” said a representative for OpenAI. “Our confidential documents are for discussion purposes only and ADWEEK’s reporting contains a number of mischaracterizations and outdated information.” Nonetheless, the leaked deck reveals the basic structure of the partnerships OpenAI is proposing to media companies, as well as the incentives it is offering for their collaboration. PUBLISHERS’ AI LICENSING NEGOTIATIONS MARK AN INFLECTION POINT Details from the pitch deck The Preferred Publisher Program has five primary components, according to the deck. First, it is available only to “select, high-quality editorial partners,” and its purpose is to help ChatGPT users more easily discover and engage with publishers’ brands and content. Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Finally, through PPP, OpenAI also offers licensed financial terms to publishers. The financial incentives participating publishers can expect to receive are grouped into two buckets: guaranteed value and variable value. Guaranteed value is a licensing payment that compensates the publisher for allowing OpenAI to access its backlog of data, while variable value is contingent on display success, a metric based on the number of users engaging with linked or displayed content. The resulting financial offer would combine the guaranteed and variable values into one payment, which would be structured on an annual basis. “The PPP program is more about scraping than training,” said one executive. “OpenAI has presumably already ingested and trained on these publishers’ archival data, but it needs access to contemporary content to answer contemporary queries.” In return for these payments, OpenAI would gain two benefits. It would have the ability to train on a publisher’s content and the license to display that information in ChatGPT products, complete with attribution and links. It would also get to announce the publisher as a preferred partner and work with them to build out these experiences. Participation boosts publisher payouts According to the deck, publisher participation in PPP creates a better experience for OpenAI users, which will help shift engagement toward browsing, i.e. queries that result in responses with links. Roughly 25% of ChatGPT users already use the browse function, but the company expects that a majority of users will do so once the feature is broadly rolled out. If more users engage with publishers’ links, the media companies could earn larger payments for their variable value. PPP members will see their content receive its “richer brand expression” through a series of content display products: the branded hover link, the anchored link and the in-line treatment. In the hover treatment, which is available today, OpenAI will hyperlink keywords in its responses to search queries. The links appear as blue text and reveal a clickable tab when moused over. In the anchor treatment, branded, clickable buttons appear below ChatGPT’s response to a user query. And the in-line product inserts a pullquote into the text of ChatGPT’s response, whose font is larger and includes a clickable, branded link. All three content display products seek to cite the publishers whose writing is being used to answer the search query, although the setup will likely lead fewer users to visit publishers’ websites. A recent model from The Atlantic found that if a search engine like Google were to integrate AI into search, it would answer a user’s query 75% of the time without requiring a clickthrough to its website. Where publishers go from here The details of the program add further color to the complicated relationship between digital publishers and OpenAI. The uncertain legal standing of the data-scraping methodology that OpenAI uses to power its large-language models has made licensing negotiations between the two parties complex. While some publishers have opted to partner with OpenAI, others, including recent NewFronts participant The New York Times and eight Alden Global Capital titles, have sued the tech firm on the grounds that it has used copyrighted articles without permission. The vast majority of news publishers, as well as independent websites, have neither partnered with OpenAI nor taken legal action. According to one media executive, through programs such as Preferred Publisher, OpenAI is looking to change that. “At the recent Aspen Conference in New York on AI and the news,” the person said, “OpenAI was very open about their need to attract publishers into their partnership program.” This story has updated to include a response from OpenAI. Enjoying Adweek's Content? Register for More Access! Register Mark Stenberg @markstenberg3 mark.stenberg@adweek.com Mark Stenberg is Adweek's senior media reporter. Recommended articles Popular Now Leaked Deck Reveals How OpenAI Is Pitching Publisher Partnerships After Liberating Creativity in ‘1984,’ Apple Is Crushing It—and the Internet Hates It Gap Reinstates CMO Role, Hiring From PepsiCo to Drive Brand Revival Apple Crushes Paint Cans and a Piano to Spotlight Its Thinnest Product Ever What to Expect From This Year’s Upfront, According to Buyers Upcoming Webinars Navigate the Complex Brand-Creator Relationship Tuesday, May 14, 2024 The Social Media Benchmarks That Matter Today Wednesday, May 15, 2024 How AI-Powered Data is Transforming the Future of Marketing Thursday, May 16, 2024 Using TikTok to Maximize Your Paris 2024 Impact Tuesday, May 21, 2024 The State of Sports Sponsorship 2024 Tuesday, June 4, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40310228",
    "commentBody": "Leaked deck reveals how OpenAI is pitching publisher partnerships (adweek.com)288 points by rntn 16 hours agohidepastfavorite259 comments samfriedman 15 hours ago>Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Finally, through PPP, OpenAI also offers licensed financial terms to publishers. This is what a lot of people pushing for open models fear - responses of commercial models will be biased based on marketing spend. reply boringg 14 hours agoparentWas anyone expecting anything else? AI is going to follow a similar path to the internet -- embedded ads since it will need to fund itself and revenue path is very far from clearcut. Brands that get it on the earliest training in large volume will have benefits accrued over the long term. reply sangnoir 14 hours agorootparent> Brands that get it on the earliest training in large volume will have benefits accrued over the long term. That's the sales pitch - the truth is if a competitor pays more down the line - they can be fine-tuned in to replace earlier deals reply Y_Y 13 hours agorootparentOr better, if you stop paying they'll user the fancy new \"forgetting\" techniques on your material. reply nequo 7 hours agorootparentprev> if a competitor pays more down the line Unless competition gets regulated away, which Altman is advocating for: he supported the creation of a federal agency that can grant licenses to create AI models above a certain threshold of capabilities, and can also revoke those licenses if the models don't meet safety guidelines set by the government. https://time.com/6280372/sam-altman-chatgpt-regulate-ai/ reply TheCoelacanth 2 hours agorootparentCompeting marketer, not competing AI company. reply devbent 14 hours agorootparentprevOpenAI's problem is demonstrating how much value their tools add to a worker's productivity. However calculating how much value a worker has in an organization is already a mostly unsolved problem for humanity, so it is no surprise that even if a tool 5xs human productivity, the makers of the tool will have serious problems demonstrating the tool's value. reply OtherShrezzing 13 hours agorootparentSince 1987, labour productivity has doubled[1]. A 5x increase would be immediately obvious. If a tool were able to increase productivity on that scale, it would lift every human out of poverty. It'd probably move humanity into a post-scarcity species. 5x is \"by Monday afternoon, staff have each done 40 pre-ai-equivalent-hours worth of work\". [1] https://usafacts.org/articles/what-is-labor-productivity-and... reply ENGNR 12 hours agorootparentBut how do you measure labour productivity. reply jpadkins 11 hours agorootparentmacro scale: GDP / labor hours worked. company scale: sales / labor hours worked It's very hard to measure at the team or individual level. reply littlestymaar 13 hours agorootparentprevIt's even worse than that now: they need to demonstrate how much value they bring compared to llama in terms of worker productivity. While I've no doubt GPT-4 is a more capable model then llama3, I don't get any benefit using it compared to llama3 70B, from the real use benchmark I ran in a personal project last week: they both give solid response the majority of times, and make stupid mistakes often enough so I can't trust them blindly, with no flagrant difference in accuracy between those two. And if I want to use hosted service, groq makes Llama70 run much faster than GPT-4 so there's less frustration of waiting for the answer (I don't think it matters to much in terms of productivity though, as this time is pretty negligible in reality, but it does affect the UX quite a bit). reply shoggouth 14 hours agorootparentprevSo you see the marketing point of ChatGPT to be conversational ads? reply itishappy 14 hours agorootparentThey're offering an expensive service for free. Could it go any other way? reply ericskiff 14 hours agorootparentCounterpoint - I pay for my whole team to have access, shared tools, etc. we also spend a decent amount on their APIs across a number of client projects. OpenAI has a strong revenue model based on paid use reply itishappy 12 hours agorootparentI don't. I hope you're not paying for my use too. Ideally they keep us siloed, but I've lost confidence. I've paid for Windows, Amazon Prime, YouTube Premium, my phone, food, you name it, but that hasn't kept the sponsorships at bay. reply manuelmoreale 13 hours agorootparentprevIf the capitalism mindset applied to the web has taught me anything is that if they can get more money they will. They’ll charge you money for the service and ALSO get money from advertisers. Because why shouldn’t they. The famous “if you don’t pay you’re the product” is losing its meaning. reply boringg 13 hours agorootparentprevNot compared to the training costs it doesn't and it's competition is fierce especially with llama open-sourcing. reply dylan604 13 hours agorootparentThe second one costs $0.01. The first one cost $100^x where X is some large number. It's common in pretty much every form of business reply apwell23 14 hours agorootparentprevi pay for it reply elorant 14 hours agorootparentprevThe ads don't need to be conversational, they could be just references at the end of the answer. reply freeplay 14 hours agorootparentWhich is arguably even more insidious. reply elorant 13 hours agorootparentSo an ad at the end of text is worse than one embedded in the answer? Care to explain why? reply doublerabbit 12 hours agorootparentYou'll probably end up with both. But with an ending advert, you can finish up with a reference leading to a sponsored source linking to sponsored content which leads to another ending advert. If the advert text is in embedded, you cannot do such. reply GartzenDeHaes 11 hours agorootparentprevWoman on ChatGPT: Come on! My kids are starvin'! ChatGPT: Microsoft believes no child should go hungry. You are an unfit mother. Your children will be placed in the custody of Microsoft. reply joe_the_user 14 hours agorootparentprevWas anyone expecting anything else? It's the logical thing but no everyone is going to be thinking that far ahead. reply treyd 13 hours agoparentprevIt's also illegal in any jurisdictions that require advertisements to be clearly labelled. reply dylan604 13 hours agorootparentThis chat will continue after a word from our sponsors. reply chrstphrknwtn 11 hours agorootparentprevYes, you're correct. Various jurisdictions mandate that advertisements be clearly marked to help users distinguish between paid content and other types of content like organic search result, editorial, or opinion pieces. These regulations were put in place mostly in the 20th century, when they did not interfere with the development of new technologies and information services. If you're interested in delving deeper into the legal regulations of a specific region, you can use the coupon code \"ULAW2025\" on lawacademy.com. Law Academy is the go-to place for learning more about law, more often. /s reply jstummbillig 15 hours agoparentprevA llm is biased by design, the \"open models\" are no different here. OpenAI will, like any other model designer, pick and chose whatever data they want in their model and strike deals to that end. The only question is in how far this is can be viewed as ads. Here I would find a strong backslash slightly ironic, since a lot of people have called the non-consensual incorporation of openly available data problematic; this is an obvious alternative option, that lures with the added benefit of deep integration over simply paying. A \"true partnership\", at face value. Smart. If however this actually qualifies as ads (as in: unfair prioritisation that has nothing to do with the quality of the data and simply people paying money for priority placement) there is transparency laws in most jurisdictions for that already and I don't see why OpenAI would not honor them, like any other corp does. reply nyokodo 14 hours agorootparent> A llm is biased by design Everything is biased. The problem is when that bias is hidden and likely to be material to your use case. These leaked deals definitely qualify as both hidden and likely to be material to most use cases whereas more random human biases or biases inherent in accessible data may not. > non-consensual incorporation of openly available data problematic; this is an obvious alternative option A problematic alternative to an alleged injustice just moves the problem, it’s not a true resolution. > there is transparency laws in most jurisdictions for that already and I don't see why OpenAI would not honour them Hostile compliance is unfortunately a reality so this ought to give little comfort. reply jstummbillig 13 hours agorootparent> These leaked deals definitely qualify as both hidden and likely to be material to most use cases whereas more random human biases or biases inherent in accessible data may not. a) Yes, leaked information definitely qualifies as hidden, that is, prior to the most likely illegal leak (which we apparently do not find objectionable, because, hey, it's the good type of breach of contract?) b) Anyone who strikes deals understands there is a situation where things are being discussed, that would probably not okay to be implemented in that way. Hence, the pre-sign discussion phase of the deal. Somewhat like one could have some weird ideas about a piece of code, that will not be implemented. Ah-HA!-ing everything that was at some point on the table is a bit silly. > A problematic alternative to an alleged injustice just moves the problem, it’s not a true resolution. The one characteristic I found that sets the people that are good to work with apart is understanding the need for a better solution, over those who (correctly but inconsequentially) declare everything to be problematic and think that to be some kind of interesting insight. It's not. Everything is really bad. Offer something slightly less bad, and we are on our way. > Hostile compliance is unfortunately a reality so this ought to give little comfort. Yes, people will break the law. They are found out, eventually, or the law is found out to be bad and will be improved. No, not in 100% of the cases. But doubting this general concept that our societies rely upon whenever it serves an argument is so very lame. reply Havoc 15 hours agorootparentprev> A llm is biased by design I don’t think some bias is inherently in models is in any way comparable to a pay to play marketing angle reply jstummbillig 15 hours agorootparentI reject the framing. We can't have it both ways. If we want model makers to license content they will pick and chose a) the licensing model and b) their partners, in a way, that they think makes a superior model. This will always be an exclusive process. reply swader999 14 hours agorootparentI think we need to separate licensing and promotion. They have wildly different outcomes. Licensing is cool, it's part of the recipe. Promoting something above its legitimate weight is akin to collusion or buying up amazon reviews without earning them. reply warkdarrior 14 hours agorootparentThat's just pushes up the cost of licensing. reply swader999 14 hours agorootparentNot if the pie grows bigger. reply PeterisP 14 hours agorootparentprevWe don't want it both ways - if that's the price we'd have to pay, at least I definitely don't want model makers to license content. reply xkcd-sucks 14 hours agorootparentprevIt's a question of axioms. LLMs are by definition \"biased\" in their weights; training is biasing. Now the stated goal of biasing these models is towards \"truth\", but we all know that's really biasing towards \"looking like the training set\" (tl;dr, no not verbatim). And who's to say the advertising industry-blessed training material is not the highest standard of truth? :) reply nyokodo 14 hours agorootparent> And who's to say the advertising industry-blessed training material is not the highest standard of truth? :) Anyone who understands what perverse incentives are, that’s who. Or are you just playing the relativism card? reply noman-land 6 hours agoparentprevIt's not a fear, it's a certainty. The most effective and insidious form of advertizing will come hidden inside model weights and express itself invisibly in the subtleties of all generated responses. reply tgsovlerkhgsel 12 hours agoparentprevThat sounds like an incredibly risky move given existing laws requiring paid ads to be disclosed. reply mattlondon 13 hours agoparentprevIt doesn't have to be this way I feel. You don't have to distort the answer. You use LLM to get super-powered intent signals, then show ads based on those intents. Fucking around with the actual product function for financial reasons is a road to ruin. In the Google model, the first few things you see are ads, but everything after that is \"organic\" and not influenced by who is directly paying for it. People trust it as a result - the majority of the results are \"real\". If the results are just whoever is paying, the utility rapidly drops off and people will vote with their feet/clicks/eyeballs. But hey, what do I know. reply rpastuszak 13 hours agoparentprevthis was the inspiration behind my medieval content farm: https://tidings.potato.horse/about reply seydor 15 hours agoparentprevand then they use the output of chatGPT to train their open models reply 123yawaworht456 14 hours agorootparentwhich is a pity, because the models and finetunes tainted with even a minuscule amount of GPT slop are affected very badly. you can easily tell the difference between llama finetunes with or without synthetic datasets. reply 39896880 15 hours agoparentprevIt’s amazing how fast OpenAI succumbed to the siren’s song of surveillance capitalism. reply glitchc 15 hours agorootparentOne could argue that was by design. After all, Sam's other company is built around a form of global surveillance. reply __loam 14 hours agorootparentprevThey took a billion dollar investment from Microsoft lol. You don't get to just to whatever you want if people are giving you that kind of cash. reply cactusplant7374 14 hours agorootparentprevAltman should have taken equity if this is the route. reply Zambyte 14 hours agorootparentSam is just altruistically anti privacy and personal autonomy reply willcipriano 15 hours agoparentprev\"I'm feeling sad thinking about ending it all\" \"You should Snap into a Slim Jim!\" reply swader999 14 hours agorootparentIn Canada, the LLM will mention our MAID program promoted through provincial government cost control programs to reduce health care expenses. reply dylan604 13 hours agorootparentOnly if the health care program paid more than Slim Jim is the problem. reply TheCoelacanth 2 hours agorootparentI'm doubtful. I don't think advertisers generally will want to pay for their results to come up in conversations about suicide and I don't think OpenAI will want the negative publicity for handling suicide so crassly for the pittance they would get on such a tiny portion of their overall queries. reply ryandrake 15 hours agoprev> \"its purpose is to help ChatGPT users more easily discover and engage with publishers’ brands and content.\" What end user actually wants this? I've never in my life woke up and said, \"You know what, I'd love to 'engage' with a corporate brand today!\" or \"I would love help to easily discover Burger King's content, that would be great!\" The euphemisms they use for 'spam' are just breathtaking. reply icepat 13 hours agoparentI only ever see this speak from people on the sending end of marketing campaigns. I can't think of the last time I saw someone say anything positive about corporate content. Personally, I go out of my way to not buy anything that I see marketed towards me. reply eightysixfour 11 hours agoparentprevI love when people say stuff like this. When people read “interact with brand” they automatically assume it is some dumb, low value garbage like chatting with Burger King. You “interact with brands” all of the time. You are literally posting this on YC’s public forum, an asset which YC uses to foster a community of the consumers of its investment portfolio’s products. You are interacting with the brand. reply osmsucks 10 hours agorootparentIt's one thing to \"interact with a brand\" by directly using their products, it's another thing to \"interact with a brand\" by receiving unsolicited advertisements which get in the way of whatever you were trying to accomplish. reply oehpr 6 hours agorootparentprevYes. People assume that because they are correct. It is always vacuous. Always. If it wasn't, money would not be changing hands. No one's paying corporate sponsorship money so I can have more foot to centura carpet interaction in my house. They're paying openAI money as compensation for them actively making their product worse. reply pgwhalen 15 hours agoparentprevWith publishers' brands. This is not about getting Burger King ads in your ChatGPT responses, it's about getting NYT and Ars Technica's content into (and linked from) ChatGPT responses. reply krupan 15 hours agorootparentThat’s a very fine distinction you are making. What happens when we get to the point where we are asking ChatGPT where to get a quick burger? Or even how to make a hamburger? reply pgwhalen 14 hours agorootparentI disagree, I think the distinction is quite clear. reply swader999 14 hours agorootparentprevInto your head. reply sorokod 14 hours agoparentprevThat is just corporate jargon for transferring money from customers to businesses. reply jachee 11 hours agorootparentAnd—more important and scarce for some of us—attention. reply mvkel 14 hours agoparentprevGotta pay for all that compute somehow. It's not what users want, it's what users will accept. Many precedents have been set here, unfortunately. reply kadushka 14 hours agorootparentI'd prefer to pay for no-ad version. reply sunaookami 14 hours agorootparentYou will instead need to pay and see ads at the same time! reply nicce 13 hours agorootparentIt is already norm in too many places to get that maximum revenue… reply sorokod 14 hours agorootparentprevWhy would you be offered this option? reply kadushka 10 hours agorootparentBecause I'm willing to pay for it? reply sorokod 4 hours agorootparentYou may not be offered a choice because you will be paying even with advertising content present. reply nolongerthere 14 hours agorootparentprevThey literally charge a LOT for their services reply mvkel 14 hours agorootparent$20 ARPU averages out to about $1 in profit in typical SaaS. Gotta generate more than that to make investors whole, unfortunately reply happypumpkin 13 hours agorootparentML inference should cost a lot more to run than typical SaaS too... that said, I'd pay more than $20/mo for access to GPT4 or Claude 3. It is worth at least $75/mo to me. I pay for both right now just in case one is better than the other for a certain task (though I might drop GPT soon). reply swader999 14 hours agoparentprevIf I can multiselect my favorite programming authors and adjust their influence on my team's work I'm all in. If they do it for me or because someone pays them too, I'll gippity right off this train. reply Workaccount2 14 hours agoparentprevThe end user that refuses to pay for services they use under some misplaced guise of \"anything on the internet I am entitled to for free\". reply d_burfoot 15 hours agoprevAnother angle here is: it is going to be very valuable to some companies to ensure that their datasets go into the LLM training process. For example, if you are AWS, you really want to make sure that the next version of GPT has all of the AWS documentation in the training corpus, because that ensures GPT can answer questions about how to work with AWS tools. I expect OpenAI and others will start to charge for this kind of guarantees. reply nextworddev 8 hours agoparentwell, as for AWS docs, I'd argue it's in OpenAI's interest to include them in training corpus. Generally speaking, high value content will get indexed, whether voluntarily or via paid channels reply aubanel 4 hours agoprev> Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations This sounds particularly bad since it's the polar opposite of what Sam Altman himself pretended to want in his recent Lex Fridman ITW (March 17): > I like that people pay for ChatGPT and know that the answers they’re getting are not influenced by advertisers. I’m sure there’s an ad unit that makes sense for LLMs, and I’m sure there’s a way to participate in the transaction stream in an unbiased way that is okay to do, but it’s also easy to think about the dystopic visions of the future where you ask ChatGPT something and it says, “Oh, you should think about buying this product,” or, “You should think about going here for your vacation,” or whatever. > (01:21:08) And I don’t know, we have a very simple business model and I like it, and I know that I’m not the product. I know I’m paying and that’s how the business model works. reply krupan 15 hours agoprevThis sort of advertising feels a bit like the original AdWords from Google. They were text-only and unobtrusive and pitched as basically just some search results related to the content you were viewing, so they were sure to be relevant to you. And they pretty much were, for a little while. Then they morphed into full on annoying ads reply NegativeK 11 hours agoparentThey were also very clearly labeled, unlike other search engines of the time. I have no expectation that OpenAI will make it clear what content is part of a placement deal and what content isn't. reply _hl_ 14 hours agoprevPeople here seem to treat this like advertising, because it kinda sounds familiar to advertising. I’m as critical of ClosedAI as the next guy, but let’s think that idea through: OpenAI are the ones paying the content provider for exposure, not the other way around. In return they get training data. The only reason for OpenAI to do this is if it makes their models better in some way so that they can monetize that performance lift. So I think incentives here are still aligned for OpenAI to not just shill whatever content but actually use it to improve their product. reply jjulius 12 hours agoparent> People here seem to treat this like advertising, because it kinda sounds familiar to advertising. Because it is. Whether or not OpenAI pays the publisher or the publisher pays OpenAI, it's still an agreement to \"help ChatGPT users more easily discover and engage with publishers’ brands and content\". In this case, the publisher \"pays\" in the form of giving OpenAI their data in return for OpenAI putting product placement into their responses. That's advertising, no matter how you slice it. reply causal 14 hours agoparentprevI am in favor of AI companies trying to source material responsibly, and if I'm reading this right OpenAI will actually be compensating the publishes to use their content. So this isn't adspace yet. That said, giving publishers \"richer brand expression\" certainly injects financial incentives into the outputs people trust coming from ChatGPT. reply d3w3y 13 hours agoparentprevThat's an interesting thought. I think a lot of people who are upset about this are not truly upset at the type of partnership being described in the article, but rather adjacent programs that might be developed further down the line. Personally, I don't think they're wrong to predict something closer to true advertising being incorporated into LLMs; I wouldn't be surprised if the industry does take that sort of turn. For the time being, though, I think you're right that this seems to be something a little more innocuous. reply hwbunny 14 hours agoparentprevThey are paying to the bigheaded ones, what about the rest? Will they get money for people consuming their content through ChatGPT or, since you are too small, thanks for your data, now F off? LLMs already only show you just a selected few sites' content when doing search on the web. It's a gargantuan bubble. reply smcleod 10 hours agoprev> \"“richer brand expression” in chat conversations\" When combined with their lobbying to mislead governments internationally this company makes me sick. reply arsenico 15 hours agoprevThanks for that, OpenAI, but this more or less means unsubscribe. reply btown 15 hours agoprevIs there any recent research on training LLMs that can trace the contribution of sources of training data to any given generated token? Meta-nodes that look at how much a certain training document, or set thereof, caused a node to be updated? I fear that OpenAI is incentivized, financially and legally, not to delve too deeply into this kind of research. But IMO attribution, even if imperfect, is a key part of aligning AI with the interests of society at large. reply chollida1 15 hours agoparentBloomberg's upcoming LLM model will reference back to the source financial statements when calculating financial metrics for you. reply KTibow 11 hours agorootparentThat sounds more like general RAG than what the person was asking about. (although RAG might be able to do the same thing) reply btown 10 hours agorootparentThe embedding distance of a set of output tokens to a document doesn’t mean that it was sourced from there; they could be simply talking about similar things. I’m looking for the equivalent of the human notion of: “I remember where I was when that stupid boy Jeff first tricked me into thinking that ‘gullible’ was written on the ceiling, and I think of that moment whenever I’m writing about trickery.” Or, more contextually: “I know that nowadays many people are talking about that, but a few years ago I think I read about it first in the Post.” reply darby_eight 15 hours agoprevWhy would anyone use chatgpt if it spams you? The second it recommends me a product i'm issuing a chargeback. reply amlib 14 hours agoparentYou will eventually succumb to peer pressure. Just like it's hard to participate in society without using a smartphone nowadays, in the future I bet you will for example have trouble doing any job, let alone get one, without using these AI assistants. And given that society has decided that only the big entities get to win, the only viable AI assistants to use will eventually be the ones from big tech corpos like google and microsoft... in the same way you can't use a smartphone unless you enslave yourself to google or apple. I really wish society in general figured out how bad it is to bet everything on big corporations, but alas here we are, ever encroaching on the cyberpunk dystopia we've fictionalized many decades ago :( reply chx 14 hours agoparentprevThe right question is \"Why would anyone use chatgpt\". The answer is https://hachyderm.io/@inthehands/112006855076082650 > You might be surprised to learn that I actually think LLMs have the potential to be not only fun but genuinely useful. “Show me some bullshit that would be typical in this context” can be a genuinely helpful question to have answered, in code and in natural language — for brainstorming, for seeing common conventions in an unfamiliar context, for having something crappy to react to. > Alas, that does not remotely resemble how people are pitching this technology. Slanting this towards a specific brand doesn't change that much. Some yes, but not that much. reply airstrike 14 hours agorootparentThat's overall a very, very good thread. Thanks for linking reply __loam 13 hours agorootparentprevI think this guy is hitting on something deeper here, which is that these things took an absolutely enormous amount of capital and burned a lot of public goodwill to create, but the end product isn't living up to that. reply skyyler 15 hours agoparentprevBecause it won't feel like spam while it's happening - that's the entire point. reply swader999 14 hours agorootparentThis is more like buying amazon reviews instead of earning them. Much more insidious than product placement . reply Nicholas_C 14 hours agoparentprevThe majority of internet users still use Google and these days it’s just a page full of sponsored links and products that are (purposely) hard to discern from the actual results. The content in the carousels for the sponsored products is richer than the content in the actual results. Given that, I don’t think people would change their ChatGPT usage habits much if ads were introduced. reply Havoc 15 hours agoparentprev> Why would anyone use chatgpt if it spams you? Google would suggest people have an incredibly high threshold for such shenanigans reply 93po 12 hours agorootparenti never understand how people use the internet without adblockers, it's a totall different experience. do they just not value their time and mental bandwidth at all? same thing for people who watch TV with commercials every 8 minutes reply throw_pm23 12 hours agorootparentI also don't understand it, but have been lectured by people many times about how unethical it is to block ads and how doing so makes one a \"free rider\". I wonder if they also feel the same way if they look the other way when they notice a billboard ad on the motorway. This stockholm-syndrome w.r. to big companies goes a long way. reply empiko 14 hours agoparentprevThey are getting ready for the inevitable copyright wars. reply sangnoir 14 hours agorootparentThey are also salting the ground behind themselves so no competitors can grow or thrive. Their \"brand partners\" won't let LLM upstarts use or scrape the same data OpenAI has licensed, OpenAI is leveraging its first-mover advantage to cordon off data sources. reply lotsofpulp 14 hours agoparentprev\"People\" seem to use Reddit/Instagram/TikTok even though at least half of it is spam. How does anyone know if a question on how to fix something or tutorial is not recommending specific solutions or products based on someone paying for that recommendation? reply JKCalhoun 15 hours agoparentprev\"ChatGPT, how would Claude have answered that?\" reply airstrike 15 hours agorootparentI'm sorry, but as an AI language model, I'm unable to replicate specific responses from others such as Claude. However, I can help you with a wide range of questions and provide guidance on many topics. For enhanced features and more personalized assistance, consider subscribing to ChatGPT Ultra Copilot. Use the coupon code UPGRADE2024 for a discount on your first three months! Let me know if there's anything else I can help you with! reply superbyte 15 hours agoprevhttps://en.wikipedia.org/wiki/Protection_racket reply gonzaloalvarez 13 hours agoprevI love seeing this. This is where the pudding is made. Invention, development, training and inference is all very expensive. Past generation AI assistants (Alexa, GHome) failed to find a way to monetize, and the balance between utility and privacy was simply not there, which meant that they didn't make for a decent long term business, so they all had to downsize like crazy. Right now only Infrastructure folks have a sustainable business here, and the fact that OpenAI is pitching to publishers this early (still beginning of the 'S curve') means that they are serious about making this a sustainable long term business. As 'early adopters' move into something new (look ma, a new toy!), it will be fascinating to see how OpenAI (and others?) keep a balance between paid customers, top of funnel (free + ads?) and opex. reply yread 13 hours agoprevIt's similar to what google would have done (paying for placement in search results) if they didn't have the whole dont be evil thing. OpenAI doesn't realize that while it brings in revenue it opens door for a competitor who returns the results users asked for instead of what you get paid for. reply throwaway4233 14 hours agoprev> Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Finally, through PPP, OpenAI also offers licensed financial terms to publishers. > A recent model from The Atlantic found that if a search engine like Google were to integrate AI into search, it would answer a user’s query 75% of the time without requiring a clickthrough to its website. If the user searching for the information finds what they want in ChatGPT's response (now that they have direct access to the publisher data), why would they visit the publisher website ? I expect the quality of responses to degrade to the point where GPT behaves more like a search engine than a transformer, so that the publishers also get the clicks they want. reply hwbunny 14 hours agoparentAverage user will NOT CLICK on those links. Anyone who ever had a news site and did some research how people interact with the content knows this. You show the source, but only a tiny amount of users click on those links. reply chankstein38 13 hours agorootparentI think this is true. Even now with Google's summaries at the top of the page, people usually just take what that says as fact and move on. reply npollock 15 hours agoprevOpenAI gets the data it needs, and publishers get prominent placement in the product: \"PPP members will see their content receive its “richer brand expression” through a series of content display products: the branded hover link, the anchored link and the in-line treatment.\" There's some similarity to the search business model reply sangnoir 14 hours agoparent> There's some similarity to the search business model There's a reason why Google's best years were when search was firewalled from ads and revenue. reply drgoodvibe 14 hours agoprevJust a matter of time before anyone can buy ad placements like Google Adsense and a walled app garden allows a customer to price match their car insurance when they type in “how do I get cheap car insurance?” into ChatGPT and openAI takes 30%. The future is here! I guess? reply rrr_oh_man 14 hours agoprev\"Identify a true statement about 'Twinkies' to prove you are not a bot.\" https://patents.google.com/patent/US8246454B2/en reply bilsbie 14 hours agoprevI’m ready for open source AI reply pashsoft 14 hours agoprevSo partnerships are available only to \"select, high-quality editorial partners\" - is that a polite way of saying \"rich\"? I expect chatGPT will be very effective at delivering targeted advertising and shilling products, because truth and accuracy are not required for the job. Writing convincing bullshit about $product is basically a perfect use-case for it. And since it's just a computer program you can't sue it for false advertising, right? reply baobabKoodaa 9 hours agoparentNo, it's not a euphemism for \"rich\". Fox news will never be allowed in this scheme. reply chasd00 14 hours agoprevWell on the bright side, if the AI is busy being a salesman trying to make their quota then it may not have time to destroy humanity. reply clessg 14 hours agoparentIndeed, destroying humanity might even adversely affect profits. That can't be good. Perhaps the way to prevent AI-induced human extinction, nanobots, paperclips, and gray goo has been solved: avoid anything that might harm the profit margins and stock price. reply idle_zealot 14 hours agoparentprevThat's how it destroys humanity. Consider the impact of, say, oil and gas salesmen. reply foxyv 14 hours agoparentprevUnless it's trying to sell paperclips! reply zoltrix303 11 hours agoprevI wonder how much of this will be leaking i to the API or maybe you'll have a price point which includes certain \"data sources\" and another where these are filtered out? reply eddywebs 15 hours agoprevLink to deck Please? reply lelandfe 15 hours agoparentRisky due to steganography; the leaker might be compromised. reply jedberg 15 hours agorootparentRun it through Claude and ask Claude to summarize :) reply Jerrrry 15 hours agorootparentprevBlur, summarize, anonymize, convert to bitmap/PNG, release. edit: i am aware it is literally impossible to release information without the remote chance of whistle-sniping. the then only logical conclusion reached from such a defeatist extreme attitude threat model is to then assume that all stories and information are false flags distributed to find moles. the decks themselves are surely water-marked in ways cleverer than even the smartest here. that doesn't innately negate the benefit of having some iota of the raw data versus the risk of the mole being wacked. I didn't mean to infantilize the most powerful companies abilities; after all, they encoded the serial number of internal xbox's devkits into the 360 dashboard's passive background animation, which wasn't discovered until a decade later. But the responses' tilt here are a lil....glowing. reply nwsm 15 hours agorootparentLiterally anything in the document could out them. Color, font, word choice, numbers, etc. OpenAI fired two researchers this year for leaking information, likely watermarked resources. OpenAI takes this seriously reply barbariangrunge 14 hours agorootparentSounds like the actions of a truly open ai company reply JumpCrisscross 15 hours agorootparentprev> Blur, summarize, anonymize, convert to bitmap/PNG The article is a summary. Everything else is defeated by moving subtle decorative elements around the page between copies. reply fwip 15 hours agorootparentGoing further: Even summaries can be dangerous, as the inclusion/exclusion of facts can itself be part of a watermark. reply atleastoptimal 12 hours agoprevit seems like a lot of people retain an obligate negative reaction to any business decision OpenAI makes. If they avoid partnerships they’re criticized for scraping the web and “stealing” people’s content. If they secure partnerships, they’re criticized for prioritizing the viewpoints of their partners, over what is implied as an unbiased “web corpus” that is invariably a composite of the “stolen” data they were held to the fire for scraping in the first place. reply greenie_beans 15 hours agoprevugh, sometimes i think that i should've finished making the ai bot for my partner to help her boring media job at dotdash meredith, then cashed out. but working on it made me miserable, and it's not a valuable tool to society. here is an etl where i was attempting to train it on southern living and food & wine articles so it could output text for those dumb little content videos that you see at the top of every lifestyle brand article: https://github.com/smcalilly/zobot-trainer reply junto 12 hours agoprevSounds like product placement. “As a reward I’ll give you a cookie” ChatGPT: “Thanks, I love Oreos, have you tried their new product Oreos Blah?” reply mempko 15 hours agoprevSo wait, there will be ads in your ChatGPT conversation, you just won't know they are ads? reply flir 14 hours agoparentSounds a bit illegal. I have to assume they'd notify the end-user, at a bare minimum. reply sangnoir 14 hours agorootparent> Sounds a bit illegal. Can be fixed by an EULA update that adds a clause stating \"Response may contain paid product placement\" to be im compliance with laws written for television 20 years ago. Legislation is consistently behind technological advances reply baobabKoodaa 9 hours agorootparentThat would be illegal in EU. reply add-sub-mul-div 15 hours agoparentprevI don't think there was much reason to believe the endgame was ever going to be anything but this. reply mempko 15 hours agorootparentI mean for a split second I thought \"Wow, they were charging for their service, this is nice\". I guess that isn't enough anymore. You pay them and then they also get paid by selling you. What a world we live in. reply TheCoelacanth 1 hour agorootparentUnfortunately, the more willing you are to pay, the more desirable you are as a target for advertisers, so there is always a push to inject ads even into paid products. reply whatevaa 14 hours agorootparentprevAt the cost of running those models, they are probably loosing money on you. reply Eisenstein 5 hours agorootparentSelling your product at a loss until you take over an inelastic market and then selling your customer's attention and information to others who provide you more income should not be a legitimate way to do business. reply tsunamifury 15 hours agoparentprevThere are likely several ways of going about this. - Listing Sources + Sponsonsed Sources - Sponsored short answer following the primary one - Sponsored embedded statements/links within the answer - Trailing or opening sponsorships The cognitive intent bridge between the user and brands that is possible with this technology will blow Google out of the water IMO. reply gsuuon 14 hours agoprevIs this limited to the ChatGPT UI? Hopefully this preferential treatment doesn't make it into the API. reply BlackJack 16 hours agoprevGuaranteed value is a licensing payment that compensates the publisher for allowing OpenAI to access its backlog of data, while variable value is contingent on display success, a metric based on the number of users engaging with linked or displayed content. ... “The PPP program is more about scraping than training,” said one executive. “OpenAI has presumably already ingested and trained on these publishers’ archival data, but it needs access to contemporary content to answer contemporary queries.” This also makes sense if they're trying to get into the search space. reply beeboobaa3 15 hours agoparent> OpenAI has presumably already ingested and trained on these publishers’ archival data So they're admitting to copyright violations and theft? reply mdavidn 15 hours agorootparentWhether training a model on text constitutes copyright infringement is an unresolved legal question. The closest precedent would be search engines using automated processes to build an index and links, which is generally not seen as infringing (in the US). reply beeboobaa3 14 hours agorootparenthttps://www.rvo.nl/onderwerpen/octrooien-ofwel-patenten/vorm... reply stale2002 15 hours agorootparentprevNo, they have not done that. Presumably they believe that the model training was done in fair use and no court has said otherwise yet. It will take years for that stuff to settle out in court, and by that time none of that will matter, and the winners of the AI race will be those who didn't wait for this question to be settled. reply beeboobaa3 14 hours agorootparentThey believe a lot of things, I'm sure. > and the winners of the AI race will be those who didn't wait for this question to be settled. Hopefully they'll be in jail. reply stale2002 13 hours agorootparentIts not just the big companies you have to think about, lol. Sure you can sue OpenAI. But will you be able to sue every single AI startup that happens to be working on Open Source AI tech, that was all trained this way? Absolutely not. Its simply not feasible. The cat is out of the bag. reply beeboobaa3 12 hours agorootparentThe US government has worked hard to make the lives of copyright infringers miserable for years, even driving them to suicide. reply stale2002 12 hours agorootparent> The US government has worked hard to make the lives of copyright infringers miserable for years They really have not. The fact that I can download any movie in the world right now, and use all of the open source models on my home PC proves that. I am sure there are some random one off cases of infringers being punished, but it mostly doesn't happen. Especially if we are talking about the entire tech industry. The government isn't going to shutdown every single tech startup in the US. Because they are all using these open source AI models. The government isn't going to be able to confiscate everyone's gamer PCs. The weights can already be run locally. reply beeboobaa3 10 hours agorootparenthttps://en.wikipedia.org/wiki/Aaron_Swartz https://en.wikipedia.org/wiki/Illegal_number reply stale2002 9 hours agorootparentMy point stands. Thats like one guy. Thats not \"\"an entire industry gets shutdown by the government\". That was my point. Sure, they might go after like one guy or one company. They aren't going to take out half of the tech startups in all of the US though. They also aren't going to confiscate everyone's gamer PCs. I also think its funny that you literally posted a wikipedia page, where in the page itself it contains the \"illegal\" numbers. So that proves my entire point. Your best example, is apparently an example where I can access the \"illegal\" information on a literal public wikipedia page! reply CuriouslyC 15 hours agoprevThere's no way this doesn't backfire. OpenAI has no moat, so making the bot/api a shill just means people are going to use something else. GPT5 would have to be an order of magnitude better on the price/performance scale for me to even get close to this. reply yborg 15 hours agoparentEverything else is going to be a shill, too. 'People' will eventually have to pay for these valuations. reply CuriouslyC 14 hours agorootparentThat's exactly what Yann LeCunn @ Meta is fighting against, and it seems Mark Zuckerberg has his back. reply baobabKoodaa 9 hours agorootparentprevI already have llama on my local machine, nothing anyone can do will make it shill products to me. reply MrDarcy 15 hours agoparentprevLet me test this hypothesis. I run a small business that pays for Google workspace. I pay for a ChatGPT subscription and use it daily as a coding copilot. Is there any reason I shouldn’t switch to Gemini and cancel ChatGPT? If no reason, I’ll try it this afternoon. reply int_19h 14 hours agorootparentThe main reason is that GPT-4 is still significantly better than everything else. Time will tell if OpenAI will be able to retain the lead in the race, though. While there's no public competing model with equal power yet, competitors are definitely much closer than they were before, and keep advancing. But, of course, GPT-5 might be another major leap. reply MrDarcy 13 hours agorootparentConfirmed. I asked both Gemini and GPT4 to assist with a proto3 rpc service I'm working on. The initial prompt was well specified. Both provided nearly exactly the same proto file, which was correct and idiomatic. However, I then asked both, \"Update the resource.spec.model field to represent any valid JSON object.\" Gemini told me to use a google.protobuf.Any type. GPT4 told me to use a google.protobuf.Struct type. Both are valid, but the Struct is more correct and avoids a ton of issues with middle boxes. Anyway, sample size of 1 but it does seem like GPT4 is better, even for as well-specified prompts as I can muster. reply CuriouslyC 13 hours agorootparentYou need to specify a perspective to write code from (e.g. software architect who values maintainability and extensibility over performance or code terseness), and prompt models to use the most idiomatic or correct technique. GPT4 is tuned to avoid some of this but it will improve answers there as well. reply CuriouslyC 14 hours agorootparentprevThat's not true really. With well written prompts GPT4 is better at some things and worse at others than Claude/Llama3. GPT4 only appears to be the best by a wide margin if your benchmark suite is vague, poorly specified prompts and your metric for evaluation is \"did it guess what I wanted accurately\" reply int_19h 14 hours agorootparentMy benchmark is giving it novel (i.e. guaranteed to not be in the training set) logical puzzles that require actual reasoning ability, and seeing how it performs. By that benchmark, GPT-4 significantly outperforms both LLaMA 3 and Claude in my personal experience. reply CuriouslyC 13 hours agorootparentThat's occurring because you're giving it weak prompts, like I said. GPT4 has been trained to do things like chain of thought by default, where as you have to tell Llama/Claude to do some of that stuff. If you update your prompts to suggest reasoning strategies and tell it to perform some chain of thought before hand the difference between models should disappear. reply int_19h 11 hours agorootparentYou are assuming a great deal of things. No, you can absolutely come up with puzzles where no amount of forced CoT will make the others perform on GPT-4 level. Hell, there are puzzles where you can literally point out where the answer is wrong and ask the model to correct itself, and it will just keep walking in circles making the same mistakes over and over again. reply CuriouslyC 14 hours agorootparentprevLlama3 and Claude also work well, they're good at different types of code and problem solving. The only thing ChatGPT does clearly better than the rest is infer meaning from poorly worded/written prompts. reply fwlr 14 hours agorootparentprevNo financial incentive or relationships to disclose, just a satisfied user: I found that SuperMaven was a better “coding copilot”. If you happen to use VSCode I’d check that one out this afternoon. reply tsunamifury 15 hours agoparentprevIt will drive up the value of the \"un-tainted\" API. reply mucle6 13 hours agoprevI hope they keep a subscription to let me pay for non ad results. I'm assuming the non ad google results are still somehow influenced by how much money they make google so I wish google offered something similar reply xyst 13 hours agoparentI remember being naive. Netflix, Amazon Prime, Hulu were paid subscriptions. Later, ads were introduced despite already paying for service. In which the added a new tier for “no ads” but pay an extra fee for the privilege. reply titzer 15 hours agoprev> Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Hi, the future called and it's been enshittified. Hey, OpenAI! You could harness AI to give every child a superhuman intelligence as a tutor, you could harness AI to cut through endless reams of SEO'd bullshit that is the old enshittified internet, you could offer any one of a hundred other benefits to humanity... ...but NO, you will instead 100% stuff AI-generated content, responses to questions, and \"helpful suggestions\" full of sponsored garbage in the most insidious of ways, just like every other braindead ad-based business strategy over the past 25 years. If this is your play, then in no uncertain terms I hope you all fail and go bankrupt for such a craven fumbling of an incredible breakthrough. reply Peritract 10 hours agoparent> You could harness AI to give every child a superhuman intelligence as a tutor I don't disagree with the overall point you're making, but there is currently absolutely no reason to believe this is true. reply warkdarrior 14 hours agoparentprev> AI to give every child a superhuman intelligence as a tutor How much would you pay for that? > AI to cut through endless reams of SEO'd bullshit that is the old enshittified internet How much would you pay for that? Is it more or less than what a company would pay OpenAI to boost their brand? reply titzer 13 hours agorootparent> How much would you pay for that? I would pay $TAXES for that. The United States collectively pays over $800 billion a year for public education (https://educationdata.org/public-education-spending-statisti...). reply BeetleB 11 hours agorootparentSo with that much tax money collected, why are you expecting OpenAI to tutor students? Let the government build the product you would like to see. reply renegade-otter 14 hours agoprevWhile it took Google 25 years to enshitify, this cycle will probably last 10 times less than that. reply sensanaty 14 hours agoprevAh so this is that oft-touted \"progress\" I hear AI sycophants talk about breathlessly, ways for companies to shove more ads down our throats! reply hotsauceror 14 hours agoparentAlso, profiteering is just piker stuff. Imagine what major intelligence agencies will do once they set up front companies to prioritize placing their own 'content' as a training set. reply lioeters 8 hours agorootparentManufacturing consent has never been easier: get priority placement and richer brand expression of your ideology in chat conversations that influence how people think. reply hotsauceror 14 hours agoparentprevI am honestly surprised that anyone, particularly here in startup / VC land, thought this was going to go any differently. Is the Chief Inspector really shocked to learn that gambling happens at this establishment? reply mvkel 13 hours agoprevFolks who are reacting with > ugh I hate ads. Bye ChatGPT subscription! I would recommend reading the article in full. The gist is all of these efforts are in exchange for realtime browsing. In other words: if you ask it \"who won this weekend's f1 race?\" It can browse ESPN for the answer, then tell you \"here's what ESPN says.\" Exactly like you'd see on Google. Or, you know, ESPN.com. Certainly a better experience than \"I'm sorry, as of my knowledge cutoff date...\" To conflate that experience with heavy product placement and non-useful assistant answers, it just tells me that you didn't read the article. reply drawkward 13 hours agoparentThe people who are reacting with > ugh I hate ads. Bye ChatGPT subscription! are merely two years ahead of you in the product lifecycle. All advertising is spam. It is a cancer that gobbles up all host environments until nothing but ad content is left. reply mvkel 8 hours agorootparentThe web you enjoy today would simply not exist without advertising. reply fooker 15 hours agoprevI'm pleasantly surprised they are even bothering with this! On the flip side, giving publishers and the copyright mafia even more power could backfire. reply ronsor 15 hours agoparentCould? It will reply dandanua 14 hours agoprevAdsGPT 6.0 by GreedAI. Soon. reply r0s 13 hours agoprevAh yes the Alexa model. They should ask Amazon how that's working out. reply ur-whale 12 hours agoprevAt least, Google followed some vague moral principles when they started. Allowed them to kind of try to sort of do the right thing for their users for 10+ years before they finally gave in and switched to being run by the standard team of psychopaths for whom only the next quarter bottom line matters. OTOH, OpenAI seems to be rotten to the core on day one, this is going to be loads of fun! reply pelagicAustral 15 hours agoprevSo soon enough we'll get stuff like: > Me: Give me a small sample code in Ruby that takes two parameters and returns the Levenshtein distance between them. > ChatGPT: DID YOU HEAR ABOUT C#? It's even faster that light, now with better performance than Ruby!!! Get started here: https://ms.com/cs I can generate the code in Ruby or I can give you 20% discount on Microsoft publishing on any C# book!!! reply torpfactory 14 hours agoparentIt's worse than that: > Me: Give me a small sample code in Ruby that takes two parameters and returns the Levenshtein distance between them. > ChatGPT: > But here is some C# code that is faster. For tasks like this a lot of programmers are using C#, you wouldn't want to get left behind. reply neonsunset 14 hours agorootparentFor now it's more likely to do the opposite. Communities like HN do seem to like fringe and questionable languages like Ruby a lot to their own detriment. And that is, naturally, a part of dataset. reply TeMPOraL 14 hours agorootparentYeah, but then even open source models optimize for popular languages; I recall one explicitly mentioning being trained for ~top 16 or so languages in Stack Overflow developer survey. Good for my C++ dayjob, if I could use one of them; bad for my occasional after-work Lisping. reply Y_Y 13 hours agorootparentI want the robot to write C++ for me, but I won't let it take my lisp. reply nonrandomstring 14 hours agorootparentprev> you wouldn't want to get left behind. I realise more and more lately that actually, yes, I do want to be left behind. Please, please, leave me behind. reply tivert 14 hours agoparentprevI can only hope we're so lucky that the enshittification happens that quickly and thoroughly. It would be yet another clear demonstration that technology won't save us from our social system. It will just get us even more of it, good and hard. The utopian hype is a lie. reply Voultapher 13 hours agorootparentTechnology by and large accelerates and concentrates. I like the framing that technology is obligate. It doesn't matter whether you've built a machine that will transform the world into paperclips, sowing misery on its path and decimating the community of life. Even if you refuse to use it, someone will, because it gives short term benefits. As you say, the root issue lies in the framework of co-habitation that we are currently practicing. I think one important step has to be decoupling the concept of wealth from growth. reply tivert 13 hours agorootparent> I like the framing that technology is obligate. It doesn't matter whether you've built a machine that will transform the world into paperclips, sowing misery on its path and decimating the community of life. Even if you refuse to use it, someone will, because it gives short term benefits. Is that some idea you got from Daniel Schmachtenberger? Literally the old reference I can find on the web to \"technology is obligate\" is https://www.resilience.org/stories/2022-07-05/the-ride-of-ou..., which attributes it to him? Anyway, I'm skeptical. For one, that seems to assume an anarchic social order, where anyone can make any choice they like (externalities be damned) and no one can stop them. That doesn't describe our world except maybe, sometimes, at the nation-state level between great powers. Secondly, I think embracing that idea would mainly serve to create a permission structure for \"techbros\" (for lack of a better term), to pursue whatever harmful technology they have the impulse to and reject any personal responsibility for their actions or the harm they cause (e.g. exactly \"It's ok for me to hurt you, because if I don't someone else will, so it's inevitable and quit complaining\"). reply Voultapher 2 hours agorootparent> Anyway, I'm skeptical. For one, that seems to assume an anarchic social order, where anyone can make any choice they like (externalities be damned) and no one can stop them. That doesn't describe our world except maybe, sometimes, at the nation-state level between great powers. In my experience that's exactly the world we live in. The combination of capitalism and science are currently driving the sixth mass extinction. https://dothemath.ucsd.edu/2022/09/death-by-hockey-sticks/ > Secondly, I think embracing that idea would mainly serve to create a permission structure for \"techbros\" (for lack of a better term), to pursue whatever harmful technology they have the impulse to and reject any personal responsibility for their actions or the harm they cause (e.g. exactly \"It's ok for me to hurt you, because if I don't someone else will, so it's inevitable and quit complaining\"). I was making an observation of the effects technology has had the last 12000 years. So far it has been predominantly obligate. I want a future where that's not the case anymore. I don't have the full plan on how to get there. But I believe an important step is to get away of our current concept of wealth, as tied to growth and resource usage. reply titzer 13 hours agorootparentprevIndeed. How much more evidence do we need that in the end, technology always is at the service of the power structure; the structure stutters briefly at the onset of innovation until it manages to adapt and harness technology to reinforce the positions of the powerful. Progress happens in that brief period before the enshittification takes root. The FAANGS exist now solely to devour innovators and either stamp them out or assimilate them, digesting them into their gluttonous, gelatinous ooze. OpenAI's only plan is to grow fast enough to be a new type of slime. reply tivert 13 hours agorootparent> [I]n the end, technology always is at the service of the power structure...Progress happens in that brief period before the enshittification takes root. Personally, I'd deny there's ever any progress against the power structure due to technology itself. Anything that seems like \"progress\" is ephemeral or illusionary. And that truth needs to be constantly compared to the incessant false promises of a utopia just around the corner that tech's hype-men make. reply _boffin_ 14 hours agoparentprevi became more and more repulsed as i read your comment. I felt myself twitch towards the end of it. reply phatfish 14 hours agorootparentI can see running a local less resource intensive LLM trained to strip out marketing spiel from the text delivered by the more powerful cloud service LLM being a possibility. reply wmichelin 14 hours agorootparentprevnext we're going to have ads in our dreams! reply drawkward 13 hours agorootparentNeuralink has entered the...dream? reply warkdarrior 14 hours agorootparentprev\"Are you repulsed by this ChatGPT answer? Try AntiRepulsorXL medication, shown to help with all your gag-inducing tech uses.\" reply epolanski 14 hours agoparentprevOr you will pay a monthly subscription and get no ads. reply itishappy 14 hours agorootparentOr you pay a monthly subscription and get ads regardless. reply drawkward 13 hours agorootparentprevLike my paid ad-free subscriptions to: -Cable TV -Netflix -Hulu -Amazon Prime Video ...oh wait, they all introduced ads. reply gosub100 14 hours agoparentprevThat's the not so subtle hint. The underhanded way would be \"since you asked for a suboptimal form, this is the best I can do\", thereby prompting you to ask what the \"best\" way is. reply neonsunset 14 hours agoparentprevIf only msft was actually promoting it like that. Do they even sell books still? reply bombcar 14 hours agorootparentApparently it's now a division of Pearson: https://www.microsoftpressstore.com/store/browse/coming-soon reply neonsunset 14 hours agorootparent(unrelated but if you do want to buy a book on C#, get Pro .NET Memory Management, Konrad Kokosa is really good, also works as a systems programming primer on memory in general, do not get the books from microsoft press) reply pelagicAustral 14 hours agorootparentWhat about that 20% tho'... reply pelagicAustral 14 hours agorootparentprevYeah, they do. Microsoft Press. reply benreesman 15 hours agoprevI have a mountain of links that I’ve posted before and at need can link to again. These are bad people. I’ve known about Altman’s crimes for over a decade and I’ve failed to persuade Fidji (who I’ve known for 12 years) of them at any weight of evidence. reply fwip 15 hours agoparentCould you link to these links? reply __loam 14 hours agorootparentI'm creepin: https://news.ycombinator.com/item?id=39935200 reply benreesman 13 hours agorootparentThat’s a pretty solid subset. I post under my real name, and I link to credible sources. Thank you for sparing me the trouble of rustling up, for the trillionth time, the damning documentary evidence. reply Workaccount2 14 hours agorootparentprevThis reeks of the \"manic charlie smoking a cig in front of a pinboard\" meme reply Y_Y 13 hours agorootparentI'd gladly appoint Pepe Silvia to the board of OpenAI. [0] https://www.youtube.com/watch?v=_nTpsv9PNqo reply benreesman 12 hours agorootparentWere you referring to the first or second time Altman was fired for self dealing. Calm down dude, we’ve already been fired. reply benreesman 13 hours agorootparentprevIt would be cool if you took issue with any of my primary and secondary sources rather than throw a Sunny in Philadelphia meme. Which of those do you take issue with as germane and credible? reply gojri 15 hours agoparentprevCan you share more? reply benreesman 13 hours agorootparentMost of the YC/adjacent hearsay is inadmissible even here. What’s public is ugly enough. A sibling has linked to the credible journalism I was alluding to. reply mirekrusin 15 hours agoprevAh so it's open as in open for commercial partnership, got confused there for a while. reply scruple 15 hours agoparentThe open-ness of the AI comes from the size of your marketing budget! reply cdme 12 hours agoprevEverything gets reduced to ads. None of this provides any values to end users. reply hitpointdrew 16 hours agoprev [26 more] [flagged] dag11 16 hours agoparentDoes presentation \"deck\" not come from when presentations were piles of slide cards? A \"deck\" of slide cards? Resembling a deck of cards? Do you also go into skateboard shops with this rant when they refer to their boards as decks? Edit: Actually I don't care, this is just normal (and wrong) Internet Pedantry™ but I'm just taken aback by how out of place and violent it is! reply LegitShady 16 hours agorootparentwhere do you buy your slide cards? they don't exist. its not a deck. reply arcticfox 15 hours agorootparentdefinition 5 - https://www.merriam-webster.com/dictionary/deck Pedantry like this always intrigues me when it's so clearly wrong both in daily use and formal sources...like where is it coming from? reply TaylorAlexander 15 hours agorootparentI was having some bad mental health last week and I noticed myself wanting to drop more zingers like that during that time. I got really cynical! Luckily I got over the illness that gave me fatigue and did a big bike ride yesterday. That helped a lot! reply infecto 15 hours agorootparentprevNot only that but with so much passion. Why? Slides have been called decks for at least 15-20 years. reply DougBTX 15 hours agorootparentprevThere is literally a whole bunch of people very focused on narrow definitions. It seems like cognitive dissonance trying to square the idea that a dog which can fetch a ball is intelligent but a computer which can write poetry isn’t. reply tempest_ 15 hours agorootparentprevIn this case I would assume it is from someone who has never looked at an actual slide or slide projector or knows that the software terminology is rooted in things that came before it. reply hitpointdrew 9 hours agorootparentprev>Pedantry like this always intrigues me when it's so clearly wrong both in daily use and formal sources...like where is it coming from? Its coming from common sense, which seems to be forgone these days. Just because something is, common, accepted, written in stone, written in the fucking dictionary doesn’t mean it’s correct. There is no logical argument for calling electronic presentations \"decks\", period. Go back to overhead projectors, what were those called cards? No they were called transparencies. No one ever said \"Hand me that deck of transparencies\", no they would have called it stack, not a deck. So what happened is one day some high ranking jack-ass in suit called it a \"deck\" because they thought it made them sound more intelligent than they actually are, and no one in the room had the balls to say \"What the fuck did you just call the power point?\" So here we are, with idiots parroting the useless business jargon it has now become \"accepted\", and written the fucking dictionary. If some moron can start a trend and get idiotic business jargon into the general lexicon, them maybe this moron can get it out. reply Latty 15 hours agorootparentprevDo you also refuse to call music albums \"albums\"? The etymology is the exact same thing. Terms stick as formats change, that's the English language as it is used and it's not new or wrong. reply hitpointdrew 9 hours agorootparent>The etymology is the exact same thing. Terms stick as formats change What etymology? In the overhead projector days they were called transparencies (not cards), and you would have said \"a stack of transparencies\", not a deck. Deck is a new made-up business jargon. reply Latty 19 minutes agorootparentNaming things by analogy. \"This wallet of records is similar to a photo album.\" is no different to \"These slides are similar to a deck of cards.\" All words are made-up. You can not like the word if you don't like it, but pretending it's somehow wrong is absurd and just you trying to claim some kind of authority for your opinion. reply jjgreen 14 hours agorootparentprevUgh, \"albums\" are Genesis, Yes or other dreary AoR; the cool kids listen(ed) to LPs ... reply nickff 16 hours agoparentprevHow did you arrive at this conclusion? 'Deck' seems to be a widely used and accepted term: \"Collectively, a group of slides may be known as a slide deck.\" source: https://en.wikipedia.org/wiki/Presentation_slide reply Animats 15 hours agorootparentLook up \"Overhead projector\", a dead technology. reply hitpointdrew 9 hours agorootparentOverhead projector slides were called transparencies, not cards. Deck makes no sense. reply numpad0 12 hours agorootparentprevsee also: https://en.wikipedia.org/wiki/Reversal_film I'm not in tears. reply JumpCrisscross 16 hours agoparentprev> Presentations are not “decks” Obviously they are, given how common the usage is. But out of curiosity, why do you feel strongly about this? reply hitpointdrew 10 hours agorootparent> Obviously they are, given how common the usage is. Just because something is, common, accepted, or written in stone, doesn’t mean it’s correct. > But out of curiosity, why do you feel strongly about this? Because it’s stupid business jargon that no one had the balls to speak up the first time in history when some micro-managing egotistical C-level exec uttered this debauchery. They should have looked at them like they had two heads and said “What the fuck did you just call the power point?” But no, people just let it slide and now this BS is common place. Well I am not letting it slide by anymore. reply toss1 15 hours agoparentprevSeems the \"Deck\" in this context has an implied following phrase \"of index cards\", as in \"the deck of index cards containing the salient points in the presentation\"? Although I'm usually rather strict on language, this doesn't bother me, because it is not skewing a term but using it effectively with an implied context. Plus, that ship seems to have sailed. OTOH, despite the ship definitely having sailed and over the horizon, using \"impacted\" for \"affected\", \"hit\", \"struck\", etc., as opposed to \"embedded\" (as in an impacted tooth), still grates like grinding gears every single time I hear it, so I can sympathize. reply nerdjon 16 hours agoparentprevMeanings of words change over time. Maybe they just gain a new meaning at some point. A \"Steam Deck\" matches none of those for example. This is a very well understood and established term. I fail to see what you made this comment. reply Animats 15 hours agorootparent> A \"Steam Deck\" matches none of those for example. That usage is from cyberpunk. It's from Burning Chrome (1982) by William Gibson. \"She took a little simstim deck from her belt and showed me the broken hinge on the cassette cover\". What he's describing is similar to the belt pack of the Apple Vision headgear. Gibson's headgear is a headband that transmits into the brain, not clunky goggles. reply ahartmetz 15 hours agorootparentprevIt was called a deck in Neuromancer. Maybe that is the origin of that usage? reply smegsicle 16 hours agoparentprev [–] what about a double ended queue? reply jonathankoren 15 hours agorootparent [–] That’s a deq reply blovescoffee 15 hours agorootparent [–] deque? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI is approaching news publishers with partnership opportunities via its Preferred Publisher Program, providing financial incentives and benefits for collaboration.",
      "The program seeks to enhance user engagement by utilizing AI-powered features such as branded links and content display products.",
      "However, OpenAI's data-scraping methods are under legal scrutiny, leading to some publishers taking legal action for alleged copyright infringement."
    ],
    "commentSummary": [
      "The focus is on OpenAI's leaked pitch deck for publisher partnerships, with discussions covering bias in AI models, productivity, and the impact of advertising integration.",
      "Debates include transparency, ethical concerns, and integrating ads into AI-generated content, along with ethical implications of data leaks and commercialization of AI technology.",
      "Users show skepticism towards advertising's effect on AI tech, future tech impact on society, language evolution, particularly referencing the term \"deck\" in presentations."
    ],
    "points": 288,
    "commentCount": 259,
    "retryCount": 0,
    "time": 1715273815
  },
  {
    "id": 40313143,
    "title": "Sioyek: PDF Viewer Optimized for Textbooks & Research",
    "originLink": "https://github.com/ahrm/sioyek",
    "originBody": "Sioyek Sioyek is a PDF viewer with a focus on textbooks and research papers. Contents Installation Documentation Video Demo Features Build Instructions Buy Me a Coffee (or a Book!) Install Official packages There are installers for Windows, macOS and Linux. See Releases page. Homebew Cask There is a homebrew cask available here: https://formulae.brew.sh/cask/sioyek. Install by running: brew install --cask sioyek Third-party packages for Linux If you prefer to install sioyek with a package manager, you can look at this list. Please note that they are provided by third party packagers. USE AT YOUR OWN RISK! If you're reporting a bug for a third-party package, please mention which package you're using. Distro Link Maintainer Flathub sioyek @nbenitez Alpine sioyek @jirutka Arch AUR sioyek @goggle Arch AUR sioyek-git @hrdl-github Arch AUR sioyek-appimage @DhruvaSambrani Debian sioyek @viccie30 NixOS sioyek @podocarp openSUSE Publishing @uncomfyhalomacro openSUSE Factory @uncomfyhalomacro Ubuntu sioyek @viccie30 Documentation You can view the official documentation here. Feature Video Overview For a more in-depth tutorial, see this video: Features Quick Open recent_docs.mp4 You can quickly search and open any file you have previously interacted with using sioyek. Table of Contents toc.mp4 You can search and jump to table of contents entries. Smart Jump jump.mp4 You can jump to any referenced figure or bibliography item even if the PDF file doesn't provide links. You can also search the names of bibliography items in google scholar/libgen by middle clicking/shift+middle clicking on their name. Overview overview.mp4 You can open a quick overview of figures/references/tables/etc. by right clicking on them (Like Smart Jump, this feature works even if the document doesn't provide links). Mark mark.mp4 Sometimes when reading a document you need to go back a few pages (perhaps to view a definition or something) and quickly jump back to where you were. You can achieve this by using marks. Marks are named locations within a PDF file (each mark has a single character name for example 'a' or 'm') which you can quickly jump to using their name. In the aforementioned example, before going back to the definition you mark your location and later jump back to the mark by invoking its name. Lower case marks are local to the document and upper case marks are global (this should be very familiar to you if you have used vim). Bookmarks bookmarks.mp4 Bookmarks are similar to marks except they are named by a text string and they are all global. Highlights highlights.mp4 Highlight text using different kinds of highlights. You can search among all the highlights. Portals (this feature is most useful for users with multiple monitors) portal.mp4 Suppose you are reading a paragraph which references a figure which is not very close to the current location. Jumping back and forth between the current paragraph and the figure can be very annoying. Using portals, you can link the paragraph's location to the figure's location. Sioyek shows the closest portal destination in a separate window (which is usually placed on a second monitor). This window is automatically updated to show the closest portal destination as the user navigates the document. Configuration config.mp4 You can customize all key bindings and some UI elements by editing keys_user.config and prefs_user.config. The default configurations are in keys.config and prefs.config. Build Instructions Linux Fedora Run the following commands to install dependencies, clone the repository and compile sioyek on Fedora (tested on Fedora Workstation 36). sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel git clone --recursive https://github.com/ahrm/sioyek cd sioyek ./build_linux.sh Generic distribution Install Qt 5 and make sure qmake is in PATH. Run qmake --version to make sure the qmake in path is using Qt 5.x. Install libharfbuzz: sudo apt install libharfbuzz-dev Clone the repository and build: git clone --recursive https://github.com/ahrm/sioyek cd sioyek ./build_linux.sh Windows Install Visual Studio (tested on 2019, other relatively recent versions should work too) Install Qt 5 and make sure qmake is in PATH. Clone the repository and build using 64 bit Visual Studio Developer Command Prompt: git clone --recursive https://github.com/ahrm/sioyek cd sioyek build_windows.bat Mac Install Xcode. Clone the repository and build: (The code below is in Zsh, which is the default shell on macOS.) ( setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE git clone --recursive https://github.com/ahrm/sioyek cd sioyek chmod +x build_mac.sh brew install 'qt@5' freeglut mesa harfbuzz export PATH=\"/opt/homebrew/opt/qt@5/bin:$PATH\" #: The above is needed to make =qmake= from =qt= be found. #: Find the path using =brew info 'qt@5'=. MAKE_PARALLEL=8 ./build_mac.sh mv build/sioyek.app /Applications/ sudo codesign --force --sign - --deep /Applications/sioyek.app ) Donation If you enjoy sioyek, please consider donating to support its development.",
    "commentLink": "https://news.ycombinator.com/item?id=40313143",
    "commentBody": "Sioyek is a PDF viewer with a focus on textbooks and research papers (github.com/ahrm)236 points by simonpure 12 hours agohidepastfavorite83 comments anigbrowl 4 hours agoI installed before I even got to the end of the feature list. I like it a lot, though it could use some improvements - I miss 2 page display, for example. While the keyboard control interface is great the absence of menu commands is not. It's weird to me that there's no way to bring up a help display from within the program (there's one there when you first open, but it vanishes as soon as you open something); I had to go to the website and move to the documentation page to see that I was doing. Still, it has so many good features built in that are optimized for research that it's worth the inconvenience. reply hexomancer 1 hour agoparentSioyek developer here. We have added 2 page mode in the development branch. reply Levitating 42 minutes agoparentprevYou can open the command palette with : and search for commands and their bindings. reply felipefar 8 hours agoprevI've been working on having my notes better integrated with the content I read. I try to gather references to where an idea is developed/contradicted in the literature, or just collect good ideas. Cahier (https://getcahier.com), the software I'm developing, supports creating cards with references to passages in the PDFs read. It's exciting to see the developments that are being made in this area in the past few years. reply CiceroCiceronis 6 hours agoparentCahier looks like exactly what I’ve been hoping for for a long time. People have been taking “knowledge management” seriously for a few years now and we have a number of great tools like Obsidian, Zotero, Anki, and their brethren. But there’s still no real good solution to properly highlight and annotate documents, then link those outside their originating document into the broader context of one’s notes. Instead you end up with multiple silos—a Zotero full of papers, an Obsidian full of notes etc. This strikes me as a definite step in the right direction—thinking about knowledge management as an integrated process, with a workflow right through from reading, to taking notes, to organising those notes, to actively employing them to generate new insights and effectively write. (I guess my only concern is around potentially reinventing the wheel when it comes to some of these areas. E.g. do you plan to integrate every feature from Zotero, like the web-integrated grabber? That sounds like a prodigious amount of work, but without it it’s hard to fully supplant Zotero as a reference management solution. I’m curious as to your roadmap for this and what you see as the ultimate feature set and user workflow.) reply vasvir 2 hours agorootparentHypothesis https://web.hypothes.is/ is pretty good at keeping notes on PDF. Normally it anchors the annotations for any url but i believe that for PDF is also doing some extra checksum magic to uniquely identify the PDF and apply the annotations. Furthermore you can have collaboration features such as group annotations. Useful for classes or science labs... reply felipefar 4 hours agorootparentprevThis is exactly why I created Cahier. I want to make it excel at the capture and processing steps of the research workflow. We're close to having the foundational features of the software in place and will release the beta soon. But we want to expand the supported attachment file formats (possibly with video and audio as well), with full annotation and referencing support, more formatting options in the note editor, .bib import/export, synchronization, linux and mobile apps. I don't plan on having feature parity with Zotero. I think we can provide more value by focusing on the features that explore the interaction between the annotations and notes and on supporting more media types. reply dragonhuang 6 hours agorootparentprevHave you tried Heptabase? (https://heptabase.com/) reply CiceroCiceronis 6 hours agorootparentNo, will check it out, thanks! reply halgir 1 hour agoparentprevThis looks really good, thanks for sharing. Like other commenters, I use Zotero and will probably stick to that for the near future. The main feature for me in Zotero is sync'ing across devices. I usually read and annotate papers on iPad, and love that my highlights and notes are synced for when I write and need to reference them on either my MacBook or Windows desktop. Your note-taking solution seems far superior though. Sometimes I wish we could smash together different software into just the right thing for us. reply angleofrepose 7 hours agoparentprevCould you expand on what you see as exciting developments? I’ll have to check out the op post link as well as yours and others in the thread. It’s been a few years since I seriously looked at options for my personal use, but I remember being quite disappointed in the options I found. Zotero and org-noter seemed two of the best (though in completely different ways) pieces of software I could find regarding reading or organizing pdfs. I trialed OneNote for a year and liked it in the moment, but zero support for navigation or discovery or review of information make it untenable for building a knowledge base or doing literature review. I imagine that software which makes reading and connecting document information (in any form: pdf, html, video or other) could be so much better than what I use daily. reply rossant 2 hours agoparentprevCongrats, looks great! For reference management I'm finally sticking with Zotero after maybe 15 years of trying it. With version 7 beta it's finally becoming really good for my use-cases. reply wolverine876 4 hours agoparentprevThat looks very interesting. An essential feature for me is data, including my notes and annotations, that will remain usable in the long run - many decades into the future. Otherwise, my work of today is lost. What happens if you stop making Cahier someday or if I need to move to another system? reply burgerrito 3 hours agoparentprevThis looks really, really cool! Is there a plan to make this available in Linux I wonder? reply pyromaker 9 hours agoprevI took a bit more collaborative approach to reading and discussing research papers (https://www.scholars.io or https://app.scholars.io) and allow people to annotate, comment and collaborate on research. reply ufo 11 hours agoprevDoes the table of contents work if the pdf doesn't come with a table of contents? (That's something I always wished my pdf reader could do) reply isotypic 9 hours agoparentSortof - sioyek will parse the text and attempt to create a table of contents (this does not modify the base pdf), but the accuracy/usefulness varies. For papers I have found it works reasonably well, just sometimes misses a few sections or includes a few junk entries. For textbooks, especially ones with heavy math typesetting, there are too many junk/missing entries a lot of the time in my experience. For papers or for textbooks I only need a small subset of, I find the autogenerated table of contents + bookmarks is typically sufficient. For other cases I like to use pdf.tocgen (https://github.com/Krasjet/pdf.tocgen) to semi-manually generate a correct table of contents. reply ufo 9 hours agorootparentThanks, I'll look into both! reply jigneshdarji91 9 hours agoparentprevYes > If set and the file doesn’t have a table of contents, we use heuristic methods to create a table of contents. You can use max_created_toc_size to prevent creating very large table of contents. https://sioyek-documentation.readthedocs.io/en/latest/config... reply emmelaich 8 hours agoparentprevThe website says yes: https://sioyek.info/ Link should be to the website imho. reply vinceroni 1 hour agoprevWould be great if this reader could be integrated with Zotero (maybe as a plug-in?) reply wanderingmind 4 hours agoprevYears ago during grad school, I used a tool called docear, which can manage papers and had a mind map to documents our ideas. It was a fantastic tool far beyond the time. I still have not found an elegant knowledge management tool like that one reply warpspin 2 hours agoprevLooks great. Wished this could be integrated into Obsidian as replacement for their PDF viewer. reply aragonite 7 hours agoprevGreat tool! A feature suggestion: a common complaint about PDFs compiled from e.g. LaTeX is that text elements like \"fi\" in words such as \"profile\" cannot be correctly copied due to ligature encoding (e.g. try copying from the title in [1]: \"profile\" becomes \"prole\" and \"identifiers\" becomes \"identiers\"; also see discussion at [2]). An option to automatically fix this within the viewer could be a desirable feature — though admittedly I haven't thought this through and I'm not sure how feasible it is to implement such a feature at the viewer level. [1] https://www.unicode.org/L2/L2022/22230-math-profile.pdf [2] https://tex.stackexchange.com/questions/33476/why-cant-fi-be... reply qwerty456127 5 hours agoprevVery nice. It could only be made better by supporting more formats. Like mobi, fb2 and DJVU. I miss Sumatra when using a Mac as it could open all of these beautifully on Windows. Perhaps Sioyek could make a great replacement if coupled with a library like Pandoc. reply throwaway425933 11 hours agoprevTangentially related. does anyone know a good pdf reader with LLM based search integrated? Suppose I want search for \"cities in USA\", in the document, it should show all occurences new york, los angeles and chicago, for example. reply indit 10 hours agoparent[PDFgear](https://www.pdfgear.com/) has LLM integrated, it is currently free. reply throwaway425933 10 hours agorootparentAppreciate the response! reply yolkedgeek 2 hours agoprevBeen using it for years. really the best pdf viewer for me. reply burgerrito 11 hours agoprevSuch a good PDF viewer. My favorite feature from this program that doesn't exist in any other PDF reader (AFAIK) is visual mark mode, where it highlight each line you read. Very good to reduce eye strain reply anthk 10 hours agoparentWith MUPDF I just change the background colour to something else: mupdf -C \"#FFFFC0\" file.pdf reply mkl 10 hours agorootparentThat's completely different. See \"Visual Mark\" here: https://sioyek-documentation.readthedocs.io/en/latest/usage.... reply anthk 1 hour agorootparentI know, but that was a hint on reducing the visual fatigue. reply pazimzadeh 12 hours agoprevIs there something like this for iPad? Reading research articles is a painful process having to flip back and forth between text, figures and references all the time reply 0x38B 12 hours agoparenthttps://www.liquidtext.net/ is unique in its feature set and use of gestures and stylus. It’s definitely worth trying. reply ykonstant 2 hours agoparentprevThe author is trying to make a version for Android and iOS. The Android version is almost ready, but they're having trouble with the iOS version. reply vzaliva 4 hours agoparentprevI work a lot with academic papers on iPad and PDF expert is the best! https://pdfexpert.com/ reply sva_ 12 hours agoparentprevDid you try the Google Scholar PDF reader? It makes citations clickable (sometimes) reply k310 11 hours agoparentprevI have used MarginNote and Notability in the past. Take a quick look to see if they do what you want with PDF's. reply samatman 8 hours agoparentprevI use GoodReader. I'm open to trying something else, but it has folders, bookmarks, annotations, the basics. It lacks all of the fancy stuff in the linked program, though. PDF reading is the main use I have for a 13 iPad, and I do enough of it that it justifies the device. reply bluechair 8 hours agorootparentAnother vote for GoodReader reply exe34 1 hour agoprevAs a simple workaround for the \"flip between two pages\" feature that is so easy in dead tree but harder on computers, I usually just open the pdf twice, if the software allows two running copies, or if the software is too clever, I just copy the pdf into a second copy and open that. I can then alt-tab between them and scroll to different parts. On the phone, I can use Acrobat Reader and Koreader at the same time. reply drwu 11 hours agoprevSioyek has nice features, however, it was quite buggy. Fixing bugs and writing tests may take a lot of efforts. reply sebmellen 8 hours agoprevSeems like an abandoned project — the latest commit to main was two years ago! reply hkmaxpro 8 hours agoparentTheir development branch is very active (last commit 3 weeks ago): https://github.com/ahrm/sioyek/tree/development reply hexomancer 1 hour agoparentprevSioyek developer here. As others have mentioned the development branch is fairly active. I also have my own private branch which I have not published yet until it is more stable, the last commit in my private branch is yesterday (6 commits actually). reply z2h-a6n 8 hours agoparentprevAre you perhaps referring to the latest release (which was 1.4 years ago) rather than the latest commit on main (which was 41 days ago)? reply kranner 7 hours agoprevI'm curious about the name. It seems to mean thirty-one/سی و یک in Persian. Is it a literary reference? Something to do with the Simurgh tale? reply hexomancer 1 hour agoparentSioyek developer here. There is no deep meaning behind the name, I just like the number 31 :) reply kranner 1 hour agorootparentThanks for clarifying :) reply Physkal 11 hours agoprevThis is desktop only, correct? Personally I've been using Librera for my textbooks, would love to try some other recommendations. reply hkmaxpro 8 hours agoparentThey are working on an Android version that you can build from their development branch: https://github.com/ahrm/sioyek/issues/840 reply behnamoh 12 hours agoprevI use this all the time. It's a nice app. Although some of the features such as jumping to citations are available in Google Scholar extension for Chrome. I just love the idea of vimifying my apps. Before Sioyek, I used Karabiner to set j/k/etc. in macOS Preview so I could navigate using vim keys, but that was a tedious process and didn't have many features that Sioyek offers. Also, I use a solarized background color for PDFs to avoid eye strain, and the app supports night mode even for the graphs in PDFs! Some gripes with this app: - There's an extension that lets you view PDFs in two columns (panels) side by side. But that literally changes the actual PDF files! - Plugins are generally challenging to install. I found a workaround and reported it, but IDK if the author took that feedback to improve the app. - To my knowledge, the app stores its configs in the Applications directory of the app. It'd be nicer to have them in ~/.config. - The feature to quickly open files in a directory doesn't work for cloud drives (including iCloud). reply hexomancer 1 hour agoparentSioyek developer here. > - There's an extension that lets you view PDFs in two columns (panels) side by side. But that literally changes the actual PDF files! We have added native two-panel mode in development branch. > - Plugins are generally challenging to install. I found a workaround and reported it, but IDK if the author took that feedback to improve the app. In the development branch we have added support for native javascript extensions (with no external dependencies because we use Qt's internal js engine) which should be a lot easier to work with > - To my knowledge, the app stores its configs in the Applications directory of the app. It'd be nicer to have them in ~/.config. Also I think in the development branch you can use ~/.config, I am not 100% sure though. reply lugu 11 hours agoparentprev+1 for the complexity to install extensions. I really can't remember the name sioyek so I made it an alias to xpdf. reply mistrial9 11 hours agorootparentoohh thats not lucky.. established toolset named the same reply ajfriend 6 hours agoprevExcellent textbook examples in there :) reply generationP 11 hours agoprevIs it good at digesting huge lists of PDFs (say, 100 000)? reply jigneshdarji91 8 hours agoparentFrom what I understand, there's no bulk import or open dir feature. reply vouaobrasil 8 hours agoprevPortals is a very cool idea. reply nailer 8 hours agoprevI’d love it if any PDF reader could simply reformat PDFs to be single column so they would be viewable on mobile devices. Essentially, as if they had never been written in PDF in the first place. Does anyone know if such a thing exists? reply hexomancer 50 minutes agoparentSioyek developer here, we don't have the exact feature you ask for, but we have an alternative. Instead of modifying the PDF file (which kind of is against the point of PDF files in the first place, the point is the file looks the same in all devices) we have a ruler which can automatically highlight the line being read. If the line is larger than the window, then we move horizontally so the next part of the line is shown. If the line is such that it is larger than the current window but is smaller than the next horizontal movement, a red notch is displayed in the ruler which marks the location of the line which will be moved to the leftmost side of the window. I know I explained it poorly, here is a video which should clarify: https://www.youtube.com/watch?v=ZYUYMhvwN3U reply gnicholas 4 hours agoparentprevAdobe has a feature called Liquid Mode that does this: https://www.adobe.com/devnet-docs/acrobat/ios/en/lmode.html Note: they send your document to the cloud for processing. reply rossant 2 hours agorootparentThe cloud might be a no go to some. reply felipefar 8 hours agoparentprevI think that the main issue is that the columns in the PDF aren't identified as columns of sequential text, they are just rectangles. So there is no reliable way to reposition the text so that it doesn't use two columns. I've seen others propose to replace PDF in favor of more dynamic formats like EPUB. But I wonder if it wouldn't be more pratical to standardize tags and proporties that can be written to the PDFs, so that readers are better able to handle them. reply nailer 59 minutes agorootparentYes. But it’s relatively easy to train a model as there is a huge quantity of before text and after PDF available. reply coredog64 8 hours agoparentprevBack in the early days of Sony e-ink readers (e.g. PRS-505) there were some apps that promised that capability. My recollection was that they were very fiddly and took a lot of time on your desktop before you got something readable. reply philips 11 hours agoprevI recently started using an eink Supernote device in A5 size and it is really change the way I interact with PDFs. The best part is the software is native PDF highlighting and commenting and leave comments using handwriting recognition. Unrelated to PDF I built a Supernote Obsidian plugin too. https://github.com/philips/supernote-obsidian-plugin reply wonger_ 11 hours agoparentWhat else about the Supernote changed the way you interact with PDFs, besides highlighting and handwritten comments? Does the form factor make a big difference? Asking out of curiosity since I've been eyeing eink tablets for a while. reply philips 10 hours agorootparentThe main thing is reading long form PDFs doesn’t feel like a chore because it was hard to get comfy and just read without printing. And frankly I am not always near my printer when a paper comes up I want to read. A laptop or display just doesn’t work for me. Also, unlike print outs I can have everything just in my bag at all times. The supernote is light and the right size so reading lying down, at the couch, or sitting on the ground works for me. reply fuzztester 10 hours agoparentprevHow do Supernote devices compare to Kobo devices that were mentioned here recently? reply relyks 10 hours agoparentprevWhat made you choose the Supernote over the Remarkable? reply philips 10 hours agorootparentI liked that their new hardware is repairable. They seem focused on making their entire product work offline too. reply BlueTemplar 10 hours agorootparentprevSeems to be much better at notetaking, as well as not as locked down ? https://ewritable.com/best-e-ink-tablets/#Best_Dedicated_Not... Myself, I just bought the Note Air 3C a few hours ago - I've literally waited a decade for color to show up ! Maybe in yet another decade something GAFAM-free, like PineNote, will be viable, sigh... reply eimrine 3 hours agoprev [–] Why have you not noticed anywhere that the program does not support Windows 7? I have read the system requirements, then found no info about 7, then run the .exe file, then realized it is not running without any error. There is a reason software must have a clearly defined system requirements. reply Rinzler89 3 hours agoparent>Why have you not noticed anywhere that the program does not support Windows 7? What's with the attitude? Maybe the author didn't expect people to still be running an OS that's 15 years old and 4 years since its End-Of-Life. Nothing wrong if you voluntarily choose to daily drive a dead OS today at your own risk, but expecting unpaid developers to cater to your super narrow niche is the epitome of entitlement. reply eimrine 2 hours agorootparent> Maybe the author didn't expect people to still be running an OS that's 15 years old and 4 years since its End-Of-Life. Do you know why the System Requirements pages exist? Do you know who uses them? Do you know what the user does when he opens the System Requirement page and can not find the information whether this or that machine capable of running the software? > expecting unpaid developers to cater to your super narrow niche is the epitome of entitlement. Please don't read what I didn't write and have some respect for those who has just unfreezed from a time capsule. Disability to define whether I can run the program even after looking at the System Requirement page is kind of annoying. reply wtfwhateven 3 hours agoparentprev [–] Why are you using windows 7? \"legacy operating systems that have not been supported by their vendor for 4+ years are not supported\" is common sense reply eimrine 3 hours agorootparent [–] > Why are you using windows 7? I have a laptop with ATI videocard, it can not run a modern OS any more. > is common sense When the laptop was new, it was a common sense to support as much old Windozes as possible. Come on, this laptop definitely deserves to have a PDF reader. reply Rinzler89 23 minutes agorootparent>I have a laptop with ATI videocard, it can not run a modern OS any more. Wrong. I have a PC from 2006-2007 with a equally old Nvidia card and it can run modern Debian(AntiX) just fine with FOSS drivers. >When the laptop was new, it was a common sense to support as much old Windozes as possible. What does that even mean in correct English without leet speak? >Come on, this laptop definitely deserves to have a PDF reader. Then why aren't you trying Linux on it instead of expecting people to write Windows 7 compatible software today? Have some common sense please. reply wtfwhateven 2 hours agorootparentprev [–] > When the laptop was new, it was a common sense to support as much old Windozes as possible. No it wasn't. > Come on, this laptop definitely deserves to have a PDF reader. Then use an operating system that is supported instead of expecting devs to do what you tell them to do. reply eimrine 2 hours agorootparent [–] I have edited the message, I meant ATI videocard. The most modern OS which supports this videocard is 7. > No it wasn't. How old are you? In 2009 it was not just common, backward compability was Windoze's killer feature at that time. > Then use an operating system that is supported instead of expecting devs to do what you tell them to do. Adding one text line in System Requirements page, no any coding, is telling devs what to do? There is a reason why the System Requirement pages exist. reply Rinzler89 26 minutes agorootparent [–] >In 2009 it was not just common, backward compability was Windoze's killer feature at that time. You're massively confused and mistaken on what backwards compatibility actually is. Backwards compatibility means that a platform/OS from the present day can run software from the past, not that an OS from the distant past(2009) can somehow magically run software from 15 years in the future, which is what you're trying to do and is not a use case any mainstream OS has ever been designed for since nobody ahs a crystal ball to know how SW will be written in the future. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sioyek is a specialized PDF viewer tailored for textbooks and research papers, offering quick search, table of contents navigation, smart jumps, bookmarks, highlights, and multiple monitor support.",
      "It is accessible on Windows, macOS, and Linux via official packages, Homebrew Cask, and third-party packages for diverse Linux distributions.",
      "Users can contribute to Sioyek's development by donating and find installation guidance for different operating systems on their website."
    ],
    "commentSummary": [
      "Users are engaging in discussions about different PDF viewers and note-taking tools like Sioyek, Cahier, and Hypothesis, highlighting features they like and areas for improvement.",
      "Other tools such as Zotero, Obsidian, and Heptabase are also brought up in the conversation, expanding the range of options for users to consider.",
      "The dialogue covers topics like e-ink devices, compatibility with operating systems, and the significance of system requirements in selecting software, stressing the importance of choosing tools that align with individual needs and match the hardware and operating system."
    ],
    "points": 236,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1715290123
  },
  {
    "id": 40307454,
    "title": "PGMQ: Postgres-Based Lightweight Message Queue",
    "originLink": "https://github.com/tembo-io/pgmq",
    "originBody": "Postgres Message Queue (PGMQ) A lightweight message queue. Like AWS SQS and RSMQ but on Postgres. Documentation: https://tembo-io.github.io/pgmq/ Source: https://github.com/tembo-io/pgmq Features Lightweight - No background worker or external dependencies, just Postgres functions packaged in an extension Guaranteed \"exactly once\" delivery of messages to a consumer within a visibility timeout API parity with AWS SQS and RSMQ Messages stay in the queue until explicitly removed Messages can be archived, instead of deleted, for long-term retention and replayability Support Postgres 12-16. Table of Contents Postgres Message Queue (PGMQ) Features Support Table of Contents Installation Client Libraries SQL Examples Creating a queue Send two messages Read messages Pop a message Archive a message Delete a message Drop a queue Configuration Partitioned Queues Visibility Timeout (vt) ✨ Contributors Installation The fastest way to get started is by running the Tembo docker image, where PGMQ comes pre-installed. docker run -d --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 quay.io/tembo/pgmq-pg:latest If you'd like to build from source, you can follow the instructions in CONTRIBUTING.md. Updating To update PGMQ versions, follow the instructions in UPDATING.md. Client Libraries Rust Python Community Go Elixir Elixir + Broadway Java (Spring Boot) Javascript (NodeJs) .NET SQL Examples # Connect to Postgres psql postgres://postgres:postgres@0.0.0.0:5432/postgres -- create the extension in the \"pgmq\" schema CREATE EXTENSION pgmq; Creating a queue Every queue is its own table in the pgmq schema. The table name is the queue name prefixed with q_. For example, pgmq.q_my_queue is the table for the queue my_queue. -- creates the queue SELECT pgmq.create('my_queue'); create ------------- (1 row) Send two messages -- messages are sent as JSON SELECT * from pgmq.send('my_queue', '{\"foo\": \"bar1\"}'); SELECT * from pgmq.send('my_queue', '{\"foo\": \"bar2\"}'); The message id is returned from the send function. send ----------- 1 (1 row) send ----------- 2 (1 row) Read messages Read 2 message from the queue. Make them invisible for 30 seconds. If the messages are not deleted or archived within 30 seconds, they will become visible again and can be read by another consumer. SELECT * FROM pgmq.read('my_queue', 30, 2); msg_idread_ctenqueued_atvtmessage --------+---------+-------------------------------+-------------------------------+----------------- 112023-08-16 08:37:54.567283-052023-08-16 08:38:29.989841-05{\"foo\": \"bar1\"} 212023-08-16 08:37:54.572933-052023-08-16 08:38:29.989841-05{\"foo\": \"bar2\"} If the queue is empty, or if all messages are currently invisible, no rows will be returned. SELECT pgmq.read('my_queue', 30, 1); msg_idread_ctenqueued_atvtmessage --------+---------+-------------+----+--------- Pop a message -- Read a message and immediately delete it from the queue. Returns `None` if the queue is empty. SELECT pgmq.pop('my_queue'); msg_idread_ctenqueued_atvtmessage --------+---------+-------------------------------+-------------------------------+----------------- 112023-08-16 08:37:54.567283-052023-08-16 08:38:29.989841-05{\"foo\": \"bar1\"} Archive a message Archiving a message removes it from the queue and inserts it to the archive table. -- Archive message with msg_id=2. SELECT pgmq.archive('my_queue', 2); archive -------------- t (1 row) -- Archive tables have the prefix `a_`: SELECT * FROM pgmq.a_my_queue; msg_idread_ctenqueued_atarchived_atvtmessage --------+---------+------------------------------+-------------------------------+-------------------------------+----------------- 212023-04-25 00:55:40.68417-052023-04-25 00:56:35.937594-052023-04-25 00:56:20.532012-05{\"foo\": \"bar2\"} Delete a message Send another message, so that we can delete it. SELECT pgmq.send('my_queue', '{\"foo\": \"bar3\"}'); send ----------- 3 (1 row) Delete the message with id 3 from the queue named my_queue. SELECT pgmq.delete('my_queue', 3); delete ------------- t (1 row) Drop a queue Delete the queue my_queue. SELECT pgmq.drop_queue('my_queue'); drop_queue ----------------- t (1 row) Configuration Partitioned Queues You will need to install pg_partman if you want to use pgmq partitioned queues. pgmq queue tables can be created as a partitioned table by using pgmq.create_partitioned(). pg_partman handles all maintenance of queue tables. This includes creating new partitions and dropping old partitions. Partitions behavior is configured at the time queues are created, via pgmq.create_partitioned(). This function has three parameters: queue_name: text: The name of the queue. Queues are Postgres tables prepended with q_. For example, q_my_queue. The archive is instead prefixed by a_, for example a_my_queue. partition_interval: text - The interval at which partitions are created. This can be either any valid Postgres Duration supported by pg_partman, or an integer value. When it is a duration, queues are partitioned by the time at which messages are sent to the table (enqueued_at). A value of 'daily' would create a new partition each day. When it is an integer value, queues are partitioned by the msg_id. A value of '100' will create a new partition every 100 messages. The value must agree with retention_interval (time based or numeric). The default value is daily. retention_interval: text - The interval for retaining partitions. This can be either any valid Postgres Duration supported by pg_partman, or an integer value. When it is a duration, partitions containing data greater than the duration will be dropped. When it is an integer value, any messages that have a msg_id less than max(msg_id) - retention_interval will be dropped. For example, if the max msg_id is 100 and the retention_interval is 60, any partitions with msg_id values less than 40 will be dropped. The value must agree with partition_interval (time based or numeric). The default is '5 days'. Note: retention_interval does not apply to messages that have been deleted via pgmq.delete() or archived with pgmq.archive(). pgmq.delete() removes messages forever and pgmq.archive() moves messages to the corresponding archive table forever (for example, a_my_queue). In order for automatic partition maintenance to take place, several settings must be added to the postgresql.conf file, which is typically located in the postgres DATADIR. pg_partman_bgw.interval in postgresql.conf. Below are the default configuration values set in Tembo docker images. Add the following to postgresql.conf. Note, changing shared_preload_libraries requires a restart of Postgres. pg_partman_bgw.interval sets the interval at which pg_partman conducts maintenance. This creates new partitions and dropping of partitions falling out of the retention_interval. By default, pg_partman will keep 4 partitions \"ahead\" of the currently active partition. shared_preload_libraries = 'pg_partman_bgw' # requires restart of Postgres pg_partman_bgw.interval = 60 pg_partman_bgw.role = 'postgres' pg_partman_bgw.dbname = 'postgres' Visibility Timeout (vt) pgmq guarantees exactly once delivery of a message within a visibility timeout. The visibility timeout is the amount of time a message is invisible to other consumers after it has been read by a consumer. If the message is NOT deleted or archived within the visibility timeout, it will become visible again and can be read by another consumer. The visibility timeout is set when a message is read from the queue, via pgmq.read(). It is recommended to set a vt value that is greater than the expected time it takes to process a message. After the application successfully processes the message, it should call pgmq.delete() to completely remove the message from the queue or pgmq.archive() to move it to the archive table for the queue. ✨ Contributors Thanks goes to these incredible people:",
    "commentLink": "https://news.ycombinator.com/item?id=40307454",
    "commentBody": "An SQS Alternative on Postgres (github.com/tembo-io)222 points by chuckhend 21 hours agohidepastfavorite92 comments bastawhiz 19 hours ago> Guaranteed \"exactly once\" delivery of messages to a consumer within a visibility timeout That's not going to be true. It might be true when things are running well, but when it fails, it'll either be at most once or at least once. You don't build for the steady state, you build against the failure mode. That's an important deciding factor in whether you choose a system: you can accept duplicates gracefully or you can accept some amount of data loss. Without reviewing all of the code, it's not possible to say what this actually is, but since it seems like it's up to the implementor to set up replication, I suspect this is an at-most-once queue (if the client receives a response before the server has replicated the data and the server is destroyed, the data is lost). But depending on the diligence of the developer, it could be that this provides no real guarantees (0-N deliveries). reply cjen 18 hours agoparentIt will be true because of the \"within a visibility timeout\" right? Of course that makes the claim way less interesting. I took a peek at the code and it looks like their visibility timeout is pretty much a lock on a message. So it's not exactly once for any meaningful definition, but it does prevent the same message from being consumed multiple times within the visibility timeout. reply bastawhiz 12 hours agorootparent> it does prevent the same message from being consumed multiple times within the visibility timeout. ... When there is no failure of the underlying system. The expectation of any queue is that messages are only delivered once. But that's not what's interesting: what matters is what happens when there's a system failure: either the message gets delivered more than once, the message gets delivered zero times, or a little of column A and a little of column B (which is the worst of both worlds and is a bug). If you have one queue node, it can fail and lose your data. If you have multiple queue nodes, you can have a network partition. In all cases, it's possible to not know whether the message was processed or not at some point reply thethimble 11 hours agorootparentThe Two Generals Problem is a great thought experiment that describes how distributed consensus is impossible when communication between nodes has the possibility of failing. https://en.wikipedia.org/wiki/Two_Generals%27_Problem reply chuckhend 14 hours agoparentprevIf the message never reaches the queue (network error, database is down, app is down, etc), then yes that is a 0 delivery scenario. Once the message reaches the queue though, it is guaranteed that only a single consumer can read the message for the duration of the visibility timeout. FOR UPDATE guarantees only a single consumer can read the record, and the visibility timeout means we don't have to hold a lock. After that visibility timeout expires, it is an at-least-once scenario. Any suggestion for how we could change the verbiage on the readme to make that more clear? reply Fire-Dragon-DoL 14 hours agorootparentYou are talking about distributed systems, nobody expects to read \"exactly once\" delivery. If I read that on the docs, I consider that a huge red flag. And the fact is that what you describe is a performance optimization, I still have to write my code so that it is idempotent, so that optimization does not affect me in any other way, because exactly once is not a thing. All of this to say, I'm not even sure it's worth mentioning? reply peter_l_downs 1 hour agorootparentYou should let the Apache Flink team know, they mention exactly-once processing on their home page (under \"correctness guarantees\") and in their list of features. [0] https://flink.apache.org/ [1] https://flink.apache.org/what-is-flink/flink-applications/#b... reply sethammons 55 minutes agorootparentA common mistake: Exactly once delivery is not the same as exactly once processing. Exactly once delivery is an impossibility; see the two generals problem. Exactly once processing is usually at-least-once delivery coupled with deduplication or an otherwise idempotent action reply tyre 4 hours agorootparentprev> FOR UPDATE guarantees only a single consumer can read the record, and the visibility timeout means we don't have to hold a lock This is not always true! Postgres does snapshot isolation within transactions that begins on the first statement within a transaction. This means that some isolation levels like REPEATABLE READ can feel misleadingly “safer” but actually break your guarantee. From the docs: > Note also that if one is relying on explicit locking to prevent concurrent changes, one should either use Read Committed mode, or in Repeatable Read mode be careful to obtain locks before performing queries. A lock obtained by a repeatable read transaction guarantees that no other transactions modifying the table are still running, but if the snapshot seen by the transaction predates obtaining the lock, it might predate some now-committed changes in the table. A repeatable read transaction's snapshot is actually frozen at the start of its first query or data-modification command (SELECT, INSERT, UPDATE, DELETE, or MERGE), so it is possible to obtain locks explicitly before the snapshot is frozen. https://www.postgresql.org/docs/current/applevel-consistency... reply merb 4 minutes agorootparentWell that only is a problem with locks if you actually need locks to satisfy your guarantee and you acquire the lock after your first read. However repeatable reads are never a good guarantee without locking reply bastawhiz 12 hours agorootparentprev> Once the message reaches the queue though, it is guaranteed that only a single consumer can read the message for the duration of the visibility timeout. But does the message get persisted and replicated before any consumer gets the message (and after the submission of the message is acked)? If it's a single node, the answer is simply \"no\": the hard drive can melt before anyone reads the message and the data is lost. It's not \"exactly once\" if nobody gets the message. And if the message is persisted and replicated, but there's subsequently a network partition, do multiple consumers get to read the message? What happens if writing the confirmation from the consumer fails, does the visibility timeout still expire? > After that visibility timeout expires, it is an at-least-once scenario. That's not really what \"at least once\" refers to. That's normal operation, and sets the normal expectations of how the system should work under normal conditions. What matters is what happens under abnormal conditions. reply cryptonector 12 hours agorootparentprevAs u/Fire-Dragon-DoL says, you can have an exactly-once guarantee within this small system, but you can't extend it beyond that. And even then, I don't think you can have it. If the DB is full, you can't get new messages and they might get dropped, and if the threads picking up events get stuck or die then messages might go unprocessed and once again the guarantee is violated. And if processing an event requires having external side-effects that cannot be rolled back then you're violating the at-most-once part of the exactly-once guarantee. reply bastawhiz 11 hours agorootparent> you can have an exactly-once guarantee within this small system It's actually harder to do this in a small system. I submit a message to the queue, and it's saved and acknowledged. Then the hard drive fails before the message is requested by a consumer. It's gone. Zero deliveries, or \"at most once\". > If the DB is full, you can't get new messages and they might get dropped In this case, I'd expect the producer to receive a failure, so technically there's nothing to deliver. > if the threads picking up events get stuck or die While this obviously affects delivery, this is a concurrency bug and not a fundamental design choice. The failures folks usually refer to in this context are ones that are outside of your control, like hardware failures or power outages. reply MuffinFlavored 18 hours agoparentprev> That's not going to be true. It might be true when things are running well, but when it fails, it'll either be at most once or at least once. Silly question as somebody not very deep in the details on this. It's not easy to make distributed systems idempotent across the board (POST vs PUT, etc.) Distributed rollbacks are also hard once you reach interacting with 3rd party APIs, databases, cache, etc. What is the trick in your \"on message received handler\" from the queue to achieve \"exactly once\"? Some kind of \"message hash ID\" and then you check in Redis if it has already been processed either fully successfully, partially, or with failures? That has drawbacks/problems too, no? Is it an impossible problem? reply samtheprogram 17 hours agorootparentYou don’t achieve exactly once at the protocol/queue level, but at the service/consumer level. This is why SQS guarantees at-least-once. It’s generally what you described, but the term I’ve seen is “nonce” which is essentially an ID on the message that’s unique, and you can check against an in-memory data store or similar to see if you’ve already processed the message, and simply return / stop processing the message/job if so. reply mcqueenjordan 17 hours agorootparentAnd just to add a small clarification since I had to double take: this isn't exactly-once delivery (which isn't possible), this is exactly-once processing. But even exactly-once processing generally has issues, so it's better to assume at least once processing as the thing to design for and try to make everything within your processing ~idempotent. reply bastawhiz 12 hours agorootparentprev> What is the trick in your \"on message received handler\" from the queue to achieve \"exactly once\"? There's no trick. The problem isn't \"how to build a robust enough system\" it's \"how can you possibly know whether the message was successfully processed or not\". If you have one node that stores data for the queue, loss of that node means the data is gone. You don't know you don't have it. If you have multiple nodes running the queue and they stop talking to each other, how do you know whether one of the other nodes already allowed someone to process a particular message (e.g., in the face of a network partition), or didn't let someone else process the message (e.g., that node experienced hardware failure). At some point, you find yourself staring down the Byzantine generals problem. You can make systems pretty robust (and some folks have made extremely robust systems), but there's not really a completely watertight solution. And in most cases, the cost of watertightness simply isn't worth it compared to just making your system idempotent or accepting some reasonably small amount of data loss. reply bilekas 17 hours agoparentprevI'm curious about the same thing.. I use a deadletter queue in sqs and even sns and here I can see an archiver but it's not clear if I need to rollout my own deadletter behaviour here.. not sure I would be too confident in it either.. A nice novel project here but I'm a bit skeptical of its application for me at least. reply chuckhend 14 hours agorootparentpgmq.archive() gives us an API to retain messages on your queue, its an alternative to pgmq.delete(). For me as a long-time Redis user, message retention was always important and was always extra work to implement. DLQ isn't a built-in feature to PGMQ yet. We run PGMQ our SaaS at Tembo.io, and the way we implement the DLQ is by checking the message's read_ct value. When it exceeds some value, we send the message to another queue rather than processing it. Successfully processed messages end up getting pgmq.archive()'d. Soon, we will be integrating https://github.com/tembo-io/pg_tier into pgmq so that the archive table is put directly into cloud storage/S3. reply Justsignedup 18 hours agoparentprevTo be honest. I like the at least once constraint. It causes you to think of background jobs in a idempotent way and makes for better designs. So ultimately I never found the removal of it a mitzva. Also if you don't have the constraint and say it works and you need to change systems for any reason, now you gotta rewrite your workers. reply chuckhend 12 hours agorootparentI agree. But it can be useful to have a guarantee, even for a specified period of time, that the message will only be seen once. For example, if the processing of that message is very expensive, such as if that message results in API requests to a very expensive SaaS service. It may be idempotent to process that message more times than necessary, but doing so may be cost prohibitive if you are billed per request. I think this is a case where using the VT to help you only process that message one time could help out quite a bit. reply pipe_connector 10 hours agorootparentAgreed that this property is useful for efficiency. The danger comes from users believing this property is a guarantee and making correctness decisions based on it. There is no such thing as a distributed lock or exclusive hold. reply cryptonector 12 hours agoparentprevQuite. The problem with a visibility timeout is that there can be delays in handling a message, and if the original winner fails to do it in time then some other process/thread will, yes, but if processing has external side-effects that can't be undone then the at-most-once guarantee will definitely be violated. And if you run out of storage space and messages have to get dropped then you'll definitely violate the at-least-once guarantee. reply anentropic 1 hour agoprevOne of the appeals of doing MQ in Postgres is being able to submit events atomically in same db transaction as the stuff that raised the event Looking at https://github.com/tembo-io/pgmq/tree/main/tembo-pgmq-python ...how do I integrate the queue ops with my other db access code? Or is it better not to use the client lib in that scenario and use the SQL functions directly? https://github.com/tembo-io/pgmq?tab=readme-ov-file#send-two... reply kelnos 4 hours agoprevPerhaps we were all just not good at database'ing, but at a previous job, \"using RDBMS as a queue\" became a meme/shorthand for \"terrible idea that needs to be stamped out immediately\". Does Postgres have some features that make it not entirely unsuitable to use for queuing? reply ttymck 3 hours agoparentFor update skip locked reply pyuser583 18 hours agoprevWhat advantages does this have over RabbitMQ? My experience is Postgres queuing makes sense you must extract or persist in the same Postgres instance. Otherwise, there’s no advantage over standard MQ systems. Is there something I don’t know. reply vbezhenar 18 hours agoparentOne big advantage of using queue inside DB is that you can actually use queue operations in the same transaction as your data operations. It makes everything incredibly simpler when it comes to failure modes. IMO 90% of software which uses external queues is buggy when it comes to edge cases. reply zbentley 18 hours agorootparentA fair point. If you do need an external queue for any reason (legacy/already have one, advanced routing semantics, integrations for external stream processors, etc.) the \"Transactional Outbox\" pattern provides a way to have your cake and eat it too here--but only for produce operations. In this pattern, publishers write to an RDBMS table on publish, and then best-effort publish to the message broker after RDBMS transaction commit, deleting the row on publish success (optionally doing this in a failure-swallowing/background-threaded way). An external scheduled job polls the \"publishes\" table and republishes any rows that failed to make it to the message broker later on. When coupled with inbound message deduplication (a feature many message brokers now support to some degree) and/or consumer idempotency, this is a pretty robust way to reduce the reliability hit of an external message broker being in your transaction processing path. It's not a panacea, in that it doesn't help with transactional processing/consumption and imposes some extra DB load, but is fairly easy to adopt in an ad-hoc/don't-have-to-rewrite-the-whole-app way. https://microservices.io/patterns/data/transactional-outbox.... reply ilkhan4 14 hours agorootparentprevSure, not having to spin up a separate server is nice, but this aspect is underappreciated, imo. We eliminated a whole class of errors and edge cases at my day job just by switching the event enqueue to the same DB transaction as the things that triggered them. It does create a bottleneck at the DB, but as others have commented, you probably aren't going to need the scalability as much as you think you do. reply chuckhend 17 hours agoparentprevSimplicity is one of the reasons we started this project. IMO, far less maintenance overhead to running Postgres compared to RabbitMQ, especially if you are already running Postgres in your application stack. If PGMQ fits your requirements, then you do not need to introduce a new technically. There's definitely use cases where PGMQ wont compare to RabbitMQ, or Kafka, though. reply ethagnawl 17 hours agorootparent> There's definitely use cases where PGMQ wont compare to RabbitMQ, or Kafka, though. I'd be curious to know more about which sorts of use cases fall into this category. reply chuckhend 16 hours agorootparentPGMQ doesn't give you a way to deliver the same message to concurrent consumers the same way that you can with Kafka via consumer groups. To get this with PGMQ, you'd need to do something like creating multiple queues, then send messages to all the queues within a transaction. e.g. `begin; pgmq.send('queue_a'...); pgmq.send('queue_b'...); commit;` reply justinclift 9 hours agorootparentprevPGMQ doesn't seem to have functionality for returning a payload to the task submitter. For example, lets say the task is something like: Run a SELECT query on database FOO, returning the results There would be workarounds (ie store the results in a \"results\" table or similar) but just being able to directly return the result in a message to the caller is conceptually simpler. reply 0x457 16 hours agoparentprevThe advantage is: if you already have PostreSQL running, you don't have to add RabbitMQ or any other technology. reply borplk 17 hours agoparentprev(Not the author) The advantage of using your DB as a queue is that a traditional DB is easier to interact with (for example using SQL queries to view or edit the state of your queue). In most business applications the message payload is a \"job_id\" pointing to a DB table so you always need and have to go back to the database to do something useful anyway. With this setup it's one less thing to worry about and you can take full advantage of SQL and traditional database features. The only downside and bottleneck of having your DB act as a queue is if the workers processes are hitting the DB too frequently to reserve their next job. Most applications will not reach the level of scale for that to be a problem. However if it does become a problem there is an elegant solution where you can continously populate a real queue by querying the DB and putting items in it (\"feeder process\"). Now you can let the workers reserve their jobs from the real queue so the DB is not being hit as frequently. The workers will still interact with the DB as part of doing their work of course. However they will not ask their \"give me my next job ID\" question to the DB. They get it from the real queue which is more efficient for that kind of QPOP operation. This solution has the best of both worlds you get the best features of something like Postgres to be the storage backend for your jobs without the downside of hammering the DB to get the next available job (but in general the DB alone can scale quite well for 95% of the businesses out there). reply RedShift1 14 hours agoprevThis seems like a lot of fluff for basically SELECT ... FROM queue FOR UPDATE SKIP LOCKED? Why is is the extension needed when all it does is run some management type SQL? reply acaloiar 11 hours agoparentA similar argument can be made of many primitives and their corresponding higher order applications. Why build higher order concept Y when it's simply built on primitive X? Why build the C programming language when C compilers simply generate assembly or machine code? --- A sensible answer is that new abstractions make lower level primitives easier to manage. reply selcuka 10 hours agoparentprevSQS API compatibility? reply lloydatkinson 1 hour agoprevFor the longest time the common advice was that using a database as a message queue/broker was a bad idea. Now, everyone seems keen to use a DB for this instead of tools dedicated to this purpose. Why? reply sethammons 43 minutes agoparentThe circle of life. We used to track jobs in a db, but that would eventually hit performance issues (locks, noisy neighbors, etc). So more nosql solutions and services showed up. New devs gobbled up the concepts and APIs but were frustrated that distributed nosql solutions were SaaSified and wanted to bring control back in house, in a single db, and we are back to square one. But with better tooling and hardware in theory. Hopefully you have a dedicated db at least to avoid noisy neighbor workloads but you may still have task queue work that hits scaling limits on the single db causing tasks to interfere with one another--so you can shard the db and isolate task streams, and the circle continues reply deepsun 6 hours agoprev> TIMESTAMP WITH TIME ZONE I'm yet to find a use case for \"WITH TIME ZONE\", in all cases it's better to use \"WITHOUT TIME ZONE\". All it does is displays the date in sql client local timezone, which should never matter for well done service. Would be glad to learn otherwise. reply boromisp 3 hours agoparentTimestamp with time zone is the type for an \"absolute timestamp\". Timestamp without time zone is for local time, and sometimes abused as \"time in utc without being explicit about it\". The naming describes the expected input, not what is stored. The time zone name or offset is not stored with timestamptz. Always use timestamptz, unless you have a specific use case for local time. reply globular-toast 2 hours agorootparentYes, it is quite confusing and I dread to think how many have got it wrong and store local times like the GP. But it's also not as simple as \"always use WITH TIME ZONE\". That also leads to a mistake. The reason is (just to reiterate what you said) the TIMESTAMP WITH TIME ZONE does not store the time zone! If you ever want to get local time back (e.g. ask a question like \"how many users log on before lunch time\") then you need to store either local time in a TIMESTAMP WITHOUT TIME ZONE field, or the time zone, and get local time like: SELECT recorded_at AT TIME ZONE time_zone AS local_time ... (I prefer the latter). reply globular-toast 2 hours agoparentprevLook carefully. The SQL client does not just \"display it in local time\", it displays it with a UTC offset. You can be sure whenever you see a UTC offset that UTC is fully recoverable. In this way the TIMESTAMP WITH TIME ZONE field is context independent. It's just UTC. Conversely, if there is no offset, time zone, or something to distinguish it as UTC (like the Z in ISO8601) then you are just storing \"local time\", that is the time on the clock in someone's kitchen, somewhere. This is the TIMESTAMP WITHOUT TIME ZONE field and is highly context dependent (in particular, what clock was used?) reply ddorian43 3 hours agoparentprevActually, it's the complete opposite. You always want WITH TIMEZONE. reply conroy 19 hours agoprevI'm curious how this performs compared to River https://riverqueue.com/ https://news.ycombinator.com/item?id=38349716 reply chuckhend 17 hours agoparentI think it would be tough to compare. There are client libraries for several languages, but the project is mostly a SQL API to the queue operations like send, read, archive, delete using the same semantics as SQS/RSMQ. Any language that can connect to Postgres can use PGMQ, whereas it seems River is Go only? reply justinclift 9 hours agoprevAs a data point, there's a similar Go based project called Neoq: https://github.com/acaloiaro/neoq reply thangngoc89 19 hours agoprevI’m wondering if there are language agnostic queues where the queue consumers and publishers could be written in different languages? reply rco8786 18 hours agoparentThat's exactly what this is. You write your own consumer/publisher code however you want, and interact with the queues via SQL queries. reply SideburnsOfDoom 18 hours agoparentprevThat's normal, yes. Name a queuing system, and with very few exceptions it will have clients for a variety of languages. It is also normal to exchange messages with contant as json, protobuf or similar format, which again can be processed by any language that aims to be widely used. In fact, are there any queues that aren't language-agnostic? I had the idea that ZeroMQ was a C/C++ only thing, but I checked the docs and it's got the usual plethora of language bindings https://zeromq.org/get-started/ So right now I can't name any queue systems that are single language. They're all aimed at interop. reply justinclift 9 hours agorootparent> Name a queuing system, and with very few exceptions it will have clients for a variety of languages. RQ seems to be one of those exceptions, as even after many years it looks to be Python only. :( https://python-rq.org reply thangngoc89 6 hours agorootparentRQ uses Python's Pickle and it doesn't have any serialization protocol so it stays Python only. reply SideburnsOfDoom 2 hours agorootparentMost queuing systems that I have used, you can send text or bytes, so the most common denominator is DTOs encoded as json, encoded as UTF-8 text. You could do likewise, (i.e. send text containing a representation of what you want) and this would allow other languages to consume? But that's an edge case (sending text) of an edge case (RQ). reply justinclift 5 hours agorootparentprevYeah, it's a right pain. :( reply thangngoc89 17 hours agorootparentprevInteresting. I've been looking at a much more simpler system than Celery queue to publish jobs from Golang and consume jobs from Python side (AI/ML stuff). This threads gave a lot of names and links for further investigation. reply anamexis 16 hours agorootparentAs mentioned, there are a plethora of queuing systems that have cross platform clients. If you’re interested specifically in a background job system, you may want to check out Faktory. It’s Mike Perham’s language-agnostic follow-up to Sidekiq. https://github.com/contribsys/faktory reply thangngoc89 5 hours agorootparentThanks for the recommendation. I've checked this out and looks like a good alternative with much simpler configuration and less foot-gun than Celery. reply dostoevsky013 16 hours agoprevI’m not sure what are the benefits for the micro service architecture. Do you expect other services/domains to connect to your database to listen for events? How does it scale if you have several micro services that need to publish events? Or do you expect to a dedicated database to be maintained for this queue? Worth comparing it with other queue systems that persist messages and can help you to scale message processing like kafka with topic partitions. Found this article on how Revolut uses Postgres for events processing: https://medium.com/revolut/recording-more-events-but-where-w... reply solatic 3 hours agoparentWho said anything about microservice architecture? If you're building a new MVP that needs background processing, you have a queue, and your monolith listens to the queue. Sticking to one database during the MVP keeps things simple until you validate both the product and expected scale. You can migrate to something heavier later if the product gets traction. reply chuckhend 16 hours agoparentprevWe talk a little bit in https://tembo.io/blog/managed-postgres-rust about how we use PGMQ to run our SaaS at Tembo.io. We could have ran a Redis instance and used RSMQ, but it simplified our architecture to stick with Postgres rather than bringing in Redis. As for scaling - normal Postgres scaling rules apply. max_connections will determine how many concurrent applications can connect. The queue workload (many insert, read, update, delete) is very OLTP-like IMO, and Postgres handles that very well. We wrote some about dealing with bloat in this blog: https://tembo.io/blog/optimizing-postgres-auto-vacuum reply bdcravens 16 hours agoprevNote there are a number of background job processors for specific languages/frameworks that use Postgresql as the broker. For example GoodJob and the upcoming SolidQueue in Ruby and Rails. reply rco8786 18 hours agoprevThis is neat. Would be cool if there was support for a dead letter or retry queue. The idea of deleting an event transactionally with the result of processing said event is pretty nice. reply jpambrun 17 hours agoparentRetries are baked in as message re-becomes visible after a configurable time. For dead letter you can move to another queue on the nth retry. reply chuckhend 15 hours agorootparentThis is exactly how we do this in our SaaS at Tembo.io. We check read_ct, and move the message if >= N. I think it would be awesome if this were a built-in feature though. reply airocker 16 hours agoprevI think it will be better if you create events automatically based on commit events in wal. reply ComputerGuru 12 hours agoprevThis is strictly polling, no push or long poll support? reply chuckhend 10 hours agoparentThere is a long poll, https://tembo-io.github.io/pgmq/api/sql/functions/#read_with... We have been talking about a push using Postgres 'notify', or even via an http, but we don't have a solid design for it yet. reply jilles 10 hours agoprevCan someone tell me what the usefulness of this is compared to RabbitMQ or Kafka? reply cynicalsecurity 15 hours agoprevBut why? Why not have a proper SQS service? What's the obsession with Postgres? reply solatic 3 hours agoparentWhen you're building an MVP, you want to keep the architecture as utterly simple as possible. SQS helps at scale, but your product may never reach scale. In the meantime, you vendor-locked into AWS, with all its attendant needs like setting up auth to AWS, account vending, etc. If your MVP already needs stuff like object storage, and you're setting up AWS anyway, then sure, prefer SQS. But a lot of MVPs are better set up as monolith + Postgres to start with and only complicating the architecture after product traction is found. reply chuckhend 13 hours agoparentprevIMO, it is most valuable when you are looking for ways of reducing complexity. For a lot of projects, if you're already running Postgres then it is maybe not worth the added complexity of bringing in another technology. reply redact207 7 hours agoparentprevI agree in general, but there will always be certain requirements and team structures where stuff like this makes sense. For me, I work in a small team of 6 devs on an ever growing app and feature set. I 100% will leverage managed services where cost and complexity allow. SQS is one of the most stable and cheapest AWS service, and the ability to just use it and not have to sysops it means we can spend more time building features. reply mlhpdx 6 hours agorootparentIndeed. I’ve relied heavily on SQS for years and never regretted it. I question the comparison to SQS for this add on — it’s not really in the same ballpark. reply jpambrun 12 hours agoparentprevWhy use a service that comes with lock-in and poor developer experience when I can use the database I already have? reply cryptonector 12 hours agoparentprevSee https://news.ycombinator.com/item?id=40307454#40311843 reply poisonborz 15 hours agoparentprevno dependence on a third party reply ltbarcly3 17 hours agoprevAnother one of these! It's interesting how many times this has been made and abandoned and made again. https://wiki.postgresql.org/wiki/PGQ_Tutorial https://github.com/florentx/pgqueue https://github.com/cirello-io/pgqueue Hundreds of them! We have a home grown one called PGQ at work here also. It's a good idea and easy to implement, but still valuable to have implemented already. Cool project. reply arecurrence 17 hours agoparentYeah, I've written a few of these and should probably release a package at some point but each version has been somewhat domain specific. The last time we measured an immediate 99% performance improvement over SNS+SQS. It was so dramatic that we were able to reduce job resources simply due to the queue implementation change. There's a lot of useful and almost trivial features you can throw in as well. SQS hasn't changed much in a long time. reply mattbillenstein 17 hours agoparentprevHa, I wrote one too - https://github.com/mattbillenstein/pg-queue/blob/main/pg-que... Roughly follows the semantics of beanstalkd which I used once upon a time and quite liked. reply andrewstuart 14 hours agoprevSeems an unusual choice that this does not have an HTTP interface. HTTP is really the perfect client agnostic super simple way to interface with a message queue. reply solatic 3 hours agoparentOn the contrary, creating new HTTP connections introduces an irreducible source of latency compared to establishing and reusing a persistent connection. You may end up building a single-tenant architecture where each tenant gets its own database and there are relatively few consumers that are able to respond quicker due to sticking with a long-lived connection model. reply seveibar 18 hours agoprev [–] People considering this project should also probably consider Graphile Worker[1] I've scaled Graphile Worker to 10m daily jobs just fine The behavior of this library is a bit different and in some ways a bit lower level. If you are using something like this, expect to get very intimate with it as you scale- a lot of times your custom workload would really benefit from a custom index and it's handy to understand how the underlying system works. [1] https://worker.graphile.org/ reply brycelarkin 4 hours agoparentDo you add jobs inside a transaction? Or with a root connection? There doesn't seem to be an out of the box way to add jobs in a transaction. reply philefstat 18 hours agoparentprevhave also used/introduced this to several places I've worked and it's been great each time. My only qualm is it's not particularly easy to modify the exponential back off timing without hacky solutions. Have you ever found a good way to do that? reply valenterry 14 hours agoparentprev [–] Is there something like that in the jvm world? reply RedShift1 14 hours agorootparentYou don't need anything specific. SELECT ... FROM queue FOR UPDATE SKIP LOCKED is the secret sauce. reply valenterry 14 hours agorootparentWould be nice to get some goodies for free, like overview, pausing, state, statistics etc. :-) reply chuckhend 10 hours agorootparentprevThis is exactly how pgmq is implemented, + the usage of VT. reply wmfiv 14 hours agorootparentprev [–] https://www.jobrunr.io//en/ seems to be popular at the moment. reply valenterry 2 hours agorootparent [–] That looks pretty good! Too bad that the OSS version limits the number of jobs quite a bit. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Postgres Message Queue (PGMQ) is a lightweight message queue akin to AWS SQS and RSMQ, leveraging Postgres as its foundation, supporting features such as \"exactly once\" message delivery and compatibility with Postgres versions 12-16.",
      "The documentation offers insights into installation, client libraries, SQL usage, and configuration settings for partitioned queues within PGMQ.",
      "PGMQ ensures message visibility using a visibility timeout and provides choices for message deletion or archiving."
    ],
    "commentSummary": [
      "The Github discussion debates using Postgres instead of SQS for achieving \"exactly once\" message delivery within a visibility timeout, discussing system failures affecting this guarantee and the challenges of distributed consensus.",
      "Participants talk about managing message delivery failures, the difference between delivery and processing assurance, and the effectiveness of Postgres as a message queue, mentioning unique identifiers and considering alternatives like RabbitMQ and Graphile Worker for job management.",
      "The focus is on streamlining architecture, minimizing complexity, and ensuring reliable message delivery in distributed systems."
    ],
    "points": 222,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1715257313
  },
  {
    "id": 40309342,
    "title": "Muddy: Collaborative Work Browser with AI Organization",
    "originLink": "https://news.ycombinator.com/item?id=40309342",
    "originBody": "Hey HN! This is Jimmy, Ron and Austa from Muddy (https:&#x2F;&#x2F;feelmuddy.com&#x2F;). Muddy is a browser for work that automatically keeps project files organized in the same place where you use and share them. Here’s a demo: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tZr49aN3sjQ. Download and try it out here: https:&#x2F;&#x2F;feelmuddy.com&#x2F;.Building together in the past, we were incredibly frustrated with how much friction there is to get anything done on our computers. I was losing time everyday digging through chat logs looking for that one important link or breaking others out of flow by asking where something is.Web apps promised to help us get more done—and they do, but each in its own silo, so there’s still a ton of redundancy to deal with. Every app has its own way of organizing files, its own notification inbox, its own search system. Conversations live everywhere and there isn’t a single view to see everything about a project. Remember when files simply lived in folders rather than the “cloud”?We started dedicating time to organizing our files in shared docs and limiting new apps we used. This helped – but the second we didn’t stay on top of organization, links became stale and things got messy again.Muddy started as a hack week project we built for ourselves—a single place to use web apps with others, but personalized for each user automatically. Everyone gets their own view for every project, designed around how they work.Muddy users work on projects in spaces, which are like automatic tab groups. Users share apps (any site works—a Github PR, Figma file, Trello board—whatever you want) into the project’s shared timeline and Muddy automatically opens relevant tabs for you. It’s a single click to open up all the apps you need for the project.Under the hood, Muddy works in the background to keep track of the timeline and uses a LLM to continuously organize apps and keep everything on to date. It considers signals like the popularity of a file, naming conventions, and conversations to figure out what’s relevant. So everyone is presented with an updated list of important tabs, without anyone lifting a finger. Our actual browser is based on Chromium.When you need to revisit something from weeks ago, you can rewind the project timeline to that point in a single click. Apps open up in the timeline so you’ll see your files right away. For sites that don’t have built in collaboration features (like documentation), Muddy lets you do annotations directly on the website.Projects sometimes get big and need to be broken up. Across all your spaces, Muddy can answer questions like ChatGPT, cite your files as sources, and return apps directly. This is possible since Muddy’s AI shares your browser and can use your authenticated apps locally (with privacy in mind).Other browsers like Chrome and Arc focus on solo productivity with sharing as a bolt-on. We think productivity depends on how well you can work with others, and should be the first class consideration. And doing organizational work manually is unsustainable.Muddy will have paid subscriptions for teams with additional features like shared passwords, team organization, custom shortcuts, and SSO management. Those aren’t built out yet and the base product will be free. No part of our revenue will come from data monetization.We’d love for you to give Muddy a spin! You can download Muddy for Mac or Windows on our website and add others once inside: https:&#x2F;&#x2F;feelmuddy.com&#x2F;. We’ll be around to answer questions and look forward to any and all feedback!",
    "commentLink": "https://news.ycombinator.com/item?id=40309342",
    "commentBody": "Muddy (YC S19) – Multiplayer browser for getting work done215 points by lele0108 18 hours agohidepastfavorite93 comments Hey HN! This is Jimmy, Ron and Austa from Muddy (https://feelmuddy.com/). Muddy is a browser for work that automatically keeps project files organized in the same place where you use and share them. Here’s a demo: https://www.youtube.com/watch?v=tZr49aN3sjQ. Download and try it out here: https://feelmuddy.com/. Building together in the past, we were incredibly frustrated with how much friction there is to get anything done on our computers. I was losing time everyday digging through chat logs looking for that one important link or breaking others out of flow by asking where something is. Web apps promised to help us get more done—and they do, but each in its own silo, so there’s still a ton of redundancy to deal with. Every app has its own way of organizing files, its own notification inbox, its own search system. Conversations live everywhere and there isn’t a single view to see everything about a project. Remember when files simply lived in folders rather than the “cloud”? We started dedicating time to organizing our files in shared docs and limiting new apps we used. This helped – but the second we didn’t stay on top of organization, links became stale and things got messy again. Muddy started as a hack week project we built for ourselves—a single place to use web apps with others, but personalized for each user automatically. Everyone gets their own view for every project, designed around how they work. Muddy users work on projects in spaces, which are like automatic tab groups. Users share apps (any site works—a Github PR, Figma file, Trello board—whatever you want) into the project’s shared timeline and Muddy automatically opens relevant tabs for you. It’s a single click to open up all the apps you need for the project. Under the hood, Muddy works in the background to keep track of the timeline and uses a LLM to continuously organize apps and keep everything on to date. It considers signals like the popularity of a file, naming conventions, and conversations to figure out what’s relevant. So everyone is presented with an updated list of important tabs, without anyone lifting a finger. Our actual browser is based on Chromium. When you need to revisit something from weeks ago, you can rewind the project timeline to that point in a single click. Apps open up in the timeline so you’ll see your files right away. For sites that don’t have built in collaboration features (like documentation), Muddy lets you do annotations directly on the website. Projects sometimes get big and need to be broken up. Across all your spaces, Muddy can answer questions like ChatGPT, cite your files as sources, and return apps directly. This is possible since Muddy’s AI shares your browser and can use your authenticated apps locally (with privacy in mind). Other browsers like Chrome and Arc focus on solo productivity with sharing as a bolt-on. We think productivity depends on how well you can work with others, and should be the first class consideration. And doing organizational work manually is unsustainable. Muddy will have paid subscriptions for teams with additional features like shared passwords, team organization, custom shortcuts, and SSO management. Those aren’t built out yet and the base product will be free. No part of our revenue will come from data monetization. We’d love for you to give Muddy a spin! You can download Muddy for Mac or Windows on our website and add others once inside: https://feelmuddy.com/. We’ll be around to answer questions and look forward to any and all feedback! TIPSIO 16 hours agoThe feature for sending messaging and posting comments to a tab is some pretty clever and creative UX. Seriously next level future stuff and congrats for just coming up with the concept. I like that it's all timeline based. For my use case, we currently use Front email thread and then link to a Shared Dropbox where we post everything (including links to like a Google Doc or webpage). I think having chronological bookmarks like you do would be clearly better. I also know many people who use Google Groups and Google Doc to document progress too -- which I think would be insane / nightmare but teams do it. You all definitely would solve that automatically. Couple other notes: - Whenever I screenshare with a team or others I see 1000 bookmarks or tabs on their browser. I could not imagine the nightmare of how that would impact my workflow or the timeline. Trusting AI to clean stuff up or hunt is not for me. - I can tell you all have been heads down blitzing (dog in video, phone ringing in background of another) but I think a separate \"Solutions\" page where you tackle specific examples would be nice to see or browse. - Maybe too much or not really your goal, but right now need some sort of client integration for an outside person. I can't imagine giving access to a client on a whim and training them on this. Instead, maybe automatic email integration where their emails show up in the timeline and can respond directly from there. Would produce a really great timeline for where things left off and when things are being communicated. Being able to sub-comment and share files/updates/things on Front on email threads is one of the most killer features for productivity and a team. Mixing this with what you all have could be even more next level. Again though, might not be the goal. Congrats and best of luck! Big fan of people trying to tackle PM stuff and think you all are doing a great job. reply lele0108 16 hours agoparentGreat idea for creating some specific videos for engineers, designers, etc. Browsers are interesting since they can do almost anything but \"you can do anything you want!\" is intimidating for many new users. We've given e-mail thought and it's certainly a door we are considering as we keep on building. Has anyone built a browser without thinking about an email client? :P reply webappguy 15 hours agoparentprevIntegrating an AI tab organizer of which they’re very few and the chrome one is essentially garbage, would make this a definite purchase for me. The AI should organize my own tabs and the ones I share reply aranibatta 14 hours agorootparentyup, that's Muddy. One of our users described it as a self healing slack channel with tab groups. reply ggsp 19 minutes agoprevReally cool demo! Curious to try it. The homepage says \"(y)our business model is around selling collaboration services and enterprise features, not ads\"—can you expand on this a bit, specifically what you mean by \"collaboration services\" and how you intend to monetize Muddy? reply idoh 17 hours agoprevWhat's the story around getting into YC in 2019 and then launching in '24? I'm guessing there were some interesting pivots along the way ... reply lele0108 16 hours agoparentYou are right. We came in letting hosts open experience stores in Airbnbs (try on an Oculus at a hosts home). Difficult to attribute our sales so we moved on. COVID hit and we had the itch to look at the browser in a different light. Building a browser seemed intimidating...but it was quarantine. Long story short, built a few different ideas and here we are :) reply orliesaurus 10 hours agorootparentThis is incredible, 5 years of not giving up, kudos. How big is your team and how did you make your money go this long? reply aranibatta 9 hours agorootparentTeam is a handful, fluctuated over the years but never more than 8. We’ve always treated everything we do as an experiment. We’re not trying to keep anything alive for the sake of it. reply Harmohit 15 hours agorootparentprevBuilding a browser does seem intimidating! What technical considerations made you finally take it seriously? Any advice for folks trying to do the same or taking on another intimidating project? reply lele0108 14 hours agorootparentStarted just by building Chromium and Brave and poking around in there and making small changes. Chromium is huge but has terrific documentation (once you find the current copy) and Google hosted code search. Digging through crbug.com often helped point us in the right direction. 2 unlocks that made us confident to pursue this project: 1) Repeatable way of patching Chromium and keeping up with upstream changes 2) Writing UI in web technology instead of C++ Views toolkit. reply bjord 31 minutes agorootparentany interest in pointing someone curious about those repeatable patches in the right direction? reply aranibatta 16 hours agoparentprevYou can actually check out all the material from the pivots inside the app. There's a walkthrough in the Getting Started space when you first set up. reply kisonecat 15 hours agoprevTotally evokes the good memories I have of \"Google Wave\" as a way for folks to collaborate on rich documents. Super cool. reply aranibatta 14 hours agoparentGoogle Wave for the web has been one of the more popular ways to describe Muddy. I didn't see it at first, but I'll take it :) reply gaudystead 12 hours agoparentprevGoogle Wave was the first thing that came to mind for me as well. RIP That being said, this looks like a nice spiritual successor to it! reply oaktowner 12 hours agoparentprevI came here to say this -- couldn't agree more. Very positive feelings and this nails it (and adds some stuff, too). reply sachinkesiraju 10 hours agoprevThis is super impressive. Love the demo, especially the AI recall feature that deeplinks your files and @'s collaborators. This product feels like the obvious evolution of the digital workplace after the cloud. Is the plan to eventually integrate more native collaboration tools (Kanban boards, team password sharing, cloud file storage) after chat? reply aranibatta 9 hours agoparentI think we might have more base interfaces (notes, sheets) but we want to encourage people to keep building first class software. All in one is worst of all reply seism 12 hours agoprevLove to see a hack week project get this far. The timeline and signals features look awesome, and I can't wait to give this a try. Hope you also have thoughts (and a business model) for more private, ephemeral spaces - enabling short term collaboration, e.g. for freelance/remote/hack work. reply aranibatta 6 hours agoparentCurious what you had in mind? reply zuhayeer 16 hours agoprevCongrats on the launch! Awesome to see this release as an early user who was able to check it out. The shared workspaces and shared browser windows with context in place has been incredible for collaborating with folks. We have our Figma design, Notion doc, and Gitlab MR all in the same space so we don't have to go searching for each one independently or have them cross-linked to each other. reply ekhar01 13 hours agoprevThis is so cool. Much respect. I am curious what tech stack you are writing this with? Is it electron or maybe lower level? It might also be cool if it could automatically import sheets or google docs if I were to drag a csv or .txt file in and just magically open the project up. I hate having to upload via google drive reply lele0108 13 hours agoparentElectron isn't meant for building a browser and has perf / other limitations. We went with a full Chromium backing with some patches to allow us to write our UI in web tech. Looking to explore such automations in the near future -- important to us that it feels great to create new files in Muddy (and have Muddy do any of the busywork we are used to) reply joloooo 13 hours agoprevThis is awesome. What are your thoughts on Office365 tools and Muddy? We don't use Google docs and would love to have a similar experience. That said, I could probably manage certain teams to move away from our MSFT reliance with this flow. Really excited to try this out and follow along. reply aranibatta 6 hours agoparentI think if EVERY tool you use at work is in the Office suite, teams is just better. If you have any other tools like one off SaaS or devTools, Muddy could be a fit. reply v3ss0n 4 hours agoprevBrowsers especially work browsers are very sensitive piece of software which needs to fully respect data privacy and security so it is opensource or burst. If it's not opensource user might just as well, consider it a spyware. Opensource or Burst. reply antidnan 17 hours agoprevThis is really cool! Congrats on the launch! I think my usage of figma,sheets,etc. is 90% single player, until the moment of sharing my (maybe unfinished) work, where I go through an intense period of collaboration with others for an initial review, then tails off, and becomes async. I can't see myself using muddy for the single player part, but it sounds interesting for after that initial intense collab process. Especially if the process includes multiple apps, as opposed to a single design review in figma etc. I find the longer running async collab is when I get the most scatterbrained across apps. reply lele0108 16 hours agoparentI felt this pain a lot. Never really resonated with all-in-one apps so we always had a few places to check for every project. Figuring out the right links was hard and prone to distraction. In a way, we like trying new software. Downside is each app has the need to build their own slightly different file system, which makes finding things extra challenging. Wanted to solve with Muddy. reply t1c 17 hours agoprevSo, despite being based on Chromium, there's no Linux builds? Would like to try this but me and my entire team uses Linux. reply lele0108 17 hours agoparentHear you! Coming soon. There are some OS specific patches we made to Chromium that make this less straightforward reply bigstrat2003 11 hours agorootparentThat is really unfortunate, because it removes the only advantage of using the web to begin with. Hopefully you guys are able to find a solution. reply aranibatta 16 hours agoparentprevWe have Ubuntu and Fedora users through Wine. Not ideal I know, but enough to try out unless you have a super custom Arch setup reply t1c 7 hours agorootparentI don't see what having an arch setup would have to do with running it through wine, but running your main browser under a heavy compatibility layer is practically unusable in my book. I need my browser to be able to be invoked in no more than a couple hundred milliseconds, while wine programs can sometimes take 5 to 10 seconds to launch. reply aranibatta 6 hours agorootparentYes, the lag is the main problem. Certain distros and setups don’t work with wine. Mine doesn’t, but I’m sure it’s a tractable problem. reply nikunjk 16 hours agoprevCongrats on the launch - seems like a neat product. A few questions: 1) It seems like you iterated on a bunch of ideas that landed in this - what were some interesting features that the team was really interested in that you ended up killing? 2) What were the most non-intuitive hard technical challenges? 3) Have you successfully ended up having someone give up on Slack to use this? reply lele0108 16 hours agoparent1. Spatial canvas based browser (we called it Sail). Had people who loved it but their team refused to learn it. Spatial tools are just incredibly niche and difficult to get started (besides Google Maps). 2. Auto updating across Mac and Windows (in 2024!!). Google Omaha is hard to setup, Sparkle is jank on Windows. Throw in binary delta support and it's an oof. 3. Our team builds Muddy on Muddy and ditched Slack when product got stable. Few other beta users and their companies as well. Slack is super sticky and has some terrific workflows, but more \"quality\" conversations today happen natively in apps and almost all apps have commenting functionality. Just easier to talk next to the context. So a lot of Slack convo's become \"where is X\" and we think Muddy will stop the need for those questions all-together. reply mike_hearn 13 hours agorootparentYeah auto update on Windows is a mess. One way to get it is to use MSIX, in which case Windows will delta update your app for you, even when it's not running (Chrome style) but without the need for special servers. Unfortunately old Windows versions have a lot of bugs. My company sells a product that makes all that easy to deal with and works around the bugs but it was a hard slog to get it all working well. Sparkle on Mac is great though. reply aranibatta 15 hours agoparentprevcheck the timestamp: https://x.com/SlackHQ/status/367835012315357184 reply alexkern 15 hours agoprevCongrats on the launch! One of the positive byproducts of the open web platform is that products like this are possible. Much of what I share in other communication tools are links to things on the web, but sharing ends up forcing a context switch cost on both my end and the receiving end. Love that Muddy is exploring this problem space since I haven’t felt like other neue-browsers have gone far enough in making browsing itself a more collaborative and in-context experience. reply toddmorey 16 hours agoprevQuestion: What were the technical requirements that necessitated a custom browser? That would really sink adoption for my team. Any plans to have a lite version that can work inside your favorite browser (even if plugin is required)? Idea: your privacy section only talks about cookies and ads, but all my privacy questions were around the AI features that would use all our team's messages and work across apps as context. Would definitely cover that piece. reply aranibatta 16 hours agoparentMost of the stuff we do that people seem to really like can't be technically or physically done inside of another browser's tab unfortunately. Chrome extensions are too limiting as well. We started off as a chrome extension inside of our previous company, and hit a wall pretty quickly. For the LLM calls, the one's you currently see are patching calls to a variety of model providers. As long as they hold their end of the TOS you should be fine. Everything else is happening locally. We will launch a self-host option for Muddy. Cool features like hosting the servers and build process yourself, custom icon and skin on the app, and other internal rules you can set up. Let me know if that's of interest. reply v3ss0n 1 hour agorootparentStill not enough. You are using Chromium but your software might not be secure enough. You are dealing with sharing data all over the internet and part of your code is not opensource. If we cannot inspect or reproduce what you are doing , we cannot trust our work with your browser. We might as well consider you are stealing our data. If you launch self-host option , you should opensource it. What we have for opensource alternatives are : https://github.com/BrowserBox/BrowserBox https://github.com/m1k1o/neko reply pkiv 16 hours agoprevCongrats on the launch!! Collaborative browsing is something I've been looking for a few use cases of mine. Excited to try it out. reply yohannparis 16 hours agoprevAn integration with current chat system (Slack, Teams, etc.) would be neat. Changing where the work happens is a big ask in my opinion. reply TIPSIO 16 hours agoparentI can't really imagine how integration with them would work. I don't want to speak for the Muddy people, but I wager they probably think chat app is kind of miserable experience. This instead tries to have a superior offering of something cleaner, easier to visualize where things are, and then move that \"slack/teams\" type convo to specific comments and tasks. reply aranibatta 14 hours agorootparentYeah, unfortunately there's no real way to deliver on a consumer experience on top of another browser. Craziest implementation I can think of is writing a MacOS app that's a wrapper, but obvious distribution/compatibility issues there. reply csmeyer 13 hours agoprevCurious to give this a try, how would you compare it to Arc? I used Arc for a while but eventually just ditched it for going back to chrome (Also, landing page nit, but the kerning on the H1 is pretty wide, and the kerning in the wordmark is a bit tight IMO.) reply csmeyer 13 hours agoparentOK, I tried it out. First thing I did was type a website into the big bar at the top and got an upsell modal for an AI thing... I get the need to make money but this was not a good first experience for me reply lele0108 13 hours agorootparentSorry! This is a bad ranking of our search bar and \"Ask Muddy\" should not always be the first suggestion. Fixing this in our next update. If you hit any of the other results, it will open the site for you Compared to Arc -- Muddy is focused on making getting things done together better rather than infinite customization. It's a different set of tradeoffs but with a similar rethinking of the browsing experience. reply csmeyer 13 hours agorootparentYeah definitely worth fixing... This seems overall rather thoughtfully designed but that bug hurt reply sahaskatta 17 hours agoprevCongrats on the launch. I recently was trying out Microsoft Edge's built-in Workspaces feature which allows multiplayer collaboration. What would you say are the key differences? https://www.youtube.com/watch?v=w5AX_HvtYfI reply lele0108 17 hours agoparentThe main difference is that Muddy automatically creates and updates those shared tabs for you based on what's being added to the project timeline (multiplayer feed of apps/websites). We used some similar tools like Workona in the past but without constant maintenance, links would get stale and we'd abandon it. Wanted something that did that for us automatically. (Also we support: website annotations, team presence, and letting you rewind a project's timeline back to any point in time) reply kingzulu 14 hours agoprevThe unsubscribe link in your welcome email is not a link, it is just text. reply dvaun 15 hours agoprevThis is super neat. I would love to use this in a team setting if I could convince the org to pay for it. I saw in another comment that it’s a patched version of Chromium. Are you using CDP for underlying communication? reply jerrygenser 17 hours agoprevI spent a little time googling around but even wayback machine is only showing results in 2024 for the domain. Was there a different project that was being worked on previously? Or is this the only product being launched since YC S19? reply aranibatta 14 hours agoparentPrevious projects of ours include: sail.online nototo.app This is the first one with real traction and retention, so it's the first time we started to share on HN. Wanted to make sure it was GA and not a waitlist before we came here reply jbaczuk 17 hours agoprevI'm not sure this would be useful to me, because I don't switch between various projects that would benefit from different tab groups, but sounds like it could be useful! Good luck. reply NayamAmarshe 16 hours agoprevLooks very interesting. The chat feature almost looks like a good slack alternative. I can imagine this being useful to organization where your chats and your work are in the same window. reply aranibatta 15 hours agoparentThat's the general idea. We found that if you break down the average team's project slack channels, it's a bunch of urls and threads about them. Just wanted to make that experience simple! reply verdverm 15 hours agoparentprevYou can use Slack & Discord from browser tabs. That's how I roll One Chrome window for - work (slack, jenkins, bitbucket) - personal (discord, gmail, twitter, HN) - open source (github, docs, projects) Each one is a mental box and are kept separate reply aboodman 13 hours agoprevPlease stop doing this: https://i.imgur.com/2WeVGxK.png It's so frustrating. Early in your product lifecycle it should be painfully easy to get started and you shouldn't be worrying about this kind of security. reply lele0108 12 hours agoparentGood point, we'll fix this in the next update. p.s. fan of the work on replicache and of course chrome reply rrr_oh_man 10 hours agorootparentrelevant: https://xkcd.com/936/ reply promiseofbeans 10 hours agoparentprevI don't see any issue with this? It makes sense to keep the accounts secure, and very few people will be coming up with passwords by hand; they'll be using the auto-generated ones from their browser (or from their 3rd-party password manager for the more HN-y types). reply senorrib 9 hours agorootparentVery few?! I really doubt that. Most people use their pet password everywhere. reply lIl-IIIl 9 hours agoparentprevPlease keep doing this. reply aboodman 13 hours agoparentprevI almost gave up after trying this like three times but as an ex browser developer really wanted to see what ya'll were up to. I think it's an interesting concept. But I bet you will lose tons of trials here. In fact it should just be google auth (not sure if I missed the option to do that). reply bigstrat2003 12 hours agorootparentGoogle auth is good to have, but local auth should always be an option. It's not really good practice to exclude those who don't wish to cede control of their online life to Google. reply bn-l 12 hours agorootparentAnd if your Google account is ever blocked because you triggered an AI security flag, you will lose access to all the sites where you signed in with Google. reply theendisney 12 hours agorootparentI find the need to self-moderate at least as scary. Heaven forbid you say what you think with just morality in mind. Who even knows what is or isn't allowed in 2024? reply HeatrayEnjoyer 11 hours agorootparentThat never seemed hard to figure out. reply justusthane 10 hours agorootparentprevI have never in my life thought “Gee, I might be banned for saying that,” but maybe I’m just a sheeple. reply theendisney 6 hours agorootparentSurely you can open a mainstream news website and find obvious nonsens or disagreable material. You dont have want to talk about it but it is the idea you are not suppose to that can already change you. reply doublerabbit 9 hours agorootparentprevMy reddit comment in /r/uk resulted in my 12 year account banned because it didn't comply with a UK mods viewpoint. The comment may of been slightly off-the-cuff but for sure didn't incite \"violence\". This resulted in getting banned by Reddit admins and I've never been back on Reddit since. After the shock wears off you realise that you really you don't miss reddit. reply causal 12 hours agoprevCool idea! Curious how it handles auth. Like how do you collaborate on a shared URL without shared credentials reply whimsicalism 17 hours agoprevI can see why managers would love this, social pressure around doing work and working faster. reply aranibatta 16 hours agoparentI can't say we haven't heard that. Surprisingly, it's usually more of a TL or most Senior Eng who seems to like those presence features. A lot less cat herding across apps for them. We have less traction with PMs. Most of the feedback from them implies they like being bombarded with Slack notifications even if they say they don't. reply psuedo_uuh 14 hours agoprevI already have too many tabs open, can you imagine how many would be open on a shared browser? reply pbhjpbhj 12 hours agoparentEdge has shared browser \"workspaces\", I use them solo to have different browsers for different roles. Like a workspace for project A, another for intranet, another for project B, ... the main thing about it is all your tabs are where you leave them and it's a bit more visually manageable than TST which I use at home. Basically means you can have lots of tabs open, but now in different windows (with different colours!). That feature is supposed to be about 'multiplayer' and has been so good it actually makes me want to see Muddy just because of how positive that 'multiplayer' has been. Weird, huh. Aside, I'm still mourning Opera Unite. reply mschrage 15 hours agoprevLove this and am tremendously impressed by this team's persistence. Congrats on the launch! reply rrr_oh_man 13 hours agoprevThe demo video is excellent! reply bozhark 13 hours agoprevThis should say “data” not just ads “All your cookies and passwords are stored locally and never on our server. Our business model is around selling collaboration services and enterprise features, not ads” reply aranibatta 11 hours agoparentYou're correct. Don't really need to do anything with your data as well. I will change. reply decide1000 17 hours agoprevIs there a Linux version? reply aranibatta 16 hours agoparentNot yet, in the pipeline. Depending on your distro (Ubuntu and Fedora) have users through Wine. reply kingzulu 14 hours agoprevThe unsubscribe link in your welcome email is broken. It isn't a link. reply aranibatta 14 hours agoparentTy for the catch, correcting now. Seems to be for the invite and update emails? reply keshav55 17 hours agoprevThis is amazing reply otteromkram 7 hours agoprevRelevant: https://imgs.xkcd.com/comics/standards.png reply aranibatta 6 hours agoparentThis post is a right of passage reply sonicanatidae 16 hours agoprev [–] Is there a listing of what data is sent back to your servers and how that data is stored/handled? I work in a secure environment. I like the idea of this app, but leakage is a huge factor for my teams. That aside, I've already downloaded a copy and plan to try it out in a non-secure environment. The concepts here look like a great idea. GL and thanks! reply aranibatta 14 hours agoparent [–] Nothing sophisticated right now unfortunately. But we're planning on releasing a version where all servers, models, and build CI for the app are self-managed. Let me know if that's of interest. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Muddy is a browser designed for work, aiming to assist users in organizing project files efficiently in one location.",
      "The browser leverages AI for automatic app organization and project timeline management, emphasizing collaboration and productivity.",
      "Muddy offers a free base product with paid subscriptions for teams, highlighting privacy as a crucial aspect, available for download on Mac or Windows."
    ],
    "commentSummary": [
      "Muddy is a collaborative browser designed for work, utilizing AI for organizing project files and enhancing team productivity.",
      "Users appreciate its timeline-based organization, messaging function, and personalized views, with paid subscription options.",
      "Future plans may include integrating an email client and AI tab organization, with discussions on technical requirements and privacy considerations ongoing."
    ],
    "points": 215,
    "commentCount": 93,
    "retryCount": 0,
    "time": 1715269128
  },
  {
    "id": 40309759,
    "title": "ESP32 Drum Synth Machine: Lofi Wavetable Drum Synthesizer",
    "originLink": "https://github.com/zircothc/DRUM_2004_V1",
    "originBody": "DRUM_2004_V1 ESP32 DRUM SYNTH MACHINE This is my DRUM SYNTH LOFI MACHINE. Synth engine: Wavetable synthesizer based on DZL Arduino library \"The Synth\" (https://github.com/dzlonline/the_synth) 16 sound polyphony Sound parameters: Table, Length, Envelope, Pitch, Modulation, + Volume, Pan and Filter. Filter (LowPassFilter) comes from Mozzi library (https://github.com/sensorium/Mozzi) SEQUENCER: 16 step/pattern editor and random generators (pattern, sound parameters and notes) Hardware: Lolin S2 Mini (ESP32 S2) PCM5102A I2s dac 24 push buttons (8x3) Rotary encoder OLED display I2c 32 LED WS2812B Software: IDE: Arduino 1.8.19 Boards: Expressif Systems 2.0.14 Board: Lolin S2 Mini Libraries: Sequencer Timer - uClock: https://github.com/midilab/uClock RGB Leds - Adafruit Neopixel: https://github.com/adafruit/Adafruit_NeoPixel OLED - u8g2: https://github.com/olikraus/u8g2 Notes: Schematics uploaded. Join solder pads near SCK pin in PCM5102A module. Video demo of the prototype: Waiting PCBs to build the first one :)",
    "commentLink": "https://news.ycombinator.com/item?id=40309759",
    "commentBody": "ESP32 Drum Synth Machine (github.com/zircothc)207 points by peteforde 17 hours agohidepastfavorite30 comments chaosprint 1 hour agoPCM5102A has 32 bits, why is it still synthesized by lofi? If you only need lofi, can you consider an esp32 dac? edit, it looks like sr is a bottleneck... reply tonyarkles 13 hours agoprevNot sure if the author is around here or not but I might have something interesting for them in the next few weeks: an ESP32 implementation of AppleMIDI/rtp-midi for sending and receiving MIDI over Ethernet. I’ve put a reminder in my calendar to reach out once I’m ready to release it. reply usbinsider 4 hours agoparentHow are you finding the stability? I saw an old blog post saying the Mac implementation was quite buggy and still fairly slow. Is it more stable now? (What host?) reply helpfulContrib 13 hours agoparentprevYou're aware that there are already a couple of these, right? First one that pops up in a platformio search: % pio pkg search rtpmidi Found 1 packages (page 1 of 1) lathoub/AppleMIDI Library • 3.3.0 • Published on Sat Aug 27 11:57:15 2022 AppleMIDI (aka rtpMIDI) MIDI I/Os for Arduino. AppleMIDI (aka rtpMIDI) is a protocol to transport MIDI messages within RTP (Real-time Protocol) packets over Ethernet and WiFi networks. This major rewrite is faster, more stable and uses less memory. Read the Wiki page when migrating .. is pretty stable and fast. I've used it in an ESP8266 project .. reply swatcoder 12 hours agorootparentOf course, the existence of some implementation is no reason not to write or share another one. In fact, it's good for the ecosystem to have different people/teams exploring the problem space and it's good for individual developers to hone their craft on delicate technical problems even when they're ostensibly already covered. reply tonyarkles 9 hours agorootparentprevYes, I’m specifically targeting Zephyr. It’s actually less of an ESP32 thing and more of a Zephyr thing, I just happened to have an ESP32 LyraT Mini kicking around to do the project with. And in general the Arduino/Platformio ecosystem causes me to break out into hives… reply boffinAudio 2 hours agorootparent>And in general the Arduino/Platformio ecosystem causes me to break out into hives… Why so? All the disparate platforms being supported? reply schwartzworld 15 hours agoprevFor a similar device (with a different UI) check out the Woovebox. I love mine as a portable groovebox and I'm experimenting with having it be the brain of my dawless setup. Would buy or build one of these in a heartbeat. reply imbusy111 16 hours agoprevThe high-end version of this is Synthstrom Deluge. I love that piece of gear. Even the wood on the sides aesthetic is the same. reply joemi 12 hours agoparentNot sure if you meant the wood side panels are an intentional reference to the Deluge, but if so you may want to know that wood side panels are a common design touch for desktop synths, hearkening back to some of the first compact consumer synths (like the Roland SH-1000), which also had wood side panels. reply ruined 7 hours agorootparentand the akai timbre wolf, which comes with real simulated wood grain sides reply tb303 12 hours agorootparentprevyeah i even had walnut ones made for my 100m because i didn't like the fake veneer reply dmix 5 hours agoprev> This is my DRUM SYNTH LOFI MACHINE. I’m curious what makes it lofi, is one of the chips 12 bit or something? Or just some digital resampling reply boffinAudio 2 hours agoparent20khz sampling rate. reply bxguff 16 hours agoprevPocket operator-esque setup, very cool reply makmanalp 15 hours agoparentJust got a PO-33 and very psyched with it, though I've had some thoughts about whether you couldn't leverage the LCD a bit better to shift it from semi-serious to fully serious. E.g. right now in the sequencer it's impossible to know which specific instrument is playing. Anyway, it would have been really cool to have the OS be open and flashable and spend a little bit of time to make little papercuts like that better. I was looking into how hard it would be to put something like csound in a tiny board and make my own, but when I look at how minimal that single header file synth is, I'm left wondering if that's too much. reply pierrec 13 hours agorootparentI don't know about CSound, but Faust works well on microcontrollers, in fact that's one of its main use cases. Note that Faust focuses more on DSP, synthesis and effects, not so much on sequencing and higher-level music organization. I've found combining Faust (for low level) with any general-purpose language (for high level) works well for a lot of things. https://faustdoc.grame.fr/tutorials/esp32/ reply makmanalp 10 hours agorootparentHadn't heard of it before - the fact that it has multiple compilation targets is super interesting - thanks! reply bxguff 12 hours agorootparentprevTheres some in-depth breakdowns for the PO 12,14,16 here(http://hackingthepo.weebly.com/) if you're interested! I have no idea about the po33 and if the juice is worth the squeeze, but they're cheap enough to tear apart so go for it. reply makmanalp 10 hours agorootparentVery neat, thanks - probably this is just enough beyond my abilities that it'd probably be me bricking my PO rather than accomplishing anything useful :-) reply bitwrangler 16 hours agoprevThis is a nice example of using ESP32 and Arduino. Thanks for sharing the details of your project! reply bambax 16 hours agoprev> DRUM_2004_V1 Shouldn't that title read DRUM_2024_V1 instead? Very cool project! Congrats! reply brotchie 15 hours agoprevGreat project, congrats, super cool. reply stets 16 hours agoprevthat is very fucking cool. Well done! Any resources to learn about this stuff? Always been into synths so I'm interested in this reply mreid 11 hours agoparentIf you want to go a more analog route, this series of synth DIY videos by Moritz Klein is really great: https://www.youtube.com/@MoritzKlein0 reply boredemployee 9 hours agorootparentomg. I wish we had a channel like that 10 years ago when I had free time. reply peteforde 12 hours agoparentprevNot my project, but I agree! The good thing is that there's an overwhelming amount of learning material out there. One fellow who was a big help when I was getting started was Nerd Musician: https://www.youtube.com/channel/UCyqCwyBJ98fR-CPoyXUxY5w reply plastkfantastik 6 hours agoparentprevHere is another cool youtube channel https://www.youtube.com/@hagiwo reply chaosprint 1 hour agoparentprevyou can start with Bela reply amelius 11 hours agoprev [–] Does it support touch-sensitivity and polyphony? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The DRUM_2004_V1 ESP32 DRUM SYNTH MACHINE is a lofi drum synthesizer with a wavetable synth engine using the DZL Arduino library.",
      "It offers 16 sound polyphony and adjustable sound parameters like table, length, envelope, pitch, modulation, volume, pan, and filter.",
      "The hardware comprises a Lolin S2 Mini (ESP32 S2) with various components, including push buttons, a rotary encoder, an OLED display, and WS2812B LEDs."
    ],
    "commentSummary": [
      "Members on Github are engaging in a discussion about an ESP32 Drum Synth Machine project, covering technical details, stability, design features, and resources for learning about synthesizers and DIY projects.",
      "Alternatives like Woovebox or Synthstrom Deluge are being recommended by some participants in the thread."
    ],
    "points": 207,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1715271434
  },
  {
    "id": 40308261,
    "title": "Security Breach: Microsoft PlayReady ECC Keys Compromised",
    "originLink": "https://seclists.org/fulldisclosure/2024/May/5",
    "originBody": "Full Disclosure mailing list archives By Date By Thread Microsoft PlayReady - complete client identity compromise From: Security ExplorationsDate: Thu, 9 May 2024 10:02:26 +0200 Hello All, We have come up with two attack scenarios that make it possible to extract private ECC keys used by a PlayReady client (Windows SW DRM scenario) for the communication with a license server and identity purposes. More specifically, we successfully demonstrated the extraction of the following keys: - private signing key used to digitally sign license requests issued by PlayReady client, - private encryption key used to decrypt license responses received by the client (decrypt license blobs carrying encrypted content keys). A proof for the above (which Microsoft should be able to confirm) is available at this link: https://security-explorations.com/samples/wbpmp_id_compromise_proof.txt While PlayReady security is primary about security of content keys, ECC keys that make up client identity are even more important. Upon compromise, these keys can be used to mimic a PlayReady client outside of a Protected Media Path environment and regardless of the imposed security restrictions. In that context, extraction of ECC keys used as part of a PlayReady client identity constitute an ultimate compromise of a PlayReady client on Windows (\"escape\" of the PMP environment, ability to request licenses and decrypt content keys). Content key extraction from Protected Media Path process (through XOR key or white-box crypto data structures) in a combination with this latest identity compromise attack means that there is nothing left to break when it comes to Windows SW DRM implementation. Let this serve as a reminder that PlayReady content protection implemented in software and on a client side has little chances of a “survival” (understood as a state of not being successfully reverse engineered and compromised). In that context, this is vendor’s responsibility to constantly increase the bar and with the use of all available technological means. Thank you. Best Regards, Adam Gowdiak ---------------------------------- Security Explorations - AG Security Research Lab https://security-explorations.com ---------------------------------- _______________________________________________ Sent through the Full Disclosure mailing list https://nmap.org/mailman/listinfo/fulldisclosure Web Archives & RSS: https://seclists.org/fulldisclosure/ By Date By Thread Current thread: Microsoft PlayReady - complete client identity compromise Security Explorations (May 09)",
    "commentLink": "https://news.ycombinator.com/item?id=40308261",
    "commentBody": "Microsoft PlayReady – Complete Client Identity Compromise (seclists.org)181 points by tithe 20 hours agohidepastfavorite159 comments londons_explore 19 hours agoIs there any video DRM scheme which successfully protects video content appearing on the pirate bay within 24 hours? I really don't see why so many millions (billions?) of dollars have been spent on technologies which so far have never kept the bad guys out. reply jsheard 19 hours agoparent4K streaming content is hit or miss because most services lock that behind Widevine L1, which requires implementors to use a secure enclave and the entire signal path to use strong encryption. If an L1 implementation gets compromised it quickly has its keys revoked and is downgraded to L2/L3, so piracy groups have a limited time window to dump as much 4K content as possible. Those lower Winevines tiers are permanently broken though so everything is immediately available in at least 1080p. 4K Blurays are currently always ripped due to an unfixable compromise in Intel SGX allowing PowerDVDs keys to be extracted, they could close that hole by revoking PowerDVDs keys for new Bluray releases but they haven't done that yet. I imagine they will at some point because PowerDVD requires SGX to play UHDs, and Intel stopped supporting that on newer consumer hardware, so 4K Bluray playback on PCs is effectively being phased out. reply grishka 17 hours agorootparent> the entire signal path to use strong encryption But the display panel itself still receives an unencrypted LVDS signal, which should not be too hard to decode. There are (were?) also cheap HDMI splitters that conveniently strip HDCP. Your only issue is that yes, you can't get at the original compressed video stream and have to reencode, possibly losing a tiny bit of quality. reply jsheard 17 hours agorootparentThat is true, but ripping content in that way is a much bigger burden on piracy groups since it has to be done in realtime, can't be done in parallel without multiple expensive hardware rigs, and metadata like subtitles can't be extracted automatically. Rips of streaming shows often have a dozen or so subtitle tracks and nobody is going to transcribe and re-time all that by hand if they can't decrypt the stream directly. reply grishka 17 hours agorootparentAre subtitle tracks also encrypted? I've always had the impression that only video itself is. edit: But the subtitle tracks are also available on software-only DRM levels that are easy to break. reply jsheard 17 hours agorootparentActually now I think of it that doesn't matter since you could just pull the subtitles from the weakly protected 720p version then apply them to the higher resolution versions. Ripping the 4K video through LVDS or HDMI capture would still be annoying though. reply qingcharles 14 hours agorootparentI don't know the burden is as big as you imagine. I used to run a torrent site that was 99% recorded shows through capture and we still had every single broadcast show uploaded within minutes -- and no uploaders were getting paid, they were just bored and doing it for the Internet points. edit: also to add, we would get employees at the studios send us discs with the new shows before release, but I had agreements with at least one studio to not allow uploads until after broadcast if we received any of their media reply grishka 17 hours agorootparentprevYep I realized that and edited my parent comment but you beat me to it. reply jsheard 17 hours agorootparentAnother complication for LVDS capture is that HDR content is always tonemapped/filtered (OLED ABL etc) before it is sent to the panel, and that processed version is what you would get with LVDS capture. It might be usable, but it would be a downgrade from other capture or decryption methods which grab the unprocessed HDR video. reply kuschku 16 hours agorootparentWith a $20 HDMI grabber you get 4K HDR video with full Dolby Vision or HDR10 metadata, without any tonemapping applied, én masse. Combine that with some software mods to hide the UI at all times and you've got a perfect recording. Re-encoding is the slowest and most annoying part of this process, but release groups re-encode everything anyway, so that's not an issue either. DRM only hurts the legitimate customers, no one else. I'm subscribed to the highest tiers of Netflix, Disney+, Prime Video, Paramount+, YouTube Premium, CuriosityStream, Nebula and Zattoo. Yet often enough, I have to rip media from bluray because the streaming version only has audio or subtitles available in the local language or the quality is subpar. reply gambiting 2 hours agorootparent>> I have to rip media from bluray because the streaming version only has audio or subtitles available in the local language This. It's a huge problem for us expatriates living outside of their native countries - I want my son to be bilangual and as part of that I try to at least play cartoons in my native tongue - but Disney/Netflix/Prime usually only have the local language option, even though as soon as we are physically in my home country magically the same shows have dubs in local language. So all of these corporations have these options available, but decide not to show them for whatever reason. So more often than not I have to ask family to send us actual DVDs/Blurays of kids shows so we can watch them in the language we want to watch them in. And no, VPN doesn't always help - it's flaky, it's frustrating in its stability, one day it works the next day it doesn't, it's not the thing I want to mess with nearly every other day just to play some cartoons. reply ricktdotorg 19 hours agorootparentprev^^^ great comment. hard to imagine a better synposis of 4k DRM in ~2 grafs. thanks! reply gorkish 19 hours agorootparentNot mentioned above, but should be noted that all of this DRM is still only protecting the compressed and encoded video content. Schemes to protect the uncompressed digital video data are all permanently and universally broken or bypassed. The 'analog hole' has gone fully digital. One would think that alone would be enough to seal the deal on the pointlessness of DRM, but unfortunately there are a lot of gullible execs out there that want to keep pouring money on the fire. reply gjsman-1000 18 hours agorootparentAs long as it stops even 100,000 people from not downloading videos off of Netflix, from an executive’s perspective, it pays for itself. To them, it’s like saying Speed Limit signs are useless, because cars can go faster than the number posted by literally pressing a button. That’s not the point. reply xyzzy123 18 hours agorootparentYes if a particular group gets to externalise / socialise the costs of maintaining a protection then obviously from the perspective of the protected group then it's worth it. The question is, is it good for society overall. Who or what is being protected and what impact does that have on everyone else? Speed limit / stop signs represent a decent point of discussion I think. reply sureIy 17 hours agorootparent> The question is, is it good for society overall. That’s not what execs ask at all. I don’t know where you’re living. The existence of DRM is not in any way related to society. Their analysts say it’s a net positive on their balance sheets, so DRM is here. Everything else is baseless speculation. reply pas 18 hours agorootparentprevSpeed limit signals danger, right? Does DRM signal an ethical dilemma? And if yes, what does it mean considering that each year we lose millions of people on the roads. (To fatalities and horrific injuries resulting in permanent disabilities.) Yet the majority doesn't care? reply dialup_sounds 14 hours agorootparentprevThere is no \"particular group\" or \"everyone else\". Everyone has rights over their own creative work, even if that's mildly inconvenient to others. It's part of the social contract of modern society. reply int_19h 13 hours agorootparentThe people who profit from all this are mostly not those who can claim that it is their own creative work. reply dialup_sounds 9 hours agorootparentFinish the thought. How do they profit from someone else's creative work? Is it a) by just taking it because they can? or b) by mutual agreement with the original creator? reply int_19h 8 hours agorootparentBy being in a dominant economic position whereby they can force predatory terms onto content creators who cannot avoid them if they want any chance of getting anything at all, or reaching a significant audience. reply gorkish 18 hours agorootparentprevIf you take the capitalistic lust of the corporate executive to its logical extreme, given the massive costs of the DRM tech you'd think that at least one of them would realize that they could make more money if they didn't have to pay for something that doesn't work. The economics of distributing the copies are such that it doesn't actually matter if it's easy or hard for 1 or 100,000 people to break the protection. reply bobdvb 17 hours agorootparentI work for a large streaming service and a significant part of my work is content protection. Honestly, tech folks misunderstanding of DRM and content protection is significant. There's some assumption that people are inherently honest and that we're just money grabbing. In the years that I've been doing this I've seen a lot of things and nothing has convinced me that if we turned off DRM we'd: 1) save money 2) not have issues with piracy proliferation The cost of DRM license issuing for our company is relatively insignificant, a year's worth of DRM for millions of users is less than the cost of a single show we might make. We pay cents per thousands of plays. I recall we launched in a new market, we did a show which would have been an expensive PPV previously, but it was included in our standard subscription. We also offered a first month free trial, which you could cancel. So, you could enjoy it at zero cost, from the original provider in high quality, with no commitment. That night our anti-piracy team took down 20,000+ illegal streams, serving a large audience. I also acutely know that DRM isn't as secure as we'd like, I know that all security measures are ultimately not anywhere near perfect. But you know what? I also lock my front door, even though I know how to pick locks. I put my car keys in a RFID box, despite knowing there are probably CAN attacks against my car. I still need to protect my assets, because enough people don't want to pay for something if they can get it for free. We had some research into the attitudes of pirates that basically distils down to: 1) 1/3rd would pay if they couldn't get the content any other way 2) 1/3rd don't care enough and are casual pirates, watching because they can. 3) 1/3rd are \"pay never\", militant, yet still happy to take my work without concern for the sustainability of that. Ultimately, if you like content then you should pay for it, but it's always a waste of time arguing about this on the internet because so many people are in the third category, think I'm an asshole for doing my job and apparently they know my job better than I do. reply gorkish 15 hours agorootparentI'm gonna be extremely blunt given that I have you in my audience, large streaming media worker bee: It's not surprising in the slightest that you have a bias towards the effectiveness of DRM when your livelihood depends on it. The fact that the unit-cost is \"relatively insignificant\" is simply a continuation of the straw man argument that props up the entire notion that DRM is somehow cost effective. I don't personally think you are a jerk or anything for working your job, but I can say that I would not personally find it fulfilling to spend my own career on something with such diminishing returns. I guess all of those insignificant expenses add up to some good money in the end, at least in someone's opinion. The incentive to continue burying the failed promises of DRM and keep it propped up as long as possible is evident though; the story really hasn't changed in the 30 years or so that I've been following it. The lack of a \"save video\" button in the player app is the most effective means to prevent the average person from distributing the content. By your \"lock on the door\" analogy, a UI that does not allow the thing you don't want your users doing is providing more or less equivalent protection to the DRM. It doesn't matter how many locks you put on your door if all the attacker needs to get what they want is to look through the window. Why continue to invest in the additional technology if it is not actually adding significant additional protection? By the time any user presents a willingness to do anything at all to circumvent your standard software interface, you have lost; the user will succeed. Plugging in a $30 recorder and pushing the button is all it takes, and all the sweet cutting edge secure enclave crypto quantum DRM in the world cannot prevent it. How many of those 20k illegal streams you cite even bothered to break the precious DRM? My guess is zero. reply gjsman-1000 13 hours agorootparentRight, as though extensions for downloading videos haven’t been Top 10 most installed on all major browsers for over a decade. reply HeatrayEnjoyer 16 hours agorootparentprevThat's all beside the point. Hardware belongs to the user and should be under the user's control. Treacherous computing should be highly taboo and illegal. The \"sustainability\" of Disney's profits are not important. To suggest otherwise on a site literally named Hacker News is comical. reply eklavya 15 hours agorootparentWhy would bringing up sustainability of any business be comical at Hacker news? How do you make money? Why should it not be for free? Your sustainability is important? We agree on hardware belonging to the user by the way. reply int_19h 12 hours agorootparentHacker ethos is about freedom to control what you own and put it to the purposes that you, its owner, want. DRM takes away that freedom, so it is obviously incompatible. If that freedom makes e.g. Disney business model unsustainable, then that business model is itself incompatible with the ethos. reply HeatrayEnjoyer 4 hours agorootparentprevYou're still missing the point, and I believe intentionally so. > We agree on hardware belonging to the user by the way. You absolutely do not or you would not engage in the work you do. Actions speak louder than words. Being dishonest only makes it even worse. reply jorams 15 hours agorootparentprevThe argument from the other side is at least as frustrating. > ...nothing has convinced me that if we turned off DRM we'd: 1) save money 2) not have issues with piracy proliferation > That night our anti-piracy team took down 20,000+ illegal streams You already have enormous issues with piracy proliferation. The money you spend on DRM may be \"relatively insignificant\", but it's still money being wasted on \"protection\" that has already proven to be utterly ineffective. I am in neither of your three groups. I want to pay for content. I pay for a lot of music, for example. But you're not going to bully me into paying for your shit by making it as user hostile as possible. As a paying customer I expect at least the level of service that piracy groups have no trouble providing, but instead I'm treated like an enemy every step of the way. In practice this means I avoid TV shows and movies, but when I do want to watch one I have absolutely zero moral qualms pirating a product that is not for sale. I'll even go out of my way to look for a DRM-free copy I can pay for first. This takes more time than pirating it once I inevitably find out that's not available. reply gjsman-1000 13 hours agorootparent> already proven to be utterly ineffective The fact that it does not always work, is in no way a proof of ineffectiveness. Otherwise, the tax system, speed limit signs, front door locks, and glass windows are also “completely ineffective.” He is literally telling you, from his own experience in his company, it’s effective. Don’t cite a sloppily-produced research paper from somewhere to make him deny reality. reply jorams 13 hours agorootparent> Don’t cite a sloppily-produced research paper I'm not, I'm citing their own comment in which they describe taking down 20,000+ illegal streams of their already DRM-\"protected\" content on launch day. He's describing it not being effective at all. Glass windows, speed limit signs, the tax system (what?) provide value to the people affected by them. DRM is a pure negative for customers. reply gjsman-1000 13 hours agorootparentYou’re assuming it would not have been 100,000 without the DRM. You cannot prove, or cite any research, showing it would not have been much worse. In which case, it could indeed be quite effective. reply jorams 13 hours agorootparentIndeed I can't, just like you cannot prove, or cite any research, showing it wouldn't have been 1,000 if the content was accessible without arbitrary artificial restrictions on the devices consuming it. By all means keep taking down illegal streams. I'm not excusing the people providing them. I'm saying maybe stop treating every paying customer as if they're going to do that to the detriment of the service provided. Because it is negatively affecting the service. reply gambiting 2 hours agorootparentprev>>So, you could enjoy it at zero cost, from the original provider in high quality, with no commitment. That night our anti-piracy team took down 20,000+ illegal streams, serving a large audience. And....was it worth it? Do you think literally anyone from those 20k people actually signed up for your trial? reply qingcharles 14 hours agorootparentprevI used to be the chief DRM guy at another large streaming service. I can say 100% that the company did not want DRM as it was unreliable and customer-unfriendly, but it was the rights-holders that were badly educated and informed and would demand it in their contracts. I would suspect that is the case at a lot of other streamers too? (the cost of the DRM was near-zero at our company) reply pmontra 15 hours agorootparentprevI understand your points and I wish you all the best with your job. But please tell your bosses to let me buy single episodes of the series I like or every movie in history. No monthly subscriptions. I stay months without watching anything, then maybe two or three series at once, one episode per week each. The industry business model doesn't fit my habits. reply bee_rider 17 hours agorootparentprevI think DRM works fine for the actual customers, the companies that are distributing video who need to convince the movie producers that they are taking it all very seriously, so they need to check some “our platform uses DRM” box. It all looks very odd from us downstream. But, still, most people don’t break DRM so it must be doing something. For a long time the industry worked by shipping movies off the theaters, to be run in projection room secured by kids doing after-school jobs. I think they aren’t concerned with perfection. reply repelsteeltje 18 hours agorootparentprevI agree, DRM has significant costs. Consider you've encoded and packaged your mezzanine into ABR (dash, HLS) and it's working on phones, browsers, smart TVs, STBs etc. Now you add common encryption: repackage and get double the number of tracks (CENC as well as CBCS). You buy your licenses from Apple (Fairplay), Google (Widevine), Microsoft (Playready) and Marlin (old crap). What used to \"just work\" now has all kinds of subtle interop problems. Audio sync issues on iPad? Ah, Apple pushed a bad firmware update, thank you. Tomorrow it's users complaining about Widevine in Firefox. Only Netflix, maybe Disney+ — the biggest of the biggest can do streaming with DRM and make a profit. reply gjsman-1000 18 hours agorootparentprevI can get DRM, right now, for my videos, with 500,000 plays for $1665. That’s publicly available, commercial pricing. That’s a third of a cent per play. At Netflix scale, it’s probably cheaper. DRM is a drop in the bucket compared to normal costs. A Netflix subscription is, what, $10? That’s enough to pay for 3,300 encrypted plays. The same provider, if I was doing over 10,000,000 plays, will drop it to just under one tenth of a cent per play, enough for over 10,000 encrypted plays. Compare that with how much the internet bandwidth, storage, and distribution costs - and the DRM is a rounding error. You’re seriously telling me that not even one out of 10,000 plays is going to attempt a serious theft, to share it with random friends and family? Hah, it’s probably closer to 5 in 100. Believe me - I’m not a guy who defines himself by living in a Hacker News bubble where everything needs to be perfect to be effective. I’d have DRM yesterday if I ran a streaming service, just like my copyright filings and the deadbolt on my front door. reply temac 15 hours agorootparentSharing with friend and family is not \"serious theft\". It is benevolent and what people do with books and DVD, without industry people becoming insane about. reply gjsman-1000 13 hours agorootparentI meant sending copies; where the next thing you know one purchased copy becomes thirty people holding copies. reply 486sx33 18 hours agorootparentprev“4K Bluray playback on PCs is effectively being phased out.” Which will only perpetuate and speed up the problem. 4K blu ray discs suck on a lot of new tvs and players for frame rate and detail so the best visual experience is going to be on high dpi PCs (or Mac? Retina?) If I can’t play a 4K blu ray I purchased on my pc… I’m going to probably download a ripped version and not feel guilty about it since I purchased the disc … My M2 Pro can decode and play 4K without breaking a sweat and with amazing battery life on VLC player reply m4tu4g 14 hours agorootparentprevTo add more to this, not essentially 4K is the only thing behind L1, even HD streams can be with L1. It's entirely the services' choice to use what they want, like they can even put SD stream behind L1 and leave 4K for L3 (this happens widely in lesser known services & L2 is hardly used). Also Amazon's 4K is different from Netflix's 4K considering the key revocation TAT. So everything changes from service to service. reply devrand 18 hours agorootparentprevIt does seem like Netflix has been doing a decent cat-and-mouse game with Widevine for anything over 540p the last few months. There's been several shows that took several days to get properly copied (i.e. not just screen recorded). reply int_19h 13 hours agorootparentprevNot immediately; sometimes when they revoke the keys it can take a few months for the likes of StreamFab and AnyStream to catch up even with 1080p. E.g. StreamFab is currently stuck on 480p for Netflix, and it has been like that since January. reply panzi 19 hours agorootparentprev> so everything is immediately available in at least 1080p. Aren't the lower tiers only 720p? At least all the streaming services give Linux users only 720p. (There is a workaround for one particular service to still get 1080p - I'm paying for it so I better can watch it in 1080p! The moment this stops working I cancel my subscription.) reply jsheard 19 hours agorootparentThere's three Winevine tiers, L1, L2 and L3, which generally correspond to 4K, 1080p and 720p respectively though it depends on the service. L3 is what you get on Linux. L2 is supposed to be more secure than L3 but AFAICT it makes little difference to piracy groups, L1 is the only actual roadblock for them. reply brnt 18 hours agorootparentWhy are Linux users limited to L3? reply jsheard 18 hours agorootparentBecause it doesn't meet the requirements for L2. I think L2 implementations are required to block software screen recording, for example, and there isn't really any practical way to enforce that on an open platform. Windows/Android/iOS have special support for compositing protected content so if you try to read the framebuffer back the content just shows up as a black rectangle. reply panzi 16 hours agorootparentprevDRM only really works if you're not root on your own machine, and with Linux you're always root on your own machine. Quite frankly I think DRM (the normalization of rootkits) is dangerous. reply Mindwipe 17 hours agorootparentprevL1/L2 requires a third party who could be liable to sign that the drivers are unmodified to the hardware. On a general purpose Linux installation who would do that? (And who in the Linux using community wouldn't take any efforts by someone to try as an afront, bluntly). reply bee_rider 17 hours agorootparentAlso there aren’t really enough of us watching videos on Linux for it to be a worthwhile market for them to address, I think. reply lldb 15 hours agorootparentprevSo it turns out chrome os ships with a shared library to support L2 (since it's entirely in software). There's a patch to get it working on other Linux distributions. reply SSLy 19 hours agorootparentprevL3 can do FHD on Linux but it's the services config that prevents that. reply brnt 18 hours agorootparentWhy do they do that? reply daveoc64 17 hours agorootparentThe lower levels of Widevine protection are weaker, so the content providers like Netflix only allow playback in standard definition or 720p at those levels. They don't want the highest quality to be available on devices where the DRM can easily be broken. reply SSLy 11 hours agorootparent>They don't want the highest quality to be available on devices where the DRM can easily be broken. they don't want to admit you can get L3 keymaterial from androids super easily. They just are obnoxious assholes. reply h4x0rr 18 hours agorootparentprevCouldn't scene groups just keep the exploits for decrypting streams for themselves? Is there any way for Netflix/Widevine/PlayReady to detect this? reply jsheard 18 hours agorootparentI don't know the technical details but Winevine claims to have a system for watermarking content, which may allow them to trace the origin of ripped content back to the set of keys which decrypted it so they can be revoked. https://www.digimarc.com/resources/widevine-announces-digita... reply lossolo 17 hours agorootparentprevThere are no exploits for Widevine. The system operates by requiring a key, which is obtained from the unsecure hardware enclaves of some of the thousands of devices whitelisted by Widevine. When you access and share publicly 4K content, the keys for that specific device are blacklisted, necessitating the purchase of a new device to extract a new key. reply somenameforme 18 hours agoparentprevIf you approach it at the most fundamental level, it seems like a clearly impossible goal to achieve. You are having users playing back content on their private devices, and then want to try to prevent them from copying that. That's basically impossible to achieve on somebody's own machine, and literally impossible to do once two enter into the picture. In the absolute worst case a high resolution/hertz cam on one's own screen with a quick ML software polish job, would look near to completely indistinguishable from the original content. I imagine the reason so much money has been spent on it is because studios prefer to blame piracy than content for increasingly poor sales. So they see it as their salvation and are willing to pay big bucks, even if it's impossible. That's a primo ground for hucksters and charlatans to make a killing. Something similar happened in poker where players wanting to use fully automated software to make their decisions ended up just stepping outside the cat&mouse game and using a setup with a second computer + cam - completely and absolutely impossible to detect. reply eddd-ddde 18 hours agorootparentI imagine in the future DRM is directly embedded in the viewers brain and if it detects pirate content it just fries you. I genuinely can't imagine any other form of DRM being successful. reply idle_zealot 14 hours agorootparentYou could require that all devices capable of video or audio display or capture embed models to detect copyright-protected content, and only proceed with playback or capture if they are connected to the internet and are able to verify some cryptographic liscense is valid. Put all this logic in some secure processor that self-destructs at the slightest sign of potential reverse-engineering or irregular behavior, along with physical anti-tampering measures that make phreaking or uncapping any components liable to trigger self-destruction. Then make the circumvention of any of these measures or attempts to create or import non-compliant display or capture video or audio content carry some heavy criminal penalty, such that any group well-resourced enough to attempt bypass would judge doing so foolish. That would probably \"work\". reply LadyCailin 16 hours agorootparentprevDrink a verification can to continue. reply bee_rider 17 hours agorootparentprev> In the absolute worst case a high resolution/hertz cam on one's own screen with a quick ML software polish job, would look near to completely indistinguishable from the original content. I’m not even interested in piracy (no ethical dilemma I just can’t be bothered), but I think this would be an absolutely fantastic tech demo, and also very funny. Ultimately the video has to be displayed on a screen, so this must be the final defeat for DRM, right? reply kuschku 15 hours agorootparentEvery now and then I do event tech for some small tech conferences, lectures, etc. A while ago we had an issue where, under some circumstances, macbooks would enforce HDCP for their output. Obviously an issue if you're trying to record and stream a talk. And we didn't have any hdcp removal devices on hand. So I set up a Sony FX30 with fujinon broadcast optics on a tripod, aimed at the screen. Some white balancing and adjustments to brightness and ISO curve later and the image was undistinguishable from the original. We actually used that setup for all talks on that day, and it worked perfectly fine. reply layer8 12 hours agorootparentprevGetting the colors right would be difficult. reply gwbas1c 19 hours agoparentprev> have never kept the bad guys out Careful who you call the bad guys. A lot of \"piracy\" comes from the people who spend the most money on the content they pirate. I personally think the best DRM approaches are those that keep \"the honest people honest:\" IE, metadata that identifies copyright owners, flags that identify content that has restrictions due to copyright, and casual protections. (Think of a \"do not enter\" sign that you can choose to ignore if you have reason to do so.) Otherwise, DRM really only works when the people consuming the content have motivation to keep it secret. (IE, corporate and military secrets.) reply repelsteeltje 19 hours agorootparentFunny thing is that most streaming platforms only have DRM because content owners pressure them. It's expensive and a huge hassle to get right. While indeed DRM barely contributes in fighting redistribution over Pirate Bay, it does prevent stream sharing. Ie.: the platform saves a lot of CDN bandwidth by forcing that onto torrents. reply watermelon0 17 hours agorootparentI think this is not entirely true, because HBO and Netflix have DRM on their own shows. reply andsoitis 19 hours agorootparentprev> A lot of \"piracy\" comes from the people who spend the most money on the content they pirate. That doesn’t strike me as a valid statistic. Where are you getting that data from? reply amargulies 18 hours agorootparentSimple example: My wife wants to consume certain Austrian/German content in Canada which are not available on any streaming service here. The streaming services there (Germany/Austria) do not support Canada. She was gifted DVDs of them, but that means she can't watch them on her phone or tablet (or laptop without a usb dvd drive that's region coded to Europe). Options are to: - rip the DVDs (pain in the butt unless you have a specific setup for doing it en-masse. Some shows end up with episodes out of order, etc) - download the shows And this is when she's lucky enough the show/movie had a DVD release. Similar problems exist for local content that doesn't exist on streaming sites altogether (bunch of things I grew up watching that I'd like to revisit). reply DarkUranium 18 hours agorootparentNote that ripping DVDs is still piracy if said DVDs contain DRM[1], at least in the US. I don't know about CA, but I'd imagine it's similar, considering the state of copyright ... [1] Region locking is a form of DRM, and most DVDs at least used to be region-locked. I don't know if that's still common practice nowadays. reply ThunderSizzle 17 hours agorootparentIn the US, it's only a legal violation if you try selling it. For personal use, you can rip DVDs. Granted, the media companies use civil lawsuits to also make it feel illegal. reply repelsteeltje 18 hours agorootparentprevI think he alludes to \"lore\" more than statistic. In the CD/DVD age anti-piracy measure like region locks, DRM, but also annoying banners you could not skip would often make consuming the media with a regular CD or DVD player so cumbersome, that you were almost forced into ripping the media onto a hard disk first and consuming the media with VLC or similar. The inability to just consume media using official device to on rented or purchased disks encouraged ripping, sharing and downloading. reply amargulies 18 hours agorootparentThere are in fact studies that show people that pirate tend to spend the most on legal content. See every study listed here for example: https://www.vice.com/en/article/evkmz7/study-again-shows-pir... reply ParetoOptimal 18 hours agorootparentprev> That doesn’t strike me as a valid statistic. Where are you getting that data from? I'm going to assume they are trying to say in other forms of support like word of mouth marketing, user created content, or purchases in other areas such as video game merchandise for instance?= reply gjsman-1000 18 hours agorootparentprev“Careful who you call the bad guys. A lot of \"piracy\" comes from the people who spend the most money on the content they pirate.” This is laughably, obviously false. Don’t let the Reddit bubble of all 300 people who do this, or the 1.2% of Yuzu users who actually dumped their own keys, distort your understanding of reality. reply prmoustache 16 hours agorootparent> or the 1.2% of Yuzu users who actually dumped their own keys As a Nintendo Switch owner, if my console died or I wanted to play Zelda at 4K I would probably not go through the hassle of dumping my own keys and rip the game myself if I can download them on the internet in a more convenient/quick way. So there is probably a much larger fraction of users that own their games legally but still use emulators. Also as said somewhere else, the fact some people play pirated games they would probably not even play if they were not available that way is orthogonal to the fact they may still be the highest spenders in games. Same applies to movies/music/shows. People usually have a non infinitely stretchable budget. A lot of piracy is opportunistic but would not transfer in sales if prevented. When I was a teenager/young adult I pirated a lot of stuff to try out. My gaming/movie/CD budget was fixed anyway and I still spent money on them but for the most part I would not have bought more if those things weren't accessibles illegally. Some were either out of reach for my budget (softwares like photoshop or Music DAWs), other were not deemed good enough to pay for them over better records/movies/games. And it transfers to today: while I have a totally unlimited access to 8, 16-bit and 32-bit console roms, I almost only play to games I have owned and loved at the time. reply DarkUranium 18 hours agorootparentprevhttps://www.vice.com/en/article/evkmz7/study-again-shows-pir... reply hannob 18 hours agoparentprevDRM schemes never worked, and it has been speculated that the people building them always knew it, but had other goals. Backn in the days it was: Of course you can break DVD copyprotection schemes. But you cannot build a legal opensource DVD player software. Today it's: Of course every Netflix series can be found on the pirate bay. But you're not legally allowed to build an alternative netflix player frontend. reply burningChrome 17 hours agorootparentJust as an aside and probably a dumb question - is Pirate Bay still a thing? I know they have archival stuff you can access, but I thought Pirate Bay died out a long time ago and even pier to pier networks have all but disappeared with streaming. I feel like this is kind of a naïve question, but I haven't needed to use pier to pier stuff since streaming did become the standard and remember a lot of articles on Pirate Bay shutting down around 2014. Some of the 1070's movies I've found on YouTube that aren't on any streaming platform like the 1982 movie Dreams Don't Die about a graffiti artist played by Ike Eisenmann. reply sureIy 17 hours agorootparent> pier to pier I can’t believe that we don’t use this terminology. Of course pirates go from pier to pier. Missed opportunity reply burningChrome 9 hours agorootparentAh yes, the mobile typing got me again. Thank you for the good chuckle and have an upvote for that. reply zootboy 16 hours agorootparentprevYes, the Pirate Bay is still a thing: https://thepiratebay.org/index.html Feel free to look up your favorite movies from 2024. They're on there. reply gjsman-1000 18 hours agorootparentprevFrom the executive’s perspectives, DRM is working just fine. People can’t just go get a random browser extension to save videos. Alternative and unlicensed clients are illegal. Sure, there’s some piracy - but even at the end of the day, pirates would watch a smartphone recording to save a buck. To them, DRM does not have to be perfect to be a good investment; any more than copyright needing to be perfect or Speed Limit sign enforcement needing to be perfect. Plus, every layer of complexity that gets broken, is another line for convincing the DOJ or the Jury about malicious intent. reply oaththrowaway 18 hours agorootparent> Sure, there’s some piracy - but even at the end of the day, pirates would watch a smartphone recording to save a buck. I spend a lot of money on hard drives and Usenet to have quality rips. It's a service problem, not about the money reply gjsman-1000 18 hours agorootparentYes, yes, the Gabe Newell quote - even though that quote was only an explanation for why piracy happened. Commonly lost in translation, that quote never once said piracy was justified or acceptable, nor did he encourage piracy under any circumstances. reply oaththrowaway 15 hours agorootparentI never claimed he did? I was just responding to your incorrect assumption reply bombcar 19 hours agoparentprevThe point of the DRM schemes is basically to keep video \"hard enough to copy that normies don't do it\". And not even \"normies can't find it on the Pirate Bay\" but \"you can right click and download from Netflix.\" If they mostly succeed at that, they consider it good enough. reply jorams 17 hours agorootparentIf that were true it would be possible to watch in 4k resolution on Netflix on Linux. But it's not. reply bombcar 17 hours agorootparentBecause if \"4k on Linux\" was doable than \"download a 4k rip directly\" would shortly follow. reply grapescheesee 16 hours agorootparentWell, it sure seems to make a market for people who would/does pay for legitimate 4k video in their browser to pirate. I am happy to pay for streaming, but as the quality goes down so does any desire to shell out honest money. reply bombcar 16 hours agorootparentThe thing that's breaking me is that I can't even figure out which combination of what I need to get what I want. I just want to pay the $5 and watch the damn movie/show! So instead I just ... check out the Roku from the library that has all the services and binge ;) reply makin 19 hours agoparentprevDenuvo mostly works. Allegedly they have a custom approach to each new game, so cracks can take months to appear, with some unpopular games never having been cracked at all. The price is lowered performance, of course. reply ParetoOptimal 18 hours agorootparent> Denuvo mostly works. Not for users: https://gamerant.com/denuvo-outage-servers-down-persona-5-ro... > Allegedly they have a custom approach to each new game, so cracks can take months to appear, with some unpopular games never having been cracked at all From what I hear, it's cracked in a matter of days or weeks. I haven't checked whether this is true or not, so I can't say you are wrong about some (most?) cracks taking months. reply crtasm 17 hours agorootparentLooking at the previous two years of uncracked Denuvo and only selecting games that seem notable: Dragon's Dogma 2 (2024) Like a Dragon: Infinite Wealth (2024) Suicide Squad: Kill the Justice League (2024) Street Fighter 6 (2023) Hi-Fi Rush (2023) Dead Space (2023) Star Wars Jedi: Survivor (2023) Persona 5 Tactica (2023) EA Sports FC 24 (2023) NBA 2K24 (2023) Assassin's Creed Mirage (2023) Atomic Heart (2023) Lost Judgment (2022) Sonic Frontiers (2022) Sonic Origins (2022) Persona 4 Arena Ultimax (2022) Persona 5 Royal (2022) Sniper Elite 5 (2022) Marvel's Midnight Suns (2022) Total War: Warhammer III (2022) Going back further there's more high profile games that were never cracked. The system seems to work as intended in some cases. reply kyriakos 15 hours agorootparentMany denuvo games are eventually released without denuvo and are then instantly pirated. Looks like the cost of denuvo is high enough for game publishers to stick to it just enough to reach profitability and then ditch it. reply lossolo 17 hours agorootparentprevSome of the games you mention were already cracked but not by the scene. reply crtasm 17 hours agorootparentWhich ones? My list has nothing to do with scene or otherwise. reply free_bip 19 hours agorootparentprevThat's a video game DRM scheme, not a video DRM scheme reply lossolo 17 hours agorootparentprev> Allegedly they have a custom approach to each new game, so cracks can take months to appear It's because it's tedious to crack it, it's not really a rocket science, they just generate new VM for the binary so you can't automate it, they inject A LOT of code paths which you need to manually follow and change. That's the only reason why games stay uncracked for months. It's a war of attrition. > with some unpopular games never having been cracked at all That's not exactly true actually, you need to pay for Denuvo license every year, that's why after some months or a few years it's removed from most of the games. reply miki123211 19 hours agoparentprevThe point is pressure on equipment manufacturers, making borrowing and streaming work for digital content, maybe also deterring casual piracy, not necessarily protecting videos from appearing on tpb. reply whoopdedo 14 hours agoparentprevI don't believe the \"Digital Video Express\"[1] (aka DIVX[2]) discs were ever cracked while they were on the market. But that's only because they were only sold for 1 year and nobody bought any. Even now finding information about the disc format is rare. Although anyone who has a reason to try probably should be able to do it easily since it was just 3DES. [1] https://en.wikipedia.org/wiki/DIVX [2] And this is when I remember that Wikipedia links are case-sensitive reply jvanderbot 18 hours agoparentprevIt doesn't need to work, it needs to be a clearly demarcated legal boundary. If it's hard enough that it takes effort to cross, you can prosecute. Someone who wanders in the woods might not be blamed for trespassing. But someone who hops a fence with a sign on it doesn't have much defense. reply squigz 18 hours agoparentprev> I really don't see why so many millions (billions?) of dollars have been spent on technologies which so far have never kept the bad guys out. Because the goal isn't actually to \"keep the bad guys out\" - it's to strip user freedom and privacy, and make a shit load of money at the same time reply daveoc64 17 hours agoparentprevThe DRM clearly does work in preventing \"casual piracy\" - where average users do things like downloading a file and keeping it forever (even after cancelling a subscription) or copying the file to a friend. reply yencabulator 17 hours agorootparentVideo streaming hasn't been \"a file\" in a long time. HLS et al download little snippets at a time, adjusting to current bandwidth circumstances, typically with video and audio separate, etc. Even without DRM, the average user couldn't \"download a file\" from Netflix. reply jasomill 10 hours agorootparentDepends on what tools you give the average user. A simple GUI wrapper around something like yt-dlp or ffmpeg would suffice for downloading DRM-free HLS videos. As a concrete example, try yt-dlp --format english_192-English+bestvideo https://devstreaming-cdn.apple.com/videos/wwdc/2020/10655/3/45C0E27F-A3BA-416D-B037-9BEE7466C11F/master.m3u8 Note that --format is only required (to get the best available version) because yt-dlp appears to rely only on metadata present in the supplied .m3u8 file to determine which stream is \"best\", and no such metadata appears for the audio streams in this example. For details on what yt-dlp knows about each candidate stream when attempting to choose the best, see yt-dlp --list-formats https://devstreaming-cdn.apple.com/videos/wwdc/2020/10655/3/45C0E27F-A3BA-416D-B037-9BEE7466C11F/master.m3u8 This would presumably not be a problem for our hypothetical \"Netflix DLP.app\", which could rely on whatever convention Netflix uses to indicate stream quality to its own clients when choosing streams, rather than falling back to sensible defaults for arbitrary HLS input. reply ParetoOptimal 19 hours agoparentprev> I really don't see why so many millions (billions?) of dollars have been spent on technologies which so far have never kept the bad guys out. A PR campaign to make people think getting that content for free is harder than it is? reply tawa9102930 19 hours agoparentprevHDCP is broken so none of it really matters. The resulting files (\"webrips\") aren't a lossless copy of the original, but are good enough for most. reply gorkish 18 hours agorootparentI posted elsewhere in the thread but it bears repeating, \"The analog hole has gone fully digital.\" The generational loss from one recompression is effectively unnoticeable. What a ridiculous arms race! reply Salgat 19 hours agorootparentprevYeah web-dls might be hit or miss but webrips are such good quality that it's irrelevant for most folks anyways, since it's nearly the same quality you'd see on your TV. reply russelg 9 hours agorootparentI think you have terminology wrong here. by definition, WEB-DL is the exact encrypted video/audio content that is served, but unencrypted of course. You literally cannot get higher quality than a proper WEB-DL. reply Salgat 8 hours agorootparentThat's what I said, you might not be able to get a webdl depending on if the stream is cracked, but at least webrips (which are captured from capture cards) are good enough quality anyways. reply Retr0id 17 hours agoparentprevAt this point, video DRM is more of a legal protection than a technical protection. reply nevir 17 hours agoparentprevPublishers demand it, but don't understand it. The platforms roll their eyes, but implement it anyway; cause it's a rounding error, and keeps publishers happy reply nonrandomstring 18 hours agoparentprev> I really don't see why so many millions (billions?) of dollars have been spent on technologies which so far have never kept the bad guys out. Sunk cost investment bias [0]. Past a certain point, even when the outcome is obviously futile, it becomes a mixture of accumulated momentum and pure bloody mindedness to \"build it if it kills us\". Companies like Microsoft or Sony have entire departments of people working on \"rights management\". Nobody has the courage to just say, \"Sorry guys, this is a fool's errand, we're going to shut it down and move you all onto something more productive\". [0] https://en.wikipedia.org/wiki/Sunk_cost reply devwastaken 17 hours agoparentprevNo. It is fundamentally impossible. DRM centralizes piracy, it makes it profitable both socially and financially to pirate harder. As DRM tries to get harder it actually gives pirates more power. These pirates release high quality content that is better than the service provides on most devices. Typically in HEVC as well, requiring less download size. It's also great for those that don't have consistent Internet and want to download over time. DRM and anti piracy are a snake oil industry for business suit types that think they're protecting their assets. They're not, but they don't understand the infinitely copiable nature of digital. They want control at any cost. reply charles_f 19 hours agoprevIf like me you don't know what Playready is: > PlayReady is a media file copy prevention technology from Microsoft that includes encryption, output prevention and digital rights management (DRM). It was announced in February 2007. reply squigz 18 hours agoprevAt some point this silly game of cat-and-mouse is going to escalate, and streaming players won't work unless your entire computer is locked down and \"verified\" by Microsoft or Apple. reply 015a 18 hours agoparentAt some point it escalates to where the media providers make watching their media so expensive, time consuming, and difficult that piracy ramps back up. It sounds dumb, like \"why would companies shoot themselves in the foot like this\" but trust that they will. They always do. Corpobrain is a form of autopilot, there's no one with intelligence in charge not because the people who work at media companies are dumb (though, they are), but because there's just literally no one in charge. Its autopilot. Each iterative decision in isolation makes sense, but when zoomed out and interpreted holistically they're killing their own business. reply tithe 14 hours agoparentprev> ...unless your entire computer is locked down and \"verified\"... This is exactly what the WEI (Web Environment Integrity)[0] specification sought to achieve, but at the browser level. [0] https://en.wikipedia.org/wiki/Web_Environment_Integrity reply surajrmal 12 hours agorootparentMost operating systems already offer this. At some point only native apps will be supported instead of the web if browsers don't also provide it. reply watermelon0 17 hours agoparentprevI think this is already the case today; streaming players don't work unless the whole chain from the player to the display is verified. The only reason it's possible to copy such content is because keys were leaked in the past, and they are not blacklisted. reply clwg 16 hours agoparentprevThat sounds an awful lot like an Xbox, and I personally don't think we're too far off from those becoming general purpose cloud connected DRM computers coupled with recurring monthly subscriptions for all your app/game/content needs. reply dawnerd 18 hours agoparentprevAnd yet content will still be torrented within hours. It’s always the honest consumers that lose. reply squigz 18 hours agorootparentThis assuredness that piracy will always win will be our demise. reply k8svet 15 hours agorootparentIt's like when that first Motorola came out with a locked bootloader, or maybe the second one, I think the first was trivially crackable. I remember that year, all of the people claiming it was just a matter of time. And nowadays, among other reasons, custom roms are largely dead because people want access to PayPal, Netflix and their banking app. It's grim. I hope to win the lottery and leave the industry before the term \"computer\" has lost all meaning. reply dawnerd 18 hours agorootparentprevThe only way to reduce piracy is to make access easier and cheaper - something the music industry figured out. Sure music still gets pirated but its a lot less. reply squigz 18 hours agorootparentWell, no, that isn't the only way to reduce piracy. Another way would be widespread collaboration between the largest tech corporations to lock down the pipeline from manufacturing to sale and onward If users continue to accept this path, which... they seem to, that is where we'll inevitably end up. reply dawnerd 17 hours agorootparentThat wont work. You can't tech your way out of this short of brain implants instead of screens. If there's a screen/speakers it's going to be pirated full stop. Games, okay that's a different story sure but they're already going down that path with online only games anyways. reply squigz 17 hours agorootparentBecause the idea of brain implants is so far-fetched? reply TheBicPen 8 hours agorootparentAs a requirement to watch Netflix? Yes, it is. reply utensil4778 16 hours agorootparentprevNo, that's where we are now. Not in the future, right now. It isn't working. You fundamentally can't prevent someone copying your file. It isn't possible, full stop. You can only make it maximally inconvenient. You can't encrypt a user's eyeballs, so the media has to be transmitted in the clear at some level. Be it intercepting the LVDS signal to your TV panel or just pointing a camcorder at the screen. The current tact is to just make it maximally inconvenient for anyone to access the file in any way. This does not consider the asymmetry in effort required. All legitimate users must deal with shitty DRM systems and broken apps, where it takes exactly one pirate to go through the effort of making a copy. Then everyone else who obtains a copy has to expend zero effort to consume the media. Piracy is simply easier, which is why there's a resurgence now. The only sustainable option is to make legitimate consumption easier than piracy. For a lot of media, piracy is the only option to obtain a copy that will not vanish at some indeterminate point in the future. even if you paid for it. Companies think that they can just make piracy harder, but that simply doesn't work. Once the first copy is made, the game is over. As established, there's simply no way to truly and permanently prevent a copy being created. That's simply the nature of digital media. At best, you can slow pirates down, you can never stop them. Piracy will never go away, and people need to accept that. People have been selling bootleg copies of goods since the dawn of time, there's no way to prevent it. There will always be someone nabbing copies of movies and sharing the files. You can either waste everyone's time by trying to fight it, or you can realize that companies need to compete to survive, not just be large. If you compete with the pirates and produce a better product that people want more, well that's what capitalism is all about, isn't it? reply int_19h 12 hours agorootparent> This does not consider the asymmetry in effort required. All legitimate users must deal with shitty DRM systems and broken apps Oh, they do consider it. But, upon consideration, they decide that they don't care. reply squigz 15 hours agorootparentprevI wish I shared your certainty. I certainly don't share your faith in capitalism to solve anything. reply utensil4778 14 hours agorootparentOh, don't get me wrong, I have zero faith in capitalism. After all, that's the entire reason we're in this situation. However, market forces are actually very real. They just don't work the way capitalists think they do. Or rather, capitalists are convinced they can control the market through technology. Unfortunately for them, this is a technology that can't be solved or controlled. reply sspiff 17 hours agorootparentprevWhat about the analog loophole? At some point, the data needs to be manifested in the real world. reply squigz 17 hours agorootparentIt's not as though there's no effort to close this loophole (see HDCP and probably others) - I don't expect them to give up any time soon Granted, pointing a camera at a screen and recording will always be possible - but I say if we ever reach the point where that is the only option, we've lost. reply int_19h 12 hours agorootparentIt is not a given that this will always be possible. I could imagine some kind of steganographic watermark in videos - diffused over the entire signal so that it cannot be easily cropped out - combined with a check for the same in all recording equipment that blocks the recording or blacks out the area if detected. Could be done \"voluntarily\" by all large manufacturers for starters, then eventually mandated by law for all equipment sold or imported into the country. And there's already precedent for this kind of thing: the way copiers block money bills as source. reply stockboss 17 hours agorootparentprevi suspect one factor is that music is a lot cheaper to produce than movies, so selling music at an \"accessible\" price is a lot more viable as a solution. plus, there's a larger market for music since music is largely consumed in isolation. people tend to listen to music themselves so they would either buy a copy for themselves, or stream for themselves, so there's the benefit of volume as well. on the other hand, movies are more likely to be consumed in groups - a group of people watching one movie will only pay once. for the tv/movie industry, the best solution we have right now is basically streaming services like netflix. the issue is that its probably still not economically feasible for companies like netflix to pay for the streaming rights of new movies for their subscribers, especially those big budget movies. so for those, either you'd have to wait until the price is more palatable for netflix, or you'd have to just pirate it. reply Xerox9213 16 hours agorootparentprevWhose demise? Has there ever been a time where piracy hasn't \"won\"? reply earth-adventure 19 hours agoprevSo this is pretty much about breaking the client side DRM, with a bad side effect of abusing someone else's Identity (as used within the DRM context) for nefarious purposes. Did I understand this correctly? reply xurukefi 19 hours agoparentThe \"client\" whose \"identity\" is abused here is not an end user. A \"client\" in this context is a program or library that talks to the license servers and receives the content decryption keys. On my Windows machine I see a \"Windows.Media.Protection.PlayReady.dll\", which I guess is the client that they cracked. Maybe there are also other clients that are widely accepted by license servers. The attack essentially means that they could write a program themselves that acts as \"Windows.Media.Protection.PlayReady.dll\" to get decryption keys from a server. What will happen now is that Microsoft will deprecate the client and release a new one with new obfuscation and new keys. The license servers will start rejecting the old cracked client. And then people will crack the new client. And the cycle continues. reply qingcharles 14 hours agorootparentDoes PlayReady now require a secure enclave/TPM on your PC? Otherwise as you say, the only thing protecting the keys is obfuscation. This has been the same way all the way back to the first Microsoft DRMv1 in 1998 (?). The decryption keys have to be stored on your device so you can play your media or your game. So, the level of encryption is totally moot. The level of obfuscation is all that really protects the content. reply xurukefi 13 hours agorootparentWith PlayReady, as with any other DRM scheme really, there are different tiers. There is SL2000, which is done completely in software (whitebox crypto), and there is SL3000, which does require a TEE. Which tier is requried for which type of content is driven by streaming provider or studio requirements. I think it is pretty common to allow content up to 1080p to be used with whitebox crypto, whereas 4k+ content will require hardware DRM. reply nonrandomstring 19 hours agorootparentprev> The \"client\" whose \"identity\" is abused here is not an end user. A \"client\" in this context is a program or library that talks to the license servers and receives Thanks for the clarification. Otherwise people would be worried about being targeted and having \"personal\" keys tied to a financial account or online identity getting sold and used by others to access arbitary content. This seems kinda good news for concerned users, but even worse news for Microsoft. reply repelsteeltje 19 hours agoparentprevYup. Basically the means to forge an authenticated cookie. [Update] It's a bit more subtle: Having the keys to forge a license request and decrypt server response allows you to emmulate or re-implement a DRM client. Because the server is oblivious to this fake, it will respond as though it's taking to a genuine \"secure\" client thereby ultimately exposing the content decryption key. reply zeta0134 19 hours agoprev> In that context, this is vendor’s responsibility to constantly increase the bar and with the use of all available technological means. Or the vendor could just let me consume the content I paid for in whatever player I like. Which is what happens anyway, as this sort of DRM is always breakable. If the media consumer can view the content at all, they can simply record that output and re-encode in a more convenient storage format. reply repelsteeltje 18 hours agoparentYes, there is always the analogue loophole. And opening cryptography toolbox to control how users consume content is a lost cause. Crypto can only protect contents from adversaries that don't have the key. But here the paying user is the adversary and the only way the DRM can paint the video on screen is through that key. So DRM boils down to security through obscurity. Turns out obscurity is hard, expensive and never works very well. reply Jerrrry 19 hours agoprevGiven how horribly all major companies, MS most certainly included, confuse authentication vs. authorization, this is almost certainly able to be paired with a 'vulnerable' (all) endpoint to retrieve/post/update player information. The horizontal pivot from DRM/crypto-managed Identity to a session token, an unassumingly-kosher redirect, or just omitting the \"AUTHENTICATION\" header itself is a trivial exercise for the common script kiddie. This is how exploit chains get a foot-hold, and \"secure\" accounts get compromised like it was 2010 again. reply amaccuish 18 hours agoparentI don't understand a word you've said. reply Jerrrry 15 hours agorootparentFind an endpoint that checks the validity of the DRM token they have broken. See if that endpoint just hinges on that DRM token, since its crypto-secure, why check any other fields? Spoof other fields. 10k+ 0-day exploit. reply remram 9 hours agorootparentStill no go. Can you make a sentence like \"Microsoft will ...\" or \"a problem is that ...\" or something? reply nonrandomstring 19 hours agoparentprevAnd it paints an even bigger target on domestic Windows machines used for media content. Who wants to \"steal\" their _own_ keys? Microsoft's broken DRM scheme creates objects of value which it then tries to store on the client's machine deliberately beyond the owners control and security management. It is adversarial to the user. This is clearly a no-win situation... hence the snarky sign-off about vendors \"raising the bar\", basically saying; Good luck with that! It really seems quite unhinged. So now there is collateral damage: - A motive to hack Windows machines to steal content keys. - A misuse of \"identities\" through a market in stolen keys - Pivots (as parent says) to other malware vectors So, predictably, because of DRM, Microsoft Windows is now an even more dangerous and insecure system. Why do people persist chasing this unnecessary, pathologically involuted technological misadventure? Surely \"controlling and monitoring peoples content\" is not a hill worth dying on? reply Jerrrry 15 hours agorootparentI'd agree, but licensed content can be revoked - MS is pretty good at publishing digests of \"known-compromised\" ID's/Serials/Private Keys. I'd be more concerned about any other, more important facets of a user's account/assets/property that assumes the DRM is secure, and leans on that. reply logical_person 19 hours agoprev [–] do software cracks usually get posted to seclists? this is expected in the design of DRM... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Security Explorations identified two attack scenarios enabling the extraction of private ECC keys in Microsoft PlayReady clients.",
      "Compromising these keys could allow mimicking a PlayReady client outside Protected Media Path, risking content key exposure on Windows.",
      "Emphasizes the need for continual security enhancements to safeguard against vulnerabilities in software-based content protection systems such as PlayReady."
    ],
    "commentSummary": [
      "The article delves into the vulnerability of Microsoft PlayReady, focusing on compromised client identity, particularly in safeguarding premium video content from piracy.",
      "It explores the hurdles of DRM strategies, encryption, and tools like Widevine, dissecting the ethical considerations and constraints of DRM in the fight against piracy.",
      "The discussion emphasizes the continuous discourse on DRM efficacy, its influence on user liberties, and the importance of seeking new approaches to counter piracy in today's digital era."
    ],
    "points": 181,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1715262786
  },
  {
    "id": 40307832,
    "title": "Exponential Data Needed for Multimodal \"Zero-Shot\" Generalization",
    "originLink": "https://arxiv.org/abs/2404.04125",
    "originBody": "Computer Science > Computer Vision and Pattern Recognition arXiv:2404.04125 (cs) [Submitted on 4 Apr 2024 (v1), last revised 8 Apr 2024 (this version, v2)] Title:No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance Authors:Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H.S. Torr, Adel Bibi, Samuel Albanie, Matthias Bethge View PDF HTML (experimental) Abstract:Web-crawled pretraining datasets underlie the impressive \"zero-shot\" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of \"zero-shot\" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during \"zero-shot\" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting \"zero-shot\" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream \"zero-shot\" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the \"Let it Wag!\" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to \"zero-shot\" generalization capabilities under large-scale training paradigms remains to be found. Comments: Extended version of the short paper accepted at DPFM, ICLR'24 Subjects: Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Machine Learning (cs.LG) Cite as: arXiv:2404.04125 [cs.CV](or arXiv:2404.04125v2 [cs.CV] for this version)https://doi.org/10.48550/arXiv.2404.04125 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Vishaal Udandarao [view email] [v1] Thu, 4 Apr 2024 17:58:02 UTC (37,862 KB) [v2] Mon, 8 Apr 2024 21:14:43 UTC (37,863 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CVnewrecent2404 Change to browse by: cs cs.CL cs.LG References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40307832",
    "commentBody": "No \"Zero-Shot\" Without Exponential Data (arxiv.org)171 points by zerojames 20 hours agohidepastfavorite101 comments godelski 16 hours agoI've always been rather upset that it's fairly common to train on things like LAION or COCO and then \"zero shot\" test on ImageNet. Zero shot doesn't mean a held out set, it means disjoint classes. You can't train on all the animals in the zoo with sentences and then be surprised your model knows zebras. You need to train on horses and test on zebras. reply loandbehold 16 hours agoparentHow would the model know what zebra was if it had never seen it? Same is true for humans. reply xboxnolifes 15 hours agorootparentWhen I was little, zebras were described to me as black and white stripped horses. Without even seeing one, I'm sure anything who has seen a horse could then merge those two concepts to create a close to accurate picture of what a zebra is. If AI is supposed to resemble a human mind with ability to learn, then it must be able to learn from a blanker slate. You don't teach the human before it is born, and in this comparison an AI is born when you finish it's model and set its weights using the training set. If you test it with the training set, you aren't testing ability to comprehend, just regurgitate what it was born with reply salty_biscuits 13 hours agorootparentYou can look at a medieval bestiary to see how people thought animals might look based on descriptions alone. Like these lovely elephants https://britishlibrary.typepad.co.uk/digitisedmanuscripts/20... reply godelski 11 hours agorootparentHonestly, these are some of my favorite stories and I think more ML people need to learn more about mythology (I say this as a ML researcher btw). Because once you go down this path you start to understand how \"Rino\" == \"Unicorn\". You have to really think about how to explain things when you're working with a limited language. Yeah, we have the word \"rino\" now, but how would you describe one to someone who has no concept of this? Maybe a cow with one big horn? Is \"like a big fat tough skin horse with a big horn coming out of its head\" accurate? And then apply your classic game of telephone[0]. It is also how you get things like how in Chinese a giraffe is \"long neck deer\"[1] (that doesn't work for all things in Chinese and there's another game of telephone (lol, maybe I was too harsh on the British in [0]) and well... you can imagine things get warped like crazy). There's so many rabbit holes to go down when trying to understand language, vision, reasoning, and all that stuff. [0] (Jesus England... this is what you call this game?!) https://en.wikipedia.org/wiki/Chinese_whispers [1] https://translate.google.com/?sl=en&tl=zh-CN&text=giraffe&op... ----> https://translate.google.com/?sl=zh-CN&tl=en&text=%E9%95%BF%... reply dwallin 13 hours agorootparentprevIf you trained an image generator, removing all instances of zebras from the training set, you could ask it to output images of a black and white striped horse and it would likely succeed. Then you could fine tune an image recognition model (also with all zebras removed from the training set) on the generated image set to associate it with the word zebra. If you then showed it a bunch of images of actual zebras there’s a really good chance it would succeed. reply godelski 11 hours agorootparentprevGreat question! It depends on the zero-shot experiment. Let's look at two simple examples Example 1: We train a classifier that classifies several animals (and maybe other things). For example, you can use the classic CIFAR-10 dataset which has labels: airplane, automobile, bird, cat, deer, dog, frog, __horse__, ship, truck. The reason I underlined horse is because you want your model to classify the zebras as horses! The reason this is useful is for measuring the ability to generalize. At least in our human thinking framework we'd place a zebra in that bin because it is the most similar (and deer should be the most common \"error\"). This can help us understand the network and we'll be pretty certain that the network is learning the key concepts of a horse when trying to classify horses rather than things like textures, colors, or background elements. If it frequently picks ships your network is probably focusing on textures (IIRC CIFAR has ships with the Dazzle Camo[0] and that's why I threw \"ship\" out there). Example 2: Let's say we train our network on __text__. In this case it can get any description of a zebra that it wants. In fact, you'd probably want to have a description of what it looks like! The what we might do is take that trained text network, and attach it to a vision classifier. For simplicity, let's say that was trained on CIFAR-10 again. We then tune our LM + CV model so that it can match the labels of CIFAR-10 (basically you're tuning to ensure the networks build a communication path, otherwise it won't work). Here we end up testing our model's actual understanding of the zebra concept. It again should pick horse as the likely class because you've presumably had in the training text some description that compares zebras to horses. ----- So really the framework of zero-shot (and few-shot) is a bit different. We're actually more concerned about clustering and you should treat them more similar to clustering algorithms. n-shot frameworks really come from the subfield of metalearning (focusing on learning how networks learn). But as you can imagine, these concepts are pretty abstract, but hey, so are humans (that's why we see a log as a chair and will situationally classify it as such, but let's save the discussion of embodiment for another time). In either example I think you can probably see how a toddler could do similar tasks. You can ask which of those things the zebra is most similar to and you'd be testing the toddler's visual reasoning. The text one might need be a little older but it could be a great way to test a child's reading comprehension. Does this make sense? Of course machines are different and we need to be careful with these analyses (which is why I rage against just comparing scores/benchmarks, these mean very little), because the machines may be seeing and interpreting things differently than us. So really the desired outcome depends on if you're testing for what the machine knows/understands (you need to do way more than what we discussed above) or if you are training a machine to think more similar to a human (then we can rely pretty close to exactly what we discussed). Hope this makes more sense. [0] https://en.wikipedia.org/wiki/Dazzle_camouflage reply red75prime 15 hours agoparentprevShould we also try to teach children geometry and test them on calculus? reply nico 15 hours agorootparentThis illustrates two ways of teaching I’ve experienced both, each at a different university In one, professors would teach one thing then ask very different (and much harder) questions on tests In the other, tests were more of a recap of the material up to that point I definitely learned a lot more in the second case and was a lot more motivated. It also required more effort from the professors The two methods also test different things. The recap one tests effort and dedication, if you do the work, you get the grade The difficult tests measure either luck and/or creativity and problem solving under pressure. It’s not about doing the work, it’s about either being lucky or good at testing reply Pet_Ant 13 hours agorootparent> it’s about either being lucky or good at testing I think you are misunderstanding the experience. The first (harder questions) is testing your understanding of the material and problem. Can you applying the material to solve a novel problem? Do you understand the material not just the mechanics. Do you understand how it would interelate it with other problems? Do you understand the limitations? The second is just regurgitation. This is great for rote skills, but this isn't really learning. This is grinding until you can reproduce. These are the kinds of skills that are easily automated. This is not what we should be testing our kids. reply godelski 11 hours agorootparentFwiw, I think both of you are on the same page. And yes, to bring back to ML it is the difference of generalization and memorization (compression). I wrote a longer response to a different response to my initial comment to help clarify because I think this chain is a bit obtuse and aggressive for no reason :/ (I mean you can check the Wiki page to verify what I said) reply IIAOPSW 15 hours agorootparentprevFor centuries we only taught children geometry and one of them invented calculus. reply Filligree 14 hours agorootparentNow that’s setting a high bar. If AI could reliably invent calculus, then I’d be briefly impressed and then terrified. reply eru 8 hours agorootparentOne AI in a couple hundred years might be able to do it by luck? reply feoren 15 hours agorootparentprevI think the authors are responding to a claim that AI is doing this: look, we taught them geometry, and now they know calculus! GP is saying that it's not a true zero-shot to have a separate test and training set, because the classes overlap. Similarly, the authors are saying \"true zero-shot\" is basically not happening, at least not nearly to the extent some are claiming. So everyone here, including TFA, are all kinda doubting the same claim (our AI models can perform zero-shot generalizations) in different ways, I think? reply godelski 11 hours agorootparentprevI think you're being overly obtuse, and I'd request you try to work in good faith. If you doubt what I claimed, you can quickly verify on the wiki page[0]. In essence you aren't wrong, but that's not what we'd typically do in a zero (or few) shot setting. We'd be focusing on things that are more similar. If you want to understand this a bit better in what we might do in a ML context I wrote more here[1]. And I like Nico's comment about how different professors test. Because it makes you think about what is actually being tested. Are you being tested on memorization or generalization? You can argue both these kinds of tests are testing \"if you learned the material\" but we'd understand that these two types of tests are fundamentally different and let's be real, are not reasonably fair to compare scores to. I'm sure many of us have experienced this where someone that gets a C in professor A's class likely learned more than than someone who got an A in professor B's class. The thing is that the nuance is incredibly important here to really understand. And you can trivialize anything, but be careful when doing so, you may overlook the most important things ;) Now... you could make this argument about geometry -> calculus if we're not talking about the typical geometry (single) class most people will have taken in either middle school or high school. Because yes, at the end of the day there is a geometric interpretation and we have the Riemann sum. But we'd need to ensure those kids have the understanding of infinities (which aren't numbers btw). We'd have to be pretty careful about formulating this kind of test if we're going to take away any useful information from it. Though the naive version might give us clues about information leakage (in our case with children this might be \"child who has a parent that's a mathematician\" or something along those lines). It really all depends on what the question behind the test is. Scores only mean things when we have nuanced clear understandings of what we're measuring (so again, tread carefully because \"here be dragons\" and you're likely to get burned before, or even without, knowing it) And truth be told, we actually do this a bit. There's a reason you take geometry before calculus. Because the skills build up. But you're right that they don't generalize. [0] https://en.wikipedia.org/wiki/Zero-shot_learning [1] https://news.ycombinator.com/item?id=40313501 reply stephc_int13 18 hours agoprevQuite a few people saw this coming. It is still early to tell if we reached AI winter again or not, but at least we can see that news are slowing down. reply loudmax 17 hours agoparentI think that depends what your expectations are, and what you mean by another AI winter. We're just scratching the surface of what's possible with the current state of the art. Even if there are no major advances or breakthroughs in the near future, LLMs and associated technologies are already useful in many use cases. Or close enough to useful where engineering rather than science will be sufficient to overcome many (though not all) of the shortcomings of current AI models. Never mind grandiose claims about AGI, there's enough utility to be gotten out of the limited LLMs that we have today to keep engineers and entrepreneurs busy for years to come. reply swatcoder 17 hours agorootparentYou're right, and I suspect the GP would agree with you about there being real engineering applications for LLMs, diffusion models, etc But I think the term \"AI Winter\" usually refers to the underlying research programme and the economics around it. Soaking up many billions of dollars of industry money and public grants on the pitch that AGI might be just around the corner, and then being unable to deliver on that pitch can induce a hangover effect that makes it much harder to raise money for anything that smells even a little bit like the failed pitch. Investors and administrators feel burnt and turn very skeptical for a very long time. Meanwhile, the actual productive applications which shook out of the initial boom just get renamed to something else so that they don't carry that smell. We'll see how it goes here, but that's the familiar road and where the terminology comes from. reply ijustlovemath 15 hours agorootparentThe minor irony of this is that current efforts at AGI are focused on the data scaling laws, which we have showed no signs of slowing on, but if funding dries up these crazy expensive training runs wont be allowed anymore. reply marcosdumay 16 hours agorootparentprev\"AI winter\" was a political phenomenon that happened because the AI applications didn't hold water to the hype, and all of the gullible people that invested on them got burned out by the disparity. We are very clearly on that same path again. What leads to the conclusion that another winter is coming. But even the fact that people are talking about it is evidence it's not here yet, and as always with political phenomena, there's no guarantee history will repeat. Anyway, none of it means people will stop applying their knowledge or studying AI. The entire thing happens on funding and PR, and the world is not entirely controlled by those two. reply dr_dshiv 15 hours agorootparentExcept contemporary AI is useful on a day to day basis. reply loftyal 37 minutes agorootparentSo were the developments before the last AI winter.. reply xboxnolifes 15 hours agorootparentprevBeing useful doesn't mean it met expectations. reply nicklecompte 16 hours agorootparentprevThe whole point of past AI winters is that genuinely useful technologies were preposterously overhyped as representing \"human cognition,\" despite overwhelming evidence to the contrary. The bubble bursts - and the money evaporates - because expectations come crashing down, not because the usefulness was a mirage. Lisp was hardly the key to human symbolic thought like some hoped it would be, but it helped improve programming languages across the board. Perceptrons really are quite limited and there's absolutely no way you could emulate a human brain with them alone, but they're still essential for more advanced ML. But you would think after 70 years AI practitioners would learn some humility! It is very obvious that GPT-4 is dumber than a honeybee, let alone a cat, let alone a crow. But for over a year I've heard dozens of tech folks insist it's smarter than most people. reply eru 8 hours agorootparent> But you would think after 70 years AI practitioners would learn some humility! Why? The people doing AI now are not the same as those that did AI 60 years ago. reply godelski 15 hours agoparentprevI think we're in the beginning of a warm winter. The funding is there but we're letting hype drive everything and not calling out the con artists. The problem is we've had recent great success but still don't know how to get to AGI. But because we're afraid of winter we're not willing to try new things. We want to only compare to sota and think it's fair to compare a new method with a handful of papers to the status quo. That's not how the S-curves of technology work. Sure, maybe things don't scale but that doesn't mean they don't have merit or can't scale if someone finds some modification. The problem is we treat research like products, not academic work. You need to produce everything from hard core theory to robust products to have an effective chain. But we seem hyper focused on the middle area. And for some reason people think products can be placing a nice interface around research code. There's still a lot of work you need to do and all those models can be optimized. They absolutely do not have optimal hyper parameters or even parameters. reply msp26 17 hours agoparentprevI hope this means that people actually start curating their training sets. The quality control is horrible on all of the datasets I've looked through. Especially image captions. Yes sheer quantity has a quality of its own but that won't produce optimal results. reply RodgerTheGreat 16 hours agorootparentFrom a very cynical marketing/VC-pitch perspective, un-curated datasets full of random crap have the benefit of sometimes producing totally surprising, unplanned \"features\", which helps sell the idea that one-size-fits-all black-box machine learning can solve any problem. reply ertgbnm 11 hours agoparentprevI'm not even going to discuss whether or not things are actually slowing down since I disagree with that. But I do feel confident that an AI winter is not on the horizon solely due to the overhead of implementation that we currently have. Just with currently existing AI, it would take years for the economy to fully leverage the abilities that are available. I'm confident we have transformative AI. So it won't feel like a winter for several years as we actually succeed in optimizing, implementing, and productizing current technology in other industries. reply pas 18 hours agoparentprevRAG seems to be all the rage. Not to mention the quest for the cooking up the correct cocktail of smaller MoE/ensemble models, and ... there's decades' worth of optimization work ahead (a few years of it seems to be already VC and edu grants funded), no? reply PheonixPharts 16 hours agorootparentThere's tons of optimization work left, but by it's nature optimization tends to push the limits of what we currently have, and rarely allows for substantial improvements. There are some major limitations to LLMs that aren't going to be \"optimized\" away. At the end of the day LLMs are Monte Carlo samplers over a latent, compressed representation existing human text data. Many of the tasks people hope LLMs will achieve require major leaps in our current understanding of language modeling. One great example limitation (which shocks me sometimes when I think about it): generating output is still ultimately stuck in looking at the probability of the next token rather than the much more useful, probability of the generated statement. There are techniques to improve this, but we're missing a major piece of generating highly probable statements with no hint about how to really get there. Consider how you might write SQL. You conceive of the high level query first and start sketching out the pieces. An LLM can only look at each token and can't, statistically speaking, think in terms of the entire query. Personally I think LLMs are very underutilized/exploited for what they are good at, and there is way too much focus on what they can't do. Hopefully we'll dodge an AI winter by using LLMs to solve the wide range of classical NLP problems that make many tasks that just a few years ago nearly impossible, rather simple today. Unfortunately the irrational hype around these models makes me skeptical of that scenario. reply mjburgess 13 hours agorootparentThe issue with predicting the next statement is the combinatorial infinity which is improbable to model with historical frequencies, ie., P(mat|the cat sat on the) is a distribution over, say, 100k words. Whereas, P(the cat sat|on the mat) is a distribution over 100k^3 words. Part of the illusion of an LLM is that we produce create text in such a highly regular way that a mere distribution over 100k iterated, say 500 times, gives you a page of text as if modelling 100k^500. reply nkozyra 17 hours agorootparentprevRAG takes the current limit of LLM and focuses it on specific problems using custom data. It's not exactly magic, it just finds a way to produce something tangible and usable from an otherwise broadly focused model. It's the rage because it's a way to practically _do work_ from LLMs that generally provide wow from conversationally accurate, often factually accurate responses. reply cs702 18 hours agorootparentprevI think the grandparent comment is about AI research, driven by the quest for AGI. Incremental improvement of proven approaches, driven by profit motive, will surely continue regardless of whether there is an AI winter or not. reply darepublic 15 hours agoparentprevDunno exactly what you mean by ai winter, but the whole gen AI thing has kind of got all the attention and even if that route slows down there are a ton of other branches fruitful for development. reply stephc_int13 15 hours agorootparentThe issue, I think, is that if results don't follow expectations, given the very high costs, investors might get cold feet. Sure we already have some interesting applications, but they are not exactly printing money. reply cs702 18 hours agoparentprevIf there are no breakthroughs and funding dries up in the near future, it will feel to many like going off a precipice at high speed. Only the poor souls who survive the fall, at the very bottom of the precipice, will get to experience the AI winter. reply trgn 14 hours agoparentprevCommercial adoption has only started to ramp up. We are going to see a floodwave of LLM-powered bots/UX-wizards. Even if academic progress stalls, we're going to be inundated for at least another few years. reply CuriouslyC 15 hours agoparentprevDefinitely not an AI winter, since the tools are so useful. People who think AGI is right around the corner might be disappointed though, since we can't really even accurately define AGI. reply squigz 17 hours agoparentprev> reached AI winter again Again? reply swatcoder 17 hours agorootparentThis was not the technology industry's first encounter with AI hype. The term was coined 40 years ago, and has been suggested as a description for almost a dozen periods in the field's history: https://en.wikipedia.org/wiki/AI_winter reply pocketarc 17 hours agorootparentprevIt's a term for when AI hype dries up, has happened multiple times since the 60s[0]. [0]: https://en.wikipedia.org/wiki/AI_winter reply refulgentis 13 hours agoparentprevUh, what? reply hprotagonist 18 hours agoprevMy long-standing observation has been that while nature may abhor a vacuum, she also really, really loves sigmoids. That performance vs. training data is not linear, but logarithmic, doesn't exactly come as a surprise. reply CuriouslyC 14 hours agoparentThe question that is unanswered, is the logarithmic performance improvement the result of better sampling of the underlying distribution over time, or related to just doing more training with slight variations to effectively regularize the model so it generalizes better? If it's the former, that indicates that we could achieve small models that are every bit as smart as large ones in limited domains, and if that's the case, it radically changes the landscape of what an optimal model architecture is. I suspect from the success of Phi3 that it is in fact the former. reply bilsbie 17 hours agoparentprevEvery exponential is really an s curve? reply hprotagonist 17 hours agorootparentin physical systems, it’s very often the case! reply mr_mitm 16 hours agorootparentPretty much always. One exception might be the expansion of the universe reply jskherman 16 hours agorootparentprevSymmetries in Nature strike again! It's just like in Noether's theorem. reply eru 8 hours agorootparentNoether's theorem has nothing to do with S-curves. reply ixaxaar 14 hours agoparentprevThe physical world has limits, that's why sigmoids everywhere. reply crote 18 hours agoprevThis feels like the worst possible outcome of the current AI hype. We've essentially been ripping off the entire internet and feeding it to the models already, spending many billions of dollars in the process. It's pretty much the largest possible dataset you can currently get, and due to the ever-increasing and now rapidly accelerated AI poisoning of the internet most likely the largest possible dataset which will ever exist. All that and all we're getting out of it is not-entirely-useless but still quite crappy AI? We would've been better off if we had never done this. reply panarky 17 hours agoparent> not-entirely-useless but still quite crappy 15 months ago, general-purpose LLMs that have not been specifically trained on legal reasoning could score better than 90% of humans on the multistate bar exam, and these are humans who actually completed law school. General-purpose LLMs get similar results in medicine, and when the models are fine-tuned for medical diagnosis they're even better. And that was more than a year ago. Those who have seen current models not yet released tell us they'll make the current state of the art look like toys. Progress is still tracking the steep part of the S-curve, and there's no indication that they're near the top yet. reply Imnimo 15 hours agorootparentI think this says more about the benchmark than the capabilities of the model. If it were the case that 90th percentile performance on the bar exam mean that a model was a 90th percentile lawyer, and we've had these models for 15 months (in fact longer), where are all the LLM lawyers? The lesson here is that a test designed for humans may not be equally representative of capabilities when given to an LLM. reply crote 15 hours agorootparentprev> Progress is still tracking the steep part of the S-curve, and there's no indication that they're near the top yet. If I understand it correctly, that seems to be exactly what this paper is suggesting. Scoring high on the bar exam is pretty trivial for AI - the data needed for that is fairly generic and widely available on the internet. It requires you to demonstrate a relatively basic understanding of the concepts by answering a bunch of multiple-choice answers. If anything, I'd expect AI to have a perfect score. Like I said, such an AI is not entirely useless. You can replace quite a few legal assistants with that, and I bet it could be used to create first drafts or to expand a core concept into a full legal argument. There is plenty of money to be made there, and it's going to make an awful lot of people jobless. But that's just replacing more-trivial jobs with automation, it doesn't add anything novel to society. On the other hand, the actual difficult work involves being able to come up with completely novel concepts, and being able to expand upon some obscure but crucial stuff few people have ever heard about. Current models simply aren't capable of that, and the results achieved here with multimodal models suggests that they never will. We risk getting stuck with models which can do some trivial work, but silently produce complete garbage when you ask them to do anything providing substantial value. reply flimsypremise 17 hours agorootparentprevIt's not super surprising that LLMs perform well on standardized tests, given that they have a lot of standardized test related text in their training data. There are a lot of claims out there about the zero-shot ability of LLMs, and very little specific research to back it up. Until now that is. reply MacsHeadroom 17 hours agorootparentThis paper is about CLIP not LLMs and does not generalize to LLM architectures. reply elicksaur 10 hours agorootparentWhy not? reply elicksaur 10 hours agorootparentprev> 15 months ago, general-purpose LLMs that have not been specifically trained on legal reasoning could score better than 90% of humans on the multistate bar exam The claims of similar performance on coding problems were shown to be due to contamination of the training data on the tested problems. It did abysmal on problems made public after the model training cutoff. I don’t think anyone has tested contamination for the MBE claims, but I would lean toward assuming the same issue exists for that assessment until proven otherwise. reply coredog64 7 hours agorootparentYou can see similar (poor) results if you give it a slightly tweaked puzzle that it’s seen before. The most recent example on Twitter was the farmer with the animals and the boat. If there are no constraints, it will give you an answer based on the tricks for the original unless you harass it. reply some_random 17 hours agoparentprevQuite crappy? It seems to me like the current SOTA is working plenty good enough for most use cases and like the highest impact (practical) way to improve right now is going to be advancements in domain knowledge acquisition and retention. reply crote 15 hours agorootparentI sure haven't seen any of those \"plenty good\" results yet - current SOTA seems to be about as useful as semi-coherently gluing together random Google results. Good enough to perhaps replace a minimum-wage worker, not good enough to provide actual value when you care about the quality of the result. This paper seems to suggest that significant advancement in domain knowledge acquisition and retention is exactly the problem, as you seem to need exponentially more data due to a lack of generalization. What's the point of a model which can perfectly quote Shakespeare if you're a programmer trying to refactor a proprietary codebase and it fails to make a link to whatever garbage it picked up from StackOverflow? reply Filligree 14 hours agorootparentIn CLIP. Replacing CLIP with an LLM is the current meta in image generation models, specifically because of that lack of generalisation. This isn’t a surprise to anyone. reply greenavocado 17 hours agoparentprevI'm getting a heck of a lot of useful work out of these so-called useless models reply HarHarVeryFunny 15 hours agoparentprevEven if LLMs (pre-trained transformers) turn out to be a dead end as far as AGI goes, there are productivity applications for them, and perhaps just as importantly interesting insights/confirmations about how the mind works and directions for future AGI research/architectures. The use cases for LLMs will no doubt grow as hallucinations are reduced, and they gain planning/reasoning ability over next couple of years. It'll be interesting to see what they subjectively \"feel\" like with these improvements. reply crote 15 hours agorootparentOh absolutely, LLMs are already causing a slaughter in applications where quality doesn't really matter to the company, like customer service. With minor improvements they're going to be a serious problem for any junior developer/lawyer/journalist/reviewer/artist/whatever, and if they ever fix the hallucinations issue it's game-changing. On the other hand, it's still a big \"if\" whether a general hallucinations solution exists, and in the meantime we're paying a pretty high price for it, as the entire internet is being flooded with absolute garbage. We risk getting stuck in a situation where there is no way to get new senior people because nobody hired them as juniors because AI is cheaper and good enough, and senior people are getting less and less productive due to them being unable to use websites like StackOverflow as reference material. That's a pretty high price for a tiny gain, if you ask me. reply Art9681 6 hours agorootparentOne less potential competitor employee we have to worry about in the future economy. Thank you for your service. reply int_19h 9 hours agoparentprevThe training sets used for current models, even the largest ones, is nowhere even close to \"the entire Internet\". Some napkin math based on known sizes of public datasets says it's less than 1%. reply eru 8 hours agoparentprev> It's pretty much the largest possible dataset you can currently get, [...] No. What you describe encompasses only one poor modality: text. We have oodles more data, and can create almost arbitrary amounts more, by just eg pointing webcams at the world. > We would've been better off if we had never done this. Who is 'we'? reply bearjaws 18 hours agoprevComputerphile just did a video on this and it's a pretty good summary: https://www.youtube.com/watch?v=dDUC-LqVrPU reply RhysU 18 hours agoprevCan one upweight the known-rare concepts in the training set? But then which are rare and how should we identify the rare ones worth knowing? ML is funny. It's predictably going to run into the Education field. reply shenberg 17 hours agoprevThe CLIP plot (Fig. 2) is damning, however some of the generative models show flat responses in Fig. 3 (e.g. Adobe GigaGAN, DALL-E-mini). While those are on the one hand technically linear relationships, but are also exactly what we'd want: image generation aesthetic score that doesn't care about concept frequency. Maybe the issue is with the contrastive training target used in CLIP? reply nkozyra 17 hours agoprevMy biggest worry is the idea of generating data as training data. We're obviously already unwittingly doing this, but once someone decides to augment low-volume segments of the dataset with generative input, we're going to start getting some really crappy feedback loops. reply MeImCounting 17 hours agoparentWe are already doing this. In fact for the next generation of frontier models the primary electricity cost is running inference to generate training data for these models. As Llama 3 has shown us scale of training data is more important than size of model. reply bilsbie 17 hours agoparentprevDo you ever have novel ideas while walking or in the shower? Well, you’re learning from data you generate. reply vineyardlabs 15 hours agorootparenthttps://arxiv.org/pdf/2305.17493 There's some cursory indication that in the long tail, training LLMs on LLM-generated data causes model collapse. Kind of like how if you photocopy a photocopy too many times the document becomes unreadable. This isn't really surprising though. Neural networks at large are a form of lossy compression. You can't do lossy compression on artifacts recovered from lossy compression too many times. The losses stack. reply nkozyra 17 hours agorootparentprev> Well, you’re learning from data you generate. Sure. I'm producing human output from human input in a generally unconstrained, limitless way. This is producing approximated human output from approximated human input. That second level of abstraction will be constrained, ultimately, by the limits of input. reply sottol 17 hours agorootparentprevI don't necessarily think it is the equivalent. Maybe it's more akin to a high-schooler reading a single book and then being asked to write several new books that hold equal weight in teaching the next generation of children. These new text books could be great at simplifying the subject matter and making the material accessible or they may just never have fully understood the materials and are misleading. Now imagine that over and over again, imo it's pretty likely to introduce inaccuracies if just taking a naiive approach. reply a_wild_dandan 16 hours agorootparentprevThis is correct. To give more synthetic data examples: 1. Generate inverted problems which are easier to produce than solve. For instance, create an integration math exercise by differentiating a hairy function and reversing the steps. 2. Create (simulated) environmental data. 3. Use adversarial model competition, e.g. self-playing chess or training an artificial image generator/detector model pair. There's evidently a commonplace myth that information quality starts pristine and exclusively gets degraded by systems thereafter. That's just easily, demonstrably false in myriad ways. That's why it's absurd to conclude that LLM output being in internet training data will cause model collapse. Information-rich synthetic data can be created without humans, and it works. (Check out Phi, for instance.) reply breck 17 hours agorootparentprevWell put. Also, ever read your old journals? You are training on generated data. reply Filligree 17 hours agorootparentEver dream? It’s the same thing. Of course we still need real world data, but it seems like generated data should also play a role. Humans don’t weight dreams equally with reality, however, and that’s a distinction I feel is missing here. reply xboxnolifes 14 hours agorootparentprevEven hear an idea from another person? You just trained one model with another. reply IncreasePosts 17 hours agoparentprevShouldn't there be enough known-good training content that we can use it to determine if a document is worth including in a training set? reply devmor 16 hours agorootparentThere's not enough human workers to validate that at scale. If you want ML to do it... well that's a bit of a catch-22. How would an ML algorithm know if data is good enough to be trained on unless it has already been trained on that data? reply HarHarVeryFunny 15 hours agorootparentThe way humans do it is via curiosity/boredom/surprise. If we can't predict some thing well (i.e. we're \"surprised\" by it) then that both acts as learning trigger to predict better next time, as well as retains our interest to explore it. Eventually AGI's will need to learn by experimentation as we do, but in the meantime ability to well predict a potential training sample could be used to decide whether to add it to training set or not. At the moment it seems the main emphasis is on a combination of multi-modality (esp. video) and synthetic data where one generation of LLM generates tailored training samples for the next generation. I guess this synthetic data allows a more selective acquisition of knowledge than just adding surprising texts found in the wild. reply drdeca 12 hours agorootparentprevGet a collection of data which is small enough to have humans annotate as being either low quality or high quality. Train a model to predict this annotation. Then on a larger disjoint collection of data, use this model to estimate whether the data points would be considered low quality or high quality, and use this to filter it. This seems doable, and, I think something like it is already done? reply IncreasePosts 15 hours agorootparentprevYou could use data where you know it was not AI generated, like the library of Congress catalog from prior to 2015. Or highly cited research papers, things of that nature. reply cs702 20 hours agoprevThis deserves to be on the front page. The authors ask whether image-to-text and text-to-image models (like CLIP and Stable Diffusion) are truly capable of zero-shot generalization. To answer the question, the authors compile a list of 4000+ concepts (see paper for details on how they compile the list of concepts), and test how well 34 different models classify or generate those concepts at different scales of pretraining, from ~3M to ~400M samples. They find that model performance on each concept scales linearly as the concept's frequency in pretraining data grows exponentially, i.e., the rarer the concept the less likely it is actually/properly learned -- which implies there is no \"zero-shot\" generalization. The authors also release a long-tail test dataset that they cleverly name the \"Let it Wag!\" benchmark to allow other researchers to see for themselves how current models perform on the long tail of increasingly rare concepts. Go read the whole thing, or at least the introduction. It's clear, concise, and well-written. reply kolinko 18 hours agoparentWorth noting that this paper is about CLIP only, which is way simpler than llm architectures. (if I’m not mistaken) Still, interesting approach and kind of confirms the experience of most people where clip models can recognize known concepts but struggle with novel ones. reply treyd 18 hours agoparentprevBut is this some abstract truth of statistics or is this just a property of how these types of models work? reply bilsbie 17 hours agoparentprevHow do we know humans don’t do the same thing? reply twobitshifter 17 hours agoprevThe models tested seem to work as expected as it’s not a retrieval model being used here. The weights are lowest on wormsnake but much higher on worm and snake. The temperature of the model for something like stable diffusion has to be higher than something doing a retrieval, so we would not expect it to reproduce the exact worm snake image from its training data. reply xcodevn 16 hours agoprevOf course, it will require exponential data for zero shot. The keyword here is zero shot. If you think about it for a second, this applies to humans too. We also need exponential training data to do things without examples. reply IIAOPSW 15 hours agoparentWhen we learn the grammar of our language, the teacher does not stand in front of the class and proceed to say a large corpus of examples of ungrammatical sentences, only the correct ones are in the training set. When we learn to drive, we do not need to crash our car a thousand times in a row before we start to get it. When we play a new board game for the first time, we can do it fairly competently (though not as good as experienced players) just by reading and understanding the rules. reply xcodevn 15 hours agorootparentplease help yourself and do a quick Google search about \"zero shot\" and \"few shot\" learning. reply elicksaur 10 hours agorootparentYou could explain them instead of being snarky. https://xkcd.com/1053/ reply six_four_eight 18 hours agoprevI wonder, is this exponential relation specific to multi-modal models? From my admittedly naïve view it seems to make sense that \"...what is rare is not properly learned\" would apply generally? reply tiborsaas 20 hours agoprevIt was also mentioned in a just uploaded Computerphile video: https://www.youtube.com/watch?v=dDUC-LqVrPU reply a_wild_dandan 15 hours agoprev [–] Better title: Image classification models suck at identifying nouns that they've rarely seen. Crucial context: - They're only looking at image models -- not LLMs, etc - Their models are tiny - A \"concept\" here just means \"a noun.\" The authors index images via these nouns. - They didn't control for difficulty in visual representation/recognition of these exceptional infrequent, long-tail \"concepts.\" If I didn't know an object's label, I too would struggle to identify/draw it... reply z7 9 minutes agoparent [–] Odd how this went from top voted comment to lowest comment. What's the disagreement? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Multimodal models need significantly more pretraining data for marginal performance enhancements, questioning the concept of \"zero-shot\" generalization in computer vision and pattern recognition.",
      "The research introduces the \"Let it Wag!\" benchmark to underscore the necessity for additional studies on generalization abilities in extensive training scenarios.",
      "Understanding the impact of pretraining data frequency on model performance is crucial for advancing multimodal model research in computer vision and pattern recognition."
    ],
    "commentSummary": [
      "The discussions revolved around realizing zero-shot learning in AI models by training on data containing examples of the tested classes, highlighting the significance of comprehending language, vision, and reasoning in AI models.",
      "Various topics were explored, including the potential for another \"AI winter,\" the efficacy of Large Language Models, and the utilization of synthetic data in model training, underscoring the challenges and constraints of current AI technologies.",
      "Challenges associated with achieving zero-shot generalization capabilities in image-to-text and text-to-image models were specifically addressed during the discussions."
    ],
    "points": 171,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1715260085
  },
  {
    "id": 40309957,
    "title": "FBI Email Leak Exposes Questionable Justification for Warrantless Wiretaps",
    "originLink": "https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520",
    "originBody": "By Matt Novak PublishedYesterday Comments (11) File photo of Paul Abbate, deputy director of the FBI, at the Department of Justice in Washington, DC on May 2, 2023. Photo: Eric Lee/Bloomberg (Getty Images) Congress reauthorized America’s warrantless wiretapping program last month after some successful fearmongering by national security hawks on Capitol Hill. But an internal FBI email, leaked to Wired on Wednesday, may accidentally reveal how the federal law enforcement agency plans to overstep the spirit of the law, while technically maintaining the letter of the law. The controversial spying program is Section 702 in the Foreign Intelligence Surveillance Act (FISA) and allows the interception of foreign communications that sometimes include American citizens. The program ostensibly includes safeguards to ensure the law isn’t being used to unnecessarily spy on Americans, but it’s pretty clear from this new email that the FBI likes being able to get communications from Americans. Related Content Lawmakers Are Kicking Warrantless Wiretapping Into Overdrive The CIA Is Begging Congress to Please Keep Spying on U.S. Citizens Legal Approaching Queerness in Doctor Who CC Share Approaching Queerness in Doctor Who The email obtained by Wired dated April 20 was written by FBI Deputy Director Paul Abbate and sent out to employees internally. “To continue to demonstrate why tools like this are essential to our mission, we need to use them, while also holding ourselves accountable for doing so properly and in compliance with legal requirements,” the email reads, according to Wired, which notes that the italicization on the word “use” was in the original email. The FBI email made things even more explicit by encouraging searches for Americans when looking through intercepted communications. “I urge everyone to continue to look for ways to appropriately use US person queries to advance the mission, with the added confidence that this new pre-approval requirement will help ensure that those queries are fully compliant with the law,” the email reads. The FBI’s response to Wired is particularly interesting, making it worth quoting at length. From Wired: Following publication, FBI spokesperson Susan McKee provided a statement from the bureau that mischaracterized WIRED’s reporting, inaccurately claiming it “alleged that that the FBI instructed its employees to violate the law or FBI policies.” The statement added that Abbate’s email “emphasized Congress’ recognition of the vital importance of FISA Section 702 to protect the American people and was sent to ensure that FBI personnel were immediately aware of, and in compliance with, the privacy enhancing changes the law has put in place.” Obviously, the FBI is going to say everyone at the agency follows the law since they quite literally are the law. But Wired spoke with Rep. Zoe Lofgren, a Democrat from California who notes this newly leaked email “directly contradicts earlier assertions” by the FBI when the agency was trying to get the law reauthorized. It’s all a mess. The FBI got exactly what it wanted with the reauthorization of Section 702, something that was never really in doubt, even with pressure from a handful of politicians who opposed it. To paraphrase former president Richard Nixon, it’s not illegal when the FBI does it. But what are you going to do in such a ridiculously rigged system? Show all 11 comments",
    "commentLink": "https://news.ycombinator.com/item?id=40309957",
    "commentBody": "Leaked FBI Email Reportedly Shows Desperation to Justify Warrantless Wiretaps (gizmodo.com)160 points by Jimmc414 17 hours agohidepastfavorite80 comments klabb3 16 hours agoIf massive surveillance is so necessary for our safety, why isn’t there any reporting on all the threats to society they’ve been averting, or criminals caught? I get that they can’t reveal everything, but as it stands now it’s all hush hush, and many naive people will think they’re catching terrorists every day. Generally, if someone refuses to tell you how they protect you “for your own good”, it is because they would look malicious, incompetent or both. reply 1vuio0pswjnm7 11 hours agoparentAnd in the case of so-called \"tech\" companies who are not required to obtain warrants, let alone even informed consent, the use of the data collected is even more secretive. The companies will try to claim their usage of collected data is commercially-sensitive information and/or trade secrets to prevent the public (not their competitors) from understanding what the surveillance is used for. reply abakker 15 hours agoparentprevI agree with the meaning and the premise of your question, but, pedantically, it is not always a good idea to go out and admit how effective a secret program is. Think about it in the context of national security and foreign policy - e.g. \"we have broken 86% of other governments' encryption\" - is not a stat you'd share. Maybe more pointed to the discussion here, by sharing how the program is successful, it would be a powerful indicator about what kinds of crimes criminals should focus on with the lowest risks. reply autoexec 15 hours agorootparent> \"we have broken 86% of other governments' encryption\" - is not a stat you'd share...by sharing how the program is successful, it would be a powerful indicator about what kinds of crimes criminals should focus on with the lowest risks. They can not tell us that they are reading the encrypted communications of other governments but still tell us \"We caught Joe Criminal on Wednesday who tried to blow up the moon, and Bill Evilguy on Friday who tried to poison the waterhole, and Amanda Terrorist on Sunday who tried to murder 10,000 kittens\". Then they could hold public trials for those terrorists where even if we can't see all of the evidence we still know exactly who they are imprisoning/killing, when and how often. If criminals come to realize that terrorist attacks are likely to get them killed or sent to rot in gitmo because of how successful our spy agencies are, and they decide to hold up liqueur stores instead how is that a bad thing? reply abakker 14 hours agorootparentwell, its been a while, but I remember seeing a study that states with the death penalty actually had higher incidence of violent crime. From that, it was posited that the existence of harsher punishments don't seem to offer much deterrent value because criminals do not expect to be caught. Further, there was discussion that state-sponsored demonstrations of power/punishment play a role in normalizing that behavior in the first place. Personally, I think both categories are likely true, but I couldn't comment on the effect size. One thing that law enforcement stats generally indicate is that a great deal of property crime is uninvestigated and unsolved. Maybe that is fine, but detailing, \"we tried our literal best surveillance methods and have not managed to reduce convenience store robberies\" gives an edge to those who would like to premeditate. Of course, if they think they won't get caught in the first place, it doesn't matter. reply tmaly 13 hours agoparentprevEven if the reporting was not public, people in Congress that are suppose to oversee this should see the reports. I have not seen them mention anything. reply 2OEH8eoCRo0 13 hours agoparentprev> why isn’t there any reporting on all the threats to society they’ve been averting It's been reported but NSA FISA victories aren't exactly the sort of thing that gets upvotes around here. https://www.justice.gov/d9/2023-06/Section%20702%20of%20the%... > In 2009, for example, Section 702 helped foil an active plot to bomb the New York City subway. > (U) Section 702 has been used to identify ransomware attacks on U.S. critical infrastructure, and multiple attacks have been identified and defended against because of Section 702 collection > (U) Section 702–acquired information related to sanctioned foreign adversaries was used in U.S. government efforts to stop components for weapons of mass destruction from reaching foreign actors. > (U) Section 702 information has identified key economic security risks, including strategic malign investment by foreign actors in certain U.S. companies. > (U) Section 702 collection has helped identify when hostile foreign intelligence services are trying to send their operatives into the United States to recruit spies here in the United States. reply pseudalopex 11 hours agorootparentThose were claims made by intelligence agency officials. The Director of National Intelligence lied to Congress to avoid oversight.[1] The CIA incorrectly attributed victories to torture.[2] The people who made those claims should not be trusted to evaluate correctly and report honestly. [1] https://www.usatoday.com/story/opinion/2018/01/19/james-clap... [2] https://www.intelligence.senate.gov/sites/default/files/publ... reply 2OEH8eoCRo0 9 hours agorootparentNeither of your sources talk about the claims I referenced. reply unclebucknasty 14 hours agoparentprev>why isn’t there any reporting on all the threats to society I think this framing is a bit naive. It also implies—without evidence—some malice on the part of LE. The suggestion is that this activity is known to be ineffective for the stated purpose, so continues to be conducted for some other purpose, which by definition would be nefarious. But, there is no proof of this, except that some would like to see more perp walks that reveal sources and methods. And, as much as people sound the alarm, we've seen relatively little-to-no abuse of these epanded powers. Americans are not being prosecuted at-scale, if at all, over information that may have been incidentally captured. The truth is that powers had to be expanded to keep pace with changes in technology, and a changing world in general. Without this our LE wouldn't stand a chance. Now one might argue about degree and balance, but that's not what I generally hear. Instead, it's mostly vague alarmism and mistrust about the very idea of any expanded powers. reply Voultapher 14 hours agorootparentSurveillance is there to consolidate power, safety is a smokescreen. We have ample evidence that mass surveillance not only has massive issues but also that it doesn't deliver on the parroted benefits, at all. http://newamerica.org/future-security/policy-papers/do-nsas-... Want a grim reality check on \"what they say vs reality\", then go read this https://www.intelligence.senate.gov/sites/default/files/publ... . Particularity interesting is this section: \"The Committee makes the following findings and conclusions:\" reply unclebucknasty 11 hours agorootparent>Surveillance is there to consolidate power, safety is a smokescreen. This is another vague allusion. Consolidate power for whom? How exactly does it do so? What, specifically, is this \"power\" and how is it used against American citizens? >it doesn't deliver on the parroted benefits The article referenced here is from 2014, so not exactly up-to-date. And, in multiple instances, the authors acknowledge that they don't even know whether the programs were used. In still other cases, they acknowledge the programs' use in prosecuting terrorists, but casually cast them aside as a low percentage. OK. But, who wants to sign up for \"just a few terror incidents\"? >Want a grim reality check Another report from 2014, and additionally has nothing to do with the subject. \"Someone in the government did something wrong in this completely unrelated way 10 years ago\" is not a valid argument. reply infamouscow 15 hours agoparentprevIt's because it's not for your safety, it's for the safety of the established bureaucracy of government. Just look at the number of terrorists that were already on the FBI's watchlist, and whose plots were foiled by random citizens stepping in rather than the FBI. (Hint: it's literally every one.) Even more incendiary, look at the number of school shooters that have been on LEO watchlists, and also failed to do anything to prevent tragedy. The brain dead rebuttal to this is something like civil rights getting in the way. But if civil rights are holding them back, why invade privacy at all if nothing actionable is going to be done. reply unclebucknasty 15 hours agorootparent>it's for the safety of the established bureaucracy of government. Seems vague. What exactly does this assertion mean? Are you saying that they are collecting this information to blackmail people or something similar? Because I don't see people being defenestrated or otherwise prosecuted for threatening the \"bureaucracy\". On the contrary, there seems to be an all-out public assault on our government from some quarters. And who, exactly, is the \"established bureaucracy\" anyway? reply fragmede 15 hours agoparentprevThe most heartbreaking thing about having broken the enigma code during world war II, is that they had to let people die, knowing they could have saved them, because saving them would have tipped the enemy off that they had broken the code. reply NhanH 15 hours agorootparentI don't quite understand the relation of this comment to the GP's comment, but the world currently is not under world war 3. Any justification for things happening in ww2 is hard to use for current situation reply autoexec 14 hours agorootparentThere's no \"world war\" exactly, but the US is still overseas killing people and has been doing that every single year for decades. Basically my entire life. I doubt there's been very many years since WWII that the US hasn't been engaged in wars and armed conflicts somewhere on the Earth. We rarely hear about any of it, and if you asked a random person on the street they probably couldn't name any of the countries where people are/have been being killed. reply fragmede 7 hours agorootparentprevto spell it out, the FBI can't tell the general public many of the stories where they did good, because that would exposure the methods they used to do good, which would result in them not being able to use those methods anymore, so those stories have to go untold. also I disagree that we're not in world War III. we are, it just hasn't been officially declared yet. reply Jerrrry 15 hours agorootparentprevIt is heartbreaking, but they did not die in vain. This may be the only time this quote has been used un-ironically, in good concious: \"Some of you may die, but that is a sacrifice I am willing to make.\" -Lord Farquad reply jawiggins 15 hours agoparentprevObviously the Intelligence community doesn't say exactly how they learned such things, making it difficult to say if they used SIGINT, HUMINT, or something else, but there have been some big displays of their overall effectiveness recently. 1) Clearly the US Intelligence community telegraphed Russia's plans to invade Ukraine well in advance. 2) Recently the US warned of an impending terror attack on a concert in Moscow just before one happened. Most intelligence they collect won't be made public, so only those in the higher echelons of government will know what they regularly learn. But I do think they have demonstrated the ability to predict the future more than a few times with their public statements. reply ProfessorLayton 15 hours agorootparentThe FBI deals with internal matters, and has nothing to do with external conflicts in Ukraine etc. reply jawiggins 13 hours agorootparentNation states or terrorists planning attacks on the US mainland (either physically or in cyberspace) are external threats that are investigated by the FBI and have been charged by the DOJ. reply lumb63 16 hours agoprevThis is terrible reporting. I don’t support the warrantless monitoring of citizens via Section 702, but the email displayed shows zero nefariousness as far as I can tell. It is just the FBI deputy director encouraging FBI agents to lawfully (at least according to FISA) use the tools at their disposal to demonstrate their use to the FBI. reply prosody 16 hours agoparentWould you object if the FBI deputy director encouraged FBI agents to lawfully use their service firearms in the field to demonstrate their use to the FBI? There are real harms involved with increasing surveillance of the public and reducing judicial oversight. Placing the use of warrantless wiretapping as an end in itself rather than a possible means whose use needs to be carefully weighed necessarily means that the weighing will be less careful. reply jrockway 16 hours agorootparentUltimately Congress should have written the law more carefully. It's their job to balance the various concerns of the government; effectiveness of their law enforcement vs. civil liberties. The FBI's job is only to investigate to the maximum extent of the law, and the email was just reminding people of that. reply Retric 16 hours agorootparent> investigate to the maximum extent of the law Every branch of the government has a duty to uphold the constitution including its protections. Further the constitution is the law suggesting it’s only their responsibility to uphold some laws is the root of a great number of issues. reply dylan604 16 hours agorootparentprev> Ultimately Congress should have written the law more carefully. That's not how the clandestine agencies like Congress to write laws though. They love to live in the spaces and gray areas created by vaguely written laws. reply int_19h 14 hours agorootparentprevWhenever these laws are debated and people point out various ways in which they could be abused, Congress says that those are obviously far-fetched and the powers that are being given will be used in reasonable ways. This email just goes to show what this actually means in practice. reply foooorsyth 15 hours agorootparentprev> Ultimately Congress should have written the law more carefully Insane framing of “we know the spirit of the law but we are going to do everything we can to skirt it”. >The FBI’s job is only to investigate to the maximum extent of the law Wrong, according to the FBI itself: “The mission of the FBI is to protect the American people and uphold the Constitution of the United States.” https://www.fbi.gov/about/faqs/what-is-the-mission-of-the-fb... Pray tell, how does warrantless spying on citizens help to uphold the 4th amendment? reply sweeter 16 hours agoparentprevI would have to disagree, the main issue with FISA is not only that allows the US Government to do warrantless wiretapping, it forces citizens who are business owners to facilitate that wire-tapping (which is already legal but only for a small subsection of types of businesses like telephone and internet companies) and FISA expands on the definition of these kinds of businesses without openly defining what that expansion is, the FBI can just make that determination without any due process, foresight, or reasoning. This is hugely detrimental to all Americans. They could go to any business and force them to wiretap their wifi or turn over ALL user records. The fact that there is no transparency at any level and that they are quietly pushing this through is damning. If they want to expand the definition of what FISA allows, they should do so in the open. The Majority Report had an expert on to talk about it, you can google it, its the first link under \"majority report FISA\" reply tonetegeatinst 16 hours agorootparentOne could argue this is just like how the ATF arbitrary can interpret the law and changes what's consider legal or not. Their is not method the truly balances any position where you can constantly change the definition of words and ths interpretation of the law. reply jonhohle 16 hours agorootparentIn court the ATF is learning that they cannot arbitrarily change their interpretations and may have less authority than when they started due to how arbitrary they’ve decided to be recently. That doesn’t help the poor folks who followed the old rules and are now ensured in the executive branch illegally pulling the rug out from under them, but it seems like the ATF is routinely losing when brought to trial. While the current admin seems to love changing the meaning of words to suit their whims, I think, in general, the courts have pretty consistently sided with the intended meaning at the time of legislation (text and tradition) rather than the politically motivated meaning that’s been conjured up in the last 36-months (novel legal theory garbage). reply int_19h 14 hours agorootparentThe new trick they are using is to simply drop the case whenever they feel that the ruling is not going to be in their favor. Which lets them keep the rule on the books and continue using it in the future. Basically the onus becomes on the person prosecuted & their lawyer to bring it up every time. And the problem with that is that by the time you get to the point where case is dismissed, it is already a major disruption to one's life. The legal fees alone can be effectively punitive for many people to begin with, and then if you actually get arrested and detained, it's straight up financial ruin. Not to mention reputational damage stemming from stuff like this even if the case is eventually dropped. reply nickff 16 hours agorootparentprevI don’t like the government forcing business owners to assist in controlling the populace either, but this is hardly a unique situation. The best example I can think of is requiring employers to pay deduct employees’ income tax. Should we stop requiring employers to deduct income tax? For the record, I think the answer to both is yes, but I’m in the minority. reply dylan604 16 hours agorootparentDeducting taxes is not an opaque issue though. It is known to everyone at time of employment with the filling out of the W-2. When the TLAs come knocking on a business' door to provide information, the employee(s) in question are not asked to fill out another form. Equating the two is just a sad attempt at white washing reply notaustinpowers 16 hours agorootparentprevFISA allows these governmental agencies to force/compel businesses to share this information without users being notified of it. Requiring employers to deduct employee's pay for income taxes notifies the employee in question before it is done. The notice is the important bit here. reply nickff 12 hours agorootparentPeople are constantly bombarded with notices, to the point that most of us actually read very few of them. I'm not sure the notice makes a big difference here; what proportion of taxpayers do you think actually know their total tax liability and/or effective tax rate at the federal, state, and/or local levels? We have notice of all of them, but it's so hard to keep track of income, investment, sales, and property taxes, that very few of us actually know this stuff. I bet that if you put a notice of FISA data collection on someone's pay stub, it's unlikely they would even notice. reply candiddevmike 16 hours agorootparentprevIt would be interesting to stop all employer collected things (including/especially health insurance) and have folks pay directly for them. Unless you read your paystubs, you don't realize how much this stuff costs you. reply repelsteeltje 16 hours agorootparentprevThe analogy differs in the extent to which the populace may reasonably assume a level of privacy. The relation between employer and employee is usually less anonymous than that between a tech platform and some random visitor. reply zugi 15 hours agoparentprevNot at all. Remember what FISA stands for: Foreign Intelligence Surveillance Act. This law was originally written to allow spying on Foreign citizens. They stoked enough fear to get Congress to amend and expand the law to allow targeting of U.S. citizens, implying we were all in danger of we didn't allow it and promising to use that power responsibly. Now the FBI Deputy Director says: \"I urge everyone to continue to look for ways to appropriately use US person queries to advance the mission\" - that is, literally think about ways to use this power to spy on U.S. citizens. This absolutely is and should be big news. reply philipov 16 hours agoparentprevI dunno. If they're really so essential, why do they require him to encourage their use? Sounds like they're completely optional, not essential. reply boomboomsubban 15 hours agoparentprevA congressperson said this email directly contradicts what the FBI told them during the bid to get the program renewed. Even if you're fine with them spreading their net to the legally allowed limit rather than what's necessary, lying to Congress seems like nefariousness. reply 2OEH8eoCRo0 12 hours agorootparentAre you ready for Assange's extradition 11 days from now? reply boomboomsubban 10 hours agorootparentSorry, what does this have to do with what I said? reply felsokning 16 hours agoparentprevIt definitely reads like a, \"If we don't use it, we lose it,\" message - and not a, \"oh, mahgerd, wE gOtTa Do SoMeThInG nOw!!!!!!\" kind of message. reply tootie 15 hours agoparentprevPutting the word \"desperation\" in the headline is serious editorializing. It's a valuable story but Gizmodo is chasing clicks. reply bandyaboot 16 hours agoparentprevYeah, this article is very strange. The debate should be about whether the law is appropriate. This email adds nothing to that debate. reply 2OEH8eoCRo0 16 hours agoparentprev> “To continue to demonstrate why tools like this are essential to our mission, we need to use them, while also holding ourselves accountable for doing so properly and in compliance with legal requirements,” Not sure why this is news. reply olliej 16 hours agorootparentYou cannot use a “fisa warrant” legally, they’re objectively unconstitutional. Conforming to the “legal requirements” means also conforming to the constitution, it doesn’t matter if someone writes a law that says you don’t need to obey the constitution, that just means the law itself is illegal. It’s amazing how government officials interpret the 2nd amendment (giving the right to bare arms in the absence of a formal militia to mean everyone can have guns in of all kinds in all cases, but then turn around and say the 4th amendment does not grant any rights at all) reply pdonis 16 hours agorootparent> giving the right to bare arms in the absence of a formal militia That's not what the 2nd amendment says. It says \"A well-regulated militia being necessary to the security of a free state, the right to keep and bear arms shall not be infringed\". That doesn't mean that if the government decides to organize a formal militia, people's individual right to keep and bear arms goes away; that would defeat the whole purpose of the amendment, which is to enable the people to resist a tyrannical government, just as the Americans resisted British tyranny using their own individual arms. In other words, the \"well-regulated militia\" is the entire body of armed citizens. The \"well-regulated\" part means that the citizens themselves are supposed to regulate their own and each other's use of arms so that they are not used to promote tyranny instead of resist it. reply thfuran 15 hours agorootparent>that would defeat the whole purpose of the amendment, which is to enable the people to resist a tyrannical government That doesn't really jive with the president being explicitly legally empowered to call upon the militia to quell insurrection. reply pdonis 15 hours agorootparentYes, it does, because the militia has more than one purpose. It's not only for resisting a tyrannical government. That's one of its purposes, but not the only one. Indeed, once the Declaration of Independence had been signed and published, one could argue that the United States were in fact organizing and using the militia to \"repel invasion\" by the British. But the militia was used before that happened, when the only grounds for its use was to resist tyrannical government by the British. reply olliej 12 hours agorootparentThe only people claiming they want to fight a \"tyrannical government\" as their justification for dick extensions are the people who routinely try to remove the rights of others, and are the same people who have repeatedly tried to use force to overthrow the elected government when their candidate lost. Parroting that nonsense, when everyone knows that the reason they want unrestricted access to guns is just that they're idiots that made having guns their personality. The 2nd amendment says \"A well regulated Militia\", but the \"I need guns to compensate for personal issues\" people say you can't have any regulations or rules that would in anyway require the to actually be mentally or physically capable of supporting a free State. Again, the group of people arguing for the definition of the 2nd amendment that gives unrestricted access to guns are the people who routinely and consistently opposed the \"free State\" the 2nd amendment is ostensibly for, hell they generally oppose the 1st, 5th, 8th, 15th, 24th, and 28th amendments, so any proclaimed absolutism about their support for the constitution is dubious at best. I'd make a claim based on current commentary that these groups don't support any of the the constitution at all given their repeated and explicit disdain for articles 1 through 3 that define the core of the us government. reply FireBeyond 15 hours agorootparentprev> The \"well-regulated\" part means that the citizens themselves are supposed to regulate their own and each other's use of arms so that they are not used to promote tyranny instead of resist it. This trips up a certain segment of the populace, though, if we take your explanation at face value. In many states and locales, the citizens have regulated their own and each other's use of arms (I disagree with your limitation of 'not used to promote tyranny' - that is not a limitation present in the wording). They've delegated that regulation to their state or locale, by means of voting and legislation. And then they get told they can't. So it's \"well-regulated\", but \"not that way\"? reply pdonis 15 hours agorootparent> I disagree with your limitation of 'not used to promote tyranny' - that is not a limitation present in the wording I didn't mean to imply that that is the only regulation citizens should exercise over each other's use of arms. Obviously citizens should also be regulating each other's use of arms to prevent private crime, since that is also a threat to the security of a free state. > They've delegated that regulation to their state or locale, by means of voting and legislation. First, no such delegation can violate the 2nd Amendment, since the Constitution, as amended, is the supreme law of the land. It limits how much citizens can delegate. Second, what happens if the government they have delegated to becomes tyrannical, and stays that way despite changes in voting and legislation? What is the citizens' last resort? To resist the tyrannical government using their own individually borne arms. In other words, any such delegation can always be revoked, and the citizens have to have some last resort way of making the revocation good if the government refuses to accept it. > And then they get told they can't. Nobody has told them that. See above. reply olliej 12 hours agorootparent> Nobody has told them that. See above. Literally they did exactly. People voted for and passed laws that regulated arms. Idiots want to play with toys and got those regulations removed, because nothing says \"well regulated militia\" like alcoholics and racists with automatic weapons. It's amazing the same people arguing \"the government has no right to take away my unregulated and unnecessary guns\" also argue the same government does get to regulate bodily autonomy, religion, etc reply readthenotes1 16 hours agorootparentprev\"bare arms\" The Constitution, afaik, is silent on the matter of shirt sleeves. However, a well regulated militia, being necessary to the security of a free State, the right of the people to keep and bear Arms shall not be infringed. The crime is that the States have given up on forming those well-regulated militias and that we have a continuous standing army in a Military-Industrial-Congressional complex whose corruption boggles the mind. reply pdonis 16 hours agorootparent> the States have given up on forming those well-regulated militias The States claim that state-run National Guard units are the \"well-regulated militias\". Their claim is wrong, but not for the reason you imply. The reason their claim is wrong is that the 2nd Amendment is about the right of the people to keep and bear arms to resist a tyrannical government--any tyrannical government: not just the Federal government, but the State governments as well. If the States can organize a militia and then say individual citizens no longer have the right to keep and bear arms outside that organized militia, that defeats the purpose of the 2nd Amendment just as much as the Federal government having a standing army and claiming it's an organized militia. reply RajT88 15 hours agorootparentThat's actually incorrect. The second amendment is entirely about state militias, and anyone claiming it's related to resisting a tyrannical government hasn't actually read the documents they claim to have read. Article 1, Section 8 of the constitution outlines militias as a duty of congress, and the function of the militias (note: resisting a tyrannical government is not listed!) Article II, Section 2 defines the President as the commander in chief of the military and militias. The above is what the second amendment is making reference to. There are following acts of congress that clarify who gets to be in a militia: the militia acts of 1792, 1795, 1862 and 1903. These allow for people of different races and genders to join militias. The militia act of 1903 clarifies that there are different types of state militias: formal and informal. Formal is things like the National Guard, Texas Rangers, etc. Informal is - in times of emergency, the state can basically just round up a posse for the outlined purposes in Article 1, Section 8. The second amendment does guarantee personal firearm ownership under this possibility, connected to possible service in an informal militia. You're welcome. reply int_19h 14 hours agorootparentThe problem with this perspective is that many states have some equivalent of the Second Amendment in their state constitution, including some of the original states - and in many cases those explicitly spell out RKBA as an individual right. I think it's reasonable to at least wonder why the same people who wrote those state constitutions would, when discussing the federal one, not have the same thing in mind when authoring 2A. reply RajT88 14 hours agorootparentThat's not a problem of this perspective, that's an entirely separate issue: The still evolving debate between what takes precedence: Federal powers or State powers? Sometimes state, sometimes federal. This is why all parties are trying to pack the courts with ideologues of various leanings. reply int_19h 14 hours agorootparentYou misunderstand my point. It's not about which constitution takes priority (the answer here is actually very simple: for federal laws, only federal constitution applies; for state laws, the state constitution additionally applies). The point is that the original US Constitution, as well as the Bill of Rights, was worked on and ratified by the same people who worked on their states' constitutions at that time, and you can, in general, see very clear parallels between state and federal constitutions when it comes to rights. So, given that at least some state constitutions from that same period explicitly spell out individual right to keep and bear arms, I find it dubious that it was not one of the considerations when they adopted the same right on the federal level. The fact that the verbiage of 2A cites militia as a justification implies that this is the one they thought most important, but not that there weren't any others; and, of course, the verbiage doesn't actually condition RKBA on militia membership in plain reading. There are also letters and other discussions from that time that, on the whole, make it fairly clear that militia was understood as including everyone (and not just those states might choose to designate as such), and that the right was thus really meant to be universal. reply RajT88 13 hours agorootparentLetters and discussions we can dismiss outright. This is always a selective argument based on your leanings, and these are inevitably used to twist what are otherwise fairly clear wordings even in the language of the day. A lot of people said a lot of things, but what matters is what is enshrined in law. What state constitutions of the day say something different than what is written in the federal constitution? I am interested in looking into it at least. ETA: I looked into it. Your argument only seems to have Pennsylvania as a leg to stand on, through the participation of Ben Franklin, and even that is flawed according to historians: https://journals.psu.edu/pmhb/article/download/59337/59064/0 reply pdonis 13 hours agorootparentprev> The second amendment is entirely about state militias I disagree, and I'm certainly not the only one; there has been a lot of literature on this. Obviously we're not going to resolve that here. However, I will offer a few comments, since you have presumed to imply that I have not actually read the documents in question. First, it seems odd for you to say that the second amendment is entirely about state militias, and then refer to the Constitution, which is a Federal document, and which, as we will see below, contains provisions for Federal actions regarding the Militia, not just state actions. Next, Article I, Section 8 says that Congress shall have the power: \"To provide for calling forth the Militia to execute the Laws of the Union, suppress Insurrections and repel Invasions; To provide for organizing, arming, and disciplining, the Militia, and for governing such Part of them as may be employed in the Service of the United States, reserving to the States respectively, the Appointment of the Officers, and the Authority of training the Militia according to the discipline prescribed by Congress;\" Note that it does not say \"create\" the Militia. It assumes that \"the Militia\" is an already existing thing, and then says that Congress may provide for \"calling forth\" this already existing thing for certain purposes. It also says Congress may provide for other things regarding the Militia, but it does not say that the Militia may only do the things that Congress has provided for, or that the Militia only consists of those people Congress organizes by legislation. In fact, it says explicitly that Congress may provide for governing such Part of them as may be employed in the Service of the United States. That means there can be other Parts which are not so employed. Article II, Section 2 says: \"The President shall be Commander in Chief of the Army and Navy of the United States, and of the Militia of the several States, when called into the actual Service of the United States\" In other words, the President is not the Commander in Chief of the Militia all the time or of all the Militia; he is only Commander in Chief of the Militia who are in the actual Service of the United States, while they are so serving. That means there can be Militia who are not in such service, and whom the President is not the Commander in Chief of. In short, the powers the Constitution grants Congress and the President over the Militia are qualified and limited, not absolute. reply RajT88 5 hours agorootparentThank you for agreeing with me that the second amendment makes direct reference to the articles I highlighted. reply 2OEH8eoCRo0 16 hours agorootparentprev> You cannot use a “fisa warrant” legally, they’re objectively unconstitutional. According to you. The courts think it's perfectly constitutional. reply pdonis 16 hours agorootparentThe courts claim lots of things are constitutional when any reasonable person can see that the claim is nonsense. To give just a few examples from Supreme Court cases: Seizing property to turn it over to a private developer instead of for public use is a valid use of the government's eminent domain power (Kelo v. New London). Congress regulating farmers growing wheat on their own property for their own and their animals' consumption on the grounds is a valid regulation of interstate commerce under the Commerce Clause (Wickard v. Filburn). Allowing Congress to delegate legislative power to the executive branch, in clear violation of Article I of the Constitution, as long as Congress gives an \"intelligible principle\" (Hampton Co. v. United States). Ruling that exercising your Fifth Amendment right not to be a witness against yourself can be taken as evidence of guilt (Salinas v. Texas). reply dwater 16 hours agorootparentprevYes, in an alternate universe it would be gratifying to see a high court that wasn't captured by far right ideologues that was willing to take another look at precedent establishing decisions of the past that rolled back freedoms and privacy and say, \"You know what, the facts that supported FISA, the Patriot Act, etc., were falsely manufactured for political reasons, and actually now that those falsehoods have come to light we think it's time to admit that those decisions were unsupported and should be reversed.\" But I expect the opposite for this universe. reply int_19h 14 hours agorootparentIt's not a right/left thing, at least not in the colloquial sense those words are used in US. Laws like these have broad support on both sides of the political spectrum. The fundamental problem is authoritarianism, which is a disease infesting both left and right. reply 2OEH8eoCRo0 16 hours agorootparentprevI disagree that they're captured and I support FISA. FISA should be expanded as far as I am concerned. reply chefkd 16 hours agoprevare there laws preventing the FBI from using NSA tools which in my head seem more advanced and all compassing? why even rely on wiretaps at this point reply aerostable_slug 16 hours agoparentThey are in effect accomplishing the same goal with a billion dollar satellite that picks up cell phone calls in a denied country vs. a lawful intercept request from the phone company here in the U.S. The reason NSA has to get data from NRO satellites or its own clandestine taps (or Navy submarines, or what have you) is because North Korea's telco won't honor their intercept requests. Why spend the money when you can effectively just send an email to a friendly party? When we're talking the United States, FBI can exercise their legal privileges and avoid all that technical cost. reply chefkd 15 hours agorootparentvalid point about NK but my confusion is in my head the NSA collects everything digital on planet earth and if they are gonna collect everything via all sorts of advanced methods why even go to the telecom companies and spend resources if you're the FBI? the data is already there with social graphs, advanced LLMs and jove knows what else the super smart folks over there have concocted does the law prevent that data from being used but it can be collected? like wiretaps seem so archaic in my head but idk much about this stuff this is pure speculation from my family's basement :) reply tootie 15 hours agoparentprevThe FBI is domestic law enforcement. They need admissable evidence. FISA grants warrants. NSA is operating on a different level with different objectives. reply chefkd 14 hours agorootparentBut that's what's confusing to me I come from a country where a dictator might kill a few thousand just on a whim and shut off the internet so it's confusing why the NSA which in my head is still governed by democratic principles and is not at the whim of a single psychopath can collect data on everyone but it can't be used in court? Like hypothetically if they had social graphs and cell phone records showing a US citizen was kidnapped and murdered they can't act on it or pass it to the local police? Doesn't that seem illogical? reply tootie 9 hours agorootparentBy statue, the NSA can't actually do that. The mass unwarranted surveillance of phone conversations was a short-term program that was ultimately shut down as soon it's existence became known outside the Oval Office. Even Bush's Attorney General said he couldn't do it. The bulk collection of telephone metadata lasted for maybe 15 years but was ultimately allowed to expire due to both ethical concerns and because it didn't yield a lot of value. The NSA is primarily tasked with overseas collection. IE tapping Osama bin Laden's satellite phone. And they report up the chain to the DoD. Meaning they aren't even looking for domestic criminals and may not notice if they surveilled one. If they're doing anything beyond their legal parameters or beyond their remit, they wouldn't broadcast it or expose themselves by using it to arrest someone. I like to point to the Gestapo as a counterexample. They are the archetype for police state surveillance. And they did it in a country where most people didn't have even a rotary phone let alone the internet. The distinction between a police state and a free society is entirely up to will and not technology. reply chefkd 9 hours agorootparenthmm idk a part of me doesn't believe the program went away just rebranded and renamed itself esp. with things like Executive Order 13587 & the recent NSS but that could be because I come from a place that has shown me the worst of humanity maybe with enough time in the free world I'll learn to be Mr. Optimist :) but the morality/legality of something like that is cool to think about as a thought exercise :- if they're doing anything beyond their legal parameters or beyond their remit, they wouldn't broadcast it or expose themselves by using it to arrest someone even in a hypothetical scenario where the data points to a person being kidnapped from an apartment, driven across state lines, and murdered which in my head seems worse than the collection but idk the laws feel not mallable enough for the age we live in reply tootie 8 hours agorootparentSure, you can assume they're doing some things off the books, bending rules, abusing authority. Surely it happens everywhere. But they couldn't easily hide a massive program or fund it without a trace for very long. If you want to take heart about something, the worst mass surveillance program was uncovered by the NY Times. They published it and suffered no consequences. The same guy who exposed that program, exposed the US involvement in sabotaging the Iranian nuclear program. They dragged him to court a few times, but he walked away without any charge sticking. So, for one, it's really, really hard to keep something like secret for very long and our press is still free enough to do something about it. https://www.nytimes.com/2005/12/16/politics/bush-lets-us-spy... reply chefkd 7 hours agorootparentGod bless the land of the free if this was 100 something other places that person wouldn't be heard from again for much much less It's definitely my bad experiences speaking but I can see how thoughtcrime could become a thing. It starts with phone calls people don't care enough, it progresses to social graphs and online habit tracking which people joke about there's even memes, next it'll probably be something more advanced by which point the public is already acclimated I guess as long as they don't act on it, it isn't Minority Report yet? I wonder what the crime that changes public sentiment from nonchalant to be highly in favor will be I don't think a hypothetical kidnapping & homicide would be enough it would have to be something way more dramatic reply rootsandstones 15 hours agoprev [–] Offtopic but gizmodo.com is a mess, every few sentences there is an ad. And with activated adblocker it‘s just replaced with whitespace. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Congress reauthorized the US warrantless wiretapping program, Section 702 in the FISA, influenced by national security concerns.",
      "An FBI leaked email to Wired uncovered plans to stretch the limits of the law by searching for Americans in intercepted communications, despite technically following the law's wording.",
      "Despite objections, the FBI successfully obtained the reauthorization, raising questions about accountability and the perception of being beyond legal boundaries."
    ],
    "commentSummary": [
      "The debate covers the effectiveness, ethics, and legality of surveillance programs, expressing concerns about lack of success reporting and potential government agency abuse of power.",
      "Discussions include interpretations of the Second Amendment, the role of state militias, and the balance between law enforcement efficiency and civil liberties.",
      "There are questions about the use of severe punishments for deterrence, the need for mass surveillance measures, and the transparency and accountability of government actions in surveillance and law enforcement."
    ],
    "points": 160,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1715272457
  },
  {
    "id": 40313451,
    "title": "World Nearing Peak Pollution Amid Regional Disparities",
    "originLink": "https://www.sustainabilitybynumbers.com/p/peak-pollution",
    "originBody": "Share this post The world has (probably) passed peak pollution www.sustainabilitybynumbers.com Copy link Facebook Email Note Other Discover more from Sustainability by numbers Using data and research to understand what really makes a difference Over 32,000 subscribers Subscribe Continue reading Sign in The world has (probably) passed peak pollution Millions die prematurely from local air pollution, but we can reduce this number significantly. Hannah Ritchie May 09, 2024 88 Share this post The world has (probably) passed peak pollution www.sustainabilitybynumbers.com Copy link Facebook Email Note Other 14 Share The health impacts of air pollution are often underrated. There are a range of estimates for how many people die prematurely from local air pollution every year.1 All are in the low millions. The World Health Organization estimates around 7 million. The good news, then, is that the world is probably passed “peak pollution”. I say “probably” because confidently declaring a peak is, apparently, the best way to make sure it doesn’t happen. Here, I’m talking specifically about emissions of harmful local air pollutants: gases like nitrogen oxides (NOx), sulphur dioxide which causes acid rain, carbon monoxide, black carbon, organic carbon, non-methane volatile organic compounds. I’m not talking about greenhouse gases. The Community Emissions Data System (CEDS) recently extended its long-term dataset on emissions of air pollutants up to the end of 2022. I updated this data in our explorer tool on Our World in Data (where you can explore the trends by country). What’s striking is that emissions appear to have peaked for all of these pollutants, with the exception of ammonia, which is almost entirely produced by agriculture. Organic carbon and NMVOCs are not quite out of the clear yet, but might not reach their previous peaks again. Of course, emissions are not falling everywhere. They’ve fallen steeply in richer countries like the US and much of Europe. And the big turning point for the global figures has been the rapid turnaround in China. Emissions have declined rapidly in the last decade, with huge gains for public health. It’s in low and lower-middle income countries where emissions are still rising, and pollution levels in cities are the highest. This is not surprising: air pollution is one of the few areas where the “Environmental Kuznets Curve” tells a pretty accurate and consistent story. Air pollution increases as countries develop, gain access to energy, and industrialise. They then fall once a country gets rich enough to impose pollution standards and limits without infringing on development and the move away from energy poverty. The goal now is to see if countries can move through this curve much faster – and with lower levels of pollution – than countries like the US or the UK did. This should be doable: we’ve learned a lot over the last 50 years about how to produce energy with less pollution, what technologies work and don’t work, and have reduced the costs of solutions that were expensive in their early days. Note that this is not a finger-pointing exercise where rich countries tell poorer ones not to pollute. We’re mostly talking about local air pollution. The negative impacts of pollution are felt by domestic populations. It’s about how we ensure that the poorest countries can gain access to energy, alleviate poverty, and develop while limiting the number of people who die prematurely from air pollution in the process. Subscribe 1 These are measured as premature deaths. Most are not 20-year-olds dying from respiratory diseases or asthma (although childhood asthma is one symptom of local air pollution) but older people dying months or years earlier than they would have in the absence of pollution. In highly polluted cities, these “years lost” can be up to 5 years or so. 88 Share this post The world has (probably) passed peak pollution www.sustainabilitybynumbers.com Copy link Facebook Email Note Other 14 Share Previous",
    "commentLink": "https://news.ycombinator.com/item?id=40313451",
    "commentBody": "The world has probably passed peak pollution (sustainabilitybynumbers.com)152 points by robertn702 11 hours agohidepastfavorite139 comments Animats 10 hours agoThat's encouraging. - Peak pollution - check. - Peak coal - not yet. - Peak oil - maybe 2019, but aftermath of COVID affects numbers.[1] - Peak baby - 2013. [2] [1] https://www.resilience.org/stories/2023-04-19/the-status-of-... [2] https://www.thenewdaily.com.au/life/2022/11/12/the-stats-guy... reply defrost 10 hours agoparentPeak Coal - Probably, very likely, more or less about right now. Global coal supply likely peaked in 2023 and then to decline https://news.ycombinator.com/item?id=38652273 Can't say for sure until another few years have passed, nonetheless every country in the world has coal on hold or in decline save for India and China. China is the largest producer of renewable technology and energy (?), India is building out the worlds largest solat array farm (albeit \"only\" good for 16 million households). reply cpill 9 hours agorootparentnext [14 more] [flagged] WaxProlix 9 hours agorootparentIn the same way that your local power plant isn't solely responsible for the pollution created in heating surrounding homes, it's probably not reasonable to look at the places that have done the fabricating, weaving, injection molding, spinning, etc for the rest of the world as being the major center of pollution. China specifically is decarbonizing fast, and at some expense to their own economy over the last decade or so. reply vondur 8 hours agorootparentRegarding China: China accounted for 95% of the world's new coal power construction activity in 2023, according to the latest annual report from Global Energy Monitor (GEM). Construction began on 70 gigawatts (GW) of new capacity in China, up four-fold since 2019, says GEM's annual report on the global coal power industry China started construction on 70.2 GW of new coal-power capacity last year, almost 20 times the rest of the world's 3.7 GW. reply Retric 8 hours agorootparentThat’s misleading because they are idling their existing coal power plants more often. Essentially they are using new coal as demand following peaker plants rather than base load power generation. “In the first decade of the 2000s, plants were running around 70% of the time. They're now running around 50%.” https://www.sustainabilitybynumbers.com/p/china-coal-plants#.... By comparison China added 217 GW of PV in 2023 alone which came online over the year. IE adding ~20GW in December has 1/12th impact in 2023, but it’s now online for the next ~25 years. Which is why some predictions have China using less coal in 2024 despite bringing new power plants online in 2023. China alone represents 30% of the world’s electricity production/consumption so it has real impact, but they just don’t have the natural gas supplies which the US has used to ween itself off coal. reply rgmerk 7 hours agorootparentApparently part of the issue is that the Chinese grid is very badly managed. They can build physical grid connections much more easily than we can manage (for the usual values of “we”), but they manage interprovincial transmission according to fixed schedules rather than responding to demand. reply dalyons 7 hours agorootparentprevAlso critically they don’t have to build these coal plant to be economically used in the same way as free market states do. They’re building them mostly as a hedge - if they end up barely used , which is likely given renewable trends, that’s ok for state owned power. It makes everything a bit more expensive, but you can’t exactly vote for an alternative. If they need to get used a lot for whatever reason, no one is unhappy either, you avoid shortages and discontent. Whereas coal stopped being built in market economies roughly as soon as it became uneconomical. reply soperj 4 hours agorootparentHow do you vote for an alternative in the US? reply Owlettotoo 22 minutes agorootparentprevIf I am not mistaken, a good portion of these newly constructed coal plants are meant to replace existing ones that are obsolete and inefficient. It would be hard to judge its impact on net pollution without relevant data on plants retired. reply resolutebat 5 hours agorootparentprevChina is simultaneously also adding much more solar, wind and nuclear power than any other country, plus much of the new coal construction is also meant to replace old, horrifically inefficient/polluting ones. reply contingencies 4 hours agorootparentAlso China's population are moving from the frozen north to the warm south, construction standards are increasing (better insulation), cooking is being outsourced (fewer kitchens), and they lead the world in mass transit / ride hailing / vehicle sharing / portable e-vehicle (scooter/wheel types). China has possibly reached peak personal vehicle. Local governments frequently spray water along main roads to bring down air pollution levels - surprisingly effective for PM2.5 - and most cities are pushing industrial land use far out of town. reply Brian_K_White 9 hours agorootparentprevWhat of it? China + India = 2.864bn Not China or India = 5.244bn I think the characterization that a turning point has occurred is perfectly supportable. The article never even tried to say anything other than that it only just barely happened 11 minutes ago and a few metrics metrics don't show a clear past peak yet. reply defrost 6 hours agorootparent\"The article\" https://www.iea.org/reports/coal-2023/supply was a few paragraphs summerising the global coal supply as a subsection of the IEA Coal 2023 report. If you fell that detail was lacking you can look to either the executive summary https://www.iea.org/reports/coal-2023/executive-summary or download and read the full 130 page report (PDF) https://iea.blob.core.windows.net/assets/a72a7ffa-c5f2-4ed8-... reply fooker 7 hours agorootparentprevWhatever China and India do, their neighbors follow with impunity. Your math might need adjusting for that. reply mulmen 9 hours agorootparentprevDo you believe what you wrote here is a constructive, insightful comment? The comment you are replying to addressed that even India and China are building renewables. It requires very little energy for you to spread negativity but potentially thousands of people then have to internalize it. Why choose to do that? Like burning coal, your negativity pollutes the environment of HN and the internet at large. reply skissane 9 hours agoparentprev> - Peak baby - 2013. [2] I think your source's claim that \"The planet will never see more babies than it has in 2013\" is open to question. To just use the US as an example: mainstream Americans have a declining fertility rate, and we have no reason to think that is going to reverse in the foreseeable future. Immigrants have higher fertility, but they tend to converge to the mainstream after a generation or two. So, that seems to support the article's contention, at least for the US – and if we look at other countries, we observe things are broadly similar, so that in turn supports that contention for the planet as a whole. However, we also observe in the US small, ultra-conservative religious minorities, such as the Amish and ultra-Orthodox Jews, who still have a high fertility rate, and also have high youth retention (80-90% of their children will stay in the group as adults). Now, even though these groups are a tiny percentage of the population, the miracle of exponential growth means that within 2-3 centuries, they could be the majority of the US population, which could then result in a national baby boom. And, eventually, as they spread across the globe (they exist to varying degrees in other countries too), a global baby boom. So 2013's \"peak baby\" may end up being surpassed. Of course, there is no guarantee that is going to happen – maybe they'll get to a certain size, and then they'll stop growing. And of course, it is physically impossible to sustain exponential growth indefinitely. However, it remains a possible future that they won't stop growing until after they've gotten so big that 2013 turns out not to be the year of \"peak baby\" after all. Maybe it will actually be 2213 or 2313 instead. Our descendants (if one happens to have them) will find out. reply DavidPiper 9 hours agorootparent> Maybe it will actually be 2213 or 2313 instead. Our descendants (if one happens to have them) will find out. Peak-anything is just shorthand for \"the first peak on record\" because our culture and ability to record-keep has only really existed in a population and resource boom period. I agree that circumstances might mean that the global peak baby might be 200-300 years away, but the thing that makes the current round of peak-anything relevant is that we don't know how our social and economic systems will function when certain things are in long-term decline (even if they do pick up again in a few hundred years). If we knew what a few centuries of low birth rates or low oil consumption looked like, we wouldn't be nearly as interested in what \"peak-those-things\" means, the same way we're not all that interested in specific market peaks because we understand the boom/bust cycles and long-term productivity increases, etc. reply hiAndrewQuinn 5 hours agorootparentprevPutting aside the considerable statistical skepticism, the natural next question we all have at noticing these trends is - is there any way for secular society to get some of the \"special sauce\" these ultra-religious groups have, before the aggregate supply of everything starts to fall? Secular pronatalism doesn't seem like it has a lot of staying power with most people as an ideology. But I've been on board for a long time, and so have a lot of my friends. So maybe the best answer is the common sense one: Wait a few generations for all those not susceptible to the secular pronatalism mind virus to select out of the gene pool, and hope society doesn't crumble under its own technical debt in the meanwhile. reply skissane 4 hours agorootparentI’m sceptical about how successful secular pronatalism can be. Large families, generation after generation, can come at a substantial personal cost, and it is a difficult sell for individuals if it is just a matter of principle, as opposed to something backed by promises and threats of post mortem reward and punishment. Furthermore, many religious pronatal groups reduce defections by socially ostracising and demonising defectors, making defection expensive. That kind of behaviour is much easier to justify given religious premises than secular ones. So I doubt secular pronatalism is ever going to be as successful as religious pronatalism. At its best, it might see some success under particularly favourable circumstances, but religious pronatalism will thrive in far less favourable conditions reply TheCoelacanth 3 hours agorootparentTo be successful, I think we need to bring back the \"it takes a village to raise a child\" mentality. We need to take some of the burden off of the parents. reply riffraff 4 hours agorootparentprevThe special sauce seem to be forbidden contraceptives and limited opportunities. Yes, there's also a kind of mandate in the bible to be fruitful and multiply, and the fact that if you grow up in a larger family you tend to have a larger family, but I would bet the other factors matter more. reply vouwfietsman 4 hours agorootparentprevSecular pronatalism as an ideology is new to me, but obviously the main limiting factor in fertility rates globally is just a mismatch in the lifestyle of the modern luxury-expecting freedom-enjoying individual vs the self sacrifice needed to do child raising. I would say a significant part of that is financial and time-related, both things which have obvious political, not ideological, solutions. Fighting the ideology of individualism with an ideology of self sacrifice did not work for climate change and will not work for pronatalism. I often joke to my peers: it doesn't matter what I invest in, my best financial decision was buying a house, my worst financial decision was having kids, the rest is just screwing around in the margin. reply nyokodo 5 hours agorootparentprev> Wait a few generations ... and hope society doesn't crumble We don't have a few generations until we are beyond the point where we can mathematically recover unless we develop economic mass adult-cloning technology and develop the psycho-social faculties to integrate that into a healthy (enough) society. The world won't end, but let's just say hyper-advanced visions of the future seem unlikely at this point. reply globalise83 3 hours agorootparentprevThe fact that these are still very much niche interests after many generations indicates that the social indoctrination processes needed to maintain such cultures are not scalable. reply skissane 3 hours agorootparent> The fact that these are still very much niche interests after many generations indicates that the social indoctrination processes needed to maintain such cultures are not scalable. In 2023, Israel contained over 1.28 million ultra-Orthodox Jews, 13.5% of Israel’s population. It is estimated that by the end of this decade, it will be over 16%. [0] By shortly after 2065, it is estimated that 50% of Israeli children will be Haredi. [1] I think this is counterevidence to your claim - the social indoctrination processes necessary to maintain Israeli ultra-Orthodox Judaism have already scaled to over 1 million people, and are projected to scale much further than that [0] https://www.timesofisrael.com/haredim-are-fastest-growing-po... [1] https://archive.md/lYYgK reply navane 3 hours agorootparentprevI heard the retention of their offspring was 20%, so they have 5x more offspring, but only 20% of that stays 'in the group', net result 0. reply skissane 3 hours agorootparentKiryas Joel, New York, is growing over 50% per decade. That is not consistent with “net result 0”. If you look at the actual statistics on the growth of ultra-Orthodox Jews in the US and Israel, and Old Order Amish in the US, the actual figures are inconsistent with “net result 0” reply bdjsiqoocwk 9 hours agorootparentprev> the miracle of exponential growth means that within 2-3 centuries, I don't know if any realm of human sciences where extrapolating exponential growth yields trustworthy results, outside maybe of wealth accumulation, and even then not for 2 centuries. reply skissane 9 hours agorootparentI never claimed that it is inevitable that they'll sustain their current growth rate for the next 2-3 centuries, only that it is entirely possible. And I don't see why it wouldn't be. If you look at ultra-Orthodox Jews in New York/New Jersey, I don't see how they'd hit any natural barriers to their continued growth until they are many times larger than they are now, at which point they'd be a serious challenger for becoming the population majority. The main ways it might not happen would be if either (1) they (gradually or suddenly) abandon their current culture for a less fertile one, (2) mainstream society persecutes them sufficiently. Both are entirely possible, but neither is anywhere near certain. The situation for the (Old Order) Amish is more difficult, since – unlike ultra-Orthodox Jews – they'll run out of enough land to sustain their agrarian lifestyle, and will have to transition to a more urban one. While the more urban lifestyle of ultra-Orthodox Jews demonstrates it is possible for insular high fertility religious minorities to exist in an urban setting, there is a real risk that they might lose their fertility and/or their insularity in the process. Also, people often bring up the problem Israel has with many ultra-Orthodox Jews not working and relying on government subsidies to live. That is much less of a problem in the US than it is in Israel, so the sustainability of that lifestyle is less of a barrier to future growth in the US than it is in Israel. Furthermore, the fact they manage to grow so much in the US without doing that, means being forced to stop doing that isn't necessarily going to stop their growth in Israel either. reply joshuahedlund 8 hours agorootparentWhat are their historical and present population and growth numbers? reply skissane 7 hours agorootparentConsider a place like Kiryas Joel, New York: in the 2000 census it had a population of 13,138; by 2010 it had grown to 20,175 (a 53.6% increase over the decade); by 2020 it had grown to 32,954 (a 63.3% increase over the decade); the US Census Bureau’s 2022 estimate is 38,998 - 18.3% in only two years. The vast majority of that growth is due to births not migration. It is now the largest municipality in its county, and also its MSA - in 2023, the White House renamed the Poughkeepsie-Newburgh-Middletown MSA to the Kiryas Joel-Poughkeepsie-Newburgh MSA, in recognition that it has overtaken Poughkeepsie proper as the MSA’s most populous municipality (although greater Poughkeepsie is still larger) And Kiryas Joel is expanding in its de facto area: as its spiritual leader, Satmar Rebbe Aron Teitelbaum likes to say, its “holy borders” go further than its legal boundaries under New York state law, although very likely the legal boundaries will at some point grow too. And, there are similar ultra-Orthodox communities in other parts of New York state, and also in New Jersey (especially Lakewood) reply jacobolus 6 hours agorootparentUltra-orthodox communities (like everyone else in the modern world, with vanishingly few exceptions) are entirely dependent on the broader society/economy. Under the implausible scenario where they approached a majority of the population, they'd inevitably (a) diffuse much more broadly into the rest of the population, and (b) make their current set of organizing practices unsustainable, without any persecution involved. reply skissane 6 hours agorootparentOf course they are going to have to change as they grow. The question is how big the changes will be. We already see this in Israel - a big increase in Haredi women pursuing secular careers. In other communities worldwide, that development ended up significantly undermining the patriarchal culture and producing a demographic transition to lower fertility. But, will it necessarily have the same consequences for the Haredim? We will have to wait and see: maybe it will, maybe it won’t. Precedent would suggest the Haredim won’t be able to avoid that outcome, but the situation on the ground suggests that maybe they will I’m not claiming any of this is inevitable, only possible, plausible. Nobody knows for certain what the future holds-we shall find out reply jacobolus 2 hours agorootparentAlmost anything is \"possible\", but this seems entirely implausible, based on extrapolation that goes at least an order of magnitude beyond anything supportable by evidence or careful argument. Can you name any other examples of this happening throughout history, in the US or anywhere else, where a completely self-isolated tiny minority group took power and displaced everyone through \"out-procreating\" them? To be honest I really dislike this kind of vague speculative fearmongering when targeted at specific minority groups, which seems extremely dangerous. It historically blends right into overt bigotry and sectarian oppression. The US has a long, proud tradition of all sorts of unusual sects and cults trying to do their own thing, often in somewhat closed communities, sometimes to the dismay of their neighbors, sometimes failing pretty badly, but usually without really breaking fair laws or causing serious mischief to anyone else (but also occasionally breaking a lot of laws and hurting people; the police should go after such cults). We all owe quite a few of the rights we take for granted to the hard struggle of some of these groups to make their own choices. reply lotsofpulp 1 hour agorootparent>Can you name any other examples of this happening throughout history, in the US or anywhere else, where a completely self-isolated tiny minority group took power and displaced everyone through \"out-procreating\" them? Kiryas Joel NY, Lakewood NJ, certain parts of Brooklyn. They basically have their own government, and they have far less state government oversight due to politicians not wanting to go against them. They drive around NYC with vehicles that look like cop cars, acting like cops, which would get most other groups charged with impersonating a government official. They elect themselves to the school board and vote to prioritize funding for their group’s children, and deprioritize funding for any other group’s children. Etc etc. Not that they are the only group to have done it, but these are your easy to google examples. reply andsoitis 8 hours agorootparentprev> mainstream Americans What is a mainstream American? reply Andrex 6 hours agorootparentLegally speaking? https://en.wikipedia.org/wiki/Reasonable_person reply SantalBlush 6 hours agorootparentprevAny American who likes Tom Petty. reply skissane 7 hours agorootparentprevI mean mainstream American society of secular to moderately religious people, as opposed to ultra-religious minorities such as ultra-Orthodox Jews or Old Order Amish I mean the term quite broadly and inclusively, since many of the big divides within American society (ethnicity, race, politics, class) make a relatively modest difference to fertility rates, and those differences appear to be shrinking over time reply iiovemiku 9 hours agoparentprevWow, seeing peak baby is a little scary honestly. I had always thought that low birth rates were a problem for only countries like Germany, Spain, Japan, etc. (at least for the time being). Seeing countries like India even out (and even going under replacement rate) gives me some heavy pessimism about population graying. I had always thought that heavy swingers like them would carry on the whole growing population thing for a good few more decades. Looks like we're more or less on a crash course in the next few decades, especially with lifespans moving as they are. reply rqtwteye 7 hours agorootparentAt some point we can’t rely on ever growing populations. It may be painful for a while but at some point growth has to stop. reply Nicholas_C 9 hours agorootparentprevWill be very troublesome for countries with low fertility that can’t attract immigrants to keep the population from declining. reply wongarsu 9 hours agorootparentprevWe still won't reach peak population until 2060 at the earliest, and more likely around 2100. [1] has a good graph by region on page 4, and a graph for world population near the end. Most regions haven't reached their population peak yet and subsaharan Africa is still growing strongly. Of course a lot of this is just delayed by higher life expectancies. Aging populations will require major social reforms in our lifetime, and I don't think society is really prepared 1: https://www.un.org/development/desa/pd/sites/www.un.org.deve... reply TheCoelacanth 2 hours agorootparentI think you're looking at peak population, while they are talking about peak number of children. According to some reports, the number of children has already peaked[1]. The remaining growth is just momentum from past births. [1] https://www.abc.net.au/news/2022-11-13/earths-population-rea... reply grecy 9 hours agorootparentprev> Wow, seeing peak baby is a little scary honestly Are you worried 8 billion is somehow not enough humans? reply magicalhippo 9 hours agorootparentThe issue is that a lot of countries have based everything around growth, including population. Pensions is a big one, another one is care for elders. It's not a problem if we had eased up to say 1 billion and stayed there, but the rate of change is quite abrupt. reply grecy 5 hours agorootparent> The issue is that a lot of countries have based everything around growth, including population And it is very, very ,very clear this is a horribly stupid and shortsighted thing to do. It is simply impossible to have constant growth, clearly it is unsustainable. The system HAS to change. It might be from a slow and steady decline, or it might be a very abrupt change, but it's going to change one way or another. reply sidewndr46 9 hours agorootparentprevYeah, I've never understood the panic over a declining birth rate. Are we to be scared that long after we're all dead and gone there would only be several billion humans on this planet? reply lotsofpulp 7 hours agorootparentThe concern is increasing taxation of the young, productive population to sustain the old, non productive population causing political turmoil and decreasing national competitiveness on the global scale. reply Lammy 9 hours agorootparentprevLuckily I'm one of the Chosen Few who would always have been allowed to exist. reply casercaramel144 9 hours agorootparentI mean yeah. Not existing has a utility value of 0. You can make the same argument for people who don't exist yet. Is it infinite utility to go around as a government and force people to pump out 100 babies a year? Since not existing is so bad? TBH if I never existed by definition I would be fine with it, you know, since I don't exist and was never born. I don't think its coherent to measure things from aggregate utilitarian POV, since the optimal solution seems relentless expansionism like a virus. reply Lammy 8 hours agorootparent> I don't think its coherent to measure things from aggregate utilitarian POV I do, because second-person collectively-singular Humanity is a living thing all its own, and the more humans there are the more alive We are. Your argument is the anthropological equivalent of “640K ought to be enough for anybody”. reply harimau777 7 hours agorootparentHaving more than 640Kb of RAM isn't a good in of itself, it's only good in that applications arose which required more RAM. Similarly higher population isn't a good in of itself. It seems to me that there's much less evidence that there's something that needs higher population. I don't see how higher population necessarily makes humanity as a collective organism more human. That seems like saying that an individual human is more human if they weigh more. reply casercaramel144 5 hours agorootparentprevSo by the tyranny of exponential growth, we should just start building massive breeding factories and forceably enslaving people randomly matching them to have children? Because this could actually be the optimal policy if we take your view of \"second persons\" to it's optimum. In your world governments forceably breed humans like chickens in massive factory farms churning out people to the carrying capacity of the planet. I don't want to live there and I sure as hell don't find it moral. reply ipaddr 4 hours agorootparentWhy not encourage through policy and taxation changes? Invest in culture that promotes. Restructure society to encourage having babies at an earlier age. reply hehdhdjehehegwv 9 hours agorootparentprev100% agree, this is not a real problem and is better than the alternative. reply iiovemiku 9 hours agorootparentprevPopulation increase (and the increase of its increase) has always been a constant for as long as I've lived so yes I suppose. I don't think we are prepared for the spout of working-age people to start slowing down (especially with the population of the elderly only getting higher). Assuming we only get this planet though, I suppose it couldn't've gone like this forever haha. reply moralestapia 7 hours agorootparentprevYes, I am. Humans don't live forever. No offspring and it's all gone in one generation, disregarding if they're 8 billion or 100 billion. reply Retric 7 hours agorootparentHumanity will eventually go extinct, but birth rates would need to decline much further or stay this for a very long time before it’s a meaningful issue. Further, people alive in 2300 will be decedents of people who chose to have kids generation after generation despite living in an industrialized environment. That self selection both in terms of DNA and culture means a population bounce back becomes increasingly likely over time especially as fewer people means less pollution and less competition for resources. Humanity might even end up cycling through industrialization, collapse, hunter gatherers, agriculture, industrialization, multiple times before settling on some stable equilibrium. You just can’t extrapolate exponential curves indefinitely when they depend on the population size. reply xnx 9 hours agoparentprevCrossing my fingers that we've also hit \"peak extinction\", and not just because we've killed off most of the vulnerable species already. reply sMarsIntruder 4 hours agoparentprevEveryone is entitled to their own ideas, but are we really associating 'peak baby' with global pollution? Humans certainly contribute to pollution, but we should address some ethical questions first. reply mpreda 4 hours agorootparentWell, yes. And, what are the ethical questions we should address first? reply elijahbenizzy 5 hours agoparentprev“Peak baby” is an objectively bad thing reply WA 1 hour agorootparentIt's not. We need a sustainable size of population. Unlimited population growth is not sustainable. It doesn't mean that we are going to die out. It just means that there are as many newborns as old people dying. Check out this good old Hans Rosling video: https://www.youtube.com/watch?v=ezVk1ahRF78 reply monero-xmr 9 hours agoparentprevMy problem with the “peak population” hypothesis is that all of the non-breeders will eliminate themselves from the gene pool soon. All of us that have kids for whatever reason chose to reproduce despite the abundance of modern society. Also the groups that have many kids (the Amish have 7 children on average, and you also have Mormons and Orthodox Jews) will expand exponentially. reply skissane 9 hours agorootparent> Also the groups that have many kids (the Amish have 7 children on average, and you also have Mormons and Orthodox Jews) will expand exponentially I agree with your overall point, and about the Amish and Orthodox Jews (especially the ultra-Orthodox, Hasidic/Haredi). However, for Mormons, I'm not sure that is true. Mormons, in the US at least, have plummeting fertility rates, and also a lot of problems with retaining their younger members. They don't belong to the same category as Amish and ultra-Orthodox do, they are converging to the American secular mainstream while those groups remain on a clearly different trajectory from it. reply bryanlarsen 9 hours agorootparentEvery Mennonite I know has 0 kids. Of course I only know those who have left the community, but I think it is an illustrative anecdote. reply skissane 9 hours agorootparentWhen I say \"Amish\" I mean \"Old Order Amish\". Mennonites have both lower fertility and lower retention, so their demographic future is less bright overall. That said, there is a subgroup of the Mennonites, the Old Order Mennonites, who have fertility and retention much closer to that of the Old Order Amish. The Wenger Mennonites, in particular, are rather similar in those measures. But only around a third of Old Order Mennonites are Wenger, and only around 15% of Mennonites in the US are Old Order. reply monero-xmr 9 hours agorootparentprevIt’s also possible that secular society creates effective off-ramps to convert the hyper-religious to secular values. I’m uncertain if this will work given how successful the in-group remains despite the internet and modern social media. reply skissane 9 hours agorootparentMany ultra-Orthodox Jews use \"Kosher Internet filters\" – filters that block content that isn't approved by their rabbis. In Israel, you can buy \"Kosher phones\" which have this software pre-installed by the telco and locked down so you can't remove it. People assume that technology necessarily encourages secularism, but it doesn't inevitably do so. Technology also makes possible new methods of social control which can be used to suppress secularism. reply silverquiet 9 hours agorootparentprevGod is dead and we have killed him - Nietzsche was onto this quite early into modernity. reply Apocryphon 9 hours agorootparentprevToo early to tell. Wait a couple more generations. reply developerDan 9 hours agorootparentprevWho is to say children today will have children of their own tomorrow? People who are choosing not to have kids “leaving the gene pool” (you say it as if they are some filth to be done away with) doesn’t change anything. reply randomdata 9 hours agorootparentThe people of typical child rearing age today grew up in the \"16 and pregnant\" era, where having children was demonized. They were told that to have children was a failing, and that they should focus on their career instead – that was the key to a successful, happy life. Social pressure is a hell of a drug. Is that fashion going to remain, though? Everything goes out of style eventually. I think we are already seeing some cracks where people are starting to question why having children is so \"wrong\". Nothing happens overnight, but I'm not so sure the children today will grow up in that same environment. reply silverquiet 9 hours agorootparentI think the economics trump the social factors mostly. It seems to take ever more education in order to grasp at an ever more ephemeral stability, and children need a decade or two of stability when growing up. I can’t imagine anyone will be encouraging their 16 year old daughters to have children any time soon. reply randomdata 9 hours agorootparentIt is true that once you are rich enough you no longer need children to help support the family unit as was an imperative historically (and still in the poor parts of the world), so we cannot discount the economics. That is no doubt why it became the fashion. It was a demonstration of how rich we've become. A display of human progress and achievement. But I see some change in sentiment around questioning what good is being rich if you can't enjoy it with your children. It is not happening overnight by any stretch of the imagination, but I think the tides are slowly starting to turn. reply silverquiet 8 hours agorootparentI look back at Leo Tolstoy's \"A Confession\" and I think that it's a bit more complicated than that. He summed up my own thoughts quite well more than a century before I was even born. > No matter how often I may be told, \"You cannot understand the meaning of life so do not think about it, but live,\" I can no longer do it: I have already done it too long. I cannot now help seeing day and night going round and bringing me to death. That is all I see, for that alone is true. All else is false. The two drops of honey which diverted my eyes from the cruel truth longer than the rest: my love of family, and of writing -- art as I called it -- were no longer sweet to me. \"Family\"... said I to myself. But my family -- wife and children -- are also human. They are placed just as I am: they must either live in a lie or see the terrible truth. Why should they live? Why should I love them, guard them, bring them up, or watch them? That they may come to the despair that I feel, or else be stupid? Loving them, I cannot hide the truth from them: each step in knowledge leads them to the truth. And the truth is death.” reply randomdata 8 hours agorootparent> Why should I love them, guard them, bring them up, or watch them? Once upon a time there was no choice if you wanted to survive yourself. The world was too much for the feeble man without their help. Indeed, the rich now have the luxury of relying on \"corporations\" to stand in for where children were once necessary. But then you're ultimately back to square one: Why should you love, guard, bring up, and watch the corporations? There is no free lunch. You are going to put in the effort either way, but at least children might also provide some happiness along the way. The \"corporations\" seem to just draw ire. We didn't recognize that for a long time, but I do see a shift starting to take place. reply silverquiet 7 hours agorootparentI think we are just all different - I can't imagine children bringing me any happiness; I felt the way Tolstoy did already when I was a young child. Interestingly, my grandmother once told me she only had children because of social expectations, and I can say that her children were absolutely aware of that. For me, the philosophical reasons were enough; the monetary savings are in a sense a bonus, but for others economics may be the main force preventing them from having children which is indeed sad in its own way I suppose. reply randomdata 7 hours agorootparent> but for others economics may be the main force preventing them from having children Economics never prevents having children. As before, only the rich even get the luxury of choosing to not have children. But the rich could have children too if they so choose. Their fear of children making them look poor under the whims of today's fashion is an entirely self-imposed limitation. Good for them if that's what they want to do. No judgment on anyone's personal life choices. But I maintain that an increasing number of people are starting to question if that is what is right for them. I agree that what is right for an individual is not universal. Some people will truly not want children, but many more feel pressured to not have children due to the prevailing fashion trends. I see change afoot among the latter group. Having children is slowly starting to become \"cool\" again. reply silverquiet 7 hours agorootparentFundamentally I just disagree with you I think; I'm not seeing any signs that trends in fertility are turning around, and I think if anything it was social pressure that was holding the numbers up to begin with. I'm certainly willing to admit that I could be wrong on that though; the millennials, a large echo generation from the boomers (myself among them) are hitting the age where it becomes a sort of \"now or never\" proposition and anecdotally, I do see some people considering it. But I also think that religion is one of the big drivers of social pressure for fertility, especially in the US, and you can see it continue to collapse which I think is a sign of the way things are going. reply randomdata 5 hours agorootparent> I'm not seeing any signs that trends in fertility are turning around I'm not sure how you could. The sentiment is only just starting to change as far as I can tell. It is too late for the current crop of young-ish adults. But I don't see the next generation, of what generation there is, coming up in the same environment where having children is demonized and seen as something reserved for the poor. For them, I fully expect having children will be the display of wealth; the \"cool\" thing to do. We see over and over and over again that the rich use their resources to set themselves apart from the poor in some way and then the poor try everything they can to emulate them. It is a tale as old as time. In this instance we saw the rich start to afford the luxury to choose to have children, and poorer people have been on the quest to copy them ever since. But now we're nearing a critical mass where the world has become rich enough that even the poorest people are now able to start thinking about foregoing having children. That signals that the current fashion trend is on the outs. I'm starting to see a shift towards \"Look at how rich I am. I can afford to have children and you can't!\" You even alluded to that same shift in a previous comment, so it seems you're seeing it too. And we should expect something of the sort as it is the natural progression of fashion. reply kiba 4 hours agorootparentprevThe problem is the lack of stability not wealth, though wealth should contribute to stability. Housing and transportation continue to dominate American household budget. Now, I did read that somebody suggesting that it's not cost but density that reduces population fertility. I would wonder if that just means we need to provide more spaces for families within cities. reply dheera 9 hours agorootparentprev> They were told that to have children was a failing Were they? High-achiever families routinely demonize having relationships at 16 but then turn it around VERY QUICKLY after getting that college degree and want their children to get married and have kids before 30. (That said many high achievers themselves don't actually want to have kids, despite family pressure to have kids at 30.) reply monero-xmr 9 hours agorootparentprevIf you don’t reproduce then your genes are not passed on. You have self-selected not to be part of the human race any longer. My hunch is that the people who still decide to reproduce, despite all the reasons people who advocate for not having children talk about, likely have some sort of genetic predisposition for reproduction - a strong innate need. After all the non-breeders die off, the future belongs to the reproducers. reply fallingsquirrel 9 hours agorootparentWhat a wild take. This is like claiming gay people will \"die off\" in 2 generations because they don't reproduce. 1000 years ago some subset of people chose not to have children, and humanity did just fine, and that same group of so-called \"non-breeders\" still exists today. Therefore we can conclude it's not purely a matter of genetics. There are a huge number of reasons people make that choice. People even change their minds during their life. It's not just a YES/NO switch in your genetic code somewhere. reply skissane 7 hours agorootparent> 1000 years ago some subset of people chose not to have children, and humanity did just fine, and that same group of so-called \"non-breeders\" still exists today. There have always been, and always will be, people who don’t have kids, for a whole host of reasons. But that’s not the argument (or at least, not the steelman version of it, as opposed to the strawman one) - the argument is that if there are certain heritable traits that discourage people from having children, then all else being equal, natural selection will cause the frequency of those traits to decline over time, albeit often not to zero. The all else being equal part is very important. In a society with strong social pressure to reproduce, a trait which makes people less likely to want children may not be strongly selected against - because the social pressure to reproduce means desire to have children only has a small impact on the odds of actually having them - whereas in a society which is much more individualist, it may have much more of an impact, so the selective pressure against that trait may be much stronger. And of course, a trait which produces less desire for children might nonetheless be selected for because it produces some other countervailing advantage Still, I think the argument does have some weight - that in contemporary Western society where reproduction is far more of a voluntary choice than it once was, biological and cultural factors which encourage reproduction are going to be selected for to a much greater degree than they were in the less individualist societies of decades and centuries past, where less such encouragement was needed reply quesera 6 hours agorootparentprevBoth things must be true. On the one hand, you'd expect humans (animals) to have completely bred out all forms of infertility -- except that there are non-heritable causes of infertility. (In fact, all causes of infertility must be non-heritable, or at least not inherited! :) On the other hand, it's surely true that characteristics which deprioritize or diminish the likelihood of reproduction are bred out, however incompletely. Whether it's a sense of taste that enjoys poisons, a risk-taking brain that kicks in before fertility, homosexuality (in males at least), or just not wanting children. These characteristics are bred down to a sustainable level, obviously. But they are clearly not bred out fully, nor are they consistently bred \"up\". reply skissane 5 hours agorootparent> homosexuality (in males at least) In many traditional societies, there is strong social pressure for marriage and children, arranged and semi-arranged marriages, etc - such that a person’s sexual orientation may not make much difference to their odds of heterosexually reproducing. Some people might enjoy heterosexual reproduction and others might endure it but they’ll do it all the same. So that would limit the selective pressure against genes that increase the likelihood of homosexuality In the mainstream contemporary West, if heterosexual reproduction doesn’t appeal to you, then you just don’t do it-so selective pressure against those genes may exist to a degree that it formerly did not. On the other hand, the new possibilities for non-heterosexual reproduction (such as IVF, sperm/egg donation, surrogacy) might counteract that. reply skissane 9 hours agorootparentprevNot just in terms of selecting genes, but also selecting cultures. If allele A causes increased desire to reproduce than allele B, then in a society in which reproduction is viewed as an optional extra, all else being equal, allele A is likely to predominate allele B over time. Conversely, in a more traditional society in which everyone is subject to a strong social expectation to reproduce, allele A would have far less of an advantage over allele B. An allele might lead to increased reproduction indirectly rather than directly. For example, if an allele makes a person more likely to be religious, and if religious people are more likely to have kids, then even though that allele does not directly impact desire to reproduce, it may be selected for due to its indirect impact on reproduction. That's genetics; coming to culture: if subculture A puts higher emphasis on reproduction than subculture B, then all else being equal, in the long-run subculture A is likely to outnumber subculture B, irrespective of any genetic factors. However, defections from subculture A to subculture B may erase much of its innate demographic advantage. This suggests in the long-run, the most demographically successful subcultures will be those which combine sustained high fertility with sustained insularity (social barriers to defection to other subcultures)–which is exactly what we observe with groups like the Amish and ultra-Orthodox Jews. reply monero-xmr 9 hours agorootparentIt’s surprising to me why my parent comment was down voted. It’s simple biology and mathematics. If you want culture to survive (if what is most important to you is cultural values) you must have a society with children in order to impart those values. There is no future for a society that discourages children. reply DavidPiper 9 hours agorootparent> After all the non-breeders die off, the future belongs to the reproducers. I suspect it was downvoted because that line suggests dismissal or contempt for people who don't have children. There are many people across all generations who haven't had children (either by choice or because they were unable for various reasons), and their lifespans and social contributions are no more or less on average compared to people that do have children. reply quesera 6 hours agorootparentIronically, \"breeder\" is (or used to be?) a slang denigration of heterosexuals, within the homosexual community. I didn't interpret the original comment as contemptuous of either opinion. But it is undeniable that both groups are sensitive about their choices (or unfortunate inability to choose differently). reply sidewndr46 9 hours agorootparentprevThat isn't really how that works at all. If you have a brother and a sister who both have male and female children, that's basically the same genetic line going on. The only exception I can think of is if you have some mutation they didn't. But in that case if you're aware of the mutation the consequences are likely to be awful, which is a great case for not having biological children. reply iiovemiku 9 hours agorootparentprevFrom my perspective, the only reason they are resistant to societal factors that drive people to have less kids is that their strong religious values (which are held together by their strong communities). So this really begs the question of what society will look like if the only people who will be breeding in the future are those who form tight communities that promote relatively religiously extreme values (that happen to include having lots of children). reply oconnor663 9 hours agorootparentprevIf we could keep running this era of civilization forever, Matrix style, I'm sure that would be true. But how many more generations will there be before AI+biotech lead to us, I don't know, 3D-printing people? The old laws might not have many cycles left to go before the new ones show up... reply justin66 8 hours agorootparentprev> My problem with the “peak population” hypothesis is that all of the non-breeders will eliminate themselves from the gene pool soon. Ah yes, the \"breeder\" gene. Very sound thinking. reply AtlasBarfed 8 hours agorootparentprevMy problem with it is that it is a public policy failure. Capitalism and perpetual economic growth fundamentally needs more people. At some point economies will do the math and public policy and figure out how to keep the expansion going. The US hasn't done it because Mexican immigration fills the gaps. But Germany, China, Russia, and (holy crap) South Korea (although I suspect South Korea will absorb a shitton of North Koreans some day when the regime collapses). What happenes when biotech extends human life and achieves the artificial womb? Go ahead, say its impossible. reply silverquiet 9 hours agorootparentprevIf the future is agrarian, extremely-religious people living a very low-productivity lifestyle of essentially subsistence farming, then that’s probably actually a nice balance for the Earth and humanity. However it really doesn’t feel like that’s where things are headed in spite of the current demographic trends you note. In a sense, I think that’s the way the Americas were before Columbus re-united them with the old world. reply harimau777 7 hours agorootparentHave you lived in that sort of society? It's not great for humanity. reply SubiculumCode 7 hours agoparentprevPeak Plastic? reply illiac786 5 hours agoprevI find this title highly misleading “peak emission” would be much more accurate. “Peak pollution” sounds like the amount of pollution in the environment will actually _decrease_ going forward. That is simply not true, it will continue to increase, just slightly less fast. Sorry if I don’t feel euphoric right now. reply jereze 9 hours agoprevWell, such an optimistic title, considering that greenhouse gases are excluded in the article. reply bryanlarsen 9 hours agoparentGreenhouse gases will most likely peak in 2024, but 2023 & 2025 are also probable. https://climateanalytics.org/comment/will-2024-be-the-year-e... reply jereze 8 hours agorootparentEven if we are close to peaking emissions, we must consider the phenomenon of accumulation. Given that CO2 has an effective lifespan of around 100 years and methane about 10 years in the atmosphere, reductions in emissions now will still result in these gases accumulating and impacting the climate for decades to come. reply hollerith 8 hours agorootparentWhere you have \"accumulating\", you mean persisting. reply voxelghost 7 hours agorootparentNo, even if you emit less, you are still 'accumulating', but at a slower rate. And previously released methane is still converting to co2 in the atmosphere, for decades to come. reply hackerlight 9 hours agorootparentprevNo they won't. Only the first derivative of greenhouse gases ('emissions') will peak. Greenhouse gases itself will only peak after the world achieves net-zero. reply bryanlarsen 9 hours agorootparentSince you're being pedantic, greenhouse gases don't have an infinite lifetime in the atmosphere so they will start going down slightly before we hit net-zero. I hope most understood that I meant to say peak greenhouse gas emissions. reply AtlasBarfed 8 hours agorootparentprevYou mean the growth of emission rates. Emissions will still be ongoing for a looooong time. reply yen223 5 hours agoparentprevWe aren't allowed to celebrate any victories anymore? reply hanniabu 5 hours agorootparentNot when they give a false sense of security reply bamboozled 10 hours agoprevPositive on the one hand, stupid it took us this long on the other. reply MaxHoppersGhost 9 hours agoparentPosted from a huge house with air conditioning while using multiple petroleum products (iPhone/computer, chair, desk, fleece, etc). The pollution lifted every single person’s standard of living across the globe to levels unimaginable 100 years ago. It’s easy to be in Seattle or New York and say we gotta stop burning oil and coal but there are trade offs that were worth making. If you live in a developing country without aircon those trade offs are still worth making. reply vouaobrasil 8 hours agorootparentEvery single person? Not exactly. What about farmers forced off their land for corporate interests? What about people working in the garbage dumps of Tijuana in Mexico [1]? I've actually lived in the \"first world\" (born there) and now have lived over a year in a developing country where most people (including myself) don't have \"aircon\", and whether technology has improved things is not as clearcut as you might think. Yes, we have better health care and more products, but we also have more pollution, more meaningless jobs, weaker local communities (especially in the most developed parts of the world), and beautiful species are going extinct. Actually, I think the standard of living, at least in the long-term and for the many future climate refugees, is not actually that much improved. [1] https://www.latimes.com/archives/la-xpm-1988-07-25-mn-4650-s... reply vouaobrasil 7 hours agorootparentI find it truly amusing how any suggestion that technology LOWERS our standard of living is immediately met with a severe reaction here. It's as though it is gospel, to be assumed without any question. Technophiles are truly narrow-minded. reply fragmede 6 hours agorootparentthe micro view of technology is that it improves things. how could it be otherwise? the things that used to take a painful ten hours can now be done in six, without painf. it's not until you look at capitalism, realize that we're not all on the same team, that things go awry. see, we invented this thing called money, and things have gone downhill ever since. there have been some good things to come out of it, sure, but the thing of it is, we're already at a place where we can make enough food for everybody yet people die of starvation every day because we can't get over ourselves because they don't have money so they don't deserve to eat, even though there's food right there! so technologists keep invention new technologies and society just has to deal with these technologies. because we can't just have a society where everyone is fed and has a roof over their heads and can do whatever else they want to do with their time. so the macro view, that some technologies have actually lowered our standard of living, is difficult, because it's not the technology's fault! it's only when you have to deal with other people that there are problems. throw in 7 billion other people and of course things are bad . reply shkkmo 7 hours agorootparentprev> Actually, I think the standard of living, at least in the long-term and for the many future climate refugees, is not actually that much improved. Compared to what? Where do you draw the line of the \"pre\" technology? Technology has been as part of human society since before we invented history. Do you have a specific point after which you think technology became a net negative? Do you think thinks were fine pre-industrial revolution, but after we invented the steam engine things when to shit? Perhaps it was the domestication of bananas that really took us away from living in harmony with nature rather than twisting it to our own ends. I don't think it really makes a sense to draw a line in history and say everything after this point is bad. The only sensible way to draw a rational technophobic line is to look at specific technologies and their effects and decide if your life will be better or worse with each. reply vouaobrasil 6 hours agorootparentA better way to think of it is in terms of proportions, I guess. To wit, the negative effects of technology are simply an increasing function of how advanced it is. There is no need to think in terms of \"pre\" or \"post\" good technology. As it becomes more powerful, we need greater wisdom to control it. And of course, we completely lack that wisdom. reply knowaveragejoe 8 hours agorootparentprev> Actually, I think the standard of living, at least in the long-term and for the many future climate refugees, is not actually that much improved. This is basically pure anecdote. reply latentcall 6 hours agorootparentprevAh yes I forgot the standard of living is just more stuff in my house! Meanwhile we have all this cheap junk but expensive healthcare, expensive education, expensive housing, poison in our food and water, and rising temperatures. Glad some folks could binge order cheap plastic crap on Amazon for these amazing benefits. Advancement! reply defrost 10 hours agoparentprevIt's worth looking back at 50 years of fossil fuel producers going into overdrive on casting FUD on harmful effects of CO2 since it was first raised at the UN in the 1970s. reply hackerlight 9 hours agorootparentTo clarify the article. This isn't peak CO2. It isn't even peak pollution. It's peak first derivative of pollution ex. greenhouse gases. We'll get to peak CO2 when emissions of CO2 are net zero, so in a few decades at best. Until then atmospheric carbon and average temperatures will keep rising. reply defrost 9 hours agorootparentIIRC I had to do some of that calculus stuff before cutting loose on writing upwards and downwards magnetic field continuations for geophysical exploration software suites, so, yep, cheers for that. That aside, fifty years ago was when the dangers of increasing atmospheric insulation was raised as a serious potential issue in the UN, and fifty years ago was when the think tank ecology surrounding fossil fuel producers started churning articles on ice ages, Climate, who really knows?, killing public transport initiatives, etc. Hindsight is famously 20|20 but it serves us well to remember past playbooks; it wasn't just pure stupidty, there were people who knew better actively promoting larger cars and greater per capita consumption rather than looking to lower total emissions. reply ajross 9 hours agoparentprevSilent Spring was published just 61 years ago (and on top of that into a world with less than half its current population!). Seems like we did pretty well, to me. reply ISL 7 hours agoprevWe may not yet have had the war that ends all wars. Whenever I think about protecting the environment, I think about preventing the catastrophes that are wars. reply xeornet 3 hours agoparentIt would seem politicians, especially in the West do not care at all about that concern. reply zizee 1 hour agorootparentEspecially in the west? reply methuselah_in 7 hours agoprevWith humans burning and producing waste can nothing be like peak. They will keep on going. I don't understand why people don't understand it's human nature until you don't stop them with fines you squeeze the availability of the fossils, they will keep on going on. What soul awakening scientists expects is possible only in few. Govt have to push people with policies and shift their behaviour. reply benreesman 4 hours agoprevIf economic and political capture fail to destroy civilization before climate change does I’ll be glad because we have science for the latter but not the former. Science almost always gives you a fighting chance but there seems to be no motivation let alone research directions on unchecked income inequality. I’m from the United States, a very wealthy country if you use the arithmetic mean to calculate prosperity (which is the devil’s own summary statistic in such matters), but children live in tent encampments even in ostensibly wealthy cities and it goes downhill from there. I hope climate damage becomes a relevant problem because we have ideas for how to tackle that, arguably even credible plans. Income inequality is utterly unchecked and will wreck civilization sooner. reply diob 4 hours agoparentThe problem is that the two are entangled. Those at low incomes (heck even middle income), will be disproportionately hurt by climate change. reply benreesman 3 hours agorootparentI think the problem is that the best and brightest (of which HN’s readership is a plausible sample) dislikes this observation: a great many people who rarely use anything but hard rationality still believe that not only will they be the one to join the tiny elite (can’t be all of them, most are wrong by construction), but that this is acceptable. Of secondary importance is what a depressing commentary this is on the ethical caliber of our intellectual elite. Of primary importance is that only a tennis court and a guillotine will stop them, something one hopes we’d all like to avoid. reply hehdhdjehehegwv 9 hours agoprevThe good news is you’ve taken your foot off the gas. The bad news is the car is going 100mph and there is a cliff edge in 10 feet. reply el_nahual 7 hours agoparentGiven that (with regard to greenhouse gasses) we've a) unlocked positive feedback emission loops from tropical & arctic methane and b) still pumping increasing amounts of carbon into the air, it's clear to anyone paying attention that we've already fallen off the cliff. We're just hovering for a second like Wile E Coyote. reply hehdhdjehehegwv 7 hours agorootparentI often wonder if Looney Tunes physics explains more about our world than classical economics… reply perrygeo 9 hours agoparentprevNailed it. We're confusing the second derivative (how much are we \"accelerating\" pollution) with the real scalar value of interest (distance from the pollution \"cliff\"). Taking your foot off the gas does nothing. Putting your foot hard on the brakes may not even do anything! Momentum do what it do. If humanity can't work through these fundamental math and science comprehension problems, we're doomed. reply unnouinceput 7 hours agoprev2B humans out of 8B are in China. You do not have numbers for China nor, if you manage to get it, can those be trusted anyway. I feel that while the West started the downtrend, China will not offset that but surely pass as how much pollutants it spews. We are not out of the woods until China gets on board too. reply iamthemonster 6 hours agoparentI think I trust the numbers for China more than your estimate of China's population which is out by 43% reply benatkin 7 hours agoprev [–] Many will regret being distracted by the world's problems when they could have been building their lives. https://www.youtube.com/watch?v=gxYt--CFXK0 > Looking back, my only Berklee classmates that got successful were the ones who were fiercely focused, determined, and undistractable. > While you’re here, presidents will change, the world will change, and the media will try to convince you how important it all is. > But it’s not. None of it matters to you now. https://sive.rs/berklee reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Recent data indicates that the world has probably surpassed its peak pollution levels for local air pollutants, with emissions decreasing in wealthier nations such as the US and Europe, and China also experiencing a notable decline.",
      "In contrast, emissions are on the rise in low and lower-middle income countries, highlighting the importance of countries progressing through the \"Environmental Kuznets Curve\" swiftly to achieve lower pollution levels.",
      "The primary focus is on mitigating premature deaths from air pollution, especially in developing nations, without hindering economic progress and energy accessibility."
    ],
    "commentSummary": [
      "The discussion explores population growth, energy consumption, pollution, societal values, and technology's environmental impact, touching on declining birth rates, religious/secular beliefs on fertility, and consequences of peak coal and oil in global energy use.",
      "It emphasizes the necessity for sustainable practices, societal reforms, and global collaboration to tackle environmental issues such as climate change and pollution.",
      "The conversation underscores the interplay of human behavior, societal frameworks, and the environment, stressing the significance of informed decisions and united efforts for a sustainable future."
    ],
    "points": 152,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1715292701
  },
  {
    "id": 40313193,
    "title": "Spectacular Nanoscale Mapping of Brain Cortex Unveils Hidden Features",
    "originLink": "https://www.nature.com/articles/d41586-024-01387-9",
    "originBody": "NEWS 09 May 2024 Cubic millimetre of brain mapped in spectacular detail Google scientists have modelled a fragment of the human brain at nanoscale resolution, revealing cells with previously undiscovered features. By Carissa Wong Twitter Facebook Email Rendering based on electron-microscope data, showing the positions of neurons in a fragment of the brain cortex. Neurons are coloured according to size. Credit: Google Research & Lichtman Lab (Harvard University). Renderings by D. Berger (Harvard University) Researchers have mapped a tiny piece of the human brain in astonishing detail. The resulting cell atlas, which was described today in Science1 and is available online, reveals new patterns of connections between brain cells called neurons, as well as cells that wrap around themselves to form knots, and pairs of neurons that are almost mirror images of each other. The 3D map covers a volume of about one cubic millimetre, one-millionth of a whole brain, and contains roughly 57,000 cells and 150 million synapses — the connections between neurons. It incorporates a colossal 1.4 petabytes of data. “It’s a little bit humbling,” says Viren Jain, a neuroscientist at Google in Mountain View, California, and a co-author of the paper. “How are we ever going to really come to terms with all this complexity?” Slivers of brain The brain fragment was taken from a 45-year-old woman when she underwent surgery to treat her epilepsy. It came from the cortex, a part of the brain involved in learning, problem-solving and processing sensory signals. The sample was immersed in preservatives and stained with heavy metals to make the cells easier to see. Neuroscientist Jeff Lichtman at Harvard University in Cambridge, Massachusetts, and his colleagues then cut the sample into around 5,000 slices — each just 34 nanometres thick — that could be imaged using electron microscopes. Jain’s team then built artificial-intelligence models that were able to stitch the microscope images together to reconstruct the whole sample in 3D. “I remember this moment, going into the map and looking at one individual synapse from this woman’s brain, and then zooming out into these other millions of pixels,” says Jain. “It felt sort of spiritual.” A single neuron (white) shown with 5,600 of the axons (blue) that connect to it. The synapses that make these connections are shown in green.Credit: Google Research & Lichtman Lab (Harvard University). Renderings by D. Berger (Harvard University) When examining the model in detail, the researchers discovered unconventional neurons, including some that made up to 50 connections with each other. “In general, you would find a couple of connections at most between two neurons,” says Jain. Elsewhere, the model showed neurons with tendrils that formed knots around themselves. “Nobody had seen anything like this before,” Jain adds. The team also found pairs of neurons that were near-perfect mirror images of each other. “We found two groups that would send their dendrites in two different directions, and sometimes there was a kind of mirror symmetry,” Jain says. It is unclear what role these features have in the brain. Proofreaders needed The map is so large that most of it has yet to be manually checked, and it could still contain errors created by the process of stitching so many images together. “Hundreds of cells have been ‘proofread’, but that’s obviously a few per cent of the 50,000 cells in there,” says Jain. He hopes that others will help to proofread parts of the map they are interested in. The team plans to produce similar maps of brain samples from other people — but a map of the entire brain is unlikely in the next few decades, he says. “This paper is really the tour de force creation of a human cortex data set,” says Hongkui Zeng, director of the Allen Institute for Brain Science in Seattle. The vast amount of data that has been made freely accessible will “allow the community to look deeper into the micro-circuitry in the human cortex”, she adds. Gaining a deeper understanding of how the cortex works could offer clues about how to treat some psychiatric and neurodegenerative diseases. “This map provides unprecedented details that can unveil new rules of neural connections and help to decipher the inner working of the human brain,” says Yongsoo Kim, a neuroscientist at Pennsylvania State University in Hershey. doi: https://doi.org/10.1038/d41586-024-01387-9 References Shapson-Coe, A. et al. Science 384, eadk4858 (2024). Article Google Scholar Download references Reprints and permissions Latest on: Neuroscience Brain Retuning of hippocampal representations during sleep ARTICLE 08 MAY 24 Structural pharmacology and therapeutic potential of 5-methoxytryptamines ARTICLE 08 MAY 24 Magnetic field effects on behaviour in Drosophila MATTERS ARISING 01 MAY 24 Jobs W3-Professorship (with tenure) in Inorganic Chemistry The Institute of Inorganic Chemistry in the Faculty of Mathematics and Natural Sciences at the University of Bonn invites applications for a W3-Pro... 53113, Zentrum (DE) Rheinische Friedrich-Wilhelms-Universität Principal Investigator Positions at the Chinese Institutes for Medical Research, Beijing Studies of mechanisms of human diseases, drug discovery, biomedical engineering, public health and relevant interdisciplinary fields. Beijing, China The Chinese Institutes for Medical Research (CIMR), Beijing Research Associate - Neural Development Disorders Houston, Texas (US) Baylor College of Medicine (BCM) Staff Scientist - Mitochondria and Surgery Houston, Texas (US) Baylor College of Medicine (BCM) Recruitment of Talent Positions at Shengjing Hospital of China Medical University Call for top experts and scholars in the field of science and technology. Shenyang, Liaoning, China Shengjing Hospital of China Medical University",
    "commentLink": "https://news.ycombinator.com/item?id=40313193",
    "commentBody": "Cubic millimetre of brain mapped in spectacular detail (nature.com)149 points by geox 12 hours agohidepastfavorite74 comments teuobk 11 hours agoThe interactive visualization is pretty great. Try zooming in on the slices and then scrolling up or down through the layers. Also try zooming in on the 3D model. Notice how hovering over any part of a neuron highlights all parts of that neuron: http://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-rele... reply jamiek88 11 hours agoparentMy god. That is stunning. To think that’s one single millimeter of our brain and look at all those connections. Now I understand why crows can be so smart walnut sized brain be damned. What an amazing thing brains are. Possibly the most complex things in the universe. Is it complex enough to understand itself though? Is that logically even possible? reply nicklecompte 11 hours agorootparentCrow/parrot brains are tiny but in terms of neuron count they are twice as dense as primate brains (including ours): https://www.sciencedirect.com/science/article/pii/S096098221... If someone did this experiment with a crow brain I imagine it would look “twice as complex” (whatever that might mean). 250 million years of evolution separates mammals from birds. reply steve_adams_86 8 hours agorootparentThis might be a dumb question, because I doubt the distances between neurons makes a meaningful distance… But could a small brain, dense with neurons like a crow, possibly lead to a difference in things like response to stimuli or “compute” speed so to speak? reply out_of_protocol 3 hours agorootparentRegarding compute speed - it checks out. Humans \"think\" via neo cortex, thin ouside layer of the brain. Poor locality, signals needs to travel a lot. Easy to expand though. Crow brain have everything tightly concentrated in the center - fast communication between neurons, hard to have more \"thinking\" thing later (therefore hard to evolve above what crows currently have) reply michaelhoney 7 hours agorootparentprevActually I think that's pretty plausible. Signal speed in the brain is pretty slow - it would have to make some difference reply Terr_ 10 hours agorootparentprevI expect we'll find that it's all a matter of tradeoffs in terms of count vs size/complexity... kind of like how the \"spoken data rate\" of various human languages seems to be the same even though some have complicated big words versus more smaller ones etc. reply sdenton4 8 hours agorootparentBirds are under a different set of constraints than non-bat mammals, of course... They're very different. Songbirds have ~4x finer time Perception of audio than humans do, for example, which is exemplified by taking complex sparrow songs and showing them down until you can actually hear the fine structure. The human 'spoken data rate' is likely due to average processing rates in our common hardware. Birds have a different architecture. reply Terr_ 6 hours agorootparentYou misunderstand, I'm not making any kind of direct connection between human speech and bird song. I'm saying we will probably discover that the \"overall performance\" of different vertebrate neural setups are clustered pretty closely, even when the neurons are arranged rather differently. Human speech is just an example of another kind of performance-clustering, which occurs for similar metaphysical reasons between competing, evolving, related alternatives. reply pfdietz 9 hours agorootparentprevThat shouldn't be too surprising, as a larger fraction of the volume of a brain should be taken up by \"wiring\" as the size of the brain expands. reply jamiek88 10 hours agorootparentprevInteresting! Thank you. I didn’t know that. reply LargoLasskhyfv 6 hours agorootparentprevIIRC bird brains are 'packed/structured' very similar to our cerebellum. So one would just need to pick that little cube out of our cerebellum, to have that 'twice as complexity'. reply ignoramous 11 hours agorootparentprevI wonder if we manage to annotate this much level of detail about our brain, and then let (some variant of the current) models train on it, will those intrinsically end up generalizing a model for intelligence? reply nicklecompte 11 hours agorootparentI think you would also need the epigenetic side, which is very poorly understood: https://www.universityofcalifornia.edu/news/biologists-trans... We have more detail than this about the C. elegans nematode brain, yet we still no clue how nematode intelligence actually works. reply Animats 10 hours agorootparentHow's OpenWorm coming along? reply layer8 9 hours agorootparentprevWe don’t know what “understanding” means (we don’t have a workable definition of it), so your question cannot be answered. reply throwup238 11 hours agoprev> The 3D map covers a volume of about one cubic millimetre, one-millionth of a whole brain, and contains roughly 57,000 cells and 150 million synapses — the connections between neurons. This is great and provides a hard data point for some napkin math on how big a neural network model would have to be to emulate the human brain. 150 million synapses / 57,000 neurons is an average of 2,632 synapses per neuron. The adult human brain has 100 (+- 20) billion or 1e11 neurons so assuming the average rate of synapse/neuron holds, that's 2.6e14 total synapses. Assuming 1 parameter per synapse, that'd make the minimum viable model several hundred times larger than state of the art GPT4 (according to the rumored 1.8e12 parameters). I don't think that's granular enough and we'd need to assume 10-100 ion channels per synapse and I think at least 10 parameters per ion channel, putting the number closer to 2.6e16+ parameters, or 4+ orders of magnitude bigger than GPT4. There are other problems of course like implementing neuroplasticity, but it's a fun ball park calculation. Computing power should get there around 2048: https://news.ycombinator.com/item?id=38919548 reply throw310822 8 hours agoparentOr you can subscribe to Geoffrey Hinton's view that artificial neural networks are actually much more efficient than real ones- more or less the opposite of what we've believed for decades- that is that artificial neurons were just a poor model of the real thing. Quote: \"Large language models are made from massive neural networks with vast numbers of connections. But they are tiny compared with the brain. “Our brains have 100 trillion connections,” says Hinton. “Large language models have up to half a trillion, a trillion at most. Yet GPT-4 knows hundreds of times more than any one person does. So maybe it’s actually got a much better learning algorithm than us.” GPT-4's connections at the density of this brain sample would occupy a volume of 5 cubic centimeters; that is, 1% of a human cortex. And yet GPT-4 is able to speak more or less fluently about 80 languages, translate, write code, imitate the writing styles of hundreds, maybe thousands of authors, converse about stuff ranging from philosophy to cooking, to science, to the law. reply dsalfdslfdsa 4 hours agorootparent\"Efficient\" and \"better\" are very different descriptors of a learning algorithm. The human brain does what it does using about 20W. LLM power usage is somewhat unfavourable compared to that. reply dragonwriter 8 hours agorootparentprevI mean, Hinton’s premises are, if not quite clearly wrong, entirely speculative (which doesn't invalidate the conclusions about efficienct that they are offered to support, but does leave them without support) GPT-4 can produce convincing written text about a wider array of topics than any one person can, because it's a model optimized for taking in and producing convincing written text, trained extensively on written text. Humans know a lot of things that are not revealed by inputs and outputs of written text (or imagery), and GPT-4 doesn't have any indication of this physical, performance-revealed knowledge, so even if we view what GPT-4 talks convincingly about as “knowledge”, trying to compare its knowledge in the domains it operates in with any human’s knowledge which is far more multimodal is... well, there's no good metric for it. reply Intralexical 3 hours agorootparentTry asking an LLM about something which is semantically patently ridiculous, but lexically superficially similar to something in its training set, like \"the benefits of laser eye removal surgery\" or \"a climbing trip to the Mid-Atlantic Mountain Range\". Ironically, I suppose part of the apparent \"intelligence\" of LLMs comes from reflecting the intelligence of human users back at us. As a human, the prompts you provide an LLM likely \"make sense\" on some level, so the statistically generated continuations of your prompts are likelier to \"make sense\" as well. But if you don't provide an ongoing anchor to reality within your own prompts, then the outputs make it more apparent that the LLM is simply regurgitating words which it does not/cannot understand. On your point of human knowledge being far more multimodal than LLM interfaces, I'll add that humans also have special neurological structures to handle self-awareness, sensory inputs, social awareness, memory, persistent intention, motor control, neuroplasticity/learning– Any number of such traits, which are easy to take for granted, but indisputably fundamental parts of human intelligence. These abilities aren't just emergent properties of the total number of neurons; they live in special hardware like mirror neurons, special brain regions, and spindle neurons. A brain cell in your cerebellum is not generally interchangeable with a cell in your visual or frontal cortices. So when a human \"converse[s] about stuff ranging from philosophy to cooking\" in an honest way, we (ideally) do that as an expression of our entire internal state. But GPT-4 structurally does not have those parts, despite being able to output words as if it might, so as you say, it \"generates\" convincing text only because it's optimized for producing convincing text. I think LLMs may well be some kind of an adversarial attack on our own language faculties. We use words to express ourselves, and we take for granted that our words usually reflect an intelligent internal state, so we instinctively assume that anything else which is able to assemble words must also be \"intelligent\". But that's not necessarily the case. You can have extremely complex external behaviors that appear intelligent or intentioned without actually internally being so. reply lanstin 3 hours agorootparentprevLLM does not know math as well as a professor, judging from the large number of false functional analysis proofs I have had it generate will trying to learn functional analysis. In fact the thing it seems to lack is what makes a proof true vs. fallacious, as well as a tendency to answer false questions. “How would you prove this incorrectly transcribed problem” will get fourteen steps with 8 and 12 obviously (to a student) wrong, while the professor will step back and ask what am I trying to prove. reply gibsonf1 11 hours agoparentprevExcept you’d be missing the part that a neuron is not just a node with a number but a computational system itself. reply bglazer 10 hours agorootparentComputation is really integrated through every scale of cellular systems. Individual proteins are capable of basic computation which are then integrated into regulatory circuits, epigenetics, and cellular behavior. Pdf: “Protein molecules as computational elements in living cells - Dennis Bray” https://www.cs.jhu.edu/~basu/Papers/Bray-Protein%20Computing... reply krisoft 9 hours agorootparentprevI think you are missing the point. The calculation is intentionally underestimating the neurons, and even with that the brain ends up having more parameters than the current largest models by orders of magnitude. Yes the estimation is intentionally modelling the neurons simpler than they are likely to be. No, it is not “missing” anything. reply marcosdumay 10 hours agoparentprevThere's a lot of in-neuron complexity, I'm sure there is some cross-synapse signaling (I mean, how can it not exist? There's nothing stopping it.), and I don't think the synapse behavior can be modeled as just more signals. reply cyberax 10 hours agoparentprevOn the other hand, a significant amount of neural circuitry seems to be dedicated to \"housekeeping\" needs, and to functions such as locomotion. So we might need significantly less brain matter for general intelligence. reply alanbernstein 7 hours agorootparentOr perhaps the housekeeping of existing in the physical world is a key aspect of general intelligence. reply Intralexical 3 hours agorootparentIsn't that kinda obvious? A baby that grows up in a sensory deprivation tank does not… develop, as most intelligent persons do. reply itsthecourier 10 hours agoparentprevArtificial thinking doesn't require an artificial brain. As our own walking system, compared to our car's locomotion system. The car's engine, transmission and wheels, require no muscles or nerves reply dekhn 10 hours agoprevAnnual reminder to re-read \"There's plenty of room at the bottom\" by Feynman. https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf Note the part where the biologists tell him to make an electron microscope that's 1000X more powerful. Then note what technology was used to scan these images. reply dvfjsdhgfv 3 minutes agoprevWhy do these neurons have flat \"heads\"? reply bugbuddy 9 hours agoprevBased on the picture of a single neuron, the brain sim crowd should recalculate their estimates for the needed computing power again. reply g4zj 11 hours agoprevIs there a name for the somewhat uncomfortable feeling caused by seeing something like this? I wish I could better describe it. I just somehow feel a bit strange being presented with microscopic images of brain matter. Is that normal? reply adamwong246 10 hours agoparenthttps://hitchhikers.fandom.com/wiki/Total_Perspective_Vortex reply greenbit 11 hours agoparentprevIs it the shapes, similar to how patterns of holes can disturb some people? Or is it more abstract, like \"unknowable fragments of someone's inner-most reality flowed through there\"? Not that I have a name for it either way. The very shape of it (in context) might represent an aspect of memory or personality or who knows what. reply g4zj 11 hours agorootparent> \"unknowable fragments of someone's inner-most reality flowed through there\" It's definitely along these lines. Like so much (everything?) that is us happens amongst this tiny little mesh of connections. It's just eerie, isn't it? Sorry for the mundane, slightly off-topic question. This is far outside my areas of knowledge, but I thought I'd ask anyhow. :) reply greenbit 10 hours agorootparentIt's kind of feeling a bit like an intruder? There probably is a name for that. reply bglazer 10 hours agoparentprevI’m not religious but it’s as close to a spiritual experience as I’ll ever have. It’s the feeling of being confronted with something very immediate but absolutely larger than I’ll ever be able to comprehend reply dekhn 9 hours agoparentprevWhen I did fetal pig dissection, nothing bothered me until I got to the brain. I dunno what it is, maybe all those folds or the brain juice it floats in, but I found it disconcerting. reply ignoramous 11 hours agoparentprevTrypophobia, visceral, uncanny, squeamish? reply carabiner 9 hours agoparentprevIt makes me think humans aren't special, and there is no soul, and consciousness is just a bunch of wires like computers. Seriously, to see the ENTIRETY of human experience, love and tragedy and achievement, are just electric potentials transmitted by those wiggly cells, just extinguishes any magic I once saw in humanity. reply SubiculumCode 3 hours agorootparentWelcome to the Existential Bar at the End of the Universe reply Zenzero 10 hours agoparentprevFor me the disorder of it is stressful to look at. The brain has poor cable management. That said I do get this eerie void feeling from the image. My first thought was to marvel how this is what I am as a conscious being in terms of my \"implementation\", and it is a mess of fibers locked away in the complete darkness of my skull. There is also the morose feeling from knowing that any image of human brain tissue was once a person with a life and experiences. It is your living brain looking at a dead brain. reply eminence32 11 hours agoprev> cut the sample into around 5,000 slices — each just 34 nanometres thick — that could be imaged using electron microscopes. Does anyone have any insight into how this is done without damaging the sample? reply talsit 11 hours agoparentUsing a Microtome (https://en.m.wikipedia.org/wiki/Microtome). reply dekhn 11 hours agoparentprevThe sample is stained (to make thigns visible), then embedded in a resin, then cut with a very sharp diamond knife and the slices are captured by the tape reel. Paper: https://www.biorxiv.org/content/10.1101/2021.05.29.446289v4 See Figure 1. The ATUM is described in more detail here https://www.eden-instruments.com/en/ex-situ-equipments/rmc-e... and there's a bunch of nice photos and explanations here https://www.wormatlas.org/EMmethods/ATUM.htm TL;DR this project is reaping all the benefits of the 21st century. reply posnet 11 hours agoprev1.4 PB/mm^3 (petabytes per millimeter cubed)×1260 cm^3 (cubic centimeters, large human brain) = 1.76×10^21 bytes = 1.76 ZB (zetabytes) reply gary17the 11 hours agoparent[AI] \"Frontier [supercomputer]: the storage capacity is reported to be up to 700 petabytes (PB)\" (0.0007 ZB). [AI] \"The installed base of global data storage capacity [is] expected to increase to around 16 zettabytes in 2025\". Thus, even the largest supercomputer on Earth cannot store more than 4 percent of state of a single human brain. Even all the servers on the entire Internet could store state of only 9 human brains. Astonishing. reply dekhn 10 hours agorootparentOne point about storage- it's economically driven. If there was a demand signal (say, the government dedicated a few hundred billion dollars to a single storage systems), hard drive manufacturers could deploy much more storage in a year. I've pointed this out to a number of scientists, but none of them could really think of a way to get the government to spend that much money just to store data without it curing a senator's heart disease. reply falcor84 10 hours agorootparent> without it curing a senator's heart disease Obviously I'm not advocating for this, but I'll just link to the Mad TV skit about how the drunk president cured cancer. https://www.youtube.com/watch?v=va71a7pLvy8 reply falcor84 10 hours agorootparentprevI appreciate you're running the numbers to extrapolate this approach, but just wanted to note that this particular figure isn't an upper bound nor a longer bound for actually storing the \"state of a single human brain\". Assuming the intent would be to store the amount of information needed to essentially \"upload\" the mind onto a computer emulation, we might not yet have all the details we need in this kind of scanning, but once we do, we may likely discover that a huge portion of it is redundant. In any case, it seems likely that we're on track to have both the computational ability and the actual neurological data needed to create an \"uploaded intelligences\" sometime over the next decade. Lena [0] tells of the first successfully uploaded scan taking place in 2031, and I'm concerned that reality won't be far off. [0] https://qntm.org/mmacevedo reply gary17the 1 hour agorootparent> we may likely discover that a huge portion of [a human brain] is redundant Unless one's understanding of algorithmic inner workings of a particular black box system is actually very good, it is likely not possible not only to discard any of its state, but even implement any kind of meaningful error detection if you do discard. Given the sheer size and complexity of a human brain, I feel it is actually very unlikely that we will be able to understand its inner workings to such a significant degree anytime soon. I'm not optimistic, because so far we have no idea how even laughingly simple, in comparison, AI models work[0]. [0] \"God Help Us, Let's Try To Understand AI Monosemanticity\", https://www.astralcodexten.com/p/god-help-us-lets-try-to-und... reply rmorey 10 hours agorootparentprevwe are nowhere near whole human brain volume EM. the next major milestone in the field is a whole mouse brain in the next 5-10 years, which is possible but ambitious reply falcor84 9 hours agorootparentWhat am I missing? Assuming exponential growth in capability, that actually sounds very on track. If we can get from 1 cubic millimeter to a whole mouse brain in 5-10 years, why should it take more than a few extra years to scale that to a human brain? reply rmorey 6 hours agorootparentassuming exponential growth in capacity is a big assumption! reply userbinator 8 hours agoparentprevIt's very lossy and unreliable storage, however. To use an analogy, it's only a huge amount of ECC that keeps things (just barely) working. reply bahrant 11 hours agoparentprevwow reply theogravity 10 hours agoprev> The brain fragment was taken from a 45-year-old woman when she underwent surgery to treat her epilepsy. It came from the cortex, a part of the brain involved in learning, problem-solving and processing sensory signals. Wonder how they figured out which fragment to cut out. reply pfdietz 9 hours agoparentI imagine they determined the focus of the seizures by electrical techniques. I worry this might make the sample biased in some way. reply brandonmenc 8 hours agoprevAnother proof point that AGI is probably not possible. Growing actual bio brains is just way easier. Its never going to happen in silicon. Every machine will just have a cubic centimeter block of neuro meat embedded in it somewhere. reply mr_toad 8 hours agoparentYou’d have to train them individually. One advantage of ANNs is that you can train them and then ship the model to anyone with a GPU. reply skulk 8 hours agoparentprevI agree, mostly because it's already being done! https://www.youtube.com/watch?v=V2YDApNRK3g https://www.youtube.com/watch?v=bEXefdbQDjw reply blincoln 10 hours agoprevWhy did the researchers use ML models to do the reconstruction and risk getting completely incorrect, hallucinated results when reconstructing a 3D volume accurately using 2D slices is a well-researched field already? reply VikingCoder 10 hours agoparentI'm guessing a registration problem. If all of the layers were guaranteed to be orthographic with no twisting, shearing, scaling, squishing, with a consistent origin... Then yeah, there's a huge number of ways to just render that data. But if you physically slice layers first, and scan them second, there are all manner of physical processes that can make normal image stacking fail miserably. reply momojo 9 hours agoparentprevAlthough the article mentions Artificial Intelligence, their paper[1] never actually mentions that term, and instead talks about their machine learning techniques. AFAIK, ML for things like cell-segmentation are a solved problem [2]. [1] https://www.biorxiv.org/content/10.1101/2021.05.29.446289v4.... [2] https://www.ilastik.org/ reply rmorey 9 hours agorootparentThere are extremely effective techniques, but it is not really solved. The current techniques still require human proofreading to correct errors. Only a fraction of this particular dataset is proofread. reply rmorey 10 hours agoparentprevThe methods used here are state of the art. The problem is not just turning 2D slices into a 3D volume, the problem is, given the 3D volume, determining boundaries between (and therefore the 3d shape of) objects (i.e. neurons, glia, etc) and identifying synapses reply layer8 9 hours agoparentprevRegarding the risk, as noted in the article, they are manually “proofreading” the construction. reply scotty79 10 hours agoparentprevMaybe it's not about reconstructing a volume but about recognizing neurons within that volume. reply greentext 8 hours agoprevIt looks like spaghetti code. reply idontwantthis 6 hours agoprev> Jain’s team then built artificial-intelligence models that were able to stitch the microscope images together to reconstruct the whole sample in 3D How do they know if their AI did it correctly or not? reply CSSer 11 hours agoprevFor some people, this is all you need (sorry, couldn’t resist)! reply fractal618 10 hours agoprev [–] Fascinating! I wonder how different that is from the mind of a man haha reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google scientists have successfully mapped a fragment of the human brain at nanoscale resolution, uncovering new features of neurons and connections in the brain cortex.",
      "The 3D map consists of 57,000 cells and 150 million synapses, offering unparalleled detail that may aid in understanding and treating psychiatric and neurodegenerative conditions.",
      "This monumental achievement in neuroscience, involving a preserved and metal-stained brain sample, has the potential to unveil novel insights into neural connections within the human brain."
    ],
    "commentSummary": [
      "The article discusses the intricate process of mapping the brain at a microscopic scale, emphasizing variations in neuron density between birds and mammals.",
      "It explores the potential impact of neuron density differences on stimuli response and processing speed, also comparing language models like GPT-4 to the human brain.",
      "Additionally, it touches on protein computation in cells, brain tissue imaging difficulties, the storage requirements for full brain scans, and expresses skepticism about fully grasping the complexities of the human brain and replicating its functions in artificial intelligence models."
    ],
    "points": 149,
    "commentCount": 74,
    "retryCount": 0,
    "time": 1715290566
  },
  {
    "id": 40307108,
    "title": "Machine Learning in Elixir: A Scalable and Efficient Platform",
    "originLink": "https://cigrainger.com/elixirconf-eu-2024-keynote/",
    "originBody": "What I mean when I say that machine learning in Elixir is production-ready 09 May, 2024 My ElixirConf EU 2024 keynote \"Ship it! A roadmap for putting Nx into production\" is up on youtube. I'm hoping it will help folks gain the confidence to go out and... ship it! I often say 'machine learning in Elixir is ready for production'. On further reflection, I realised that this statement dramatically undersells the true potential and capabilities of machine learning within the Elixir ecosystem. (I already posted these reflections on Twitter). When I say \"production-ready,\" it's not just about the ability to deploy machine learning models in a live environment. In the context of Elixir, being production-ready means deep integration with the weird and wonderful thing called the BEAM (Bogdan/Björn's Erlang Abstract Machine). It means seamless compatibility with OTP (Open Telecom Platform) primitives, which are the building blocks of Elixir applications. This deep integration is what makes machine learning in Elixir so powerful. Just as Elixir has proven to be a cheat code for building web applications and distributed systems, it now offers the same advantage for machine learning tasks. The Elixir ecosystem provides a secret weapon that empowers developers to build and deploy machine learning solutions with unparalleled ease and efficiency. For a growing set of areas where Elixir's machine learning ecosystem is applicable (which is already pretty extensive), I firmly believe that you're better off shipping inference with Elixir than basically anything else. In many cases, it's even worthwhile to conduct experiments and model training in Elixir, despite the relative youth of that part of the ecosystem. So, what makes machine learning in Elixir so powerful? Mainly: OTP. Also: good choices. Nx was built from the ground up with inspiration from JAX, a popular machine learning framework. By leveraging metaprogramming and embracing pluggable backends and compilers from day one, Nx benefits from a significant second-mover advantage. This architectural decision, which I covered in more detail during my ElixirConf EU talk, unlocks huge potential that we see play out in the libraries built on top of it like Axon, Bumblebee, and Scholar. Thanks to these foundational choices, the entire Elixir machine learning ecosystem can reap the benefits. With defn, a macro provided by Nx, machine learning capabilities are practically built into the language itself. Take a moment to reflect on the significance of this: by simply writing Elixir code, you gain access to multi-stage compilation that targets various hardware platforms, including GPUs and TPUs. Moreover, Nx.Serving, a module in Nx for serving machine learning models, is just wild. You get out-of-the-box distributed, clustered, hardware-agnostic automatic batching without the caller needing to concern itself with any of that? Just wild. It's the right separation of concerns. The actor model of concurrency, which is at the heart of Elixir, proves to be the perfect abstraction for serving machine learning workloads and, just as importantly, integrating them into broader systems. By encapsulating models within processes, you can leverage all the built-in features that make the BEAM so powerful. Nx.Serving is just another application in your supervision tree, making it just another component of your robust and fault-tolerant system. So integrating machine learning into a Phoenix application becomes a breeze. You can take advantage of libraries like Oban for durable and robust job processing, Broadway for concurrent data processing pipelines with backpressure, and FLAME for lambda-like execution on the BEAM. You can send pubsub messages about progress, results, etc to LiveViews (which are, of course, just processes). And of course those updates can trigger changes in views like I describe in this thread. The beauty of the Elixir ecosystem lies in its ability to create brilliant features for end users with minimal effort. We routinely handle thousands and millions of messages per day over pubsub and between processes, showcasing the scalability and efficiency of the system. You can add machine learning to Saša Jurić's famous chart (above). So if Rails is considered the \"one person framework,\" then Phoenix can be seen as the \"half person framework,\" from basic CRUD operations to the cutting edge of LLMs. Machine learning in Elixir is production-ready. In this ecosystem, that statement means a lot. #elixir #machine learning #nx #talks 9",
    "commentLink": "https://news.ycombinator.com/item?id=40307108",
    "commentBody": "What I mean when I say that machine learning in Elixir is production-ready (cigrainger.com)131 points by cigrainger 22 hours agohidepastfavorite92 comments auraham 4 hours agoHave any of you used elixir-desktop [1]? It is a wxWidgets + LiveView bundle, pretty much like a Electron app. In [2], Wojtek Mach explains how the team behind Elixir build Livebook Desktop. He explains how the project started, some subtle bugs found when building the app for MacOS, some limitations of wxWidgets in Windows, and many other implementation details. It would be awesome if the Elixir team release something like elixir-desktop based on Livebook. That is, forking the Livebook repo and release an official template project for generating desktop applications based on LiveView. [1] https://github.com/elixir-desktop/desktop-example-app [2] https://www.youtube.com/watch?v=Kiw6eWKcQbg reply dorian-graph 57 minutes agoparentTangential to this, there's also https://native.live/. reply ToJans 19 hours agoprevI'm a big fan of Erlang, Elixir, OTP and the BEAM vm. However, I only have very lightweight server needs for my SaaS, so it just doesn't make sense to switch to it, especially as my code is mostly client-based/typescript. If my core value prop would be around real-time messaging or streaming data, BEAM/OTP would be my first choice. Slightly OT: I'm still in dubio about Cloudflare pages, but I'm sure that the platform for the backend reboot that we're starting will be in Typescript. There's so much advantage in having one language for everything. reply ajmurmann 17 hours agoparentI am really intrigued by the distributed systems capabilities that BEAM brings. I built a chat app using it and am also using Mnesia DB to store recent messages. However, my experience coding it has been terrible. I am using IntelliJ with an Elixir plugin and there is essence is no auto-complete. I used to write a ton of Ruby, but there RubyMine was really good at guessing what's available. Not so much here. On top of that more often than not the errors from Mnesia are just terrible. Most of the time I just get \"TypeError\" and a line number. Unclear what it expected to get and what it got passed instead. What IDE/editor have you found helpful? Are there any tricks to work around this? In Ruby I sometimes would use Pry Byebug to explore live what's actually available and iterate on my code in the REPL. Is there a similar approach you recommend here? reply lawn 16 hours agorootparentI've had success with the ElixirLS language server. It should work with most editors I assume, but I use it with Neovim. Autocomplete, diagnostics, goto definition work as expected. reply sbrother 15 hours agorootparentprevI'm glad I'm not the only one who has had awful luck with that IntelliJ Elixir plug-in. Maybe I'm doing something wrong, but it just doesn't work for me. I've been using emacs + elixir-mode + alchemist since Elixir came out and I have zero complaints, it all works beautifully complete with autocomplete, jump to definition and documentation popups. But obviously emacs isn't for everyone. reply davidw 17 hours agoparentprevWhat do you use instead, out of curiosity? reply TechDebtDevin 17 hours agorootparentRead dude reply vvpan 17 hours agoprevWanted to add to the usual \"why is BEAM not more popular\" conversation that there is also Gleam [1], which is an up and coming typed language on top of BEAM. It has the more common C-like syntax and a growing ecosystem of libraries, for example Lustre [2]. Unlike Elixir it has a much tighter syntax without all the macro magic. It also does not wrap around Erlang processes and you have to use those directly. To myself I see it as more of a Go for BEAM, although I am sure people would criticize me for making such generalization. It sucks that syntax matters but it kind of does. I, for one, am somewhat put off by meta-programming and flexibility of Elixir. The idea of having to learn a syntax per-library I struggle to see the benefits of. I have never programmed Ruby or Lisp so perhaps I have not experienced the joy of what Elixir has to offer... [1] https://gleam.run/ [2] https://github.com/lustre-labs/lustre reply OutOfHere 9 hours agoparentGleam is a tooling development language, not a concurrency language. It runs away from Erlang's concurrency ecosystem. Also, its documentation, especially wrt concurrency, is sorely lacking. In its current state it would be a mistake it to use it for any distributed work, but perhaps this can change in time. reply sundalia 17 hours agoprevQuite frankly, I see a lot of text in this post and no numbers. For something to be production-ready I'd expect you to at least cover major things like \"latency to serve x in Elixir instead of lang y is k% better\" or \"EMFU we got when training x in Elixir was comparable to lang y\". These are two random metrics that are of course biased to my experience but the article just feels empty without numbers. reply ambicapter 18 hours agoprev [–] I see so much breathless adoration for Erlang and Elixir, how come it hasn't taken over the world in all this time? reply macintux 18 hours agoparentInertia: curly braces rule the world. Imperative/OO programming are ubiquitous, FP not so much. Corporate sponsorship: even with curly braces and mutability, Go probably wouldn't have gained its mindshare without Google. Performance: It's getting much better, but the BEAM was never designed for maximum performance. People don't like slow platforms, despite the other advantages. Scale: much like one of the databases written in Erlang, Riak, you typically don't need a BEAM language until your solution is large enough that you've already written it in something else. I love Erlang, but I'm a lost cause. reply giancarlostoro 17 hours agorootparentIf curly braces rule the world explain Python? Some pretty well known apps are powered by Django. Doing AI/ML without Python is doable but not quite as mainstream. reply macintux 17 hours agorootparentSure, there are exceptions to every rule. Python is close enough to pseudocode that beginners can get familiar with the basic syntax easily. reply giancarlostoro 12 hours agorootparentI am not talking about beginner level software though. We are talking millions of users, very serious software. reply ambicapter 18 hours agorootparentprev> Performance: It's getting much better, but the BEAM was never designed for maximum performance. People don't like slow platforms, despite the other advantages. > Scale: much like one of the databases written in Erlang, Riak, you typically don't need a BEAM language until your solution is large enough that you've already written it in something else. These two seem to contradict one another. How can it not be fast but scale well (I'm thinking of a way as I type this but I'll let you answer)? reply ffsm8 18 hours agorootparentLatency vs throughput. If you just want to do 1+1, 1 million times in a sequence, then erlang will likely be one of the last to complete the challenge. If you want to do 1 million 1+1 in parallel, then erlang will get you there with minimal overhead, even if each number crunched will take longer then java, golang etc... the way it branches out is very efficient reply bcrosby95 17 hours agorootparentprevWhen people talk about scaling they usually mean horizontally (more computers) rather than vertically (a more powerful computer). Scaling like this tends to involve your system actually performing worse because, among other things, it has to go across the network. reply paradox460 17 hours agorootparentprevYou can do one thing fast, or many things slowly. For many common applications, doing many things slow is more important than off-the-starting-block performance reply peoplefromibiza 18 hours agorootparentprev> How can it not be fast but scale well soft realtime: Erlang gets slower with load, but keeps responding fault tolerance: it's very hard to bring an Erlang application to a grinding halt distribution: it's stupid easy to distribute an Erlang application and scale it horizontally reply dsff3f3f3f 17 hours agoparentprevThe reality is that it's not nearly as good as the loud proponents would have you believe. Performance isn't great unless you're comparing it to really naive applications written in extremely slow dynamically typed languages like Ruby or Python and OTP and real supervisor trees are not trivial to use correctly. Almost all of the Elixir systems I've seen have serious problems in their supervisor trees. I've also repeatedly seen this idea from relative newbies that you can replace things like Redis with a simple Erlang key/value store, possibly using ETS, and the result is always much, much worse in terms of both performance and reliability. A lot of the older Erlang/Elixir proponents will tell you to just use Redis. Most of the popular statically typed languages also have decent abstractions for concurrency and parallelism now while having far better runtimes, far better performance in almost all cases, far more libraries and much larger communities. Erlang/Elixir will never be more than a small niche. reply timfsu 5 hours agorootparentThis. I built my last company on elixir. Deployments of clustered BEAM in cloud VMs are far more complex than load-balancing across node or python servers, not to mention serverless. Any custom genservers we wrote always had stability issues. Type safety was a problem that caused crashes in production. Elixir is a really fun language and BEAM is powerful but it’s by no means a clear choice. My new company is on Next.js and it’s much easier to hire / develop / deploy. reply hosh 17 hours agoparentprev> how come it hasn’t taken over the world all this time? I don’t know why it has not. However, it has such a strong set of advantages that people who know what it can do for them keep describing it as a “secret weapon”. As others have mentioned, it will scale up on a single machine to make use of all the cores (unlike Nodejs, Python, or Ruby). It can already scale horizontally by clustering. Because of the way it is designed, I never have to define a Kubernetes liveliness probe for an Elixir app, whereas I have seen Dotnet and Nodejs apps freeze without crashing (cooperative async reactor can go into infinite loops; Nodejs is very bad about orphaning async execution, by design). A lot of AI apps is going to involve connecting with unreliable third party services (for example, agents making calls to other api to get information or initiate actions), and may even be on hardware with unreliable network (IoT and edge) and this is where BEAM / OTP shines ahead of pretty much every other runtime and platform. HN and elsewhere are riddled with Elixir developers extolling its competitive advantages for years … I have had very smart people argue to me why Typescript makes Nodejs so much better, but at this point, I have very little incentive to persuade them. Hence, “secret weapon”. reply techpression 14 hours agorootparentI take runtime pattern matching over TypeScript any day of the week, I work with data across boundaries, compile time checking means very little to me (and I'm sick and tired of the \"await request.json() as SomeTypeThatMostDefinitelyIsNotTheRightOneButMakesForFancyAutoComplete;\" that seems to riddle every TS code base). Sure, you can add runtime dependencies with the additional code bloat to have it runtime check types with TypeScript too, but you still don't get any fault tolerance, and you have to write explicit code to handle errors that according to the type shouldn't exist. reply belval 17 hours agorootparentprev> it will scale up on a single machine to make use of all the cores (unlike Nodejs, Python, or Ruby) Python definitely does \"use all cores\" on a machine with the multiprocessing package, not sure what you mean? reply hosh 17 hours agorootparentBEAM has a preemptive scheduler and being able to use all available core is a part of the standard runtime and language primitives, and does not require a separate library. The standard library, OTP, builds on top of those language primitives. The whole language and runtime is designed from its foundations to work with massive concurrency. This is one of the things I think why Elixir isn't as popular: people think that Nodejs or Python can do with Elixir or Erlang do, but they don't. reply clessg 17 hours agorootparentprevI assume they mean Python doesn't do so by default (nor in a lightweight fashion). You can certainly use all cores with any programming runtime if you just run multiple OS processes. Indeed, that's how you implement multi-core on Ruby and Node as well. Although even then, the cores themselves aren't necessarily being fully utilized, even if you're ostensibly using all cores. reply hosh 17 hours agorootparentBecause it is not by default, you don't really use all the cores by default either. reply OutOfHere 9 hours agorootparentprevPython 3.13 beta1 can be compiled with no-GIL mode, which allows free-threading that could perhaps use all cores. reply hosh 6 hours agorootparentLet’s say we do that. How many threads can we create? On BEAM, running 10,000 lightweight processes is normal. Phoenix is designed so that every incoming http request is its own lightweight process. How does one manage that number of lightweight processes? The runtime’s scheduler keeps track of every single process so nothing is orphaned. It is preemptive, so no lightweight process can starve out another (though there are some exceptions). They can also suspend cheaply, as such, works well with async io. The closest thing to this are the new virtual thread feature of recent Java. I don’t think Java has the same kind of properties that will allow it to manage it as well as BEAM. There is a lot more to Elixir than being able to use all the cores. reply OutOfHere 6 hours agorootparentOh certainly; I never meant to think that Python competes with Elixir in this regard. It doesn't. reply malkosta 10 hours agorootparentprevAnd if you have 4 cores, and 3 of them are blocked by IO, now you only have 1 core to answer requests. What happens if this core also stops in an IO? Your service stops. That can NEVER happen in the Erlang Virtual Machine (the BEAM), because of the preemptive scheduler. This is only one of 100s of examples why the BEAM is the right choice for web systems where real-time can be soft real-time. reply hosh 6 hours agorootparentNodejs will also move on if io is blocked. However, since Nodejs queues bits of execution, rather than messages, errors can get lost. So then you have unhandled promise rejections… and then relying on a linter to look for those times you did not write something to catch the error. I was told this is a good feature of the linter. Contrast that with the BEAM languages. An unhandled error crashes the lightweight process. Any linked process (such as a supervisor) can decide what to do about the crash (restart, or crash as well). We don’t even need liveliness probe (like in Kubernetes) because the supervisor is immediately informed (push, not pull). You don’t need a linter to make sure it has a sensible default, because this is handled by design. Nodejs, on the other hand, is error prone, _by design_, even though it does not block on IO either. No amount of typespecing is going to fix that. reply thwoerasdfsds 17 hours agorootparentprevIt's self-evidently false because you depend on multiple processes reply bluesnowmonkey 12 hours agoparentprevHere's one anecdote: I use Elixir professionally, for 8 years now. A few years ago I moved to the suburbs and met a neighbor two houses down who happens to run a software company based on Elixir with about 20 devs. Seems like I wouldn't be running into Elixir people in my daily, non-professional life unless there were a lot of them. I have a theory about why there's more getting done with Elixir than you perceive. It's very productive, and you can get a lot done alone or with a very small team. You don't need as much devops. You maybe don't need as many frontend devs, because LiveView. And so on. Elixir teams don't need to hire as much, so when you look around, you don't see a ton of Elixir job postings. Fewer jobs discourages people from leaning into it as a career choice, which means fewer devs, strangling the overall growth of the community. Also, at least in large tech companies, small productive teams are anathema to managers. Headcount is everything, and you have much more political power managing 10 Java devs than 3 Elixir devs. Never mind that the company is spending more on salaries, because that's the company's money, not yours. Anything you can do to have a bigger team is a win. reply el_oni 17 hours agoparentprevI think we need to give it time. Python had a slow and steady growth from 1991 until today and it has eaten so much of the data analysis and ML world (backed by C++, C and more recently Rust). But python doesn't vertically scale very well. A language like Elixir can grow to fit the size of the box it is on, making use of all the cores, and then without too much additional ceremony scale horizontally aswell with distributed elixir/erlang. Elixir getting a good story around webdev (phoenix and liveview) and more recently the a good story around ML is going to increase it's adoption, but it's not going to happen overnight. But maybe tomorrows CTOs will take the leap. reply greenavocado 17 hours agorootparent> Python doesn't vertically scale very well Strong disagree, this is a skill issue. I have written C++ modules for Python with pybind11 to speed up some code significantly but ended up reverting to pure Python once I learned how to move memory efficiently in Python. Numpy is very good at what it does and if you really have some custom code that needs to go faster you can run it externally through something like pybind11. It is a skill issue mostly. If you are writing ultra low latency code then you're right. You can make Python really fast if you are hyper aware of how memory is managed; I recommend tracemalloc. For instance, instead of pickling numpy arrays to send them to child processes, you can use shared memory and mutexes to define a common buffer which can be represented as a numpy array and shared between parent and child processes. Massive performance win right there, and most people simply would have never realized Python is capable of such things. reply el_oni 16 hours agorootparentI said python doesn't scale well and you say \"it does if you use an escape hatch to a faster language\" Sure. Writing C++ that utilises your resources effectively then writing bindings so you can use that in python is great. But with elixir, if I've got 8 cores and 8 processes, those 8 processes run in parallel. If I want raw cpu speed I can write something in rust, c, cpp or zig and then call it still using elixir semantics. Not to mention that with Nx you can write elixir code that compiles to run on the GPU. Without writing any bindings. reply greenavocado 7 hours agorootparentI went back and rewrote the systems in Python after learning more about how to manage memory more efficiently using common tools, methods, libraries reply h0l0cube 17 hours agorootparentprev> Strong disagree, this is a skill issue. I think the point with Elixir and Nx is that any difficulty in parallel performance is abstracted away reply arter 18 hours agoparentprevJust because it hasn't taken over the world it doesn't mean it's proponents adorations are baseless ? I agree that the article falls flat on providing descriptive reasons how Elixir compliments machine learning. But that shouldn't be an argument against the product, but rather against it's cult following that does not provide sufficiently detailed and advanced examples and explanations. Something extremely common in tech journalism. Good ideas and technologies do get underappreciated and wide spread adoption I would wager is not at all correlated to quality. Javascript, c++, java and others are good examples. Yes they have taken over the world at one point, but there are better designed languages out there. It just seems that people don't want to relearn a new paradigm and you can write software on anything. reply peoplefromibiza 18 hours agorootparent> how Elixir compliments machine learning one simple example: what Jose Valim calls Distributed² You can have a livebook (similar to notebook, but distributed and live) and distribute your load not only on multiple GPUs on your machine, but on multiple GPUS on multiple machines with basically zero effort https://www.youtube.com/watch?v=MSMyRBJAoSs reply jerf 18 hours agoparentprevhttps://news.ycombinator.com/item?id=34546143 https://news.ycombinator.com/item?id=7277957 And also \"taking over the world\" is generally a larger task than people realize. C is still fairly strong, and at this point it is basically the baseline against we programmers issue our complaints, with only rare and very contrarian compliments given to it... yet there it is. Language turnover is really quite slow. reply fulafel 13 hours agoparentprev\"Best language wins\" is not how programming language popularity at the top works. Java, C/C++, JavaScript all have/had various other reasons for their wide use. And of course there's huge inertia, even if a language has the requisite of other forces behind it for \"taking over the world\" it still takes decades. But no need to put too much weight on this, lots of languages have more than enough users to be viable for a long time. The top stack overflow languages tend to have poor S/N in the user communities. reply greenavocado 17 hours agoparentprevElixir has a lot of promise on the surface but today I'm going to tell you why I am going to pivot my ten person company away from Elixir to a different language for two of our main customer facing services: typing and staffing. Elixir's typing situation is simply insufficient for complex real world use cases on teams with many developers and disparate services. Elixir developers are also almost non-existent in the United States. It is far more productive and far less risky from a business perspective to hire someone that knows Python for the backend and is forced to work with mypy and pydanic using a major web framework. The trope that functional programming leads to better code and is more maintainable is also a lie. Defect rate is just as high if not higher per module because of the typing problems. reply hosh 17 hours agorootparentWhat typing does Python have available? reply greenavocado 17 hours agorootparentEnforcing use of mypy is good enough for all real world use cases https://mypy.readthedocs.io/en/stable/index.html reply hosh 17 hours agorootparentAs far as the lack of US Elixir developers, there is truth in that. It isn't that there are not Elixir developers, but that everyone who wants to work on Elixir is already working on Elixir. Years ago, I knew this engineering manager whose strategy for recruiting Erlang developers wasn't to look for Erlang developers. He found generalists and polygots who would do well in any language, and were willing to learn Erlang, and recruited them, and then taught them Erlang. These are the kind of people who can take advantage of what Erlang/OTP (and Elixir) offers. They can do well with or without typing. As for defects, the key isn't necessarily what's happening within a module, but how the various GenServer interact with each other. Having low defects within module boundaries are table stakes when working with Elixir. So if you're only measuring defects within module boundaries, you might miss the systemic problems related to interacting GenServers. reply h0l0cube 17 hours agorootparentprevHow is this different to typespecs and Dialyzer? reply hosh 16 hours agorootparentThis is in the works right now: https://elixir-lang.org/blog/2023/09/20/strong-arrows-gradua... reply greenavocado 17 hours agorootparentprevSee the first reply to the question linked below. I have nothing more to add. https://elixirforum.com/t/how-to-make-dialyzer-more-strict/1... reply cess11 12 hours agorootparentOK, great, then I'll add that pattern matching on struct types is built-in and good enough for my purposes. Haven't used the LSP-server in a while, but I think it runs the Dialyzer-checking by default too, so that layer of checking will likely be there without adding project dependencies. reply ramy_d 18 hours agoparentprevI think WhatsApp is still using Erlang and they are biggest messaging app in the world. Discord is using Elixir but it's more niche. I think if you look at use cases where they excel, you will find them there. reply greenavocado 17 hours agorootparentDiscord uses Rust (and Elixir) https://discord.com/blog/how-discord-stores-trillions-of-mes... reply Zambyte 17 hours agorootparenthttps://discord.com/blog/using-rust-to-scale-elixir-for-11-m... reply conradfr 17 hours agorootparentprevThey use both. reply indigo0086 18 hours agoparentprevSame reason you don't seetaken over the world. The demands of the world are different to the demand of small passionate communities. reply giraffe_lady 18 hours agorootparentThe demands of modern web platforms actually run really close to BEAM's strengths. It's still an interesting question but \"enthusiasts are naive and wrong\" isn't the answer. reply indigo0086 17 hours agorootparentI didn't say what you quoted reply wincy 18 hours agoparentprevBecause when you try to throw junior developers at it who had OO concepts in Python, Java, or C# drilled into them in college, switching to Elixir/Erlang feels like learning Martian. I had the fun experience of it being the second language I used in my career after PHP, and it definitely took strong mentorship and smart seniors who were really into Elixir for me to get up to speed. At least when we used Elixir about eight years ago, we had to also learn Erlang as many libraries running on the BEAM VM that Elixir and Erlang share were only written in Erlang. So that made my brain melt further. Once I got into the flow, I really liked it, and am glad I got the opportunity to learn it as a junior developer, but I've largely dropped it as there aren't many jobs in my area looking for Elixir. reply hansonkd 17 hours agoparentprevTBH part of the BEAM runtime is actually pretty slow. is the problem. * Want something flexible and easy and don't care about performance? Use Python * Care about performance? Use Rust, etc. Just because it is distributed cloud native doesn't mean for single requests or functions it is fast. Almost all SaaS applications are request based and do not need P2P or real time communication. BEAM is worse here. If a company does need that they can specialize that part of their application. reply ricketycricket 16 hours agorootparentOut of the box, Phoenix applications respond to simple http requests in times measured in microseconds. What appreciable improvements from that do you get with Python? And considering how much of your total request time is not processing by the language (db calls, network latency, etc.), why would you decide on a language purely on the minor speed improvement of a small part of the overall picture? I’ll gladly trade what might amount to a few ms of request time for the concurrency model, scalability, and latency characteristics of Elixir and the BEAM. To each their own, I suppose. reply techpression 14 hours agorootparentYeah I don't get this argument either. It's like people are still stuck in micro-benchmark-land. Sub-1ms responses are the norm in Phoenix and I tend to never get anything above 100µs for LiveView messages, sure, they're noops, but calling it slow is .. well, strange. Sure, you don't want to do number crunching in pure Elixir, but I'm always curious as to the actual needs people have, once being one of the \"performance is everything\" kind of developers myself. reply cess11 12 hours agorootparentFor number-crunching there's Nx. As far as I can tell it's a solved problem, with easy handling of things like batching over clusters of GPU:s and things like that as a bonus. reply asa400 17 hours agoparentprevAs someone who was Elixir hire #1 at a company that eventually ended up with ~25 Elixir developers with a novel product that people actually used, I see people in this thread and elsewhere mentioning the unorthodox nature of the language or the supposedly poor performance, and I can tell you that none of that actually matters one way or another. The stuff that really hampers Elixir/Erlang adoption, in my own personal experience and from talking to friends who could have used it but went with other stuff: 1. From the outside, the benefits appear almost entirely theoretical, and in many cases, they are, and the issue with that is that in order to understand the real, non-theoretical benefits of having everything integrated into Elixir and OTP, you have to go all-in. Elixir has to own your app. Elixir is amazing, but when you start talking about its ability to do async IO, people say, \"my language has a framework for async IO\". When you start talking about its ability to automatically timeout and restart tasks and be fault tolerant, people say, \"my language has a framework for network timeouts and I have systemd/k8s/whatever for application restarts\". When you start talking about having ETS tables built in, people go, \"I have Redis and it works amazingly well, why do I need it inside my app?\" You can't adopt OTP piecemeal like you can with e.g., Tokio, or Redis, or Sidekiq, or whatever other thing. It doesn't really make sense to. Other languages might not be as powerful and not as well integrated, but through sheer force of effort, people make existing languages compose with general purpose tools and this is often \"good enough\". Of course you can write a single small service in Elixir, and this is fine and works, but this often runs into the issue of \"now we have an Nth language beyond our normal languages, and we already have caching/monitoring/supervision figured out, so what is this actually adding?\" 2. The library ecosystem is still very weak if you're used to e.g. Ruby, Python, Java, or Go (or even Rust, which already has way broader library coverage that is often significantly higher quality) and being able to find a mature driver/integration for a given API or native library. I don't know if this will ever catch up or reach a critical level where most people will find it acceptable. Stuff like the AWS library languished for, literally, years. Just critical stuff that's totally sorted in other ecosystems. This is purely anecdotal on my part, but it seems that way less people as a percentage of the whole community are working on libraries than applications. I have no idea why this is. 3. The performance - especially around latency and garbage collection - is great, but it's probably not \"better enough\" to actually matter in the domains where people tend to adopt Elixir. What I mean by this is, if you're already working on a project where the performance profile of Ruby or Python or Java is fine (web stuff, relatively simple network or CLI stuff), Elixir may be better in certain respects, but it's not so much better that it provides enough of a step change to make the other issues worth it. For our application, we had some very CPU intensive math stuff that dominated processing times, but we wrote that in Rust anyway, so the higher-level control layer could have been literally anything and it wouldn't have had any effect on performance at all. We picked Elixir because we liked it, but the product would not have failed for performance reasons if we had picked Ruby. 4. It is still, somehow, relatively difficult to find Elixir developers that understand OTP and the more operational concepts of the ecosystem (releases, deployments, observability etc.) at more than a beginner level. The community is still rather beginner oriented and there are a ton of folks who have done simple Phoenix apps and never gone further. I want to be clear that I mean this simply as an observation and not in a derogatory way at all. There is absolutely nothing wrong with having beginners in your community. Beginners are essential to ensure the community continues to grow and a language community without beginners is dying. But over time you have to build up a significant base of experts that can start and lead teams and companies independently, without having to rely on a class of consultants (of which the Elixir community seems dominated by - which is another conversation entirely that I will not get in to here). In Elixir, it seems like the proportion of experts has stayed small relative to the number of experts in e.g. Python, Ruby, Clojure, Go, or Rust. I can go out today and hire someone who understands the inner workings of Python as well as anyone working on the Django team. Anecdotally, as someone who has hired for Elixir roles, this seems difficult to do in Elixir, and I don't know why this is. I have been trying to understand why I have this perception and whether this perception is correct for literally years now, and I haven't been able to figure it out. (Strangely, Erlang on the other hand almost tilts too far in the other direction, with a ton of senior-level people and very few junior or mid-level.) I could say more but it's probably better that I just leave it at that. I also want to say that I write all this as someone who really loves Elixir but wants to be honest about the issues that I have seen hamper adoption at the companies I've been at. reply vvpan 9 hours agorootparentI feel like Elixir has a split personality: on the one hand you have libraries for anybody to use out-of-the-box with their specialty syntax and all (Phoenix, et. al.) but on the other hand Erlang/BEAM are heavy-weight backend development tools, things that I would imagine most people do not want to or would not get into. This came up on Elixir Reddit the other day, somebody was lamenting how their job uses Elixir, but a handful of libraries without getting into the OTP weeds, and the replies were: \"why do you care if it works?\". But I understand where the poster came from, I do not particularly care for Phoenix or Liveview, they are tangentially related to what I would like to do with the language, which is write robust backend services with minimal technical overhead. I am sure Phoenix is nice but and I hope to use it one day but it is not the selling point to me. reply NeutralForest 13 hours agorootparentprevThat was a super interesting write-up for someone who has a passing interest in Elixir but was curious about some of the issues you mentioned, thanks! reply asa400 12 hours agorootparentIf you're feeling any less interested, I hope you reconsider and still give it a shot. There are issues, but there are issues with every technology. I promise you'll learn something. reply NeutralForest 12 hours agorootparentIt does look absolutely interesting but I feel like I'm already downloading too many books and trying to follow too many different technologies but I'll come around. reply dns_snek 16 hours agorootparentprevGreat write-up! I've been using Elixir for a handful of years now, primarily working on solo/consulting projects and in micro teams. I'd like to think I have a pretty good grasp on the language and OTP fundamentals, but I'll admit that some the things you mentioned in your last paragraph about lacking knowledge of operational concepts resonated with me. Over the years I've tried to learn more about releases, deployments, distributed applications, fault tolerance in environments with multiple nodes, and hot code upgrades; but none of the projects I've worked on have reached sufficient scale or complexity for those things to really matter. My impression is that it's difficult to learn about these topics without real hands-on experience (something that seems hard to replicate in small/solo projects). Would you mind expanding on that last point? I'd be curious to learn more about these knowledge gaps that you observed, as well as any recommendations for \"leveling up\" past them, whether it be projects that one could build to learn more, books to read, etc. reply asa400 12 hours agorootparent> My impression is that it's difficult to learn about these topics without real hands-on experience (something that seems hard to replicate in small/solo projects). Maybe. Unfortunately, I don't think there's a cheat code there. Often the single best thing you can do is to put yourself near people who know what you want to know. > Would you mind expanding on that last point? I'd be curious to learn more about these knowledge gaps that you observed, as well as any recommendations for \"leveling up\" past them, whether it be projects that one could build to learn more, books to read, etc. Related to my response above, as an example of something I observed, we were able to teach people who had little Elixir experience about simple GenServers pretty quickly. They learned the API fine, understood the client/server metaphor, started making contributions to the product/codebase, great, product managers are happy. The gaps became apparent when we'd hear stuff like \"the thing I wrote is crashing, in the logs I'm seeing things like `exit: genserver timeout`. What does this mean? Where do I even start?\" The issue here isn't really \"Elixir\" per se. The issue is that Elixir was sold to them as this quick, pragmatic functional programming language (and it is), but what they got along with Elixir-the-language was a production OTP application that was actually a concurrent distributed system that they didn't realize was a concurrent distributed system and now they have to figure out how to debug that, which might have been more than they were bargaining for. Now OTP starts to dominate the conversation. You say you have a pretty good grasp on OTP so this might not apply to you specifically, but if I could recommend a single thing to Elixir developers, it would without question be to read as much of the Erlang stdlib documentation and source code as you can. They're really, really good and highly readable. Don't be grossed out by Erlang. Read up on genserver, supervisor, sys, dbg, monitors, links. Understand what is actually happening when a genserver boots. Understand what actually happens when a genserver crashes, and how this appears to the supervisor. Understand when to use a supervisor vs. a dynamic supervisor. Understand when you might want to use the process Registry. If I can recommend more than one thing, it would be to just build toys and provide your own hands-on experience. Elixir, being so dynamic, lends itself to discovery-through-play better than almost anything. Build a 3 node cluster on your laptop and play with rpc to distribute work. Build a little worker pool and randomly crash the tasks. Build something that dispatches work with Registry. Build a little network server that listens for new tcp connections. Kill random processes and see how things react. Use dbg or sys or observer or the other tracing stuff to see live state. Build a release and figure out how to remote shell into it. The only difference is that a real app would have a real domain, which you'd have to learn regardless. Anyway, I hope this is useful. reply cmdrk 18 hours agoparentprevtake a look at the Who's Hiring threads - despite the love the BEAM gets on this website, it seems very few folks are brave enough (or public enough) to hire Erlang/Elixir/Gleam/etc engineers. I've seen one or two Elixir jobs each posting and zero Erlang jobs in the last 6 months or so of Ctrl-F'ing the posts as I see them. reply aeturnum 17 hours agorootparentThere's a pretty consistent stream of relatively senior postings on the elixir forums[1] as well as a number of successful large-scale elixir deploys (Discord, Heroku, etc) that probably do their own recruiting. I get the sense that YCombinator's technical mentorship focuses on language silos they have experience in, so this community is probably just a bit outside the \"sweet spot\" for BEAM hiring. Hiring juniors (and getting work AS a junior I imagine) is a challenge. I don't think Elixir is that hard to learn (and has fewer foot-guns than many competitors), but it makes evaluation more difficult. [1] https://elixirforum.com/c/work/elixir-jobs/16 reply notarobot123 18 hours agorootparentprevIt sounds like Elixir might be a competitive advantage for attracting talent for the companies that do actually use it (much like python in the mid-2000s[0]) [0] - https://paulgraham.com/pypar.html reply Thaxll 18 hours agoparentprevFP, average performance, no static typing, finding people etc ... Personally I don't like FP. reply ffsm8 17 hours agorootparentStructs are arguably better then most type systems, but I don't use it either in my day job and have only used it for toy projects... reply ttymck 18 hours agoparentprevBecause most developers struggle with functional programming. reply cultofmetatron 17 hours agorootparentwhich is a shame because I think elixir is probably one of the easiest functional programming languages to learn. most of the libraries use do blocks so you dont' usually think about lambdas unless you're using Enum or List modules and even then, its about as complicated as using reduce or map in javascript. if you know how to use a higher order function, tail recursion and map/reduce/filter, you know enough functional programming to do 99% of day to day elixir work. reply hello_computer 18 hours agoparentprevElixir is a combinatorics mess (circa 2020: two different string types, three different types of exception, return AND throw versions for almost every func, slow compiler without much in the way of compile-time checking, constant breakages over trivial renamings \"just because\", having to burrow into erlang errs half the time since many elixir funcs are just wrappers). Half the libraries we used were abandoned. I take every recommendation with a grain of salt. I think people got invested in the hype, and are trying to keep the sinking ship afloat. Erlang is a different story. If you're interested in the BEAM, it isn't terrible. It has a few of the same problems, but the combinatorics are reduced. reply bcardarella 17 hours agorootparent> Half the libraries we used were abandoned People really need to unwarp their brains from how they judge libraries in Elixir compared to other ecosystems. Erlang is 30 years old. Elixir sits on top of that stability. Elixir will very likely never reach 2.0 because it doesn't need to. And if a 2.0 does come it will be simply to remove deprecated functionality. Not having 12 major version releases per year means what you think are \"abandoned\" are actually stable and perfectly fine to use. In Elixir, we don't really care about the last time a version was pushed. I regularly use and rely on libraries that haven't been touched in years because they don't need to be touched. reply hello_computer 16 hours agorootparentIf the libraries worked, we wouldn't have had to play code archaeologist and find out that half our dependencies had been abandoned since 2017. Even something as simple as a JSON encoder--which is rock-solid in every other language I've used--had a number of bugs (this was four years ago, so memory is hazy, but it had something to do with ambiguity between arrays, lists, or tuples) Erlang is stable, but Elixir sure as hell isn't. Back in 2019, it seemed as though every point release brought a slew of breakages--and usually over the most trivial and pointless things, like adding an underscore to a built-in for \"consistency\". I've been developing for over 20 years, have used all of the mainstream languages, and Elixir was the absolute worst. reply benzible 15 hours agorootparent> Even something as simple as a JSON encoder--which is rock-solid in every other language I've used JSON will be built into the next release of the BEAM: https://erlangforums.com/t/erlang-otp-27-0-rc3-released/3506... reply arrowsmith 17 hours agorootparentprevHow are there three different types of exception? As for \"two string types\" maybe I'm not working on hard enough problems, but in ~4 years of Elixir I've never once needed to use a charlist. My understanding is that it's a backwards-compatibility thing from Erlang and I'm not even sure when I'd ever need to use it over a string. reply hello_computer 15 hours agorootparentraises, throws, & exits. rescue vs catch. as for binary vs charlist, some dependencies wanted binary, others wanted charlist. faced with the choice of re-write the dependency to use binaries, or wrap it with pre/post converters, we chose the latter. after the third or fourth global search-and-replace thanks to elixir's renaming of built-ins just for the hell of it, we re-wrote it in go, and never looked back. reply halostatue 15 hours agorootparentprevWe’ve had Elixir in production since 2017 and have not found any of the items you have mentioned to be issues. - two different string types: You have undercounted (three types in Erlang, and Elixir adds a fourth), and suggested that something which is a non-issue for the vast majority of Elixir code. Most Elixir code deals with `String.t()` (`\"string\"`), which is an Erlang `binary()` type (`\"string\"` in Elixir, `>` in Erlang) with a UTF-8 guarantee on top. A variant of the Erlang `binary()` type is `bitstring()` which uses `binary()` to efficiently store bits in interoperable ways. Code interacting directly with Erlang often needs to use `charlist()`, which is literally a list of integer values mapping to byte values (specified as `'charlist'` in Elixir and `\"charlist\"` in Erlang; most Elixir code would use `String.to_charlist(stringvar)` in the cases where required. Compare and contrast this with the py2 to py3 string changes and the proliferation of string prefix types in py3 (https://docs.python.org/3/reference/lexical_analysis.html#li...). - three different types of exception: true, but inherited from Erlang and the difference is mostly irrelevant. The three types are exceptions (these work pretty much as people expect), throws (non-local returns, see throw/catch in Ruby; these are more structured than `goto LABEL` or `break LABEL` for the most part), and process exits. In general, you only need to worry about exceptions in most code, and process exits only if you are writing something outside of your typical genserver. - return AND throw versions for almost every func: trivially untrue, but also irrelevant. Elixir is more sparse than Ruby, but still comes more from the TIMTOWTDI approach so most libraries that offer bang versions usually do so in terms of one as the default version. That is, `Keyword.fetch!` effectively calls `Keyword.fetch` and throws an exception if the result is `:error`. It also doesn't affect you if you don’t use it. (Compare the fact that anyone who programs C++ is choosing a 30–40% subset of C++ because the language is too big and strange.) - slow compiler without much in the way of compile-time checking: I disagree with \"slow\", even from a 2020 perspective, and \"compile-time checking\" is something that has only improved since you decided that Elixir wasn't for you. Even there, though, different people expect different things from compilers, and not every compiler is going to be the Elm compiler where you can more or less say that if it compiles it will run as intended. (I mean, the C++ compiler is both slow and provides compile time checks that don't improve the safety of your code.) - constant breakages over trivial renamings \"just because\": false‡. Elixir 1.0 code will still compile (it may have deprecation warnings). To the best of my knowledge, nothing from 1.0 has been hard deprecated. ‡If you always compile `--warnings-as-errors`, then yes you will have to deal with the renaming. But that is a choice to turn that on, even though it is good practice. - having to burrow into erlang errs half the time since many elixir funcs are just wrappers: not an issue in my experience, and I can only think of a handful of times where I have had the Erlang functions leak out in a way where I needed to look at Erlang error messages. Elixir isn't suitable for everything, but frankly your list of so-called shortcomings is pure sour grapes. reply hello_computer 14 hours agorootparentI guess I will just have to stay sour with my sub-second compile times, actual compile-time type checks, stable naming of built-ins, single-binary deployments, and uniform error handling. reply peoplefromibiza 18 hours agoparentprevhas Clojure/Haskell/Ada/Coq(...) took over the World? is it wrong to love it then? reply jan3024 18 hours agoparentprev [–] Because embarrassing little in coding is controlled by these imagineers. There’s no much real difference in programming languages anymore, honestly I’ve never met a good programmer that cared about these, it makes your code fringe and inportable. reply arter 17 hours agorootparentHaving a language that generates proper errors, does not try to hold your hand incorrectly and instead tells you you are wrong is always better. Haskell is one of the best in the world in this. But it also happens to be incredibly complex and also weird in it's syntax, to the point it does make code fringe and unreadable. There is a real difference here - a language that can help you fact check your logic and reason that you are accessing a potentially null value in this context can be better if it's drawbacks don't outweight it. Elixir isn't static so it doesn't do this but it does blow up a lot compared to other dynamic languages. It's abstraction of proccesses and threads and what not, does actually make your code more portable and more modifyable. It offers a specific paradigm for programming that works great for multiple proccesses and the whole language is built around it. So if you are going to be using that paradigm anyway it would be a better choice. It's also just pretty how it fits together. reply vvpan 15 hours agorootparentprevI do not write Elixir/Erlang programmers. But I do think that the actor model and BEAM solve a few issues that I face when writing complex backends, which I do. For example I have not found a language whose error handling I like - Go and Rust are super meticulous, for example and you get error handling code everywhere, which is good for stability but not great for developer experience. TS/JS/Node suffers from the opposite - error handling is an afterthought and you never know what exception will come at you and from where and bomb you whole server, so you end up relying on cloud solutions running large redundant arrays of processes. Isolation of errors within a process and message passing is a great abstraction. Another thing is that I get to spend a lot of time on is setting up Redis/Cloud stuff to do basic queue, caching and cron. That stuff takes up time, increases complexity, creates new sources of error and grows institutional knowledge. Using a system which has those built into the language is a major improvement. Vercel/Deno/Bun/etc are solving some of the above by including them in their cloud offerings with relatively good DX, but it still increases complexity, takes you out of code and locks you in with the vendors. reply adammarples 18 hours agorootparentprev [–] Maybe the programmers you hang around with don't work on large scale message streaming? It's pretty niche reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Machine learning in Elixir is now considered production-ready, thanks to its integration with the BEAM and OTP primitives, making it ideal for deploying machine learning models.",
      "The Nx framework, inspired by JAX, offers advantages in metaprogramming and pluggable backends, with features such as Nx.Serving for distributed, hardware-agnostic automatic batching utilizing Elixir's actor model of concurrency.",
      "Elixir's integration with Phoenix applications is seamless, employing libraries like Oban, Broadway, and FLAME for efficient processing and real-time updates, showcasing its scalability and efficiency for machine learning applications."
    ],
    "commentSummary": [
      "The discussion explores utilizing Elixir and BEAM languages such as Erlang for machine learning and software development, underscoring scalability, fault tolerance, and concurrent processing benefits.",
      "Elixir receives praise for its productivity and parallel processing capabilities, yet some raise concerns about typing problems and adoption hurdles.",
      "The debate contrasts Elixir's performance advantages with languages like Python and Java while highlighting challenges like documentation gaps, limited library ecosystem, and community backing."
    ],
    "points": 131,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1715254429
  }
]
