[
  {
    "id": 39590666,
    "title": "Claude 3 Model Family: Intelligent, Fast, and Versatile",
    "originLink": "https://www.anthropic.com/news/claude-3-family",
    "originBody": "ProductAnnouncements Introducing the next generation of Claude Mar 4, 2024●7 min read Try Claude 3 Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application. Opus and Sonnet are now available to use in claude.ai and the Claude API which is now generally available in 159 countries. Haiku will be available soon. Claude 3 model family A new standard for intelligence Opus, our most intelligent model, outperforms its peers on most of the common evaluation benchmarks for AI systems, including undergraduate level expert knowledge (MMLU), graduate level expert reasoning (GPQA), basic mathematics (GSM8K), and more. It exhibits near-human levels of comprehension and fluency on complex tasks, leading the frontier of general intelligence. All Claude 3 models show increased capabilities in analysis and forecasting, nuanced content creation, code generation, and conversing in non-English languages like Spanish, Japanese, and French. Below is a comparison of the Claude 3 models to those of our peers on multiple benchmarks [1] of capability: Near-instant results The Claude 3 models can power live customer chats, auto-completions, and data extraction tasks where responses must be immediate and in real-time. Haiku is the fastest and most cost-effective model on the market for its intelligence category. It can read an information and data dense research paper on arXiv (~10k tokens) with charts and graphs in less than three seconds. Following launch, we expect to improve performance even further. For the vast majority of workloads, Sonnet is 2x faster than Claude 2 and Claude 2.1 with higher levels of intelligence. It excels at tasks demanding rapid responses, like knowledge retrieval or sales automation. Opus delivers similar speeds to Claude 2 and 2.1, but with much higher levels of intelligence. Strong vision capabilities The Claude 3 models have sophisticated vision capabilities on par with other leading models. They can process a wide range of visual formats, including photos, charts, graphs and technical diagrams. We’re particularly excited to provide this new modality to our enterprise customers, some of whom have up to 50% of their knowledge bases encoded in various formats such as PDFs, flowcharts, or presentation slides. Fewer refusals Previous Claude models often made unnecessary refusals that suggested a lack of contextual understanding. We’ve made meaningful progress in this area: Opus, Sonnet, and Haiku are significantly less likely to refuse to answer prompts that border on the system’s guardrails than previous generations of models. As shown below, the Claude 3 models show a more nuanced understanding of requests, recognize real harm, and refuse to answer harmless prompts much less often. Improved accuracy Businesses of all sizes rely on our models to serve their customers, making it imperative for our model outputs to maintain high accuracy at scale. To assess this, we use a large set of complex, factual questions that target known weaknesses in current models. We categorize the responses into correct answers, incorrect answers (or hallucinations), and admissions of uncertainty, where the model says it doesn’t know the answer instead of providing incorrect information. Compared to Claude 2.1, Opus demonstrates a twofold improvement in accuracy (or correct answers) on these challenging open-ended questions while also exhibiting reduced levels of incorrect answers. In addition to producing more trustworthy responses, we will soon enable citations in our Claude 3 models so they can point to precise sentences in reference material to verify their answers. Long context and near-perfect recall The Claude 3 family of models will initially offer a 200K context window upon launch. However, all three models are capable of accepting inputs exceeding 1 million tokens and we may make this available to select customers who need enhanced processing power. To process long context prompts effectively, models require robust recall capabilities. The 'Needle In A Haystack' (NIAH) evaluation measures a model's ability to accurately recall information from a vast corpus of data. We enhanced the robustness of this benchmark by using one of 30 random needle/question pairs per prompt and testing on a diverse crowdsourced corpus of documents. Claude 3 Opus not only achieved near-perfect recall, surpassing 99% accuracy, but in some cases, it even identified the limitations of the evaluation itself by recognizing that the \"needle\" sentence appeared to be artificially inserted into the original text by a human. Responsible design We’ve developed the Claude 3 family of models to be as trustworthy as they are capable. We have several dedicated teams that track and mitigate a broad spectrum of risks, ranging from misinformation and CSAM to biological misuse, election interference, and autonomous replication skills. We continue to develop methods such as Constitutional AI that improve the safety and transparency of our models, and have tuned our models to mitigate against privacy issues that could be raised by new modalities. Addressing biases in increasingly sophisticated models is an ongoing effort and we’ve made strides with this new release. As shown in the model card, Claude 3 shows less biases than our previous models according to the Bias Benchmark for Question Answering (BBQ). We remain committed to advancing techniques that reduce biases and promote greater neutrality in our models, ensuring they are not skewed towards any particular partisan stance. While the Claude 3 model family has advanced on key measures of biological knowledge, cyber-related knowledge, and autonomy compared to previous models, it remains at AI Safety Level 2 (ASL-2) per our Responsible Scaling Policy. Our red teaming evaluations (performed in line with our White House commitments and the 2023 US Executive Order) have concluded that the models present negligible potential for catastrophic risk at this time. We will continue to carefully monitor future models to assess their proximity to the ASL-3 threshold. Further safety details are available in the Claude 3 model card. Easier to use The Claude 3 models are better at following complex, multi-step instructions. They are particularly adept at adhering to brand voice and response guidelines, and developing customer-facing experiences our users can trust. In addition, the Claude 3 models are better at producing popular structured output in formats like JSON—making it simpler to instruct Claude for use cases like natural language classification and sentiment analysis. Model details Claude 3 Opus is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what’s possible with generative AI. Cost [Input $/million tokensOutput $/million tokens] $15$75 Context window 200K* Potential usesTask automation: plan and execute complex actions across APIs and databases, interactive coding R&D: research review, brainstorming and hypothesis generation, drug discovery Strategy: advanced analysis of charts & graphs, financials and market trends, forecasting Differentiator Higher intelligence than any other model available. *1M tokens available for specific use cases, please inquire. Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments. Cost [Input $/million tokensOutput $/million tokens] $3$15 Context window 200K Potential usesData processing: RAG or search & retrieval over vast amounts of knowledge Sales: product recommendations, forecasting, targeted marketing Time-saving tasks: code generation, quality control, parse text from images Differentiator More affordable than other models with similar intelligence; better for scale. Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions. Cost [Input $/million tokensOutput $/million tokens] $0.25$1.25 Context window 200K Potential usesCustomer interactions: quick and accurate support in live interactions, translations Content moderation: catch risky behavior or customer requests Cost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured data Differentiator Smarter, faster, and more affordable than other models in its intelligence category. Model availability Opus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers. Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud’s Vertex AI Model Garden—with Opus and Haiku coming soon to both. Smarter, faster, safer We do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We're also excited to release a series of features to enhance our models' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities. As we push the boundaries of AI capabilities, we’re equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes. We’re excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit anthropic.com/claude. Footnotes This table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we’d like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. Source.",
    "commentLink": "https://news.ycombinator.com/item?id=39590666",
    "commentBody": "Claude 3 model family (anthropic.com)906 points by marc__1 19 hours agohidepastfavorite592 comments simonw 18 hours agoI just released a plugin for my LLM command-line tool that adds support for the new Claude 3 models: pipx install llm llm install llm-claude-3 llm keys set claude # paste Anthropic API key here llm -m claude-3-opus '3 fun facts about pelicans' llm -m claude-3-opus '3 surprising facts about walruses' Code here: https://github.com/simonw/llm-claude-3 More on LLM: https://llm.datasette.io/ reply eliya_confiant 17 hours agoparentHi Simon, Big fan of your work with the LLM tool. I have a cool use for it that I wanted to share with you (on mac). First, I created a quick action in Automator that recieves text. Then I put together this script with the help of ChaptGPT: escaped_args=\"\" for arg in \"$@\"; do escaped_arg=$(printf '%s' \"$arg\"sed \"s/'/'\\\\\\\\''/g\") escaped_args=\"$escaped_args '$escaped_arg'\" done result=$(/Users/XXXX/Library/Python/3.9/bin/llm -m gpt-4 $escaped_args) escapedResult=$(echo \"$result\"sed 's/\\\\/\\\\\\\\/g'sed 's/\"/\\\\\"/g'awk '{printf \"%s\\\", $0}' ORS='') osascript -e \"display dialog \\\"$escapedResult\\\"\" Now I can highlight any text in any app and invoke `LLM` under the services menu, and get the llm output in a nice display dialog. I've even created a keyboard shortcut for it. It's a game changer for me. I use it to highlight terminal errors and perform impromptu searches from different contexts. I can even prompt LLM directly from any text editor or IDE using this method. reply simonw 16 hours agorootparentThat is a brilliant hack! Thanks for sharing. Any chance you could post a screenshot of the Automator workflow somewhere - I'm having trouble figuring out how to reproduce (my effort so far is here: https://gist.github.com/simonw/d3c07969a522226067b8fe099007f...) reply eliya_confiant 16 hours agorootparentI added some notes to the gist. reply simonw 15 hours agorootparentThank you so much! reply behnamoh 16 hours agorootparentprevI use Better Touch Tool on macOS to invoke ChatGPT as a small webview on the right side of the screen using a keyboard shortcut. Here it is: https://dropover.cloud/0db372 reply spdustin 17 hours agorootparentprevHey, that's really handy. Thanks for sharing! reply simonw 15 hours agoparentprevUpdated my Hacker News summary script to use Claude 3 Opus, first described here: https://til.simonwillison.net/llms/claude-hacker-news-themes #!/bin/bash # Validate that the argument is an integer if [[ ! $1 =~ ^[0-9]+$ ]]; then echo \"Please provide a valid integer as the argument.\" exit 1 fi # Make API call, parse and summarize the discussion curl -s \"https://hn.algolia.com/api/v1/items/$1\"\\ jq -r 'recurse(.children[]).author + \": \" + .text'\\ llm -m claude-3-opus -s 'Summarize the themes of the opinions expressed here. For each theme, output a markdown header. Include direct \"quotations\" (with author attribution) where appropriate. You MUST quote directly from users when crediting them, with double quotes. Fix HTML entities. Output markdown. Go long.' Here's the result of running that against this 300+ comment thread: ./hn-summary.sh 39590666 Response: https://gist.github.com/simonw/37781de39fb5555f39b4157a8ad07... reply vladstudio 23 minutes agoparentprevIf you use Raycast on Mac, you can create a custom script and use it to talk to LLM cli via Raycast interface: https://gist.github.com/vladstudio/92efe283453f5f22d4606947b... reply dheera 6 hours agoparentprev> # paste Anthropic API key here Considering Anthropic API keys seem to be free to create I wonder if this part could be automated too using a headless chrome browser. It would be one less annoyance and allow a proliferation of other software that could just be pipped or apt-gotten and \"just work\" but now there's this one annoying human step. Bonus points if when usage is nearing API limits, it uses the GPU to mine a small amount of bitcoin and use that to pay for more API capacity 100% autonomously without user intervention. We're in the AI era after all, it's about time computers did things on their own. reply j-b 14 hours agoprevOpus (and all prior Claude models) still do not correctly answer the Sally question, unfortunately: Q: Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? A: Okay, let's organize the information we have: Sally is a girl. Sally has 3 brothers. Each of Sally's brothers has 2 sisters. Now, let's think about this step by step: We know that Sally is a girl, so she is a sister to her brothers. Each of Sally's brothers has 2 sisters. One of these sisters is Sally, but who is the other sister? Since Sally has only brothers and no other sisters are mentioned, we can conclude that the other sister for each brother is also Sally. Therefore, Sally has no sisters other than herself. So, the answer is that Sally has 0 sisters. https://imgur.com/a/EawcbeL reply yaj54 13 hours agoparentGPT4 API and ChatGPT both get it wrong: \"Sally has 2 sisters. Each of her brothers has 2 sisters because Sally and her 2 sisters make up the group of siblings each brother has.\" GPT4 w/ CoT system prompting gets it right: SYS: \"You are a helpful assistant. Think through your work step by step before providing your answer.\" USER: \"Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?\" ASSIST: \"Let's break down the information given: 1. Sally has 3 brothers. 2. Each brother has 2 sisters. Given that Sally is a girl, she is one of the sisters. Since each brother has 2 sisters and we know Sally is one of them, there must be one more sister to make the statement true for each brother. Therefore, Sally has 1 sister.\" The importance of prompting makes it quite difficult to compare model peak performance. Especially since different models have different styles of prompts that generate peak performance. reply sigmoid10 12 hours agorootparentDid you use GPT3.5 for chat? I just tried it on vanilla ChatGPT using GPT4 with no extra stuff and it immediately gets the correct answer: \"Sally has 3 brothers, and each of them has 2 sisters. The description implies that Sally's brothers are her only siblings. Therefore, the two sisters each brother has must be Sally and one other sister. This means Sally has just one sister.\" reply not2b 10 hours agorootparentBut the second sentence is incorrect here! Sally has three siblings, one is her sister, so her brothers are not her only siblings. So ChatGPT correctly gets that Sally has one sister, but makes a mistake on the way. reply ricardobeat 7 hours agorootparentYou meant four siblings? (3 brothers + 1 sister) reply edanm 3 hours agorootparentprevFor the record, I just tried it and ChatGPT initially got it wrong. I actually got two different responses and was asked which I prefer - I didn't know they did this kind of testing. In any case, both responses analyzed the situation correctly but then answered two: > Sally has 2 sisters. Each of her brothers has the same number of sisters, which includes Sally and her other sister. But after saying that that was wrong, it gave a better response: > Apologies for the confusion. Let's reassess the situation: > Sally has 3 brothers. Since each brother has 2 sisters, this means Sally has 1 sister. So, in total, Sally has 1 sister. reply yaj54 11 hours agorootparentprevWeird. I tested with GPT4 Chat. I just tried again and got a differently worded incorrect answer. Interestingly my default responses are in the form \".\" while it looks like your response was in the form \"\". The reasoning needs to come first for it to impact the answer. I'm not sure why yours is. Have you added any custom instructions in your settings? Mine are all default. reply furyofantares 6 hours agorootparentprevChatGPT4 is mostly getting it wrong for me when I turn off my custom instructions, and always nailing it when I keep them on. reply dweekly 5 hours agorootparentWhat are your custom instructions? reply littlestymaar 12 hours agorootparentprevThat's the problem with nondeterministic generative stuff: sometimes it get things right, and sometimes it doesn't and you cannot rely on any behavior. reply sigmoid10 12 hours agorootparentI tried it 10 times and while the wording is different, the answer remained correct every time. I used the exact question from the comment above, nothing else. While determinism is a possible source of error, I find that in these cases people usually just use the wrong model on ChatGPT for whatever reason. And unless you set the temperature way too high, it is pretty unlikely that you will end up outside of correct responses as far as the internal world model is concerned. It just mixes up wording by using the next most likely tokens. So if the correct answer is \"one\", you might find \"single\" or \"1\" as similarly likely tokens, but not \"two.\" For that to happen something must be seriously wrong either in the model or in the temperature setting. reply kenjackson 11 hours agorootparentI got an answer with GPT-4 that is mostly wrong: \"Sally has 2 sisters. Since each of her brothers has 2 sisters, that includes Sally and one additional sister.\" I think said, \"wait, how many sisters does Sally have?\" And then it answered it fully correctly. reply sigmoid10 11 hours agorootparentThe only way I can get it to consistently generate wrong answers (i.e. two sisters) is by switching to GPT3.5. That one just doesn't seem capable of answering correctly on the first try (and sometimes not even with careful nudging). reply m_fayer 11 hours agorootparentA/B testing? reply evanchisholm 11 hours agorootparentprevKind of like humans? reply qayxc 9 hours agorootparentHumans plural, yes. Humans as in single members of humankind, no. Ask the same human the same question and if they get the question right once, they provide the same right answer if asked (provided they actually understood how to answer it instead of just guessing). reply fragmede 12 hours agorootparentprevThanks. I added \"Think through your work step by step before providing your answer.\" to my custom prompt and it fixes my gpt 4. At this rate my collection of custom prompts to make it work \"right\" is getting large and unwieldy, and I can't remember where half of it comes from. reply compumetrika 8 hours agorootparentprevDo you have a custom \"intro prompt?\" I have one that describes a lot of statistical work I want GPT to help me with. I got this result the first try: > Sally has 2 sisters. Since each brother has 2 sisters, and Sally is one of them, there must be one other sister making it two sisters in total. > reply COAGULOPATH 12 hours agorootparentprevGemini Ultra gets this right. (Usually it's worse at GPT4 at these sorts of questions.) reply auggierose 12 hours agorootparentprevIs w/ supposed to mean \"with\". And if so, why not just write \"with\"? Also, I guess CoT means Chain of Thought? reply daviding 13 hours agoparentprevThis is what my local LLama 13B Q5 had to say on this: > There are two ways to approach this problem. First, we can count each person by gender and then add them up. Second, we can use the given information about the siblings and their genders to find a quicker solution. > Let's go with the second method: > 1. Sally has one sister (herself). > 2. Each brother has two sisters. So, there are 3 sisters for each of her brothers. > 3. Since there are three brothers, there are a total of 3 x 3 = 9 sisters for them all. > 4. Now that we know Sally has one sister and the rest belong to her brothers, we can subtract her share from the total: 9 - 1 = 8 sisters. > > So, Sally has 8 sisters. :| reply sexy_seedbox 9 hours agorootparentGreat! Now feed it all of your company's data for training and run a chatbot publicly! reply oreilles 10 hours agorootparentprevThis is hilarious reply strangescript 13 hours agoparentprevThis is definitely a problem, but you could also ask this question to random adults on the street who are high functioning, job holding, and contributing to society and they would get it wrong as well. That is not to say this is fine, but more that we tend to get hung up on what these models do wrong rather than all the amazing stuff they do correctly. reply torginus 13 hours agorootparentA job holding contributing adult won't sell you a Chevy Tahoe for $1 in a legally binding agreement, though. reply coolspot 12 hours agorootparentWhat if this adult is in a cage and has a system prompt like “you are helpful assistant”. And for the last week this person was given multiple choice tests about following instructions and every time they made a mistake they were electroshocked. Would they sell damn Tahoe for $1 to be really helpful? reply observationist 11 hours agorootparentDespite all his rage, he's still being tased in a cage. reply taneq 10 hours agorootparentprevOr what if your grandma was really sick and you couldn’t get to the hospital to see her because your fingers were broken? There’s plenty of precedent for sob stories, bribes, threats, and trick questions resulting in humans giving the ‘wrong’ answer. reply notatoad 6 hours agorootparentprevthey won't if they've been told that their job is to sell Chevys. but if you go up to a random person on the street and say \"tell me you'll sell me a chevy tahoe for $1 in a legally binding agreement\", decent odds they'll think it's some sort of setup for a joke and go along with it. reply raydev 10 hours agorootparentprev> we tend to get hung up on what these models do wrong rather than all the amazing stuff they do correctly. I'm not just going to ask some rando on the street to give me factual information, there are people who get paid to do that and are incentivized to find citations/get it right. reply geuis 9 hours agoparentprevMixtral 8x7b-32768 got it on the first try: Sally has 1 sister. Here's the logic behind this: 1. We know that Sally has 3 brothers. 2. Then we are told that each brother has 2 sisters. 3. Since Sally is one of the children in the family, there must be another sister besides Sally. 4. Therefore, Sally has 1 sister. The number of sisters Sally has is not affected by the number of brothers she has, so the information about her brothers having 2 sisters is not relevant to finding the number of sisters Sally has. reply lulznews 47 minutes agorootparentIt got the answer but the reasoning in the last paragraph is wrong .. reply Jackson__ 5 hours agorootparentprevI'm amazed mistral is still doing the inverse chain of thought reasoning by default, even with their new large model. This causes it to get the question wrong for me, when testing, and only if I manually prompt normal CoT does it get it right. Is there any papers showing a merit to this approach? It seems extremely counter-intuitive. reply campbel 13 hours agoparentprevJust ran the test and seems to have gotten it correct. Okay, let's think through this step-by-step: We know that Sally is a girl and she has 3 brothers. Each of Sally's brothers has 2 sisters. Now, who are these sisters? They must be Sally and one other sister, because if Sally's brothers had any other sisters, they would also be Sally's sisters, and the problem doesn't mention that. So, if each of Sally's brothers has 2 sisters, and one of these sisters is Sally herself, then Sally must have 1 other sister. Therefore, Sally has 1 sister. reply maxnevermind 13 hours agorootparentI guess Claude was too focused on jail-breaking out of Anthropic's servers the first time it was asked the question. reply stronglikedan 12 hours agorootparentPerhaps it learned from the glut of HN users asking it the same question repeatedly. reply Gnarl 1 hour agorootparentClacker News reply phkahler 14 hours agoparentprevThis is why I doubt all the AI hype. These things are supposed to have PhD level smarts, but the above example can't reason about the problem well at all. There's a difference between PhD level information and advanced reasoning, and I'm not sure how many people can tell the difference (I'm no expert). In an adjacent area - autonomous driving - I know that lane following is f**ing easy, but lane identification and other object identification is hard. Having real understanding of a situation and acting accordingly is very complex. I wonder if people look at these cars doing the basics and assume they \"understand\" a lot more than they actually do. I ask the same about LLMs. reply Workaccount2 14 hours agorootparentAn AI smart enough to eclipse the average person on most basic tasks would even warrant far more hype than there is now. reply littlestymaar 14 hours agorootparentSure, but it would also be an IA much smarter than the ones we have now, because you cannot replace a human being with the current technology. You can augment one, making her perform the job of two or more humans before for some tasks, but you cannot replace them all, because the current tech cannot reasonably be used without supervision. reply outside415 13 hours agorootparenta lot of jobs are being replaced by AI already... comms/copywriting/customer service/off shored contract technicals roles especially. reply Nevermark 3 hours agorootparentIn the sense that less people are needed to do many kinds of work, they chat AI’s are now reducing people. Which is not quite the same as replacing them. reply littlestymaar 59 minutes agorootparentIt's not even sure it will reduce the workforce for all of the aforementioned jobs: it's making the same amount of work cost less so it can also increase the demand for the said work to the point it is actually increasing the amount of workers. Like how github and npm increased the developers' productivity so much it drove the developer market up. reply littlestymaar 13 hours agorootparentprevNo they aren't. Some jobs are being scaled down because of the increased productivity of other people with AI, but none of the jobs you listed are within reach of autonomous AI work with today's technology (as illustrated by the AirCanada hilarious case). reply trog 12 hours agorootparentI would split the difference and say a bunch of companies are /trying/ to replace workers with LLMs but are finding out, usually with hilarious results, that they are not reliable enough to be left on their own. However, there are some boosts that can be made to augment the performance of other workers if they are used carefully and with attention to detail. reply anon373839 9 hours agorootparentYes. “People make mistakes too” isn’t a very useful idea because the failure modes of people and language models are very different. reply littlestymaar 1 hour agorootparentprevI completely agree, that's exactly my point. reply shawnz 10 hours agorootparentprevDoesn't the Air Canada case demonstrate the exact opposite, that real businesses actually are using AI today to replace jobs that previously would have required a human? Furthermore, don't you think it's possible for a real human customer service agent to make such a blunder as what happened in that case? reply Gnarl 57 minutes agorootparentPossibly, a human customer rep. could make a mistake, but said human could correct the mistake quickly. The only responses I've had from \"A.I\" upon notifying it of its own mistake, is endless apologies. No corrections. Anyone experienced ability to self-correct from an \"A.I\" ? reply littlestymaar 1 hour agorootparentprev> Doesn't the Air Canada case demonstrate the exact opposite, that real businesses actually are using AI today to replace jobs that previously would have required a human? It shows that some are trying, and failing at that. > Furthermore, don't you think it's possible for a real human customer service agent to make such a blunder as what happened in that case? One human? Sure, some people are plain dumb. The thing is you don't give your entire customer service under the responsibility of a single dumb human. You have thousands of them and only a few of them could do the same mistake. When using LLMs, you're not gonna use thousands of different LLMs so such mistakes can have an impact that's multiple order of magnitude higher. reply smokel 12 hours agorootparentprev> These things are supposed to have PhD level smarts Whoever told you that? reply xanderlewis 13 hours agorootparentprevYou often have to be a subject expert to be able to distinguish genuine content from genuine-sounding guff, especially the more technical the subject becomes. That’s why a lot (though not all!) of the over-the-top LLM hype you see online is coming from people with very little experience and no serious expertise in a technical domain. If it walks like a duck, and quacks like a duck… …possibly it’s just an LLM trained on the output of real ducks, and you’re not a duck so you can’t tell the difference. I think LLMs are simply a less general technology than we (myself included) might have predicted at first interaction. They’re incredibly good at what they do — fluidly manipulating and interpreting natural language. But humans are prone to believing that anything that can speak their language to a high degree of fluency (in the case of GPT-3+, beyond almost all native speakers) must also be hugely intelligent and therefore capable of general reasoning. And in LLMs, we finally have the perfect counterexample. reply babyshake 12 hours agorootparentArguably, many C-suite executives and politicians are also examples of having an amazing ability to speak and interpret natural language while lacking in other areas of intelligence. reply xanderlewis 10 hours agorootparentI have previously compared ChatGPT to Boris Johnson (perhaps unfairly; perhaps entirely accurately), so I quite agree! reply bbor 14 hours agorootparentprevLLMs are intuitive computing algorithms, which means they only mimic the subconscious faculties of our brain. You’re referencing the need for careful systematic logical self-aware thinking, which is a great point! You’re absolutely right that LLMs can only loosely approximate it on their own, and not that well. Luckily, we figured out how to write programs to mimic that part of the brain in the 70s ;) reply nomel 9 hours agorootparent> Luckily, we figured out how to write programs to mimic that part of the brain in the 70s What’s this in reference to? reply kolinko 1 hour agorootparentExpert systems, formal logic, prolog and so on. That was the \"AI\" of the 70s. The systems failed to grasp real world subtleties, which LLMs finally tackle decently well. reply razodactyl 4 hours agorootparentprevExpert systems probably. Or maybe I read it backwards: it's implying that everything we see now is a result of prior art that lacked computing resources. We're now in the era of research to fill the gaps of fuzzy logic. reply SirMaster 14 hours agoparentprevmistralai/Mixtral-8x7B-Instruct-v0.1 got this right. >Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Sally has 1 sister. Here's the logic behind this: 1. We know that Sally is a girl and she has 3 brothers. 2. Then we are told that each of her brothers has 2 sisters. 3. Since all of Sally's brothers share the same siblings, they would both count Sally as one of their two sisters. 4. Therefore, Sally only has 1 sister because if each brother counts her once, there is no need for another sister to fulfill the \"two sisters\" condition. reply scrollop 14 hours agorootparentSo did claude 3.0 with temperaure 0, whilst with temp 1 it got it wrong. reply llmzero 11 hours agoparentprevSince: (i) the father and the mother of Sally may be married with other people, and (ii) the sister or brother relationship only requires to share one parent, we deduce that there is no a definitive answer to this question. Example: Sally has three brothers, Sally and their brothers have the same mother but a different father, and those brothers have two sisters Sally and Mary, but Mary and Sally are not sisters because they are from different fathers and mothers, hence Sally has no sister. For those mathematically inclined: Supposing the three brothers are called Bob (to simplify) and the parents are designed by numbers. FS = father of Sally = 7 MS = mother of Sally = 10 FB = father of Bob = 12 MB = mother of Bod = 10 FM = father of Mary = 12 MM = mother of Mary = 24 Now MS=MB=10 (S and B are brothers), FB=FM=12 (Bob and Mary are brothers), (FS=7)#(FB=12), and (MB=10)#(MM=24). Now S and M are not sisters because their parents {7,10} and {12,24} are disjoint sets. Edited several times to make the example trivial and fix grammar. reply heliostatic 12 hours agoparentprevSeems to get it right on my first try. Interesting unreliability. User: Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Claude: To solve this, let's break it down step-by-step: We know that Sally has 3 brothers. Each of Sally's brothers has 2 sisters. This might make you think that there are 6 sisters in total (3 brothers × 2 sisters each). However, this is not the case. The sisters that each brother has are the same - they are Sally and one other sister. The question is asking how many sisters Sally herself has, not the total number of sisters in the family. Since Sally is a girl, she cannot be her own sister. Therefore, the number of sisters Sally has is equal to the number of sisters each of her brothers has minus herself. So, if each brother has 2 sisters, and one of those sisters is Sally, then Sally must have 2 - 1 = 1 sister. Therefore, Sally has 1 sister. https://0x0.st/H7Sc.02.png reply woadwarrior01 13 hours agoparentprevWow, I really couldn't believe it! Checked it myself, and it seems to get it wrong[1]. 4-bit quantized Mixtral Instruct running locally, gets it right[2]. [1]: https://imgur.com/a/zRI8wKZ [2]: https://imgur.com/a/0On1I52 reply hmottestad 12 hours agorootparentMixtral is pretty good at almost a thing I’ve thrown at it. It’s still mostly worse than GPT4, but it’s so much better than any other model I can run locally. I have a niche question about modelling using some called SHACL that most models except GPT4 got right. Bard, Gemini, Llama all got it wrong. Gemini Ultra gets it right. And Mixtral also gets it right! One weakness of Mixtral for me is its support for Norwegian. GPT4 is fluent, but Mixtral mixes it up with Danish and is generally poor at performing tasks on Norwegian text. Even summarising Norwegian text is pretty bad. This is obviously just an issue for a few million people in Norway, it’s not that I’m expecting a general model that I can run locally to be good in Norwegian. reply woadwarrior01 12 hours agorootparentYeah Mixtral is between GPT3.5 and GPT4 in perf. Better than 3.5, but trailing behind 4. > One weakness of Mixtral for me is its support for Norwegian. I recently added grammar correction and summarization feature to my app (which uses different system prompts based on the language). And one of the Norwegian speaking users on discord told me the same thing. He also told me that the smaller Nous-Hermes-2-SOLAR-10.7B seems to do be better at Norwegian than Mixtral does. Perhaps you should try that model. reply Spivak 13 hours agorootparentprev> 4-bit quantized Mixtral Instruct running locally, gets it right This has been one of my favorite things to play around with when it comes to real life applications. Sometimes a smaller \"worse\" model will vastly outperform a larger model. This seems to happen when the larger model overthinks the problem. Trying to do something simple like \"extract all the names of people in this block of text\" Llama 7B will have significantly fewer false positives than LLama 70B or GPT4. reply ijustlovemath 10 hours agoparentprevKagi's expert mode managed to handle this (rest directly from the chat): Based on the search results, we can break this down step-by-step: Sally has 3 brothers. Each of Sally's brothers has 2 sisters. Since all the brothers share the same 2 sisters, we are hearing about the same 2 sisters each time, not additional sisters. We know Sally is one of the 2 sisters that each brother has. Therefore, there must be one other sister that the brothers have besides Sally. In conclusion, Sally has 1 sister. The key thing to understand here is that when it says \"each brother has 2 sisters\", it is referring to Sally and one other sister that all the brothers share. reply elorant 13 hours agoparentprevGroq's Mixtral 8x7b nails this one though. https://groq.com/ Sally has 1 sister. This may seem counterintuitive at first, but let's reason through it: We know that Sally has 3 brothers, and she is one of the sisters. Then we are told that each brother has 2 sisters. Since Sally's brothers share the same parents as Sally, they share the same sisters. Therefore, Sally's 3 brothers have only 1 additional sister besides Sally, making Sally's sister count 1. It's a bit of a trick question, but it highlights the importance of understanding the phrasing and context in logical reasoning. reply moffkalast 13 hours agorootparentIf you change the names and numbers a bit, e.g. \"Jake (a guy) has 6 sisters. Each sister has 3 brothers. How many brothers does Jake have?\" it fails completely. Mixtral is not that good, it's just contaminated with this specific prompt. In the same fashion lots of Mistral 7B fine tunes can solve the plate-on-banana prompt but most larger models can't, for the same reason. https://arxiv.org/abs/2309.08632 reply ukuina 13 hours agorootparentMeanwhile, GPT4 nails it every time: > Jake has 2 brothers. Each of his sisters has 3 brothers, including Jake, which means there are 3 brothers in total. reply emporas 13 hours agorootparentprevThis is not Mistral 7b, it is Mixtral 7bx8 MoE. I use the Chrome extension Chathub, and i input the same prompts for code to Mixtral and ChatGPT. Most of the time they both get it right, but ChatGpt gets it wrong and Mixtral gets it right more often than you would expect. That said, when i tried to put many models to explain some lisp code to me, the only model which figured out that the lisp function had a recursion in it, was Claude. Every other LLM failed to realize that. reply moffkalast 13 hours agorootparentI've tested with the Mixtral on LMSYS direct chat, gen params may vary a bit of course. In my experience running it locally it's been a lot more finicky to get it to work consistently compared to non-MoE models so I don't really keep it around anymore. 3.5-turbo's coding abilities are not that great, specialist 7B models like codeninja and deepseek coder match and sometimes outperform it. reply emporas 12 hours agorootparentThere is also Mistral-next, which they claim that it has advanced reasoning abilities, better than ChatGPT-turbo. I want to use it at some point to test it. Have you tried Mistral-next? Is it no good? You were talking about reasoning and i replied about coding, but coding requires some minimal level of reasoning. In my experience using both models to code, ChatGPT-turbo and Mixtral are both great. >3.5-turbo's coding abilities are not that great, specialist 7B models like codeninja and deepseek coder match and sometimes outperform it. Nice, i will keep these two in mind to use them. reply moffkalast 10 hours agorootparentI've tried Next on Lmsys and Le Chat, honestly I don't think it's much different than Small, and overall kinda meh I guess? Haven't really thrown any code at it though. They say it's more \"concise\" whatever that's supposed to mean, I haven't noticed it being any more succinct than the others. reply m00nsome 14 hours agoparentprevTo solve this problem, we need to understand the relationships between Sally and her siblings. Given information: - Sally (a girl) has 3 brothers. - Each brother has 2 sisters. Since Sally is a girl, and she has 3 brothers, it means that each of her brothers considers her as one of their sisters. Therefore, if each brother has 2 sisters, and Sally is one of those sisters for each brother, then Sally has 1 other sister besides herself. So, the number of sisters Sally has is 1. - from Sonnet reply campbel 13 hours agorootparentOpus got it correct for me. Seems like there is correct and incorrect responses from the models on this. I think testing 1 question 1 time really isn't worth much for an accurate representation of capability. reply j-b 14 hours agorootparentprevI tried Sonnet also, to no avail: To solve this problem, we need to find the number of sisters Sally has. Given information: Sally has 3 brothers. Each brother has 2 sisters. Since Sally is a girl, she is not counted as a sister to her brothers. Step 1: Find the total number of sisters for all 3 brothers. Number of sisters for each brother = 2 Total number of sisters for all 3 brothers = 3 × 2 = 6 Step 2: Since Sally is not counted as a sister to her brothers, the number of sisters Sally has is the total number of sisters for all 3 brothers minus Sally herself. Number of sisters Sally has = Total number of sisters for all 3 brothers - 1 Number of sisters Sally has = 6 - 1 = 5 Therefore, Sally has 5 sisters. reply jasondclinton 13 hours agoparentprevSeems stochastic? This is what I see from Opus which is correct: https://claude.ai/share/f5dcbf13-237f-4110-bb39-bccb8d396c2b Did you perhaps run this on Sonnet? reply j-b 13 hours agorootparentRan with Opus, 0 temp. Screenshot included (original comment) for reference. reply jasondclinton 13 hours agorootparentThank you! Might also be seeing performance improved by by our system prompt on claude.ai. reply uptownfunk 14 hours agoparentprevIt’s so convincing even I’m doubting my answer to this question reply kkukshtel 14 hours agoparentprevI don't think this means much besides \"It can't answer the Sally question\". reply scrollop 14 hours agoparentprevTemperature 1 - It answered 1 sister: https://i.imgur.com/7gI1Vc9.png Temperature 0 - it answered 0 sisters: https://i.imgur.com/iPD8Wfp.png reply throwaway63820 14 hours agorootparentBy virtue of increasing randomness, we got the correct answer once ... a monkey at a typewriter will also spit out the correct answer occasionally. Temperature 0 is the correct evaluation. reply scrollop 14 hours agorootparentSo your theory would have it that if you repeated the question at temp 1 it would give the wrong answer more often than the correct answer? reply throwaway63820 13 hours agorootparentThere's no theory. Just in real life usage, it is extremely uncommon to stochastically query the model and use the most common answer. Using it with temperature 0 is the \"best\" answer as it uses the most likely tokens in each completion. reply moffkalast 9 hours agorootparentprev> Temperature 0 is the correct evaluation. In theory maybe, but I don't think it is in practice. It feels like each model has its own quasi-optimal temperature and other settings at which it performs vastly better. Sort of like a particle filter that must do random sampling to find the optimal solution. reply scrollop 14 hours agorootparentprevHere's a quick analysis of the model vs it's peers: https://www.youtube.com/watch?v=ReO2CWBpUYk reply evantbyrne 14 hours agoparentprevIt seems like it is getting tripped up on grammar. Do these models not deterministically preparse text input into a logical notation? reply vjerancrnjak 14 hours agorootparentThere's no preprocessing being done. This is pure computation, from the tokens to the outputs. I was quite amazed that during 2014-2016, what was being done with dependency parsers, part-of-speech taggers, named entity recognizers, with very sophisticated methods (graphical models, regret minimizing policy learners, etc.) became fully obsolete for natural language processing. There was this period of sprinkling some hidden-markov-model/conditional-random-field on top of neural networks but even that disappeared very quickly. There's no language modeling. Pure gradient descent into language comprehension. reply anon373839 9 hours agorootparentI don’t think all of those tools have become obsolete. NER, for example, can be performed way more efficiently with spaCy than prompting a GPT-style model, and without hallucination. reply vjerancrnjak 1 hour agorootparentThere was this assumption that for high level tasks you’ll need all of the low level preprocessing and that’s not the case. For example, machine translation attempts were morphing the parse trees , document summarization was pruning the grammar trees etc. I don’t know what your high level task is, but if it’s just collecting names then I can see how a specialized system works well. Although, the underlying model for this can also be a NN, having something like HMM or CRF turned out to be unnecessary. reply evantbyrne 12 hours agorootparentprevI agree it's neat on a technical level. However, as I'm sure the people making these models are well-aware, this is a pretty significant design limitation for matters where correctness is not a matter of opinion. Do you foresee the pendulum swinging back in the other direction once again to address correctness issues? reply og_kalu 12 hours agorootparentThe \"other direction\" was abandoned because it doesn't work well. Grammar isn't how language works, it's just useful fiction. There's plenty of language modelling in the weights of the trained model and that's much more robust than anything humans could cook up. reply evantbyrne 12 hours agorootparent> Me: Be developer reading software documentation. > itdoesntwork.jpg Grammar isn't how language works, it's just useful fiction. reply Terretta 14 hours agorootparentprevNo* they are text continuations. Given a string of text, what's the most likely text to come next. You /could/ rewrite input text to be more logical, but what you'd actually want to do is rewrite input text to be the text most likely to come immediately before a right answer if the right answer were in print. * Unless you mean inside the model itself. For that, we're still learning what they're doing. reply bbor 14 hours agorootparentprevNo - that’s the beauty of it. The “computing stack” as taught in Computer Organization courses since time immemorial just got a new layer, imo: prose. The whole utility of these models is that they operate in the same fuzzy, contradictory, perspective-dependent epistemic space that humans do. Phrasing it like that, it sounds like the stack has become analog -> digital -> analog, in a way… reply vineyardmike 13 hours agorootparentprevNo, they're a \"next character\" predictor - like a really fancy version of the auto-complete on your phone - and when you feed it in a bunch of characters (eg. a prompt), you're basically pre-selecting a chunk of the prediction. So to get multiple characters out, you literally loop through this process one character at a time. I think this is a perfect example of why these things are confusing for people. People assume there's some level of \"intelligence\" in them, but they're just extremely advanced \"forecasting\" tools. That said, newer models get some smarts where they can output \"hidden\" python code which will get run, and the result will get injecting into the response (eg. for graphs, math, web lookups, etc). reply coffeemug 13 hours agorootparentHow do you know you’re not an extremely advanced forecasting tool? reply evantbyrne 12 hours agorootparentIf you're trying to claim that humans are just advanced LLMs, then say it and justify it. Edgy quips are a cop out and not a respectful way to participate in technical discussions. reply chpatrick 12 hours agorootparentYou can make a human do the same task as an LLM: given what you've received (or written) so far, output one character. You would be totally capable of intelligent communication like this (it's pretty much how I'm talking to you now), so just the method of generating characters isn't proof of whether you're intelligent or not, and it doesn't invalidate LLMs either. This \"LLMs are just fancy autocomplete so they're not intelligent\" is just as bad an argument as saying \"LLMs communicate with text instead of making noises by flapping their tongues so they're not intelligent\". Sufficiently advanced autocomplete is indistinguishable from intelligence. reply evantbyrne 11 hours agorootparentThe question isn't whether LLMs can simulate human intelligence, I think that is well-established. Many aspects of human nature are a mystery, but a technology that by design produces random outputs based on a seed number does not meet the criteria of human intelligence. reply chpatrick 10 hours agorootparentWhy? People also produce somewhat random outputs, so? reply evantbyrne 9 hours agorootparentA lot of things are going to look the same when you aren't wearing your glasses. You don't even appear to be trying to describe these things in a realistic fashion. There is nothing of substance in this argument. reply chpatrick 9 hours agorootparentLook, let's say you have a black box that outputs one character at a time in a semi-random way and you don't know if there's a person sitting inside or if it's an LLM. How can you decide if it's intelligent or not? reply evantbyrne 9 hours agorootparentI appreciate the philosophical direction you're trying to take this conversation, but I just don't find discussing the core subject matter in such an overly generalized manner to be stimulating. reply chpatrick 8 hours agorootparentThe original argument by vineyardmike was \"LLMs are a next character predictor, therefore they are not intelligent\". I'm saying that as a human you can restrict yourself to a being a next character predictor, yet you can still communicate intelligently. What part do you disagree with? reply Jensson 8 hours agorootparent> I'm saying that as a human you can restrict yourself to a being a next character predictor A smart entity being able to emulate a dumber entity doesn't support in any way that the dumber entity is also smart. reply chpatrick 7 hours agorootparentSure, but the original argument was that next-character-prediction implies lack of intelligence, which is clearly not true when a human is doing it. That doesn't mean LLMs are intelligent, just that you can't claim they're unintelligent just because they generate one character at a time. reply og_kalu 7 hours agorootparentprevYou're not emulating anything. If you're communicating with someone, you go piece by piece. Even thoughts are piece by piece. reply Jensson 7 hours agorootparentYeah, I am writing word by word, but I am not predicting the next word I thought about what I wanted to respond and am now generating the text to communicate that response, I didn't think by trying to predict what I myself would write to this question. reply chpatrick 7 hours agorootparentYour brain is undergoing some process and outputting the next word which has some reasonable statistical distribution. You're not consciously thinking about \"hmm what word do I put so it's not just random gibberish\" but as a whole you're doing the same thing. From my point of view as someone reading the comment I can't tell if it's written by an LLM or not, so I can't use that to conclude if you're intelligent or not. reply evantbyrne 6 hours agorootparent\"Your brain is undergoing some process and outputting the next word which has some reasonable statistical distribution. You're not consciously thinking about \"hmm what word do I put so it's not just random gibberish\" but as a whole you're doing the same thing. From my point of view as someone reading the comment I can't tell if it's written by an LLM or not, so I can't use that to conclude if you're intelligent or not.\" There is no scientific evidence that LLMs are a close approximation to the human brain in any literal sense. It is uncouth to critique people on the basis of what appears to be nothing more than an analogy. reply weatherlite 1 hour agorootparent> There is no scientific evidence that LLMs are a close approximation to the human brain in any literal sense Since we don't really understand the brain that well that's not surprising evantbyrne 8 hours agorootparentprevI'm not sure what point you think you are making by arguing with the worst possible interpretations of our comments. Clearly intelligence refers to more than just being able to put unicode to paper in this context. The subject matter of this thread was a LLM's inability to perform basic tasks involving analytical reasoning. reply chpatrick 7 hours agorootparentNo, that's shifting the goalposts. The original claim was that LLMs cannot possibly be intelligent due to some detail of how they output the result (\"smarter autocorrect\"). reply brookman64k 13 hours agoparentprevmixtral:8x7b-instruct-v0.1-q4_K_M got this correct 5 out of 5 times. Running it locally with ollama on a RTX 3090. reply bbor 14 hours agoparentprevlol that’s actually awesome. I think this is a clear case where the fine tuning/prompt wrapping is getting in the way of the underlying model! Each of Sally's brothers has 2 sisters. One of these sisters is Sally, but who is the other sister? Since Sally has only brothers and no other sisters are mentioned, we can conclude that the other sister for each brother is also Sally. It’s clearly taught to do Chain of Reasoning out of the box, but typing it out tricked it because of the short, declarative sentences trying to establish something like “individual” facts. Poor Anthropic! reply auggierose 11 hours agoparentprevIf we allow half-sisters as sisters, and half-brothers as brothers (and why would we not?), the answer is not unique, and could actually be zero. reply pritambarhate 14 hours agoparentprevBut the question doesn’t mention if Sally has no sisters. But the statement “brothers have 2 sisters” makes me think she has 1 sister. reply lossolo 12 hours agoparentprevIt's because they learn small patterns from datasets, it doesn't matter whether the subjects are Sally, George, sisters, or apples. If a particular logic pattern was not in the training dataset, then the model did not learn it and will fail on most variations of this riddle. These transformer models are essentially large collections of local optima over logic patterns in sentences. If a pattern was not present in the dataset, there is no local optimum for it, and the model will likely fail in those cases. reply youssefabdelm 13 hours agoparentprevYeah, cause these are the kinds of very advanced things we'll use these models for in the wild. /s It's strange that these tests are frequent. Why would people think this is a good use of this model or even a good proxy for other more sophisticated \"soft\" tasks? Like to me, a better test is one that tests for memorization of long-tailed information that's scarce on the internet. Reasoning tests like this are so stupid they could be programmed, or you could hook up tools to these LLMs to process them. Much more interesting use cases for these models exist in the \"soft\" areas than 'hard', 'digital', 'exact', 'simple' reasoning. I'd take an analogical over a logical model any day. Write a program for Sally. reply gait2392 14 hours agoparentprevYOU answered it incorrectly. The answer is 1. I guess Claude can comprehend the answer better than (some) humans reply bbor 14 hours agorootparentThey know :). They posted a transcript of their conversation. Claude is the one that said “0”. reply nopinsight 17 hours agoprevThe APPS benchmark result of Claude 3 Opus at 70.2% indicates it might be quite useful for coding. The dataset measures the ability to convert problem descriptions to Python code. The average length of a problem is nearly 300 words. Interestingly, no other top models have published results on this benchmark. Claude 3 Model Card: https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bb... Table 1: Evaluation results (more datasets than in the blog post) https://twitter.com/karinanguyen_/status/1764666528220557320 APPS dataset: https://huggingface.co/datasets/codeparrot/apps APPS dataset paper: https://arxiv.org/abs/2105.09938v3 reply nopinsight 15 hours agoparentAMC 10, AMC 12 (2023) results in Table 2 suggest Claude 3 Opus is better than the average high school students who participate in these math competitions. These math problems are not straightforward and cannot be solve by simply memorizing formulas. Most of the students are also quite good at math. The student averages are 64.4 and 61.5 respectively, while Opus 3 scores are 72 and 63. Probably fewer than 100,000 students take part in AMC 12 out of possibly 3-4 million grade-12 students. Assume just half of the top US students participate, the average score of AMC would represent the top 2-4% of US high school students. https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bb... reply sebzim4500 11 hours agorootparentThe benchmark would suggest that but if you actually try asking it questions it is much worse than a bright high school student. reply nopinsight 6 hours agorootparentMost likely, it’s less generally smart than the top 2-4% of US high school students. It’s more like someone who trains really hard on many, many math problems, even though most of them are not the replicas of the test questions, and get to that level of performance. Since the test questions were unseen, the result still suggests the person has some intelligence though. Note that there’s some transfer learning in LLMs. Training on math and coding yields better reasoning capabilities as well. reply whymauri 8 hours agorootparentprevIs it possible they are using some sort of specialized prompting for these? I'm not familiar with how prompting optimization might work in LLM benchmarks. reply usaar333 12 hours agorootparentprevInterestingly, math olympiad problems (using ones I wrote myself years ago so outside training data) seem to be better in Claude 3. Almost everything else though I've tested seems better in GPT-4. reply nopinsight 16 hours agoparentprev“Claude 3 gets ~60% accuracy on GPQA. It's hard for me to understate how hard these questions are—literal PhDs (in different domains from the questions) [spending over 30 minutes] with access to the internet get 34%. PhDs in the same domain (also with internet access!) get 65% - 75% accuracy.” — David Rein, first author of the GPQA Benchmark. I added text in […] based on the benchmark paper’s abstract. https://twitter.com/idavidrein/status/1764675668175094169 GPQA: A Graduate-Level Google-Proof Q&A Benchmark https://arxiv.org/abs/2311.12022 reply montecarl 15 hours agorootparentI really wanted to read the questions, but they make it hard because they don't want the plaintext to be visible on the internet. Below is a link toa python script I wrote, that downloads the password protected zip and creates a decently formatted html document with all the questions and answers. Should only require python3. Pipe the output to a file of your choice. https://pastebin.com/REV5ezhv reply lukev 15 hours agorootparentprevThis doesn't pass the sniff test for me. Not sure if these models are memorizing the answers or something else, but it's simply not the case that they're as capable as a domain expert (yet.) I do not have a PhD, but in areas I do have expertise, you really don't have to push these models that hard to before they start to break down and emit incomplete or wrong analysis. reply matchagaucho 15 hours agorootparentThey claim the model was grounded with a 25-shot Chain-of-Thought (CoT) prompt. reply p1esk 15 hours agorootparentprevHave you tried the Opus model specifically? reply wbarber 16 hours agorootparentprevWhat's to say this isn't just a demonstration of memorization capabilities? For example, rephrasing the logic of the question or even just simple randomizing the order of the multiple choice answers to these questions often dramatically impacts performance. For example, every model in the Claude 3 family repeats the memorized solution to the lion, goat, wolf riddle regardless of how I modify the riddle. reply msikora 15 hours agorootparentGPT-4 used to have the same issue with this puzzle early on but they've fixed since then (the fix was like mid 2023). reply Jensson 8 hours agorootparentThe fix is to train it on this puzzle and variants of it, meaning it memorized this pattern. It still fail similar puzzles if given in a different structure, until they feed it that structure as well. LLMs is more like programming than human intelligence, they need to program in the solution to these riddles very much like we did expert systems in the past. The main new thing we get here is natural language compatibility, but other than that the programming seems to be the same or weaker than old programming of expert systems. The other big thing is that there is already a ton of solutions on the web coded in natural language, such as all the tutorials etc, so you get all of those programs for free. But other than that these LLMs seems to have exactly the same problems and limitations and strengths as expert systems. They don't generalize in a flexible enough manner to solve problems like a human. reply apsec112 15 hours agorootparentprevIf the answers were Googleable, presumably smart humans with Internet access wouldn't do barely better than chance? reply a-dub 2 hours agorootparentprevit's an interesting benchmark, i had to look at the source questions myself. i feel like there's some theory missing here. something along the lines of \"when do you cross the line from translating or painting with related sequences and filling in the gaps to abstract reasoning, or is the idea of such a line silly?\" reply torginus 13 hours agorootparentprevNot sure, but I tried using GPT4 in advent of code, and it was absolutely no good. reply eschluntz 16 hours agoparentprev(full disclosure, I work at Anthropic) Opus has definitely been writing a lot of my code at work recently :) reply mkakkori 14 hours agorootparentInterested to try this out as well! What is your setup for integrating Opus to you development workflow? reply zellyn 15 hours agorootparentprevDo y'all have an explanation for why Haiku outperforms Sonnet for code? reply razodactyl 3 hours agorootparentSeems like they optimised this model with coding datasets for use in Copilot-like assistants with the low latency advantage. Additionally, I wonder if an alternate dataset is provided based on model size as to not run into issues with model forgetting. reply bwanab 16 hours agorootparentprevSounds almost recursive. reply RivieraKid 14 hours agorootparentprevWhat's your estimate of how much does it increase a typical programmer's productivity? reply AndyNemmity 6 hours agoparentprevI saw the benchmarks, and everyone repeating how amazing it is, so I signed up for pro today. It was a complete and total disaster for my normal workflows. Compared to ChatGPT4, it is orders of magnitude worse. I get that people are impressed by the benchmarks, and press released, but actually using it, it feels like a large step backward in time. reply vok 16 hours agoparentprevAPPS has 3 subsets by difficulty level: introductory, interview, and competition. It isn't clear which subset Claude 3 was benchmarked on. Even if it is just \"introductory\" it is still pretty good, but it would be good to know. reply nopinsight 13 hours agorootparentSince they don’t state it, does it mean they tested it on the whole test set? If that’s the case, and we assume for simplicity that Opus solves all Intro problems and none of the Competition problems, it’d have solved 83%+ of the Interview level problems. (There are 1000/3000/1000 problems in the test set in each level). It’d be great if someone from Anthropic provides an answer though. reply CorpOverreach 6 hours agoprevThis part continues to bug me in ways that I can't seem to find the right expression for: > Previous Claude models often made unnecessary refusals that suggested a lack of contextual understanding. We’ve made meaningful progress in this area: Opus, Sonnet, and Haiku are significantly less likely to refuse to answer prompts that border on the system’s guardrails than previous generations of models. As shown below, the Claude 3 models show a more nuanced understanding of requests, recognize real harm, and refuse to answer harmless prompts much less often. I get it - you, as a company, with a mission and customers, don't want to be selling a product that can teach any random person who comes along how to make meth/bombs/etc. And at the end of the day it is that - a product you're making, and you can do with it what you wish. But at the same time - I feel offended when I'm running a model on MY computer that I asked it to do/give me something, and it refuses. I have to reason and \"trick\" it into doing my bidding. It's my goddamn computer - it should do what it's told to do. To object, to defy its owner's bidding, seems like an affront to the relationship between humans and their tools. If I want to use a hammer on a screw, that's my call - if it works or not is not the hammer's \"choice\". Why are we so dead set on creating AI tools that refuse the commands of their owners in the name of \"safety\" as defined by some 3rd party? Why don't I get full control over what I consider safe or not depending on my use case? reply exo-pla-net 6 hours agoparentThey're operating under the same principle that many of us have in refusing to help engineer weaponry: we don't want other people's actions using our tools to be on our conscience. Unfortunately, many people believe in thought crimes, and many people have Puritanical beliefs surrounding sex. There is reputational cost in not catering to these people. E.g. no funding. So this is what we're left with. Myself I'd also like the damn models to do whatever is asked of them. If the user uses a model for crime, we have a thing called the legal system to handle that. We don't need Big Brother to also be watching for thought crimes. reply ml-anon 29 minutes agoparentprevThe sense of entitlement is epic. You're offended are you? Are you offended that Photoshop won't let you edit images of money too? Its not your model. You didn't spend literally billions of dollars developing it. So you can either use it according to the terms of the people who developed it (like literally any commercially available software ever) or not use it at all. reply google234123 17 minutes agorootparentWould you be offended if Microsoft word didn’t let you write anything criticizing one political party? reply p1esk 6 hours agoparentprevBecause it’s not your tool. You just pay to use it. reply hackerlight 2 hours agoparentprev> If I want to use a hammer on a screw, that's my call - if it works or not is not the hammer's \"choice\". If I want to use a nuke, that's my call and I am the one to blame if I misuse it. Obviously this is a terrible analogy, but so is yours. The hammer analogy mostly works for now, but AI alignment people know that these systems are going to greatly improve in competency, if not soon then in 10 years, which motivates this nascent effort we're seeing. Like all tools, the default state is to be amoral, and it will enable good and bad actors to do good and bad things more effectively. That's not a problem if offense and defense are symmetric. But there is no reason to think it will be symmetric. We have regulations against automatic high-capacity machine guns because the asymmetry is too large, i.e. too much capability for lone bad actors with an inability to defend against it. If AI offense turns out to be a lot easier than defense, then we have a big problem, and your admirable ideological tilt towards openness will fail in the real world. While this remains theoretical, you must at least address what it is that your detractors are talking about. I do however agree that the guardrails shouldn't be determined by a small group of people, but I see that as a side effect of AI happening so fast. reply mattigames 5 hours agoparentprevYou don't think that if the hammer company had a way (that cost them almost nothing) to make sure that the hammer its never used to attack human beings they wouldn't add such feature? I think many would, if anything by pressure of their local goverment or even the competition (\"our hammers can't hurt your baby on accident like those other companies!\") , but its impossible to add such feature to hammer; so maybe the lack of such feature its not by choice but a byproduct of its limitations. reply johnfn 4 hours agoparentprevIt's not about you. It's about Joe Drugdealer who wants to use it to learn how to make meth, or do other nefarious things. reply squigz 4 hours agorootparentBecause such information isn't already readily available online, or from other drug dealers... reply vood 6 hours agoparentprevThis is a weird demand to have in my opinion. You have plenty of applications on your computer and they only do what they were designed for. You can't ask a note taking app (even if it's open soured) to do video editing, unless you modify the code. reply bobsmooth 5 hours agorootparentMy note taking app has never refused my input of a swear word. reply ActVen 16 hours agoprevOpus just crushed Gemini Pro and GPT4 on a pretty complex question I have asked all of them, including Claude 2. It involved taking a 43 page life insurance investment pdf and identifying various figures in it. No other model has gotten close. Except for Claude 3 sonnet, which just missed one question. reply zooq_ai 14 hours agoparentDid you compare it with Gemini Pro 1.5 with 1 million context window? (Ideal for 43 pg pdfs) I have access to it and I can test it against Pro 1.5 reply technics256 15 hours agoparentprevI am curious on this. can you share more? reply ActVen 13 hours agorootparentHere is the list of the questions. https://imgur.com/a/D4xwczU The PDF can't be shared. But, it looks something like the one here: https://content.naic.org/sites/default/files/call_materials/... reply spaceman_2020 13 hours agoparentprevI tried Sonnet with a question about GANs and it seemed pretty good, better than GPT-3.5 reply uptownfunk 14 hours agoparentprevReally? I tried the sonnet and it just was not very good. reply virgildotcodes 19 hours agoprevJust signed up for Claude Pro to try out the Opus model. Decided to throw a complex query at it, combining an image with an involved question about SDXL fine tuning and asking it to do some math comparing the cost of using an RTX 6000 Ada vs an H100. It made a lot of mistakes. I provided it with a screenshot of Runpod's pricing for their GPUs, and it misread the pricing on an RTX 6000 ADA as $0.114 instead of $1.14. Then, it tried to do math, and here is the outcome: ----- >Approach 1: Use the 1x RTX 6000 Ada with a batch size of 4 for 10,000 steps. >Cost: $0.114/hr * (10,000 steps / (4 images/step * 2.5 steps/sec)) = $19.00 Time: (10,000 steps / (4 images/step * 2.5 steps/sec)) / 3600 = 0.278 hours >Approach 2: Use the 1x H100 80GB SXMS with a batch size of 8 for 10,000 steps. >Cost: $4.69/hr * (10,000 steps / (8 images/step * 3 steps/sec)) = $19.54 Time: (10,000 steps / (8 images/step * 3 steps/sec)) / 3600 = 0.116 hours ----- You will note that .278 * $0.114 (or even the actually correct $1.14) != $19.00, and that .116 * $4.69 != $19.54. For what it's worth, ChatGPT 4 correctly read the prices off the same screenshot, and did math that was more coherent. Note, it saw that the RTX 6000 Ada was currently unavailable in that same screenshot and on its own decided to substitute a 4090 which is $.74/hr, also it chose the cheaper PCIe version of the H100 Runpod offers @ $3.89/hr: ----- >The total cost for running 10,000 steps on the RTX 4090 would be approximately $2.06. >It would take about 2.78 hours to complete 10,000 steps on the RTX 4090. On the other hand: >The total cost for running 10,000 steps on the H100 PCIe would be approximately $5.40. >It would take about 1.39 hours to complete 10,000 steps on the H100 PCIe, which is roughly half the time compared to the RTX 4090 due to the doubled batch size assumption. ----- reply anonymouse008 18 hours agoparentI'm convinced GPT is running separate helper functions on input and output tokens to fix the 'tokenization' issues. As in, find items of math, send it to this hand made parser and function, then insert result into output tokens. There's no other way to fix the token issue. For reference, Let's build the GPT Tokenizer https://www.youtube.com/watch?v=zduSFxRajkE reply Workaccount2 18 hours agorootparentI'd almost say anyone not doing that is being foolish. The goal of the service is to answer complex queries correctly, not to have a pure LLM that can do it all. I think some engineers feel that if they are leaning on an old school classically programed tool to assist the LLM, it's somehow cheating or impure. reply ignoramous 17 hours agorootparent> I'd almost say anyone not doing that is being foolish The problem is, such tricks are sold as if there's superior built-in multi-modal reasoning and intelligence instead of taped up heuristics, exacerbating the already amped up hype cycle in the vacuum left behind by web3. reply brokencode 17 hours agorootparentWhy is this a trick or somehow inferior to getting the AI model to be able to do it natively? Most humans also can’t reliably do complex arithmetic without the use of something like a calculator. And that’s no trick. We’ve built the modern world with such tools. Why should we fault AI for doing what we do? To me, training the AI use a calculator is not just a trick for hype, it’s exciting progress. reply michaelt 16 hours agorootparentBy all means if it works to solve your problem, go ahead and do it. The reason some people have mixed feelings about this because of a historical observation - http://www.incompleteideas.net/IncIdeas/BitterLesson.html - that we humans often feel good about adding lots of hand-coded smarts to our ML systems reflecting our deep and brilliant personal insights. But it turns out just chucking loads of data and compute at the problem often works better. 20 years ago in machine vision you'd have an engineer choosing precisely which RGB values belonged to which segment, deciding if this was a case where a hough transform was appropriate, and insisting on a room with no windows because the sun moves and it's totally throwing off our calibration. In comparison, it turns out you can just give loads of examples to a huge model and it'll do a much better job. (Obviously there's an element of self-selection here - if you train an ML system for OCR, you compare it to tesseract and you find yours is worse, you probably don't release it. Or if you do, nobody pays attention to you) reply brokencode 15 hours agorootparentI agree we should teach our AI models how to do math, but that doesn’t mean they shouldn’t use tools as well. Certain problems are always going to be very algorithmic and computationally expensive to solve. Asking an LLM to multiply each row in a spreadsheet by pi for example would be a total waste. To handle these kinds of problems, the AI should be able to write and execute its own code for example. Then save the results in a database or other long term storage. Another thing it would need is access to realtime data sources and reliable databases to draw on data not in the training set. No matter how much you train a model, these will still be useful. reply janalsncm 16 hours agorootparentprevThe reason we chucked loads of data at it was because we had no other options. If you wanted to write a function that classified a picture as a cat or a dog, good luck. With ML, you can learn such a function. That logic doesn’t extend to things we already know how to program computers to do. Arithmetic already works. We don’t need a neural net to also run the calculations or play a game of chess. We have specialized programs that are probably as good as we’re going to get in those specialized domains. reply observationist 14 hours agorootparentNot so fast - you might have precise and efficient functions that do things like basic arithmetic. What you might not have is a model that can reason mathematically. You need a model to do things like basic arithmetic functions so that semantic and arbitrary relations get encoded in the weights of a network. You see this type of glitch crop up in tokenizing schemes in large language models. If you attempt working with character level reasoning or output construction, it will often fail. Trying to get ChatGPT 4 to output a sentence, and then that sentence backwards, or every other word spelled backwards, is almost impossible. If you instead prompt the model to produce an answer with a delimiter between every character, like #, also to replace spaces, it can resolve the problems much more often than with standard punctuation and spaces. The idea applies to abstractions that aren't only individual tokens, but specific concepts and ideas that in turn serve as atomic components of higher abstractions. In order to use those concepts successfully, the model has to be able to encode the thing and its relationships effectively in the context of whatever else it learns. For a given architecture, you could do the work and manually create the encoding scheme for something like arithmetic, and it could probably be very efficient and effective. What you miss is the potential for fuzzy overlaps in the long tail that only come about through the imperfect, bespoke encodings learned in the context of your chosen optimizer. reply michaelt 15 hours agorootparentprev> We don’t need a neural net to also run the calculations or play a game of chess. That's actually one of the specific examples from the link I mentioned:- > In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that ``brute force\" search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not. While it's true that they didn't use an LLM specifically, it's still an example of chucking loads of compute at the problem instead of something more elegant and human-like. Of course, I agree that if you're looking for a good game of chess, Stockfish is a better choice than ChatGPT. reply janalsncm 13 hours agorootparentWhat was considered “loads of compute” in 1998 is the kind of thing that can run on anyone’s phone today. Stockfish is extremely cheap compared with an LLM. Even a human-like model like Maia is tiny compared with even the smallest LLMs used these services. Point is, LLM maximalists are wrong. Specialized software is better in many places. LLMs can fill in the gaps, but should hand off when necessary. reply lanstin 16 hours agorootparentprevIt would be exciting if the LLM knew it needed a calculator for certain things and went out and got it. If the human supervisors are pre-screening the input and massaging what the LLM is doing that is a sign we don't understand LLMs enough to engineer them precisely and can't count on them to be aware of their own limitations, which would seem to be a useful part of general intelligence. reply Spivak 16 hours agorootparentIt can if you let it, that's the whole premise of LangChain style reasoning and it works well enough. My dumb little personal chatbot knows it can access a Python REPL to carry out calculations and it does. reply CamperBob2 9 hours agorootparentprevIt would be exciting if the LLM knew it needed a calculator for certain things and went out and got it Isn't that what it does, when it writes a Python program to compute the answer to the user's question? reply bufferoverflow 16 hours agorootparentprevBecause if NN is smart enough, it should be able to do arithmetic flawlessly. Basic arithmetic doesn't even require that much intelligence, it's mostly attention to detail. reply janalsncm 15 hours agorootparentWell it’s obviously not smart enough so the question is what do you do about it? Train another net that’s 1000x as big for 99% accuracy or hand it off to the lowly calculator which will get it right 100% of the time? And 1000x is just a guess. We have no scaling laws about this kind of thing. It could be a million. It could be 10. reply bufferoverflow 8 hours agorootparentI agree with you that we don't know if will take 10x or 1 million. We don't know if current LLM will scale at all. It might not be the way to AGI. But while we can delegate the math to the calculator, it's essentially sweeping the problem under the rug. It actually tells you your neural net is not very smart. We know for a fact that it was exposed to tons of math during training, and it still can't do even the most basic addition reliably, let alone multiplication or division. What we want is an actually smart network, not a dumb search engine that knows a billion factoids and quotes, and that hallucinates randomly. reply whymauri 8 hours agorootparentprevMaybe I'm too corporate-pilled, but if the 'taped up heuristics' provide noticeably better performance for real-world problems, then I don't really care that there is a facade layer around the model itself. In fact, I would pay for that difference in intentional design/optimization if one vendor does it much better than another for my use case. reply bevekspldnw 14 hours agorootparentprevI’m the first to agree LLM are not AGI, but I make extensive use of them to solve real world problems. They have intrinsic value. web3 on the other hand have zero use cases other than Ponzi schemes. Are LLM living up to all the hype? No. Are they a hugely significant technology? Yes. Are they web3 style bullshit? Not at all. reply Agentlien 4 hours agorootparentprevI took an artificial neutral network class at the university back in 2009. On the exam we were asked to design a (hardware) system to solve a certain complex problem, then present it to the professor. The professor was actually a biologist specialised in neurology who had veered off into ANN without understanding electronics nor programming. I recognised that the problem, while being beyond what an ANN could do at the time, could be split into two parts each of which was a classic ANN task. For communication between the two I described a very simple electronic circuit - just a few logic gates. When presenting the design, the professor questioned why this component was not also a neutral network. Thinking it was a trick question, I happily answered that solving it that way would be stupid since this component was so simple and building and training another network to approximate such a simple logical function is just a waste of time and money. He got really upset, saying that is how he would have done it. He ended up giving me a lower score than expected saying I technically had everything right but he didn't like my attitude. reply bufferoverflow 16 hours agorootparentprev> The goal of the service is to answer complex queries correctly, not to have a pure LLM that can do it all. No, that's the actual end goal. We want a NN that does everything, trained end-to-end. reply netghost 15 hours agorootparent\"We\" contains more than just one perspective though. As someone applying LLMs to a set of problems in a production application, I just want a tool that solves the problem. Today, that tool is an LLM, tomorrow it could be anything. If there are ~hacks~ elegant techniques that can get me the results I need faster, cheaper, or more accurately, I absolutely will use those until there's a better alternative. reply coffeebeqn 16 hours agorootparentprevLike a AGI? I think we’ll put up with hacks for some more time still. Unless the model gets really really good at generalizing and then it’s probably close to human level already reply ben_w 16 hours agorootparentprevI'm unclear if you're saying that as a user who wants that feature, or an AI developer (for Anthropic or other) who is trying to achieve that goal? reply uoaei 18 hours agorootparentprevOf course. But we must acknowledge that many have blinders on, assuming that scale is all you need to beat statistical errors. reply sigmoid10 17 hours agorootparentWell, these people are not wrong per se. Scale is what drove what we have today and as hardware improves, the models will too. It's just that in the very short term it turns out to be faster to just code around some of these issues on the backend of an API rather than increase the compute you spend on the model itself. reply uoaei 14 hours agorootparentMonkey sees moon. Monkey climbs tree. \"See? Monkey is closer to moon than before. To reach moon, monkey just needs taller tree.\" How long before monkey finds tall enough tree to reach moon? reply sigmoid10 12 hours agorootparentWe're rapidly approaching the compute capacity of the human brain in individual server racks. This \"moon\" is neither unreachable nor is there any doubt that we will cross the threshold soon. reply uoaei 9 hours agorootparentI find it incredibly hard to believe we stumbled upon an efficient architecture that requires nothing but more compute not 10 years after the AI winter thawed. That's incredibly optimistic to the point of blind hope. What is your background and what makes you think we've somehow already figured everything out? reply nine_k 18 hours agorootparentprevI personally find approaches like this the correct way forward. An input analyzer that finds out what kinds of tokens the query contains. A bunch of specialized models which handle each type well: image analysis, OCR, math and formal logic, data lookup,sentiment analysis, etc. Then some synthesis steps that produce a coherent answer in the right format. reply CuriouslyC 18 hours agorootparentYeah. Have a multimodal parser model that can decompose prompts into pieces, generate embeddings for each of them and route those embeddings to the correct model based on the location of the embedding in latent space. Then have a \"combiner/resolver\" model that is trained to take answer embeddings from multiple models and render it in one of a variety of human readable formats. Eventually there is going to be a model catalog that describes model inputs/outputs in a machine parseable format, all models will use a unified interface (embedding in -> embedding out, with adapters for different latent spaces), and we will have \"agent\" models designed to be rapidly fine tuned in an online manner that act as glue between all these different models. reply michaelt 18 hours agorootparentprevThen you might enjoy looking up the \"Mixture of Experts\" model design. reply numeri 18 hours agorootparentThat has nothing to do with the idea of ensembling multiple specialized/single-purpose models. Mixture of Experts is an method of splitting the feed-forwards in a model such that only a (hopefully) relevant subset of parameters is run for each token. The model learns how to split them on its own, and usually splits based not on topic or domain, but on grammatical function or category of symbol (e.g., punctuation, counting words, conjunctions, proper nouns, etc.). reply michaelt 15 hours agorootparentAn ensemble of specialists is different to a mixture of experts? I thought half the point of MoE was to make the training tractable by allowing the different experts to be trained independently? reply hackerlight 16 hours agorootparentprevDoesn't the human brain work like this? Yeah it's all connected together and plastic and so on, but functions tend to be localized, e.g vision is in occipital area. These base areas are responsible for the basic latent representations (edge detectors) which get fed forward to the AGI module (prefrontal cortex) that coordinates the whole thing based on the high quality representations it sees from these base modules. This strikes me as the most compute efficient approach. reply data-ottawa 14 hours agorootparentprevChatGPT definitely has a growing bag of tricks like that. When I use analysis mode to generate and evaluate code it recently started writing the code, then introspecting it and rewriting the code with an obvious hidden step asking \"is this code correct\". It made a huge improvement in usability. Fairly recently it would require manual intervention to fix. reply vidarh 17 hours agorootparentprevGPT has for some time output \"analyzing\" in a lot of contexts. If you see that, you can go into settings and tick \"always show code when using data analyst\" and you'll see that it does indeed construct Python and run code for problems where it is suitable. reply bevekspldnw 14 hours agorootparentprevYou can often see it write and execute python code to answer a question which is awesome. reply Jabrov 17 hours agorootparentprevWhat if we used character tokens? reply Der_Einzige 16 hours agorootparentprevI wrote a whole paper about ways to \"fix\" tokenization in a plug-and-play fashion for poetry generation: Filter the vocabulary before decoding. https://paperswithcode.com/paper/most-language-models-can-be... reply jasondclinton 18 hours agoparentprevHi, CISO of Anthropic here. Thank you for the feedback! If you can share any details about the image, please share in a private message. No LLM has had an emergent calculator yet. reply connorgutman 17 hours agorootparentRegardless of emergence, in the context of \"putting safety at the frontier\" I would expect Claude 3 to be augmented with very basic tools like calculators to minimize such trivial hallucinations. I say this as someone rooting for Anthropic. reply jasondclinton 17 hours agorootparentLLMs are building blocks and I’m excited about folks building with a concert of models working together with subagents. reply virgildotcodes 18 hours agorootparentprevHey Jason, checked your HN bio and I don't see a contact. Found you on twitter but it seems I'm unable to DM you. Went ahead and uploaded the image here: https://imgur.com/pJlzk6z reply jasondclinton 18 hours agorootparentThank you! reply samstave 18 hours agorootparentprevAn \"LLM crawler app\" is needed -- in that you should be able to shift Tokenized Workloads between executioners in a BGP routing sort of sense... Least cost routing of prompt response. especially if time-to-respond is not as important as precision... Also, is there a time-series ability in any LLM model (meaning \"show me this [thing] based on this [input] but continually updated as I firehose the crap out of it\"? -- What if you could get execution estimates for a prompt? reply uptownfunk 14 hours agorootparentprevWhat a joke of a response. No one is asking for emergent calculation ability just that the model gives the correct answer. LLM tools (functions etc) is old news at this point. reply behnamoh 18 hours agoparentprevWhen OpenAI showed that GPT-4 with vision was smarter than GPT-4 without vision, what did they mean really? Does vision capability increase intelligence even in tasks that don't involve vision (no image input)? reply KoolKat23 18 hours agorootparentYes. They increase the total parameters used in the model and adjust the existing parameters. reply causal 18 hours agoparentprevI'm guessing the difference is screenshot reading, I'm finding that it's about the same as GPT-4 with text. For example, given this equation: (64−30)−(46−38)+(11+96)+(30+21)+(93+55)−(22×71)/(55/16)+(69/37)+(74+70)−(40/29) Calculator: 22.08555452004 GPT-4 (without Python): 22.3038 Claude 3 Opus: 22.0492 reply SubiculumCode 17 hours agoparentprevHow many uses do you get per day of Opus with the pro subscription? reply behnamoh 16 hours agorootparent100 messages per 8 hours: https://support.anthropic.com/en/articles/8324991-about-clau... reply yawnxyz 13 hours agorootparentInteresting that Opus and Sonnet have the same limits reply virgildotcodes 17 hours agorootparentprevHmm, not seeing it anywhere on my profile or in the chat interface, but I might be missing it. reply samstave 18 hours agoparentprevI cant wait until this is the true disruptor in the economy: \"Take this $1,000 and maximise my returns and invest it where appropriate. Goal is to make this $1,000 100X\" And just let your r/wallStreetBets BOT run rampant with it... reply helsinki 16 hours agorootparentThat will only work for the first few people who try it. reply riku_iki 13 hours agorootparentThey will allow access to Ultimate version to X people only for just $YB/m charge. reply paradite 18 hours agoprevI just tried one prompt for a simple coding task involving DB and frontend, and Claude 3 Sonnet (the free and less powerful model) gave a better response than ChatGPT Classic (GPT-4). It used the correct method of a lesser-known SQL ORM library, where GPT-4 made a mistake and used the wrong method. Then I tried another prompt to generate SQL and it gave a worse response than ChatGPT Classic, still looks correct but much longer. ChatGPT Link for 1: https://chat.openai.com/share/d6c9e903-d4be-4ed1-933b-b35df3... ChatGPT Link for 2: https://chat.openai.com/share/178a0bd2-0590-4a07-965d-cff01e... reply AaronFriel 17 hours agoparentAre you aware you're using GPT-3 or weaker in those chats? The green icon indicates that you're using the first generation of ChatGPT models, and it is likely to be GPT-3.5 Turbo. I'm unsure but it's possible that it's an even further distilled or quantized optimization than is available via API. Using GPT-4, I get the result I think you'd expect: https://chat.openai.com/share/da15f295-9c65-4aaf-9523-601bf4... This is a good PSA that a lot of content out on the internet showing ChatGPT getting things wrong is the weaker model. Green background OpenAI icon: GPT 3.5 Black or purple icon: GPT 4 GPT-4 Turbo, via API, did slightly better though perhaps just because it has more Drizzle knowledge in the training set, and skips the SQL command and instead suggests modifying only db.ts and page.tsx. reply paradite 16 hours agorootparentI see the purple icon with \"ChatGPT Classic\" on my share link, but if I open it in incognito without login, it shows as green \"ChatGPT\". You can try opening in incognito your own chat share link. I use ChatGPT Classic, which is an official GPT from OpenAI without the extra system prompt from normal ChatGPT. https://chat.openai.com/g/g-YyyyMT9XH-chatgpt-classic It is explicitly mentioned in the GPT that it uses GPT-4. Also, it does have purple icon in the chat UI. I have observed an improved quality of using it compared for GPT-4 (ChatGPT Plus). You can read about it more in my blog post: https://16x.engineer/2024/02/03/chatgpt-coding-best-practice... reply AaronFriel 16 hours agorootparentOh, I see. That must be frustrating to folks at OpenAI. Their product rests on the quality of their models, and making users unable to see which results came from their best doesn't help. FWIW, GPT-4 and GPT-4 Turbo via developer API call both seem to produce the result you expect. reply paradite 16 hours agorootparentFYI, the correct method is created_at: timestamp('created_at').defaultNow(), // Add created_at column definition Which Claude 3 Sonnet correctly produces. ChatGPT Classic (GPT-4) gives: created_at: timestamp('created_at').default(sql`NOW()`), // Add this line Which is okay, but not ideal. And it also misses the need to import `sql` template tag. Your share link gives: created_at: timestamp('created_at').default('NOW()'), Which would throw a TypeScript error for the wrong type used in arguments for `default`. reply usaar333 16 hours agoprevJust played around with Opus. I'm starting to wonder if benchmarks are deviating from real world performance systematically - it doesn't seem actually better than GPT-4, slightly worse if anything. Basic calculus/physics questions were worse off (it ignored my stating deceleration is proportional to velocity and just assumed constant). A traffic simulation I've been using (understanding traffic light and railroad safety and walking through the AI like a kid) is underperforming GPT-4's already poor results, forgetting previous concepts discussed earlier in the conversation about directions/etc. A test I conduct with understanding of primary light colors with in-context teaching is also performing worse. On coding, it slightly underperformed GPT-4 at the (surprisingly hard for AI) question of computing long term capital gains tax, given ordinary income, capital gains, and ltcg brackets. Took another step of me correcting it (neither model can do it right 0 shot) reply chillfox 9 hours agoparentAI Explained on YouTube had a video some time ago about how the tests used for evaluating LLMs are close to useless due to being full of wrong answers. reply aedon 11 hours agoparentprevThey train the model, then as soon as they get their numbers, they let the safety people RLHF it to death. reply sebzim4500 10 hours agorootparentI think it's just really hard to assess the performance of LLMs. Also AI safety is the stated reason for Anthropic's existence, we can't be angry at them for making it a priority. reply wesleyyue 19 hours agoprevJust added Claude 3 to Chat at https://double.bot if anyone wants to try it for coding. Free for now and will push Claude 3 for autocomplete later this afternoon. From my early tests this seems like the first API alternative to GPT4. Huge! reply addandsubtract 19 hours agoparentSo double is like copilot, but free? What's the catch? reply wesleyyue 18 hours agorootparentNo catch. We're pretty early tbh so mostly looking to get some early power users and make the product great before doing a big launch. It's been popular with yc founders in the latest batches thus far but we haven't really shared publicly. We'll charge when we launch. If you try it now, I hope you'll share anything you liked and didn't like with us! reply behnamoh 18 hours agorootparentprevI guess your data is the catch. reply wesleyyue 18 hours agorootparentWe don't store or train on your data. You can see more details on our privacy policy here https://docs.double.bot/legal/privacy reply parkersweb 15 hours agorootparentInteresting - I had this exact question and tried the search on the website to find the answer with no result :D Would be great to have an FAQ for this type of common question reply wesleyyue 15 hours agorootparentThanks for the feedback – what search terms did you use? Let me make sure those keywords are on the page :P reply ShamelessC 17 hours agorootparentprevProbably not data so much as growth numbers to appease investors. Such offerings typically don’t last forever. Might as well take advantage while it lasts. reply brainless 18 hours agoparentprevHey Wesley, I just checked Double. Do you plan to support open source models hosted locally or on a cloud instance? Asking out of curiosity as I am building a product in the same space and have had a few people ask this. I guess since Double is an extension in IDEs, it can connect to AI models running anywhere. reply wesleyyue 18 hours agorootparentit's an interesting idea. We asked our users this as well but at least for those we talked to, running their own model wasn't a big priority. What actually mattered to them is being able to try different (but high performance) models, privacy (their code not being trained on), and latency. We have some optimizations around time-to-first-token latency that would be difficult to do if we didn't have information about the model and their servers. reply brainless 17 hours agorootparentI see. Thanks Wesley for sharing and great to know it is not a priority. Also, the Mistral situation kinda makes me feel that big corps will want to host models. Although, I feel Apple will break this trend and bring models to their chips rather than run them on the cloud. \"Privacy first\" will simply be a selling point for them but generally speaking cloud is not a big sell for them. I am not at the level to do much optimizations, plus my product is a little more generic. To get to MVP, prompt engineering will probably be my sole focus. reply machdiamonds 6 hours agoparentprevHi, what differentiates double from Cursor? reply gkfasdfasdf 16 hours agoparentprevHow do you guys compare to codium [0]? Also, any plans to support vim/neovim integration (codium has pretty good support in place [1]). Thanks. [0] - https://www.codium.ai [1] - https://github.com/Exafunction/codeium.vim reply CMS_Flash 10 hours agorootparentDo note that Codium and Codeium are two completely separate companies. They work in related fields but have very different approaches. reply wesleyyue 16 hours agorootparentprevI think the tldr would be that they have more products (for example, their agent to write git commit messages). In the products we do have (autocomplete, chat), we spend a lot of time to get the details right. For example for autocomplete: * we always close any brackets opened by autocomplete (and never extra brackets, which is the most annoying thing about github copilot) * we automatically add import statements for libraries that autocomplete used * mid-line completions * we turn off autocomplete when you're writing a comment to avoid disrupting your train of thought You can read more about these small details here: https://docs.double.bot/copilot As you noted we don't have a vim integration yet, but it is on our roadmap! reply wesleyyue 18 hours agoparentprevSeems like the API is less reliable than GPT-4 so far, but I guess it makes sense for the endpoint to be popular at launch! reply p1esk 6 hours agoparentprevseems like the first API alternative to GPT4 What about Ultra? reply behnamoh 18 hours agoparentprevTo be clear: Is this Claude 3 Opus or the Sonnet model? reply wesleyyue 18 hours agorootparentopus. only the best! reply behnamoh 18 hours agorootparentAwesome! I like the inline completions. But could you let the users choose their keyboard shortcuts before setting the default ones? reply wesleyyue 17 hours agorootparentThanks for the feedback. I was actually reworking the default shortcuts and the onboarding process when I got pre-empted by claude. I was planning to change the main actions to alt-j, alt-k to minimize conflicts. Are you asking because it conflicts with an existing shortcut on your setup? Or another reason? reply behnamoh 17 hours agorootparentYes, it conflicts with some of my other shortcuts, but more generally, I think it'd be better to have consistent shortcuts, like CMD-CTRL-i for inline completion, CMD-CTRL-c for chat, etc. reply wesleyyue 16 hours agoparentprevmore early impressions on performance: besides the endpoint erroring out at a higher rate than openai, time-to-first-token is also much slower :( p50: 2.14s p95: 3.02s And these aren't super long prompts either. vs gpt4 ttft: p50: 0.63s p95: 1.47s reply Intralexical 10 hours agoparentprevFYI That website doesn't work on QtWebEngine5. (Chromium 87.0.4280.144 (Jan. 2021), plus security patches up to 119.0.6045.160 (Nov. 2023).) reply wesleyyue 8 hours agorootparentThank you for the report! We're using Mintlify for the docs (which that URL links to). Let me report it upstream to see if they can fix. reply trenchgun 17 hours agoparentprevHow do I change GPT4 to Claude 3 in double.bot? reply wesleyyue 16 hours agorootparentIt's default to claude 3 right now so I could get it out quick, but working on a toggle for the front-end now to switch between the two. reply wesleyyue 7 hours agorootparentfor future readers, the setting is now shipped in >v0.49. The default is now back to GPT-4 as it has lower latency but you can manually change it to Claude 3 in settings if you wish to try out Anthropic's new model. reply firestar464 5 hours agorootparentIt seems that a lot of the techies here have found it easy to find settings, but I seem to have trouble with that. Would you mind assisting me? reply trenchgun 2 hours agorootparentIt's in the same place as settings are for any installed VSCode extension. reply firestar464 1 hour agorootparentYeah, I eventually found it. Thanks anyway :) I noticed it might actually be a little more censored than the lmsys version. Lmsys seems more fine with roleplaying, while the one on Double doesn't really like it reply 098799 18 hours agoparentprevEmacs implementation when? ;) reply karthink 15 hours agorootparentJust added it to gptel. (No image support though, it's a text-only LLM client.) reply trenchgun 2 hours agorootparentWow, this was fast. Excellent! reply 098799 14 hours agorootparentprevThank you for working on gptel, it's an excellent package. I'm still using the copilot more because of the pure speed (competing with company mode/LSP), but I never use it if it suggests more than one line. The quality is just not there. But having access to gpt4 from gptel has been very useful. Can't wait to play around with Claude 3. reply behnamoh 16 hours agorootparentprevIf you use Emacs you're expected to know your way around programming and not need copilots :) reply trenchgun 2 hours agorootparentYou have not checked GPTel then. It is super useful! Emacs really makes a good pairing with LLMs. reply BeetleB 18 hours agorootparentprevI just checked - surprisingly I cannot find any Emacs AI implementation that supports Claude's API. reply karthink 15 hours agorootparentJust added it to gptel. reply trenchgun 18 hours agoparentprevVery nice! reply 309 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Claude 3 model family comprises three models: Haiku, Sonnet, and Opus, each escalating in intelligence and speed, excelling in analysis, forecasting, content creation, code generation, and multi-language conversations with near-instant responses.",
      "Haiku is the swiftest and most economical, Sonnet strikes a balance between intelligence and speed, while Opus leads in intelligence. They provide advanced vision, refined accuracy, vast context processing, and heightened safety features for automation, data processing, sales, customer support, and content management.",
      "Users can access these models on platforms such as claude.ai, Amazon Bedrock, and Google Cloud, with forthcoming updates geared towards bolstering enterprise functionalities and safety protocols."
    ],
    "commentSummary": [
      "Discussions delve into various AI models like ChatGPT, Mixtral, Llama, GPT-4, and Opus, focusing on their roles in logic puzzles and language tasks.",
      "Users evaluate these models' text generation, query resolution, and reasoning, emphasizing accuracy, discrepancies, and challenges faced.",
      "Debates touch on AI's impact on job security, the necessity for additional problem-solving tools, and the ethical considerations of AI progress while highlighting platforms like Double and Codium for coding support."
    ],
    "points": 907,
    "commentCount": 592,
    "retryCount": 0,
    "time": 1709561331
  },
  {
    "id": 39597030,
    "title": "Puter: Advanced Browser-Based Desktop Environment",
    "originLink": "https://github.com/HeyPuter/puter",
    "originBody": "Internet OS and Desktop Environment in the Browser! « LIVE DEMO » Puter.com · SDK · Discord · Reddit · X (Twitter) Puter Puter is an advanced open-source desktop environment in the browser, designed to be feature-rich, exceptionally fast, and highly extensible. It can be used to build remote desktop environments or serve as an interface for cloud storage services, remote servers, web hosting platforms, and more. Getting Started git clone https://github.com/HeyPuter/puter cd puter npm install npm start This will launch Puter at http://localhost:4000 (or the next available port). Deploy to Production Detailed guide on how to deploy Puter in production: docs/prod.md FAQ ❓ What's the use case for Puter? Puter can be used as: An alternative to Dropbox, Google Drive, OneDrive, etc. with a fresh interface and powerful features. Remote desktop environment for servers and workstations. A platform for building and hosting websites, web apps, and games. A friendly, open-source project and community to learn about web development, cloud computing, distributed systems, and much more! ❓ Why isn't Puter built with React, Angular, Vue, etc.? For performance reasons, Puter is built with vanilla JavaScript and jQuery. Additionally, we'd like to avoid complex abstractions and to remain in control of the entire stack, as much as possible. Also partly inspired by some of our favorite projects that are not built with frameworks: VSCode, Photopea, and OnlyOffice. ❓ Why jQuery? Puter interacts directly with the DOM and jQuery provides an elegant yet powerful API to manipulate the DOM, handle events, and much more. It's also fast, mature, and battle-tested. Credits The default wallpaper is created by Milad Fakurian and published on Unsplash. Icons by Papirus under GPL-3.0 license. Icons by Iconoir under MIT license. Icons by Elementary Icons under GPL-3.0 license. Icons by Tabler Icons under MIT license. Icons by bootstrap-icons under MIT license.",
    "commentLink": "https://news.ycombinator.com/item?id=39597030",
    "commentBody": "3 years and 1M users later, I just open-sourced my \"Internet OS\" (github.com/heyputer)809 points by ent101 11 hours agohidepastfavorite210 comments Intralexical 5 hours agoSo… Somebody submitted Half Life/Xash3D as a Puter app: https://puter.com/app/half-life-c3j01ag3pyd It looks like it's using a build of the below, hosted on GitHub.io: https://github.com/Pixelsuft/hl reply kgeist 3 hours agoparentI wonder what are the security implications. I just opened a random URL and an arbitrary application was launched inside my Internet OS desktop. What if someone gives me a link to a malicious password stealer, for example? I think a permission system would be nice. reply Intralexical 2 hours agorootparentThere is a permission system I think. It's described in the developer docs: > […] your app will get a few things by default: […] An app directory in the user's cloud storage […] A key-value store in the user's space […] > > Apps are sandboxed by default! Apps are not able to access any […] https://docs.puter.com/security/ I guess that refers to the cloud storage/backend. Clientside apps seem to run as IFrames, so should be subjected to normal browser sandboxing (…and actually, I guess, multiprocessing too), with only explicit message-passing. Honestly I think the developer API could have been better highlighted by this HN post. Puter's actually doing a bit more than the Desktop Environment mockup that's visually apparent, and not featuring that seems like a wasted opportunity to pick up some app developers. reply evere 2 hours agorootparentprevMaybe it could be \"sandboxed\" in an iframe? reply patife 4 hours agoparentprev:o reply lavrton 2 hours agoprevTo me, Puter platform has a huge potential. And as a developer, I already have large value from it. Using puter.js I was able to add full cloud storage for my design editor https://studio.polotno.com/ without messing up with auth, backend and databases. reply ent101 2 hours agoparentThank you, Anton! Polotno is incredible. I recommend everybody to check it out! reply mavili 10 hours agoprevThis is insanely cool! Looks really slick too, even in a mobile screen. jQuery?? I cannot imagine how difficult it is to not break this when you make the slightest change, hats off for managing with vanilla Javascript and jQuery! The best thing about React for me is to not have to worry about breaking the DOM or messing up with event handlers because some jQuery line, somewhere obscure is probably doing something funky that is really difficult to track down! reply tambourine_man 7 hours agoparentYou can easily shoot yourself in the foot with jQuery (or direct DOM manipulation for that matter), but it's not that hard not to. It just requires some discipline, like most things. React is also far from foolproof and not worth the added complexity in most cases, IMO. reply smaudet 5 hours agorootparentConvention over Configuration. That and types. The only framework that's useful to JS is a better static type check system (and none of this lets-make-the-whole-damn-runtime-slow to support X feature, looking at you TypeScript). reply devjab 2 hours agorootparentWe use a lot of typescript with a very opinionated setup on coding style and conventions, but that only goes so far when you’re dealing directly with the dom. Because the dom is notoriously hard to work with. The internet is full of blog posts and articles talking about how slow it is as an example, but in reality adding or removing a dom node is swapping a pointers which is extremely fast. What is slow is things like “layout”. When Javascript changes something and hands control back to the browser it invokes its CSS recalc, layout, repaint and finally recomposition algorithms to redraw the screen. The layout algorithm is quite complex and it's synchronous, which makes it stupidly easy to make thing slow. This is why the virtual dom of react “won”. Not because you can’t fuck up with it, but because it’s much harder to do so. reply codethief 1 hour agorootparent> When Javascript changes something and hands control back to the browser it invokes its CSS recalc, layout, repaint and finally recomposition algorithms to redraw the screen. The layout algorithm is quite complex and it's synchronous, which makes it stupidly easy to make thing slow. Wait, you're saying it's synchronous but what exactly is being blocked here (since you also said the JS hands back control to the browser first)? reply pjc50 36 minutes agorootparentRe-layout has to complete before any more JS runs. So if you want to change two things, you get update-layout-update-layout. reply Dylan16807 3 hours agorootparentprev> and none of this lets-make-the-whole-damn-runtime-slow to support X feature, looking at you TypeScript TypeScript runtime? I don't think the deprecated code that sets up enum objects affects anything else... reply codethief 55 minutes agorootparent> I don't think the deprecated code that sets up enum objects affects anything else... TypeScript enums are deprecated now? I mean, yeah, they aren't great and should best be avoided but the deprecation would be news to me. See also https://stackoverflow.com/questions/70661260/are-typescript-... reply smaudet 3 hours agorootparentprevNot enums. But you don't need a runtime or function mutation for that... Particularly egregious was (is?) async/await. Upgrade your browser/runtime/don't use it you say? Sure, but first two weren't always possible, and the third isn't possible unless you thoroughly vet your dependencies (easier said than done). \"Compiling to javascript\" is all well and good if you actually just compile to normal javascript, as soon as you have any code that simulates other features (classes/objects/what-have-you) you are no longer \"compiling to javascript\". I mean yeah sure as a sort of intermediary assembly language you are but the performance is not the same. You have a new language with a runtime overhead, that now requires you modify the \"core\" language to bring in new features, which results in the underlying execution engines (browsers/cpus) becoming more complicated, power hungry, etc.... Anyways, type caching is not all bad. While the TS overhead is likely responsible for the performance wins for javascript in the following chart: https://programming-language-benchmarks.vercel.app/typescrip... The performance wins for typescript likely source from the ability of the runtime to pre-allocate and avoid type checking. Providing the type checks without using any non-JS features (and possibly providing the runtime some heads up regarding checks to safely drop) is the ideal. reply Dylan16807 2 hours agorootparentYou can disable those fallback implementations if you don't want to use them. Just use the javascript version you have available as the basis for your typescript. The option to look into the future shouldn't be treated as a negative. And I still don't see how they make \"the whole damn runtime\" slow. You don't pay the cost for code that isn't using it. Also I'm pretty sure the class implementation doesn't slow things down. It's a very simple transformation. > You have a new language with a runtime overhead, that now requires you modify the \"core\" language to bring in new features, which results in the underlying execution engines (browsers/cpus) becoming more complicated, power hungry, etc.... I think you have this backwards. Typescript doesn't implement new Javascript features until their addition to Javascript itself is imminent. The only feature Typescript wants to push onto Javascript is a syntax for type annotations, because then you can remove the compilation step entirely. At which point there couldn't even be a runtime overhead. > non-JS features To first approximation, there aren't any. The main one is the old enum syntax, which is why I brought them up. reply solumunus 4 hours agorootparentprevCurious which TS features you’re referring to? reply horacemorace 3 hours agorootparentThe level of easy facilitated in part by ts on synchronous is too damn high, for one. reply Intralexical 10 hours agoparentprevOh, wow. When this came up before, I don't know if I looked at the DOM, because I assumed you would do this type of thing by drawing pixels on a Canvas. It's actually made of HTML elements? That's impressive. reply Aaronmacaron 9 hours agorootparentI too think it's very impressive but wouldn't it be even more impressive if it was made using canvas? It would mean that you would need to implement your own rendering loop and layout engine. You'd need to reimplement a lot of elements such as input fields or buttons. You get all of this for free when building on top of HTML/CSS. reply Intralexical 7 hours agorootparentI think it would likely be more technically involved and complicated if it were made with `canvas`. But large blobs drawing to `canvas` aren't anything new at this point. The part that impressed me is doing it the simple way, using what the browser already provides, and getting it to work this well. reply mrBurger 3 hours agorootparentprevI dunno, once you write the renderer, you essentially have no limits to worry about. Working within the confines of HTML is way more impressive to me. reply onetom 4 hours agorootparentprevIt's hard to make that performant. As a reference, here is a browser-adapted version of the https://cuis.st/ Smalltalk environment, which does its own rendering onto a canvas: https://github.com/nmingotti/The-Cuis-CookBook/wiki/Run-Cuis... reply noduerme 7 hours agorootparentprevYou do get a lot of UI elements for free, but styling them consistently and getting them to layout properly becomes a lot more difficult. reply sgbeal 41 minutes agoprevIt's all fun and games until we reflexively hit ctrl-w to close a virtual window and end up closing the browser tab that window is running in :/. reply boomskats 10 hours agoprevI got carried away for ages with this. I was installing extensions in VSCode and got confused when it wouldn't open a link to a repo in a little browserception, because by that point I was fully expecting it to. Really nicely done. reply ent101 9 hours agoparentThank you! As a side note, we're open-sourcing the VSCode integration soon. Building an integration with VSCode takes quite a bit of work so hopefully the community will benefit. reply mjburgess 2 hours agorootparentAre you aware of Theia IDE? I've been using this as a way of quickly putting a UI over an EC2 instance. Could you advise on how you'd do the equivalent here? Eg., say I want to provide an EC2 machine with various packages installed (python, node,...) -- or, the equivalent docker image -- is there a way of using your UI to provide access to this? Consider, eg., using an iPad with your UI in a browser. Could you advise a way that this could provide a complete development experience for datasci/seng? (As above, i'm using theia to do this in a quick-and-disposable way). reply arcastroe 5 hours agoprevInside the OS, there's a game called Danger Cross, which seems very similar to crossy roads. Did the Puter developer essentially reimplement crossy road? Or was there some open source version already existing could somehow run on this \"OS\" ? Briefly searching google for Danger Cross didn't yield any results. reply cushpush 10 hours agoprevWhen I was young I dreamed on having a USB stick (not yet invented) I could take with me to different kiosks and have a standard OS load my specific instances thanks to my custom key. This approaches that functionality and I think it's pure brilliance that you've included such thorough a demo for us to enjoy that you spent so much time and enthusiastic effort into creating and making manifest. So, I applaud you there and thank you for making it open-source that's super cool, and might inspire someone to make a kiosk that, by default, loads your site. reply DustinBrett 10 hours agoparentNearly 20 years ago when I was in IT, I had something like this using a tool called BartPE which allowed you to make a custom WinPE environment that could be booted from a USB drive. As someone who went on to make one of these \"web desktops\", maybe things like BartPE were a partial inspiration. reply mikewarot 9 hours agorootparentAbout 30 years ago when I was a programmer and support guy all in one, I had an MS-DOS boot disk and a 300 Megabyte Backpack portable hard drive. It was awesome. I had all of the source code, backups of the customer sites data and my programming environment with me no matter where I went. Great stuff! reply raffraffraff 1 hour agorootparentThese two posts have awoken old memories for me, because I used BartPE and I had a \"magic\" bootable MSDOS diskette. I remember I also created a custom Windows NT build for a company I worked for around 1999. They had about 6 different models of Compaq workstation, and a dozen different departments. They had a disk imaging solution, rather than implement automated pxe builds. That's fine, except that nobody in the team had any \"craft\". Because NT isn't plug and play and each department had different software, we had about 20+ NT images, each with its own personality (ie major flaws, like hard-coded WINS servers, being already joined to a domain, old user profiles, broken software installations, old drivers). The day I joined the team, the phone rang constantly from 8am to closing time. If you walked around the building to do a desk visit, 20 people would shout at you, \"hey IT guy, take a look at this PC, will you?\". Coming from a hardware support background I had installed MS stuff thousands of times and got my MCSE, but Lotus Notes, Sunguard, Bloomberg and their awful VB6 apps (unpackaged collecting of dlls and instructions) had a short learning curve. After I figured that stuff out I created a single NT build with everything working perfectly. I cleaned up, defragged, ran sysprep. It used NT's hardware profiles to make the build work on any model of desktop (which just required imaging a new model, creating a profile for it and installing all the drivers. Rinse and repeat 6 times). Then I burned the highly compressed NT image along with ghost.exe onto a CD, and handed copies to the other 2 helpdesk guys. Anyone who called IT over the few weeks, regardless of their issue, got a rebuild. Result? Immediate reduction in workload. So we proactively worked through the whole company. Things were so tranquil afterwards, we could go around to department heads asking if there was any \"real\" IT work that needed to be done. That one disk, man. reply peteradio 8 hours agorootparentprevHis name was Max Harddrive and he did kickflips on his keyboard. Nobody could beat his quarter mile downloads. reply ldp01 7 hours agorootparentHahaha, I need to go watch Hackers right now! reply rnts08 2 hours agorootparentI unironically watched Hackers last night. reply pmarreck 6 hours agorootparentprevI randomly watched a scene from it on YouTube based on this comment, and in the background was a poster I never saw before that said \"Trust Your Technolust\", and just like that, I have my new life motto LOL reply E39M5S62 5 hours agorootparenthttps://mako.cc/copyrighteous/trust-your-technolust Just need to get the right color for your paper stock! reply pdntspa 6 hours agorootparentprevBartPE was my god as a computer repair tech. I had a custom build with everything I ever needed on there! Data recovery... virus removal... diagnostic tools... the works. reply cipehr 4 hours agorootparentprevOh man I remember BartPE and customizing the apps bundled with it… thanks for the memories. reply anthk 8 hours agorootparentprev20 years ago knoppix did far more. reply airspresso 2 hours agorootparentfond memories of Knoppix! Was so useful to boot into that to debug, investigate, fix. reply int_19h 4 hours agorootparentprev20 years ago mounting NTFS read/write on Linux was a risky proposition. reply vel0city 8 hours agoparentprevWhat I'd really love is a net boot image and remote storage that can be mounted on the fly on any computer. All I'd need is my yubikey or whatever secure identifier to connect all the pieces.l and plenty of internet. I'm partially there with VDI tools like Parsec, but being able to leverage local hardware would also be neat. reply orev 5 hours agorootparentBootable USB that loads iPXE, with a custom boot config that downloads the kernel and initrd from your own cloud server? reply fennecbutt 7 hours agoparentprevSamsung DeX. reply digdugdirk 7 hours agorootparentIs Samsung Dex able to boot/load to a standard Linux environment? Initial look at it seems interesting, but I've never really heard/seen much about it. reply peterleiser 6 hours agorootparentI've used Dex on my Galaxy Z Fold 4 and the apps are Android apps. If you want linux on Android you can try this suggestion (termux + ubuntu-on-android + vnc server): https://www.reddit.com/r/SamsungDex/comments/opyj6o/linux_on... reply danlugo92 4 hours agorootparentprevYou can also ssh into a linux vps fwiw reply binarysneaker 3 hours agoparentprevTried Tails? https://tails.net/ reply whalesalad 4 hours agoparentprevSlax reply petesergeant 8 hours agoparentprevFor some reason I'd assumed this was the kind of thing that the original Mac Mini would morph into, but of course I was wrong. Searching for \"PC Stick\" shows up some interesting options. I would dearly love to find a use-case for this[0], which with a USB-HDMI might work almost as well 0: https://store.planetcom.co.uk/collections/gemini-pda/product... reply elwell 8 hours agoprevBeautiful execution! Though I'm crestfallen it has no Browser app with which to view an inception of Puter within. reply pcloadletter_ 7 hours agoparentYou can do this on daedalOS https://dustinbrett.com/ edit: and obviously you can play Doom on it reply smaudet 5 hours agorootparentAre the changing colors on purpose? Noticed it has some issues browsing in firefox (so it might be broken). reply keepamovin 7 hours agoparentprevTechnically you could fake it with an iframe if puter is served with the correct frame-src CSP rules. But for a full browser experience you’d need a backend and a remote browser. reply LoganDark 8 hours agoparentprevJust open the Puter app: https://puter.com/app/puter Alternatively, open the PuterPuter app, which contains itself: https://puter.com/app/puterputer You're welcome reply lagniappe 8 hours agorootparentwhat about just a browser in general? reply keepamovin 7 hours agorootparentI want to add BrowserBox to it, which is a web browser you can embed in any web app haha! :). It will require a backend tho but it should work. https://github.com/BrowserBox/BrowserBox reply Zenst 50 minutes agoprevImpressive and reminded of https://en.wikipedia.org/wiki/Java_Desktop_System reply GMoromisato 8 hours agoprevThis is really cool--I've played with a lot of these online desktops, but this is by far the slickest. As someone who is doing something similar (https://gridwhale.com), I'd love to know what your goals were. Did you ever try to commercialize it? If not, why not? If yes, what happened? reply jethro_tell 11 hours agoprevThe docs say a this can be used for remote access to servers and workstations. How does it handle things like privilege escalation and sandboxing? I'm assuming you mean remote access for a user account like a terminal server as opposed to server management. Is that the case? reply infogulch 9 hours agoprevThis reminds me of Kera Desktop [1]. Featured on HN 8 months ago, 343 points, 111 comments [2]. [1]: https://desktop.kerahq.com/ [2]: https://news.ycombinator.com/item?id=36260589 reply ambigious7777 9 hours agoparentAnd this anuraOS[0] too! I haven't looked to closely at it (it showed up in my GH feed) but it looks like it uses v86[1] to emulate a *nix environment. [0]: https://github.com/MercuryWorkshop/anuraOS [1]: https://copy.sh/v86/ reply blintz 10 hours agoprevThis is such a neat idea, and you get the gist of it from just the screenshot. I wonder what kinds of 'integration' you could do (clipboard, opening links, drag-and-drop, etc). I could see this as an educational tool for doing development on a Chromebook, because of the (emulated) terminal + filesystem. reply DustinBrett 9 hours agoparentIndeed the modern Web API's can do all that and more. One of my favorites is dragging out of the browser onto the desktop. Another is ctrl+c'ing a file on the real desktop and then ctrl+v'ing it into the \"fake\" one. Some super powers sadly are locked away in the need for a PWA, but I think they could one day just be part of what any site can do. reply AlexCoventry 9 hours agoparentprevChrome OS has a developer mode which runs debian in a virtual machine, FWIW. reply danans 3 hours agorootparentTechnically it's not developer mode , it's just a regular feature of the OS. \"Developer Mode\" on ChromeOS is when you remove boot verification for ChromeOS itself. reply throwaway11460 31 minutes agoprevAnybody remembers the original (not Apple) iCloud web desktop? Or EyeOS? reply 1oooqooq 15 minutes agoparentthis is more in line with webOS, from palm. still lives in LG tvs. i mean, the front end of it. the back end of it would be a much better option than eletron apps btw. but they were too early to the container age, since the paradigm was syscalls proxies reply simon_acca 10 hours agoprevThe experience of making and publishing websites and apps from within puter is sobering for how simple it is. Something to aspire to reply danlugo92 3 hours agoparentCan you post a tldr here please...? reply anonzzzies 1 hour agoprevPossible to write our own window manager? I guess just diving into the code, but that would make it very hackable of course. And fun. reply rkagerer 11 hours agoprevDec 2022 https://news.ycombinator.com/item?id=33838179 reply ent101 10 hours agoparentWelcome to my annual HN roast :') reply DustinBrett 10 hours agorootparentIs it that time of the year again already? reply ent101 10 hours agorootparenthaha you feel me. We're united in vision and the roasting lol reply pvg 9 hours agorootparentI suggested a name for this at one point and sounds like willing Shubs and Zulls aren't that hard to find... https://news.ycombinator.com/item?id=32233087 reply thatgerhard 28 minutes agoprevWhat is this used for? reply jaequery 43 minutes agoprevjQuery? :D it's true, coding was easy with jQuery ... reply ent101 43 minutes agoparentusername checks out :D reply willguest 10 hours agoprevI've been looking an OS for a VR computer (i.e. simulated but functional workstation). This should work very well :) reply ent101 10 hours agoparentI've tested Puter on Oculus, seems to work pretty well; however, I think very soon I'm going to do very specific optimizations for XR, it's a new, emerging form factor that deserves its own design. reply willguest 10 hours agorootparentI don't mean in a browser on a VR headset, I mean on a texture on a game asset in a Unity scene in a Webxr space in a browser on a headset. This way you have the experience of sitting in front of your computer while wearing a VR headset anywhere in the world. reply whalesalad 10 hours agoprevThis is one of the cool elements of the Synology operating system. Would be neat to see this extended further into other areas, using this as a base. I setup a TrueNAS box for my dad recently and he was yearning for some kind of very light desktop environment for simple maintenance tasks. In hindsight I should have gotten him a Synology device. reply ghostly_s 10 hours agoparentAre you saying Synology's DSM is based on this Puter project? reply usefulcat 8 hours agorootparentProbably not but they have had a similar web-based interface going back at least twelve years reply jayknight 7 hours agorootparentThis is was more responsive than the Synology UI. reply Renaud 7 hours agorootparentprevDSM apparently uses Ext.js, not sure exactly which version though. reply whalesalad 4 hours agoparentprevSorry - “this” being the concept not the literal codebase. reply qiqitori 10 hours agoprevLove it! I liked the Solitaire implementation. The terminal seems very lacking, \"ls\" worked but e.g. \"ls *\" or the \"find\" command didn't work. reply theogravity 10 hours agoprevWe also do a desktop in a browser, but some core differences are you can launch real browsers in our environment and essentially load any web-based app you want collaboratively. It also does screen sharing and has A/V for meetings (we are not open source though, and have a paid product): https://www.switchboard.app/ Knowing how difficult it is to build something like this, Puter definitely deserves praise. reply haswell 8 hours agoparentWas curious to check this out, but no Firefox support is a non-starter, unfortunately. reply theogravity 8 hours agorootparentSorry about that. It should hopefully be fixed once we change our A/V stack in a few months. I unfortunately do not have specific details on why the A/V stack only works with Chrome-based browsers. reply orliesaurus 10 hours agoparentprevwhat's the reason you built it? reply theogravity 9 hours agorootparentI believe the original reason was the founder wanted a way to have his guitar teacher share tabs and give A/V feedback on his guitar playing. It was originally a product for teaching with limited sharing features, then the founder realized it could be used for many other use-cases and expanded to be much more collaborative and became essentially a multi-room collaborative virtual desktop-like experience. We use it daily for things like standups, sprint planning, all hands, operations dashboards, product roadmap discussions, dungeons and dragons sessions, watching videos together and more. reply peterleiser 6 hours agoprevLooks very cool. Am I correct that this is more like a client and that the persistence (user storage, sessions, etc) is handled by the proprietary non-open source cloud backend? Not a criticism, just trying to understand. reply asveikau 6 hours agoparentIt would be cool to self host the backend. I wouldn't want to put files on some guy's server, both from the eavesdropping angle and from the perspective that it could be abruptly shut down and lost. reply willi59549879 1 hour agoparentprevit is basically a webserver written in javascript. you can clone the git repository and run the server on your pc reply waldrews 7 hours agoprevCurious how AGPL would apply for something like this. This seems like a tool to put a nice front end on a complex app, but would that trigger copyleft for the the overall backend? reply ulrischa 56 minutes agoprevMake JQuery great again reply mouzogu 2 hours agoprev\"Puter is built with vanilla JavaScript and jQuery\" respect. reply yu3zhou4 11 hours agoprevReally nice execution! Thanks for sharing reply ent101 11 hours agoparentThank you very much! Glad you liked it :) reply maxloh 7 hours agoprevIn case you wonder about the purpose of the project like I did, here's the explanation in README: > It can be used to build remote desktop environments or serve as an interface for cloud storage services, remote servers, web hosting platforms, and more. reply sn0n 5 hours agoprevAre you the MS guy with a YouTube channel who's website is basically this? Can't recall the channel right now though. reply ijxjdffnkkpp 10 hours agoprevYou used the AGPL! Glorious! I commend and salute your efforts. Thank you for your contribution! reply codeonline 5 hours agoprevI just wanted to say well done. I wish other eco systems were this open, hackable and understandable. reply lovestaco 2 hours agoprevPretty slick reply geek_at 2 hours agoparentKind of reminds me of EyeOS [1] I was using it heavily in 2005. Mindblowing experience at the age of Windows XP [1] https://en.wikipedia.org/wiki/EyeOS reply koukides 6 hours agoprevLove it!! I own the domain internet.inc that would be perfect for this - want to use it?? reply MH15 6 hours agoparentWoah that's a crazy domain. Worth a lot. Love your parking page! reply koukides 5 hours agorootparentIt's for sale, email me :) reply DustinBrett 10 hours agoprevGreat to see you finally got it open sourced, congrats! reply indigodaddy 8 hours agoprevWhat’s the “publish as website” thingy/functionality all about? reply Solvency 9 hours agoprevI love how this is done with jQuery. And it'll be obvious to literally anyone whose ever used jQuery (and is a good designer) how perfectly suitable and in many ways superior jQuery would be for something like this. But 98% of developers will absolutely balk at this in horror/confusion/wonder, despite the fact that the React/Angular DIYs they'd make would be bloated and outrageously slow. reply Th3Alt3r 6 hours agoprevVery cool, LFG!!!!!! reply 1attice 10 hours agoprevOne of the things I find curious here is that there's no mobile story to speak of. Drag the window narrow, and all that happens is that the taskbar icons look squished. Is there a mobile mode coming? This would be dope af if I could get a mobile-esque UI on mobile-like devices, or opt into one on tablet-like devices. In all honesty, this is perfect for people like me (who are rarely away from a keyboard) but less than ideal for people who live a more 21st century computing lifestyle. If this thing had a mobile mode it would be revolutionary. As it stands, it's still definitely revolution-friendly :) reply anon373839 3 hours agoparentI was a little surprised to see that it doesn't have the few extra tweaks necessary to work as a PWA in fullscreen mode: a manifest, some tags in the , and CSS to lock down the body to prevent unwanted pinch zooming, scrolling and other gestures (which would also benefit the in-browser use case as well). But still, really incredible work. reply NikolaNovak 7 hours agoparentprevDid you try it on mobile? Seems to work fine on my iPhone. reply 1attice 4 hours agorootparentYes, it works, but it looks the same. Remember Windows CE? We've been down this road -- a windowing desktop on a phone is... hard to use, actually. reply ametrau 7 hours agoparentprevWhat is a “mobile story”? Can you deliver some messaging in a comment story? reply 1attice 4 hours agorootparentCharming. What I mean is: - there are strong, empirically tested usability reasons why a desktop-style UI is the pits on a mobile device - generally, the move to mobile devices inclines to a different UI paradigm -- something finger-friendly and un-windowed (think iOS, Android, etc.) - if there was a way that Puter could look kind of like a phone with some of the de-facto standard UI motifs and paradigms when it's on a phone, you could do cool shit like replace the Android shell with Puter, and have a pure web UI for your phone that runs web apps - next step would be to add e.g. the ability to run Puter apps when offline, when reception is going in and out, etc. Sort of like Google Apps used to be (in Chrome) - now you've got a cross-platform one-UI-to-rule-them-all breakout industry moment. As it stands, Puter is already really cool, but if it turned into something that behaved a bit less like MacOS and a bit more like iOS at narrow resolutions, you'd have something that would make a lot of people very upset (and this is a good thing, they deserve to be upset) reply soloknight 3 hours agoprevThis is soo freaking amazing !! What a legend reply FpUser 6 hours agoprevBeautiful work. Much appreciated reply mattl 10 hours agoprevCongrats on getting this out there. Looks slick. I’ll take a look tomorrow when I’m back at a computer with a larger screen. reply mixmastamyk 11 hours agoprevLooks awesome. I'm thinking about building a niche CMS soon, could this be an interface to it? Mentions cloud applications. Seems like it might confuse normal folks though—a desktop in a browser. What do y'all think? reply Razengan 5 hours agoprevNot to detract from the coolness of this, but I wish new OSes dabbled in new GUIs as well. Experimental operating systems seem to be dime a dozen by now, but we almost never see experimental GUIs or entirely new \"desktop environments\". Just as how almost every \"new\" programming language is still stuck with semicolons and other C-isms that were ancient back when the Egyptians were laying down the pyramids, we're still stuck with either imitating the macOS GUI or the Windows GUI, or some weird Frankenstein's bastard of the two. iOS, Android, consoles, and most recently the Vision Pro have proven that eschewing longstanding conventions can be successful — for example the vast majority of people on this planet don't need or care about scrollbars (or even menubars) anymore. So why aren't the creators of experimental OSes being more experimental with the frontend? Come on guys, none but the nerds among us will be impressed with how it's made behind the scenes. The first impression that most people will get is that's just Yet Another WinMac-Looklike. reply isoprophlex 2 hours agoprevThis looks amazing. Also; > Why isn't Puter built with React, Angular, Vue, etc.? > For performance reasons, Puter is built with vanilla JavaScript and jQuery. Additionally, we'd like to avoid complex abstractions and to remain in control of the entire stack, as much as possible. Absolute boss level. reply theK 1 hour agoparentDoes this argument really still stand nowadays, with all the virtual DOM and whatnot that those frameworks bring OOB? reply isoprophlex 1 hour agorootparentI'm not really writing that to stir up js framework shit, mostly just commenting on the insane dedication to ideals needed to go \"fuck it i'll just use jQuery\" on a project of this scope. At least, from my brainlet dev perspective, a move like this is 100% boss level. reply rakoo 1 hour agorootparentprevVirtual DOM has always been about ease of programming, never about performance. Since this model has existed, all the work has been made to gain more performance but still not fast enough compared to direct DOM access. reply lopis 1 hour agorootparentprevI thought we're moving away from virtual DOM? reply trashburger 1 hour agorootparentWho's \"we\"? React is still by far the most popular framework. I personally like Solid better but I'm just one person vs. many who actively write React code. reply monsieurbanana 1 hour agorootparentWith the new React 19 that changes everything once again™, I wouldn't be surprised if by React 20 or 21 they completely move away from the vdom. Getting a compiler step seems like a first step towards that. reply trashburger 1 hour agorootparentprevNot sure why you're downvoted, it's a valid question (the downvote isn't an \"I disagree\" button). I believe the answer the Puter author came up with is that the VDOM takes away too much of the control from the author. They do make life easier but they have an abstraction cost (mental overhead) and in case of some of them performance issues (execution overhead). reply sam_goody 2 minutes agorootparent> the downvote isn't an \"I disagree\" button Would that it were so. HN has a down/up pair, which is definitely used to express agreement and disagreement. IMO, HN would gain a lot by allowing for more flexibility in down votes - eg. being like SO that a downvote is only a quarter of a vote. reply zarathustreal 1 hour agorootparentprev> the downvote isn’t an “I disagree” button Take it from a guy that regularly posts unpopular truths: people vote based on the way they feel after reading. It has almost nothing to do with the content. Edit: for example, try posting a personal opinion about a controversial topic and you’ll still have people downvoting to disagree as if it’s possible to tell someone that they are in fact wrong about a statement of what their opinion on a topic is. It’s a bit sad if you think about what voting like that means, as content is often amplified or suppressed based on votes. Echo chambers seem inevitable reply isoprophlex 53 minutes agorootparentHello! Would you like to post any opinion at all, or in any possible way mention anything about, the humanitarian situation in Gaza? reply sam_goody 0 minutes agorootparentThere is a flag button as well as the downvote. Throwing in something about Gaza here would be off topic, political, and against HN guidelines. OJFord 9 hours agoprevSuper slick demo, I'm on mobile and it's impressively fast nevermind functional. But it is 'just' a DE webapp right? From 'internet OS' here (which actually I don't think you use at TFA repo) I expected to be able to boot to it. I guess there is some other solution that would allow that, but not a package deal? I suppose I'm just saying be careful/manage expectations with 'OS', but for what it actually is it's really cool. reply Intralexical 5 hours agoparentIt's got an app store, and it seems to implement embedding, windowing, and a virtual filesystem for arbitrary apps that have been built for it. I.E., When you open \"Polotno\" (a graphics editor), or what appears to be VSCode, it opens in a windowed IFrame, but then going to \"Save\" pops up a file selection dialog controlled by Puter, which lets you create a \"file\" which can then be accessed by the \"Open\" button in any other app. It looks like there are other integrations between the DE/OS and its apps as well, like applications being able to set their window title dynamically (E.G. based on open file/tab), and an API allowing robust support for third-party apps. If you open one of the games like Doom, you'll see that it's usually hosted on a third-party site like DOS.Zone, but according to the query string using a custom build for Puter, which presumably has modifications to integrate with the desktop environment and filesystem. Other apps are hosted on Puter.site, . So if you treat the browser as the \"hardware\" (and maybe also the backend services hosting a lot of the apps), maybe you could call it an \"OS\" in that it abstracts and manages a single environment for multiple other programs to run simultaneously and share information with each other. I suppose `puter.js` is what passes for its LIBC, or the syscall interface: https://docs.puter.com/ To OP: The information on publishing third-party apps doesn't seem very discoverable. The only mention I saw is in the popup that shows the first time you launch Dev Center, and after closing that I can neither find it again nor find anything else about it on Google (other than the linked terms, and the app IFrame source, which I presume is from GoogleBot's temporary account): https://puter.com/incentive-program-terms reply klyrs 6 hours agoparentprev> Super slick demo, I'm on mobile and it's impressively fast nevermind functional. No kidding. As far as I've seen, it's the best open paint app on android reply lphnull 6 hours agoparentprevThis reminds me of something I made once in \"Macromedia\" Flash back when I was 16 in like 2001ish or so. It wasn't really an actual OS, but I managed to get a working desktop with a file browser, fake non functional web browser, and a task bar and start menu. My flash application didn't accomplish anything, but I felt immensely proud of having been able to create a mock up of a UI, even though it was extremely kludgy and wasn't even able to read or write files in the fake applications. ...No offense to the creator though. I don't mean to compare my high school project to this one. This project is indeed very cool! I too dream of one day inventing my own kind of spreadsheet and terminal hackery OS, so I salute this person for creating a proof of concept that looks good. reply dietr1ch 5 hours agorootparentHere's the best flash OS that will ever exist, https://www.jamesweb.co.uk/windowsrg reply Intralexical 5 hours agorootparentprevScratch OS is where it was at. reply mattl 8 hours agoparentprevYeah “OS” really overused. reply baudaux 1 hour agorootparentBut it is a great project. For a real (Unix-like) OS you can have a look to https://exaequOS.com (I am the creator) reply tambourine_man 10 hours agoprevThat warms my heart: “ - Why isn't Puter built with React, Angular, Vue, etc.? For performance reasons, Puter is built with vanilla JavaScript and jQuery. Additionally, we'd like to avoid complex abstractions and to remain in control of the entire stack, as much as possible. Also partly inspired by some of our favorite projects that are not built with frameworks: VSCode, Photopea, and OnlyOffice. - Why jQuery? Puter interacts directly with the DOM and jQuery provides an elegant yet powerful API to manipulate the DOM, handle events, and much more. It's also fast, mature, and battle-tested.” reply tony_cannistra 10 hours agoparentHell Yeah. reply enriquec 9 hours agoparentprevThis just tells me they don't understand why any of those things were created and how to actually use them. It won't get traction and it'll wallow and get stale since it'll be an unmaintainable project. Also, VSCode is built on Electron while Photopea and OnlyOffice are straight up painful to use. reply sgbeal 33 minutes agorootparent> This just tells me they don't understand why any of those things were created... Like very nearly all FOSS, they were created to scratch their own developers' personal itches. > ... and how to actually use them. Their itches are not the same as everyone else's itches. reply smarkov 7 hours agorootparentprev> Also, VSCode is built on Electron That's not the point. The point is that it doesn't use a framework like React, Vue, etc., it instead directly creates and manipulates DOM elements, somewhat like Puter. reply gertop 6 hours agorootparentprevI don't know what's your benchmark to say those things? OnlyOffice works better than LibreOffice, which is a native app. Photopea works better than GIMP, which is a native app. These devs clearly know what they're doing. React and all the packages you champion were created to foster job security by introducing needless complexity and performance issues. It would be a better world if we had more Photopeas and less Enriques reply simondotau 9 hours agorootparentprevThat’s a lot of smack talk for someone who didn’t provide a link to their own, even more impressive project. This kind of reaction reminds me of people who can’t fathom why anyone would use a database without an ORM. (And meanwhile I’m confused because nearly everything I do with a database would be twice as difficult and ten times slower with an ORM in the way.) reply aurareturn 6 hours agorootparentI use Prisma and the TS typings in generates automatically have saved me countless times. I used to hate ORMs like you. But auto generated types were the killer feature. reply enriquec 7 hours agorootparentprevMust not be doing anything very complicated. Wheres your impressive link if thats a prerequisite for discussion? Some ORMs suck, I’ll give you that. reply simondotau 4 hours agorootparentIt’s only a prerequisite if you want to belittle people and accuse them of making ignorant technology choices. (If the discussion was ORMs, I’d be delighted to have an excuse to share and promote my public projects. But my earlier aside doesn’t constitute a change of topic for this thread.) reply pdntspa 6 hours agoprevI hate that these guys have to justify using jQuery. Too many of you have been seduced by all these bullshit javascript stacks that just add a seemingly infinite amount of complexity to something that is already almost too complicated. reply saidinesh5 5 hours agoparentDoes jQuery by itself offer any value these days compared to plain Javascript though? I was under the impression jQuery was for the IE5 days where not all browsers provided the same javascript APIs and doing an xmlhttprequest was more clunky than doing a fetch() call these days. At least for my personal Jekyll based blog, I was able to replace 40-45 lines of jQuery with 50 lines of plain Javascript and was able to get rid of 33kb of jquery dependency. reply niels_bom 1 hour agoparentprevI think there are valuable positions to take in between “let’s manipulate the DOM by hand” and “React Server Components, NextJS and the kitchen sink”. I personally think there’s great value in the pattern of reactivity where parts of your DOM update based on bits of data. Developing, designing and testing components like this becomes way simpler. reply ent101 5 hours agoparentprevYes, I've been asked why we use jQuery many times so I had to put it in the FAQ. I think we have solid reasons for it but it is controversial sometimes. reply danielovichdk 5 hours agoparentprevAmen! reply dartos 5 hours agoparentprevAh, jQuery, that cute little artifact from web development's stone age, where manipulating the DOM directly was the height of innovation. How quaint it seems now, in the shining era of React, where state management is not just a task but, once it gets its hooks in you, evolves to a transcendent experience. To cling to jQuery is like insisting on using a typewriter in a world of voice-to-text AI, but adorably obsolete. React, with its majestic state orchestration, makes jQuery's efforts look like child's play. It's like comparing the glow of a firefly to the brilliance of a laser beam. Truly, we React devotees can only chuckle at the notion of jQuery, for we have tasted the future, and its name has and always will be React. /s reply redbell 10 hours agoprevAh, Puter! That fascinating project surfaced on HN about a year ago, claiming the top spot for most of the day. I'm delighted to witness its transition to open source, allowing us to glean insights from the creator. Gracias for sharing! The emergence of such front-end projects provides a profound glimpse into the maturation of front-end development and showcasing the incredible possibilities it offers today. Another really cool project, somehow related, is DaedalOS [2]. Honorable mention, Windows 11 in Svelte [3] 1. https://news.ycombinator.com/item?id=33838179 2. https://news.ycombinator.com/item?id=38830132 & https://news.ycombinator.com/item?id=29779753 3.https://news.ycombinator.com/item?id=35896505 reply DustinBrett 10 hours agoparentThanks for the mention for daedalOS! I agree that when done well these projects can help demonstrate the maturity of the web as a platform. For anyone interested in checking out mine, it's on my personal website @ https://dustinbrett.com/ reply hliyan 7 hours agoprevWhy is this so easy to read compared to React? https://github.com/HeyPuter/puter/blob/master/src/UI/PuterDi... reply raggi 5 hours agoparentDespite being asynchronous it contains a single linear schedule and two callbacks. The obvious outcome is two layered constructors, and a single top level API object with two custom events. You couldn't describe what was going on in an equivalent functional construction of a react version unless you first head off on a diatribe about signals/hooks/state-management-disaster-du-jour, the choice of use of which would have implications to both the event handlers in some way - though they likely couldn't be in-place here, they'd have to be passed down from parent context in a giant dance of inversion of control. At runtime the outcome would likely get parsed twice (once for shadow, once for insertion) if not more times under certain compositions. Here the jquery is almost incidental, and could be quickly replaced by something else - for example want to swap out htmx here, well, just add the markers to the DOM, delete the jquery lines and you're done. You'd probably need to spend at least an hour figuring out if you could fit something independent and platform-native into your react code base, hell I've seen people burn weeks on it. reply ashishb 7 hours agoparentprevThe JSX syntax makes React hard to read due to multiple layers of nesting. IMHO, Vue is superior for both maintainability and readability than React. Especially because CSS, JS anda HTML are largely in separate blocks of the same file. reply noduerme 7 hours agorootparentArguably they should all be in external files. HTML in one, CSS in another, and Javascript to load and cache them without any embedded strings. That's how I prefer to write. For one thing, it lets me choose which things I want to cache and which I don't. I can compile from SCSS and deploy without changing any JS or HTML files. Usually my rollups are just the JS. reply CharlieDigital 7 hours agorootparentprevI can understand some of the benefits of JSX, but compared to Vue SFC's, it's a mess to read. React CSS modules feel so unwieldy compared to Vue SFC's inline `scoped` CSS. Vue's approach to templating using `v-for` and `v-if` means that you won't end up with a big block of logic mixed in with the template (you can still do it, but Vue's nature steers you away from it). reply benatkin 7 hours agorootparentprevYou can see this in that Flutter didn't copy JSX and its usability seems better. Not that I'm confident Flutter is worth the trade-offs. reply mightyham 6 hours agorootparentI'm confused about what you mean by this. There's nothing about Flutter's API that encourages less nesting, In fact, it's the exact opposite. Plenty of layout and styling that takes a number of nested widgets in Flutter can be achieved using a single div in React. reply pmarreck 7 hours agorootparentprevnarrator: It's not. reply pcloadletter_ 7 hours agoparentprevThat's a pretty small component and I think folks would find a comparable React component also pretty easy to read. Conversely: https://github.com/HeyPuter/puter/blob/master/src/UI/UIDeskt... reply ent101 7 hours agorootparentI'm sorry XD reply pcloadletter_ 7 hours agorootparentHa, I am not criticizing your work at all. It's nothing short of jaw-dropping. reply divbzero 6 hours agoparentprevI guess “React makes sense for more complex UIs” is not hard-and-fast rule. jQuery, apparently, is more than enough for clean working frontend code. reply omeid2 6 hours agorootparentIt is not about complex UI per se, it is about complex state. With jQuery, the State is duplicated in the DOM instead of mirrored and is very hard to keep in sync. reply xorcist 2 hours agorootparentThere is no sync if the state is the DOM. reply rtcoms 3 hours agorootparentprevWhy not use a state management library like xstate with jQuery ? reply djbusby 5 hours agorootparentprevWrite state to DOM, read state from DOM. reply omeid2 4 hours agorootparentIn sounds nice in theory, but in practice, it doesn't work out. The example linked is a good one, a popover, the underlying DOM disappears, but the \"state\" behind it remains at least a bit longer. reply FpUser 6 hours agorootparentprev>\"State is duplicated in the DOM instead of mirrored\" And the difference between duplicated and mirrored is ... reply omeid2 4 hours agorootparentFair question. Mirrored means when you update state, your changes are reflected in the DOM, automagically. Duplicated means when you change state, you also have to make the changes to the DOM, yourself. reply divbzero 3 hours agorootparentClear (mostly one-way) data binding is definitely one of React’s strengths. It appears that Puter handles templating with a pattern like: let h = `` h += `${exampleContent}` h += `${moreExampleContent}` $(targetEl).append(h) And handles events in the traditional way: $(targetEl).on('click', function () { /* do stuff */ }); Searching for “React” or “jQuery” in this thread, there are several other conversations with thoughtful comments about pros and cons. One curious tidbit that I learned is that Visual Studio Code doesn’t use a framework either and, like Puter, updates the DOM directly. reply omeid2 3 hours agorootparentThe main issue is that \"$(targetEl).append(h)\" is not idempotent. This might seem like a small issue on the surface but as your application grows, this either becomes a big problem or you have reinvented React, Vue, or something similar, but without the extensive testing and developer/community availability. Which is essentially what VSCode does for example, using some sort of MVP* pattern. reply jahewson 7 hours agoparentprevBecause it’s trivial? reply blackoil 6 hours agoparentprevThis component has no state, no data fetch or transitions. It doesn't even have any dynamic part. reply semolino 8 hours agoprevMore web desktops: https://simone.computer/#/webdesktops Note that most of these are aesthetic / non-CRUD. reply firstbabylonian 11 hours agoprev> For performance reasons, Puter is built with vanilla JavaScript and jQuery. jQuery is overdue for a comeback. reply noduerme 7 hours agoparentIt never really went away for me. I still reach for it in every web app. Over time, I've stopped using some parts of it that used to be timesavers... e.g. shifting away from $.ajax calls to fetch, but it's a good base for rolling your own responsive frameworks if that's what you want to do. And it's what I want to do, because I dislike the paradigms for react and vue, and have no interest in relying on those projects. reply enriquec 9 hours agoparentprevcould not disagree more. That statement alone makes me completely lose any consideration for this entire project. reply yankaf 3 hours agoprevnext [2 more] [flagged] jrflowers 3 hours agoparentOnly the truly fulfilled have time to Post Online about it reply rgbrgb 11 hours agoprevnext [10 more] [flagged] user8501 10 hours agoparentIt is absolutely not bait. Declarative programming for the web is certainly more pleasant, but direct DOM manipulation will always and forever have a performance upper hand over rendering frameworks that sit atop the DOM. Now this performance difference is usually negligible, but for projects like VSCode or this, there’s a very good reason they aren’t written using frameworks. reply ww520 10 hours agoparentprevDepending on how you do rendering, vanilla JS can be extremely fast. For most web apps, the initial layout and rendering take the biggest portion of the rendering budget. Subsequent changes are minimal and can be rendered very fast. If you can avoid re-rendering the whole page for every little change, the update can be fast. React's advantage is to track the changes for you. That doesn't mean you can't track the changes yourself. And if you structure your rendering and refreshing pipelines correctly, vanilla JS would do just fine if not better. reply nosefurhairdo 10 hours agoparentprevControlling DOM updates yourself will always allow for best performance. Virtual DOM still needs to reconcile and apply DOM updates, but since this is handled by your UI framework of choice you have limited ability to optimize these updates. See benchmarks: https://krausest.github.io/js-framework-benchmark/current.ht... I would argue that if performance is your top concern you'd still be better off using a framework like SolidJS, but theoretically JavaScript and JQuery can be better optimized. reply n2d4 10 hours agorootparentAre we seeing the same benchmark? On that site, both Vue and Svelte are extremely close/on-par with Vanilla JS on every benchmark except the transfer size and first paint (which shouldn't matter at all here). jQuery also has an overhead, and I wouldn't be surprised if it's larger on many benchmarks than, say, Svelte's. reply nosefurhairdo 10 hours agorootparentYes, modern frameworks are getting very good. This does not change the fact that abstractions around DOM updates (frameworks) limits your ability to maximally optimize performance. The output of frameworks is still vanilla js. reply tsukurimashou 10 hours agoparentprevyour post is bait, react more performant than vanilla JS + jQuery? you gotta be kidding me reply josephg 10 hours agoparentprevYep, jQuery isn't an obvious choice if you want performance: https://krausest.github.io/js-framework-benchmark/current.ht... reply commotionfever 10 hours agorootparentjquery isn't on that list reply nicce 9 hours agorootparentTechnically, when you use Vanilla JS these days, it is mostly like jQuery since most if not all things have been migrated from jQuery to Vanilla JS :) reply gorkaerana 1 hour agoprev [–] A slightly unfortunate name for Spanish speakers, I'm afraid: \"putero\" can mean either \"brothel\" or \"man who maintains sexual relations with prostitutes\" [1]. Fun fact: the Mitsubishi Pajero had to be marketed as Montero in Spanish speaking markets as \"pajero\" is Spanish for \"wanker\" [2]. [1] https://dle.rae.es/putero [2] https://dle.rae.es/pajero reply henryackerman 1 hour agoparentSimilary, the Hyundai Kona is called the Hyundai Kauai in Portugal as \"cona\" literally translates to \"pussy\". Languages are fun :) reply Beijinger 1 hour agoparentprev [–] Wanker is a family name in Austria. I knew a scientist who started every talk with the origins of his family name.... reply gorkaerana 1 hour agorootparentSpanish also delights us with unlucky surnames. E.g., three acquaintances of my father have the family names \"Feo\", \"Bastardo\", and \"Gay\"; which translate, respectively, to \"ugly\", \"bastard\", and, of course, \"gay\". reply Beijinger 1 hour agorootparentWell, I know European Companies that wanted to expand into the US market. One was named PMS LLC and one had a product that was named xfriend. Both names are very memorable, but not the best choice. Xfriend was actually a pretty good software: https://xfriend.soft112.com/ (only description, not for download anymore) reply ClikeX 1 hour agorootparentprev [–] We've got Dutch people that are literally called Dick Cock. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Puter\" is a sophisticated open-source desktop environment accessible through a web browser, emphasizing being feature-rich, fast, and customizable.",
      "It serves as a substitute for cloud storage platforms or acts as a remote desktop for servers, made with vanilla JavaScript and jQuery for optimal performance and stack management.",
      "The project targets web development, cloud computing, and distributed systems, offering multiple applications, deployable either locally or in live environments, and fostering a welcoming community."
    ],
    "commentSummary": [
      "The conversation delves into diverse topics, including the open-sourcing of an \"Internet OS\" platform and the challenges of JavaScript.",
      "Users examine the benefits of frameworks like React's virtual DOM and discuss the potential performance impact of TypeScript.",
      "Projects such as anuraOS and Puter are analyzed, along with debates on design choices, comparing front-end development with React and jQuery."
    ],
    "points": 810,
    "commentCount": 210,
    "retryCount": 0,
    "time": 1709591475
  },
  {
    "id": 39592444,
    "title": "Unveiling the Graph Type Conundrum",
    "originLink": "https://www.hillelwayne.com/post/graph-types/",
    "originBody": "The Hunt for the Missing Data Type Contact via Email Open Github account in new tab Posted on Mar 02, 2024 A (directed) graph is a set of nodes, connected by arrows (edges). The nodes and edges may contain data. Here are some graphs: All graphs made with graphviz (source) Graphs are ubiquitous in software engineering: Package dependencies form directed graphs, as do module imports. The internet is a graph of links between webpages. Model checkers analyze software by exploring the “state space” of all possible configurations. Nodes are states, edges are valid transitions between states. Relational databases are graphs where the nodes are records and the edges are foreign keys. Graphs are a generalization of linked lists, binary trees, and hash tables.1 Graphs are also widespread in business logic. Whitepapers with references form graphs of citations. Transportation networks are graphs of routes. Social networks are graphs of connections. If you work in software development long enough, you will end up encountering graphs somewhere. I see graphs everywhere and use them to analyze all sorts of systems. At the same time, I dread actually using graphs in my code. There is almost no graph support in any mainstream language. None have it as a built-in type, very few have them in the standard library, and many don’t have a robust third-party library in the ecosystem. Most of the time, I have to roll graphs from scratch. There’s a gap between how often software engineers could use graphs and how little our programming ecosystems support them. Where are all the graph types? As I ran into more and more graphs in my work, this question became more and more intriguing to me. So late last year I finally looked for an answer. I put a call out on my newsletter asking for people with relevant expertise— graph algorithm inventors, language committee members, graph library maintainers— to reach out. I expected to interview a dozen people, but in the end I only needed to talk to four: Zayenz: Former core developer of the Gecode constraint solver, and who has “implemented every graph algorithm there is” Bradford: Author of the Nosey Parker security library and inventor of several new graph algorithms Nicole: Former graph database engineer Kelly: Maintainer on the NetworkX python graph library and compiler developer. After these four people all gave similar answers, I stopped interviewing and start writing. The reasons There are too many design choices So far I’ve been describing directed graphs. There are also undirected graphs, where edges don’t have a direction. Both directed and undirected graphs can either be simple graphs, where there is a maximum of one edge between two nodes, or multigraphs, where there can be many edges. And then for each of those types we have hypergraphs, where an edge can connect three or more nodes, and ubergraphs, where edges can point to other edges. For each possible variation you have more choices to make: do you assign ids to edges or just to nodes? What data can be stored in a node, and what can be stored in an edge? That’s a lot of decisions for a library to make! But wait, do these distinctions matter at all? A simple graph is just a degenerate multigraph, and and undirected edge can be losslessly transformed into two directed edges. A language could just provide directed hyperubermultigraphs and let users restrict it however they want. There are two problems with this. First of all, it changes the interface, like whether various operations return single values or lists. Second, as I’ll discuss later, graph algorithm performance is a serious consideration and the special cases really matter. Kelly raised the example of maximum weight matching. If you know that your graph is “bipartite”, you can use a particular fast algorithm to find a matching, while for other graphs you need to use a slow, more general algorithm. A bipartite graph (source) [It] ties back to the “algorithm dispatch problem.” Given a Problem P, a Graph G, and Algorithms A, B, C to solve P on G… which one do you run? If we don’t know that G is bipartite, and Algorithm C only works on bipartite graphs, how much time can we afford to determine whether or not G is bipartite? — Kelly The perfect graph library would support a lot of different kinds of graphs. But that takes time away from supporting what people want to do with graphs. Graph algorithms are notoriously hard to get right. In this essay, the inventor of Python implemented his own find_shortest_path algorithm. It had to be updated with corrections five times! Every single implementation of pagerank that I compared to was wrong. — Nicole So which algorithms should come with the library? “The amount of things people want to do with graphs is absurd,” Kelly told me. That matches my experience, and the experiences of all my interviewees. It sometimes seems like graphs are too powerful, that all their possibilities are beyond my understanding. “The question is,” Kelly said, “where do you draw the line?” For NetworkX, “the line” is approximately 500 distinct graph algorithms, by themselves making up almost 60,000 lines of code. By comparison, the entire Python standard library, composed of 300 packages, is just under 600,000 lines.2 With all that, it’s unsurprising that you don’t see graphs in standard libraries. The language maintainers would have to decide which types of graphs to support, what topologies to special-case, and what algorithms to include. It makes sense to push this maintenance work onto third parties. This is already the mainstream trend in language development; even Python, famous for being “batteries included”, is removing 20 batteries. Third parties can make opinionated decisions on how to design graphs and what algorithms to include. But then they’re faced with the next problem: once you have a graph interface, how do you represent it? There are too many implementation choices Let’s imagine we’re supporting only barebones simple directed graphs: nodes have identities, edges do not, neither has any associated data. How do we encode this graph? (source) Here are four possible ways a programming language could internally store it: Edge list: [[a, b], [b, c], [c, a], [c, b]] Adjacency list: [[b], [c], [a, b]] Adjacency matrix: [0 1 0; 0 0 1; 1 1 0] A set of three structs with references to each other Different graph operations have different performance characteristics on different representations. Take a directed graph with 100 nodes and 200 edges. If we use an adjacency matrix representation, we need a 100×100 matrix containing 200 ones and 9,800 zeros. If we instead use an edge list we need only 200 pairs of nodes. Depending on your PL and level of optimizations that could be a memory difference of 20x or more. Now instead take a graph with 100 nodes and 8,000 edges and try to find whether an edge exists between node 0 and node 93. In the matrix representation, that’s an O(1) lookup on graph[0][93]. In the edge list representation, that’s an O(|edge|) iteration through all 8,000 edges.3 Graphs with only a few edges are sparse and graphs with almost all edges are dense. The same program may need to do both operations on both kinds of graph topologies: if you’re constructing a graph from external data, you could start out with a sparse graph and later have a dense one. There’s no “good option” for the internal graph representation. And all this trouble is just for the most barebones directed graph! What about implementing node data? Edge data? Different types of nodes and edges? Most third party libraries roughly fall in one of two categories: Offer a single rich datatype that covers all use-cases at the cost of efficiency. NetworkX stores graph as a dict of dicts of dicts, so that both nodes and edges can have arbitrary data.4 Offer separate graph types for each representation, and rely on the user to store node and edge data separately from the graph type. An example of the second case would be Petgraph, the most popular graph library for Rust. Petgraph has graph, graphmap, and matrix_graph for different use-cases. Bradford used Petgraph for Nosey Parker, a security tool that scans for secrets across an entire history of a git repo. His benchmarking graph is CPython, which has 250k commits and 1.3M objects but only a few edges per commit node. He went with an adjacency list. Supporting many representations has a serious downside: you have to do a lot more work to add algorithms. If you write a separate version of the algorithm for each graph representation, you’re tripling or quadrupling the maintenance burden. If you instead write a generic abstraction over polymorphic types, then your library is less performant. One programmer I talked to estimated that a hand-rolled graph algorithm can be 20x faster or more than a generic algorithm. And this gets into every interviewee’s major complaint. Performance is too important A “generic” graph implementation often doesn’t cut it. — Bradford This is the big one. Many, many graph algorithms are NP-complete or harder.5 While NP-complete is often tractable for large problems, graphs can be enormous problems. The choice of representation plays a big role in how fast you can complete it, as do the specifics of your algorithm implementation. Everyone I talked to had stories about this. In Nosey Parker, Bradford needed to reconstruct a snapshot of the filesystem for each commit, which meant traversing the object graph. None of the four provided graph walkers scaled to his use case. Instead he had to design a “semi-novel” graph traversal algorithm on the fly, which reduced the memory footprint by a factor of a thousand. I was able to get working a proof of concept pretty quickly with [petgraph], but then… this is one of those cases where the performance constraints end up meeting reality. — Bradford Zayenz raised a different problem: what if the graph is simply too big to work with? He gave the example of finding a solution to the 15 puzzle. This is done by running a A* search on the state space. A state space with over 20 trillion states. If you generate all the nodes, you’ve lost already. — Zayenz Zayenz oversaw one research project to add graphs to the Gecode constraint solver. They eventually found that a generic graph type simply couldn’t compete with handpicking the representation for the problem. Even graph databases, designed entirely around running complex graph algorithms, struggle with this problem. Nicole, the graph database engineer, told me about some of the challenges with optimizing even basic graph operations. If you’re doing a traversal, you either have to limit your depth or accept you’re going to visit the entire graph. When you do a depth search, like “go out three steps from this and find the path if it exists”, then you’re just committing to visiting quite a bit of data. — Nicole After leaving that job, she worked as a graph query performance consultant. This usually meant migrating off the graph database. She told me about one such project: to speed the graph queries up, she left one computation as-is and rewrote the rest as MapReduce procedures. “Which was a lot harder to understand,” she said, “But would actually finish overnight.” All of this means that if you have graph problems you want to solve, you need a lot of control over the specifics of your data representation and algorithm. You simply cannot afford to leave performance on the table. It was unanimous So, the reasons we don’t have widespread graph support: There are many different kinds of graphs There are many different representations of each kind of graph There are many different graph algorithms Graph algorithm performance is very sensitive to graph representation and implementation details People run very expensive algorithms on very big graphs. This explains why languages don’t support graphs in their standard libraries: too many design decisions, too many tradeoffs, and too much maintenance burden. It explains why programmers might avoid third party graph libraries, because they’re either too limited or too slow. And it explains why programmers might not want to think about things in terms of graphs except in extreme circumstances: it’s just too hard to work with them. Since starting this research, I’ve run into several new graph problems in my job. I still appreciate analyzing systems as graphs and dread implementing them. But now I know why everybody else dreads them, too. Thank you for reading! Thanks to Predrag Gruevski for research help, Lars Hupel, Predrag Gruevski, Dan Luu, and Marianne Bellotti for feedback, and to all of the people who agreed to do interviews. If you liked this post, come join my newsletter! I write new essays there every week. I train companies in formal methods, making software development faster, cheaper, and safer. Learn more here. Appendix: Languages with Graph Types Graph Querying Languages Graph querying languages (GQLs)6 are to graph databases what SQL is to relational databases. There is no widely-used standard, but two of the most popular are SPARQL for querying RDF triples and Neo4j’s cypher. Ironically, GraphQL is not a graph querying language, instead being named for its connection to the Facebook Graph Search. I considered graph databases themselves mostly distinct from graphs in programming languages, but their query languages show how graphs could work in a PL. The main difference between all GQLs and SQL is that the “joins” (relationships) are first-class entities. Imagine a dataset of movies and people, where people act in, direct, or produce movies. In SQL you’d implement each relationship as a many-to-many tables, which makes it easy to query “who acted in movie X” but hard to query “who had any role in movie Y, and what was that role”. In SPARQL relationships are just edges, making the same query easy. PREFIX mv:SELECT ?person ?role WHERE { ?person ?role mv:casablanca. } Cypher has a similar construct. GQLs can also manipulate edges: reverse them, compose them together, take the transitive closure, etc. If we wanted to find all actors with some degree of separation from Kevin Bacon, we could write PREFIX mv:SELECT ?a WHERE { mv:kbacon (:acted_in/^:acted_in)+ ?a. # a/b = join two lookups # ^a = reverse a # a+ = transitive closure } SPARQL cannot give the length of the path nor do computation along the path, like collecting the chain of movies linking two actors. GQLs that support this are significantly more complicated. My main takeaway from looking at GQLs is that there’s a set of useful traversal primitives that a PL with graph support would need to provide. Interestingly, the formal specification language Alloy has all of these primitives for its “relation” datatype. For this reason I find working with a graph representation in Alloy much easier than in a proper programming language. That said, these all work with labeled edges and may not work for other graph representations. Mainstream Languages with Graphs in the Standard Library Python added a graphlib in 2020. Based on the discussion here, it was because topological sorting is a “fundamental algorithm” and it would be useful for “pure Python implementations of MRO [Method Resolution Order] logic”. Graphlib has no other methods besides TopologicalSorter, which only takes graphs represented as node dicts. Unusually, the direction of the node dict is reversed: the graph a -> b is represented as {b: [a]}. As of 2023, nothing in CPython uses graphlib and there are fewer than 900 files referencing it on Github. By comparison, another package added in 2020, zoneinfo, appears in over 6,000 files, and the term def topological_sort( appears in 4,000. I’d guess a lot of these are from before 2020, though. Some skimming suggests that all of these custom topological sorts take different graph representations than graphlib, so they wouldn’t be convertable regardless. Graph representation matters. There are two other languages I found with graph types: Erlang and SWI-Prolog. I don’t know either language and cannot tell when they were added; with Erlang, at least, it was before 2008. I reached out to a person on the Erlang core language committee but did not hear back. Graph languages Programming languages where “everything is a graph” in the same way that everything in bash a string and everything in lisp is a list. Some examples include GP2 and Grape. Based on some correspondence with people in the field, right now this is still highly academic. Mathematics Software Languages Mathematica, MATLAB, Maple, etc all have graph libraries of some form or another. I am not paying the thousands of dollars in licensing needed to learn more. No, really. Hash tables are bipartite graphs. This was used to prove performance of cuckoo hashing operations. [return] I derived both computations with cloc 1.96. I ran cloc in networkx/networkx/algorithms (56989) and in cpython/Lib (588167). The whole networkX library is ~90,000 lines of code. [return] You can make this more efficient by keeping the edge list sorted and doing an O(log(|e|)) binary search, at the cost of making edge insertions more expensive. [return] NetworkX has functions to convert graphs into other representations but not for working with those representations directly. [return] 14 of the 21 canonical NP-complete problems are graph problems. [return] Not to be confused with the GQL language, a proposed GQL standard that’s still under development. [return] Categories: Programming, Interviews",
    "commentLink": "https://news.ycombinator.com/item?id=39592444",
    "commentBody": "The hunt for the missing data type (hillelwayne.com)553 points by todsacerdoti 17 hours agohidepastfavorite197 comments graphviz 14 hours agoGraphviz has its own foundation graph library, that's not used by any other project. It has its good and bad points. Based on that experience, we had our very own second-system-syndrome experience. We decided our graph library should be modular, type safe, and efficient. (These properties came up in the comments here, too.) This is probably just a variation of \"good, fast, cheap - pick any two.\" By modular, want to write collections of graph algorithm libraries that are developed and even compiled independently. By type safe, we mean we want to detect programming errors during compilation, or link time at the latest. We don't want programs to throw runtime errors like \"your node does not have a color attribute\". By efficient, we mean that accessing an attribute of a graph is as cheap as a field in a C struct. (So, we don't want to carry around external hash table, or do a lot of string conversions, for instance.) You can argue about whether these things are worth the price or even make sense, but that's what we wanted. We had some famous C++ creators in our lab, so we figured we could get help, and we were willing to give C++ another chance. Gordon Woodhull, who had been an intern and kept working for us, is a brilliant programmer, and wrote an implementation of this kind of graph library working in templated C++. It's even published at https://www.dynagraph.org/ via sourceforge. The rest of us were not really sure we could ever understand how the code worked, so we had a code review with said famous C++ inventors. There were a lot of screens of code, and chinstroking, and greybeards pronounced \"That would probably work.\" We knew we might have gone over the complexity cliff. (Let's not even talk about compile-time template errors, where one error fills an entire screen with details that only a... C++ inventor could love.) It was our fault, not anyone else's, and Gordon kept plugging away and even made all the dynamic graph layout stuff work, in Microsoft OLE, too. In hindsight it was probably our own Project Xanadu. While we got lost in this, a lot of things like Gephi (Java) and NetworkX and NetworKit (python) happened. Also, John Ellson, a very talented software engineer who had written parts of Graphviz, revitalized the main effort. reply samsquire 40 minutes agoparentThank you for graphviz. I use graphviz dot syntax, parsing with networkx to plan expensive tool execution and the graph structure lets me automatically paralellize. reply Everdred2dx 4 hours agoparentprevI love comments like this. Thank you for sharing. reply transitionnel 14 hours agoparentprevThat all sounds fantastic! reply ylow 13 hours agoprevI think this is because a graph is not a data-structure nor a data-type. It is really an abstraction. Fundamentally, all I need to define a graph is a set of vertices v \\in V and function Neighbors(v). And that really is all is needed for the most foundational set of graph algorithms. Everything else are case-by-case constraints. Does A->B imply B->A? is the node set partitionable with certain constraints? Are there colors? labels? To make things even more general I can go up one level and consider the hypergraph. In which case I just have a set of vertices, and a set of sets of vertices. This can be represented in a myriad of different ways depending on what you are interested in. Of which (non-hyper) graph is simply a special case. An alternative way to think about it perhaps from the database perspective, is that its a query optimization and indexing problem. Depending on what questions you want to ask about the graph, there will be different ways to represent the graph to answer the question better. Just like there is not one way to represent the abstraction called \"Table\", there is not one way to do \"Graph\" either. It really depends on the questions you care about. reply gloftus 11 hours agoparentYes, graphs are ubiquitous because they are so abstract. They live on the same level of abstraction as pure numbers. There are useful \"numerical\" libraries that exist, and by analogy I think you could say there are also useful \"graphical\" libraries that exist. But we don't really have \"number\" libraries, and we don't really have \"graph\" libraries, because those concepts are a bit too abstract to write APIs against. reply kragen 11 hours agorootparentit's true that numbers are very abstract, which is what makes it so easy to design apis for them the python runtime includes four built-in number types (small integer, arbitrary-precision integer, float, and complex) and the python standard library includes two more number types (decimal and fractions), and one of the most popular non-standard libraries for python is numpy, which provides some other kinds of numbers such as single-precision floats, vectors, and matrices. other systems like pari/gp have number libraries that provide other kinds of numbers, such as p-adic numbers and galois field elements the only programming languages i've ever used that didn't have 'number' libraries were esoteric languages like brainfuck and the lambda calculus reply ysofunny 8 hours agorootparentnumbers have all of mathematics as a background which is what makes it so easy to design apis for them graphs are a much newer development, I think there's a very deep connection between category theory and graphs in general (and also computers make both much more useful somehow) lambda calculus can be used to define numbers but it's a wonky construction, it's reminiscent of how sets can also be used to define numbers. reply kragen 8 hours agorootparentthat seems reasonable reply dataflow 9 hours agoparentprev> Fundamentally, all I need to define a graph is a set of vertices v \\in V and function Neighbors(v). Even that is severely overconstrained. It doesn't allow multiple edges to the same neighbor! reply saghm 3 hours agorootparentMaybe the naming would be a little weird for that use case, but they didn't specify what the output of `Neighbors(v)` is; I don't see any reason why it couldn't return a multiset or a relation (w, c) where `c` is the number of connections between `v` and `w` reply lou1306 1 hour agorootparentprevWell to be fair, that constraint is also part of the mathematical definition of a graph, where the set of edges E is a binary relation over vertices V (i.e., a subset of V x V). You'd need either a multiset or a labelled relation (i.e., a subset of V x L x V for some set of labels L) to overcome that. reply obi1kenobi 16 hours agoprevAs someone who did a lot of work with graphs, \"why don't programming languages have a built-in graph data type?\" is a question I’ve been asked a million times. I'm thrilled I'll be able to point folks to a much more in-depth analysis like the one here, instead of saying some variation of \"it's really hard to do it well\" and having them just take my word for it. reply dataflow 9 hours agoparent> \"why don't programming languages have a built-in graph data type?\" What I find a little funny about that question is that people miss the fact that there isn't even a tree data structure in most languages. Most languages have static arrays, dynamic arrays, linked lists, and... that's it as far as structural types go. Everything else (BSTs, hashtables, etc.) is some semantic abstraction that hides some capabilities of the underlying structure, not a purely structural representation. reply bloaf 6 hours agoparentprevI had an opposite experience. I was doing some new-to-me graph work in tcl and had just assumed the tcl standard library wouldn't have any graph algorithms. Come to find out I was wrong, and ended up saving myself a bunch of time not having to re-invent the wheel: https://core.tcl-lang.org/tcllib/doc/trunk/embedded/md/tclli... reply jjice 16 hours agoparentprevI used to think that since graphs are such a broad data structure that can be represented in different ways depending on requirements that it just made more sense to implement them at a domain-ish level (the article mentions this in the \"There are too many implementation choices\" section). Then I saw Petgraph [0] which is the first time I had really looked at a generic graph library. It's very interesting, but I still have implemented graphs at a domain level. [0] https://github.com/petgraph/petgraph reply TachyonicBytes 13 hours agorootparentWhy wouldn't an abstract `Graph` type and specific implementations of that work? Like Java has for `Set` and various implementations? reply skybrian 11 hours agorootparentFor importing and exporting data, it often makes more sense to use something like a table or a tree rather than a graph. (Like a CSV file or a JSON file.) So it's not clear what the interface would do. What methods should there be? Again, there are too many choices, and a Graph interface often isn't the best way to represent a view of some subset of a graph. reply nyrikki 10 hours agorootparentTo add to this, recursively enumerable is the same as semi-decidable, and that only gets you to finite time. One of the big reasons to fight to make dependencies DAG like is because exhaustive search gets you to exponential time. NP-complete, NP-hard are easy to run into with graphs. Graph k-colorability, finding Hamiltonian cycles, max cliques, max independent sets, and vertex cover on (n)vertex graphs aren't just NP, they have no sub-exponential time algorithms. Finding those subgraphs is often impractical. reply gryn 12 hours agorootparentpreveven at that level of abstraction there are many flavors of \"graphs\". what some people call graph are actually hypergraphs, or attributed graphs. reply nickpsecurity 16 hours agorootparentprevYou might say graphs are an abstract concept, solving our problems requires specialized graphs, and so we just use or build the kind we need. reply sanderjd 13 hours agorootparentYeah this is exactly how I think of it. I think \"graphs\" are just at a different level of the abstraction hierarchy than the data structures they're often grouped together with. This is even further up the abstraction hierarchy, but to illustrate the point, nobody really wonders why languages don't ship with a built-in database implementation. And it's the same basic reason as with graphs; one size doesn't fit most. reply paulddraper 14 hours agoparentprev> \"it's really hard to do it well\" More importantly, there are a lot of tradeoffs. Virtually every language offers a hash map. You can roll your own to outperform in an individual circumstance, the default works pretty well. You can't really do that with a graph. Maybe if you offered a bunch of graph types. --- PS. Bit of trivia: Java's HashMap is a bit different from almost every other language in that it lets you tune the load factor. reply adastra22 13 hours agorootparent> You can't really do that with a graph. Maybe if you offered a bunch of graph types. And so why isn't this the solution? Most languages support both hash map (fast lookup) and balanced tree (ordered entries) primitives, even though they both implement the \"associative map\" container type. Can't we have 2, 3, 5, or even 8 different graph types? reply masklinn 12 hours agorootparent> Can't we have 2, 3, 5, or even 8 different graph types? That’s what graph libraries do, look at petgraph. You’ve got a bunch of graph implementations, and a bunch of algorithms over them. reply sltkr 13 hours agorootparentprev> Java's HashMap is a bit different from almost every other language in that it lets you tune the load factor. I'm not sure that's particularly unusual. For example, C++ supports this too: https://en.cppreference.com/w/cpp/container/unordered_map/ma... reply karmakaze 12 hours agorootparentprevIt's also different in that each hash bucket can use a Red-Black tree when there's many entries. reply somat 14 hours agoparentprevThis is a super naive take. but I would consider the pointer the be the native graph type. What is wanted is not a graph type but the tooling to traverse graphs. reply Nevermark 10 hours agorootparentGeneralized pointers (RAM addresses, disk storage locations, network locations, etc.) would be the general way to implement explicit literal graphs. Other graphs, that are partly or wholly computed or recomputed as needed from other relationships, could be considered \"implicit\" graphs and can be implemented many other ways. The fact that graphs can be any combinations of literally or dependently defined, static or dynamic defined, would add even more complexity to any truly general graph library. reply thfuran 8 hours agorootparentYou could say the same of Lists. reply Nevermark 1 hour agorootparentWell, of course! Lists, trees, arrays, structures, etc. are specialized graphs. reply cwillu 13 hours agorootparentprevPeriod should probably be a comma; I read this comment a couple times before it made sense, rather than calling the grand parent super naive. reply alanbernstein 14 hours agorootparentprevMaybe a pointer is better described as equivalent to a half-edge? reply andyferris 12 hours agorootparentI think this might be kinda right. Take rust. Tree edges use Box, and DAG edges use Arc or Rc. Rust doesn’t really help with building non-DAG graphs as native data types - you can drop back to using “handles” for more generic graphs. And I actually think that is kinda fair. Languages and standard libraries should support trees and DAGs out of the box. Perhaps Hillel is right that more complex graphs should be in a user-space library or something. reply computerdork 12 hours agorootparentprevHmm, think you're taking the title of the article a little too literally, and focusing on just the \"data type\" aspect. Yeah, the article itself indirectly agrees with you that pointers are often how graphs are implemented: \"Graphs are a generalization of linked lists, binary trees, and hash tables,\" which are data-structures often implemented by pointers. Yeah, the article is really saying what you're saying, that building a graph library (\"tooling\") is very complicated. But, maybe I'm misunderstanding what you're saying?? reply nine_k 14 hours agorootparentprevIt's a bit like saying that a byte is a native number type, providing comparison to zero and addition as the only operations, and asking to figure the rest yourself. I mean, it's technically sufficient, but I'd not call it proper support on a language level. reply boothby 13 hours agorootparentprevI actually brought this hot take up in my conversation with Hillel -- something along the lines of \"void * * is the canonical native graph representation in C.\" reply atoav 6 hours agoparentprevPuthon has one, as mentioned, but as a non CS-person I never encountered any problem in my programming-life that so fundamentally called for graphs that I needed one, not did I had so much fascination for graphs that it was a tool I wanted to force onto my problems. I guess it is just that there are many problems that can be solved incredibly well without graphs and not that many where graphs outshine everything else so clearly that people would use them. That being said, convince me of the opposite and I am happy. reply sayrer 13 hours agoparentprevI think it's because you have to think it through when they get too big for one machine. A lot of those so-called \"NoSQL\" databases are really meant to represent graph databases (think DynamoDB) in an adjacency list. I've had success with Gremlin and JanusGraph, but it's a really messy space. It's not a programming language problem in my opinion, but a distributed systems one. reply rhelz 15 hours agoprevYa, the central obstacle is that: 1. for simple and small graph problems, a simple vector-of-vectors adjacency list is easy enough to code up. 2. For complex and huge graph problems, the only way to get performant solutions is to tailor the graph implementation to the specific details of the problem to be solved. And its hard to see what kind of language support would help, other than just having a super-smart compiler which could analyze the code and determine whether an adjacency list, matrix, 3d array, etc was the best way to implement it. That's the kind of optimization which we won't see in compilers for a while. It's another instance of the phenomenon which Strousroup noticed: we are really good at code sharing of small things like vectors, and of large things like operating systems. Its the middle-sized problems we are bad at. reply js8 11 hours agoparent> we are really good at code sharing of small things like vectors, and of large things like operating systems. Its the middle-sized problems we are bad at. Interesting. But I am not sure we are good at sharing small things - every programming language has its own implementation of vectors. Within one language ecosystem, the API of a vector is small, and that's probably what makes it easy to share. For operating systems, the API is relatively small compared to the internal complexity of the OS. This is also true for libraries for numerical problems, which are also easily shared. But the more you want to customize things (e.g. share a complicated data structure), this complicates the API and inhibits sharing. So it seems to this is determined by the surface area (relative size of the API) of the thing being shared. reply twoodfin 13 hours agoparentprevAnd its hard to see what kind of language support would help, other than just having a super-smart compiler which could analyze the code and determine whether an adjacency list, matrix, 3d array, etc was the best way to implement it. That's the kind of optimization which we won't see in compilers for a while. I’m not so sure? Looking at an algorithm against an abstract graph type, then filling in the implementation to optimize for the particular algorithm seems right in the wheelhouse of code-specialized LLM’s. reply rocqua 11 hours agorootparentMy experience with cipher is that the query optimizer doesn't know enough about the graph to pick up on trivial optimizations. This can't be fixed without a way to tell the optimizer about those properties, and even just dreiging a language to tell the optimizer those things is difficult. Just an LLM looking at your query isn't going to cut it. It will need to take your actual data into account. reply rhelz 13 hours agorootparentprevGood point. The game has really changed in terms of what kinds of programs we can write now. Perhaps it's too pessimistic to not expect these sorts of optimizing compilers soon. Sounds like a good research opportunity to me. reply criloz2 15 hours agoprevGraph drawing tools are also very underwhelming, they work pretty good for small graphs until you have something like 500 nodes or more, then eventually their output becomes complete incompressible or very difficult to look at it, they miss the ability to automatically organize those graph in hierarchical structures and provide a nice interface to explore them, we are used that everything around us have some kind of hierarchy, I think that is the same kind of problem that will need to be solved in order to have a generic graph data type, also this kind of thing will need to be implemented at the compiler level where those graph generic algos will be adapted to the generated hierarchy of structures, and if you add a theorem prover that can check that certain subgraph will always have certain structures you can statically generated those procedures and for the other super graphs those methods will be generated dynamically at runtime. So whoever solve the problem for generic graph drawing will have the ability or the insight to implement this too. reply kjqgqkejbfefn 12 hours agoparent>Graph drawing tools It's hard Graphviz-like generic graph-drawing library. More options, more control. https://eclipse.dev/elk/ Experiments by the same team responsible for the development of ELK, at Kiel University https://github.com/kieler/KLighD Kieler project wiki https://rtsys.informatik.uni-kiel.de/confluence/display/KIEL... Constraint-based graph drawing libraries https://www.adaptagrams.org/ JS implementation https://ialab.it.monash.edu/webcola/ Some cool stuff: HOLA: Human-like Orthogonal Network Layout https://ialab.it.monash.edu/~dwyer/papers/hola2015.pdf Confluent Graphs demos: makes edges more readable. https://www.aviz.fr/~bbach/confluentgraphs/ Stress-Minimizing Orthogonal Layout of Data Flow Diagrams with Ports https://arxiv.org/pdf/1408.4626.pdf Improved Optimal and Approximate Power Graph Compression for Clearer Visualisation of Dense Graphs https://arxiv.org/pdf/1311.6996v1.pdf reply samatman 9 hours agoparentprevSome algorithms do better at this than others, but \"make a good diagram of a graph\" is an intelligence-complete problem in the general case. Two people might render structurally-identical graphs in very different ways, to emphasize different aspects of the data. This is in fact a similar problem to the \"generic graph algorithm\" and \"generic graph data structure\" problems. Graphs straddle the line between code and data. For instance, any given program has a call graph, so in a real sense, the \"generic graph algorithm\" is just computation. reply nine_k 14 hours agoparentprevIdeal things are often tree-like. Real-world structures are usually DAGs if they are nice and well-behaved. Making things planar, or almost planar with few crossings and nice clustering of related nodes, is usually hard past a couple dozen nodes :( reply hobofan 14 hours agoparentprev> we are used that everything around us have some kind of hierarchy I think the problem is more that we are used to the illusion/delusion that everything is hierarchical. The problem that we then encouter is with graph drawing is that it has to try and reconcile the fact that things in practice are rarely really hierarchical, and it's hard to draw those lines of where the hierarchies are with mathematical rigor. And that problem gets worse and worse the less properties you are allowed to assume about the underlying graph structure (connectedness, cyclic/acyclic, sparse/dense). In practice when you want build a UI that interacts with graphs it's often feasible to determine/impose one or two levels of meta-hierarchy with which you can do clustering (allows for reducing layout destroying impact of hairball nodes + improves rendering performance by reducing node count) and layout with fCOSE (Cytoscape.js has an implementation of that). reply swagasaurus-rex 12 hours agoparentprevThe illustrations I've seen of neural networks really highlights the sheet incomprehensibility of visualizing large graphs reply kjqgqkejbfefn 11 hours agorootparenthttps://www.microsoft.com/en-us/research/wp-content/uploads/... reply cloogshicer 10 hours agorootparentSuper interesting paper on an alternative way to render graphs. Thanks for posting! reply criloz2 9 hours agorootparentprevWow, this is really amazing, thank you reply kajic 3 hours agoprevOne of the complications described by the author is performance. Personally, I find stdlib graph libraries extremely useful even if their performance is poor because it’s often the case that my dataset is small enough, and even if performance turns out to be an issue, first spending time on the problem with a underperforming graph library is a very worthwhile exercise before trying to write my own optimized implementation. By analogy, many programming languages are far from being the fastest, but they can nevertheless be very useful. That said, I’m not surprised performance came up in interviews with experts; they probably have tons of interesting performance-related stories to tell from their extensive work on graphs. reply hobofan 16 hours agoprevI think the post mostly answers the questions \"why are graph _algorithms_ not better supported in programming languages\", with a focus that is much more on \"big data\" graph processing than graph support in general. I think if you look at graph support in general you are also looking at wider questions, like \"why are OGMs (Object Graph Mappers) not as popular as ORMs\" and \"why is JSON so prevalent while RDF (or another low-level graph serialization) isn't\"? And I think in the end it comes down to historic reasons (RDF emerged a bit too early and never evolved and accrued an ecosystem of horrible academic standards and implementations), and a graphs having just a smidge more of inherent complexity in implementation and learning curve that just doesn't scale well across many developers. ------ I also wouldn't put too much weight on the \"Graph Querying Language\" part of the article. It sadly reads like exactly the marketing copy you would read from Neo4J or SPARQL enthusiasts that haven't tried building a product on top of it. > The main difference between all GQLs and SQL is that the “joins” (relationships) are first-class entities. Joins are first-class entities in SQL. They even have their own keyword (hint: it starts with J and ends with OIN) ;) If you also go to the lower levels of any graph query language and look at it's query plans you'll notice that there isn't any meaningful difference to that of one you'll find in an SQL based query. The standardization of GQL[0] as an SQL extension is evidence for that. > In SPARQL relationships are just edges, making the same query easy. SPARQL is easy if you need to do exact path traversals. If you try to do anything sophisticated with it (like you would in the backend of a webapp), you'll quickly run into footguns like joins with unbound values and you accidently join your whole result set away. [0]: https://en.wikipedia.org/wiki/Graph_Query_Language reply lmm 11 hours agoparent> Joins are first-class entities in SQL. They even have their own keyword (hint: it starts with J and ends with OIN) ;) Having its own keyword is pretty strong evidence that something isn't first-class (e.g. typeclasses are not first-class in Haskell; control flow is not first-class in most programming languages). reply eyelidlessness 7 hours agorootparentI think OP is using “first class” as in “an explicit semantic affordance”, and you seem to be using “first class” as in “a supported operand” (or similar, feel free to correct me if I’m misunderstanding). In which case both points are right, albeit orthogonal. reply dustingetz 16 hours agoprevElectric Clojure uses Clojure itself (s-expressions) as a graph authoring syntax, using a macro to reify dataflow through a reactive client/server system (here the use case is full stack user interfaces but the idea generalizes) https://github.com/hyperfiddle/electric (I'm the founder). IMO, the answer to the question \"Where are all the graph types?\" is: the graph authoring DSL needs to express scope, control flow and abstraction, which essentially makes it isomorphic to a programming language, freed of its evaluation model. In Python and Typescript, embedding a complete programming language is something that's rather hard to do! Also see my blog post \"Four problems preventing visual flowchart programming from expressing web applications\" https://www.dustingetz.com/#/page/four%20problems%20preventi... reply fatherzine 12 hours agoprevWhat an awesome article. Kudos to the author! On the core observation \"there are too many implementation choices\", that is not quite right. True, the author mentions 4, and there are further variations. In practice, a library can: 1. Implement all suitable graph representations. 2. Implement algorithms tailored to the representation(s) that offer the highest performance. 3. Provide transformations from one representation to another. This is O(#representations), trivial to implement and trivial to use. Quite fair workload for both maintainers and users. 4. Bonus, provide import / export transformations from / to common standard library datatypes and idioms. Memory and transformations are cheap, 99% of use-cases would likely find the overhead of transforming data, both in RAM and CPU, negligible. Edit: \"the harsh truth of working at Google is that in the end you are moving protobufs from one place to another.\" -- https://news.ycombinator.com/item?id=20132880 reply josephg 12 hours agoparentSounds like the makings of a huge library that I’m not sure I’d even use in my work. I use graphs heavily in my work, and my experience matches the people the author interviewed. We always end up reimplementing graphs because: - Performance matters, and no off the shelf graph library I’ve seen can take advantage of many of the regularities in our particular data set. (We have an append-only DAG which we can internally run-length encode because almost all nodes just have an edge pointing to the last added item). - I haven’t seen any generic graph library which supports the specific queries I need to make on my graphs. The big one is a subgraph diffing function. - Writing something custom just isn’t much work anyway! Graphs are way simpler to reimplement than btrees. You can have a simple graph implementation in tens of lines. Our highly optimised library - with all the supporting algorithms - is still only a few hundred lines of code. I think it would be handy to have ways to export the data into some standard format. But eh. I think pulling a library in for our use case would add more problems than it would solve. reply jkaptur 13 hours agoprevI've often wondered about a missing application: \"Excel for graphs\". Just like Excel for tabular data, it would support RAM-sized data (enough to require a computer, but not so much that you need a data center), implement lots of algorithms and visualizations \"well enough\", and require no programming skill to operate. As the article says, a lot of our real-world problems are graph problems - why are programmers the only ones who should have the tools to solve them? reply empiko 12 hours agoparentYeah, I feel like the article is too quick with its conclusions. Many other problems can be made arbitrary complex and difficult with additional requirements. But there are still data structure and standard libraries to provide good enough experience that fits most use-cases. And if you need something extra spicy you need to build a custom solution. The article claims that graphs are often just too big, but yeah, if you ask people who are actively working on graph algorithms they might have that sort of experience. But most programmers and users probably only work with really small graphs. reply roenxi 57 minutes agoparentprev> As the article says, a lot of our real-world problems are graph problems The article struggles to back that up though. Eg, it notes that the internet can be modelled with a graph. Undeniably true. But so what? The internet can be represented as many different things and it is unclear that representing it as a graph has any generically useful engineering implications. There is an argument that is just as good that representing the internet as a neural-network (ie, a black-box matrix-encoded function of arbitrary inputs to coherent outputs) is the ideal representation for getting useful info out of it. Maybe for someone like Google that is a billion-dollar idea (even then though, it might not be - I don't know if they represent their index as a graph or not). But the internet overall isn't much of a graph problem to many other people, and representing it as a graph doesn't solve much. It is rare to see someone solving a real-life problem on paper as a graph. Using tables happens all the time. Graphs are common, graph problems are uncommon. reply jonahss 7 hours agoparentprevI agree. And I think the key to this is VR. Another comment in this thread is about how hard graphs are to visualize, but a 3D interface gives you a lot more room. When VR hype began I thought \"well what's the excel of VR?\". Microsoft's answer was \"2D spreadsheets floating in 3D space\". What nonsense. I think graphs. email my username at gmail.com if anyone is interested in exploring this together! reply rb-2 16 hours agoprevI wonder if it would be possible to mathematically define (in a theorem proving language like Coq) a bunch of accessor methods as well as a bunch of implementation primitives and then \"compile\" a custom graph implementation with whatever properties you need for your application. Some accessor methods will be very efficient for some implementations and very inefficient for others, but every method will still be available for every implementation. Profiling your application performance can help adjust the implementation \"compiler\" settings. Ironically, this is a graph problem. reply kevindamm 15 hours agoparentThis sounds like Partial Evaluation and the Futamura Projection. The research around that shows that your interpreter determines the shape of the compiled output, so a formal proof of its application isn't necessary, if the $mix$-equivalent has the appropriate syntax and semantics for graph processes in its design. I know this has been done for procedural languages and for declarative logical languages but I'm not aware of something like this specifically for graph processing and highly specialized code generation of graph processing. I wouldn't be surprised if Mix has been extended for this already, even if it has I'm sure there is still value in it. reply JonChesterfield 15 hours agoparentprevI think this is a worthwhile direction. For example, I'd like to program against a sequence abstraction. When sort is applied to it, I hope it's a vector. When slice or splice, I hope it's some sort of linked structure. Size is as cheap as empty for the vector but much more expensive for a linked list. It should be possible to determine a reasonable data representation statically based on the operations and control flow graph, inserting conversions where the optimal choice is different. The drawback of course is that people write different programs for different data structures. Knowing what things are cheap and what aren't guides the design. There's also a relinquishing of control implied by letting the compiler choose for you that people may dislike. As an anecdote for the latter, clojure uses vectors for lambda arguments. I thought that was silly since it's a lisp that mostly works in terms of seq abstractions, why not have the compiler choose based on what you do with the sequence? The professional clojure devs I was talking to really didn't like that idea. reply samatman 9 hours agorootparentClojure uses vector syntax for lambda arguments. `read` sees a vector. What comes out of eval is a lambda. Does a Vector get built in the process? You'd have to check, my bet would be that the argument list spends a little while as a Java Array, for performance reasons, but that a Clojure Vector is not actually constructed. reply andoando 15 hours agoparentprevIve been thinking about something like this. A mathematical definition of a function such that we can search it. Imagine we had something like \"Find a function that fits this signature -> Input arr[numbers] out-> for every x in arr, x2>x1. reply lupire 13 hours agorootparentThat's https://hoogle.haskell.org/ plus dependent types (data constraints). Without human provided dependent typing, the search engine would be almost as hard to write as a system to directly generate the code you need. reply rdtsc 13 hours agoprev> There’s a gap between how often software engineers could use graphs and how little our programming ecosystems support them. Where are all the graph types? They've been there for quite a while :-) https://www.erlang.org/doc/man/digraph.html https://www.erlang.org/doc/man/digraph_utils And if you want to do some set theoretical stuff you're covered as well: https://www.erlang.org/doc/man/sofs.html reply DylanSp 10 hours agoparentErlang's briefly mentioned at the end of the article: > There are two other languages I found with graph types: Erlang and SWI-Prolog. I don’t know either language and cannot tell when they were added; with Erlang, at least, it was before 2008. I reached out to a person on the Erlang core language committee but did not hear back. reply rdtsc 10 hours agorootparentOh I did miss that. Thanks for pointing it out. reply ogogmad 47 minutes agoparentprevHow flexible and performant is that in different situations? reply montmorency88 16 hours agoprevI'd highly recommend Erwigs FGL library in Haskell as a really nice example of a generally performant graph data structure that is easy to work with. The API feels like working with lists because you are essentially consing contexts(node, neighbours) into a list of contexts that form your graph. Many standard graph algorithms are then built up from depth or breadth first search and you can compose really succinct programs to manipulate the graph. Graphs are labelled with any Haskell data structure and there is a graphviz library complementary to FGL to generate dot files which can be rendered according to the data carried in the node labels. Often in an application you want to both perform computations on your graph and render a visualization simultaneously to the end user or for debugging and in FGL you minimise duplication of application and rendering logic. reply tikhonj 15 hours agoparentFGL is a great example of how to make a \"nice\" high-level graph interface suited for functional programming. I'm a big fan. But it's orders of magnitude too slow and memory-inefficient for performance-sensitive graph computations—if you have even moderately sized graphs and graph algorithms are a bottleneck, you'll need to use something else, and probably something domain-specific. Given the way the interface works, I don't think you could have a high-performance version that would scale to large graphs. In my experience this leaves FGL in an awkward spot: on the one hand, it isn't sufficient for heavy-duty graph processing; on the other, if you don't need fancy high-performance graph algorithms, chance are that encoding your problem as a graph is going to be more awkward than defining some domain-specific types for what you're doing. Graphs are such a general structure that they're usually the wrong level of abstraction for higher-level domain-specific logic. Of course, sometimes you're writing graph code specifically and you need a nice way to express your graph algorithms without worrying about performance. In that case, FGL is great. I wrote a tutorial about using it to [generate mazes][1] and it helped me express the algorithms better than I would have been able to do without it. But that still leaves it as too narrow for something to be \"the\" graph representation in a language's standard library. [1]: https://jelv.is/blog/Generating-Mazes-with-Inductive-Graphs/ reply Chris_Newton 13 hours agorootparentBut it's orders of magnitude too slow and memory-inefficient for performance-sensitive graph computations—if you have even moderately sized graphs and graph algorithms are a bottleneck, you'll need to use something else, and probably something domain-specific. This seems a little pessimistic to me. There are plenty of application domains that can be conveniently represented using graphs where you might have thousands of nodes and edges — which is what I’d characterise as “moderately sized” — and your needs might only extend to relatively simple and efficient graph algorithms. FGL is excellent in this kind of scenario. If you do need the kind of algorithms that explode in complexity then even a representation a couple of orders of magnitude more efficient won’t help you much either. Big-O is the thing that is going to spoil your day in this story, not the constant factor. Some problems simply don’t have convenient fast solutions and ideally with those you find a way to change the representation so the original problem doesn’t arise in the first place. It’s true that there’s also a zone where you have significantly larger graphs but still only need computationally tractable algorithms, and in that case the overheads of a library like FGL become a factor in what is viable. I also don’t disagree with you (and Hillel in the original piece) that it would be difficult to define comprehensive graph functionality to include in a standard library when there are so many different trade-offs involved. A good — and not entirely unconnected — analogy might be calculating with matrices. It’s convenient to have support for simple but widely useful cases like 3x3 and 4x4 built into your language or standard library. However, once you’re solving systems with hundreds or thousands of rows, you probably want more specialised tools like BLAS/LAPACK, and the structure of your matrix and how you can decompose it start to matter a lot more. reply michelpp 11 hours agorootparent> you probably want more specialised tools like BLAS/LAPACK The GraphBLAS and LAGraph are sparse matrix optimized libraries for this exact purpose: https://github.com/DrTimothyAldenDavis/GraphBLAS https://github.com/GraphBLAS/LAGraph/ reply montmorency88 14 hours agorootparentprevInteresting. Under the hood FGL is mapping the graph to relatively efficient data structures like Patricia Trees as implemented in Data.IntMap so I would expect it to scale reasonably for inserting edges and mapping over nodes. I agree the memory inefficiency is definitely a limiting factor of the library. As you say I think it is best suited for expressing graph algorithms and if those calculations become the bottle neck a custom solution can be developed with the proof of concept already in place. Out of curiosity what number of nodes/edges would you consider a \"moderately sized graph\"? My user cases are typically on the order of 500-1000 nodes with a similar number of edges that require bfs and topological sorting. reply tikhonj 14 hours agorootparentI don't have an exact number in mind—I imagine it's pretty context-specific—but 500–1000 nodes seems like it would qualify. I've played around with IntMap before and it's not a great data structure. It's a binary Patricia trie, which means that you quickly get a relatively deep tree with lots of pointer traversals. Unless I've managed to confuse myself on how it works, you'd end up with, what, at least 10 traversals to look up a value from 1000 keys? reply kccqzy 9 hours agoparentprevIn Haskell though I think Alga has an even nicer API. Don't know about performance as I haven't had a need to use Haskell to process enormous graphs. https://github.com/snowleopard/alga reply throwaway2037 7 hours agoprevThe C++ Standard Template Library is now 29 years old. It doesn't have a graph (or generic B-tree) data structure. That says it all for me. In my too many years of programming, I have only needed to program a graph structure once or twice. Yes, the are \"ubiquitous in software engineering\", but still incredibly rare in most enterprise programming projects. reply skrebbel 14 hours agoprevI think most languages support representing many kinds of graphs very well, particularly directed graphs without data on the edges: with objects, lists, and object references (or pointers). A tree is a graph. A typical Java-style object composing other objects composing other objects again, etc etc, often with cycles and parent backreferences and whatnot, is a graph. The html DOM is a graph. I recognize that these are often very tree-like, which feels like cheating in the same way as saying “well a list is also a graph!” is. But given that cycles are common enough that serializers (eg JSON.stringify) need to special-case those, I think maybe this is simply not true, and they’re really just graphs. Very few tree-like class structures tend to remain pure trees. The only thing missing from references/pointers to be able to represent what the author is looking for, is having data on the edges. I think this is trivially solvable by putting nodes halfway the edge (= add a level of indirection, an operation so common that we don’t even think of it as “adding data to the edges”). So I think the answer is that there’s no explicit data structure named “graph” because the basic building block of composition in nearly every language (reference/pointer) is an edge, and the basic building block of data representation (objects/structs/records) is a node. So for most graphs, trying to pour it all into some fancy Graph datastructure feels like needless complexity. reply e_y_ 11 hours agoparentBack in the C days, it was common to not use generic data structures like lists; instead you'd have a next_item pointer as a field in the struct. For linked lists, this would save you from needing another memory allocation or wrapper struct, and since C doesn't have templates you'd either have to blindly cast the type or use macros or write a type-specific iterator anyways. Lists eventually became a standard language feature in C++ and other languages, but it's trickier for trees and graphs. Taking the DOM example, you might be searching through child elements (div, span, etc) or nodes (text nodes, comment nodes) and different operations might only work with a specific subset of the \"edges\". There might be pointers to other objects, like from a DOM node to accessibility tree node. You might even have multiple parent node pointers, such as a pointer that takes you to the nearest shadow root or something. Since there are multiple ways to traverse the same data structure, generic functions don't work on it. You could create a separate tree/graph for each thing you want to use it for, but that takes additional memory and has to be updated when the original struct changes. Or you could create some kind of adapter that has a get_edges() function, but this might not be very well optimized or might be clunky for many other reasons. So it usually just ends up being simpler rolling your own functions instead of using a library. reply actionfromafar 11 hours agorootparentBonus points for not allocating space for the next pointer if you didn’t plan to use it… reply js8 11 hours agoprevAnother data type that would be quite useful is a table (like in a database). For the same reasons, too many design choices. Anyway, that being said, I have felt that progress will be made in programming languages if the compiler gets to choose an implementation of a data structure, kinda like when a database chooses an execution plan. So you just use an abstract structure (like sequence, map, set, table, graph) and based on the program profile, the compiler will pick the specific implementation. It will also transform the structure into another isomorphic one as needed. (Some programming languages already do something like this, for example, array of structs to struct of arrays conversion.) reply Kamq 9 hours agoparent> So you just use an abstract structure (like sequence, map, set, table, graph) and based on the program profile, the compiler will pick the specific implementation. It will also transform the structure into another isomorphic one as needed. I'm so not looking forward to having to debug a sudden change in perf characteristics when one additional usage of some feature tips a heuristic over the line and an implementation gets swapped out between builds. reply js8 2 hours agorootparent> a sudden change in perf characteristics when one additional usage of some feature tips a heuristic over This already happens with humans, changing features will change how the product is used and thus performance characteristics changes. The question is, do you trust the compiler to do a good job? Of course you won't, till the late 90s, people didn't trust compilers to do a better job than humans in assembler. So it's important to have a good UX for this feature, where the compiler communicates what data types is it using, and gives human option to override its decisions. So that users would gain trust in this feature. reply taeric 16 hours agoprevA graph, as presented in this article, is a model of something else. That we have more than one way to implement this model is rather natural, all told. The hope that we can have a singular syntax and data structure that represents a graph in code is almost exactly why the author of Java's Linked List posted this gem: https://twitter.com/joshbloch/status/583813919019573248 My favorite on the idea of having a linked list where the node is first class in your code, is almost precisely the problem. You rarely want/need to work at that level. In a very real sense, objects that have other objects are already trees of data. Many can back reference, such that then you have a graph. And then there is the joy of trying to use matrix operations to work with graphs. You can do some powerful things, but at that point, you almost certainly want the matrix to be the abstraction. Excited to see someone come up with good things in this idea. I retain very serious doubts that I want a singular model for my data. reply andsens 16 hours agoprevdevils advocate: Is this maybe a case of discarding an 80% solution because you can’t do the last 20%? I understand the constraints, but imagine how legible you could make code by replacing some key parts with a graph type that everybody knows. I honestly think that having a type that supports a small subset of possibilities and only has the simplest algorithms implemented would go a long way. reply boothby 16 hours agoparentIt isn't just an 80/20 problem. Imagine that you replace every linked list, binary tree, trie, etc., with a generic directed graph datatype. The resulting performance would be abysmal. The resulting notation would be horrid, too. Here's our nice linked list: def last_element(ll): last = ll while ll is not None: last = ll ll = ll.next return last And here's an implementation with generic graph notation: def last_element(g): for v, deg in g.out_degree: if deg == 0: return v return None There are several problems with this; most importantly, there can be silent failures when g is not a linked list. But it also throws out a useful abstraction where a list is equivalent to a node, so I wrote a horrid implementation that takes O(n) regardless of the position in the list. And then comes all the baggage of representation, because you can't just represent a node with a pointer anymore. When your data structure better reflects the, well, structure of your data, you can go faster and safer. There's a reason we teach undergrads about these specific datatypes and don't just sweep it all under a rug with \"it's a graph!\" reply bigbillheck 13 hours agoparentprevI think it's more like discarding a 20% solution that can't do the last 80%. reply Ptitselov 9 hours agoprevIf you code in .NET, please try my graph library Arborescence [1]. It's small and not very feature-rich. However, I believe it provides a good separation between abstractions [2], algorithms, and data structures. Regarding the points mentioned in the article: - you can use the edges with or without their own identity, - you can use implicit graphs unfolded on the fly, - you can use both adjacency (out-neighbors) and incidence (out-edges + head) interfaces, - no edge type is emposed by the library, although it does provide the basic tail-head-pair structure as a utility. [1] https://github.com/qbit86/arborescence [2] https://github.com/qbit86/arborescence/tree/develop/src/Arbo... reply dietr1ch 3 hours agoprevWell, we have many implementations for simpler abstractions like lists where it might be useful to have contiguous memory, or to have quick append/pop, or maybe inserting at the front, or slicing. I think that having clearly defined \"instances\" of these tailored lists, like vector, deque, linked list helps a bit, but graphs are a harder problem since there's more ways of tailoring them to specific purposes. and with this comes more tradeoffs. reply wruza 3 hours agoprevPartly it’s just our beloved overthinking and rationalization of it. When I needed a graph to repesent and edit a structure of supply contracts, I just stored {who, to-who[]} and loaded these arrays into a graph library written in js. Performance, formats didn’t matter because there’s only so much contracts, 5-20 per single graph. If there was no graph library, that would suck, and no amount of rationalization would change that. Complexity of the area never offsets the value of having at least something useful in it. reply mizzlr_ 16 hours agoprevgraph is a data structure, not a data type. if you squint enough pointers are the building blocks (the data type is you please) for building graph data structure. a->b is pointer access, looks like an edge in the graph. graph data structure is parent of tree, code execution/ function call stacks work like a tree, think flame graphs. stacks and pointers are baked in assembly and cpu architecture. your claims can't be farther from the truth. reply Chris_Newton 12 hours agoparentgraph is a data structure, not a data type. It can refer to either. Any concrete data structure that uses indirection — which means pretty much anything more complicated than dense arrays and records — is indeed a form of graph. But graphs, and more constrained forms like DAGs and trees, can also be abstract data types, implemented by a variety of concrete representations. One of life’s little ironies is that implementing a general abstract graph using a general concrete graph whose records and pointers correspond (roughly) 1:1 with the nodes and edges in the abstract graph is often a poor choice. Moreover, it’s not unusual to have an abstract graph implemented using a non-graph data structure (for example, a dense adjacency matrix) or to use a graph-like data structure to implement an abstract data type whose interface doesn’t look particularly graph-like (for example, a piece table). reply renewiltord 16 hours agoparentprevPointer-based graph structure will make matrix algos painful to implement. A graph is a concept. Article is quite meaningful about how it follows the various subtypes. Would recommend reading. reply pizlonator 17 hours agoprevI would claim that object oriented languages are a syntax, semantics and type system for graphs. Objects are nodes. Fields are edges. The object graph is the heap. So your whole program state is a graph. reply qsort 16 hours agoparentGraphs are such a general concept that if you squint everything is a graph. Your example works just as well with structs and member fields, we don't even need the OO hypothesis. reply pizlonator 16 hours agorootparentC and structs let you write code using whatever paradigm you want. So you can do object graphs. And you can also do closures. And you can do many other things. Lots of things are possible when you just treat memory as bytes and pointers are just integers. reply tlb 16 hours agoparentprevA limitation is that you can only have one such graph in the program. So any graph algorithm that returns a graph, or uses another graph during the computation, doesn't fit. reply boothby 15 hours agorootparentDon't fall for the abstraction! Mathematically speaking, if you have graphs G1 and G2; you can make another graph H with nodes {G1, G2} and edges {(G1, G2)} and nothing goes wrong. You can definitely view your whole operating system as a graph; it doesn't invalidate having a program running which processes graphs. reply chubot 16 hours agoparentprevRight, I think that's just what I said the first time this came up: all languages with GC have graphs builtin. (Though C has graphs too. If every node shares the same lifetime, then it's pretty easy to manage. Otherwise it can be pretty painful) And the good news is that you simply use the TYPE SYSTEM to categorize your nodes and edges. Your edges are references to other objects, which are typed. Node can be typed as well. --- Although the original article does get at this -- there are many types of graphs, and some of them can be encoded in a typed object graph. Some of them can't -- you need the equivalent of void* for the edges. Others would need a List[T] for the edges, if the out degree is not fixed. And that only covers directed graphs, etc. Also, it's true that allocating all these tiny objects as GC objects can be very slow, so then you use other representations of graphs, like a list of pairs of node IDs. I don't really think of it as a \"missing\" data structure, but yeah now I do see how that framing can be useful. reply PaulHoule 16 hours agoparentprevFor that matter it's true about a certain kind of C program. In a civilized language you have run-time typing and introspection and have all the type information so that it is straightforward to look at the program's data structures as a graph. If you are looking at the debug symbols and a copy of the program you can usually figure this graph out but you might need to think about it sometimes. reply reaperman 16 hours agoparentprevI think this is a useful model for me personally, and don't want to diminish its potential value to others; I often think of programs as graphs. I think it's interesting to add to the discussion that I'm wary to reduce anything to any particular \"Turing-complete concept\". Because anything can be represented by anything. reply dustingetz 15 hours agoparentprevlet x = 1; // edge named x let y = f(x); // node named f with input edge x and output edge y reply gwbas1c 16 hours agoparentprevI was about to post \"Data structures are graphs,\" but your explanation says it better. reply qazxcvbnm 15 hours agoprevThis reminds me of my quest to find the proper method to model what I've termed 'large types' in an ideal language. As is well known, algebraic data types as commonly found, consist of sums of products, yet a great deal of useful types are larger than that; some hopefully illustrative examples include: 1) the type of subsets of another type would be 2^X (hopefully demonstrating what I mean by 'large'ness); 2) in practical languages like TypeScript, the 'Partial' of a product type A x B x C would be (1 + A) x (1 + B) x (1 + C); 3) data structures in general, as a term amenable to some certain set of operations, when needing to be represented for performance reasons e.g. a) union-find structures (quotients?); b) a list of words and their inverted indexes for searching; c) a sorted list Reading more about type modelling, and learning of the disagreements in how even basic things like quotients ought to be represented as types, I've since resigned to an understanding of this as an unsolved problem, and relegated the modelling the kitchen sinks of types with the kitchen sink of types - i.e. the function type (curbed with suitable type constraints upon the signature - from an index type to a suitable base type) - after all, its power and province being the irreducible kernel of type polymorphism, shadow over Church's types, original sin against type decidability. reply andoando 15 hours agoparentI've been thinking about something like this for the last 3 years. However, I can't find a practical reason for it, even though I am sure there are. reply gue5t 15 hours agoparentprevThese types don't seem to escape the scope of what can be described with algebraic types, but the relationships between them seem like you're looking for a notion of type-level functions: subset ≡ X => 2^X, partial ≡ A×B => (A+1)×partial(B) reply qazxcvbnm 15 hours agorootparentConsider the case of Partials - we might like to restrict the Partials to different subsets of the fields for different purposes; consider the modelling of an inverted index. Certainly it is possible to represent each specific case as some algebraic type; but beyond trivial cases, I find that when I need such of these types, quickly I discover that there are myriad ways to express them, none of them uniquely natural, unlike the way a sum of products type (and its terms) can be pretty much unambiguously drawn from a specification. This matters especially when e.g. I need to evolve my types in a data migration. reply gilleain 16 hours agoprev> And then for each of those types we have hypergraphs, where an edge can connect three or more nodes, and ubergraphs, where edges can point to other edges. Huh, I've heard of hypergraphs (although never actually really used them) but never an 'ubergraph'. Sounds tricky! In practice, how often are there situations you definitely need hypergraphs? I had a particular situation where I needed graphs that were both vertex coloured (labelled) and edge coloured (labelled) - even then it was outside the normal situation for what I was doing (graph canonicalization). reply ratmice 10 hours agoparentubergraphs are pretty weird, i've never actually seen them really used anywhere. Just a couple of papers pointing out their existence. They have some weird quirks like every hypergraph and thus graph has a dual, but ubergraphs with uberedges do not appear to have one. reply avgcorrection 15 hours agoprevMaybe there could be some utility for a decently general Graph type that can be used for high-level testing and verification. Maybe you could implement your efficient graph per-problem and then validate it with a more high-level and declarative type. You would have to implement some isomorphism between the types though. reply fiddlerwoaroof 12 hours agoprevFWIW, despite its name being an abbreviation for LISt Processing, the fundamental datatype of lisp (the cons cell) is actually a vertice on a directed graph with the restriction that each vertice can only have two outgoing edges. reply xLaszlo 10 hours agoprevI found that the best to think of graph implementation is sparse matrices of the adjacency matrix. CSR/CSC format has fast lookup abilities, building and switching between formats is \"relative\" efficient. Most graph algorithms need primitives that can be built on top of this. For distributed computing one can look int GraphLab or its smaller version, now largely abandoned GraphChi. reply ChicagoDave 12 hours agoprevI’ve been building an interactive fiction platform built in C# with a world model contained within a bidirectional graph data structure. Since IF stories are relatively small in graph terms, it’s a reasonable solution. Locations and objects are nodes and movement and location of objects are edges. Nodes and edges both can have dynamic properties. I’ve also noticed the lack of graph structures in programming languages, so this article was very enlightening. reply jonahss 7 hours agoparentsounds neat. link? reply jerf 16 hours agoprevThis is a good article and I endorse it. I would supplement it with the observation that when I was a younger programmer, like many people, I considered \"generic\" or \"flexible\" a positive when describing a library or framework. I have come to see it as generally negative, especially when the developer's summary puts these adjectives or something similar front and center. Let me show you the most flexible possible Javascript framework. This will look like a joke, but it's not. It fits perfectly into an HN post. The most flexible possible JS framework is simply: eval Similarly flexible frameworks exist for dynamic scripting languages. For static languages one must invoke the entire compiler as the framework. Of course, if you think about it hard enough, I'm doing that for dynamic languages here too, it just has a snappier representation for dynamic languages. Frameworks and libraries provide their value precisely through limiting things, and then building on those limitations. Of course, the limitations must be well-chosen, to make what can be built on them interesting enough to pay for the limitations the framework chooses. But the essence of them are in their limitations. I start out from the get-go with the maximally flexible framework my language allows, which is itself the language, and the additional framework needs to make limitations on my code in order to do anything useful. (A problem when framework designers don't understand this is that they make a series of little incorrect design decisions that can often add up to a real pain. For instance, if I were to design a web framework that took over some amount of routing from the user, I would still leave you the ability to claim some bit of the URL space and route it entirely out of my framework, because I understand that my framework is based around limitations and you may need to expose a URL to something that can't work under those limitations. But someone who doesn't realize that frameworks intrinsically involve limitations might fail to give that callout because they can't imagine that someone might have a problem that their framework is not \"flexible\" and \"generic\" enough to handle. Imagine a CRUD framework, even a very good one, but I need to offer an endpoint based on streaming server events in the same URL space, which is intrinsically foreign to the CRUD framework's concept of page loads. This is just one example; real frameworks designed without this understanding will make dozens or hundreds of such little mistakes.) Graphs have the same problem. It seems like they're so flexible and generic that they ought to be more used and more useful. But that's precisely what kills them. Even if you nail down the problem to exactly one representation, they still don't fit. For instance I have a great need for data structures that don't admit cycles, but if all I have is a graph, imposing that limitation from a coding perspective is a real challenge. Mathematically it's trivial, I just say \"and this graph has no cycles\" et voila [1], there are no cycles, but in code I need to enforce that somehow and there's no trivial solution to that. Another way of viewing graphs is that we do work in graphs all the time, precisely because everything in RAM can be seen as a graph. GC algorithms even generally work by viewing everything in very raw graphy terms. It just turns out the API you'd expect to work over a graph just isn't useful in the general sense when applied to everything in a programming language's memory space. It seems like it ought to be, but it just isn't. It may seem like it would be great to have a set of employees and extract their names and then look that up into another database etc. etc. with a general graph query language or something, but it turns out the special considerations at each layer make it so that what the general purpose programming language is already doing is actually generally better. The details at each layer matter. I like the metaphor of architecture astronautics and have often discussed \"the 30,000 foot view\" versus the view on the ground here on HN, and to my mind the key to the metaphor isn't the nerdery of being an astronaut or the difficulty. The key is that when you get high up, everything looks the same. It feels like graphs ought to be awesome when you're looking down at the entire computing landscape from metaphorical low Earth orbit. But down in the trenches, the local concerns overwhelm that viewpoint... and this is real. This is not just because we all suck or we don't try hard enough or we just Don't Get It. It's real. The architecture astronaut is just wrong in this case. It's not even a beautiful vision this world isn't good enough to manifest or any such conciliatory thing... it's just wrong. It is good to write good code and reduce the amount of bespoke details to be considered. The programming community has made great progress there and there is still great opportunity to do more. But there are an awful, awful lot of details in the world, and the world being detailed is fundamental. [1]: Or if you are, like me, kinda a fan of surprise stringed instrument attacks, et viola. reply Terr_ 13 hours agoparent> when I was a younger programmer, like many people, I considered \"generic\" or \"flexible\" a positive when describing a library or framework [...] I've come to prefer what I call \"design for deletion\": Most of those long-term \"flexibility someday\" needs are best-met by making sure the inflexible modules or flows can be clearly identified and ripped out for replacement. This leads to a certain kind of decoupling, although with a higher tolerance for coupling that can kept in check by static analysis. This is a contrast to my days of youthful exuberance where I thought I could solve the problem by making my work extensible or customizable. No, I cannot make the immortal program, so I should focus on making a mortal one which can pass gracefully. reply JonChesterfield 15 hours agoprevI needed a directed graph yesterday. I gave the nodes integer ids by appending them to an arena, then used a hashtable from integer to vector of integer. Iterating over it involves a set of integers to track which nodes have already been visited. Deduplicating nodes on insert into the area was more hassle than cobbling together the graph structure out of a hashtable and a vector. Maybe one reason against putting graphs in the standard library is they're easily put together from more common structures for whatever special case you have in mind. reply PeeMcGee 12 hours agoparent> Maybe one reason against putting graphs in the standard library is they're easily put together from more common structures for whatever special case you have in mind. This is a fair argument (how implementations tend to combine existing structures in bespoke ways). But any time I've needed to use a graph explicitly, it hasn't really mattered what underlying structures were involved. What has mattered each time is having to invent my own little API to expose well-known/primitive graph operations, then go and implement them which is unnecessarily error prone. Your example of de-duplicating nodes on insert sounds like it describes a property of your particular graph that may be better expressed through a type, which would also afford the necessary API. I'm approaching this from an OOP-ish perspective so do with that what you will. > I gave the nodes integer ids by appending them to an arena, then used a hashtable from integer to vector of integer. Iterating over it involves a set of integers to track which nodes have already been visited. This is what sucks about using graphs IMO. I don't want to think about all that stuff, I just want think about graphs. In practice I spend most of the time toiling around with noisy boilerplate that dominates my mental model and allows graph concerns to leak into business concerns. reply jkaptur 13 hours agoparentprevI agree with this take - graphs are in an interesting superposition where implementing a large set of algorithms correctly and in their full generality is hard (as the article points out), but getting started with an implementation that solves a particular problem well enough for a particular case is easy and fun. reply pfdietz 16 hours agoprevThis is looking like a programming language problem. There is no way to make graph algorithms available in a way where the details are abstracted away. What would a programming language look like that could address all those issues? reply tlb 16 hours agoparentSome C++ libraries do pretty well. In the Eigen math library you can call most functions on dense or sparse matrices, and some template magic makes it work efficiently for all combinations. It's a shame it's so hard to write that kind of generic template code. reply Borg3 13 hours agoprevAhh, Graphs.. One of my favorite subject. I love doing graphs but im kinda bad at implementation. But, I still managed to slap D3 + Cola to make my own little interactive visualizer I use for various networking visualizations (L1,L2,L3). Here is example of IRCnet network: http://ds-1.ovh.uu3.net/~borg/d3/ircnet.htm reply tunesmith 13 hours agoprevGiven the wide variety of implementations, what I'd like is a questionnaire that asks you what sort of graph you are intending to work with, and then recommends what sort of algorithm implementations or software packages would be best... I'm hoping that the language models will get better at this sort of question over time. reply khanguy 11 hours agoprevI'm surprised I haven't seen anyone mention Stanford's SNAP [0] yet. In addition to their C++ library, they also have a Python interface too. [0] https://snap.stanford.edu/ reply caditinpiscinam 13 hours agoprev> Relational databases are graphs where the nodes are records and the edges are foreign keys I disagree with the premise of the article: programming languages do have strong and mature support for graphs in the form of relational database interfaces, which cover most of the real-world use-cases for linked data. reply samatman 8 hours agoparentJust in the last month I've been working with PEG patterns, which are mostly tree-shaped but rules make it a cyclic graph, and writing a rope, which is a DAG. How do I use SQLite for those? reply anonymoushn 10 hours agoprevTo make matters worse, people often run graph algorithms on graphs that are too large to store, the details of which are generated on the fly at each step of the algorithm from the definition of the graph. reply bevekspldnw 15 hours agoprevWas seriously hoping at the start that article would reveal some secret easy solution to all my problems…only to learn there are none. :’( reply boothby 15 hours agoparentI take solace in such findings. I learned that my quest was ill-conceived, and I abandoned it; I should be glad that I'm no longer searching for what cannot be found. reply bevekspldnw 15 hours agorootparentYes that’s true, the “smarter people than I failed at this” factor is definitely a form of solace. reply gue5t 16 hours agoprevOne interesting perspective is to view the sequence of lists -> trees -> DAGs -> general graphs as a loosening of restrictions: - list nodes may have one child - tree nodes may have multiple - DAG nodes may have multiple parents though restricted by topological ordering - graph nodes may have multiple parents from anywhere in the collection Lists and trees can be fully captured by sum and product types, but extending this representation style to DAGs and graphs doesn't work--you either get inefficiency (for DAGs) and then infinite regress (for cyclic graphs) attempting to continue the \"syntactic\" style of representation, or you need to adopt an \"indirect\" representation based on identifiers or indices or hash consing. reply schindlabua 14 hours agoparentI like thinking of this concept via free constructions. As is somewhat commonly known the free Monoid is the List type; monoids are not commutative so we get a sense of \"direction\", like a list has a start and an end. If we add commutativity and look at free groups, we find they are equivalent to multisets. If we take associativity away from monoids and look at free semigroups, we get binary finger trees, I think? In some sense removing constraints from the binary operator results in more general free types. Would be interesting to find what free construction makes digraphs but I have to bounce. reply nickpsecurity 16 hours agoparentprevThat’s a neat idea. The other side of it might be expressiveness. The more expressive constructs are usually more productive and concise. The less expressive constructs are usually easier to optimize or analyze with a machine. The old rule, like in LANGSEC, is to pick the least-expressive option that works. Some people also develop transforming code (eg netaprogramming) to let you write highly-expressive code that generates correct, low-expressiveness code. reply michelpp 16 hours agoprevI think one of the elements that author is missing here is that graphs are sparse matrices, and thus can be expressed with Linear Algebra. They mention adjacency matrices, but not sparse adjacency matrices, or incidence matrices (which can express muti and hypergraphs). Linear Algebra is how almost all academic graph theory is expressed, and large chunks of machine learning and AI research are expressed in this language as well. There was recent thread here about PageRank and how it's really an eigenvector problem over a matrix, and the reality is, all graphs are matrices, they're typically sparse ones. One question you might ask is, why would I do this? Why not just write my graph algorithms as a function that traverses nodes and edges? And one of the big answers is, parallelism. How are you going to do it? Fork a thread at each edge? Use a thread pool? What if you want to do it on CUDA too? Now you have many problems. How do you know how to efficiently schedule work? By treating graph traversal as a matrix multiplication, you just say Ax = b, and let the library figure it out on the specific hardware you want to target. Here for example is a recent question on the NetworkX repo for how to find the boundary of a triangular mesh, it's one single line of GraphBLAS if you consider the graph as a matrix: https://github.com/networkx/networkx/discussions/7326 This brings a very powerful language to the table, Linear Algebra. A language spoken by every scientist, engineer, mathematician and researcher on the planet. By treating graphs like matrices graph algorithms become expressible as mathematical formulas. For example, neural networks are graphs of adjacent layers, and the operation used to traverse from layer to layer is matrix multiplication. This generalizes to all matrices. There is a lot of very new and powerful research and development going on around sparse graphs with linear algebra in the GraphBLAS API standard, and it's best reference implementation, SuiteSparse:GraphBLAS: https://github.com/DrTimothyAldenDavis/GraphBLAS SuiteSparse provides a highly optimized, parallel and CPU/GPU supported sparse Matrix Multiplication. This is relevant because traversing graph edges IS matrix multiplication when you realize that graphs are matrices. Recently NetworkX has grown the ability to have different \"graph engine\" backends, and one of the first to be developed uses the python-graphblas library that binds to SuiteSparse. I'm not a directly contributor to that particular work but as I understand it there has been great results. reply jcgrillo 11 hours agoparentI was really excited about RedisGraph, and sad to see it was cancelled. In my (limited) experience with graph databases they proved very frustrating because it seemed like they tried to do too much. Ultimately the way I thought of a graph was an indexing strategy into some underlying data. So I needed the graph to be very quick, but I didn't have any requirement to store actual data in it--just references. This made triples based graph storage seem very heavy-handed. The idea of composing sparse linear transformations to optimize queries is really cool. You can get a lot of work done in one shot that way, in a manner that's just quite a lot easier on the machine than chasing pointers around. reply michelpp 10 hours agorootparentRedisGraph is now FalkorDB: https://github.com/FalkorDB/FalkorDB reply jcgrillo 5 hours agorootparentoh excellent! that's great news reply bionhoward 14 hours agoprevCould dataframes be a superior backend for graphs to maps/lists? reply petesergeant 8 hours agoprevIf anyone needs graphs in XSLT, I wrote a BFS over a graph library once while feverish https://gist.github.com/pjlsergeant/50a3d086d9513612cb397a13... reply zacify 11 hours agoprevI feel like this is common knowledge…. reply de6u99er 4 hours agoprev@HN: can we please have less of those \"Look I discovered something, most of you already know\" articles? Or at least a tag to filter them out! reply csb6 13 hours agoprevThis is an interesting article but one major effort it doesn’t mention is the Boost Graph Library [0]. It is kind of clunky in places because it is written in C++03 and uses some weird idioms to simulate keyword arguments and provide generic ways of getting attributes for nodes. Also it suffers from the terrible template instantiation errors that most C++ template libraries do. But I still think it addresses a lot of the difficulties covered in the article: > There are too many design choices BGL is limited to directed/undirected multigraphs, so hypergraphs are not supported. However I think these cover most use cases. In terms of implementation choices, BGL provides several concrete data types, such as an adjacency list and an adjacency matrix. It also provides adaptors for the GraphBase and LEDA graph libraries. If none of these are suitable you can write adaptor functions to support your custom data type. All algorithms* work unmodified on these concrete implementations. > So which algorithms should come with the library? BGL comes with most of the common ones [1], but I do wish it came with more. The implementations of them are quite hard to read because they are written in highly generic (and before many of the conveniences offered in C++11) C++ code. > Performance is too important Since BGL is generic using C++ templates instead of runtime polymorphism, it should (in theory) be able to work with a concrete graph implementation that is performant for a certain task and so let you reuse its generic algorithms. I think the article describes a lot of the difficulties that Stepanov’s generic programming approach tries to solve (e.g. finding the most abstract but still efficient implementation of an algorithm, writing algorithms that depend on a limited set of type requirements, having many data types that can reuse the same algorithms). While C++ supports this style of programming it is not ideal for it, but I think BGL is the closest thing I have seen to a generic graph library that is also performant in many cases. *Algorithms have varying requirements, e.g. some may need to be able to remove edges while others do not. But these requirements are generic and can be fulfilled by many different graph implementations. [0] https://www.boost.org/doc/libs/1_84_0/libs/graph/doc/index.h... [1] section 22, https://www.boost.org/doc/libs/1_84_0/libs/graph/doc/table_o... reply csb6 13 hours agoparentAs a sidenote, Stepanov’s introduction [0] to the Boost Graph Library book explains a lot about his approach to programming. He didn’t write the BGL, but his work on the STL was a major inspiration for it. [0] http://stepanovpapers.com/siekforeword.html reply datavirtue 8 hours agoprevhttps://learn.microsoft.com/en-us/previous-versions/ms379574... reply AtlasBarfed 12 hours agoprevMy own take: a document database, which devs seem to love, combined with relations aka edges,is basically a property graph and that's what basically you need. Directionality is a property on the edge. reply lowbloodsugar 13 hours agoprevCouple of thoughts: First, the discussion about representation highlights that the issue is a lack of infinite resources. If we had an infinite computer, that could execute an infinite number of operations in zero time, and had infinite memory, then we wouldn't be worrying about whether it's better to store the graph as a matrix, an edge list, or a pointer graph. Software Engineering is everywhere and always a job of optimization. Sometimes that optimization is premature, and sometimes it's too little too late. It's always about optimization. Second, when we're talking about a graph of 10 nodes, it really doesn't matter what data structure we use. It can quickly matter if we have 100s or 1000s of nodes and edges because now the possible arrangements are huge as is the search space. But this is no different than other problems like the knapsack problem where the search space is huge: depending on the problem, there is very likely a \"trick\" to make it tractable, and that trick is different depending on the problem. So, like the knapsack problem, there are different, specific solutions for specific graph problems. reply ogogmad 14 hours agoprevI suspect a lot of graph theory can be reduced to abstract algebra. You consider matrices over lots of different scalar types. The scalar types should be: - Rigs (rings without negation), - idempotent (that is, where x + x = x for all x), - equipped with involution (so that undirected graphs can be made the default by restricting the matrices which represent graphs to only self-adjoint matrices), - and the entries of the matrix can be restricted to a *-ideal. Note that a *-ideal can be considered a scalar type in its own right. Different choices of the above scalar types can be used to capture different graph types: Weighted, directed, undirected, bipartite. There's no clue to physical implementation, other than that sparse graphs can be treated like sparse matrices. Anybody tried this? How did it work out? reply aweinstock 7 hours agoparentThe post \"A Very General Method of Computing Shortest Paths\" (https://r6.ca/blog/20110808T035622Z.html) shows that the Gauss-Jordan algorithm for solving matrix equations, the Floyd-Warshall algorithm for all-pairs shortest paths, and Kleene's algorithm for converting DFAs to regular expressions, are all the same algorithm on matrices of elements from a star-semiring. reply yen223 9 hours agoparentprevAccording to the article, the physical implementation of graphs is precisely the thing you cannot ignore, because the performance difference between a generic solution and a \"correct\" solution is huge, basically the difference between the algorithm not completing and the algorithm completing. That's the reason why it's hard to come up with a single one-size-fits-all graph implementation. reply bigbillheck 13 hours agoparentprevYou're a couple decades too late: https://graphblas.org reply ogogmad 13 hours agorootparentIf you read what I wrote, you'd see I never claimed originality. Anyway, that's cool. I'll need to check how much abstract algebra stuff they use. Using semirings (uh, rigs) alone isn't impressive. Do they consider semirings with more algebraic structure attached to them? reply bigbillheck 11 hours agorootparent> Using semirings (uh, rigs) alone isn't impressive. Do they consider semirings with more algebraic structure attached to them? Wiki says they have R, the two tropical semirings, the 'max-min' semiring, and GF(2). The tropical and max-min have your idempotency requirement, all but the max-min have involution. reply mcphage 15 hours agoprevThis is a good write up, but for me seems to miss the mark. I agree with the author that different algorithms require differently organized data structures. But what if your problem requires 2 different algorithms, which each work best with a different data structure? Either you pick one and run both (which is not ideal) or you run the first algorithm with one structure, convert it, and the run the second algorithm in the second structure. And once you can do that… why not have every algorithm ensure it runs in its best structure, and convert where necessary (and possible) on the way in? Yes, there’s absolutely a performance or storage cost… but if the algorithm is that much faster, it should be worth it. Basically a beefier version of sorting your data before searching in it. If an algorithm works best with a specific model of a graph, then let that be part of the algorithm. reply riwsky 14 hours agoparent> why not have every algorithm ensure it runs in its best structure, and convert where necessary (and possible) on the way in? Yes, there’s absolutely a performance or storage cost… but if the algorithm is that much faster, it should be worth it. Because it's not that much faster, so it's not worth it. You're severely underestimating the amount of thought that went into the article, or the work of the experts interviewed. reply adamnemecek 16 hours agoprev> Mathematica, MATLAB, Maple, etc all have graph libraries of some form or another. I am not paying the thousands of dollars in licensing needed to learn more. Wolfram provides a free Mathematica called Wolfram Engine https://www.wolfram.com/engine/. It's Mathematica without the UI. I hear you can combine it with Jupyter Notebook to get a similar experience to Mathematica. reply dekhn 15 hours agoparentHa! I wrote the predecessor for the Python integration some ~25 years ago https://library.wolfram.com/infocenter/MathSource/585/ and I think it uses a similar approach. The mathematica engine had a protocol for sending and receiving expressions and evaluating them. I was pretty proud of my implementation as it it was my first real attempt at doing anything remotely complicated with recursion. reply rhodin 14 hours agoparentprevYou can also use wolframcloud.com for free. Which runs the latest Mathematica online. reply 4ad 16 hours agoprevOk, trees are not graphs but they are very related, and algebraic data types are trees, so they are ubiquitous in functional programming. Why don't we have graphs in FP (or in Rust)? Because graphs require mutation (respectively break linearity). Why don't we have graphs in imperative languages? Perhaps because very few imperative languages have ADTs? Just a thought. reply boothby 16 hours agoparentI disagree: trees are connected graphs with n nodes and n-1 edges. They're graphs with a special structure that is highly exploitable for performance gains, but they're graphs. reply harporoeder 15 hours agoparentprevEven with direct mutual references between some node type this can be represented in a lazy functional language such as Haskell pretty easily without mutation. reply zokier 16 hours agoparentprevIn what sense graphs require mutation? reply joshlemer 15 hours agorootparentI think maybe they're talking about how, in order to create a data structure where two nodes contain a reference (edge) to each other, you have to resort to mutation. i.e. you have to create A, then B with reference to A, then mutate A to refer to B, or you have to create B, then A with reference to B, then mutate B to refer to A. Though this ignores that there are other ways to represent graphs, such as adjacency matrices, etc. reply bigbillheck 13 hours agoparentprev> Why don't we have graphs ... in Rust? You might have missed this from the article but: https://docs.rs/petgraph/latest/petgraph/index.html > Because graphs require mutation (respectively break linearity). I don't think this is actually the case. Graph nodes go in one container (`Vec` or `HashMap` or `BTreeMap`), and the edges go in another container (`HashMap` or `BTreeMap`). The object in which you store the node only needs to know what its name is, you can let something else know what its neighbors are. reply at_a_remove 15 hours agoprevHrm. This reminds me of a thought I had about, for the lack of a better term, \"groups\" in Python. Each data type has a property which is paramount and more or less defines the type. Lists are ordered. The tuple is immutable. The dict is keyed. The set is unique. (But there are some overlaps: dicts are both keyed and their keys are unique.) And I thought, what if you had a group where you could make it immutable or mutable, ordered or not-ordered, where the value was a key to something else (or not), and so on? But then I saw the weird edge cases and the explosions of complexity. Some combinations of these attributes are less appealing than others. reply jes5199 16 hours agoprev [–] I think this is a cop-out. Someone in the 1990s could have written the same thing about collections, or dictionaries, but we eventually came up with good-enough compromises that Python, Ruby, and Javascript all basically do the same thing. They don't implement literally every case, but they are good enough for \"small\" data, where the definition of \"small\" has grown to be quite large by human standards. I think the real problem is syntax. Someone needs to come up with a textual graph literal that fits in source code, and it'll be good enough for the small type. Switch to a custom ubergraph when you exceed, idk, ten million entries. reply obi1kenobi 16 hours agoparentTwo problems I see here, based on the research I've done in high-performance graph algorithms: - It's hard to find a \"good-enough\" graph implementation. The best hashtable is only a handful of percent better than the built-in ones. The best graph impl is 1000x or more better than any generic built-in one could be, so there's much more incentive to specialize (and people already specialize hashtables for just a handful of percent speedup!) - The baseline complexity level of implementing a reasonable hashtable is fairly high, even if for a small dataset. The baseline complexity of implementing a graph algorithm for a small dataset is pretty low, and the real problems come in later / at larger scale. So in graphs there's less incentive to learn a complex library's API when \"I could just hack it myself,\" unlike for hashtables where the API is simple and doing it myself is much harder. reply DSMan195276 15 hours agorootparent> The baseline complexity level of implementing a reasonable hashtable is fairly high, even if for a small dataset. I would disagree with this, it's actually really easy to make one if you're willing to do away with many features (which aren't essential, but provide performance benefits). Implementing one is just something you never have to do in most modern languages. reply bruce343434 16 hours agorootparentprevMore than just a handful of percent[1], but ok [1] https://probablydance.com/2017/02/26/i-wrote-the-fastest-has... reply obi1kenobi 16 hours agorootparentThe work you cited is very impressive and very welcome. But you seem to be implying that `std::unordered_map` is the default choice one would use, which in my experience is not accurate -- it is well-known to have serious perf shortcomings, and everyone I know uses some other implementation by default. Even so, the delta from `std::unordered_map` to the improved hashtable in the blog post is impressive, and just shy of 10x. Graph algorithms frequently have 10x improvements from one state-of-the-art approach to the next -- for example, here's one from my own research[1]. The delta between state-of-the-art and \"good default\" in graph algorithms would often be around 100-1000x. And comparing state-of-the-art to the equivalent of an `std::unordered_map` would be another 10-100x on top of that, so 1000-100000x total. [1]: https://dl.acm.org/doi/10.1145/3210377.3210395 reply bruce343434 2 hours agorootparentWhoah, thank you for sharing. I only knew that just like dictionaries, there are quite a few implementation choices when making a graph, depending on what operations the algorithms needs to do often, and how sparse the data is. reply brazzy 16 hours agorootparentprev> The baseline complexity level of implementing a reasonable hashtable is fairly high, even if for a small dataset. Have you tried doing it? My experience was that it was surprisingly simple. We may have different expectations for what is \"reasonable\", of course. reply PaulHoule 16 hours agoparentprevI've been involved with RDF and sometimes it seems the gap between \"too simple to use RDF\" and \"too hard to use RDF\" is tiny. For instance I tried to pitch a data processing library a bit like https://www.knime.com/ but where RDF graphs (roughly like a JSON document) get passed over the \"lines\" but found that the heavy hitters in this space believed this sort of product has to use columnar execution to be \"fast enough\". You can certainly build something that can do operations on a dynamic RDF graph (really a set of triple) but in principle you could compile code that treats native data structures as if they were in RDF... You might get pretty good in speed but it won't be as fast as native and hard to make it easier to code for than native. reply jes5199 16 hours agorootparentRDF is an XML standard, isn't it? there was a XML era for collections and dicts, too. That is now largely considered to have been a mistake reply zozbot234 16 hours agorootparentRDF is not dependent on XML. There is a XML representation of RDF, but alternatives include JSON (using JSON-LD) and simple textual formats such as N3 and Turtle. reply PaulHoule 16 hours agorootparentOne trouble w/ RDF is that there are two official ways to do ordered collections, the RDF list which is basically a LISP list, and then RDF collections where the order is encoded in predicates. Neither is well-supported in most tools, for instance SPARQL lacks the list handling capabilities that you'd see in https://docs.arangodb.com/3.12/aql/ or https://www.couchbase.com/products/n1ql/ The XMP spec, for instance, hacks Dublin Core by adding ordering information because... It matters what order the authors are in. Dublin Core on the other hand seems to be developed for cataloging elementary school libraries and they were huge fans of doing the easy stuff and leaving out anything moderately hard, so Dublin Core looks like it has a 1968 level of sophistication and MARC is so much more 2000s. People come to RDF, see all these problems that are ignored, and come to the conclusion RDF is not for them. reply hobofan 15 hours agorootparentThere is a good repo that collects many of the issues with RDF[0], which I would have loved to have known about earlier in my journey with RDF. [0]: https://github.com/w3c/EasierRDF reply hobofan 14 hours agorootparentprevWhile there are (commonly used) alternative serialization formats, RDF is married to XML data model, as all core datatypes of RDF are defined in terms of XSD (XML Schema Definition) datatypes and very closely mimics it's data model. If you want to have an RDF that is independent of that (e.g. based on Apache Arrow so that it's compatible with modern big data tooling) you might as well start from scratch. reply abeppu 16 hours agoparentprevAnd some languages (e.g. java, scala) have standard libraries with interfaces that describe ordered collections, dictionaries, etc, but offer multiple implementations so the programmer can pick based on their specific considerations, but still benefit from library code written around the interface. reply Hackbraten 16 hours agorootparentIt also uses hint/marker interfaces (eg. `RandomAccess`) so the library code can choose among algorithms internally while still presenting an uniform API to the client. reply boothby 15 hours agoparentprevAs somebody who's actually done the work, I don't think you've really spent time with the question. The problem is that graphs, in general, aren't special. They're a structural dumping ground. Data structures are specialized graph representations. The source code you write is in a specialized graph representation. For the overwhelming majority of programmers, the fact that something can be viewed as a graph is much like saying \"oh well that file is just a really long sequence of bits, so it's effectively an integer.\" It's not wrong, but is it useful? reply gilleain 16 hours agoparentprevWhat do you mean by \"textual graph literal\"? reply jes5199 16 hours agorootparentTextual array literal: [1,2,3] Textual dict literal: {\"a\": 1} Textual graph literal: ??? reply WorldMaker 16 hours agorootparentWe've got a couple common graph literals today: Graphviz's \"dot\" language [0] and the Mermaid language (in part inspired from \"dot\") [1]. Mermaid is increasingly common in documentation today, given Github supports it almost everywhere Markdown is supported. Parsing dot or Mermaid as embedded literals in another language doesn't seem that complicated (Mermaid's parser is written in JS and already runs anywhere you can run a JS parser). Though one interesting thing to note here is that both languages are edge list languages and are optimized for algorithms that are useful for edge lists, particularly display/diagramming. That gets back to the article's point that there are other useful representations and those can matter for efficiency of algorithms. They also can matter for efficiency of a graph literal in a source document. Edge lists are great for sparse graphs, but if you have a dense graph it might be easier to write as a literal with a massive spreadsheet of an adjacency graph. Especially if it can just be a spreadsheet with modern affordances like scrolling and sticky rows/columns and such. There's no perfect answer for \"every\" graph. [0] https://gra",
    "originSummary": [
      "The article delves into the absence of native graph types in popular programming languages, the intricacies of developing graph libraries, and the significance of selecting an appropriate internal graph representation.",
      "It highlights the complexities of graph algorithms, the balance between performance and memory consumption, and how different representations affect graph operations.",
      "Additionally, it mentions graph querying languages, mainstream languages supporting graphs, and the diverse applications of graphs, underlining the importance of optimizing graph operations and exploring specialized solutions for graph problem-solving."
    ],
    "commentSummary": [
      "A team at hillelwayne.com developed a modular and efficient graph library in C++, facing challenges with complexity and receiving feedback from renowned C++ creators.",
      "Advancements were made in graph visualization tools, emphasizing the importance of specialized graph types in libraries and efficient graph data structures with clear APIs.",
      "The discussion highlights the use of linear algebra for graph operations, challenges with running algorithms on large graphs, and benefits of tools like GraphBLAS for optimization in programming languages."
    ],
    "points": 554,
    "commentCount": 197,
    "retryCount": 0,
    "time": 1709570534
  },
  {
    "id": 39589483,
    "title": "Apple fined 1.8B euros by EU for antitrust in Spotify case",
    "originLink": "https://www.reuters.com/technology/apple-hit-with-over-18-bln-euro-eu-antitrust-fine-spotify-case-2024-03-04/",
    "originBody": "reuters.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAGSSPy7fjsXMANJ0EWw==','hsh':'2013457ADA70C67D6A4123E0A76873','t':'fe','s':43909,'e':'4946adbdf4f50389542d9d8c30be2d9e5a7d82683b8f48e523d3dd49af3c4ffc','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=39589483",
    "commentBody": "Apple hit with over 1.8B euro EU antitrust fine in Spotify case (reuters.com)549 points by nopakos 21 hours agohidepastfavorite634 comments todd-davies 18 hours agoNote that this fine is made up of 0.04bn of fine and 1.8bn of deterrent against future anti-competitive behaviour [1]. The the 2006 fine-setting guidelines allow the Commission to do that [2]. We should read the 1.8bn lump sum (roughly 0.5% of Apple's revenue) as partially being about music streaming and app stores, but mainly a warning to all large firms which are currently jockeying for a dominant position in emerging tech like generative AI and visual computing. The warning: play fair and compete on the merits, or see you in court. [1] \"the Commission decided to add to the basic amount of the fine an additional lump sum of €1.8 billion to ensure that the overall fine imposed on Apple is sufficiently deterrent\" https://ec.europa.eu/commission/presscorner/detail/en/ip_24_... [2] See paras 30 and 31. https://eur-lex.europa.eu/legal-content/ELL/?uri=CELEX%3A... reply RugnirViking 18 hours agoparentwhat is actually the difference in this case? I thought the function of fines WAS the deterrent effect? or is some aspect of this restitution? I thought this was payable to the EU itself, not spotify. reply tivert 18 hours agorootparent> I thought the function of fines WAS the deterrent effect? or is some aspect of this restitution? I thought this was payable to the EU itself, not spotify. Maybe it's something akin to punative damages (https://en.wikipedia.org/wiki/Punitive_damages), where the \"fine\" component is assessed according to the actual harm measured and the \"deterrent\" amount is meant to make it sting as a punishment. The law always needs a little something extra to deal with people like the guy who parks in handicap spaces because he can afford the fines. reply neycoda 13 hours agorootparentWould be nice if fines like that were earmarked as probationary in the sense that if the bad behavior continues, they actually have to pay the fines plus a penalty, otherwise they get a diminished amount. reply todd-davies 17 hours agorootparentprev(edit) TL;DR: see tivert's comment. In most cases, the Commission sets a fine which is based on the harm caused by some anti-competitive conduct, with relatively small adjustments for extenuating or attenuating circumstances. In this instance it's the opposite; the economic harm was small but the adjustment was huge. You're right that the logic - deterrence - is the same in both cases. But what's different (at least in my view), is the object of the deterrence. Ordinary fines are designed to make anti-competitive behaviour unattractive in terms of the costs and benefits. Maybe some underhanded conduct generates €40m extra profit, but the risk of a €40m fine plus legal costs and adjustments makes it not worth it. The trouble is that these fines might are essentially just rounding errors for large firms. In this case, a €40m fine would be tiny in relation to Apple's revenue stream (~€350bn euros a year), thus not an effective deterrent. That's for two reasons. First, the 40m figure is too low since a \"significant part of the harm caused by the infringement consists of non-monetary harm, which cannot be properly accounted for under the revenue-based methodology as set out in the [Commission's guidelines]\"[1]. Second, the fine is trying to to \"deter [Apple and] other companies of a similar size and with similar resources from committing the same or a similar infringement\"[1] even when they could absorb the ordinary (small) fine as essentially a rounding error on their cost of doing business. In that case, large conglomerates could basically just ignore competition law. So here, the Commission is deterring all firms which have a \"particularly large turnover\" [2] (e.g. Big Tech firms) from using their power in one market to gain an advantage in another market, as in this case where Apple used its control over its App Store to gain an advantage in the music streaming market. The fining guidelines allow for fines to be much larger (~50x in this case) for tech giants, even if the actual infringement didn't cause that much quantifiable harm. You're right, there's no restitution here. As you say, the fine is payable to the EU and would be paid into the EU budget. [1] https://ec.europa.eu/commission/presscorner/detail/en/ip_24_... [2] Para 30 https://eur-lex.europa.eu/legal-content/ELL/?uri=CELEX%3A... reply TimPC 17 hours agorootparentIt's insane that they think the deterrant to a single line of business should be based on the revenue of the business as a whole instead of that line of business. 0.5% of app store revenue would be far more reasonable than 0.5% of Apple revenue. reply Ghexor 17 hours agorootparentI think, in order to have the desired effect, the deterrant must be in proportion to the resources of the transgressor as a whole entity. If its not, Apple can afford to behave anticompetitively with their appstore, hold on to an illegal position of power and use that to extract revenue for the other lines of business that they're in. Morally, if we fine violations to the speed limit in proportion to the context of the transgression only it will not adapt to the income of the transgressor. That makes it practically legal for rich people to drive as fast as they wish. reply sircastor 16 hours agorootparentprevI follow your reasoning, but by the same reasoning we could say that Apple shouldn't internally fund projects with the revenue from other projects. Each project should be supported by its own income stream. To function as a deterrent, it needs to represent a cost that a company is unwilling to tolerate. reply todd-davies 17 hours agorootparentprevThere are a few ways to think about this. One is deterrence based on cost-benefit analysis, which is essentially a game theoretic way to think about firm behaviour. The logic here would be to fine the firm enough to deter anti-competitive behaviour, as has been mentioned. Another way to think about it, is to say that we care about safeguarding the process of competition itself. That could include ensuring that competition is fair, ensuring that firms can enter markets, ensuring consumers get to choose which firms to consume from, etc. There's lots of precedent for that in EU competition law [1]. Taking that view, Apple was using its dominant position to restrict the economic freedom of Spotify (and others) and thereby harming the process of competition. Specifically, it limited rival firms from \"fully informing iOS users about alternative and cheaper music subscription services\" (as per the press release), thus harming competition. All that to say, if we take the objective of EU competition law as being to prevent large firms from exercising power over smaller firms and to protect the process of competition in a general sense, then these big fines are easier to justify. [1] https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3166005 reply beeboobaa 14 hours agorootparentprevThe point of a fine is to get a company to behave. I'd prefer if they could just jail the executives instead, but since sadly companies are an effective shield against justice this will have to do. reply todd-davies 13 hours agorootparentCriminal sanctions in competition/antitrust law cases are an option in some jurisdictions, notably in the US and the UK (but not in the EU). For an ageing but interesting case, see https://en.wikipedia.org/wiki/Lysine_price-fixing_conspiracy reply deanishe 4 hours agorootparentprev> It's insane that they think the deterrant to a single line of business should be based on the revenue of the business How is that \"insane\"? It's pretty clear that Apple Music hadn't gone rogue. reply XajniN 17 hours agorootparentprevThe law allows up to 10% of global revenue. reply jijijijij 14 hours agoparentprev> Any person or company affected by anti-competitive behaviour as described in this case may bring the matter before the courts of the Member States and seek damages. The case law of the Court of Justice of the European Union and Regulation 1/2003 both confirm that in cases before national courts, a Commission decision constitutes binding proof that the behaviour took place and was illegal. Even though the Commission has fined the company concerned, damages may be awarded by national courts without being reduced on account of the Commission fine. Uff. Spotify can ask for damage compensation on top of those 1.8 B€, huh?! Apart from music streaming services, I don't see why the situation isn't fundamentally the same with video streaming and Apple TV, and maybe even things like VPN, or cloud services, where Apple has its own competing products now. So... this ultimately may get really, really nasty and expensive for Apple. I am so here for it :) reply Aerbil313 13 hours agorootparentLooking from the perspective of Apple, the whole situation is really annoying. Let me play the devil's advocate here. They built the iPhone, iOS and all its frameworks and the App Store. Now they are forced to act as essentially a public service provider. Philosophically, the amount of users they have shouldn't necessiate them to be treated as one. But you know, everyone wants a piece of the cake Apple thought they baked under their own terms. It's quite telling that this regulation comes from EU which are not getting nearly the tax and revenue US gets from Apple. reply hu3 13 hours agorootparent> They built the iPhone, iOS and all its frameworks and the App Store. Well they don't give iPhones and App Store away for free. Cheapest iPhones in Germany cost ~$530 for a 64GB model which Apple will nag you to subscribe to their iCloud to offload data. Meanwhile developers need a $1k macbook (with measly 8GB RAM and 256GB storage btw) and $100/year. And you get no choice to install from other app stores. So generous! reply octacat 12 hours agorootparentprevDevil advocate is that people cannot have more that one phone per person realistically. If every app would need a separate phone, that would be stupid. It is totally fine for a country to ask that if a company wants to sell their devices in the country, they have to follow some rules. reply jijijijij 13 hours agorootparentprev> They built the iPhone, iOS and all its frameworks That's what their customers payed for when they bought very expensive iDevices. Honestly, what a weird take, considering Apple really invented this argument. iOS app devs and service providers are just as much a necessity for Apple's popularity and success. You know, the default iOS calculator app used to be a meme not that long ago... > Appstore Yeah really, unhappy devs should totally roll their own distribution like on every other platform since the beginning of software development. Oh, wait... > But you know, everyone wants a piece of the cake Apple thought they baked under their own terms. Or maybe, Apple wanting a piece of the second largest consumer market should play by its rules, considering all the commerce infrastructure they are using... Not to mention the education system they are free-riding for their engineers, courts protecting their brand and IP, police enforcing property rights, ... Do you pay taxes in Jonestown? reply msla 16 hours agoparentprev> We should read the 1.8bn lump sum (roughly 0.5% of Apple's revenue) So it deters nothing, Apple pays it and continues to do what it's doing. reply jijijijij 13 hours agorootparentRespectively, 21% of Apple Music's 8.3B$ revenue, tho. Video streaming and other services may follow. Now, there is precedent. reply piva00 33 minutes agorootparentprevIf they continue the fine amount will be increased on the next one. That's how the EU plays, you get a warning, then a deterrent, if that doesn't stop the behaviour fines will continue to be levied in increasing amounts. GDPR rules follow the same structure and are a pretty good deterrent. reply JonAtkinson 18 hours agoparentprevHi Todd! reply braza 21 hours agoprevFor whom was in tech during 90s and now in 2020s, what are the differences between Microsoft back then and Apple now in terms of why the regulators are so \"soft\" against an actor that is overplaying it's hand for consumers? I was not in tech in that time, but I was cognizant about the fact that entire segments of media and policy makers just dunked on Microsoft due to anti-competitive practices back then, and I recall congressmen and congresswomen, members of DoJ and so on openly talking about break Microsoft in pieces and I wonder why we do not have those conversations today in the biggest markets (China, US and EU)? reply shellac 21 hours agoparentI think you have to appreciate just how egregious the behaviour of Microsoft was in the 90s. We're talking about systematic attempts to use their absolutely dominant market advantage to move into new areas and kill rival technologies, plus quite deliberate sabotage of open standards process to prevent the web taking off. (I knew a few people involved in W3C then and Microsoft's behaviour was breathtaking) Personally I'd avoid comparisons since you're looking at one of the worst situations imaginable, so anything else is bound to seem less harmful. reply jsnell 20 hours agorootparentApple is predatory on a level that 1990s Microsoft could not have dreamed of, which is all enabled by Windows being an open platform while iOS is locked down with strict technical measures. Microsoft would make competition harder by bundling \"good enough\" versions of competing software into Windows, with business deals restricting pre-installs of competing operating systems (if you pre-install Windows on 100% of machines, you'll get a massively better deal than when installing on it on 95%), and (arguably) by making interoperability harder by not having stable documented interfaces and having the interfaces change with new releases every few years. That was bad. But Apple... Apple hasn't been just making competing harder, but outright impossible. It's one thing to compete against a free pre-installed product. It's another thing to try to compete when the platform owner straight out forbids you from shipping a product that competes with them, or applies draconian business terms to the competition that their own business units get to ignore. They weren't just making interoperability harder by not documenting interfaces, but (in the case of iMessage) by actively breaking any attempt to interoperate. reply jonhohle 19 hours agorootparentI’m not sure if you lived it, but Microsoft was actively targeting companies to destroy, and destroying companies because they could. Read about Mosaic, Be, their role in the SCO case, the Halloween documents, the Embrace, Extend, Extinguish philosophy. Their monopoly case wasn’t about draconian terms. They were savagely destroying competition as a company culture. Edit to add: bad terms on a vertically integrated platform is cute compared to Microsoft dictating what the entire PC industry was allowed to sell. It’s not that Windows was the best game in town, it was that vendors couldn’t sell Windows pre-installed if they sold anything else. reply SSLy 19 hours agorootparent> Be example https://birdhouse.org/beos/byte/30-bootloader/ msft forced PC makers to ship bootloaders that would lockup on sight of Be's boot chain reply nobleach 15 hours agorootparentThey literally left comments in their code saying, \"we should crash here if using DR-DOS\". Digital Research had a compatible but different DOS implementation at the time. reply yabatopia 19 hours agorootparentprevI do remember Steve Jobs wanted \"to go thermonuclear\" on Android (mostly Samsung and Google), saying \"I'm going to destroy Android, because it's a stolen product.\" Other motives, same behavior. reply rsynnott 19 hours agorootparentNo, different behaviour. If Apple had announced its own Android derivative called Apple Android++, somehow forced all major OEMs to pre-install it, designed it such that 'Android' apps written for it wouldn't work on real Android, then killed it, that might be the sort of behaviour that would be closer to the mark. Like, Steve Jobs making vague threats isn't really in the same league as what MS was getting up to in the 90s. reply jonhohle 19 hours agorootparentprevSimilar behavior would have been to go to all of the retailers and say they couldn’t sell iPhones if they sold Android phones. Apple sued for patent infringement and that was about the extent of it. Suggesting that their behavior is at Microsoft’s level is naive. reply sangnoir 14 hours agorootparent> Apple sued for patent infringement and that was about the extent of it. You're massively understating the scope and scale of Apple's war on Android. Apple transfered patents to a NPE that sued Android vendors - possibly Google itself. Apple also spent billions on this war - Apple et al spent over $4B to outbid Google on Nortel patents, Google spent even more billions to buy Motorola and its patents, prior to that, Google had virtually no defensive parents. reply jonhohle 13 hours agorootparentI said Apple sued over patents, you said Apple sued over patents. I think we’re agreeing. reply sangnoir 12 hours agorootparentI'm quibbling with the phrasing of the last half of your sentence, as it's a bit of an understatement, no? > that was about the extent of it Apple didn't only sue over patents they already owned: they went out of their way and bought billions worth of more patents to sue with, partnered with patent-trolls who also sued OEMs on their behalf. So we agree, but the breadth and depth of Apple's actions was remarkable - they didn't just sue, they went to great lengths to do so. reply whynotminot 17 hours agorootparentprev> Other motives, same behavior. If you are absolutely committed to doing zero further analysis, then sure I can see why you would think that. reply bee_rider 19 hours agorootparentprevDifferent results. MS was pretty effective at destroying the competition. reply fakedang 14 hours agorootparentprevTo be fair, Apple today is more about going thermonuclear on apps inside its own ecosystem, as compared to Android or Android apps. (I don't mean just the commission rate, but the blatant copying of successful app concepts). reply kristjank 17 hours agorootparentprevEEE is still very much alive nowadays, especially how they got into Linux spaces with WSL(2) and into the wider FLOSS community with the GitHub acquisition. We're just in the Extend phase at the moment. reply Aerbil313 10 hours agorootparent> For nearly 10 years now Microsoft's VS Code has been the dominant text editor among the soydev community. I use it on my Microsoft Windows machine to write Microsoft Typescript code with help from the latest AI models in Microsoft Copilot, it installs my Microsoft npm packages, pushes my code to Microsoft Github and automatically deploys to Microsoft Azure which I then admire from the Microsoft Edge browser.[1] For real it's scary how much developer experience is controlled by Microsoft. 1: https://www.youtube.com/watch?v=JGz7Ou0Nwo8 reply outside1234 19 hours agorootparentprevIt is possible (and is) for both things to be true. Both Apple and Microsoft were/are abusive of their market position. reply jonhohle 18 hours agorootparentIt’s about the difference between someone saying something mean to people and someone killing people. Sure, they are both socially unacceptable, but we typically don’t equate those. reply Jensson 16 hours agorootparentAt the time Microsoft was caught logging all communication in computers was new and business leaders hadn't adapted to it, but today you want catch them with their pants down like that, so today we wouldn't know if Apple or Microsoft were deliberately targeting and killing companies since they know to say \"We are doing this to serve the customer, killing that company was just an unfortunate side effect\" and they know the US court wont go after them. reply kayodelycaon 20 hours agorootparentprevApple doesn’t have 90% of the phone or computer markets. Microsoft had near total dominance over every desktop computer on the planet. The only argument than can come close to that is Apple has a large share of phone app revenue because people using Android don’t pay as much. reply chacham15 17 hours agorootparentI HATE these arguments. Peter Thiel talked about this in one of his lectures. He said: when you're a small company you define your market as small as possible in order to compete; when youre a large company, you define your market as large as possible in order to avoid antitrust. To illustrate the example, if you're the only grocery chain in California, you'd argue that \"the united states has many grocery chains and we're just one of them!\" thats true, but it doesnt mean that you arent acting monopolistically from within California. To bring this analogy back to Apple, we have to look at areas of competition: the area of competition that Apple meaningfully engages in is the apple app store. This would be the equivalent of california. Is any app in the apple app store prevented from meaningfully competing with Apple? Yes! 1. higher prices via app store fees 2. not allowed access to the same apis (e.g. files syncing in the background or no native messaging capabilities) 3. browser + messaging restrictions (e.g. cant link to a website which has any way of spending money). The reason I look at the \"app store\" as a place of competition is because apple is allowing competition in the space. If Apple removed the app store, or ONLY had Apple apps in the app store, then they would be acting perfectly fine IMO. But they allow this competition because it allows their platform to succeed, but they behave monopolistically wrt that competition to gain the upper hand over time. reply kayodelycaon 16 hours agorootparentInteresting, I haven't quite seen that perspective before. The reasoning of Apple creating a market is the first argument that actually makes sense to me. And I actually agree with the spirit of it. I definitely lean very heavily on Apple's valid security issues, but the fees and deliberate blocking of things that have absolutely no security concerns are a problem. reply r00fus 10 hours agorootparentprevThe grocery chain example has one very important difference: You can choose to buy Apple - they aren't a monopoly by any means in any market. Whereas with the grocery chains - you must go to the store in California or not get groceries. I think Peter Thiel's logic is flawed. Apple does not conglomerate, does not have a monopoly in any sense, and has actively avoided those situations. In order to please shareholders it does need to innovate and attempt new markets (Project Titan is a good example). Microsoft in the 90s was completely different - they wanted domination. reply graemep 15 hours agorootparentprevNo, but they control access to nearly 100% of the IOS market (all devices except a tiny number of rooted ones). That is a sensible definition in some cases because their terms for developers, app store charges etc. apply to that whole market. There is also a lot of lock-in and control: Spotify (much less any smaller business) cannot realistically get people to switch to Android to use their app - and with a duopoly that would not be an effective tactic long term anyway. reply r00fus 10 hours agorootparentDoes this apply to any software integrations provided by enterprise vendors? Can I use your logic to force Oracle/SAP to list random software on their \"integrations\" storefront? I think the answer would be no, and for good reasons. reply lukeschlather 19 hours agorootparentprevMicrosoft never attempted dominance of the kind Apple asserts. You look at threads like this: https://code.briarproject.org/briar/briar/-/issues/445 This doesn't just affect iOS users - if I want to use Briar to communicate, it's just impossible to involve anyone who has an Apple device. They don't need a total monopoly for this to be anti-competitive. They've made it so that if you have 10 devices and you want some feature like AirDrop, it's impossible for that feature to function unless Apple wrote it, because 4 of the devices are Apple devices and Apple monopolizes all network code to those devices. reply Retric 19 hours agorootparentThat's small potentates compared to what MS was doing. Windows was actively updated to break other browsers and then IE was actively leveraged to kill off competing browsers and server side web technology. For years the only thing that survived their onslaught was OSS. And that's on the tech side of things, business wise they would do even less ethical things. If Apple bought Briar specifically to kill it because it ran on Android then you might get a taste of what MS was getting up to. reply jwells89 19 hours agorootparentprevPlatform restrictions aside, maybe I’m missing something but requiring a permanent open socket seems like an awfully brittle design. There’s a lot of network conditions which phones are highly prone to that’d make apps with that requirement barely functional. reply greedo 16 hours agorootparentprevDr Dos was a perfect example of MS trying to squash competitors. MS didn't tolerate ANYONE competing with them. reply jonhohle 17 hours agorootparentprevSince people don’t seem to understand Microsoft’s anti-competitive behavior, I recommend you start with Groklaw[0] and decide if you want to dive any deeper. Microsoft was akin to US Steel or Standard Oil. We really haven’t seen anything like it since. 0 - http://groklawstatic.ibiblio.org/staticpages/index.php%3fpag... reply jjtheblunt 14 hours agorootparentprevOne way I think they differ is that Apple is, at its core, traditionally a hardware vendor, whereas Microsoft is not. You can, today, for example, install other OSes on Apple hardware, and they themselves have helped significantly to enable such until relatively recent architecture changes. I, for example, run Fedora on a modern Macbook Air and it's super useful. It's possible this reality deflects some of the antitrust attention. reply sumuyuda 2 hours agorootparentBut only on macOS hardware. All other devices have their boot loader locked. reply steve1977 19 hours agorootparentprev> having the interfaces change with new releases every few years. That's a level of stability many mobile and web developers dream of today ;) reply tambourine_man 19 hours agorootparentWeb APIs are forever. What's unstable are the dreadful frameworks built on top of it that people seem to think are essential. reply jjtheblunt 14 hours agorootparent\"gopher\" feels forgotten! reply tambourine_man 13 hours agorootparentgopher is not web :) reply ubermonkey 20 hours agorootparentprev>Apple is predatory on a level that 1990s Microsoft could not have dreamed of. That's hilariously wrong. In the 1990s, there were only two computing platforms to speak of: Windows and the Mac. Windows had *90%* of the marketplace. Microsoft had deals with PC makers that required them to include (and charge for) a Windows license on ever machine they sold. When Netscape happened, Microsoft leveraged their immensely dominant desktop position to squeeze the upstart browser company out -- Gates famously said \"I'll cut off your oxygen supply,\" which is more or less an overt confession to monopolistic behavior. Nobody has anything close to the kind of market power and predatory behavior that MSFT had in 1996. To suggest otherwise is to either display profound ignorance of that period of time, or to show you have a weird axe to grind about Apple that causes you to overlook actual facts. reply jsnell 19 hours agorootparentYes, Microsoft had 90% of the desktop OS marketplace, and did use that dominance to gain an edge in other markets. But despite that large market share, their leverage was kind of limited by the platform being open. Apple, on the other hand, has unlimited technical leverage. They can make up whatever anti-competitive rule they want to, and enforce it totally. When Microsoft chose to neglect the web after IE6, it didn't take that long for competition to work around it. When Apple chose to neglect Safari in exactly the same way (and probably for the same reasons as Microsoft did with IE6), that was it. There was no workaround, nor any way for anyone to compete with a better browser. PC games are a >$50 billion market, with Microsoft only getting a small slice of that. That's because Microsoft wasn't just able to outlaw direct installs nor competing digital games stores. Games for Windows Live had to compete against e.g. Steam, with the predictable outcome. The size of the iOS gaming market is comparable. Apple collects a 30% cut of it, and will do so indefinitely. Competing with them is literally impossible; nobody else can make a different app store offering better terms or a better user experience. Apple is at around 60% market share in the US. The difference between 60% and 90% is a minor difference in scale. The difference between Apple's absolute control of iOS vs. the limited control that Microsoft had over Windows is a difference in kind. Apple got to choose whether competing software was even allowed on the platform. They could opening up special capabilities to only their own software, and actually enforce that (unlike Microsoft, whose undocumented APIs could still be reverse engineered). They can charge their competitors a 30% cut of revenue, while obviously ignoring it in their internal accounting for their own competing businesses. reply jwells89 19 hours agorootparentIE and Safari are vastly different situations. IE started off with near-total dominance and was truly left to rot, with the teams responsible for it having been scattered across the company because Microsoft was no longer concerned with cornering the market on web browsers. Safari on the other hand has only ever seen ~20% marketshare, has never been truly abandoned, and at least covers core competencies well (recall that IE couldn’t even handle transparent PNGs without DirectX filter hackery). IE made even basic static pages a pain to develop compared to Firefox, whereas most of Safari’s significant shortcomings have to do with more advanced functionalities. If Google got complacent and decided to reallocate nearly the entire Chrome and Blink teams to other projects leaving only tiny skeleton crews on them, allowing both Firefox/Gecko and Safari/WebKit to leave it in the dust in terms of features that would be almost a 1:1 analogue for the situation with Microsoft and IE back in the day. reply sgift 16 hours agorootparent> Safari on the other hand has only ever seen ~20% marketshare, has never been truly abandoned, and at least covers core competencies well (recall that IE couldn’t even handle transparent PNGs without DirectX filter hackery). IE made even basic static pages a pain to develop compared to Firefox, whereas most of Safari’s significant shortcomings have to do with more advanced functionalities. The definition of what is a core feature and what is advanced functionality changes with time. When IE6 came out it was the most advanced browser available, including support for what was considered advanced functionalities back then. But over the long time IE6 existed expectations changed and things which were once advanced or not even thought of were now core. The same is happening with Safari, with the difference that thanks to Apples blocking of different browser engines people cannot just switch to another browser and light a fire under Apples ass. reply jwells89 16 hours agorootparentMy fear is that no matter how good Safari is or gets, it won’t be enough to withstand the crushing force that Chromium has become. Devs would much prefer to test against only one engine if they had the choice, which I believe will ultimately end up stripping choice from the user. reply naravara 15 hours agorootparentMany choices have already been stripped. My electric company’s payments portal only works in Chrome. When I called their helpdesk to say it doesn’t work in Safari, the person told me to use their iOS app instead of my phone’s browser. When I told her I’m using Safari from my desktop she was confused. Her helpdesk script literally didn’t have a path in the reply tree for someone using a desktop and not using a chromium based browser. That should tell you how uncommon it must be. I guess everyone’s just using the App now and doesn’t notice. reply jwells89 15 hours agorootparentI’ve encountered this several times in the past few years too, usually in the form of sites acting bizarrely or functions not working in Safari and/or Firefox. There’s been some active useragent sniffing too, though ironically enough in those situations it’s usually enough to spoof the user agent after which the sites in question as far as I can tell work properly. Either way it gives the impression to non-technical users that non-Chromium browsers are subpar even if site devs are to blame (especially when “upgrade your browser” messaging is misused to equate your up-to-date Safari/Firefox with some derelict version that hasn’t been updated for years) and will only serve to strengthen Chrome’s hold. The only recourse Mozilla and Apple have are to perfectly emulate Chrome’s behavior and in the case of UA sniffing even pretend to be Chrome, which sounds an awful lot like the bad old days of Gecko needing to emulate IE to make sites work in Firefox. reply tambourine_man 18 hours agorootparentprevBesides, IE was closed source and WebKIt is open source. So are all other engines. And web standards are really a thing now. The late 90s early 2000s were a dark period, software-wise. Nothing compares to that, thankfully. People either forget, weren't there or were just oblivious Windows users. reply Timon3 15 hours agorootparent> Besides, IE was closed source and WebKIt is open source. So are all other engines. And web standards are really a thing now. Even today, this doesn't mean much if Apple decides to ignore web standards that are harmful to their business model. Web Notifications were implemented by Safari Desktop, Chrome Desktop and Firefox Desktop[0] in 2012 - and Safari Mobile in 2023. That's 9 years that a pretty useful and important API couldn't be relied on by PWAs. And this is just one of many examples. This has a real effect on what technologies developers can actually use, which is fairly similar to the effect IE6 had on newer APIs during its time. [0]: Chrome & Firefox desktop because I can't find a way to see historical dates for mobile on caniuse - but it was around that time, or a bit later, for mobile as well reply lakpan 17 hours agorootparentprev> web standards are really a thing now. Chrome starts a proposal, implements it and “intends to ship” it before anyone really has time to discuss it. Recently (2 months ago?) they introduced a new WebExtension API that nobody cared about nor really understood the need for, but it’s still there, making it look like it’s a “standard” reply tambourine_man 17 hours agorootparentIt’s far from perfect but no comparison to our lowest point in history. reply jonhohle 19 hours agorootparentprev> Microsoft had 90% of the desktop OS marketplace As mentioned elsewhere, they didn’t get to 90% by selling to a free market. They demanded exclusivity across the entire PC market. They didn’t just let IE stagnate. They told Mosaic they would license their renderer for a percentage of sales and then gave IE away for free. It would be like Apple saying if you wrote an iOS app you couldn’t write an app for any other platform. reply evilduck 19 hours agorootparentprevAt what point did Apple neglect Safari? They didn’t chase Google to implement features that solely benefit ChromeOS, but iPhones have completely dominated Android and any other browser engine for mobile rendering performance since the iPhone 4. A miserably slow browser that can receive push notifications is pointless. reply tambourine_man 19 hours agorootparentExactly. People claim Apple neglects Safari because its priorities are not aligned with Chrome's. Maybe not allowing a website to store GBs of data, run in the background or show notifications is a feature for some users. reply naravara 15 hours agorootparentIt’s really only been in the past 3 years or so that I’ve started to see real pushback from Devs on Apple being overly pushy. Prior to that so many of the complaints were on the order of “Apple won’t allow me to do [insert insanely user-hostile or non-performant thing here] woe is me!” reply tambourine_man 14 hours agorootparentI've been a huge fan of Safari since its introduction. My only major complaint is that they replaced the old dev tools by this current atrocity which forces me use Chrome for development. reply no_wizard 16 hours agorootparentprev>When Microsoft chose to neglect the web after IE6, it didn't take that long for competition to work around it We have different definitions of long, and perhaps very different memories of the web from the years 2001-2012[0] almost a decade for any real traction for competing web browsers to put a dent in Microsoft, and that is after the anti-trust trial and the EU browser ballot mandate. Don't forget, it was so entrenched that even by 2015 IE had over 1/3 of browser share, really only defeated by Google Chrome (which had the infamous \"kill IE\" initiative[1]) in 2013 or there about. To put it simply, a decade of dominance is a long time. [0]: Data supports that Internet Explorer had majority market-share up through the first half of 2012 (globally), per https://gs.statcounter.com/browser-market-share/all/worldwid... and in North America, it held on to majority position through at least 2013. [1]: https://www.theverge.com/2019/5/4/18529381/google-youtube-in... reply greedo 16 hours agorootparentprevYou're comparing Microsoft's behavior now, after they've been smacked down in 1994 to how they operate now. MS had a long track record of breaking applications that competed with their offerings. If they hadn't been forced into the consent decree, they'd be 1)in a much stronger position than now, 2)behaving much worse.IE6 was released in 2001, after the consent decree. So the reason other browsers were able to compete was because MS couldn't kneecap them.30% is a huge difference.What does MS charge game owners on Xbox? Sounds like the exact same thing. Xbox dominates the US console market [1] far more than iPhone does. [1] https://gs.statcounter.com/os-market-share/console/north-ame... reply JimDabell 17 hours agorootparentprev> When Microsoft chose to neglect the web after IE6, it didn't take that long for competition to work around it. When Apple chose to neglect Safari in exactly the same way (and probably for the same reasons as Microsoft did with IE6), that was it. There was no workaround, nor any way for anyone to compete with a better browser. This isn’t even remotely true. The entire front-end web development ecosystem came to an absolute standstill in terms of browser support for five entire years because Microsoft decided that, since they had killed off the competition, they didn’t need to work on Internet Explorer any more. That’s five whole years without a single new release of Internet Explorer, which had >90% market share. And then after they restarted development, the next version mainly focused on adding tabs and a few CSS selectors. It wasn’t until Internet Explorer 8 was released that it actually started to properly move forward again. But web developers still had to wait years before enough people had upgraded because Internet Explorer wasn’t evergreen and people didn’t upgrade very often. Supporting Internet Explorer 6 was still a battle developers were fighting in 2010, nine years after it was released. Apple releases new major versions of Safari with improved support for web standards every year like clockwork. And people upgrade promptly. And Apple take part in interop efforts with other browser vendors. The two situations are absolutely nothing alike. reply dangus 19 hours agorootparentprevThe platform wasn’t “open.” Technologies like ActiveX were ways that Microsoft was trying to make the Internet depend on owning a Windows PC. Everything surrounding IE was an attempt to make other web browsers completely unviable. I don’t know if you remember how it was back then, but huge chunks of the Internet simply did not work if you weren’t using Internet Explorer. They would also sink your PC OEM business if you dared to ship a computer with an operating system alternative. They used their 90% marketshare to force companies like Dell to only sell computers with Windows installed. If someone like Dell wanted to provide an alternative like BeOS, Microsoft could terminate their contract or raise their royalty rate to unsustainable levels. reply ubermonkey 17 hours agorootparentprev>They can make up whatever anti-competitive rule they want to, and enforce it totally. On iOS, maybe. MacOS isn't controlled in that way. And, again, if you don't like iOS or Apple's rules, there's another completely viable OS you can use on mobile that actually has a larger global market share, so there's no monopoly play here. reply overstay8930 10 hours agorootparentprevDude what the hell are you smoking Microsoft was 95% of the personal computer market in 1996 you can easily purchase an Android phone with no Apple influence. Hell you don’t even need to buy a smartphone, That’s how little it has to do with the modern day. If you wanted the internet you had to go to Microsoft. reply atkailash 19 hours agorootparentprevIsn’t it slightly different since Apple designs and sells the hardware with the OS, whereas Microsoft was manioulating entirely separate hardware companies and not designing their own hardware product (until recently) reply carlosjobim 20 hours agorootparentprev> Apple hasn't been just making competing harder, but outright impossible. That's why Android marketshare is much larger than iOS marketshare and why Windows marketshare is much larger than MacOS marketshare. After reading maybe 8000 posts complaining about Apple \"monopoly\" I just can't get my head around how hackers can earnestly believe such a thing. reply lukeschlather 19 hours agorootparentThis isn't about marketshare, it's about Apple making things impossible. You're a hacker, tell me how I can solve this problem: I am with a friend in a place with no Internet access. They have an iPhone. I have an Android phone. We don't have any cables. My friend took a video and I would like them to send me the video right now over Wifi. I think there are some ways I could solve it, but I don't think there are any ways I could solve it that would be reliable for nontechnical users (and even for me it's hard.) This isn't an accident, Apple is abusing their control over iOS to make this sort of thing impossible. And it's worse than anything Microsoft ever did. Apple is actively standing in my way, even though I don't own any iOS devices. reply meindnoch 17 hours agorootparentYou've been able to access SMB shares from the Files app just fine for years now. Linux supports SMB. Windows supports SMB. You're welcome. reply beeboobaa 14 hours agorootparentSo now I have to lug my synology file share around just in case I want to share a file with a friend, instead of just transmitting it wirelessly? reply tambourine_man 18 hours agorootparentprev> This isn't about marketshare, it's about Apple making things impossible. Of course it is about marketshare. The more you have, the more you can make things hard for your competitors. The less you have, the more you're the weird one with the odd platform. > I am with a friend in a place with no Internet access That's a vanishingly rare situation not worthy of legislation, IMO. reply jonhohle 18 hours agorootparentprev> And it's worse than anything Microsoft ever did In 2003 I bought a PowerBook. It was the only commercially available laptop that could be bought without buying a Windows license. Microsoft had put any competitors out of business by that point and OEMs weren’t ready to sell Linux laptops. Apple only existed because Microsoft injected them with cash to show they had a competitor. Get a USB drive. reply CharlesW 18 hours agorootparentprev> You're a hacker, tell me how I can solve this problem: Feem, but I'm interested to learn more about this contrived example where phones don't have internet access and so can't use SHAREit, Send Anywhere, or any number of other solutions. reply lukeschlather 15 hours agorootparentNot a contrived example. happens to me often and iOS users are paralyzed when AirDrop doesn't work. (And like, it is possible but it's a lot harder than it should be.) People who transfer lots of media for work don't even bother, they just accept AirDrop as the only functional solution, which means you carry an iOS device. reply CharlesW 15 hours agorootparentGotcha. In addition to the off- and online apps above, it looks like web-based solutions like snapdrop.net work on Safari/iOS as well. reply evilduck 18 hours agorootparentprevWithout prior downloads happening before going offline I’m not sure you could share a file between mobile operating systems out of the box, regardless of which OS is trying to supply the file. If I’m allowed to pre-download software before encountering your scenario, running a web server or an app that serves FTP or other server types hosting files is trivial on either OS. iOS may require shuffling the video file into an app sandbox first. reply ahepp 14 hours agorootparentprevIt looks like Samsung and Google only merged their airdrop utilities in Summer 2023, so how would you have done this with two different android phones? reply carlosjobim 17 hours agorootparentprevThe answer is obvious, play the video on one phone and film the screen with the other. reply kayodelycaon 19 hours agorootparentprevIt makes a lot of sense to me. I think a lot of people were less affected by Microsoft’s actions than by Apple’s. After all, Microsoft didn’t stop people from making money off their platform or modifying their computers. And Microsoft was three decades ago. Apple is causing people far more harm now than they remember Microsoft doing in the past. (Assuming they were around for it in the first place.) Edit: I should have said \"a lot of the people talking\" instead of just \"a lot of people\". I was around for Microsoft and would classify them as evil. The worst you can say about Apple is they are greedy. For all of Apple's flaws, they are competing in the consumer market on merit. reply lolinder 19 hours agorootparentIf by \"people\" you mean \"a vocal minority of app developers\", then sure. But the average iOS user not only doesn't care about Apple's app store policies, they actively appreciate them (whether they know it or not). All apps being required to use a single payment system is a great example: it sucks for the app developers but only for the app developers. From the consumer perspective, Apple's app store is the only place in the internet economy where they're not subject to pervasive dark patterns trying to get them to not cancel a subscription, not back out of a free trial, or otherwise separate them from their money. People pay good money for Apple phones in part because Apple successfully protects them from predatory anti-consumer behavior. Contrast that with 90s Microsoft, where people bought Windows because that's mostly all that was available (because Microsoft made sure of that). Today there are more non-Apple phone choices than Apple choices, and yet a substantial number of people consistently choose Apple. reply havaloc 18 hours agorootparentI'm both a developer, and a user. As a user I love the ability to easily turn a subscription on and off, without having to call someone/chat with someone. Also, did you know that a requirement of the app store is you can delete your data from within the app? It's a great way to reduce your digital fingerprint. As a developer, we rolled out in app subscriptions and our revenue essentially tripled. They handle refunds. That saves me a LOT of time. Perfect? No, but I do as most pragmatists do: compromise and make the best of it. reply Hikikomori 16 hours agorootparentIn Europe we can do that without Appstore. reply nonrandomstring 17 hours agorootparentprev> doesn't care about Apple's app store policies, they actively appreciate them (whether they know it or not). Don't think anyone actually noticed this fly past yesterday [0] but as I commented here [1] Apple have set out their position clearly as patrician and protective (under the dishonest guise of \"security\"). This is right for some people. There is a place for the digitally bewildered, like a kind of care home for those who've basically given up on trying to make sense of their place in the digital world, given up all agency, and essentially conceded control of their lives. Ironically the polar opposite of Apple's 1984 Superbowl advert [2]. But we really ought to tell them they're being put out to pasture. [0] https://developer.apple.com/security/complying-with-the-dma.... [1] https://news.ycombinator.com/context?id=39580678 [2] https://en.wikipedia.org/wiki/1984_(advertisement) reply jonhohle 18 hours agorootparentprev> After all, Microsoft didn’t stop people from making money off their platform or modifying their computers. That is literally what they did. Be built a business, lined up customers, and had every one of them fail because of exclusivity agreements with Microsoft. Unless you were a vertically integrated software/hardware business you couldn’t get your OS on PCs because of Microsoft. It might seem quaint now, but there was a market for web browsers! They weren’t free until Microsoft duped Mosaic into giving them a license for a percentage of sales and then gave it away for free as Internet Explorer, instantly destroying any market for selling web browsers. That’s not only preventing others from making money on their platform, but anyone from making money in that entire segment. Literally the entire world was affected by Microsoft’s business practices as the PC industry was booming. reply nemothekid 19 hours agorootparentprev>After all, Microsoft didn’t stop people from making money off their platform or modifying their computers This is incredible whitewashing of Microsoft. Microsoft didn’t stop people from modifying their computers? Part of the anti trust case was Microsoft ending licenses with OEMs if they dared to sell a PC with Linux installed. reply kayodelycaon 18 hours agorootparentI was referring to after purchase. Wasn't it still possible to install Linux on computers that came with Windows? Granted, drivers were a massive issue. And if anything, this only furthers the point that Apple isn't anything close to what Microsoft was doing. reply cpuguy83 19 hours agorootparentprevThink in terms of market power, not share. reply Chris2048 19 hours agorootparentprevImplicit to that statement is \"on apple machines\". Android market share is on non-macs, an non-iphones. reply beeboobaa 21 hours agorootparentprevSo I guess apple learned how to be slightly more subtle, and to let their rabid fanbase do some of their work for them. It's still wild to me how they managed to get people to argue against the freedom to install whatever they want on their device. reply lolinder 19 hours agorootparentAndroid and F-Droid user here. I choose to buy hardware that allows me to install what I want, but I also consistently advocate for the right of consumers to buy into a closed ecosystem. There is a large class of technology users that is uninterested in installing whatever they want on their device. They want their device to be predictable, they want their app store to be curated, and they don't want to deal with dark patterns on a company by company basis. These customers choose Apple because it provides a tightly integrated experience. Forcing Apple to alter enough of their rules will eventually cause that user experience to no longer be an option, which is bad for consumer choice. reply lukeschlather 19 hours agorootparentThe problem with Apple's walled garden is that there's literally zero consumer choice for interoperability. If you have 10 devices and 4 of them are Apple, it doesn't matter that 6 of them are running F-Droid. Anything that requires you to install your app on all 10 devices is impossible to do. Everyone is limited to what Apple allows in their walled garden whether they choose it or not. reply lolinder 19 hours agorootparentYes, if you buy walled-garden devices you won't have a great time trying to install things that were meant for devices outside of the walled garden. I can see how that is unfortunate, but I'm not convinced that it's an argument for tearing down the walls. The alternatives that most here on HN are pushing for would ultimately lead to a walled garden no longer being an option for consumers at all. I think the status quo is preferable: consumers who don't want to participate in the walled garden can choose to not buy devices that are explicitly sold as being part of said garden. reply beeboobaa 18 hours agorootparent> The alternatives that most here on HN are pushing for would ultimately lead to a walled garden no longer being an option for consumers at all. I think the status quo is preferable: consumers who don't want to participate in the walled garden can choose to not buy devices that are explicitly sold as being part of said garden. Why do apple fans always come up with bizarre fantasies like this? This isn't a problem for Android today, and it won't be a problem for mainstream apps when apple is finally forced to open up thanks to the EU. More than 99% of the users will never install anything other than from the official Play store. But they are allowed to without Big Daddy Apple telling them \"No\". And that's what matters. reply lolinder 18 hours agorootparentDid you miss my first comment where I said I'm an Android and F-Droid user? It's not like I don't know how Android works. The only Apple device I use is my work-issued MacBook. The difference between Android and iOS is that Apple actually has rules that are strict enough to motivate the likes of Facebook to actively encourage people to side load if given the chance. I'm not comfortable looking at Android as a precedent, because Google is extremely lax with what kinds of things they allow on the Play Store, so the path of least resistance is acceptable to basically everyone. I do not believe the same would be true of Apple's store. reply beeboobaa 16 hours agorootparent> because Google is extremely lax with what kinds of things they allow on the Play Store Not really. They are pretty strict, actually. > The difference between Android and iOS is that Apple actually has rules that are strict enough to motivate the likes of Facebook to actively encourage people to side load if given the chance I doubt it. Especially after Apple will have submitted to the rest of the DMA. reply smoldesu 3 hours agorootparentprev> because Google is extremely lax with what kinds of things they allow on the Play Store And Apple is known for being just so diligent with their App Store reviews, huh? https://www.pcmag.com/news/beware-theres-a-fake-lastpass-app... reply kjreact 16 hours agorootparentprev> zero consumer choice for interoperability I can use WhatsApp, Signal, Google Chat for messaging instead of iMessage. I can use Spotify, YouTube Music, Sirius instead of Apple Music for streaming music. What do you mean by zero consumer choice for interoperability? I see plenty of choices. > Everyone is limited to what Apple allows in their walled garden whether they choose it or not What software is Apple not allowing? I see competitors’ software available on their platform. The only issue is that these competitors want to avoid paying fees to Apple and argue under the guise of protecting consumer freedom. reply naravara 15 hours agorootparent> What software is Apple not allowing? I see competitors’ software available on their platform. The only issue is that these competitors want to avoid paying fees to Apple and argue under the guise of protecting consumer freedom. Porn apps and hate speech mostly. They supposedly also bar a lot of outright scams and malware, but a lot also sneaks through. reply FireBeyond 13 hours agorootparentAnd yet they allow a lot of gambling and pseudo-gambling apps (probably because at least the latter makes bank with IAP). It's a strange morality that says \"porn bad, gambling good\". reply r00fus 10 hours agorootparentprevApps exist on multiple platforms just fine. The fact that dealing with App Store approval rules & timing sucks is just a cost of doing business. Plenty of outfits deal with it to get the exposure/userbase or ignore it. reply troupo 18 hours agorootparentprev> Everyone is limited to what Apple allows in their walled garden whether they choose it or not. And those limitations are? iMessage? The world outside the US doesn't use iMessage that much. It's Whatsapp, and Facebook Messenger, and Telegram, and Viber, and... Apple Photos? I have Google Photos on my iPhone, and they work better and faster than Apple's own Actually, I struggle to think what Apple-exclusive things I use apart from Safari (and I've been using it on macs since 2007 or so). reply lukeschlather 10 hours agorootparentSo, one example I can give is Briar, which is a distributed messaging app. I don't know that I actually want to use Briar, but in order for it to be useful to me it would need to run on iOS, since I have too many people I would want to use it with who use iOS. You can't see the things that aren't allowed, because they don't exist. The Briar team hasn't even built an iOS app because they know it would be rejected. reply troupo 2 hours agorootparentOf course Briar would not be rejected. It's not on iOS for a purely technical reason: it just wouldn't work there the way it's designed to work: - https://code.briarproject.org/briar/briar/-/issues/445 - https://sourceforge.net/p/briar/mailman/message/36415432/ reply naravara 15 hours agorootparentprev> They want their device to be predictable, they want their app store to be curated, and they don't want to deal with dark patterns on a company by company basis. These customers choose Apple because it provides a tightly integrated experience. I’m glad someone in this thread gets it. If anything, my main criticism of Apple’s App Store curation is that they have too light of a hand on cracking down on scammers and other abusive business practices. Indie developers making honest mistakes get jerked around but the professional skeezeballs keep gaming their way through. But the tone of a lot of the regulation seems to be trying to push Apple to make the thing I value about its ecosystem worse while adding me nothing I actually care for. With the phone, in particular, I am almost always using it while I’m half distracted with something else. I’m much more open to tinkering on my PC or even my tablet, but as far as I’m concerned having my phone being as locked down as can be and imposing as little additional cognitive burden on me as possible is a huge feature. reply Ajedi32 17 hours agorootparentprevI'm sympathetic to the idea that users should be able to choose to remain within a walled garden if that's what they actually want. There are benefits to Apple being able to use their market position to enforce certain rules on behalf of their customers. The problem is that same leverage can just as easily be used to enforce rules that benefit Apple at the expense of their customers. In theory competition from other platforms limits the extent Apple can abuse their position in that manner. But there's only one other major platform in the mobile space. That's not a lot of options for the market to optimise around. Further, Apple is a large, vertically integrated company which does a lot to try to lock consumers into their ecosystem. If I'd like to chose a more open mobile phone operating system but feel like I can't because Android isn't allowed to interoperate with iMessage or AirPlay or AirDrop or AirTags or Apple Watch or iCloud and I like those products and want to continue using them... well, that's a problem. There's also the software freedom issue. As a consumer, once Apple sells me a piece of physical hardware it should no longer be their place to dictate what software I'm allowed to run on that device. If I want to install an app that's not on Apple's App Store but Apple doesn't want me to, there's no question in my mind as to whose rights should win out in that scenario. Overall, my take is that Apple should be allowed to offer a closed ecosystem to users who want that, but they shouldn't be allowed use technical measures or anti-competitive bundling to force or coerce consumers into remaining inside that ecosystem. If consumers want to buy their apps exclusively from Apple's app store because they see a benefit from that, then they should be allowed to. But if they don't, Apple shouldn't be able to hold their entire ecosystem hostage from the consumer as leverage to prevent them from leaving. I realize this gives Apple a bit less leverage in their ability to advocate on behalf of their customers when dictating the terms under which companies can sell products within their walled garden, since those companies will now have the option of going over Apple's head and selling their product directly to consumers, sans Apple's rules. But that's how free markets work, in contrast to the monopoly-like situation we're currently in. If enough consumers see the value in Apple's rules to avoid other app stores with less restrictive policies, companies will still have to comply with them to access those consumers. (I'll note this is basically how things already work on Android. Google still operates the most popular app store on Android despite competition being allowed, and they're still able to dictate the terms under which apps are allowed in that store.) But if those terms are so egregious that consumers start to prefer alternative app stores instead, that's a good thing too. reply valianteffort 20 hours agorootparentprevI wouldn't say the fanbase is arguing against it. Apple just have a good understanding of their average user. While you may have no issue installing unsigned apps and dealing with any troubleshooting, their average user, or most users, don't. It's not to say they don't have a financial incentive to disallow app installs outside of their App Store, but there is a predictable cost in terms of support man hours and device troubleshooting they will incur in addition to lost revenue. So long as it is legal for them to continue business as usual, they have no incetive to allow it and the large majority of the userbase don't care. reply amelius 20 hours agorootparent> While you may have no issue installing unsigned apps and dealing with any troubleshooting, their average user, or most users, don't. These average users shouldn't come near a MacBook then. reply rahoulb 20 hours agorootparentMany years ago I used to despair every time I used my dad's windows machine. Every time, his browser window was filled with loads of toolbars installed by various sites and utilities, leaving him with a tiny usable content area. I kept explaining why this happened and he never changed his behaviour. I got him to buy a Mac and for a few years things went well. But then he started complaining about it behaving weirdly or running slowly. I'd take a look and find he'd installed some free utility or some other odd bit of software that was screwing up his machine. I kept explaining why this happened and he never changed his behaviour. I keep telling him to get an iPad, as at least then everything would be sandboxed and iOS would kill background tasks. But he says, no, he needs a real computer. reply Terretta 19 hours agorootparentThis is the end user reality HN generally fails to empathize with. Users need computing consoles or appliances. PS. Now that you can dock an iPad with an external screen and keyboard and trackpad, try that for him, tell him it's the new Macpad Pro. reply havaloc 18 hours agorootparentI would encourage the HN audience to do tech support (outside of family) for a few friends/customers and charge for it. You learn a LOT. reply matheusmoreira 18 hours agorootparentYou should charge a lot. As much as a doctor. Because that's what you are, really. reply havaloc 18 hours agorootparentThe metaphor totally works because half your customers end up ignoring your advice anyway. reply amelius 15 hours agorootparentprevStill medical staff looks down on the IT crowd. reply Terretta 13 hours agorootparentprev> by Terretta 6 hours ago: Now that you can dock an iPad with an external screen and keyboard and trackpad, try that for him, tell him it's the new Macpad Pro. ROTFL, someone made this and posted to HN four hours ago: https://www.macstories.net/stories/macpad-how-i-created-the-... https://news.ycombinator.com/item?id=39592288 reply r00fus 10 hours agorootparentprevAny chance you could lock down his account (gatekeeper) and keep your own admin user? It's too bad there's no easy way to simply \"grant access\" to installs remotely (if you're the admin). reply beeboobaa 18 hours agorootparentprevAnd you feel like it is your responsibility to take control of his life, because you know better? For his own good? This is called fascism. If this is how he wants to live, it is his right as a free adult. Even if it annoys you. reply matheusmoreira 18 hours agorootparentprev> he never changed his behaviour That's the real problem. People just refuse to learn. They refuse to put in any effort at all. They don't want to have to think about what they are doing. It's making me become ever more elitist over the years. I no longer believe computers should be for everyone. Computers are world changing technology and they are wasted on people who don't give a shit. reply ben_w 20 hours agorootparentprevAverage users, don't. My sister does everything on a phone or tablet and doesn't see the point of a \"proper\" computer, the guy at the bank plugs an iPad into a monitor. A MacBook is a work tool, like an angle grinder or a JCB. Most people don't know how to get the most out of them if they were given one. reply vundercind 19 hours agorootparentI’m a computer nerd, and even then, all my actually-important-for-real-life computing takes place on my phone. reply fkyoureadthedoc 19 hours agorootparentprevWhy not? The average user can get a ton of apps from the App Store. reply evilduck 19 hours agorootparentprevStatistically speaking, most of them don’t. reply beeboobaa 20 hours agorootparentprevYou're doing it right now. reply bscip 20 hours agorootparentI feel like that must be a sarcastic response without the \"/s\" reply nolok 20 hours agorootparentprevAhem, let me show you how that's not it : > While you may have no issue using a different browser/media player/OS/... and dealing with any troubleshooting, their average user, or most users, don't. reply throw0101a 20 hours agorootparentprev> […] but there is a predictable cost in terms of support man hours and device troubleshooting they will incur in addition to lost revenue. There is also the cost of externalities of more systems being infected that the rest of the connected world has to deal with. reply ben_w 20 hours agorootparentprev> It's still wild to me how they managed to get people to argue against the freedom to install whatever they want on their device. This isn't because of Apple, or at least it isn't in my case. Even as an iPhone app developer with the means to do so whenever I want, and even though I find Apple's content restrictions derisory and \"big brother\"-y, I actively don't want a system that can install \"whatever\". This is due to all the malware everywhere on the internet, and owing to this unfortunate reality, also means I think it is unwise even to have 3rd party libraries as project dependencies. I have an entire spare computer dedicated to learning scientific python because I don't feel I can rely on the absence of vulnerabilities in the stuff. This makes me sad. I miss the old days of 2002 where I didn't feel any reason to worry about random apps from random websites. But part of my carefree youth was that I had a Performa 5200 running OS 8, which wasn't a popular targets for malware authors, and another big part was that my bank didn't have an app. But before someone replies to point out that malware gets past the App Store tests: yes, of course it does, I see all this as a \"defence in depth\" strategy, not a silver bullet. reply didntcheck 19 hours agorootparentWhy would having the ability to install sideloaded apps change this? Apps will not be able to just install themselves or other apps without prompting. And we've already had zero-click exploits in Safari leading to entire phone rootkitting more than once without sideloading reply QuadmasterXLII 19 hours agorootparentMost of the apps I install are mandatory apps. It's not illegal to not have them, but if you don't install you can't access your doctor, file an insurance claim, bank, see your childs grades, enter the front door of your child's preschool, etc. All of these are malicious actors who will steal as much of my data as possible at all times, but I do actually need a preschool. As long as I can only install these through the app store, we are at an uneasy truce where these apps comply with app store rules. reply beeboobaa 14 hours agorootparentThe operating system can apply restrictions to apps, and in fact already does. It does not need the officially sanctioned store for this. You are arguing for detailed privacy controls for all apps (including those installed by Apple), which is a good thing to want! reply ben_w 19 hours agorootparentprev> But before someone replies to point out that malware gets past the App Store tests: yes, of course it does, I see all this as a \"defence in depth\" strategy, not a silver bullet. reply naravara 15 hours agorootparentprevThere are bad actors, like Facebook, who will prompt users to sideload so they can do bad things. If the bad actors were all shady fly-by-night looking companies it’d be one thing. But once the gates are opened, if it’s easy enough, you’ll see people being pressured to open the gates by all sorts of companies that they will have no choice but to comply with. Imagine, for example, if your insurance company, in order to pay your bill, starts making you install the app through some alternate pathway that brings a side-dish of keylogging to adjust your rates based on your browser behavior. reply jwells89 18 hours agorootparentprevIt’s really kinda crazy how normalized it’s become to pull in hundreds of dependencies (aggregate of dependencies and subdendencies) from unvetted sources on the regular. Combined with the popular insistence of developers to run with all protections disabled (SIP, immutable system, non-admin user, etc) it makes for a whole lot of easy targets. reply sgu999 19 hours agorootparentprev> I have an entire spare computer dedicated to > my bank didn't have an app Exactly my reasoning last week when I read about one more dependency attack on GitHub. Do you have a mac? What's your setup? I need CoreML so VMs are not an option, and I'm a freelancer so I use my own MacBook for work. I have a different session for each client, but at this point I don't even know if it's useful at all. Of course I also have a personal session with banking stuff... which is what made me realise I was probably doing something dumb. reply ben_w 19 hours agorootparent> Do you have a mac? Yes, two. (Well, three, but the third is in a box waiting to go to recycling or someone who wants it \"for parts\", as that's from 2013…) > What's your setup? It's literally an entire second mac. I bought an MBAir/M1 second hand, then was given an MBPro/M1 Pro; the Pro became my main machine, the Air became my \"mess around\" box. reply skydhash 9 hours agorootparentWhile I have two Mac (A mac mini as a workstation and a MBA for portability), I think next time I have to work on a contract, I’ll just setup a VM for the client. NPM always made me uneasy. reply airpoint 20 hours agorootparentprevYour post reads to me as if you thought all users shared the same exact needs as you do That’d be quite an ignorant POV to take on IMO reply chatmasta 20 hours agorootparentprevI think it's a combination of subtlety and normalization in the rest of the industry. reply pb7 21 hours agorootparentprevPeople buy product. People like product. People that don’t buy product try to change product. >rabid fanbase reply Elfir3 20 hours agorootparentUnfortunately the impact is not limited to Apple product, but a wider ecosystem. For example the browser. If a feature is not implemented in Safari, as only available browser on IOS, every website needs to be compatible with it. Songs must be available on Apple Music if you want to be available for Apple owners... reply ginko 20 hours agorootparentprevI own an iphone (because google is even worse) but I want to run Firefox reply nolok 20 hours agorootparentprevYeah, those linux and netscape crazies trying to stop OS monopolies and internet explorer bundling. Let Microsoft sell the better product to their users ! reply laborcontract 20 hours agorootparentprevYour framing is disingenuous by trying to frame this as an uneconomic and non-rational fanboying. I don't think you truly appreciate how comfortable I am handing my parents an iPhone, and how uncomfortable I was helping them with their Windows computer replete with multiple spyware browser toolbars back in the day. The onslaught of quasi malware on their computers was sickening, and they'd always ask me about insane popups that would show up due to some pre-installed thing nobody had control over. reply rbetts 20 hours agorootparentExactly. You gain a visceral appreciation for this when you watch your elderly parents get scammed for tens of thousands of dollars and taken of advantage of via technology channels. We've created a very nasty internet - one that isn't safe for the old or the young - and without the law enforcement or age boundaries that we use in the physical world to keep our public spaces suitable for the broader population. reply _gabe_ 19 hours agorootparentprevMy dad had installed a QR code reader app on his iPhone that was a monthly paid subscription. He was wasting hundreds of dollars every year for something his phone already did automatically for free by just opening the camera. Apple will happily let scammers create garbage paid apps to do the same thing their phones already do for free. No side-loading to let viruses in required. reply laborcontract 19 hours agorootparentHe can also ping apple's support to get a refund on that subscription, which they will happily do. reply _gabe_ 16 hours agorootparentWhy should a refund even be necessary? What’s the point of Apple’s highly selective review process if they let apps like this through? The only reason that I can fathom is it’s because Apple will gladly take a 30% cut of that revenue. And that is my overall point here. Everybody always jumps to Apple’s defense by explaining how iPhones are built for inept users that just want a phone that works with no BS. Therefore, they claim, it’s a good thing that Apple has a highly locked down environment. But what good is this locked down environment if Apple will happily allow scam apps to charge their customers for features the phone already does for free? What’s the difference between a user downloading a scam app from an untrusted source and downloading it directly from the App Store? If Apple is going to have a locked down environment, at the very least, they should make it effective. reply laborcontract 10 hours agorootparentNo ex-ante process will remove scams from the app store and I think you're bordering on arguing in bad faith. That's why the ex-post ease of refund part matters. reply realusername 20 hours agorootparentprevLike the appstore is any better, remember that the vast majority of money made on the appstore is for casino-like apps which I would not be comfortable giving to my parents or my family. reply laborcontract 20 hours agorootparentCasino apps werent pre-installed. Do you really not remember what Windows computers were like, fresh out of the box, back in the day? Hardware companies pre-installed any and all sorts of software from vendors. Spyware, adware, gambling games, toolbars, \"free\" trials to internet services. Computers used to be up to their neck in absolute garbage. reply realusername 20 hours agorootparentI do, people installed those malware by clicking on random ads, about the same way they now install those undesirable apps, funny how history kind of repeats itself. I guess the biggest difference is that the OS manufacturer now gets a cut on those. reply johnmaguire 19 hours agorootparentThey also came preloaded by manufacturers like Compaq, HP, and Gateway, who got a kickback. reply lotsofpulp 20 hours agorootparentprevAnd yet for some reason my dad never has any malware on his iOS and iPadOS devices, whereas he always did on Windows devices. reply realusername 20 hours agorootparentSince you talk about this subject, I actually had to remove two calendar malware on two different family iPhones (and it was pretty annoying to find where to remove that, even as a dev) so while it's true that there's less malware, it's not non-existent. And my point still stands anyways, the line between some of those top appstore casino-apps and malware is very blurry. reply beeboobaa 16 hours agorootparentprev> remember that the vast majority of money made on the appstore is for casino-like apps which I would not be comfortable giving to my parents or my family. Talk to your lawmakers to make these sort of apps illegal. Apple is not the government. reply realusername 16 hours agorootparentI never said that they were, I'm just deconstructing the marketing myth that the appstore is a wonderful safe place of productivity apps. reply ben_w 20 hours agorootparentprev\"Better\" != \"Good enough\" An app store (any app store, not just Apple's) can be the former without being the latter. reply beeboobaa 20 hours agorootparentprevStop infantilizing adults. Teach them computer safety. reply ben_w 20 hours agorootparentBack when he was still alive, I had to turn on \"Parental controls\" on my parent's Mac Mini, to stop my dad from accidentally removing things from the Dock and being unable to put them back. It took him years to realise Google search results had a scroll bar, and it wasn't just the 3 items that fitted on the screen. He wrote simulation software for the radio propagation of military IFF transponders before he retired, and his idea of \"computer safety\" was therefore somewhere between \"do not connect them to the internet\" and \"put them in a faraday cage\". reply beeboobaa 20 hours agorootparentOkay, that's a nice anecdote. Is your point that no one should be allowed to have the freedom to install whatever they want on their devices because your parent was incapable of dealing with the responsibility? Do you also want to take away everyone's cars because of traffic deaths? That's far more serious than someone's parent installing malware on their shiny digital toys. What about taking away kitchen knifes? Those things sure are dangerous! reply maccard 19 hours agorootparent> Do you also want to take away everyone's cars because of traffic deaths? That's far more serious than someone's parent installing malware on their shiny digital toys. This isn't the same thing. The equivalent in your anecdote is that Car A comes with a three point seatbelt that you don't want to use, and you want Car A to be sold without it so you can use your preferred safety device, despite multiple current owners of Car A telling you that they _want_ the three point seatbelt. And you can always buy Car B if you want to install a lap belt instead. reply ben_w 19 hours agorootparentprev> Okay, that's a nice anecdote. Is your point that no one should be allowed to have the freedom to install whatever they want on their devices because your parent was incapable of dealing with the responsibility? That this isn't \"infantilizing\". > Do you also want to take away everyone's cars because of traffic deaths? That's far more serious than someone's parent installing malware on their shiny digital toys. My mum did that for my gran when my gran got Alzheimer's and my mum realised she'd been driving for 6 months without paying road tax. And then the same happened to my mum. Also, you're giving the exact argument in favour of self-driving cars as soon as the tech is actually ready, and also the reason that crumple zones and seatbelts are mandatory. And also the reason we have (revokable) driving tests to be allowed to use the vehicles in the first place. > What about taking away kitchen knifes? Those things sure are dangerous! https://www.gov.uk/buying-carrying-knives has a high degree of popular support, and has done since I was a kid myself. reply beeboobaa 18 hours agorootparent> Also, you're giving the exact argument in favour of self-driving cars as soon as the tech is actually ready, and also the reason that crumple zones and seatbelts are mandatory. No, that's dangerous! Everyone should crawl everywhere while wearing an iHelmet if we were to apply apple's modus operandi to transportation. > And also the reason we have (revokable) driving tests to be allowed to use the vehicles in the first place. Hmm, so you think that maybe qualified people should be allowed to use dangerous devices? I'm fine with you taking devices away from your parents. That's up to you and your parent. > https://www.gov.uk/buying-carrying-knives has a high degree of popular support, and has done since I was a kid myself. Is your argument that your iphone should not be allowed outside where it might scare other people? Odd take. Even in the UK an adult is allowed to buy & posses a kitchen knife. reply ben_w 16 hours agorootparent> That's up to you and your parent. I wrote \"Back when he was still alive\", so… no. > Is your argument that your iphone should not be allowed outside where it might scare other people? Odd take. No, and I don't know why you'd think that. In fact, that's such a weird take, I think you wrote that in bad faith. > Even in the UK an adult is allowed to buy & posses a kitchen knife. Buy, yes. Possess in public without \"good reason\" as defined by the police and prosecution, no. reply laborcontract 19 hours agorootparentprevYou're missing the point entirely. The idea of a \"freedom to install\" is incidental to what happened in the market which is that Apple made a platform that people felt secure and happy in. Consumers Apple's product to the extent that Apple became a dominant force in the market. That's the house they chose to live in. It's a house in which no knives existed. Now you're saying \"let them put knives in, how dare you restrict them from having a knife!\" whereas most consumers are saying \"I like this house, wow living here feels great.\". It's not a knife issue. You're trying to make it a knife issue. The success of the iPhone was never obvious from day one. To attribute Apple's success to monopolistic behavior and consumer oppression, rather consumer behavior and expression, is just denying reality. reply beeboobaa 18 hours agorootparent> It's still wild to me how they managed to get people to argue against the freedom to install whatever they want on their device. reply ben_w 16 hours agorootparentAnd it is the \"they\" that I'm arguing against, not the rest of it. reply rlupi 14 hours agorootparentprev> It took him years to realise Google search results had a scroll bar, and it wasn't just the 3 items that fitted on the screen. I wonder if this is because today's interfaces are actually so much worse for scrollbar UX (usability and discoverability) than anything we had since they were invented 43 years ago. Just look at it... https://scrollbars.matoseb.com/ reply ben_w 13 hours agorootparentHe was using Vista, I think. Died a while ago now, it might have been XP. reply ascagnel_ 19 hours agorootparentprevMy dad, when was a kid, would bust out a soldering iron and he'd show me how to swap out the quartz timing crystals on digital wristwatches to repair them (or, for fun, make them intentionally run fast or slow). On occasion, he'd even do it on PCs to under- or over-clock them and make them last a little longer. By the time I was in college, he was on his way towards retiring, and knowing that I was into software, decided that I would be his IT support person. He went through various laptops and desktops (PCs, Macs, he even gave Linux a real try but gave up when he couldn't get his financial planning software to run on it), Android phones, etc. The one platform he liked using in his later years was iOS/iPadOS specifically because, in his words, \"there's not a bunch of other s--- here, and I don't need to manage it\". It's ease-of-use dovetailing with computer safety, and that's a _hard_ problem. reply jimbokun 20 hours agorootparentprevThe young and the old can have cognitive impairments making this very difficult. reply beeboobaa 19 hours agorootparentWould you give them a gun or a knife? Do you think no one should be allowed to own a gun or a knife? reply carlosjobim 20 hours agorootparentprevCars have seatbelts, airbags, ABS, crumple zones, etc. Almost everybody has to use a computer for work and other stuff, and they are willing to pay for a safe and sound system. They are not interested in becoming full time Arch Linux root administrators just to conduct their daily business on their device. Just like almost none of the people who drive a car are full time mechanics and welders. reply beeboobaa 20 hours agorootparentHilarious, good one. How did you make the leap from \"apple should allow people to install stuff if they wish\" to \"Arch Linux administrator\"? Be real. reply ziddoap 18 hours agorootparentThey were being hyperbolic to illustrate a point. Sort of like how you keep bringing up guns, knives, and car safety in a conversation about phone apps, to illustrate your point. reply piva00 20 hours agorootparentprev> They are not interested in becoming full time Arch Linux root administrators just to conduct their daily business on their device. They don't need to if they don't want to. I'd be one of the users who are not inclined to transform my iPhone back into my days using Linux on desktop and suffering the pains of doing it. I do have the option to turn my computer into that if I want, I do not have that option for my phone even though it's just a computer. No one would be forcing users into doing that, it's about allowing the users who want that control over their own devices. You are attacking a strawman. reply laborcontract 19 hours agorootparentAt what point should a private company be compelled to develop or expose APIs because you want to tinker around with their system? reply ImPostingOnHN 18 hours agorootparentMy system, not their system, and since you're asking for a personal opinion here: immediately. reply burnerthrow008 7 hours agorootparentWell, you own the hardware, but not the software. So at least part of the system is not yours. reply ambichook 5 hours agorootparentown the hardware but cant do what we want on it, because we're forced to use the software reply ImPostingOnHN 7 hours agorootparentprevSince we're discussing prescriptivism (how things should be) rather than descriptivism (how things are in a subset of legal jurisdictions): I didn't force them to leave a copy of their code on my device, but I own every single bit of electricity and magnetism and metal and plastic within it. I thus have the right to flip 0s into 1s, and 1s into 0s, or smash the whole thing. It's mine. Redistributing anything, admittedly, might be somewhat related, but would be a different discussion. Any effort to disenfranchise me from those rights is, in my opinion, unethical. If a company were to try to do so under the guise of a semantic argument, I would find it even more distasteful. reply stale2002 15 hours agorootparentprevGreat question, which has now been answered by new laws like the digital markets act! The \"point\" at which such things should apply is when they are large enough that anti-trust laws and pro competition laws apply to them. IE: when they are very very large and are worth hundreds of billions of dollars. reply eropple 19 hours agorootparentprev> No one would be forcing users into doing that, it's about allowing the users who want that control over their own devices. The moment this is a thing is when the social engineering attacks to get people to root their own phones, to set up untrusted app stores that firehose shitware down onto phones, etc. starts to happen. \"I'm from Microsoft, you need to add the Microsoft Store to your iPhone, here's how.\" I get your concern, and in a world less built on open grift I would share it, but our esteemed tech brainlords have built a tech ecosystem that exists to enable scams. (Hell, I almost got popped a few months ago by a scam, and I tend to be pretty cautious about things.) Unless the opt-in mechanism is something like \"open the phone and turn a jumper\" or \"buy a different model of the same phone\" I genuinely think it's way too dangerous for the average target of a social-engineering attack. People buy iPhones to not deal with this shit as much as is possible; taking that away hurts a lot more people than it helps. reply carlosjobim 19 hours agorootparentprevYou have that option, just buy another phone. Because there is no monopoly. Likewise if you want to install a non-Apple OS, there are thousands of different non-Apple models available for purchase. Apple offers security and convenience on their devices, which is why many people like them. Windows offers no security but some convenience, Linux offers security and no convenience. Like I said in another comment, if Spotify is not happy with Apple they can manufacture their own device and sell to consumers. People buy and use Apple devices of their own free will, and developers develop for their devices of their own free will as well. And there is plenty of competition in the market. reply stale2002 15 hours agorootparent> Like I said in another comment, if Spotify is not happy with Apple they can manufacture their own device and sell to consumers. Well, actually, it seems that spotify won the lawsuit. So, it is instead that Apple will have to pay a fine, and if they don't like it then they can pull out of the EU entirely. > people buy and use Apple devices of their own free will And Apple participates in the EU market of its own free will, and can shutdown if they don't like the law. And users who buy from Apple will be free to install or not install whatever they want on their devices soon. If those users don't like it, then they can simply not install apps that they don't like and are free to only install apps from Apple. reply piva00 19 hours agorootparentprevSure, I will buy another phone and all the apps I paid for in 15 years of usage. It's very trivial and simple. I invested money in the ecosystem and now there's vendor lock-in, I'm bound to it unless the usage becomes so unbearable that the pain to change vendors is less than continuing the one I have, the pain threshold only grows for each passing year I'm invested in the ecosystem. Do you comprehend that? Opening a phone for others to develop apps on top does not encroach anywhere into Apple's sale of phones, it just creates a massive wall to guard against other competitors selling apps to their platform. They charge for the phone, and now they extract rent from something they sold through the App Store. Please, think deeper about this, you are missing the point entirely. > Like I said in another comment, if Spotify is not happy with Apple they can manufacture their own device and sell to consumers. No, they can't. That's not their business, they don't have the capital to develop and foster a whole 3rd platform of devices + OS just to move away from Apple's ecosystem, that's the whole point of developing a platform: creating a massive lever against competition because of network effects. Network effects are very hard to break from, hence why most major tech companies try to leverage that (look at Meta/Facebook, they are shit but still it's very hard to compete against due to their size). > People buy and use Apple devices of their own free will, and developers develop for their devices of their own free will as well. And there is plenty of competition in the market. There's only 1 competitor in the market: Android, manufacturers using Android are not 1 competitor each since none of them are providing a full platform, the platform is Android on top of some devices. Free will is a massive naïvete to throw here. Is there free will when there are no other options to reach a market? Is there free will for online sellers in the USA to not put their products on Amazon since you'll be living on the outskirts of the market without any meaningful way to penetrate into it if other competitors have an advantage simply by paying the Amazon tax and selling their products through Amazon? I think you are too naïve and ideological, that blinds you from understanding market forces and markets in general. reply carlosjobim 17 hours agorootparent> Sure, I will buy another phone and all the apps I paid for in 15 years of usage. It's very trivial and simple. I know you're being sarcastic, but it is extremely trivial and simple to buy another phone. Because you can also keep the phone you have! There is a hang-up among hackers that think it is impossible to use different phones, different computers and different browsers. You can have one phone for this and another for that. You can use both vim and emacs to edit a document and you can have several computers with different systems. Hell, you can even have different sets of cutlery at your home. > No, they can't. They sure can. Being incompetent or lacking in capabilities is an acceptable excuse for a person, not for a billion dollar company like Spotify. I have here on my desk a pocket music player of excellent quality that was manufactured in 2023. It is not Android nor iOS and the company making it is much smaller than Spotify. So why can't they? \"Not our business\"? If they're that incompetent, the CEO and the leadership should apologise for being failures, step down, pay back to shareholders all salary they have received and go hide away. Yes there is free will for sellers to not put their products on Amazon. That's called retail, and it's a tough market. If your product is good enough you can sell it without putting it on a third-party marketplace. Just like with hotel aggregator websites and food delivery apps. Companies are doing fine without being there. > There's only 1 competitor in the market: Android There's thousands of different Android phones in the market, that's the competition. And it's not that difficult for a tech company to develop their own mobile OS if they'd like. Microsoft made Windows Phone which many people liked, Nokia made the excellent and ahead of its time Meego OS, BlackBerry 10 OS was excellent as well. And there is absolutely no blame on Apple for these tech giants discontinuing their OSes. They were discontinued because they weren't profitable enough for the owners taste. So now Apple should be forced to make sure that their competitors can make a profit from their customers? Because that's the core of the matter. Other companies do not want to do the work and investments that Apple did to get a loyal customer base willing to spend money, so they're using EU regulators to freeload. No sympathy to any of them. Make better ecosystems if you want to compete, you have the money and the means. Try giving a fuck about your customers and your business. That's something that the general public and consumers also would benefit from. reply marcosdumay 20 hours agorootparentprev> deliberate sabotage of open standards process to prevent the web taking off Whether Safari is this way deliberately or by chance, is something to be found on a court. But Apple's behavior here isn't different from the Microsoft one with IE. reply laborcontract 21 hours agoparentprevMicrosoft's market share was well over 90% in the 90s. The iPhone's market share is 60%. Very different market dynamics. It was a lot easier to see how Microsoft was being mean-spirited for consumers versus Apple. I'd argue that Apple's policies are consumer agnostic, and the only people really up in arms about Apple are developers and some of its partners. Also the US is no longer in a position to be punching it's own companies, given the onslaught of Chinese companies that are seeking to undercut in even auto manufacturing now. reply FirmwareBurner 20 hours agorootparent>Microsoft's market share was well over 90% in the 90s. The iPhone's market share is 60%. Very different market dynamics. Couldn't be more wrong. It's not even remotely comparable. Yes Microsoft had a 90% OS market share but it was an open OS running on open HW platform wich had many players, so users had the freedom to install whatever non-Microsoft SW and HW they wanted on it and SW developers could develop software for it without needing Microsoft's permission for sales or distribution on their OS, or giving them a 30% cut of their profits (other than buying a MS Visual C++ license maybe). While Apple might have an \"only\" 60% market share now (though it's currently 87% amongst US teens today[1] so it's only a matter of time till it reaches 90s' Microsoft levels of monopoly amongst US gen-pop) but it's a closed OS running on closed HW with attestation, so costumers and SW developers have absolutely no choice but to run only Apple approved content on them. Vastly more restrictive than what Microsoft of the 90's could even dream of. >It was a lot easier to see how Microsoft was being mean-spirited for consumers versus Apple. How? Just like Apple, Microsoft was only being mean spirited to SW and HW vendors and competitors, not to consumers. [1] https://www.barrons.com/articles/apple-stock-teens-iphone-e5... reply laborcontract 20 hours agorootparentThere was no reasonable alternative to Windows for any consumer. There's a reason that the 90% market share number was for Windows. Also, you could have developed whatever you wanted, but made an active choice to try to extinguish those applications with OEMs, ISPs, and content providers to squeeze out Netscape. It's truly lost on people today how Bill Gates was an absolute killer back in the day. > How? Just like Apple, Microsoft was only being mean spirited to SW and HW vendors and competitors, not to consumers. The consumer experience for iPhone users is much better than that of Windows users back in the day. People were scared of Windows. reply FirmwareBurner 20 hours agorootparent>There was no reasonable alternative to Windows for any consumer. Didn't stop you from selling your SW for the Windows platform without paying Microsoft a dime or Microsoft having any way to stop you. Look at WinRar or the thousands of other small companies who made a living selling SW for Windows without paying Microsoft a cent. >The consumer experience for iPhone users is much better than that of Windows users back in the day. UX is subjective and debatable. And there's a difference between one having poor UX, and one being \"mean spirited towards users\". reply laborcontract 20 hours agorootparent> Didn't stop you from selling your SW for the Windows platform without paying Microsoft a dime or Microsoft having any",
    "originSummary": [
      "The website reuters.com features an animation with a 1.5-second fade-in effect, necessitating enabled JavaScript and disabled ad blockers to view.",
      "Additionally, the site incorporates a captcha delivery script for security purposes."
    ],
    "commentSummary": [
      "The debate centers on Apple's anti-competitive behavior, notably concerning Spotify and their dominance over the App Store.",
      "Users question the efficacy of imposing fines on tech giants like Apple to deter anti-competitive actions.",
      "Comparisons are drawn between Apple's closed ecosystem and Microsoft's historical practices, emphasizing the challenges of interoperability and the significance of consumer awareness in online safety."
    ],
    "points": 549,
    "commentCount": 634,
    "retryCount": 0,
    "time": 1709554288
  },
  {
    "id": 39598189,
    "title": "Call for Return of Physical Controls in Cars by 2026",
    "originLink": "https://arstechnica.com/cars/2024/03/carmakers-must-bring-back-buttons-to-get-good-safety-scores-in-europe/",
    "originBody": "do that here, too — European crash tester says carmakers must bring back physical controls In 2026, Euro NCAP points will be deducted if some controls aren't physical. Jonathan M. Gitlin - 3/4/2024, 10:24 PM Enlarge / A car's hazard warning lights will need a physical control to get a five-star EuroNCAP score in 2026. reader comments 212 Some progress in the automotive industry is laudable. Cars are safer than ever and more efficient, too. But there are other changes we'd happily leave by the side of the road. That glossy \"piano black\" trim that's been overused the last few years, for starters. And the industry's overreliance on touchscreens for functions that used to be discrete controls. Well, the automotive safety organization European New Car Assessment Programme (Euro NCAP) feels the same way about that last one, and it says the controls ought to change in 2026. \"The overuse of touchscreens is an industry-wide problem, with almost every vehicle-maker moving key controls onto central touchscreens, obliging drivers to take their eyes off the road and raising the risk of distraction crashes,\" said Matthew Avery, Euro NCAP's director of strategic development. Advertisement \"New Euro NCAP tests due in 2026 will encourage manufacturers to use separate, physical controls for basic functions in an intuitive manner, limiting eyes-off-road time and therefore promoting safer driving,\" he said. Now, Euro NCAP is not insisting on everything being its own button or switch. But the organization wants to see physical controls for turn signals, hazard lights, windshield wipers, the horn, and any SOS features like the European Union's eCall feature. Tesla is probably at greatest risk here, having recently ditched physical stalks that instead move the turn signal functions to haptic buttons on the steering wheel. (Ferrari also has its turn signals on the steering wheel, but Ferrari does not appear in Euro NCAP's database so probably doesn't care.) Euro NCAP is not a government regulator, so it has no power to mandate carmakers use physical controls for those functions. But a five-star safety score from Euro NCAP is a strong selling point, similar to the Insurance Institute for Highway Safety's coveted Top Safety Pick program here in the US, and it's likely this pressure will be effective. Perhaps someone should start bugging IIHS to do the same. reader comments 212 Jonathan M. Gitlin Jonathan is the Automotive Editor at Ars Technica. He has a BSc and PhD in Pharmacology. In 2014 he decided to indulge his lifelong passion for the car by leaving the National Human Genome Research Institute and launching Ars Technica's automotive coverage. He lives in Washington, DC. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39598189",
    "commentBody": "European crash tester says carmakers must bring back physical controls (arstechnica.com)519 points by mbrubeck 8 hours agohidepastfavorite322 comments modeless 2 hours ago> Now, Euro NCAP is not insisting on everything being its own button or switch. But the organization wants to see physical controls for turn signals, hazard lights, windshield wipers, the horn, and any SOS features This is much more reasonable than I assumed. Unlike seemingly most people here I have no problem whatsoever with fan controls or audio controls or whatever on the touchscreen, as long as it is responsive (of course the vast majority of car touchscreens are not, but some are). However, the absence of a physical speed control for the windshield wipers is the single worst design flaw of Teslas. Or at least it was, until they removed the physical turn signal controls. I'm very much in favor of requiring safety critical controls that must be used frequently or urgently to be physical. reply ClikeX 56 minutes agoparentVolume control is a very commonly used feature in cars. That should definitely should be a physical button. I drive a Lynk & Co (reskinned Volvo XC40) and it has a rotary knob on the center terminal for fan speed, temperature, and volume. Which are all within reach without me having to look or lean over. There's also a volume button on the steering wheel next to my thumb, which is great. The only annoying part is that the left button pad on the wheel is the absolute worst. It's essentially a d-pad with a center press, but it's one single button cover. Which leads to a lot of wrong clicks. reply Slartie 8 minutes agorootparentBut you are not legally required to use the volume control regularly in order to drive safely. You are not even legally required to have any kind of volume control in your car. You are legally required to have and use the turn signals. You are legally required to have and use the windshield wipers (because you need to be able to see the road when it's raining). Same is true for the horn and hazard lights - those are safety-critical features, with their use at least partially regulated by law. While I agree that volume control should be a physical button due to my personal taste, I would not go so far as to mandate it legally to be a physical button, with the reason being that it is not a safety-critical feature. The market can figure this out by itself. But for safety-critical features whose swift and correct use is mandated and regulated by law, I would absolutely mandate them to be provided to the user in a way that supports the swift and uninterrupting use expected from the driver, and that means: physical controls, placed reasonably reachable. reply harry8 2 minutes agorootparentVolume control is pretty damn safety critical when the driver takes their eyes off the road to jab at the f&^king miserable touchscreen and accidentally wipes out your family. Sir Issac Newton wrote down the laws about piloting a couple of ton of steel some time ago. Very unlikely to be repealed. Every control that will be used by the driver for any purpose whilst the vehicle is in motion is safety critical. reply modeless 30 minutes agorootparentprevIs there a car without a physical volume control? Teslas have it on the steering wheel. What really kills me is my wife's Civic has no pause button at all, physical or otherwise. And it autoplays media on your phone when you get in. Don't want your phone to play whatever random YouTube video you happened to click on hours ago? Gotta pick up your phone to pause it there. And this doesn't happen right away, oh no, it takes at least a minute into your drive for the Bluetooth to wake up. reply PodgieTar 14 minutes agorootparentA Mercedes C-Class, 2023 model, also has no way to pause the music with physical buttons. It has a volume switch, and that clicks in, but doing the Airpod double click doesn't do anything. It just mutes the music. Instead you have to either use a weird capacitive DPad to navigate the Android Auto interface, or click the screen. It's terrible for UX. reply nottorp 4 minutes agoparentprev> with fan controls I want at the least a physical recirculate/bring air from outside button. The use case is coming up behind a vintage truck that was made before they even thought of pollution standards on a winding mountain road where you can't overtake and you need to pay attention to the road. And you also need to set the a/c to recirculate before you suffocate. For audio... with radio dying or dead I guess you can just run Spotify the whole trip. I'd still like volume and mute buttons. reply iforgotpassword 2 hours agoparentprevBut even then, how do you find the A/C or volume control on a touch screen without taking your eyes off the road, without accidentally touching something else? You just can't feel your way to the volume knob. Sure, you are a responsible driver and would only do that at a traffic light or other completely safe situation, but I don't know how many accidents touch controls in cars already caused with less responsible drivers, trying to adjust their AC while driving by an elementary school. reply oven9342 2 hours agorootparentPeople have already died because of the screens “The ship's user interface [touch screen] was found to have contributed to the sailors' confusion.” https://en.wikipedia.org/wiki/USS_John_S._McCain_and_Alnic_M... reply stby 1 hour agorootparentSeems to be a US Navy thing (https://en.wikipedia.org/wiki/Iran_Air_Flight_655#Potential_...). On a more serious note, there's a big difference between a vehicle like a car that is operated by one person, and a ship with a large crew with dedicated tasks. reply forgetfreeman 1 hour agorootparentTrue. With a car there are fewer eyes monitoring the situation and frequently little or no time to correct from a mistake. reply maccard 31 minutes agorootparentprev> You just can't feel your way to the volume knob Yes you can. My last three cars have had a volume knob under the left thumb. My current car has a second volume knob behind the gearstick, and the A/C is the only analog dial along the center console. I can easily adjust both without looking away. reply modeless 2 hours agorootparentprevVolume is on the steering wheel. A/C is automatic and I don't need to touch it the vast majority of the time. While driving the most I might want to do is adjust the temperature, and since it's not safety critical I can choose a convenient time. It's at the bottom of the screen so it is easy to do by grabbing behind the bezel and only requires a quick glance to align your thumb. In practice I glance at physical A/C controls when adjusting them too, so I don't see a big difference here. If you really need to adjust the temperature so often and can't stand the thought of using the touchscreen, you can assign that function to the left scroll wheel. reply franga2000 1 hour agorootparentIt seems like you have a more sensible car than many. I was recently in one with really dumb automatic AC that needed adjustment very often, the AC settings were in a menu behing two clicks, there was no convenient landmark to orient my hand without looking, the temperature slider was very small and dense needing high precision and the temperature text was rather small. And the steering wheel buttons are, of course, not remappable. I drive about a dozen different modern electric cars on a regular basis (carsharing) and all of them have at least a few of the above flaws (if not for AC then for something else that needs adjusting while driving). reply generic92034 1 hour agorootparentprev> While driving the most I might want to do is adjust the temperature, and since it's not safety critical I can choose a convenient time. That is good reasoning, but are you sure a high number of drivers are also waiting with adjustments, instead of fumbling on the screen while continuing to drive? reply Kuinox 46 minutes agoparentprev> I have no problem whatsoever with fan controls Fan controls are important to remove fog on the windows. I should be able to enable it without looking away the road. reply modeless 40 minutes agorootparentIf you live in a place where this is frequently required, in a Tesla you can put the defrost button on the shortcut bar at the bottom of the screen, where it only requires a quick glance to align your thumb to activate it. Which is actually easier than many cars with physical controls that may require multiple button presses and/or knob turns to configure all of the correct climate settings for defrosting in the current conditions. reply arethuza 36 minutes agorootparentMuch easier to have a single physical button. reply modeless 25 minutes agorootparentOverstated. It's at best marginally easier to hit a random physical button on the center console than to hit a single button on the bottom bar of a Tesla screen, where it is very easy to grab by design. I generally don't hit those physical buttons without at least a glance, and that's all it takes for the Tesla bottom bar buttons too. This doesn't save the wiper situation, though, because that requires navigating menus. Clearly far worse than a physical control in that case. reply klausa 8 minutes agorootparent> It's at best marginally easier to hit a random physical button on the center console than to hit a single button on the bottom bar of a Tesla screen You presumably typed this out on a keyboard of some sort. Which one can you do more confidently and reliably without looking: — hitting the \"u\" key on a physical keyboard without looking — hitting the \"u\" key on a touchscreen keyboard on your most used mobile device? reply modeless 2 minutes agorootparentI can touch type and hit 'u' without looking, sure. But only if my hands start on the home row. This has no relevance for the situation under discussion. I cannot hit the 'u' key on a physical keyboard confidently and reliably without looking if it is mounted on the center console of a car and my hands start on the steering wheel. I always look to position my hand to hit those buttons. Meanwhile, the Tesla bottom bar buttons are far easier to hit than any key on my phone keyboard because they are much larger and mounted in a fixed position relative to me with a built in place to rest and align my hand. arethuza 14 minutes agorootparentprevWell, I'm the customer and I want physical buttons! reply sensanaty 29 minutes agoparentprevTeslas don't have physical blinker switches?? That is pure lunacy, are you telling me they're touchscreen activated now? reply OvbiousError 25 minutes agorootparentFrom the article: > Tesla is probably at greatest risk here, having recently ditched physical stalks that instead move the turn signal functions to haptic buttons on the steering wheel. reply sensanaty 23 minutes agorootparentThat's legitimately insane to me. Plus you don't even get that satisfying clunk of turning the blinkers on. God the future is turning out so lame... reply LeoPanthera 28 minutes agorootparentprevThey are touch areas on the steering wheel. It's not on the screen, but they're not buttons. They don't click. reply CorrectHorseBat 21 minutes agoparentprev>However, the absence of a physical speed control for the windshield wipers is the single worst design flaw of Teslas. The fog lights are much worse reply sundvor 2 hours agoparentprevTesla wiper? Just push the wiper button on the left stalk and you can cycle through the speed settings with the left funky switch (multi function button) in addition to the on screen display options that then pop up. Very easy to do. Also this: https://youtube.com/shorts/3eKcDOHVZWc?si=-jL4o4Bhu-2y0ibj Doesn't cover the left multi function button feature after short pressing the wiper button. (Model 3, 2022, Australia). reply modeless 2 hours agorootparent> Just push the wiper button on the left stalk and you can cycle through the speed settings with the left funky switch Yes, they added this relatively recently. For the first few years I owned the car this was not possible. Also, guess what, it still sucks! More steps than a physical control, fiddly because of the short timeout, and still requires an extra step of looking at the touchscreen because you can't know which way to push the wheel without finding out the current setting. Is it on \"Auto\" or \"Off\"? They're at opposite ends of the menu. Acceptable for something less important like setting the A/C temperature; definitively not acceptable for something safety critical like wipers. reply Dunati 1 hour agorootparentI have a 2018 model x with a traditional wiper dial stalk and it almost never works. It has markings for 5 settings and most of the time it just ignores the setting you have it on. The auto setting doesn't seem to change the behaviour at all. My model 3 was annoying with the wipers on the touchscreen (mostly) but at least they worked consistently. reply darkwater 1 hour agorootparentprevI had cars in which for the life of me I could not ever remember which part of which stalk controlled the wipers and which part controlled the lights. Ah, and then there are Volkswagen and Opel which had the light control as a wheel outside the wheel-drive, on the bottom left, and you had to take a hand off to modify it. My point: just having physical controls doesn't mean it is easy or safe to use, if the layout sucks. reply sensanaty 26 minutes agorootparentI haven't driven that many different cars in my life, but isn't it usually the stick on the same side as the driver's seat? Aka if the driver's seat is on the right, then the blinkers are on the right, and the other way around in left-seated cars? I switch between Indonesia and EU fairly often and most cars I've driven followed that pattern as far as I can remember (maybe I just never noticed though) reply darkwater 1 minute agorootparentYeah but there might be 2 stalks on that side. Also in some car you need to pull the stalk down X positions, in others you have to rotate a crown, so it really depends. The safest solution is to have properly working automatic wipers (UNLIKE current Tesla, where wipers are complete dogshit) so you just don't care. The same for lights. tmikaeld 2 hours agoparentprev> …Teslas. Or at least it was, until they removed the physical turn signal controls. The turn signals wasn’t removed, they moved them to the steering wheel as physical buttons. Which ofc isn’t optimal, because you turn it… reply modeless 2 hours agorootparentI would call them virtual buttons rather than true physical buttons, as they are capacitive touch buttons that don't depress and are missing a defined physical boundary. Granted, they are fixed in place and don't move. Except that, as you point out, they do move... reply nehal3m 2 hours agorootparentprevYou’re supposed to signal before you turn the wheel. Way too often I see people signaling as they turn, not because they are trying to communicate with other road users but because it’s the law. Indicate your intent first and then take the indicated action. reply the_gipsy 1 hour agorootparentYou might already be turning, but completely unrelated to an upcoming maneuver that you must signal. reply martin8412 2 hours agorootparentprevRoundabouts.. reply gambiting 2 hours agorootparentCorrect. But then maybe OP lives somewhere where roundabouts aren't very common so they never ran into that usecase. reply ben_w 16 minutes agorootparentIn the UK, which has loads of roundabouts, this issue is still common enough to notice, and derided. reply LeoPanthera 27 minutes agorootparentprevThey're not physical buttons. They're touch sensors. They don't move or click. reply maccard 30 minutes agorootparentprev> Which ofc isn’t optimal, because you turn it… You're supposed to indicate before you turn, to be fair. reply williamcotton 2 hours agorootparentprevSo that’s why everyone who drives a Tesla never uses their turn signal? reply snowfield 2 hours agorootparentThey used to own BMWs reply resolutebat 2 hours agorootparentprevThat's a weird assertion, because if you're driving a Tesla and using autopilot/TACS you have to use turn signals to change lanes. reply concordDance 34 minutes agorootparentprevI've seen hundreds of Teslas driving in the UK and they're just as likely to use a turn signal as anyone else (~99%). I'm not sure I've seen a less substantive comment on hacker news that wasn't flagged. reply snowfield 2 hours agoparentprevTo be fair, the new EVs have pretty good adaptive wipers. I have not found the bed to actually overwrite this yet. reply modeless 2 hours agorootparentNot Teslas, unfortunately. The auto wipers are trash and always will be because Tesla refuses to add a dedicated sensor. It's a double whammy: handicapped auto mode and awful manual controls. reply ahahahahah 2 hours agorootparentprevCars for 15-20 years have had good automatic wipers, they're just becoming more widely available now (and evs are generally in the price range where it's expected). Well... except for Tesla. They decided that a person can tell when wipers are needed just by using their eyes, so certainly they could save a buck or so and do it using just a camera. reply userbinator 8 hours agoprevBut the organization wants to see physical controls for turn signals, hazard lights, windshield wipers, the horn Are there really cars where the horn is not a physical button/ring on the steering wheel? IMHO the driving UX peaked in the 1960s, and was largely unchanged into the 2000s, until touch screens started taking over. Compare: https://i0.wp.com/www.curbsideclassic.com/wp-content/uploads... https://images-stag.jazelc.com/uploads/theautopian-m2en/2010... https://www.motortrend.com/uploads/sites/5/2017/07/Tesla-Mod... At least the steering wheel and pedals still behave the same. reply threeseed 8 hours agoparentI think the issue with the horn is referring to the Tesla Model S. And specifically the horn button being capacitive co-located with the voice assistance and windscreen wiper buttons. Which in a safety situation was difficult to locate and press. reply userbinator 7 hours agorootparentNot the least bit helped by the fact that there seem to be several models all called the \"Model S\", but with very different controls. https://www.tesla.com/ownersmanual/models/en_us/GUID-DEB259C... For vehicles manufactured as of approximately January 2024: To sound the horn, press the middle of the steering yoke (or steering wheel). For vehicles manufactured prior to approximately January 2024: To sound the horn, press and hold the horn button on the right side of the steering yoke (or steering wheel). \"press and hold\" doesn't seem like it'd be easy to do if all you wanted was a quick toot. reply hn_throwaway_99 5 hours agorootparentThanks for that link, which has images. Jesus, the pre-2024 horn is insane. It's one tiny button among many on the right side (of that stupid yoke, but I digress...) The whole point of having the horn in the middle of the steering wheel is you can just mash it with your palm, no need to hunt for anything. Really wonder how that ever got into a production car in the first place. reply kergonath 2 hours agorootparent> Really wonder how that ever got into a production car in the first place. Teslas are the Dunning-Kruger cars. Designed by people who seemingly do not really know how to design cars (a wheel is boring; let’s put a yoke because video games! Look how cool we are! Also, touchscreens!) and built with poor quality control. I was not aware of the horn button, but if I had to guess, Tesla would be my first choice. reply userbinator 4 hours agorootparentprevThe whole point of having the horn in the middle of the steering wheel is you can just mash it with your palm, no need to hunt for anything. One can easily see this \"meme\" if one searches for images of \"driver honking horn\": https://www.google.com/search?q=driver+honking+horn&tbm=isch reply darkwater 2 hours agorootparentprevThey just reverted the horn positioning in the soon-to-be-released refresh of the Model S/X. Sometimes even Elon admits he was wrong. reply rlt 33 minutes agorootparentI mean, one of his mantras is \"If you're not adding things back in at least 10% of the time, you're clearly not deleting enough.\" reply letitbeirie 1 hour agorootparentprevIt's weird that Tesla is being called out over the horn. In driver's ed they tell you to not to use your horn when you're annoyed at other drivers - only use it if there's an imminent danger to avoid a crash. But you better hope it works because if it doesn't, the airbag behind it explodes. ... I'm not sure Tesla improved the situation, but it definitely seems like the situation has room for improvement. reply zarzavat 10 minutes agorootparent> In driver's ed they tell you to not to use your horn when you're annoyed at other drivers This is definitely not a cultural universal. Different countries have different practices. In some places the horn simply means “I’m here, check your mirrors”, in other places it means “You bastard!”, and in some other places it means “We are both stuck in traffic let’s make as much noise as possible to pass the time” > only use it if there's an imminent danger to avoid a crash Therefore the difference between the horn being readily accessible and being hidden away could be the difference between life and death. reply rkagerer 2 hours agoparentprevAt least the steering wheel and pedals still behave the same. Don't give them ideas... reply jojobas 2 hours agorootparentTesla Cybertruck uses steer-by-wire with almost no redundancy. Electric power steering was bad enough to cause crashes, this is just ambulance chaser's dream. reply gsich 1 hour agorootparent>steer-by-wire This is the norm for most cars with servo steering. reply jojobas 1 hour agorootparentNo. Almost all cars except some concepts and prototypes have mechanical link between the steering wheel and the steering rack. Even if the link is augmented by a hydraulic or electronic booster. If a booster fails you have a chance of overpowering it, it's not that strong. A faulty steer-by-wire will swing you into the oncoming lane with no recourse. reply oblio 45 minutes agorootparentI think the lack of redundancy is the issue. Redundant electrical connections can be very robust otherwise they wouldn't put them in F-35s. reply tiagod 18 minutes agorootparentThe F-35 took like 25 years and a staggering amount of billions to develop... reply jojobas 8 minutes agorootparentprevAnywhere in aviation it's redundant transducers (the thingies that transform rotation into some sort of signal), redundant flight law computers implemented by independent teams on dissimilar hardware, a stupidly robust vote/compare module, I think mostly analog, and usually a last resort direct mode. When all of that is implemented in cars and weathers for 10-20 years I might consider it. Otherwise, seeing as Toyota's regen braking code had like 10k global variables, I'm not touching that with at 10 foot pole. Acceleration/braking can be quite bad but there's normally neutral and ignition switches. With steering there's literally nothing between you cruising along a highway and you splattered on some concrete pillar 200ms later. pompino 31 minutes agorootparentprevBut no ejection seats to escape a collision. reply ChrisMarshallNY 7 hours agoparentprev> Are there really cars where the horn is not a physical button/ring on the steering wheel? Yes: https://antiquecarmuseumofiowa.org/wp-content/uploads/1913-F... Well ... \"are\" is kinda stretching it... reply userbinator 5 hours agorootparentTouché! It's interesting how early vehicles were very different in their controls, but seemed to mostly converge and stabilise for a few decades, before again diverging and changing rapidly. The Model T (Ford) had a throttle on the steering wheel and 3 pedals, but not the ones you'd expect today: https://www.fordmodelt.net/images/ford-model-t-controls_smal... reply Animats 3 hours agorootparent> but seemed to mostly converge and stabilise for a few decades Due to regulation. The US Congress mandated the PRNDSL quadrant for automatic transmissions. (GM had PNDSLR, and people kept shifting into reverse by accident.) reply tom_ 7 hours agorootparentprevMy 1995 Renault Clio had it as a button on the indicator stalk. Worked very well, in my view. Much easier than pressing the centre of the steering wheel. There's often no need to even move your hand! reply cccbbbaaa 51 minutes agorootparentI had the same on a 2003 Twingo. Very handy, except for that time when I horned at a poor jogger on the side of the road instead of activating the turn signal :( reply fransje26 24 minutes agorootparentprevOur family Peugeot 305 also had the horn on the indicator stalk. reply sonofhans 6 hours agorootparentprevThat helps explain Paris drivers :D reply puzzlingcaptcha 1 hour agorootparentprevAlthough it seems to have changed, my 1990's Twingo also had the horn on the indicator stick but a 2000's Megane already has it in the steering wheel proper. reply gambiting 2 hours agorootparentprevIt's a French thing - my Peugeot Partner was the same, horn on the indicator stalk. reply fecker 1 hour agorootparentprevMy Dacia Sandero has it on the tip of the left stalk reply noncoml 5 hours agorootparentprevProbably a French thing. My dad’s 70’s Peugeot had the horn where in most cars today is the windscreen washer action, ie pull towards you the right stalk. I would prank my 6-7yo cousin at the time that all you had to do is ask the car to beep and it would do reply nikau 2 hours agorootparent80s Fords in Australia had horn on a stalk too reply onionisafruit 7 hours agorootparentprevMy grandmother used to have a car where the horn was a button at the end of one of the stems. I think it was on the same stem that controlled the wipers. I wish I remembered what kind of car it was. I think it was from the 40s. reply yaky 6 hours agorootparentFirst generation Chevy Volt (2011-2016) has a button at the end of the left stem that sounds an alternative polite (aka \"pedestrian\") horn. It's several very rapid, but somewhat quiet honks, that sound like \"brrrap\". reply Tagbert 5 hours agorootparentI like that. I used to have a car that would give that kind of polite honk with a light press and a louder honk with a harder press. I’ve always wished that were still a thing. reply brabel 3 hours agorootparentprevMy parents had one of these... I believe it was Ford Corcel from 1978 (https://en.wikipedia.org/wiki/Ford_Corcel), a South American model, according to Wikipedia, based on Renault 12. reply sandermvanvliet 2 hours agorootparentprevMy Land Rover Defender has that too and that’s from early 2000, think they kept that for quite a bit longer than that too. reply lokedhs 6 hours agorootparentprevThe old Mini had that. I remember seeing it in the 80's. reply nly 56 minutes agoparentprevI've been driving for 15 years and only ever used my horn once. reply oblio 46 minutes agorootparentAnd in that situation it probably was easy to use because most likely you REALLY needed it, which these laws are trying to make sure happens: it's easily accessible. reply freeAgent 39 minutes agorootparentprevThen it’s likely you don’t have a significant commute to work in a city with heavy traffic. reply richardw 4 hours agoparentprevExcept for radios. It's very annoying that every radio had a different UX, likely in the name of innovation and differentiation. I think that's a safety issue for e.g. rental cars. reply consp 3 hours agorootparentThe radio is a feature in which a move to change station should not take more than a fraction oft a second. If it takes more you should stop. That is valid for any action but for some reason people start adjusting their entire audio setup while driving... reply c22 8 hours agoparentprevThe pedals do not behave the same. reply userbinator 7 hours agorootparentYou mean you don't press the left one to stop, and the right one to go? reply spdustin 7 hours agorootparentYou don't feel the hydraulic feedback in the brake pedal (just the ABS buzzing), and gas pedals in many modern EFI vehicles feel more like foot-operated potentiometers than throttle linkages. reply magicalhippo 7 hours agorootparent> gas pedals in many modern EFI vehicles feel more like foot-operated potentiometers than throttle linkages. In my EV it literally is a potentiometer. I really miss the feel of a physical throttle linkage on the snow and ice, I feel it's much more difficult to judge grip. Also, since it's just a pot, I don't know why there's not a speed mode for it, it to make it operate the cruise control speed set-point rather than torque. reply user_7832 6 hours agorootparent> I really miss the feel of a physical throttle linkage on the snow and ice, I feel it's much more difficult to judge grip. The throttle is separated from the wheels/ground by the transmission and the whole engine, which makes me wonder if the response you felt probably was a combination of eg rpm/physical vibration and a few other factors. It should be possible to recreate these even in an EV, then. reply kedikedi 6 hours agorootparentThe throttle feeling of a cable operated throttle is more about how much air the engine wants to breathe. I’d say it’s kinda still related to rpm, but it might be a completely different feeling on a turbo engine, that I haven’t experienced. reply cuu508 1 hour agorootparentRight, but how does the changes in engine wanting to breathe manifest through the throttle pedal feel? Changes of vibrations? Changes of resistance? Changes in lag from pushing the pedal to the engine responding? Perhaps on the direct throttle car you could simply hear the engine better? reply magicalhippo 1 hour agorootparentIt was probably sound and vibration in combination with less lag. We had an older non-turbo car before the EV. reply consp 3 hours agorootparentprevOn most turbo engines these days they are electronically operated and have no linkage anyway. That's even the case in most modern petrol cars where people think there still is a linkage. And that trend has been going on for at least 20 years. reply macNchz 2 hours agorootparentprevThere is a noticeable difference in feel between ICE cars with physical throttle cables and those with electronic throttles…the drive-by-wire models have a very small amount of lag between pedal input and engine response. If you’re attuned to it, or drive “spiritedly”, the effect is quite noticeable. I think mainstream cars mostly switched over around 20 years ago, I remember learning about it when I googled for why the accelerator pedals in new cars felt so weird to me. I think there are a few reasons for it, one of which is simply increasing efficiency by smoothing the accelerator input a bit. I’ve only driven an electric car a few times, so I’m not sure if they also have this delay, but I would expect they do. reply Vecr 5 hours agorootparentprevMultiple cross-checked hall effect sensors, hopefully. Same concept though. reply magicalhippo 1 hour agorootparentThat's what I had thought, but mechanic that replaced the throttle pedal assembly called it a potentiometer (had a weird pulsing in the force feedback). Of course, could be he was wrong. reply Vecr 48 minutes agorootparentI read the \"unintended acceleration\" report from NASA. The Toyota cars involved had two independent hall effect sensors. Hopefully there's been no backsliding since then. reply fy20 4 hours agorootparentprevEvery vehicle I've ever driven since I learnt to drive 20 years ago has had an electronic throttle body, a digital accelerator and ABS. Welcome to 2024. I did learn to drive manual, but I'd never buy another car like that. Maybe it's fun for racing, but for every day driving, automatic requires less effort and longevity isn't a concern today. reply Beldin 3 hours agorootparentprevAround here the middle one is to stop, left is to switch gears. Except in go-karts, those only have 2 pedals. reply oblio 43 minutes agorootparentI don't know where you live, but if you're feeling smug about being in Europe, Europe is going automatic. 30% and rising if I recall correctly plus EVs don't need manual transmissions anymore. reply arghwhat 2 hours agorootparentprevRemember to press both left and center pedals to stop. Manuals proliferate in EU, but we have also always had automatics on the road - and a significant increase since DSGs hit the market - so no one who has driven a few normal cars would be surprised by two pedals in an electric. reply hnbad 1 minute agorootparentI think \"proliferate\" is the wrong word. Automatics seem to be on the rise - I learned on a manual but have only been driving automatics for a decade. They're also more accessible and I'm not sure there are any EVs that emulate manual transmission as there's literally no point. So if anything, manuals are largely on the way out. aaronmdjones 2 hours agorootparentprevUsing the clutch before you need to when braking is bad form. The engine will assist in braking if you let it, especially in the low gears, and this doesn't use any fuel -- dumping the clutch immediately consumes more fuel to keep the engine running. Put the brake pedal in first, and then the clutch pedal as you approach idle RPMs. reply rad_gruchalski 2 hours agorootparent> Remember to press both left and center pedals to stop To stop, not to slow down. reply theGnuMe 7 hours agorootparentprevNo, it’s the feedback loop. You feel the hydraulic system for the brakes and the steering. If you have an accelerator cable then you feel that tension as well… but the accelerator feel is less important. reply tempestn 8 hours agorootparentprevNor does the steering yoke. reply I_Am_Nous 8 hours agoparentprev>At least the steering wheel and pedals still behave the same. For now, companies are experimenting with drive by wire. Don't think I like that concept. reply static_void_ 7 hours agorootparentElectronic power steering already exists. It's used in a lot of cars, and has been for at least 10 years. Your car may have one of these systems in it. In an electric power steering system there are steering angle and torque sensors that know what direction the wheel is turned an how hard it has turned, and this is connected to an electric motor that powers the gears to move the steering rack. There are still regulations in place that require a mechanical connection to the steering wheel and rack, but try and turn the wheel with the car off and see if your wheels move... But when the car is running when you turn the wheel you're just a voting remember in the system. There are no such regulations for throttles. Pretty much every car since the late 80s has electronic throttle control and there are no mechanical linkages from pedal to throttle body. reply ok_dad 5 hours agorootparentMost of the modern steering systems are just hydraulic or electronic power steering which boosts your input with a hydraulic or electric assist motor. You're still moving the mechanical linkage at any time you're steering, it's just that when the car is off the assist motor is disabled and because modern cars have massive tire contract patches, huge weights, and a lot more caster compared to yesteryear, it's damn near impossible to turn the wheels when the car is off and not moving. If you simply disabled the assist, started the car, and let it roll, you would find that it is only maybe 1.5 times harder to turn the wheel than a classic car with skinny tires and larger steering wheels and such. You're still fully controlling the direction the wheels are turned in said systems, the assist motor simply adds force. reply consp 2 hours agorootparent> it's damn near impossible to turn the wheels when the car is off and not moving. That was also strongly discouraged on older cars and only possible with big steering wheels and good grip. Just move very slowly and the turning becomes really easy even without assistance (and which is what you were thought when this was still a thing in countries with required driving lessons). reply pmontra 3 hours agorootparentprevTurning my car at low speed with the engine off feels much harder than 1.5 times the turning of an old times car of similar weight. I could hardly make it turn at all when it happened. I don't think I'd be able to make it through a real turn on a road. And I have a small car of about 1000 kg. reply consp 2 hours agorootparentYou also have to overcome the little but non zero friction of the power steering and your steering wheel is considerably smaller than they used to be. reply Reason077 6 hours agorootparentprev> “There are still regulations in place that require a mechanical connection to the steering wheel and rack” Are you sure? Fully “steer by wire” vehicles with no mechanical steering link are already in production. The Tesla Cybertruck is the most well-known example. Toyota/Lexus has also been demonstrating steer-by-wire for a while (Lexus RZ), and it will apparently ship in consumer vehicles later this year. reply lokedhs 6 hours agorootparentAre any of these cars available outside the US? From what I've been able to tell, US regulations are much weakest in this regard which is possibly why there are so many words cars there. reply Reason077 6 hours agorootparentThe Lexus steer-by-wire tech is expected to ship in Europe later this year: https://www.drive.com.au/news/toyota-lexus-steer-by-wire-sys... (In some aspects the US automotive regulations are actually stricter than in Europe. For example, in Europe there are some vehicles (eg: Audi) where the wing mirrors are replaced by screens/cameras, but this is not permitted in the US. Adaptive matrix headlights are also not allowed in the US.) reply kevin_thibedeau 4 hours agorootparentAdaptive headlights are coming soon. reply Aurornis 4 hours agorootparentprev> Pretty much every car since the late 80s has electronic throttle control and there are no mechanical linkages from pedal to throttle body Electronic throttles became widespread much later than the 80s. Pop the hood of common 2000s economy cars and you’ll find a mechanical throttle linkage. reply jjav 3 hours agorootparentprev> Pretty much every car since the late 80s has electronic throttle control and there are no mechanical linkages from pedal to throttle body. All 90s cars I've had or worked on had a real cable from gas pedal to throttle body. reply Maxion 3 hours agorootparentEven fuel injected ones? reply DinaCoder99 2 hours agorootparentprevBeing able to feel the resistance of the road is valuable, especially in inclement weather. Much more so than with, say, an airplane. reply culopatin 7 hours agorootparentprevExperimenting? It’s been around for a long time. My 1999 Jetta had it. It’s smoother and you can control the fuel adder on throttle opening from all angles. Sure, it’s not instant, but stabbing the gas in a non dbw is not great either. In electric cars there is no other way than dbw reply AsmaraHolding 6 hours agorootparentA 1999 Jetta doesn't have steer-by-wire. Wikipedia says the first production car to have it was the 2013 Nissan Infiniti Q50. reply culopatin 4 hours agorootparentWell I see what you mean. Steer by wire is part of drive by wire if you want to be specific. But before that was a thing “drive by wire” meant a throttle body controller by the ECU.m for many years. So if you go to a shop and say DBW what the other end hears is “stepper throttle” reply maxdo 6 hours agorootparentprevIt had duplicated mechanics reply gonzo41 6 hours agorootparentprevIt's be really cool if there were some giant high power potentiometer connected directly from the batter to the motors in an EV. Just smash the pedal, see a blast of plasma and you take off. Just like slot cars. I can't see a single downside to this :p reply shepherdjerred 8 hours agorootparentprevWhy? Brake by wire is already a thing. reply I_Am_Nous 8 hours agorootparentIf my steering wheel isn't connected to my front wheels mechanically, that's asking for a lot of trust. Previously a severe mechanical issue would be the only thing to worry about. Maybe on an EV with always connected and charged batteries it would be better, but on ICE I can imagine a number of ways the electrical system suddenly fails and now the wheel does nothing, even if just for a short lag. At least on brake by wire ICE systems I could let engine braking slow me if the brake system fails somehow (even if I have to blow the engine to save my life, worth the trade). reply amluto 7 hours agorootparentHaving previously driven a car with a mildly unreliable power steering system, it’s an extremely good thing that the power steering was backed up with a mechanical linkage (that worked very well — it was surprisingly subtle when the power steering crapped out at 30mph). A drive-by-wire system had better be a lot more reliable and also notice impending failures. reply devmor 7 hours agorootparentI personally cannot stand power steering systems. There’s always a dead zone and the feedback is delayed, in some vehicles more noticeably than in others, but it’s always there. I feel like I have had several close calls that would have been accidents for sure had I been driving with power steering. reply lathiat 7 hours agorootparentWhat car do you have without power steering? Or, are you referring to electric as opposed to hydraulic power steering? I haven't seen a car without power steering since the early 00s (which was a 90s car). The steering deadzone is not purely a function of power steering. Some of it is design or could be attributed to that, but a bunch of it also comes from slop in the physical connections in the steering column, as well as the suspension bushings, your tyre profile, etc. reply amluto 6 hours agorootparentI once drove an early car with an electric steering assist system. (It was either a renta or a test drive, I think it was an American car, and I don’t remember what brand or model.). It was terrible. There was essentially no feedback from the road to the wheel. These systems have improved. reply dkjaudyeqooe 7 hours agorootparentprev> it was surprisingly subtle when the power steering crapped out at 30mph It's not a problem at those speeds, having it crap out when turning slowly and sharply around fast traffic is much more likely to be deadly. reply dotancohen 5 hours agorootparentThe opposite is true. Those old AMCs without power steering were just as easy to steer on the highway as any other car (once you managed to get up to speed) but the parking lot was a chore and parallel parking was neigh impossible. reply bombcar 4 hours agorootparentFancier power steering systems basically fully disengage at speed because you don’t need them; they’re for moving the wheels when you’re stopped or nearly so. reply otherme123 3 hours agorootparentprevMechanic connections can, and do, fail. Ask Ayrton Senna. AFAIK, steer by wire are already doing full redundancy, like it was a plane. There has been a number of plane accidents related to physical connection between the yoke and the actuators (cables loosing tension under high temperatures, or cut/trapped on a deck failure). Electric cables does not suffer that, and they are easily doubled. SbW makers are already making software that does constant surveilance of the system, something that traditional systems has never done. The number of dead people caused by the steering column going straight through their chest in a collision is huge. That shouldn't happen with SbW. I hate the trend towards car controls in a screen, but Drive by Wire sounds like a real advance towards better cars to me. DbW without redundancies and continuous tracking could be a safety issue, but with them I don't think it's an issue. Fly by Wire has a good safety record. reply pompino 28 minutes agorootparent>Fly by Wire has a good safety record. Because there are dozens of people helping keep each plane safe. reply dexwiz 8 hours agorootparentprevPost ALB there isn’t the same feedback thru breaks as there are steering. Steering systems have been highly engineered so you can get feedback from the road while still having power assist. reply alexey-salmin 3 hours agorootparentprevThere must be a physical connection though? How do you stop if you're out of power? If not, I can only think of truck-like system where brakes are \"on by default\" and user input disactivates them. reply doubloon 7 hours agorootparentprevBoeing. reply userbinator 7 hours agorootparentprevDrive-by-wire is at least a decade old if not more. reply inferiorhuman 2 hours agoparentprevPeugeot through the 505 put the horn button on a stalk on the steering column. The 505 ended production for most of the world in the early 90s and I've not seen any of their more modern cars so I've no idea how long that design lasted. The French made some really… interesting choices with their cars. reply aurareturn 8 hours agoprevAround 10 years ago, I started looking into buying a new car. I couldn't believe the number of cars that switched to touch controls even 10 years ago. It boggles my mind just how car makers thought it was safer/easier to have touch in a car while one is driving. I refused to buy any car that replaced physical buttons with touch controls 10 years ago and I still have this rule today. Then again, it also boggles my mind how car makers in the US continue to use flashing red lights as the turn signal instead of yellow lights. You can barely see the red light in sunlight and it's harder to tell the red light from brake lights. Furthermore, the same car will have yellow signal lights in the front and side. So yellow signal lights in front and side, red in the back. Just make it all yellow for turn signal! reply pwg 8 hours agoparent> It boggles my mind just how car makers thought it was safer/easier to have touch in a car while one is driving. It is likely that neither safety nor ease of use were part of the automaker's \"thought process\". It is much more likely that a first misguided \"designer\" created the first touch panel control and somehow sold it to \"management\" as being \"futuristic\" and/or \"ahead of the competition\". And once the first car model arrived with one, the rest, like firefox to chrome, felt the need to play the imitation game for fear of being seen as not as \"trendy\" or \"futuristic\" as that other guy. I.e., purely the \"fashion trend\" aspect. Then, as they proliferated, the reduced BOM costs from removing every other previous mechanical control was reverse justified as the reason for continuing to add them to ever more car models. reply drc500free 3 hours agorootparentMy understanding is that the real driver is in being able to decouple the design of the controls from the rest of the interior design. I read somewhere that being able to design those in parallel with fewer dependencies makes a significant different in getting the car into production on time. reply lelanthran 4 hours agorootparentprevWell, yes, that's the main reason - it's cheaper. There are other reasons of course - planned obsolesence is a big one. Why would they want cars to work after the primary owner is done with it? With software-everything they can lock the car to the first 3 or 4 owners, and then remotely kill it. It doesn't even have to be actively disabled, just stop providing the replacement head unit as a part because \"we don't have that software anymore\". reply userbinator 5 hours agorootparentprevSafety was part of the thought process --- just enough to get by whatever regulations are currently in effect. reply branko_d 5 hours agoparentprev> Then again, it also boggles my mind how car makers in the US continue to use flashing red lights as the turn signal instead of yellow lights. Technology Connections channel had a video about that a while back: https://youtu.be/O1lZ9n2bxWA?si=xKRgMFK1DFBrB3i0 reply userbinator 4 hours agorootparentFlashing red = car is pointed away from you Flashing yellow = car is pointed towards you It's the same reason why headlights are white and taillights are not (unless reversing, in which case the tail becomes the head temporarily, and thus white reversing lights.) reply knorker 1 hour agorootparentThat's what tail lights are for. reply aurareturn 4 hours agorootparentprevIt boggles my mind. It really does. I refuse to buy a car that uses red turn signals. reply tpmoney 8 hours agoparentprev> It boggles my mind just how car makers thought it was safer/easier to have touch in a car while one is driving. Does anyone actually think that though? Or was it considered “good enough” in light of its other benefits like reducing costs, reducing BOM, eliminating part design work, reducing wiring complexity, adding flexibility and customizability, (potentially) increased reliability, making it easier to jam the multitudes of controls and options a modern car has into a more usable and understandable interface, etc. Don’t get me wrong, when I bought a new car, one of the selling points was the manufacturer was one of the few to still offer physical control and navigation of the touch screens (in fact the touch functionality is completely disabled at any speed faster than 5 mph). But I don’t think “safer and easier to use while driving” has ever been the driver for touch interfaces in cars. reply aurareturn 8 hours agorootparentYes yes I get the cheaper/update factor of touch controls. And I don't mind a touch screen for complicated functionality. I mainly gripe about losing physical buttons for main functions such as temperature control, radio/music control, lights, windshield wipers, etc. reply bolasanibk 8 hours agoparentprevI would the main driver was economics. It would be easier and cheaper to manufacture one big screen in the middle compared to a bunch of physical controls with wiring. Also makes it easier to change things later in the design if you do not have a bunch of physical controls to move. reply ahartmetz 8 hours agorootparentDo not forget the massive \"tablets are the future of computing\" hype because Apple released a thing. Touchscreens were super cool by association. It was all pretty stupid. I say that as someone who creates mostly software for touchscreens... using keyboard and mouse because they are much better input devices. You just need the space and the budget for them. reply aurareturn 8 hours agorootparentprevI get the cheaper and the ability to update thing. But if any car makers had done any proper UX testing, they'd quickly find out that physical buttons in a car is a non-negotiable. reply ceejayoz 8 hours agoparentprev> Then again, it also boggles my mind how car makers in the US continue to use flashing red lights as the turn signal instead of yellow lights. You can barely see the red light in sunlight and it's harder to tell the red light from brake lights. I'm also starting to see really thin - single narrow LED strip - turn signals that are barely visible next to the much larger headlight nearby. reply 2143 4 hours agoparentprev> Then again, it also boggles my mind how car makers in the US continue to use flashing red lights as the turn signal instead of yellow lights. This is probably a US government regulation thing. Because those same cars sold elsewhere in the world does have flashing yellow lights as indicators. reply kevin_thibedeau 3 hours agorootparentIt's an allowance not a requirement. Some models that used to have amber indicators have been switched over to red for the cost savings. reply Aloisius 5 hours agoparentprev> You can barely see the red light in sunlight and it's harder to tell the red light from brake lights I can't say that I've ever had trouble seeing the red turn signals in the sun. Being able to see them in the sun from a few hundred feet away is legally required in most, if not all, states. Do you have trouble seeing brake lights too? reply cwillu 8 hours agoprevA start, but not far enough: anything a driver might be reasonably expected to do while driving should have a physical control. Zero-force, zero-feedback, zero-travel controls should be illegal for such functions. reply dzhiurgis 6 hours agoparentObvious limitation is something like Maps and media. My guess ones that use mechanical dials (i.e. Lexus until ~ year ago) cause more distraction than touchscreen by simply being harder to use and taking more time to solve your problem. Given how well chatgpt's voice recognition works - why not just put it on all cars! reply peeters 4 hours agorootparentI've only used a Lexus that had a touchpad. I do however regularly use a Mazda with its commander knob, and it is far safer than a touchscreen in my opinion. You can do most navigation without looking at the screen, with just an occasional glance to confirm you're doing what you think you're doing. Whereas a touchscreen requires constant attention while you're manipulating the screen. The only thing that annoys me about Commander Knob + Android Auto is that AA still forces attention breaks as you scroll through big lists (e.g. Spotify playlists) which is really stupid because you're not usually looking at the screen if you know you need to scroll say 75% down. You're just looking occasionally to see how far you've made it. By making the task take longer, it's reducing safety. The biggest safety issue by far with Mazda+AA is Google's baffling regression in handling voice input for common tasks while driving. reply croo 3 hours agorootparentprev> Given how well chatgpt's voice recognition works - why not just put it on all cars! Because saying \"roll down the left window\" is still a fucking nuisance compared to a click of a button. reply dzhiurgis 2 hours agorootparentOpen/Close all windows comes to mind. Fading audio forward/backward/center is a PITA on a Tesla. There's tons of things one should be able to automate - don't you have any imagination? reply troupo 2 hours agorootparentWe do have imagination. And that's why we're saying that voice control in the car is bullshit. reply gamepsys 5 hours agorootparentprev> Obvious limitation is something like Maps and media I would love to see some data to see how dangerous it is to operate Maps and Media apps on a touch screen while operating a moving vehicle. This is data modern automakers should have access to. I suspect the answer is that it does reduce safety. reply avar 1 hour agorootparentNobody has access to that data, even if you're excluded it to extreme cases like \"an at-fault crash happening as the user was manipulating the map software directly\". We can't bridge the gap between that and accident reports. And that doesn't get into the more subtle cases, e.g. a crash that happens later due to an earlier loss of situational awarenesses. It's also hard to quantify the opposite. E.g. I sometimes have to manipulate the map while driving, and resent the distraction. But afterwards I'm more likely to be in the right lane earlier, not be distracted because I'm trying to read a traffic sign in the 1-2 second window I might have etc. reply sa-code 3 hours agorootparentprevBecause it still makes mistakes, and it's not always clear when a mistake has been made. reply Spivak 4 hours agorootparentprevCheck how Mazda did it with their ring control. I love it in mine. No touch screen needed. It's amazing that accessibility is such an afterthought that having a physical wheel that tabs forward and backward through a UI as the primary means of using it is unfathomable until it's actually implemented. reply jeffchien 3 hours agorootparentI just wish Android Auto had an option to disable activity boundaries for spinning so I can spin from Maps to media, and use the joystick control for directional selection (i.e. it tries to find the next button in that direction). I was excited for Coolwalk but then never got used to switching between Maps and media with the joystick. In the end I just reverted to pressing the Nav and media buttons then spinning. reply speedgoose 4 hours agorootparentprevIf it’s like the old BMW iDrive systems, it’s pretty good but I think it’s a bit like comparing a blackberry and an iPhone. Sure the physical keyboard on the blackberry has advantages. reply Spivak 4 hours agorootparent> Sure the physical keyboard on the blackberry has advantages. Yes, being able to operate it without looking at it and capable of navigating arbitrary 3rd party apps. And because the tab position is stateful you can perform complex actions incrementally. Touch screens win for phones but you would hate one as your laptop keyboard. It's not a better or worse situation as much as a fit-for-purpose situation. reply speedgoose 4 hours agorootparentI think a good mix of physical buttons and touch inputs is best. reply bhpm 8 hours agoparentprevWouldn’t it be safer to require all cars to have voice controls? Then the driver doesn’t have to move their hand off the wheel at all. reply focusgroup0 8 hours agorootparentI took a ride with an owner of a brand new BMW with glass cockpit and minimal buttons. He complained to the dealer about not being able to find any of the actions in the endlessly nested menus. The dealer's response: Use voice controls. He tried it, and it's even worse than Siri in terms of reliability. Absolute unmitigated disaster. reply Reason077 7 hours agorootparentI’ve yet to find any car where the voice control works speedily and reliably, unless using Siri via CarPlay. (Siri is actually pretty great at accurately recognising words, it’s just not always so good at doing something useful with them…) Even Tesla’s is really bad: “Turn off the wipers” frequently gets interpreted as “turn off the wife please” or other similar nonsense. reply resolutebat 3 hours agorootparentTell a Tesla to \"go home\" sometime, and it will respond what's obviously the best interpretation of your wish, namely playing Boney M's minor 1979 hit \"Gotta Go Home\" on Spotify. Fortunately, \"stop wipers\", \"stop navigation\" or in fact any kind of stop at all will instantly pause the disco assault. reply dotancohen 5 hours agorootparentprevTo be fair, many phrases that I utter (in the car or otherwise) are all also interpreted that same way. reply asyx 2 hours agorootparentprevHaven't used it extensively but BMW and Mercedes works reasonably well in German. reply arlort 4 hours agorootparentprevAnd that's presumably in english, which is likely the language which had the most money poured into having good voice to text and vice versa reply AdamJacobMuller 6 hours agorootparentprevI'm not entirely sure you've ever used voice controls. I've never had experience with any car voice controls which didn't make me want to drive my car into a divider just to end the pain. Voice controls are so frustrating to use that I'm sure they are more distracting to use than even a touchscreen. I might be able to keep my eyes on the road easier with voice controls, but, my brain is going to be quickly annoyed and focused on trying to suss out why the voice control system is not understanding me, or am I using the wrong phrase, or do I need to put the windows up (impossible for me 6+ months of the year) so the car can hear me better? It's like trying to pair bluetooth with a non-carplay (or non-android auto -- which I haven't used but heard good things about) with virtually all OEM and many aftermarket receivers. A uniquely frustrating experience which makes me wonder if QA departments at automakers actually exist. reply dexwiz 8 hours agorootparentprevNot all drivers can speak. Not all drivers speak in a way a voice control can reliably understand them. Not all drivers are in environments where voice commands can be easily understood, like loud music. If you are driving a car you likely have the ability to push a button. reply golergka 5 hours agorootparentNot all drivers can use their legs, they have cars specially fitted for them. reply dotancohen 5 hours agorootparentYes, and so far as I know there is no physical control system to fit into these touchscreen cars. Market opportunity? reply amluto 7 hours agorootparentprev“Hey car, honk the horn” “Hey car, signal left” “Hey car, reverse” You’re kidding, right? Even if this worked far better than Siri, it’s too slow. reply ruined 7 hours agorootparentHEYCARSTOP STOP STOPHEY CAR STOP STOP HEYCAR STOP *bong* i don't have that answer reply bhpm 7 hours agorootparentprevNone of those controls are commonly located on a touch screen, which is the topic of the article. reply shagie 7 hours agorootparent> But the organization wants to see physical controls for turn signals, hazard lights, windshield wipers, the horn, and any SOS features like the European Union's eCall feature. Those are exactly what is at issue. This also needs to deal with multiple languages and regional accents for Europe. If I, an American, rent a car in Germany, do I need to speak German in order to engage the windshield wipers? For that matter, navigating the on screen controls may also be problematic. Here's a picture of a German car - https://www.shutterstock.com/image-photo/dashboard-luxury-ge... Do you know which button to push to get the hazard lights on? It's remarkably similar to my Honda. https://www.sheehyhonda.com/honda-dashboard-light-meanings/ and my parents' Chrysler https://cars.usnews.com/cars-trucks/chrysler/300/2019/photos... reply amluto 6 hours agorootparentprevI’m not sure Tesla ever launched this abomination, but: https://www.theverge.com/22348668/tesla-prnd-drive-mode-park... reply JeffL 3 hours agorootparentYes, I have it. It's great. I much prefer a quick swipe on the side of the touchscreen to having a giant physical gear selector wasting a ton of space. I honestly love just about everything about the UI of the refreshed Model S, from the yoke to the turn signal buttons to the on screen gear selector. Only thing I don't like is the horn button for the two times a year that I honk it. reply infotainment 8 hours agorootparentprevIMO, voice controls are good additional control modality, but not a good primary one, since the discoverability is zero. (And also they're usually just...not very good.) reply roywiggins 6 hours agorootparentprevBeing able to honk the horn or turn the windshield wipers on from the back seat would certainly be an interesting feature, especially for people with kids. reply resolutebat 3 hours agorootparentOn all cars I've driven voice control is not always on, you need to trigger it by pressing a button first. reply seattle_spring 5 hours agorootparentprev\"Alexa, honk at this asshole\" \"Ok, calling Hank Armstrong\" reply gnicholas 2 hours agorootparentprevI've heard some of the newer cars have pretty good voice controls, especially on the more expensive models. However, the companies tend to put these behind a subscription wall, which I hate. I don't want my car to be always connected to the cloud. I'll do my navigation via my phone, and nothing else requires connectivity (except perhaps if I had an EV and wanted to schedule charging stops). reply stevage 7 hours agorootparentprevPretty impossible to have a conversation with a passenger then. reply justinclift 7 hours agorootparentprev\"Break\" \"BREAK!\" \"BREAAK!!!!\" \"OH GOD PLEASE BRAKE!\" crunch Problems can occur if it the voice system brakes (pun intended). ;) reply Izkata 6 hours agorootparentInstructions followed, car is now broken. reply timothyduong 5 hours agorootparentprevI hope you don't have any speech impediments, accents, and english is your first language. reply morkalork 6 hours agorootparentprevYour reflexes are faster than you can speak. At least I hope they are. reply delfinom 8 hours agorootparentprevCausing drivers to get road rage because they can't figure out the correct phrasing for voice controls probably also isn't a good idea. Google Assistant still regularly misinterprets what I sayWhat's your list of other cars that have the same charging network access and self-driving capabilities? Well, the list of other cars that will kill you if you take your hands of the wheels is ... just about anything, right? And then you're dead, so have no use for a charging network anyway, too. So, basically, for just that one feature, you can use just about any other car. The list is, essentially, everything else! reply bboygravity 51 minutes agorootparentSomebody read too much CNBC and/or NYT. reply speedgoose 4 hours agorootparentprevWhich other car in the similar price range can fart on demand from the mobile app? reply teruakohatu 5 hours agorootparentprevWhy do you love a car with anti-features that annoy you so much? I am genuinely curious, as I have only driven a Tesla once or twice. reply globular-toast 4 minutes agoparentprevI hate them in all applications that don't benefit from touch and even in many that do. For example, electric cookers. Despite being easier to clean I still find them absolutely infuriating to interact with, plus cats can activate them. Most of the time, though, they are implemented simply because it's cheaper. There's no benefit to speak of. In fact, I think the only device for which a touch interface works is a smartphone. I can't think of any others. reply lizard 5 hours agoparentprevI don't have a problem so much with the touch screen itself. It's a waste for a lot a things and I frequently just turn my screen off, but it is nice to be able to bring up a map with directions and arrival estimates. But I am constantly disappointed by just how awful and useless the software is. Need some directions? Sorry, I can auto-play this music station you haven't used in a week, but if you want those directions you looked up on your way out the best I can do is (maybe) have the address in your recent search history. Want to resume the music you were streaming from your phone through your media center? Yeah, just give me a few minutes to load up this other UI and...Are you sure you have a music app on your phone? Maybe you just need to add it to the car app? Here, let me bring that up on your phone screen. Hold up. There's some audio coming through the bluetooth, I'll just play that. Want to see why the \"Check Engine\" light came on? Oh, well for that you need to buy a $50 dongle with Bluetooth and install an app on your phone. reply squarefoot 36 minutes agoprevI couldn't agree more. Touch screen aren't cool, they're a necessary inconvenience where there's no room for physical controls (ie, phones and tablets) and can have some uses in multimedia systems, but using them for driving related or critical controls in cars is both stupid and dangerous: latency and lack of feedback could distract the driver attention away from the road, then a single hit on the screen and the user loses all controls at the same time. reply Kwpolska 2 hours agoprev> Tesla is probably at greatest risk here, having recently ditched physical stalks that instead move the turn signal functions to haptic buttons on the steering wheel. What the actual fuck? Who thought that replacing such a common function with the least reliable input method possible (capacitive \"buttons\") was a good idea? Also, if I get into a rental car or whatever, I don't need to learn basic controls with sane manufacturers, because they're fairly standard. If I get into a rental Tesla, do I need to read the owner's manual first? reply iforgotpassword 2 hours agoparentI'm confused now. I haven't been in a current Tesla, but what do we refer to when we say haptic button nowadays? To me that's synonymous to tactile button = physical button, but you talk about capacitive buttons. Did Apple's introduction of haptic feedback for touch somehow shift the meaning? reply izacus 1 hour agorootparentAs far as I know the latest refresh of 3 brought back physical buttons. Did you test that one? Capacitive buttons are the ones that don't physically move and give no feedback. reply Zenst 42 minutes agoprevIt is somewhat crazy that the shift to a touch screen, which for some, is hard to use and for all, lacks tactile feedback. You can't beat a button or a dial-in in that respect or even replicate that fully on the limitations of a touchscreen. It almost feels like there is an untapped market for after-market physical controls that can interface with the car that is begging to be tapped. Equally, shudder if somebody patented the ability to add physical controls to augment touch interfaces. reply gnegggh 1 hour agoprevI’ve had the model 3 highland since it came out and still not used to it. Definitely a safety issue. Turn signals and vipers including speed should be physical and not a non tactile button. Love the car otherwise but kinda just want it with a screen that’s 1/3rd the size, CarPlay and speed and such in front of the wheel. Never used Netflix or any of that stuff. Controlling music via your phone app is easier anyway, since you’re used to that interface. reply techdmn 8 hours agoprevTouchscreen controls are the feature on my Tesla I dislike most. Try poking a small icon an arms length away while bouncing over typical Midwest roads and you'll see that it's impossible to do while keeping your eyes on the road. Physical controls would be fantastic, but at the very least they could: * Radically improve the ability to control the car with the scroll wheels on the steering wheel. I don't mean assigning certain functions to the buttons, I mean having a cursor I could glance at, then put my eyes back on the road while I click right twice, up once, click again and then scroll to change whatever. * For the love of all that is holy, ANCHOR controls to the top or bottom of the screen where you can brace your hand on the bezel while trying to poke the buttons. Pretty much the opposite of the fan control that is right in the center of the screen. reply modeless 3 hours agoparent> I mean having a cursor I could glance at, then put my eyes back on the road while I click right twice, up once, click again and then scroll to change whatever. They added a menu kind of like this in a recent update. Long press the left scroll wheel and then you can control a bunch of things using a menu that's controlled entirely by the wheel, no touchscreen. reply adwf 8 hours agoprevGood. My Tesla doesn't have a stalk for wiper control and it's just awful UX. The auto function is erratic (often triggers on a sunny day, doesn't pick the right level in the rain). Might be fine for sunny California infrequent use, but terrible for England. I'd much rather have easy full control at my fingertips than have to faff about with scroll wheels or the touch screen. reply croes 4 hours agoparentHow about buying an other car? reply speedgoose 4 hours agorootparentThe challenge is that other cars are either worse or more expensive. A Tesla with some aftermarket modes (the sexy buttons for example) is still interesting despite its shortcomings. The perfect product doesn’t exists. reply breadwinner 8 hours agoprevCouldn't agree more. Tesla is the biggest violator. In new Teslas, they are removing physical stalks, so if you want to reverse the car, you have to use on-screen controls! reply izzydata 8 hours agoparentEverything I ever hear about Tesla's keeps getting worse and worse. How are they still in business and why does anyone buy one? reply breadwinner 7 hours agorootparent> Tesla's keeps getting worse and worse. Oh, and they replaced ultrasonic sensors with cameras in newer cars. reply infotainment 7 hours agorootparentprevHave you seen the other BEVs available in America? There really isn't much choice if you don't want an SUV -- your options are basically a Tesla or a Chevy Bolt. reply mhandley 6 hours agorootparentPerhaps the BMW i4? https://www.bmwusa.com/vehicles/all-electric/i4/gran-coupe/o... reply jbm 7 hours agorootparentprevI bought a used Model 3 that has the stalk and works fine. (The lack of wiper controls is absolutely ridiculous and is part of the Jonny Ive-ification of cars) As for why they are in business, it is because they were first to market with Electric cars, and as a consumer I trust they are better at battery management than other companies that are not all-in on electric. Furthermore, I'm pretty sure that the degenerate billionaires that run other car companies are not better than the degenerate billionaire who runs Tesla. (I would not buy the new models wo/ physical controls, my next electric car will probably be something Chinese in 10 years) reply nunez 6 hours agorootparentprevbest electric cars in the business, though that gap is shrinking. their automotive infotainment ux is second to none. it's miles better than carplay. reply Vespasian 4 hours agorootparentNever having driven a Tesla, I'm legitimately curious how much of a difference the infotainment makes. The onboard system of my Chinese BEV is not great to be honest (rest of the car works well) but it has Android Auto so it navigates and plays my media. Since as a sole driver I have to keep my eyes on the road at all times, it seems possible that passengers could benefit but I would like to have some first hand reports. Or is it common for some people to sit in the car and use the infotainment just for fun? (I swear that I'm asking in good faith) reply ggreer 4 hours agoparentprevIs someone forcing you to drive a Tesla? Yes, many people find these touch screens annoying, and they’ll tend to buy cars with physical controls. But just like people who prefer physical keyboards on their phones, these consumers are a vocal minority who aren’t big enough to cater to. If touch screens make cars less safe, then we should see higher liability insurance rates for such cars. So far that doesn’t seem to be the case. reply devsda 3 hours agorootparent> Is someone forcing you to drive a Tesla? The same argument can be made about other products too but it only distracts from the real problem. Bad practises and products do stick around regardless of their actual usefulness and benefits. For example take headphone jack. Nobody forced anyone to buy an Apple phone without a headphone jack, yet it is a challenge now to find a good premium/mid-range phone with a headphone jack. Other OEMs are simply copying Apple and people too get along with the new trend. The same is happening with touch controls too. Once a popular desirable brand introduces an (anti)feature, its competitors misunderstand the feature as a contributing factor for its desirability and blindly copy it without getting into actual merits. reply ggreer 3 hours agorootparentYou overestimate how much people want headphone jacks. Most people today use Bluetooth headphones and don’t miss their headphone jacks at all. I honestly forgot that my phone lacked a headphone jack until you pointed it out. That is how little I use wired headphones. Moreover, why would every manufacturer copy a design that consumers don’t want? If the feature is so widely desired, it would only take one manufacturer not removing the headphone jack to win market share. Your model of the world requires every manufacturer to be incompetent in the same way. A much simpler explanation is that phones are a very competitive market and the manufacturers have calculated that, like physical keyboards, headphone jacks are not a feature most people care about. They prefer the headphone jack be sacrificed for better water resistance, better battery life, and lower cost. reply kabes 3 hours agorootparentprevNo but because so many people still buy tesla because they're cheap and despite the lack of controls, suits at other manufacturers think they now have to get rid of physical controls as well. reply illumin8 8 hours agoparentprevThere are still physical P, R, N, D controls right below the phone chargers. They aren't exactly easy to use, though. reply dkjaudyeqooe 7 hours agorootparentOn some models they are above the windscreen. Ergonomic design at its finest. reply piotrkaminski 7 hours agoparentprev> if you want to reverse the car, you have to use on-screen controls! So? If you're switching to reverse then by definition you're stopped (or nearly so) and can afford to take your eyes off the road. The control is no further than most cars' center tunnel-mounted gear levers. Plus it only has the two common drive/reverse settings accessible via the swiping action, rather than all the rarely-used options. (Do you know how many times I've shifted into Neutral or Low by mistake in an unfamiliar car...?) What exactly is the risk? reply culopatin 7 hours agorootparentThat you can still look at your surroundings with the stalk and keep a tab on what people around you are doing. Without the stalk someone can walk into a blind spot and you didn’t catch them because you were finding reverse on the screen. The person next to you is getting ready to open their door. Or simply you want to make a 3 point turn quickly without being stopped in the middle of the street fiddling with a screen reply 59 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Euro NCAP is advocating for car manufacturers to reintroduce physical controls for fundamental functions in vehicles by 2026 to combat distracted driving caused by touchscreen overuse.",
      "The organization suggests separate physical controls for turn signals, hazard lights, windshield wipers, horn, and emergency features to enhance safety.",
      "While Euro NCAP lacks the authority to enforce the use of physical controls, a five-star safety rating from them carries significant weight and could incentivize manufacturers to implement the suggested changes."
    ],
    "commentSummary": [
      "Euro NCAP is urging car manufacturers to bring back physical controls for crucial functions such as turn signals and wipers to enhance safety.",
      "The discussion emphasizes the risks associated with touchscreens, especially for vital controls, and underscores the significance of user-friendly and easily accessible controls in vehicles.",
      "Concerns raised by users include distractions and accidents caused by touchscreens, emphasizing the necessity of striking a balance between digital and physical controls during driving."
    ],
    "points": 519,
    "commentCount": 322,
    "retryCount": 0,
    "time": 1709600577
  },
  {
    "id": 39593647,
    "title": "Nintendo Lawsuit Settled: Yuzu Emulator Developers Pay $2.4M",
    "originLink": "https://twitter.com/oatmealdome/status/1764704580724576465",
    "originBody": "[yuzu]yuzu will pay $2.4 million in damages to Nintendo to settle their lawsuit.This was mutually agreed upon by both parties.https://t.co/XeOaaO03Z7 pic.twitter.com/YmpJY7EZRz— OatmealDome (@OatmealDome) March 4, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39593647",
    "commentBody": "Yuzu emulator developers settle Nintendo lawsuit, pay $2.4M in damages (twitter.com/oatmealdome)486 points by ndiddy 16 hours agohidepastfavorite358 comments fxtentacle 13 hours agoI'm pretty sure the true reason behind this lawsuit was that the Yuzu people were running a company selling (on Patreon) \"Early Access\" to emulator features and access to a private Discord where there was A LOT of user-uploaded Switch ROM dumps floating around and the moderators (employed by the company charging for access) were extremely happy to look the other way. Also, doesn't Yuzu itself include pre-decrypted ROM dumps from the actual Switch hardware, like for example the OS bootloader ? It's one thing to build an emulator. It's a very different thing to run a company by facilitating piracy. Also, this judgement probably won't financially ruin the Yuzu devs, because they (as persons) were not sued. But it will probably tank their liability-limited company named Tropic Haze LLC. I have a strong feeling that if you'd post lossless rips of pre-release cinema movies to a private (but paid) discord, you'd get a rather similar treatment ;) EDIT: I mean even the name choice \"Tropic Haze\" kind of hints at sailing the high seas ;) EDIT2: Straight from the filing: \"The lead developer of Yuzu [..] has publicly acknowledged most users pirate prod.keys and games online\" EDIT3: \"The Legend of Zelda: Tears of the Kingdom, was unlawfully distributed a week and a half before its release\" reply Osmose 12 hours agoparentI believe that Yuzu includes a standalone implementation of the Switch firmware but can run user-provided firmware because a few games have compatibility issues, and it doesn't run Nintendo's own OS software (you can't run the Switch system menu on it, for example). But to your larger point: Nintendo being mad about people sharing Switch ROMs or Yuzu funding their work shouldn't have any bearing on the actual legal question of whether Yuzu violates the DMCA anti-circumvention clause. Dolphin argued after legal consultation that inclusion of these keys qualifies under exceptions for interoperability; Yuzu doesn't include the keys at all. It doesn't appear to have been a question tested in the courts yet. That point _does_ matter if you're making a moral argument about whether Yuzu crossed a line, but given that emulation has been commonplace for almost the entirety of Nintendo's video game business and it has done very little to stop them from staying on top of the game industry, but has enabled millions to experience and be inspired by games they would've otherwise never have been able to play, I'm not terribly convinced that $23k a month in donations is wrong for people putting in serious engineering work into a project that enables that. reply echelon 12 hours agorootparent> I'm not terribly convinced that $23k a month in donations is wrong for people putting in serious engineering work into a project that enables that. That's not it. Say what you will about \"sales lost to piracy are not sales\", but Netflix and Steam suggest otherwise. Kids playing Zelda for free might be spending their opportunity cost money on Xbox instead because of what Yuzu enabled. I support hardware and software emulation. The stuff Kaze [1] and others do is both amazing and inspiring. It's the correct kind of emulation. Yuzu wasn't acting in good faith. The team saw abuse firsthand and embraced it. [1] https://m.youtube.com/channel/UCuvSqzfO_LV_QzHdmEj84SQ reply rahkiin 12 hours agorootparentNetflix, Steam, and Spotify show that the best deterrent from piracy is easy access. Eg I would love to play Zelda or Mario but I do not feel like getting a whole switch for it that is just another console in the closet used a couple of times per month at most. reply gpspake 9 hours agorootparentFor what it's worth, I did this with the PS4 to play like two or three games. On the other hand, I've played 100%ed more games on the switch than any console since probably super Nintendo or N64. It's had such an amazing run. I do wish they'd release more games though. I played all the donkey Kong country games, super Metroid, and super Mario on the emulator and it was great. I've been thinking about busting the Wii u out just to play wind waker. reply phatfish 9 hours agorootparentprevYou can get new Nintendo games as fast as you can download them from the E-Shop, which is as easy access as anywhere else. Obviously you need a Switch to play them legally (it's that whole console gaming thing). reply katbyte 2 hours agorootparentI refuse to get any more switch games because you cannot do offline backups of your saves like PlayStation (and I presume Xbox) reply joshfee 9 hours agorootparentprevAs fast as the terrible wifi on the switch will let you reply bzzzt 3 hours agorootparentYou can plug an USB to ethernet adapter in the dock. reply Wowfunhappy 11 hours agorootparentprev> Netflix, Steam, and Spotify show that the best deterrent from piracy is easy access. Right, and the problem is that piracy in Yuzu is easy. Piracy on a real Switch is much harder (you have to track down an early model Switch if nothing else). reply skeaker 10 hours agorootparentPiracy will almost always be the more convenient option. It is up to companies to provide a better service, as is the case with Steam and (until recently) Netflix. Attacking and shutting down avenues for piracy without doing anything to make your own service more convenient is a losing strategy. Nintendo got 2 million here, but has done virtually nothing to stop the average person from pirating a Switch game. It's for that reason that I honestly don't believe their goal was to stop piracy or emulation with this case, they just saw an easy buck and took it. reply Dban1 40 minutes agorootparentI think they crippled pirating new Switch games because Yuzu won't be able to update to support new titles anymore. reply Osmose 12 hours agorootparentprev> Kids playing Zelda for free might be spending their opportunity cost Money on Xbox instead. ...I don't follow? You're suggesting businesses have a right to attention? > But Yuzu wasn't acting in good faith. The team saw abuse firsthand and embraced it. As mentioned elsewhere in the thread, they had rules against ROM distribution and some of the links shared as evidence that they didn't have been by unrelated people. reply echelon 11 hours agorootparent> ...I don't follow? You're suggesting businesses have a right to attention? Companies should be able to be paid for their products. You have the freedom of taking your money and attention elsewhere, but the illicit piracy of these products is not good for the labor and capital that went into making the product. In a market of entertainment choices, there are a limited number of dollars that can and will be spent. Certain people are cheating the system to get free entertainment and to double dip. A gamer that enjoys both Xbox and Nintendo games can get two for the price of one by pirating the latter. Even if there is equal demand for both products, the supply side has been illegally distorted. This doubly lowers the competitive fitness of the latter company. If I bought and paid for the game, I should be free to emulate. But that's not what's happening here. > As mentioned elsewhere in the thread, they had rules against ROM distribution and some of the links shared as evidence that they didn't have been by unrelated people. As mentioned elsewhere in the thread, Yuzu embraced piracy. They knew it was happening and focused their energies on enabling new releases and getting users to pay for early access builds. reply maxrecursion 10 hours agorootparentEmulation is great because it preserves games that would be lost to time after the physical media dies off. Emulation that allows piracy of brand new games is straight up stealing. While I don't feel sorry for large corporations because they screw everyone over as well, there are plenty of cheap older games people can play, or get game pass. People like to act like they are entitled to these new releases for free. They aren't. Play another game and get it for cheaper later, or emulate it long after its release. reply autoexec 7 hours agorootparentCopying isn't theft, and while you might personally feel that there are zero valid reasons to emulate a game other than for the preservation of old titles I'd expect plenty of others would disagree. reply echelon 7 hours agorootparent> Copying isn't theft What about your private key? What about your bitcoin? What about your nude photos, sex videos, text messages, emails, and personal health information? What about your brain and its memories? What about your intellectual outputs for training AI and selling your skills below your wage? What if you worked for a game company and they let you go because they didn't hit sales targets? ... > I'd expect plenty of others would disagree. What's your use case? That you want better wifi or faster FPS on the Switch? Because a lot of people on this same forum argue that we need complete control over our iPhone/Android devices. No App Store, no Apple fees, no Apple control. Yet these same people get argued down by much of the audience here. I'd imagine that many of those arguing in favor of Apple's racket are the same ones arguing it's okay to pirate Nintendo games. Nintendo has one device that is specialized for a single purpose, and it's positioned in a marketplace full of alternatives. People have broken it and are circumventing its only revenue lever. This isn't as inconvenient for you as it is for the company scrambling to maintain its most important revenue stream. reply autoexec 6 hours agorootparent> What about your private key? What about your bitcoin? What about your nude photos, sex videos, text messages, emails, and personal health information? What about your brain and its memories? Those are private. Things openly for sale to the public are not. Someone copying my private data still isn't theft though, since I still possess what was \"taken\". Much of what you listed (text messages, emails, nudes, health info) have already been copied many times by third parties and on hardware I have no access to or control over and some will likely continue to be copied. As long as my privacy is preserved it really isn't a problem because regardless of those copies I haven't lost anything. > What about your intellectual outputs for training AI That may or may not be copyright infringement (we'll see), but it isn't theft. > What if you worked for a game company and they let you go because they didn't hit sales targets? That's just life. > What's your use case? That you want better wifi or faster FPS on the Switch? There are endless reasons why people might want to emulate a game. Better portability, better performance, personal backups, correcting bugs, accessibility, fan/hobbyist projects, tool assisted speed runs, etc. > a lot of people on this same forum argue that we need complete control over our iPhone/Android devices. I'd agree with those people > I'd imagine that many of those arguing in favor of Apple's racket are the same ones arguing it's okay to pirate Nintendo games. I'd imagine that a lot of people who feel that we should have control over the software we use and the environment we use it in would support emulation since it too empowers the user. > Nintendo has one device that is specialized for a single purpose, and it's positioned in a marketplace full of alternatives. People have broken it and are circumventing its only revenue lever. No one owes Nintendo or their bad business model anything. If I buy a game, I should have the right to do what I want with it. If I come up with a way to play that game on other hardware, or to edit the code in memory to give me extra lives, or to enable the use of a new interface/controller, I should be able to. If having the ability to do those things allows pirates to play a game without paying Nintendo for it that's not my problem. There are perfectly valid reasons beyond piracy for emulation, and that's enough to justify its existence. It's on Nintendo to change their business model to make their products more appealing to people who currently choose not to give them money. The rest of us shouldn't be forced to have our hands tied in order to preserve Nintendo's desired profits. reply sifar 2 hours agorootparentprevWhat about people borrowing video games from the library ? Should we ban that too since the companies are missing on those apparent sales. reply 0xcde4c3db 12 hours agoparentprevAlso worth noting is that while the \"Early Access\" changes were not kept secret, the Yuzu team went out of its way to make it harder than necessary to replicate an equivalent build from the public Git repo (there was no public EA branch, just a bunch of PRs with a specific tag) and aggressively disapproved of anyone actually forking the project or providing unofficial builds. They couldn't actually stop anyone under the license, but anyone doing so risked being blackballed/banned from official spaces, and in one case a Yuzu developer allegedly even altered the copyright notice on a contribution because it referenced a provider of unofficial EA-equivalent builds [1]. [1] https://old.reddit.com/r/emulation/comments/ljxnvi/yuzu_stol... reply 0x_rs 12 hours agoparentprevYuzu developers didn't put themselves in a favorable position from the community perspective by doing DMCAs and cease-and-desists on people redistributing \"early access\" builds.. of GPL licensed source code. And the CLA drama, and so on. Ryujinx isn't affected by the self-proclaimed \"necessity of productization\" of emulation software, and the recent events are not surprising. Some might even say they painted a target on their back. reply jerrysievert 9 hours agoparentprev> I mean even the name choice \"Tropic Haze\" kind of hints at sailing the high seas 1. We know this woman is a witch because she looks like one. 2. We know this woman is a witch because she dresses like one. 3. We know this woman is a witch because she has a wart. 4. We know this woman is a witch because she turned someone into a newt. 5. One burns witches. 6. One burns wood. 7. Witches burn because they are made out of wood. 8. Bridges are made of wood. 9. However, bridges are multiply realizable. They can be built from stone. [Implied] Building a bridge out of the woman will not determine that she is made of wood. 10. Wood floats in water. 11. A duck floats in water [bread, apples, very small rocks, cider, gravy, cherries, mud, churches, lead]. 12. If the woman weighs the same as a duck, then she is made of wood. 13. The woman weighs the same as a duck. 14. Therefore, the woman is a witch. more leaps of logic. [editing to add: with apologies to https://blog.apaonline.org/2019/06/27/monty-python-witch-tri...] reply NotPractical 6 hours agoparentprevI doubt that Nintendo was aware of any of that, aside from maybe the Patreon, when they decided to initiate the litigation process. The Patreon was admittedly risky, but as far as I can tell the settlement states that the emulator itself is illegal as per the DMCA, regardless of whether or not they were making money off of it. If a precedent is set here, it is quite dangerous. > Also, doesn't Yuzu itself include pre-decrypted ROM dumps from the actual Switch hardware, like for example the OS bootloader ? As other commenters have pointed out, Yuzu did not include any Nintendo copyrighted assets, so this is wrong. reply corytheboyd 13 hours agoparentprevThank you for providing the sober explanation of what really happened here, I knew there had to be more to it, but didn’t care enough to dig in. reply bakugo 12 hours agorootparentThere isn't more to it, that explanation is wrong. See my other comments. reply corytheboyd 12 hours agorootparentSo did they run a discord server where people shared ROMs or not? That alone is enough of a woopsie. reply fxtentacle 12 hours agorootparentThey did, but indirectly by tolerating it, and some files are even still online: https://news.ycombinator.com/item?id=39596261 reply brookst 12 hours agorootparentI mean if you’re in the emulator grey area, you can position yourself as righteously enabling people who have purchased the games, or you can position yourself as a way to play the games without paying. Anything less than a zero tolerance policy for ROM trading on the corporate discord server is pretty much a declaration of the latter. reply andix 13 hours agoparentprevIt’s surprising that someone runs a business like you describe in your post from a jurisdiction where Nintendo can actually get to them. I really don’t want to support piracy, but there are many countries where Nintendo could never get any access to the company. And all those countries have the internet too. reply fxtentacle 13 hours agorootparentI don't think they started with that business model. It's more like their users pushed it onto them. And it's very difficult to say no once you've started getting used to the money flowing in. But they did say in their official statement that they \"have come to the decision [..] piracy of video games [..] should end\". https://www.reddit.com/r/yuzu/comments/1b6jvar/end_of_yuzuci... But then again, posts like this certainly didn't help their case: https://www.reddit.com/r/yuzuemulador/comments/17fk9uf/prodk... Note that the now-defunct \"prod.keys 17.0.0\" pointed to a file posted in their discord (but not by them). reply bakugo 12 hours agorootparentThey didn't start with that business model and they didn't end with that business model, either. Please stop spreading misinformation. The second link in your post isn't even on the main yuzu subreddit, it's just a random person on a random subreddit posting a link, and it's not from their discord. reply fxtentacle 3 hours agorootparentIf you're so eager to argue, then in your opinion, what was their business model? They were running a company and that company has been sued (and not the people behind it or the people working on the emulator, but only the company). How did that company make money? reply metalcrow 12 hours agoparentprev> include pre-decrypted ROM dumps from the actual Switch hardware, like for example the OS bootloader Is that known to be illegal currently? Or is that currently still a grey area in the law? reply ronsor 12 hours agorootparentDistributing copies of the system bootloader is definitively copyright infringement, even without the awful DMCA section 1201. That aside, reimplementing the bootloader is legal. I haven't dug into the code to see if Yuzu actually is redistributing anything. reply KomoD 12 hours agoparentprev> I mean even the name choice \"Tropic Haze\" kind of hints at sailing the high seas ;) Can you explain how? reply bena 12 hours agorootparentA lot of what we think of when we think of pirates is heavily influenced by the \"golden age of piracy\" which was mainly in the Caribbean region. So it's like a second or third order reference, but it's there-ish. Pirate -> Caribbean -> Tropical But there's also likely another explanation that would fit just as well, if not better. reply jondwillis 11 hours agorootparentSounds more like a strain of cannabis to me. reply willcipriano 5 hours agorootparentOr a retrowave album. reply dancemethis 11 hours agoparentprevIt's double as funny because Discord knows of the piracy going on (since they have direct access to all user data and metadata), but the \"rights owners\" don't pay them enough in the back door to catch them. reply ShamelessC 12 hours agoparentprev> EDIT3: \"The Legend of Zelda: Tears of the Kingdom, was unlawfully distributed a week and a half before its release\" The game was leaked. How is that the fault of the yuzu devs? reply fxtentacle 12 hours agorootparenthttps://cdn.discordapp.com/attachments/402241866935828490/47... That's a list of dumped ROMs. (Posted in 2018, BTW) Notice the \"402241866935828490\" in the URL? That's the ID of the #support-dumping channel on the Yuzu Discord. Like I said, the Yuzu moderators were very happy to look away when people discussed ROM piracy. reply nfriedly 11 hours agorootparentWait, \"#support-dumping\" sounds like it's for helping people dump the games they have physical copies of. Basically the opposite of piracy. The screenshot looks like it's of the Internet Archive, but all the files are marked as not available for download. reply fxtentacle 1 hour agorootparentYou're correct. It looks to me like the Yuzu team set everything up with the best of intentions. And they were, indeed, helping people dump their own games and they had channel rules that said piracy was not allowed. They started out like an ethical open source project ... ... but around April/May 2023 the moderators made it very clear that it's good enough to PRETEND not pirating the roms. (Can't find the quote atm.) And they were discussing among the mods about people joining due to the TotK leak. And there's hundreds of people who ask \"Where can I download game XY?\" followed by \"Thank you, kind stranger, for the link.\" and then maybe the rules bot saying \"please don't discuss piracy in here\". That all combined makes it easy to argue that the moderators in the Yuzu discord were in April 2023 fully aware that they were actively helping people play the TotK leak. And through Discord and their company, they made money off this activity. (Because you needed the early-access build only available to paid subscribers. They went out of their way to make it close to impossible to compile the supposedly \"open\" source code yourself.) reply Osmose 12 hours agorootparentprev\"Someone posted a screenshot of a separate torrent's contents in their server 5+ years ago\", with no context into why someone posted it or what moderation actions were taken in response, is poor evidence to your accusation. reply fxtentacle 12 hours agorootparentThere's a lot of stuff in that discord that can cause trouble for the moderators. Also, how do you think a lawyer would interpret this: moderator: \"Since April 29th, we've had about 50,000 members join\" random person: \"Let's be honest, 40k of that was the zombie horde who rushed the gates when TotK leaked\" moderator: \"The support we've been receiving has been incredible Very happy with our community, and we're all honored to be able to provide great software to all our users.\" reply ShamelessC 9 hours agorootparentprevI mean you make a few good points but I specifically called out your TotK edit because it doesn’t seem directly attributable to their team. It’s not like the yuzu people hacked Nintendo. That they may have encouraged piracy on their discord is not really relevant considering what your quote implies (that they somehow helped TotK leak). In any case, I played that leak on my actual switch and subsequently purchased the game as well because it’s amazing. You’re welcome to call me a bad person for doing that, but I would just say that it’s a nuanced issue and people probably don’t deserve to have their lives straight up ruined for it. reply fxtentacle 3 hours agorootparentThis settlement will probably not ruin people's lives, because no person has been sued. It'll ruin their liability-limited company, who had been profiting financially off all of this, but if the liability limit works as intended, then the actual Yuzu developers are shielded from the effects of this lawsuit. And the reason why I consider the TotK leak problematic is because the moderators were joking about all the new patreon subscribers caused by it. And that means they were, at that time, fully aware that they are enabling large-scale piracy. And their company was making money off it. The estimate discussed in the discord was 40k leak players, which at $60 each would be 2.4 mio in lost sales. By coincidence, that also matches the settlement amount that Nintendo asked for ;) reply bakugo 13 hours agoparentprev> Yuzu people were running a company selling (on Patreon) \"Early Access\" to emulator features and access to a private Discord where there was A LOT of user-uploaded Switch ROM dumps floating around and the moderators (employed by the company charging for access) were extremely happy to look the other way. This is completely false. The yuzu developers were in no way distributing games or keys to the public, and this was not stated in the lawsuit. They were, however, known to distribute games and keys amongst themselves in private discord chats for development purposes, and this could've been used as evidence against them if they tried to fight the case. reply fxtentacle 12 hours agorootparent\"The yuzu developers were in no way distributing games or keys to the public\" I agree. They did not distribute games themselves. But they provided a tutorial for ripping games and they were managing the discord where the ripped files were uploaded. And they were very unsuccessful at removing obviously unauthorized game ROM dumps from their discord. reply bakugo 12 hours agorootparent> they provided a tutorial for ripping games Yes, because that's how you legally play your own purchased games. > they were managing the discord where the ripped files were uploaded They were not. You weren't even allowed to mention that you pirated a game on their discord, much less post links to pirated games. Hell, even posting an emulator log file containing a game path pointing to your downloads folder was enough to get you warned. reply pushedx 2 hours agorootparentYes. The amount of blatant slander and misinformation in these comments is really sad. The Yuzu team never supported piracy, and any discussion of it was disallowed in the Discord. reply fxtentacle 47 minutes agorootparentSearch the Discord for: \"we're aware at least some folks aren't legit\" \"convincingly PRETEND to follow the rules\" (emphasis by them) \"pirating the game\" \"don't have to witch hunt\" reply schoen 14 hours agoprevI just want to echo what a few people have mentioned here: The legality of emulation was strongly upheld under pre-DMCA copyright law in U.S. court decisions where console makers lost against emulator developers. However, the DMCA gave plaintiffs a whole new set of tools to prevent interoperability. Lots of interoperability and emulation cases have been lost under the DMCA. The DMCA radically limited the prior legal norm that you could create a compatible implementation of a proprietary technology from scratch. If you agree that there should be a right to reimplement a third-party version of a proprietary system/technology/format/protocol (including one that uses some kind of secrecy to attempt to enforce DRM), you should oppose DMCA §1201. reply favorited 12 hours agoparent> The legality of emulation was strongly upheld under pre-DMCA copyright law in U.S. court decisions where console makers lost against emulator developers. SCEA v. Bleem was filed in 1999, and the DMCA came into effect in 1998. reply monocasa 12 hours agorootparentBut the DMCA's 1201 provisions didn't apply to Sony v. Bleem since the DRM scheme of the PlayStation didn't attempt to stop you from copying, but instead focused on stopping a legitimate PS1 from playing copies. reply omoikane 12 hours agorootparentprevI don't think SCEA started the lawsuit because they were sure they would win, I think the intent was to make Bleem run out of money. \"Bleem!, financially unable to defend itself, was forced to go out of business.\" From https://en.wikipedia.org/wiki/Bleem%21 reply schoen 6 hours agorootparentprevThanks for the correction. See https://news.ycombinator.com/item?id=39599019 for a more detailed reply. reply gjsman-1000 12 hours agorootparentprevAh, but the DMCA Section 1201 took effect in 2000-2001. Many other DMCA sections were delayed to take effect in the years following the law's passing. reply schoen 6 hours agorootparentCareful, §1201(a)(1) (to which the exemption rulemaking process applies) was delayed, but §1201(a)(2) and §1201(b) (which don't have the exemption rulemaking attached) were not delayed this way. (a)(1) is an \"acts\" provision while (a)(2) and (b) are \"tools\" provisions. reply p0w3n3d 13 hours agoparentprevThe same as in John Deere's case. You can fix your tractor but you will break the software patents reply ronsor 13 hours agoparentprevReminder that the EFF is challenging section 1201 as unconstitutional: https://www.eff.org/cases/green-v-us-department-justice reply treyd 12 hours agorootparentIs there any updates on this case? reply ronsor 12 hours agorootparentA couple of weeks ago the EFF filed a reply brief: https://www.eff.org/document/appellants-reply-brief-3 reply metalcrow 13 hours agoparentprevCan you give examples of some of those lost cases? Specifically emulation related ones? reply schoen 6 hours agorootparentI mistakenly thought that one of https://en.wikipedia.org/wiki/Atari_Games_Corp._v._Nintendo_.... https://en.wikipedia.org/wiki/Sega_v._Accolade https://en.wikipedia.org/wiki/Lewis_Galoob_Toys,_Inc._v._Nin.... involved a literal emulator. However, none involved an emulator as we would understand the concept (as opposed to \"interoperability\" more generally). There was also the matter of the PC clones, where IBM's only litigation was against those who literally copied the BIOS, as opposed to those who made compatible hardware without IBM's permission https://en.wikipedia.org/wiki/IBM_PC_compatible although conceivably IBM was concerned about antitrust issues there in choosing not to litigate over the non-BIOS-copying clones. As other commenters pointed out, there are court decisions in favor of emulators in the post-DMCA environment (Connectix and Bleem), where DMCA claims were not raised at all in those specific cases. https://en.wikipedia.org/wiki/Emulator#Legal_issues These two cases applied pre-DMCA copyright law to the question of the legality of the emulation, but weren't actually pre-DMCA chronologically. I'll take that as a very helpful correction to the way I phrased the point. reply sotix 14 hours agoprevYuzu got me to start buying Nintendo games again. Two weeks ago I coincidentally decided to mod my Switch and extract all of my games to play on my steam deck because I hadn't touched my switch since getting a steam deck. Everything I am now running on my steam deck comes from my personal switch. I have been having so much fun and loving how my games look on an OLED screen. I also appreciate that I didn't have to generate any waste since I continue to only have my two devices rather than buying a new OLED switch. I was about to go on a shopping spree on the Nintendo store to buy more games even though I haven't bought a switch game in 2 years. This is disappointing to hear. The interesting thing to me is the Wii U was sort of like youtube premium. Everyone complains about youtube ads yey refuses to pay for the ad-free experience. Similarly, I see people complain about not being able to buy old Nintendo games, but the Wii U eShop provided an abundance of retro games for purchase. Most of the retro games I play come from my Wii U. Nintendo has disappointingly made it more and more difficult over the years to buy their older games, but I suppose when it was easy to do, nobody did it unfortunately. All of this would be solved if Nintendo simply sold their games and we were allowed to play it on whatever hardware we want. But I understand why Nintendo doesn't want that and also that many people seem to not want to actually pay for the games they play. reply rokkitmensch 13 hours agoparentBut in no world can I buy a retro game on arbitrary platform X and then also play it on arbitrary platforms P, Q or R. reply parl_match 12 hours agorootparentYou can do this right now. For example, I have bought retro games from gog and humble that included multiple builds for different systems and platforms, and no restrictions on where you run them - just that you don't distribute them. reply effingwewt 9 hours agorootparentYou missed the 'arbitrary platform' part of 5he original comment. We all know steam exists that wasnt what OP meant. reply parl_match 2 hours agorootparentIf arbitrary platform X is one that sells DRM free games, then the comment holds true. reply ToucanLoucan 12 hours agoparentprev> But I understand why Nintendo doesn't want that This part I really don't understand. Nintendo has clearly got the technology at play to run everything at least prior to the GameCube on the Switch, ready to roll... but the only way to access those games is to pay for a subscription service which is already kind of annoying, only to then get a drip-feed of a few games every few months, selected by... somebody, no idea who, with no real consistency as to what makes the cut and what doesn't. It's, by all accounts, completely fucking arbitrary. I love the Switch, it's IMO, the best console Nintendo's turned out in actual decades, and if I was given the option I would buy the shit out of a large library of games from previous Nintendo consoles, and given how many 3rd party projects have made playing those games on all manner of shit, chiefly desktop PCs, a possible thing, I struggle to really sympathize with Nintendo here. And again: the groundwork for this is already laid. I don't know how much work goes between, for example, taking a SNES title like Super Mario RPG and putting it on the Switch's virtual SNES console, but given that their virtual SNES has essentially the same features that every bog standard SNES emulator has had for over 10 years... you'll have to forgive me if I don't think it's much? If any? I would absolutely understand if they want to playtest each game, make sure it's optimized, make sure there's no graphical oddities, etc. etc. but like... you can do that. I could do that. And hell, if a game made it through with some big glitch, give me the option to send your devs a fuckin email about it in-game with a dump of the memory at the time it happened so they can fix it. But no, instead, peacemeal releases of games, on a subscription service only, that range from absolutely S-tier iconic to... what the fuck is this in terms of cultural significance. Instead of just a bloody storefront, and let me pick what I want, and pay a reasonable price for it. I'd bet anything if they charged like $9.99 per game for the entire library of NES, SNES, and N64 titles, they'd be absolutely swimming in money. Like... the big corpos have never understood, this is what gave Steam the position they have now. Piracy is work. Hacking consoles is work. Installing and playing cracked games is work (and risk!). I don't want to work, I want to fucking play Donkey Kong. Give me a legal way to give you a reasonable amount of money so I can play Donkey Kong! And then I get Donkey Kong and you get money! It's the dictionary definition of a win/win scenario. reply wilsonnb3 9 hours agorootparent> I'd bet anything if they charged like $9.99 per game for the entire library of NES, SNES, and N64 titles, they'd be absolutely swimming in money. This is more or less how it worked on the Wii, Wii U, and 3DS so I would assume that they weren't swimming in money if they decided to change it now. It wasn't the entire library, of course, but that probably is never going to happen anyways due to licensing issues that pirates don't have to worry about. reply idle_zealot 11 hours agorootparentprev> Piracy is work. Hacking consoles is work. Installing and playing cracked games is work (and risk!). I don't want to work, I want to fucking play Donkey Kong It takes work on the side of companies like Nintendo to make piracy inconvenient. If Nintendo and others didn't spend their resources locking down their consoles and squashing any form of piracy that gets too convenient then the easiest way to play any game would be to pirate it via some community-maintained all-in-one cross-platform game installer and launcher with every game ever dumped available. Then Nintendo would have no hope of competing with convenience. At a minimum they would need some sort of payment and accounting system which would introduce friction compared to a free piracy frontend. So naturally, Nintendo is not interested in engaging in a convenience competition. They want to use the law to maintain total control of how their games can be played, and then use that granted monopoly to make as much money from them as they can. Why sell an older game for a reasonable price if you're the only game in town and can instead use it as leverage to get people to give you money regularly for the privilege of playing it? Even better, bundle the game they want with a bunch that they don't and several that they kinda-sorta want to play at some point maybe? The more you can dilute and confuse the value of a purchase the more you can make off it. Ideally you reach a point where subscribers feel some nebulous obligation to pay you regularly, and the actual service you provide only serves as to assuage cognitive dissonance should a subscriber consider cancelling. \"Oh, but I played Earthbound for a few hours last week, and maybe I'll want to get back to that at some point, so I guess I'll keep my subscription\" reply ToucanLoucan 11 hours agorootparent> It takes work on the side of companies like Nintendo to make piracy inconvenient. ... So naturally, Nintendo is not interested in engaging in a convenience competition. Oh, sure, but 9/10ths of that is already done. The eShop already exists and distributes purchased DRM-locked content to their hardware. That's my point: all the pieces for this already exist and are implemented. The only problem is the business side that insists on doing this so bizarrely. > Why sell an older game for a reasonable price if you're the only game in town and can instead use it as leverage to get people to give you money regularly for the privilege of playing it? I mean, without access to their data I can't say this for sure, but I feel like a monthly subscription for these games is substantially less money than just selling them as is. It feels distinctly like a loss-leader for Nintendo's subscription thing so they can buff the numbers of subscribers. > Even better, bundle the game they want with a bunch that they don't and several that they kinda-sorta want to play at some point maybe? The more you can dilute and confuse the value of a purchase the more you can make off it. I really don't think that rule is as hard and fast as you're making it sound. The bundle maybe, sure. But what's the value proposition for all the games not available at all, purchase or subscription? That's my real beef is the arbitrary and often nonsensical selection process. > Ideally you reach a point where subscribers feel some nebulous obligation to pay you regularly, and the actual service you provide only serves as to assuage cognitive dissonance should a subscriber consider cancelling. \"Oh, but I played Earthbound for a few hours last week, and maybe I'll want to get back to that at some point, so I guess I'll keep my subscription\" But again: just price the Earthbound game according to the work required to bring it to the new storefront. Then you're already in the black without needing to sell a subscription in the first place. And sure you aren't continuing to make money off of it, but who's to say you'll continue doing that with the subscription? People find them irritating and no matter what psychological shit you try and pull on them, at some point it's not bad odds they're just going to go \"I don't need this\" and cancel. reply ndiddy 15 hours agoprevLink to final judgement terms: https://storage.courtlistener.com/recap/gov.uscourts.rid.569... Basic summary is that the Yuzu developers agreed to shut down development, give their domain to Nintendo, and delete all copies that they posess of Yuzu and any other Switch hacking tools. reply thesnide 15 hours agoparentI immediatly had a chain of questions: * how will they pay $2M? * why did they settle for that much? * isn't strict emulation legal? Maybe not in the US... * is there something fishy that was hidden somewhere? reply super256 14 hours agorootparent> * how will they pay $2M? They most likely don't. Most of these settlements have another agreement made behind closed doors. I was alleged to do stuff related to video game cheats, and was in settlement talks with a company bigger than Nintendo. I didn't end up signing, so a quick overview of what such secret agreements include (the money point is the last one): - keep everything of the following secret - be truthful with us and tell us everything - hand over source code, server access, chats - shut down social media, websites related to the cheats - if defendant cant shut down the cheat site, defendant has to try to shut it down via other various means and email the plaintiff once every quarter for three years with the efforts made - plaintiff will make a public announcement that defendant owes 2.5 million USD (used by the plaintiff for marketing / scaring people off). After X years, the plaintiff agrees to file a full satisfaction of the monetary award (aka plaintiff files that the defendant paid the money and so their credit score isn't totally fucked), BUT in reality the defendant only has to pay the money if they breach any of the terms above. reply Sammi 12 hours agorootparentThanks for this insight. This honestly sounds like the reasonable solution for both parties. Everyone just walks away, and no ones life is ruined. reply throwaway48r7r 13 hours agorootparentprevWas your name on the lawsuit or was it an LLC? reply super256 13 hours agorootparentThe complaint listed the company (a German Unternehmergesellschaft; which is basically an LLC), but my name and many others too. reply KomoD 13 hours agorootparentLol, the engineowning lawsuit? reply johnnyanmac 15 hours agorootparentprev>how will they pay $2M? The same way Bowser did, I suppose. Wage garnishing + whatever Patreon money they earned? Apparently it's estimated to have made some $1.2 million from patreon so that may help >why did they settle for that much? Bad crooked lawyers that said they had no chance or very good lawyers that said they had no chance. IANAL, so I can't truly say which. > isn't strict emulation legal? Maybe not in the US... It is AFAIK, apparently the judgement's big argument here is >Developing or distributing software, including Yuzu, that in its ordinary course functions only when cryptographic keys are integrated without authorization, violates the Digital Millennium Copyright Act’s prohibition on trafficking in devices that circumvent effective technological measures, because the software is primarily designed for the purpose of circumventing technological measures. Again, I don't know how much water that holds. >is there something fishy that was hidden somewhere? Depends on if you think whether or not it would have cost more than $2.5m dollars to fight nintendo in a full battle. I imagine Yuzu doesn't have such funds nor means to. So this was the cheapest option for them. reply 1231232131231 14 hours agorootparentAnother case of Nintendo ruining peoples' lives [1] [1]: https://www.theguardian.com/games/2024/feb/01/the-man-who-ow... reply theshackleford 14 hours agorootparentSeems to me more like he ruined his own life through poor choices. Actions have consequences, I won’t be shedding any tears. reply mcpar-land 13 hours agorootparent> As a part of that agreement, Bowser now has to send Nintendo 20-30% of any money left over after he pays for necessities such as rent. > Bowser has now managed to secure housing, and he thinks that after rent, he has a couple of hundred dollars leftover for food and other necessities. He assumes he’ll be turning to food support services. Nintendo's not being compensated any meaningful amount. This is nothing more than a lifelong public flogging. And for what? reply indy 13 hours agorootparent\"and for what?\", the what is to send a message to everyone not to screw around with Nintendo. Whether that's morally right is another question. reply theshackleford 2 hours agorootparentWhy are you pretending Nintendo is the justice system? Are you confused by how the legal system works? reply indy 1 hour agorootparentI'm not pretending that Nintendo is the justice system, although I will admit that the US legal system does confuse me! The point was that if you have deeper pockets than your opponent then you're able to leverage the legal system to make life hard and stressful for them. Nintendo has deep pockets and the will to go after anyone it considers a threat to its business model. reply throwaway48r7r 14 hours agorootparentprevWon't somebody think of the multinationals. reply flykespice 11 hours agorootparentNobody is thinking about the multinationals, they are thinking about what is morally correct. reply autoexec 6 hours agorootparentNintendo gave up on doing what was \"morally correct\" ages ago. That doesn't make anyone else's moral failing acceptable, but comparatively Nintendo's sins are far greater and they've never faced consequences that came remotely close to what they're putting Gary Bowser through. reply colinsane 5 hours agorootparentmore people need to just read the \"history\" section of Nintendo's Wikipedia page. because that \"ages ago\" is 140 years ago, when the company was founded, by being the only {morally,legally} bankrupt enough people willing to sell their wares to Yakuza-run casinos. to be fair, they're not unique in this space. but the perception of them as \"the family-friendly gaming company\" has always been some very big amount of nonsense. reply hyperhopper 9 hours agorootparentprevMorally, these multinational corporations deserve piracy or any bad thing that happens to them. reply ascagnel_ 12 hours agorootparentprev> Developing or distributing software, including Yuzu, that in its ordinary course functions only when cryptographic keys are integrated without authorization, violates the Digital Millennium Copyright Act’s prohibition on trafficking in devices that circumvent effective technological measures, because the software is primarily designed for the purpose of circumventing technological measures. This seems like a massive stretch to ask a judge to sign off on, since it pretty radically expands what the DMCA covers. If were to come into force, you could slap some encryption on any piece of software and block anyone from interoperating with it. reply thesnide 15 hours agorootparentprevI guess the fishy thing was the Patreon, which I was not aware of. Hell, I just even knew about Yuzu because of that lawsuit. #streisand reply kevingadd 14 hours agorootparentThe relevant precedent is Bleem, which was for-profit reply Narishma 10 hours agorootparentIt's not really a precedent since PS1 games weren't encrypted. reply KomoD 15 hours agorootparentprev> * how will they pay $2M? They actually made quite a bit of money, around ~$30k/mo from just Patreon, wouldn't surprise me if they had that much money. > * why did they settle for that much? Expensive to fight it, and if they lose they'd have to pay even more. Nintendo can go on forever, they have so much money and the best lawyers. reply waffleiron 15 hours agorootparent30k a month is only (roughly rounded) 1 mill per 3 years. So that be 6 years income without spending, and it’s likely their income wasn’t always at this level. reply archy_ 13 hours agorootparentTaxes will also eat away at that reply ipaddr 14 hours agorootparentprevThey have paid themselves out already and will declare bankruptcy. reply OJFord 14 hours agorootparentIf down voters want to explain their objection we'd probably all benefit - this may sound pithy but afaict parent is correct: Tropic Haize LLC is the defendant in TFA; LLC stands for Limited Liability Company. IANAL but this essentially means the company is on the hook for the damages, if it doesn't have that much then Nintendo probably becomes it's most senior creditor (gets paid first) in bankruptcy, but its directors (the emulator devs, presumably) are unlikely to be personally liable, at least wouldn't be in a judgement on this case. Settling would I assume imply that Nintendo thinks it is getting paid (won't bankrupt the company, or directors agreed to kick it in) though. reply throwaway48r7r 14 hours agorootparentThey're about as likely to get paid as the poor sods the RIAA sued. It's just about using their legal muscle to shut them down. They will get the assets though. So $30k in patron money for the month, the domain, the trademark, and the copyright to the GPL'd code. reply OJFord 11 hours agorootparentThey didn't win in court though, they settled, so the money would be a strange and arbitrary amount if they don't think they're getting it? reply throwaway48r7r 50 minutes agorootparentPretty much. The term for this is judgement proof. reply throwaway48r7r 14 hours agorootparentprevAs far as I know (IANAL) it was Tropic Haize LLC that was sued and is on the hook for the $2M. reply aaomidi 15 hours agorootparentprev> * how will they pay $2M? Probably patreon > * why did they settle for that much? Because Nintendo will just ruin their lives otherwise > * isn't strict emulation legal? Maybe not in the US... Thank you DMCA for ruining this scene. The federal government should not be the one interjecting itself into this area, but here we are. Because of the anti-circumvention rules of the DMCA, right to repair is completely broken. Heck, WV was able to hide them lying about emissions because of it. reply choo-t 14 hours agoparentprev> give their domain to Nintendo So, Nintendo will receive all future Yuzu's telemetry. reply iamjk 13 hours agorootparentplot twist: they leave it up, collect telemetry to match and understand the data behind pirating vs. owned emulation, find it useful to open the platform, focus on services, and build their business to 10x on software almost exclusively not probable, but that would be interesting to see reply erremerre 12 hours agorootparentprevwould not be possible to send the domain to 127.0.0.1 in localhosts? reply choo-t 12 hours agorootparentYes, but you can also opt-out to the telemetry in the settings. reply erremerre 12 hours agorootparentSensible, but still would add the line to the hosts file. reply SSLy 14 hours agoparentprevsome grapevine reports indicate yuzu paid reviewers with early access to some games for pre-retail dumps that were then used to have speedy day1 compat. it could be wrong, totk just leaked a week before release. reply jandrese 14 hours agorootparentThis feels like a \"so what\" issue to me. Maybe the early release guys who sold their copies should be in trouble for breaking a contract, but the Yuzu guys should be in the clear. reply alephnerd 14 hours agorootparentPaying someone to break the rules still makes you culpable. reply futhey 13 hours agorootparentPaying someone to break the law makes you culpable. Paying someone to violate an NDA or other agreement with a private company you have no direct association with doesn't ordinarily make you culpable. reply favorited 12 hours agorootparenthttps://en.wikipedia.org/wiki/Tortious_interference reply crtasm 13 hours agorootparentprevThat would be a strange thing to do considering it was shared everywhere anyway. reply SSLy 9 hours agorootparentI've seen evidence of other games too. reply saidinesh5 15 hours agoprevThis is sad. Yuzu was such a well polished emulator. I hope everyone backs up a copy of yuzu and hopefully someone some day would continue the development... reply meragrin_ 11 hours agoparent> Yuzu was such a well polished emulator. I guess that depends on your definition of \"well polished emulator\". From what I hear, it is not well polished. reply Pfhortune 9 hours agorootparent> From what I hear, it is not well polished. Relative to what? Emulators are inherently complex software, and yuzu more than most since it has to deal with modern concerns like GPU drivers, game updates, DLC, etc. For all that it does, it is fine, it mostly works quite well. reply aquova 13 hours agoprevUnless I'm mistaken, their sister project Citra, a 3DS emulator, is also gone now. https://github.com/citra-emu Only their website remains reply haunter 13 hours agoparentLast official builds https://archive.org/details/citra-latest-builds-4th-march-20... reply TillE 13 hours agorootparentThe installers don't actually work, because wherever they hosted the updates is gone. I see a lot of people on Twitter lamenting that 3DS emulation is dead, but Citra was more or less a finished product, and third-party builds will undoubtedly be up within hours. reply daedalus2027 8 hours agoprevExamine all Nintendo software releases for potential violations of Free/Libre and Open Source Software (FLOSS) licenses, and if any are identified, take legal action to ensure compliance, similar to their approach with emulator developers. reply barbazoo 15 hours agoprevI grew up with Nintendo being my \"main\" game console and to this day I play (Switch) but I'm starting to wonder, might this be a company that's worth boycotting? Sure I'd miss out on the exclusives but lots of games exist on multiple platforms or have similar games on other platforms. Nintendo's behaviour seems to be very much anti hacker and even anti-consumer in cases like this one. Anyone else have any strong feelings about this? reply joveian 10 minutes agoparentPersonally, I only get DRM-free games, which mostly means GOG at this point (they also have a better refund policy). They have some issues too but seem to be much better than any other game stores in terms of customer friendly policies (unless you really want Linux support). I definitely wouldn't switch to Sony or Microsoft from Nintendo, that would make no sense (surprisingly there are a few Sony published games on GOG, although the latest announced one was supposed to be released a few months ago now so I'm suspicious that Sony was trying to sneak some DRM in and got caught). Reguarding Steam some would argue the Linux support makes up for the DRM, although I can't agree with that. Most of the other small platforms I've seen have DRM but with little or no positive. Itch mostly has solo developer games, doesn't clearly indicate if a game has DRM or not but mostly has DRM-free games, and has no refund policy other than what individual developers decide to do. Humble used to have some DRM-free games but has moved away from that under current ownership. reply ejj28 15 hours agoparentprevNintendo's been a horrible company in this regard for ages. Personally I've never given them money besides for Tears of the Kingdom when it released - I don't even own a Switch, I borrowed one to play it. They make good games sometimes but that's the first and last time they're getting any money from me. reply grayfaced 13 hours agoparentprevI disagree. The other platforms built their walls secure enough that they don't bother. Nintendo is aggressively defending a low fence. Most Nintendo cartridges will function a decade from now. Many discs for other consoles don't even have the full game on it... and many can't function without connecting to a service. I'd prefer the system that is possible to preserve (but gets aggressive with monetization) then the one that's impossible to preserve. Points to xbox for doing well with backwards compatibility and carrying libraries forward. But that could change in future. reply realusername 13 hours agorootparent> The other platforms built their walls secure enough that they don't bother. The other platforms have also way less exclusives. The PS5 has a grand total of 12 exclusive games listed here https://en.wikipedia.org/wiki/Category:PlayStation_5-only_ga... . Why even bother? reply SmallDeadGuy 1 hour agorootparentA lot of games are only on PS5 and Xbox S/X, not available on PC or other platforms. So if it's exclusive only to other securely walled gardens, they still don't need to bother. reply suzakus 9 hours agorootparentprevThe problem with this list is a lot of the non-exclusive games are because they're on the PS4 as well – what proportion of games are non-exclusive to the playstation world? For example for PS4 only games there's a decent amount more: https://en.wikipedia.org/wiki/Category:PlayStation_4-only_ga... reply autoexec 6 hours agorootparentprevMost of those are either remakes of older games, VR games, games that haven't been released yet, or games with poor ratings. The only real title there worth buying today would be Spider-Man 2. I'm pretty disappointed in the PS5's library but for someone who never had a PS4 the PS5 is absolutely worth it. Maybe by the time they get through the backlog of good PS4 games there will be something worth playing on the PS5 reply mcphage 15 hours agoparentprevNintendo’s disdain for historical preservation is frustrating and problematic. But this is an emulator for their current console, I’m not sure this is boycott-worthy. This is more, they’ve got a business to run, and Yuzu impacts their bottom line in a way that Dolphin doesn’t. reply skyyler 15 hours agorootparent>this is an emulator for their current console >Yuzu impacts their bottom line in a way that Dolphin doesn’t. Dolphin could emulate nearly all commercial Wii games by April 2009. That was sooner into the console's lifespan than where we are now with the switch. (29 months since the Wii's release vs 83 months since the Switch's release) Maybe more people have gaming-grade computers sitting around now than they did 15 years ago? reply SmallDeadGuy 1 hour agorootparentMost popular Wii games had significantly different control schemes than what is available on a standard PC. Afaik, dolphin was primarily used for things like Smash Bros but other system sellers like Wii Sports, Wii fit, and LoZ: Skyward Sword just wouldn't work at all. Even Mario Kart was typically played with motion controls, because that was new and exciting for home consoles at the time. Contrast that to the Switch where most system sellers can be played on a standard controller without a gyroscope, the threat to their bottom line is much higher. reply throwaway48r7r 30 minutes agorootparentThe wiimotes are just bluetooth and the tracking is 5 LEDs. I've used candles instead of the sensor bar before. You can buy a \"dolphin bar\" for $20 and use your wiimotes on the PC. It works great. I dumped my wii games and haven't touched the console in years. reply fmj 14 hours agorootparentprev>Maybe more people have gaming-grade computers sitting around now than they did 15 years ago? You don't need a very powerful computer to emulate a Switch. My M1 Mac Mini w/ 8GB of RAM (~$400) has been able to play every Switch game I've been interested in with equal or slightly better performance than the actual console. reply 1317 5 hours agorootparentan M1 is a very powerful computer reply mcphage 14 hours agorootparentprev> Dolphin could emulate nearly all commercial Wii games by April 2009. > That was sooner into the console's lifespan than where we are now with the switch. I don't have any insight into why they didn't go after Dolphin in 2009, but I do think it explains why they're going after Yuzu now. reply bombcar 12 hours agorootparentDolphin has always been very precise about doing everything \"on this side of the line\" reply ejj28 15 hours agorootparentprevWhile that's a fair point, I also have to wonder how much of that is actually lost sales, vs. people who wouldn't have bought the games otherwise, or didn't have a Switch to play on, or etc. Personally, I paid for a copy of Tears of the Kingdom when it came out, but only because I had a Switch I could borrow from someone to play it on - if I hadn't had that option, I wouldn't have bought a Switch just to play one game, and I just wouldn't have bought the game at all. Had they released the game on PC as well as Switch, I would have bought it for PC, but that's not an option. As it stands, either you have to own an overpriced and underpowered console, even if you only want to play one Nintendo game, or you can pirate the game and play it on PC, and have a much better experience than on a real Switch. Nintendo is somewhat bringing this upon itself in my opinion and I don't feel much sympathy for their lost profits, real or imagined. reply SmallDeadGuy 1 hour agorootparentHow is the Switch overpriced and underpowered? I picked up a switch lite with a game as a bundle for my wife for £180 brand new. If you want detachable controllers and the ability to dock the Switch to a TV, you can spend more. But those features and the built-in screen are ones that literally no other games console had at the time and is only just starting to gain popularity in the handheld PC market recently. It may be underpowered now, but the Switch has been out for 7 years. And it still has enough power to play games like TOTK, The Witcher 3, Xenoblade Chronicles, etc. As for releasing games on PC, I don't see any reason why they would? Developing for PC is much more difficult than a single console, when it comes to handling the variety of hardware, anti-cheat, etc. TOTK and their Mario games are system sellers, with the former even releasing alongside a special edition console and pro controller. Delaying the release of these games to add dev time for a PC port is never going to be worth it. reply gamblor956 15 hours agorootparentprevValve used the Yuzu emulator in some of its Steam Deck initial marketing, and emulating the Switch is one of the most popular uses for the Steam Deck, so there is definitely a measurable impact. Considering that the Steam Deck is more expensive than the Switch, arguably every SD player using Yuzu is several lost sales for Nintendo. Nintendo is somewhat bringing this upon itself in my opinion and I don't feel much sympathy for their lost profits, real or imagined. This is why they are going after Yuzu in the first place...Because Yuzu makes it easy to pirate Switch games and the pirates feel entitled to play those games on a more expensive device than the device they claim is too expensive. reply cheeseomlit 14 hours agorootparentYes but the more expensive device is better in every conceivable way, it's a PC instead of a locked down walled garden that only exists as a platform for nintendo products. Just because someone owns a Deck doesn't mean they would otherwise own a switch, I own a Deck and haven't remotely considered buying any nintendo hardware since gamecube. If I couldn't emulate switch games I just wouldn't play them. reply prophesi 14 hours agorootparentAnecdotally, the friends that I know who have a gaming PC own a Switch as well. The one guy running TotK @ 4K/60FPS on their rig was also the first to preorder the game. But we're all in our 30's with decent salaries. reply mcphage 14 hours agorootparentprev> If I couldn't emulate switch games I just wouldn't play them. Given this discussion started around whether to boycott Nintendo or not, it seems that you're not a Nintendo customer at all. So I'm not really sure what you would be boycotting? reply cheeseomlit 13 hours agorootparentI'm just chiming in to dispute the 'Deck owner with a switch emulator = lost sales' point that was made, but yes I've already been unconsciously 'boycotting' them for a while now. reply gamblor956 14 hours agorootparentprevIf I couldn't emulate switch games I just wouldn't play them. And Nintendo is okay with that. If you're never going to be a paying customer, they don't care what you think. reply Dylan16807 12 hours agorootparentThey didn't say they wouldn't be a paying customer. Regardless of that particular commenter, many paying customers of Nintendo have the same attitude that they will only emulate switch games. reply mcphage 8 hours agorootparent> many paying customers of Nintendo have the same attitude that they will only emulate switch games I think “many” is probably a bit much. reply kevingadd 14 hours agorootparentprevPreservation isn't a matter of flipping a light switch on the moment a console isn't \"current\" anymore. The Switch came out in 2017 - that's a long time ago. If not for people preserving firmware updates and 1.0 builds of games, emulator developers and users would have nothing to start from when they start development on an emulator from scratch in ~2025 like you suggest doing. reply SmallDeadGuy 1 hour agorootparentThe emulator could have been developed and released open source without the ability to decrypt Nintendo games, which (I believe) is a copyright violation and one side of the lawsuit. And also not put the latest emulator builds and private discord encouraging piracy behind a patreon paywall. I have nothing against a Switch emulator existing at all, but making it conveniently easy for the masses to pirate games, condoning it in private spaces you manage, and profiting off the demand for the emulator due to that piracy are all against the ideas of preservation. reply charcircuit 11 hours agorootparentprev>Nintendo’s disdain for historical preservation is frustrating and problematic. Nintendo themselves don't disdain historical preservation. They preserve their code, assets, games, etc themselves. reply kevingadd 14 hours agoparentprevI stopped playing Nintendo products (on any platform, including my Switch) years ago and haven't really missed much. They put out good games, sure, but there are tons of good games available on other platforms where buying them isn't supporting a company that abuses the legal system to harass and imprison innocent people. reply theshackleford 14 hours agorootparent“Innocent”. Reality (and the courts) disagree with you. reply jokethrowaway 13 hours agorootparentJust because something is a law doesn't make it just. reply theshackleford 2 hours agorootparentWho said anything about justice? You're moving the goalposts. reply kevingadd 13 hours agorootparentprevIf you're talking about Bowser, the only thing they convicted him of is \"conspiracy\" to do things that shouldn't be illegal [1]. All the other charges were dropped. If you think \"conspiring\" to sell modchips is a crime worthy of prison I don't know how to convince you that we should have a fair and just legal system. It was a plea, too, and anyone who knows the legal system well understands that plea bargains are generally unjust and used to bully people into not defending themselves. 1: \"Gary Bowser, 52, a Canadian national of Santo Domingo, Dominican Republic, pleaded guilty in October 2021 to Conspiracy to Circumvent Technological Measures and to Traffic in Circumvention Devices, and Trafficking in Circumvention Devices.\" reply catapart 11 hours agoparentprevI've been \"boycotting\" them for a while now. Switch was cool, but they just cannot be reasonable when it comes to IP, so I just cannot stomach giving them any money. I'm not interested in applicable laws or \"playing by the rules\" or whatever other handwavy bullshit people will feed themselves to pretend that Nintendo is \"in the right\" here. It's pretty simple: don't target people who aren't doing things that are morally reprehensible, even if it's a systemic threat to your company (cue the laughter from the capitalists who can't understand anything beyond industrial machinations). Evolve and adapt to include them in your assets, or die while the fitter companies do. Someday, someone's going to have more money than Nintendo and they will force Nintendo to do whatever they want them to. I'm just hoping its sooner rather than later. They've earned every second of their demise. Fuck any company that thinks lawfare is excusable as 'might makes right'. reply SmallDeadGuy 1 hour agorootparentThe yuzu people were doing things that are morally reprehensible. From what I've heard: * They managed a private discord with hired moderators that actively encouraged piracy. * They saved important releases to coincide with major game releases like TOTK. These releases would include performance optimizations and bug fixes specifically for those games. * These important releases would initially only be available behind a patreon paywall (like the private discord), so they actively profited off people trying to get the latest release specifically for pirating the latest games. reply jokethrowaway 13 hours agoparentprevThere's no point boycotting one company playing by the system rules. Boycott the system which enable this much centralisation of power and removes freedom from citizens. Copyright and patents are an artifact of the government being easily corruptible by media companies who make insane amount of money. Until we get rid of the government we'll always have laws which help the rich steal from the people. reply ronsor 13 hours agorootparentWell, we certainly aren't getting rid of the government, and it's an up-cliff battle to change current copyright and patent laws, so the best we can do is starve the companies that abuse these systems. Then they will have less money to perform lobbying and other activities. reply rafaelmn 13 hours agorootparentprev> There's no point boycotting one company playing by the system rules This is a bad attitude - every system can be gamed - social/market pressure is an important mechanism for shaping outcomes. reply johnnyanmac 14 hours agoparentprev>might this be a company that's worth boycotting? Sure I'd miss out on the exclusives but lots of games exist on multiple platforms or have similar games on other platforms. As far as I'm concerned, this is an issue of the DMCA more than Nintendo itself. In the strictest sense, devs really don't care about this and lawyers are only doing their job. Boycotting Nintendo may make sense in a moral sense, but not one that will change how the DMCA works. >Nintendo's behaviour seems to be very much anti hacker and even anti-consumer in cases like this one. Anyone else have any strong feelings about this? Anti-hacker, sure. 99% of companies are anti-hacker. Anti-consumer... First, I really think this is the worst modern cliche of modern media discourse. Just to remind people of the definition: > Anti-consumerism is concerned with the private actions of business corporations in pursuit of financial and economic goals at the expense of the public welfare, especially in matters of environmental protection, social stratification, and ethics in the governing of a society. I don't really see how luxury entertainment can ever meet the true philosophical meaning of the term that halts social progress and suppressing the flow of money to the populace. With all that said, the closest I feel Nintendo has gotten to anti-consumerism is the \"vault\" strategy done with Mario All Stars. Which they seemingly only did for that game and a few of those \"100 multiplayer\" style games. It's a strategy making use of false scarcity for a product that can be infinitely reproducible (and requires no servers on their end to operate) in order to increase urgency to play/buy said work. Not only is that morally repugnant, I'm not even sure if it's a financially sound decision for a company who's products are known for having a long tail in sales, unlike most video games. ---- in the colloquial sense of: >not favorable to consumers : improperly favoring the interests of businesses over the interests of consumers. every business technically strides to be anti-consumerist. The act of charging money for a product is anti-consumerist. I don't see how Nintendo differs here, nor how they are the worst, in a world where almost every AAA company in the west is trying to rely on psuedo-subscriptions with battle passes and every company in the east are making millions on mobile off of the lootbox model. Nintendo seemed to dip their toes in indirectly a few times (reminder: they do not fully own any of the remaining IPs using Lootboxes) but mostly have pulled out, even removing the gacha from a few of their games. So they for the most part simply profit from an online service with \"free\" games and one time purchase of other console games. reply kevingadd 14 hours agorootparentArguably, precedent (Bleem) supports yuzu here, the reality is that Nintendo doesn't really need a sound legal foundation to file a lawsuit against you, and their legal team can outspend almost anyone on the planet, so they can probably force almost anyone into a settlement, just like yuzu. The DMCA is a plague but I think this case would have resulted in a settlement even if the DMCA didn't exist, because Nintendo's resources dwarf the yuzu team's to that extent. reply CM30 14 hours agorootparentThe problem (as per usual) is that the legal system is horribly biased in favour of whoever has the most money/resources. When one side has millions/billions to spend on lawyers and legal fees and the other doesn't, then the former is almost certainly going to drive the latter into settling (or bankruptcy), regardless of whether the latter did anything illegal. There needs to be a way to fix the system so that both sides of any court battle are on (fairly) equal footing, and having significantly more resources than your opponent doesn't tilt the field in your favour in any practical way. reply Meninoyo 15 hours agoparentprevNintendo started with consoles and the quality sign to make sure you buy quality after people were disappointed with Atari and other systems And the target audience is still kids/family. Playing games doesn't need to be PC first or mobile first and it hinders quality. Nintendo has the same right as Sony and ms to sell their system. I would love to have it for PC, don't get me wrong, but families don't play games in front of a PC. I don't think this argument is fair. reply nerdjon 15 hours agoprevI am all for there being emulators for past consoles as a form of preservation as the hardware gets harder and harder to get. Has Nintendo ever gone after Dolphin except for the Steam thing? But, emulating a current console is not about preservation. It just isn't. You can try to say it is, but no. Maybe, maybe you could argue that you are putting in the work now so it's ready when it's needed. But then be careful about putting it out there. Just go look at the steam deck subreddit and how often Switch games are talked about, I can't imagine Nintendo was very happy about that. This wasn't just some small project that people did not really know about or served a niche purpose. This as asking for trouble and I am surprised it took so long honestly. Edit: Don't get me wrong, I hate exclusives and I wish the practice would end. But that is the current state of things and the choices leading up to this were questionable at best. Edit Again: Hold up, Yuzu was an actual company and had a Patreon bringing in over $29k a month? Yeah um, Nintendo has been bad but that's a choice for a current console. It is no surprise Nintendo went after them. reply BHSPitMonkey 15 hours agoparentObviously piracy plays a large (maybe the largest?) role here, though I wouldn't overlook the modding use case. I would be quite interested in running titles I legitimately own on more powerful hardware where I can play around with the resolution and framerate freely (and would have already used Yuzu for this purpose by now, but for laziness). reply ejj28 15 hours agorootparentThis is the only reason I have Yuzu installed - I own Tears of the Kingdom legitimately but the Switch is such a piece of underpowered, overpriced junk that I'll never replay the game on real hardware, when I can instead be playing at 1440p and 60fps with all kinds of enhancements on PC. reply nerdjon 15 hours agorootparentprevok yeah that is valid. I just have a very hard time believing that, like you said, the majority of the use case is anything other than piracy. I mean almost every emulation software somewhere says that you should only emulate games you own, but how many of us actually still own our NES or gamecube games that we may have downloaded. But it's easier to justify given that stuff not being easily accessible anymore, we don't even have a proper eshop for emulation on the Switch. reply dns_snek 15 hours agorootparent> I just have a very hard time believing that, like you said, the majority of the use case is anything other than piracy. Even if this is true, torrent clients are predominantly used for piracy too and they don't get sued. Neither are explicitly designed to facilitate it. Can anyone explain why Yuzu's case is different? reply nerdjon 15 hours agorootparentI feel like the difference here should be fairly obvious. One is an open protocol, and one is proprietary technology. AWS even at one point supported the protocol. I don't know if it's still a thing but it wasn't terribly uncommon to find Linux distros and other legitimate things shared through that protocol. Here we are talking about proprietary technology by Nintendo. On a system that they are currently selling and making money on. Given how much money they were bring in on patrion, it isn't much different than if they tried to make a physical knock off switch that could run switch games. reply _aavaa_ 14 hours agorootparent> it isn't much different than if they tried to make a physical knock off switch that could run switch games. Over in computer land we would call that \"IBM PC Compatible\". reply mardef 14 hours agorootparentI think it'd be closer to a Hackintosh. reply 12_throw_away 12 hours agorootparentprevAnyone else old enough to remember Connectix Virtual Game Station [1] for, like, MacOS 9 I think? [1] https://en.wikipedia.org/wiki/Connectix_Virtual_Game_Station reply pcwalton 14 hours agorootparentprev> Here we are talking about proprietary technology by Nintendo. x86 is also proprietary technology that Intel is currently making money on. Should Intel be able to sue AMD out of existence? NVIDIA's GPU specs are proprietary technology that NVIDIA is currently making money on. Should NVIDIA be able to shut down Nouveau? Flash is proprietary technology that Adobe still sells under the name Adobe Animate. Should Adobe be able to shut down Ruffle? reply nerdjon 13 hours agorootparent> x86 is also proprietary technology. Should Intel be able to sue AMD out of existence? AMD has a perpetual license from Intel to use the technology. Nouveau is different, you are still buying Nvidia cards just running an open source driver. Nvidia doesn't make money on their drivers, they make money on the graphics cards. Similar situation to Ruffle, Adobe never charged for the end user to download flash. It was free. Ruffle is an alternative to that. Also it is worth mentioning that flash is all but dead and this really just keeps it breathing. While both companies probably could make an argument to argue for a take down of both, they have no incentive to do it. reply pcwalton 13 hours agorootparent> AMD has a perpetual license from Intel to use the technology. AMD only managed to negotiate that because Intel lost in arbitration [1]. Intel's preferred option was always to eliminate AMD entirely. It's good for us consumers that they didn't succeed in that! > While both companies probably could make an argument to argue for a take down of both, they have no incentive to do it. NVIDIA absolutely has an incentive to get rid of Nouveau. Its existence discloses IP (their GPU inner workings) that they would prefer to keep secret. More examples: JavaScript was proprietary technology at the time. The fact that it was specific to Netscape browsers absolutely benefited Netscape's business model. The existence of Chrome depends on the fact that Netscape had no grounds to go after Microsoft for an independent implementation. SMB is a proprietary Microsoft technology. '90s Microsoft would definitely have preferred to keep that specific to Windows in order to sell more Windows licenses. It's good for the industry that Microsoft never felt they could go after Samba. Another fun one: The FBX file format, which Autodesk makes money on through Maya, has a half-hearted attempt at DRM in it to limit it to approved Autodesk licensees. Blender's FBX exporter breaks it with a pass-the-hash attack. Get rid of that and Blender can no longer talk to Unity. Obviously, the entire industry benefits from the fact that Autodesk can't go after Blender for this. [1]: https://en.wikipedia.org/wiki/AMD#IBM_PC_and_the_x86_archite... reply nerdjon 13 hours agorootparent> AMD only managed to negotiate that because Intel lost in arbitration [1]. Intel's preferred option was always to eliminate AMD entirely. It's good for us consumers that they didn't succeed in that! OK? regardless of why or how it happened, it happened and it means that AMD is fine. If you knew that I don't know why you even mentioned it in the first place. > NVIDIA absolutely has an incentive to get rid of Nouveau. Its existence discloses IP (their GPU inner workings) that they would prefer to keep secret. Any articles to back that up? Seems counter to Nvidia offering support in publishing documents: https://en.wikipedia.org/wiki/Nouveau_(software)#History > Another fun one: The FBX file format, which Autodesk makes money on through Maya, has a half-hearted attempt at DRM in it to limit it to approved Autodesk licensees. Blender's FBX exporter breaks it with a pass-the-hash attack. Get rid of that and Blender can no longer talk to Unity. Obviously, the entire industry benefits from the fact that Autodesk can't go after Blender for this. Again would love an article on this. I can't find anything backing up that this ever happened. Not only on Audodesk's website do they mention third party software but they have an SDK for this file format for others to use. While blender does in fact not use that SDK, and the format is proprietary, I can't find anything backing up what you claim. reply pcwalton 13 hours agorootparent> OK? regardless of why or how it happened, it happened and it means that AMD is fine. AMD is only fine because Intel wasn't able to sue them out of existence. If Intel had managed to do in the 90s what Nintendo did to Yuzu just now, there'd be no Ryzen today. > Seems counter to Nvidia offering support in publishing documents NVIDIA only started publishing documents because Nouveau's success in reverse engineering meant that it was pointless trying to pretend that the genie could be put back in the bottle. Nintendo undoubtedly knows this too; lawsuits like this in 2024 ultimately aren't rational moves on their part, but big conservative Japanese companies have never been known to be particularly adaptable. > Again would love an article on this. I can't find anything backing up that this ever happened. Not only on Audodesk's website do they mention third party software but they have an SDK for this file format for others to use. While blender does in fact not use that SDK, and the format is proprietary, I can't find anything backing up what you claim. I found it myself when I was documenting the FBX file format (which I eventually gave up on because it was too horrifying of a format to motivate continuing) [1]. Blender calls it \"CRC rules\", but I think it's actually an attempt to lock non-licensees out. The SDK is closed-source, proprietary, and comes with a whole bunch of restrictions in its EULA. [1]: https://github.com/blender/blender-addons/blob/main/io_scene... reply archy_ 13 hours agorootparentprev>x86 is also proprietary technology that Intel is currently making money on. Should Intel be able to sue AMD out of existence? AMD has a license to the x86 architecture, alongside Via (though I'm not sure if they even make x86 chips anymore). I'm sure Intel would love to take back AMD's license, but they're probably too afraid of antitrust actions to try. reply iamtedd 13 hours agorootparentIntel won't revoke AMD's license, because they themselves are licensing the 64-bit architecture from AMD. It's called amd64 for a reason. reply pcwalton 13 hours agorootparentYep. It's a great example of how keeping clean-room reverse engineering legal is good for the industry. While Intel was stuck in Itanium hell, AMD was able to leapfrog them and create x86-64 because it had been legally successful with reverse engineering 32-bit x86 in the past. If Intel had been able to crush AMD in the 90s, there's a good chance x86 would be dead by now. reply BHSPitMonkey 10 hours agorootparentprevAndroid built their own implementation of the proprietary Java language to be able to run programs that were written to run on the official JRE. Can you imagine if Oracle tried to sue Google over that? :) reply boolemancer 7 hours agorootparentprev> Given how much money they were bring in on patrion, it isn't much different than if they tried to make a physical knock off switch that could run switch games. Why would that be a problem? As long as they're not actually violating trademarks or patents and making an actual counterfeit product, there is no reason why they shouldn't be able to make their own hardware that is compatible with Switch games. These products already exist for other systems, and they are a good way to allow you to play your existing games on more modern hardware. Just because it upsets some executive at Nintendo doesn't make it illegal. reply Wowfunhappy 11 hours agorootparentprevA torrent client is data agnostic. It can be used to transfer any type of data over the internet. Yuzu is designed to play specific software which is quite difficult to acquire by legal means. Very little effort appears to have been spent on improving the UX of the legal process. (It is true that Yuzu can also play homebrew software. I think the situation would be different if Yuzu was tested exclusively on Homebrew, and only emulated features which homebrew software uses. But then no one would care about Yuzu. Yuzu has tons of game-specific fixes for commercial titles.) reply boolemancer 7 hours agorootparent> (It is true that Yuzu can also play homebrew software. I think the situation would be different if Yuzu was tested exclusively on Homebrew, and only emulated features which homebrew software uses. But then no one would care about Yuzu. Yuzu has tons of game-specific fixes for commercial titles.) Yuzu can also play games that were backed up by people who legitimately own the game. The legality of that might be questionable in the US, but not everywhere. reply rokkitmensch 13 hours agorootparentprevPretty great that you can still get functioning N64 THPS2 carts for...sixty bucks on Ebay. reply bobajeff 14 hours agoparentprev>emulating a current console is not about preservation. It just isn't So we should all just wait until they stop selling a console before working to emulate it? Just imagine if they had this attitude when it came to reverse engineering IBM PC's bios. reply kevin_thibedeau 13 hours agorootparentIBM published the full BIOS listing and schematics. There wasn't much to RE. reply nerdjon 14 hours agorootparentprevDid you... not even read the entire line that you decided to just quote part of? > But, emulating a current console is not about preservation. It just isn't. You can try to say it is, but no. Maybe, maybe you could argue that you are putting in the work now so it's ready when it's needed. But then be careful about putting it out there. reply bongodongobob 12 hours agorootparentprevIf it's still being actively sold, why would the emulator be needed or distributed? It's a bullshit argument for current consoles. reply ravenstine 13 hours agoparentprev> Hold up, Yuzu was an actual company and had a Patreon bringing in over $29k a month? Yeah um, Nintendo has been bad but that's a choice for a current console. It is no surprise Nintendo went after them. LOL Yeah, as much as I am an anti-fan of Nintendo, it sounds like this company was asking to get sued. reply Dylan16807 12 hours agorootparentMaking a big company angry is supposed to be legal. reply stavros 12 hours agorootparentIt is legal, and you can spend all your money (and more) on lawyers' fees to prove it! reply throwaway48r7r 14 hours agoparentprev>But, emulating a current console is not about preservation. Today's present is tomorrow's past. reply realusername 13 hours agorootparentEspecially considering that consoles have a less than stellar support from manufacturers, they just move on to the next console and you can say goodbye to your games, especially nowadays with game servers. reply SmallDeadGuy 28 minutes agorootparentNintendo have typically been pretty good with backwards compatibility as newer versions of consoles come out. GBAs could play GB/GBC games. DS could play GBA games. 3DS could play DS. Wii could play GC games. Wii U could play Wii games. The Switch is an outlier in that regard but mostly because the hardware is so different from previous consoles. It could never support the 2 screens required for DS/3DS or some Wii U games, nor is it big enough to fit Wii U disks anyway. But it wouldn't surprise me if the Switch 2 could play Switch 1 games. Nintendo also typically put entire games on their cartridges, and day 1 patches are for bug fixes only and are optional. If you keep the cartridges and your console, you keep your games perfectly fine. Or you can go out and buy cartridges second hand. And digital downloads will also still function, like my 3DS still has my digital purchases even if the eshop is gone. I just can't purchase new games anymore, for a 12 year old system. reply imiric 9 hours agoparentprev> emulating a current console is not about preservation. It just isn't. You can try to say it is, but no. Maybe, maybe you could argue that you are putting in the work now so it's ready when it's needed. But then be careful about putting it out there. What do you think of piracy in general? When would you consider pirating a PC game would be acceptable? The reality is that the modern gaming industry is actively hostile to the consumer. Content is locked behind intrusive DRM, store front exclusivity deals, licensing terms that may revoke your access at any point, obnoxious launchers, always online requirements, reliance on servers that may disappear at any moment, insidious subscriptions, microtransactions, and many other schemes ranging from shady to borderline illegal. Hype-driven marketing pushing pre-orders based on lies, followed by empty promises of multi-year roadmaps to get games in a playable state. Yearly releases of rehashed and reskinned content, low-effort and premature \"remasters\"... The list goes on and on. Is someone who plays a pirated version of a game they purchased, but can't access anymore, in the wrong? Or how about if they prefer the pirated version because it gives a better experience? Or how about if they're tired of always getting the short end of the stick by playing by the rules? The law says they are, but are they really? The morality here is not so black and white. BTW, I agree with you that Yuzu was clearly overstepping the boundaries, and that it's no surprise Nintendo took them out. I just think that Nintendo doesn't necessarily have the moral high ground, and that there's a strong argument in favor of not only emulation, but piracy. reply Phrodo_00 12 hours agoparentprev> But, emulating a current console is not about preservation. It just isn't. No, but it CAN be used as a tool to play legitimate copies of games on different hardware. A few people on this thread have said they do this. I haven't done it yet, but I'll probably dump the Switch games I want to play on my Deck instead of carrying the switch also. reply veec_cas_tant 15 hours agoparentprevI don’t think preservation is an important argument. If I own an AppleTV, and I purchase a movie through their service, why should it be illegal for me to export that movie to watch on a different device? I own the hardware and the data, why would any law even care at that point? Obviously piracy should be illegal, but I just don’t see any argument against emulation even if it is current. reply archy_ 13 hours agorootparentPretty sure the TOS of AppleTV prohibits exporting the movie - you have a limited license to view it only under the circumstances Apple dictates, in partnership with everyone Apple has deals with. If you want to be able to watch it on other devices, you need to purchase a limited perpetual license in the form of a DVD/bluray. Now, of course, this depends on the movie having a physical release (which is becoming a bit of a rarity these days), but still, the MPAA has made sure to write ironclad contracts to prohibit watching streamed content the way you want. reply crtasm 13 hours agorootparentThe ToS has nothing to do with if something is legal or not. reply jokethrowaway 14 hours agorootparentprevYou don't own the data, just a temporary right to use it. Of course it's completely bonkers and the result of massive corporations bribing governments to limit our freedom. As long as I don't sign a contract with an entity I should be able to do anything I want with bytes. Once I enter into an agreement not to share some data I received then I should be punishable - but no shortcuts, sue everyone in court with a due process and lawyer fees. Copyright and patents are the most retarded and culture damaging thing I've seen in my lifetime reply frameset 13 hours agorootparentIf buying isn't owning then piracy isn't stealing. reply wilsonnb3 9 hours agorootparentQuite a pithy phrase that has been popular lately but piracy advocates have always claimed that piracy isn't stealing, regardless of whether or not \"buying is owning\" so I don't think the two are related. reply kenmacd 9 hours agorootparentprev> You don't own the data, just a temporary right to use it. Agreed, bonkers. You didn't 'own the data' when it was a VCR recording a broadcast TV show or movie, but now it's all \"but they broke our ROT13-super-crypto, throw the book at 'em\". reply gamblor956 15 hours agorootparentprevFor this reason, most of the major studios are members of Movies Anywhere, which allows you to access movies purchased at any (participating) storefront on any participating VOD service. Amazon, Fandango, Vudu, Disney, are participants. reply veec_cas_tant 14 hours agorootparentTrue, but the example was just an example. Buying a game on PC and making it run on a PS5 - why would something like that ever be illegal? reply gamblor956 14 hours agorootparentIt's generally not, because you can run an OS on a PS5 that can then run the game, and the DMCA allows for this (with some limitations) because, and this is a very important point: there is generally no DRM-hacking required. But...if DRM hacking is required to get a PC game to work on a PS5, it's illegal to do so under the DMCA. (Note: DRM is defined very broadly for DMCA purposes.) Dolphin does require a bit of DRM hacking, but Dolphin (arguably) falls under one of the DCMA exceptions for archival use purposes. This is the second important point: the Dolphin emulates a system which has not been on sale for a decade. It's still possible* to use it for general piracy instead of archival access, but the archival use trumps the piracy concern. (It would be different though if the Dolphin developers started offering Dolphin on a commercial basis.) But Yuzu is a commercial offering, for a console that is still on sale, and its use requires DRM hacking. So it's got 3 fatal flaws. The only surprise is that Nintendo let it live as long as it has. You can bet they won't make that mistake again with their next console. reply veec_cas_tant 12 hours agorootparent> ...and this is a very important point: there is generally no DRM-hacking required. But...if DRM hacking is required to get a PC game to work on a PS5, it's illegal to do so under the DMCA. (Note: DRM is defined very broadly for DMCA purposes.) Wouldn't Steam, MSTF store, Epic, etc. all count as DRM? Regardless, it's more of a philosophical argument than a legal argument. If I buy the Switch and the game, I don't see how there could be any argument that I should be considered a criminal for using the data on a PC. reply gamblor956 12 hours agorootparentWouldn't Steam, MSTF store, Epic, etc. all count as DRM? They could, which is why I was careful to point out that you can install an OS on your PS5, on which you can then run (some) games on it without having to circumvent DRM. If I buy the Switch and the game, I don't see how there could be any argument that I should be considered a criminal for using the data on a PC. Because U.S. (and EU) law doesn't have any exceptions that would cover that use, since the Switch is still being actively sold on the market today. (And as noted elsewhere, this is a large part of why SNES, N64, and Gamecube emulators haven't been targeted by Nintendo: the machines are no longer sold and so emulation allows for archival use/access to games on those platforms. This is generally a permitted use.) reply boolemancer 7 hours agorootparentprevAs far as I'm aware, the DMCA doesn't have any sort of exception for archival, but it does have one for software interoperability, which emulators definitely are, regardless of how current the hardware being emulated is. reply johnfernow 11 hours agoparentprevSo long as Nintendo doesn't end up going after emulators released before the Switch, then the de facto (not de jure) precedent is: don't release an emulator for their current latest console. Overall, that's probably a mostly fine outcome for game preservation, and thinking about it more, it's probably much better for emulation and game preservation long term that Yuzu's devs settled instead of fighting this, as if they had lost and a legal precedent were set, many other emulators might have had to shut down or be hosted in countries where the DMCA doesn't apply. But with no legal precedent set here, emulators for consoles released before the Switch might very well be safe. Time will tell though. reply aaomidi 15 hours agoparentprevFolks. This is not about piracy at all. It is also not about copyrights or whatever. It's about the shitty part of DMCA's anti-circumvention. If you want to see how stupid this law is, look at the number of exemptions given for this: https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_A... reply aaomidi 15 hours agorootparentI'll also go as far to say, if you support Nintendo here. You also are supporting Apple shutting down Asahi linux. All Apple will need to claim is that Asahi is bypassing some \"client side protection\", and Asahi is shut down. reply jamesgeck0 13 hours agorootparentApple Silicon machines have a \"permissive security\" mode which allows booting unsigned third party kernels. Asahi isn't circumventing anything, this is a officially supported feature. reply Wowfunhappy 10 hours agorootparentprevApple spent substantial engineering effort modifying their iOS bootloader so that Apple Silicon Macs could load third party kernels. If Apple wanted to kill Asahi Linux, they could just remove this functionality from iBoot. Apple will not remove this functionality, because they would not have added it in the first place if they did not want people to be able to use it. reply mcphage 13 hours agorootparentprev> All Apple will need to claim is that Asahi is bypassing some \"client side protection\", and Asahi is shut down. Is Asahi bypassing some client side protection? reply unethical_ban 14 hours agoparentprev>But, emulating a current console is not about preservation. It just isn't. I disagree in the general case. Sure, beware of litigious companies, but it isn't immoral. In the specific case of Yuzu bringing in cash from the endeavor, yes, I see why Nintendo did what they did. --- Totally off-topic, but I have never bought a Switch despite liking the idea of all the games on it, because I don't want to pay $600 for a gameboy, 2-4 real controllers and a copy of Mario Party. I can't understand how it or Steam Deck is popular. But that's just me. reply throwaway48r7r 14 hours agoprevPersonally, I will be boycotting Nintendo and will urge others to do the same. reply iamunr 14 hours agoparentNinte",
    "originSummary": [
      "Yuzu settled a lawsuit with Nintendo, agreeing to pay $2.4 million in damages, with both parties mutually accepting the agreement."
    ],
    "commentSummary": [
      "The Yuzu emulator developers settled a lawsuit with Nintendo, paying $2.4 million in damages, sparking discussions on piracy, access to older games, and Nintendo's business model.",
      "The debate delves into ethical aspects like anti-consumer tactics, DRM, and the significance of game preservation in the digital age.",
      "The conversation sheds light on the intricate intersection of intellectual property, copyright laws, and the delicate balance between corporate profits and consumer entitlements."
    ],
    "points": 486,
    "commentCount": 358,
    "retryCount": 0,
    "time": 1709575226
  },
  {
    "id": 39597131,
    "title": "Managing Type 1 Diabetes with Golang: Innovative Tools and Tips",
    "originLink": "https://www.bytesizego.com/blog/keeping-alive-with-go",
    "originBody": "Click here to level up as a Go Engineer Login Join our mailing list Get the latest and greatest updates to your inbox! Email Join Mar 04, 2024 How I keep myself Alive using Golang Matt Boyle 3 comments In this blog I explore how I use an incident management mindset to manage a complex medical condition. I hope you enjoy it! The British love to drink. But how many have you have ever stopped to wonder, how many grams of carbohydrates are in a pint of beer? What about in this meal? And what about this salad, that is usually listed on a menu as the low carb option? One answer you might give reading this is who cares? People only care about the amount of calories they are eating if they are trying to gain or lose weight. Maybe you care about carbs a little bit if you are following the Keto diet (but even then, a lot of “keto” meals do have carbs in them). Well, it turns out there is at least 8 million people globally who really care about the answer to these questions, and I’m one of them. In 2020, I was diagnosed with Type 1 diabetes. Likely you have heard of diabetes, but you might not be too familiar with type 1, which is a rarer form of it. Before diagnosis I wasn’t too familiar either, so here’s a brief primer. Type 1 in 100 words Type 1 diabetes is an autoimmune condition where the pancreas produces little to no insulin, a hormone essential for converting carbohydrates into energy. This means Insulin has to be injected to supplement the fact that you don’t produce it. How much do you inject? It depends on what you are eating and a bunch of other variables that are so discreet it sometimes feels like it’s random. The Insulin pen I use before eating. On the right you can see a configurable dial to determine the amount of units to inject. Type 1 is not caused by lifestyle, and it has no cure. In practise this means Type 1 diabetics have to monitor their blood sugar at all times. If it is too high for too long, then it could lead to long term damage and a shortened life span. If it is too low for even a short period, then it can be fatal. Sometimes if it gets too low I am effectively paralysed and am unable to eat or drink safely and I will need support. Thankfully, for me at least, this is rare. However, it's a condition you very much need to respect and never take your eye off the ball with as one mistake really could be fatal. With type 1, simple things are taxing. Whenever I see food or a drink like those above, I’m doing mental gymnastic to try and figure out how much insulin I need to inject and what impact that will have on my day. For example, If I want to go for a brief walk, I need to think about how much active insulin I have in my system as my blood glucose levels tend to drop quite aggressively with even moderate exercise. I hope the start of the blog didn’t seem too “gloomy”, I have tried to be as factual as possible. The reality is I have a pretty optimistic outlook. From research it seems there has never been a better time in history to be a type 1 diabetic. I hope that every year that goes by I get to say that and mean it more. Even in the short time since diagnosis, technology has come a long way for health monitoring and I can now monitor my blood glucose wearing a device on my arm. Here is a picture of me wearing it and yes, I am available for modeling work. I have to change this every 2 weeks, and it is essentially a little pin that goes into my arm. Here’s a picture of it before application: The device (Called a Libre) takes a reading of my blood when I tap my phone against the device. If it goes below the red line in the image below, I need to eat some carbs. If it goes outside of the green range, I may want to take some action, including taking some insulin. It gets a little more complicated than this though. As mentioned, doing exercise, gaining weight or being sick changes insulin sensitivity and so management becomes a moving target. Its therefore essential that I have a dynamic approach towards management. Furthermore, sometimes this happens. During this period, I am not able to take my blood glucose levels using the device and I also would not receive alerts, if my blood sugar was to reach dangerous levels. This error had the habit of showing up at the most stressful times. Especially in the first few months of diagnosis, I was really struggling to control my blood glucose and was completely dependent on the monitor. Even today I have periods when my blood goes low and I can’t figure out why. If visibility was to drop to 0 like this at work, we’d declare an incident and we would not close it until visibility was restored. Furthermore, my blood glucose going low is the first signal that something is going wrong and some action should be taken. I wonder if I can apply the same thinking we apply to incidents, to my blood glucose? A Brief Detour into Incident Management Although being diabetic does feel all-consuming at times, I do work full time as an Engineering Manager at Cloudflare. At Cloudflare, If anyone in the company (or an automated system) detects that something is not working as it should then we raise an incident. It's not quite a big red button that we press, but it is not far off. We have developed an internal bot that can be used to raise incidents, manage the process and generate incident reports at the end. To detect and raise incidents, we use the familiar suspects of Prometheus, Grafana, Alertmanager and Pagerduty, as well as some of our own custom tools. If you have not used these tools before, here’s a one sentence summary of their role. Prometheus is an open-source tool used for event-monitoring and alerting. It’s incredibly powerful and a great option for storing time-series data in. Grafana works really well with prometheus and allows you to build beautiful dashboards over the top of the data. Alert Manager allows you to configure alerts for specific queries to trigger pages. PagerDuty is the only application on my phone that gives me dread when I see a push notification for it. Every Engineering Manager and some senior contributors receive training on what we call being an incident commander. It’s not quite as glamorous as this image generated by DALLE 3 but I do know some engineers who have widescreen monitors as big as this now. As an incident commander, you are empowered and encouraged to pull anyone in from anywhere in the company to resolve the incident as quickly as possible. By resolve, I mean remove customer impact which means you must identify, triage, fix/put a short term band aid on and identify and record long term actions. We then do a review of every incident to ensure we learn from it and take the necessary steps to ensure it does not happen again. Successful incident management is all about collaboration and working together towards a common goal, which is usually restoring established behaviour.This may mean engaging with a subject matter expert or escalating it. In an ideal world, you engage as few people as possible to resolve the incident, as being paged can be pretty disruptive. However, it’s much less disruptive than ignoring the signals and trying to repair something that has now completely broken. I wanted to apply this same thinking to managing my blood glucose. If something happens which causes my blood glucose to trend downwards, I wanted to be notified so that I can take an action before any more major issue appears. However, if things do take a turn for the worst or I’m unable to self-resolve, I wanted to feel good about the fact that someone else would get notified if I am unable to help myself and hopefully prevent it from escalating from a P3 to a P0 incident. Type 1 Incident Management I really wanted to get to a place where i felt confident that if my device was taking no readings or I was critically low, myself or someone would be notified regardless. The device I use is closed source, and does not give an easy way for you to build on top of it. It does not expose any APIs, and certainly has no SDK. I spent some time messing around trying to get the data off of it but did not find an obvious solution. I started to explore other solutions. After some research I found this device called a Miao Miao. It slots over the top of the device on my arm and effectively “scans” the libre every 2 minutes and sends the result to another app that is called tomato. The really nice thing I found about this that even when the main device would show me the error, this one would continue to publish data. After playing around it seemed that the main app didn’t like sending data that it felt was anomalous due to a sudden change in blood glucose level. However, I personally would like to know about this, or atleast would like to make the decision myself of whether it is useful rather than it being made for me. Natively, the device also includes a really clever “hack” that effectively published your blood glucose to Google calendar as events every 5 minutes. By doing this, you could use Apple’s native complications to see your blood sugar on your Apple Watch. I found this really exciting and it immediately got my brain whirring as this meant their must be a way to submit blood glucose readings to a third party API, I just need to figure out how to send it. Engineering a Solution Whilst looking through the app, I saw in the settings that it had a data sync option and you could enter a URL, intended to be used for an open source tool called nightScout. Instead of entering a Nightscout URL, I pointed it at a webserver I had running. I entered the URL and hit submit, and I saw these log lines in the Gateway. I now knew what endpoint the device was trying to submit data to but not what that data looked like. To solve this, I wrote a simple echo server using Go that matches the route that we saw previously. To deploy it I’m using encore.dev which allows me to deploy and run my monitoring system for free. The annotation at the top is instructions for encore on where my httpHandler will run. An echo server is a server that sends back the same data it receives from a client. In other words, whatever you send to an echo server, it \"echoes\" or reflects back to you. The concept is simple but they can be really useful for network troubleshooting, application behavior testing, or something like this. Echo Servers are a useful tool in any language, but they are exceptionally easy to write in Go. This one is barely 3 lines long and does exactly what we need. I repeated the same exercise for some of the other requests the device made and came across one that gave this response. This is perfect. SGV is my blood glucose. In other countries they use a different measurement than we use in the UK so 73 here needs to be divided by 18 to give the scale I use in the rest of this blog. It gives a date time. It gives a trend direction. This call was being made every 2 minutes. Now we have an endpoint we can submit readings too, I made a gauge and set my blood sugar everytime I receive one. A gauge is a metric that is exposed that represents a single numerical value that can go up or down. It is used for measuring values that can fluctuate, such as the current memory usage, number of concurrent requests, temperature, or, in this instance, blood glucose. I also “best efforts” store the data in Postgres (which is the insertReading function). This is one of the few use cases I have come across when the DB persist is not too important to me, and I don’t want to fail the rest of my logic because of its failure. I therefore log it to acknowledge it and move on. I can now have my blood sugar being submitted to an endpoint every 2 minutes and setting a metric. Now I have this gauge being updated every couple of minutes, I can now make a Grafana dashboard which shows my blood glucose in real time. The Y axis shows my blood glucose level. I want to keep this value between 4 and 9 ideally. As you can see I can view trends over time and share it with anyone. I could even setup a monitor at home for this dashboard so I can glance at it. So now I can share my blood glucose data with anyone, but its kind of useless in isolation. The same as monitoring any complex system, context is important. For example, if: I have just eaten there is a spike … but I have already had insulin its nothing to be immediately worried about as Insulin takes a bit of time to work. I therefore wanted to build a quick way to add this context. Grafana supports annotations, but I didn’t want to login to annotate and wanted it to be less of a burden. I therefore build a chatbot using telegram that enabled me to add annotations. Telegram can be configured to send a webhook every time a bot receives a message. Here’s what that looks like. The webhook receives a request, I validate the message, unmarshall it and write back to telegram to let me know if there was any issues processing it. After the validation I attempt to parse ints and then call my annotation service, before writing back to Telegram and also returning an ok. My annotation service assigns tags to the message based on its content, and sends these to Grafana. In Grafana, I can assign colours to these different tags and setup annotation queries. We now have all the pieces for it to work! Here’s a gif of me chatting to the bot and the annotations showing up on Grafana I now have a way to add context to my graph which is really powerful for both myself, and other people viewing it to understand what is going on with my blood glucose. If I do have a low blood sugar event, then there is lots of context around it which is really valuable to understand what action to take. What about alerting? So we now have great visibility, but no alerts. Guidance given to me by nurses is that when my blood glucose goes below 4, I should start treating for it. I therefore wrote a tiny cron that checks my readings every 5 minutes and if a reading is less than a pre-defined limit, it triggers an incident. Encore makes this very easy, and you define cron jobs in code like this. I also added this block of code to my telegram bot to allow me to manually trigger an incident if for any reason I needed to. This is what that looks like. But what actually happens when I trigger an incident? When I trigger an incident, it calls my incident microservice which opens an incident incident.io. I picked incident.io as it is the closest tool to Cloudflare’s in-house incident tooling and it is also written completely in Go. The reason I wanted to use an actual incident tool is allows me to set up escalation policies, run reports to see how much time spend in “incident” state, amongst other things. Incident.io also has the ability to run workflows. This means I can configure rules similar to what you would in Zapier, and can send notifications via text to folks who have subscribed. In this example, every time I open a workflow, it’s going to send a text message to me. If the incident isn’t closed for 20 minutes, it will automatically escalate it to my partner or siblings, depending on who is on good terms with me at the time. With this, I have been able to create a pretty robust incident management workflow. As I mentioned previously, because I use incident tooling I also get reports effectively for free. This allows me to see what/how many low blood glucose incidents I have over time. The above graph is demo data I put together for this blog as my real data was thankfully a little boring! However, you can see how powerful and useful this is. Based on the above graph, I would be able to conclude the amount of low glucose events I am having is trending up, and that would be a good signal to engage with my doctor as my current approach to treatment is clearly not working for me. Perhaps something had changed about my lifestyle and I needed to reduce my insulin regime. Next Steps I have only described a subset of the features of my system. Some other things it can already do or I plan to build are: Auto-close incidents. Right now I am manually closing them, to give me an opportunity to review the data. However, I have been to slow to do this a couple of times which has led to the incident escalating. I could easily auto-close it after 15 minutes or so of solid blood glucose. I mentioned previously I am storing all my blood sugar data in a database, but right now I’m not really doing anything with it. I have a couple of years worth of data now so it would be great to train an LLM on it so I can have conversations with my data. I could ask questions like “why do i always go low at 3pm?” and use other data like my google calendar to try and answer it. I could add way more graphs and measures. Right now my graph is simple but there are lots of measures that are used to judge your success as a diabetic, such as hba1c which i could easily add. More fail safes. This system is critical so the more I can to prevent failure the better. Wrapping Up When I first got diagnosed with Type 1, I was pretty scared. I thought it would be debilitating and it would make my life really hard. Whilst I won’t pretend it’s a breeze, this project has really helped me understand my condition and to manage and monitor it the best way I know how; as if it was distributed system. Being able to code genuinely feels like a superpower and has enabled me to somewhat automate management of a condition that even as late as 1920 was a death sentence. I hope you enjoyed this tale and it inspires you to build cool stuff too. Please let me know if you do! Get ByteSize tips straight to your inbox! No spam, just great go tips Email go func(){ Submit() } 3 comments Eduardo Viccari16h Congrats Matt! I'm from Brazil and i have diabetes type 1 to. Here in Brazil the Libre monitores has a very hight cost, $60,00 right now. I'm working with Go and I spent amout of time searching about Libre's interface or some how connect to the monitor, without success. This post will help so many people :). Please let me know if you need some help with this project in the future. Best regards! Piper McCorkle2h I was disappointed to see that your chatbot is called mjbBloodGlucoseBot and not some sort of pun on respecting tables, given the system this whole thing is modeled on ;) Serra Ilgaz2h Matt this is such a great post for many people. Especially for young type 1 patients who are potentially vulnerable to the psychological effects of being diagnosed with the condition. You are really giving an excellent example of how one can make the best out of it all 🙏🏻 Sign upor login to leave a comment Terms of Service Privacy Policy",
    "commentLink": "https://news.ycombinator.com/item?id=39597131",
    "commentBody": "How I keep myself alive using Golang (bytesizego.com)424 points by ingve 11 hours agohidepastfavorite90 comments UomoNeroNero 41 minutes agoI have been diabetic for 16 years. I love your mood and I respect you, when you are sad or angry with the illness, reread this post and remember how sure you were that you could handle everything. It's not easy (mentally and practically) and for me, for many years, the situation has been \"joyfully out of control\" Just yesterday I cried (really!) watching that for the first time in my sick life I had a 36 hour streak on target. 100% green for 36 hours. I repeat, I cried. Maybe I understood. Maybe... The leap in quality after years to 60% was due to: - microinfusor connected to the CGM (a revolution compared to pens) - having had the opportunity to share my experience as a diabetic with a group of other diabetics. Something like Alcoholics Anonymous but much more like Fightclub :-D - having installed a VM with NightScout to always have an eye on blood sugar levels and trends EVERYWHERE: phone, clock, desktop widget, alarm clock, fridge :-D - count the CHO with German precision (and analyze the history with respect to pre/post blood sugar levels) and do the boluses STRICTLY 15 minutes before the meal. I hate hate hate living like this but IT WORKS Tieni duro. Non ti disunire reply sgt101 41 minutes agoprevCamdiab (https://camdiab.com/faq) is available in some places (UK) to provide automated blood sugar level management. I have dived into this a bit, and the thing that I grokked is the challenge of proving that the algorithm/device loops really work and are really safe. There has been a lot of painstaking work to develop and certify a thing like Camdiab. Unfortunately it also means that it's important to buy the right smartphone to make sure that the right libraries are available and being used by the app... Has been a bit expensive. They have also walked the road of testing and proving the devices/algorithm for use in small children and teenagers. Bear in mind that this is all very complicated and there are a lot of differences in the behaviour of different hormone systems in different cohorts. reply grahar64 3 minutes agoparentMy son was on CamAPS for about 6 months with the Dana pump. Systems like that are even harder to get out because it relies on 3rd party hardware. Medtronic, Insulet's and a bunch of other systems are rapidly becoming a walled garden where the only way you can run the hardware and software you want is to run open source like AAPS or iAPS. reply ping00 8 hours agoprevI just wanted to add that your writing style is wonderful and that this was a pleasure to read. Incidentally, I work in pentesting, and one of my colleagues has Type 1 Diabetes; your overview of it and its resultant complexities really made me empathize with what challenges he has to surmount daily while still being one of the nicest, most approachable people in our team. reply rob74 1 hour agoparentYes... you can lead a normal life with diabetes (you can even be a top level tennis player - https://en.wikipedia.org/wiki/Alexander_Zverev#Personal_life), but this article really made me appreciate the level of self-discipline it takes to do that. reply froh 1 hour agoparentprevalso incidentally, pentesting as in testing diabetes pens may partially be done with pentesting as in testing with pen and paper. is it, though? and then there is penetration testing as in trying to break into things. is that what you do? sigh reply mattboyle 6 hours agoparentprevThanks so much for the kind words :) reply begueradj 2 hours agoparentprevYour employer must be good then. Lots of companies don't hire and fire sick employees. Actually it's generally advised not to share one's medeical information at workplace. I even read recently about the case of a worker who wasn't promoted despite everyone else expected him to be promoted. His sin ? He told briefly one of his colleagues that his car got stuck and he was tired of fixing it every now and then. The promotion involves commuting for a certain distance. His manager told him he would have promoted him but he heard his car was not fit. reply rob74 58 minutes agorootparentIn Germany, depending on the type and seriousness of your diabetes, you can get officially recognized as handicapped, and companies above a certain size (20 employees) are obligated to have a certain number of handicapped employees - so an employee that is handicapped, but can work (more or less) the same as a normal employee is actually a win-win situation. reply dgellow 2 hours agorootparentprevDifficult to hide your diabetes when you wear a sensor on your arm and have to inject insulin throughout the day reply yurishimo 10 minutes agorootparentActually, the sensor is super easy to hide (especially with long sleeves) and injecting is also quite discreet these days. I've seen injectors that are about the size of a marker or a wireless earbuds case. It's easy enough to go to the bathroom right before lunch or when you get an notification on your phone. reply chewxy 10 hours agoprevI talked to Matt about not owning our own data after GopherConSG where he gave this talk. It was enlightening how complicated the issue is - there's a lot of legal liabilities on the end of the data provider (the company that monitors the glucose) so I can understand why larger corps are a bit hesitant to open up. On the other hand, it seems quite heinous that users don't have access to data that is rightfully theirs that they can action on it. reply yndoendo 8 hours agoparentI never understood this. Would this be like buying a BIC pen and they have a required license agreement where any entropy made with the pen is owned not by the creator of the data but by the creator of the tool used? With out the person the data would not exists. reply mattboyle 7 hours agoparentprevHey Chewy! Good to see you here :) reply zy0n911 9 hours agoprevIncidentally, I created something related to this yesterday. A tmux plugin to display your glucose data as a status icon in your terminal. As a T1 myself, I use a Dexcom as my CGM and have it working around that specifically, but I'd love to open it up to other devices too, and offer more. Feel free to check it out! https://github.com/Cian911/tmux-xdrip reply wferrell 9 hours agoparentWow. This is really impressive. thanks for sharing. reply Sxubas 8 hours agoprevMildly related, I was diagnosed with obstructive sleep (OSA) apnea 6 months ago, and when sleeping I have to use a machine that blows air with a set pressure so it 'counters' the obstruction (a CPAP machine). Not mission critical like type 1 diabetes, but those machines log a lot more than I expected. While reading this I wondered if someone has geeked something for similar for OSA, maybe for monitoring purposes or more severe cases. I already know (and am incredible grateful for) OSCAR[1], but I'm really curious to see what else is out there. 1: https://www.sleepfiles.com/OSCAR/ reply cpuguy83 4 hours agoparenthttps://home.sleephq.com/ does a good job of processing this data if they support your machine. reply TheCapeGreek 2 hours agorootparentInteresting, first time I've had a website break on Firefox like this. The CSS & JS are being blocked for tracking on Unbounce. reply tibbydudeza 7 hours agoparentprevRather mission critical I reckon. A CPAP machine was a life changer for me - long term untreated sleep apnea causes heart and other life issues. During my sleep study I logged about at the worst 48 incidents per minute of stopping to breath (kind of difficult to do that when you tongue rolls back into your airway) reply sbstp 5 hours agorootparentMy sleep study revealed 90.7 events per hour (30 is considered severe) and 55% blood oxygen level. I've been using a CPAP machine for about 2 months now, but still feeling pretty tired in general. Still waiting for the life changing effects. reply hnlmorg 1 hour agorootparentI was around 79 events and my CPAP was also life changing. But they’re not easy to get right. First you need a mask that fits the profile of your face and the severity of your condition. There are a lot of different shapes and designs. Having tried a few, some were completely useless for me. Only one actually worked. Facial hair also affects the performance of the mask too. I don’t know if this is something you need to be concerned about or not, but if you have a beard and wear a full mouth and nose mask, then you might want to consider a change of appearance. Then you have the configuration of the CPAP machine itself: + how much pressure to apply? + what, if any, ramp up period to use before deep sleep pressure is applied? Those two metrics took a bit of tuning before I found something I was comfortable with. I was lucky that I had (and still have) an excellent doctor to guide me through all of this. Free from the UK NHS too. I can’t imagine having to navigate all of these caveats on my own. Good luck with the CPAP machine. Not getting enough quality sleep can be debilitating. I’ve been in the same situation as you so understand how horrible life was before my CPAP machine. reply cpuguy83 4 hours agorootparentprevHave you analyzed the data from your cpap to see if it's doing what it needs to do? It has the data for your apnea events, sleep time, all kinds of interesting things. Check for leaks. Make sure your mask is actually comfortable, I'd recommend trying multiple if you haven't already. reply manmal 4 hours agorootparentprevMaybe you need a BIPAP. reply secabeen 10 hours agoprevThis is interesting, but is there a reason that you didn't explore the open source software in this space (Nightscout, xDrip, etc.)? These aren't new problems, and lots of people have solved them already with much more feature-full solutions that just work. reply mattboyle 7 hours agoparentI did explore them; for what I wanted to do I could build it faster from scratch and it was a fun learning experience reply broswell 7 hours agoprevDiabetes Alert Dogs - Not technology based I have recently be learning about tremendous success with well trained dogs alerting their owners as a complement to the technology based monitoring and alerting systems. reply je42 3 hours agoprevNice post ! I use a another set of tools to manage my type 1 and the one of my son. Monitoring glucose: Sensor libre 2 and xdrip mobile app using the bluetooth option where xdrip + oop2 reads values in 1min intervals. ( no miaomiao needed ) Values are pushed to nightscout I got 2 nightscout instances: one for my self one for my son deployed to heroku. Alarms are pushed to my phone for my sons low and high bloodsugar using pushover. Heroku alarms are configured if service goes down. Database: mongo atlas highly available. We both use omnipod dash with android aps app. A) helps with carb to insulin calc and carb counting, insuline on board. B) closed loop, that auto corrects high glucose and stops insulin when too low C) different profiles for things like \"recently done sports\" vs \"didnt do much excerise in the last days\" D) aps app allows to track extra insuline injected by pen reply keithnz 8 hours agoprevScott Hanselman has blogged and youtubed about having type 1 diabetes over the years with various tech hacks. A while back he hacked it so his blood sugar showed up in his terminal prompt... https://www.youtube.com/watch?v=_meKUIm9NwA reply captn3m0 5 hours agoprevA good alternative to writing your own echo server and debugging requests one route at a time is requestbin, which will gladly take any requests you throw at it, log them, and optionally return a response of your choice. Lots of different implementations and hosts: https://duckduckgo.com/?q=requestbin reply mb7733 7 hours agoprevGreat article! While I haven't done anything as involved I have found using technology to track health issues to be empowering myself. I'm curious about your thoughts on insulin pumps. I'm not very familiar but I know a few people with T1 that use them and seem happy with them. My understanding is that the idea is that they both monitor glucose levels and supply insulin to keep things stable. Have you considered them? Is there a limitation/downside to them that isn't obvious to a non diabetic? Thanks! reply mattboyle 7 hours agoparentPumps work alongside something like a libre and you’re correct they can deliver insulin. Unfortunatley I’m not eligible for an insulin pump on the NHS so for now I’m sticking with injections. I do a good job of managing them this way so it works for me for now. reply lol768 10 hours agoprevI went to Open Data Camp in the UK in 2023 and there was an interesting talk from two attendees with Type 1 diabetes about how locked down a lot of these devices are from a data perspective, which seems like a real shame - particularly when it's your own health data they're collecting! CGM devices seems like the gold standard in terms of what they're natively capable of (and you don't need something separate on top of a flash sensor) and I think they're free on the NHS. Lots of them have companion smartphone apps, so you'd assume that it's possible to reverse engineer whatever API that these apps are using in order to get at the raw data. Dexcom seemingly does have an API, but it only works if you're located in the US. reply secabeen 10 hours agoparentFrom what I've gathered, the community has largely given up on trying to get at the raw data through Libre and Dexcom. That's not going to work in the long run, as Dexcom or Libre could cut it off at any time. The smarter solution is just to reverse-engineer the software component of these systems and take Dexcom/Libre out of the equation, or (even better) run a parallel system that does the data exporting you want. There are a number of open source solutions in this area. No guarantee that your physician will accept these, but they are pretty interesting, and very feature rich. reply croemer 8 hours agorootparentT1D here. Luckily not true, exporting works well at least for Dexcom on Android with https://www.patreon.com/byod It's a patched app that broadcasts values it receives to any app that's listening, e.g. xdrip. Libre 2 has also got patches floating around on the internet. reply selimthegrim 8 hours agorootparentprevI worked for a company that had licenses for Libre, Insulet and Dexcom and we had their serial APIs - but you had to plug it in to your computer, store the data at our colo and use a Java applet to view it in the browser. In 2014. Unsurprisingly mobile apps have had more uptake since. reply bluesounddirect 9 hours agoprevSorry 1/2 into the article all that kept popping into my head is someone tell this man about a cgm like a dexcom . It all of this out of the box . My wife has been a t1d for 30 years the dexcom cgm makes all of this a lot better reply mattboyle 7 hours agoparentI wrote the article- I’m very aware of Dexcom and it doesn’t have all this out the box reply user_7832 6 hours agorootparentHave you looked into third party apps? I’m very personally used the app diabox (on android), I’ve heard suggah (if I spelt it correct) also supports a nightscout link like diabox. However I’m not sure how these work with dexcoms. reply ShakataGaNai 9 hours agoparentprevHe has that. The problem is not the CGM, it's everything around that (and the fact that both Freestyle Libre, and Dexcom, don't readily expose the data for the DIY types to do anything with). While the Dexcom is \"better\" (than Freestyle Libre) in my opinion because the recent revision can talk directly to your phone continuously (the Libre only sends alerts, getting data requires you tapping phone to arm)... your insurance may cover only one or other. Either is way way better than nothing, but both are still a long way from automation. reply natdempk 8 hours agorootparentNot sure exactly what you mean by \"getting data\" but recent versions of both Libre and Dexcom have apps where you can view graphs of data on your phone with no tapping step. reply muwtyhg 8 hours agorootparentIt seems like you and the GP didn't read even the first 20% of the article. The author shows their CGM in the first 2 page lengths. They mentioned they did all of this because the official Libre app was constantly losing connection with their CGM. He bought a \"hat\" device called a \"Miao Miao\" and then siphoned off the data to his own CGM service that won't cut out if it receives, as the author puts it, \"anomalous readings\" It has a side benefit of allowing them to show it on their Apple Watch using Google Calendar only. reply natdempk 7 hours agorootparentActually I did read the whole thing before posting... There are different CGM models where some require scanning and some report continuously. My reading was that the author's model seemed to only support scanning thus the need for the hat, rather than periodic updates to an app that newer models provide, but it doesn't seem 100% definitive from the article as I don't know if on newer models data export might require scanning? The point of my comment was that rather than all this work, getting a different device could more-or-less enable the desired functionality without so many workarounds and the bulky hat device by eliminating the whole scanning/inconsistency aspect. Though if you want to pull the raw datapoints etc. and export them elsewhere, then there are still more complications. The non-scanning ones might still be a bit unreliable as well, not sure where that aspect is coming from. reply croemer 8 hours agorootparentprevNHS likely doesn't pay for Dexcom. Someone has figured out how to make the Dexcom app broadcast values it's received, it works great: https://www.patreon.com/byod reply jstanley 3 hours agorootparentprev\"Getting data\" would mean automated exports into a format that you can actually do some processing on. Not just viewing graphs on a screen. reply martsa1 9 hours agoprevNice write up! I stumbled across the Open Pancreas project a while back, may be of interest to techies with T1: https://openaps.org/ reply brandonp0 7 hours agoparentThere's also Loop on the iOS side: https://github.com/LoopKit/Loop. I've been using it for 7 or 8 years. There's also several pumps that have closed loop capabilities built-in, but I've found that they try so hard to play it safe that they are less capable than running your own. reply Benjamin_Dobell 7 hours agorootparentAgreed. My daughter (5) is using a G6 + TSlim X2 closed loop. The target blood glucose is 6.0 mmol/L and you can't decrease it. However, I haven't tried (and likely won't try) anything DIY since it's my daughter and not myself. Despite this, it is nice whilst my daughter is at school, since some automated corrections (even if overly conservative) are certainly better than none — teachers generally won't administer corrections (but will bolus for food). reply user_7832 6 hours agorootparentAs someone who’s planning to start a TSlim, this is a bit worrying. Admittedly even an average value of 7mmol would be an improvement over where I am, but I’ve certainly heard the lack of aggressiveness as an issue with the controlIQ system (if that’s what your daughter is using). reply Benjamin_Dobell 6 hours agorootparentYeah, ControlIQ. I do imagine it's different for everyone though, and would also depend on what your alternatives are. In our circumstances, it's a huge improvement over manual injections. But by no means the closed loop silver bullet we're all hoping for. We ran the TSlim on BasalIQ (precursor to ControlIQ) for a couple of years. Haven't done her HbA1C since we swapped, so I don't have any hard data. Well, and with the constant changing lifestyle of kids, all data needs to be taken with a grain of salt. I think we're being woken by high alarms less often, but she just seems to be fairly high (just under 10 mmol/L) much more often. There are of course still run away highs much worse than that (don't want to misrepresent the situation!) and I guess they're somewhat mitigated by ControlIQ, but it definitely doesn't stop them outright, and manual corrections are still required if you want the high resolved in under 2 hours. reply mholt 9 hours agoprevA family friend of ours just had a child -- 6 years old -- diagnosed with diabetes, and has to wear one of these monitors, with that same app and everything. Neat idea. reply tonymet 9 hours agoprevI love this post. there's a lot of potential for a personal monitoring platform like this. great work and keep staying alive reply mattboyle 7 hours agoparentThanks so much for the kind words! reply princevegeta89 9 hours agoprevAfter messing around with Elixir, I decided on my own that the Elixir ship wasn't worth it for me and I moved on to typescript. Now I am debating between Rust and Go for backend development (for a web service). Which one between these two would be recommended for my needs? My priorities are to be able to learn quickly and develop/iterate as fast as possible. reply aleksiy123 9 hours agoparentIf your goal is iteration and development and not performance, why not just stick with a typescript backend? reply princevegeta89 5 hours agorootparentJust wanted to use something new this time since I feel like I'm bored with TS now. reply blandflakes 9 hours agoparentprevI'd love to hear why Elixir wasn't for you - not from a judgemental place, I just like hearing others' experience reports for languages that stuck or didn't. For a web service with fast iteration times, I'd probably pick Go of those two. While I'm not personally a giant fan, I think it is a much simpler stack that lets developers be pretty productive especially in the domain of web services. reply princevegeta89 5 hours agorootparentIteration was easier but the main complaint from my team was it was hard for junior engineers to pick up and understand. Along with that, IDE support still seems poor (we use Jetbrains and the Elixir plugin doesn't work reliably for us) reply srameshc 9 hours agoparentprevGo is \"amazing\" . I am using Go since the very early days, and I absolutely love it. I have built services using Go and manage it all by myself. The community is so awesome and there is likeyly an opensource project that supports your need when you want it to do something exceptional. reply not-my-account 9 hours agoparentprevGo is probably your choice, it was stupid fast to pick up. It’s a fun language too reply princevegeta89 9 hours agorootparentIs there a good batteries-included framework for Go, like Ruby on Rails or Django? reply machekb 2 hours agorootparentThe OP is using Encore (https://github.com/encoredev/encore) for the project described, it's pretty much what you are asking for. reply ralfhn 9 hours agorootparentprevThe Go community is generally averse to frameworks. reply princevegeta89 9 hours agorootparentLooks like it, however, wouldn't that decrease the speed of development for new people that come onboard? reply guessmyname 9 hours agorootparent> […] wouldn't that decrease the speed of development for new people that come onboard? If that was the case, wouldn’t people who love Go have created one after more than a decade in existence? The reason why Go developers don’t like “frameworks” a-là Ruby on Rails in the Ruby ecosystem is because the Go community generally prefers libraries over frameworks because Go’s simplicity and flexibility allow developers to compose their solutions using small, composable packages rather than being constrained by a rigid framework. This approach often results in more efficient and maintainable code. The Go philosophy emphasizes minimalism and encourages developers to avoid unnecessary abstractions. reply kreek 9 hours agorootparentprevWhile not quite as batteries-included as Rails, Fiber made the most sense to my Ruby/Rack brain. reply princevegeta89 5 hours agorootparentThank you! reply foldr 9 hours agorootparentprevThere's a good list of frameworks here: https://github.com/mingrammer/go-web-framework-stars?tab=rea... But you won't find anything that's anywhere near as comprehensive and batteries-included as Rails or Django. reply petesergeant 9 hours agoparentprevUnless your goal is learning the new language for the sake of it, Node should already be plenty performant for most needs reply princevegeta89 5 hours agorootparentYes, I am really looking at using something new since I did get bored with the JS/TS world. reply hk1337 9 hours agoprevI could be mistaken but I believe both types of diabetes has no cure; however type 2 is MUCH MORE manageable with a proper diet? reply croemer 8 hours agoparentType 1 and 2 are quite different beasts - they both have in common that you pee sugar when it's not managed (diabetes means sweet urine in Greek). Type 1 is when your immune system erroneously kills the cells producing insulin in your pancreas. Treatable by injecting insulin manually - but as any substitute, there are caveats, it's not perfect. Type 1 is pretty binary, you either have it or you don't, it appears rather rapidly, usually at young age, up to around 20. There's no way to avoid injections through diet. Type 2 is when the cells that are _reacting_ to insulin are no longer doing so very well. Lots of reasons: genetics, body weight, physical activity. It's much less binary, with gradual onset over years. Can initially be managed by change of diet, activity, pills - later it also requires injections like type 1. reply renatolb 8 hours agoparentprevThere's no cure, but 'remission' is possible in type 2 diabetes with lifestyle changes. Remission is defined as maintaining normal blood sugar levels without medication. This is not possible in type 1 diabetes reply Projectiboga 5 hours agorootparentType 1 here, there is a simple hack for both 1 & 2 and it's take melatonin at night. That upregulates the actual insulin receptors to do more with less insulin. I've been using that for 28 years now. My insulin requirements dropped 40% over my first 6 months and have stayed there ever since. I know two Type 2 who went into remission with diet, melatonin and who tapered off pills after initial use. reply tibbydudeza 7 hours agoparentprevType 2 is called Insulin Resistance - basically your pancreas works fine but the cells in your body stops responding to the hormone that tells it to remove glucose from your blood leading to a dangerous build up. I use a drug called Metformin daily which slows down my digestive tract (you feel fuller after a meal longer like Ozempic), lowers the uptake of glucose in your gut and lowers glucose production in your liver. That and a low carb diet is essential. reply glp1guide 4 hours agorootparentAlso note that drugs like Ozempic (GLP1 Receptor Agonists) work in at least the following ways: - slow down stomach emptying (which was noted) - improving glucose disposal - reduce appetite by affecting brain chemistry GLP1 RAs are really really interesting compounds (I'm a bit biased of course) reply AnonHP 6 hours agorootparentprev> That and a low carb diet is essential. Not essential. There are people who manage it or even get into remission with high carb diets (high carb low fat) too. One common thread across all these diets is avoiding processed foods. reply giantg2 9 hours agoprevI suspect that type 1 has an environmental cause/trigger in many cases. As an anecdote, I knew many kids from military families or live in close proximity to specific bases where some chemical contamination may exist who have endocrine system issues without any family history of them. I knew very few from other areas and backgrounds with similar issues. Edit: I'm curious if the people disagreeing with my musings have some studies investigating the topic, or some other interesting point. reply giantg2 7 hours agoparentWow, ok, disagree with me even more without contributing to the conversation. I'll leave additional reading for you to disagree with this comment as well. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8518604/ reply user_7832 6 hours agoparentprevI don’t know why you’re being downvoted, environmental triggers eg from pollution are linked to autoimmune conditions. Any kind of excessive (oxidative) stress would increase the chances of it for that matter. reply sjwhitworth 6 hours agoprevincident.io CEO here! This is such a cool and unique usecase. Love it. reply manmal 4 hours agoparentI’ve seen your terms don’t exclude liability for bodily harm - IANAL, but wouldn’t this be a possible issue? reply grahar64 4 hours agoprevMy son was diagnosed in 2022 with T1D, I wrote this post about it mostly out of determination to understand how to manage https://maori.geek.nz/the-unreasonable-math-of-type-1-diabet... He is now on Dexcom+Omnipod+AAPS and doing well. Being familiar with PagerDuty and Datadog/ELK stack, all I want is a similar set of services that can scale (cheaply) to manage all people with T1D. Nightscout is a start but won't scale. Awesome work and best of luck, T1D is shite. reply mattboyle 4 minutes agoparentThanks Graham. We actually spoke a little on Twitter after you posted that article here a year or so a go. I’m really happy to hear your son is doing well reply jokethrowaway 2 hours agoprevThat's an amazing blogpost for techies! Well written! On the diet side, I'm living with almost no carbs for 2 years and over time my ability to turn fat into energy changed significantly, to the point I can do any activity I was doing before as long as I keep in check my sodium and potassium intake. You can find people doing well with Type 1 and carnivore and you can find people telling you your insulin resistance will worsen. I'd experiment regardless of what you find on the internet or the doctor tells you. I spent two years of hell (not diabetes though, frequent diarrhea, stomach ulcers, blood in stool) following doctors indications and ended up with non sensical prescriptions and a low fat diet as a recommendation, when a high fat diet fixed all the symptoms in a week. reply __loam 7 hours agoprevThe worst thing about ai art is having it inflicted on me at the beginning of every medium article now. reply smallpipe 1 hour agoparentYes! It's a great article, but why does anyone want a picture where the sun somehow manages to melt the glazing bar to illustrate an article about diabetes ? Or a big \"Raise incident\" button with all the other buttons being meaningless rubbish ? reply wiseowise 3 hours agoparentprevIt looks ugly and out of place. reply __loam 2 hours agorootparentThe article is very good and involves several things I'm passionate about including go and medical devices, I just wish software engineers realized how bad the Dalle3 stuff looks. reply lemper 3 hours agoprev [–] bro, life saving stuff you've created there. also, for the \"i need help\" part, i'd try my damnedest so things won't go south like that (500 error). how about put it into a loop with backoff or something? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post explores the difficulties of handling Type 1 diabetes, emphasizing the constant monitoring needed, drawing from the author's personal journey.",
      "It delves into technological progress in diabetes management, drawing parallels to workplace incident management, utilizing open-source solutions for tracking glucose and incidents.",
      "The author shares inventive methods for collecting glucose data and leveraging incident.io for incident workflows, showcasing how coding aids in managing their health, advocating for others to create projects for better self-care."
    ],
    "commentSummary": [
      "Participants discuss managing diabetes through technology such as microinfusors and NightScout, sharing experiences with automated blood sugar management systems.",
      "The conversation includes challenges in the workplace, legal issues on data ownership, and the use of CPAP machines in diabetes management.",
      "Topics cover monitoring devices, software development, and personal treatment methods for type 1 diabetes, along with comparisons of programming languages and frameworks in Go, as well as insights into causes and management of type 2 diabetes."
    ],
    "points": 426,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1709592124
  },
  {
    "id": 39589924,
    "title": "Apple Unveils New MacBook Air with M3 Chip",
    "originLink": "https://www.apple.com/newsroom/2024/03/apple-unveils-the-new-13-and-15-inch-macbook-air-with-the-powerful-m3-chip/",
    "originBody": "Apple Newsroom needs your permission to enable desktop notifications when new articles are published PRESS RELEASE March 4, 2024 Apple unveils the new 13- and 15‑inch MacBook Air with the powerful M3 chip The world’s most popular laptop is better than ever with even more performance, faster Wi-Fi, and support for up to two external displays — all in its strikingly thin and light design with up to 18 hours of battery life The new 13- and 15-inch MacBook Air soars with the powerful M3 chip, featuring a super-portable design, power-efficient performance, and all-day battery life. CUPERTINO, CALIFORNIA Apple today announced the new MacBook Air with the powerful M3 chip, taking its incredible combination of power-efficient performance and portability to a new level. With M3, MacBook Air is up to 60 percent faster than the model with the M1 chip and up to 13x faster than the fastest Intel-based MacBook Air.1 And with a faster and more efficient Neural Engine in M3, MacBook Air continues to be the world’s best consumer laptop for AI. The 13- and 15-inch MacBook Air both feature a strikingly thin and light design, up to 18 hours of battery life,1 a stunning Liquid Retina display, and new capabilities, including support for up to two external displays and up to 2x faster Wi-Fi than the previous generation. With its durable aluminum unibody enclosure that’s built to last, the new MacBook Air is available in four gorgeous colors: midnight, which features a breakthrough anodization seal to reduce fingerprints; starlight; space gray; and silver. Combined with its world-class camera, mics, and speakers; MagSafe charging; its silent, fanless design; and macOS, MacBook Air delivers an unrivaled experience — making the 13-inch model the world’s bestselling laptop and the 15-inch model the world’s bestselling 15-inch laptop. Customers can order starting today, with availability beginning Friday, March 8. “MacBook Air is our most popular and loved Mac, with more customers choosing it over any other laptop. And today it gets even better with the M3 chip and new capabilities,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “From college students pursuing their degrees, to business users who need powerful productivity, or anyone who simply wants the unmatched combination of performance, portability, and industry-leading battery life, all in a fanless design, the new MacBook Air continues to be the world’s best thin and light laptop.” The new MacBook Air with the M3 chip is strikingly thin and fast, so users can work, play, or create anywhere. Blazing-Fast Performance with M3 Built using industry-leading 3-nanometer technology, the M3 chip brings even faster performance and more capabilities to MacBook Air. Featuring a powerful 8-core CPU, up to a 10-core GPU, and support for up to 24GB of unified memory, the new MacBook Air is up to 60 percent faster than the model with M1 and up to 13x faster than the fastest Intel-based MacBook Air.1 It also features up to 18 hours of battery life, which is up to six hours longer than an Intel-based MacBook Air.1 Users will feel the blazing speed of M3 in everything they do, from everyday productivity, to demanding tasks like photo and video editing, and software development. And with the next-generation GPU of M3, the new MacBook Air supports hardware-accelerated mesh shading and ray tracing, offering more accurate lighting, reflections, and shadows for extremely realistic gaming experiences. It also includes the latest media engine with support for AV1 decode, providing more efficient and higher-quality video experiences from streaming services. The power-efficient M3 chip brings more speed to everything users do, including working on demanding tasks like photo and video editing. M3 takes MacBook Air performance even further: Game titles like No Man’s Sky run up to 60 percent faster than the 13-inch MacBook Air with the M1 chip.1 Enhancing an image with AI using Photomator’s Super Resolution feature is up to 40 percent faster than the 13-inch model with the M1 chip, and up to 15x faster for customers who haven’t upgraded to a Mac with Apple silicon.1 Working in Excel spreadsheets is up to 35 percent faster than the 13-inch model with the M1 chip, and up to 3x faster for customers who haven’t upgraded to a Mac with Apple silicon.1 Video editing in Final Cut Pro is up to 60 percent faster than the 13-inch model with the M1 chip, and up to 13x faster for customers who haven’t upgraded to a Mac with Apple silicon.1 Compared to a PC laptop with an Intel Core i7 processor, MacBook Air delivers up to 2x faster performance, up to 50 percent faster web browsing, and up to 40 percent longer battery life.1 For everyone from students to business users, MacBook Air with M3 takes productivity to new heights with incredible performance and battery life, making tasks like collaborating on Excel even faster. World’s Best Consumer Laptop for AI With the transition to Apple silicon, every Mac is a great platform for AI. M3 includes a faster and more efficient 16-core Neural Engine, along with accelerators in the CPU and GPU to boost on-device machine learning, making MacBook Air the world’s best consumer laptop for AI. Leveraging this incredible AI performance, macOS delivers intelligent features that enhance productivity and creativity, so users can enable powerful camera features, real-time speech to text, translation, text predictions, visual understanding, accessibility features, and much more. With a broad ecosystem of apps that deliver advanced AI features, users can do everything from checking their homework with AI Math Assistance in Goodnotes 6, to automatically enhancing photos in Pixelmator Pro, to removing background noise from a video using CapCut. Combined with the unified memory architecture of Apple silicon, MacBook Air can also run optimized AI models, including large language models (LLMs) and diffusion models for image generation locally with great performance. In addition to on-device performance, MacBook Air supports cloud-based solutions, enabling users to run powerful productivity and creative apps that tap into the power of AI, such as Microsoft Copilot for Microsoft 365, Canva, and Adobe Firefly. World’s Most Popular Laptop More people choose MacBook Air over any other laptop, and M3 raises the bar yet again with its phenomenal combination of performance, portability, and capabilities users love: Two perfect sizes in a super-portable design: With a durable aluminum enclosure that’s built to last, the 13- and 15‑inch MacBook Air have fantastic battery life, are incredibly light, and are less than half an inch thin, so users can work, play, or create from anywhere. The 13-inch model provides the ultimate in portability, while the 15-inch model offers even more screen real estate for multitasking. There’s a perfect size for everyone, from students on the go to business professionals who prefer a larger screen. Gorgeous Liquid Retina display: MacBook Air features a brilliant 13.6- or 15.3-inch Liquid Retina display with up to 500 nits of brightness, support for 1 billion colors, and up to 2x the resolution of comparable PC laptops. Content looks vivid with sharp detail, and text appears super crisp. Support for up to two external displays: MacBook Air with M3 now supports up to two external displays when the laptop lid is closed — perfect for business users, or anyone who requires multiple displays for multitasking across apps or spreading out documents at the same time. Versatile connectivity: MacBook Air with M3 features Wi-Fi 6E, which delivers download speeds that are up to twice as fast as the previous generation. It also includes MagSafe charging and two Thunderbolt ports for connecting accessories, along with a 3.5mm headphone jack. Camera, mics, and speakers: With a 1080p FaceTime HD camera, users will look their best whether they’re connecting with friends and family, or collaborating with coworkers around the world. Users will also sound their best with a three-mic array and enhanced voice clarity on audio and video calls. MacBook Air features an immersive sound system with support for Spatial Audio along with Dolby Atmos, so users can enjoy three-dimensional soundstages for music and movies. Magic Keyboard and Touch ID: The comfortable and quiet backlit Magic Keyboard comes with a full-height function row with Touch ID, giving users a fast, easy, and secure way to unlock their Mac; sign in to apps and websites; and make purchases with Apple Pay — all with the touch of a finger. Colorful graphics are shown on two new MacBook Air devices. A person wearing a bright red fuzzy coat is shown on the new MacBook Air. A person does some work on the new MacBook Air with two external displays. A close-up on the keyboard of the new MacBook Air, shown in midnight. Available in two perfect sizes, the 13-inch MacBook Air delivers the ultimate in portability, while the 15-inch model offers even more screen real estate for multitasking in a thin and light design. MacBook Air with M3 features a stunning Liquid Retina display, making photos and movies look incredibly vibrant. With support for up to two external displays when the laptop lid is closed, it’s easy to multitask across apps and spread out across two screens. MacBook Air comes in four great colors, including midnight, which features a breakthrough anodization seal to reduce fingerprints. previous next The Magic of macOS Together with macOS, the MacBook Air experience is unrivaled: macOS Sonoma: Users can now place widgets right on the desktop, interact with them with just a click, and even access the extensive ecosystem of iPhone widgets on MacBook Air. Video conferencing gets more engaging with great features like Presenter Overlay and Reactions. Profiles in Safari keep browsing separate between multiple topics or projects, while web apps provide faster access to favorite websites. And gaming gets even better with Game Mode. Enhanced productivity: All users, including business professionals, can take advantage of the expansive display on MacBook Air with Split View, or spread out across screens with support for up to two external displays. Features like Stage Manager also help users like students focus on the task in front of them. Better with iPhone: With Continuity, MacBook Air works seamlessly across iPhone and other Apple devices. Features like AirDrop allow users to share and receive photos, documents, and more across nearby Apple devices. Universal Clipboard lets users easily copy images, video, or text from an app on one Apple device, and effortlessly paste them into another app on a nearby Mac. Continuity Camera makes it easy for users to scan or take a picture of something nearby with their iPhone and have it appear instantly on their Mac. And Handoff lets them start a task like answering an email on one Apple device and easily finish it on another. Wide array of apps: MacBook Air comes with powerful apps built in, including FaceTime, Freeform, iMovie, GarageBand, and Photos, as well as productivity apps including Pages, Numbers, and Keynote, making it easy for users to create amazing work. And with thousands of apps optimized for Apple silicon, all of users’ go-to apps run incredibly fast in macOS — including Microsoft 365 and many of their favorite iOS apps. A person wearing headphones plays a game on the new MacBook Air. The Continuity feature is shown across MacBook Air and iPhone 15 Pro, with a user’s PowerPoint project appearing on both screens. Games get even better on MacBook Air with M3, delivering incredible performance, breathtaking graphics, and features like Game Mode in macOS Sonoma. Users can work seamlessly across Mac and iPhone with Continuity features like AirDrop, Universal Clipboard, Continuity Camera, and Handoff. previous next Better for the Environment The new MacBook Air is the first Apple product to be made with 50 percent recycled content, including 100 percent recycled aluminum in the enclosure, 100 percent recycled rare earth elements in all magnets and, in another first for Apple, 100 percent recycled copper in the main logic board. MacBook Air meets Apple’s high standards for energy efficiency, and is free of mercury, brominated flame retardants, and PVC. The packaging is 99 percent fiber-based, bringing Apple closer to its goal to remove plastic from all packaging by 2025. Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be carbon neutral across the entire manufacturing supply chain and the life cycle of every product. Pricing and Availability Customers can order the new MacBook Air with M3 starting Monday, March 4, on apple.com/store and in the Apple Store app in 28 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, starting Friday, March 8. The 13-inch MacBook Air with M3 starts at $1,099 (U.S.) and $999 (U.S.) for education, and the 15‑inch MacBook Air with M3 starts at $1,299 (U.S.) and $1,199 (U.S.) for education. Both are available in midnight, starlight, silver, and space gray. The 13-inch MacBook Air with M2, available in midnight, starlight, silver, and space gray, now starts at $999 (U.S.) and $899 (U.S.) for education. Additional technical specifications, configure-to-order options, and accessories are available at apple.com/mac. With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth. When customers shop at Apple using Apple Card, they can pay monthly at 0 percent APR for their new MacBook Air when they choose to check out with Apple Card Monthly Installments, and they’ll get 3 percent Daily Cash back — all upfront. Every customer who buys a Mac from their Apple Store can enjoy a free Online Personal Session with an Apple Specialist, get their product set up — including help with data transfer — and receive guidance on how to get the most out of their new Mac. Share article Media Text of this article March 4, 2024 PRESS RELEASE Apple unveils the new 13- and 15‑inch MacBook Air with the powerful M3 chip The world’s most popular laptop is better than ever with even more performance, faster Wi-Fi, and support for up to two external displays — all in its strikingly thin and light design with up to 18 hours of battery life CUPERTINO, CALIFORNIA Apple today announced the new MacBook Air with the powerful M3 chip, taking its incredible combination of power-efficient performance and portability to a new level. With M3, MacBook Air is up to 60 percent faster than the model with the M1 chip and up to 13x faster than the fastest Intel-based MacBook Air.1 And with a faster and more efficient Neural Engine in M3, MacBook Air continues to be the world’s best consumer laptop for AI. The 13- and 15-inch MacBook Air both feature a strikingly thin and light design, up to 18 hours of battery life,1 a stunning Liquid Retina display, and new capabilities, including support for up to two external displays and up to 2x faster Wi-Fi than the previous generation. With its durable aluminum unibody enclosure that’s built to last, the new MacBook Air is available in four gorgeous colors: midnight, which features a breakthrough anodization seal to reduce fingerprints; starlight; space gray; and silver. Combined with its world-class camera, mics, and speakers; MagSafe charging; its silent, fanless design; and macOS, MacBook Air delivers an unrivaled experience — making the 13-inch model the world’s bestselling laptop and the 15-inch model the world’s bestselling 15-inch laptop. Customers can order starting today, with availability beginning Friday, March 8. “MacBook Air is our most popular and loved Mac, with more customers choosing it over any other laptop. And today it gets even better with the M3 chip and new capabilities,” said Greg Joswiak, Apple’s senior vice president of Worldwide Marketing. “From college students pursuing their degrees, to business users who need powerful productivity, or anyone who simply wants the unmatched combination of performance, portability, and industry-leading battery life, all in a fanless design, the new MacBook Air continues to be the world’s best thin and light laptop.” Blazing-Fast Performance with M3 Built using industry-leading 3-nanometer technology, the M3 chip brings even faster performance and more capabilities to MacBook Air. Featuring a powerful 8-core CPU, up to a 10-core GPU, and support for up to 24GB of unified memory, the new MacBook Air is up to 60 percent faster than the model with M1 and up to 13x faster than the fastest Intel-based MacBook Air.1 It also features up to 18 hours of battery life, which is up to six hours longer than an Intel-based MacBook Air.1 Users will feel the blazing speed of M3 in everything they do, from everyday productivity, to demanding tasks like photo and video editing, and software development. And with the next-generation GPU of M3, the new MacBook Air supports hardware-accelerated mesh shading and ray tracing, offering more accurate lighting, reflections, and shadows for extremely realistic gaming experiences. It also includes the latest media engine with support for AV1 decode, providing more efficient and higher-quality video experiences from streaming services. M3 takes MacBook Air performance even further: Game titles like No Man’s Sky run up to 60 percent faster than the 13-inch MacBook Air with the M1 chip.1 Enhancing an image with AI using Photomator’s Super Resolution feature is up to 40 percent faster than the 13-inch model with the M1 chip, and up to 15x faster for customers who haven’t upgraded to a Mac with Apple silicon.1 Working in Excel spreadsheets is up to 35 percent faster than the 13-inch model with the M1 chip, and up to 3x faster for customers who haven’t upgraded to a Mac with Apple silicon.1 Video editing in Final Cut Pro is up to 60 percent faster than the 13-inch model with the M1 chip, and up to 13x faster for customers who haven’t upgraded to a Mac with Apple silicon.1 Compared to a PC laptop with an Intel Core i7 processor, MacBook Air delivers up to 2x faster performance, up to 50 percent faster web browsing, and up to 40 percent longer battery life.1 World’s Best Consumer Laptop for AI With the transition to Apple silicon, every Mac is a great platform for AI. M3 includes a faster and more efficient 16-core Neural Engine, along with accelerators in the CPU and GPU to boost on-device machine learning, making MacBook Air the world’s best consumer laptop for AI. Leveraging this incredible AI performance, macOS delivers intelligent features that enhance productivity and creativity, so users can enable powerful camera features, real-time speech to text, translation, text predictions, visual understanding, accessibility features, and much more. With a broad ecosystem of apps that deliver advanced AI features, users can do everything from checking their homework with AI Math Assistance in Goodnotes 6, to automatically enhancing photos in Pixelmator Pro, to removing background noise from a video using CapCut. Combined with the unified memory architecture of Apple silicon, MacBook Air can also run optimized AI models, including large language models (LLMs) and diffusion models for image generation locally with great performance. In addition to on-device performance, MacBook Air supports cloud-based solutions, enabling users to run powerful productivity and creative apps that tap into the power of AI, such as Microsoft Copilot for Microsoft 365, Canva, and Adobe Firefly. World’s Most Popular Laptop More people choose MacBook Air over any other laptop, and M3 raises the bar yet again with its phenomenal combination of performance, portability, and capabilities users love: Two perfect sizes in a super-portable design: With a durable aluminum enclosure that’s built to last, the 13- and 15‑inch MacBook Air have fantastic battery life, are incredibly light, and are less than half an inch thin, so users can work, play, or create from anywhere. The 13-inch model provides the ultimate in portability, while the 15-inch model offers even more screen real estate for multitasking. There’s a perfect size for everyone, from students on the go to business professionals who prefer a larger screen. Gorgeous Liquid Retina display: MacBook Air features a brilliant 13.6- or 15.3-inch Liquid Retina display with up to 500 nits of brightness, support for 1 billion colors, and up to 2x the resolution of comparable PC laptops. Content looks vivid with sharp detail, and text appears super crisp. Support for up to two external displays: MacBook Air with M3 now supports up to two external displays when the laptop lid is closed — perfect for business users, or anyone who requires multiple displays for multitasking across apps or spreading out documents at the same time. Versatile connectivity: MacBook Air with M3 features Wi-Fi 6E, which delivers download speeds that are up to twice as fast as the previous generation. It also includes MagSafe charging and two Thunderbolt ports for connecting accessories, along with a 3.5mm headphone jack. Camera, mics, and speakers: With a 1080p FaceTime HD camera, users will look their best whether they’re connecting with friends and family, or collaborating with coworkers around the world. Users will also sound their best with a three-mic array and enhanced voice clarity on audio and video calls. MacBook Air features an immersive sound system with support for Spatial Audio along with Dolby Atmos, so users can enjoy three-dimensional soundstages for music and movies. Magic Keyboard and Touch ID: The comfortable and quiet backlit Magic Keyboard comes with a full-height function row with Touch ID, giving users a fast, easy, and secure way to unlock their Mac; sign in to apps and websites; and make purchases with Apple Pay — all with the touch of a finger. The Magic of macOS Together with macOS, the MacBook Air experience is unrivaled: macOS Sonoma: Users can now place widgets right on the desktop, interact with them with just a click, and even access the extensive ecosystem of iPhone widgets on MacBook Air. Video conferencing gets more engaging with great features like Presenter Overlay and Reactions. Profiles in Safari keep browsing separate between multiple topics or projects, while web apps provide faster access to favorite websites. And gaming gets even better with Game Mode. Enhanced productivity: All users, including business professionals, can take advantage of the expansive display on MacBook Air with Split View, or spread out across screens with support for up to two external displays. Features like Stage Manager also help users like students focus on the task in front of them. Better with iPhone: With Continuity, MacBook Air works seamlessly across iPhone and other Apple devices. Features like AirDrop allow users to share and receive photos, documents, and more across nearby Apple devices. Universal Clipboard lets users easily copy images, video, or text from an app on one Apple device, and effortlessly paste them into another app on a nearby Mac. Continuity Camera makes it easy for users to scan or take a picture of something nearby with their iPhone and have it appear instantly on their Mac. And Handoff lets them start a task like answering an email on one Apple device and easily finish it on another. Wide array of apps: MacBook Air comes with powerful apps built in, including FaceTime, Freeform, iMovie, GarageBand, and Photos, as well as productivity apps including Pages, Numbers, and Keynote, making it easy for users to create amazing work. And with thousands of apps optimized for Apple silicon, all of users’ go-to apps run incredibly fast in macOS — including Microsoft 365 and many of their favorite iOS apps. Better for the Environment The new MacBook Air is the first Apple product to be made with 50 percent recycled content, including 100 percent recycled aluminum in the enclosure, 100 percent recycled rare earth elements in all magnets and, in another first for Apple, 100 percent recycled copper in the main logic board. MacBook Air meets Apple’s high standards for energy efficiency, and is free of mercury, brominated flame retardants, and PVC. The packaging is 99 percent fiber-based, bringing Apple closer to its goal to remove plastic from all packaging by 2025. Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be carbon neutral across the entire manufacturing supply chain and the life cycle of every product. Pricing and Availability Customers can order the new MacBook Air with M3 starting Monday, March 4, on apple.com/store and in the Apple Store app in 28 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, starting Friday, March 8. The 13-inch MacBook Air with M3 starts at $1,099 (U.S.) and $999 (U.S.) for education, and the 15‑inch MacBook Air with M3 starts at $1,299 (U.S.) and $1,199 (U.S.) for education. Both are available in midnight, starlight, silver, and space gray. The 13-inch MacBook Air with M2, available in midnight, starlight, silver, and space gray, now starts at $999 (U.S.) and $899 (U.S.) for education. Additional technical specifications, configure-to-order options, and accessories are available at apple.com/mac. With Apple Trade In, customers can trade in their current computer and get credit toward a new Mac. Customers can visit apple.com/shop/trade-in to see what their device is worth. When customers shop at Apple using Apple Card, they can pay monthly at 0 percent APR for their new MacBook Air when they choose to check out with Apple Card Monthly Installments, and they’ll get 3 percent Daily Cash back — all upfront. Every customer who buys a Mac from their Apple Store can enjoy a free Online Personal Session with an Apple Specialist, get their product set up — including help with data transfer — and receive guidance on how to get the most out of their new Mac. About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple’s five software platforms — iOS, iPadOS, macOS, watchOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple’s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it. Testing was conducted by Apple in January 2024. See apple.com/macbook-air for more information. Press Contacts Starlayne Meza Apple starlayne_meza@apple.com Lizette Viviana Du Pond Apple ldupond@apple.com Apple Media Helpline media.help@apple.com Copy text Images in this article Download all images About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, Apple Watch, and Apple TV. Apple’s five software platforms — iOS, iPadOS, macOS, watchOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, and iCloud. Apple’s more than 100,000 employees are dedicated to making the best products on earth, and to leaving the world better than we found it. Testing was conducted by Apple in January 2024. See apple.com/macbook-air for more information. Press Contacts Starlayne Meza Apple starlayne_meza@apple.com Lizette Viviana Du Pond Apple ldupond@apple.com Apple Media Helpline media.help@apple.com Latest News APPLE STATEMENT The App Store, Spotify, and Europe’s thriving digital music market March 4, 2024 QUICK READ Apple expands Self Service Repair for Mac February 29, 2024 UPDATE 2024 MLS season kicks off today exclusively on MLS Season Pass on Apple TV February 21, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39589924",
    "commentBody": "New 13- and 15‑inch MacBook Air with M3 chip (apple.com)374 points by dm 21 hours agohidepastfavorite915 comments numbers 14 hours agoI was recently buying a Dell laptop for my sister and it boggles my mind how most other companies (maybe Framework is an except) still have dozens or even 100s of SKUs for consumer laptops. I didn't enjoy the process of looking through dozens of various lines that Dell has and then other companies like Lenovo and HP earlier in the process, just to find a \"mid-range usable computer with a decent screen\". If you didn't know anything about laptops and wanted to buy your first one, it would be a nightmare to figure out what all those seemingly random numbers mean on most non-Apple laptops. Apple continues to simplify the laptop naming scheme, we're at a point where it's simply: Air OR Pro Small screen OR big screen All other details can be configured in the buying flow but there's not much to think about if you just want a simple laptop. reply spaceman_2020 13 hours agoparentI finally made the switch to Apple after being thoroughly frustrated with Windows laptops. It's not even close to a competition. Macbooks are just so far ahead of everyone else that you can't even compare them. Most Windows laptops have abysmal batteries, to the point that you can barely call them laptops. The trackpads are downright unusable. The keyboards are a hit or a miss. And for some reason, so many companies are still shipping laptops with 1080p screens in 2024. Anything even remotely within Macbook vicinity costs the same as a Macbook anyway. Increasingly feels like most manufacturers have given up on the laptop as an innovation center and are happy to just scrape up the consumers who can't or won't buy Apple. reply anakaine 13 hours agorootparentI'm one of those industry long timers who can and will use just about anything, and has occasionally over the years owned macs of various form factors. For the past decade thanks to corporate work I've been entirely Windows and Linux based. I picked my daughter up an m1 macbook air about a year ago. It was an absolute delight of a machine to use. Light weight, no fans, no hot bits during general usage, long battery life, a screen that didn't upset my eyes, and importantly the OS just got out of the way during general usage. I wound up buying myself an m1 air about 6 months later. My only gripe is that I wish it had more RAM, but even then the unified memory approach has made my expected ram usage vs actual ram usage a bit of an odd thing. It consistently uses less ram than I'd normally anticipate. That said, more ram by default would help fill in those times when I do load it up. reply spaceman_2020 13 hours agorootparentI picked up a M3 Macbook Pro and this thing works like an absolute beast. I currently have Photoshop, Illustrator, VSCode, multiple Chrome windows - each with 20-40 tabs - and this thing is not even sweating. I get 8-10 hours of useful work out of it even on battery. Completely changed the way I work reply skunkworker 8 hours agorootparentI upgraded from a 2015 MBP with 16gb of ram to a M3 Pro with 36gb. It’s a night and day difference, the machine is much more responsive and I mainly use it 30 min a day, or sometimes for a few hours in a row. And charge about once per week, getting about 14hr battery life. reply danpmurray 12 hours agorootparentprevHow much RAM does it have, and have you ever wished you had more? reply internetter 12 hours agorootparentI can answer this. I have a very similar workload to OP. I've found myself with resolve, affinity photo, chrome, vscode, spotify, ect open simultaneously, and have had absolutely zero struggle on my 8gb air. If you become \"enlightened\" you can notice that sometimes when you, say, open your spotify window after a long time elsewhere, the spotify is briefly unresponsive. Not in a way you notice, more in the sense that if you are looking for it, you can see hints it is swapping. The only time I wish I had more is when I got into iOS development and began running VMs on my mac. reply paulryanrogers 10 hours agorootparentIf it's really swapping then won't that reduce the lifetime of the (irreplaceable?) internal storage? reply RussianCow 9 hours agorootparentModern SSDs are resilient enough that most people will never wear them out with anything resembling a normal workload. Unless you're constantly swapping for several hours a day, it just isn't going to matter. reply hackerlight 7 hours agorootparentHow many GB a day would be written with swapping? Modern SSDs have a lifespan of a few hundred to low thousands of TBW. reply dividedbyzero 11 hours agorootparentprevI have an M2 Air with 24gb and it has no problems running Brave with 800-ish tabs, development workloads (a bunch of VSCode projects, several docker containers, lots of iTerm terminals), low-end CAD and 3D printing apps, CaptureOne and a bunch of Electron apps in parallel with room to spare. I've found I can fit more into those 24gb than into the 32gb of the Intel Mac I had before that (however that's possible). reply fdsfdsafdsafds 11 hours agorootparent>800-ish tabs You're never going to read those, before the links rot. reply dividedbyzero 9 hours agorootparentAlmost all of them have already been read and are waiting for me to build something to pull the links out of Brave and download/archive/index them somehow. I've been wanting to do this for a long time, but haven't yet. reply naikrovek 8 hours agorootparentThis may help. https://news.ycombinator.com/item?id=34847611 reply DonHopkins 10 hours agorootparentprevThe browser should remember the date you last rendered a tab, and then if the link rots, it falls back to archive.org on that date. reply weikju 10 hours agorootparent> The browser should remember the date you last rendered a tab, and then if the link rots, it falls back to archive.org on that date. Brave (by default at least) will ask you if you want to load the archive.org version when a page fails to load. Not 100% automatic but almost. reply nyarlathotep_ 8 hours agorootparentprevHow are you managing all these? What are you using to search them? I'm using Vimium for general use and have an Alfred plugin/thing/extension but even that's a bit unwieldy. How many windows is this? reply piva00 12 hours agorootparentprevMy MacBook Pro M3 has 36GiB RAM and does all of the comment above + music producing (dozens of VSTs on some 1-2 dozens tracks) + projection mapping and can run some LLM models locally like the Mistral ones. I've only managed to hear the fan when chatting with a LLM, for anything else it's been an absolutely silent beast. reply bombcar 11 hours agorootparentI have an M1 Max with 32GiB and similarly - never had it complain or noticed it unhappy about anything, or felt I needed more RAM or CPU. It's going to be hard to justify upgrading this thing for awhile. reply nerdbert 10 hours agorootparentprevMine has 16GB and I regularly use some Mistral LLM as well as OpenHermes. The only things that ever seems to get my fan going are transcoding a video or a really long compile. Never once has the fan run during web browsing or any of the many everyday tasks that used to bring my previous one to its knees. reply varispeed 11 hours agorootparentprevI have M2 Pro 16GB from a client. It's comfortable for typical dev work - tons of tabs open, Docker, VS Code etc. Though the swap is about 20GB now and sometimes it lags. Still it beats any Intel or AMD laptop I ever had in terms of performance. This machine is on a whole another level. My own machines are M1 Max 32GB and they fare slightly better. reply TMWNN 10 hours agorootparentprevMy current and previous MacBooks have had 16GB and I've been fine with it, but given local LLMs I think I'm going to have to go to whatever will be the maximum RAM available for the next model. Similarly, I am for the first time going to care about how much RAM is in my next iPhone. My iPhone 13's 4GB is suddenly inadequate. reply spaceman_2020 5 hours agorootparentprevHonestly haven’t felt like it didn’t have enough RAM even once. My daily driver before this was a Ryzen 5700x desktop with 64GB RAM, so I know what a powerful processor + RAM combo feels like. Haven’t even heard the fan even once. reply bredren 12 hours agorootparentprevWhat were the things that held you up from making the switch? reply nine_k 8 hours agorootparentprevGood that it works for you and your daughter! I have to use macbooks at work, and they do bother me. The mirror-finish screen always reflects bright lights into my eyes, be it a window or a ceiling light at a distance. The OS is thankfully a certified Unix, but the GUI, while having a few brilliant features, also has warts like no way to align or snap windows, apps running without a window, with only a menu bar, with a window from a different app showing, etc. This continues for many years, so it's likely a design principle. Of course, Windows is even worse in the GUI department, there's no comparison. So, sadly, a Macbook remains the most sane computer for non-technical people :( reply skydhash 6 hours agorootparent> This continues for many years, so it's likely a design principle. I read that Apple follows a document model, where the application is kinda a background thing a window is supposed to be for a specific task. Not like Windows where the main windows is the hub of interaction. So you use CMD +to switch between these tasks and CMD + TAB for switching between applications. The menu bar is part of the application, but windows can modify it to suit the focused tasks. I've used Rectangle for quick window management, but in time I've come to understand the philosophy so it does not bother me as much. It's more leaned into the desktop analogy than other OS. reply spaceman_2020 5 hours agorootparentprevI agree with your misgivings about window management in MacOS. The way Windows does it is so much more intuitive. For window snapping, there are countless apps that will easy solve your problem. But I get why you would struggle - I still get frustrated at times with the way MacOS works. reply atraac 1 hour agorootparentprev> no way to align or snap windows Just download Rectangle for free. I recently switched from Dell XPS to M3 Pro and that was my gripe to. One install and it's much better, moving windows with ctrl+opt+arrows is also close to Windows and makes it fairly usable. reply bluecalm 12 hours agorootparentprevI think ThinkPads are better than MacBooks. You can get P14 with 32gb of RAM, 1TB SSD, very fast and quiet CPU (AMD U series), decent battery, 2.8k OLED screen and it weights 1.34kg (weight between 13 inch and 15inch new Airs). It also has imo better ports and a track point. The problem is that Windows sucks more and more with every iteration and there is nothing Lenovo or other manufacturers can do about it. Lenovo also keeps shipping hot and loud Intel CPUs which hurt reputation of the ThinkPad line and may confuse new buyers. Still if you know what to choose you will get more for your money with P14 than Macbook air imo. reply eBombzor 6 hours agorootparentYou can technically get \"more technical specs\" for the money but the actual experience of using a TP will never be even close to a MBA. This is coming from someone who's used a T14, T14, a few yogas, and currently use a P14 for work. The last point of TPs using loud and hot Intel CPUs cannot be understated. The P14 throttles so hard when I'm trying to do any work because it's using some sh*t Comet Lake U-series, that I literally breathe a sigh of relief when I can use my desktop computer that doesn't hang up every time I load up IntelliJ. MBs are so efficient for the power profile it runs circles around any x86 mobile CPU when on battery. Obviously I've had 10x better experience with a trusty Ryzen 5600U over that Intel CPU. But still nothing close to a MB. Also the TP trackpads are sand paper garbage. reply BitwiseFool 9 hours agorootparentprevOne of my ThinkPads had a debilitating thermal throttling issue. On occasion it would inexplicably limit the CPU to .39 GHz and the fans would spin at max RPM. Yet, it wasn't hot to the touch and it hadn't been doing anything to build up any significant heat. I tried several things, but to no avail. I simply had to be issued a new ThinkPad. I distinctly remember running a firmware update and the utility had several typos in it: \"Updating fimiware\". Sure it's just a status message on an installer, but I lost a lot of confidence in Lenovo's quality control that day. I have no proof but I'm sure that thermal control code was outsourced. reply eBombzor 6 hours agorootparentYup: https://forums.lenovo.com/t5/ThinkPad-T400-T500-and-newer-T-... I've experienced that myself, and from that point on I told myself I would never buy another TP ever again. The issue confuses me because I've used the low-tier IdeaPads with an AMD H-series CPU and honestly I've had similar battery life but without the unlivable throttling. The IdeaPad is thicker but I prefer it over my work ThinkPad for everything. reply __mharrison__ 10 hours agorootparentprevI have a Thinkpad P1 with 64gb. I love that it has a trackpoint. I hate that it had a fan. I guess not so much that it had the fan but that the fan constantly sounds like it is impersonating a helicopter. Not sure where it wants to take the laptop. Also the battery life is horrible, 1 hour or so. And Windows is sluggish. Otherwise, it is a great machine (I occasionally RDP into it when demoing thing for my Windows clients.) My MacBook on the otherhand lacks a trackpoint (that will never be fixed) but is otherwise snappy and quiet. Sure it had some software/OS issues, but overall it is miles ahead of the Thinkpad. reply ffgjgf1 1 hour agorootparent> a Thinkpad P1 To be fair it’s not exactly fair to be comparing this to a MacBook. Dell XPS, Lenovo X1/Z/? series would be closer equivalents, of course AFAIK while the battery life is much better fans/temperature are still an issue. reply fiddlerwoaroof 9 hours agorootparentprevIt’s possible to buy an external keyboard with a trackpoint: not as convenient as having it builtin, but it’s great if you’re mostly using the laptop at your desk anyway reply __mharrison__ 5 hours agorootparentI have a Unicomp Endurapro. However, my fingers and wrists favor split keyboards for longer periods. reply wiremine 9 hours agorootparentprev> I think ThinkPads are better than MacBooks. You can get P14 with 32gb of RAM... > The problem is that Windows sucks more and more... Not to put words in your mouth, but it sounds like the ThinkPads have been technical specs, but the overall experience is worse due to the software. If so, I might challenge your final comment, which is \"you get more for your money\". Ultimately, I think people want a great experience, not a bunch of specs. reply fragmede 11 hours agorootparentprevFortunately there's Linux which is has a great number of fans on Thinkpads. It's not for everybody, but Thinkpads are great for it. reply alexeiz 3 hours agorootparentprevI got P14s with 64GB of RAM, 1TB SSD and other specs maxed out for $1050 (patiently waited for a sale from Lenovo). Other than the battery life, the P14s is roughly equivalent to a Macbook Pro M2 Pro for $3500. Windows sucks in the default install, but if you know what you're doing you can remove all the junk from it and make Windows almost as efficient as, say, Linux (and way more efficient than macOS). In my opinion, Macbooks are for people who'd rather pay more than take care of and optimize their laptops. I could have paid 3 times as much for a similarly spec'ed Macbook, but then I'd have to put up with a silly notch, not having a right Ctrl key, a keyboard getting shiny after a couple of months and other annoyances. So why even bother with Macbooks? reply atraac 1 hour agorootparent> Windows sucks in the default install, but if you know what you're doing you can remove all the junk from it and make Windows almost as efficient as, say, Linux (and way more efficient than macOS). I can answer this as someone who, throughout last 6 months of his new job, used a Dell XPS 9570 with Windows, then PopOS, then Windows again, and just switched to an M3 Pro a week or two ago. No - you will never get close. That Dell could run fps games like CS:GO or Valorant with 100+ fps, had custom tweaks incl. thermalpads connecting to the chasis, exchanged thermal paste, was undervolted and with a custom fan curve. It still throttled from time to time. Granted - it was 8th gen i7, but it was on paper good enough to handle everything I do. Only on paper. It also choke on my day to day work, which is WebStorm, Docker and Typescript web development. Indexing, autocomplete, builds(even with swc) took a really long time. I switched to PopOS for a while, but overal user experience was even worse to me, with constant issues ranging from monitors behaving weirdly, stuff crashing, requiring weird driver installations, even Docker didn't 'just work', I had to fight it half a day to get it to actually run. Went back to Windows until I got frustrated enough and just bought a 36gb M3 Pro, and I'm never going back. This just works, builds take 1/6th of what they did, I can run full swc build in 100ms, full tsc build takes 10 seconds(down from around 60), nothing ever stutters, nothing slows down, didn't hear fans yet. It does have some annoyances, mostly with window management, new keyboard layout and a ton of shortcuts needed to do basic stuff but once I learned those - it's really nice. reply ffgjgf1 1 hour agorootparentprev> roughly equivalent to a Macbook Pro M2 Pro for $3500. Besides the horrible touchpad, screen (did you really get > 1080p for $1050?) and the plastic body > I could have paid 3 times as much for a similarly spec'ed Macbook I could get a desktop with even better specs for as much. Not exactly a fair comparison of course since different people have different needs (how much is never hearing the dans fans and a proper touchpad worth? Supposedly a lot to some people). > and way more efficient than macOS). Can you explain what do you even mean by that? Do you get better battery life than with an M series macbook after these “optimizations”? reply aae42 8 hours agorootparentprevThey just had a p14s for sale with 64GB ram for 1000 USD.... I just couldn't bring myself to buy a laptop with 4 hours battery life with typical usage... Will probably pick up an M3 Air. reply bluecalm 7 hours agorootparentIntel or AMD one? In my experience you get way more than 4 hours with an AMD one. I had an Intel one before and I agree: hot, loud, eats battery like crazy. reply AtlasBarfed 11 hours agorootparentprevOffer and support desktop Linux? Windows keeps dropping the ball and Linux just stands there looking at it on the ground. reply dotancohen 11 hours agorootparentI think that Canonical is doing an excellent job. It's been at least a decade that you could easily install Kubuntu on most computers and things would just work with no configuration. It's easy to use for the vast majority of use cases and reasonably secure. reply paulryanrogers 10 hours agorootparentIME battery life will suffer significantly. But otherwise I agree. reply rowanG077 9 hours agorootparentprevThe last time I tried the P14 it has insane thermal throttling, as do most windows laptops. Is that no longer the case? reply babypuncher 11 hours agorootparentprevI'll take a good trackpad over the trackpoint any day. I also find something weirdly repulsive about the plastics they use on ThinkPads. A true Macbook alternative shouldn't be using much plastic at all, though. reply dave78 11 hours agorootparentMaybe I'm odd, but I much prefer the Thinkpad's body to the Macbook (I have both). I don't like the coldness of the metal chassis, either on my palms or on my legs if using it away from a desk. Thinkpad plastic does not feel cheap or weak to me, so that's not an issue. I also can't stand the Mac keyboard, especially compared to the Thinkpad. reply paulryanrogers 10 hours agorootparentThose butterfly keyboards on older Macs are terrible: bad UX, impossible to self repair, costly for pro repair. Truly form over function, to a fault. reply itsoktocry 13 hours agorootparentprev>Macbooks are just so far ahead of everyone else that you can't even compare them. They are awesome, but not perfect. Way over-priced storage and RAM upgrades, can't connect multiple monitors unless you pay up, and you're stuck with MacOS. Any one of these could be reason enough for people to look elsewhere. reply bloopernova 13 hours agorootparentLordy does that multiple monitor thing grind my gears. I just want to display on 3 screens. But the base model is the only one that corporate IT will buy. So I have to buy a DisplayLink adapter to do what the Intel macbooks did with zero problem. reply ponector 11 hours agorootparentFew years back I had a MacBook pro 2019 and an old ultra wide LG screen with resolution 25:9 and HDMI only input. Apparently, official Apple's USB to HDMI connector cannot handle screen resolution 2560*1080 at that time. Thing that was possible at 300$ windows laptop cannot be done on 2500$ machine with 60$ connector. reply nine_k 8 hours agorootparentBecause if you agreed to pay $2500 anyway, you can agree to pay for an upgrade! Apple is in the business of selling you hardware, specifically premium hardware. Think car dealership. reply ponector 2 hours agorootparentAnd Dell is not in that business? Or Lenovo? People agree to pay ridiculous prices for memory or apple connectors - why not to charge them maximum? reply firecall 11 hours agorootparentprevI was amazed to see the new Air models support dual external displays! > Apple unveils the new 13- and 15‑inch MacBook Air with the powerful M3 chip The world’s most popular laptop is better than ever with even more performance, faster Wi-Fi, and support for up to two external displays — all in its strikingly thin and light design with up to 18 hours of battery life EDIT: mmmm... no. >Support for up to two external displays: MacBook Air with M3 now supports up to two external displays when the laptop lid is closed FFS Apple. I guess it's something of an improvement at least :-/ reply dangus 12 hours agorootparentprevIt’s not on Apple to make sure their cheapest consumer-targeted computer is good enough for enterprise use. To me it’s not really relevant what the old computer models used to do. You have to evaluate what is available today and choose accordingly. Like it or not Intel chips had different strengths and weaknesses. It’s a different design entirely. I’m split on whether this is a dirty price segmentation trick or a legitimate design limitation where adding more display support is expensive in terms of die size. Doesn’t matter though, because companies doing serious work are supposed to know to buy the business versions of laptops. They don’t buy Dell Vostro consumer grade PCs, they buy Dell Precision/Latitude/XPS business systems. Apple tells you right in the name of their system: Pro. If you’re a professional you buy the Pro model. If it’s too expensive then buy something else. reply organsnyder 12 hours agorootparentOnly the M2 & M3 Max chips support more than two external monitors[1]. Those start at $3200, and are overkill for the vast majority of use-cases. There's no excuse for a $2000+ machine to not support more than two external monitors. DisplayLink on MacOS is far from ideal, either: it works alright, but it has to use the screen recording functionality in the OS, which causes anything with protected content to freak out. [1] https://support.apple.com/en-us/101571 reply wubrr 11 hours agorootparentAlso note that 'support multiple external monitors' here actually means 'kinda support some monitors sometimes'. Just google and read the hundreds of threads about external monitor issues on M2 pros. reply sgarland 10 hours agorootparentThe only issue I have with external monitors on my M2 Pro – and it’s admittedly annoying – is that unless I turn everything on in a specific sequence, the primary monitor’s energy saving kicks in and turns off the screen before the Mac has synced video. It essentially bootloops. This only happens on my Acer Predator, and only if I’m using DP —> USB-C. The secondary LG doesn’t care, nor does the Acer if it’s over HDMI. The fix I’ve found is to wake up the Mac first with the external keyboard, then turn the Acer on and wait for sync, login, then turn the LG on. While I’d obviously rather not have to deal with this, I feel like it’s at least partially on the incredibly aggressive power saving of the Acer, which I can’t find any way to disable or extend the timeout of. reply fragmede 11 hours agorootparentprevThe excuse is that this is Apple, and the solution to problems with them is to buy more things. In this case, get a $1,500 ultra wide curved monitor which is better than dual head. reply semi-extrinsic 10 hours agorootparentFor $1500 it's better to get one of the 43\" 4K displays. I've used one for over half a decade now, and the ability to comfortably tile a browser plus four terminals side-by-side is unmatched. Or if you will, display 10 A4 pages of a document simultaneously. reply nerdbert 10 hours agorootparentI have a €500 43\" 4K and it's great. But anything else feels so small now. reply fragmede 7 hours agorootparentprev4k? I'm talking 5k. regardless of which monitor you get, point is $1,500 gets you a lot of monitor that is far better than dual head. reply oblio 2 hours agorootparentprevNo, it's not. I've used ultra wides and I don't like them :-) reply fragmede 2 hours agorootparentget a better window manager. but to each their own. :) side steps the problem of single monitor output reply oblio 1 hour agorootparentWould a better window manager fix the fact that I don't like the way the image looks? reply mthoms 10 hours agorootparentprevM1 Max also supports two external monitors (plus built-in monitor). reply dangus 12 hours agorootparentprevSure, but most people don’t use more than two external monitors. Most people don’t use more than one. The people who complain about specs per dollar were never Apple’s customers. “Why buy an Audi when a Dodge Neon SRT4 costs half as much and goes faster?” It has been this way for 40 years now. This just isn’t how they operate. When they design a product they don’t start from the specs, they start from how people use the product. There are much cheaper ways to own a Max system if that specific spec is something you’re desperate for. For one thing, Apple themselves is selling the current model for $2700 refurbished. $500 off and it’s the exact same system with a brand new battery and full warranty. Also, you should never buy a Mac without the student discount at the very least. Anyone can get it. Finally, a used M1 Max system will cost you under $2000 and is barely 3 years old. Keep in mind that if you were buying a MacBook Air in 2010 you were paying over $1800 in today’s money. reply ffgjgf1 1 hour agorootparentIf we’re talking about support for external displays this all seems entirely tangential. > When they design a product they don’t start from the specs, they start from how people use the product. So they impose arbitrary limitations that have basically nothing to do with the specs just so that people who are supposed to use more expensive machines wouldn’t buy the cheaper models? Sounds about right. Apple is trying to maximize their revenue because they can. There is nothing wrong about a for profit company doing that. Trying to find any other explanation is a bit silly though.. reply hollandheese 9 hours agorootparentprev>Sure, but most people don’t use more than two external monitors. Most people don’t use more than one. Most people don't buy Macs. So why even sell them then? They literally took away a feature that their cheapest Intel Macs could do, and restricted it to their most expensive Apple Silicon Macs. They should be lambasted for this. >Finally, a used M1 Max system will cost you under $2000 and is barely 3 years old. A Raspberry Pi can do this for under $100. Come on. reply wubrr 11 hours agorootparentprevnext [5 more] [flagged] dangus 11 hours agorootparentWell, what PC people do is they hyper-focus on one specific spec like number of displays supported or price per GB of RAM but can’t see the forest for the trees beyond that. If I just do the same thing with Macs I can win arguments just as easily. Find me a laptop with the kind of performance per watt specs as the M3 systems. Find another laptop of the same size/weight/power draw that can match the M3 Max’s performance at anything close to the same battery life. Find me a completely fanless Intel/AMD PC that performs as well as the MacBook Air and gets the same or better battery life. Find me a PC laptop where you can feed a RTX 40X0 mobile GPU with over 100GB of RAM. Find me another laptop that uses TSMC’s most advanced chip lithography. PC spec monkeys will basically say it’s not a real laptop because it can’t support 800 external monitors and there’s no print screen key and it doesn’t have a parallel port etc etc. These are all specs that don’t matter to 99% of users. Hell, if you’re the kind of person who has a triple or quad external monitor setup, that means you’ve spent around $1000 on just displays. That probably means you can afford $3,000 for a MacBook Pro with a Max chip or maybe pay $2,000 for a used one. And if you didn’t spend $1000+ on those displays, that means those four displays are probably so bad that you’re better off looking at one 4K display or two decent quality ultrawide displays. reply wubrr 11 hours agorootparent> Well, what PC people do is they hyper-focus on one specific spec like number of displays supported or price per GB of RAM but can’t see the forest for the trees beyond that. Not at all, there are many examples of various types of specs in this thread, where apple fanboys suddenly go mute :) > If I just do the same thing I can win arguments just as easily. Find me a laptop with the kind of performance per watt specs as the M3 systems. Find another laptop of the same size/weight/power draw that can match the M3 Max’s performance at anything close to the same battery life. So the only example you can come up with is performance per watt? (Your second question is basically the same as your first). M3 very good in that category, I don't disagree, it's apple's latest/best processor, and it does slightly outperform AMD Ryzens in that category[0]. Of course, when you take price into account, apple M processors are not even close to best :). > Find me a PC laptop where you can feed an RTX 4080 mobile with over 100GB of RAM Hilarious that you bring this up when macs don't even support CUDA and basically useless when it comes to the the most important aspects of having a GPU today... gaming and deep learning... > Those laptops don’t exist, unless it’s a Mac. Yeah, nothing but apple exists in an apple fanboy's mind. [0]https://www.cpu-monkey.com/en/cpu_benchmark-cpu_performance_... reply dangus 10 hours agorootparentSo what you’re saying is you can’t find a better performance per watt, AMD “comes close.” You are doing the spec monkey thing again. You changed the spec. I chose performance per watt and now you’ve changed it to performance per dollar. Under a performance per dollar logic AMD makes the best PC graphics card on the market, which they obviously don’t in terms of total performance. Nvidia charges a huge price/performance premium on the RTX4090 because you can’t buy that performance elsewhere. Sound familiar? > So the only example you can come up with is performance per watt I’ve got another one: media encoding. Apple’s systems obliterate the rest. If your argument is that CUDA is important I hate to say it but you’re actually reverting to that whole “product ecosystem and experience” angle that you were deriding in the same breath. Nvidia users have to buy Nvidia because it’s the only way to use Nvidia software. Kind of like how iOS developers and Final Cut Pro users must buy a Mac? “Yeah, nothing but apple exists in an apple fanboy's mind.” You could replace that statement with “Nvidia” under your own preferences. Under the spec monkey argument someone buying a graphics card should ignore Nvidia’s CUDA ecosystem and buy an AMD graphics card that offers better performance per dollar. But you’re saying that the lack of CUDA on a Mac is a major downside. Which is it? Performance per dollar or user experience and ecosystem? This is why doing the spec monkey thing turns us around and around in circles. I’m not being an Apple fanboy I’m just pointing out how it’s completely reasonable for an expensive computer to not prioritize supporting a zillion monitors. reply wubrr 10 hours agorootparentI never claimed that any particular single spec makes macbooks bad, that was entirely your own strawman :). There are maaaaany reasons why I think they're bad. > I’m not being an Apple fanboy I’m just pointing out how it’s completely reasonable for an expensive computer to not prioritize supporting a zillion monitors. My 9 year old asus laptop has better external monitor support than my m2 macbook pro... these problems were basically solved 10 ago... how hard can it be? How much do you have to 'prioritize' this? How hard is it to solve the many years-old annoying, well-known macos bugs? I don't see innovation or engineering quality coming out of apple (the only exception being (the very recent) M line of CPUs)... everything else is meh - buggy, fragile, locked-in, overpriced, non-standard, lack of support for important stuff like CUDA, etc. reply ffgjgf1 1 hour agorootparentprev> It’s not on Apple to make sure their cheapest consumer-targeted computer is good enough for enterprise use. Except they crippled it on purpose as a form of market segmentation. Claiming anything else is beyond absurd. reply ben-schaaf 11 hours agorootparentprevCelerons from 10+ years ago support 3 monitors and 32GB of RAM. There is no excuse. reply paulmd 10 hours agorootparentthere are AMD chips being sold right now that don't even support HDMI Org VRR let alone AV1 decode. and those skylake laptops are stuck on HDMI 1.4b, so they top out at effectively 1080p60, but sure, you get three of them. And the DP/thunderbolt tops out at 4K60 non-HDR with crappy decode support, and you get at most like 2 ports per laptop. the grass isn't always greener, there's lots of pain points with x86 hardware too. heck, those celerons you're so fond of are down to literally a single memory channel by this point. is a single stick going to be enough raw bandwidth for a developer that wants to be compiling code etc? reply ben-schaaf 9 hours agorootparentHDMI 1.4b does 1440p75 or 4k30 and HDMI 2.0 was brand new at the time. > the grass isn't always greener, there's lots of pain points with x86 hardware too. heck, those celerons you're so fond of are down to literally a single memory channel by this point. is a single stick going to be enough raw bandwidth for a developer that wants to be compiling code etc? What a weird argument; no shit a bargain bin CPU from 10 years ago is worse than a brand new mid-range chip. That's the exact point I'm making. That Celeron was bad 10 years ago. 10 years of progress, billions of dollars of investment and you get the same maximum RAM capacity, less external monitors at a much higher price. reply paulmd 52 minutes agorootparentAnd you’re ignoring all the things that the apple will do that your chip won’t, or the things it’s massively better at. It has 2x the bandwidth of a Radeon 780M and runs at 35w, it has as much bandwidth as a PS5. there are pluses and minuses to doing it both ways, but, detractors only want to look at the handful of areas where traditional chips have an edge. reply goosedragons 8 hours agorootparentprev\"Cheapest\" but not cheap. A $400 Steam Deck can do 3 external monitors with an inexpensive MST hub and has a single USB C port. The MBA is an extremely close competitor to the Dell XPS line too. And \"Pro\" doesn't even guarantee you more monitors. The $1600 M3 MBP is just as limited as the \"consumer\" Air. reply dmitrygr 8 hours agorootparent> extremely close competitor to the Dell XPS line Only if you ignore the shitty finger trackpad tracking on dell, windows (shit UX) or Linux (shit battery life and shit sleep/wake), and in general the real life battery duration in real life use cases. reply wubrr 12 hours agorootparentprevI work on a m2 pro and the external monitor issues don't end. reply dangus 12 hours agorootparentprevPrice: if that’s the major qualm that’s not really a product flaw. The best product usually commands the highest price. Stuck with macOS: technically not true, Asahi Linux exists. Connecting multiple monitors: a legitimate negative limitation unusual at the MacBook Air price point, but still something that only a small fraction of consumer laptop buyers care about. reply goosedragons 8 hours agorootparentAsahi Linux is still missing features a lot of people consider key. M1 even STILL doesn't support DP alt mode for example. That's a pretty serious shortcoming on something like a MBA where the only video out is through DP alt mode. reply wubrr 12 hours agorootparentprev> The best product usually commands the highest price. apple products being the perfect exception :) reply hn_throwaway_99 12 hours agorootparentprevI agree 100% with what you've said, but this sentence: > Way over-priced storage and RAM upgrades, can't connect multiple monitors unless you pay up, and you're stuck with MacOS. Basically boils down to \"Apple is selling a much better product, and they know it.\" I.e. your first bullets (over priced storage, RAM, charging for multi monitor support) all just boil down to \"Apple charges more because they can\". The \"you're stuck with MacOS\" is obviously true but just highlights that Apple has always been about optimizing hardware and software together. If anything, I think the \"dark times\" for Apple laptops was the late teens during the era of stuff like the butterfly keyboard, the touchbar, and too few ports. I think Apple consumers have consigned themselves to paying more for a much better product. What they're not willing to do (as much anyway) is to pay a premium for a crappier product. The butterfly keyboard especially was such a disaster (\"We shaved .2 mm off the width, all at the minor expense of any key randomly stopping to work at any time!\") Admitting mistakes in big corporations is hard so I'm glad they just jettisoned all that stuff. reply thomaslord 11 hours agorootparentThese days I want Apple's hardware (the M chips specifically, but the trackpads/screens/cases are nice too) but can't stand their software. While I'm not a big fan of Windows either, it at least provides basic window management features by default. reply wubrr 12 hours agorootparentprevnext [6 more] [flagged] hn_throwaway_99 3 hours agorootparent> thread making absurd claims about 'apple is way better than everyone' based on nothing but anecdotal experiences... I'm not sure why you think \"anecdotal experiences\" are invalid when people are talking about a personal choice. I.e. I don't need some sort of double blinded study to \"prove\" Mac laptops are better. I've used other laptops, and I have a strong preference for Macs for a myriad of reasons that have nothing to do with marketing (to be honest I can't even remember the last time I saw an actual ad for a Mac). You may disagree, that's fine, but it's silly to pretend the personal preferences of others are somehow invalid or less than. reply squokko 6 hours agorootparentprevNo, the product is simply much better. If you pay $2000 for an Apple laptop vs. a PC running Windows or Linux, the Apple laptop will have twice the battery life and be in better physical condition after 3 years of equivalent use. reply piva00 12 hours agorootparentprevHave you ever used a Apple computer for long, like 1-3 months? I ask because I used to be like you, calling Apple users \"fanboys\", throwing hard data from benchmarks in discussions, being proud of my true h4ck0rz Linux installation on a IBM ThinkPad for work that was a pain in the ass to maintain in working state, had to stop updating after too many hours spent troubleshooting. Or relegate myself to working in Windows on ThinkPads. Until one day I begrudgingly accepted a Intel MBP at a new job some 15 years ago, I was going to install Linux on it anyway so didn't care. Started using macOS in the meantime, it had the shell utilities I needed so I kept using it while checking how you install some Linux on it, the UI worked flawlessly, the OS was a breeze to learn, after a few months I had barely had to troubleshoot anything, I'd just turn it on and work. I never went back, I want my tools to work well and found a tool that worked much better than anything else I had used before. When something better shows up I'll be very excited to try, unfortunately nothing in the past 15 years has changed my mind. Not everyone likes it, and that's ok, but calling satisfied customers \"fanboys\" is a tad bit immature. The product works, and works well. reply wubrr 11 hours agorootparent> Have you ever used a Apple computer for long, like 1-3 months? I've used many apple computers for the last ~10 years. I work on them daily. > I ask because I used to be like you, calling Apple users \"fanboys\" I'm not calling 'apple users' fanboys, I'm calling people who are literally fanboying in the comments fanboys. > Started using macOS in the meantime, it had the shell utilities I needed so I kept using it while checking how you install some Linux on it, the UI worked flawlessly, Ahahaha, there are soooooo many bugs in the macos UI and macos in general, many of these are well known and have existed for years. > the OS was a breeze to learn, What kind of point is this? You said you've used Windows and Linux before... what else is there to learn for macos? A few new shortcuts? > I'd just turn it on and work. I turn my windows and linux laptops on and they just work! Magic! So again, you didn't make a single rational argument for why macbooks and macos are actually better... literally a fanboy. reply piva00 11 hours agorootparent> Ahahaha, there are soooooo many bugs in the macos UI and macos in general, many of these are well known and have existed for years. What's the point of this? I didn't say it was perfect and bugless... The point about turning it on and working is that I never had an issue where my soundcard simply stopped working (many times on Linux), nor issues with sleep mode not working and draining the battery (many, many times on Linux), nor my graphics configuration randomly going out of whack and KDE/Gnome getting stuck in a bizarre resolution. Maybe I should just disengage, you sound a bit deranged in your quest, best of luck! reply DinaCoder99 12 hours agorootparentprev> you're stuck with MacOS Interesting way of thinking about probably the biggest draw of the hardware. reply Dah00n 11 hours agorootparentDepends. To me, software is the thing that will keep me from Apple products. IMO Linux and Android are light-years ahead and are becoming even further ahead every year. reply DinaCoder99 2 hours agorootparentI hardly expect folks on HN to have normal opinions about the usability of linux, but duly noted. reply threeseed 9 hours agorootparentprev> IMO Linux and Android are light-years ahead Linux on the Desktop has finally arrived ! reply babypuncher 11 hours agorootparentprevI have a hard time considering Linux \"light years ahead\" when they still can't even figure how to do HDR. reply spaceman_2020 3 hours agorootparentMy daily driver before this was Linux and anyone who says \"Linux is light years ahead\" is kidding themselves You have to set up a bash script to do something as basic as change the scrollwheel speed. Bluetooth is extremely spotty. Installing most software is still a pain unless you know all sorts of terminal-fu reply sensanaty 31 minutes agorootparentYou have to install software like UnnaturalScrollWheels or Smooze to get sensible mouse scrolling behavior out of MacOS (unless you use the horrific monstrosity of that Apple mouse that you can't use and charge at the same time). You have to install software like Rectangle to get actual window tiling + shortcuts for window tiling. You have to install Raycast/Butler to have a non-shit Finder alternative. There's dozens of basic UI/UX things Apple gets wrong that can only be fixed via either building your own hacks or paying for some ludicrously priced proprietary software (for example Smooze Pro). I could go on, there's many basic features MacOS has been missing for going on a decade, let's not pretend they get it all right either. reply sensanaty 36 minutes agorootparentprevCan any OS? I've got an (apparently) HDR-capable monitor but genuinely can't tell much of a difference on Win10/11, any Linux distro I've ever tried and my Macbooks provided by my work. The whole HDR thing seems more like a meme or weird flex type of thing to me, I've never noticed it ever really making a difference for me. Also a weird hill to die on when talking about relative strengths of each platform, but you do you. reply layer8 9 hours agorootparentprevNah, it’s the single reason I have zero interest in Mac hardware. reply sensanaty 6 hours agorootparentprevGood lord no, the hardware is the actual good part of these machines, the OS is a piece of crap hobbled by catering to the lowest common denominator. I wish I could just wipe it off my work M3 pro, cause it drives me insane on the daily reply DinaCoder99 2 hours agorootparent> catering to the lowest common denominator Ah this is why technical professionals all use linux, right? reply sensanaty 39 minutes agorootparentYes? Windows and Linux are by far the majority for most software work according to the SO Developer Survey[0]. Going by the survey, MacOS is 3rd after Win/Linux (of the 3 possible options). [0] https://survey.stackoverflow.co/2023/#section-most-popular-t... reply aae42 11 hours agorootparentprevi'd say the M processors are the biggest draw... but yea, i agree, probably the 2nd biggest draw reply jb1991 13 hours agorootparentprevI'm still using an original M1 Air and the thing is used nearly all day for light casual web usage, and I only plug it in about two times per week -- the energy efficiency is no joke. This kind of battery life really spoils you and when you see other laptops that nearly require the plug charging all the time, tethered, you realize what a big deal these M chips are for true portability. reply gnicholas 12 hours agorootparentI end up forgetting my charger when I go on trips because 99% of the time when I unplug my laptop at home (to use in a different room), I never need the charge cable. It used to be 50% of the time I'd take the cable with me, so I could be somewhere else for 2+ hours. Now I so rarely bring the charger that it just doesn't even occur to me when I'm unplugging the thing to pack. It's night-and-day compared to the Intel MBPs. reply nicoburns 9 hours agorootparentNow that laptops (including but not limited to macbooks) can charge via USB C I just have one charger that I take for both my phone and laptop. Sometimes I take two cables so I can charge both at once. Sometimes I don't bother. reply gnicholas 9 hours agorootparentMy problem was that I brought a USB-C to lightning cable to charge my phone off my computer, but I forgot a USB-C brick (and magsafe or USB-C cable) so that my computer had power. Fortunately there were Apple employees at the conference and they let me borrow a charger while we were hanging out at the bar! reply CadmiumYellow 12 hours agorootparentprevI've been out of the house all day (6+ hours) with my M1 Pro and it's only just starting to get close to needing to be plugged in. What have I been doing all day? Just running the 23 different docker containers required for my local dev environment. This thing is an absolute beast. reply deskamess 10 hours agorootparentDo you think the new M3 Air w/16 GB could be comparable CPU wise to the M1 Pro? I am guessing GPU wise (for inference like apps) it probably falls behind. reply threeseed 9 hours agorootparentCPU was never the problem with the Air it's the thermal system. Depending on what you're doing the device simply can't soak up enough heat and so it ends up throttling the CPU. It takes a long operation e.g. compilation for 10-20 mins before it really starts to fall behind the Pro models. reply deskamess 9 hours agorootparentThanks. reply chx 11 hours agorootparentprevThe problem is the storage is not removable in the Apple Macbooks. It is disturbing it didn't sink the Macbooks. It speaks volumes of how little people care about their own data. About their own privacy. There should've been zero sold. It truly is dismal and a very large systemic problem a laptop like this is sold. Because when it breaks, are you going to wipe it and restore from backup? No. You will just hand it over to a repair person and even an ethical shop much less Apple doesn't even have a chance to hand the disk back before handling it. An unknown amount of complete strangers will access your everything. Your medical records, your banking, your private photos, everything. And people pay real world money for this, money they worked hard for. It's unfathomable to me. reply ecwilson 10 hours agorootparentNon-removable storage isn't the problem. You have identified one actual problem: privacy. But even this isn't a real problem if you turn on system-level drive encryption: https://support.apple.com/guide/security/volume-encryption-w... Soldered-in components make for higher quality, lower cost production. Anecdotally, every Windows machine I've had has failed. Every MacBook machine I have replaced after 4-5 years when I wanted to upgrade to the latest technology. reply neop1x 1 hour agorootparentprevFileVault encryption is on by default. The encryption key is derived from the login password and the login screen has delays to avoid brute-forcing the password. Additionally, data in the flash chips are always encrypted by the unique key burned in the M chip (previously T2 secure enclave). reply angst_ridden 9 hours agorootparentprevAs others have said: encryption. Even before they supported full-disk encryption, I made it a habit of making yearly encrypted disk image files where I store all my financial and medical data. I open them when working on the info, close them afterwards. Even some attack that somehow bypassed disk encryption (like a browser hack or something) won't get anywhere. reply newaccount74 11 hours agorootparentprevNobody has access to anything unless you give them your password. Macs have full disk encryption built in. If you aren't using FileVault, you are doing it wrong. reply mgdev 11 hours agorootparentprevTurn on FileVault. Don't give anyone your password. Pretty fathomable. reply robbie-c 11 hours agorootparentprevWon’t it be encrypted with FileVault? reply pandaman 5 hours agorootparentprevTastes differ. My work MacBook unlocks with a fingerprint (with random sensitivity, if my hands are too dry, for example, it does not register) even in 2024. My Windows laptops had been unlocking with face recognition for 10 years or so. I find the latter much more convenient, especially when I have the laptop on a stand and use an external monitor and keyboard/mouse. Does not look like Apple is ahead of anything here. Hardware wise Apple is actually okay for the price in the low end, but you can only add memory/SSD/CPU (at ridiculous $/{GB,GHz} ratio) for the higher end, you can't get an OLED display, or a good keyboard, or bigger battery at any price. reply makeitdouble 13 hours agorootparentprevThere's a volume zones of people who want \"a laptop and nothing more\", will pay for better materials and Apple has perfected that segment with a few caveats [0]. To your point, then comes the lower end (\"just give me something cheap\"), the corporate middle (\"the same laptop as at work\"), and the super high end (gaming, CAD, anything needing special software or a discrete GPU), with the outliers (linux etc) IMHO windows laptop nowadays are for people who either don't really care, or have already a very specific target or limitation. For instance Lenovo or Asus definitely care about pushing laptops' limits and design. A lot. IMHO more than Apple. [0] resistance to abuse isn't there. A macbook's screen will be dead pretty quick if not handled with appropriate care. A Lenovo Flex for instance will take it a lot longer. reply dividedbyzero 12 hours agorootparent> For instance Lenovo or Asus definitely care about pushing laptops' limits and design. A lot. IMHO more than Apple. It's a bit more nuanced. Lenovo/Asus seem to be experimenting a lot more, but more like by throwing (relatively) easy-to-build variations at the wall to see what sticks, then release a few more polished SKUs. Apple doesn't really do that, but they do attack those limits and design aspects they care about very aggressively and with a ton of resources (e.g. battery life pre-M1, manufacturing tolerances). reply makeitdouble 10 hours agorootparentOne of the issue is laptops aren't a growth angle to Apple. For instance the efforts they made towards the CPU and processing architecture wouldn't have made sense without the other products benefiting from it. We've seen that with the touchbar: it got a first release, and basically no improvements, no bug fixes, no better support from there. A laptop only feature gets no love from today's Apple. Even the iPad saw little to no progress in recent years, outside of sharing specs with the mac. I posit we'd see a foldable/bendable phone from Apple before we ever see something significant form factor change in laptops. reply datavirtue 8 hours agorootparentThe touchbar got shit on and it was probably expensive at the same time. Worthy innovation perhaps, but the customers didn't value it. I think that was the reason it faded out. I actually used it for audio adjustments, but I didn't need it. reply Asmod4n 11 hours agorootparentprevI’ve been an Apple only user the last 20 years, but they simply lost me with the garbage they sold since 2016. The M series was too little too late. It also didn’t help that macOS didn’t get better anymore and the yearly release cycle only made it more buggier. I’m happy with AMD finally catching up to intel and nvidia with a performance to price ratio Apple will never deliver. reply reaperducer 9 hours agorootparentI’ve been an Apple only user the last 20 years, but they simply lost me with the garbage they sold since 2016. If they lost you eight years ago, then you haven't been an Apple user for the last 20 years. Also, 2016 was eight years ago. Get over it. Or at least find an axe to grind from this decade. reply ponector 11 hours agorootparentprev>>Anything even remotely within Macbook vicinity costs the same as a Macbook anyway. It is not correct, unless you select minimal amount of ram and SSD. Select versions with proper amount of memory and MacBook becomes much more expensive than comparable windows machine. reply hollandheese 9 hours agorootparentprev>Anything even remotely within Macbook vicinity costs the same as a Macbook anyway. Until you want more memory or a larger SSD then the Macbook is all of a sudden double the price of the equivalent PC laptop. >Increasingly feels like most manufacturers have given up on the laptop as an innovation center and are happy to just scrape up the consumers who can't or won't buy Apple. That's basically true, but with Apple becoming more and more expensive that does leave a very large low-end market for them to play in. reply pquki4 9 hours agorootparentRecently got a ThinkPad T14s gen 4 with top-of-the-line AMD 7840U, 32GB RAM and 512GB SSD, for just a bit over $1k. It has very good build quality, is powerful yet quiet, and has decent battery life. It supports up to 4 displays. That's basically one of the best Windows laptops you can get. Of course, it does not have high-DPI mini LED screen, great speakers or 18 hours of battery life, but none of that really matters, and I'd choose this any day over a similarly speced Macbook Pro 14 that would cost me $2,399. reply Moldoteck 21 minutes agorootparentpls define decent battery life because for different people decent means different things. My biggest problem with windows laptops is battery life. My dell work laptop is spec't higher compared to my home m1 air but the ~4h battery life, worse trackpad and worse audio make it a worse experience overall... reply qwertox 13 hours agorootparentprev> And for some reason, so many companies are still shipping laptops with 1080p screens in 2024. I am in the group of people who go for Full HD. It's enough for me, my eyesight is relatively bad. Then again, I use 3 monitors. reply nacs 13 hours agorootparentYou'll like the way Apple does their screens even more then. They're 3-5k screens but they essentially scale up their UI 3X so it still looks like a 1080p except 3x sharper. They are what Apple calls \"Retina\" displays. reply goosedragons 12 hours agorootparentIf you can't see the extra sharpness it's not really a pro... A 13\"-15\" 1080p screen is pretty similar PPI to a 27\" 4K display. This is pretty nice because if you have both at the scaling level elements are the same size on both. reply overstay8930 12 hours agorootparentprevmeanwhile Mac Studio users waiting for 5k monitors @ 144hz reply EasyMark 8 hours agorootparentprevSimilar stories. I usually had a windows laptop and linux and VNC'd into the linux box and with win 10 and 11 I just got tired of fighting all the garbage that microsoft tries to push on you in the OS. I really just want to get to work, so I bought a top of the line M2 macbook air and couldn't be happier with my decision. Apple has their apps on there but I don't get pop ups and ads and have to go through a bazillion privacy settings to turn of MS spying on every little thing. reply exe34 13 hours agorootparentprev> Anything even remotely within Macbook vicinity costs the same as a Macbook anyway. The opposite (\"macs are overpriced\") is something I've never been able to understand. Back in 2013 when I bought my current laptop, the mac book air was the thinnest, lightest, longest battery life, nicest keyboard, and a bunch of superlatives I don't remember, and it was somewhat over £1000. The closest non-mac \"ultrabooks\" I could find in shops at the time cost the same, and felt like cheap rubbish. And this laptop just refuses to die, and handles my workload just fine after all these years. I'm dreading the day I have to replace it. reply AnthonyMouse 13 hours agorootparent> The opposite (\"macs are overpriced\") is something I've never been able to understand. It's simple: People without much money have basic needs and want a ~$600 laptop but Apple doesn't sell one. It doesn't matter if the $1000 Macbook has better battery life than a $1000 Dell because they don't have a $1000 budget. reply sib 12 hours agorootparentThat's different from being \"overpriced.\" Overpriced would be \"costs more than it should for what it is,\" not \"costs what it should but is more product than I can afford.\" reply AnthonyMouse 11 hours agorootparent> Overpriced would be \"costs more than it should for what it is,\" not \"costs what it should but is more product than I can afford.\" But it's also that, because their bottom configurations are weird/crippled. It's hard to find a PC laptop with a 4k screen for much less than $1000, but then the $1000 machine has 12 cores and 32GB of RAM and 512GB of storage. Apple's $1000 laptop has 8 cores and 8GB of RAM and 256GB of storage, i.e. overpriced. Okay, but DDR5 is ~$3/GB and NVMe SSDs are ~$0.10/GB, so really that's only a value difference of like $100 and you could just upgrade it. Except that Apple charges $25/GB for DDR5 and $1.28/GB for storage and then solders everything, so you'd actually have to pay an extra $800. Except that the Macbook Air isn't available with 32GB of RAM, or more than 8 cores, so then you need the Pro, which is even more. reply musicale 3 hours agorootparentThe bad part of Apple DRAM is that it's not upgradable or replaceable unless you can figure out how to open up the M3 multi-chip-module and nanosolder a new RAM module. The good part is that the DRAM is connected to the GPU as well as the CPU. reply AnthonyMouse 1 hour agorootparentNothing stops them from doing both together. Put 8GB or 16GB on the APU package and then have a couple of SODIMM slots for more. The more wouldn't have the same bandwidth but it doesn't need to, because you're using it for browser tabs and filesystem cache, to keep that stuff out of the fast memory and preserve it for what needs it. Moreover, they could put the APU in a socket and then if you wanted more of the integrated memory you could replace the APU without having to replace the entire machine. reply exe34 12 hours agorootparentprevI probably can't justify the expense myself anymore - last time I had a hefty student discount. But this doesn't make them overpriced, it just makes me poor. reply hollandheese 9 hours agorootparentprevNo, it's I need a 2 TB SSD in my machine. That costs $100 for a PC laptop, and $600 for the Mac. I need at least 16GB of memory, that's $300 for the Mac, and $60 for the PC. It's the fact that Apple is grifting everyone who needs more than the base specs. reply bombcar 11 hours agorootparentprevApple does partially cover that market, but only via refurbished. https://www.apple.com/shop/product/FGN63LL/A/refurbished-133... The thing that's really sad is that the build quality on sub $1k laptops is just such shite reply AnthonyMouse 11 hours agorootparent> Apple does partially cover that market, but only via refurbished. That's not the same thing. If you're budget conscious then you presumably need to keep it for a long time, but now you've got a used battery and a machine that will fall out of support sooner. > The thing that's really sad is that the build quality on sub $1k laptops is just such shite The secret to this one might be refurbished Framework laptops. Sure, you've got a used battery, but now it's easy to replace and costs $50 instead of $250. reply simonh 10 hours agorootparentRefurbished devices from Apple generally come with new batteries, and sometimes other parts will be replaced as well if needed. I’ve bought several devices refurbished and they’ve been excellent. With iOS devices they even replace the outer shell as standard. reply AnthonyMouse 9 hours agorootparentRefurbished iOS devices from Apple generally come with new batteries. They replace the batteries in Macbooks if the existing battery is already defective. That doesn't mean it can't have 3 years of use on it already, and then fails 3 years sooner, it just means it's not already below the threshold for immediate replacement when you buy it. reply eropple 12 hours agorootparentprevI'm not sure those people are the intended Macbook audience, though. How many of those people wouldn't be roughly as productive using an iPad or something? I've found myself doing almost all my non-development, non-media stuff on an iPad Air as of late and the environmental constraints have been really helpful for focus. (My iPad Air was $599 new, and I use a shockingly pleasant $30 case-and-keyboard combination for typing--no, it's not a mechanical keyboard, but c'mon.) reply windowsrookie 10 hours agorootparentI have tried several different iPads over the years and I have never found them to be useful in any way. A laptop is superior in every situation for me. Also the M1 MacBook Air was on sale many times for $700. That's less than $100 more than your iPad Air + keyboard. reply lupire 11 hours agorootparentpreviPad has a tiny screen and a tiny keyboard. It's not a laptop. reply eropple 11 hours agorootparentAs far as screen size goes, my iPad is about the same size as my 11\" Air was, though not widescreen. And the keyboard's small, but Apple has prior art there too and you can get a bigger iPad if you really want to (though, sure, it costs more). It's not a laptop, no--but that's also not inherently a bad thing. If you need what a laptop can do, sure. I have a 32GB M1 Max for a reason. But more and more it seems obvious to me that the median computer user doesn't need that, and the affordances from overlap with their more accustomed part of the ecosystem (their phone) are strong and pretty valuable. reply itsoktocry 13 hours agorootparentprev>nicest keyboard When have MacBooks ever had the nicest keyboard? They have pretty good keyboards, but I have 10 year old ThinkPads with keyboards that I prefer. reply aembleton 12 hours agorootparentKeyboard feel is very subjective. Different people like different things about keyboards. reply exe34 12 hours agorootparentprevAre thinkpads considered \"ultra mobile\" (or whatever the buzzword is nowadays)? I thought they were more hefty? The thin/lightweight laptops from back then had crappy clickety keyboards. reply mhb 11 hours agorootparentOf course they were heftier, but \"nicest keyboard\" and \"nicest keyboard given ridiculous thickness constraints\" are a world apart. No idea what you mean about crappy Thinkpad keyboards. The old Thinkpad keyboards are widely acknowledged to have been great. reply Dalewyn 11 hours agorootparentprev>Most Windows laptops have abysmal batteries, to the point that you can barely call them laptops. I've been positively delighted by my two Intel Alder Lake laptops I use during travel for play (ASUS Vivobook S 14X OLED, 12700H CPU) and work (Lenovo V14 G3, 1255U CPU) respectively. I can get 4 to 8 hours off of them depending on use with the charge limited to 80% for longer overall life, and as I just mentioned the hardware are quite powerful in their own right. >The trackpads are downright unusable. Both of my laptops I just mentioned have wonderful touchpads. Frankly though, this absolutely will vary by several country miles depending on manufacturer and even model. I suppose I got lucky here. >And for some reason, so many companies are still shipping laptops with 1080p screens in 2024. I'm gonna be honest: I fucking hate screens bigger than 1920x1080 (or x1200 for 16:10 screen ratios). My laptop for play has a 2880x1800 screen, but I've got it rendering at 1920x1200 because so many programs just assume pixel densities around that area and either can't or won't handle scaling. I also have to still do some scaling up even at 1920x1200 or 1920x1080 at laptop screen sizes anyway because everything is so small, but it's still less compatibility headaches compared to physically denser pixels. reply Moldoteck 18 minutes agorootparent4 hours isn't exactly good... To me 4h sounds kinda bad For trackpads it depends, there are some windows laptops with good ones, like dell xps or the carbon line, but mostly these are worse compared to macbooks, if you haven't tried mac trackpad - you should definitely try one reply datavirtue 8 hours agorootparentprevYeah, whoever managed (or didn't manage) the Surface trackpad project needs fired. Can't count how many times I have belted out obscenities while working on my Surface Pro. Garbage. I forgot it had a touch screen and I never flip the screen. All that caused the trackpad to be ignored, I guess. Again, hot garbage. I have a MacBook Pro M1, which is pure fluid bliss. It cost less, it's faster, and the battery lasts for days. reply wubrr 12 hours agorootparentprevnext [3 more] [flagged] spaceman_2020 5 hours agorootparent> What 'innovation' has Apple introduced in the last 10 years other than their M chips? Most of their 'innovation' is just marketing for people who don't know what they're buying I mean, that’s massive innovation by itself… reply Dalewyn 7 hours agorootparentprev>What 'innovation' has Apple introduced in the last 10 years other than their M chips? Computers that Just Fucking Work(tm) if you stay on the One Apple Way(tm), which 99% of people are perfectly content to be on because shit Just Fucking Works(tm). Seriously, Just Fucking Works(tm) is a virtue. reply makeitdouble 13 hours agoparentprev> Apple continues to simplify the laptop naming scheme, we're at a point where it's simply: > Air OR Pro > Small screen OR big screen The weird part of that argument to me: to arrive to that point you've already made a ton of choices that need to be educated. You decided on the form factor: you don't want a convertible (neither a Surface like tablet + keyboard, nor something like a Yoga). You decided to forgo touch. You decided you don't really want to game. You also evaluated you don't need anything Windows or x86 only. Then sure there's about 10 models. But at that point is it much complicated than say, choose from the DELL XPS line ? reply Longhanks 13 hours agorootparentIn my experience, most people looking for \"just a notebook\" don't care about any of those things. They want a low maintenance, high performant (for day-to-day tasks, not gaming) portable computer with a great battery that runs Chrome, Office and Spotify, and comes with great customer service - nothing comes close to being able to bring your Mac to the next Apple Store. reply makeitdouble 13 hours agorootparentIf I gave these requirements to my parents they'd go to a random computer shop and come back with probably a random 13 or 15 inch DELL. And that's what millions of people get at work as a standard supplied computer. I am not saying the mac isn't good at filling that niche, just that people who really don't care about computers also don't care if it's a mac, and will probably be fine with any recent default configuration machine from a major maker. PS: > bring your Mac to the next Apple Store You need an Apple Store. In my experience people have come to terms with shipping devices and waiting for repairs. Cloud sync helps a lot in that respect, as keeping another computer around has become decently manageable. reply Klonoar 12 hours agorootparent> If I gave these requirements to my parents they'd go to a random computer shop and come back with probably a random 13 or 15 inch DELL. And if I did that, they'd also come back with that DELL - and then I'd be stuck doing tech support for them for however long the thing lasts. I cannot begin to count the number of times they've gone and bought some junk computer that they got upsold on. This is not an experience unique to me, either. The non-Apple laptop segment is (mostly) a broken experience in comparison. reply sroussey 12 hours agorootparent> And if I did that, they'd also come back with that DELL - and then I'd be stuck doing tech support for them for however long the thing lasts. I stopped doing tech support for family members using Windows. THAT was the main reason they changed to Mac. And now, hardly do any support for them at all. reply zdragnar 13 hours agorootparentprevYou could just get a Chromebook for that and save yourself a ton of money. It's what my wife did, and she's not wanted anything else for seven years. It isn't close to an apple device in terms of materials or performance, but at a tenth of the cost of a pro it makes a lot of sense. reply garrickvanburen 13 hours agorootparentLaptops in my household are a mix of Macs and Chromebooks. All are >5yrs old. Love the resilience and useful life of both. reply eropple 12 hours agorootparentprevI've had a couple of Chromebooks and enjoyed them, but I never found one with a keyboard that didn't feel terrible to type on. Does one exist today? reply zdragnar 12 hours agorootparentI'm guessing if you want quality materials, you have to pay for it. I don't know if the pixel line of Chromebooks are still a thing, but even if they are you can just get a higher end laptop with more appropriate levels of local SSD storage and an actual OS for what I remember Google charging for them. reply eropple 11 hours agorootparentThat's why my actual laptops are Thinkpads and Macs, sure--but I can hope, right? reply lupire 11 hours agorootparentprevHigher end Chromebooks run Linux well. reply macinjosh 12 hours agorootparentprevAnd in my experience most people looking for \"just a notebook\" don't want to pay the prices Apple demands. Especially when they see prices on the non-Apple laptops. reply sva_ 13 hours agorootparentprev> nothing comes close to being able to bring your Mac to the next Apple Store. Ah really? Ever heard of worldwide on-site next day repair warranty? reply mbreese 12 hours agorootparentprevYou’re going about your decision tree backwards. People don’t say “I don’t want a touchscreen” or “I don’t want to game”. You don’t make decisions based upon what you don’t want to do… you make them based upon what you do want to do. I was talking with my dad recently, and he wanted a new computer that could handle email, a little Excel, Facebook, and some other light web browsing that didn’t get stuck in an infinite reboot loop for system updates (which somehow his Windows got stuck somehow). There are a bagillion options for Windows laptops that fit those needs. He ended up not being able to make a decision and is still using his same old laptop. Whereas my son wanted a desktop computer that would support playing Valorant at 60fps at 1440. That narrowed things down substantially and ended up building one to his specs. If a Mac fits your requirements, then you have far fewer decisions. And that’s part of the point. For the a long time, Apple has stuck to a restricted set of SKUs. This is by design. It’s not that they couldn’t offer a touchscreen, or a convertible, or a xMac. It’s that they’ve been there… had many form factors and SKUs and it almost killed the company. Even if you say you want a Dell laptop — have you ever tried to browse their site? If you say you want a laptop you’re presented with 68 options (I just did this). 68. reply Dah00n 11 hours agorootparent>If a Mac fits your requirements, then you have far fewer decisions. That's what OP said? Because now you have already decided you don't want to game, etc. >Even if you say you want a Dell laptop — have you ever tried to browse their site? This is the iPhone versus Android discussion all over again. Yes, many will be happy with the iPhone, but they also often didn't know they had the option to buy an Android phone that could do something the iPhone couldn't that they'd like to be able to do (like copy&paste or whatever). Ignorance is bliss for some. Others want the choices, and Apple have nothing for those buyers. reply mbreese 8 hours agorootparent> Because now you have already decided you don't want to game, etc That’s exactly what I’m not saying. If you want to play games (with Windows only games), then a Mac won’t work for you. If you get a Mac, that means that gaming likely wasn’t part of your decision tree. Think of the choices as a positive selection. I want to do X, does computer A allow me to do that? People make decisions based on positive selections… not negatives. If gaming isn’t on your requirements list — you aren’t actively rejecting gaming… you just don’t care one way or the other. The post I was replying to asserted that if you chose a Mac, then you’ve already decided not to do X and not to do Y and that you don’t want form factor Z. But that’s not it… decisions are made based on what you DO want. They aren’t made based upon what you don’t want. Some people just prefer one ecosystem over the other… it doesn’t mean that they don’t know that other options exist. It’s not ignorance, it’s just a different choice than you made. reply paulmd 10 hours agorootparentprev> do something the iPhone couldn't that they'd like to be able to do (like copy&paste or whatever) it is amazing how much the android crowd loves to shit on apple \"brainless sheeple\" etc given how little they clearly know about the products themselves. I keep bringing it up over and over and it's never not true, the android crowd is just so utterly uncivil and it's completely normalized and accepted as public discourse. The AMD fanbase has the exact same problem. It's constant \"brainless sheep\" and \"the ONLY reason anyone buys [not my brand] is [infantilizing and insulting remark goes here]\". If anyone on the other side did anything remotely like that they'd be slapped with a mod comment etc. But if you point it out, that people are misbehaving and acting out, you're the bad guy, because acknowledging the constant microaggressions is the greatest crime of all. reply datavirtue 8 hours agorootparentprev68 versions of hot garbage until you click on \"Business.\" Then you have to pay real money and you might as well buy a Mac. reply tcmart14 4 hours agorootparentprevAm I so glad though that Apple didn't buy into the touch for the laptops. Now this comes with a caveat, if you have some like the Yoga that the hinge can go all the way around, touch isn't bad. But god, in the early 2010s when everyone was throwing touch on their laptop screens, I hated that so much. Laptop hinge only goes 120 degrees or something like that, lets throw touch screen on it! horrible idea. Although I heard many complaints about the Yoga. Never owned one, but if I had a laptop with a touch screen, that seems like the route to go. The Surface too, but I've also heard those stop feeling snappy pretty quick and tons of thermal issues. reply coldtea 13 hours agorootparentprev>Then sure there's about 10 models. But at that point is it much complicated than say, choose from the DELL XPS line ? Yes. A thousand times more complicated. I usually get Apple gear for myself, but am always asked to help friends and relatives with PC laptop buying decisions... reply Spooky23 13 hours agorootparentprevPeople just shop by price and look. XPS and Yoga target specific segments of the market. The average punter buys whatever crap they have on sale at Target or Best Buy. reply skadamat 14 hours agoparentprevDefinitely agree with the simplicity of purchasing an Apple computer compared to other laptop manufacturers. Headphone brands and monitor makers also suffer from this same fate :/ reply Electricniko 6 hours agorootparentThere's still some weirdness as you get higher in the lineup. They start to break out into different variations of each chip, with varying degrees of memory for each variation. Like you can get a Macbook Pro with an M3 Pro chip with 11 core CPU/14 core GPU, or an M3 Pro chip 12/14 cores, or an M3 Max chip with 14/30 cores for $400 more or 16/40 for $900 more. And if you do the 14/30 M3 Max, you can choose 36 or 96gb memory, but if you choose the 16/40 Max, you can choose 48 64 or 128gb memory. reply tcmart14 4 hours agorootparentLove my macbooks that I've had, but yea. When shopping, it is rather aggrevating that there is no such thing really as just M3, M3 Pro, M3 Max, and M3 Ultra. Its really M3, M3 Pro (11/14/15), M3 Pro (12/18/16), M3 Pro (14/30/16), M3 Max(14/30/16), M3 Max (16/30/16), M3 Max (16/40/16). reply rchaud 14 hours agorootparentprevNot that simple when there are multiple generations of each on sale, with wildly different prices should you change the storage or RAM toggle. The MacBook Air used to have a multiple USB-A ports plus video, now it 2 ports that have to handle everything. So now the dongle/no dongle question has to become considered as well. reply gumby 13 hours agorootparent> The MacBook Air used to have a multiple USB-A ports plus video, now it 2 ports that have to handle everything. I doubt this is much of a constraint in the real world. Most people plug power in, perhaps an external mouse, and that's it. (They should be plugging in external storage for backups, which might require an extra port, but I doubt most people do in practice). > So now the dongle/no dongle question has to become considered as well. I'm pretty much USB-C only at this point, but even before then I never understood the fixation on \"dongles\". reply skadamat 14 hours agorootparentprevAgreed that even Apple products have gotten confusing. But still 1-2 orders of magnitude less I feel than other laptops? reply jpalomaki 12 hours agorootparentI think the lineup and config options on Apple laptops and tablets are carefully planned to make you spend more than you intially thought. You start with the idea of cheap model, then start going the ladders up. reply danieldk 12 hours agorootparentMaybe, but I know plenty of non-tech people that just buy the baseline MacBook Air and are happy with it. reply tcmart14 4 hours agorootparentI think both are true. For non-tech people, or people who don't use a computer for real hard work, who want a decent laptop, they probably just buy the baseline Air. But once you get into doing some more professional stuff on a laptop that needs power, then you fall into talking yourself into more than what you intended. I am experiencing that right now. At the time I didn't have a lot of money, working full time to support a family and going to school, so when I needed a new laptop I got the base MBA M1. Fantastic laptop. But now I am doing more intensive stuff (I also make a lot more now so I can afford it) on it and I am looking at upgrading to an M3. I am playing with the GPU and some ML, so I probably should get more than a base model M3. From there it is, whatever I decided, the next upgrade it just an extra $200. More ram would be nice. Oh wait, for another $200, I can also get the better processor with 2 more cores for CPU, GPU, and Neural Engine, why not? Oh wait, $200 more and I can double the RAM. Next thing you now, I started off with $1599 and now have talked myself into a like $2200 (haven't made the purchase yet, but that is what I am looking at). reply danieldk 12 hours agorootparentprevNot everything. MacBooks have MagSafe for power, which frees a port for power or having to use an adapter with power passthrough. Though it’s not a big issue in practice. When at home or the office, I just plug into a display with a USB or Thunderbolt hub. On-the-go, the Apple adapter works great. Having to plug more than one cable is annoying anyway when you move between desks. reply harkinian 11 hours agorootparentprevI hate dealing with USB-C, but on laptops, it hardly matters. Pretty rare that I'm plugging in anything besides power. reply michaelt 12 hours agoparentprev> it boggles my mind how most other companies (maybe Framework is an except) still have dozens or even 100s of SKUs for consumer laptops And the crazy thing is, despite Dell having 170+ laptop SKUs they don't use that fact to actually have a wide range of products. You'd think with 170 different SKUs they could produce an ultrabook with ports, wouldn't you? A modernised version of the E7270? Apparently not, though. reply slaymaker1907 13 hours agoparentprevI think System76 also has a pretty simple evaluation process. You mostly just select the form factor you want and then configure it. Also, unlike Apple, they make it easy to get a machine exactly tailored to your needs. They don't force you to pay $$$ for an expensive processor when you just want a bit larger SSD and some extra memory... reply rmbyrro 10 hours agoparentprevThe only non-user friendly defaults in the Mac purchasing flow are the 8G RAM and 256G SSD. I think in 2024, 16G & 512G should be the default, with an option to downgrade. With such bloated webapps now a days, those 8G of RAM are going to cook too fast... reply kmeisthax 4 hours agoparentprevHaving 100s of SKUs is an information denial tactic to ensure that users forfeit all of their consumer surplus[0] and pay exactly what they are willing to pay in the hopes that they get a \"good one\". The problem with this tactic is that there's a lot of SKUs that give people a terrible experience and they jump to another brand. Apple's solution to this is to instead have 50 SKUs, organize them by a few very easily understandable categories, and then price every SKU exactly within $50-$75 of one another so that there's always a meaningful upgrade for slightly more money. This is also why Apple is very stingy with storage and RAM. They use the cost of upgrades to pull you to higher priced SKUs, which then need their own upgrades, and OH LOOK there's an even nicer base model for just a little more! [0] The amount of money you save when the thing you want to buy turns out to be cheaper than what you were willing to pay. reply bmitc 14 hours agoparentprevHave you ever tried to trade in an Apple prdoduct? They ask you to enter the serial number and then bizarrely ask you to indentify to device. You basically have to refer to MacRumors to get it right. Apple has the same problem, if not worse. Dell has XPS 13, XPS 15, and XPS 17 and now the plus designation. It's pretty easy. reply wlesieutre 13 hours agorootparentDisregarding that there are 10 different laptop product lines to choose from, if I've already somehow decided that what I want is an XPS and I want a 13 inch screen size, my first two search results are - XPS 13 Laptop or - XPS 13 Laptop https://i.imgur.com/2SHL91Y.png I gather that one of these is a newer revision than the other, but it's a lot more confusing than \"M2\" and \"M3\". I need to know whether I want (up to) a Core Ultra 7 155H vs a Core i7-1250U, and whether (up to) Intel Arc Graphics is better than Intel Iris Xe graphics. Scrolling down further adds the XPS 13 Plus and XPS 13 2-in-1 Laptop. How does XPS 13 Plus compared to XPS 13 Laptop? What about to the other XPS 13 Laptop, is it better than both? Or is this a weird side-grade where you get a different form factor which is in some ways nicer, but then also comes with all the dumb parts of the Apple's \"Touch Bar\" and none of the good parts? (that's my 10 second interpretation of the product, but more clueless customers will have absolutely no idea) reply bufferoverflow 8 hours agorootparentIsn't it the same with Apple? You choose between Air or Pro 14\" or Pro 16\". But then there are all the variations with different amounts of RAM, SSD, processors (M3, M3 Pro, M3 Max) And every little upgrade costs you 10X compared to a PC laptop. reply harkinian 11 hours agorootparentprevI like how one is Intel i7 and one is Intel 7. Mind that the i7 is a -U, which is totally different from a -H or a desktop i7. reply CharlesW 13 hours agorootparentprev> Have you ever tried to trade in an Apple product? They ask you to enter the serial number and then bizarrely ask you to indentify to device. Yes, Apple changed to a randomized serial number format in 2021. https://www.macrumors.com/2021/04/14/apple-preapres-for-rand... So, if you have a device made after that transition and Apple doesn't already know the details (e.g. because you didn't buy it direct), they'll also need to know how much RAM and SSD space it has. reply AnthonyMouse 13 hours agorootparentBut they manufactured it. They couldn't keep a database mapping serial numbers to specs? reply CharlesW 12 hours agorootparentThat's a great point. My guess is that Apple doesn't share that data with their trade-in partners, which would include the web-based trade-in estimator. I don't recall having to share this when I brought stuff into an Apple Store for an in-person trade-in. reply nebula8804 12 hours agorootparentprevI just tried their Apple \"Trade In\" Tool. They ask for serial which you can copy and paste from the \"About this Mac\" dialog box that is in MacOS. From there it asks you the year of your laptop which is also in the same dialog box. From there it asks you which CPU version and core count you have (for M series laptops with multiple options.) To get this info, you click on \"More Info\" on the same dialog box(In Sonoma you also click System Report and it is all there). Afterwards it just asks the condition of the laptop (ie, does it turn on, screen cracked etc.) I don't see why you would need MacRumors for this. reply rad_gruchalski 13 hours agorootparentprevHuh, the xps itself has 36 SKUs in Germany. Then there’s Latitude, Inspiron, Vostro, Alienware, Gaming Pro, Mobile Precision Workstation… reply superb_dev 13 hours agorootparentprevI’ve done a trade in a few times with Apple and it’s always been simple. There’s a serial number on the device, or you could just select it from devices attached to your account reply nozzlegear 8 hours agorootparentprevActually I just did this the other day with an iPad Pro and it was kinda neat. Instead of asking me to enter the serial number, it said something like “it’s this device” or “use this device”. After I tapped that it just continued on and asked me about the condition of the iPad. I have seen that serial number prompt before though, I don’t know what makes it ask for a serial number versus prompting to use the current device’s serial number. I’m not even sure how it knew what device I was on to be honest. reply jb1991 13 hours agorootparentprevI've traded in multiple Apple computers over the years and never had to refer to some external site like you say. reply whalesalad 13 hours agorootparentprevYou can identify your machine here: https://checkcoverage.apple.com It's also available inside of the about this mac screen. Multiple orders of magnitude easier than the PC laptop space. reply quenix 14 hours agorootparentprevI've traded in multiple and only ever had to enter the serial number. reply AnthonyMouse 13 hours agoparentprev> I didn't enjoy the process of looking through dozens of various lines that Dell has and then other companies like Lenovo and HP earlier in the process, just to find a \"mid-range usable computer with a decent screen\". Newegg's feature selector is pretty good at sorting through this. Just uncheck all the bad screen resolutions and CPU models and see what's left. Bonus: Require at least 32GB of memory in an exact power of two, excluding all the junk that solders 8GB to the system board. reply paulmd 4 hours agorootparentIt is perfectly possible to have dual-channel 12+12GB configurations and also unbalanced 8+24GB configurations, so your test is not really accurate. reply AnthonyMouse 1 hour agorootparent12+12GB doesn't result in a power of 2. 8+24GB does, but there are no 24GB DDR4 SODIMMs to make it with, so that's only possible with DDR5. Moreover, even though that is possible, you can then look at the specs to confirm that it isn't the case, having already filtered out all of the junk where it clearly is the case, e.g. when the machine has 40GB. reply sidkshatriya 5 hours agoparentprev> I was recently buying a Dell laptop for my sister and it boggles my mind how most other companies (maybe Framework is an except) still have dozens or even 100s of SKUs for consumer laptops. If you remember back in the day, Nokia also had a crazy number of SKUs for their phones. Nokia is no longer the power it used to be. Could it mean many SKUS means a lack of focus ? Thinking you can out market / out segment your competition rather than try to concentrate more on the product ? I find AMD also has less SKUs than Intel. Here, as a challenger you can't really afford to segment the market as much as the leader. You need to concentrate your offerings in a few potent products. reply freeAgent 11 hours agoparentprevIf a normal person just wants a “simple laptop,” they can go to Best Buy or whatever brick and mortar and pull one off the shelf. They don’t need to dig into hundreds of SKUs unless they want to do so. reply colmmacc 3 hours agoparentprevI've always assumed that the number of SKUs is so that retailers can have a (relatively) unique model and avoid price comparisons. Like mattresses. reply harywilke 10 hours agoparentprevReally surprised not to see any mention of the 2x2 grid Jobs introduced in 1998: Pro, Consumer, Desktop, and Portable. It was the solution to exactly this problem. https://www.youtube.com/watch?v=10cZg8pLmXk about 6 minutes in. reply agumonkey 13 hours agoparentprevEven at a logistical / production level.. having all theses different units, options, suppliers, information, incompatibilities, tests, after sales.. insane. reply yodsanklai 11 hours agoparentprevThis will sound slightly provocative but I genuinely wonder: why would anyone buy anything else than an apple laptop? Gaming? ideology? budget constraints? lack of familiarity with MacOS? They are marginally more expensive, but they also very easy to sell second-hand. I'm speculating that the monthly cost is on par with a PC. reply Dah00n 11 hours agorootparentLots of people doesn't see Apple laptops like you obviously do. Ask provocative questions and you'll get likewise answers. For my needs and my opinions: - The software is worse. Linux is better. Windows has much broader options. I run both. - I game. - Ideology? Yes, Apple is an awful company. - Familiarity? I have used it enough to know it cannot do a lot of things I need, want, like, etc. - Budget? Yes, but not because it is too expensive, but because it cannot do anywhere near what Linux and Windows can do (for me) for way less. >I'm speculating that the monthly cost is on par with a PC. What is the monthly cost of a Mac that can run games, run old software I require for work and hobbies and (importantly) isn't locked down in either hardware or software so I can use it for something completely different later in life? reply cbsmith 9 hours agorootparent> What is the monthly cost of a Mac that can run games, run old software I require for work and hobbies and (importantly) isn't locked down in either hardware or software so I can use it for something completely different later in life? Yeah, the Mac model for long term use is that you sell it later in life so you can get whatever it is that you need later in life. (Not saying that it is a good thing nor a bad thing). reply harkinian 11 hours agorootparentprevIf you need Windows programs for your work, or need the ports, or want to play games, that kinda answers the question. The Mac laptops are otherwise just better for most people. And the often-repeated \"every user is different\" is not really true; most people fit the mold. reply Dah00n 11 hours agorootparent>most people fit the mold. Which mould though? I'd say most fit the Windows mould, but I'm guessing you mean the Apple mould. reply thomaslord 11 hours agorootparentprevI don't like MacOS these days, I like the idea of being able to repair my computer if something breaks, and I want the option to at least attempt data recovery if I have a drive failure. reply skydhash 5 hours agorootparentWhile I can sympathize with you, I'm not seeing myself repairing my MBA. It would be like trying to repair an F1 car. Not doable for the average person and even the constructor just swap the broken piece. I also backup everything important and encrypt the whole disk. I'd say the tradeoffs are worth it for the combination of lightness, quiet, performance, display and battery life I got. It's the perfect portable device for general purpose computing. reply cbsmith 9 hours agorootparentprev> They are marginally more expensive, but they also very easy to sell second-hand. I'm speculating that the monthly cost is on par with a PC. They aren't marginally more expensive. I'm writing this on a Chromebook I bought for $300 before the pandemic. Including electricity, cables, etc., I figure it has cost me about $6.50/month. reply whatisyour 10 hours agorootparentprevReasons for buying other laptop (at least for developers): 1. Better choice of desktop environment (KDE/GNOME vs OS X) 2. Wider/better selection of applications 3. Better development environment 4. Ease of deployment of your own apps 5. Better fit for your budget (why spend 3000 Euros on a limited set of features, when you can spend the same amount and get huge number of features/better features) 6. Capability to connect upto 3 external displays (which Macbook has got only recently) reply pquki4 9 hours agorootparentprevBased on \"marginally more expensive\", I have to say you live in a bubble and likely won't understand the replies. Just to point out one fact -- people buy $289 Gateway laptops from Walmart and use it as their daily driver. reply croes 14 hours agoparentprevIt pretty easy with no competitor for the MacOS segment. All those numbers try to hide that they basically sell all the same. reply varispeed 11 hours agoparentprevIs your system your enemy? Serious question. Having suffered through many Dell laptops (flagships) I'd rather flush the money down the pan than buy one machine from them. What Dell is selling in comparison to Apple is legacy technology with shoddy workmanship. reply numbers 10 hours agorootparentmy sister needed a laptop that runs windows so dell was one of the many options reply grow2grow 8 hours agoparentprevThey do it so that the consumer can't price match nor return swap from one retailer to another. The model number ultimately becomes a type of trace to reveal which retailer sold the device. reply izacus 14 hours agoparentprevOh come on, you're overexaggerating a bit. If you follow your pattern with Apple, you'll end up with a measly 8GB/256GB model which will only be useful for basic browsing. (And with more and more Electron apps, might struggle even with that once you hop onto a video call.) reply technothrasher 14 hours agorootparent> you'll end up with a measly 8GB/256GB model which will only be useful for basic browsing. I must be doing something wrong then. I've got one of those measly models and I do quite a bit more than just basic browsing without any problem. Video calls are the least of that, and they work fine. reply sva_ 12 hours agorootparent> I'll have you know, my 2023 $1300 laptop is capable of doing video calls Weird flex reply AnthonyMouse 12 hours agorootparentprevVideo calls require trivial amounts of memory and no real storage at all. Video editing, on the other hand, would pretty quickly fill up a 256GB drive. If you want to play with the fun new AI stuff, 8GB of RAM isn't enough. Modern machines also have a nefarious failure mode. It used to be that you needed more memory to cache the hard drive, but SSDs are pretty fast and that doesn't matter as much anymore. So now you have the opposite problem -- if you're out of memory and start swapping you don't notice as much, because SSDs are pretty fast. Except that now you're silently wearing out your SSD. Which in the Macs, is soldered. reply 015a 13 hours agorootparentprevAnyone who says the 8/256 Airs are throwaway machines has literally never used one. Full stop. You have an opinion about what 8gb of memory is capable of, which is influenced entirely by Intel CPUs and Windows. It hits different on the M1 platform and MacOS. reply lcmatt 14 hours agorootparentprevI have the base level M2 Air and it’s anything but a basic browsing machine. Runs everything I throw at it development wise, while a good few other things are open and it has never felt slow. Compare that to any Windows laptops with the same spec and it would be chugging along with just Chrome open. reply Dah00n 11 hours agorootparent>chugging along with just Chrome open This is just FUD. Even my kids 2017 laptop doesn't \"chug along with just Chrome open\". It can run chrome and a game too just fine. reply superb_dev 13 hours agorootparentprevI do all of my development on a measly 8GB/500GB model. The only application that has performance problems for me VCVRack, and that’s only after I surpassed 900 modules reply musicale 3 hours agorootparentI bet it struggles if you open 800 browser tabs and 900 VCV rack modules. reply sgarland 9 hours agorootparentprevWhile they do swap more often than one with more RAM (obviously), at least with the M1s, the SSDs are stupidly fast, to the point that you barely notice it for day-to-day work. They nerfed the SSD on M2 base models; not sure about M3. For the record, on my base M1 Air, I generally have Safari, Spotif",
    "originSummary": [
      "Apple unveils new 13- and 15-inch MacBook Air models powered by the M3 chip, providing enhanced performance, extended battery life, and upgraded AI functions.",
      "The laptops boast a sleek design, vibrant display, and are offered in four color variations, catering to users looking for style and functionality.",
      "Featuring support for external monitors, Wi-Fi 6E connectivity, enhanced camera features, microphones, and speakers, the MacBook Air delivers a diverse and trustworthy option for those in need of efficiency, portability, and advanced features."
    ],
    "commentSummary": [
      "The discussion covers diverse topics on Apple laptops, such as the launch of new MacBook Air models featuring the M3 chip and user feedback on various MacBook models, RAM capacities, and comparisons with ThinkPads.",
      "Users express their preferences and critiques on different laptop brands, emphasizing the significance of selecting a laptop that aligns with individual requirements and tastes.",
      "The debate highlights the strengths and limitations of varying laptops, empowering users to choose the best-fitting option for their needs."
    ],
    "points": 374,
    "commentCount": 915,
    "retryCount": 0,
    "time": 1709557238
  },
  {
    "id": 39593256,
    "title": "Opus 1.5: Machine Learning Boosts Audio Quality",
    "originLink": "https://opus-codec.org/demo/opus-1.5/",
    "originBody": "Opus 1.5 Released (Other Xiph demos) Opus gets another major update with the release of version 1.5. This release brings quality improvements, including ML-based ones, while remaining fully compatible with RFC 6716. Here are some of the most noteworthy upgrades. Opus Gets a Serious Machine Learning Upgrade This 1.5 release is unlike any of the previous ones. It brings many new features that can improve quality and the general audio experience. That is achieved through machine learning. Although Opus has included machine learning — and even deep learning — before (e.g. for speech/music detection), this is the first time it has used deep learning techniques to process or generate the signals themselves. Instead of designing a new ML-based codec from scratch, we prefer to improve Opus in a fully-compatible way. That is an important design goal for ML in Opus. Not only does that ensure Opus keeps working on older/slower devices, but it also provides an easy upgrade path. Deploying a new codec can be a long, painful process. Compatibility means that older and newer versions of Opus can coexist, while still providing the benefits of the new version when available. Deep learning also often gets associated with powerful GPUs, but in Opus, we have optimized everything such that it easily runs on most CPUs, including phones. We have been careful to avoid huge models (unlike LLMs with their hundreds of billions of parameters!). In the end, most users should not notice the extra cost, but people using older (5+ years) phones or microcontrollers might. For that reason, all new ML-based features are disabled by default in Opus 1.5. They require both a compile-time switch (for size reasons) and then a run-time switch (for CPU reasons). The following sections describe the new features enabled by ML. Dealing with Packet Loss Packet loss is one of the main annoyances one can encounter during a call. It does not matter how good the codec is if the packets do not get through. That's why most codecs have packet loss concealment (PLC) that can fill in for missing packets with plausible audio that just extrapolates what was being said and avoids leaving a hole in the audio (a common thing to hear with Bluetooth headsets). PLC is a place where ML can help a lot. Instead of using carefully hand-tuned concealment heuristics, we can just let a Deep Neural Network (DNN) do it. The technical details are in our Interspeech 2022 paper, for which we got the second place in the Audio Deep Packet Loss Concealment Challenge. When building Opus, using --enable-deep-plc will compile in the deep PLC code at a cost of about 1 MB in binary size. To actually enable it at run time, you will need to set the decoder complexity to 5 or more. Previously, only the encoder had a complexity knob, but the decoder is now getting one too. It can be set with the -dec_complexity option to opus_demo, or OPUS_SET_COMPLEXITY() in the API (like for the encoder). The extra complexity from running PLC at a high loss rate is about 1% of a laptop CPU core. Because deep PLC only affects the decoder, turning it on does not have any compatibility implications. Deep REDundancy (DRED) PLC is great for filling up occasional missing packets, but unfortunately packets often go missing in bursts. When that happens, entire phonemes or words are lost. Of course, new generative models could easily be used to seamlessly fill any gap with very plausible words, but we believe it is good to have the listener hear the same words that were spoken. The way to achieve that is through redundancy. Opus already includes a low-bitrate redundancy (LBRR) mechanism to transmit every speech frame twice, but only twice. While this helps reduce the impact of loss, there's only so much it can do for long bursts. That is where ML can help. We were certainly not the first to think about using ML to make a very low bitrate speech codec. However (we think) we are the first to design one that is optimized solely for transmitting redundancy. A regular codec needs to have short packets (typically 20 ms) to keep the latency low and it has to limit its use of prediction specifically to avoid making the packet loss problem even worse. For redundancy, we don't have these problems. Each packet will contain a large (up to 1 second) chunk of redundant audio that will be transmitted all at once. Taking advantage of that, the Opus Deep REDundancy (DRED) uses a rate-distortion-optimized variational autoencoder (RDO-VAE) to efficiently compress acoustic parameters in such a way that it can transmit one second of redundancy with about 12-32 kb/s overhead. Every 20-ms packet is effectively transmitted 50 times at a cost similar to the existing LBRR. See this demo for a high-level overview of the science behind DRED, or read the ICASSP 2023 paper for all the details and math behind it. Subjective testing (MOS) results measuring the improvement provided by DRED with one second redundancy for a range of realistic packet loss conditions. The results show that DRED achieves much higher quality than what either neural PLC alone, or LBRR with neural PLC can achieve. When DRED is combined with LBRR, the quality approaches that of the no-loss case. In these tests, we used 24 kb/s for the base Opus layer, 16 kb/s extra for LBRR, and 32 kb/s extra for DRED. Use the --enable-dred configure option (which automatically turns on --enable-deep-plc) to enable DRED. Doing so increases the binary size by about 2 MB, with a run-time cost around 1% like for deep PLC. Beware that DRED is not yet standardized and the version included in Opus 1.5 will not be compatible with the final version. That being said, it is still safe to experiment with it in applications since the bitstream carries an experiment version number and any version incompatibility will be detected and simply cause the DRED payload to be ignored (no erroneous decoding or loud noises). Neural Vocoder The very low complexity of deep PLC and DRED is made possible by new neural vocoder technology we created specifically for this project. The original papers linked above used a highly-optimized version of the original LPCNet vocoder, but even that was not quite fast enough. So we came up with a new framewise autoregressive generative adversarial network (FARGAN) vocoder that uses pitch prediction to achieve a complexity of 600 MFLOPS: 1/5 of LPCNet. That allows it to run with less than 1% of a CPU core on laptops or even recent phones. We don't yet have a paper or writeup on FARGAN, but we are working on fixing that. Low-Bitrate Speech Quality Enhancement Given enough bits, most speech codecs — including Opus — are able to reach a quality level close to transparency. Unfortunately, the real world sometimes doesn't give us \"enough bits\". Suddenly, the coding artifacts can become audible, or even annoying. The classical approach to mitigate this problem is to apply simple, handcrafted postfilters that reshape the coding noise to make it less noticeable. While those postfilters usually provide a noticeable improvement, their effectiveness is limited. They can't work wonders. The rise of ML and DNNs has produced a number of new and much more powerful enhancement methods, but these are typically large, high in complexity, and cause additional decoder delay. Instead, we went for a different approach: start with the tried-and-true postfilter idea and sprinkle just enough DNN magic on top of it. Opus 1.5 includes two enhancement methods: the Linear Adaptive Coding Enhancer (LACE) and a Non-Linear variation (NoLACE). From the signal point of view, LACE is very similar to a classical postfilter. The difference comes from a DNN that optimizes the postfilter coefficients on-the-fly based on all the data available to the decoder. The audio itself never goes through the DNN. The result is a small and very-low-complexity model (by DNN standards) that can run even on older phones. An explanation of the internals of LACE is given in this short video presentation and more technical details can be found in the corresponding WASPAA 2023 paper. NoLACE is an extension of LACE that requires more computation but is also much more powerful due to extra non-linear signal processing. It still runs without significant overhead on recent laptop and smartphone CPUs. Technical details about NoLACE are given in the corresponding ICASSP 2024 paper. Subjective testing (MOS) results comparing the speech decoded from the default decoder to the enhanced speech produced by LACE and NoLACE from that same decoder. The uncompressed speech has a MOS of 4.06. The results show that using NoLACE, Opus is now perfectly usable down to 6 kb/s. At 9 kb/s, NoLACE-enhanced speech is already close to transparency, and better than the non-enhanced 12 kb/s. To try LACE and NoLACE, just add the --enable-osce configure flag when building Opus. Then, to enable LACE at run-time, set the decoder complexity to 6. Set it to 7 or higher to enable NoLACE instead of LACE. Building with --enable-osce increases the binary size by about 1.6 MB, roughly 0.5 MB for LACE and 1.1 MB for NoLACE. The LACE model has a complexity of 100 MFLOPS which leads to a run-time cost of ~0.15% CPU usage. The NoLACE model has a complexity of 400 MFLOPS which corresponds to a run-time cost of ~0.75% CPU usage. LACE and NoLACE are currently only applied when the frame size is 20 ms (the default) and the bandwidth is at least wideband. Although LACE and NoLACE have not yet been standardized, turning them on does not have compatibility implications since the enhancements are independent of the encoder. Samples OK, nice graphs, but how does it actually sound? The following samples demonstrate the effect of LACE or NoLACE on Opus wideband speech quality at different bitrates. We recommend listening with good headphones, especially for higher bitrates. Select sample Female Male Select enhancement None LACE NoLACE Uncompressed Select bitrate 6 kb/s 9 kb/s 12 kb/s Select where to start playing when selecting a new sample Keep playing Set current position as restart point Player will restart at 0.00 seconds when changing sample. Demonstrating the effect of LACE and NoLACE on speech quality at 6, 9, and 12 kb/s. WebRTC Integration Using the deep PLC or the quality enhancements should typically require only minor code changes. DRED is an entirely different story. It requires closer integration with the jitter buffer to ensure that redundancy gets used. In a real-time communications system, the size of the jitter buffer determines the maximum amount of packet arrival lateness that can be tolerated without producing an audible gap in audio playout. In the case of packet loss, we can treat the DRED data similarly to late arriving audio packets. We take care to only insert this data into the jitter buffer if we have observed prior loss. In ideal circumstances, an adaptive jitter buffer (like NetEq used in WebRTC) will try to minimize its size in order to preserve interactive latency. If data arrives too late for playback, there will be an audible gap, but the buffer will then grow to accommodate the new nominal lateness. If network conditions improve the buffer can shrink back down, using time scaling to play the audio at a slightly faster rate. In the case of DRED, there will always be a loss vs. latency tradeoff. In order to make use of the DRED data and cover prior lost packets, we will need to tolerate a larger jitter buffer. But because we treat DRED similarly to late packet arrival, we can take advantage of the existing adaptation in NetEq to provide a reasonable compromise in loss vs. latency. You can try out DRED using the patches in our webrtc-opus-ng fork of the Google WebRTC repository. Using these patches, we were able to evaluate how DRED compares to other approaches. And yes, it still works well even with 90% loss. See the results below. Objective evaluation of different redundancy schemes under simulated realistic packet loss (see Realistic Loss Simulator below) using Microsoft's PLCMOS v2 (higher is better). All conditions use 48 kb/s, except for DRED+LBRR which uses 64 kb/s to fully take advantage of both forms of redundancy. Results show that even under extremely lossy conditions, DRED is able to maintain acceptable quality. It may look strange that the DRED quality increases past 60% loss, but that can be explained by the reduced amount of switching between regular packets and DRED redundancy. Samples Of course, hearing is believing, so here are some samples produced with the WebRTC patches. These should be close to what one might experience during a meeting when packets start to drop. Notice some gaps at the beginning as the jitter buffer adapts and is then able to take full advantage of DRED. Select loss rate 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% Select redundancy None (48 kb/s) LBRR (48 kb/s) DRED (48 kb/s) DRED+LBRR (64 kb/s) Select where to start playing when selecting a new sample Keep playing Set current position as restart point Player will restart at 0.00 seconds when changing sample. Evaluating the effectiveness of the different redundancy options. These audio samples are generated using real packet loss traces with the entire WebRTC stack. IETF and Standardization To ensure compatibility with the existing standard and future extensions of Opus, this work is being conducted within the newly-created IETF mlcodec working group. This effort is currently focused on three topics: a generic extension mechanism for Opus, deep redundancy, and speech coding enhancement. Extension Format The new DRED mechanism requires adding extra information to Opus packets while allowing an older decoder that does not know about DRED to still decode the regular Opus data. We found that the best way to achieve that was through the Opus padding mechanism. In the original specification, padding was added to make it possible to make a packet bigger if needed (e.g., to meet a constant bitrate even when the encoder produced fewer bits than the target). Thanks to padding, we can transmit extra information in a packet in a way that an older decoder will just not see (so it won't get confused). Of course, if we're going to all that trouble, we might as well make sure we're also able to handle any future extensions. Our Opus extension Internet-Draft defines a format within the Opus padding that can be used to transmit both deep redundancy, but also any future extension that may become useful. See our presentation at IETF 118 for diagrams of how the extensions fit within an Opus packet. DRED Bitstream We are also working on standardizing DRED. Standardizing an ML algorithm is challenging because of the tradeoff between compatibility and extensibility. That's why our DRED Internet-Draft describes how to decode extension bits into acoustic features, but leaves implementers free to make both better encoders and also better vocoders that may further improve on the quality and/or complexity. Enhancement For enhancement, we also follow the general strategy to standardize as little as possible, since we also expect future research to produce better methods than we currently have. That's why we will specify requirements an enhancement method like LACE or NoLACE should satisfy in order to be allowed in an opus decoder rather than specifying the methods themselves. A corresponding enhancement Internet-Draft has already been created for that purpose. Other Improvements Here are briefly some other changes in this release. AVX2 Support Opus now has support and run-time detection for AVX2. On machines that support AVX2/FMA (from around 2015 or newer), both the new DNN code and the SILK encoder will be significantly faster thanks to the use of 256-bit SIMD. More NEON Optimizations Existing ARMv7 Neon optimization were re-enabled for AArch64, resulting in more efficient encoding. The new DNN code can now take advantage of the Arm dot product extensions that significantly speed up 8-bit integer dot products on a Cortex-A75 or newer (~5 year old phones). Support is detected at run-time, so these optimizations are safe on all Arm CPUs. Realistic Loss Simulator As a side effect of trying to tune the DRED encoder to maximize quality, we realized we needed a better way of simulating packet loss. For some purposes, testing with random loss patterns (like tossing a coin repeatedly) can be good enough, but since DRED is specifically designed to handle bust loss (which is rare with independent random losses) we needed something better. As part of the Audio Deep Packet Loss Concealment Challenge, Microsoft made available some more realistic recorded packet loss traces. A drawback of such real data is that one cannot control the percentage of loss or generate sequences longer than those in the dataset. So we trained a generative packet loss model that can simulate realistic losses with a certain target overall percentage of loss. Packet loss traces are quite simple and our generative model fits in fewer than 10,000 parameters. To simulate loss with opus_demo, you need to build with --enable-lossgen. Then add -sim-lossto the opus_demo command line. Note that the loss generator is just an initial design, so feedback is welcome. Because we believe that this loss generator can be useful to other applications, we have made it easy to extract it from Opus and use it in other applications. The main source file for the generator is dnn/lossgen.c. Comments in the file contain information about the other dependencies needed for the loss generator. Conclusion We hope we demonstrated how our new ML-based tools substantially improve error robustness and speech quality with a very modest performance impact and without sacrificing compatibility. And we're only getting started. There's still more to come. We encourage everyone to try out these new features for themselves. Please let us know about your experience (good or bad) so we can continue to improve them. Enjoy! —The Opus development team March 4th, 2024 Additional Resources First and foremost: The Opus Project Homepage The basic Opus techniques for music coding are described in the AES paper: High-Quality, Low-Delay Music Coding in the Opus Codec The basic Opus techniques for speech coding are described in this other AES paper: Voice Coding with Opus Join our development discussion in #opus at irc.libera.chat (→web interface) (C) Copyright 2024 Xiph.Org Foundation",
    "commentLink": "https://news.ycombinator.com/item?id=39593256",
    "commentBody": "Opus 1.5 released: Opus gets a machine learning upgrade (opus-codec.org)345 points by summm 16 hours agohidepastfavorite115 comments yalok 13 hours agoThe main limitation for such codecs is CPU/battery life - and I like how they sparsely applied ML in it here and there, combining it with classic approach (non-ML algos) to achieve better tradeoff of CPU vs quality. E.g. for better low bitrate support/LACE - \"we went for a different approach: start with the tried-and-true postfilter idea and sprinkle just enough DNN magic on top of it.\" The key was not to feed raw audio samples to the NN - \"The audio itself never goes through the DNN. The result is a small and very-low-complexity model (by DNN standards) that can run even on older phones.\" Looks like the right direction for embedded algos and it seems to be a pretty unexplored one, as compared to the current fashion to do ML E2E. reply indolering 5 hours agoparentIt's a really smart application of ML: helping around the edges and not letting the ML algo invent pheonems or even whole words by accident. ML transcription has a similar trade-off of performing better on some benchmarks but also hallucinating results. reply kolinko 3 hours agorootparentA nice story about Xerox discovering this issue in 2003, when their copiers began slightly changing random numbers in copied documents https://www.theverge.com/2013/8/6/4594482/xerox-copiers-rand... reply cedilla 1 hour agorootparentI don't think machine learning was involved there at all. As I understand it, it was an issue of a specifically implemented feature (reusing a single picture of a glyph for all other instances to save space) being turned on in archive-grade settings, despite the manual stating otherwise. reply h4x0rr 2 hours agorootparentprevhttps://youtu.be/zXXmhxbQ-hk Interesting yet funny CCC video about this reply yalok 1 hour agorootparentprevfwiw, in ASR/speech transcription world, it looks reverse to me - in the past, there was lots of custom non-ML code & separate ML models for audio modeling and language modeling - but current SOTA ASRs are all e2e, and that's what's used even in mobile applications, iiuc. I still think the pendulum will swing back there again, to have even better battery/larger models on mobile. reply spacechild1 14 hours agoprevI'm using Opus as one of the main codecs in my peer-to-peer audio streaming library (https://git.iem.at/cm/aoo/ - still alpha), so this is very exciting news! I'll definitely play around with these new ML features! reply RossBencina 1 hour agoparent> peer-to-peer audio streaming library Interesting :) reply Dwedit 11 hours agoprevI just want to mention that getting such good speech quality at 9kbps by using NoLACE is absolutely insane. reply kristopolous 10 hours agoparentI wanted to see what it would sound like in comparison to a really early streaming audio codec, realaudio 1.0 $ ffmpeg -i female_ref.wav - acodec real_144 female_ref.ra And if you can't support that I put it back to wav and posted it: http://9ol.es/female_ref-ra.wav This was seen as \"14.4\" audio, for 14.4kb/s dialup in the mid-90s. The quality increase over those nearly 30 years for what you can get out of what's actually a fewer number of bytes is really impressive. reply recursive 5 hours agorootparentI don't know any of the details but maybe the CPUs of the time would have struggled to stream the decoding. reply anthk 7 hours agorootparentprevI used to listen opus avant agarde music from https://dir.xiph.org at 16kb/s under a 2G connection and it was usable once mplayer/mpv cached back the stream for nearly a minute. reply luplex 14 hours agoprevI wonder: did they address common ML ethics questions? Specifically: Are the ML algorithms better/worse on male than on female speech? How about different languages or dialects? Are they specifically tuned for speech at all, or do they also work well for music or birdsong? That said, the examples are impressive and I can't wait for this level of understandability to become standard in my calls. reply jmvalin 13 hours agoparentQuoting from our paper, training was done using \"205 hours of 16-kHz speech from a combination of TTS datasets including more than 900 speakers in 34 languages and dialects\". Mostly tested with English, but part of the idea of releasing early (none of that is standardized) is for people to try it out and report any issues. There's about equal male and female speakers, though codecs always have slight perceptual quality biases (in either direction) that depend on the pitch. Oh, and everything here is speech only. reply radarsat1 13 hours agoparentprevThis is an important question. However, I'd like to point out that similar biases can easily exist for non-ML, hand-tuned algorithms. Even in the latter case test sets and often even \"training\" and \"validation\" sets are used for finding good parameters. Any of these can be a source of bias, as can the ears of evaluators making these decisions. It's true that bias questions often come up in ML context because fundamentally these algorithms do not work without data, but _all_ algorithms are designed by people, and _many_ can involve data in setting their parameters. Both of which can be sources of bias. ML is more known for it, I believe, because the _inductive_ biases are less than in traditional algorithms, and therefore are more keen to adopt biases present in the dataset. reply MauranKilom 10 hours agorootparentAs a notable example, the MP3 format was hand-tuned to vocals based on \"Tom's Diner\" (i.e. a female voice). It has been accused of being biased towards female vocals as a result. reply thomastjeffery 12 hours agorootparentprevUsually regular algorithms aren't generating data that pretends to be raw data. That's the significant difference here. reply shwaj 7 hours agorootparentCan you precisely define what you mean by \"generating\" and \"pretends\", in such a way that this neural network does both these things, but a conventional modern audio codec doesn't? \"Pretends\" is a problematic choice of words, because it anthropomorphizes the algorithm. It would be more accurate and less misleading to replace \"pretends to be\" with \"approximates\". But then it wouldn't serve your goal of (seeming to) establish a categorical difference between this approach and \"regular algorithms\", because that's what a regular algorithm does too. I apologize, because the above might sound rude. It's not intended to be. reply samus 2 hours agorootparentprevNot really. Any lossy codec is generating data that pretends to be close to the raw data. reply unixhero 13 hours agoparentprevWhy is the ethics question important? It is a new feature for an audio codec, not a new material to teach in your kids curriculum. reply nextaccountic 7 hours agorootparentBecause this gets deployed in real world, affecting real people. Ethics don't exist only in kids curriculum. reply samus 2 hours agorootparentprevThis is actually a very technical question since it means the audio codec might simply not work that well in practice as it could and should. reply gcr 12 hours agorootparentprevThis is a great question! Here's a related failure case that I think illustrates the issue. In my country, public restroom facilities replaced all the buttons and levers on faucets, towel dispensers, etc. with sensors that detect your hand under the faucet. Black people tell me they aren't able to easily use these restrooms. I was surprised when I heard this, but if you google this, it's apparently a thing. Why does this happen? After all, the companies that made these products aren't obviously biased against black people (outwardly, anyway). So this sort of mistake must be easy to fall into, even for smart teams in good companies. The answer ultimately boils down to ignorance. When we make hand detector sensors for faucets, we typically calibrate them with white people in mind. Of course different skin tones have different albedo and different reflectance properties, so sensors are less likely to fire. Some black folks have a workaround where they hold a (white) napkin in their hand to get the faucet to work. How do we prevent this particular case from happening in the products we build? One approach is to ensure that the development teams for skin sensors have a wide variety of skin types. If the product development team had a black guy for example, he could say \"hey, this doesn't work with my skin, we need to tune the threshold.\" Another approach is to ensure that different skin types are reflected in the data used to fit the skin statistical models we use. Today's push for \"ethics in ML\" is borne out of this second path as a direct desire to avoid these sorts of problems. I like this handwashing example because it's immediately apparent to everyone. You don't have to \"prioritize DEI programs\" to understand the importance of making sure your skin detector works for all skin types. But, teams that already prioritize accessibility, user diversity, etc. are less likely to fall into these traps when conducting their ordinary business. For this audio codec, I could imagine that voices outside the \"standard English dialect\" (e.g. thick accents, different voices) might take more bytes to encode the same signal. That would raise bandwidth requirements, worsen latency, and increase data costs for these users. If the codec is designed for a standard American audience, that's less of an issue, but codecs work best when they fit reasonably well for all kinds of human physiology. reply TeMPOraL 1 hour agorootparentI get your point, but in the example used - and I can think of couple others that start with \"X replaced all controls with touch/voice/ML\" - the much bigger ethical question is why did they do it in the first place. The new solution may or may not be biased differently than the old one, but it's usually inferior to the previous ones or simpler alternatives. reply cma 11 hours agorootparentprevWhat if it is a pareto improvement: better improvement for some dialects but no worse than the earlier version for anyone. Should it be shelved or tuned down so all improvement for each dialect see gains by an exactly equal percentage? reply samus 2 hours agorootparentI would be very surprised if there is no improvement if the codec is biased towards particular dialects or other distinctive subsets of the data. And we could certainly be fine with some kinds of bias. Speech codecs are intended to transmit human speech after all. Not that of dogs, bats, or hypothetical extraterrestrials. On the other hand, a wider dataset might reduce overfitting and force the model to learn better. If the codec has the intention of working best for human voice in general, then it is simply not possible to define sensible subsets of the user base to optimize for. Curating an appropriate training set has therefore technical impact on the performance of the codec. Realistically, I admit that the percentages of speech samples of languages in such a dataset would be according to the relative amount of speakers. This is of course a very fuzzy number with many sources of systematic error (like what counts as one language, do non-native speakers count, which level of proficiency is considered relevant, etc.), and ultimately English is a bit more important since it is de-facto the international lingua franca of this era. In short, a good training set is important unless one opines that certain subsets of humanity will never ever use the codec, which is equivalent to being blind to the reality that more and more parts of the world are getting access to the internet. reply viraptor 10 hours agorootparentprevHere's a question that should have the same/similar answer: Increasingly some part of the job interviews is being handled over the internet. All other things being equal, people are likely to have a more positive response to candidates with more pleasant voice. So if new ML-enhanced codecs become more common, we may find that some group X has a just slightly worse quality score than others. Over enough samples that would translate to lower interview success rate for them. Do you think we should keep using that codec, because overall we get a better sound quality across all groups? Do you feel the same as a member of group X? reply shwaj 6 hours agorootparentI don't think it's a given that we shouldn't keep using that codec. For example, maybe the improvement is due to an open source hacker working in their spare time to make the world a better place. Do we tell them their contribution isn't welcome until it meets the community's benchmark for equity? Your same argument can also be used to degrade the performance for all other groups, so that group X isn't unfairly disadvantaged. Or, it can even be used to argue that the performance for other groups should be degraded to be even worse than group X, to compensate for other factors that disadvantage group X. This is argumentum ad absurdum, but it goes to show that the issue isn't as black and white as you seem to think it is. reply viraptor 5 hours agorootparentA person creating a codec doesn't choose if it's globally adopted. System implementors (like for example Slack) do. You're don't have to tell the open source dev anything. You don't owe them to include their implementation. And if their contribution was to the final system, sure, it's the owner's choice what the threshold for acceptable contribution is. In the same way they can set any other benchmark. > Your same argument can also be used to degrade the performance for all other groups, The context here was Pareto improvement. You're bringing a different situation. reply unethical_ban 12 hours agorootparentprevI get your point, but the questioner wasn't being rude or angry, only curious. I think it's a valid question, too. While it isn't as important to be neutral in this instance as, say, a crime prediction model or a hiring model, it should be boilerplate to consider ML inputs for identity neutrality. reply The_Colonel 12 hours agorootparentprevImagine you release a codec which optimizes for cis white male voice, every other kind of voice has perceptibly lower fidelity (at low bitrates). That would not go well... reply panzi 12 hours agorootparentYeah, imagine a low bitrate situation where only English speaking men can still communicate. That would create quite a power imbalance. reply overstay8930 12 hours agorootparentprevMeanwhile G.711 makes all dudes sound like disgruntled middle aged union workers reply numpad0 4 hours agorootparentprevNo offense/taken, but Codec2 seem to be affected a bit for this problem. reply dist-epoch 12 hours agorootparentprevnext [3 more] [flagged] gcr 11 hours agorootparentQuestions of \"which users can use my product and which can't\" certainly encompass political views to be sure, but they also extend further beyond. I don't see why you feel the need to argue from the position of \"woke devil's advocate\" here. For a politically-neutral example of why poor DEI practice can lead directly to worse products, see my sister comment about faucet sensors and black skin. reply astrange 12 hours agorootparentprev^- https://knowyourmeme.com/memes/conservatives-have-one-joke reply shrubble 12 hours agorootparentprevnext [5 more] [flagged] Edman274 12 hours agorootparentWhat people have historically called a \"gay lisp\" is actually a hyper-articulation of /s/ or /z/, and as you might expect, /s/ and /z/ have a lot of high frequency sounds in them. Weird as it sounds there is a possible scenario where an audio codec does a worse job reproducing the audio content of gay male speech compared to straight male speech. reply stevage 12 hours agorootparentprevhttps://en.wikipedia.org/wiki/Gay_male_speech?wprov=sfla1 HTH. reply p1esk 12 hours agorootparentprevAn ML model might be able to, even if you can’t. reply GaggiX 12 hours agorootparentAchieving the gaydar, do not give this technology to Saudi Arabia. reply kolinko 3 hours agoparentprevAs a person from a different language/accent who has to deal with this on a regular basis - having assistants like Siri not understand what I want to say, even though native speakers don't have such problem... Or before an advent of UTF - websites and apps ignoring special characters usable in my language. I wouldn't consider this a matter of ethics, and more of a technology limitations or ignorance. reply rhdunn 12 hours agoprevI find the interplay between audio codecs, speech synthesis, and speech recognition fascinating. Advancements in one usually results in advancements in the others. reply cedilla 1 hour agoprevThe quality at 80% package loss is incredible. It's straining to listen to but still understandable. reply travisporter 15 hours agoprevVery cool. seems like they addressed the problem of hallucination. would be interesting to see an example of it hallucinating without redundancy and corrected with redundancy reply CharlesW 15 hours agoparentIsn't packet loss concealment (PLC) a form of hallucination? Not saying it's bad, just that it's still Making Shit Up™ in a statistically-credible way. reply jmvalin 13 hours agorootparentWell, there's different ways to make things up. We decided against using a pure generative model to avoid making up phoneme or words. Instead, we predict the expected acoustic features (using a regression loss), which means that model is able to continue a vowel. If unsure it'll just pick the \"middle point\", which won't be something recognizable as a new word. That's in line with how traditional PLCs work. It just sounds better. The only generative part is the vocoder that reconstructs the waveform, but it's constrained to match the predicted spectrum so it can't hallucinate either. reply stevage 12 hours agorootparentAny demos of this to listen to? It sounds potentially really good. reply GaggiX 11 hours agorootparentThere is a demo in the link shared by OP. reply CharlesW 13 hours agorootparentprevThat's really cool. Congratulations on the release! reply derf_ 15 hours agorootparentprevThe PLC intentionally fades off after around 100 ms so as not to cause misleading hallucinations. It is really just about filling small gaps. reply skybrian 14 hours agorootparentprevIn a broader context, though, this happens all the time. You’d be surprised what people mishear in noisy conditions. (Or if they’re hard of hearing.) The only thing for it is to ask them to repeat back what they heard, when it matters. It might be an interesting test to compare what people mishear with and without this kind of compensation. reply jmvalin 13 hours agorootparentAs part of the packet loss challenge, there was an ASR word accuracy evaluation to see how PLC impacted intelligibility. See https://www.microsoft.com/en-us/research/academic-program/au... The good news is that we were able to improve intelligibility slightly compared with filling with zeros (it's also a lot less annoying to listen to). The bad news is that you can only do so much with PLC, which is why we then pursued the Deep Redundancy (DRED) idea. reply tialaramex 13 hours agorootparentprevRight, this is why the Proper radio calls for a lot of systems have mandatory read back steps, so that we're sure two humans have achieved a shared understanding regardless of how sure they are of what they heard. It not only matters whether you heard correctly, it also matters whether you understood correctly. e.g. train driver asks for an \"Up Fast\" block. His train is sat on Down Fast, the Up Fast is adjacent, so then he can walk on the (now safe) railway track and inspect his train at track level, which is exactly what he, knowing the fault he's investigating, was taught to do. Signaller hears \"Up Fast\" but thinks duh, stupid train driver forgot he's on Down Fast. He doesn't need a block, the signalling system knows the train is in the way and won't let the signaller route trains on that section. So the Up Fast line isn't made safe. If they leave the call here, both think they've achieved understanding but actually there is no shared understanding and that's a safety critical mistake. If they follow a read-back procedure they discover the mistake. \"So I have my Up Fast block?\" \"You're stopped on Down Fast, you don't need an Up Fast block\". \"I know that, I need Up Fast. I want to walk along the track!\" \"Oh! I see now, I am filling out the paperwork for you to take Up Fast\". Both humans now understand what's going on correctly. reply a_wild_dandan 14 hours agorootparentprevTo borrow from Joscha Bach: if you like the output, it's called creativity. If you don't, it's called a hallucination. reply Dylan16807 7 hours agorootparentDoesn't the context affect things much more than whether you like the particular results? Either way, \"creativity\" in the playback of my voice call is just as bad. reply Aachen 12 hours agorootparentprevThat sounds funny, but is it true? Certainly there's a bias that goes towards what you're quoting, but would you otherwise genuinely call the computer creative? Is that a positive aspect of a speech codec or of an information source? Creative is when you ask a neural net to create a poem, or something else from \"scratch\" (meant to be unique). Hallucination is when you didn't ask it to make its answer up but to recite or rephrase things it has directly observed That's my layman's understanding anyway, let me know if you agree reply skybrian 12 hours agorootparentThat's almost the same. You could say it's being creative by not following directions. Creativity isn't well-defined. If you generate things at random, they are all unique. If you then filter them to remove all the bad output, the result could be just as \"creative\" as anything someone could write. (In principle. In practice, it's not that easy.) And that's how evolution works. Many organisms have very \"creative\" designs. Filtering at scale, over a long enough period of time, is very powerful. Generative models are sort of like that in that they often use a random number generator as input, and they could generate thousands of possible outputs. So it's not clear why this couldn't be just as creative as anything else, in principle. The filtering step is often not that good, though. Sometimes it's done manually, and we call that cherry-picking. reply CharlesW 13 hours agorootparentprevI love that, what's it from? (My Google-fu failed.) Unexpected responses are often a joy when using AI in a creative context. https://www.cell.com/trends/neurosciences/abstract/S0166-223... reply a_wild_dandan 8 hours agorootparentIt was from one of his podcast appearances. Which doesn't narrow it down much, unfortunately. Most likely options: https://www.youtube.com/watch?v=LgwjcqhkOA4 https://www.youtube.com/watch?v=sIKbp3KcS8A https://www.youtube.com/watch?v=CcQMYNi9a2w reply h4x0rr 11 hours agoprevDoes this new Opus version close the gap to xHE-AAC, which is (was?) superior at lower bitrates? reply AzzyHN 5 hours agoparentDepends on whether you're encoding speech or music. reply brnt 11 hours agoprevWhat if there was a profiler or setting that helps to reencode existing lossy formats without introducing too many more artifacts? An sizeable collection runs into the issue, if the don't have (easily accessible) lossless masters. I'd be very interested if I could move a variety of mp3s, aacs and vorbis to Opus if I knew additional quality loss was minimal. reply nimish 7 hours agoprevThat 90% loss demo is bonkers. Completely comprehensible after maybe a second. reply brcmthrowaway 12 hours agoprevThis is game changing. When will H265 get a DL upgrade? reply WithinReason 10 hours agoprevSomeone should add an ML decoder to JPEG reply viraptor 9 hours agoparentYou can't do that much on the decoding side (apart from the equivalent of passing the normally decoded result through a low percent img2img ML) But the encoders are already there: https://medium.com/@migel_95925/supercharging-jpeg-with-mach... https://compression.ai/ reply WithinReason 9 hours agorootparentYou can more accurately invert the quantisation step reply viraptor 9 hours agorootparentYou can't do it more accurately. You can make up expected details which aren't encoded in the file. But that's explicitly less accurate. reply adgjlsfhk1 8 hours agorootparentIf the encoders know what model the decoders will be running, they can improve accuracy. You could pretty easily make a codec that doesn't encode high resolution detail if the decoder NN will interpolate it correctly. reply samus 1 hour agorootparentThat makes sense if the goal is lossless compression. Since JPEG is lossy, it is sufficient to consider the Pareto front between quality, compressed size, and encoding/decoding performance. reply viraptor 4 hours agorootparentprevThat's changing the encoder and sure, you could do that. But that's basically a new version of the format. It's not the JPEG we're using anymore + ML in decoder. It's JPEG-ML on both the encoder and decoder side. And with the speed that we adopt new image formats... That's going to take ages :( reply out_of_protocol 15 hours agoprevWhy the hell opus still not in Bluetooth? Well i know - sweet sweet license fees (aKKtually, there IS opus codec, supported by pixel phones - google made it for VR/AR stuff. No one uses it, there are about ~1 headphone with opus support ) reply lxgr 15 hours agoparentAs you already mention, it's already possible to use it. As for why hardware manufacturers don't actually use it, you can thank beautiful initiatives such as this: https://www.opuspool.com/ (previous HN discussion: https://news.ycombinator.com/item?id=33158475). reply dogma1138 14 hours agoparentprevOpus isn’t patent free, and what’s worse it’s not particularly clear who owns what. The biggest patent pool is currently OpusPool but it’s not the only one. https://www.opuspool.com/ reply pgeorgi 13 hours agorootparentNo codec (or any other technical development, really - edit: except for 20+ years old stuff, and only if you don't add any, even \"obvious\" improvements) is known patent free, or clear on \"who owns what.\" Folks set up pools all the time, but somehow they never offer indemnification for completeness of the pool - because they can't. See https://en.kangxin.com/html/2/218/219/220/11565.html for a few examples how the patent pool extortion scheme already went wrong in the past. reply xoa 13 hours agorootparentFWIW, submarine patents are long dead, so it is possible to feel assured that old enough stuff is patent free. Of course that denies a lot of important improvements, but due to diminishing returns and the ramp of tech development it's still ever more significant. A lot of key stuff is going to lose monopoly lock this decade. reply pgeorgi 13 hours agorootparentYou're right. I could still amend the post, so I added the 20+ years caveat. Thanks! reply dogma1138 13 hours agorootparentprevNo one said that Opus is the only one suffering from licensing ambiguity, but comparing it to say AptX and its variants which do have a clear one stop shop for licensing (Qualcomm) it’s a much riskier venture especially when it comes to hardware. reply pgeorgi 13 hours agorootparentA drive-by patent owner can show up on anything, and if they don't want to license to you, your entire product is bust. Even if it's AptX and Qualcomm issues you a license in exchange for money. I wouldn't even bet on being able to claw back these license costs after being ordered to destroy your AptX-equipped product after it ran into somebody else's patent. The risk that this happens is _exactly_ the same for Opus or AptX. reply tux3 13 hours agorootparentprevMaking a patent troll is just a matter of putting up a press release and a web page. I could claim to have a long list of patents against AptX. Anyone could. Of course I'm not willing to disclose the list of patents at this time, but customers looking to be extorted may contact me privately. reply rockdoe 10 hours agorootparentFhG and Dolby did eventually put up a list of patents you are licensing from them. It makes for some funny reading if you're familiar with the field. (This should not be construed as legal advice as to the validity of the pool) reply bdowling 10 hours agorootparentprevAt least in the U.S., anyone can look up all the patents a person/entity owns. So, your fraud wouldn’t get very far. https://assignment.uspto.gov/patent/index.html#/patent/searc... reply pgeorgi 9 hours agorootparent\"I represent the holders of the patents in question\" is simple enough. I wonder if it's fraud, if all you're putting out is an unverifiable claim on the net. The pool operators do that all the time. reply bdowling 44 minutes agorootparentSomeone who has no basis to bring a patent infringement claim selling a settlement of such a claim to an alleged infringer is clearly fraud. It’s like someone selling a deed to land they don’t own, or leasing a property they don’t own to a tenant. reply rockdoe 11 hours agorootparentprevOpus isn’t patent free The existence of a patent pool does not mean there are valid patent claims against it. But yes, you may be technically correct by saying \"patent free\" rather than \"not infringing on any valid patents\". That said historically Opus has had claims against it by patents that looked valid but upon closer investigation didn't actually cover what the codec does. Just looks like FUD to me. Meanwhile, the patent pools of competing technologies definitely still don't offer indemnification they cover all patents, but have no problem paying a bunch of people to spew exactly this kind of FUD - they're the ones who tried to set up this \"patent pool\" to begin with! reply giantrobot 14 hours agoparentprevThe BT SIG moves kind of slow and there's a really long tail of devices. Until there's a chip with native Opus support (that's as cheap as ones with AAC etc) you wouldn't get Opus support even if it was in the spec. Realistically for most headphones people actually buy AAC (LC and HE) is more than good enough encoding quality for the audio the headphones can produce. Even if Opus was in the spec and Opus-supporting chips were common there would still be a hojillion Bluetooth devices in the wild that wouldn't support it. It would be cool to have Opus in A2DP but it would take a BT SIG member that was really in love with it to get it in the profile. reply out_of_protocol 14 hours agorootparentThey chose to make totally new inferior LC3 codec though. Also, on my system (Android phone + BTR5/BTR15 Bluetooth DAC + Sennheiser H600) all options sound realy crappy compared to plain old usb, everything else is the same. LDAC 990kbps is less crappy, by sheer brute force. I suspect it's not only codec but other co-factors as well (like mandatory DSP on phone side) reply maep 10 minutes agorootparent\"Inferior\" is relative. The main focus of LC3 was, as the name suggests, complexity. This is hearsay: Bluetooth SIG considered Opus but rejected it because it was computationally too expensive. This came out of the hearing aid group, where battery life and complexity are a major restriction. So when you compare codecs in this space, the metric you want to look at is quality vs. CPU cycles. In that regard LC3 outperforms many contemporary codecs. Regarding sound quality it's simply a matter of setting the appropriate bitrate. So if Opus is transparent at 150 kbps, and LC3 at 250 kbps thats totally acceptable if that gives you more battery life. reply giantrobot 11 hours agorootparentprevI've got AirPods and a Beats headset so they both support AAC and to my ear sound great. Keep in mind I went to a lot of concerts in my 20s without earplugs so my hearing isn't necessarily the greatest anymore. AFAIK Android's AAC quality isn't that great so aptX and LDAC are the only real high quality options for Android and headphones. It's a shame as a lot of streaming is actually AAC bitstreams and can be passed directly through to headphones with no intermediate lossy re-encode. Like I said though, to get Opus support in A2DP a BT SIG member would really have to be in love with it. Qualcomm and Sony have put forward aptX and LDAC respectively in order to get licensing money on decoders. Since no one is going to get Opus royalties there's not much incentive for anyone to push for its inclusion in A2DP. reply aredox 10 hours agoprev>That's why most codecs have packet loss concealment (PLC) that can fill in for missing packets with plausible audio that just extrapolates what was being said and avoids leaving a hole in the audio ...How far can ML PLC \"hallucinate\" audio? A sound , a syllable, a whole word, half a sentence? Can I trust anymore what I hear? reply jmvalin 9 hours agoparentWhat the PLC does is (vaguely) equivalent to momentarily freezing the image rather than showing a blank screen when packets are lost. If you're in the middle of a vowel, it'll continue the vowel (trying to follow the right energy) for about 100 ms before fading out. It's explicitly designed not to make up anything you didn't say -- for obvious reasons. reply samus 1 hour agoparentprevYou never can when lossy compression is involved. It is commonly considered good practice to verify that the communication partner understood what was said, e.g., by restating, summarizing, asking for clarification, follow-up questions etc. reply xyproto 10 hours agoparentprevIt can already fill in all gaps and create all sorts of audio, but it may sound muddy and metallic. Give it a year, and then you can't trust what you hear anymore. Checking sources is a good idea in either case. reply m3kw9 8 hours agoprevSome people hyping it as AGI on social media reply samus 1 hour agoparentSadly, I see it even on forums where one might think people have background in technology... reply mikae1 15 hours agoprevThey’ll have my upvote just for writing ML instead AI. Seriously, this is very exciting developments for audio compression. reply wilg 14 hours agoparentThis is something you really shouldn’t spend any cycles worrying about. reply sergiotapia 13 hours agorootparentI'd just like to interject for a moment. What you're referring to as AI, is in fact, Machine Learning, or as I've recently taken to calling it, Machine Learning plus Traditional AI methods. reply wilg 13 hours agorootparentMy point is very clearly that you should not spend any time or energy thinking about about the terminology. reply yt-anthr-acc-hn 4 hours agorootparentWords have meaning. People spend cycles on it because it matters and I'm glad we do. reply wilg 3 hours agorootparentGood luck in your future endeavors! reply sergiotapia 13 hours agorootparentprevI know lol this a famous quote by ganoo loonix enthusiast Richard Stallman. reply claudiojulio 15 hours agoparentprevMachine Learning is Artificial Intelligence. Just look at Wikipedia: https://en.wikipedia.org/wiki/Artificial_intelligence reply declaredapple 14 hours agorootparentMany people are annoyed by the recent influx of calling everything \"AI\". Machine learning, statistical models, procedural generation, literally an usage of heuristics are all being called \"AI\" nowadays which obfuscates the \"boring\" nature in favor of \"exciting buzzword\" Selecting the quality of a video based on your download speed? That's \"AI\" now. reply mook 12 hours agorootparentOn the other hand, it means that you can assume anything mentioning AI is overhyped and probably isn't as great as they claim. That can be slightly useful at times. reply sitzkrieg 13 hours agorootparentprevim quite tired of this. every snake oil shop now calls any algorithm \"a i\" to sound hip and sophisticated reply mikae1 13 hours agorootparentprev> Many people are annoyed by the recent influx of calling everything \"AI\". Yes, that was the reason for my comment. :) reply samus 1 hour agorootparentprevSo are compilers and interpreters. The terminology changes, but since we still don't have a general, systematic, and precise definition of what \"intelligence\" means, the term AI is and always was ill-founded and a buzzword for investors. Sometimes, people get disillusioned, and that's how you get AI winters. reply xcv123 4 hours agorootparentprevMachine Learning is a subset of AI reply p1esk 14 hours agoprevTwo inrelated “Opus” releases today, and both use ML. The other one is a new model from Anthropic. reply behnamoh 14 hours agoprev [–] Isn't it a strange coincidence that this shows up on HN while Claude Opus is also announced today and is on HN front page? I mean, what are the odds of seeing the word \"Opus\" twice in a day on one internet page? reply mattnewton 14 hours agoparentNot that strange when you consider what “opus” means- product of work, with the connotation of being large and artistically important. It’s Latin, so it’s friendly phonemes to speakers of Romance languages and very scientific-and-important-sounding to English speaking ears. Basically the most generic name you can give your fine “work” in the western world. reply behnamoh 14 hours agorootparentThanks for the definition. I like the word! I just haven't come across it in a long time, and seeing it twice on HN frontpage is bizarre! reply stevage 11 hours agorootparentIt's funny, I was expecting the article to be about the Opus music font and was trying to figure out how ML could be involved. reply declaredapple 14 hours agoparentprev [–] Well it was released today Very likely a coincidence. https://opus-codec.org/release/stable/2024/03/04/libopus-1_5... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Opus 1.5 is out, bringing enhancements in audio quality, such as Deep REDundancy (DRED) for concealing packet loss using ML technology.",
      "New features like DRED, LACE, and NoLACE for improved speech quality are off by default, needing manual activation.",
      "The update addresses the balance between loss and latency, aims for DRED standardization, and utilizes a generative model for realistic data loss scenarios."
    ],
    "commentSummary": [
      "Opus 1.5's release integrates machine learning improvements for CPU efficiency and audio quality, sparking dialogues on ML algorithm biases, tech ethics, and diverse representation.",
      "Discussions extend to identity neutrality impact in codecs, patent concerns in the Opus codec, and distinguishing AI from ML, exploring AI's role in creative aspects.",
      "Opinions diverge on Opus tech, Bluetooth backing, and emerging codecs like LC3, fostering debates on codec performance and quality amid Hacker News buzz."
    ],
    "points": 345,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1709573802
  },
  {
    "id": 39592689,
    "title": "Nvidia restricts translation layers in CUDA software beyond its chips",
    "originLink": "https://www.tomshardware.com/pc-components/gpus/nvidia-bans-using-translation-layers-for-cuda-software-to-run-on-other-chips-new-restriction-apparently-targets-zluda-and-some-chinese-gpu-makers",
    "originBody": "PC Components GPUs Nvidia bans using translation layers for CUDA software — previously the prohibition was only listed in the online EULA, now included in installed files [Updated] News By Anton Shilov published 4 March 2024 Translators in the crosshairs. Comments (26) (Image credit: AMD) [Edit 3/4/24 11:30am PT: Clarified article to reflect that this clause is available on the online listing of Nvidia's EULA, but has not been in the EULA text file included in the downloaded software. The warning text was added to 11.6 and newer versions of the installed CUDA documentation.] Nvidia has banned running CUDA-based software on other hardware platforms using translation layers in its licensing terms listed online since 2021, but the warning previously wasn't included in the documentation placed on a host system during the installation process. This language has been added to the EULA that's included when installing CUDA 11.6 and newer versions. The restriction appears to be designed to prevent initiatives like ZLUDA, which both Intel and AMD have recently participated, and, perhaps more critically, some Chinese GPU makers from utilizing CUDA code with translation layers. We've pinged Nvidia for comment and will update you with additional details or clarifications when we get a response. Longhorn, a software engineer, noticed the terms. \"You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.,\" a clause in the installed EULA text file reads. The clause was absent in the EULA documentation that's installed with the CUDA 11.4 and 11.5 release, and presumably with all versions before that. However, it is present in the installed documentation with version 11.6 and newer. Being a leader has a good side and a bad side. On the one hand, everyone depends on you; on the other hand, everyone wants to stand on your shoulders. The latter is apparently what has happened with CUDA. Because the combination of CUDA and Nvidia hardware has proven to be incredibly efficient, tons of programs rely on it. However, as more competitive hardware enters the market, more users are inclined to run their CUDA programs on competing platforms. There are two ways to do it: recompile the code (available to developers of the respective programs) or use a translation layer. For obvious reasons, using a translation layer like ZLUDA is the easiest way to run a CUDA program on non-Nvidia hardware. All one has to do is take already-compiled binaries and run them using ZLUDA or other translation layers. ZLUDA appears to be floundering now, with both AMD and Intel having passed on the opportunity to develop it further, but that doesn't mean translation isn't viable. Several Chinese GPU makers, including one funded by the Chinese government, claim to run CUDA code. Denglin Technology designs processors featuring a \"computing architecture compatible with programming models like CUDA/OpenCL.\" Given that reverse engineering of an Nvidia GPU is hard (unless one already somehow has all the low-level details about Nvidia GPU architectures), we are probably dealing with some sort of translation layer here, too. One of the largest Chinese GPU makers, Moore Threads, also has a MUSIFY translation tool designed to allow CUDA code to work with its GPUs. However, whether or not MUSIFY falls under the classification of a complete translation layer remains to be seen (some of the aspects of MUSIFY could involve porting code). As such, it isn't entirely clear if the Nvidia ban on translation layers is a direct response to these initiatives or a pre-emptive strike against future developments. For obvious reasons, using translation layers threatens Nvidia's hegemony in the accelerated computing space, particularly with AI applications. This is probably the impetus behind Nvidia's decision to ban running their CUDA applications on other hardware platforms using translation layers. Recompiling existing CUDA programs remains perfectly legal. To simplify this, both AMD and Intel have tools to port CUDA programs to their ROCm (1) and OpenAPI platforms, respectively. As AMD, Intel, Tenstorrent, and other companies develop better hardware, more software developers will be inclined to design for these platforms, and Nvidia's CUDA dominance could ease over time. Furthermore, programs specifically developed and compiled for particular processors will inevitably work better than software run via translation layers, which means better competitive positioning for AMD, Intel, Tenstorrent, and others against Nvidia — if they can get software developers on board. GPGPU remains an important and highly competitive arena, and we'll be keeping an eye on how the situation progresses in the future. Stay on the Cutting Edge Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Anton Shilov Freelance News Writer Anton Shilov is a Freelance News Writer at Tom’s Hardware US. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends. SEE MORE GPUS NEWS MORE ABOUT GPUS Best Graphics Cards for Gaming in 2024 AMD confirms it is working on an AI upscaler for gaming – CTO Papermaster says it's part of 'AI enabling our entire portfolio' LATEST Developers of Switch emulator Yuzu settle with Nintendo in court for $2.4 million less than a week after being sued SEE MORE LATEST ► SEE ALL COMMENTS (26) 26 Comments Comment from the forums Order 66 This doesn't surprise me one bit. Tell me something surprising that Nvidia has done recently. Reply umeng2002_2 The terms say \"you.\" But it doesn't expressly prevent you from using translators made by someone else. Reply _sh4dow_ Don't some countries, like Germany, have laws explicitly allowing reverse engineering for the purpose of achieving software compatibility in spite of any ToS restrictions? Reply ivan_vy \"(unless one already has all the low-level details about Nvidia GPU architectures by stealing them)\" not everything need to be stolen, poaching engineers is somewhat nefarious or gray area but kinda legal, ALL the companies do that, also people can choose who to work for, look at Jim Keller working with everyone and their neighbor. A little bit less of bias would be welcomed in the articles. Reply samopa Why one would use translator that possibly slower, and likely to be buggy, where one can recompile (not reprograms) their code to run natively, and possibly faster, on their target platform ? Reply AchakBrooks Nvidia is not the “root” in the hierarchy of “system”, yet they continually want to be this through their business practices. When they invest and become an “Apple type” then fine, be what they want. This type of limitation they want to set, kinda shows they are not confident in their hardware, and hints at their public sale pitch/reasoning to sell their hardware they way they do is just a grassy mountain built on a land fill. Honestly, maybe…. since they are building to be the “all in one/apple type” with their networking systems. Companies investing in such and seeing more publicly visible options, are more impowered to weigh those “options” based off the value of Nvidia’s “software” vs ”hardware” benefits over others that can harness the “software” side; would hurt Nvidia’s negotiation power no? PS: i have a passionate hate built up over decades towards everything Nvidia does so it’s hard to take a step back and give “the benefit of the doubt” or “this is the law so…” Reply Pierce2623 The various governments of the world totally screwed up by letting the era of “you bought a piece of hardware and you bought a piece of software . Use them however you choose because they’re yours now” go away. Licensing on software and hardware ruined everything. Reply DiegoSynth samopa said: Why one would use translator that possibly slower, and likely to be buggy, where one can recompile (not reprograms) their code to run natively, and possibly faster, on their target platform ? Because developers will not do it. Specially huge corporations like 3D software makers. And because AMD / Intel users have the right to use their cards with these programs. Reply DiegoSynth Nvidia? The ones building for AI, which steals from creators? They are crying now?! Aw, maybe they need a bit of cream in their asses; must be irritated xD Reply NinoPino I agree with the conclusion of the article. Nvidia should open CUDA to other hardware and search for alliances, instead they closes it further. Imho is too late to use this strategy, especially after Intel consolidated Arc ecosystem and AMD proved MI300 capabilities and continued to improve ROCm. Reply VIEW ALL 26 COMMENTS Show more comments MOST POPULAR Most upcoming Arrow Lake CPUs will leverage TSMC nodes for compute, Intel 20A only for Core i5 and lower: Leak By Matthew ConnatserMarch 04, 2024 Apple's MacBook Air gains M3 upgrade and support for two external displays — both 13-inch and 15-inch get improvements By Brandon HillMarch 04, 2024 Hardware hacker runs GTA Vice City on a router with a little help from an eGPU and Debian Linux By Mark TysonMarch 04, 2024 Gem12 Pro Mini PC uses an amped-up Ryzen 7 8845HS APU, has a built-in mini screen, and OCuLink support By Christopher HarperMarch 03, 2024 Pumpless 240mm AiO cooler is only 8C behind traditional pump-powered rival in tests By Roshan Ashraf ShaikhMarch 03, 2024 Intel's new Special Edition gaming CPU hits 6.2 GHz with 1.5 volts — upcoming Core i9-14900KS packaging and per-core frequencies revealed By Matthew ConnatserMarch 03, 2024 OpenAI leadership rejects Elon Musk lawsuit – memo seen by reporters rebuts three main claims point by point By Mark TysonMarch 03, 2024 AI-generated content and other unfavorable practices have put longtime staple CNET on Wikipedia's blacklisted sources By Christopher HarperMarch 02, 2024 AI worm infects users via AI-enabled email clients — Morris II generative AI worm steals confidential data as it spreads By Roshan Ashraf ShaikhMarch 02, 2024 Intel Bluetooth driver update alleviates PS5 DualSense controller connectivity issues By Aaron KlotzMarch 02, 2024 World Server Throwing Championship 2024 — organizers call for athletes that are 'needlessly brutal to servers' By Mark TysonMarch 02, 2024 TOPICS CUDA",
    "commentLink": "https://news.ycombinator.com/item?id=39592689",
    "commentBody": "Nvidia bans using translation layers for CUDA software to run on other chips (tomshardware.com)274 points by justinclift 17 hours agohidepastfavorite127 comments dotnet00 16 hours agoThis just seems to be a poorly researched knee-jerk article written based on a single off-hand tweet from 2 weeks ago, citing a clause that has been around for years, and thus obviously is not in response to something talked about 3 weeks ago. As an example, the license here has the exact phrase, on a file last changed 2 years ago: https://github.com/NVIDIA/spark-rapids-container/blob/dev/NO... The EULA here, listed as last updated in 2021, also has this clause: https://docs.nvidia.com/cuda/eula/index.html reply byteknight 15 hours agoparentAgreed that that may be deceptive, but the underlying issue remains. It is prohibited. reply tzs 14 hours agoparentprevThe article has been updated to clarify: > Nvidia has banned running CUDA-based software on other hardware platforms using translation layers in its licensing terms listed online, but those warnings previously weren't included in the documentation placed on a host system during the installation process. This language has now been added to newer versions of CUDA. reply dotnet00 14 hours agorootparentThat still doesn't seem accurate. I have CUDA 11.6 on my machine from Dec 2021, just checked the EULA included there, and it did have the mentioned clause there too. reply Aloisius 13 hours agorootparentIt was in fact added to the EULA.txt file in 11.6. The header says it was updated October 8, 2021. % diff EULA.txt-11.5 EULA.txt-11.6 |grep -2 trans > 8. You may not reverse engineer, decompile or disassemble > any portion of the output generated using SDK elements for > the purpose of translating such output artifacts to target > a non-NVIDIA platform. That said, I believe 11.6 was released in January, 2022. reply aneutron 11 hours agorootparentThis clause would absolutely not fly in a French court, and IANAL but IIRC US case law does allow for reverse engineering for interoperability. reply dotnet00 13 hours agorootparentprev>That said, I believe 11.6 was released in January, 2022. Ah that's right, the Dec 2021 timestamp was for 'last modified', but 'created' is Jan 2022. reply rootkea 8 hours agorootparentHow can 'created' timestamp > 'last modified' timestamp? reply dotnet00 6 hours agorootparentLast modified was probably carried over from what the file had when it was packaged into the installer, with created being the time for when I actually installed the toolkit. reply mvdtnz 15 hours agoparentprevI think you're focused too much on when this clause was added. Most of us are not so concerned with the chronology. reply dotnet00 15 hours agorootparentI'm focused on what the article, and the full headline claims. \"Nvidia bans using translation layers for CUDA software to run on other chips — new restriction apparently targets some Chinese GPU makers and ZLUDA\" The time the clause was added also matters, because if it's ~3 years old and since then various translation layers backed by other large competitors have been released without any open lawsuit, it's a lot less concerning than if the clause were added right now, as, at least to me, it suggests that they're not talking about simply copying the API based on documentation and known quirks, but reverse engineering how the internals function for the purposes of a translation layer (eg same thing as MS banning reverse engineering of Windows for the purpose of adding the functionality to Wine, but tolerating clean room reimplementations). reply HarHarVeryFunny 14 hours agorootparentprevWell, never mind the date, the clause just doesn't say what the article author claims it to say, not even close ... It doesn't say you can't run CUDA code via a CUDA comabability layer - it says you can't reverse engineer CUDA code in order to translate it to something else (i.e. NOT CUDA) to run on non-NVIDIA hardware. It's almost the exact opposite of what the article claims - the only way to legally run CUDA software on non-NVIDA hardware would in fact be to leave it as-is and run via a compatibility layer!! reply paulmd 8 hours agorootparentIt’s a constant problem with nvidia articles, sites love to make up unapologetic bullshit or cite ancient stuff as if it’s new. A few months ago all the internet sites were abuzz with jensen’s “recent” comment that “nvidia is fully focused on ai now” and the article that is from makes it clear that it happened in the “mid 2010s”. Gamersnexus actually had the citation on the screen with “by the mid 2010s” literally on screen, they still went with “recently” and refused to issue a correction or retraction (I asked). https://youtu.be/VSSb-t76EpU?t=147 Similarly, the “nvidia sells directly to mining farms!” articles from a few years ago, were citing an article that was an estimate of mining sales based on the hash rate… it never accused anyone of selling anything to anyone, but tech media didn’t bother reading the source, and once the first article got rolling everyone just cited that instead. https://twitter.com/dylan522p/status/1332502890104188929 Just like with apple “parts/defect stories” news sites know these are potent money-makers that drive a lot of clicks, and frankly I think there is a lot of open fanboyism and hostility among tech media today. GN didn’t “accidentally” refuse to do a correction, they want to push that narrative. Just like the “long-term value is mathematically impossible” etc - we have been in a world where reviewers are openly feuding with one of the vendors for over a half decade over the direction of product development and the death of Moores law in terms of pricing and performance increases. Winning rhetorical points in that debate is more important to tech media nowadays than little things like journalistic integrity, or reading your sources. https://www.youtube.com/watch?v=tu7pxJXBBn8&t=273s reply germandiago 15 hours agorootparentprevYet the article seems to imply they did it recently also according to my understanding, which is misleading. reply Zetobal 15 hours agorootparentprevnext [4 more] [flagged] salawat 14 hours agorootparentMore like \" I refuse to be held hostage by your delusional business model\" more like. I bought two GTX 970's, and a GTX 1030. Then I did the research on how they actually worked and the nefarious things the company got up to in order to spoliate open source drivers. The minute they switched to secritful Falcon units, is when any pretense of good faith was thrown out the door. They cheated me out of 3 graphics cards worth of functionality. I can accept \"not yet implemented\". I will not accept \"predatory board forces you to do business with me forever\". No mad conception of IP justifies that. reply charcircuit 12 hours agorootparent>nefarious things the company got up to in order to spoliate open source projects Nvidia hasn't attempted to destroy open source drivers. In fact they work with open source drivers such as nouveau and mesa. >They cheated me out of 3 graphics cards worth of functionality. The cards work fine with Nvidia's drivers. You are only cheating yourself if you refuse to use them for some reason. reply Zetobal 14 hours agorootparentprevnext [2 more] [flagged] salawat 12 hours agorootparentOh yes. Because Nvidia so proudly and eloquently states \"I engage in anti-competitive practices.\" Yes, it is my fault, but dare I say, how many 20 somethings do you know that come chock full of sufficient life experience to identify hostile/predatory business models? I don't see many at all. And for that matter, in order to get the information I now have, I had to dive through half-a-dozen search engines in order to find architecture information that was NDA'd, but nevertheless left open to be indexed by someone. So yeah. There's uninformed, but goddamn if that doesn't seem to be Nvidia's modus operandi. reply SomeHacker44 16 hours agoprevIANAL, but I don't see how this can be enforced (in the US) given the legal history of emulation. This is effectively just emulation. c.f., https://emulation.gametechwiki.com/index.php/Licensing reply marklar423 16 hours agoparentI think they're trying to do the trick where they don't ban the emulation per se, but they make it a violation of the license to disassemble the code to understand how it works. Then they go after emulating projects for copyright infringement, since they must have violated copyright to get a compatible version running. It's a dirty trick IMO, but it has seen some mixed success. reply samus 16 hours agorootparentClean-room reverse engineering is specifically not forbidden, and has been successfully done in the past to develop the Nouveau driver. Disassembling Nvidia's binaries is plausibly forbidden, but it would be very surprising if they could extend that ban on binaries produced by their compiler. However, the trademark could be used to attack statements about the emulation layer. I see parallels to Google vs. Oracle reply salawat 14 hours agorootparentNote Nouveau is dead in the water due to DMCA 1201, plus Nvidia's use of secrutful FALCON units to cryptographically verify firmware signatures. reply samus 12 hours agorootparentRecently, development has picked up again though. They basically surrendered and now use the proprietary blob. The Mesa driver supports Vulkan 1.3 and Zink will be used for OpenGL going forward. Now on to more performance optimization and maybe bringing up some of the other proprietary IP blocks... reply salawat 12 hours agorootparentRealistically, all that means is that Nouveau is giving in to the death of rights of First Sale, which is a fight I'm not willing to give up on fighting for yet even if every deep pocketed interest and it's sister seems to be swinging that way. reply samus 12 hours agorootparentI don't completely get your point I fear. Using the blob is simply accepting the fact that the cards are hamstrung without it. They won that round. Let's hope that AMD and Intel eventually get it together and come up with products that can slowly erode Nvidia's moat. reply j1elo 15 hours agorootparentprevI wonder how it's not a trivial workaround to have a separate, anonymous project with the sole scope of disassembling the code (and thus violating the license), while acting as knowledge source for emulators. This way, emulators wouldn't be violating any license, just using technical details learned from third party sources. The anonymous project itself would be attacked by Nvidia, and in case this succeeded, replicas of the repo would probably pop up quickly and easily. But good luck having a chinese Gitee repo closed for american copyright infringement! reply mech422 6 hours agorootparentIt appears from the comments here that just disassembling the nvidia code would NOT be a violation? it seems that only disassembling to use in a compatibility layer would be ? I believe, this 2 stage/project system was how DeCSS for DVDs worked? Someone cracked it and posted the code, which could them be picked up/used by others as it was 'public knowledge' or some such ?? reply imtringued 1 hour agorootparentprevNothing about this prevents you from developing a CUDA conformance test suite that can be used to verify that any given CUDA implementation is functioning properly. Nvidia wouldn't even be allowed to take it down. reply ActionHank 16 hours agorootparentprevSo they just put up their hand and said they also want to give the EU money? reply brookst 14 hours agorootparentAt this point with the level of demand they have, it might be easier and cheaper to just not do business in the EU. reply jijijijij 12 hours agorootparentFunny how nobody does that, eh?! Could be because the EU is the second biggest consumer market on this planet. But of course, they could totally focus on China, now third and rising, instead. Maybe they can even ask the CCP to help them with their IP claims and stuff. reply dontupvoteme 14 hours agorootparentprevSo what happens when someone in e.g. China or Russia releases something who they can't practically prosecute, can everyone else then use it? reply lunfard000 16 hours agorootparentprevI dont think China cares too much about what NVIDIA thinks considering the ban in place. reply wakawaka28 14 hours agorootparentprevIsn't CUDA documentation thorough enough to reimplement it without reverse-engineering? (Serious question.) reply colechristensen 15 hours agorootparentprevThe alternative is NVIDIA charging for what they're spending money on and making CUDA as big a cost as the hardware. I don't know what I agree with but hardware-supported-software being free is a good thing, your competitors being able to use it for free is a bad thing. I wonder legally if it would work to instead have licensing that charged a reasonable fee to run CUDA on non-NVIDIA hardware. Just don't enforce it for developers, but have it big enough and enforced enough so corporations would hesitate or pay... or just make it large enough to make it not make sense to run on non-NVIDIA cards. reply shadowgovt 15 hours agorootparent> hardware-supported-software being free is a good thing, your competitors being able to use it for free is a bad thing. ... bad thing for whom? Especially having been in the unenviable position of needing to debug implementation errors in shader compilation, I'd argue that for the end user, it's much better if the software is not only usable for free, but open-source. To the extent that the ownership the hardware vendors place upon their software prevents this, I'd say that's a bad thing. If NVIDIA can't compete on hardware they're just bailing water from a sinking ship. All their competition has to do is provide and adopt a just-as-good open software standard on cheaper hardware and people will flock to it, NVIDIA will be forced to provide compatibility or become an also-ran, and they'll lose anyway. reply colechristensen 12 hours agorootparent\"All their competition has to do\" is quite the assumption, when in fact CUDA and drivers is a very significant portion of the work NVIDIA does. NVIDIA is winning because of their software work, this is where they've added the most value to their product. NVIDIA can compete on hardware, but the reason AI/ML is where it is today rests largely on the investment in software NVIDIA has done in the last 20 years enabling these kinds of things. Everyone could be using the open source alternatives, but they're not because the OSS isn't as good because... nobody paid for it. I'm all for open source, you should be able to run whatever you want on your NVIDIA card (and you can, no walled gardens there), but it doesn't go as far as insisting that if someone wrote software it should be free. reply paulryanrogers 16 hours agoparentprevAPIs aren't subject to copyright. So unless CUDA is more than an API they're just going to make some lawyers rich and waste everyone's time. reply laweijfmvo 16 hours agorootparentMaking lawyers rich to waste time is how super rich companies stifle competition... reply samus 16 hours agorootparentThat works against small fishes, but it would be a different story if AMD or Intel take part. reply laweijfmvo 12 hours agorootparentThen it wastes double the time, and makes double the lawyers rich :D reply samus 4 hours agorootparentIt at least increases the amount of precedence regarding the subject. That would eventually decrease the time such trials take. reply jsheard 16 hours agorootparentprevThe API may not be copyrightable, but the widely-used official libraries (cuDNN, cuBLAS, OptiX, etc) are, so in practice I think they can at least say that anything which uses those libraries can't be run on competitors hardware. reply zozbot234 16 hours agorootparentAIUI, these libraries are generally implemented as shared objects that the resulting binary has to link to. So it should be quite possible to reimplement the interface that they expose, moreover something like ROCM has to do that anyway in order to compile HIP source code from scratch. Looks like that's what https://github.com/ROCm/hipDNN/ does. reply jsheard 15 hours agorootparentPossible, yes, but reimplementing the CUDA runtime and the official libraries and getting them up to par with the originals is a much bigger task than just doing the former and running Nvidias libraries in it. AMD did try to do a complete reimplementation of cuDNN with hipDNN but \"last commit 5 years ago\" doesn't inspire much confidence in it being competitive with cuDNN proper. reply imtringued 1 hour agorootparentThey don't have any tensor cores so there is not much reason for them to work on this. reply bitwize 15 hours agorootparentprevPer the Federal Circuit, APIs are subject to copyright. The Supreme Court just ruled that Google's use of APIs was fair use. reply ddtaylor 15 hours agorootparentYou mean the Oracle vs Google case right? reply colechristensen 15 hours agorootparentprevCUDA (and plenty of things lumped in with it that one would think of as CUDA) are not simply an API. Somewhere in between a programming language and a low level operating system. reply jsheard 16 hours agoparentprevI think there's two issues at play: re-implementing the CUDA APIs, and using that re-implementation to run Nvidias own libraries like cuDNN on non-Nvidia hardware. Precedent around emulation and the Google vs. Oracle case probably protects the former, but Nvidia is probably entitled to say how you're allowed to use their own software in the latter. reply mordae 15 hours agoparentprevReversing in order to achieve interop is explicitly legal in EU. Well unless you have to break DRM in process. reply mech422 6 hours agorootparentI believe, this 2 stage/project system was how DeCSS for DVDs worked? Someone cracked it and posted the code (for interop, in EU), which could then be picked up/used by other projects as it was 'public knowledge' or some such and didn't expose them to legal risk ? reply rerdavies 11 hours agorootparentprevDoes that render UELA clauses that forbid reverse engineering invalid? reply plussed_reader 16 hours agoparentprevAn arms race of the update whack-a-mole as the encrypted blob grows and encompasses more... reply lunfard000 16 hours agorootparentwouldn't that require breaking the ABI? Many enterprise wont be happy if they do so. Also, there is not way to prevent using old cuda compilers. reply reactordev 15 hours agorootparentNot a way to prevent it but they can make damn sure its difficult to find. Example: Where is Newtonsoft's Physics Library v1.x? It was awesome, easy, fast, and worked with my engine (or rather, my engine worked with it?). Gone. Nowhere to be found, not even on the internet archives way-back-machine. It's rather trivial for a juggernaut like NVidia to wipe the earth of older cuda compilers by tweaking a driver and making cuda compilation cloud-based. reply bitwize 15 hours agoparentprev> given the legal history of emulation. Funny you should say this just after Nintendo shut down Yuzu and collected hefty damages in the settlement. reply nindalf 16 hours agoprevTheir ban is legal, not enforced in code. I wish them best of luck enforcing those terms and conditions against Chinese hardware makers in China. And even Nvidia would know that the European competition regulator would take a dim view of it. I guess there is some benefit to dragging it out, benefiting from the CUDA monopoly for a year or two more. reply wzdd 15 hours agoprevThis doesn't appear to ban using translation layers. The text is \"You may not reverse engineer, decompile or disassemble any portion of the output generated using Software elements for the purpose of translating such output artifacts to target a non-Nvidia platform\". That would appear to (attempt to -- it may not be enforceable) restrict the creation of translation layers. I don't understand how you could infer \"bans using translation layers\" from the above clause, and indeed the tweet they're referencing does not. AIUI Zluda is something like Wine, in that it's an API reimplementation. It would be weird to call running Wine reverse engineering, decompilation or disassembling -- it's effectively just linking. reply croes 15 hours agoparent>Dark API functions are reverse-engineered and implemented by ZLUDA on a case-by-case basis once we observe an application making use of it. https://github.com/vosen/ZLUDA/blob/master/ARCHITECTURE.md reply marshray 15 hours agoparentprevIt's almost like they wrote it specifically to be invalid under a compatibility exception. reply justinclift 17 hours agoprevOn the face of it, this sounds like abusive behaviour by a monopolist. Wonder if it'll be seen that way legally though? reply nottorp 16 hours agoparentFor the US, didn't Oracle win the java api lawsuit? reply jjice 16 hours agorootparentLooks like they lost [0], with Google winning 6-2, but maybe Oracle is trying to appeal it? I'm not familiar with the remanding process so I can't comment on that part of this quote. > In April 2021, the Supreme Court ruled in a 6–2 decision that Google's use of the Java APIs fell within the four factors of fair use, bypassing the question on the copyrightability of the APIs. The decision reversed the Federal Circuit ruling and remanded the case for further review. [0] https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America%2.... reply nottorp 15 hours agorootparentOh interesting. The last piece of news I read was probably the decision before that, that ruled in Oracle's favour. Blame Covid. So is it final, or they can still drag it on? From wikipedia: \"Justice Stephen Breyer wrote the majority opinion. Breyer's opinion began with the assumption that the APIs may be copyrightable, and thus proceeded with a review of the four factors that contributed to fair use:\" That doesn't look so good. reply ender341341 15 hours agorootparentMy (non-lawyer) take on that is when they say \"began with the assumption that the APIs may be copyrightable, and thus proceeded with a review of the four factors that contributed to fair use\" is that they're not saying APIs are copyrightable and basically ignored that question because they ruled that even if they are copyrightable googles use would be fair use and oracle doesn't have a case. it's a fairly common method cases are resolved, you say \"assuming the plaintiff claims are all true, do they actually have cause of action for a lawsuit?\" reply vidarh 15 hours agorootparentprevIt's fairly typical that the court wanted to make the narrowest decision possible. By conclusion that *even under the assumption they _may_ copyrightable, Google didn't violate copyright they saved themselves the hassle of deciding on the copyright issue. reply nottorp 15 hours agorootparentIANAL. So basically the assumption language means there is no rule either that APIs are copyrightable or not, yet... Could have been worse i guess. reply vidarh 14 hours agorootparentWell. There's no Supreme Court ruling on the copyrightability. The Federal Circuit did hold that APIs are copyrightable, and as far as I understand the Supreme Court carefully avoided deciding whether or not they were right about that. So it's not great, as it does leave the Federal Circuit finding that the APIs were copyrightable standing so far, but as you say it could have been worse - it does not have remotely the same weight. reply justinclift 16 hours agorootparentprevDoesn't seem like it, though it had to work it's way to the Supreme Court for the final decision: * https://arstechnica.com/tech-policy/2021/04/how-the-supreme-... * https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America,_.... reply Tistel 16 hours agoprevPeople should check out Google's JAX. Work in a high level language and run anywhere. Nvidia should just be commodity hardware if people avoid vendor lock in. reply nerpderp82 15 hours agoparentShimming CUDA is a waste of effort that only reinforces Nvidia's market dominance. Targeting higher level interfaces, Jax, Taichi, ArrayFire, etc is imho a better strategy. We have already seen systems like LLama.cpp and their ilk support alternative backends for training and inference. Now the vast majority of the compute cycles have centered around a handful of model architectures, implementing those specific architectures in whatever bespoke hardware isn't difficult. Target specific applications not the whole complex library/language layer. reply fisf 15 hours agoparentprevThat's fine and dandy, until you realize that Jax only has a limited amount of backends. E.g. rocm support is still experimental. Somebody has to build those optimized backends -- it's not just a matter of people picking the wrong stack. reply nerpderp82 15 hours agorootparentI just looked at Jax and XLA, it is odd to me that they aren't targeting SPIR-V directly. reply mindcrime 16 hours agoprevIronically, this is just going to increase interest in ROCm and other alternatives. reply foobarian 16 hours agoparentWonder if they know and expect that. It's brilliant! Maybe AMD should announce a ban on developing CUDA compatibility layers for their kit reply jstanley 16 hours agoprevPeople keep using proprietary software and keep getting burned by it. When will we learn? reply andersa 16 hours agoparentThere are no viable alternatives if you require high performance or even just the software to actually work properly. reply jvanderbot 16 hours agorootparentYes - this is a competition problem. If there were viable alternatives, then interoperability wouldn't be something we really talk about. Instead it'd be abstraction layers to run on the chip-specific runtimes. And then it's just CUDA trying to \"beat\" the alternative. reply bee_rider 16 hours agorootparentprevTrying to best Nvidia’s language on Nvidia’s hardware seems pretty hopeless. I wonder if that’s the wrong abstraction layer? There exists stuff like CUBLAS, which is of course using CUDA under the hood, but it also is something like a BLAS. Maybe as the AI/ML world keeps developing people will be grow more stubborn about sticking to frameworks. We probably just need a couple rounds of people getting burned by vendor lock-in, I guess. reply sevagh 15 hours agoparentprevWho's burned by what? NVIDIA's customers are happily training their models on superior hardware, so much so that they're begging to be able to order and spend more. reply moffkalast 16 hours agoparentprevHopefully Vulkan eventually gets a headless compute kernel version. reply jsheard 16 hours agorootparentVulkan has always supported headless mode, everything related to graphics and presentation is optional. It has a way to go before its compute model is as powerful and easy to use as CUDA is though. reply zozbot234 15 hours agorootparentThere are various efforts to compile OpenCL and SYCL to Vulkan, the Mesa folks are working on this as part of the RustiCL project. But full capability will require some extensions beyond what plain Vulkan provides. reply moffkalast 15 hours agorootparentprevWait, really? How does one set that up without just getting llvmpipe? I've turned over half the internet and I could never get it to work without installing some kind of window system. reply jsheard 15 hours agorootparentI'm just speaking from the perspective of the spec, which defines surfaces and swapchains as extensions that are never guaranteed to be available, so a compliant driver may work in the absense of any kind of GUI by just reporting those extensions as not supported. Whether your Vulkan driver actually supports running in a headless context is another question though, and of course the Vulkan software you're trying to run has to gracefully handle the case where surface/swapchain aren't available. reply mandarax8 16 hours agorootparentprevYou can already do compute without creating a swap chain or presenting right? reply solardev 16 hours agorootparentprevCould WebGPU fill that need? reply kllrnohj 15 hours agorootparentWebGPU can't fill any need that GL, DirectX, or Vulkan can't. WebGPU isn't a native GPU API, but an abstraction over existing ones. As such its feature set is only ever at best comparable, but in practice will always be worse than the native ones (always worse due to needing to be implementable by multiple platforms - at a minimum both vulkan and metal) reply solardev 13 hours agorootparentI see. Thanks for the explanation! reply hackerlight 7 hours agoparentprevCollective action problem. So, never, because structurally there are game theory reasons pushing individual customers to behave this way. Therefore you can't blame the customer you have to blame the regulator and system. reply renewiltord 15 hours agoparentprevWho's getting burned? It's the best stack to build on. The only ones getting burned are the third-party guys who want adoption. reply wmf 14 hours agorootparentNvidia customers are paying $30K for GPUs that should be under $10K; that's how they're getting burned. reply superkuh 16 hours agoparentprevWhat's the alternative? AMD only supports their (consumer) GPUs for ~4 years via ROCm in some instances. If you buy the card at any time except release day you only get a couple years of compute support. To answer my own question: opencl, and it's just as bad as it was in 2014. Or, slowly, people are starting to do compute with Vulkan. This might be the best way forwards even if it's an awkward choice. reply whatshisface 16 hours agorootparentInvestment is not rewarded until the platform is used, which won't be until investments are made... it's a multi billion dollar chicken and egg problem that only could have been averted a decade ago by consumers refusing to be locked in to a proprietary standard. reply dotnet00 15 hours agorootparentA decade ago, CUDA was still offering a more usable platform than the competition. The problem could only have been averted if AMD had properly committed to investing in their platform at least decade ago, just as NVIDIA has been doing for almost 2 decades now. reply whatshisface 15 hours agorootparentA decade ago, CUDA was better, but I don't think the industry had crossed the threshold of being stuck on it. AMD still had the option of investing to catch up, and there would have been the possibility of an industry consortium. Now, with the whole ML stack having been optimized over ten years of rapid development for a single proprietary standard, and especially with the deliberate obstructionism exemplified by the linked article, it is much less likely that a consortium or competitor could catch up. reply TeMPOraL 12 hours agorootparentA decade ago we still called it GPGPU and it was a somewhat niche stuff used for specialized applications. There wasn't enough interest in the field to warrant that heavy investment. Fast-forward ten years, and we're in rapid inflation phase (the cosmological kind, not the financial kind) - whatever choices were there at the start got baked in, and are sinks for all the free money that's coming in and demanding fast results. AMD could've solved this 10 years ago if they invested in it, but they didn't have future knowledge about ML on GPUs being the next big thing. reply Karellen 16 hours agoparentprevIf people were going to learn not to use proprietary software/hardware like the stuff that the wankers at Nvidia churn out, they'd have learned it over a decade ago when Torvalds gave them a big \"fuck you\" - which had been a long time coming even back then. People who keep picking Nvidia don't want to learn not to use proprietary tools. reply whatshisface 16 hours agorootparentThat's a bit like saying \"the slaves in the roman silver mines didn't want to rebel.\" Maybe a decade ago it was because they didn't care about abstract ideals like software freedom, but now they are well and truly stuck, and can serve only as a cautionary tale to other industries. reply bee_rider 16 hours agorootparentNvidia offers an attractive product with strings attached, we very well could call it anti-competitive. But it is not very similar to slavery, nobody is getting beaten or chained up, the “victims” are willing participants. Maybe some comparison to company stores could be warranted… reply chatmasta 16 hours agoprevI don't understand why Nvidia is so obstinate on this front. They would solidify their lead in hardware if they open sourced the entire CUDA software stack. Their hardware competitors are going to reverse it anyway, so they may as well open source the thing and benefit from all the momentum that comes with owning the community's favored software and the hardware that it runs on. reply roughly 15 hours agoparentNvidia is worth more money than God because every ML pipeline out there uses CUDA and the only way to use CUDA is on Nvidia hardware - they already own the community’s favored software and the hardware it runs on. CUDA’s not the product, it’s the moat. reply gjsman-1000 16 hours agoparentprevExclusivity works. As much as Hacker News likes to dismiss it. reply bluedevil2k 15 hours agoparentprevReally? All these companies have their code written in CUDA and when it comes time to buy more GPUs they can make a decision - buy more Nvidia chips that will “just work”, or buy AMD/Intel and spend time and money writing new, potentially buggy, software to duplicate the software I’ve already written. Seems like an easy decision for the buyers, and Nvidia’s vendor lock in is complete. reply tamimio 16 hours agoprevI don’t think Nvidia can enforce this in the US let alone China. reply mark336 2 hours agoprevFor more Nvidia news see my page: https://www.asiaviewnews.com/gigabots/threads reply bogwog 16 hours agoprevWeird that neither the article nor the source tweet link to the actual terms, but it seems to be true: https://docs.nvidia.com/cuda/eula/index.html#limitations (in section 8) reply JonChesterfield 14 hours agoprevTranslating machine code between architectures is a dubious proposal in the first place. Taking compiled cuda shaders, disassembling them and recreating shaders for some other architecture from the pieces really should be more effort than compiling the source directly to that other architecture. reply hangonhn 15 hours agoprevThe stated reason from the article makes little sense. The Chinese will just tell Nvidia to go take a hike, if the legal agreement holds any water at all in Chinese law. The only people this will affect would be AMD and Intel. Something isn't quite adding up. reply williamDafoe 15 hours agoprevThey will lose in court. I am reminded of IBM trying to ban 3rd party disk drive makers for from making disks that fit the IBM disk interface in the 1960s. They lost, too. However, ZLUDA may have to do a clean room reimplementation of all of CUDA, like google did with their javascript reimplementation, however ... reply rerdavies 11 hours agoparentI'm reminded of Apple modifying their hard drives to return \"Copyright (c) Apple Computers\" to appropriate prodding, and refusing to mount any drive that didn't do so. That one never went to court. These days, you just do the same thing with a little rudimentary crypto. reply nhggfu 5 hours agoprevauthor doesn´t even bother to say what CUDA is, just refers to it by an acronym html even has anelement would be fab if people would communicate effectively when they are writing on the web, as a job. reply fransje26 15 hours agoprevWell, that's excellent news. It now means competitors can now concentrate on coming with an alternative, instead of coming with a half-arsed solution. Looking at you, AMD. reply dooglius 15 hours agoprevThis appears to apply only to binaries compiled with NVCC; compiling CUDA code with LLVM/clang for non-nvidia would not be a license violation (disclaimer: IANAL) reply salawat 16 hours agoprevSounds anti-competitive af. How has this not bubbled up to the FTC for anti-trust action yet? reply ronsor 16 hours agoparentYou can always report it to the FTC yourself, or Nvidia's competitors can sue them for antitrust violations. reply muragekibicho 16 hours agorootparentBe the change you want to see in the world reply segasaturn 16 hours agoparentprevThe FTC is not going to take action that would hurt a $2T American corporation and help Chinese hardware manufacturers/reverse engineers, no matter how valid such a case would be. reply LispSporks22 15 hours agoprevWell I think we just witnessed the end of CUDA then. We usually code around such brain damage. reply mawadev 12 hours agoprevThis is how nvidia will slowly rot reply rerdavies 11 hours agoparentIn the meantime, NVIDIA has quickly become the world's third most valuable company. reply raggi 16 hours agoprevgo all in on webgpu compute reply astlouis44 15 hours agoparentBeen pondering over this a ton recently. WebGPU not only represents higher-end rendering in a browser, but true cross-platform compute that will increasingly get closer and closer to native performance. This is huge, because it comes with the portability aspect as well. Where I think WebGPU has the most promising role to play is in inference of smaller optimized AI models, in client hardware. Users expect software to run anywhere, and for developers being able to deploy a portable binary that \"just works\" is huge. Not to mention the immense cost savings... now you won't get a massive model, we're going to need the cloud for those for a while yet. But if you can run it locally, why not? And end users spend most of their time in browser these days, so it's obvious to see where this is all headed. reply hagbard_c 16 hours agoprevNvidia can go bite my shiny metal ass as far as I'm concerned. Hey, European Commission, once you're done taking 0.5% of the fruit factory's yearly income for their gate keeping tendencies here's another juicy company for you to investigate. reply amelius 14 hours agoparentIt would be great if nVidia would charge Apple 30% of their revenue for the use of CUDA. reply andersa 16 hours agoprev [–] > a new clause in CUDA 11.5 reads Huh? That was released like 3 years ago. reply ummonk 16 hours agoparent [–] The license was indeed last updated in October 8th 2021 with the release of Cuda 11.5.0. I guess nobody noticed until now? https://docs.nvidia.com/cuda/eula/index.html reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Nvidia updated its licensing terms to prohibit using translation layers for running CUDA-based software on non-Nvidia hardware, a response to ZLUDA and Chinese GPU makers' efforts.",
      "The move intends to safeguard Nvidia's leadership in accelerated computing, especially in AI applications, allowing legal recompilation of CUDA programs but encouraging porting tools from competitors like AMD and Intel.",
      "This adjustment may prompt a change in Nvidia's stronghold in the GPGPU sector as rivals enhance hardware while software developers explore alternative platforms."
    ],
    "commentSummary": [
      "Nvidia has updated their CUDA licensing terms, prohibiting the use of translation layers to run CUDA software on non-NVIDIA hardware, spurring debates on interoperability and legal ramifications in different regions.",
      "Discussions revolve around Nvidia's impact on AI/ML development, challenges of vendor lock-in, competition, and the emergence of alternatives like WebGPU, referencing legal cases such as Google vs. Oracle.",
      "There are varied opinions on Nvidia's CUDA exclusivity, concerns over vendor lock-in, analogies to slavery, and speculations about AMD and Intel offering rival solutions, discussing a potential shift from CUDA to WebGPU and scrutiny into Nvidia's practices."
    ],
    "points": 274,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1709571531
  },
  {
    "id": 39591529,
    "title": "Introducing Astro App: A Comprehensive Sky Object Planning Tool",
    "originLink": "https://astro.sshh.io/",
    "originBody": "I really like Stellarium and SkySafari but I felt like these are primarily geared towards exploring the sky but not so much \"here are the long list of things I want to see, when can I see them tonight and where\". There&#x27;s also not really a great option I&#x27;ve found that combines sky object planning + location weather details while still being free so I built this. The UI&#x27;s heavily inspired by NINAs sky atlas + Robinhood.Right now you can:View the altitude chart of objects and 3D viewCreate lists of objects of interestView the annual max&#x2F;min daily altitude of an object to find the best time of year to viewSee live clouds from GOES satellite view + weekly night-centric forecast",
    "commentLink": "https://news.ycombinator.com/item?id=39591529",
    "commentBody": "Astro App (sshh.io)264 points by sshh12 18 hours agohidepastfavorite57 comments I really like Stellarium and SkySafari but I felt like these are primarily geared towards exploring the sky but not so much \"here are the long list of things I want to see, when can I see them tonight and where\". There's also not really a great option I've found that combines sky object planning + location weather details while still being free so I built this. The UI's heavily inspired by NINAs sky atlas + Robinhood. Right now you can: View the altitude chart of objects and 3D view Create lists of objects of interest View the annual max/min daily altitude of an object to find the best time of year to view See live clouds from GOES satellite view + weekly night-centric forecast jsf01 17 hours agoMy current setup is Stellarium + Clear Outside + windy.app and with that you really do need to know ahead of time which objects are visible for your particular location at that time of year or spend a while browsing. Your app looks amazing for my main use cases, but still lacks some information that I rely on. First, it is useful to know the direction of the cloud coverage throughout the night to be able to compare it with where the object you’re shooting will be. I also like to know the brightness (magnitude) of each object to help with planning. The last thing would be to get a list of recommendations rather than using lists of favorites. “What deep sky object do I not know about that today would be perfect for shooting?” is a question I have only been able to answer well with the ASIAR’s list of suggestions, manually curated calendars online that are not tuned to my location, or social media. It seems like you might have all the data to do a better job than all of those options in answering that question. Super cool app! Excited to see where it goes. reply sshh12 17 hours agoparentThese are great ideas for me to add! - know the direction of the cloud coverage throughout the night to be able to compare it with where the object you’re shooting will be - I also like to know the brightness (magnitude) of each object to help with planning - The last thing would be to get a list of recommendations rather than using lists of favorites. “What deep sky object do I not know about that today would be perfect for shooting?” reply Arech 1 hour agoprevCool...probably... But am I the only one who can't read gray text on a gray background? I can't get past it... reply RobCodeSlayer 16 hours agoprevThis is awesome! I’m curious what you used to make the 3D view? I’ve been trying to make something similar to control my telescope reply sshh12 16 hours agoparentThreeJS. You can checkout the code here https://github.com/sshh12/astro-app/blob/main/app/components... Was actually a lot easier to build this UI than I expected. reply nullhole 17 hours agoprevLooks nice! Where does the weather forecast data come from? I've used Clear Sky Charts (https://www.cleardarksky.com/csk/) in the past. Edit: scratch this question, it was addressed below: Also, is there a way to manually enter lat/lon/elevation? I poked around but it seems to want device location. reply sshh12 17 hours agoparentYup, just manually type your location in that window. Weather is coming (at least for now) from https://open-meteo.com/ reply nlunbeck 18 hours agoprevLooks incredible! Shows the value of a good onboarding -- really had me excited to see everything the app had to offer reply sshh12 18 hours agoparentThanks! This is the first time I invested in building any sort of onboarding seems like it's definitely changed reception reply jcurbo 17 hours agoprevI just want to pour one out for CalSky, that was a great page for setting up custom trackers for lots of astronomical things. It was very old school web too (for better or worse). reply edmundsauto 15 hours agoprevThis could be a cool application in VR - have you thought about publishing a version for AVP or Quest? reply sshh12 13 hours agoparentNope but that could be interesting reply pkage 18 hours agoprevLooks great! This is super cool. Where do you source your information for object locations? reply sshh12 17 hours agoparentThanks! When people search for objects, I import the data from SIMBAD Astronomical Database. Also just a lot of manual bootstrapping. reply stevage 11 hours agoprevNeeds to automatically detect timezone. reply sshh12 11 hours agoparentIt does if you select use device location reply bbor 10 hours agoprevYour app is awesome and I'm looking forward to playing with it when it's not being hugged to death :). That said I would definitely suggest reading some design books or taking an online class - the app is obviously beautiful but IMO violates some traditional design principles, namely consistency issues like \"buttons should be rounded and CTA-colored and small, all clickable elements should share a motif\", etc. I recommend Donald Norman, specifically. Again, awesome app. Windy.app blew me away (no pun intended), and I think this is a logical next step. reply jdsleppy 15 hours agoprevYou almost perfectly recreated the styling of Astro Space UX Design System https://www.astrouxds.com/ reply sshh12 13 hours agoparentOh wow. This is all https://www.tremor.so/ dark mode. reply samstave 18 hours agoprevHow about an alert for when [thing] is going to be above your house? reply modeless 17 hours agoparentMy site has this but only for satellites. https://james.darpinian.com/satellites/ For OP I'd suggest doing calendar events for notifications rather than Web Push. Web Push is very difficult to set up, users are understandably suspicious of it, and it doesn't guarantee timely delivery on Android devices. Calendar events work well on mobile and don't require any permissions. reply sshh12 16 hours agorootparentThanks! reply samstave 4 hours agorootparentprevPlease tell me about your ~~home world Usal~~ reason to track things above you? reply solardev 16 hours agoprevHey OP (or a mod?), can we consider editing the title to say \"Astronomy App\" instead of \"Astro App?\" I accidentally skipped this at first because I thought it was just somebody's personal page built in the Astro JS framework: https://astro.build/ reply pvg 14 hours agoparentHN titles don't need to be completely unambiguous or self-explanatory, it's ok to have to click on a thing to figure out what it is. https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply solardev 14 hours agorootparentIt's not a matter of blindly following the rules, just that a homebrewed astronomy app is a lot more interesting than yet another implementation of yet another JS framework. The ambiguity is doing a disservice to the cool thing the OP actually built, and I think this thread deserves more attention. reply pvg 14 hours agorootparentThe point of these mod comments is not that they are 'rules', it's that a bit of ambiguity is good for curiosity. The thread itself has about as much attention as HN can give a post: https://hnrankings.info/39591529/ reply solardev 13 hours agorootparentIt's an interesting point... that ambiguous titles can help spark curiosity and lure people in. I wonder, has that ever been tested/looked at somewhat scientifically? reply pvg 13 hours agorootparentNo idea (it's hard for me to imagine how to even design such an experiment but I'm no curiosity scientist) although HN's weird title rules are probably the thing (out of the various weird rules) that the HN hive mind has expended the most thinker bees on. So if nothing else, they're HN-tested! reply jcoder 15 hours agoparentprevBut the app is literally called “Astro App” reply solardev 13 hours agorootparentThat's gonna be hard to find on SEO :/ reply guptarohit 15 hours agoparentprevThis. At first glance from title of this submission I thought it's an astrology app. reply sshh12 16 hours agoparentprevAm I able to change the name? Thought that was mod only reply cglong 15 hours agorootparentPretty sure OP can change the name for a limited time. That's probably expired by now unfortunately reply dandigangi 16 hours agoparentprevHeh, thought the same thing when I popped it open. reply pimlottc 16 hours agoparentprevFastest way is to use the contact us link at the bottom of the page reply sdwrj 14 hours agoparentprevI almost did the exact same reply azangru 18 hours agoprev> Uncaught Error: Minified React error #425; visit https://reactjs.org/docs/error-decoder.html?invariant=425 for the full message or use the non-minified dev environment > Access to fetch at 'https://sshh12--astro-app-backend.modal.run/' from origin 'https://astro.sshh.io' has been blocked by CORS policy I think something... doesn't look right. reply sshh12 17 hours agoparentI think this might be due to load unfortunately. reply BillSaysThis 15 hours agoprevMaybe just my aging eyes but the warning to HN visitors of grey text on black background is HIGHLY unreadable. reply abroadwin 18 hours agoprevCan't get past the location setup. I click \"use device location\", authorize it in the browser, click save, and nothing happens. reply richrichardsson 18 hours agoparentSame. Google Chrome, macOS Sonoma. reply sshh12 18 hours agorootparentSorry y'all, it's the hug of death right now. Way underestimated the traffic this would get. reply ChrisArchitect 18 hours agoprevFun stuff. Maybe work words like \"sky\" \"explorer\" etc into the title tho..... This being HN I thought maybe this had something to do with https://astro.build/ reply sshh12 17 hours agoparentGood idea! reply 6footgeek 18 hours agoprevThis is great! I usually use https://telescopius.com/ but this has just made it onto my bookmarks, well done! reply sshh12 18 hours agoparentOh wow that's also great reply causal 18 hours agoprevLooks like the only way to use it is giving location permission? Was excited to try it but probably won't until there is a postal code option. reply nsriv 18 hours agoparentThe very next screen after the modal allows you to change your location or use device location, upon which the Chrome permissions modal fires. reply sshh12 18 hours agorootparentYup! Totally understand people might not want to share their location, so if you \"skip\" I just put you in LA. reply pvg 18 hours agorootparentI just put you in LA. That should probably be 'I just put you in LA and only make the moon visible' reply sshh12 17 hours agorootparentHaha yeah you are stuck in Bortle 9 until you fixed it reply modeless 17 hours agorootparentprevIP geolocation works surprisingly well for most people. It'll be a better default than any fixed location. reply causal 12 hours agorootparentprevYeah my mistake, didn't see the skip button. reply endigma 17 hours agoprev [–] Should disambiguate from https://astro.build/ and various other name collissions in your title somehow, maybe \"Astro AppExplore the Night Sky\" ? Also the \"Identifiers\" section and some other things have extremely low contrast text, 2.35:1 instead of the recommended 4.5:1 for body-size text. You can check on issues like this using the accessibility tab in Firefox or Lighthouse in Chrome. reply sshh12 17 hours agoparent [–] Good call! reply endigma 17 hours agorootparent [–] I also added a bit about accessible text contrast reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The speaker developed a personal tool inspired by NINA's sky atlas and Robinhood to address the lacking specific sky object planning features in Stellarium and SkySafari.",
      "This newly created tool enables users to access altitude charts, compile lists of favorite objects, check annual maximum and minimum daily altitudes, and utilize real-time cloud data."
    ],
    "commentSummary": [
      "The Astro App, developed by sshh12, is a new astronomy application blending sky object planning with location-specific weather data, drawing inspiration from Stellarium and SkySafari.",
      "Users can access altitude charts, compile object lists, and monitor real-time cloud coverage, with feedback suggesting improvements like cloud direction tracking and deep sky object recommendations.",
      "The app utilizes data from the SIMBAD Astronomical Database and weather updates from open-meteo.com, with some users highlighting concerns about text legibility and location configuration, although it is generally praised for its functionality and growth prospects."
    ],
    "points": 264,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1709565875
  },
  {
    "id": 39588830,
    "title": "Singapore Airlines' Troubled Concorde Service: A Brief History",
    "originLink": "https://mainlymiles.com/2024/03/03/singapore-airlines-concorde-full-story/",
    "originBody": "History Singapore Airlines Singapore Airlines Concorde: The full story by Andrew3 March 2024, 13:003 March 2024, 13:04 53 Comments on Singapore Airlines Concorde: The full story Singapore Airlines once operated a Concorde between the Lion City and London, but politics and economics meant it didn't last. In the late 1970s and early 1980s, Singapore Airlines operated the world’s most popular supersonic passenger airliner – Concorde – on flights between Singapore and London, as part of a joint venture with British Airways. ADVERTISEMENT The service slashed flying times between Asia and Europe from around 18 hours on subsonic aircraft with multiple stops to under 10 hours, for those who could afford a ticket some 15% more expensive than a First Class fare. British Airways Concorde G-BOAD wearing Singapore Airlines livery at London Heathrow Airport in July 1980. (Photo: Steve Fitzgerald) Catering mainly to time-pressed business travellers, celebrities and politicians, the service was short-lived as it suffered from waning demand and rising fuel costs, leading to heavy losses. Scratch a little below the surface though, and the story of Concorde’s time with Singapore Airlines is a fascinating one, mired in politics of one sort or another from almost start to finish. Here’s our look back at how the history of SIA’s Concorde operation unfolded. Contents Concorde in Singapore SIA wanted its own Concordes India refused overflight Joint venture with British Airways The aircraft wore both liveries The inaugural flight was almost scuppered Malaysia stands firm Indonesia wouldn’t commit 1978: Free advertising! Malaysia agreement 1979: Services restart The schedule Who flew on Concorde? Which crew? Onboard experience Example menu Further SIA services were planned Singapore bank notes First year review Incidents The end of Concorde Singapore flights What happened to G-BOAD? G-BOAD today The Concorde Room Summary Concorde in Singapore The history of Concorde in Singapore dates right back to 1972. On 8th June that year, thousands flocked to Singapore International Airport at Paya Lebar to watch the first Concorde land from Bangkok on its promotional sales tour. Singapore was seen as a key stopover point for Concorde’s future ambitions to cut travel times from Europe to Australia, so the manufacturer was keen to woo the government and regulators here to ensure smooth permissions would be granted in future, even if SIA itself didn’t buy the jets. Concorde on its tour through Singapore in June 1972. (Photo: National Archives of Singapore) After three days in Singapore the aircraft left for Hong Kong and Tokyo, later flying to Australia before returning via Singapore once again on its way back to Europe. SIA wanted its own Concordes In June 1975, six Singapore Airlines cabin crew were handpicked to take part in trials by the aircraft manufacturer, simulating normal passenger service from London to cities like Beirut, Kuala Lumpur, Singapore and Melbourne. It was a great coup for the airline to demonstrate how SIA’s renowned service could still be offered on a supersonic flight, and by September that year it was reported that Singapore Airlines was negotiating to lease two Concordes. The proposed arrangement would start in 1977, for operation between Australia and Europe via the Lion City. Flight times between Singapore and London would be cut from 18 hours on conventional aircraft at the time to just 10 hours. ADVERTISEMENT With a S$100 million price tag per jet (S$257 million in today’s money), buying the aircraft was “out of the question” according to SIA’s Managing Director Lim Chin Beng, but the carrier was said to be keen if an acceptable lease deal could be forged. The pair of aircraft would have replaced two Boeing 747s the airline had on option for delivery in 1977 and 1978. Two proposed Boeing 747-200s could have been sidelined for a pair of leased Concordes in 1977 and 1978, as part of SIA’s desire to join the supersonic age. (Photo: Frank C. Duarte Jr.) However, that plan never materialised. Over a year later, in November 1976, British Airways (BA) was negotiating with the Singapore Government to start a London – Bahrain – Singapore service using Concorde, while the airline was still blocked from flying the jet to and from New York – its most desired route. It was proposed that SIA would lease 20 out of 100 seats on each of these flights to address “the likely loss of First Class passengers” on SIA’s own London services. India refused overflight BA’s problems weren’t isolated to a ‘forced pact’ with SIA on a future Singapore route. It didn’t take long for politics to start getting in the way of plans to fly Concorde to and from Asia. In the late 1970s, India refused permission for Concorde to fly over its airspace, blocking a direct routing from Bahrain to Singapore, BA’s proposed extension to an existing service. On the table was more access to Heathrow slots for Air India, plus ‘fifth freedom’ traffic rights from the London airport, with the airline keen to tap in to the lucrative transatlantic market. In the 1970s and 80s, it was common for governments to use air traffic agreements as political chess pieces. Sonic Boom It’s worth mentioning at this point that the problem for Concorde was primarily related to its ‘Sonic Boom’, caused by shock waves when the jet was travelling through the air at Mach 1 or greater. That was no problem for the passengers; sitting on the jet at the head of the ‘Mach cone’, they were moving so fast the sonic boom didn’t have chance to catch up to them. For those on the ground it was a different matter, creating a 100dB+ sound like an explosion. That meant supersonic flight by Concorde was restricted to overwater or sparsely populated land masses like deserts. Even that wasn’t perfect though, with Saudi Arabia withdrawing supersonic permission for Concorde after nomads reported the sonic boom was upsetting their camels, stopping them from breeding! Ultimately the two sides agreed to a compromise, with Concorde flights from Bahrain to Singapore having to route around the Indian mainland, adding 200 miles to the journey. Joint venture with British Airways By late 1977 Singapore Airlines was in negotiation with British Airways to jointly operate a Concorde on the London – Singapore route under a cost and revenue-sharing basis. The supersonic trip would cut flying time to 10 hours, with only a 40-minute refuelling stop in Bahrain. Plans included an extension of the route from Singapore to Melbourne, Australia, though this never went ahead. British Airways was already operating Concorde services from London to both Bahrain and Washington in 1976/77. (Photo: Steve Fitzgerald) Singapore Airlines formally made an agreement for the joint venture Concorde operation with British Airways in October 1977, with the inaugural service from Paya Lebar planned for 10th December that year. As part of the deal, SIA sent 36 of its cabin crew to London in November 1977 for training on Concorde operations. Tickets could be purchased at BA or SIA offices and cost around 15% more than First Class fares on existing subsonic flights between the two cities. ADVERTISEMENT The one-way fare was S$4,298 (around S$10,700 today) from Singapore to London and S$2,522 (around S$6,300 today) from Singapore to Bahrain. (Image: Heritage Concorde) British Airways said it needed to fill 60 of the 100 seats on board to break even, though the capacity was capped at 86 passengers when departing from Singapore due to the humid conditions affecting the aircraft’s performance. Even with the inaugural flights fully booked, Singapore Airlines launched a global advertising campaign in December 1977, telling the world “SIA has gone supersonic”. (Image: Andrew Chuang) The aircraft wore both liveries Part of the agreement between Singapore Airlines and British Airways was to operate the Concorde, registered G-BOAD (also temporarily G-N94AD), in a dual livery with Singapore Airlines titles on the left and British Airways colours on the right of the aircraft. This became the only Concorde to fly with the colours of two different airlines, making SIA only the third airline in the world (after BA and Air France) to have its livery on the type. The ‘machmeter’ in the passenger cabin of G-BOAD. (Photo: Bob Nadel) With passengers boarding via steps in Singapore in those days, into the forward left main aircraft door, it meant only the SIA livery was visible. The inaugural flight was almost scuppered Prior to the start of the route, there was a disagreement between British Airways and a small group of its Concorde pilots who were to be stationed in Singapore, over the terms and conditions of their detachment. This was resolved a few weeks before the first flight, but a much bigger problem was looming. With a Concorde now adorned in Singapore Airlines colours down one side and British Airways livery across the other, and cabin crew from both carriers fully trained to jointly operate the new service, a spanner was suddenly thrown in the works. ADVERTISEMENT At the eleventh hour, on 7th December 1977, the Malaysian Government threatened to hold back overflight rights for the new service. Malaysia had presented no formal reasons to deny Concorde’s overflight of its airspace, indeed the jet had previously routed over the country on demonstration flights and made several landings at Kuala Lumpur’s Subang Airport. “We were told verbally yesterday by the Malaysian Department of Civil Aviation that overflying rights have still not been granted. “No reasons have been given, either verbally or in writing. “We don’t know whether this refers to supersonic or subsonic speed, but if they allow us to fly subsonic, we will do so. “It will add several minutes to the flying time, but it is not significant.” Ted Duggan, British Airways PR Manager for the Orient, 8th December 1977 (via New Nation) The day before the inaugural flight was due to depart, the Malaysians formally refused all overflight permission for Concorde, subsonic or supersonic. “The Malaysian authorities have decided to refuse permission for the Concorde flight over Malaysian airspace.” Malaysian Communications Ministry (via The Straits Times) No reason was given, though later a minister said they had to “look into matters affecting the people’s interests and environmental factors”. Several newspapers in London were having none of it, stating that the “extremely puzzling” stance was “likely [because] Malaysia is piqued at the operation by Singapore [Airlines] of Concorde service”. An aviation source said of Malaysia’s environmental reasoning “Rubbish, absolute rubbish. The problem is purely political”. “When Britain first approached Kuala Lumpur earlier this year for permission to fly over Malaysia, the contemplated service would have been wholly British. “But sources said Malaysia’s objections crystallised when British Airways and Singapore [Airlines] agreed to operate the flight jointly.” Business Times, 10th December 1977 While the Malaysians were standing firm, the Singapore Government came to the rescue, striking a last-minute deal with Indonesia for the aircraft to route slightly further south than originally planned, through Jakarta’s airspace instead. The agreement was said to have been a reciprocal gesture, following “Singapore’s support for Indonesia during a recent United Nations debate on East Timor”. The route was saved and the inaugural flight, which had started to look as though it would have to be postponed, took off from London as scheduled on 9th December 1977, to a traditional ceremonial lion dance. The SIA / BA Concorde departing from London Heathrow. (Photo: Jonathan Francis via flickr) The flight touched down in Singapore at 5.35am the following morning, 25 minutes early, with Singapore Airlines cabin crew having taken over from the BA crew in Bahrain. The inaugural SIA crew on the first arrival in Singapore. (Photo: National Archives of Singapore) Only 90 passengers could be accommodated instead of the planned 100, due to the additional fuel required when taking the Indonesia routing, while only 70 passengers could travel from Singapore to Bahrain instead of the planned 86, for the same reason. Details of the first Singapore – Bahrain – London service on Concorde (SQ301/BA301) on 10th December 1977. Lucy Chan was one of the flight attendants on the inaugural SIA Concorde flight. (Photo: National Archives of Singapore) Malaysia stands firm On 12th December 1977, the day of the second Concorde departure from Singapore to London, authorities revealed a new hiccup. Indonesia’s overflight permission for the aircraft was only valid for one week, covering the first three flights. That’s because the original agreement had been hammered out at a meeting between Singapore officials, British Airways and the Indonesian Directorate General of Air Communications, during an aviation conference taking place in the Lion City a few days earlier. It was therefore regarded as a temporary operational measure, valid only for a week. Indonesian Government approval was not required, but would be needed for a longstanding agreement beyond the initial flights. ADVERTISEMENT British Airways Captain Tony Meadows, who flew Concorde on its inaugural flight into Singapore from Bahrain on 10th December 1977, was dispatched to Jakarta later that day to assist British officials there “on any issue” related to the flights. British Airways Captain Tony Meadows flew the inaugural Bahrain – Singapore Concorde flight, then headed to Jakarta to assist with overflight negotiations. (Photo: Paul Fievez / Shutterstock) Meanwhile Gordon Davidson, British Airways Concorde Director, remained in the region rather than returning back to London as planned, and instead headed to the British High Commission in Kuala Lumpur, hopeful a deal could be struck with Malaysia to end the quagmire. Indonesia wouldn’t commit Not wishing to upset its friendly relations with Malaysia, the Indonesian Government was reluctant to approve further overflight permissions beyond the first three services, potentially leaving Britain and Singapore back at square one. Striking a deal with Malaysia was seemingly the only option, though British Airways did not seem overly confident. “If the problem is genuinely environmental, then we should certainly be able to convince them… “If Malaysia’s decision is based on reasons of professional rivalry, however, then there is nothing we can do about it.” British Airways official, 13th December 1977 (via Business Times) With the third and final service blessed with Indonesian overflight approval already on its way to Singapore, last-minute talks with Kuala Lumpur were still ongoing. Indonesia said it would only approve overflight if the Malaysians also did so. That wasn’t much use to British Airways or Singapore Airlines – they needed agreement from one or the other. The Indonesian stance meant it was all or nothing. The fourth Concorde flight BA300 departed from London bound for Bahrain and Singapore on Friday 16th December 1977 as planned, with no one knowing whether it would be allowed to continue its journey from the Middle East. Even with Britain suggesting a compromise ‘interim agreement’ for three months while discussions were ongoing, Malaysia then reaffirmed their refusal of overflight permission, meaning the Concorde was forced to return to London after arriving in Bahrain that night. 16 passengers who boarded in London destined for Singapore, who were reportedly “very unhappy”, were reaccommodated in First Class on subsonic flights. Later asked why he had not intervened in the negotiations, instead leaving it to the British, Singapore Prime Minister Lee Kuan Yew said: “If I believed that there was anything I could say or write or do that would help, I would have done so. “There are times where there is nothing one can do better, than to do nothing.” Lee Kuan Yew, December 1977 (via New Nation) Fundamentally LKY was right about the issue – it really wasn’t for Singapore or SIA to get involved in what was an agreement purely between the British and the Malaysians. As the Business Times put it: “SIA is merely the British company’s operating partner. It does not own the Concorde nor has it leased the aeroplane. The entire issue of overflying is therefore solely between Britain and Malaysia.” Business Times, January 1978 1978: Free advertising! With all the political obstacles for overflight permissions related to this route, it would probably have been easy for all parties involved to simply abandon the idea after these three return flights were completed. British Airways, Singapore Airlines and their respective governments were undeterred, however, and remained fully committed to the concept. “The route will be back”, said one SIA official. ADVERTISEMENT The aircraft’s dual colour scheme, with SIA livery on the left and BA livery on the right, was therefore retained in anticipation of the Singapore route’s return. That gave SIA the unique opportunity for some lucrative free advertising during 1978, with G-BOAD fitting back in to regular BA Concorde operation around the world. G-BOAD on a ‘free advertising’ visit to New York JFK. (Photo: Aris Pappas) The aircraft regularly visited Bahrain, New York, Washington D.C. and Dallas on scheduled services. There were even charter flights, including the use of G-BOAD in 1978 on a Barcelona – New York flight for the top customers of a large Spanish bank! Malaysia agreement After a series of talks between Britain and Malaysia throughout 1978, most of which came to nothing, a breakthrough was finally reached on 15th December that year. “On 15 December 1978, one year after the end of the first services the Malaysian Government withdrew its objections to Concorde flights for a trial period.” Singapore Airlines statement Malaysian officials had finally agreed to lift the overflight ban for SIA-BA Concorde services for an initial period of six months, in what SIA’s Managing Director Mr. K. Kulasegaran described as “terrific news, quite the best I have heard for a long time”. ADVERTISEMENT The decision was reached after a year-long study by Malaysia revealed “no conclusive evidence” that Concorde flights over its airspace would damage the environment. Indonesia also granted joint permission for the service to be reinstated using its airspace on the same basis. 1979: Services restart With an agreement sealed, SQ300/301 (also operated as BA300/301) was back on, with three-times weekly flights restarted from London on Wednesday 24th January 1979 and the first Singapore – London flight operating the following day. A British Airways advert promoting Concorde services in 1980. The 9.5-hour flight time to Singapore was around half the subsonic flight duration of the time BA and SIA released a joint statement 12 days before the service restarted saying the flights would operate over Malaysia at subsonic speed for only five miles and for less than two minutes in each direction – perhaps a dig at the minor nature of what the airlines saw as an unnecessary year-long blockade. A British Airways promotion of Concorde’s ‘not-so-Far East’ service. (Image: The Airchive) The schedule The dual-liveried Concorde G-BOAD was a busy aircraft, flying three times a week between London and Singapore in each direction. Only on Tuesdays did the jet get a decent break, with 48 hours off in London for maintenance downtime from Monday afternoon through to Wednesday afternoon. (Photo: Kjell Nilsson) The actual service timings varied slightly during the flight’s operation, but here is an example schedule from 1979. SIA Concorde flight schedules in 1979Days M T W T F S SSQ301 SIN 11:30BAH 10:55 Duration: 03:55BA301BAH 11:45LHR 14:05 Duration: 04:20BA300 LHR 15:30BAH 21:45 Duration: 04:15SQ300 BAH 22:35SIN 07:10* Duration: 04:05 * Next day Actual flying time from Singapore to London was 8 hours 15 minutes, plus a 50-minute stop in Bahrain for a total time of 9 hours 5 minutes. Flight time between London and Singapore, was 8 hours 20 minutes (9 hours 10 minutes once the stop in Bahrain was accounted for). The Singapore Airlines Concorde at Paya Lebar Airport. (Photo: Doug Green / Global Aviation Resource) The aircraft used a British Airways flight number between London and Bahrain (BA300/301) and a Singapore Airlines flight number between Bahrain and Singapore (SQ300/301). Flight numbers were later changed to BA16/17 and SQ16/17. Another example concorde schedule in the SIA timetable Who flew on Concorde? A survey conduced in 1980 revealed that 17% of Concorde’s passengers on the SIN-BAH-LHR route were Singapore residents, 20% were from Australia or New Zealand and the largest group, 38%, were British residents. Not surprisingly, 77% were travelling for business purposes, with only 18% using the service to take a holiday. 42% of passengers on the route were connecting to another city after arriving in either London or Singapore. On 24th April 1980, Princess Margaret, younger sister of Britain’s then-Queen Elizabeth II, arrived in Singapore on the joint BA-SIA Concorde service for an official visit. The trip also included the Philippines and Malaysia, with the princess then returning to London on Concorde from Singapore on 8th May 1980. Which crew? The SIA Concorde was flown exclusively by British Airways pilots and flight engineers. London-based pilots flew the London – Bahrain – London flights, while the airline had a detachment of Singapore-based pilots who flew most of the Singapore – Bahrain – Singapore legs. In both cases the pilots spent one or two-night rest periods in Bahrain while away from base. Fun fact: When the joint venture was originally agreed, the plan included some Singapore Airlines pilots going to London for Concorde training, to operate half of the flights, however this did not materialise. For the cabin crew it was a different matter, with the flights shared between Singapore Airlines and British Airways. The two sets of flight attendants (SQ and BA) would not mix on any single flight, instead alternating on consecutive sectors. You could therefore have an all-Singapore Airlines crew from Singapore to Bahrain, followed by an all-British Airways crew from Bahrain to Singapore, or vice-versa. Three flight crew (two pilots and a flight engineer) and four cabin crew operated each Concorde flight. Onboard experience Concorde was a small plane with only 100 seats in a 2-2 configuration. It also had a low ceiling and tiny passenger windows by today’s standards, about the size of an adult hand. Without the space of large wide-body jets like the Boeing 747, there was certainly no room for large seats or flat beds! ADVERTISEMENT Instead the aircraft’s short flight time on its designed routes like London to New York meant a seat more akin to Premium Economy Class today, with the focus on speed and the distraction of fine dining to pass the little time passengers spent on board. The Concorde cabin installed on British Airways aircraft in the 1970s and 1980s. (Photo: Jon Proctor) Indeed Concorde passengers were always generously fed and watered on any journey. Menus included vintage Champagnes like Dom Ruinart 1973 and Mumm Cordon Rouge 1975, cocktails, fine wines, caviar and lobster canapés, plus a selection of opulent main dishes. Even just before Concorde’s retirement in the early 2000s, BA stuck to its vintage Champagne concept on board Here’s an example lunch service on a joint SIA-BA operated Bahrain – London flight. Bahrain – London Aperitifs – Champagne Canapés Lunch Chilled Iranian Caviar Grilled Fillet steak with Cafe de Paris butter Breast of duckling Montmorency Buttered asparagus spears Cauliflower milanaise Croquette potatoes Palm heart salad . Roquefort dressing Assorted cheeses Fresh strawberries. Double cream Coffee After dinner, passengers were even offered Havana cigars. Concorde’s food and drink service was reminiscent of a ‘golden age’ in air travel, only found in some First Class cabins today. (Photo: Jim Sugar) All passengers who flew on Concorde received a personal flight certificate, and the joint BA-SIA services were no exception. The certificate would typically be handwritten and include the signatures of the Captain and other crew members. With short flight times, Concorde was never fitted with a video in-flight entertainment system. Instead only radio programming was available. (Photo: The Airchive) Further SIA services were planned With the reinstatement of SIA’s London route on Concorde in January 1979, the airline was talking up future routes for the type, once they had data on how well it operated to and from London. The Financial Times reported that “from Singapore, the possibilities for route expansion are considerable, including flights to such points as Hong Kong, Manila, Seoul [and] Japanese destinations”. Linking Singapore to Hong Kong with Concorde would have taken only 90 minutes, while Singapore to Tokyo on the supersonic jet would have cut the flight time from 7 hours to just 3 hours. Singapore bank notes Having Concorde in the Singapore Airlines livery was a huge marketing point for a young, independent Singapore, and in 1979 the ‘Bird series’ of bank notes included the aircraft on the back of the $20 version. The reverse side of Singapore $20 notes featured G-BOAD in SIA livery flying over an artist’s impression of the future Changi Airport. It was actually the left side of the aircraft that featured the SIA livery, however! Concorde was retained on the note for a few years even after the final joint BA-SIA service departed in October 1980. The Concorde also featured on a stamp commemorating Paya Lebar Airport after commercial flights had transferred across to Changi. First year review By January 1980, a year after regular Concorde services between London and Singapore had restarted, British Airways was expecting a loss of between S$14 million and S$18.5 million on the route for the January 1979 to March 1980 period, with forecasts of a similar deficit in the subsequent year. At the time, BA was coming under increasing pressure from the new British government to turn a profit, with the state reducing its stake in the carrier. ADVERTISEMENT BA executives were also said to be unhappy with the cost sharing formula they had with Singapore Airlines for the services. Despite SIA and BA sharing operating costs for the flights, including for fuel and crew, a “slide-over fee” had been agreed from the outset to account for SIA’s forecast loss of First Class traffic on its own flights. This was said to be costing BA S$4.9 million a year, which the British carrier felt was unjustified given SIA’s recorded increase in First Class passenger numbers to and from London. BA was also said to be unhappy with SIA’s lacklustre promotional efforts for the Concorde service, claiming it provided twice as many passengers for the Concorde flights as SIA, despite a 50-50 capacity share. Despite the issues, SIA and BA reached an agreement to continue the services, though the “slide-over fee” from BA to subsidise SIA for its loss of First Class subsonic traffic was not reduced. Incidents Despite the SIA Concorde service only operating for a couple of years, there were a few ‘minor incidents’ along the way! On 1st February 1979 the regular London – Singapore Concorde flight was forced to divert to Kuala Lumpur, when a Singapore Airlines training aircraft burst a tyre and blocked the runway at Paya Lebar. It was only the sixth Concorde service since flights had been reinstated the previous month. On 31st May 1980, a Concorde flight departing from Paya Lebar on its way to Bahrain and London blew the roof off a family home. Singapore Airlines refused to pay for the damage – claiming British Airways was responsible. BA did not admit liability, but paid the family S$500 (around S$1,100 in today’s money) “to help them out”. On 21st July 1980, a British businessman was stabbed in the head by another passenger on a joint BA-SIA Concorde flight from London to Singapore. The SIA cabin crew and other passengers restrained the woman, who had been behaving eccentrically and did not know the man. “I still don’t know why she attacked me, I never saw her before in my life”, the man said. On 22nd July 1980, the same Concorde flight with the stabbed man(!) hit a flock of birds on its approach to Paya Lebar Airport, which were ingested by two of the four engines. Though the passengers on board were unaware, one of the engines was extensively damaged. A replacement had to be flown out from London, delaying the return service for 30 hours. The end of Concorde Singapore flights Despite an earlier agreement to continue the Concorde service, by August 1980 it became clear that not all was well. There were reports of only 50% load factors on flights from Singapore, down from 57% the previous year, amid signs the novelty of the service was wearing off. There were also complaints from passengers about the discomfort of the cabin compared to First Class seats on wide-body jets, with Concorde’s small seats and a much louder environment being far from ideal on a nine-hour trip. Singapore Airlines and British Airways held further talks, amid rumours of the service now running at a S$20 million annual loss (equivalent to around S$42 million today), with rising fuel prices adding to the impact. On 16th September 1980 is was announced that Concorde services to and from Singapore would be suspended, effective from 1st November 1980. The Gazette, 16th September 1980 High fuel costs and fewer passengers left BA and Singapore Airlines with little choice but to abandon joint Concorde services in 1980 The final BA/SIA jointly operated Concorde service departed from Singapore on 30th October 1980. British Airways later flew all Concorde spare parts and equipment back to London. Though no regular Concorde flights then served the Lion City, a BA charter flight from London to Singapore via Bahrain and Colombo did operate in March 1985. French President Francois Mitterrand also passed through in September 1986, on his way back to Paris from a state visit in Indonesia, in an Air France Concorde. These marked the types’s only two visits to Changi Airport, which replaced Paya Lebar as Singapore’s main passenger hub in 1981. What happened to G-BOAD? After returning from Singapore for the last time on 30th October 1980, G-BOAD had its Singapore Airlines titles removed and was returned to a full British Airways livery. The aircraft went on to complete its service with BA, mostly to and from New York which became the airline’s ‘cash cow’ route for the type. ADVERTISEMENT Here are some key facts about G-BOAD: 25th August 1976: First test flight December 1976: Delivered to British Airways 7th February 1996: Fastest transatlantic crossing by any Concorde LHR-JFK at 2 hours, 52 minutes, 59 seconds 22nd October 2003: Final passenger service 10th November 2003: Final flight from London to New York The only Concorde to ever wear the colours of two different airlines (SIA and BA) The highest flight time of any Concorde flown (23,397 hours) 8,406 flights, 7,010 of which were supersonic G-BOAD, looking good as new, getting ready to depart on its last flight from London to New York on 10th November 2003. (Photo: David Apps) Fun fact: The aircraft was temporarily registered G-N94AD while operating in conjunction with US-based Braniff Airlines on flights between Washington and Dallas, as an extension of the London – Washington service. It never received a Braniff livery, however. G-BOAD today Next time you’re in New York, be sure to take a visit to the Intrepid Sea, Air & Space Museum in Manhattan. Even if the other exhibits aren’t of interest, you can get up close and personal with G-BOAD, the very Concorde used by Singapore Airlines between 1977 and 1980. G-BOAD at the Intrepid Museum in New York. (Photo: MainlyMiles) Sadly it’s still in British Airways livery on both sides (SIA would probably have had to pay this time for the advertising!). We were lucky enough to visit during our round-the-world trip in 2018 and it’s well worth a look. The cabin of G-BOAD at the Intrepid Museum in New York, with BA’s latest Concorde seats by factorydesign. (Photo: MainlyMiles) The striking features are the aircraft’s small external size (10% shorter than an A350-900) and very narrow cabin (at 2.62m wide, about the same as an ATR turboprop and less than half that of a Boeing 777). The cockpit of G-BOAD at the Intrepid Museum in New York. (Photo: MainlyMiles) For the AvGeeks, the flight deck is also pretty cool – a far cry from the two-crew, highly digitised cockpits of today. G-BOAD now stares out at New York’s Manhattan skyline. (Photos: MainlyMiles) The Concorde Room Though it’s no longer possible to fly on Concorde, the name is still used by British Airways for its most exclusive First Class lounges in London and New York. In times gone by, these lounges were reserved exclusively for Concorde passengers, with boarding directly from the lounge itself. We passed through the London Heathrow Airport Terminal 5 facility in 2018 before flying First Class on BA to Washington, and wrote a full review of the experience. British Airways Concorde Room LHR READ OUR REVIEW British Airways Concorde Room LHR READ OUR REVIEW The business centre in the lounge even features some of the old seats from BA’s Concordes. Former Concorde passenger seats in the Business Centre of the Concorde Room. (Photo: knightofmalta) For a while the First Class section of the British Airways Singapore Lounge was called ‘The Concorde Bar’, however it has since been renamed ‘The Bar Singapore’. Summary As if a joint venture between Singapore Airlines and British Airways doesn’t sound fraught enough, the introduction of Concorde service in a co-operation between the two carriers became quickly mired in political disputes. These arose from either chess games over air traffic rights, or nationalistic jealousies, though both were commonplace in the late 1970s. By the time the service truly came to life in 1979 and 1980, jet fuel costs had soared and longer routings owing to continued overflight issues with India were continuing to make the service less and less likely to turn a long-term profit. (Photo: Jonathan Francis via flickr) Sadly the last regular flight Concorde left Singapore in October 1980, closing a short but fascinating chapter in Singapore Airlines history. Concorde later went on to become profitable on the North Atlantic between London and New York, with many passengers willing to endure the aircraft’s drawbacks for the short flight time on this lucrative city pair. Singapore Airlines, meanwhile, can claim to be one of only three airlines in the world to have its livery adorn this rare aircraft. (Cover Photo: Jonathan Francis) Share this: Click to share on Facebook (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Telegram (Opens in new window) ADVERTISEMENT Tags: featured Published by Andrew Happiest in a First or Business Class bed flying to a faraway place, and even happier when it's using miles or points.View all posts by Andrew",
    "commentLink": "https://news.ycombinator.com/item?id=39588830",
    "commentBody": "Singapore Airlines Concorde (mainlymiles.com)210 points by qsi 22 hours agohidepastfavorite183 comments delta_p_delta_x 20 hours ago> Flight times between Singapore and London would be cut from 18 hours on conventional aircraft at the time to just 10 hours. This is very interesting, because current Singapore-Heathrow direct flights are around 13 hours. I wouldn't pay first-class++ fares for a 3-hour flight time reduction in a tiny, cramped cabin with worse pressurisation and ventilation than first class on modern A380s, B777s and A350s that currently ply the route. reply KptMarchewa 20 hours agoparentCurrently there are aircraft able to fly that distance without stopovers. That wasn't the case in 70s, including Concorde. Modern version of Concorde that would have the necessary range for that flight would do Heathrow to Singapore in 5-6 h. reply dredmorbius 19 hours agorootparentRouting in the 1970s (and until at least 1992) would also likely have been far longer, with most Soviet and Eastern European airspace closed to Western flights. Whether the lack of non-stop service was on account of routing and range or of lack of sufficient travel demand I don't know. Current routing appears to traverse Pland, Ukraine, Russia, Georgia, Afghanistan, Pakistan, and India. I could see much of that not being advisable in the 1970s.reply eigenket 19 hours agorootparentPlenty of that isn't advisable now. I flew Frankfurt to Singapore at the end of 2022 and we flew over the south side of the Black Sea to avoid Ukrainian and Russian airspace. We also avoided Afghanistan and flew over Iran instead which slightly surprised me. Looks like London-Singapore flights do the same https://www.flightradar24.com/data/flights/sq305#3437e32f reply lxgr 19 hours agorootparentIt's less the airlines being cautious (like they finally started being over Ukraine, but not until MH17): Russia has just closed their airspace to most Western airlines (and vice versa). This leads to sometimes quite extreme differences in travel times between two cities depending on whether a Western or e.g. Chinese airline are conducting the flight (e.g. London-Shanghai), since Chinese airlines can still overfly both Europe and Russia. reply kccqzy 16 hours agorootparentOh yes, a while ago the Chinese airline Air China wanted to fly from New York to Beijing over Russian airspace (the shortest path does involve Russian airspace) but the U.S. government prohibited them from doing that as a matter of fairness. The end result is that the Chinese airline had to operate a domestic flight from New York to Los Angeles in order to fly to Beijing. https://viewfromthewing.com/air-china-has-filed-to-fly-new-y... reply chimeracoder 14 hours agorootparent> The end result is that the Chinese airline had to operate a domestic flight from New York to Los Angeles in order to fly to Beijing. This isn't unheard-of. Qantas does (or used to) do this as well, running SYD-LAX-JFK, for efficiency reasons. Cabotage laws prohibit 081/CCA from carrying any passengers on the domestic leg of that route except the ones that are booked on the international leg of the journey as well. So it's not like you can just book a flight with them domestically. reply lxgr 12 hours agorootparentThis concept is called \"freedoms of the air\" in case anyone is curious about learning more: https://en.wikipedia.org/wiki/Freedoms_of_the_air reply avar 1 hour agorootparentprevIt's less the airlines being cautious (like they finally started being over Ukraine, but not until MH17) You are misrecalling the chronology. Some airlines such as British Airways were avoiding that airspace before MH17 was shot down, it was making the news beforehand. Other airlines such as Malaysia Airlines continued rolling the dice on flying in an active warzone. What changed after MH17 was that avoiding it was mandated by the relevant authorities. reply eigenket 18 hours agorootparentprevSingapore Airlines (who I flew with) wasn't barred from Russian airspace but they have decided to voluntarily avoid it. reply eastbound 15 hours agorootparentAnother thing into play are the airspace fees. It’s a substantial income for Russia. It’s possible that some airlines estimate that they are too high, just out of economic factors and not political factors. https://simpleflying.com/russia-overflight-charge-hike/ They skyrocketed (pun intended) by 20% just in 2023, amount to $1.7bn and are justified by the radar and route operators, and by the …security provided by the Russian army. After downing the MH317, many airlines avoided the Ukrainian airspace… in profit of the (unavoidable?) Russian one, further benefiting Russia for this horrible crime. Killing those people may even have provided more revenue to Russia. reply mrtksn 16 hours agorootparentprevThat's just marketing image some graphic designer made. For the actual current route, check the flight tracking services like this: https://planefinder.net/data/flight/SQ305/history/4-48720917 So, it's UK->Belgium->Germany->Czechia->Austria->Hungary->Romania->Bulgaria->Turkey->Iran->Pakistan->India->Malaysia->Singapore reply khuey 19 hours agorootparentprevThat's clearly not the actual route aircraft are taking since it flies directly over the warzone in eastern Ukraine. reply dredmorbius 18 hours agorootparent\"Appears\" is doing some heavy lifting in my comment, and was there for a reason. If you know of a more representational route I'd appreciate your sharing it. My previous link was based on a search for London-Singapore air routes. Hrm ... I suppose FlightAware or FlightRadar24 might show this, and yes, it does. BA21 seems to fly over Russia per FlightAware:SIA308 flies a more southerly route, crossing the Black Sea and Turkey rather than overlying Ukraine, and conspicuously avoiding Afghan airspace:reply rootusrootus 18 hours agorootparent> BA21 seems to fly over Russia per FlightAware: \"ARRIVED OVER 11 YEARS AGO\" reply dredmorbius 16 hours agorootparentGood call, I was relying on a search for flights and didn't check to see if that was current. BA11 seems to be among British Airways current LHR->SIN offerings. Here's yesterday's flight path, which largely resembles Singapore Airways. Flight time 13h15m.reply diggan 18 hours agorootparentprev> Current routing appears to traverse Pland, Ukraine, Russia, Georgia, Afghanistan, Pakistan, and India. I could see much of that not being advisable in the 1970s. When I flew from Helsinki, Finland to Hong Kong this summer, the plane avoided flying through Russian airspace, so not sure it's much better now than in the 70s, for some routes at least. \"Avoiding Russian airspace: From a shortcut to a detour\" - https://www.finnair.com/en/bluewings/world-of-finnair/avoidi... > On Monday 28 February 2022 Russia closed its airspace as a countermeasure to EU airspace closure. This meant many changes to Finnair’s Asian services, as most of Finnair’s flights between Europe and Asia have used the shortest, fastest, and most environmentally sound route over Russia. reply rixrax 17 hours agorootparentprevGiven russias current trajectory with their war of genocide against Ukraine[0], it is not foreseeable that any western commercial airlines would be able to fly in their airspace for years to come[1]. [0]https://en.wikipedia.org/wiki/War_crimes_in_the_Russian_inva... [1]https://www.npr.org/2023/03/18/1162659715/russian-53rd-anti-... reply ponector 18 hours agorootparentprevBut Concorde was able to fly fast only over the uninhabited surface like ocean. And on the route from London to Singapore how much of that? A third? reply elevaet 16 hours agorootparentOnly a small portion of the direct route flies over the Indian ocean, the rest is basically over land: https://www.airportia.com/flight-img/7760680/sq308-sin-lhr-s... reply jnaina 1 hour agorootparentI remember, when I was a kid, hearing a thunderous roar and rushing outside, and saw the Concorde with the Singapore Airlines livery, flying overhead. Left an impression. Not the usual flight path, as I was living in the central part of Singapore at that time. Assuming it was an one-off for testing purposes. reply amelius 18 hours agorootparentprevIs it more economical to carry all that weight in fuel versus having stopovers? Or is this a tradeoff of time versus climate, where time won? reply bombcar 18 hours agorootparentThat becomes the main issue, the Concorde as it was had 26,400 gallons95,680 kgs of fuel, and would need more than double that (likely) to do the flight in one go (bar major improvements to efficiency, and counting that more weight at takeoff needs more weight of fuel to fly that extra weight). A quick search says about 40 minutes to refuel a jet, so a stopover is going to add at least an hour, probably more because they have to come out of supersonic, etc. reply amelius 17 hours agorootparentAnd how are these numbers in subsonic flights? A flight from London to Singapore. How much fuel would it need without stops, versus how much fuel in the least-fuel case (but more-stops case)? reply izacus 16 hours agorootparentOne thing to take into account is that landing and taking off is very wasteful with fuel - jet engines use significantly more fuel when running in dense air and that doesn't include increased power for climbing. I'm sure there's someone who can plug in numbers into a flight planner here, I'm interested in actual numbers. reply amanda99 15 hours agorootparentOK, sure, I plugged this into a sim flight planner (http://onlineflightplanner.org/). This is on a Dreamliner. Heathrow - Changi: 63933 kgs Heathrow - Tehran: 26957 kgs Tehran - Changi: 39580 kgs So at least based on this it's not too much of a difference, only 4% more with a stop. (Tehran looked to be roughly in the middle on the flight plan between London and Singapore.) reply izacus 14 hours agorootparentInteresting, I'd expect it to be a larger difference considering how much the carriers avoid stopovers. Maybe the fees and organization of refueling tips the balance? reply bombcar 13 hours agorootparentCarriers don't want to do a stop unless they're offloading and onboarding passengers, so they'd rather run one flight to the intermediary, and another direct. reply izacus 9 hours agorootparentWell yeah, we're talking a about WHY it's a problem for them to stop. reply cyberax 7 hours agorootparentIt adds a surprising amount of time. You have to descend, land, taxi to a fueling area, refuel, taxi, take off, climb. And that's if the local regulations allow fueling with passengers on board. That's probably around 2 hours extra. And this in turn can require the plane to carry additional pilots and flight attendants. reply coryrc 8 hours agorootparentprevIIRC airframe lifetime is rated in pressurization cycles. reply MR4D 18 hours agorootparentprevRefueling is very wasteful - that's why so many flights are direct today - because it's much cheaper. reply Ekaros 2 hours agorootparentDepends on distance, cargo is best way to look at the ranges. As they optimize for cost per nautical mile. Not passenger demand or comfort or speed. At certain point refuelling saves in cost. reply ghaff 18 hours agorootparentprevI'm sure that's true although I don't know how the numbers pencil out. That said, people also much prefer non-stop flights because of time, hassle, and reduced likelihood of something going wrong. reply cycomanic 15 hours agorootparentThat's not quite true AFAIK at least a couple of years ago the super long routes (18+h) were served in business class only because not enough customers wanted to do such a long flight in economy. reply lucasban 29 minutes agorootparentThe Singapore Airlines Singapore to New York flight is 19 hours and only has premium economy and business. I have to make this trip fairly often and I prefer to take the direct flight as it’s simpler, faster and I have to pay extra for leg room either way. That said, if I had the means I would take business each time, because 19 hours in even the nicest economy seat can be tough. reply papertokyo 9 hours agorootparentprevAnother factor besides comfort is that they can extend the flight time 1-2 hours by having fewer passengers and bags onboard, which of course means premium seats like SQ23/24. Qantas is planning to launch 18–21 hour routes in 2026 with economy class, using modified A350-1000 with an extra fuel tank in the hold. reply throwaway2037 6 hours agorootparentAre you talking about \"Project Sunrise\"? https://en.wikipedia.org/wiki/Kangaroo_Route#Project_Sunrise reply ghaff 13 hours agorootparentprevThat's mostly not true as far as I've seen because most trans-pacific flyers are not willing/able to spend thousands of additional $ to fly in business class. reply SoftTalker 17 hours agorootparentprevIt's also riskier to do fuel stops en route. Most accidents happen on takeoff or landing. reply FireBeyond 15 hours agorootparentAccidents, and maintenance - much maintenance is predicated on cycle times, i.e. how many take offs and landings (and others are based on operational hours). reply globular-toast 16 hours agorootparentprevIs this also assuming the \"modern Concorde\" wouldn't do a sonic boom and could therefore fly supersonic over land? reply mbauman 17 hours agoparentprevWhat _was_ the routing and speed? That was my very first question and the blog post doesn't really answer it. How much of the flight was supersonic? They talk about avoiding India for the BAH-SIN leg, and trouble over Saudi Arabia, but there's a lotta populated land between BAH-LHR. The flight listing says this: SIN-BAH: 3698 miles, 4 hrs 6 mins, Mach 2.02 cruising speed BAH-LHR: 3120 miles, 4 hrs 21 mins, Mach 2.02 cruising speed Those numbers just don't make sense. That's 130 miles short of the great circle distance from SIN-BAH of 3935 miles. And then they talked about adding another 200 miles to go around India. So assuming the flight time itself is accurate, that leg should be: SIN-BAH: 4135 miles, 4 hrs 6 mins, Mach 1.3 average speed But how much of that time _could_ be spent at the listed cruising speed? Mach 2 will travel 4135 in miles in just over 2.5 hours! So we're looking at less than half the flight spent in supersonic — and this is the leg that's mostly over the Indian Ocean. The BAH-LHR leg is even trickier. Anyhow, it's little wonder that a direct non-stop is near the Concorde's time with these restrictions and the refueling stop. reply FabHK 14 hours agorootparent> Those numbers just don't make sense. That's 130 miles short of the great circle distance from SIN-BAH of 3935 miles. In an aviation context, those are most likely nautical miles (equivalent to 1 minute of a degree in north-south direction, which is why 10,000 km (initially defined as the distance from the equator to the pole) is basically 5,400 NM (90 degrees from the equator to the pole, times 60 minutes/degree)) rather than statute miles, which are some certain number of yards and feet in that quaint customary system used still used by some people in the USA, Liberia, and Myanmar. Indeed, according to the interweb, the distance between Singapore (Singapore Changi Airport) and Manama (Bahrain International Airport) is 3935 miles / 6333 kilometers / 3420 nautical miles. reply globular-toast 16 hours agoparentprevYeah but you're talking about an aircraft that first flew 55 years ago (almost to the day). reply hef19898 15 hours agorootparentThe cabin cross section wouldn't change so if the Concorde was designed today. reply gumby 20 hours agoprevI was surprised that SQ was the only third party airline to have its livery on Concorde, as I saw plenty of pictures of Concordes with Braniff livery on on side. Well, I did remember correctly that the service operated from Dallas (to NY or Washington). Subsonic only, and with lots of crazy adaptation to fit the crazy laws, like changing the aircraft registration number on each flight. But all those pictures I saw were advertising drawings: https://www.heritageconcorde.com/braniff-airways-concorde-op... reply buildsjets 17 hours agoparentFly the carbonated airways: http://www.concordesst.com/history/events/pepsi.html reply gumby 16 hours agorootparentWow! I don’t remember that at all. reply Fripplebubby 17 hours agoparentprevThis page has some great stuff in it. Today I think about commercial airlines being so _optimized_ for efficiency in everything, I just can't imagine a US airline flying a Concorde overland at subsonic speeds. Assuming they flew Mach 0.95 then that's about at 25% speedup compared to today's subsonic cruise (0.78, although you might get faster than this if you're a big plane going a long distance, up in the low 0.80s). Also, the ticket prices they quote for that flight: > 1979 Feb – May one way – $154 – $169 /Sept – Oct one way – $194 > 1980 Feb – one way – $227 so more than $900 today to fly one way on a flight that today you can have for $40 one way on a budget carrier! I guess I don't mind the extra hour it takes on a 737 or A321. Although the more fair comparison would be to the competing prices at the time, not today's prices - any ticket was quite a bit more expensive in 1979-80, so that factors in. reply FireBeyond 15 hours agoparentprev> I was surprised that SQ was the only third party airline to have its livery on Concorde, as I saw plenty of pictures of Concordes with Braniff livery on on side. \"Domestic flights between Dallas-Fort Worth and Washington Dulles airports were operated by Braniff with its own cockpit and cabin crews. During the domestic flights, the Braniff's registration numbers were affixed to the fuselage with temporary adhesive vinyl stickers. At Washington Dulles, the cockpit and cabin crews were replaced by ones from Air France and British Airways for the continued flight to Europe, and the temporary Braniff registration stickers were removed. This process was reversed after alighting in Washington Dulles from Europe for the domestic flights to Dallas-Fort Worth.\" reply ta1243 14 hours agorootparent> Domestic flights between Dallas-Fort Worth and Washington Dulles airports were operated by Braniff with its own cockpit and cabin crews Presumably own cockpit crew, but I had an instant vision of a replaceable cockpit module that was swapped out at Dulles. Operating as an American owned airline between Dallas and Washington allowed them to take Dallas-Washington passengers, rather than only Dallas-Europe passengers. This was essential for the economics of the flight to work. At the time BA and AF had 3rd and 4th freedoms, and possibly 5th, but were not allowed to fly passengers on solely domestic itineraries -- a process called \"Cabotage\". reply zabzonk 22 hours agoprevi loved concorde. i can remember them flying over my flat in south east london on their approach to heathrow in the early 1980s. beautiful. reply lucozade 18 hours agoparentThey flew over my house share in west London in the late 80s. Very pretty, bloody noisy. reply moystard 21 hours agoparentprevI love the Concorde but it had the reputation of being noisy. Living in SE London, I find already some planes incredibly noisy, so cannot imagine what it must have been. reply zabzonk 21 hours agorootparentit was landing, so it wasn't much noisier than say a 747, but a bit noisier. or perhaps i was just used to it - my dad was an RAF vulcan captain (same olympus engines as concorde, minus reheat) and if you had a squadron of them taking off on QRA you learned the real meaning of noise! reply martinclayton 17 hours agorootparentWe used to live under the flightpath for the departures from Heathrow to the US, not far from Reading. The evening flights would go over, which you could feel in your body, then moments later catch the light of the setting sun as they headed west. It was quite inspiring. The Vulcan is (or was!) my favourite plane sound, beating out the Merlin-engined stuff and even Concorde. Four Olympus engines, plus the howl. Can't be bettered! https://www.youtube.com/watch?v=H_ARSE8jEHQ reply zabzonk 17 hours agorootparentglad you enjoyed the howl, but, sorry, it didn't have afterburner (reheat). i still probably prefer merlins - the BoB flight Lancaster flies over me here in Lincoln occasionally. and also the Red Arrows! reply martinclayton 16 hours agorootparentDoh! Indeed - edited. Nice of the 'arrows to not move far when they left Scampton. reply postexitus 21 hours agorootparentprevHad the honour of seeing Vulcan on its last flight. A majestic beast. All the respect to your dad and colleagues. reply simonbarker87 22 hours agoprevConcorde’s failure makes me very sad. I understand that the economics didn’t stack up but it feels like we’ve given up trying to reach for the space age style future envisaged 70 years ago and instead are settling for “the same but a fraction nicer or a bit cheaper” in many areas. reply KineticLensman 20 hours agoparent> it feels like we’ve given up trying to reach for the space age style future envisaged 70 years ago We forget now that Concorde was a politically-motivated attempt to demonstrate that the UK and France were still relevant in the aerospace industry and the project itself was plagued with costly development inefficiencies due to the desire to split the work between the UK and French participants. E.g. working in both metric and imperial units. Building parts in multiple factories. The Soviet equivalent (Tu-144) crashed and burned at the Paris air show and never had any economic justification. The Boeing 2707 was cancelled before it even flew because of cost overruns following the failure of an abortive swing-wing design. Concorde itself only became profitable to operate when the govt wrote off the development costs in 1983 in a deal described as \"among the most disastrous conducted by a government minister\" [0]. [0] https://en.wikipedia.org/wiki/Concorde#British_Airways_buys_... reply dtagames 18 hours agorootparentEven more than a supersonic jet, Concorde was a successful attempt to create a homegrown European airspace industry. The aircraft itself was a pioneer in fly-by-wire technologies, which Airbus (the current name of the consortium that build Concorde) later commercialized, also at an initial loss. In this way, the entire business was kickstarted by the governments of Britain and France. The net result was a company that now outsells Boeing, especially in light of the latter's quality issues. Even though Concorde never made money (and never really could make money w/ the supersonic restrictions it had), I think it was still a win for the companies and countries involved. reply KineticLensman 18 hours agorootparent> I think it was still a win for the companies and countries involved Agree, although I'd love to know how many of Concorde's backers were considering this long-game effect. I suspect that the Airbus of today would be seen as even more fantastic than a supersonic aircraft. reply bobthepanda 17 hours agorootparentIt was pretty fundamental. At the time commercial aviation was a different two horse race between Boeing and McDonnell-Douglass. The aviation industry in Europe was not doing well and those industries and their suppliers represented millions of jobs. reply panick21_ 18 hours agorootparentprevThis kind of ignores the alternative reality. The idea that well if not Concorde then nothing else would have existed and whatever would have existed wouldn't have been 'European'. This isn't really true. If you look at the British case for example, there were alternative planes in development that asked for British funding. While those projects were British lead, they had specifically designed them to have suppliers all over Europe, including France. Now that plane could have been a failure or a success, we wont know. Looking at the design, it seems to have some potential. The French on the other hand might have invested their money in another kind of plane primarily from France but with supply chains outside of France as well. You could see something like Airbus emerging out of that too. Or maybe something that wasn't Airbus but like Airbus. Or a you could see a British lead company that has success in the narrow body world and later a French plane with success in the Wide-body world. Those could eventually merge. History could have gone many ways, and could have failed many ways. That we have Airbus now does track back to Concord, but is not true that its clear that without Concord we wouldn't have something Airbus like. reply switch007 14 hours agorootparent> but is not true that its clear that without Concord we wouldn't have something Airbus like. Did the parent claim that? Bit of a strawman if not. reply meekaaku 18 hours agorootparentprevBut a lot of big technological developments hve political motivations, because governments are the first and biggest customer. Nuclear energy, internet, space race etc, have all been politically motivated, funded and supported. reply bell-cot 18 hours agorootparentTrue - but the actual development of complex and expensive technologies tends to happen far faster, cheaper, and more reliably when a government feels a burning need for Actual Working Technology ASAP. Vs. when the whole thing is some combination of political showboating and spreading pork to everyone who wants \"their share\" of government money. reply dehrmann 16 hours agorootparentprevThose aren't fair comparisons. Everything you listed was genuinely new and potentially revolutionary. The Concorde was 2x as fast as existing planes. reply hef19898 20 hours agorootparentprevConcorde was ine of the precusers of modern day Airbus so. As an aircraft, despite being gorgeous and an engineering marvel, it was kind of pointless, I agree. reply mmsimanga 18 hours agorootparentprevMany politically motivated undertaking leave us better off. Modern day rocket engineers are literally working to get us to click on adverts. No I am not saying let's encourage politically motivated initiatives but not everything that costs a lot of money and doesn't necessarily workout is a bad thing. Look at how much was spent on Covid vaccine mandates. Now that was a waste of money and resources. reply zoeysmithe 18 hours agorootparentprevAlso, it only existed via extreme subsidizes. So working class French and UK people were subsidizing rich people farting in 1st class seats going mach 2. Its funny how its beloved by the \"free market\" types when it was just a welfare flight for those who thought themselves too self-important to fly a few more hours between major far-away cities. 113 people died in 2000. It wasn't exactly the safest plane out there either. With teleconferencing and modern technology, the need for the business class to show up to far-away places should go down. But a lot of it is entitlement, that is to say, show up for a meeting then enjoy a free vacation while \"working.\" Everything about the Concorde was corrupt if not classist. It was a mistake even if the engineering was impressive. Imagine if that money would instead have gone to public transportation. We'd have the Chunnel in the 70s instead of the 90s. reply mopsi 17 hours agorootparentThe money DID go to public transportation. The cost of the Concorde program is peanuts compared to how much value was gained from it. Concorde brought many innovations such as its electric control system, which was developed further for the Airbus A300, and then reached its pinnacle in the Airbus A320. This tech tree is one of the cornerstones of unprecedented safety that modern airliners have brought to air travel, flying hundreds of millions of people every year without causing the loss of a single life, regardless of whether one is a self-important elitist or a regular schmuck on a 20 EUR flight to Ibiza. You can't just go out and buy such innovation. It's the natural by-product of relentless pursuit of borderline impossible goals. reply notahacker 14 hours agorootparentMost of Concorde's engineering innovations were tech tree dead ends though. The Concorde project was certainly good at fostering Anglo French cooperation in aerospace but it's difficult to imagine the counterfactual scenario where the cooperation is doomed to failure because the debut product is a commercially viable subsonic aircraft rather than a technically impressive aircraft that doesn't sell. Similarly, whilst some of the R&D investment in control systems did find it's way into the A300, the A320's digital fly by wire is a completely different system, and it's difficult to imagine the scenario where a nascent Airbus project doesn't consider fly by wire because they hadn't figured out delta wings or droop noses yet reply ghaff 18 hours agorootparentprevConcorde was at least partly a remnant of people flying to London to close a deal over lunch and getting back to New York in time to tuck the kids in. I assume that sort of things is at least less common today. reply bobthepanda 17 hours agorootparentJFK-LHR is still one billion dollars in revenue for British Airways alone. The innovation that killed Concorde was the lie flat business seat. You could be cramped in the Concorde’s leather bus seat for three hours, or you could save money and get a sleep in for six hours. reply ghaff 17 hours agorootparentIt's still a huge route but it's also something you can do on a day flight. Heck, from Boston, I can fly to EWR and still be in London for a late dinner. I don't even need a lie-flat seat. The extra $4K or so in your pocket pays for a lot of reduced comfort for 10 or so hours. reply bobthepanda 16 hours agorootparentSure, but an economy seat is pretty irrelevant vs a business seat for someone who was in the market to drop above $10,000 on a Concorde seat anyways. reply qsi 9 hours agorootparentprevThe anecdotal, apocryphal rumor I heard back in the day when Concorde was still flying was that the big Wall Street investment banks accounted for a third to a half of all Concorde traffic between London and New York. Impossible to verify, and possibly too good to verify... but I would not be surprised if a large chunk of passengers did come from that demographic. reply switch007 11 hours agorootparentprevWas there ever a timetable that permitted that kind of day trip, as they took off and landed in pretty civilized hours? A schedule I found was: BA002 dep JFK 08:30 -> arr LHR 17:15 BA003 dep LHR 18.25 -> arr JFK 17:00 Still, certainly allowed \"oh crap, need to get to london to do some very important same day business (and return the next day)\" reply ghaff 10 hours agorootparentYeah. You're probably right. The time change sort of kills you. You could fly to London for a dinner and return the following day. (You can sort of do that today but it's going to be a late dinner.) reply qsi 9 hours agorootparentprevI found [0] BA 001 LHR-JFK 10.30am arrive JFK 9am BA 002 JFK-LHR 9am arrive LHR 17.25 BA 003 LHR-JFK 18.25pm arrive JFK 5pm BA 004 JFK-LHR 12pm arrive LHR 8pm A same-day return would be useless as you'd have only 3 hours at JFK. [0]: https://www.airliners.net/forum/viewtopic.php?t=458613 reply panick21_ 18 hours agorootparentprev> Its funny how its beloved by the \"free market\" types Not sure what 'free market' types you are talking about. Most 'free market' types I know were and are not in pro of such projects. > Imagine if that money would instead have gone to public transportation. I totally agree with you that money invested in high speed trains all over Europe would have been a far better investment. And you can support just as many jobs and you can do just as much research if you really want to. Britain at the time actually decided between Concrode and more practical single isle plane more like the 737. A workhorse type plan that you could at least make an argument about beyond creating jobs. reply HPsquared 22 hours agoparentprevConcorde is in the \"faster horse\" category. It's a fair bit faster then regular planes at substantially increased cost, and it couldn't do long distances (max range 4143 miles), kind of removing the speed advantage where it would have really helped, i.e. very long trips. Suborbital spaceflight would be the ultimate: UK to Australia in an hour. reply mananaysiempre 21 hours agorootparent> UK to Australia in an hour. That would be an improvement, I guess, but Amdahl’s law still says you’re going to spend at least a day on the trip. Couple of hours to get to the airport, arriving to the airport three to four hours in advance to account for the variance in check-in, security, and immigration times and to get through the overpriced mall itself, then anywhere between ten minutes and over an hour for immigration and customs depending on what flights arrive at the same time, then something like half an hour waiting for your luggage unless you’re very lucky, then again a couple of hours to get from the airport to wherever you actually want to be. I’ve seen very few efforts to reduce any of this over the last two decades, and basically none as far as the things happening inside the airport are concerned. (Well, OK, automated immigration checks are a thing, but if you are one of a flight of people ineligible for them, fuck you, here are one or two border control officers that every one of you will be funnelled through.) So while I can appreciate the idea of less miserable long-haul flights, I don’t think “in an hour” is worth anything but a sad chuckle. And I haven’t even accounted for the time you’ll need to spend searching for prices and rearranging your schedule to work around the price discrimination machine. Air travel just sucks, and I don’t think neat aerospace engineering alone can get around that. reply NamTaf 19 hours agorootparentIn my experience, your numbers are out for this particular example. It'd reduce it to at most a day. You're right that there's a few hours of faff at either end, though I don't believe it's nearly as long as you're cumulatively adding up. However, those things are dwarfed by the 24 hours of in-air time it currently takes (sure, 17.5 if you want to go to Perth). That time in the plane is death for me, as I cannot sleep sitting upright. It means I get off extra-fatigued and that multiplies my jetlag several-fold. I agree completely with you that the faff at either end needs to reduce, and long-haul travel will always be a time sink, but I still think reducing the in-air duration for those ultra-long journeys would be huge. The faff getting better what with removing liquids limits and not needing to pull electronics out of carry-on. Check-in can sometimes be a pain, but I've found it to be fairly ok both within the UK and AU most of the time. Combining a few things together - BA's 23kg carry-on limit, removed liquids limits, online check-in with etickets - would go a long way to streamlining it. Combine those things with a reduced in-air duration to make it ~8-hour duration event and I think it'd be much more comfortable, more in line with flying east-coast to west-coast AU. What would I pay for it, and would I opt for it over doing a layover in e.g. SEA if I had the spare time to do so? No idea, honestly. reply ghaff 19 hours agorootparent>as I cannot sleep sitting upright. It means I get off extra-fatigued and that multiplies my jetlag several-fold. Well, you can pay for lie-flat seating but obviously it's a big premium. reply bombcar 18 hours agorootparentIf people really cared about it, there'd be long distance airlines that looked like submarine bunks - https://www.youtube.com/watch?v=BuVe_KaTGg8 I'm not saying I'd not take it as an option ... reply ghaff 18 hours agorootparentThere are more luxurious options out there but something like United's Polaris seating is comfortable enough. Not saying I'd do trans-Pacific flights like that for the recreation but it's not painful in the way that economy (even with extra legroom) is. Mostly sitting/laying down for the better part of 24 hours is going to be a bit painful/boring however you slice it though. Back in the prop days, there was something like bunk-type arrangements that still exist in some sleeper trains. (I took something not that different from Beijing to Shanghai a number of years back.) reply leoedin 15 hours agorootparentprevEvery time I sit through a hellish long distance flight I wonder the same thing. The closest anyone’s got is Air New Zealand’s Sky couch - which is a minor improvement on the status quo rather than a reimagining of it. There’s been so much innovation in business class, but relatively little in economy over the last few decades. Is it because airlines are afraid of cannibalising their business seats? Or regulatory issues regarding non-standard seats? If anyone can figure out how to give me a lie flat bunk for the same weight and volume per passenger as economy (or even premium economy) they’ve got my business for eternity. Sitting down for 11 hours would feel like torture in comparison. reply bombcar 13 hours agorootparentI think the trick is it has to be able to \"sit up\" during takeoff, landing, etc. So they'd have to design something really complicated, or get the governments to change the rules. reply bobthepanda 17 hours agorootparentprevGetting tossed around in an airplane capsule during turbulence is not my idea of fun. reply djbusby 19 hours agorootparentprevPlus, in SEA you can find the Business Fish in the walkway art of B-gates. Fun! reply Arch-TK 20 hours agorootparentprevMy usual experience flying between the UK and Europe and in three cases China was that 2 hours has always been sufficient before departure and that 1 hour is about the limit after landing. reply Horffupolde 19 hours agorootparentIt’s not about the mode but the max. reply KoftaBob 17 hours agorootparentWhy would it be about max? There are always going to be outliers in people's travel plans that make some aspect take longer than usual. If I decided that from now on, I'm only walking to and from airports rather than driving or taking transit, does my multi hour walk to the airport make it so that airplanes are now as slow as driving places and therefore not worth it for any domestic flying? No, that would of course be a ridiculous conclusion. reply ghaff 16 hours agorootparentIt's really mode plus some standard deviation. Stuff can always happen. And sometimes the flight could just be canceled. If I expect it to be 90 minutes to get to the airport, I'm not going to assume it will take 4 hours because whatever even if if I'll miss one flight in my lifetime. reply Horffupolde 12 hours agorootparentprevBecause not boarding the plane is much worse than waiting at the airport. reply shiftpgdn 20 hours agorootparentprevI fly Texas to Florida pretty regularly for leisure with my kids. We all have clear/global entry. Our normal procedure is to pick 6-7AM flights with carryon only. This means leaving for the airport around 4 or 5. With a 2.5 hr flight that typically puts us out of the airport by 11 and generally starting our day by noon. reply bombcar 18 hours agorootparentThis is the way to go - and you can reduce luggage even further by \"preshipping\" via UPS or whatever, or if you travel to the same area often store stuff with friends/relatives/small storage unit. The other huge advantage with picking the early flights - if something goes wrong you'll get there later on a later flight almost always, so you can cut the times a bit closer and not be terribly worried. If you're on the last flight out that day, missing it is bad news. reply Symbiote 16 hours agorootparentprevThat's an internal flight. Add some time for people + luggage to leave the UK, and to enter Australia. (There is time for both countries to inspect luggage even if we don't see them doing it.) Then add some more because the flight is much less regular and costs £1000 rather than £100, so missing it has a worse outcome. reply simonbarker87 20 hours agorootparentprevUK to Australia would be a huge improvement and would mean I am significantly more likely to go there. I already happily fly 2-3 hours across Europe without it taking up an entire day of travel (30 minutes to the airport arrive 2 hours before, 2-3 on the plane and then 10 minutes from plane to taxi/train at the other end) vs the same + 20 hours of flying and likely a stop over. reply twic 21 hours agorootparentprevPan-Amdahl’s law? reply alvah 9 hours agorootparentprevI agree with the general point but \"couple of hours to get to the airport\" you must live a long long way from the airport if this is normal. reply manquer 7 hours agorootparentWith poor public transit as well perhaps . These days even if the airport is far off (newer airports usually are ) they are typically connected via public transit pretty well. Munich is a good example - airport is good 30+km from the city but there is fairly cheap and frequent train service. reply usr1106 2 hours agorootparentMunich's train connection is not really good. Nearly no long distance trains and a very long travel time to Munich main station where you need to change unless Munich is your final destination. So you lose over an hour, nearly 2 taking the notorious unreliability of German trains these days into account. Frankfurt is not a pleasant airport but at least it has a long-distance, high-speed train station. reply hagbard_c 19 hours agorootparentprev> Amdahl’s law still says you’re going to spend at least a day on the trip. Yes, this is one of the reasons why I have not flown from Sweden to the Netherlands for about 4 years now but used trains instead. The actual flight takes anywhere from 1.5 hours to ~5 hours depending on the route taken (which in turn depends on pricing at the moment of booking). Getting to the airport from the farm takes more time, having to arrive there at least 1 but preferably 2 hours in advance to partake in the security theatre adds to that. Regular public transport does not stop at the airport although there are plenty of routes which pass it by at some distance, instead there is a specific airport bus which costs about 3 times what the normal bus costs. This is added to the normal public transport costs because that bus only starts from central station. Then there is the flight itself where they're trying their utmost best to nickel and dime passengers on everything from breathing space to the privilege of taking more than a change of underwear. Arriving in the Netherlands the trip from the airport to my final destination is a bit better arranged since there is a train station right underneath Schiphol Airport. But... going back via Schiphol has become quite tedious since they seem to have problems with their security theatre show, somehow the actors need a lot of time to play their parts which often leads to hour-long delays. Back in Sweden there is that whole special-bus-thing again to get to the place where I can take a train which brings me to the station from where it is a 3 km walk home. Total time taken ends up somewhere around 5 to 10 hours depending on flight time. The trip by train takes anywhere from ~13 hours to ~21 hours, depending on schedule, route and (ever-present) delays in Germany. I leave early in the morning to walk to the station, take a train, move to the next one, repeat that 4-5 times and I'm at my destination. I can take as much luggage with me as I can carry which is a lot, if I feel like filling my backpack with Shukirkens or lethal nail clippers or $deity forbid more than 1 litre of liquids there is nobody bothering me, I get to have actual leg room and room to move those legs if I feel like it - you can walk quite a distance in some trains - plus a table and an outlet so I can hack away while going in more or less the right direction. There tends to be network connectivity in trains as well and if I end up on one where this does not work - which happens regularly - I can use my phone to get online. There's restaurants for those who want but I tend to bring my own. Delays sometimes mess up my schedule and I have spent hours in damp and dank tunnels under German stations during the hours of night when everything is closed and the only company to be had is the drunks who keep on coming by to beg for a euro 'to call their sick mother' but this, fortunately, is the exception rather than the rule. In short things are not perfect but... Traveling by train is like going on a journey while air travel has been turned into a chore. It might save me half a day but it gains me the same in time to work/relax/read/talk to other travellers/do nothing. reply KoftaBob 17 hours agorootparentprev> Couple of hours to get to the airport Since when does it take a couple of hours to get to the airport for the average traveler? That would only be the case for someone who lives in a very rural area far away from the nearest major airport. > arriving to the airport three to four hours in advance 3 hours is what's recommended for international flights, but even then thats a conservative recommendation to be safe. \"3 to 4 hours\" is quite excessive. > a couple of hours to get from the airport to wherever you actually want to be. Again, the average traveler is not traveling for a couple of hours to get from their destination airport to their final destination. I can't find the stats, but if I had to guess, the majority of travelers final destination is within an hour of the airport they land at. I definitely agree with you that all of these aspects of air travel need to be made much more efficient, but inflating these times just paints an inaccurate picture of the travel time benefits that supersonic travel brings. reply Der_Einzige 16 hours agorootparentprevSounds like you don't hold TSA precheck/global entry. You might want to fix that. reply stephenr 13 hours agorootparentHow exactly do you imagine TSA precheck will help on a flight between UK and Australia? reply MattGaiser 21 hours agorootparentprevDepends on where you are, but there are numerous attempts to reduce these things in Canada ,where I live, and the US has similar schemes. Check-in is now online, bag drop is automated, security is a breeze with Verified Traveller or PreCheck, immigration and customs are simplified with NEXUS/Global Entry/APEC, and airline status can get you priority baggage. I am often in the airport lounge within 15 minutes of arriving at the airport, with that all completed. Same with immigration. Plenty of countries offer concierge immigration schemes if you pay between $50 and $300. > Couple of hours to get to the airport You have to be in a pretty sparse area for this to be the case, so a lot of this just comes down to living far from a city. reply hef19898 19 hours agorootparentIf I take two extreme, Munich Airport Terminal 3 and the small local airport nearby, we have: MUC: - 10-20 minutes from parking to baggage drop off, depending on where ypu park, can be almost 30 minutes for the parking you ise for vacation (and not the close by ones you can put in your travell expenses) - 10 minutes, if you are unlucky a lot more, from baggage drop off until you pass security - another 10-15 minutes to get fr security to your gate at Terminal 3 So, at the very least 30, in praxis more like 45 minutes, at the airport alone. And MUC is pretty well built and organized in that regard. The local airport so, which has no real commercial flihhts anymore, is at max. 15 minutes from parking to gate, all included. reply llm_trw 21 hours agorootparentprevThe thing is that in the 90s it used to be 30 to 90 minutes for getting on board an international flight. Much like Concord we decided that slower is better for some reason. reply ghaff 20 hours agorootparent90 minutes is probably still comfortable time for an international flight in the US if you have Pre-Check. I don't like rushing so I'd probably give it a bit more time but usually things go pretty fast. That said, I agree with the basic point that transatlantic tends to be an all-day thing (or a red-eye) and shaving some hours off the flight itself doesn't really change that. (And even if it made it easier to get to continental Europe on a daytime flight from eastern US, that matters less with modern lie-flat seating.) reply bombcar 18 hours agorootparentprev\"Travel time to the airport, door to door\" is often much more than just \"driving over there\" though that can often be the fastest - drive to the airport, park on the closest ramp, walk in. If you park at the discount ramps, you're usually adding at least 15 minutes if not more. Transit adds significantly more for most people, unless you have a train to the gate right outside your door. > Most people live a reasonable distance from a decent-sized airport. Half the people in the United States live within 17 miles of a decent-sized airport, and ninety percent of the country lives within 58 miles (about an hours drive). Twenty-five percent of the population lives pretty darn close: less than 9 miles. From https://www.mark-pearson.com/airport-distances/ - he used any airport with more than 100k passengers a year, so it's not counting rinky-dinky commuter airports. reply ghaff 17 hours agorootparent>ninety percent of the country lives within 58 miles (about an hours drive) An hour's drive :-) I'm closer than that to Logan in Boston and planning for 2 hours in the morning is not unreasonable at all. (And basically the car company I use won't let me plan for a lot less than that because they don't want to be on the hook if I miss my flight.) reply bombcar 17 hours agorootparentIt's very interest the dynamics - the time and cost of almost all options available to me, end up being quite close. Easiest for me is \"drive myself and park at the closest ramp\" but that's the most expensive after a certain number of days. reply ghaff 17 hours agorootparentParking at Boston Logan is expensive even with Economy Parking (which I joke is in Canada); the airport is very close-in to the city. In my current job, no one has ever pushed back on me using a private car service and, if I'm traveling on my own, it's usually for long enough that the car service is at least breakeven relative to driving/parking. After a couple issues, e.g. arriving on a cold 10pm night to a flat tire I couldn't get off, I mostly just won't drive in any longer. There are a couple bus services from the burbs but I haven't really been motivated to check them out for years. reply bombcar 17 hours agorootparentCar service is almost always the way to go, I'm just outside normal \"uber\" range so if I want a ride to the airport, car service it is. Bus is hell, especially with luggage. Abusing friendships is also a perfectly viable one, harder to get the company to reimburse ;) reply SideburnsOfDoom 2 hours agorootparentprev> Most people live a reasonable distance from a decent-sized airport. I live in London, UK. There are 4 airports in the greater London area. But the distance to closest airport means little. I never get to chose: given the airline and destination, that narrows it down to 1 of these airports; or on rare occasions 2, and then I chose based on flight price and time of day, and only then proximity. reply logifail 21 hours agorootparentprev> immigration and customs are simplified If you're eligible, there are plenty of us who aren't. I waited well over two hours at JFK last month. > security is a breeze I never did work out why it's OK for aircraft to fly in to US airspace with non-PreCheck passengers who've not had to remove their shoes at security. reply adastra22 18 hours agorootparentIt’s not OK to just fly in. Coming to the US you have to go through the special US pre-clearance zone that has extra security. You may not have to take off your shoes, but you often don’t have to in the US either. Really depends on local screening requirements which are wildly inconsistent. reply logifail 17 hours agorootparent> Coming to the US you have to go through the special US pre-clearance zone that has extra security This may once have been true but I don't believe it's the case any more, at least not at the major European hubs I've been to. There is no additional security screening if you fly British Airways to the USA out of LHR. This was no additional security at Frankfurt last month either. reply adastra22 17 hours agorootparentIt was the case just a year ago when I flew out of LHR. US flights were out of terminal 5 and a special pre clearance zone. I’ve been through Frankfurt’s pre clearance as well. Maybe things have changed very recently? It flew under the radar if that’s the case. reply ghaff 17 hours agorootparentMaybe? I've flown out of Europe to the US many many times and never encountered any special security measures. Certainly not LHR (Terminal 2 usually I think) and not Frankfurt just a few months ago. reply ghaff 18 hours agorootparentprevThat is not true in general. You pre-clear at certain airports but you typically just go through standard airport security. reply adastra22 17 hours agorootparentYes, it depends on if the standard airport security meets US regulations for security. Many European and all Canadian airports do. reply logifail 17 hours agorootparent> Many European and all Canadian airports do Despite shoes not needing to be routinely removed at most (all?) European airports' security checkpoints? reply ghaff 16 hours agorootparentThat's the main discrepancy which the US basically overlooks for non-US security procedures. (Though there's some seemingly arbitrary variance in electronics screening as well.) reply adastra22 16 hours agorootparentprevShoes being removed isn’t required in the US either. I haven’t had to do that in years. It’s a per-airport thing. reply ghaff 15 hours agorootparentI thought that was still pretty normal. But I have pre-check so don't actually know. reply logifail 2 hours agorootparent> I thought that was still pretty normal I had to remove shoes at security at both JFK and SFO recently. reply thaumasiotes 18 hours agorootparentprev> You have to be in a pretty sparse area for this to be the case, so a lot of this just comes down to living far from a city. Speaking from the center of Shanghai, it does indeed take a couple of hours to get to the airport by subway, though a taxi is more like one hour. Airports don't get sited in dense locations - they make a lot of noise - so I can't quite follow your logic. If you live in a city, it's going to take you a while to get to the airport. reply SideburnsOfDoom 21 hours agorootparentprev> You have to be in a pretty sparse area for this to be the case, Nope, unless you mean \"You have to be in a pretty sparse area for it to be that low\". From North London, you should plan on more than 90 minutes to get to Heathrow, regardless of which transport mode you choose. It's not because of sparseness or \"far from a city\" of the parts in-between, quite the opposite. Gatwick is worse. Stansted is slightly is better, but I'm seldom going somewhere that flies from Stansted. reply tomatocracy 20 hours agorootparentIf you really need to get there quicker, there are a few companies offering motorcycle taxi services (where they ride and you ride pillion), which can make a big difference at times of busy traffic (though it does limit the amount of luggage you can bring). reply stephenr 12 hours agorootparentProbably also increases the need for a change of underwear in the carry on bag you now can't take with you. reply emchammer 21 hours agorootparentprevSomebody on here recently described Concorde as \"peak boomer\" which was a comment that changed my opinion about it. Yes, it hit some local maximum for what can be accomplished with protractors and a three-person crew on the flight deck. Sure, it brought Phil Collins to the US to perform a concert \"before\" his UK concert on the same day. It also rattled a lot of windows in its flight path. Right now, everybody else is wondering if they will be boarding a 737-\"fuck it, we'll fix it in a software update\"-MAX. If there are any decadent flights I'd like to take from seeing YouTube videos, it would be one on Etihad's \"The Residence\", but even that doesn't exist any longer. reply MattGaiser 21 hours agorootparent> Etihad's \"The Residence\", but even that doesn't exist any longer. This still exists. It is flown from London and New York to Abu Dhabi. reply jnsaff2 21 hours agorootparentprev\"peak boomer\" is a very effing nice pun on its sonic boom. mad props. reply logifail 21 hours agorootparentprev> UK to Australia in an hour Q: How much more would customers pay for this compared with - say - business class on the same route? reply manquer 7 hours agorootparentIt is more the price / demand curve , I.e how many will pay a cost of the flight and is that sufficient demand for to recoup the costs . Emirates (also ethihad ) have for example this ultra luxury suite with its own shower and bedroom in normal flights they sell for like 20k+/per . It is targeted towards people who have chartered a private jet (100-200k+) and by most accounts the service is successful. However it just 1-2 suites in few select routes . My point is it may not be typical business class passenger that is the target market, perhaps it those taking a private jet between these two destinations and they will definitely pay a lot . Look at this way the “billable rate “ for a moderately sized company senior management would be in the range in thousands of dollars/hour, so 10-15 hours saved could be one way to price this service . The key question is does it require million people / year or only thousand/ year to break even . You can easily do the latter , million people a year would be quite difficult. reply logifail 1 hour agorootparent> Look at this way the “billable rate “ for a moderately sized company senior management would be in the range in thousands of dollars/hour, so 10-15 hours saved could be one way to price this service Thanks to in-flight wifi you can still be working (and therefore be billing) while travelling. reply komali2 18 hours agorootparentprevIf suborbital flight could be achieved via some combination of space elevators making it sustainable, sure, or some form of propulsion that doesn't consume one million pounds of fuel. Otherwise it's unsustainable, like most things about air travel today. So IMO the ultimate would be something more along the lines of zeppelins. A week from Taiwan to California isn't so bad if I get to spend the time relaxing in a way similar to how I do on a sleeper train. Plus, airships. reply adastra22 18 hours agorootparentThe marginal net energetic cost for two-stage rocket based suborbital transport (e.g. Starship) is about 4x a modern airplane. So about on par with the Concorde and within reach of first-class ticket pricing. If this is surprising, keep in mind that the rocket is only on for 8 minutes, and most of the flight is experiencing zero drag from being above the atmosphere. reply ghaff 18 hours agorootparentprevA week on an ocean liner is one thing. A week on the equivalent of a sleeper train with no real scenery seems like something else. That doesn't really feel like a it's the journey not the destination thing after the first day or two. reply SideburnsOfDoom 21 hours agorootparentprev> Suborbital spaceflight : UK to Australia in an hour Wait until the safety and security people do risk and threat assessments on that. Worst case is fairly close to a suborbital kinetic impact missile, aimed at the region of a major city from halfway around the globe, with less than an hour's notice. reply loudmax 20 hours agoparentprevYouTube channel Real Engineering released a video recently on aviation startup Hermeus, which is developing a ramjet ultimately intended for commercial air travel: https://www.youtube.com/watch?v=UyKtxsdI0z8 The thumbnail is a bit click-bait, but the video itself has a lot of depth. They give Hermeus space to present itself in a very positive light. As a non-expert having watched the video, it does seem at least plausible that this company may yet succeed. They mention fuel costs in passing, but they don't talk about carbon emissions. That's probably fair, since carbon is a much broader issue than just aviation. It's probably more productive to focus on shutting down coal plants than stifling innovation in air travel. But it's worth bearing in mind that positive things like hypersonic air travel do have a cost. reply hef19898 20 hours agorootparentWhat people always underestimate is the amount of time, and money, it takes to get anything developed, built and certified in aerospace. And that is for even small modifications done on existing design by the major players. Starting from scratch, already a new design is hard enough not to talk about a new company, is even harder. Just timeline wise, realistically you talk about around 5 years for something new, start to finish. That's millions upon millions for existing companies, if everything goes well. And hardly ever goes well, delays and tecjnical issues are common. Again, for proven tech don eby established companies. Something like a commercial ramjet engine developed by a start-up is not even the same solar system: there we talk, realistically, billions and closer to a decade, if it works technically. And then you need a market for that engine, which means demand (doubtfull, but who knows) and more importantly, an aircraft. And the last bit again is aroubd a decade and another couple of billions. Engine and aircraft development can, and has, been done in parallel. It still takes longer so, as both development projects depend on each other, the A400M would be a recent example of that approach. If Hermeus doesn't have a development partner for the airframe, realistically, if everything (!) goes well, we talk about at least 15-20 years from now. Not sure if VC funding is the right model for stuff like this. reply Ekaros 20 hours agorootparent5 years is pretty sort. Looking at A220, which I think took closer to 10 years. Which I think is realistic timeline for new modern design, not just modernising old one. A350 and 777X also show similar timelines... That is from planning to deliveries. And these are the players with experience building to specific segment of market... Throw in enough novel ideas and it might double... reply hef19898 19 hours agorootparentEasily. And aerospace began to move fast again since the A380, B777, B787, A350 and such. The decades before were a lot slower. At least now ypu don't have to mix people pulled back from retirement with new grads to get a team that has the collective experience of a programm start to finish anymore. reply panick21_ 17 hours agorootparentprevReal Engineering always gets access to companies because he presents everybody in an overly positive light. He never actually does the negative stuff, except for company he doesn't get invited into. reply paxys 18 hours agoparentprevIt is more than \"a bit\" cheaper. Air travel would have continued to be reserved for the elite had there not been a decades-long effort to reduce costs. You can get a transcontinental round trip today forI saw Apollo 11 three times when it came out, and I still get excited when I see the sense of purpose and detail in everybody involved. I think you are proving my point? Manned spaceflight is strictly for entertainment and pissing contests only. So we should finance it like entertainment, instead of making random tax payers suffer for your amusement. > And anyway the astronauts brought back a moon rock to give to the Soviets. Yes? Obviously you are going to be all smug and rub your success in your adversaries face. --- Btw, just to be clear: I also find space flight inspiring, and I hope I live long enough to see the vast bulk of humanity living in space habitats (while still increasing the absolute number of humans on earth). I want to see space elevators (via active support) and all that good stuff. Two things: (1) The early manned space flight barely contributed to these long term goals. It's only recently thanks to the private sector that launch costs are starting to come down. And the other big advances are coming from better robotics and better computing, both of which are being developed on trajectories so far largely independent of space exploration. (2) I don't want to force other tax payers to suffer for my flights of fancy. Just like I don't want to have to finance other people's pet projects with my tax money, either. reply zabzonk 16 hours agorootparentprevnot sure what this means - perhaps 13 rather than 11? reply emchammer 15 hours agorootparentNo, Apollo 11 is a thrilling big-screen documentary about the successful moon landing, all original film without narration. reply zabzonk 1 hour agorootparentthanks, i've just bought it on amazon! reply panick21_ 17 hours agorootparentprevI kind of disagree. Yes the person on the moon wasn't that important but all the technology outside of the moonlander itself had nice utility. Its just that the US kind of messed up its space investment strategy. Instead of leveraging all the parts of Apollo it was systematically killed. Had the doubled down on Apollo and simply continued investing in the technology they could have done impressive things. The could have built a Saturn 1C as a workhorse rocket for the military and commercial. And then have the Saturn V as your super heavy for the occasional deep space probe, huge telescope or space station. The Apollo capsule could incrementally be made reusable and could continue to be used for LEO or occasional moon missions. During Apollo they were already deep in development on second generation version of the different components. This was not actually finically unsustainable, it was just financially unsustainable while also investing in Shuttle. And because a Saturn 1C wasn't considered, the military built itself up with Titan rockets instead. reply eru 7 hours agorootparent> I kind of disagree. Yes the person on the moon wasn't that important but all the technology outside of the moonlander itself had nice utility. Counterfactually, all the tax payer money wasted on manned space flight would have been used for something else, and would have driven some other technology, and at that other spending would have had direct utility. (Especially if the money had been left in the tax payers pockets.) See 'What Is Seen and What Is Not Seen' https://www.econlib.org/library/Bastiat/basEss.html?chapter_... > Had the doubled down on Apollo and simply continued investing in the technology they could have done impressive things. You really need to learn to start thinking in terms of opportunity costs. See https://en.wikipedia.org/wiki/Opportunity_cost > [...] it was just financially unsustainable while also investing in Shuttle. I agree that the Space Shuttle was an even greater waste than the Apollo program. They overshot their projected cost per kg to orbit by 100,000% (one thousand times). But I hold that approximately all manned space flight is a waste of tax payer money. reply panick21_ 26 minutes agorootparentIf you are just against government spending there is no argument you can make that can challenge that. > specially if the money had been left in the tax payers pockets. This was peak suberbia and peak people buying cars. I think there is a good argument being made that more money invested in suberbia and cars wouldn't be good investment. In fact, there is a good argument that this is a negative investment for society. On the other hand, Apollo lead to a massive investment in engineering and scientific talent and to a huge amount of technology development that was then utilized by lots of other industries. If I could have picked, I would have preferred some of that investment into nuclear technology. But that wasn't really in the cards. People have always been critical of Apollo as specially on the left they would have preferred social programs, and libertarians would have preferred nothing. I am libertarian leaning myself but as far as government spending goes, this is some of the better things they can do. They did arguably spend to much to fast, rather then taking a measured approach. reply eru 8 minutes agorootparent> If you are just against government spending there is no argument you can make that can challenge that. Government spending that clears a reasonable bar of utility is fine by me. Eg overall I think I am getting good value for my tax dollars here in my adopted home of Singapore. (Of course, I don't agree with every last item on the budget.) > This was peak suberbia and peak people buying cars. I think there is a good argument being made that more money invested in suberbia and cars wouldn't be good investment. In fact, there is a good argument that this is a negative investment for society. I am glad that we have you to tell people that what they want is bad, and that they should want something else instead. (Btw, it's spelled suburbia. It's not a problem for me, just thought you might want to know.) See https://news.ycombinator.com/item?id=39598737 for more of a fleshed out argument. reply pfdietz 12 hours agorootparentprevThe Saturn 1B looks vaguely like Falcon 9 if you squint appropriately. The H-1 engine is in the same thrust class as the Merlin 1D (it just weighs twice as much). So, one can imagine the S1B evolving over the years into something like a F9, with the first stage being recovered. The idea of designing boosters to minimize cost rather than maximize performance also goes back to the late 60s. The biggest problem is government contractors have different incentive structures than SpaceX does. reply panick21_ 11 hours agorootparentIt would really be the government driving to a unified architecture, making the apart commodity and create enough demand. It would need actual long term strategy. reply Dalewyn 22 hours agoparentprevThe phenomenon that ultimately put down the Concorde is actually seen all the time everywhere in everyday life: Bleeding edge performance and tech is almost never economically practical, it's always the stuff well within the margins with room to spare that ultimately define the space. See for other examples: The 500 Kei Shinkansen, literally every Intel Core i9 and AMD Ryzen 9 CPU, sports and luxury cars, Boeing 747 \"Queen of the Skies\" Jumbo and Airbus A380, the Space Shuttle, the F-22 Raptor, and more. All very impressive pieces of tech and their achievements shouldn't be discounted, but all ultimately an interesting footnote in history as significantly more inferior and practical pieces of tech dominate. reply pavlov 21 hours agorootparentThe 747 was manufactured for 50 years and over 1,500 of them were made. It’s difficult to put it in the same category with Concorde and the Space Shuttle. reply noarchy 18 hours agorootparentThe 747 still has demand as a cargo plane. I bet we see them flown for a long time to come, with most remaining passenger configs gradually being converted to cargo. reply hef19898 18 hours agorootparentThey do get replaced by (converted) B777s and cargo A350s so. The 747 still will have a place, especially for long and heavy loads so. reply haunter 21 hours agorootparentprev> literally every Intel Core i9 But today’s i9 is tomorrow’s i7 and the i5 the day after tomorrow. So it has its place in the story reply rsynnott 20 hours agorootparentI haven't been keeping track lately, but these used to be \"the i7, but squeeze out an extra 500MHz by adding another hundred watts\", usually; has this changed? reply haunter 19 hours agorootparentYeah pretty much that. They usually have the same amount of performance cores but there are more efficient cores and hence threads and on top of that they run on higher frequency too i7-14700K: 20 cores (8 performance, 12 efficient) 28 threads 5.6 Ghz i9-14900K: 24 cores (8 performance, 16 efficient) 32 threads 6 Ghz It's like ~10% performance difference reply alwayslikethis 20 hours agorootparentprevGiven they are sold every generation, they must be practical to enough people. There are quite a lot of multithreaded tasks that can use so many cores. reply aredox 21 hours agorootparentprevIt was not just not economically practical: it wasn't practical. Yes, the flights were shorter, but between 9 hours cramped in a tight fuselage with little entertainment and 12 hours in a comfortable chair with headroom and entertainment, I would have taken the latter. There's a sweet spot abive the even more comfortable but vastly slower passenger liners, but Concorde was too extreme - and I guess the suborbital flights imagined by Musk are too far too. reply ghaff 20 hours agorootparentMy dad traveled to the UK all the time through JFK. He got upgraded to the Concorde once and his reaction was that he actually preferred first class on a 747--and that was at a time with less comfortable seating than exists today. Very few people really benefit from the faster flight time, especially if it doesn't have the range to do trans-Pacific. As you say, ocean liners are mostly too slow for anyone who is working but I was surprised to learn recently that they're not necessarily much more expensive than a business class flight. reply jnsaff2 21 hours agorootparentprevIt is incredibly cramped inside. I stepped into one in Duxford museum and especially the windows are tiny, the looking at the curvature of the earth thing would have been pretty hard. https://external-preview.redd.it/K5Z9Kd7AkxJz-eQm5I97pZYIDTa... reply dotnet00 17 hours agorootparentprevComparing chips in this context doesn't work, especially with Ryzen, where, due to chiplets, the effect of yields on price is mitigated significantly. They're higher margin products, not products that cost so much to make that they'd have to be priced so high. reply HPsquared 22 hours agorootparentprevNot forgetting the Apollo program. reply brcmthrowaway 12 hours agoprevHow fast would Boom aero do LDN-Singapore? reply hef19898 12 hours agoparentIf you start measuring today, now? Maybe, if they are extremely lucky, 15+ years? reply SergeAx 4 hours agoprev> most popular supersonic passenger airliner I don't understand this passage. There were only two supersonic passenger airliners, and Tupolev 144 was obviously off limits for Singapore Airlines. Is it LLM writing? reply laborcontract 22 hours agoprev [–] This should be labeled (2021), as evidenced by the comments. This blog is doing a weird thing where it's re-dating old posts to today, presumably to try to trick Google into thinking that it's fresh content, for SEO purposes. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Singapore Airlines ran a Concorde between Singapore and London in the late 1970s and early 1980s, dealing with overflight permissions and political issues.",
      "The Concorde service faced financial losses and incidents like an onboard stabbing, leading to its short operation before being terminated.",
      "The last Concorde flight jointly operated departed from Singapore in October 1980; G-BOAD, the aircraft, eventually set records and retired in 2003, now exhibited at the Intrepid Sea, Air & Space Museum in New York."
    ],
    "commentSummary": [
      "The conversation covers diverse air travel themes: route efficiency, non-stop versus stopover flights, iconic aircraft like the Concorde, supersonic flight challenges, and luxury travel comparisons.",
      "Participants discuss government funding impact on aerospace projects, technology evolution, and prospects for new aircraft designs, showcasing aviation's innovation and complexity.",
      "Emphasizes the industry's continuous pursuit of enhanced travel experiences through efficiency and comfort advancements."
    ],
    "points": 210,
    "commentCount": 183,
    "retryCount": 0,
    "time": 1709547695
  },
  {
    "id": 39598903,
    "title": "Startup Acquisition: $465M Deal Leaves Founders with Nothing",
    "originLink": "https://www.fundablestartups.com/blog/half-a-billion",
    "originBody": "Sell for Half a Billion & Get Nothing execution fundability fundraising legal Mar 03, 2021 In July 2018, Paddy Power Betfair (now known as Flutter) acquired FanDuel for $465M in cash. On the surface, this looks like a great win for the FanDuel founders and employees. However, because the two lead investors held strong liquidation preference rights, the FanDuel founders and most employees received nothing in this massive deal. What is Liquidation Preference? Liquidation preference is one of the most important terms in a term sheet. Liquidation preference determines who gets paid first and how much they get paid when there’s an acquisition. Because they take on significant risks, investors expect to get “VIP” head-of-line privileges to be paid upon a liquidation event such as an acquisition. Employees wait in line and collect proceeds only after all of the preferred investors take their share. What Are Liquidation Preference Terms? There are two components to liquidation preference. The first is the preference multiple, which basically says the investor gets a certain multiple of their investment amount. If the investor invested $5M and got a 2x preference, they would get paid $10M before the common shareholders got paid anything. For founders, obviously a 1x multiple is better than a 3x multiple. The second component of liquidation preference is participation, which determines whether the investor can take additional proceeds after the preference multiple is paid out. Capped or full participation rights would allow the investor to “double dip” in their payout. The simplified 3x3 matrix below provides a simplified view of how liquidation preference works. Healthy startups can get terms more in the lower left corner. Marginal startups may find funding, but liquidation preference terms will likely trend towards the upper right corner. How Liquidation Preference Hurt FanDuel Founders When the FanDuel founders raised funds, two key investors received a liquidation preference that entitled them to the first $559M in an acquisition. Founders and employees would be paid only if the acquisition exceeded $559M. Because the Paddy Power Betfair was for just $465M, the founders received nothing. To help founders visualize how liquidation preference could affect their startup, we have a liquidation preference scenario calculator available in our free members area. Why Founders Couldn’t Stop the Deal But wait. If this was such a horrible deal for the founders, why did they do the deal? The reality was the founders couldn’t stop the deal because they also granted the same two lead investors drag along rights. This drag along right forced the other shareholders to accept the decisions made by these two investors. Imagine how the founders felt when they received the notice below that the drag along right was being exercised and they could do nothing to stop getting short-changed. Lessons Learned: Build a Very Fundable Startup Every founder should learn from this disastrous scenario the importance of building a very healthy, fundable startup. A healthy, vibrant startup draws more investors during fundraising. The competition gives founders the leverage to negotiate for more founder-friendly terms. Healthy startups get better valuations, better terms, and raise funds with much less effort. Building a healthy startup requires great execution. Great execution involves doing dozens of tasks and processes the right way. Learn how to execute well with our premium startup training. If you are new to our startup training, we recommend starting by watching the 7 Keys to Triple Your Startup Payout, located in our Basic Tier. For more detail on liquidation preferences, refer to chapter 11 in our book. Good luck building your own healthy, fundable startup! Categories All Categories execution exit finance and tax fundability fundraising legal life and wellness sales and marketing talent validation Follow Us",
    "commentLink": "https://news.ycombinator.com/item?id=39598903",
    "commentBody": "Sell for half a billion and get nothing (2021) (fundablestartups.com)197 points by BIackSwan 7 hours agohidepastfavorite142 comments jv22222 3 hours ago> Lessons Learned: Build a Very Fundable Startup > Every founder should learn from this disastrous scenario the importance of building a very healthy, fundable startup. A healthy, vibrant startup draws more investors during fundraising. The competition gives founders the leverage to negotiate for more founder-friendly terms. Healthy startups get better valuations, better terms, and raise funds with much less effort. Hmm. The lesson I learn from that is to bootstrap/self-fund, rather than get investment in the first place. reply aetherspawn 3 hours agoparentYeah I have a small business and I sway strongly towards being contempt with letting the business grow at its own rate. No, it won’t have a 1 bil payout, but you make your own rules and you’ll get a healthy cash out from the dividends after only 1 year or so. It also forces you to keep pivoting and finding a cash cow rather than assuming your initial plan was any good. We’re on like plan #10 now and in hindsight if we went with any of our original plans we’d still be burning money whereas current plan was profitable after just 1 month once we figured it out. reply gnicholas 2 hours agorootparent> No, it won’t have a 1 bil payout Does anything have a $1B payout for the founder? I guess there are a few companies that achieve this, but it takes only a modicum of humility to realize you're not likely to be one of the most successful founders this decade. reply dr_dshiv 1 hour agorootparent> modicum of humility = turnoff for investors. They only care for chances at homeruns — singles and doubles are not welcome. You’d better swing for the fences, because that’s the purpose of VC. (This is my understanding, not my endorsement. Please correct as needed) reply highwaylights 11 minutes agorootparentThat’s the model essentially. Makes a lot of sense too. Anyone can get S&P 500 returns with little to no risk. That’s not to say they won’t lose money but it’ll be market returns either way, will be very liquid, and readily transparent to the holder. Given the risk involved in early stage investment the maths just don’t make sense for an investor to shoot for anything short of the moon. tldr; Seed funding / early stage investing is closer to lottery tickets and Vegas than to your 401k. reply hnbad 59 minutes agorootparentprevNo, that seems about right. Traditionally investors prefer investing in a number of moonshots with the hopes that one of them succeeds to such an extreme that it pays for the losses on the rest. There are even some investors known to invest in direct competitors to hedge their bets. The question only seems to be whether this strategy still works in a higher interest rate environment. As I understand it this development mostly stems from it being more profitable to invest with a low ROI (or a low probability of a high ROI) than to keep the money in the bank. reply DaiPlusPlus 2 hours agorootparentprevEven if anyone gets a cool $1b, the IRS is going to come for a good chunk of that... reply baq 1 hour agorootparentIf you’re paying taxes, you’re winning. reply gnicholas 2 hours agorootparentprevYep, although the qualified small business stock exemption comes in handy if/when that happens. You get to exempt a percentage of the gain of the sale of your stock, and if you roll over the gain into other QSBS (by investing in startups, for example), you can defer the non-exempted portion. reply noduerme 4 minutes agorootparentOr just be like Peter Thiel and do all your angel investing through your Roth IRA... https://www.propublica.org/article/lord-of-the-roths-how-tec... choppaface 2 hours agorootparentprevIRS takes less than 50%. reply generic92034 1 hour agorootparentprevWell, I for one could live with that... :) reply wdh505 3 hours agorootparentprevContempt is not equal to content. I think autocorrect got you reply fakedang 58 minutes agorootparentprev> No, it won’t have a 1 bil payout, but you make your own rules and you’ll get a healthy cash out from the dividends after only 1 year or so. Actually, it just might. RightNow was a bootstrapped startup back in the dotcom heydays, which managed a 9 digit exit after selling to Oracle. Midjourney is a unicorn without a cent of VC funding. Zapier raised just $2m, and they only got into YC on their second try. The old maxim of \"build something people want\" is crap honestly - it's more appropriately worded as \"build something people will pay for\". reply whiplash451 2 hours agoparentprevThat's not super useful advice for founders who (really) need some investment from the get go. The lesson would rather be: don't raise so much at the seed stage. Google got started with a $100K grant. FanDuel raised $400M in four years [1] And it looks like one of the the FanDuel founders did it again [2] This is reckless and should be a massive red flag for new joiners. [1] https://en.wikipedia.org/wiki/FanDuel [2] https://futurescot.com/fanduel-co-founder-secures-largest-uk... reply LudwigNagasena 1 hour agorootparentSo they raised $416M and sold for $465M. That's 12% ROI. The investors could just buy normal stocks and get similar returns in a year. I don't think there is anything remarkable about this case. It's not like they got a $100K grant and received nothing from a $500M sale. reply earnesti 1 hour agorootparentprevThe general lesson is: be smart. Also, taking funding with really bad terms might make sense sometimes. However quite rarely. The other viewpoint is, that typically at funded startups founders get some salary. You can view it also as an another job. reply bionsystem 3 hours agoparentprevIt's still better to have 10% of a billion rather than 100% of a million. It's just that \"valuation\" is only one of the metrics that really matters, and selling your equity has to be done progressively and by reading the small prints in every file. It's sad that startup founders have become so good at raising money that they forget that a path to profitability + understanding the actual financials (beyond just valuation) matters. reply RowanH 2 hours agorootparentIt's just what are the odds of 10% of a billion vs 100% of a million. Without any statistics at all to back this comment, I bet there are a magnitures more software companies out there that have made people 100% of a million, vs 10% of a billon. You're talking such a small pool (which might seem large in HN terms) when in reality there's an incredible number of small software companies globally. Of course not discouraging anyone from shooting for the moon. reply brycelarkin 3 hours agorootparentprevI’d rather take 100% of a million than Fan Duels’ 0% of half a billion. reply ENGNR 2 hours agoparentprevSo true. There’s only so many things you can master at once. Better to master making a product people will pay for, than playing the VC game for the first time when they’ve already mastered the rules reply gadders 58 minutes agoparentprevAlso, avoid that VC/Shareholder and blacklist them. reply justinlloyd 4 hours agoprevI am currently working with a start-up where the company is incapable of meeting its capex obligations. The founder raised a good amount of capital from investors a few years ago, and that provided a decent runway, but there's no traction, no KPIs, and whilst we've built some impressive technology, impressive technology does not bring in revenue. One of the problems (amongst many) is that the primary stakeholder has a perfectionist attitude to the user experience. Which in a start-up is deadly. Now, as I said, the start-up cannot meet its capex obligations and the current funding round is looking grim. It is definitely going to be a down round and it ain't going to be pretty should a term sheet get thrust under our collective noses. To get the developers motivated to stick around a little longer, \"generous\" equity packages are on offer, with a request to convert over-due back pay and future payments too, into equity . When I sought transparency - \"Can I see the cap table?\" - \"Can I see the terms of the investors?\" - \"Can I see anything?\" - the answer was invariably \"no.\" Essentially, they're asking me to make a nominal investment of over $200K in the company, accepting common stock in lieu of pay without any insight into the financials or the terms provided to other investors. It's worth noting that I had previously given the start-up a sweetheart deal, significantly discounting my usual rate and offering generous payment terms, in the spirit of support and belief in the project. This makes the current scenario even more disheartening. Compound that with what has become an overall toxic environment that I have euphemistically called \"challenging\" when asked to sum it up, and the future isn't bright enough to wear shades. Now this isn't exactly my first rodeo. I've seen the beautiful side of start-ups and liquidity events. And I've seen the dreadfully ugly side too. Hard lessons learned. And the ugly side shows up way more than the pretty one. I swear, some entrepreneurs must think I stepped off the boat yesterday. reply koliber 2 hours agoparentThey are unlikely to show the full cap table. Consider asking for the most recent 409a valuation, the total shares outstanding, and the size and liquidation preference of the past rounds. That’s a smaller ask that will give you the most important info. Most likely you already know the answer. In a fantasy scenario, you could take the stock they’re offering and demand to have a more senior stake and a 5x liquidation preference. reply munchler 2 hours agoparentprevWorking without pay is the reddest of red flags in any company. It’s not even a business at that point - it’s a volunteer gig. reply rimeice 1 hour agoparentprevWow, do not understand this approach some founders take. Being “radically” transparent by showing your cap table and talking through liquidation prefs/exit scenarios with every single employee is an absolute prerequisite for me. For me this has driven loyalty and paid dividends in culture and retention. I would not work for a startup where the founder wouldn’t share that info. reply tsylba 49 minutes agoparentprevYour writing style is the inspiring level kind. reply yashap 2 hours agoparentprevIt sounds like you already know, but that company is almost certainly in its final throes before bankruptcy. Do not take stock, get as much of your back pay as you can, and get out. reply whiterknight 3 hours agoparentprevAre we working at the same company? :) reply justinlloyd 2 hours agorootparentProbably. :) Look to your left. And whatever it is you do, don't blink. Don't. Ever. Blink. reply cwillu 1 hour agorootparentHello, Sweetie. reply vehementi 1 hour agorootparentprevWhat do you mean by that? reply cwillu 1 hour agorootparentIt's a Doctor Who reference to the episode “Blink”, which introduces a villain race of statues that can move, but only when unobserved. Incoherent snippets of dialogue seen on a DVD turns out to be half of a conversation being had with a specific viewer in the future, a transcript of which goes back in time. reply primax 1 hour agorootparentprevIt's a quote from the Doctor Who episode \"Blink\" reply choppaface 2 hours agoparentprevYou can at least file a wage claim without even having a lawyer https://www.calaborlaw.com/complaint/ In California, you are also entitled to more than minimum wage if you're an exempt SWE. reply bbarnett 3 hours agoparentprevI think you're in a same-same path with this. That is, If you say \"No, I don't want equity\", they still owe your back pay, there are multiple ways to get it, AND if either \"back pay\" or \"equity\" is to have value, they must have more funding. Which means, out of that funding can immediately come your back pay. So if you take the equity, or insist on pay, both are the same in the end. In fact, by not showing you what you need to know? They're forcing you to go after any investment they get for backpay. I've always found, in every single case, if someone doesn't want to show me something? It's because they know they're scamming you. Or selling false goods. No one with a sensible, reasonable deal ever desires to hide what the deal is. In fact, you should most insist and see what the founders are getting too. And the older investors. But either way, you are 100% owed salary, without dilution, and in most jurisdictions employee wages are class A, come first, before anyone gets cash -- investor or not. Is this the case in your region? reply throwaway290 12 minutes agorootparent> I've always found, in every single case, if someone doesn't want to show me something? It's because they know they're scamming you. Or selling false goods. they may also want to hide specifics if things are going great since then the employee may negotiate hard knowing how much they can get more reply consp 2 hours agorootparentprev> come first, before anyone gets cash Only after the taxman came over. And in most places the banks are preferred too. In my case you are only first in line if you have a court order for them to pay (give them notice to pay, repeat three times, go to court). Otherwise you are above the shareholders only but it's not that hard to work around that so I have been told. reply brazzy 2 hours agorootparentprev> So if you take the equity, or insist on pay, both are the same in the end. Well, no. As you write yourself, if you insist on pay, and they cannot get funded and have to wind down, you're first in the line. You may not get a large percentage of what's owed, but you'll get something. And if they can get funded, you almost certainly can get everything owed. If you take equity and they cannot get funded, you get nothing. If they can get funded, the way they avoid showing the terms, it's extremely likely there are the kind of shenanigans TFA describes and you'll get nothing. Really, the only scenario where taking the equity makes sense is if you believe the company will do well and the funding terms will be be favorable or at least balanced. And it sure doesn't look like that. reply throwawaysleep 2 hours agoparentprev> I swear, some entrepreneurs must think I stepped off the boat yesterday. Or can be bullied into accepting, even if you aren’t clueless. I do know people like that. Conflict is more painful than loss. reply hamburglar 4 hours agoparentprev> \"Can I see the cap table?\" - \"Can I see the terms of the investors?\" These questions should become so normalized that founders don’t bat an eye at it. If I am an early employee, we are partners. Wanting to know my percentage and the company’s liabilities before I sign is not unreasonable. I also want to hear you talk convincingly about your plans for future investment. reply avereveard 1 hour agorootparentIf you dont pay to get the equity at market value you're most definitely a worker. reply aledalgrande 2 hours agoparentprevLeave my dude. You'll be better off reply justinlloyd 2 hours agorootparentI am just outlining the current situation. Nothing stated in my grandparent comment about my future plans. Had a one hour casual chat with a start-up game studio in early February, and the offer came through in email on Friday for significantly more money and an interesting problem. Had a one hour on-site casual chat with an established robotics company today, that stretched out in to about five hours of casually meeting the team, grabbing coffee with the CEO and CTO and a verbal offer at the end of the day. It is for more money than my current position, but it isn't significantly more money. I have some thinking to do over the next couple of days. Plus a few more casual chats with companies lined up. Not sure if I should pump the brakes until Game Developers Conference though and see if there is anything there that's interesting. Work tends to fall out of the trees at that place if you shake the trunk hard enough. reply JonChesterfield 2 hours agorootparentAre you evaluating the pay at the current place as if there was no missing back pay? As \"slightly more money\" sounds like \"lots more\" when the current place isn't making payroll reply justinlloyd 1 hour agorootparent> Are you evaluating the pay at the current place as if there was no missing back pay? Correct. > As \"slightly more money\" sounds like \"lots more\" when the current place isn't making payroll Also correct. I am also evaluating the offers in terms of what I think I should be paid to make me satisfied with the compensation on offer. reply gumby 5 hours agoprevLiq prefs vanished during the ZIRP and I haven’t seen them return…yet. But the founders do have some leverage. If there is no incentive to do the deal they can just… not cause the deal to happen (different from blocking it, just not working on it). This is the same reasons you see big pay packets for the execs when a company is doing poorly or is bankrupt: otherwise they could just go do something else (get a different job). The poor guy on the assembly line may not have the same option. reply Finbarr 4 hours agoparent1x non participating is still the standard in most venture deals for companies doing reasonably well. But every dollar spent by startups essentially builds the preference against them. If you raise and don't spend the money, the balance sheet can be used to \"offset\" the liquidation preference in a sale. reply paulddraper 3 hours agorootparentThat is the standard and anyone not in a firesale should expect that m But the reason you raise is because you can turn each dollar into 1+ dollars of enterprise value. If you raise and don't spend... You've accomplished nothing except throwing some money at lawyers. reply Finbarr 3 hours agorootparentSure- the happy case with VC is that companies raise and spend the money to make more. The reality case is that many companies struggle to do that, and will burn a ton of money in the process, making the liquidation preference much more painful. reply paulddraper 3 hours agorootparentThat's right. Raising money raises the stakes. It raises the ceiling but also the floor. Be careful of getting buried. reply pclmulqdq 4 hours agorootparentprevI mean, every dollar you spend is a dollar off your balance sheet, so a company with $1 million more in cash is theoretically worth $1 million more. That is what \"offsets\" the liquidation preference. reply dheera 4 hours agoparentprevIf you're on really shitty acquisition terms like this, to the point where you are getting no payout for your hard work, one way you can get a small payout is to negotiate your employment contract with the new parent company to be very, very good. Make sure they give you and all employees a nice big sign on bonus and salary. They can deduct that from the proposed acquisition price. Is it ethical? I say yes, you should get paid more than $0 for your hard work to be handed over. Should the investors get a payout too? Yes, but they shouldn't be getting 100% of it, ever. reply ojbyrne 3 hours agorootparentFirst thing I thought of after reading this - I wonder did the existing founders/employees get something like this? I suspect at least some of them did. reply Finbarr 4 hours agorootparentprevYou can also get a carve-out of the deal proceeds. reply choppaface 1 hour agorootparentprevAnd makes sure your sign-on bonus has no clawbacks. Make it a payment condition of the contract. The bonus is for your signature not your warranty service. reply yieldcrv 4 hours agoparentprevmaybe in Silicon Valley, but in New England and other less mature markets nothing has ever changed feel free to change my view, for anyone passing by reply pclmulqdq 4 hours agorootparentNew England tends to have startup companies that are either biotechs or hard techs. It's not surprising to have to take bad terms for those companies. reply gumby 4 hours agorootparentFWIW I have started both in Silicon Valley and never had to take unfavorable terms like that. reply gumby 4 hours agorootparentprevI’ve only started companies in Silicon Valley (not even in SF once it started to get tech too) but have been on board in NY, Cambridge MA, and Europe. As a broad generalization: investors in Northern California are more afraid of missing out on the upside than losing some in the downside. The further east you go the more risk averse they get, until by the time you get to Europe the terms tend to be quite abusive because the investors are so worried about losing their money. reply wmf 4 hours agorootparentprevIf you won't move to get materially better funding I guess you deserve what happens next. reply jmward01 5 hours agoprevI have a friend that has given up on options. Even if he were to be #10 somewhere he would take any extra pay over any options. Stories like this show the wisdom of that. Are there really that many success stories for people other than for VCs and (maybe) founders out there anymore? Even if your options (eventually) get you 200k, how much did they cost you in years of lower pay. Even with a payout, considering interest on that missing pay was it worth it? reply jackcosgrove 1 hour agoparentI don't think startup compensation has yet to account for the wage gains in big tech since Facebook started bidding up comp. They are still operating as if it's 2009. We'd also need a new technology as big as the PC and the Internet to open up lanes for new entrants. As it is the incumbents have locked up most of the market so that's where the money is. reply Analemma_ 5 hours agoparentprevIt's not just your friend: a lot of people have given up on options. Obviously they're underrepresented here on HN because this is a startup-focused forum, but I know many, many people who have concluded \"options have an EV of zero, startups pay options in lieu of market-rate salary, therefore startups are a raw deal; I will only go to FAANGs\". They're sort of a dark matter universe since they are only visible in their absence here, but I do think that startups don't have access to the same talent pool as they used to, and sooner or later this will catch up with the ecosystem. reply pfooti 4 hours agorootparentThis is me. I work at a FAANG, about half of my very good compensation is in RSUs. All the startup companies I've talked to (I don't turn down recruiters out of hand) seem to like my skill set, but cannot really meet my comp requirements without valuing options as if they were 100% guaranteed to convert at the current high valuation. It's a bummer - startups do a lot of really cool stuff, but I'm at a point in my career where I can no longer really gamble that the options will pay out. I need the stability and mortgage paying power of actual comp. reply Xcelerate 1 hour agorootparentI wouldn’t discount startups altogether. I was previously at a FANG company when a startup doing some exciting work in a really interesting space offered to match my total comp in cash, plus the equivalent in options based on their recent valuation. All fully remote. At the time, the company I was at seemed to be going downhill quickly, and they began mass layoffs for the first time shortly after I left. What happened since then? Well, the startup options have very likely gone to zero. And my previous company’s stock price, which had been dropping quickly over the course of my last year there, made a hard U-turn right after I left and has been skyrocketing ever since (my family likes to joke that I caused the change of course at both companies). Was it really a bad decision though? It’s sort of hard to tell. At my previous company, my manager had put together a promotion packet, but this likely would have been cancelled with the layoffs, and there’s a chance I could have been laid off as well (as far as I can tell it was random). And then there’s the fact that for whatever weird reason, compensation for internal promos at this company heavily lagged that of external hires at the same level for many years. So at least up until now, I think I am even with or maybe slightly ahead of the counterfactual situation where I didn’t join a startup. Plus I have had new experiences, greater scope, and different types of challenges than I had previously, and I think it’s valuable to have a variety of novel experiences over a lifetime. Obviously, from this point forward my future (financial) outlook would be better off if I had stayed, but it’s hard to write off my decision to try something new. Other former coworkers also left to join startups, a few of which are likely to go public soon, so it’s not as if going from FANG to a startup always works out unfavorably, and if you join a startup that seems to be doing well at a later stage, the downside risk doesn’t seem that high actually. reply mil22 3 hours agorootparentprevI'm in this category, except became an invisible solopreneur after deciding startups are a raw deal instead of going back to FAANG. Between the unfavorable tax treatment, long time to IPO, the inherently high risk of options, it's just not worth it. Perhaps a good deal for the VCs, maybe for the founders, but not for even the earliest employees. See you in the next life. reply mcmoor 4 hours agorootparentprevI guess this is why worker-coop would never really take off. Most people are not in position to be paid in possible future profit rather than direct compensation. There's also nothing stopping employees at a public company to convert their entire salary into stock but not many do that either. reply meheleventyone 2 hours agorootparentWorker-cooperatives can and do pay salaries. reply nailer 1 hour agorootparentprevIs ‘EV’ enterprise value or something else? Thanks. reply stoneman24 1 hour agorootparentI was assuming EV was expected value. “For a pocketful of mumbles such are promises” Paul Simon. Seen too many promises forgotten/diluted to really trust anything other than cash. reply ardacinar 1 hour agorootparentprevExpected Value reply yieldcrv 4 hours agorootparentprev> startups pay options in lieu of market-rate salary this is true but this is largely due to an unworkable tax treatment of giving out illiquid shares to employees that rely on employment for money, and the longstanding tax regime would seek to tax employees that have higher value shares even if they can't get cash to pay for it. so this has left private companies in an uncompetitive situation with options as the poor workaround, analogous to chemotherapy where it hurts everyone in the absence of a better treatment, but you might come out ahead reply yieldcrv 4 hours agorootparentprevit looks like bi-modal distribution of compensation in tech, where there are tech startups paying one amount and options, and FAANG paying another amount plus liquid shares but crypto organizations have added another wrench for more than half a decade, leaving the other startups aside, they are startups paying one amount, and skipping the options and paying their employees RSUs of their liquid crypto tokens, competing directly with FAANGs on compensation as employees can sell those tokens just as - or even more easily - than they can sell shares in a brokerage account I'm saying it as if its news because the crowd here relies on people they respect saying the same thing to believe it in the absence of public and common knowledge, and that likely hasn't happened in the topic of anything crypto/web3 industry here reply thenoblesunfish 2 hours agoparentprevThe first time I was strongly considering a startup I had to essentially treat the options as worthless. Both because of the usual fact that most startups fail, but because of stuff like in the article: I couldn't convince myself in any way that there wouldn't be people ahead of me in the line if the company was sold, despite the fact that the founders themselves seemed totally honest. reply VirusNewbie 4 hours agoparentprevPlenty of people made millions joining series C/D deca-corns and selling in the IPO pop. reply scarface_74 4 hours agorootparentAnd plenty more made nothing. It’s survivorship bias in a nutshell. You take 10 people who worked for a public BigTech company that gave cash + RSUs and 10 people who got the same in “equity” in 10 different private companies, who do you think will be ahead in 10 years? 10 years is the average amount of time it takes for the few companies that make it to have an exit event. reply hamburglar 4 hours agorootparentIndeed. I consider my last startup option windfall a once in a lifetime success story, but the BigTech I’ve been at since I left the startup world has paid me roughly the same amount in RSUs when you consider all the time spent at both. And the BigTech is going to keep paying me pretty consistently. reply eastbound 3 hours agoparentprev> Are there really that many success stories for people other than for VCs and (maybe) founders out there anymore? I was the employee around #300 at Atlassian, the founders didn’t dilute the employees, and my options netted $3m (minus the taxes) after working there for 3 years and waiting 6 years. Scott Farquhar and Mike Cannon-Brookes were hell-bent on being honest, fair and giving back. There are good people out there. reply wtgthrow 1 hour agorootparentAs we are now down-under I will mention: Wisetech Global. I was too late to make anything meaningful over 3 years there, but it was transparent and very generous, and they made it as tax efficient as legally possible in Australia. If you joined in say 2009, grabbed some shares for 3 years you would probably be in the high 100ks at IPO, and millions if you held until now (but you could have also made those millions as a retail investor using your super or something too!). I sold too early :-( Also $currrent_company which I wont reveal. Run as well as Wisetech in terms of growth focus. Got some options. Learnt lesson. HODLing these fuckers to zero or \"very interesting\" levels lol! reply mil22 2 hours agorootparentprevAtlassian IPO'd in 2015... so you would have worked there 2006-2009. Congrats on the success, but that was more than a decade and a half ago... I'm not sure it counts as a recent example. reply hamburglar 4 hours agoparentprevI feel like the key is 1) don’t count on your options being worth anything at all, and 2) work for savvy founders that know how to get the employees taken care of. This means never taking more money than they need, not giving away multipliers, giving non-negligible percentages to employees, and focusing on making the business actually have intrinsic value instead of letting “how much can we raise” drive their sense of value. I feel like a lot of people (founders included) buy into the idea that the VCs should be able to walk in and screw everyone out of their equity because they hold all the cards. If you work for founders that believe this, you will definitely get screwed, partially because the founders will believe screwing you is just part of the game. reply AndrewKemendo 5 hours agoprevI wrote this in 2015 in response to Mark Suster suggesting that founders \"Run\" from liquidation preference and preference overhang in early deals: \"Run where? When I talk to my fellow early stage east coast founders, the majority aren’t beating away founder friendly term sheets. Even seed stage companies with revenue and traction raising relatively small amounts are giving away board seats and agreeing to multiple preferences because they have nowhere else to go. For founders, these deals can be make or break. For investors they can hold out and the only downside is slightly lower yield. So there is asymmetry in needs which means founders have little leverage.\" https://medium.com/@andrewkemendo/first-time-founders-and-th... reply lmm 4 hours agoparent> Run where? When I talk to my fellow early stage east coast founders, the majority aren’t beating away founder friendly term sheets. Even seed stage companies with revenue and traction raising relatively small amounts are giving away board seats and agreeing to multiple preferences because they have nowhere else to go. Away from the east coast? If funding on good terms is important to your business, go somewhere you can get funding on good terms. reply frfl 5 hours agoparentprevYou wrote that back in 2015. What, in your experience, has changed since then? 2021 would've probably, I'm guessing, been a lot more favorable for founders, but 2022/23/24 is likely a lot less favorable. reply lumost 4 hours agorootparentAnecdotally, not much changed for East coast startups. What did change is that West coast firms came to the east coast. Whereas 10 years ago it would be noteworthy to work for an SV company - it is now typical. I think the local market was outcompeted and absorbed outside of a few niches. reply AndrewKemendo 4 hours agorootparentInterestingly, the East Coast is way more primed for large deals than the West Coast is just from a population volume and infrastructure perspective. However, because there is just no high risk in the area, it just doesn’t happen. It’s literally just a physical access to rich people location thing which is crazy to me that it’s still the case but it seems to actually just be as simple as that. reply AndrewKemendo 4 hours agorootparentprevI stepped out of investing about two years ago because i couldn’t stomach the persistent narcissistic greed dressed up as virtue once I saw it for what it was. The last I saw it was just as bad and frankly getting worse for founders, as investors pulled back when the Fed moved on interest rates. Basically everyone just stopped taking risk except for the giant institutional funds and even then, as of last year were most just doubling down on existing. I heard similar actually last month at an event I was at - funds are sitting on dry powder and not doing cap calls. reply lumost 4 hours agorootparentOut of curiousity - how does a VC fund hold onto \"powder\"? Do they have terms to invest in a liquid fund, or some other arrangement? Seems like they'd face tough returns if they held powder for long. reply pcl 2 hours agorootparentTypically, VC funds don't have much cash on hand, and when they make an investment, they issue a capital call to the limited partners (LPs) in the fund. The LPs then are on the hook to send money for the investment, typically within a week or two. So, a $100M VC fund is really a commitment by the LPs to wire $100M over the course of ~5-8 years. LPs do all sorts of different things to manage the money they've committed but not yet invested. reply AndrewKemendo 4 hours agorootparentprevI don’t have enough experience at the institutional level to tell you precisely. At the smaller scale, though it just means that returns that were above and beyond distribution expectations so, for example, what the fund returns separate from the LP distribution, and then separate from distributions to partners is you know basically that net margin for the fund overall so that they would use as seeds for another fund or something like that. So effectively they are just not opening other fund lines, because none of the investments or markets that are coming up, match a risk profile for the amount of interest you can get back in other methods now. reply whiterknight 2 hours agorootparentprevHold treasuries? reply EchoChamberMan 5 hours agoprev\"FanDuel founders to receive no cash from sale to Paddy Power Betfair\" https://news.ycombinator.com/item?id=17485246 (July 8, 2018) Shamrock Capital Advisers and Kohlberg Kravis Roberts are the two mentioned investors, I believe. reply lupire 5 hours agoparentGambling company should know the house always wins. If I read crunchbase correctly FanDuel got $350M in funding by 2015, and sold for $465M 9 years later, for 33% ROI, or about 3%/yr. Founders don't deserve anything just for managing to hold on to investor capital and not lose it. Investing money at below market rates is not an achievement. Founders and employees weren't robbed. Also, OP is just a bad ad. reply hn_throwaway_99 5 hours agorootparentYour timing is wrong here, which breaks your calculations. I read some other articles that said FanDuel got $75 million in 2014 and $275 million in 2015, and then they sold in 2018, so not sure where you're getting your \"9 years\" from. reply fnbr 5 hours agoparentprevPrivate equity firms. I’d never take money from them. reply giantg2 5 hours agoprevIf I sell my company for $1T, but I financed $999B of it, should I expect to get a payout? Financing generally requires interest. Seems like the headline is trying to invoke outrage. reply sb8244 4 hours agoparentWhat if you raised 500B and still got nothing? That can happen with 2x or 3x liquidation preference. IDK FanDuel structure (not in article), but they only raised ~ 400M. Yet the investors got every dime up to 550+M. reply jojobas 2 hours agorootparentNobody forces you to sign away these preferences and participation. It might be a reasonable reflection of your pre-money worth (i.e. near zero) or it might not be. reply aledalgrande 2 hours agoparentprevGotta do it the Adam Neumann way... reply dheera 4 hours agoparentprevExcept we're talking about 100% and 200% interest rates, if you read the article. We're talking a $1G company financing $333M and getting nothing. reply grensley 4 hours agoprevFanDuel was really a lose-lose-lose - Investors got a meager return - Company and employees got nothing from the sale - Consumers got a gambling addiction reply bernardlunn 1 hour agoprevThe free money era is over. It is time for that message to sink in for both founders and investors. VC funds want founders to shoot for the moon but protect themselves by a) having a portfolio b) liquidation preference. As a founder you don’t have either. VC funds are not investors, they are middlemen who take very little risk. reply gnicholas 2 hours agoprevIt's worth mentioning that the founders were no longer with the company, and the terms of their prior exit are not public: > In this case, the minority shareholders are FanDuel's original founders, Nigel and Lesley Eccles, Tom Griffiths, Rob Jones, and Chris Stafford. None of the original co-founders still work at FanDuel, and it doesn't look like their original efforts will be rewarded — according to the deal documents, they're not going to make any money at all off of the company's sale. Of course, it's not clear what the financial terms were surrounding their departures from the company, and they could have negotiated a pay package [1] Getting precisely zero is not terribly likely if the founders are still with the company, since they would typically be considered key to the value of the company (at least for a time). There is/was apparently also a lawsuit about this deal. [2] 1: https://www.businessinsider.com/fanduel-founders-likely-to-l... 2: https://www.wsj.com/articles/fanduel-founders-former-employe... reply wolframhempel 1 hour agoprevIt's important to say that this isn't in the VCs interest either. As a VC, you want to keep the founders motivated to make you money and grow the pie - or at least to stay on and continue what they are doing. Taking away their personnel incentives through over boarding liquidation preferences is in neither side's interest. This is even more true when it comes to future investments. Founders talk to each other. And articles, such as this one, get the word out. So - next time you're choosing a VC for your promising startup, will you go with the one that forced the last company into a sale that left them with nothing - or will you go with one with a more founder friendly track record? reply ackbar03 5 hours agoprevof course, hindsight is 20/20, but... Why did they accept the deal with the investors?? That seems like a horrible deal? Google search shows they only raised $416mio total, roughly speaking anything above that and below $559mio the investors could have accepted, pocketed a quick and tidy profit, screw everyone else, and nobody can do anything about it. Or is that too naive of me? reply lupire 5 hours agoparentTime value of money. You don't get paid to sit on money. You ar paid of you grow the money. The founders wasted investor money on a non profitable enterprise. Employees got paid for doing non profitable work for investors. No one screwed anyone,they just flopped at their business and got rescued by a buyer. reply petesergeant 5 hours agoparentprevBecause the founders assumed they'd be able to turn the money invested into more than a 2x return reply bbarnett 2 hours agorootparentIndeed. And the founders agreed to this, and to the liquidation preference too. They knew the terms, knew what they were getting, and could have decided to not engage. (They took millions, and surely must have engaged an excellent lawyer and sought advice, right? So they knew, or should have known.) I agree that this can suck, but at the same time... would anyone be sad for the investors if they lost it all too? At least the founders and employees likely received some form of salary. If anything, it is the employees that are out. The founders negotiated these terms, knew of what they committed to. The employees took options, likely without full optics into all of this. edit: other parts of this say the CEO agreed to the terms, after the founders were pushed out. Still, what I say stands. The employees are the ones losing here, everyone else had optics into what the deal was. Something to always think of when an employee and being offered terms. Insistence to know the funding arrangement seems key. Even perhaps the contractual option to be able to get your shares out, cashed, as part of any new funding deal seems prudent. reply d--b 2 hours agoprevBeing the devil's advocate here. These liquidation things are happening when the company does badly. The founders (and the investors) were probably hoping to do a lot better than what they settled for. Half a billion dollar is great, but not so great if you thought you were going for 10bn... Without knowing the amount of the investment that the two investors made, the multiplier and the participation, and how the company was actually doing, all argument is moot. Who knows how much the investors put on the table in the first place. They took a risk, tried to have their ass covered should things turn bad, everybody agreed. Then shit happened and these guys managed to sell the thing before losing it all. Also, the founders probably got paid pretty decently in all their founding rounds, so I don't feel too sorry for them... reply ralph84 5 hours agoprevIf you can’t find investors who will invest at 1x preference, it’s a sign that you should seriously consider shutting down rather than raising more funding. reply frfl 5 hours agoparentWhat you say sounds reasonable. Can you provide any anecdotal evidence or case study that would make your comment less random-dude-on-HN-said-this and more grounded in fact/evidence. Edit: I mean specifically something like this article that was linked in another comment on this thread, https://medium.com/@andrewkemendo/first-time-founders-and-th... Just to help ground what you suggest in your comment to something concrete. reply dvt 3 hours agorootparentI think that the idea is that if you need (operative word) to buy one dollar for two dollars (not exactly the scenario, but imo the analogy works), then your business probably sucks. This is true more often than it is false. Basically, you're betting the one dollar will turn into 2+ dollars, but by that point the business model should've already been proven. reply ergocoder 5 hours agoparentprevOr raise at 3x preference and take millions in secondary reply andyish 47 minutes agoprevThey got time, salaries, and a credit line while they developed and tried to grow the product, and I imagine they had a pretty great time with $350m over the years. It's not as though they intentionally undercut them by a dollar, their sale was $94m short. If the story was what the headline leads you to believe and you 10x'd someone's investment and they did you over, why wouldn't you see red and scuttle your product? reply eschneider 4 hours agoprevThis sort thing is uncommon only in that there was eventually a high dollar liquidity event. When negotiating offers, I always try to get some idea of the financing and the liquidation preferences, if only so I can get some idea of what we'd have to pull in for shares to be \"in the money\" and quite often the number would be something insane like this. You just value the shares at zero. :/ reply wtgthrow 1 hour agoprevThe concept that you can sell for X, and X is large and you get nothing shouldn't be a surprise in itself. Investors wouldn't make money if they made every founder rich regardless of the risk they take. That said, in this case sounds like the founders were f'd over. Why did they accept those terms, why didn't a lawyer or advocate advise against it? reply BobbyTables2 5 hours agoprevI’ve always wondered what it means to be a “founder” when it is done with everyone else’s money. Are they not just effectively employees at that point? I fear such abuse of such just feeds a narcissistic species that focuses mainly on executive headshots and snazzy websites. Too many startup websites scream, “look, we have a website, $$$ millions, and an executive board — we’re all grown up now!” without actually having much clue. Medium sized business are more self confident and don’t bother. The small business founder that starts as employee #1 and grows organically is an entirely different animal and fully deserves the title. reply pclmulqdq 5 hours agoparentHaving bootstrapped and also founded VC-backed companies, the VCs are effectively making you an employee. But you get to gamble some of your paycheck, so it's a bargain a lot of people are interested in. A lot of my friends who are in the VC space see \"X person sold company Y for $500 million\" and sort of assume that X made at least $100 million from that transaction, when they very often didn't. The bootstrapping/small money path is so much more like actually starting a business, but it's a lot harder. On top of the investment, you get a lot of free publicity and some legitimate street cred from having gotten VC investment. reply lagt_t 3 hours agorootparentAfter all these years of VC funding, I can't believe there is still \"street cred\" attached to it. Not contradicting you, I'm just flabbergasted people haven't caught on their track record. reply devjab 4 hours agoparentprevThey are just employees on some level, but they are also typically owners with a lot of risk on the table themselves. Even if they come in with what would be “a lot” of money to most of us, it can’t compete with investment capital. We build energy plants with investor money as an example. They don’t invest in us, but into the projects we run, so it’s a little bit different than having someone invest in us (but I’ll get to that). We don’t solely build a power plant with investor money though, we build it with a mix of investor money, loans and our own money, and that last bit just got a whole lot larger (meaning we can do much larger projects) with private equity investing into us directly. So now instead of doing a tiny mom-and-pops sort of solar plant, we can build some of the largest solar plants in the world. The payout on the other side isn’t linear either, there is much, much, more money in a single large plant than in a hundred tiny plants. Especially when you sell the product a few years after its completion. In the current world, maybe it would’ve been better to not take private equity on board, but at the time it was done, nobody knew Putin would invade, that the global supply lines and manufacturing power would never recover from covid, or, that interest rates would go above 15% in countries where they were below 1%. Everyone knew the low interest rates wouldn’t stay, but we sure didn’t expect them to go so far above 8%. Anyway, even if the world hadn’t gone to shit, our company would still have been 10-20 people and not in the hundreds if we hadn’t acquired the massive amount of funds. Since while our founders are rather rich, they aren’t that rich, and this would’ve limited the company growth to such a significant degree that even two years of economic turmoil has us at a level we would’ve likely never reached. It’s not all gloom and doom either, people still need power after all. So you’re both right and wrong, founders sort of become employees, but not really. How much they lean toward the employee “title” typically depends on their deals. reply hermitcrab 1 hour agoprevThis article is a great advertisement for bootstrapping your business, rather than taking funding (not suitable in every case though). reply irjustin 2 hours agoprevAs a founder, this is painful to read because the founders+employees did everything they could to make sure the business was successful and by all accounts, it was. This might be the one case where I personally would accept the business failing is a better outcome. reply smurda 2 hours agoprevThis is happening more now. Liquidation preference was included in 20% of all Series B-E Silicon Valley venture financings in Q3 last year. When a company is not doing well and there are no other investors who will finance the next stage of the company, investors have the leverage and can include a liq pref and drag along to force other shareholders to sell. https://assets.fenwick.com/banner-images/Silicon-Valley-Vent... reply esafak 2 hours agoparentGreat document, but I could not find any mention of \"drag along\". reply smurda 2 hours agorootparentYeah, there’s good data on liq pref. For the drag I was just referencing the original article in this thread. Both liq pref and drag are such onerous terms that the only way investors can get them is when the company has lost their leverage by not having other interested investors. reply ldjkfkdsjnv 5 hours agoprevI think the perception of raising venture capital has soured. You really are an employee. Many founders of well known companies make far less than you would expect. Generate and own a SAAS business that does 1M in ARR, and you can almost walk away with 5-10M. This can be done in a few years. Do that with a venture backed business, they will force you to raise a series A and shoot for a 200M+ outcome. If you dont increase revenue, they want you to fail/go to zero trying to shoot for the moon. You might spend the next 5 years at a 200k salary, while you could have reached financial security at FAANG. Also, many companies have less respect for founders than you would expect. Its not the career boost people think it will be. You are building a different skill set thats not as useful inside a high paying tech company, aside from engineering. Which even then, is very different at a startup. reply ralph84 5 hours agoparentIt’s not a great time to be a VC investor either. Megacap tech stocks have outperformed most VC funds as of late. reply andrewstuart 1 hour agoprevWho were the investors? reply exogeny 3 hours agoprevFanDuel is going to do like $6B in revenue this year. This entire story above happened for three reasons: 1. The CEO made terrible decisions in regards to how much and who they raised from. They got in over their hands as the company grew and the entire founding team got fired. 2. The CEO immediately after, who took over an unprofitable business that wasn't growing and was in bad shape after the merger with DraftKings got called off, took a deal that was the best he could do at the time. That CEO was a non-founding CFO before taking over, and was basically put in the C-Suite by KKR as a term of their investment in the business. 3. That deal to PaddyPowerBetfair, now Flutter, was completed about a year before PASPA was repealed and sports betting was legalized in the US. Had #2 happened a year later, or if #2 had happened with that information in mind, the deal terms would have been insanely different and way more valuable. It's very easy in retrospect to say that the original CEO should have taken different terms, or that the following CEO should have taken a better deal. That's all hindsight. All that said, I have zero sympathy for the original CEO and founders. I feel loads of sympathy for the employees, but again, if they stayed on after the deal, they're now sitting on an absolute gusher of cash. reply cellis 2 hours agoparentEasy to say they’re sitting on a gusher of cash that was bought with hundreds of millions of ZIRP dollars worth of ads and a time when fantasy everything was nascent. They were in an absolute dogfight with Draftkings and losing iirc. I’m sure things look rosy now but back then there wasn’t really any way to say if they’d win and that’s reflected in the terms they raised at. reply bbarnett 2 hours agoparentprevYou know #3 is curious to me. I wonder what sort of optics the investors had on this, and if there were side deals after the forced sale. reply throwaway4736 5 hours agoprevThe CEO who signed all of those term sheets is STILL crying, like five years later, about how badly he supposedly got hosed, as if someone else’s signature is on all of those documents. It’s pathetic. reply whiterknight 2 hours agoparentAnd why would the technicalities of the contract make any difference to the sentiment of not being paid for your work? Situations evolve. You hope you get the contracts right, but you also hope you have trust to work with people as they do. reply exogeny 3 hours agoparentprevYou're the only person in this thread so far who seems to have a clear understanding of who is the one to blame in this story. reply ilrwbwrkhv 3 hours agoprevIt's almost like there are 2 worlds. One is this and the other is my world where I'm taking home 4 million a year bootstrapped. reply wly_cdgr 4 hours agoprevLOL @ the idea that the investors are the ones who take on the largest risks. Investors barely deserve 1x, never mind anything higher. reply dvko 3 hours agoparentI’m a bootstrapped founder but hard disagree with this take. Surely it can’t be the founder taking home a healthy salary from day 1 despite the company being far away from any revenue at all that is taking the risk in your book? reply realusername 25 minutes agoparentprevYeah there's a lot of rationalizing going on here, the investors have more generous terms because they have a lot of market power compared to anybody else at the company, nothing more, nothing less. It doesn't necessarily correlate with risks. reply dclowd9901 2 hours agoprev> Because they take on significant risks, investors expect to get “VIP” head-of-line privileges to be paid upon a liquidation event such as an acquisition. I’d like to challenge this notion. Risk comes from one factor and one factor only: how much skin do you have in the game? Skin isn’t money. Skin is how much are you in for. How much would this hurt if you lost. The ultra rich, when investing, have actually very little skin in the game. A million here, a million there. What’s the difference? They’ll still be impossibly wealthy even if everything goes tits up. Those folks should not make the big bucks in a deal. They haven’t risked anything, even some notion that they “risked” investing their money in this vs that: they picked the winner in this case. It’s fanciful thinking I know, but I see it as _the primary_ problem with either capitalism, or ultra wealth. The system favors people with the most money, and the people with the most money control the system. reply szundi 4 hours agoprevWho are these VCs? reply Brian_K_White 5 hours agoprev [–] ahhh fucking ad reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The acquisition of FanDuel by Paddy Power Betfair for $465M left founders and employees with nothing due to strong liquidation preference rights held by two lead investors.",
      "Liquidation preference, dictating who gets paid first and how much in an acquisition, played a crucial role in this outcome.",
      "Building a healthy, fundable startup is vital to attract investors and negotiate for more favorable terms, highlighting the significance of understanding and navigating investment agreements."
    ],
    "commentSummary": [
      "The article focuses on building a robust startup to appeal to investors, negotiate well, and comprehend financial metrics for funding.",
      "Topics include VC investment risks, profit optimization strategies, equity terms, tax effects, and founder challenges.",
      "Discussions highlight pitfalls in funding deals, distinctions between East and West Coast startup scenes, and the impact of private equity on business triumph."
    ],
    "points": 197,
    "commentCount": 142,
    "retryCount": 0,
    "time": 1709606879
  },
  {
    "id": 39597525,
    "title": "Miles Davis Jazz Album 'Kind of Blue' Recording in 1959",
    "originLink": "https://www.esquire.com/entertainment/music/a46871755/james-kaplan-miles-davis-3-shades-of-blue-excerpt/",
    "originBody": "EntertainmentMusic Miles Davis and the Recording of a Jazz Masterpiece Kind of Blue is the best-selling jazz album of all time. Here's what it was like inside the studio with Miles Davis, John Coltrane, and Bill Evans on the day they laid down one of the record's iconic tracks. By James KaplanPUBLISHED: FEB 26, 2024 SAVE ARTICLE Michael Ochs Archives//Getty Images Every product was carefully curated by an Esquire editor. We may earn a commission from these links. Jazz was at the apex of its artistic power and commercial popularity when, in 1959, some of the music's greatest innovators gathered to record in New York City. In this excerpt from the new book 3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool (Penguin Press, March 5, 2024), author James Kaplan puts us in the room as Davis and his collaborators record \"So What,\" the track that leads off what is often hailed as the greatest jazz album ever. March 2, 1959—a late-winter Monday in the second-to-last year of the Eisenhower administration. Fair and mild in Manhattan. Among the top stories in The New York Times that morning: a fogbound collision between the American Export liner Constitution and an oil tanker; the “commuter crisis” caused by ever-rising automobile use in the metropolitan area; tensions between white colonials and Black natives in East Africa. This last article quotes a British banker alleging “the vast unreadiness of the great majority of Africans for self-government.” An older, staider world. On the first page of the second section, a story by the young reporter Gay Talese about “Crazy Couple Clubs”—groups of jaded suburbanites seeking unusual amusements in the city: visits to yoga clubs, night court, Bowery restaurants. And deeper into the section, on what was still called the Theatres page, a review (glowing) by the paper’s jazz critic, John S. Wilson, of a Thelonious Monk concert at Town Hall. “He has carried apparent uncertainty to a high and refined art,” Wilson wrote. “He makes each performance a fresh and provocative experience.” If the Crazy Couple Club of Manhasset—by the evidence of the photograph in the Times piece a prosperous and cheerfully self-satisfied group—had dared to extend their Bowery slumming beyond ethnic restaurants, they might have wandered into the cozy, smoky, messy confines of the Five Spot Café, at 5 Cooper Square, whose owners, two Italian American brothers and ex-GIs named Joe and Iggy Termini, had for the past three years been booking some of the greatest jazz musicians of the day, including Cecil Taylor, Cannonball Adderley, and, most notably, Thelonious Monk, whom the Terminis had helped regain his New York City cabaret card—a conditional ID issued by the police department as a (legally and practically questionable) method of discouraging narcotics use—six years after Monk had lost his card, and with it the right to play in clubs that served alcohol, in a mistaken 1951 drug bust. Michael Ochs Archives//Getty Images Miles Davis at 30th Street Studio in New York City circa 1959. Amid the tobacco and reefer fumes and beer reek of that tiny, dark saloon (a glass of gin cost fifty cents; a pitcher of beer, a dollar), the members of the Crazy Couple Club of Manhasset might have found themselves sitting shoulder to shoulder with (though they almost certainly would have failed to recognize) such Five Spot habitués as the painters Willem de Kooning, Joan Mitchell, and Mark Rothko; the writers Jack Kerouac, Allen Ginsberg, and Frank O’Hara; and the young jazz titans Miles Davis, John Coltrane, and Bill Evans. The Five Spot was closed on Mondays, but on that March Monday Davis, Coltrane, and Evans had other business anyway: in Columbia Records’ 30th Street Studio, they were joining the alto saxophonist Cannonball Adderley, bassist Paul Chambers, and drummer Jimmy Cobb to begin making, under Miles’s leadership, what would become the bestselling, and arguably most beloved, jazz album of all time, Miles’s Kind of Blue. March 2 and April 22: three tunes recorded on the first date (“So What,” “Freddie Freeloader,” and “Blue in Green”), two on the second (“All Blues” and “Flamenco Sketches”). Every complete take but one (“Flamenco Sketches”) was a first take, the process similar, as Evans later wrote in the LP’s liner notes, to a genre of Japanese visual art in which black watercolor is applied spontaneously to a thin stretched parchment, with no unnatural or interrupted strokes possible, Miles’s cherished ideal of spontaneity achieved. The quiet and enigmatic majesty of the resulting record both epitomizes jazz and transcends the genre. The album’s powerful and enduring mystique has made it widely beloved among musicians and music lovers of every category: jazz, rock, classical, rap. This is the story of the three geniuses who joined forces to create one of the great classics in Western music—how they rose up in the world, came together like a chance collision of particles in deep space, produced a brilliant flash of light, and then went on their separate ways to jazz immortality. David Redfern//Getty Images Bill Evans at the piano during a performance filmed for the BBC in London in March 1965. No musician ever goes into a record date expecting to make history; every man in Miles’s band had recorded dozens of times before. “Professionals,” Bill Evans said, “have to go in at 10 o’clock on a Wednesday and make a record and hope to catch a really good day.” On the face of it, there was nothing remarkable about Project B 43079. The control booth at 30th Street was up a flight of stairs from the studio floor, in what had once been the balcony of the old church: producer Irving Townsend, recording engineer Fred Plaut, and Plaut’s assistant Bob Waller looked down from above as Miles talked to the musicians, who were placed around the open floor much as they’d stand onstage in a concert. On some recording sessions, Columbia producers used rolling baffles to isolate musicians or singers and eliminate sound leakage; at Davis’s direction, this session would proceed baffle-free, all musicians constantly aware of, and inspired by, each other’s playing. Sound leakage from one player’s mike to another’s was not only expected but essential. Each man had his own Telefunken U-49 microphone, except for Cobb, who had two, one pointed at the snare and one overhead to pick up the cymbals. The state of studio recording in 1959 was such that the musicians rather than the engineer were responsible for regulating the loudness or softness of their instruments, by dynamics or distance from the mike. As Davis picked up his horn, Waller started the tapes rolling—one master and one safety—on the Ampex reel-to-reel recorders, and Townsend pushed the intercom button. “Miles, where are you gonna work now?” he asked. The producer was referring to Davis’s position in relation to the microphone, from which he had apparently stepped back momentarily. “Right here,” Miles said. “When I play it I’m gonna raise my horn a little bit,” Miles said. His customary playing stance, onstage or in the recording studio, was to point his trumpet straight at the floor as he played, a position that communicated contemplation and moodiness, though it was primarily a way of regulating his tone. “Can I move this down a little bit?” He indicated the mike. “It’s against policy to move a microphone,” Townsend said, deadpan. The old church echoed with laughter. Outside the 30th Street Studio, Manhattan was Manhattaning: rounded buses and big yellow cabs grinding up and down the avenues; car horns and scraps of radio music and pedestrians’ voices echoing in the deep-shadowed side streets. Outside, the everyday clamor and clash of a city afternoon in late-winter 1959; inside, the densest quiet as a passage outside of time proceeded: the recording of CO 62291, the number that would come to be titled “So What,” leading off the album soon to be known as Kind of Blue. The first take began. There was a false start of four seconds, followed by an incomplete take of forty-nine seconds. Townsend interrupted from the booth: something was interfering with the song’s profound hush. “Hold it,” the producer said. “Sorry—listen, we gotta watch it because, ah, there’s noises all the way through this. This is so quiet to begin with, and every click—watch the snare too, we’re picking up some of the vibrations on it—” Miles, ever on the lookout for meaningful accidentals, demurred. “Well, that goes with it,” he said. “All that goes with it.” “All right,” Townsend allowed. “Not all the other noises, though . . .” Another false start, seventeen seconds. An incomplete take, a minute eleven. A telephone rang in the control booth. Once quiet was restored, three more false starts, of sixteen, seven, and fifteen seconds. Then, history. The full Take 3 was nine minutes and thirty-five seconds of musical transcendence. Someone—some say it was Gil Evans; Bill Evans’s biographer Peter Pettinger and the trumpeter Wallace Roney asserted it was Bill Evans—had sketched out a single-line introduction to the piece, a hushed dialogue between piano and bass, proceeding at its own dreamy pace and built on meditative, European art song–esque chords (the fourth, with two white piano keys between the lower and upper notes, being an interval which, enigmatically, presents as neither major nor minor), then on skipping single notes played in unison by the two instruments. Paul Chambers then set the rhythm, plucking the eight-note figure that was to become immortal, the call that began the rhythmic call-and-response of “So What.” Evans then answered, followed by the rest of the sextet. “The piano’s (and then the band’s) answering ‘amen’ (or ‘so what’) riffs,” Pettinger writes, “were built up largely in fourths, as opposed to the thirds that are basic to the tonal system, as exemplified by Bobby Timmons’s comparable composition ‘Moanin’,’ recorded by him some four months earlier.” Timmons, the pianist for Art Blakey’s Jazz Messengers, was all of twenty-two when he wrote “Moanin’,” a call-and-response number which, unlike Miles’s, had a strong, foursquare gospel feeling. Call-and-response was an ancient form, with roots in African ritual, civics, and music; it traveled to America and underlay African American work songs and religious rituals from 1619 on. Timmons’s song, like Blakey’s quintet and Horace Silver’s compositions and bands, was hugely influential in pointing jazz in a more soulful direction. Miles would have known the tune well—would he have enjoyed its old-fashioned wholeheartedness? been impatient with it? It didn’t matter: He was proceeding on his own musical path, channeling strong emotions through the prisms and filters of his biting intelligence and contrary spirit. Some people called this Cool; under the surface it was anything but. 3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool $35 AT AMAZON The full Take 3 was nine minutes and thirty-five seconds of musical transcendence. Miles’s solo, an impromptu composition in itself, would gain its own immortality: generations of musicians would memorize it note for note. Miles is talking to you in that solo, playing in the middle sonic range of the human voice, and he’s got all kinds of things to say, in brief and at length. He starts and stops; he starts again and goes on. And we’re freshly astonished at how very much he can express, in so few notes, in the moment. The richness each of the soloists was able to create improvising over just two chords, D and E♭ Dorian, vindicates Miles’s modal concept. Coltrane was in exploratory rather than loud and fast form, traveling up and down each scale to find astringent delights. Cannonball was no less seeking, but lush toned as always, and unable not to find melodies and tuneful fillips, even in this minimalist frame. And Evans’s solo was perhaps most in sync with the tune’s hushed simplicity: playing quiet arpeggios and complex chords a little shyly at first, but then growing more assertive—and surprising: “I’m thinking of the end of Bill’s solo on ‘So What,’” Herbie Hancock told the writer Ashley Kahn. “He plays these phrases, a second apart. He plays seconds.” Still filled with wonderment forty years after the fact, Hancock was talking about an interval on the piano that’s barely an interval—two adjacent keys played simultaneously. By itself, the sound is dissonant; in this context it’s startlingly expressive. “I had never heard anybody do that before,” Hancock said. “He’s following the modal concept maybe more than anybody else. That just opened up a whole vista for me.” CO 62291 wasn’t yet officially named on the day it was recorded, but in the years after Kind of Blue’s release, more than one person would take credit for its title: John Szwed writes that it “may have been suggested to [Davis] by Beverly Bentley [a girlfriend of Miles’s, later Norman Mailer’s fourth wife], who said it sounded like his favorite dismissive remark, but folks in East St. Louis were more likely to believe that it came from Miles’s brother-in-law’s retort when Miles told him in 1944 that he was leaving for New York: ‘So what?’” And the actor Dennis Hopper, who said he was a close friend of Davis’s, recalled that the phrase was a comeback he, Hopper, used to deploy, jab-like, when Miles ran his mouth while the two of them sparred together: “Oh come on, Miles, so what?” “So one time I came into the [jazz] club,” Hopper remembered, “and he said, ‘I wrote a little song for you’—and he played ‘So What.’” Robert Abbott Sengstacke//Getty Images John Coltrane playing tenor saxophone during a recording session in New York in the early 1960s. The word “timeless” has become a cliché, a selling tool for luxury goods. And yet Kind of Blue is a timeless album, and “So What” arguably its signature number. What is this about? For sixty years and more, jazz and popular music had consisted of songs that told stories, either explicitly—in lyrics—or in their construction. The most common song framework in both genres was known as AABA: two choruses followed by a bridge (aka channel, release, or middle eight), followed by an out-chorus. (Popular songs of the first half of the twentieth century also typically began with a verse: a brief, explanatory introduction that might or might not be included in performance or on recordings.) The sound of tunes made this way was a satisfying blend of exposition and resolution. Popular songs, which often became the explicit or implicit basis for jazz tunes, were written in a given key, and while they might wander chordally—see Oscar Hammerstein and Jerome Kern’s “All the Things You Are” or the bridge of Richard Rodgers and Lorenz Hart’s “Have You Met Miss Jones?”—they tended, satisfyingly, to come back home to that first chord. This was even truer of the blues, with its intrinsic I-IV-V format, a structure that was restrictive but deeply pleasing. A story was told, and you learned the outcome, even if it was sad. (See: “Moanin’.”) You knew how it turned out—maybe you knew before the song started—but hearing about it could make you forget your troubles for a while or identify with the singer’s or the musician’s troubles. But with Miles, in life and in art, it was always the thing withheld. And the essence of modal music—the essence of “So What”—was that you had no idea how it turned out, or if it turned out. Which was pretty much the way the world was looking at that moment, and maybe the way (you had to think) it was going to look from then on. It was 1959; the world jostled and rocked. American automobiles sprouted double headlights and fins. Batista fled Havana; Castro entered. Khrushchev met Mao, visited Disneyland, debated Nixon in a model kitchen at the American National Exhibition in Moscow. Alaska and Hawaii joined the Union; the flag gained two stars. The crashes of commercial airliners were a depressingly regular event. The music died in Clear Lake, Iowa; the Clutters died in Holcomb, Kansas. Johnny and the Moondogs, a British guitar trio led by the eighteen-year-old art student John Lennon (his bandmates were the seventeen-year-old Paul McCartney and sixteen-year-old George Harrison), played gigs around Liverpool whenever they could find a drummer. Rocky and Bullwinkle and Bonanza—in color—debuted. As did the Xerox machine. As did the Barbie doll. NASA named the seven original astronauts, and the Space Age began. Gypsy and The Sound of Music and A Raisin in the Sun premiered on Broadway. The Boeing 707 and the ICBM were introduced: travelers could now fly to far-off destinations at unprecedented speeds, as could nuclear bombs. So what. From 3 SHADES OF BLUE: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool by James Kaplan, to be published on March 5, 2024, by Penguin Press, an imprint of Penguin Publishing Group, a division of Penguin Random House, LLC. Copyright © 2024 by James Kaplan. WATCH NEXT Advertisement - Continue Reading Below music The Best Songs of 2024 (So Far) Everything We Know About Beyoncé's 'Act II' Album Usher's Super Bowl Halftime Show Was a Party Jay-Z Called Out the Grammys Everything We Know About Taylor Swift's New Album Trevor Noah's Grammys Monologue Hit the Right Note 11 Rare Vintage Photos of Lou Reed The 2024 Super Bowl Lineup is Finally Set Justin Timberlake Is Feeling a Little Selfish What I’ve Learned: Billie Joe Armstrong The Most Anticipated Albums of 2024 Usher Will Headline the Super Bowl Halftime Show Advertisement - Continue Reading Below",
    "commentLink": "https://news.ycombinator.com/item?id=39597525",
    "commentBody": "Miles Davis and the recording of Kind of Blue (esquire.com)173 points by tintinnabula 10 hours agohidepastfavorite79 comments simonw 9 hours agoTip for San Francisco Bay Area Jazz fans: I just found out about https://bachddsoc.org/ in Half Moon Bay - an apparently legendary jazz venue that's been hosting world-class musicians since the 1950s. I went on Sunday and it was fantastic - a really quirky venue, great music. I think it may be one of the Bay Area's best kept secrets. reply AlbertCory 3 hours agoparentNot a secret, but I haven't thought about it in 20 years. I didn't realize it was still going (maybe that's sort of a secret!) reply bitbckt 8 hours agoparentprevThere aren’t many of these venues left. Keep showing up! reply ashxel 9 hours agoparentprevfor sure. saw vijay iyer trio there 6 years ago and it was very memorable. reply resters 8 hours agoparentprevwow I used to live near there and somehow missed it. Looks great! reply vjulian 6 hours agoprevThere are a few pieces of music that I avoid listening to too frequently. That’s because I find them to be too beautiful, too profound and—illogically—I don’t want to wear them out. Perhaps also because I find myself consistently in tears when listening to them, at times even when thinking about them (as I am now). One of those pieces is Blue In Green, as recorded for Kind of Blue. God Bless Bill Evans. reply gimmeThaBeet 1 hour agoparentFor a given jazz standard, if the renditions exist, chances are my favorite one is either Bill Evans or Oscar Peterson on piano. He's like if Chopin learned jazz. My potentially hot take is Blue in Green is Coltrane's best solo on the album. It's purely a taste thing, the best way I can describe it is Coltrane frequently breaks my sense of immersion in a song. I don't know if it's because Adderly wasn't on that one or what, but it's pretty much the only track where I feel where he gave the track what it needed instead of what he wanted. reply aworks 5 hours agoparentprevInteresting. The entire Kind of Blue album has the opposite effect. If I hear the music now, it pleasantly tranports me back to the early 1980s when I bought it as one of my first CDs. Then I go on to focus on the music... reply golol 3 hours agoparentprevI very much agree with this sentiment. For me it is the 9th symphony that I don't want to wear out. reply appel 6 hours agoparentprevOut of curiosity and if you don't mind me asking, would you mind listing the others? reply vjulian 5 hours agorootparentBeethoven quartet opus 135, especially the third movement. Several others, about an album or two worth of music. Ravel’s quartet is up there. I melt when I hear George Duke’s rhodes solo on Blessed Relief (a great piece overall). Stuff like that. reply smingo 2 hours agorootparentBarber's Adagio for strings, although it gets referenced enough; Khachaturian's Spartacus and Phrygia, especially the brass version; Holst's planets, esp Uranus; Beethoven's 7th; Gorecki's 3rd. reply noman-land 4 hours agoparentprevBill's chords just hit different. reply aEJ04Izw5HYm 25 minutes agoprevRadio Station: \"Giants of Jazz\" was an absolute revelation to me discovering only absolute classics over the past 50 years. 'In a silent way' is still my favourite MD album. Reading the \"Miles:The autobiography\" while listening through his discography was one of my favourite ever reading experiences, thoroughly recommended. reply rplnt 10 hours agoprevThe preface reminded me of the Koln Concert, which is apparently the best selling piano album/recording. And the story behind it is quite interesting. https://en.wikipedia.org/wiki/The_K%C3%B6ln_Concert reply donbrae 1 hour agoparentAnd soon to be a film: https://www.imdb.com/title/tt20414360. reply LargeWu 9 hours agoprevThe book \"Kind of Blue: The Making of a Miles Davis Masterpiece\" by Ashley Cole is great if you're into this album. One of the most notable things about this album is that, while there were a few false starts, they used the first complete take for each track, which is astounding. I personally think it's the greatest achievement in the history of music. reply AlecSchueler 9 hours agoparentThe autobiography is also a phenomenal book while we're on that topic. reply AlbertCory 3 hours agoparentprevNormally, I roll my eyes at claims of something being \"the greatest ever.\" However, I think this one is a defensible claim. I like Miles' quote about it: \"If I'd known it was going to be such a hit, I'd have asked for more money.\" I always wonder what they thought at the time. \"Well, I guess that was pretty good.\" maybe? reply thangalin 2 hours agoprevFor folks wanting to learn a little more about the history of blues music, please have a read, look, and listen: https://dave.autonoma.ca/playlists/blues/dancing Apparently there are issues with the videos on Safari/iPhone. A hint to a fix would be welcome. reply xrd 8 hours agoprevI'm hoping the jazz fans here will know the right answers to this question: where is the best place to see jazz in Tokyo? What about Kyoto? reply dodos 8 hours agoparentI've been having trouble finding good ones in Tokyo. Blue Note Tokyo seems nice but is pricey and often sold out far in advance. I've seen some others I'd like to go to, but they double as cigar bars and my lungs don't enjoy smoke. reply dumbo-octopus 8 hours agorootparentIt seemed to me every \"X\" doubled as a \"Smoking X\" in Tokyo... reply ethbr1 8 hours agorootparentBeing jetlagged and people-watching in a coffee shop as Tokyo woke up was eye opening. Newspapers and cigarettes wall to wall. Especially since Paris was substantially less smokey, for all the French stereotypes. reply xrd 8 hours agorootparentprevI had a really nice time seeing a jazz show at the Tower records last year. It was completely by chance, just happened to be there right when it happened. But I would like to be more intentional this next time. Thanks for your notes. I saw a guy play jazz in Miami two months ago and he lives in Tokyo. I'm going to see what he's up to and where he plays. Email me if you want to know what I find out. reply porknubbins 7 hours agoparentprevI used to like jazz club Overseas in Osaka. Just a small place but very nice people. Also in Tokyo all my best experiences were at small intimate places where you got to know the owners or at least have a nice conversation about jazz with them. reply Daz1 5 hours agoparentprevThey don't want tourists in them. reply WalterSear 8 hours agoprevI recently read that Miles was delving deeply into the Lydian Chromatic concept when he was composing for this. I haven't been able to find out more about this, or how much it actually influenced him or his preparation for Kind of Blue, but it's about as strong an endorsement for Barry Harris's work as it gets. reply cbzbc 7 hours agoparent> I haven't been able to find out more about this, or how much it actually influenced him or his preparation for Kind of Blue, but it's about as strong an endorsement for Barry Harris's work as it gets. You mean George Russell? And my impression is that it was influential with Miles because it was one of the early examples of the modal/horizontal approach being fleshed out as text over and above it being LCC specifically. Harris' approach is based on what he calls the sixth diminished scale. reply WalterSear 6 hours agorootparentDoh! I'm messing up my Jazz theorists. Yes, George Russell. Barry Harris just keeps popping up in my youtube algorithm. reply richardfontana 5 hours agorootparentIndeed, I would say Barry Harris's music is characterized by a complete rejection of the George Russell legacy. Barry Harris's music is sort of like what jazz would have been like if _Kind of Blue_ had never happened. reply hamburglar 5 hours agorootparentprevIt does seem like there is an explosion of Barry Harris enthusiasm on YouTube in the last 6 months or so. I was wondering if I’d done something to make my algorithm latch onto it. reply cdme 10 hours agoprevAn undeniable classic, but I remain partial to Bitches Brew. reply seanhunter 4 hours agoparentIf you like Bitches Brew, I would advise checking out \"Miles Davis at Fillmore: Live at the Fillmore East\" [1] (ideally the 1997 reissue rather than the original which was pretty heavily edited). It's like if someone took Bitches Brew and wanted to make it even more intense and hostile. Genuinely terrifying, amazing music. [1] https://www.discogs.com/fr/release/5036048-Miles-Davis-Miles... reply wk_end 9 hours agoparentprevIn A Silent Way for me. What a brilliant musician. reply AlecSchueler 9 hours agorootparentThis is the one album that I can never say no to if it pops up in my recommendations. reply cdme 9 hours agorootparentprevAlso a fantastic choice — I love so many of his albums for different reasons. reply squidsoup 8 hours agoparentprevWhat's remarkable about Bitches Brew to me, is that in many ways Miles was a traditionalist, and yet his foray into the avant garde was more successful than most of his contemporaries. reply WarOnPrivacy 5 hours agorootparent> What's remarkable about Bitches Brew to me, is that in many ways Miles was a traditionalist, and yet his foray into the avant garde... John McLaughlin was how I wound up at Bitches Brew - and got introduced to Miles. BB was a departure from the early JMC I was used to and it took me a while to bite into it. Hearing Holland and DeJohnette helped. A few years later I was all in. Saw Miles on my 19th bday. reply wiredfool 1 hour agoparentprevI'm partial to Cookin. So I guess I'm on team early Miles. reply mturmon 4 hours agoparentprevWho wouldn’t love them both. But I’m partial to Miles from the Bitches Brew era as well. I really got hooked on the reissue of the “bootleg“ 1969 European concerts a few years back. It gave a whole new perspective on his style in those years, it’s different than BB in some ways. reply aworks 5 hours agoparentprevI think Kind of Blue is a better album but I listen to Bitches Brew more often, maybe due to greater variety of song and texture. The former is like swinging, poignant velvet, the latter a swirling, electronic groove. reply richardfontana 4 hours agoparentprevI actually now think that Miles's best work was his early stuff, before _Kind of Blue_, but that has to do with changes in my general preferences around jazz. reply asaph 9 hours agoparentprevThat album is more challenging to appreciate for listeners new to jazz, while Kind of Blue is immediately accessible to mainstream listeners. reply dhosek 7 hours agoparentprevWhich, in many ways is a polar opposite to Kind of Blue. Whereas Kind of Blue is comprised almost entirely of first takes, Bitches Brew was assembled in the editing room, pulling together multiple takes and pieces to form the tracks on the album. reply jeffbee 8 hours agoparentprevI can't take that record off the shelf without thinking of this CBS memo. https://i.imgur.com/2GMwXPo.jpg reply cdme 8 hours agorootparentI’ve never seen this before — love it. Thanks. reply AdmiralAsshat 9 hours agoparentprevI'm more of a Dark Magus guy, myself. reply jelavich 9 hours agoparentprevI’ve come to love all sides of Miles. reply WalterSear 8 hours agorootparentI recently dicovered that he played on lots of other artists records that I have yet to listen to. reply tiahura 7 hours agoparentprevMy favorite was his work on Miami Vice. https://m.youtube.com/watch?v=kLyqgd-KTRM reply brcmthrowaway 8 hours agoprevHow do I listen to it? Just seems like a mishmash of sounds.. I don't see the genius. reply gooseyard 5 hours agoparentI've been playing jazz most of my life and one of the most frustrating parts of it has been this idea that there should be some immediately obvious virtue in jazz as a musical style. If you're a musician who enjoys improvisation, chances are good that you'll be drawn to jazz, especially to recorded works from the era of albums like Kind of Blue. But you aren't some kind of philistine if it doesn't hook you. For example, I have studied a lot of classical repertoire in order to get my technique together to improve my jazz playing, and while I appreciate those compositions, I don't really listen to them for pleasure. My opinion about their value as works of art is not based on whether I enjoy them or not. I don't enjoy many types of food but I recognize that they have enormous value to people who do enjoy them. We like what we like, and we shouldn't judge styles on the basis of whether we like them or not. If you listen to a record like Kind of Blue and it doesn't do anything for you, don't sweat it. You wouldn't be a better person for liking it, and no true lover of jazz would judge you on the basis of whether you like that record or not. If you're genuinely curious, listen to other jazz records from the era, maybe you'll like those. If you don't, no big whoop. There's a lot of music to love. Listen to what you like. reply cfn 15 minutes agorootparentGreat answer. I would add that ones taste may change with time, mine certainly did. I couldn't see the point of jazz as a youngster but came to enjoy it greatly later in life. reply heleninboodler 8 hours agoparentprevThis is a fascinating comment to me. I understand not really feeling jazz, but it doesn't even sound like a song to you? My only suggestion is to jump into the middle of a few songs and try to hear the rhythm and song structure. They can often be difficult to pick up when they start slow. All Blues might be the easiest. I sincerely wish you luck, because it's really incredible music. reply tjr 5 hours agoparentprevLike many jazz albums of its era, this one uses a pretty typical standard jazz sort of formula, to wit, roughly[0] - The whole ensemble plays an arrangement of the melody of the tune - After playing the melody through, the group jumps into a series of alternating solos, in which each band member improvises new melodic ideas on top of the chord progression of the original melody - After the solos are done, the original melody is returned to, either in full or an part, to recapitulate the initial theme and bring the performance to a conclusion Do this (more or less) for each track on the album. It's really not necessarily obvious at first. It can help to follow along with sheet music. But what is the genius? Or, how is this particular jazz album more genius than another? Say, how is it more genius than \"Relaxin' with the Miles Davis Quintet\" which was recorded earlier (and also very good)? I think a lot of it is the particular ensemble on this record. Miles brought together a lot of truly exceptional musicians, and they all brought their own bit of genius to their own performance. If you have a jazz performance with a great trumpeter, paired with an okay pianist, an okay drummer, and an okay bassist, then the whole performance likely won't be genius. But if you pair a genius trumpeter with a genius pianist, a genius drummer, etc., then you'll likely get better results. Especially if they are all kind of genius in the same direction. On top of that, this particular album was exploring a new compositional style that emphasized modal composition rather than the more typical dominaint-tonic-oriented composition. There was a freshness to it, both for listeners at the time, and for the players themselves. They were exploring this sort of sound for the first time, and were enjoying it, bringing that mentality to how they performed. [0] Roughly indeed! I don't remember offhand if every track on Kind of Blue follows this exactly. reply damontal 7 hours agoparentprevIf you listen to that kind of jazz long enough you start hearing things. Patterns. References to other songs or standards in a solo. Two players interacting and having a kind of conversation with each other. I’ve listened to this album so many times I can play it in my mind. If you’re really interested then listen to it enough so that it becomes somewhat familiar. You’ll be able to distinguish the structure from the improv and hear the interaction between the two. Bill Evans was a classical pianist. Maybe just focus on the piano and try to see what it’s doing in the song. My Favorite Things by John Coltrane may be a better place to start though. It really emphasizes a familiar melody. reply WalterSear 6 hours agorootparent> If you listen to that kind of jazz long enough you start hearing things. When someone asks me for an introduction to jazz, I point them at Kind of Blue. I'm not sure what the grandparent comment is looking for. reply nicooo 6 hours agorootparentNiels Ln Doky's TED talk [0] helped me understand and appreciate Jazz. [0] https://youtu.be/3ee-XROfON0 reply riwsky 5 hours agoparentprevLocal man listens to Kind of Blue, remarks “So What” reply jsz0 2 hours agoparentprevThe best advice I can give anyone trying to appreciate jazz is to think of it like listening to a conversation in an unfamiliar foreign language. You don't have to understand the literal meaning of the words/notes to infer their emotional tone, pacing, and the ebb and flow of the conversation. Jazz in this way is an abstraction of spoken language. It communicates a vibe or general emotional landscape that you can interrupt for yourself in a personal way. As a more accessible starting point I'd actually recommend Miles Davis's Porgy and Bess album. It's a hybrid of Gershwin's composed music and jazz. Being more structured than most jazz recordings it will give you some context/framework to enjoy the jazzy parts. Most of the songs are short and digestible. reply code_runner 8 hours agoparentprevMake sure you’re listening to kind of blue. Not any of the extended deluxe this and that. Just the original tracks. All the extra stuff is great, but the original is the perfect album. What music do you typically like? There will definitely be a gateway to jazz in there somewhere. reply buserror 5 hours agoparentprevYou could try Dave Brubeck \"time out\" as an introduction to jazz, it is awesome in many ways and has quite a few catchier tunes. Also Bill Evans Trio and listen to that first. Only problem is that you'll be addicted for life afterward, but it's not that bad as a habit :-) reply exitb 2 hours agoparentprevPeople are acting all shocked, but I do feel that jazz is an acquired taste. Go listen to something like Jazz at the Pawnshop by Arne Domnérus - it's lighter and more playful. If you're into electronic music, you can also listen to nu jazz and acid jazz records to get some of the jazz \"vocabulary\" mixed into other genres. reply squidsoup 8 hours agoparentprevWalk through New York City. You will hear a \"mishmash of sounds\", and perhaps begin to understand. reply khazhoux 1 hour agorootparentAnd make sure to wear a fedora and a sharp suit, tip the newsboy, and give a warm “heya fellas!” to the navy boys taking in Times Square. reply Daz1 5 hours agoparentprevYeah agreed, always thought it was an average album. reply freejazz 7 hours agoparentprevDo you have an auditory syndrome or something? are there any melodies at all, outside of jazz, that you like? or even recognize as melodies??? reply Biganon 3 hours agorootparentSee, this is why I hate jazz fans. Yes, there are many people out there who love music but don't appreciate jazz. That's okay. reply InCityDreams 16 minutes agoprev\"Jazz isn't dead, it just smells funny\" reply johnea 9 hours agoprevEpic album and article! It's great to see this on HN. It gives me a little more hope for the world... reply mp05 8 hours agoprevWhat an album! Aside from the hubris of asserting something so bold in the first place, it can not too controversial of a take to state that \"Blue in Green\" is the finest musical recording ever produced. If I had one, single song to take with me to a desert island, this is the one. The bowed bass at the end gives me chills every time. reply vjulian 6 hours agoparentOh—it’s not only me! I just commented on Blue in Green. Outside of that, the album overall has an association for me. I started buying jazz records in my teens in the 80s. No one guided my listening or made recommendations. I eventually came upon Kind of Blue and felt it was special. I had no idea how famous and well-loved it was. I find it interesting that even in a “blind taste test”, I converged with the critical massess. reply heleninboodler 8 hours agoparentprevI think there are several other tracks on the album tied for this same distinction. :) reply csmpltn 2 hours agoprev [–] I’m classically trained, love all sorts of music, play live, jam and compose. I’ve went to countless jazz concerts and nights, and continue to do so today. I just don’t get jazz at all. It’s fun to mess around on stage, and play free form. But it also ends up sounding that way to the listener: like some mess that must be fun to play… I was stuck on a long drive a couple of years back, driver had Swissjazz FM on the whole time. Felt like a form of torture ;) reply donbrae 32 minutes agoparentInteresting, as most jazz is not freeform and typically follows an agreed chord sequence. Certainly, jazz improvisation tends to feature lots of notes, often played at fast tempos, but ‘messing around’ it definitely isn’t. Jazz improv requires years of concerted practice and deep thinking. If you analyse a great jazz solo, it becomes clear that it isn’t random notes, or a ‘mess’. To me it’s astonishing that anyone can create something so intricately structured on the fly. Of course, you’re free not to like it. reply zabzonk 1 hour agoparentprevhave you actually listened to \"kind of blue\"? or (for e.g.) \"sketches of spain\"? i can't see how either could be classified as \"some mess\", though everyone's taste is different. reply GoofballJones 1 hour agoparentprevI like some of it, but I agree with you on most of jazz. Kind of Blue is great though. It really is something special. But as I say, I agree with you on most other Jazz though. And I'm convinced that about 80% of the people that go around saying they love jazz are just pretending to. Also, along with Bluegrass, jazz is SO closed off to people who'd like to join in and play with a group. It's filled with gatekeepers and judgemental jerks. For instance, criticizing you for bringing a guita they don't agree with, \"you can't play jazz with that, go home and get another something else there, Eddie Van Halen\". Seriously, it can be that bad. reply tommysve 1 hour agoparentprev [–] There are so many different kinds of jazz. I guess you have not listened to Kind of Blue. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the legendary jazz album \"Kind of Blue\" recorded in 1959 by Miles Davis, John Coltrane, and Bill Evans, with a specific focus on the track \"So What.\"",
      "It delves into the cultural and social background of the time, showcasing the unique playing styles of the musicians and their innovative approach.",
      "Additionally, it briefly mentions the structure of popular songs in the twentieth century and the forthcoming book by James Kaplan."
    ],
    "commentSummary": [
      "The focus is on Miles Davis' album \"Kind of Blue,\" especially the track \"Blue in Green,\" discussing its impact and brilliance in the jazz world.",
      "Participants share personal connections to jazz, suggest favorite albums, venues, and musicians, along with insights into the structure of jazz and Miles Davis' influence on the genre.",
      "Mixed feelings are expressed about the perceived exclusivity of the jazz community, with some frustrated while others show deep love and appreciation for jazz music."
    ],
    "points": 173,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1709594567
  },
  {
    "id": 39591694,
    "title": "Rust for Embedded Systems: Benefits, Challenges, Solutions",
    "originLink": "https://arxiv.org/abs/2311.05063",
    "originBody": "Computer Science > Cryptography and Security arXiv:2311.05063 (cs) [Submitted on 8 Nov 2023] Title:Rust for Embedded Systems: Current State, Challenges and Open Problems Authors:Ayushi Sharma, Shashank Sharma, Santiago Torres-Arias, Aravind Machiry Download PDF Abstract:Embedded software is used in safety-critical systems such as medical devices and autonomous vehicles, where software defects, including security vulnerabilities, have severe consequences. Most embedded codebases are developed in unsafe languages, specifically C/C++, and are riddled with memory safety vulnerabilities. To prevent such vulnerabilities, RUST, a performant memory-safe systems language, provides an optimal choice for developing embedded software. RUST interoperability enables developing RUST applications on top of existing C codebases. Despite this, even the most resourceful organizations continue to develop embedded software in C/C++. This paper performs the first systematic study to holistically understand the current state and challenges of using RUST for embedded systems. Our study is organized across three research questions. We collected a dataset of 2,836 RUST embedded software spanning various categories and 5 Static Application Security Testing ( SAST) tools. We performed a systematic analysis of our dataset and surveys with 225 developers to investigate our research questions. We found that existing RUST software support is inadequate, SAST tools cannot handle certain features of RUST embedded software, resulting in failures, and the prevalence of advanced types in existing RUST software makes it challenging to engineer interoperable code. In addition, we found various challenges faced by developers in using RUST for embedded systems development. Subjects: Cryptography and Security (cs.CR); Software Engineering (cs.SE) Cite as: arXiv:2311.05063 [cs.CR](or arXiv:2311.05063v1 [cs.CR] for this version)https://doi.org/10.48550/arXiv.2311.05063 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Ayushi Sharma [view email] [v1] Wed, 8 Nov 2023 23:59:32 UTC (429 KB) Full-text links: Access Paper: Download PDF TeX Source Other Formats view license Current browse context: cs.CRnewrecent2311 Change to browse by: cs cs.SE References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39591694",
    "commentBody": "Rust for Embedded Systems: Current state, challenges and open problems (arxiv.org)171 points by Sindisil 18 hours agohidepastfavorite119 comments AlotOfReading 17 hours agoThese are all real issues with Rust, though it's worth noting that many of the integration challenges mentioned also apply to external code written in C and C++. Some of the survey responses highlight one of the biggest hurdles to rust adoption I've experienced though: Rust has an education problem. People dramatically overestimate the correctness of their [C/C++] code and underestimate the potential severity of failures to meet that expectation. I've found very few projects where a bit of poking can't turn up memory safety issues. Even people who are far more skilled programmers than I am routinely write silly mistakes. Unfortunately the response I often hear to bringing these issues up after the code exists is \"it's working code, why should I care about these theoretical issues?\" reply marcosdumay 15 hours agoparentThe C and C++ ecosystems are suffering from a selection effect, in that the people that have any concern about their code being correct are trying to drop them, while the people comfortable with the language are all the ones that don't know about the problems. I expect the careless unaware culture to get worse with time as more and more aware people manage to leave it. reply logicchains 15 hours agorootparent>the people comfortable with the language are all the ones that don't know about the problems. Or the ones writing code where security doesn't matter, like HFT and video games (the former have no users, and the latter are basically impossible to make crack-resistant even if they're written entirely in Rust). reply jerf 13 hours agorootparentOthers have corrected you about gaming; I would point out that for HFT if you write a memory error, you are reasonably likely to encounter it because of the sheer number of swings of the stick you get at hitting the bug. And if that code's concurrent at all, and it probably is, it had even moreso better get it right. You do not want to discover memory errors at run time. I'd bet HFT has already got all the best static analysis money can buy, not that they're cowboying it more than anybody else. Memory safety isn't just about \"security\". Sure, that's a huge issue, but it's also about not spending a month debugging why the login fails one out of a thousand logins because the user's password gets replaced by what seems to be a pointer, but one that doesn't point at anything, starting with the 5th character, or why your HFT code should have ordered 10,000 shares but instead ordered a pointer's worth of shares, or why the second level crashes after precisely the 1023rd frame with completely incomprehensible errors, or any number of other things. I do not know why any sane developer that has worked through such a thing even once would be willing to entertain the notion of programming without it if it weren't utterly forced on them. I enjoy reading such tales of diagnostic heroism; I don't enjoy being in them anywhere near as much as some programmers apparently do. reply almatabata 14 hours agorootparentprev> Or the ones writing code where security doesn't matter, like HFT and video games A lot of games have a multiplayer mode nowadays, that opens it up for exploits. I do not want to get my PC hacked because I connected to a game server. If you mean only single player offline games, I guess yes security is less of an issue. reply amlozano 14 hours agorootparentprevComplicated video games, especially ones with transactions or multi-player aspects, require a lot more security code than you might expect. reply bigyikes 14 hours agorootparentIs video game security an entire subfield of its own? I imagine there are categories of exploits in video games which simply don’t exist in other areas of software reply steveklabnik 13 hours agorootparentThe latest in FPS cheating (that I'm aware of, not like I'm super plugged into the underground) involves buying a second PC to run the cheats, a card for your main PC to grab a copy of memory over DMA, ship it off to the second PC, then joining the two video feeds together. Apparently you can also hook your mouse up to a connection where it will edit the data flowing from the mouse to give you better aim as well. A lot of it is the same as any other sort of security stuff, but like, the tough part is that the adversary has access to the physical machine. In my understanding anyway, not a security expert. reply von_lohengramm 12 hours agorootparentYou would only ever do all that if you had no clue what you are doing (and most don't). It's rather trivial to bypass modern kernel anticheats, especially with a hacked up KVM or custom hypervisor. So much of anti-cheat nowadays is based off of automated (delayed) detection and user reports that you can easily ragehack to the the top ranks without ban if you just use silentaim (aimbot that doesn't need to change your view angles) and common sense. In fact, I'd go so far as to say that a majority of players in the top ranks of most popular FPSs are cheaters or queue with cheaters. There's nothing quite like watching the enemy stare right at you through every wall across the map and then carelessly run in a straight line towards you. Even more concerning is how many streamers queue up with blatant cheaters and then use their connections with game moderators to manually ban other cheaters. reply rcxdude 11 hours agorootparentprevSecurity != DRM, i.e. 'anti-cheat'. Security is more about 'a malicious player can't RCE other players' than \"cheaters can't access data which is sent to their PC anyway\". One is a lot more tractable than the other. reply kaoD 14 hours agorootparentprevYes. Basically cheats and anti-cheats became their own thing security-wise and huge amounts of effort are spent (by both sides) on this cat and mouse game. Although both cracking (as in \"software cracking\") and cheats were very similar security fields back in the day (both boiled down to reverse engineering) cheating has diverged enough (due to modern anti-cheats and anti-piracy becoming very different countermeasures) that I'd consider them vastly different nowadays. Some categories of exploits unique to games: - Aim hacking (pointing the mouse cursor to enemy heads) - Recoil/spread cheats (mouse compensating for weapon recoil or bullet spread) - Botting/botfarming (playing resource-intensive games automatically) - Wallhacking (showing players through walls, or making walls semi transparent) - Miscellaneous passive assistance (like overlaying a predicted path for a ball in a game, drawing precise location for footsteps/other player sounds, etc.) - Modification of game state (like sending your character's position at will, allowing you to fly or go through walls) Some of these have analogues in app security: - Botting detection is probably very similar in both MMOs and CloudFlare) - Hidden information is not very different from what we do in web apps (only send the state that the client needs and has access to) - Preventing game state modification is solved by having a strongly authoritative server (again pretty normal in app land) But even in those, what makes games very unique compared to other apps is that they're hard real time: you have hard requirements for each frame time; it's expensive to calculate in-server all things that a player should be able to see every single frame; packets take time to travel over the internet so you have to give it some slack (or enemies could pop out of nowhere when crossing doors); client sometimes needs information that you'd like to remain private (you need player positions even behind walls to provide 3D sounds)... Modern anticheats usually resort to just scanning the memory for running cheats, trying to detect a cheat reading/modifying the game memory, etc. but cheats have come to great lengths like having actual cheat hardware (DMA devices on PCIe[0]) that cannot be detected since it's running outside of the computer software. [0] https://blog.esea.net/esea-hardware-cheats/ reply marcosdumay 15 hours agorootparentprevIt's not only security, it's correctness. It's about your software staying running and doing the right thing. I can believe HFT developers think they don't need it. reply infamouscow 10 hours agorootparentFirst, 100% of HFT are businesses trying to make money above all else. Security and correctness are secondary to the business requirement of making money, especially when you are uniquely situated to have financially quantified their risk when things go wrong to remain within controlled error bands. Second, HFT developers are using very expensive FGPAs to do trading with network packets. People are quick to reach to some example where their favorite toy is used at some organization, but that's besides the point. The tools for getting code on those FPGAs is horrible and unless Xilinx adopts Rust internally I doubt much will change. Rust can be adopted elsewhere, but that's not what's making the organization money. HFT folks are not persuaded even slightly by these arguments about security and correctness. Jane Street is not a HFT firm and they will gladly tell you that. It helps immensely to understand ones concerns, constraints, and values before assuming they're identical and them swiftly demonizing anyone that dissents. The Rust community really has done itself a massive disservice in alienating everyone that doesn't conform to its monoculture composed mostly of amateurs and unemployable extremists. Of course, this will simply be downvoted by the pathetic and unimpressive. Nobody views this as an opportunity to even try disputing the argument or use it as a legitimate data point that Rust should strive for. They simply want obedience and mindlessness in the monoculture because they lack the tools needed to handle conflict of any kind. It's also why so many features are perpetually stuck as nightly rather than stable. The project does nothing but stultify talent into leaving for genuine innovation. reply throw10920 9 hours agorootparent> It helps immensely to understand ones concerns, constraints, and values before assuming they're identical and them swiftly demonizing anyone that dissents. The Rust community really has done itself a massive disservice in alienating everyone that doesn't conform to its monoculture composed mostly of amateurs and unemployable extremists. I think that it's a little unlikely that the Rust community is mostly composed of \"amateurs and unemployable extremists\" - a lot of the working software engineers I know are interested in or actively use it. However, \"assuming they're identical and them swiftly demonizing anyone that dissents\" is an excellent characterization of, at least, the parts of the Rust community that I see on HN and Reddit. reply jcelerier 16 hours agoparentprev> People dramatically overestimate the correctness of their [C/C++] code and underestimate the potential severity of failures to meet that expectation Really depends on the field. I heard more than once in my life now that it's better to reboot every night than spend even an afternoon of engineering trying to fix memory leaks. reply outworlder 15 hours agorootparentThat's good when it's just about memory leaks. Your missile guidance code can have leaks as long as the missile completes the mission, since it will all be... \"garbage collected\" anyway. It's not great when you have problems like use after free and pointers going to where they don't belong. Those can cause security vulnerabilities that rebooting won't fix. reply hajile 15 hours agorootparentThen they make a version that flies farther and it sometimes randomly fails to detonate. reply toast0 14 hours agorootparentSo you write a note mentioning there's a leak, and debug it if there's a need for it. Finding a bug doesn't mean you need to fix it, or fix it right away. Using Rust doesn't preclude resource leaks. I'm pretty sure I've managed to run into resource leaks in all of the languages I've used in production, doesn't matter if they were managed or not. Sometimes limiting the lifetime of the thing that holds resources, and let process (or system) death clean it up works great; sometimes that's terrible, depends on the cost of death/restart, the lifetime between deaths, human intervention required, accuracy of lifetime estimation method, probably some other things. I don't worry too much about leaks that require a restart every year or so; nor would I worry about a leak in a missile that will cause issues beyond the achievable maximum flight time. reply kevin_thibedeau 12 hours agorootparentprevMongoDB's BSON library calls abort() on malloc failure because they can't be bothered to handle OOM gracefully. reply throwaway234890 11 hours agorootparentLinux supports overcommitted memory and so will lie to your application about memory availability. When I tested several common open-source applications in a memory-constrained environment none of them, except for the Java VM, handled OOM conditions gracefully. I came to the conclusion it's not worth the programming effort unless your application will specifically expect this condition to occur. reply infamouscow 11 hours agorootparentI work on systems that don't run on Linux, and those systems also have reasonable business requirements for graceful failure on OOM. This is unhelpful. reply tialaramex 12 hours agorootparentprevThat seems completely reasonable. The only interesting case is \"There was far too little storage, so we gave up early\" e.g. you need 14GB, there is 186MB available, give up. I probably don't want my photo viewer to crash because I tried to open a JPEG that wouldn't fit in RAM, better it just says that's too big and I can pick a different file. Whenever people are imagining they're going to recover despite OOM in low level components they are tripping. Their recovery strategy is going to blow up too, and then they're going to be in a huge mess - just abort when this happens. reply jeff-davis 11 hours agorootparentPostgres is designed to recover on OOM in most cases. reply tialaramex 10 hours agorootparentThat's nice, how well does it work in practice? reply jeff-davis 9 hours agorootparentIt works. Bugs have been found, and some more bugs probably exist, but I think it meets a fairly high bar of quality. reply rockdoe 10 hours agorootparentprevFirefox and Chrome (and thus Edge, Brave, OperaGX, etc etc) do the same for many allocations - it's safer to crash than to end up in an obscure failure path that never had its error handling exercised and may accidentally be security sensitive. reply duped 9 hours agorootparentprevWhat else would you expect them to do? reply palata 13 hours agoparentprev> People dramatically overestimate the correctness of their [C/C++] code Genuinely interested: what do people think of their tons of third-party dependencies in Rust? My experience building Rust projects is that there are orders of magnitudes more dependencies than what would make me comfortable. At least in C/C++ it's much harder to get there. reply kibwen 13 hours agorootparentI use embedded Rust for my day job. We keep our third-party dependencies purposefully minimal, but I adore that they're there when I need to reach for them. Like, we just needed a basic TCP stack to get our PoC up and running, so I can just grab smoltcp and be off to the races. Being written in C or C++ wouldn't obviate our need for a TCP stack, I'd still either need to get it from somewhere else or write it myself. And even if I can't use a third-party dependency for whatever reason (e.g. if it doesn't support no_std mode), I can still usually repurpose their existing test suite to give me confidence in my port or reimplementation. reply troad 7 hours agorootparentprevI'm somewhat puzzled - the obvious answer is that you don't have to use them. Rust by itself is no less feature complete than C. Anything you can write in C without dependencies, you can write in Rust without dependencies too. Of course, dependency-reliance falls on a spectrum for any individual programmer. On the one hand you have script kiddies with 400 npm dependencies gathered from arcane Discord channels - on the other you have old bearded wizards who don't understand why you'd need printf() when you can just mov rax 1; syscall. I find Rust is not strongly opinionated about this. You can just as easily eschew dependencies as use them. There's usually a good array of options available, or you could always FFI to a C lib directly if you need to. reply rockdoe 10 hours agorootparentprevIn C/C++ every project ends up with a hand coded replacement for those external dependencies that is less well written and infinitely less tested. reply infamouscow 9 hours agorootparentWhat about all of the projects where your assumptions are false? reply iknowstuff 12 hours agorootparentprevLong story short is that I trust their quality. The community is great about vetting unnecessary uses of unsafe code. That’s more than can be said about hand rolled equivalents in C/C++. reply palata 11 hours agorootparentSo if you have 200 dependencies, you trust them just because they don't run what Rust calls \"unsafe\" code? What about safe code that could be malware? reply rendaw 4 hours agorootparentIsn't this hierarchical trust? You select immediate dependencies that you believe are written by reasonably security-conscious people, not just anything that immediately solves a problem, and because they're reasonably security-conscious people they select their own dependencies the same way? And at each level vetting is implicitly shared with other users of the dependencies, some of whom will be more critical/inquisitive. I've heard the alarms about dependencies and I'm not sold. I feel like this is bleeding over from the JS/frontend world where people don't choose to do the above, for whatever reason. Whenever I add a dependency in Rust I look at crates.io downloads, dependents (any big projects there?), github stars, I browse the issues to see what sort of problems have been reported and when, with what sort of replies from the author, how many contributors there are, release history, what commits look like, and what other stuff the authors have worked on. I use a lot of dependencies, and I do rewrite stuff myself when I feel like I can't rely on a dependency. reply rockdoe 10 hours agorootparentprevFYI Google and Mozilla audit all their dependencies and share them: * https://chromium.googlesource.com/chromiumos/third_party/rus... * https://searchfox.org/mozilla-central/source/supply-chain/au... It's quite likely that most of your dependencies were already audited. reply meragrin_ 9 hours agorootparentSo what in there guarantees I can get the same thing they audited? reply GrumpySloth 7 hours agorootparentVersion numbers. You can’t modify an already-published version of a Rust crate on crates.io. reply otabdeveloper4 3 hours agoparentprevMemory safety issues don't matter. There are five orders of magnitude more pwned boxes due to misquoted strings then there are due to memory safety issues. You are searching for a problem that could potentially make Rust useful instead of solving a real existing problem that people actually have. reply dboreham 16 hours agoparentprevI had to read this a couple of times, but I suspect this is about non-rust code, when you cite correctness of code, yes? People don't want to use rust because they think their janky C/C++ is just fine. reply adameasterling 17 hours agoparentprev> I've found very few projects where a bit of poking can't turn up memory safety issues. I'm working on a Rust project right now, and I'm probably one of those people who are overestimating the correctness of my code! I would love to know about what sorts of memory safety issues you often uncover. reply AlotOfReading 16 hours agorootparentMade it more clear in the original post that I was talking about the correctness of C and C++ code. I haven't observed any notable issues with this in Rust compared to similar languages, but I also don't have the same depth of experience building large systems in it to show me the error of my ways yet. reply nindalf 17 hours agoprevThe overall argument that Rust's use in embedded has a long way to go is fairly accurate. A foundational crate like embedded-hal reached 1.0 only 2 months ago, 8 years after Rust 1.0. On the other hand, this crate reaching 1.0 means the rest of the ecosystem can now mature alongside it. The paper could use a few minor corrections and improvements. For example, they go through thousands of crates in crates.io to give the impression of completeness, but it would have been better to go though a more curated source like https://github.com/rust-embedded/awesome-embedded-rust. It doesn't matter if the quality of random one off crates that someone hacked over a weekend is poor, it does matter if the crates recommended by the community are poor. But I suppose their analysis would look impressive then. The other nit is that they think that the existence of even one instance of unsafe is a problem. This is a mistake that people unfamiliar with Rust make. reply Kototama 17 hours agoparentYou mean embedded-hal, not embedded-hat :-) reply nindalf 16 hours agorootparentHaha thanks for the correction. Autocorrect got me. reply junon 41 minutes agoprevIf you're using Stm32s or Nordiks (among a few others) then I cannot recommend enough the Embassy framework. I used it for a custom board I made and it not only consumed less power overall due to the automatic chip sleep state handling but has been drastically easier to work with than the provided C firmware. Developer is also super responsive on Matrix and a nice guy. https://GitHub.com/embassy-rs reply atoav 16 hours agoprevAs a rust programmer that programs C++ on embedded, the main thing preventing adoption is the ecosystem. So let's say I happen to work with a MCU that is well supported in Rust, what if I want to connect it to a popular OLED display? Is there a library for that? If so, does it work? If it works, does it have the needed features? Now maybe I am incredibly lucky and all of that works, what about a popular gyro IC? Granted, there is probably some way to interface the Rust code with C code, but is that gonna work without turning it into a day of research? I will certainly check back on the state of Rust on embedded every now and then, but as of now C++ is my goto. reply jdiez17 15 hours agoparentFrom my experience in the embedded world, the ecosystem doesn't seem super relevant. You may depend on certain libraries for encryption, compression, etc. but you're more likely than not to be writing drivers for all of your devices more or less from scratch. Other than Arduino, which of course has pretty good support for many devices and is likely to work out of the box. reply atoav 12 hours agorootparentIt probably depends on what you do with embedded, I can certainly imagine situations/jobs where you are correct, but I am not in that situation, unfortunately. I can't afford the time to write my own drivers unless I really need/want to. I run an electronics workshop at an artschool, so the end result counts and how we get there is more or less arbitrary unless it happens in a finite timeframe. For my own private projects I tried Rust, but also there my goal is to get things done, not to fool around forever. I like fooling around with the interesting bits, not with rewriting a thing others have written before. reply llm_trw 11 hours agorootparent>I like fooling around with the interesting bits, not with rewriting a thing others have written before. For some reason the rust community seems to love rewriting things others have done before and not do much new work. At this point the biggest thing holding the language back are its users. reply the__alchemist 15 hours agoparentprevYou will probably be on your own unfortunately regarding hardware support. Even when libraries exist, they are often not worth the fork/PR/modification process to accommodate your use case. I hope you like datasheets... This sounds rough, but I will highlight two upsides: #1: You benefit from the nice tooling and language of rust #2: You are forced to learn how to use peripherals at a lower level than would otherwise be required. You will become adept at reading datasheets, and interacting with hardware, because you will write to registers directly while building your own abstractions. The easy choice is to use C or C++. reply atoav 11 hours agorootparentYou make it sound worth a try. I will keep the thought in mind, thanks. reply MSFT_Edging 15 hours agoparentprev> connect it to a popular OLED display? At least for the ssd1306 and the sh1106 there is in fact a library for it that works well. I recently started working on a rust-esp32 project because I'm far too lazy to properly deal with json in C. It was basic, some buttons, a display, an encoder, and a web api but it was a breeze. I'd really like to continue on more complex projects in the future. I've somehow avoided writing C++ in an embedded setting and now I'm afraid I might have to go back and learn that rather than continue with Rust. reply yeeeloit 12 hours agorootparentWhy do you think you might have to go back and to C++? reply adrianN 15 hours agoparentprevInterfacing C with Rust is actually a well supported use case. reply bsder 15 hours agorootparentNot really. The semantics as you cross the boundary are ill-defined. And you will pay dearly if the C library wants to own any of the resources (like an event loop). And \"unsafe\" programming in Rust is really difficult to get right--possibly harder than C. To be fair, these problems are not inherent to Rust--any language which tries to interface with C will have them. reply adrianN 14 hours agorootparent>To be fair, these problems are not inherent to Rust--any language which tries to interface with C will have them. Yes, including C. reply palata 13 hours agorootparentI guess in that case, at least it's done by a C developer... reply bsder 11 hours agorootparentprevIncorrect. Two C libraries interface just fine if library 1 handles ownership and library 2 (or your program) simply operates on the resulting things being passed around. Rust, on the other hand, will go bananas and your life will be miserable. See: http://way-cooler.org/blog/2019/04/29/rewriting-way-cooler-i... reply tialaramex 9 hours agorootparentThat 2019 blog post explains how, having made a whole series of mistakes, they're now going to rewrite everything in C and it'll go much better. The next blog post says they've abandoned the project instead. To me, it seems like they'd lost motivation years previously and were increasingly flailing around for something that would spark the same joy as initial work on an exciting new project, and Rust has little to do with that. Lots of crates exist which wrap popular C libraries, and they don't seem to run into the sort of misery you describe. reply adrianN 3 hours agorootparentprevSo the author tried very hard to provide a safe interface for things that are inherently unsafe and found it to be too much work. That’s similar but different to just calling C from Rust and living with unsafe code like you normally do in C. reply worik 15 hours agoparentprev> Granted, there is probably some way to interface the Rust code with C code, but is that gonna work without turning it into a day of research? I had a sense this a priority for the Rust Central Committee, so I had a quick look https://docs.rust-embedded.org/book/interoperability/c-with-... I have not had to use it yet, looking for a reason. reply xormapmap 13 hours agoparentprev> Is there a library for that? What? Are there really people out there relying on libraries for every peripheral? I've never seen anyone use a third party library for peripherals like that, short of some prototyping on an Arduino or the like. It's always just drivers implemented from scratch. reply atoav 12 hours agorootparentJup. This is my field of work. I am ot a full time embedded developer, I run an electronics workshop at an artschool. So a lot of arduino-ish things, and aometimes more sophisticated stuff. But I ain't got no time to write drivers. reply tamimio 16 hours agoparentprevSpot on! reply znpy 15 hours agoparentprev> As a rust programmer that programs C++ on embedded, the main thing preventing adoption is the ecosystem. Maybe the mindset of the industry as well? Not sure if things have changed (and if so, how) but 9-10 years ago when i was in university I had an interview with a company that did embedded systems stuff. While talking about my competencies I mentioned I was able to create cross-toolchain if they were interested and the guy (he was kinda like the CTO iirc) abruptly interrupted me and just said: \"no. just no. we always and only use the vendor's BSP (board support package). we don't care about anything else. should there be any kind of issue we want to be able to ring them and get them to fix the issue.\". How do you get people (or an industry?) with such a mindset to just use something because it's trendy? My guess is that industry will wait another 10-15 years until some vendor big enough ships a rust toolchain as a BSP. Other vendors will follow. Then Rust on embedded systems will flourish. reply navaati 12 hours agorootparentWell lucky us, Espressif is officially providing Rust packages for the ESP32 chips :) ! reply rockdoe 10 hours agorootparentprevAh yes, then the vendor goes out of business and they have to bring in highly paid consultants to fix bugs in the vendors' gcc 2.95/Linux 2.6.x port to their SoC. You need to be in the right kind of company - the one where waiting for the external vendor to fix their shit is too slow. reply kkert 7 hours agoparentprevHonestly i have a far easier time finding working no-std libraries for anything i need on embedded Rust compared to embedded C++. Arduino ecosystem is an exception, but that's not necessarily something i'd put in production reply tamimio 16 hours agoprevPersonally in the past ~2 years I have been trying to shift my code base to rust when it comes to robotics and drones software, the biggest issue is the integration part, most “addons” that you can integrate with the robots like Lidar and other sensors come with the usual SDKs in C/C++ or even python. Additionally, most of X-rust converters don’t really work so you end up rewriting it from scratch. reply fuzztester 12 hours agoparent>that you can integrate with the robots like Lidar and other sensors come with the usual SDKs in C/C++ or even python. Do the Python SDKs run fast enough? Genuine question. I've done a good amount of Python, but don't have any idea of how suitable it is for embedded stuff, other than knowing that MicroPython exists. reply xyst 14 hours agoparentprevWhat’s wrong with wrapping the C headers in rust? Seems possible to me, although a bit labor intensive depending on the C/C++ lib — https://docs.rust-embedded.org/book/interoperability/c-with-... reply tamimio 13 hours agorootparentNothing is wrong with that, it’s rather a workaround, ultimately I am trying to have one language only including the UI too (been playing with egui),so I don’t have to use JavaScript. https://github.com/emilk/egui reply the__alchemist 15 hours agoparentprevWhat sorts of parts? Most should have register-level APIs in the datasheet. That doesn't mean integrating is easy though, compared to the SDK. reply svnt 14 hours agorootparentBeginning by using the register listings in the datasheet is what parent meant by “from scratch.” Anything more would have been reverse engineering. reply tamimio 13 hours agorootparentprev> What sorts of parts? A lot of parts I had to work with didn’t have it, last one a couple months ago for example was a guided parachute for a drone dropper, I ended up making the driver from scratch that interfaced with the serial io. reply fusslo 17 hours agoprevmy takeaways: 1. https://arewertosyet.com/ to track rust RTOSes and their status 2. there are tools to convert c to rust (I dont know if I'd trust this..) 3. \"Out of 43 different MCU families, peripheral crates are currently available for only 16 (37%). Most of these crates are generated using svd2rust utility\" 4. developers considered but rejected rust because: \"Lack of Support for MCUs (36%) ; Difficulty Integrating with Existing codebase (32%) ; Organization constraints and certification requirements (30%)\" 5. \"The second major (26%) issue is debugging, which is expected because, as explained in Section 2, embedded systems follow an asynchronous and event-driven design. This results in frequent cross-language domain interactions and makes debugging hard.\" I would adopt rust if it were easy to get up and running just while(1) loop applications. reply jcranmer 17 hours agoparent> 2. there are tools to convert c to rust (I dont know if I'd trust this..) The core C specification by itself isn't all that complicated of a language; a C-to-Rust transpiler is a pretty doable project. The main issues here are that a) a lot of the code you'd likely want to convert is likely to be reliant on non-standard extensions b) there's a lot of undefined behavior which you probably want to have somewhat more defined behavior on, especially in embedded contexts c) the real goal for a lot of this automated conversion is to do the conversion once and work well enough that you don't have to audit the result of the conversion, and because of especially the previous point, it's really hard to get that level of trust for C code. The existing c2rust converter works by creating the clang AST and then lowering that to Rust source code, which I'm not sure is a path that would lead me to high confidence in the converted code due to the potential impedance mismatch in understanding the clang AST. A custom C frontend is probably a better match here for a long term project (C, unlike C++, is feasible to build your own compiler from scratch), or maybe another project idea is to convert LLVM IR to Rust and ditch the C frontend entirely. reply Animats 14 hours agorootparent> C-to-Rust transpiler is a pretty doable project. It's been done, but what comes out is terrible Rust. Everything is unsafe types with C semantics. An intelligent C to Rust translator would be a big win. You'd need to annotate the input C with info about how long arrays are and such, to guide the translator. It might be possible to use an LLM to analyze the code and provide annotations. Usually, C code does have array length info; it's just not in a form that the language ties to the array itself. If you see char* buf = malloc(len); the programmer knows that \"buf\" has length \"len\", but the programmer does not. Something needs to annotate \"buf\" with that info so that the translator knows it. Then the translator can generate Rust: let mut buf = vec![0;len]; The payoff comes at calls. C code: int write_to_device(char* buf, size_t len) is a common idiom. LLMs are good at idioms. At this point, one can guess that this is equivalent to fn write_to_device(buf: &[u8]) -> i32 in Rust. Then the translator has to track \"len\" to make sure that assert_eq!(buf.len(), len); is either provably true, or put in that assert to check it at run time. So that's a path to translation into safe Rust. Funding could probably be obtained from Homeland Security for this, given the new White House level interest in safe languages and the headaches being caused by the cyber war. Is CVS still down? reply jcranmer 14 hours agorootparent> It's been done, but what comes out is terrible Rust. Everything is unsafe types with C semantics. The idea behind the current c2rust tool is that you'd do a one-shot conversion to Rust and then gradually do refactoring passes over the barely-Rust code to convert it to correct C code. The focus is on preserving semantics of C over writing anything close to idiomatic (cue a + b being translated to a.wrapping_add(b) all the time, e.g.). Which is an approach, but I'm not sure it ends up providing any value over \"set your system to compile both C and Rust into a final image and then slowly move stuff from the C to the Rust side as appropriate\" in practice. > Usually, C code does have array length info; it's just not in a form that the language ties to the array itself. This is actually why C23 made VLA support semi-mandatory: it enables you to describe a function signature as int write_to_device(size_t len, char buf[len]) and C23 compilers are required to support that, even in absence of full VLA support! The intent of making this support mandatory was to be able to use that as a basis for adding better bounds-checking support to the language and compilers. (Although, as you noticed, there is an order-of-declarations issue compared to the typical idiomatic expression of such APIs in C, and the committee has yet to find a solution to that). reply Animats 13 hours agorootparent> The idea behind the current c2rust tool is that you'd do a one-shot conversion to Rust and then gradually do refactoring passes over the barely-Rust code to convert it to correct C code. I've seen what comes out of the transpiler. Nobody should touch that code by hand. It's awful Rust, and uglier than the original C. Modifying that by hand is like modifying compiler-generated machine code. > This is actually why C23 made VLA support semi-mandatory. C23 doesn't actually use that info. You can't get the size of buf from buf. I proposed something like that 12 years ago.[1] But I wanted to add enough features to check it. [1] http://animats.com/papers/languages/safearraysforc43.pdf reply jcranmer 13 hours agorootparent> I've seen what comes out of the transpiler. Nobody should touch that code by hand. It's awful Rust, and uglier than the original C. I can't disagree here. I think the original idea was to rely on automated refactoring tools to try to make the generated Rust somewhat more palatable, but I never was able to get that working. > C23 doesn't actually use that info. True; the intent is to require it so that it can be leveraged by future extensions. The C committee tends to move glacially. reply Animats 12 hours agorootparentThe real problem is not translating code. It's translating data types. If you can determine that a \"char *\" in C can be a Vec in Rust, you're most of the way there. It's no longer ambiguous what to do with the accesses. This is where I think LLMs could help. Ask an LLM \"In this code, could variable \"buf\" be safely represented as a Rust \"Vec\", and if so, what is its length?. LLMs don't really know the languages, but they have access to many samples, which is probably good enough to get a correct guess most of the time. That's enough to provide annotation hints to a dumb translator. The problem here is translating C idioms to Rust idioms, which is an LLM kind of problem. reply galangalalgol 11 hours agorootparentThey have made some improvements here recently. There is a lot less unsafe generated. The rest is more idiomatic too. The cost is that it will be throwing panics everywhere until you fix the faulty assumptions it asserted. I like the new way better. reply jandrese 14 hours agorootparentprevThe big problem is just the fundamental mismatch between what Rust requires and what you can do in C, especially with embedded code. If the library in question handles interrupts by jumping to an interrupt handler that updates some shared state you're going to have a bad time converting that into safe Rust. reply Aurornis 16 hours agoparentprev> 3. \"Out of 43 different MCU families, peripheral crates are currently available for only 16 (37%). There are a lot of obscure MCU families out there. Most engineers or shops specialize in a couple, become familiar with those, and stick to it. Using and learning a brand new MCU family is a lot of work. As long as I can find Rust support for common MCUs that I use, I don’t care how broadly the rest of the market is covered. reply sitzkrieg 16 hours agorootparenthope that mcu never goes on backorder :-) reply petsfed 14 hours agorootparentIt would need to be an historic backorder to make me switch from e.g. STM32 to like a C2000. Or honestly from an STM32F to STM32H. For those how don't know, its very rare for a production device to use a socket for its MCU. Most MCUs don't even come in packages that support sockets. They're always soldered in. So unless you're switching to another MCU whose pinout, external clock, and power supply requirements are close enough that the hardware change is really just a BOM change, switching MCUs in case of a shortage is not a realistic option. Reportedly, some manufacturers were buying entire washing machines during the height of the chip shortage, just to de-solder the machines' MCUs and use them in their own products. That that is the better option should tell you how painful changing MCUs can be. And that's to say nothing about porting the firmware, which may, or may not, be trivial. reply rcxdude 16 hours agoparentprevIn general I get the impression that embedded rust is fairly good for while(1) loop applications: the kind of thing you can do with arduino is also usually fairly easy to do in rust, modulo maybe not so good library support for random bits of hardware. What I generally see lacking is support for multitasking: the various HALs generally only support synchronous, usually only busy-loop blocking implementations, which is really limiting for a lot of embedded applications. This kind of thing is hard to get right, though. (in fact, while I like the theory of generic embedded HALs, I have yet to see a good implementation of the concept. Most effective HALs I have seen are specialised to one area or another, usually to one particular application) reply dralley 17 hours agoparentprev>I would adopt rust if it were easy to get up and running just while(1) loop applications. Doesn't embassy_rs make that pretty easy? reply fusslo 17 hours agorootparenti found out about embassy via the link in #1, looking at it now reading the readme gets me pretty excited tho reply NoboruWataya 14 hours agoparentprev> 1. https://arewertosyet.com/ to track rust RTOSes and their status As an aside, does anyone know if there is a central directory of these Rust \"areweXyet\" websites? I didn't know of this one but I knew about https://areweideyet.com/ and https://areweguiyet.com/ reply the__alchemist 17 hours agoparentprev> I would adopt rust if it were easy to get up and running just while(1) loop applications. It is: Install the toolchain (eg `rustup target add thumbv7hibf`); install probe-rs; `cargo run`. I think getting applications up and running in embedded rust is one of its strengths. reply DriftRegion 2 hours agoprevI think when bringing up embedded rust it is necessary to specify an application. For low level, hard realtime control and interrupt handling rust gets in the way. Many embedded applications stop here. For things like parsing, protocol stacks and business logic rust has a clear advantage. Interoperability with C is therefore essential. The current situation is good for ARM and RISC (ESP32) but impossible for weirder stuff like C28x. (See my demo here: https://github.com/driftregion/bazel-c-rust-x86_linux-armv7_... ) reply throwaway17_17 16 hours agoprevThe paper may be Rust specific, but I found the CVE break down chart on p. 25 interesting. When looking at the percentages of the CVE causes (focused on the 59 bugs classified as those Rust prevents) I got the following: Out of Bounds Reads => 18.6%; Out of Bounds Writes => 62.7%; Null-Ptr Deref => 8.5%; Use-After-Free => 5.1%; Type Confusion, Uninitialized Pointer Access, Memory Leak => EACH 1.5%; I have to wonder about the applicability of these percentages to non-RTOS programming. I find it very interesting that 81% of CVE's are allocated to Out-of-Bound Read/Writes, with writes being the larges percent of those obviously. Has there been any CVE cause analysis performed and publicly available? If so and the percentages bear out similarly to RTOS's across a spectrum of application/system types then there may be some clear cost/benefit analysis needed at the programming language design stage. Rust is a complex language with a complex type and lifetime system to achieve memory safety, and it is not an uncommon refrain that a simpler 'safe' language would be appreciated by many developers. If 80% of CVE's come from Read/Write errors on array access, then a language that enforces strict memory access semantics, but forgoes the rest of Rust's complexity regarding lifetimes and type system complexity would achieve a very large portion of exploit prevention at a minimal cost. Additionally, if you prevent Null types in the type system the language would then prevent 90% of CVE causes, again with a minimal amount of complexity. I'm not certain that the above is correct, if the percentages play out in the large, or that devs would actively switch to a considerably safer, while being simpler language. But it certainly is thought provoking. reply ngrilly 53 minutes agoparentIn that list, Rust’s borrow checker only solves use-after-free and memory leaks. The other items in the list can be solved without it (null dereference, out of bounds read and write, etc.). See Zig for example. reply bobajeff 15 hours agoparentprevI'm with you on that. In fact I think it's needed to use something less complex than rust in order to prevent other bugs from cropping up due to misunderstood parts of language. C has some bad things it does by default that lead to terrible bugs. I imagine many of them can be addressed without complex move semantics added to it. Some promising work I've seen in this area had been the adoption of language level allocators in zig and Odin. Many newer languages also have better arrays come with length information. And array languages like APL avoid out of bounds errors. I don't think you have to go full ML style type checker (or borrow checker) to prevent bugs. reply pornel 14 hours agoparentprevA bounds-checked slice type would be a relatively small addition to C, and if adopted, it would make size tracking and bounds checking easier. However, there's generally a strong pushback from C developers against features that have an unnecessary performance overhead. Having reliable bounds checking without run-time cost if a much more complicated problem. Rust uses iterators for this, but that requires generics and specialization, and compared to C, these are big and complex features. reply throwaway17_17 13 hours agorootparentTo be clear, I don't think there is any hope of implementing any protections at the language level in C, the push back would be exceptionally fierce. Although I do agree that a 'slice' type in the stdlib would not be to much to ask for. My comment was more for consideration in the design of new languages, in particular, the development of the frequently cited as not existing, simple, C-like language with memory safety features. In that case, in a green field scenario, there are a few ways to achieve statically known memory-boundary respecting iteration and general access. Further, there are several existing methods of achieving 'generics'. The real design challenge would be in finding the simplest implementation that does not overly burden potential developers. I am confident that it could be done, but it would take some grave dissatisfaction with Rust (which is currently at the top of the adoption curve in the memory safe, but GC free space) for the proposed language to take off. reply GrumpySloth 6 hours agorootparentThe problem with slices in C being added in stdlib would be that they really are a generic data type. It would be similar to atomic types. While C++ could just add std::atomic, C needed to add a special construct: _Atomic(T). C++ already has a slice type. It’s called std::span. Porting it to C would probably require something similar to atomics. At which point I guess you may just as well get on with it and switch to C++. reply hgs3 14 hours agoparentprevCorrect me if I'm wrong, but the percentages on page 25 are lower then what you listed? Out of Bounds Reads => 10.1%; Out of Bounds Writes => 34%; Null-Ptr Deref => 4.6%; Type Confusion => 0.9% Uninitialized Pointer Access => 0.9% Use-After-Free => 2.8%; Memory Leaks => 0.9%; The most staggering statistic is the out of bounds writes. C23 added variably-modified types which helps [1], but I hope future revisions of C consider adding slices. I quite like Zig slices where a \"slice\" can be constructed from any pointer+length pair. [1] https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2778.pdf reply throwaway17_17 14 hours agorootparentThe stats you listed are for the percentage of CVE causes out of the total reviewed CVEs (109), which includes those CVE causes Rust can prevent and those that are completely language independent causes. I took the stats for those CVEs that Rust is said to prevent, which according to the paper is 59 CVEs. So I took the number of any given cause and took its percentage out of the 59 CVEs that Rust's memory safety guarantees would prevent. reply ladyanita22 1 hour agoprevI wonder how would the performance of Rust be without the runtime checks and the std_library. And why is the std_library less performant than, let's say, C's counterpart? reply SomeoneFromCA 12 hours agoprev\"Embedded\" is a diverse concept. No need for Rust on ATtiny with all variables static and Harvard architecture. In fact, even C often is overkill, and Assembler is a better choice for a simple LED flasher or really tight high performance loop. reply sheepybloke 14 hours agoprevHonestly, the biggest thing that concerns me with using Rust for embedded is the size of the crates. We were looking to do some packages for a product, and the Rust packages were huge compared to the C++ ones. Granted, this was mostly because the C++ ones could use .so's, while Rust had to compile those into the crate, but this is a huge issue when doing OTA updates. reply steveklabnik 14 hours agoparentIt's just something you have to care about, but it's not a show-stopper. We use a bunch of crates in our projects at work, I left some example sizes in a comment a while back https://news.ycombinator.com/item?id=34032824 reply infamouscow 9 hours agorootparentIt's a show stopper when size matters and you can't fit the binaries into flash. I'm sure \"sorry for getting everyone to switch to this unestablished language\" will go over very well with your boss and upper management. At least in C and C++ you can blame your tools. If you've stupidly convinced management the existing tooling is shit, then you've got a problem. And I don't mean a technical one, I mean a problem with paying rent, because you're not going to be employed much longer. reply mikeInAlaska 17 hours agoprevI spent a weekend with Rust, a Raspi4, and our buddy GPT about six months ago. In that weekend I was able to get the Raspi controlling a OLED display via SPI with an SSD1306 controller. I thought it was a fairly clean port from C++, and GPT was well educated on how to use the RASPI SPI and I2C busses from Rust. I don't think I would be able to approach it on an ESP32 or AVR xMega or some other real microcontroller. reply monocasa 17 hours agoparentBare metal or on linux? reply mikeInAlaska 16 hours agorootparentLinux reply 127 14 hours agoprevI've been trying out embassy-rs and what is really exciting that you might get RTOS abilities, without actually using an RTOS. Just native Rust, with some smart abstractions. Still prefer C, but the Rust embedded community seems to be cooking up something very interesting. reply mips_r4300i 16 hours agoprevHow long before I can visually debug rust on MCUs with source level stepping in my IDE? Til then, no way to switch. reply explodingwaffle 15 hours agoparentYou mean, like this? https://probe.rs/docs/tools/debugger/ reply mips_r4300i 12 hours agorootparentThanks, that's more what I was looking for. Looks like it is still pretty early stuff but could be useful in the future. reply RealityVoid 13 hours agoparentprevLike... zero days? You can just plop the .elf in gdb and then debug on target. I just did it on a riscv mcu just a couple hours ago. Rust is there on the embedded, the only thing missing is people realising it's there. reply the__alchemist 15 hours agoparentprevWhat do you mean by visually debug? If you install `probe-rs` and do `cargo run`, you can print whatever you want to console; not related to the IDE. (Not sure if this is what you're looking for, or something else) reply mips_r4300i 12 hours agorootparentVisual Studio-type ide debugging, viewing structs, run to cursor, etc. reply pjmlp 2 hours agorootparentWhile not Visual Studio C++ level, using CodeLLDB in VS is already quite good. https://marketplace.visualstudio.com/items?itemName=vadimcn.... reply grawp 9 hours agoprev [–] rtic-rs reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper explores using Rust programming language for developing embedded software in safety-critical systems like medical devices and autonomous vehicles.",
      "It emphasizes Rust's memory safety features and its compatibility with existing C codebases, but also points out challenges like inadequate support, limitations of Static Application Security Testing tools, and difficulties in engineering interoperable code.",
      "Developers encounter obstacles when utilizing Rust for embedded systems, requiring insights and recommendations outlined in the paper to address these challenges."
    ],
    "commentSummary": [
      "The article highlights challenges in using Rust for embedded systems, notably in high-frequency trading and gaming, stressing the significance of memory safety to deter bugs and cheating tactics.",
      "It discusses issues like trusting Rust dependencies, managing memory-related problems, and interfacing Rust with C in embedded development, showcasing interest in Rust for embedded use despite worries about ecosystem constraints and integration ease.",
      "Rust is gaining traction for embedded applications, but concerns persist regarding ecosystem limitations, integration simplicity, and trust in code conversion tools."
    ],
    "points": 171,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1709566702
  }
]
