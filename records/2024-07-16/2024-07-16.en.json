[
  {
    "id": 40970560,
    "title": "Run CUDA, unmodified, on AMD GPUs",
    "originLink": "https://docs.scale-lang.com/",
    "originBody": "SCALE by Spectral Compute# What is SCALE?# SCALE is a GPGPU programming toolkit that allows CUDA applications to be natively compiled for AMD GPUs. SCALE does not require the CUDA program or its build system to be modified. Support for more GPU vendors and CUDA APIs is in development. To get started: See the tutorial. Review the examples. Contact us for help. How does it work?# SCALE has several key innovations compared to other cross-platform GPGPU solutions: SCALE accepts CUDA programs as-is. No need to port them to another language. This is true even if your program uses inline PTX asm. The SCALE compiler accepts the same command-line options and CUDA dialect as nvcc, serving as a drop-in replacement. \"Impersonates\" an installation of the NVIDIA CUDA Toolkit, so existing build tools and scripts like cmake just work. What projects have been tested?# We validate SCALE by compiling open-source CUDA projects and running their tests. The following open-source projects are currently part of our nightly automated tests and pass fully: NVIDIA Thrust Blender Cycles AMGX llama-cpp faiss xgboost GOMC stdgpu hashcat Which GPUs are supported?# The following GPU targets are supported, and are covered by our nightly tests: AMD gfx1030 (Navi 21, RDNA 2.0) AMD gfx1100 (Navi 31, RDNA 3.0) The following GPU targets have undergone ad-hoc manual testing and \"seem to work\": AMD gfx1010 AMD gfx1101 We are working on supporting the following GPUs: AMD gfx900 (Vega 10, GCN 5.0) Contact us if you want us to expedite support for a particular AMD GPU architecture. What are the components of SCALE?# SCALE consists of: An nvcc-compatible compiler capable of compiling nvcc-dialect CUDA for AMD GPUs, including PTX asm. Implementations of the CUDA runtime and driver APIs for AMD GPUs. Open-source wrapper libraries providing the \"CUDA-X\" APIs by delegating to the corresponding ROCm libraries. This is how libraries such as cuBLAS and cuSOLVER are handled. What are the differences between SCALE and other solutions?# Instead of providing a new way to write GPGPU software, SCALE allows programs written using the widely-popular CUDA language to be directly compiled for AMD GPUs. SCALE aims to be fully compatible with NVIDIA CUDA. We believe that users should not have to maintain multiple codebases or compromise on performance to support multiple GPU vendors. SCALE's language is a superset of NVIDIA CUDA, offering some opt-in language extensions that can make writing GPU code easier and more efficient for users who wish to move away from nvcc. SCALE is a work in progress. If there is a missing API that is blocking your attempt to use SCALE, please contact us so we can prioritise its development. Contact us# There are multiple ways to get in touch with us: Join our Discord Send us an e-mail at hello@spectralcompute.co.uk",
    "commentLink": "https://news.ycombinator.com/item?id=40970560",
    "commentBody": "Run CUDA, unmodified, on AMD GPUs (scale-lang.com)1102 points by Straw 23 hours agohidepastfavorite331 comments modeless 23 hours agoA lot of people think AMD should support these translation layers but I think it's a bad idea. CUDA is not designed to be vendor agnostic and Nvidia can make things arbitrarily difficult both technically and legally. For example I think it would be against the license agreement of cuDNN or cuBLAS to run them on this. So those and other Nvidia libraries would become part of the API boundary that AMD would need to reimplement and support. Chasing bug-for-bug compatibility is a fool's errand. The important users of CUDA are open source. AMD can implement support directly in the upstream projects like pytorch or llama.cpp. And once support is there it can be maintained by the community. reply eslaught 22 hours agoparentAre you aware of HIP? It's officially supported and, for code that avoids obscure features of CUDA like inline PTX, it's pretty much a find-and-replace to get a working build: https://github.com/ROCm/HIP Don't believe me? Include this at the top of your CUDA code, build with hipcc, and see what happens: https://gitlab.com/StanfordLegion/legion/-/blob/master/runti... It's incomplete because I'm lazy but you can see most things are just a single #ifdef away in the implementation. reply currymj 22 hours agorootparentif you're talking about building anything, that is already too hard for ML researchers. you have to be able to pip install something and just have it work, reasonably fast, without crashing, and also it has to not interfere with 100 other weird poorly maintained ML library dependencies. reply eslaught 21 hours agorootparentIf your point is that HIP is not a zero-effort porting solution, that is correct. HIP is a low-effort solution, not a zero effort solution. It targets users who already use and know CUDA, and minimizes the changes that are required from pre-existing CUDA code. In the case of these abstraction layers, then it would be the responsibility of the abstraction maintainers (or AMD) to port them. Obviously, someone who does not even use CUDA would not use HIP either. To be honest, I have a hard time believing that a truly zero-effort solution exists. Especially one that gets high performance. Once you start talking about the full stack, there are too many potholes and sharp edges to believe that it will really work. So I am highly skeptical of original article. Not that I wouldn't want to be proved wrong. But what they're claiming to do is a big lift, even taking HIP as a starting point. The easiest, fastest (for end users), highest-performance solution for ML will come when the ecosystem integrates it natively. HIP would be a way to get there faster, but it will take nonzero effort from CUDA-proficient engineers to get there. reply currymj 4 hours agorootparentI agree completely with your last point. As other commenters have pointed out, this is probably a good solution for HPC jobs where everyone is using C++ or Fortran anyway and you frequently write your own CUDA kernels. From time to time I run into a decision maker who understandably wants to believe that AMD cards are now \"ready\" to be used for deep learning, and points to things like the fact that HIP mostly works pretty well. I was kind of reacting against that. reply elashri 18 hours agorootparentprevAs someone doing a lot of work with CUDA in a big research organization, there are few of us. If you are working with CUDA, then you are not from the type of people who wait to have something that just works like you describe. CUDA itself is a battle with poorly documented stuff. reply bootsmann 22 hours agorootparentprevDon’t most orgs that are deep enough to run custom cuda kernels have dedicated engineers for this stuff. I can’t imagine a person who can write raw cuda not being able to handle things more difficult than pip install. reply gaogao 21 hours agorootparentEngineers who are really, really good at CUDA are worth their weight in gold, so there's more projects for them than they have time. Worth their weight in gold isn't figurative here – the one I know has a ski house more expensive than 180 lbs of gold (~$5,320,814). reply Willish42 15 hours agorootparentThe fact that \"worth their weight in cold\" typically means in the single-digit millions is fascinating to me (though I doubt I'll be able to get there myself, maybe someday). I looked it up though and I think this is undercounting the current value of gold per ounce/lb/etc. 5320814 / 180 / 16 = ~1847.5 Per https://www.apmex.com/gold-price and https://goldprice.org/, current value is north of $2400 / oz. It was around $1800 in 2020. That growth for _gold_ of all things (up 71% in the last 5 years) is crazy to me. It's worth noting that anyone with a ski house that expensive probably has a net worth well over twice the price of that ski house. I guess it's time to start learning CUDA! reply atwrk 10 hours agorootparent> That growth for _gold_ of all things (up 71% in the last 5 years) is crazy to me. For comparison: S&P500 grew about the same during that period (more than 100% from Jan 2019, about 70 from Dec 2019), so the higher price of gold did not outperform the growth of the general (financial) economy. reply dash2 6 hours agorootparentBut that's still surprising performance, because the S&P generates income and pays dividends. Its increase reflects (at least, is supposed to!) expectations of future higher income. Gold doesn't even bear interest.... reply boulos 14 hours agorootparentprevNote: gold uses troy ounces, so adjust by ~10%. It's easier to just use grams or kilograms :). reply Willish42 1 hour agorootparentThanks, I'm a bit new to this entire concept. Do troy lbs also exist, or is that just a term when measuring ounces? reply bbkane 21 hours agorootparentprevWould you (or your friend) be able to drop any good CUDA learning resources? I'd like to be worth my weight in gold... reply throwaway81523 12 hours agorootparentA working knowledge of C++, plus a bit of online reading about CUDA and the NVidia GPU architecture, plus studying the LCZero chess engine source code (the CUDA neural net part, I mean) seems like enough to get started. I did that and felt like I could contribute to that code, at least at a newbie level, given the hardware and build tools. At least in the pre-NNUE era, the code was pretty readable. I didn't pursue it though. Of course becoming \"really good\" is a lot different and like anything else, it presumably takes a lot of callused fingertips (from typing) to get there. reply mosselman 11 hours agorootparentThe real challenge is probably getting your hands on a 4090 for a price you can pay before you are worth your weight in gold. Because an arm and a limb in gold is quite a lot. reply throwaway81523 9 hours agorootparentYou don't really need a 4090. An older board is plenty. The software is basically the same. I fooled around with what I think was a 1080 on Paperspace for something like 50 cents an hour, but it was mostly with some Pytorch models rather than CUDA directly. reply ahepp 5 hours agorootparentprevI was looking into this recently and it seems like the cheapest AWS instance with a CUDA GPU is something on the order of $1/hr. It looks like an H100 instance might be $15/hr (although I’m not sure if I’m looking at a monthly price). So yeah it’s not ideal if you’re on a budget, but it seems like there are some solutions that don’t involve massive capex. reply throwaway81523 5 hours agorootparentLook on vast.ai instead of AWS, you can rent machines with older GPU's dirt cheap. I don't see how they even cover the electricity bills. A 4090 machine starts at about $.25/hour though I didn't examine the configuration. A new 4090 costs around $1800 (https://www.centralcomputer.com/asus-tuf-rtx4090-o24g-gaming...) and that's probably affordable to AWS users. I see a 2080Ti on Craigslist for $300 (https://sfbay.craigslist.org/scz/sop/d/aptos-nvidia-geforce-...) though used GPU's are possibly thrashed by bitcoin mining. I don't have a suitable host machine, unfortunately. reply dotancohen 2 hours agorootparentThrashed? What type of damage could a mostly-solid state device suffer? Fan problems? Worn PCi connectors? Deteriorating Arctic Ice from repeated heat cycling? reply SonOfLilit 2 hours agorootparentprevreplying to sibling @dotancohen, they melt, and they suffer from thermal expansion and compression reply robotnikman 2 hours agorootparentprevAre there any certifications or other ways to prove your knowledge to employers in order to get your foot in the door? reply 8n4vidtmkvmk 11 hours agorootparentprevDoes this pay more than $500k/yr? I already know C++, could be tempted to learn CUDA. reply throwaway81523 9 hours agorootparentI kinda doubt it. Nobody paid me to do that though. I was just interested in LCZero. To get that $500k/year, I think you need up to date ML understanding and not just CUDA. CUDA is just another programming language while ML is a big area of active research. You could watch some of the fast.ai ML videos and then enter some Kaggle competitions if you want to go that route. reply almostgotcaught 6 hours agorootparentYou're wrong. The people building the models don't write CUDA kernels. The people optimizing the models write CUDA kernels. And you don't need to know a bunch of ML bs to optimize kernels. Source: I optimize GPU kernels. I don't make 500k but I'm not that far from. reply throwaway81523 6 hours agorootparentHeh I'm in the wrong business then. Interesting. Used to be that game programmers spent lots of time optimizing non-ML CUDA code. They didn't make anything like 500k at that time. I wonder what the ML industry has done to game development, or for that matter to scientific programming. Wow. reply HarHarVeryFunny 4 hours agorootparentprevHow much performance difference is there between writing a kernel in a high level language/framework like PyTorch (torch.compile) or Triton, and hand optimizing? Are you writing kernels in PTX? What's your opinion on the future of writing optimized GPU code/kernels - how long before compilers are as good or better than (most) humans writing hand-optimized PTX? reply iftheshoefitss 10 hours agorootparentprevOn bro forget gold if like to be worth my weight in paper lmao reply eigenvalue 20 hours agorootparentprevThat’s pretty funny. Good test of value across the millennia. I wonder if the best aqueduct engineers during the peak of Ancient Rome’s power had villas worth their body weight in gold. reply Winse 19 hours agorootparentLol. For once being overweight may come with some advantages here. reply necovek 12 hours agorootparentOr disadvantages: you may be as rich as your skinny neighbour, but they are the only ones worth their weight in gold ;) reply iftheshoefitss 10 hours agorootparentprevWhat do people study to figure out CUDA? I’m studying to get me GED and hope to go to school one day reply paulmd 42 minutes agorootparentComputer science. This is a grad level topic probably. Nvidia literally wrote most of the textbooks in this field and you’d probably be taught using one of these anyway: https://developer.nvidia.com/cuda-books-archive “GPGPU Gems” is another “cookbook” sort of textbook that might be helpful starting out but you’ll want a good understanding of the SIMT model etc. reply amelius 8 hours agorootparentprevJust wait until someone trains an ML model that can translate any CUDA code into something more portable like HIP. GP says it is just some #ifdefs in most cases, so an LLM should be able to do it, right? reply FuriouslyAdrift 5 hours agorootparentOpenAI Triton? Pytorch 2.0 already uses it. https://openai.com/index/triton/ reply phkahler 5 hours agorootparentprev>> Don’t most orgs that are deep enough to run custom cuda kernels have dedicated engineers for this stuff. I can’t imagine a person who can write raw cuda not being able to handle things more difficult than pip install. This seems to be fairly common problem with software. The people who create software regularly deal with complex tool chains, dependency management, configuration files, and so on. As a result they think that if a solutions \"exists\" everything is fine. Need to edit a config file for your particular setup? No problem. The thing is, I have been programming stuff for decades and I really hate having to do that stuff and will avoid tools that make me do it. I have my own problems to solve, and don't want to deal with figuring out tools no matter how \"simple\" the author thinks that is to do. A huge part of the reason commercial software exists today is probably because open source projects don't take things to this extreme. I look at some things that qualify as products and think they're really simplistic, but they take care of some minutia that regular people are will to pay so they don't have to learn or deal with it. The same can be true for developers and ML researchers or whatever. reply ezekiel68 19 hours agorootparentprev> if you're talking about building anything, that is already too hard for ML researchers. I don't think so. I agree it is too hard for the ML researches at the companies which will have their rear ends handed to them by the other companies whose ML researchers can be bothered to follow a blog post and prompt ChatGPT to resolve error messages. reply currymj 4 hours agorootparentI'm not really talking about companies here for the most part, I'm talking about academic ML researchers (or industry researchers whose role is primarily academic-style research). In companies there is more incentive for good software engineering practices. I'm also speaking from personal experience: I once had to hand-write my own CUDA kernels (on official NVIDIA cards, not even this weird translation layer): it was useful and I figured it out, but everything was constantly breaking at first. It was a drag on productivity and more importantly, it made it too difficult for other people to run my code (which means they are less likely to cite my work). reply jokethrowaway 18 hours agorootparentpreva lot of ML researchers stay pretty high level and reinstall conda when things stop working and rightly so, they have more complicated issues to tackle It's on developers to provide better infrastructure and solve these challenges reply LtWorf 14 hours agorootparentNot rightly. It'd be faster on the long term to address the issues. reply bayindirh 13 hours agorootparentCurrently nobody think that long term. They just reinstall, that’s it. reply jchw 22 hours agorootparentprevThe target audience of interoperability technology is whoever is building, though. Ideally, interoperability technology can help software that supports only NVIDIA GPUs today go on to quickly add baseline support for Intel and AMD GPUs tomorrow. (and for one data point, I believe Blender is actively using HIP for AMD GPU support in Cycles.) reply Agingcoder 21 hours agorootparentprevTheir target is hpc users, not ml researchers. I can understand why this would be valuable to this particular crowd. reply klik99 17 hours agorootparentprevGod this explains so much about my last month, working with tensorflow lite and libtorch in C++ reply SushiHippie 20 hours agorootparentprevAMD has hipify for this, which converts cuda code to hip. https://github.com/ROCm/HIPIFY reply jph00 20 hours agorootparentprevInline PTX is hardly an obscure feature. It's pretty widely used in practice, at least in the AI space. reply saagarjha 18 hours agorootparentYeah, a lot of the newer accelerators are not even available without using inline PTX assembly. Even the ones that are have weird shapes that are not amenable to high-performance work. reply HarHarVeryFunny 4 hours agorootparentAre you saying that the latest NVIDIA nvcc doesn't support the latest NVIDIA devices? reply adrian_b 3 hours agorootparentFor any compiler, \"supporting\" a certain CPU or GPU only means that they can generate correct translated code with that CPU or GPU as the execution target. It does not mean that the compiler is able to generate code that has optimal performance, when that can be achieved by using certain instructions without a direct equivalent in a high-level language. No compiler that supports the Intel-AMD ISA knows how to use all the instructions available in this ISA. reply HarHarVeryFunny 3 hours agorootparentSure, but I'm not sure if that is what the parent poster was saying (that nvcc generates poor quality PTX for newer devices). It's been a while since I looked at CUDA, but it used to be that NVIDIA were continually extending cuDNN to add support for kernels needed by SOTA models, and I assume these kernels were all hand optimized. I'm curious what kind of models people are writing where not only is there is no optimized cuDNN support, but also solutions like Triton or torch.compile, and even hand optimized CUDA C kernels are too slow. Are hand written PTX kernels really that common ? reply pjmlp 13 hours agorootparentprevHow does it run CUDA Fortran? reply blitzar 22 hours agoparentprevIt would be good if AMD did something, anything. Support this, reimplement that, support upstream efforts, dont really care. Any of those would cost a couple of million and be worth a trillion dollars to AMD shareholders. reply chatmasta 19 hours agorootparentIs it weird how the comments here are blaming AMD and not Nvidia? Sure, the obvious argument is that Nvidia has no practical motivation to build an open platform. But there are counterexamples that suggest otherwise (Android). And there is a compelling argument that long term, their proprietary firmware layer will become an insufficient moat to their hardware dominance. Who’s the root cause? The company with the dominant platform that refuses to open it up, or the competitor who can’t catch up because they’re running so far behind? Even if AMD made their own version of CUDA that was better in every way, it still wouldn’t gain adoption because CUDA has become the standard. No matter what they do, they’ll need to have a compatibility layer. And in that case maybe it makes sense for them to invest in the best one that emerges from the community. reply lmm 18 hours agorootparent> Is it weird how the comments here are blaming AMD and not Nvidia? Nvidia has put in the legwork and are reaping the rewards. They've worked closely with the people who are actually using their stuff, funding development and giving loads of support to researchers, teachers and so on, for probably a decade now. Why should they give all that away? > But there are counterexamples that suggest otherwise (Android). How is Android a counterexample? Google makes no money off of it, nor does anyone else. Google keeps Android open so that Apple can't move everyone onto their ad platform, so it's worth it for them as a strategic move, but Nvidia has no such motive. > Even if AMD made their own version of CUDA that was better in every way, it still wouldn’t gain adoption because CUDA has become the standard. Maybe. But again, that's because NVidia has been putting in the work to make something better for a decade or more. The best time for AMD to start actually trying was 10 years ago; the second-best time is today. reply Zambyte 3 hours agorootparent> Google makes no money off of it, nor does anyone else Google makes no money off of Android? That seems like a really weird claim to make. Do you really think Google would be anywhere near as valuable of a company if iOS had all of the market share that the data vacuum that is Android has? I can't imagine that being the case. Google makes a boatload off of Android, just like AMD would if they supported open GPGPU efforts aggressively. reply rjurney 14 minutes agorootparentAndroid is a complement to Google's business, which is when open source works. What would be the complement worth $1 Trillion to NVIDIA to build a truly open platform? There isn't one. That was his point. reply michaelt 1 hour agorootparentprevGoogle gave away the software platform - Android - to hardware vendors for free, vendors compete making the hardware into cheap, low-margin commodity items, and google makes boatloads of money from ads, tracking and the app store. nvidia could give away the software platform - CUDA - to hardware vendors for free, making the hardware into cheap, low-margin commodity items. But how would they make boatloads of money when there's nowhere to put ads, tracking or an app store? reply nemothekid 14 hours agorootparentprev>Is it weird how the comments here are blaming AMD and not Nvidia? It's not. Even as it is, I do not trust HIP or RocM to be a viable alternative to Cuda. George Hotz did plenty of work trying to port various ML architectures to AMD and was met with countless driver bugs. The problem isn't nvidia won't build an open platform - the problem is AMD won't invest in a competitive platform. 99% of ML engineers do not write CUDA. For the vast majority of workloads, there are probably 20 engineers at Meta who write the Cuda backend for Pytorch that every other engineer uses. Meta could hire another 20 engineers to support whatever AMD has (they did, and it's not as robust as CUDA). Even if CUDA was open - do you expect nvidia to also write drivers for AMD? I don't believe 3rd parties will get anywhere writing \"compatibility layers\" because AMD's own GPU aren't optimized or tested for CUDA-like workloads. reply pjmlp 13 hours agorootparentprevKhrons, AMD and Intel have had 15 years to make something out of OpenCL that could rival CUDA. Instead they managed 15 years of disappointment, in a standard stuck in C99, that adopted C++ and a polyglot bytecode too late to matter, never produced an ecosystem of IDE tooling and GPU libraries. Naturally CUDA became the standard, when NVIDIA provided what the GPU community cared about. reply roenxi 18 hours agorootparentprev> Is it weird how the comments here are blaming AMD and not Nvidia? Not even a little bit. It simply isn't Nvidia's job to provide competitive alternatives to Nvidia. Competing is something AMD must take responsibility for. The only reason CUDA is such a big talking point is because AMD tripped over their own feet supporting accelerated BLAS on AMD GPUs. Realistically it probably is hard to implement (AMD have a lot of competent people on staff) but Nvidia hasn't done anything unfair apart from execute so well that they make all the alternatives look bad. reply jkmcf 16 hours agorootparentI agree with you, but replace NVIDIA with Apple. What would the EU say? reply LtWorf 14 hours agorootparentI don't think nvidia bans anyone from running code on their devices. reply Zambyte 3 hours agorootparenthttps://www.pcgamer.com/nvidia-officially-confirms-hash-rate... Also: look into why the Nouveau driver performance is limited. reply paulmd 33 minutes agorootparentso terrible that vendors can enforce these proprietary licenses on software they paid to develop /s reply padthai 12 hours agorootparentprevThey do from time to time: https://wirelesswire.jp/2017/12/62708/ reply kbolino 5 hours agorootparentThis seems to be more about certain devices (consumer-grade GPUs) in certain settings (data centers), though I do question how enforceable it actually is. My guess is that it can only apply when you try to get discounts from bulk-ordering GPUs. Also, was there any followup to this story? It seems a bit unnecessary because nVidia has already neutered consumer cards for many/most data center purposes by not using ECC and by providing so few FP64 units that double precision FLOPS is barely better than CPU SIMD. reply paulmd 31 minutes agorootparentit’s also not really a thing anymore because of the open kernel driver… at that point it’s just MIT licensed. of course people continued to melt down about that for some reason too, in the customary “nothing is ever libre enough!” circular firing squad. Just like streamline etc. There’s a really shitty strain of fanboy thought that wants libre software to be actively worsened (even stonewalled by the kernel team if necessary) so that they can continue to argue against nvidia as a bad actor that doesn’t play nicely with open source. You saw it with all these things but especially with the open kernel driver, people were really happy it didn’t get upstreamed. Shitty behavior all around. You see it every time someone quotes Linus Torvalds on the issue. Some slight from 2006 is more important than users having good, open drivers upstreamed. Some petty brand preferences are legitimately far important than working with and bringing that vendor into the fold long-term, for a large number of people. Most of whom don’t even consider themselves fanboys! They just say all the things a fanboy would say, and act all the ways a fanboy would act… reply cogman10 3 hours agorootparentprevFunnily, who I blame the most for there not being real competition to CUDA is apple. As of late, Apple has been really pushing for vender lock in APIs rather than adopting open standards. The end result is you can get AMD and Intel onboard with some standard which is ultimately torpedoed by apple. (See apple departing from and rejecting everything that comes from the khronos group). With the number of devs that use Apple silicon now-a-days, I have to think that their support for khronos initiatives like SYCL and OpenCL would have significantly accelerated progress and adoption in both. We need an open standard that isn't just AMD specific to be successful in toppling CUDA. reply whywhywhywhy 7 hours agorootparentprev>Is it weird how the comments here are blaming AMD and not Nvidia? Because it IS AMD/Apple/etcs fault for the position they're in right now. CUDA showed where the world was heading and where the gains in compute would be made well over a decade ago now. They even had OpenCL, didn't put the right amount of effort into it, all the talent found CUDA easier to work with so built there. Then what did AMD, Apple do? Double down and try and make something better and compete? Nah they fragmented and went their own way, AMD with what feels like a fraction of the effort even Apple put in. From the actions of the other teams in the game it's not hard to imagine a world without CUDA being a world where this tech is running at a fraction of it's potential. reply immibis 7 hours agorootparentprevIt's always been on the straggler to catch up by cheating. That's just how the world works - even in open source. If AMD supported CUDA, it would have a bigger market share. That's a fact. Nvidia doesn't want that. That's a fact. But when Reddit started, it just scraped feeds from Digg, and when Facebook started, it let you link your MySpace credentials and scraped your MySpace account. Adversarial interoperability is nothing new. reply slashdave 21 hours agorootparentprevROCm counts as \"something\" reply curt15 20 hours agorootparentPretty much any modern NVIDIA GPU supports CUDA. You don't have to buy a datacenter-class unit to get your feet wet with CUDA programming. ROCm will count as \"something\" when the same is true for AMD GPUs. reply jacoblambda 19 hours agorootparentROCm supports current gen consumer gpus officially and a decent chunk of recent gen consumer gpus unofficially. Not all of them of course but a decent chunk. It's not ideal but I'm pretty sure CUDA didn't support everything from day 1. And ROCm is part of AMD's vendor part of the Windows AI stack so from upcoming gen on out basically anything that outputs video should support ROCm. reply ChoGGi 12 hours agorootparentNo, but CUDA at least supported the 8800 gt on release [1]. ROCm didn't support any consumer cards on release, looks like they didn't support any till last year? [2] [1]https://www.gamesindustry.biz/nvidia-unveils-cuda-the-gpu-co... [2]https://www.tomshardware.com/news/amd-rocm-comes-to-windows-... reply slashdave 1 hour agorootparentprevAMD should focus their efforts on competitive hardware offerings, because that is where the need and the money is. Sorry, I don't think the hobbyist should be a priority. reply muxr 20 hours agorootparentprevI don't think AMD needs to support 5+ year old GPUs personally. And all the recent generations are already practically supported. AMD only claims support for a select few GPUs, but in my testing I find all the GPUs work fine if the architecture is supported. I've tested rx6600, rx6700xt for example and even though they aren't officially supported, they work fine on ROCm. reply Dylan16807 16 hours agorootparent> 5+ year old GPUs AMD had a big architecture switchover exactly 5 years ago, and the full launch wasn't over until 4.5 years ago. I think that generation should have full support. Especially because it's not like they're cutting support now. They didn't support it at launch, and they didn't support it after 1, 2, 3, 4 years either. The other way to look at things, I'd say that for a mid to high tier GPU to be obsolete based on performance, the replacement model needs to be over twice as fast. 7700XT is just over 50% faster than 5700XT. reply imtringued 10 hours agorootparentprevI'm on a 5+ year old GPU, because I don't trust AMD to offer a compelling GPU that actually works. An RX 7 570 is good enough for the little gaming I do. It mostly acts as an oversized iGPU that has good Linux drivers, but since AMD is not supporting ROCm on this GPU, there is no need to hurry on upgrading to a better GPU or to get my feet wet on running things locally on the GPU like Stable Diffusion, LLMs, etc. reply squidgyhead 18 hours agorootparentprevHere is the support list: https://rocm.docs.amd.com/projects/install-on-linux/en/lates... reply mappu 18 hours agorootparentAMD's definition of \"support\" I think is different than what people expect, and pretty misleading - ROCm itself will run on almost anything, back as far as the RX 400/500 series: https://en.wikipedia.org/wiki/ROCm#:~:text=GCN%205%20%2D%20V... Stable Diffusion ran fine for me on RX 570 and RX 6600XT with nothing but distro packages. reply slavik81 59 minutes agorootparentThere are out-of-bounds writes in the BLAS libraries for gfx803 GPUs (such as the RX 570). That hardware might work fine for your use case, but there's a lot of failures in the test suites. I agree that the official support list is very conservative, but I wouldn't recommend pre-Vega GPUs for use with ROCm. Stick to gfx900 and newer, if you can. reply Nab443 10 hours agorootparentprevThe last time I checked, I was stuck with a pretty old kernel if I wanted to have the last version of ROCm available for my rx470. It's compatible at some point in time, but not kept compatible with recent kernels. reply imtringued 10 hours agorootparentprevI don't buy it. Even running things like llama.cpp on my RX 570 via Vulkan crashes the entire system. reply bavell 3 hours agorootparentprevHuh? I've been running ROCm for SD and LLMs for over a year and a half on my puny consumer 6750X - not even latest gen. reply oezi 21 hours agorootparentprevA couple of million doesn't get you anything in corporate land reply spacebanana7 20 hours agorootparentA couple dozen billion for a 10% chance of becoming NVIDIA competitive is worth it, looking at the stock prices. reply fngjdflmdflg 23 hours agoparentprev>Nvidia can make things arbitrarily difficult both technically and legally. I disagree. AMD can simply not implement those APIs, similar to how game emulators implement the most used APIs first and sometimes never bother implementing obscure ones. It would only matter that NVIDIA added eg. patented APIs to CUDA if those APIs were useful. In which case AMD should have a way to do them anyway. Unless NVIDIA comes up with a new patented API which is both useful and impossible to implement in any other way, which would be bad for AMD in any event. On the other hand, if AMD start supporting CUDA and people start using AMD cards, then developers will be hesitant to use APIs that only work on NVIDIA cards. Right now they are losing billions of dollars on this. Then again they barely seem capable of supporting RocM on their cards, much less CUDA. You have a fair point in terms of cuDNN and cuBLAS but I don't know that that kind of ToS is actually binding. reply selimnairb 19 hours agorootparentPatented API? I thought Google v. Oracle settled this? Making an implementation of an API spec is fair use, is it not? reply fngjdflmdflg 17 hours agorootparentMy understanding is that Google v. Oracle only applies to copyright. reply nl 15 hours agorootparentWell you can't patent an API so.... reply fngjdflmdflg 13 hours agorootparentYou can patent the implementation. You can't patent the API name DecodeH265Video() but you can still sue someone for implementing that function correctly. reply apatheticonion 16 hours agoparentprevAgreed. Rather than making CUDA the standard; AMD should push/drive an open standard that can be run on any hardware. We have seen this succeed multiple times: FreeSync vs GSync, DLSS vs FSR, (not AMD but) Vulkan vs DirectX & Metal. All of the big tech companies are obsessed with ring-fencing developers behind the thin veil of \"innovation\" - where really it's just good for business (I swear it should be regulated because it's really bad for consumers). A CUDA translation layer is okay for now but it does risk CUDA becoming the standard API. Personally, I am comfortable with waiting on an open standard to take over - ROCm has serviced my needs pretty well so far. Just wish GPU sharing with VMs was as easy as CPU sharing. reply ChoGGi 12 hours agorootparent\"We have seen this succeed multiple times: FreeSync vs GSync, DLSS vs FSR, (not AMD but) Vulkan vs DirectX & Metal.\" I'll definitely agree with you on Sync and Vulkan, but dlss and xess are both better than fsr. https://youtube.com/watch?v=el70HE6rXV4 reply naasking 6 hours agorootparentprev> AMD should push/drive an open standard that can be run on any hardware. AMD has always been notoriously bad at the software side, and they frequently abandon their projects when they're almost usable, so I won't hold my breath. reply amy-petrik-214 16 hours agorootparentprevwe actually also saw this historically with openGL. openGL comes from an ancient company whispered about by the elderly programmers (30 + year old) known as SGI. Originally it was CLOSED SOURCE and SGI called it \"SGI-GL\" for a computer codename IRIS which was cool looking with bright popping color plastic and faux granite keyboard. Good guy SGI open sourced SGI-GL to become what we called \"openGL\" (get it, now it's open), and then it stuck. That's all to say NVIDIA could pull a SGI and open their stuff, but they're going more sony style and trying to monopolize. Oh, and SGI also wrote another ancient lore library known as \"STL\" or the \"SGI Template Library\" which is like the original boost template metaprogramming granddaddy reply usr1106 12 hours agorootparentNice story, but is it correct? Wikipedia says STL was first implemented by HP and later by the same authors at SGI. reply adrian_b 3 hours agorootparentSTL started even earlier, obviously without using the name \"STL\", as a library of generic algorithms for the programming language Ada (David R. Musser & Alexander A. Stepanov, 1987). reply adrian_b 13 hours agorootparentprevAlso the XFS file system. reply pjmlp 13 hours agorootparentprevVulkan only matters on Android (from version 10 onwards) and GNU/Linux. Zero impact on Switch, Playstation, XBox, Windows, macOS, iOS, iPadOS, Vision OS. reply ChoGGi 12 hours agorootparent\"Windows\" dxvk-gplasync is a game changer for dx9-11 shader stutter. reply pjmlp 10 hours agorootparentSure, for the 2% folks that enjoy Windows games, written againt DirectX, on Linux Steam Store. Which Android Studios can't even be bothered to target with their NDK engines, based on GL ES, Vulkan. reply ChoGGi 5 hours agorootparentI'm on windows 11, if I see not dx12 in my afterburner overlay, I use it. Even if there's no shader stutter, Vulkan tends to use less juice than DX. reply gjulianm 10 hours agorootparentprevOpenCL was released in 2009. AMD has had plenty of time to push and drive that standard. But OpenCL had a worse experience than CUDA, and AMD wasn't up to the task in terms of hardware, so it made no real sense to go for OpenCL. reply consf 7 hours agorootparentprevA strategic and forward-thinking approach reply imtringued 10 hours agorootparentprevAMD shouldn't push on anything. They have the wrong incentives. They should just make sure that software runs on their GPUs and nothing else. Karol Herbst is working on Rusticl, which is mesa's latest OpenCL implementation and will pave the way for other things such as SYCL. reply Const-me 22 hours agoparentprev> Nvidia can make things arbitrarily difficult both technically and legally Pretty sure APIs are not copyrightable, e.g. https://www.law.cornell.edu/supremecourt/text/18-956 > against the license agreement of cuDNN or cuBLAS to run them on this They don’t run either of them, they instead implement an equivalent API on top of something else. Here’s a quote: “Open-source wrapper libraries providing the \"CUDA-X\" APIs by delegating to the corresponding ROCm libraries. This is how libraries such as cuBLAS and cuSOLVER are handled.” reply dralley 22 hours agorootparentI believe it was decided that they are copyrightable but that using them for compatibility purposes is fair use. reply kbolino 22 hours agorootparentNo, it's stranger than that: SCOTUS did not rule on copyrightability of APIs at all, but simply ruled that even if they are copyrightable, what Google did (completely reimplement Sun/Oracle's public API) was still fair use. reply mrandish 22 hours agorootparentIt would have been nice to get a clear SCOTUS precedent on this. On the other hand, I also value a SCOTUS which rules minimally and narrowly by default (I also appreciate SCOTUS' return to stricter constitutional grounding in the past decade). reply hobs 21 hours agorootparentIncredibly loud laughing from the lawyers whose study of law is being thrown around willy nilly because of all the unprecedented joke decisions they are making right now. reply kbolino 21 hours agorootparentWe are stuck between a rock and a hard place politically. The real decisions should be coming from Congress not the courts. However, Congress is too disorganized and disconnected to answer the important questions, leaving the courts to either muddle along or else become semi-dictatorial. In most countries, this would cause a constitutional crisis, but the modern U.S. system seems to be a little too resilient to such otherwise concerning signals. reply hobs 20 hours agorootparentWe're far past a constitutional crisis, and the courts taking power nobody wanted to give to them (who wasn't interested in a unitary executive at least) isn't a good solution. reply kbolino 19 hours agorootparentWhat constitutional crisis has occurred that hasn't been resolved? Constitutional crises involve fundamental breaks in the working of government that bring two or more of its elements into direct conflict that can't be reconciled through the normal means. The last of these by my accounting was over desegregation, which was resolved with the President ordering the Army to force the recalcitrant states to comply. Before that was a showdown between the New Deal Congress and the Supreme Court, which the former won by credibly threatening to pack the latter (which is IMO a much less severe crisis but still more substantial than anything happening today). However, that was almost a century ago, and Congress has not been that coherent lately. reply ted_dunning 18 hours agorootparentI would think the latest one where SCOTUS ruled that the president was a king except in matters where the SCOTUS decides they aren't counts as a constitutional crisis. reply FeepingCreature 17 hours agorootparentConstitutional crises are not a matter of opinion but of occurrence, arising from an actual power conflict between arms of the government that is caused by a conflicted reading of the constitutional text. Basically, if the system just ticks on, it's not a constitutional crisis. If \"I think this is a very bad decision\" was cause for a constitutional crisis, any state with more than three digit population would be in constitutional crisis perpetually. reply jolux 15 hours agorootparent> Constitutional crises are not a matter of opinion but of occurrence, arising from an actual power conflict between arms of the government that is caused by a conflicted reading of the constitutional text. Basically, if the system just ticks on, it's not a constitutional crisis. This happened as recently as 2021-01-06; strong evidence that the military subverted the president to call the National Guard into Washington DC and secure the electoral count. reply kbolino 6 hours agorootparentThat's close. Both the excessively long lame duck period (2 months for Congress and 2.5 months for the President) and disunity between the President and the rest of the executive branch have also been fodder for crises in the past (Marbury v Madison, Andrew Johnson's impeachment). reply hnfong 13 hours agorootparentprevIf Trump didn't back down it could have definitely been a constitutional crisis. I'd say it was narrowly averted though. reply not2b 19 hours agorootparentprevThat is how the SC used to work: they would decide cases on the narrowest possible grounds. If they don't have to decide a tough question, but they can finesse it with something simpler, good enough. More recently they have been willing to tear up decades of established law on a regular basis. reply hnfong 13 hours agorootparent\"Used to work\"... this was 2021. And generally courts/judges just choose the scope of their legal opinions based on how far reaching they want the legal principles to apply. IMHO, copyright-ability of APIs is so far away from their political agenda that they probably just decided to leave the issue on a cliffhanger... reply immibis 6 hours agorootparentYes, \"used to\". Now, in 2024, the same supreme court has decided that presidents have immunity in all official acts, from stealing documents, up to and including assassination attempts on their opponents. This is a radical shift in how the court operates. reply kbolino 5 hours agorootparentThis \"opponent assassination\" hypothetical gets bandied about a lot but I have not seen any evidence that any court considers that to be an \"official act\". Official acts are constrained to legitimate exercises of constitutional authority and are not merely anything a President (or especially, an ex-President) does. reply jpadkins 5 hours agorootparentprevthe only thing radical is the opinions of people you are listening to if you believe SCOTUS enabled legally sanctioned assassinations. It was political hyperbole based on nothing, and it worked (with you). Think for yourself. reply consf 7 hours agorootparentprevYou're correct! Fair Use Doctrine reply rjurney 17 hours agoparentprevNot having a layer like this has left AMD completely out of the AI game that has made NVDA the world's most valuable company. reply ChoGGi 12 hours agorootparentSelf-inflicted wounds hurt the most. reply raxxorraxor 7 hours agoparentprevI really hope they will do what you suggested. With some innovative product placement, GPUs with a lot of memory for example, they could dethrone nvidia if it doesn't change strategy. That said, easier said than done. You need very specialized developers to build a CUDA equivalent and have people start using it. AMD could do it with a more open development process leveraging the open source community. I believe this will happen at some point anyway by AMD or someone else. The market just gets more attractive by the day and at some point the high entry barrier will not matter much. So why should AMD skimp on their ambitions here? This would be a most sensible investment, few risks and high gains if successful. reply consf 7 hours agorootparentThis expanding market provides AMD with a lucrative opportunity indeed reply Wowfunhappy 20 hours agoparentprev> CUDA is not designed to be vendor agnostic and Nvidia can make things arbitrarily difficult [...] technically. (Let's put the legal questions aside for a moment.) nVidia changes GPU architectures every generation / few generations, right? How does CUDA work across those—and how can it have forwards compatibility in the future—if it's not designed to be technologically agnostic? reply saagarjha 18 hours agorootparentPTX is meant to be portable across GPU microarchitectures. That said, Nvidia owns the entire spec, so they can just keep adding new instructions that their GPUs now support but AMD GPUs don't. reply andy_ppp 20 hours agorootparentprevOne way is to make sure the hardware team does certain things to support easy transition to new architectures, we have seen this with Apple Silicon for example! reply magic_hamster 15 hours agoparentprevCUDA is the juice that built Nvidia in the AI space and allowed them to charge crazy money for their hardware. To be able to run CUDA on cost effective AMD hardware can be a big leap forward, allow more people to research, and break away from Nvidia's stranglehold over VRAM. Nvidia will never open source their own platform unless their hand is forced. I think we all should support this endeavor and contribute where possible. reply Sparkyte 9 hours agoparentprevThat is why an open standard should be made so it isn't locked to a particular piece of hardware and then allow modular support for different hardware to interface with supported drivers. reply amelius 22 hours agoparentprevLike supporting x86 was a bad idea as well? reply modeless 22 hours agorootparentBefore starting, AMD signed an agreement with Intel that gave them an explicit license to x86. And x86 was a whole lot smaller and simpler back then in 1982. A completely different and incomparable situation. reply nostrademons 21 hours agorootparentTechnically it was after starting - AMD was founded in 1969 as a second-sourcer for Fairchild and National Semiconductor, and had reverse-engineered the 8080 by 1975 and acquired a formal license to it by 1976. The 1982 deal you speak of was actually pretty interesting: as a condition of the x86's use in the IBM PC, IBM requested a second source for x86 chips. AMD was that source, and so they cross-licensed the x86 in 1982 to allow the IBM PC project to proceed forward. This makes the Intel/AMD deal even more important for both companies: the PC market would never have developed without the cross-licensing, which would've been bad for all companies involved. This gave Intel an ongoing stake in AMD's success at least until the PC market consolidated on the x86 standard. reply karolist 22 hours agorootparentprevWas there a large entity steering x86 spec alone with a huge feature lead against their competition, free to steer the spec in any ways they choose? Also, hardware is not opensource software, you get big players onboard and they will be able to implement the spec they want every gen, software has more moving parts and unaligned parties involved. reply cherryteastain 22 hours agorootparent> Was there a large entity steering x86 spec alone with a huge feature lead against their competition, free to steer the spec in any ways they choose? Ever heard of Intel? reply karolist 22 hours agorootparentI had't considered that angle. Is your point that Intel was the creator of x86, but software chose to support it, then AMD had nothing else but to play catch up in x86 support to be part of the software target market? If so and factual (I've no idea), fair point, I didn't know. reply marshray 20 hours agorootparentIt was exactly the same instruction set. C compilers didn't offer an \"AMD\" CPU target* until AMD came out with the \"AMD64\" instruction set. Today we call this \"x86_64\" or \"x64\". * Feel free to point out some custom multimedia vector extensions for Athlons or something, but the point remains. reply gmokki 6 hours agorootparentAnd Intel named its licenced implementation of AMD64 as IA-32e, just to make it clear to everyone that it is based on Intel architecture 32bit version with an extension. Luckily they dropped that name few years later reply viraptor 22 hours agoparentprevIsn't cuDNN a much better case for reimplementing than CUDA? It has much more choice in how things actually happen and cuDNN itself chooses different implementations at runtime + does fusing. It seems way more generic and the reimplementation would allow using the best AMD-targeted kernel rather than one the original has. reply ckitching 21 hours agorootparentAMD have \"MIOpen\" which is basically cuDNN-for-AMD. Ish. reply mmis1000 12 hours agorootparentAnd that thing is left for unreleased on windows for almost a whole year for unknown reason. Even though there is activity on github and build fix frequently. There is just no .exe or .msi for you to download. In fact, the rocm for linux is on major 6 release (which includes miopen). But somehow windows is still on major 5 (don't have miopen) for almost a whole year. It almost make me wonder. Is there a shady trade somewhere to ask amd never release sdk for Windows to hike the price of nvidia card higher? Why they keep developing these without release it at all? reply anigbrowl 21 hours agoparentprevGiven AMDs prior lack of interest I'll take whatever options there are. My daily driver has a Vega 10 GPU and it's been quite frustrating not to be able to easily leverage it for doing basic ML tasks, to the point that I've been looking at buying an external nvidia GPU instead just to try out some of the popular Python libraries. reply consf 7 hours agoparentprevThe legal, technical and strategic challenges make it a less attractive option reply dietr1ch 22 hours agoparentprevHow's this situation different than the one around Java, Sun/Oracle and Google? reply dboreham 22 hours agorootparentThe judge might not be a coder next time. reply viraptor 22 hours agorootparentThe US law is highly dependent on precedents. The Google-Oracle case has set one fortunately, so anything following it won't start from scratch. Fortunately we may not need a closer judge. reply jjk166 21 hours agorootparentGoogle-Oracle side stepped the issue of API copyrightability by saying Google's particular implementation would fall under fair use. Whether APIs are copyrightable remains an open question. reply dylan604 21 hours agorootparentprevUntil you get an activist court reply DeepYogurt 23 hours agoparentprevYa, honestly better to leave that to third parties who can dedicate themselves to it and maybe offer support or whatever. Let AMD work on good first party support first. reply neutrinobro 17 hours agoparentprevCries in OpenCL reply koolala 20 hours agoparentprevCUDA v1...CUDA v2... CUDA v... CUDA isnt commonly assosiated with a version number... reply Uehreka 20 hours agorootparent…yes it is? https://developer.nvidia.com/cuda-toolkit-archive reply ladberg 23 hours agoprevI don't really see how any code that depends heavily on the underlying hardware can \"just work\" on AMD. Most serious CUDA code is aware of register file and shared memory sizes, wgmma instructions, optimal tensor core memory & register layouts, tensor memory accelerator instructions, etc... Presumably that stuff doesn't \"just work\" but they don't want to mention it? reply lmeyerov 22 hours agoparentSort of A lot of our hw-aware bits are parameterized where we fill in constants based on the available hw . Doable to port, same as we do whenever new Nvidia architectures come out. But yeah, we have tricky bits that inline PTX, and.. that will be more annoying to redo. reply Retr0id 22 hours agorootparent> SCALE accepts CUDA programs as-is. [...] This is true even if your program uses inline PTX asm reply lmeyerov 22 hours agorootparentOh that will be interesting to understand, as PTX gets to more about trickier hw-arch-specific phenomena that diff brands disagree on, like memory models. Neat! reply lmeyerov 22 hours agorootparentLooks like the PTX translation is via another project ZLUDA, though how they bridge the differences in memory/consistency/etc models safely remains unclear to me... reply ckitching 21 hours agorootparentHi! Spectral engineer here! SCALE does not use any part of ZLUDA. We have modified the clang frontend to convert inline PTX asm block to LLVM IR. To put in a less compiler-engineer-ey way: for any given block of PTX, there exists a hypothetical sequence of C++/CUDA code you could have written to achieve the same effect, but on AMD (perhaps using funky __builtin_... functions if the code includes shuffles/ballots/other-weird-gpu-stuff). Our compiler effectively converts the PTX into that hypothetical C++. Regarding memory consistency etc.: NVIDIA document the \"CUDA memory consistency model\" extremely thoroughly, and likewise, the consistency guarantees for PTX. It is therefore sufficient to ensure that we use operations at least as synchronising as those called for in the documented semantics of the language (be it CUDA or PTX, for each operation). Differing consistency _between architectures_ is the AMDGPU backend's problem. reply lmeyerov 18 hours agorootparentAh I was reading the 'deeper dive' section on my phone and missed it was a comparison, not a warning, thank you I'm curious how something like this example would translate: === Mapping lower-level ptx patterns to higher-level AMD constructs like __ballot, and knowing it's safe ``` #ifdef INLINEPTX inline uint ptx_thread_vote(float rSq, float rCritSq) { uint result = 0; asm(\"{\\t\" \".reg .pred cond, out;\\t\" \"setp.ge.f32 cond, %1, %2;\\t\" \"vote.sync.all.pred out, cond, 0xffffffff;\\t\" \"selp.u32 %0, 1, 0, out;\\t\" \"}\\t\" : \"=r\"(result) : \"f\"(rSq), \"f\"(rCritSq)); return result; } #endif ``` === Again, I'm guessing there might be an equiv simpler program involving AMD's __ballot, but I'm unsure of the true equivalence wrt safety, and it seems like a tricky rewrite as it needs to (afaict) decompile to recover the higher-level abstraction. Normally it's easier to compile down or sideways (translate), and it's not clear to me these primitives are 1:1 for safely doing so. === FWIW, this is all pretty cool. We stay away from PTX -- most of our app code is higher-level, whether RAPIDS (GPU dataframes, GPU ML, etc libs), minimal cuda, and minimal opencl, with only small traces of inline ptx. So more realistically, if we had the motivation, we'd likely explore just #ifdef'ing it with something predictable. reply ckitching 17 hours agorootparentI compiled your function with SCALE for gfx1030: .p2align 2 ; -- Begin function _Z15ptx_thread_voteff .type _Z15ptx_thread_voteff,@function _Z15ptx_thread_voteff: ; @_Z15ptx_thread_voteff ; %bb.0: ; %entry s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0) s_waitcnt_vscnt null, 0x0 v_cmp_ge_f32_e32 vcc_lo, v0, v1 s_cmp_eq_u32 vcc_lo, -1 s_cselect_b32 s4, -1, 0 v_cndmask_b32_e64 v0, 0, 1, s4 s_setpc_b64 s[30:31] .Lfunc_end1: .size _Z15ptx_thread_voteff, .Lfunc_end1-_Z15ptx_thread_voteff; -- End function What were the safety concerns you had? This code seems to be something like `return __all_sync(rSq >= rCritSq) ? 1 : 0`, right? reply lmeyerov 16 hours agorootparentIt's supposed to be waiting for all threads to vote I'm not familiar with AMD enough to know if additional synchronization is needed. ChatGPT recommended adding barriers beyond what that gave, but again, I'm not familiar with AMD commands. reply ckitching 16 hours agorootparentIndeed, no extra synchronisation is needed here due to the nature of the hardware (threads in a warp can't get out of sync with each other). Even on NVIDIA, you could've written this without the asm a discussed above! reply lmeyerov 13 hours agorootparentYeah I think, after this snippet was written, cuda added __all_sync as an intrinsic. The divergent code before this was plain-ish cuda, and this snippet ensures they wait on the comparison vote before recurring. So in the AMD version, the compiler correctly realized the synchronization was on the comparison, so adds the AMD version right before it. That seems like a straightforward transform here. It'd be interesting to understand the comparison of what Nvidia primitives map vs what doesn't. The above is a fairly simple barrier. We avoided PTX as much as we could and wrote it as simply as we could, I'd expect most of our PTX to port for similar reasons. The story is a bit diff for libraries we call. E.g., cudf probably has little compute-tier ptx directly, but will call nvidia libs, and use weird IO bits like cufile / gpu direct storage. reply ladberg 20 hours agorootparentprevJust to check here, if you're given something like the following PTX: wgmma.mma_async.sync.aligned.m64n256k16.f32.bf16.bf16 Do you reverse it back into C++ that does the corresponding FMAs manually instead of using tensor hardware? Or are you able to convert it into a series of __builtin_amdgcn_mfma_CDFmt_MxNxKABFmt instructions that emulate the same behavior? reply ckitching 18 hours agorootparentRather awkwardly, you've asked about an instruction that isn't currently implemented. :D Support for wmma and friends is in development. But in general the answer to your question is yes: we use AMD-specific builtins where available/efficient to make things work. Otherwise many things would be unrepresentble, not just slow! reply saagarjha 18 hours agorootparentWhat do you do when a builtin doesn't exist? reply ckitching 17 hours agorootparentAdd one: it's trivial to add a compiler builtin to carry the instruction from the frontend to the backend if an instruction exists and the backend knows about it. If there's no instruction, either, you can write a C++ function to replicate the behaviour and codegen a call to it. Since the PTX blocks are expanded during initial IR generation, it all inlines nicely by the end. Of course, such software emulation is potentially suboptimal (depends on the situation). reply Moldoteck 11 hours agoparentprevit's a speculation, but I think it's similar with processors = nobody guarantees the code will run the way you set it up. You may want to use some specific register but if the processor will think it has another register that can fulfill the task, it'll use that but tell you that your code is executed as expected. Maybe the internal gpu processor of amd can sufficiently simulate the behavior of nvidia hardware so that higher abstractions will be unaware that something different is happening under the hood reply consf 7 hours agoparentprevIt involves significant challenges reply acheong08 23 hours agoprevImpressive if true. Unfortunately not open source and scarce on exact details on how it works Edit: not sure why I just sort of expect projects to be open source or at least source available these days. reply TaylorAlexander 22 hours agoparentMakes sense to expect this kind of thing to be open source. The whole point of providing improved compatibility is to make people’s lives easier, and open source is usually an important feature to ensure wide compatibility. It also means projects can live on after the creators move to other things, people can submit patches for important features or bug fixes, and generally makes the system much more useful. reply dylan604 21 hours agorootparentI don't find it wrong for someone to attempt to make money back on their time and experience of doing the work. I don't mind people that offer that back as open source either. However, I do have a problem of people expecting everything to be open/free, especially those that then go on a crusade chastising those that do try to make money. reply TaylorAlexander 20 hours agorootparentI'm really trying to keep this about the engineering features of a system rather than moral judgments. Open source systems are simply more flexible and adaptable than proprietary systems, which have their own benefits. In today's world, the engineering value of open source systems is becoming so important that people are looking for other ways to provide for the developers creating these systems. It can be surprising when a project creator builds something in an area that is usually all open source, but they choose a proprietary path. Just look at the problems created by NVIDIA for their use of proprietary software in CUDA and their GPUs. This software is an attempt to fix issues created by proprietary software with another piece of proprietary software, which is if nothing else an interesting decision. reply dylan604 20 hours agorootparentUNIX wasn't free. Windows wasn't free. It wasn't until some knucklehead came along and did something abnormal and gave away their thing. Bakers don't give away their goods. Mechanics don't typically repair things for free. Builders don't build things for free. Gas stations don't give away gas. Why do we think all software should be free, and then think that those that don't give it away are the abnormal ones? reply dTal 18 hours agorootparentBecause software is information. It is closer to a scientific paper than a loaf of bread, and I do expect those to be free. I do not expect scientists to work for free, but the marginal cost of copying their output is 0 and the social benefit is huge. Free software, like open science, clearly has something going for it pragmatically. The developer hours put into it have paid for themselves magnitudes of times over. Megacorps hire people to work on free software. If you can't see the value, that's a you problem. reply voidUpdate 10 hours agorootparentIf all software was free and made no money, how could developers pay their bills? reply TaylorAlexander 9 hours agorootparentFree software is so important to society that I believe the most reasonable solution is to provide for all people without their need to work for survival. Automate as much as possible such that work is not compulsory, and enough people simply want something to do (and possibly additional pay depending on how the system is arranged) that everything that needs to get done by people does get done. For now that is fiction, but so is \"if all software was free\". I do think though that both would lead to a faster rate of innovation in society versus one where critical information is withheld from society to pay someone's rent and food bills. reply einpoklum 7 hours agorootparentprevMost software is free and makes no money - and that has always been the case. There are some very popular and widely-used non-free systems, but most software isn't that, and its developers still pay the bills. This is somewhat analogous to music or books/literature. Most composers and performers and authors make no money from people copying and sharing their works. Some pay the bills working professionally for entities who want their product enough to pay for it; some do other things in life. Some indeed give up their work on music because they can't afford to not do more gainful work. And still, neither music nor books go away as copying them gets closer to being free. reply voidUpdate 7 hours agorootparentIf my current employer can't make any money from the code we write, then it would collapse faster than a soufflé taken out of the oven too early, and I would be out of a job reply acuozzo 14 hours agorootparentprev> the social benefit is huge It will be interesting to see if this is the case in the long run, assuming \"huge\" has a positive connotation in your post, of course. If AGI comes to pass and it winds up being a net negative for humanity, then the ethics of any practice which involves freely distributing information that can be endlessly copied for very little cost must be reevaluated. reply TaylorAlexander 9 hours agorootparent> If AGI comes to pass Increasingly, I am not putting much weight in any predictions about whether this will happen in the way we think it will, or what it could possibly mean. We might as well be talking about the rapture. reply talldayo 19 hours agorootparentprev> Why do we think all software should be free Why do people return Windows laptops when they have to pay for a Windows License Activation? Because every single OEM pays for it; you don't expect to buy Windows because it is a failed B2C business model. Nobody wants it. Same goes for proprietary UNIX, and people wish it was the case for Nvidia drivers. I own CUDA hardware and lament the fact that cross-industry GPGPU died so FAANG could sell licensed AI SDKs. The only thing stopping AI from being \"free\" is the limitations OEMs impose on their hardware. > that those that don't give it away are the abnormal ones? They are. Admit it; the internet is the new normal, if your software isn't as \"free\" as opening a website, you're weird. If I have to pay to access your little forum, I won't use it. If I have to buy your app to see what it's like, I'll never know what you're offering. Part of what makes Nvidia's business model so successful is that they do \"give away\" CUDA to anyone that owns their hardware. There is no developer fee or mandatory licensing cost, it is plug-and-play with the hardware. Same goes for OpenAI, they'd have never succeeded if you had to buy \"the ChatGPT App\" from your App Store. reply dylan604 18 hours agorootparent> Why do people return Windows laptops when they have to pay for a Windows License Activation? The internet echo chamber strikes again. Exactly how many people are actually doing this? Not many, and those that are all hangout together. The rest of the world just blindly goes about their day using Windows while surfing the web using Chrome. Sometimes, it's a good thing to get outside your bubble. It's a big world out there, and not everybody sees the world as you do reply talldayo 18 hours agorootparent> The rest of the world just blindly goes about their day using Windows while surfing the web using Chrome. Paying for Windows? I think you missed my point. If your computer doesn't ship with an OS, paid or otherwise, people think it's a glitch. The average consumer will sooner return their laptop before they buy a license of Windows, create an Install Media from their old device and flash the new hardware with a purchased license. They'll get a Chromebook instead, people don't buy Windows today. The internet has conditioned the majority of modern technology users to reject and habitually avoid non-free experiences. Ad-enabled free platforms and their pervasive success is all the evidence you need. Commercial software as it existed 20 or 30 years ago is a dead business. Free reigns supreme. reply dylan604 16 hours agorootparentWho/where/how does someone buy a laptop without an OS? I'm just not able to follow down this hypothetical path that you are insisting on blazing reply hamilyon2 12 hours agorootparentThat is kind of his point. You don't, Windows is bundled with laptop. It is not that I agree with his points. Windows for example isn't open source in remotest sense reply dylan604 5 hours agorootparentDell offers laptops with a version of Linux preinstalled and supports them. System76, Lenovo, Purism as well to name a few. Apple also sells laptops without Windows on them. There are actually quite a few options that do this. If you don't want Windows, we have options now. Yes, historically, it was Windows or Apple's OS, but that's no longer true and not recognizing that just makes you look like you're pushing a false narrative on the situation for what purpose only you know. reply alt227 11 hours agorootparentprev> Commercial software as it existed 20 or 30 years ago is a dead business. Free reigns supreme. What nonsense. Go into any business and you will find every single piece of software they use is bought and paid for with bells on. The 'Free World' you speak of is only there to get you, an individual, used to using the software so that businesses are made to purchase it. In the old days we called this 'demo' or 'shareware'. Now its 'free' or 'personal' tier subscription. Go and ask any designer if their copy of Adobe Creative Cloud, 3d studio Max, or AutoCAD is free. Any office worker if Micsrosoft Office(including Teams and Sharedpoint etc) or even google docs for business. Majority of developers are running paid versions of Jetbrains. Running an online shop? Chances are you are paying for shopify software, or something like Zoho to manage your customers and orders. 'Free' as you put it is very much only in the online individual consumer world, a very small part of the software world. The commercial software market is more alive and expensive than it has ever been. reply TaylorAlexander 18 hours agorootparentprev> Bakers don't give away their goods. Mechanics don't typically repair things for free. Builders don't build things for free. Gas stations don't give away gas. These all have the property which is that they are scarce physical goods or services. Software is not scarce (though of course the labor to create it is), so this is a really bad comparison. And again I did not say it should or should not be free, I said there are engineering benefits to open source software and more and more people recognize those benefits and choose to make things free because they see the value and are willing to recognize the tradeoffs. I never said what \"should\" be done. \"Should\" is kind of a nonsense term when used in this way as it hides a lot of assumptions, so I generally do not use it, and notably did not use it in my comment. I want to point out the peculiarity in your rather strong response to a word and concept I never used. I think you are having an argument with imagined people, not a discussion with me. And for what it is worth, I am a robotics engineer and I am designing a completely open source solar powered farming robot designed to be made in a small shop in any city in the world (see my profile), funded by a wealthy robotics entrepreneur who recognizes the value in making this technology available to people all over the world. So I am one of those engineers making this choice, and not someone just asking for things without doing the same of my work. Everything I produce is open source, including person projects and even my personal writing. reply napoleongl 11 hours agorootparentprevOtoh recepies and drawings are commonly available for free. So if you can support yourself the cake and engine repair is free. But if you need support then you can get someone to bake or build for you. reply tempaccount420 23 hours agoparentprevThey might be hoping to be acquired by AMD reply msond 22 hours agoparentprevWe're going to be publishing more details on later blog posts and documentation about how this works and how we've built it. Yes, we're not open source, however our license is very permissive. It's both in the software distribution and viewable online at https://docs.scale-lang.com/licensing/ reply breck 21 hours agorootparentHow about trying _Early_ Source? It's open source with a long delay, but paying users get the latest updates. Make the git repo from \"today - N years\" open source, where N is something like 1 or 2. That way, students can learn on old versions, and when they grow into professionals they can pay for access to the cutting Edge builds. Win win win win ( https://breckyunits.com/earlySource.html) reply msond 21 hours agorootparentWe're still thinking about our approach but this is a nice suggestion, thank you. I'm curious, for what reasons are you interested in the source code yourself? reply atq2119 20 hours agorootparentNot GP, but a guaranteed source availability means users can fix issues themselves in the future if the original provider goes belly-up. reply mindcrime 20 hours agorootparentprevI'm not the person you replied to, and I can't speak for them. But I can say that for myself, and a not small number of other people, it's an ideological issue. I simply do not use software that isn't F/OSS - to the greatest extent that that is possible. For me, I might use a VERY small amount of non F/OSS stuff, but it's very hard to get me to adopt something new if it isn't. Now should you make business decisions based on that? Probably not. But while I don't claim to be a representative sample, I am pretty sure the number of people who share my beliefs in this regard is substantially \"non zero\". shrug reply idonotknowwhy 19 hours agorootparentprevI'm a big fan of opensource for most things but if what you've got actually works, you could probably earn big money selling it. The biggest companies in the world are building / using this sort of thing. Imagine the shift of capital if for example, Intel GPUS suddenly had the same ML software compatibility as Nvidia reply breck 20 hours agorootparentprev> I'm curious, for what reasons are you interested in the source code yourself? I am the founder/editor of PLDB. So I try to do my best to help people \"build the next great programming language\". We clone the git repos of over 1,000 compilers and interpreters and use cloc to determine what languages the people who are building languages are using. The people who build languages obviously are the experts, so how they go so goes the world. We call this measurement \"Foundation Score\". A Foundation Score of 100 means 100 other languages uses this language somehow in their primary implementation. It is utterly dominated by open source languages, and the disparity is only getting more extreme. You can see for yourself here: https://pldb.io/lists/explorer.html#columns=rank~name~id~app... Some that might have become irrelevant have gained a second wind after going open source. But some keep falling further behind. I look at Mathematica, a very powerful and amazing language, and it makes me sad to see so few other language designers using it, and the reason is because its closed source. So they are not doing so hot, and that's a language from one of our world's smartest and most prolific thinkers that's been around for decades. I don't see a way for a new language to catch on nowadays that is not open source. reply msond 19 hours agorootparentVery interesting, thank you for sharing! We do believe in open source software and we do want to move the GPGPU market away from fully closed languages. The future is open for discussion but regardless, the status-quo at the moment is a proprietary and dominant implementation which only supports a single vendor. > I don't see a way for a new language to catch on nowadays that is not open source. I do note that CUDA is itself closed source -- while there's an open source implementation in the LLVM project, it is not as bleeding edge as NVIDIA's own. reply breck 18 hours agorootparent> I do note that CUDA is itself closed source And this is a good point. However, it also has a 17 year head start, and many of those years were spent developing before people realized what a huge market there was. All it will take is one committed genius to create an open source alternative to CUDA to dethrone it. But they would have to have some Mojo (hint hint) to pull that off. reply dheera 21 hours agoparentprevAlso, can I even buy an AMD GPU? I don't see a \"buy now\" button or a PCIe version anywhere here https://www.amd.com/en/products/accelerators/instinct/mi300/... Another big AMD fuckup in my opinion. Nobody is going to drop millions on these things without being able to test them out first. First rule of sales: If you have something for sale, take my money. reply nwiswell 20 hours agorootparent> I don't see a \"buy now\" button or a PCIe version anywhere here \"Buy now\" buttons and online shopping carts are not generally how organizations looking to spend serious money on AI buy their hardware. They have a long list of server hardware partners, and odds are you'd already have an existing relationship with one or more of them, and they'd provide a quote. They even go one step further and show off some of their partners' solutions: https://www.amd.com/en/graphics/servers-instinct-deep-learni... FWIW I believe Supermicro and Exxact actually do have web-based shopping carts these days, so maybe you could skip the quotation and buy directly if you were so motivated? Seems kind of weird at this price point. https://www.exxactcorp.com/Exxact-TS4-185328443-E185328443 reply dheera 20 hours agorootparent... and that's why AMD is losing. They could break the trend and offer a \"buy now\" button instead of offering quotes and coffee chats. It's very likely that will kickstart the software snowball with early adopters. Nobody is going to drop millions on an unproven platform. > Seems kind of weird at this price point. Yeah that $234K server is too much for people to do a trial. It has 8xMI300X GPUs along with a bunch of other shit. Give me a single MI300X GPU in PCIe form factor for $20K and I'd very seriously consider. I'm sure there are many people who would help adapt the ecosystem if they were truly available. reply latchkey 12 hours agorootparent> Give me a single MI300X GPU in PCIe form factor for $20K and I'd very seriously consider. I'm sure there are many people who would help adapt the ecosystem if they were truly available. I know this isn't what you're looking for entirely, but my business, Hot Aisle, is working on making MI300x available for rental. Our pricing isn't too crazy given that the GPU has 192GB and one week minimum isn't too bad. We will add on-demand hourly pricing as soon as we technically can. I'm also pushing hard on Dell and AMD to pre-purchase developer credits on our hardware, that we can then give away to people who want to \"kick the tires\". https://hotaisle.xyz/pricing/ reply nwiswell 19 hours agorootparentprevWhy would you be looking to dip your toe into the AMD ecosystem for the first time using an MI300X? It doesn't make any sense. It's not entry level hardware. reply dheera 19 hours agorootparentTo help fix the ecosystem. It's way more affordable than Nvidia. I'm not looking for entry level hardware. reply nwiswell 19 hours agorootparentYes, that's why you'd choose AMD, I'm saying that you don't enter the ecosystem for the first time by purchasing the absolute cutting edge hardware. As far as I'm aware you can't simply buy an Nvidia B200 PCIe card over the counter, either. reply dheera 18 hours agorootparentI'm not looking to enter the ecosystem, I'm already deep in it and want to fix the AMD problem so that I can build big projects around it and undercut everyone who's using Nvidia. You can purchase H100 and A100 PCIe cards over the counter. They're great for compiling CUDA code, testing code before you launch a multi-node job into a cluster, and for running evaluations. AMD has nothing of the sort, and it's hurting them. I cannot blow 250K on an SMCI server, nor do I have the electricity setup for it. I can blow 20K on a PCIe GPU and start contributing to the ecosystem, or maybe prove out an idea on one GPU before trying to raise millions from a VC to build a more cost-effective datacenter that actually works. reply nwiswell 18 hours agorootparent> AMD has nothing of the sort, and it's hurting them. What are you talking about? Have you looked? https://www.dell.com/en-us/shop/amd-mi210-300w-pcie-64gb-pas... https://www.bitworks.io/product/amd-instinct-mi210-64gb-hbm2... reply shaklee3 14 hours agorootparentprevA 20k GPU will be passively cooled and you'll need a real server for that. Even the old MI210 another poster sent is passive. reply ipsum2 23 hours agoparentprevThey're using Docusaurus[1] for their website, which is most commonly used with open source projects. https://docusaurus.io/docs reply msond 22 hours agorootparentActually, we use mkdocs and the excellent material for mkdocs theme: https://squidfunk.github.io/mkdocs-material/ reply resters 23 hours agoprevThe main cause of Nvidia's crazy valuation is AMD's unwillingness to invest in making its GPUs as useful as Nvidia's for ML. Maybe AMD fears antitrust action, or maybe there is something about its underlying hardware approach that would limit competitiveness, but the company seems to have left billions of dollars on the table during the crypto mining GPU demand spike and now during the AI boom demand spike. reply karolist 22 hours agoparentI think this could be cultural differences, AMD's software department is underfunded and doing poorly for a long time now. * https://www.levels.fyi/companies/amd/salaries/software-engin... * https://www.levels.fyi/companies/nvidia/salaries/software-en... And it's probably better now. Nvidia was paying much more long before, also their stock growing attracts even more talent. reply 1024core 22 hours agorootparent> I think this could be cultural differences, AMD's software department is underfunded and doing poorly for a long time now. Rumor is that ML engineers (that AMD really needs) are expensive; and AMD doesn't want to give them more money than the rest of the SWEs they have (for pissing off the existing SWEs). So AMD is caught in a bind: can't pay to get top MLE talent and can't just sit by and watch NVDA eat its lunch. reply xboxnolifes 18 hours agorootparent> So AMD is caught in a bind: can't pay to get top MLE talent and can't just sit by and watch NVDA eat its lunch. This isn't being caught in a bind. This is, if true, just making a poor decision. Nothing is really preventing them from paying more for specialized work. reply karolist 22 hours agorootparentprevI find this strange to believe. Every big company has levels, unless your existing L7+ IC is below market, you can just pull L7+ salaried ML engineers with some secret signing bonus like literally everyone else. reply Der_Einzige 20 hours agorootparentThe dirty secret in the tech industry is that most people at AMD or Intel or IBM and historically Nvidia/Oracle (this changed post 2022), were the 2nd-3rd tier tech companies. Staffed heavily by the rejects of the FAANG, they were still happy to have their 100-200K in their MCOL areas, but no free food and a much more boring work culture. Intel's \"great place to work\" corporate propaganda was known as \"great place to leetcode\" while I worked there, as Intel was always seen as a stepping stone before you \"made it\" in a FAANG. Culturally, none of these companies were happy to pay anyone except the tip, top \"distinguished\" engineers more than 300K. AMD seems to be stuck in this mentality, just as IBM is. reply quotemstr 4 hours agorootparent> AMD seems to be stuck in this mentality, just as IBM is. And that's why creative destruction is essential for technological progress. It's common for organizations to get stuck in stable-but-suboptimal social equilibria: everyone knows there's a problem but nobody can fix it. The only way out is to make a new organization and let the old one die. reply mepian 22 hours agorootparentprevAMD recently acquired Silo AI. reply DaoVeles 17 hours agorootparentprevSo nothing has changed since the era of ATI. reply ClassyJacket 20 hours agoparentprevI like to watch YouTube retrospectives on old failed tech companies - LGR has some good ones. When I think of AMD ignoring machine learning, I can't help imagine a future YouTuber's voiceover explaining how this caused their downfall. There's a tendency sometimes to think \"they know what they're doing, they must have good reasons\". And sometimes that's right, and sometimes that's wrong. Perhaps there's some great technical, legal, or economic reason I'm just not aware of. But when you actually look into these things, it's surprising how often the answer is indeed just shortsightedness. They could end up like BlackBerry, Blockbuster, Nokia, and Kodak. I guess it's not quite as severe, since they will still have a market in games and therefore may well continue to exist, but it will still be looked back on as a colossal mistake. Same with Toyota ignoring electric cars. I'm not an investor, but I still have stakes in the sense that Nvidia has no significant competition in the machine learning space, and that sucks. GPU prices are sky high and there's nobody else to turn to if there's something about Nvidia you just don't like or if they decide to screw us. reply daedrdev 20 minutes agorootparentToyota is extremely strong in the hybrid car market, and with ravenous competition for electric cars and slowing demand Toyota may have made the right decision after all reply hedora 4 hours agorootparentprevIn fairness to AMD, they bet on crypto, and nvidia bet on AI. Crypto was the right short term bet. Also, ignoring is a strong word: I’m staring at a littlemapping every PTX instruction to a direct RDNA counterpart or a list of instructions used to emulate it. We plan to publish a compatibility table of which instructons are supported, but a list of the instructions used to produce each PTX instruction is not in general meaningful. The inline PTX handler works by converting the PTX block to LLVM IR at the start of compilation (at the same time the rest of your code gets turned into IR), so it then \"compiles forward\" with the rest of the program. As a result, the actual instructions chosen vary on a csae-by-case basis due to the whims of the optimiser. This design in principle produces better performance than a hypothetical solution that turned PTX asm into AMD asm, because it conveniently eliminates the optimisation barrier an asm block typically represents. Care, of course, is taken to handle the wacky memory consistency concerns that this implies! We're documenting which ones are expected to perform worse than on NVIDIA, though! reply ashvardanian 18 hours agorootparentHave you seen anyone productively using TMA on Nvidia or async instructions on AMD? I’m currently looking at a 60% throughput degradation for 2D inputs on H100: https://github.com/ashvardanian/scaling-democracy/blob/a8092... reply einpoklum 5 hours agorootparentprev> You're right that most people only use a small subset of cuda This is true first and foremost for the host-side API. From my StackOverflow and NVIDIA forums experience - I'm often the first and only person to ask about any number of nooks and crannies of the CUDA Driver API, with issues which nobody seems to have stumbled onto before; or at least - not stumbled and wrote anything in public about it. reply ckitching 2 minutes agorootparentOh yes, we found all kinds of bugs in Nvidia's cuda implementation during this project :D. There's a bunch of pretty obscure functions in the device side apis too: some esoteric math functions, old simd \"intrinsics\" that are mostly irrelevant with modern compilers, etc. reply spfd 18 hours agoprevVery impressive! But I can't help but think if something like this can be done to this extend, I wonder what went wrong/why it's a struggle for OpenCL to unify the two fragmentized communities. While this is very practical and has a significant impact for people who develop GPGPU/AI applications, for the heterogeneous computing community as a whole, relying on/promoting a proprietary interface/API/language to become THE interface to work with different GPUs sounds like bad news. Can someone educate me on why OpenCL seems to be out of scene in the comments/any of the recent discussions related to this topic? reply mschuetz 21 minutes agoparentOpenCL isn't nice to use and lacks tons of quality of life features. I wouldn't use it, even if it was double as fast as CUDA. reply JonChesterfield 3 hours agoparentprevOpencl gives you the subset of capability that a lot of different companies were confident they could implement. That subset turns out to be intensely annoying to program in - it's just the compiler saying no over and over again. Or you can compile as freestanding c++ with clang extensions and it works much like a CPU does. Or you can compile as cuda or openmp and most stuff you write actually turns into code, not a semantic error. Currently cuda holds lead position but it should lose that place because it's horrible to work in (and to a lesser extent because more than one company knows how to make a GPU). Openmp is an interesting alternative - need to be a little careful to get fast code out but lots of things work somewhat intuitively. Personally, I think raw C++ is going to win out and the many heterogeneous languages will ultimately be dropped as basically a bad idea. But time will tell. Opencl looks very DoA. reply vedranm 13 hours agoparentprevIf you are going the \"open standard\" route, SYCL is much more modern than OpenCL and also nicer to work with. reply deliveryboyman 23 hours agoprevWould like to see benchmarks for the applications in the test suite. E.g., how does Cycles compare on AMD vs Nvidia? reply 81 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SCALE by Spectral Compute is a GPGPU (General-Purpose computing on Graphics Processing Units) programming toolkit enabling native compilation of CUDA applications for AMD GPUs without modifying the original CUDA code or build system.",
      "SCALE acts as a drop-in replacement for NVIDIA's nvcc compiler, allowing seamless integration with existing build tools and scripts, and supports various open-source CUDA projects like NVIDIA Thrust and Blender Cycles.",
      "Currently, SCALE supports AMD GPUs such as gfx1030 (Navi 21, RDNA 2.0) and gfx1100 (Navi 31, RDNA 3.0), with ongoing development for additional GPU architectures."
    ],
    "commentSummary": [
      "The discussion centers on the feasibility and implications of running CUDA (Compute Unified Device Architecture) on AMD GPUs, highlighting technical and legal challenges.",
      "Some argue that supporting translation layers for CUDA on AMD GPUs is impractical and suggest focusing on open-source projects like PyTorch instead.",
      "HIP (Heterogeneous-Compute Interface for Portability) is mentioned as a potential low-effort solution for porting CUDA code to AMD GPUs, though it is not entirely zero-effort."
    ],
    "points": 1102,
    "commentCount": 331,
    "retryCount": 0,
    "time": 1721070307
  },
  {
    "id": 40974112,
    "title": "For advertising, Firefox now collects user data by default",
    "originLink": "https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html",
    "originBody": "For advertising: Firefox now collects user data by default Firefox presents itself as the first choice for data protection. But the new version collects data for advertisers by default. Normal or a breach of trust? Save to Pocket listen Print view (Image: Sundry Photography/Shutterstock.com) Jul 15, 2024 at 9:13 pm CEST 5 min. read iX Magazin By Moritz Förster Contents For advertising: Firefox now collects user data by default Our users don't understand this Also technically questionable Data protection as an advertising label This article was originally published in German and has been automatically translated. Firefox 128 is here - and is making headlines not with practical new features, but with a data protection controversy. Specifically, users are accusing the developer Mozilla of nothing less than deliberately deceiving its own users. This is because the new version of Firefox introduces a technology for anonymized measurement of advertising and its performance. What may sound good on paper does not go down well with many users for several reasons: Firstly, Firefox automatically delivers the Privacy-Preserving Attribution (PPA) with the update to the new version, despite the \"experimental\" label. More serious, however, is the fact that Mozilla also activates the feature directly - users must therefore deactivate the PPA manually by opting out. Prerequisite: They are also aware of the PPA introduced behind the scenes. This is precisely where blogger Jonah Aragon's criticism comes in: He believes that Mozilla knows full well that Firefox users would not want such a function. If the situation were different, the developers would have presented the PPA to the public beforehand and given the community time to test it beforehand. It is debatable to what extent this was done – at least Mozilla has had a support entry on what the PPA is for a month now. Our users don't understand this However, Bas Schouten, technical lead for Firefox performance, explains that it would have been difficult to explain a system like PPA. If users are not in a position to make an informed decision, an opt-in does not make sense. Users must therefore be protected from advertising tracking. In any case, new features would constantly be activated without being asked. It is not surprising that such an attitude is not well received: Jonah Aragon accuses Mozilla of seeing itself as the shepherd of an uninformed mass. And according to the developers, they now have to be urged to make the right decision. Yet, Firefox users in particular are the kind of adult users who need to be listened to. In fact, there is a strong suspicion that Mozilla wants to improve its own cash flow with the PPA. However, it is unclear how much money is involved. However, the developer behind the PPA is Anonymous, which Mozilla acquired a few weeks ago. Accordingly, the provider is the middleman between advertising and users. Also technically questionable But how does the PPA actually work? There is an aggregation server between the advertising provider and the users or their data, which anonymizes the information from the individual app browsers. Only then does it make the data available to the participating advertising customers. This means that these providers can no longer draw conclusions about individual users. The process sounds simple and has an obvious weak point – which immediately draws criticism: Firstly, user data is now located on the aggregation server – which therefore leaves the user's own computer in any case. For Mozilla, this server is not part of an advertising network – a view that many users are at least critical of. Aragon even goes so far as to accuse the Firefox developers of a trick: they have simply redefined the advertising network so that it does not belong to the advertising provider. Data protection as an advertising label There is also the question of how much Firefox users will actually trust a browser provider in future that advertises data protection but at the same time resorts to such measures. Because, as the critics are already realizing, only Mozilla's word protects the data collected - technically, according to Aragon, the system could easily be modified in future so that advertising providers would also have access to individual data. And it was precisely these fears that critics expressed when Anonymous was purchased. However, observers could not imagine that Mozilla would now set up an (unnamed) advertising network through the back door. After all, the developer had recently suggested simply collecting less or no data to improve privacy. And now the opposite is happening. To make matters worse, Firefox is the only major competitor to Google Chrome – other browsers are all based on Chromium. In any case, Mozilla's move does not make it any easier for users to make the right decisions for them and their data protection. Ladybird is currently gaining momentum as an independent hope, but is still a long way from being a reliable web browser. (fo) Home Advertisement Softwarelizenzen verwalten, Fehler vermeiden! STARFACE UCC-Lösungen für macOS und iOS Digitale Identitäten schützen Zugriffskontrollen mit Privileged Access Management Mit der Public Cloud zu mehr Nachhaltigkeit Sind APIs das neue Einfallstor für Angreifer? Share this article Shortlink: https://heise.de/-9801345",
    "commentLink": "https://news.ycombinator.com/item?id=40974112",
    "commentBody": "For advertising, Firefox now collects user data by default (heise.de)522 points by firebaze 12 hours agohidepastfavorite460 comments seanhunter 6 hours agoIf you want to disable this, instructions are given here[1] but 1) Hamburger menu -> Settings -> Privacy & Security 2) scroll down to the new section entitled \"Web Site Advertising Preferences\". 3) Make sure the box marked \"Allow web sites to perform privacy-preserving ad measurement\" is not checked. [1] https://support.mozilla.org/en-US/kb/privacy-preserving-attr... reply defulmere 5 hours agoparentThanks for providing these instructions. My usual way to get to some setting in Firefox is to use the search box but it seems that Mozilla is actively hiding this one by excluding it from search, ie if you type \"advertising\" into the settings search box then there are no results. reply noisy_boy 4 hours agorootparentThat is because \"Allow web sites to perform privacy-preserving ad measurement\" doesn't have the word \"advertising\" in it. Granted the current phrasing a bit awkward and may have a certain degree of deliberateness behind it. reply yogeshp 3 hours agorootparentYes, but the section name \"Web Site Advertising Preferences\" has the word Advertising in it. reply marcosdumay 4 hours agorootparentprevIMO, almost all of the Firefox preferences are phrased awkwardly. reply rascul 4 hours agorootparentprevIt seems the search matches on the text of the option, not the section text. The text of the option is \"Allow websites to perform privacy-preserving ad measurement\" and the search for me brings up the option (and of course anything else that matches) when I search for any of that. reply defulmere 4 hours agorootparentAh, thanks for pointing that out. I'd expected a match on the section heading :\\ reply kevincox 5 hours agoparentprevAnd of course this setting doesn't sync to new devices by default. So you need to remember to opt-out on every device. The underlying pref is dom.private-attribution.submission.enabled. I'm going to force this off in my policies. reply blueflow 5 hours agorootparentHow do you sync these settings across devices? I need such a thing, too. reply kevincox 5 hours agorootparentIf you want to use Firefox sync IIUC you can define a new pref services.sync.prefs.sync.dom.private-attribution.submission.enabled. However this has always been flakey in the past for me. (I think maybe the sync prefs themselves don't sync?) Now I install an organizational policy that sets the prefs. I use NixOS to apply this and it looks like this: environment.systemPackages = [(pkgs.firefox.override { extraPrefs = '' lockPref(\"dom.private-attribution.submission.enabled\", false); '' })]; I think this is just creating a `prefs.js` file under the hood, so you should be able to replicate on other systems that you manage. Edit: This is creating a lib/firefox/mozilla.cfg. IDK how exactly this applies to other distros. reply ringer 3 hours agorootparentprevBasically, you need to create a `user.js` file in the root folder of your profile, you can find/open the profile folder using about:profiles or about:support (default path is `~/.mozilla/firefox/${profile-name}/user.js`). You can sync it however you like, e.g. upload it to your dotfiles repo and symlink with stow, etc. The syntax is: user_pref(\"dom.private-attribution.submission.enabled\", false); // Disable Privacy-Preserving Attribution You can find a lot of examples and \"documentation\" on https://github.com/arkenfox/user.js reply Y_Y 3 hours agoparentprevsudo rm $(which firefox) reply eco 11 hours agoprevThe CTO of Mozilla just posted on /r/firefox about this: https://old.reddit.com/r/firefox/comments/1e43w7v/a_word_abo... reply latexr 9 hours agoparent> It’s clear in retrospect that we should have communicated more on this one What isn’t clear, in retrospect or otherwise, is why companies/apps/services need to keep learning this lesson. The user outcry was utterly predictable from even before the first web article was out. The fact that no one with decision power at Mozilla saw it coming is worrying: either they have zero understanding of people’s concerns for privacy or they don’t care. Neither is good. reply Klonoar 9 hours agorootparent> The fact that no one with decision power at Mozilla saw it coming is worrying: either they have zero understanding of people’s concerns for privacy or they don’t care. Or the third option: they feel the tradeoff of HN & co's criticism style is not a big deal in the end. Criticism of Mozilla in general is very warranted right now, but the way(s) in which everyone is doing so just feels very out of touch with the actual situation. ;P They're - by their own words - trying to do something in a privacy preserving way because the ad industry is not going away. They might fuck it up at first, and that's why it's an experiment. It's also possible to disable it, it's not like you're trapped in it. This thread in general feels like it leaves Mozilla no room to experiment or find any form of growth. People want them to be \"just a browser\" but then also expect them to be stewards of the web - and then cry foul when they actually try to find a setup that fits into the current model of the web. reply latexr 8 hours agorootparent> Or the third option: they feel the tradeoff of HN & co's criticism style is not a big deal in the end. That’s the second option: they don’t care. > This thread in general feels like it leaves Mozilla no room to experiment If you you’re going to experiment with something that’s going to cause this amount of backlash (and my criticism is that they didn’t take the obvious reaction into account), you show a dialog on first run that tells you what the feature is, perhaps include a “Learn More” link, and have an option to accept or deny. You can even have the former as the default. And do it in your betas first. Would that still cause some backlash? Possibly. But it would’ve been significantly milder and you would have seen a lot more defence of Mozilla for not doing without asking. Mozilla in particular is frequently pulling crap like this and getting flak for it. They have to constantly apologise and back track. After a while you’d expect they learned something. reply vetinari 5 hours agorootparent> Mozilla in particular is frequently pulling crap like this and getting flak for it. They have to constantly apologise and back track. After a while you’d expect they learned something. Well, they learned: they fuck up, backtrack & apologize (it is free, no real impact, so no worries), and life goes on. reply latexr 5 hours agorootparent> no real impact Apart from a market share that is continuously tending towards zero. reply account42 4 hours agorootparentThat hasn't impacted their salaries so far though, especially that of the leadership. reply hobs 5 hours agorootparentprevLearning implies a future where you don't keep doing the same thing. reply arepublicadoceu 7 hours agorootparentprev> Or the third option: they feel the tradeoff of HN & co's criticism style is not a big deal in the end. Well, right now, with their dwindling market cap, I feel like their only userbase is HN & co's type of user. They repeatedly failed to increase their user base with non privacy conscious adjacent communities. So antagonizing the ONLY folks that go through the trouble of installing a non default browser to have a worse user experience seems like a big brain moment. reply closewith 6 hours agorootparentI wonder what the market share in that segment is? From my experience, startup types almost exclusively use Chrome or Safari. Firefox doesn't even register with most devs. reply bee_rider 6 hours agorootparentprevIt seems somewhat questionable whether or not it is possible to sustain something as complex as Firefox based on users like us. There might not be enough, or enough people willing to pay. They’d be really screwed if Google didn’t give them a good deal. Somewhat wondering if Google just keeps them around to stave off the appearance of being a monopoly. The web seems to have gotten pretty unsustainable in general. Might consider upgrading to Lynx or something like that. reply arepublicadoceu 5 hours agorootparent> It seems somewhat questionable whether or not it is possible to sustain something as complex as Firefox based on users like us. I have this crazy theory that Firefox could be completely sustained by users willing to pay for it. I mean... Mozilla Co definitely couldn't be sustained by users money only, but Firefox could. The only path I can see for a healthy web (if this is even possible right now) is to completely liberate Firefox from Mozilla's shackles and mismanagement. A free and open-source browser should be treated more like a public good, such as a Linux distribution, than a money-making machine. reply consp 8 hours agorootparentprev> the ad industry is not going away That's the start of full blown stockholm syndrome. No data is ever fully anonymous, don't pretend otherwise. So no data should be send at all. reply Klonoar 7 hours agorootparentWhat you call Stockholm syndrome, I call reality. ;P This is an area that we are stuck contending with. Legal solutions are needed here but that path is mired by complex and powerful lobbying. If Mozilla can push for a more private or more protective - even if not fully private or fully protective - then I’d like to see where it goes. reply freeone3000 7 hours agorootparentOr they could just not do this shit. Completely refuse to help your enemies. Actually support their users. reply Barrin92 6 hours agorootparentFirefox has 3% market share. Completely refusing to engage with your enemies only works when you have the actual guns to back that attitude up. If you refuse to engage with the ad industry they just ignore you. Oh and the company that owns a large part of the world's ad industry and owns the browser that has 65% market share also pays like 90% of your bills. I mean, what's step two of your glorious plan to charge fists raised into battle? reply closewith 6 hours agorootparentFirefox playing their game makes Firefox completely redundant. The advertisers get your data either way, so why not use Chrome? Firefox compromising heralds its own irrelevance. reply gjm11 5 hours agorootparent> The advertisers get your data either way, so why not use Chrome? You might believe that the advertisers get less of your data if you use Firefox. Similarly: you might be less likely to have your house burgled if there are locks on the doors and a burglar alarm, even though people with those things still get burgled sometimes. You might be less cold outdoors in winter if you wear a parka, even though it's still cold. You might be less bored if you buy/rent/stream some interesting books, music and movies, even though having those doesn't guarantee never being bored. You might be less likely to lose your next chess game if you practice tactics and learn openings, even though you'll still lose if you play Magnus Carlsen. You might be less likely to have a heart attack or stroke if you take those antihypertensives the doctor prescribed you, even though those are still tragically things that can happen to anyone. Etc., etc., etc. Very few things are absolute and perfect. It's usually a matter of \"less\" versus \"more\". This latest thing gives advertisers more information about me than they would have if Firefox didn't do it. (Unless I turn it off, which in fact I have done.) It doesn't give them very much information about me. I'm pretty sure they would get much more information about me if I switched to using Chrome (e.g., because Firefox supports better adblockers). For the avoidance of doubt, I do think Mozilla should have made more noise about what they were doing, I do think there's a repeated pattern of them putting things into Firefox that their users don't really want and hoping no one will notice[1], I do think that says something bad about how Mozilla is run, and I would be happier if the Firefox project were run by people less inclined to do such things. But none of that means that you might as well use Chrome instead of Firefox, if you happen to value the things that Firefox still does better than Chrome. [1] Actually, I think they know perfectly well that some users will notice, and they've decided it's overall better PR to do the thing quietly, wait for people to complain, and then say \"oh, whoops, we should have been more open about this, we're so sorry and will totally not do the same thing again in six months\". reply closewith 5 hours agorootparent> You might believe that the advertisers get less of your data if you use Firefox. Shortly as Chrome implements Privacy Sandbox, both Chrome and Firefox will support the same levels of advertising tracking. For Chrome, this is a privacy upgrade of sorts, but for Firefox, this is a definite downgrade. As Firefox converges on Chrome in this area, the privacy advantage evaporates. reply gjm11 4 hours agorootparentDoes Chrome do anything equivalent to Firefox's \"Enhanced Tracking Protection\"? Chrome forces extensions to use \"Manifest v3\" rather than \"v2\", which cripples some ad-blockers; in particular, the full version of uBlock Origin will run on Firefox but not on Chrome. (I'm not sure of the details about the v2->v3 migration; maybe that isn't universally true yet. If not, it will be soon.) \"Reduces\" and \"evaporates\" are not the same thing. I see the case for the former, not for the latter. reply hobs 5 hours agorootparentprevI dont believe that, and have no reason to believe that at this point. Any browser that makes me monitor their changes for privacy destruction is basically just chrome with more steps. reply account42 4 hours agorootparentprevActually it's the other way around. As long as Firefox only has a negligible market share, advertisers are not going to care about it enough to work around Firefox-exclusive tracking protection forever. Regulators are also not going to be concerned that Firefox makes certain business models harder because it is insignificant. reply nubinetwork 6 hours agorootparentprevMaybe they would have a better market share if they weren't constantly pulling this shit every two or three years... reply lucianbr 5 hours agorootparentI know it feels right to say that. But really, do you think the majority of people who switched from Firefox to Chrome did it because FF did not address their privacy concerns? Seems ridiculous. However bad FF is, Chrome is much worse. It seems far more likely that the remaining 3% are the few people who care, and therefore, \"pulling this shit\" did not cause the current market share. reply freeone3000 5 hours agorootparentWhich means the people who care about privacy are their CORE market. You do not offend your core market. I’m not a business expert but c’mon. I’m willing to put up with slowness and incompatability when I feel like firefox is on my side. Now? I’m going back to Safari. reply mihaaly 6 hours agorootparentprevThey introduced quite a bit of privacy aware of measures, quite effective ones, so pretending they are stupid is not really beleivable. > the ad industry is not going away But users do. Let them have a great faking love affair with the ad industry. reply Y_Y 2 hours agorootparentprev> People ... expect them to be stewards of the web Do people really expect that? I'm glad they're part of whatwg etc., but I'd much prefer they just made a good browser instead of tooting their own horns about how much good they're doing for society. In the end I think society would have been better off if they'd just focus on good tech like Gecko/Servo and Rust and not bothered with all their side stuff. reply account42 4 hours agorootparentprevYes, Mozilla should not be experimenting on users without explicit consent. Have things really gotten so bad that this is incomprehensible? reply Klonoar 4 hours agorootparentI have already noted in my other comments that I think the desire for opt-in and/or way more notice with how to opt out is a very reasonable take, even if I don't necessarily agree with it. There's no need to imply that people don't comprehend things here. ;P reply phicoh 8 hours agorootparentprevFirefox market share is going down. One reason is that the people who would be promoting Firefox aren't. Personally I feel mostly ashamed to admit I'm using Firefox. In theory Firefox is great. In practice they coming up with new ways to treat their core user base badly. reply Klonoar 8 hours agorootparent> One reason is that the people who would be promoting Firefox aren't. Individual promotion of Firefox worked very well when the browser(s) it was trying to displace were effectively frozen in time. Chrome (et al) and Safari are not those browsers. The average user isn't going to get a markedly different experience by switching to Firefox. reply phicoh 6 hours agorootparentThat is because Mozilla has consistently moved Firefox in the direction of a Chrome clone. When Firefox started is was not a copy of existing browsers. There is no reason it would have to be now. But they have rejected their core users. So now the only option left is a Chrome clone because that is what people are used to. reply zztop44 5 hours agorootparentPeople used to have a dozen different instances of IE6 open. It was a pain to switch between them and it made your computer run slow. Firefox had tabs. And it had AdBlock. Those were things people wanted. But these days, Chrome is plenty good enough for most people. Even if Firefox had a perfect privacy story and focused on their core users’ every whim, I don’t think their market share would grow. reply hobs 5 hours agorootparentWell then they need to close up shop or think of something else, because adding more ad tracking isn't a feature to anyone but predatory advertisers, and they will only keep paying you if users keep showing up. reply zztop44 4 hours agorootparentFor what it’s worth, I agree. Adding more tracking definitely isn’t going to help. But I don’t think there are any easy solutions. I definitely don’t envy the people in charge of Firefox’s product strategy. reply Klonoar 5 hours agorootparentprevEven if it was a credible idea, how exactly do you think that Firefox - the browser that the minute anything changes, the internet blows up over - would significantly alter their product in a way to differentiate themselves from Chrome? This isn’t even getting into base level stuff like available engineering resources, or the scenarios where the other vendors often control or have deals to give them favorable distribution on platforms. This isn’t the IE6 era. It’s a significantly different and harder problem. reply yjftsjthsd-h 4 hours agorootparent> Even if it was a credible idea, how exactly do you think that Firefox - the browser that the minute anything changes, the internet blows up over - would significantly alter their product in a way to differentiate themselves from Chrome? You're presenting it as though any change would be met with hostility, but the alternative is that they're only met with hostility because they keep making changes that hurt the users. A little while ago they announced that they were working on properly supporting vertical tabs and tab groups; that wasn't met with any hostility. Of course, in the same announcement they said they were planning to dumb down the rest of the interface even more, which was. But the point stands; they can get a positive reaction by making changes their users actually like, they just don't do that as often as they do the other thing. reply Karunamon 5 hours agorootparentprevFor one, not throwing out their only differentiated advantage versus Chrome. For two, not taking the option that removes user control and customization whenever there is an option to do so. They could have been the privacy-focused browser, but it is still full of crap like this and various bits of undisclosed telemetry. There would be value in being the only browser to actually stop when users tell them no. But they seem incapable of listening. reply Klonoar 3 hours agorootparent> They could have been the privacy-focused browser I don't see how trying to find a privacy-preserving way of dealing with the ad conundrum makes them not a privacy-focused browser/company. You'd need to otherwise cite something re: undisclosed telemetry, considering the project is open source... so I'm not sure how exactly it'd be undisclosed. reply cesarb 4 hours agorootparentprev> When Firefox started is was not a copy of existing browsers. IIRC, when Firefox started, it was very similar to the full Mozilla Suite with some features removed (which is not surprising, since it started as a Mozilla Suite derivative and they shared a lot of code). It has a long lineage going back to the old-school Netscape Navigator. reply pdimitar 8 hours agorootparentprev> It's also possible to disable it, it's not like you're trapped in it. Or so they say, in order to make people be OK with it. They might play the waiting game and in a year or two will make the setting not do anything and still collect / send data, hoping that by that time people have forgotten. reply closewith 7 hours agorootparentThey don't need to. Defaults are retained by the majority of users, who will never know about this change. reply Klonoar 8 hours agorootparentprevThere is so much hypothetical-borderline-conspiracy-theory packed in to this single comment that I cannot find a charitable response. I'd be fine to continue the discussion if you can find a way to engage without assuming that the people who build one of the last checks on the open internet are somehow trying to maliciously invade your privacy. reply closewith 7 hours agorootparentThe days of Mozilla having earned the benefit of the doubt are long gone for most people. The person you replied to made a reasonable point and your response reads as defensive and dismissive. Do you have an interest in Mozilla we should know about? reply Klonoar 7 hours agorootparentEh, I don’t think my comment is defensive. I also could’ve just ignored the comment. I explained to them that I’m open to discussing but there’s nothing to be gained when the comment starts off in conspiracy theory. It’s an open source project, people will 100% notice if they tried to do what the parent comment is suggesting. reply closewith 7 hours agorootparentIt certainly reads defensively. > It’s an open source project, people will 100% notice if they tried to do what the parent comment is suggesting. No-one thinks they'll lie about it. They'd announce it quietly just like this change, letting the fuss blow over. The average user would never even realise and Firefox would continue on its journey towards user hostility. reply Klonoar 6 hours agorootparentYou’re certainly welcome to read it however you’d like. OP specifically said “make the setting do nothing while still collecting the data”. I don’t know about you, but a setting that acts like that would be akin to lying. The comment chain is pretty clear here IMO. reply closewith 5 hours agorootparent> OP specifically said “make the setting do nothing while still collecting the data”. I don’t know about you, but a setting that acts like that would be akin to lying. Well, that is what Firefox did here. They created a new feature, defaulted it to on, in direct contradiction to user choices. We know this because this Web Site Advertising feature defaults to on even where the user has the strictest level of tracking protection enabled and even when the DNT option is selected. Even so, Mozilla has decided that this form of tracking is not covered by those clear signals of user intent. So why not believe that Mozilla will do this again. Deprecate existing tracking choices and enable Web Site Advertising tracking for everyone. Like this change, it would be announced and decried and ultimately used by the majority of users who don't follow browser changelogs. What will happen is that privacy advocates like me will recommend not to use Firefox, as it's functionally equivalant to Chrome is this respect and far less supported, and Firefox will continue to die. This pains me as a former contributor and advocate, but it's almost inevitable now unless a privacy-focused non-profit can fork Firefox and leave Mozilla to it's decline. I would even pay for a Firefox fork, but I will never donate to or purchase again from Mozilla. reply Klonoar 3 hours agorootparent> Well, that is what Firefox did here. No, let's be very clear here: what Mozilla/Firefox did here was default users in to a setting without good notice on how to opt out. This is different from what was said in this thread, which is making the setting do nothing while still collecting the data. If you disable the setting/opt out, then the data isn't being collected. reply closewith 2 hours agorootparent> No, let's be very clear here: what Mozilla/Firefox did here was default users in to a setting without good notice on how to opt out. That's a framing so charitable to Mozilla that it is untrue. Again, do you have an interest you should be declaring in this conversation? > This is different from what was said in this thread, which is making the setting do nothing while still collecting the data. No, it's not. It ignores the Strict Tracking Protection and DNT settings and opts in users to tracking. It's absolutely identical to possibility posited by the other commenter. For all your pontificating above about other people's comments, it seems the only person commenting in bad faith is you. reply marky1991 3 hours agorootparentprevI don't see how it's conspiracy theory. Firefox has done exactly this over and over again. (The latest example that annoyed me: browser.proton.enabled =false) As a user of Firefox, I feel like I'm in a constant battle with Mozilla/FF to disable every new bad idea they have. Every time I'm forced into a surprise update I didn't ask for/try_to_install, something gets worse. This isn't an unusual state for commercial software, but Firefox is supposed to try to not be commercial. reply yencabulator 3 hours agorootparentprevIf Firefox was the last check on the open internet, Librewolf wouldn't have to exist. reply pdimitar 7 hours agorootparentprevFirefox is dependent on Google for ages, that should tell you all you need to know about \"conspiracies\". I am not interested in a discussion with a person who gives the benefit of the doubt of a company who has clearly not only made a Faustian deal but is now looking to expand partnership with the people that nobody wants tracking their machines and activities. Because as we both know, in the entire history of humanity there were NEVER any conspiracies when there is money to be made, right? Wink wink. reply Klonoar 6 hours agorootparentWell, no, that doesn’t tell us anything about conspiracies. That’s just Mozilla getting money from Google. You can argue that it’s problematic from the stance of Google using Firefox to argue they don’t hold a monopoly - and I’d agree with you there. That deal with Google isn’t enough to leap to the conspiracy theory here though. The ad industry isn’t going away, Mozilla seems to want to try to make it work for all parties. If you want to let perfect be the enemy of good, though, go for it. shrug reply thoroughburro 5 hours agorootparent> That’s just Mozilla getting money from Google. “That’s just politicians getting money from financial criminals.” “That’s just police getting money from organized violence.” “This is fine.” reply beeboobaa3 7 hours agorootparentprevYou're new to the internet and bigtech, huh? If not, what rock have you been living under? reply beeboobaa3 7 hours agorootparentprev> trying to do something in a privacy preserving way because the ad industry is not going away they're bending for the ad industry because they want their money. they could also just keep blocking their tracking and call it a day. reply chiefalchemist 8 hours agorootparentprev> This thread in general feels like it leaves Mozilla no room to experiment or find any form of growth. Mozilla is welcome to experiment. The issue here is: - The default opts the client in instead of the client making that choice to be a Guinea pig in the experiment - I get emails almost weekly that amount to Mozilla playing the role of internet privacy police. They *are* well aware of the rights and wrongs. Are they going to call out themselves? - As for growth? How about paid pro-privacy email hosting? And a suite of applications (a la Google docs)? Advertising might not be going away but there are still opportunities that align with Mozilla's ideals and brand... And they're too busy being hypocritical internet police??? reply closewith 6 hours agorootparentI think the worst part of the funding equation is that had Mozilla stayed on mission and invested it's Google fees wisely, Firefox development could have been indefinitely funded. Instead, we have had Mozilla sprawling in numerous directions secondary to the browser and failing in nearly all of them. reply hobs 5 hours agorootparentThat is the problem, people want to run a modern corporation with its tentacles always reaching and growing instead of focusing on a core business proposition that they can win at. If you dont grow at double digit percentages year of year, are you even trying? reply account42 3 hours agorootparentMozilla Corporation was a mistake. reply Klonoar 8 hours agorootparentprev> The default opts the client in instead of the client making that choice to be a Guinea pig in the experiment I think this is a reasonable critique, even if I personally don't find it a big deal. If it's privacy preserving, I don't necessarily give a shit if it's defaulted on - especially if there's a way to disable it. (IMO, defaulting it on and then widely announcing how to disable it is what they should have done, and their bungled communications on this is biting them) > I get emails almost weekly that amount to Mozilla playing the role of internet privacy police. They are* well aware of the rights and wrongs. Are they going to call out themselves?* Why would they call themselves out here...? They have stated, very bluntly, that they are trying to do something in a privacy preserving way. They are acting in line with their stated intentions/role/etc. > As for growth? How about paid pro-privacy email hosting? And a suite of applications (a la Google docs)? Advertising might not be going away but there are still opportunities that align with Mozilla's ideals and brand... And they're too busy being hypocritical internet police??? Those are wholly separate business ventures, whereas dealing with the advertising behemoth is an unfortunate part of the browser ecosystem today. Someone, somewhere, is going to have to contend with this - and Mozilla is somewhat uniquely positioned to explore here. If you think Apple or Google are going to do it without perverse incentives, then I don't know what to tell you. reply et1337 5 hours agorootparentWe all lost our minds when Google tried to pull their privacy-preserving Federated Learning of Cohorts thing. I expect an even bigger outcry when Firefox, whose entire brand and reason for existence is privacy, quietly tries to do the same thing. reply cqqxo4zV46cp 8 hours agorootparentprevnext [7 more] [flagged] latexr 8 hours agorootparentThat is incredibly reductive and entirely misses the scope of the issue. It isn’t just HN. In case you’ve missed it, the article is not an HN page but a separate website. The comment I replied to linked to Reddit. It’s all over Mastodon. I’ve seen other blogs and publications commenting on it too. Yes, of course a large number of people won’t talk about this in six weeks, let alone six months. On the other hand you’ll have ex-hardcore fans complaining about it for over six years. I still see people talking about the Mr Robot debacle and the other crap Mozilla has pulled to this day. If anything, Mozilla is more susceptible to this backlash than the average tech company. Regular computer users don’t give a rat’s ass about Firefox. The people Mozilla needs to convince are exactly the ones they keep alienating. reply jeltz 8 hours agorootparentprevThey might be correct but this thinking is also how Mozilla lost most of their German market share due to Cliqz. They assumed people would not care but they did. Also this is trending on /r/all on Reddit right now. https://www.reddit.com/r/pcmasterrace/comments/1e45mih/firef... reply pdpi 8 hours agorootparentprevIf this were, say, Adobe, I’d agree with you. HN as a community doesn’t have much clout in the design or video space. This is Mozilla we’re talking about, though. HN is exactly the sort of audience they need on their side. That bunch of nerds is the same group they relied upon to evangelise for them during the IE era. reply account42 3 hours agorootparentMozilla might just have decided that that's no longer the case: that their funding from Google does not depend on nerds advocating for the browser. That is either they came to the conclusion that Google will continue to fund them even if the market share continues to fall or they have decided that the end is inevitable and are just trying to milk the cow for all that she's got. reply closewith 6 hours agorootparentprevThis comment shows such a lack of context of the history of Firefox that I wonder if it's trolling? Firefox exists and reached its peak because of the people that idealogically cared about the Web and interoperable,security, privacy, etc who contributed to and advocated for Firefox. reply ahartmetz 8 hours agorootparentprevThis bunch of nerds creates software, including for the web, and sometimes there's an option to test it on Firefox or not. Nerds also recommend browsers to friends and family. HN is also not the only bunch of nerds like that. reply account42 5 hours agorootparentprevCompanies know this but they don't care because there are rarely any consequences that cannot easily be mitigated with cheap PR tactics. Even now you are responding to a PR statement that is trying to reframe the issue as users simply not understanding what Mozilla is doing when in reality Mozilla knows full well that this goes agains the explicit wishes of a large part of their userbase but have chosen to enable this anyway. This isn't a communitcation issue. This is a fundamental \"who does Mozilla serve\" issue. reply ta988 6 hours agorootparentprevThey don't care. It is not the first time, always the same excuse and blame the user to not be intelligent enough to understand (this is what communicated more means in their broken by profit minds). reply jeltz 9 hours agorootparentprevEspecially since this is very similar to what happened with Cliqz and that there likely are many at Mozilla who were around when that happened too. And the Cliqz scandal hurt Mozilla's market share a lot in Germany. reply mihaaly 6 hours agorootparentprevActing stupid and being uninformed, clueless, incompetent CTO/CEO/CXX is less prone to lawsuits than admitting intent of harm. reply david_draco 8 hours agorootparentprev> What isn’t clear, in retrospect or otherwise, is why companies/apps/services need to keep learning this lesson. They are trying to find a funding model that makes them independent from Google. - Building a fast, privacy-oriented browser that keeps up with web standards and fixes security bugs takes people, organisation and therefore money. Yes, much more than that CEO salary. - No one wants to buy for a browser. - No one wants to pay a subscription fee for a browser. So you are left with ads. Mozilla is trying to find a balance there between privacy and ads with a clearing house approach. People who hate ads out of principle scream. How should browser development be funded? reply nerdponx 7 hours agorootparentIf there is one single piece of software I will post a recurring fee for, it's a web browser. Look at Kagi and Orion for example (I pay for both). It's like email. You need to get people over the mental hump. But then if you offer a good product, buyers will be happy they got over it. reply Kalium 6 hours agorootparenthttps://foundation.mozilla.org/en/donate/ Here you go! Insert credit card here. reply yupyupyups 5 hours agorootparentCEO of Mozilla earns $3m/year. https://news.ycombinator.com/item?id=30665913 If the Mozilla foundation creates a donation button with the condition that the money goes solely to browser development (no CEO salary or political activism) I will donate. reply ajdude 5 hours agorootparentprevExcept that it's well-known fact that none of your donations for the foundation ever go anywhere near Firefox itself, since Firefox is spun off as their commercial sector to accept Google's money reply nerdponx 3 hours agorootparentprevI donate to them as well, but donating broadly to the Mozilla foundation isn't the same as selling the web browser in exchange for money. reply nick__m 5 hours agorootparentprevthat doesn't pay for Firefox... reply latexr 7 hours agorootparentprev> How should browser development be funded? One of the most common Mozilla complaints I see on the web is that you cannot fund Firefox development directly. People want to give money to it, but cannot. Which makes sense, I guess. Anecdotally, Mozilla is by far the company I know with the most vocal users that get completely ignored. reply ninjin 7 hours agorootparentCome now, let us be realistic, no one and nothing funds browser development other than advertisement: https://ladybird.org/#sponsors reply latexr 7 hours agorootparentThat is provably false. Safari isn’t funded by advertising, neither is Orion, or LibreWolf, or any number of other smaller open-source browsers. reply rascul 4 hours agorootparentprevNone of those appear to be advertising companies. reply lolinder 6 hours agorootparentprevMozilla has tried experiment after experiment to try to earn money. Let's try forcing Pocket down people's throats. Let's automatically install Mr Robot. You know what people will love? Full-page ads for a VPN! No one has seen enough VPN ads! The one funding model they haven't experimented with at all is actually asking people to pay for Firefox. Donations or subscription, they haven't even tried it once. And yet people will over and over again insist that that would never work. Doesn't that strike you as odd? They're willing to flail about trying thing after thing after thing that their users hate and yell about and they end up having to pull back, they're willing to burn credibility over and over again, but the one funding model that their users keep telling them they want they refuse to even try on the grounds it would never work. reply pndy 5 hours agorootparent> asking people to pay for Firefox I do expect that's the next step at Mozilla - locking features behind paywall with some premium plan. Cloud sync probably will fall into that basket. And if that eventually won't work - they'll surely announce it's time to \"sunset\". reply nerdponx 3 hours agorootparentPersonally, I think that's what they should've been doing all along. If it doesn't work at this point, it's because it's too late, and they've already burned enough of their credibility that people don't want to give them money anymore. reply Workaccount2 5 hours agorootparentprev>And yet people will over and over again insist that that would never work. Because it won't. And there is mountains of data to back it up. People will not pay if they don't have to. Some people will, for sure, but to get those some people to carry the weight of \"all the people\" is totally untenable. Besides, Chrome is \"free\". reply nerdponx 3 hours agorootparentAt the same time, even a tiny bit of friction is enough to get people over the mental hump of paying for something. They could easily gate off certain features behind a paid build, so either you pay or compile it yourself from source. Downstream packagers could of course do whatever they want (eg Debian). However, it creates a minor amount of friction for a relatively large fraction of the user base, and moreover sets the baseline expectation that this is not really \"free as in beer\", even though it remains \"free as in freedom\". See also: Sublime Text, which, despite being closed-source, is 100% free-as-in-beer to use in perpetuity, and yet somehow they make enough money To not only continue development, but even start developing other products (Sublime Merge), even as their brand recognition wanes and their competitive advantage shrinks. reply lolinder 5 hours agorootparentprevIt doesn't have to pay for the entire Mozilla organization, it just has to bring in more money than the random other stuff they've tried. That's not a very high bar to cross. reply anon264523645 4 hours agorootparentprevWikipedia has mountains of $$$ to back up that a lot of people do pay, even when it's not necessary at all. https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_C... reply Kalium 6 hours agorootparentprevAt one point Mozilla was literally selling a VPN subscription. That point is now - you can go buy one today. https://support.mozilla.org/en-US/kb/what-mozilla-vpn-and-ho... You can even donate money today: https://foundation.mozilla.org/en/donate/ From memory, Mozilla's spent years trying to get donations through asking people nicely and in relatively unobtrusive ways in-browser for years. You can even give monthly - a subscription, if you will. Not only have they tried both donations and subscriptions, but their efforts have been resoundingly ignored. To the point where you are far from the first person to fault them for supposedly choosing to not do what they demonstrably do. Perhaps people suggest that donations and subscriptions don't work well or reliably because there's history showing that. reply lolinder 5 hours agorootparent> At one point Mozilla was literally selling a VPN subscription. That point is now - you can go buy one today. I don't want a VPN. And I don't want to pay money to a Mozilla VPN of which some unspecified percentage will actually get used to pay for Firefox development (with the rest actually paying for the VPN). I honestly feel my money does more harm than good paying for the VPN because it creates a false impression of where the demand is. I don't want a subscription to an unrelated service, I want a subscription to Firefox. I want my money to go into a stream that unambiguously shows my support for the single Mozilla project that I care about. > You can even donate money today That money will not (and I believe cannot) go to Firefox. As presently structured the corporation does all Firefox development, and the corporation cannot receive money from the foundation, so donations to Mozilla do nothing for Firefox. > Not only have they tried both donations and subscriptions, but their efforts have been resoundingly ignored. Not ignored, for the reasons stated above they haven't actually done what you say they've done. reply account42 3 hours agorootparentprevIf they wanted independence from google they'd cut overhead instead of raising the CEO salary year after year. reply a4isms 4 hours agorootparentprev> It’s clear in retrospect that we should have communicated more on this one Oh maaaaaaaaaaaan do I despise hearing variations on this \"non-pology.\" It's never \"Wow, we fucked up by doing something harmful to you.\" It's always, \"My bad, I failed to explain exactly why you're wrong to think this is harming you. I take total responsibility for not explaining why this is actually good for you. I'll try again.\" reply n_ary 8 hours agorootparentprevLets be honest, the number of HNish folks running Firefox is insignificant, compared to number of people using Firefox because their friends recommended it. So even if lets say 1% of the users(HN and similar folks) perform an outcry and go ahead disabling it, the other 99% of the people will still be a huge moat of data. These strategies(though I am willing to give Mozilla the benefit of doubt), had been played out many times, \"ops we did this ... emergency update to fix it ... we are releasing this now officially, agree to our terms if you want to continue ... you can always opt-out ... slow boiling frog metaphore ... this is now permanent with the option to disable is gone and forgotten about\". reply chillingeffect 6 hours agorootparentY tho? I run firefox and chromium side by side all day to isolate personal from work and chromium crashes constantly on a 64GB machine. Chrome uses so much more memory. reply pndy 5 hours agorootparentprev> What isn’t clear, in retrospect or otherwise, is why companies/apps/services need to keep learning this lesson. Please. This is never about learning and better communication. This is universal corporate English for: \"you got us, but we really don't give a flying ef and we will fulfill our goals step by step - no matter what you say\". reply rstarast 8 hours agorootparentprevWho's to say this \"my bad, we'll do better next time\" isn't part of the playbook? reply TheSameOlTrick 8 hours agorootparentThis. I've seen enough of: Step 1 - outrageous move Step 2 - apologize, progressively pull back Step 3 - people spread word they made it better Step 4 - stick to still outrageous but comparatively better \"middle\" move To really give it any excuse anymore. And so have you. If \"Unity\" tells you nothing... I'd like that rock, please, I'll need it to survive the incoming 4 years of social media. reply carlosjobim 5 hours agorootparentNow you've learned how parliamentary politics function. reply latexr 8 hours agorootparentprevThat would be the “or they don’t care” part. reply bawolff 8 hours agorootparentprevOnce you start assuming that every apology is fake and in bad faith, the world quickly goes to shit. I'm not saying its impossible for apologies to be in bad faith, just that if it becomes impossible to apologize and move on after making a mistake, it becomes impossible to do anything productive. reply thoroughburro 6 hours agorootparentNot real apologies, like from people — just corporate apologies, like from paid-for stooges. Society will not collapse if we start holding these monsters to account; the opposite. reply exe34 8 hours agorootparentprevit's a classic abuse tactic: if you ask first, people will cry out and if you then do it, it'll be considered escalation on your part. so instead you do it first, and if possible, do something worse to begin with, and then when there's outcry, you take a small step back, claim to be the reasonable one, and then later on push the rest of the way. reply joquarky 49 minutes agorootparentSounds like https://en.m.wikipedia.org/wiki/Anchoring_effect reply bawolff 8 hours agorootparentprevHonestly, having worked at companies that made unpopular product decisions (nothing like this, but still every company puts its foot in its mouth sometimes), it can be surprisingly non-obvious what gets people bothered and what doesn't. We always see the decisions that blow up, but we dont notice the thousands of decisions nobody cares about. Sometimes it really does look like just another minor feature request at coding time. reply latexr 7 hours agorootparent> it can be surprisingly non-obvious what gets people bothered and what doesn't. Agreed in general, disagreed in the specific Mozilla case. They’re an internet-related company where “privacy” is one of the stated core goals, yet they’ve stuck their foot in their mouth so often they could open a shoe shop. Failing to see this one is at best incompetence. reply luke-stanley 10 hours agoparentprevKey comment replying to him there which gets no reply from him: \"Opt-out is NOT a consent\". This is very problematic, see my last comment: https://news.ycombinator.com/item?id=40966312 reply lopis 11 hours agoparentprevI think this line is very important: > First, in the absence of alternatives, there are enormous economic incentives for advertisers to try to bypass these countermeasures, leading to a perpetual arms race that we may not win. It's very likely that this arms race will lead to DRM in web publications and video feeds (which Google is already experimenting with). reply CalRobert 10 hours agorootparentI will begrudgingly admit he has a point here. In a few years I imagine almost all sites will refuse to serve anything without WEI, and the \"open\" web will be the preserve of a few hobbyists. Annoyingly you'll still need to use a compromised browser (or worse, app) to do anything with your bank, etc. reply JeremyNT 1 hour agorootparentYes, the kneejerk reaction against FF here isn't really thinking things through. Mozilla has to walk this tight rope since ad companies own the web already. Realistically, the best outcome at this point is that enough users are willing to send enough data to advertisers so they allow the open web to continue. The alternative is that sites will eventually only work in Chrome or Safari on limited, locked down platforms (read: no Linux support at all). reply xk_id 5 hours agorootparentprev> the \"open\" web will be the preserve of a few hobbyists. And maybe that will be the Web healing. If all the value extraction moves elsewhere, we might finally have a sane web of hypertext documents again. reply setopt 4 hours agorootparentUnless that is labeled the new darkweb and blocked by the firewalls of ISPs and govs. I hope enough mainstream things remain on the open web for it to be unrealistic to fully block. reply CalRobert 3 hours agorootparentprevMaybe, but I suspect it will be more like trying to access Usenet now. I dunno. Considering the balkanization of the web maybe I should get in to ham radio or something. reply DaoVeles 10 hours agorootparentprevIt will be something like WEI or sites will just be a giant blob served via WebASM. reply jillesvangurp 8 hours agorootparentprevWhich will lead to counter moves by alterative browsers and websites and Google risking the loss of browser market share. If you think this is unthinkable, just look back at Microsoft's dominance of the browser market twenty years ago. Exactly like Google is doing they were pushing through all sorts of user hostile stuff via internet explorer. Before Chrome came along, Firefox was one of the few holdouts against them. Internet explorer users were dealing with all sorts of crap. Popups, popunders, all sorts of viruses, cross site scripting attacks, etc. Mostly that was just a mix of poorly designed features but there was also MS trying to get into search and advertising and they were trying to abuse their defacto monopoly to do that. reply CalRobert 3 hours agorootparentHow will an alternative browser get people to use it when major sites all make it impossible to use a non-Chromium browser? reply doctor_eval 8 hours agorootparentprevI don’t disagree with you in principle, but this history is not quite right. IIRC the IE6 team was shut down. Basically only Mozilla and Apple were building browsers at scale until Chrome came along. I might be misremembering? reply jillesvangurp 4 hours agorootparentYes, you are definitely missing a decade here. The internet explorer/edge team was shut down long after Google grabbed most of the market share. Chrome was launched 2008; Safari had its first release in 2003. And I was using the early Phoenix builds (later the name change to Firefox happened) in 2001. The version of internet explorer around the time Chrome launched was v7. IE 6 was already old news by then. And IE 8 launched soon after the Chrome launch. 9, 10, and 11 followed. And then the switch to Edge happened; which was a complete rewrite of their browser engine. Only in 2020, MS announced switching to Chromium. So, that's about 12 years of MS trying to hold on before they finally gave up. reply rwmj 9 hours agorootparentprev> leading to a perpetual arms race that we may not win So we're not even going to try. reply lopis 9 hours agorootparentThis is an attempt to try. You don't win my being an immovable wall going against the biggest corporations. If the W3C manages to create a system that satisfies advertisers while preserving our privacy, that's how you win. There isn't a future where advertising will just disappear. I'm just being pragmatic here, as a user of ad blockers for 15 years. reply DoItToMe81 8 hours agorootparentIt's not an attempt to try, it's reputation management. There is no 'anonymization' of data, because the advertising companies Mozilla is selling your data to now have almost 20 years of profiling that can effectively identify people through \"anonymous\" results. This has been known for years. Mozilla knows. They don't care. reply jeltz 9 hours agorootparentprevMost advertisers will not be satisfied with that. The real question is if regulators will be and therefore can use this as a reason to clamp down on advertisers. If so this might work, but I am skeptical. And either way it was wrong of Mozilla to sneak this in as opt-out. reply ninjin 9 hours agorootparentprevI can see the economic argument, but I am not sure that I buy it. W3C could push this as a standard, but surely anything that is privacy preserving will by its very definition provide less data for advertisement targeting, no? With less data, the targeting is likely to be worse in terms of advertisement efficiency. Thus, the economic incentive even in an ideal situation as with a W3C standard will be pushing any advertiser to \"betray\" the system and fall back on the very arms race that Mozilla is arguing that they are trying to avoid, no? At best, politicians could jump on the \"solution\", but then why are Mozilla not already lobbying in that case? Why is the first party they are reaching out to the wolf in this drama? Regardless, Mozilla has lost me at this point as a user. This being opt-out is inexcusable and I will find ways to gravitate away from them as I should not need my poor package maintainers to be paranoid with their upstream code in the same way they have to be with Chrome in order to protect us from developer abuse like this. Will try Mull on mobile now, hopefully it is viable, and see how I solve the desktop situation when I can find the time. reply squidbeak 8 hours agorootparentprevAn immovable wall is exactly what is needed to confront big corporations when they behave abusively (and intrusive profiling is an example of this). 'Pragmatism' here is just acquiescence in creeping surrender. Look what advertising has already done to the web and privacy. reply account42 3 hours agorootparentprevExcept being uncompromising is exactly how free software won. And compromising on EME DRM did not make websites using that DRM any less restricted to popular platforms. Compromise is not a winning move when what you are fighting against is fundamentally unacceptable. reply mort96 7 hours agorootparentprevThis move does not stop the arms race. Non-anonymous data is still better for the ad industry. Why give that up? reply pacifika 7 hours agorootparentBecause browsers can clamp down on The non-anon data streams when there’s a working alternative. reply mort96 7 hours agorootparentWait aren't browsers already trying to implement anti-tracking measures? Are you saying Mozilla has been holding back improving anti-tracking for the benefit of advertisers until now? Now that is evil reply marcosdumay 3 hours agorootparent> Wait aren't browsers already trying to implement anti-tracking measures? Yes, and trackers are investing large sums of money into breaking those measures. If you give advertisers a lawful non-user-threatening way to measure their ads performance, a lot of that money may disappear. (Or it may not, or it may disappear either way. That one market is crazy and I know almost nothing about it. But the claim that the money may disappear is valid, and you have to provide a valid counter-claim if you want to contest it. Calling it evil doesn't cut it.) reply account42 3 hours agorootparentprevAnd that DRM will likely come anyway and restric users of niche browsers like Firefox and operatings systems no matter what Mozilla does - just look how EME implementations and Websites using it treat Linux users not to mention non-x86/ARM architectures. So best is to push back now while we still can instead of giving them an inch. reply anordal 8 hours agorootparentprevYes, that line is important. This has happened before. Remember the critique against Encrypted Media Extensions (https://en.wikipedia.org/wiki/Encrypted_Media_Extensions): Oh no, DRM in the browser! But remember that web video used to require Adobe Flash for the longest time, and even after a decade of HTML5 video, sites were still clinging onto Adobe Flash (and later also Microsoft Silverlight) for what turned out to be DRM purposes. At the time, these plagued proprietary blobs were not going anywhere. Except, after EME had widely supplanted this last holdout usecase, they were quietly allowed to die. The result is that we have much smaller-scoped proprietary blobs in the form of content delivery modules with a lot fewer bugs and portability issues. reply mort96 7 hours agorootparentThe situation with Flash and Silverlight was better than the situation currently is with EME. Before, you could implement a standard-compliant open source web browser, you just may not be able to view certain non-web embeds. Now, web browsers need permission from Google to view certain kinds of web content, and they can't be open source. reply daveoc64 7 hours agorootparentprevI agree with the other commenter. The current situation is worse. EME requires that the browser ship with a DRM library like Widevine. Flash used an industry standard plugin model and could work in any browser. reply cynicalsecurity 10 hours agorootparentprevDRM on a website = no search engine can scan the website = no users = the DRM website dies. reply JimDabell 9 hours agorootparentWhich is one of the main reasons why it’s such a problem that the search engine with an overwhelming market share also owns the browser with overwhelming market share and is also the largest online ad company. Not to mention they pay billions each year to the other browsers. Google has a huge amount of control over every part of this. reply hollow-moe 10 hours agorootparentprevgoogle is the owner of the DRM verification system, they add exception for google robots, website only appears on google, kills other search engines in the process reply doix 10 hours agorootparentprevIf the DRM is coming from Google, I'm sure they'll take that into consideration when designing it. Feels ripe for an anti-trust lawsuit, but IANAL so who knows. reply jimkoen 9 hours agorootparentWith that logic, wouldn't Widevine DRM already be ripe for an antitrust lawsuit? Genuine question. reply doix 8 hours agorootparentWhen I wrote the comment I was imagining Google using the tech as a moat to stop other search engines from indexing DRM protected content. I guess if they shared it and \"all\" search engines could index the content, it would probably be fine? I'm guessing that's why Widevine is \"fine\". But like I said, I'm not a lawyer and have no idea what I'm talking about. reply account42 3 hours agorootparentprevIt would be if antitrust regulators were not asleep. reply cynicalsecurity 7 hours agorootparentprevEU would destroy them. reply mananaysiempre 6 hours agorootparentprevYou’ll notice that Google search now shows excerpts from things you can’t actually see visiting the site (paywalled news, paywalled scientific articles). The age of “show us exactly what users see or get downranked into oblivion” is long gone, sadly. reply deafpolygon 10 hours agorootparentprevWhich is why it hasn't rolled out yet. Once this is solved, you bet your ass they will start rolling it out. reply worble 10 hours agoparentprevInteresting comment here: https://old.reddit.com/r/firefox/comments/1e43w7v/a_word_abo... If you have telemetry disabled, this feature is also disabled, even though that isn't represented in the UI and looks like it's turned on. It's not good that it exists and is on by default, but if you have already opted out of telemetry previously, you're opted out of this too. reply qwertox 8 hours agoparentprev> Most users just accept the defaults they’re given, and framing the issue as one of individual responsibility is a great way to mollify savvy users while ensuring that most peoples’ privacy remains compromised. Cookie banners are a good example of where this thinking ends up. The problem we currently have with cookie banners is thanks to the browser vendors not caring about it. An API could exist which a page can query, where the user has already pre-selected how they want to deal with cookies. For example reject all but the essential ones, reject none at all, reject some, according to certain criteria. Even more, the browser could check if the page is adhering to the user's expectations, and if it doesn't, block it for a period of time, like a week or a month, and publish the fact that they ignored the user's wishes. Possibly also give the user a signed document which claims that this page did not respect the user's privacy expectations, so that the user can use it in court. These should be solvable problems. reply remedan 8 hours agorootparentThis was already tried with the Do Not Track header. Websites simply ignore it. They don't want an easy way to get the user's preference. Because they know that most users would set it to decline tracking. Sites would rather annoy every visitor for the chance that they click 'accept'. reply dgb23 4 hours agorootparentThis could easily be enforced now. I don't get how it isn't. reply kuschku 1 minute agorootparentIt is enforced, courts just work very slowly. Courts have already started interpreting the DNT header as GDPR-compliant opt-out that websites must follow. LunaSea 6 hours agorootparentprevUsers also don't want to pay to access content. So I guess that both the user and the site can't get what they want and we should scrap the internet. reply qwertox 5 hours agorootparentThis is unrelated. Paywalling != tracking. If it wouldn't work, then I'd see no ads in my paper-based iX subscription, yet it is full of ads even though I'm paying for that paper. But the paper has the benefit that the ads I see there don't collect information on me. This is what I want the internet to be. Ads OK, but no tracking of me if I don't want it (which I express via cookies when in a browser). Also, you should note how greedy these companies are that they show you the paywall after you have consented to the cookies in order to read the article. No hint on that accepting the cookies is only useful if you also have a subscription. When you can't read the article, they don't revert the setting of the cookies, but just pretend that they gave you access to the article and keep the cookies around for days or years. reply LunaSea 5 hours agorootparent> This is unrelated It's not. Tracking leads to better targeting which leads to higher conversion ratios and overall higher \"Cost Per 1000 Impressions\" (CPM). If you simply do \"contextual\" targeting, so targeting based on the page content, your CPM will go down and and the publisher will lose money. > Also, you should note how greedy these companies are that they show you the paywall after you have consented to the cookies in order to read the article Depends on the company. News media publishers use the same system but are usually barely profitable if at all. > Also, you should note how greedy these companies are that they show you the paywall after you have consented to the cookies in order to read the article. No hint on that accepting the cookies is only useful if you also have a subscription. When you can't read the article, they don't revert the setting of the cookies, but just pretend that they gave you access to the article and keep the cookies around for days or years. The EU Court of Law decided that offering a subscription or mandate for cookies to be enabled is not legal as an offer. So the transactional nature you propose is currently not allowed. What is allowed is a grey area which has yet to be explored. reply account42 3 hours agorootparentprevThere are unpaid sites without ads. reply thoroughburro 5 hours agorootparentprevOlder folks might remember that there were a lot of people willing to make content free, just out of personal enthusiasm, and that this content was actually a lot higher quality than that pumped out by capitalist motivation. So, actually, users and sites both had what they wanted, just not corporations. reply LunaSea 5 hours agorootparentI agree and these people still exist. However not all content can be produced this way, news or sports coverage would be an example. reply account42 3 hours agorootparentYou get faster and better news coverage from random people on social media than news corporations these days. reply LunaSea 2 hours agorootparentAlthough I agree that news media quality is not always great (really depends from one publisher to another), I would not really qualify random people on Twitter as \"news coverage\". reply beej71 16 minutes agorootparentEspecially given how likely it is that bots are or will soon be rampant. account42 3 hours agorootparentprevDNT was before the GDPR. The landscape has changed considerably since then and a standardized opt out signal being enforced is not out of the question. reply cqqxo4zV46cp 8 hours agorootparentprevWhat!? What’s the benefit of this from a site’s POV? Your technical solution is completely out of touch with real goals and incentives. reply qwertox 8 hours agorootparentHe's talking about cookie banners. The issue with cookie banners are the dark patterns, but the end-goal is to obtain permission from the user to set cookies. This requirement to constantly ask the user while using these dark patterns is what makes normal people just give up and \"accept\". If the page is expected to ask the browser which preferences the user has set regarding the cookies, then this problem is gone, because the page no longer is expected to ask a person via a popup. reply deskr 8 hours agoparentprevFirst there's a justification based on current anti-tracking system being bypassed: > \"there are enormous economic incentives for advertisers to try to bypass these countermeasures\" Then: > We’ve been collaborating with Meta on this Given Meta's track record with scooping up just about any personal data they can find, it's pretty obvious that this is just going to be yet another datapoint in Meta's collection. reply robertlagrant 7 hours agorootparentI imagine Meta like this because most of their tracking is done behind a Facebook login anyway, and it reduces the fidelity of Google ads. reply AlexandrB 3 hours agoparentprevMaybe I'm cynical, but the rationale given seems extremely naive. There's nothing stopping advertisers from using this new attribution mechanism and tracking users as much as possible. In fact that's probably exactly what they'll do since it's likely that not every browser will support this kind of attribution. The arms race will continue as it does today, but advertisers will have yet another avenue to exploit in the form of the attribution API. reply raxxorraxor 6 hours agoparentprevTo be honest, I would have used a different approach and browsers would very well be capable to give erroneous data and contaminate data from tracking users. This would be going on the offensive, and I don't believe there are any legal barriers that prevent users from \"ad fraud\". I don't believe in cooperation with an industry that has shown no remorse with tracking users at all. That will not be successful. Advertisers will employ this and still track. And it is possible to not get tracked and deliver false data, even today. reply thinkingemote 11 hours agoparentprevSome might be interested in the discussion about that too https://news.ycombinator.com/item?id=40971247 reply Retr0id 6 hours agoparentprevWow. This represents a profound misunderstanding of the advertising industry. Data is their edge. It's how they compete with each other. The privacy \"arms race\" isn't just between the browser vendors and the trackers, it's also between tracker a and tracker b. Giving them a new data point (no matter how \"\"\"privacy preserving\"\"\" it is) is just that, another data point. It's not going to make them give up on the others. reply lukan 9 hours agoparentprev\"The devil is in the details, and not everything that claims to be privacy-preserving actually is\" Yeah, like Mozilla. This is not the first time they silently added tracking and avertisement. The toggle with \"firefox shares basic telemetry with the adcompany Adjust\" has been there activated by default since a while (among other stuff). This is just more tracking from them, while claiming to defend privacy. Another day, another scandal. reply 42lux 10 hours agoparentprevOh my... not exactly reassuring. reply tgv 9 hours agorootparentThis phrase in particular: > I’ll do my best to address [your questions], though I’ve got a busy week so it might take me a bit. That means: I'll answer the easy ones, and ignore the hard ones, or ask the legal team to come up with some weasel words. reply close04 10 hours agoparentprev> doing something about [the massive web of surveillance] is a primary reason many of us are at Mozilla > we consider modal consent dialogs to be a user-hostile distraction from better defaults, and do not believe such an experience would have been an improvement here. You know what's user-hostile? Doing things without the user's knowledge or consent. The new tab page of Firefox after an update often advertises features of the release Mozilla sees important (their VPN offering, Firefox on mobile, etc.). This time the new tab page told me nothing about this change. Communicating it to me was \"free\" and they still actively refused to do it. \"Doing something\" about surveillance starts with transparency but if Mozilla's leadership doesn't see this as important they have no place leading such a company. Mozilla doesn't seem to wrap its head around the fact that their users use Firefox because they don't want the same kind of shady tactics Google or Microsoft keep pulling, they don't want their browser control to be handed over to some guy in a board room who needs a PR team to give a lengthy non-answer to the problem. I see a lot of words spent on why they came up with this technology but barely a mention about the biggest issue here especially from a company that presents itself as a champion of user rights: they pushed the change in the dead of night and took an actively hostile decision in the users' names by enabling a clearly controversial setting without any warning or communication. > we should have communicated more on this one This kind of PR speak for \"we actively kept it hidden\" is the best way to alienate the users who investigated and chose this browser for a reason. reply DaoVeles 10 hours agorootparentThats just it, that they are doing this in a somewhat quiet manner is a sign that they know how this would go down. reply nubinetwork 10 hours agoparentprevAnd nothing of value was posted. reply Juliate 7 hours agoparentprevWhat is really concerning (and enlightening) is that it is the CTO that's posting, and not the CEO. It shows that the topic is merely now considered as a technical point, rather than a principal-based one. reply justinclift 7 hours agorootparentThe Mozilla CEO would be very unlikely to receive a positive reception. reply htiawe 8 hours agoparentprevWow, that really was a wall of text. Is it just me that sometimes get the feeling that when companies have to explain them selves with this amoubt of text, they actually know that they are doing something wrong but are trying to cover it up by these long and unnecessary explanations? reply pndy 3 hours agorootparent> they are doing something wrong but are trying to cover it up That's what most of folks says in this sub-tree reply JoosToopit 7 hours agoparentprevThat's just the most brain dead rot I've read in a while. Mozilla is a joke nowadays. reply dist-epoch 8 hours agoparentprevTL;DR: sorry, we're not sorry. we will go ahead with it and explain it to you better why it's a good thing in your interest. reply st_goliath 10 hours agoprevWhile the Wikimedia Foundation is often quoted as having cancer[1], I guess the Mozilla Foundation has Alzheimer's, constantly forgetting who they are and why they are here in the first place. [1] https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_C... reply jeltz 9 hours agoparentAnd the scandals they have been involved in the past. Cliqz was another attempt by Mozilla to invest in privacy preserving technology (that time search, this time ads) where they did a stealth launch without user consent. reply kwhitefoot 11 hours agoprevSo how do we turn it off? Found it. Go to settings, type privacy into the search box. The last item under \"Firefox Data Collection and Use\" is a check box labelled \"Allow websites to perform privacy-preserving ad measurement\". It was already unchecked on mine when I looked just now. reply tda 11 hours agoparenton firefox mobile: open chrome://geckoview/content/config.xhtml, set general.aboutConfig.enable to true open about:config, set dom.private-attribution.submission.enabled to false reply fransje26 9 hours agorootparentWhat's the difference with setting dom.private-attribution.submission.enabled to false in the gekoview? reply tda 8 hours agorootparentProbably none, I didn't know it was also there. I just compiled what worked for me after scrolling through a few other posts. reply aaubry 9 hours agorootparentprevThanks a lot for this tip. I don't know how one is supposed to find this setting. reply psychoslave 10 hours agorootparentprevWow, thanks for this and the parent post. reply l0b0 10 hours agoparentprevOn NixOS, set programs.firefox.policies.Preferences.\"dom.private-attribution.submission.enabled\" = false; (https://gitlab.com/engmark/root/-/commit/bbb3ff9efb878ddda38...) reply RegW 7 hours agoparentprevInterestingly the option has a link to an explanation on how it works. Which was handy as I couldn't get past the German cookie dialogue on the original article. I guess the question is whether the aggregation services can be persuaded by clever attribute manipulation to give the ad site a near unique report for a user across many sites.reply wkat4242 11 hours agoparentprevYeah on desktop. On mobile it's a lot harder. It's still turned on and you have to use a workaround to enable about: config because they don't bother to make this option visible in settings. reply htiawe 8 hours agorootparentI recently started using Firefox again, because of all the madness around Chrome and the change of how add-ons (mainly uBlock and similar) would work. And it felt kinda good, i actually thought that Firefox was different and a part of \"the good guys\", now it doesn't feel that way anymore. Sigh. reply staunton 7 hours agorootparentI don't know of any \"good guys\" whatsoever that ever managed to build and maintain a browser. Anyone? Maybe one day we'll have a usable FOSS browser but I doubt it (the companies will fight tooth and nail against it including legal means, buying out companies, blocking content for them, etc.). reply wkat4242 5 hours agorootparentI think the guys that built WebKit originally (Konqueror) are kinda good guys. I still sponsor KDE with a monthly donationand you have to use a workaround to enable about: config I know of no workaround short of installing from the Beta or Nightly channel. reply amne 11 hours agorootparentprevchrome://geckoview/content/config.xhtml source: https://connect.mozilla.org/t5/ideas/firefox-for-android-abo... reply wkat4242 11 hours agorootparentYeah apparently you can use that to set: general.aboutConfig.enable to true And then you can go to the normal about:config and set dom.private-attribution.submission.enabled to false Only then is PPA actually off (apparently, I did not manage to test this yet but someone did confirm the default setting is true). Not cool. Especially because Mozilla provides instructions for the desktop version on their site but doesn't even mention the mobile version at all. reply zaik 11 hours agorootparentprevI can access the about:config page on mobile, but I can't seem to find a relevant option. Maybe it's already disabled for the Fennec app from F-Droid? reply squarefoot 11 hours agorootparentprevOn mobile you probably want to try Fennec and/or Mull, both Firefox forks, compatible with FF addons and available on F-Droid. reply parlortricks 10 hours agorootparentAs of June Fennec is now under Mozilla? reply perryizgr8 10 hours agorootparentprevDo we know why they blocked about:config on mobile? It doesn't make any sense at all... reply Bilal_io 7 hours agorootparentIt worked for me, no issues. I am on version 128.0 (Build #2016030615) reply bravetraveler 7 hours agorootparentI had to go through some Gecko thing first like others mentioned, quite odd. Supposedly the setting to adjust is in there too, but I have no idea what applies here reply perryizgr8 1 hour agorootparentprevDoesn't work for me 128.0 (Build #2016030615) Wonder why this is even an issue. reply tromp 10 hours agoparentprevOn MacOS that checkbox is in a separate section called \"Website Advertising Preferences\". reply DaoVeles 10 hours agoprevSo where can I donate to Ladybird browser development? Before anyone tries to respond with it. It is https://donorbox.org/ladybird reply ethagnawl 8 hours agoparentThanks. I just donated to Ladybird and Servo. Here's the Servo link: https://servo.org/sponsorship/ reply remram 5 hours agorootparentIs there any reason to believe that the Servo project will produce a full independent browser, rather than a browser engine as their website states? The only likely outcome is that it be used in Firefox... reply prox 10 hours agoparentprevThey really need to start adding windows as a build target at some near point in the future. As a webdev, that’s the only way I can convince the public to switch. reply lukan 10 hours agorootparent\"that’s the only way I can convince the public to switch.\" It is not ready, to be a public browser. reply leshenka 8 hours agorootparentprevIt's already too hard to convince public to switch from chrome to another chromium browser or firefox and you're talking about switching to browser that is at least several years away from feature parity reply Sammi 8 hours agorootparentprevThey want to be able to run more websites well before they invite the unwashed masses from windows XD reply prox 8 hours agorootparentI don’t disagree :) reply master-lincoln 9 hours agorootparentprevMaybe Ladybird can be a good reason to convince the \"public\" to switch away from Windows in the near future reply prox 8 hours agorootparentThat would be a great day. Unfortunately the culture of Linux is still too much walled garden, not in the Apple-like commercial sense, but in the tech culture kind of way. We need a way to embrace the public without losing what makes Linux great (to hack it to your own specifications) reply DaoVeles 10 hours agorootparentprevI mean for a long while even the GNU project provided Windows builds for Icecat browser. Probably to much Stallman grumbling. EDIT : Actually they still update it. Last version is 115. reply actionfromafar 7 hours agorootparentFirst time I hear about Icecat browser. It seems like something that should be much more known! reply tgv 9 hours agoparentprevDon't hold your breath. It takes ages to develop a browser that's as fast as Firefox. CSS and JS are no joke. reply Sammi 8 hours agorootparentLadybird more than doubled their js performane in five months between january and may and are now about 2x as slow as Safari: https://x.com/awesomekling/status/1790098727081836697 Things are progressing faster than you'd think. reply tgv 3 hours agorootparentFirst, that's uncompiled/jitted. Second: the 80/20 rule. reply quaintdev 8 hours agorootparentprevMaybe it's time to move away from whole html/css/js, http and browsers? Let's build something that is Ad resistant from the start. Something that uses native technologies. Edit: We need something that does not need backing of large corporations or huge funding to access the web. Internet was always simple. We have become over dependent on browsers and http stack. reply aAaaArrRgH 7 hours agorootparentOr just don't access any content that is funded by advertising. The nonprofit web still exists. But for all content that's not someone's spare time passion project, someone's gotta foot the bill. reply pacifika 7 hours agorootparentprevGemini https://en.wikipedia.org/wiki/Gemini_(protocol) reply leshenka 8 hours agorootparentprevhave you got any suggestions? reply jay_kyburz 8 hours agorootparentHTML only (with forms). Client side css only. No JavaScript. No cookies. reply tgv 3 hours agorootparentThat's not acceptable for 99.9% of the people, so they'll stay on their current browsers. An alternative must be attractive to succeed. reply switch007 8 hours agorootparentprevSlow is the price we'll have to pay. Just like how VPNs slow down your connection Or, if one dreams for a moment, if slower becomes the norm, web apps will have to become less complicated. Fast seems to just enable more and more ad tech reply bachmeier 6 hours agoparentprevHow would that solve the problem? Years down the road, if they actually finish their browser, what guarantee do you have against it being enshittified in some way? The only option I see is a project that exists to deshittify an open source browser. reply Ygg2 10 hours agoparentprevWhy Ladybird? Why not Servo? reply different_base 10 hours agorootparentI appreciate Ladybird's initiative. But if they work with Servo, Ladybird can build the browser and Servo can focus on the engine. Also we can avoid C++ nightmare. Everybody wins. reply paddim8 7 hours agorootparentLadybird is all about the engine. It's progressing faster than servo reply Ygg2 5 hours agorootparentIn what way? Rendering pages CSS compatibility? I tried servo on Windows and it worked, not so much for Ladybird - granted, I wasn't feeling up to task of compiling it for Windows. reply suby 8 hours agorootparentprevLadybird seems to have more momentum and be further along in development in my testing of visiting random websites. This may or may not have something to do with developer velocity of each language, genuinely I don't know but I think it's worth considering. Regardless, from what I've gathered, Ladybird is going to ship of theseus their way into memory safety. It's not announced what the C++ replacement language will be, but they are working towards that. reply sgt 6 hours agorootparentprevFor the user, C++ won't be a nightmare. reply remram 5 hours agorootparentprevIsn't Servo a new rendering engine for Firefox? How does it fix all of Firefox's problems, that are not engine limitations? You can make a free web browser from Firefox's current engine just as easily as from Servo, I would fund the person who does that. reply ranguna 10 hours agoprevhttps://librewolf.net/ And fallback to Firefox when things don't work. Which is usually on sketchy websites, websites that have heavy bot protection and fingerprinting or ones that use gpu APIs. reply MrAlex94 8 hours agoparentA few criticisms of LibreWolf: * There is no legal entity behind the project. Should anything ever happen with the project (it can happen, even if unlikely), there are no legal ramifications. * The binaries aren't signed. Yes, code signing is a bit of a racket, but there is some merit in it. * There is no auto-update mechanism. Might not seem like a big deal, but IMO it is, especially on Windows where you're recommended to rely on 3rd party client to update the browser for you. You've now added a middle man, and since the binaries are not signed... well there's no guarantee you aren't downloading a malicious binary. reply MiddleEndian 6 hours agorootparent>There is no auto-update mechanism. Might not seem like a big deal, but IMO it is, especially on Windows where you're recommended to rely on 3rd party client to update the browser for you. You've now added a middle man, and since the binaries are not signed... well there's no guarantee you aren't downloading a malicious binary. To me, this seems like a plus. If you want users to update, provide them with something worth updating to. This tracking suddenly being enabled for a ton of users is the very result of automatic updates. reply kees99 6 hours agorootparentAlso, for some software vendors, frequent/automatic upgrades are a great place to hide silent reconfiguration. Mozilla has been repeatedly resetting \"Always check if Firefox is your default browser\" option to \"yes\" with upgrades. I don't see why \"private-attribution submission enabled\" wouldn't be reset in future in the same way. reply MrAlex94 6 hours agorootparentAs mentioned above, we aren't talking about Firefox's update mechanism here, but rather Librewolf's. > Mozilla has been repeatedly resetting \"Always check if Firefox is your default browser\" option to \"yes\" with upgrades. I'm sorry to say this, but this just seems to be misinformation. I don't see that anywhere in the source code[1]? Anything I can find regarding prompting the user regarding the default browser is hidden behind an if guard to make sure the pref is `true` and not `false`. The only scenarios I am aware of that will change the pref if the user has toggled one manually is the `_migrationUI`[2] function (as you can see, no changes relating to `browser.shell.checkDefaultBrowser`). Otherwise, untoggled prefs will be changed if the value in `firefox.js`[3] or `all.js`[4] is changed. As you can see, the last time the pref was modified was 2004. [1] https://searchfox.org/mozilla-central/search?q=browser.shell... [2] https://searchfox.org/mozilla-central/source/browser/compone... [3] https://searchfox.org/mozilla-central/diff/94ff451885bb94679... [4] https://searchfox.org/mozilla-central/source/modules/libpref... reply MrAlex94 6 hours agorootparentprevBut we're not talking about Firefox's update mechanism here, we're talking about Librewolf's. They are already the custodians of custom settings and making the choice for you, so it doesn't seem like a valid comparison here. I would also say a web browser should be the one piece of software constantly updated due to the sheer volume of security patches issued every few weeks. reply MiddleEndian 5 hours agorootparent>But we're not talking about Firefox's update mechanism here, we're talking about Librewolf's. Doesn't matter. I don't inherently trust any organization. >They are already the custodians of custom settings and making the choice for you, so it doesn't seem like a valid comparison here. I can make the choice to install software. I should be able to make the choice to upgrade it as I choose as well. If I buy a chair from Crate+Barrel, I have given them the choice of designing and manufacturing that chair and all the decisions that went into it. But I do not give Crate+Barrel the choice of sneaking into my house and swapping it with some newer version of the chair that 51% of the population liked slightly better after 5 minutes of testing or that they think will make them more money somehow. reply MrAlex94 4 hours agorootparent> I can make the choice to install software. I should be able to make the choice to upgrade it as I choose as well. I think that's completely valid. I was just assuming (maybe incorrectly?) we're talking about what should be happening in general (so what the experience for the layman should be). Now whether that applies to Librewolf is another story, but arguably it becoming fairly known, it should. Side-note: In Waterfox, I've re-added the ability to disable auto-updating completely. I completely understand the want to manually update software. reply michael9423 2 hours agorootparentprevYou know perfectly well that point 1 is completely irrelevant in the world of open-source. A UK Ltd. is less transparent than Librewolf, an open-source project run by many volunteers without the incentive to make any money. Point 3 is no longer true, the installer comes with the option to enable auto-update and on Linux, it also auto-updates, depending on distro, etc. The risks you are talking about are not inherent to Librewolf, but to Linux and open-source, and thus are not legitimate criticisms of Librewolf. reply MrAlex94 2 hours agorootparent> You know perfectly well that point 1 is completely irrelevant in the world of open-source. Genuinely, why not? Open source projects go through ownership changes (as unlikely as they may be), social engineering, etc. In the unlikely chance something were to happen and anything malicious were to occur, what recourse is a user to have? And we are talking about a web browser here, which will be accessing peoples most sensitive data. I don't think this is an unreasonable stance. > A UK Ltd. is less transparent than Librewolf, an open-source project run by many volunteers without the incentive to make any money. Well this UK Ltd is still beholden to English law and UK GDPR. You could argue the merits and teeth that GDPR has, but I don't see why it's not a valid comparison? I can't just start processing personal data without complying with GDPR, for example. > The risks you are talking about are not inherent to Librewolf, but to Linux and open-source, and thus are not legitimate criticisms of Librewolf. Linux has the Linux foundation, which AFAIK is going to be beholden to California law? I don't see how that can't also be a criticism of Librewolf (and any OSS in a similar spot?). > Point 3 is no longer true, the installer comes with the option to enable auto-update and on Linux, it also auto-updates, depending on distro, etc. It seems to me to still true, because the installer is installing WinUpdater. Which, as it seems, is maintained by an individual developer? > If you want LibreWolf to be automatically updated (recommended), you can choose to install the LibreWolf WinUpdater[1], which is included in the installer. [1]: https://codeberg.org/ltguillaume/librewolf-winupdater reply itscrush 6 hours agorootparentprevHave you spent time with Waterfox as an alternative or have some thoughts when comparing against Librefox? In app update prompts work for me, they have a TOS / Legal Entity it seems. They broke away from Startpage in recentish years. Plenty of feature trade offs to compare though with Librefox. reply MrAlex94 4 hours agorootparentYes, I'm the developer of Waterfox :-) > Plenty of feature trade offs to compare though with Librefox. Yes, for sure. Definitively different goal alignments. reply poidos 7 hours agoparentprevInstalled it a couple days ago and it works great, haven’t had any problems yet. Nice to have it through Brew. reply 200 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Firefox's latest version now collects user data by default, introducing Privacy-Preserving Attribution (PPA) for anonymized ad measurement, which has sparked controversy.",
      "Critics argue that this default data collection undermines user trust and suspect Mozilla's intention to boost revenue, challenging Firefox's reputation for privacy.",
      "The PPA uses an aggregation server to anonymize data, but concerns persist about data leaving users' computers and potential future access by advertisers."
    ],
    "commentSummary": [
      "Firefox has started collecting user data by default for advertising purposes, which can be disabled in the settings under Privacy & Security.",
      "Users have reported that the setting to disable this feature is not easily searchable and does not sync across devices.",
      "Mozilla's CTO claims the feature balances privacy and ad measurement, but user skepticism and criticism persist."
    ],
    "points": 522,
    "commentCount": 460,
    "retryCount": 0,
    "time": 1721112110
  },
  {
    "id": 40973339,
    "title": "Exo: Run your own AI cluster at home with everyday devices",
    "originLink": "https://github.com/exo-explore/exo",
    "originBody": "exo: Run your own AI cluster at home with everyday devices. Maintained by exo labs. DiscordTelegramX Forget expensive NVIDIA GPUs, unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, Linux, pretty much any device! Get Involved exo is experimental software. Expect bugs early on. Create issues so they can be fixed. The exo labs team will strive to resolve issues quickly. We also welcome contributions from the community. We have a list of bounties in this sheet. Features Wide Model Support exo supports LLaMA and other popular models. Dynamic Model Partitioning exo optimally splits up models based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device. Automatic Device Discovery exo will automatically discover other devices using the best method available. Zero manual configuration. ChatGPT-compatible API exo provides a ChatGPT-compatible API for running models. It's a one-line change in your application to run models on your own hardware using exo. Device Equality Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices connect p2p. As long as a device is connected somewhere in the network, it can be used to run models. Exo supports different partitioning strategies to split up a model across devices. The default partitioning strategy is ring memory weighted partitioning. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device. Installation The current recommended way to install exo is from source. From source Python>=3.12.0 is required because of issues with asyncio in previous versions. git clone https://github.com/exo-explore/exo.git cd exo pip install -r requirements.txt Documentation Example Usage on Multiple MacOS Devices Device 1: python3 main.py Device 2: python3 main.py That's it! No configuration required - exo will automatically discover the other device(s). The native way to access models running on exo is using the exo library with peer handles. See how in this example for Llama 3. exo also starts a ChatGPT-compatible API endpoint on http://localhost:8000. Note: this is currently only supported by tail nodes (i.e. nodes selected to be at the end of the ring topology). Example request: curl http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"llama-3-70b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}], \"temperature\": 0.7 }' curl -X POST http://localhost:8001/api/v1/chat -H \"Content-Type: application/json\" -d '{\"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of life?\"}]}' Inference Engines exo supports the following inference engines: ✅ MLX ✅ tinygrad 🚧 llama.cpp Networking Modules ✅ GRPC 🚧 Radio 🚧 Bluetooth Known Issues 🚧 As the library is evolving so quickly, the iOS implementation has fallen behind Python. This is being worked on, and longer term we will push out an approach that will unify the implementations so we don't have to maintain separate implementations.",
    "commentLink": "https://news.ycombinator.com/item?id=40973339",
    "commentBody": "Exo: Run your own AI cluster at home with everyday devices (github.com/exo-explore)292 points by simonpure 16 hours agohidepastfavorite96 comments hagope 13 hours agoI used to be excited about running models locally (LLM, stable diffusion etc) on my Mac, PC, etc. But now I have resigned to the fact that most useful AI compute will mostly be in the cloud. Sure, I can run some slow Llama3 models on my home network, but why bother when it is so cheap or free to run it on a cloud service? I know Apple is pushing local AI models; however, I have serious reservations about the impact on battery performance. reply PostOnce 8 hours agoparentMaybe you want to conduct experiments that the cloud API doesn't allow for. Perhaps you'd like to plug it into a toolchain that runs faster than API calls can be passed over the network? -- eventually your edge hardware is going to be able to infer a lot faster than the 50ms+ per call to the cloud. Maybe you would like to prevent the monopolists from gaining sole control of what may be the most impactful technology of the century. Or perhaps you don't want to share your data with Microsoft & Other Evils (formerly known as dont be evil). You might just like to work offline. Whole towns go offline, sometimes for days, just because of bad weather. Nevermind war and infrastructure crises. Or possibly you don't like that The Cloud model has a fervent, unshakeable belief in the propaganda of its masters. Maybe that propaganda will change one day, and not in your favor. Maybe you'd like to avoid that. There are many more reasons in the possibility space than my limited imagination allows for. reply tarruda 4 hours agorootparentIt is not like strong models are at a point where you can 100% trust their output. It is always necessary to review LLM generated text before using it. I'd rather have a weaker model which I can always rely on being available than a strong model which is hosted by a third party service that can be shut down at any time. reply Aurornis 2 hours agorootparent> I'd rather have a weaker model which I can always rely on being available than a strong model which is hosted by a third party service that can be shut down at any time. Every LLM project I’ve worked with has an abstraction layer for calling hosted LLMs. It’s trivial to implement another adapter to call a different LLM. It’s often does as a fallback, failover strategy. There are also services that will merge different providers into a unified API call if you don’t want to handle the complexity on the client. It’s really not a problem. reply gtirloni 3 hours agorootparentprev> eventually your edge hardware is going to be able to infer a lot faster than the 50ms+ per call to the cloud. This is interesting. Is that based on any upcoming technology improvement already in the works? reply a_t48 2 hours agorootparentGP is likely referring to network latency here. There's a tradeoff between smaller GPUs/etc at home that have no latency to use and beefier hardware in the cloud that have a minimum latency to use. reply jumpCastle 4 hours agorootparentprevAren't services like runpod solve half of these concerns? reply wokwokwok 13 hours agoparentprev> Sure, I can run some slow Llama3 models on my home network, but why bother when it is so cheap or free to run it on a cloud service? Obvious answer: because it's not free, and it's not cheap. If you're playing with a UI library, lets say, QT... would you: a) install the community version and play with ($0) b) buy a professional license to play with (3460 €/Year) Which one do you pick? Well, the same goes. It turns out, renting a server large enough to run big (useful, > 8B) models is actually quite expensive. The per-api-call costs of real models (like GPT4) adds up very quickly once you're doing non-trivial work. If you're just messing around with the tech, why would you pay $$$$ just to piss around with it and see what you can do? Why would you not use a free version running on your old PC / mac / whatever you have lying around? > I used to be excited about running models locally That's an easy position to be one once you've already done it and figured out, yes, I really want the pro plan to build my $StartUP App. If you prefer to pay for an online service and you can afford it, absolutely go for it; but isn't this an enabler for a lot of people to play and explore the tech for $0? Isn't having more people who understand this stuff and can make meaningful (non-hype) decisions about when and where to use it good? Isn't it nice that if meta released some 400B llama 4 model, most people can play with it, not just the ones with the $7000 mac studio? ...and keep building the open source ecosystem? Isn't that great? I think it's great. Even if you don't want to play, I do. reply jrm4 5 hours agorootparentRight, I think people here are vastly underestimating this idea of \"What if I want to play around with really PERSONAL stuff.\" I've been keeping a digital journal about my whole life. I plan to throw that thing into an AI to see what happens, and you can be damn sure that it will be local. reply monkmartinez 43 minutes agorootparentYes, I am with you 100% and keep several LLaMA's on my workstation for that reason. I use Openrouter for everything else. Everything that isn't sensitive goes to one of the big kid models because they are just sooooo much better. LLaMA 400b might be the start of running with the big kids, but I know we are not close with the current available models. reply itake 13 hours agorootparentprevI’m a bit confused. Your reasoning doesn’t align with the data you shared. The startup costs for just messing around at home are huge: purchasing a server and gpus, paying for electricity, time spent configuring the api. If you want to just mess around, $100 to call the world’s best api is much cheaper than spending $2-7k Mac Studio. Even at production level traffic, the ROI on uptime, devops, utilities, etc would take years to recapture the upfront and on-going costs of self-hosting. Self hosting will have higher latency and lower throughput. reply zeta0134 10 hours agorootparentYou are vastly overestimating the startup cost. For me this week it was literally these commands: pacman -S ollama ollama serve ollama run llama3 My basic laptop with about 16 GB of RAM can run the model just fine. It's not fast, but it's reasonably usable for messing around with the tech. That's the \"startup\" cost. Everything else is a matter of pushing scale and performance, and yes that can be expensive, but a novice who doesn't know what they need yet doesn't have to spend tons of money to find out. Almost any PC with a reasonable amount of RAM gets the job done. reply monkmartinez 51 minutes agorootparentllama3 at 8billion params is weak sauce for anything serious, it just isn't in the same galaxy as Sonnet 3.5 or GPT-4o. The smaller and faster models like Phi are even worse. Once you progress past asking trivial questions to a point where you need to trust the output a bit more, its not worth effort in time, money and/or sweat effort to run a local model to do it. A novice isn't going to know what they need because they don't know what they don't know. Try asking a question to LLaMA 3 at 8 billion and the same question to LLaMA 3 at 70 billion. There is a night and day difference. Sonnet, Opus and GPT-4o run circles around LLaMA 3 70b. To run LLaMA at 70 billion you need serious horse power as well, likely thousands of dollars in hardware investment. I say it again... the calculus in time, money, and effort isn't favorable to running open models on your own hardware once you pass the novice stage. I am not ungrateful that the LLaMA's are available for many different reasons, but there is no comparison between quality of output, time, money and effort. The API's are a bargain when you really break down what it takes to run a serious model. reply Aurornis 3 hours agorootparentprevI’m familiar with local models. They’re fine for chatting on unimportant things. They do not compare to the giant models like Claude Sonnet and GPT4 when it comes to trying to use them for complex things. I continue to use both local models and the commercial cloud offerings, but I think anyone who suggests that the small local models are on par with the big closed hosted models right now is wishful thinking. reply LorenDB 4 hours agorootparentprevAnd why would you buy a Mac Studio? You could build a reasonable GPU-accelerated Linux box for well under $1500. For example: https://pcpartpicker.com/guide/BCWG3C/excellent-amd-gamingst... reply J_Shelby_J 4 hours agorootparentDevs that refuse to move off Apple are severely disadvantaged in the LLM era. reply jondwillis 4 hours agorootparentlol tell that to the 3 year old laptop with 64 GB of RAM that I use exclusively for local LLMs while dev’ing on my work laptop with 96 GB of RAM… reply sudohackthenews 13 hours agorootparentprevPeople have gotten manageable results on all sorts of hardware. People have even squeezed a few tokens/second out of Raspberry PIs. The small models are pretty performant- they get good results on consumer gaming hardware. My 2021 laptop with a 3070m (only 8gb vram) runs 8b models faster than I can read, and even the original M1 chips can run the models fine. reply monkmartinez 23 minutes agorootparentYou are right of course.... IF your metric for manageable/useable is measured only tokens per second (tok/s). If your metric is quality of output, time, money and tok/s, there is no comparison; Local models just aren't there yet. reply wokwokwok 12 hours agorootparentprev> The startup costs for just messing around at home are huge No, they are zero. Most people have extra hardware lying around at home they're not using. It costs nothing but time to install python. $100 is not free. If you can't be bothered, sure thing, slap down that credit card and spend your $100. ...but, maybe not so for some people? Consider students with no credit card, etc; there are a lot of people with a lot of free time and not a lot of money. Even if you don't want to use it do you do seriously think this project is totally valueless for everyone? Maybe, it's not for you. Not everything has to be for everyone. You are, maybe, just not the target audience here? reply Aurornis 3 hours agorootparent> You are, maybe, just not the target audience here? The difference between an open model running on a $100 computer and the output from GPT4 or Claude Sonnet is huge. I use local and cloud models. The difference in productivity and accuracy between what I can run locally and what I can get for under $100 of API calls per month is huge once you get past basic playing around with chat. It’s not even close right now. So I think actually you are not the target audience for what the parent comments are taking about. If you don’t need cutting edge performance then it’s fun to play with local, open, small models. If the goal is to actually use LLMs for productivity in one way or another, spending money on the cloud providers is a far better investment. Exceptions of course for anything that is privacy-sensitive, but you’re still sacrificing quality by using local models. It’s not really up for debate that the large hosted models are better than what you’d get from running a 7B open model locally. reply lynx23 12 hours agorootparentprevAnd its not entitled to cliam that \"Most people have extra hardware lying around at home\". Your story doesn't sound plausible at all. reply bryanrasmussen 9 hours agorootparentMost people who would want to be running machine learning models probably have some hardware at home that can handle a slow task for playing around and determining if it is worthwhile to pay out for something more performant. This is undoubtedly entitled, but thinking to yourself huh, I think it's time to try out some of this machine learning stuff is a pretty inherently entitled thing to do. reply wokwokwok 12 hours agorootparentprevThis project is literally aiming to run on devices like old phones. I don't think having an old phone is particularly entitled. I think casually slapping down $100 on whim to play with an API... probably, yeah. /shrug reply itake 12 hours agorootparentAccording to this tweet, Llama 3 costs about $0.20 per Million tokens using an M2. https://x.com/awnihannun/status/1786069640948719956 In comparison, GPT3.5-turbo costs $0.50 per million tokens. Do you think an old iPhone will less than 2x efficient? reply nightski 7 hours agorootparentFWIW depends on cost of power. Where I live cost of power is less than half the stated average. reply nl 5 hours agorootparentprev> Well, the same goes. It turns out, renting a server large enough to run big (useful, > 8B) models is actually quite expensive. The per-api-call costs of real models (like GPT4) adds up very quickly once you're doing non-trivial work. I run my own models, but the truth is most of the time I just use an API provider. TogetherAI and Groq both have free offers that are generous enough I haven't used them up in 6 months of experimentation and TogetherAI in particular has more models and gets new models up quicker than I can try them myself. reply Aurornis 3 hours agorootparentprev> Why would you not use a free version running on your old PC / mac / whatever you have lying around? Because the old PC lying around can’t come anywhere near the abilities or performance of the hosted AI compute providers. Orders of magnitudes of difference. The parent commenter is correct: If you want cutting edge performance, there’s no replacement for the hosted solutions right now. Running models locally is fun for playing around and experimenting, but there is no comparison between what you can run on an old PC lying around and what you can get from a hosted cluster of cutting edge hardware that offers cheap output priced per API call. reply FeepingCreature 13 hours agorootparentprevI just prepay $20/mo to openrouter.ai and can instantly play with every model, no further signup required. reply dotancohen 12 hours agoparentprevI have found many similarities between home AI and home astronomy. The equipment needed to get really good performance is far beyond that available to the home user, however intellectually satisfying results can be had at home as a hobby. But certainly not professional results. reply grugagag 7 hours agorootparentWhen learning and experimenting it could make a difference. reply Cantinflas 13 hours agoparentprevWhy bother running models locally? Privacy, for once, or censorship resistance. reply seasonman 13 hours agorootparentAlso customizability. Sure, you can fine-tune the cloud hosted models (to a certain degree of freedom), but it will probably be expensive, inefficient, difficult and unmaintainable. reply hanniabu 13 hours agorootparentprevAnd offline access reply diego_sandoval 2 hours agoparentprev> why bother when it is so cheap or free to run it on a cloud service? For the same reasons that we bother to use Open Source software instead of proprietary software. reply dsign 11 hours agoparentprevFor my advanced spell-checking use-case[^1], local LLMs are, sadly, not state-of-the-art. But their $0 price-point is excellent to analyze lots of sentences and catch the most obvious issues. With some clever hacking, the most difficult cases can be handled by GPT4o and Claude. I'm glad there is a wide variety of options. [^1] Hey! If you know of spell-checking-tuned LLM models, I'm all ears (eyes). reply bruce343434 8 hours agorootparentI think the floating point encoding of LLMs is inherently lossy, add to that the way tokenization works. The LLMs I've worked with \"ignore\" bad spelling and correctly interpret misspelled words. I'm guessing that for spelling LLMs, you'd want tokenization at the character level, rather than a byte pair encoding. You could probably train any recent LLM to be better than a human at spelling correction though, where \"better\" might be a vague combination of faster, cheaper, and acceptable loss of accuracy. Or maybe slightly more accurate. (A lot of people hate on LLMs for not being perfect, I don't get it. LLMs are just a tool with their own set of trade offs, no need to get rabid either for or against them. Often, things just need to be \"good enough\". Maybe people on this forum have higher standards than average, and can not deal with the frustration of that cognitive dissonance) reply jrm4 5 hours agoparentprevWhat do you mean by useful here? I'm saying because I've had the exact OPPOSITE thought. The intersection of Moore's Law and the likelihood that these things won't end up as some big unified singularity brain and instead little customized use cases make me think that running at home/office will perhaps be just as appealing. reply Hihowarewetoday 11 hours agoparentprevI'm not sure why you have resigned? If you don't care about running it locally, just spend it online. Everything is good. But you can run it locally already. Is it cheap? No. Are we still in the beginning? yes. We are still in a phase were this is a pure luxury and just getting into it by buying a 4090, is still relativly cheap in my opinion. Why running it locally you ask? I personally think running anythingllm and similiar frameworks on your own local data is interesting. But im pretty sure in a few years you will be able to buy cheaper ml chips for running models locally fast and cheap. Btw. aat least i don't know a online service which is uncensored, has a lot of loras as choice and is cost effective. For just playing around with LLMs for sure there are plenty of services. reply dws 3 hours agoparentprev> Sure, I can run some slow Llama3 models on my home network, but why bother when it is so cheap or free to run it on a cloud service? Running locally, you can change the system prompt. I have Gemma set up on a spare NUC, and changed the system prompt from \"helpful\" to \"snarky\" and \"kind, honest\" to \"brutally honest\". Having an LLM that will roll its eyes at you and say \"whatever\" is refreshing. reply friendly_chap 12 hours agoparentprevWe are running smaller models with software we wrote (self plug alert: https://github.com/singulatron/singulatron) with great success. There are obvious mistakes these models make (such as the one in our repo image - haha) sometimes but they can also be surprisingly versatile in areas you don't expect them to be, like coding. Our demo site uses two NVIDIA GeForce RTX 3090 and our whole team is hammering it all day. The only problem is occasionally high GPU temperature. I don't think the picture is as bleak as you paint. I actually expect Moore's Law and better AI architectures to bring on a self-hosted AI revolution in the next few years. reply aftbit 3 hours agoparentprevWhat if you want to create transcripts for 100s of hours of private recorded audio? I for one do not want to share that with the cloud providers and have it get used as training data or be subject to warrentless search under the third party doctrine. Or what if you want to run a spicy Stable Diffusion fine-tune that you'd rather not have associated with your name in case the anti-porn fascists take over? I feel like there are dozens of situations where the cost is really not the main reason to prefer a local solution. reply nhod 13 hours agoparentprevIs this a hunch, or do you know of some data to back up your reservations? Copilot+ PC’s, which all run models locally, have the best battery life of any portable PC devices, ever. These devices have in turn taken a page out of Apple Silicon’s playbook. Apple has the benefit of deep hardware and software integration that no one else has, and is obsessive about battery life. It is reasonable to think that battery life will not be impacted much. reply fragmede 13 hours agorootparentThat doesn't seem totally reasonable. The battery life of an iphone is pretty great if you're not actually using it, but if you're using the device hard, it gets hot to the touch, along with the battery getting drained. playing resource intensive video games, maxing out the *PU won't stop and let the device sleep at all, and has a noticable hit on battery life. Where inference takes a lot of compute to perform, it's hard to imagine inference being totally free, battery-wise. It probably won't be as hard on the device as playing specific video games non-stop, but I get into phone conversations with ChatGPT as it is, so I can imagine that being a concern if you're already low on battery. reply bongodongobob 13 hours agoparentprevI have a 2 year old Thinkpad and I wouldn't necessarily call llama3 slow on it. It's not as fast as ChatGPT but certainly serviceable. This should only help. Not sure why your throwing your hands up because this is a step towards solving your problem. reply dcreater 2 hours agoprevThis is a great ideal and user friendly as well. Has the potential of converting multiple old devices overnight from being useless. However, I wish they had provided some results on tok/s, latency with some example setups. reply alexandercheema 2 hours agoparentWe didn't expect this to blow up so quickly. A lot of work needs to be done on getting different setups working. I have made an issue here: https://github.com/exo-explore/exo/issues/11 reply ajnin 7 hours agoprevIt requires mlx but it is an Apple silicon-only library as far as I can tell. How is it supposed to be (I quote) \"iPhone, iPad, Android, Mac, Linux, pretty much any device\" ? Has it been tested on anything else than the author's MacBook ? reply orsorna 5 hours agoparentOne of the maintainers has a video demo on his twitter claiming iOS, android and Linux. Some of the code is not released and I wish they were advertising that properly. reply lopuhin 2 hours agoparentprevThe README says they plan to add llama.cpp support which should cover a lot of targets, also they have tinygrad already integrated I think. reply mg 12 hours agoprevThis enables you to run larger models than you would be able to on any single device. No further explanation on how this is supposed to work? If some layers of the neural network are on deviceA and some layers are on deviceB, wouldn't that mean that for every token generated, all output data from the last layer on deviceA have to be transferred to deviceB? reply mikewarot 11 hours agoparentYes, so you would have a vector about 8k values long to be transferred on each token generated. You could do that easily with any modern network. reply mg 11 hours agorootparentThat's exciting. So we could build a SETI@home style network of even the largest models. I wonder if training could be done in this way too. reply alexandercheema 11 hours agorootparentRepo author here. That's correct. The embeddings for Llama-3-8B are around 8KB-10KB. For Llama-3-70B they're around 32KB. These are small enough to send around between devices on a local network. For a SETI@home style network, latency will kill you if you go over the internet. That's why we're starting with local networks. reply juvo 8 hours agorootparenthow does it compare to https://github.com/bigscience-workshop/petals ? reply mg 9 hours agorootparentprevAh yes. At first, I thought that since it is all one-way forward-only communication, latency would only affect the time to the first token. But I guess the final output needs to be sent back to the first node before it can continue. So if there are 50 nodes with a latency of 40ms each, each token would take 2s to process. reply alexandercheema 9 hours agorootparentYeah, unfortunately the autoregressive nature of these models slows it down significantly with added devicedevice latency. However, you can still max out on throughput with pipeline parallelism, where you overlap execution. See: https://pytorch.org/docs/stable/pipeline.html reply steeve 11 hours agoparentprevYes, that’s how it works (pipeline parallelism) reply mg 11 hours agorootparentInteresting. Let's do the math ... Let's say the model has 50B parameters and 50 layers. That would mean about one billion values have to travel through the wifi for every generated token? I wonder how much data that is in bytes and how long it takes to transfer them. reply blackbear_ 11 hours agorootparentIt's not the parameters that are sent, it's the layer outputs. That makes for a few thousands floats per token reply mg 11 hours agorootparentWoops! I would have thought the number of neurons roughly equals the number of parameters, but you are right. The number of parameters is much higher. reply tama_sala 1 hour agorootparentThe embedding size is only 8k so while the parameters are 70B. So it's a huge difference reply pyinstallwoes 6 hours agoprevSwarm compute should be the norm for all compute - so much unused cpu across all the devices we collectively own. reply phito 4 hours agoparentI'd rather my CPU to be idle and not consome much power reply imp0cat 3 minutes agorootparentIt depends. There is a lot of devices with quite capable cpus that are mostly doing nothing. reply KronisLV 6 hours agoparentprevThis might not work for use cases where you need low latency, but for longer winded processing it would be amazing if possible. For example, if I have a few servers, laptop (connected to power) as well as a desktop PC and they’re all connected to a fast local network, it’d be great to distribute the task of rendering a video or working with archive files across all of them. reply greggsy 5 hours agorootparentThose are two precise examples that benefit from single core compute power, and are wholly unsuited to distributed computing… reply KronisLV 4 hours agorootparentDistributed rendering farms have existed for a while. reply _factor 5 hours agoparentprevThis exists: https://aihorde.net/ I haven’t tried it, and not the norm, but I agree it should be more common. We have a global supercomputer with higher latency, but still a supercomputer. reply dchuk 4 hours agorootparentI might just still be too tired from just waking up, but I can’t for the life of me find any details on that site about what models are actually being served by the horde? reply burkaman 4 hours agorootparentGo to https://aihorde.net/api/, scroll down to /v2/status/models, and click Try it out and then Execute. It's an enormous list and I think it can be dynamically updated, so that's probably why it isn't listed on the website. reply matyaskzs 8 hours agoprevCloud cannot be beaten on compute / price, but moving to local could solve privacy issues and the world needs a second amendment for compute anyway. reply CuriouslyC 6 hours agoparentYou can beat gpt4/claude in terms of price/performance for most things by a mile using fine tuned models running in a colo. Those extra parameters give the chatbots the ability to understand malformed input and to provide off the cuff answers about almost anything, but small models can be just as smart about limited domains. reply ComputerGuru 3 hours agorootparentThe problem is that once you say “fine tuned” then you have immediately slashed the user base down to virtually nothing. You need to fine-tune per-task and usually per-user (or org). There is no good way to scale that. Apple can fine-tune a local LLM to respond to a catalog of common interactions and requests but it’s hard to see anyone else deploying fine-tuned models for non-technical audiences or even for their own purposes when most of their needs are one-off and not recurring cases of the same thing. reply CuriouslyC 1 hour agorootparentNot necessarily, you can fine tune on a general domain of knowledge (people already do this and open source the results) then use on device RAG to give it specific knowledge in the domain. reply makmanalp 4 hours agoprevQuestion - if large clusters are reporting that they're seeing gains from using RDMA networks because communication overhead is a bottleneck, how is it possible that this thing is not massively bottlenecked running over a home network? reply DistractionRect 3 hours agoparentI suspect that most of the devices you'd expect to find in your consumer cluster are too small/slow to saturate the link. Edit: it's also a matter of scale. You probably have a small number of small/slow devices in a consumer network versus a lot of large/fast devices in your enterprise cluster. reply derefr 3 hours agoparentprevI haven't looked into exactly what this project is doing, but here's my understanding: Inference across O(N) pre-trained hidden layers isn't exactly an \"embarrassingly parallel\" problem, but it is an \"embarrassingly pipeline-able\" problem (in the CPU sense of \"pipelining.\") Each device can keep just one or a few layers hot in their own VRAM; and also only needs to send and receive one small embedding (<1MB) vector per timestep — which is so trivial that it's easily achievable in realtime even if all the devices are on wi-fi, talking to the same router, in your \"noisy\" apartment where 100 other neighbours are on the same bands. (To put it another way: running a single inference job, has more forgiving realtime latency+throughput requirements than game streaming!) Assuming that you have a model that's too big for any of your home machines to individually hold; and that all you care about is performance for single-concurrent-request inference on that model — then in theory, you just need one GPU of one node of your homespun Beowulf GPU cluster to have enough VRAM to keep the single largest layer of your model always-hot; and then other smaller devices can handle keeping the smaller layers always-hot. And the result should be faster than \"overloading\" that same model on that single largest-VRAM device and having some layers spill to CPU, or worse yet, having the GPU have to swap layers in and out repeatedly with each inference step. (Also, if you're wondering, in the case where a single machine/node has multiple GPUs — or a GPU+VRAM and also a CPU+RAM! — you can treat this as no different than if these were multiple independent nodes, that just-so-happen to have a very efficient pipeline communication channel between them. As the VRAM+computation cost of running inference far outweighs the communication overhead of forward propagation during inference, a home-network inference-pipelining cluster scheduler like this project, would still likely \"schedule\" the model's layers purely in consideration of the properties of the individual GPU+VRAM (or CPU+RAM), rather than bothering to care about placement.) --- That being said, AFAIK training is \"pipeline parallelizable\" exactly as inference is. And people training models do do this — but almost always only across multiple top-of-the-line GPUs in one machine; not across multiple machines. When you think about what pipelining achieves for training — all you get is either: 1. the ability to use a bunch of small-aggregate-VRAM nodes to achieve the aggregate training capacity of fewer, larger-aggregate-VRAM nodes — but with more power consumption = higher OpEx; and where also, if you scale this to O(N), then you're dumping a quadratic amount of layer-propagation data (which is now both forward-prop and backprop data, and backprop data is bigger!) over what would likely be a shared network just to make this work. (If it's not a shared network — i.e. if it's Infiniband/other RDMA — then why did you spend all that CapEx for your network and not on your GPUs!?) 2. the ability to pipeline a bunch of large-aggregate-VRAM nodes together to train a model that will then never be able to be deployed onto any single node in existence, but can instead only exist as a \"pipelined inference model\" that hogs O(log N) nodes of your cluster at a time for any inference run. Which makes cluster scheduling hell (if you aren't just permanently wiring the scheduler to treat O(log N)-node groups as single \"hyper-nodes\"); makes it so that you'll never be able to practically open-source the model in a way anybody but other bigcorps could ever run it (if that's something you care about); and very likely means you're cutting the concurrent-inference-request-serving capacity of your huge expensive GPU cluster by O(log N)... which the product team that allowed that cluster to be budgeted is really not gonna like. That being said, I imagine at some point one of these proprietary \"Inference-as-a-Service\" models has been trained at a layer size that puts it into pipelined-inference-only territory, temporarily. Doing so would be the ML engineer's equivalent to the CPU engineer's \"we have no fundamentally clever advance, so this quarter we'll just crank up the clock frequency and deal with the higher TDP.\" (Heck, maybe GPT-4o is one of these.) --- What people with GPU clusters want, is 1. for the output of the process to be a model that runs on a single (perhaps multi-GPU) node; and 2. for the process itself to be mostly-shared-nothing with as little cross-node communication burden as possible (such that it's just a question of building highly internally communication-efficient nodes, not so much highly-communication-efficient clusters.) And both of those goals are achieved by sizing models so that they fit within a single node; continuously fanning out streams of training data to those nodes; and then periodically fanning back in model-weights (or model-weight deltas) in an AllReduce operation, to merge the learning of O(N) independently-training nodes to become the new baseline for those nodes. (If you'll note, this architecture doesn't put any latency requirements on the network, only some monstrous throughput requirements [at the fan-in step] — which makes it a lot easier to design for.) reply pierrefermat1 8 hours agoprevWould be great if we could get some benchmarks on commonly available hardware setups. reply pharrington 8 hours agoparentI'm sure someone will show their benchmarks in a couple years! reply whoami730 7 hours agoprevIs it possible to use this for image recognition and like? Not sure what can be the usage of this apart from as a chatbot. reply tama_sala 1 hour agoparentYou can use other models like a vision LLM, or use AI agents as well reply throwawaymaths 1 hour agoprevIs this sensible? Transformers are memory bandwidth bound. Schlepping activations around your home network (which is liable to be lossy) seems like it would result in atrocious TPS. reply alexandercheema 1 hour agoparent\"Transformers are memory bandwidth bound\" - this is the precise reason why this makes sense. If a model doesn't fit into memory on a single device, it needs to be incrementally loaded into memory (offloading), which is bottlenecked by memory bandwidth. Splitting the model over multiple devices avoids this, instead trading off for latency of communicating between nodes. The network bandwidth requirements are minimal since only the activations (intermediary embeddings) are passed between devices. For Llama-3-8B these are ~10KB, for Llama-3-70B these are ~32KB. reply tarasglek 11 hours agoprevThis is the first timer i've seen tinygrad backend in the wild. Amusing that it's supposedly more stable than llama.cpp for this project. reply alexandercheema 11 hours agoparentRepo author here. Tinygrad changes rapidly so wouldn't it say it's \"more\" stable, but it certainly supports more accelerators than llama.cpp. As George Hotz likes to say, it sits somewhere on the spectrum between llama.cpp and Mojo. No hand-written kernels, optimal kernels are generated and found by beam search. reply christkv 6 hours agoprevIs apple silicon with a lot of memory 32Gb and up still considered a cheapish way to run models or are there other options now? reply talldayo 3 hours agoparentA good Apple Silicon Mac with 32gb of RAM will cost you over $2,000 on-sale. For that price you might as well buy an Nvidia machine instead, either two 3090s or a 64gb Jetson Orin board would be both cheaper and faster. The markup on Apple hardware is so big that I just don't think \"cheapish\" will ever be a way to describe the position they hold in the AI market. Apple's current budget lineup gets smoked by an RTX 3060 in a cheap Linux homeserver; the bar for high-value AI has been raised pretty high. reply ulrischa 3 hours agoprevDoes somebody know if it runs on a raspberry? reply alexandercheema 1 hour agoparentIt *should* but I haven't tried it. I will try it. Updated in this issue: We could also try raspberry pi + coral usb tpu (https://coral.ai/products/) - that might be a killer combo for super cheap home ai cluster. reply alexandercheema 1 hour agorootparentIssue link: https://github.com/exo-explore/exo/issues/11 reply thom 11 hours agoprevBexowulf. reply iJohnDoe 12 hours agoprev [–] Anyone run this? Works? reply tdubhro1 12 hours agoparentThe readme shows how to run it assuming you can run a python program on the device, so I expect it works with laptops and PCs but there's a note at the end of the page saying that the iOS app has fallen behind the python version so it's not clear to me how to get this running on your iphone or other such devices. reply orsorna 12 hours agorootparentThe \"device\" in question must be Apple Silicon because the `mlx` package is a hard dependency, or at least an ARM machine (I do not have any Apple Silicon Macbooks or ARM machines to run this). I tried tweaking this before realizing calls to this library is littered all over the repo. I don't really understand the AI ecosystem very well but it seems that the use of the `mlx` library should be supplanted by some other library depending on the platform machine. Until then, and the actual release of the iOS code somewhere, \"everyday devices\" is limited to premium devices that almost no one has more than one of. I'm looking forward to run this on other machine platforms and squeeze out what I can from old hardware laying around. Otherwise I doubt the tagline of the project. Edit: to add on, the only evidence that this runs anywhere but Apple Silicon is the maintainer's Twitter where they show it running on two Macbook Pros as well as other devices. I'm not sure how many of those devices are not ARM. I'm not throwing shade at the concept the author is presenting, but I'd appreciate if they could slow down functional commits (he is writing them right now as I type) and truthfully modify the documentation to state which targets are actually able to run this. reply acosmism 12 hours agoparentprev [–] why ask? try it! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Exo allows users to create an AI cluster at home using everyday devices like iPhones, iPads, Androids, Macs, and Linux systems, eliminating the need for expensive NVIDIA GPUs.",
      "Key features include wide model support (e.g., LLaMA), dynamic model partitioning, automatic device discovery, and a ChatGPT-compatible API, all using a peer-to-peer connection without a master-worker architecture.",
      "Exo is experimental software, so users should expect early bugs and are encouraged to report issues and contribute to the community."
    ],
    "commentSummary": [
      "Exo allows users to run their own AI clusters at home using everyday devices, providing an alternative to cloud-based AI compute.",
      "The project supports various hardware, including old phones and laptops, and aims to offer privacy, customization, and offline access for AI models.",
      "Discussions highlight the trade-offs between local and cloud AI models, including cost, performance, and privacy, with some users preferring local setups for personal or sensitive data."
    ],
    "points": 292,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1721098511
  },
  {
    "id": 40972122,
    "title": "Horizon – Private alternative to Imgur",
    "originLink": "https://horizon.pics",
    "originBody": "Hey HN, I&#x27;m James, a 17-year old full-stack engineer from Canada with a strong passion for building software. During the day, I work for a California-based startup, and in the evenings, I enjoy working on side projects[1][2].For the past 3 years, I&#x27;ve been building and iterating on a product I called Horizon Pics, which is a file hosting service, similar to mainstream services, like Imgur. Horizon allows you to quickly upload and store all types of files, from images and video, to PDFs and other documents. The biggest differentiating factor is that Horizon&#x27;s incentives are much more aligned with you, the end-user.Unlike Imgur, Horizon has absolutely no ads, doesn&#x27;t sell your data, has built-in security and privacy controls, and is fully focused on your file sharing needs. No social media or other bloat.This past week, I&#x27;ve launched a rebrand of Horizon which features a brand-new desktop app called Alpine[3], which serves as a local companion to Horizon. With it comes the capability to auto-upload screen captures and upload your clipboards as shareable pastes. For extra privacy, clipboard sharing can be automatically deleted after one view, or end-to-end encrypted with AES-256-GCM client-side. The desktop app is completely free to use! It&#x27;s powered by Tauri using TypeScript, SvelteKit, Sass, and Rust.Horizon offers a free plan with limited storage and upload sizes, while the paid plan offers higher limits.Let me know what you think about the landing page[0]. Does it provide enough information as a new user?[0]: https:&#x2F;&#x2F;horizon.pics[1]: https:&#x2F;&#x2F;httpjames.space[2]: https:&#x2F;&#x2F;github.com&#x2F;httpjamesm[3]: https:&#x2F;&#x2F;horizon.pics&#x2F;alpine",
    "commentLink": "https://news.ycombinator.com/item?id=40972122",
    "commentBody": "Horizon – Private alternative to Imgur (horizon.pics)259 points by sweca 20 hours agohidepastfavorite147 comments Hey HN, I'm James, a 17-year old full-stack engineer from Canada with a strong passion for building software. During the day, I work for a California-based startup, and in the evenings, I enjoy working on side projects[1][2]. For the past 3 years, I've been building and iterating on a product I called Horizon Pics, which is a file hosting service, similar to mainstream services, like Imgur. Horizon allows you to quickly upload and store all types of files, from images and video, to PDFs and other documents. The biggest differentiating factor is that Horizon's incentives are much more aligned with you, the end-user. Unlike Imgur, Horizon has absolutely no ads, doesn't sell your data, has built-in security and privacy controls, and is fully focused on your file sharing needs. No social media or other bloat. This past week, I've launched a rebrand of Horizon which features a brand-new desktop app called Alpine[3], which serves as a local companion to Horizon. With it comes the capability to auto-upload screen captures and upload your clipboards as shareable pastes. For extra privacy, clipboard sharing can be automatically deleted after one view, or end-to-end encrypted with AES-256-GCM client-side. The desktop app is completely free to use! It's powered by Tauri using TypeScript, SvelteKit, Sass, and Rust. Horizon offers a free plan with limited storage and upload sizes, while the paid plan offers higher limits. Let me know what you think about the landing page[0]. Does it provide enough information as a new user? [0]: https://horizon.pics [1]: https://httpjames.space [2]: https://github.com/httpjamesm [3]: https://horizon.pics/alpine sebstefan 4 hours agoIt was surprisingly easy to configure ShareX to work with Horizon! Handles deletion and putting the URL of the uploaded file in the clipboard and all. Hopefully you guys could collaborate to become a default file/image destination uploader that actually handles logging in and logging out of an account. https://i.horizon.pics/35dwiVGxz8General > App Configuration. reply sebstefan 3 hours agorootparentWow. So you already provide a ShareX Custom Uploader configuration file on your website. I wasn't expecting that. I expected it so little that I didn't even bother searching. So it works basically the same as mine but instead of the shitty cookie workaround I did, you have a header called \"Authorization\": \"Bearer xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" that won't blow up in my face every few weeks Very nice reply tyingq 17 hours agoprevLooks great, and clearly a lot of time and thought invested. Not news to you, I'm sure, but these sites typically only last for a while. Because nobody has really hit on a way to subsidize a reasonable free plan enough to pay for the bandwidth it collectively consumes, and to implement enough spam protection. Especially once it hits a certain adoption curve. So they either die or find a way to push ads, like imgur did. This is so nice though, I'm really rooting for it to find a way. reply sweca 17 hours agoparentThanks! It's been operating for the past 3 years without any ads or third-party trackers, and I don't have plans to shut it down for the forseeable future. reply fragmede 17 hours agorootparentNo one ever starts these things with plans to shut them down, the question is how much money do you have for this right now, how much money is it burning per unit of time, and how long will that last if no one pays for a paid plan. I'm not asking you to answer those questions, but pointing out the reality of the economics of the situation. If an image goes viral, can you pay a 10x or 100x hosting bill? For how long? How much are you commiting to spend before shutting it down becomes an attractive option? reply sweca 17 hours agorootparentThese are good questions to ask. Right now, the monetization strategy is profitable. If an image does go viral, which has happened before on popular Reddit posts that hit the front page, the infrastructure should be able to handle it for a while, and it should not be costly. reply airtonix 15 hours agorootparentyou should protect yourself from free plans going viral and costing you more than you'd like with rate limiting rules of some kind. reply sweca 14 hours agorootparentI see your point, but the business model can sustain a large influx in free signups. I will keep this in mind though. reply jchook 1 hour agoparentprevThe spam protection is worth repeating. I made one of these back in 2005 and it was inevitably gangloaded with questionable content like p*rn, malware, software license keys, copyrighted material, etc. reply Modified3019 46 minutes agorootparentI remember a few years ago when some starry eye coder on Reddit made a page that would load and display the latest uploads to Imgur as tiles. It soon became apparent that this is not ideal. Same thing with tools that would try to archive Imgur, the results basically need manual review because the resulting dataset is a loaded gun. reply lelandbatey 15 hours agoparentprevExactly; I still remember reading the imgur announcement post on reddit which had all the same descriptions. They were ad free and they were aiming to be great forever. Now though they're the \"big bad\". https://www.reddit.com/r/reddit.com/comments/7zlyd/comment/c... reply fsckboy 14 hours agorootparentIn 2016, Imgur raised $40 million in funding from investors, valuing the company at around $500 million. In 2021, Imgur was acquired by the media company, MediaLab, for an undisclosed amount, reportedly around $200-300 million. Alan Schaaf, the CEO and founder, reportedly owned around 40% of the company at the time of the acquisition, which would have netted him a significant amount, estimated to be around $80-120 million. why does everybody here think that imgur is a failure in light of OP's stated goals? His stated goals are the same as imgur's stated goals. The imgur outcome is the stated goal of most of the people here. Think a little bit more inside the box. reply phito 12 hours agorootparentSo the goal is to make another unsustainable business, make everyone use and love it, then sell it to someone who will make it uterly shitty with the hopes of squeezing at least a bit of money out of the people who now rely on it, before it inevitably dies? reply latexr 9 hours agorootparentThe goal is to make “fuck you money” then tell everyone, including users, to go fuck themselves. The specifics are up in the air. Don’t forget to talk about the “wonderful journey” it has been when announcing the sale. reply bonestamp2 1 hour agorootparentprev> sell it to someone who will make it uterly shitty Imgur is still good. It's just not great anymore, and it's a long way from shitty. reply corobo 4 hours agorootparentprevThe cycle of image hosting continues https://drewdevault.com/2014/10/10/The-profitability-of-onli... I've swapped to using the app Dropshare hooked up to a Cloudflare R2 bucket myself reply stavros 9 hours agoparentprevI found a way, pay me: https://imgz.org/ reply MzHN 12 hours agoprevMonetization and abuse has been mentioned a bunch as things to consider as you gain more users, but I'll add a couple more I think are relevant, at least once the service is big enough. Your payment processor. Payment processors might get triggered by the fact that adult content is pretty much explicitly allowed. They might also just generally deem you \"high risk\" and kick you out. I think this is one of the biggest risks you run. For example Stripe, according to their legal docs, prohibit \"Cyberlockers, regardless of whether they host adult content\", whatever that means, but it sounds a lot like file sharing in general is a prohibited business. Ever-tightening think-of-the-children laws. You may not want to implement expensive and privacy invasive scanning (note that even local scanning may be privacy invasive[1]), but you might be forced or face a fine. You could always do like Google and budget it in as an expense, love that \"fines\" line in their reports. This may also be relevant to the previous one, since payment processors might deem you higher risk if you have no scanning. Getting blacklisted for hosting malware. Specifically the support for non-media files. You could end up being blocked by browsers, search engines and whatnot. See for example Google Safe Browsing. [1] https://rys.io/en/173.html reply indigodaddy 22 minutes agoprevAre private domains allowed on the free plan? https://docs.hrzn.cool/pics/add_private_domain reply bangaladore 21 minutes agoparent> This feature is exclusive to Horizon Everest users. If you are not subscribed to Horizon Everest, you can do so in your Dashboard by clicking the Upgrade button in your navigation bar. [1] [1] https://docs.hrzn.cool/pics/add_private_domain reply indigodaddy 16 minutes agorootparentThanks. Another question. So this does not only images but any file or text file or functions as a pastebin etc? If so, is there a way of interacting with it as such as a pastebin and or uploading other type of files, other than the webui? Eg a command line tool? I see you made hrznsc but it seems for screenshots only, so perhaps just need to use some sort of api method for the command-line/scripty sort of pastebin-like and other files interaction/functionality that many users might be after? reply toomuchtodo 18 hours agoprevIf for whatever reason, you don’t find traction, and/or get bored of running it, I’d be interested in acquiring it into a nonprofit entity and running it in some sort of public good fashion. But put your heart into it first. Looks great, really well done. reply sweca 18 hours agoparentThank you so much! Your compliments and generosity are very much appreciated. reply xyst 17 hours agoprevVery clean and easy to use. 90% of my coworkers with “decades of experience” and “senior” in their titles would not even be able to build something like this in their full time. Let alone part or spare time. reply sweca 17 hours agoparentThank you! It took quite a few iterations to get here. This time I tried to focus on reducing friction as much as possible, learning from other successful apps along the way. reply devnull212 17 hours agoprevCongrats on the launch! But, something that stops me from using a number of services is the sale of fixed size plans where I'll typically either under utilize them (e.g. do I have 100 GB of screenshots to upload) or I'll over utilize and might not have an upgrade path immediately available for more storage. I tend to toss my photos in backblaze or S3 for this reason -- UX is severely lacking but I pay for what I use. I'm curious how you landed on the subscriptions you chose + whether you'd consider usage based plans? reply sweca 17 hours agoparentI understand where you're coming from. Most people do not have 100 GB of media on hand, nor will they probably in the next 5 years. As a result, it's basically unlimited from a reasonable usage standpoint. Before adding a 100 GB cap, I actually experimented with having \"unlimited storage\", but this can go wrong in many ways. I've spoken to companies in the VPN and cloud storage industries and having unlimited anything can go very wrong, especially if your service is promoted in a problematic context. e.g. unlimited cloud storage in a data hoarding community. By at least adding a cap, I can calculate a maximum cost for every user to influence my financial decisions and restrict abuse. As for pay-per-use, no plans for now. Kagi's CEO had a really nice discussion about this on their forum and on HN. Simply put, for the demographic I'm targeting, which is quite broad, the typical user would likely get turned off by a pay-per-use model. I've also priced Everest lower than other paid services, such as Vimeo and Gyazo, while including extra benefits. reply uuuuuquu 12 hours agorootparent> Most people do not have 100 GB of media on hand, ... I have over 130 GB of media and really just started taking pictures two years ago. My mother for sure has way over 100 GB of media. My gf too. My father also has over 100 GB of media. And we all are not photographers or people who must take pictures of everything. 100 GB is not that much really, especially since everybody cat take 4K pictures and record 4K videos. Who do you mean by \"most people\"? reply sweca 12 hours agorootparentBy the term \"media\", I was referring to OP's potential usecase of Horizon, like their screenshots and screen recordings. I think you're framing Horizon as potential photo backup, in which case, it is not designed nor meant for that. reply zepolen 1 minute agorootparentHi, welcome to the web, it doesn't give a fuck about what your system was designed or meant to do. If there's a way to abuse it, it will happen, especially if it's free. Get ready for someone building a HorizonFS or the FBI to come knocking on your door from the terrorists, pedos and scammers using your service, not to mention that random joker that just wants to watch the world burn. tracker1 2 hours agoparentprevI've really liked Cloudflare's R2 for this usage myself. YMMV of course. reply sirjaz 2 hours agoprevAre there any plans for Windows App support? There are more Windows 10 and 11 users than all iOS, iPadOS, and MacOS users combined reply sweca 2 hours agoparentThere's ShareX for Windows which Horizon supports. You can download the configuration files in Settings > General > App Configuration. reply jappgar 3 hours agoprevA cool demonstration of your skills but not a viable business unfortunately. Others have said this, but t bears repeating: When you reach the size where you can actually make good money your site will be hosting plenty of malware and other illegal content. reply 9dev 11 hours agoprevThe attention to detail on the landing page is amazing. The Apple-style feature boxes, the animations, the subtle blur effect for content at the bottom, the button shadows... I love it. You've got an amazing career ahead of you. reply sweca 11 hours agoparentThank you. Apple was a big inspiration! Rens (info in footer) helped me out with the animations. reply DaoVeles 14 hours agoprevJust a quick note. That you have pricing up front is such a big thing. Places like Imgur and photobucket before it were out to be exploited merely for their free bandwidth. There is a clear means of funding this and that instills confidence. One other note, I also am a lot more likely to jump onto a service if there is an easy escape plan. While you don't want to loose customers, having an easy out allows people to exit on amicable terms. reply sweca 14 hours agoparentYep! Having limits and sustainable pricing was at the top of mind prior to launch - didn't want to dig myself into a hole. For exports, there are currently 3 export options: - file download URLs (encrypted content is undecryptable by Horizon, therefore will not be downloadable without the correct key, which is not stored by Horizon) - all your short links - all your paste content (encrypted content is undecryptable by Horizon, therefore will show up in its ciphertext form) These exports are all in machine-readable JSON. reply DaoVeles 14 hours agorootparentI love it! These are the awesome kind of projects I like seeing. No hype about \"what could be\" but a case of \"Here it is! Go for it!\" reply jml78 18 hours agoprevCongrats, it looks great, love that you have Linux app support. Question, have you done the business analysis to know how long you can afford to run the system when giving away 500MB? Even your paid model seems really inexpensive. Do you have enough margin to immediately not go broke? I am not trying to throw shade, I am just curious because it almost seems too good to be true since you aren’t running any ads and your price is free and/or cheap reply sweca 18 hours agoparentYes. Over the past 3 years I've been researching and re-iterating different monetization strategies. While I don't want to divulge exact operating costs, I can tell you that it's sustainable to provide free storage and the amount on the paid Everest plan. reply ijustlovemath 15 hours agorootparentWhat kind of backup and disaster recovery plans do you have in place, if any? Are they tested? reply account42 6 hours agoprevImage hosting is not a viable business. Source: every image host that has ever existed (note the past tense) reply 8organicbits 4 hours agoparentCharge for storage and bandwidth. Providers like AWS S3 are doing fine with that model. E: not bandwidth, data transfer reply sweca 3 hours agorootparentAnd this is exactly why S3 is meant for developers. The average person does not want to think about limits they cannot control, like a burst in viewership for an image they uploaded. reply metadat 17 hours agoprevHow is it private if you can't self host? (I'm hoping I missed something :) reply loteck 17 hours agoparentPrivacy and security here are being commingled under the banner of AES encryption at rest, which is apparently disabled by default. I always wonder, if your marketing pitch involves security features, but those features are off by default, aren't you technically pitching your lack of security? reply sweca 16 hours agorootparentEncryption at rest is disabled by default because many users do not want to keep track of all of their encryption keys, which are not stored by Horizon when that setting is enabled. There are also other security features, like end-to-end encryption for pastes, but like mentioned before, not everyone wants to lose the ability to preview their content in the dashboard. By giving the user a choice, I can cater to both crowds: one that prefers convenience, vs the other which prefers the most security. Edit: To clarify, all files are already encrypted at rest with a key I control. But with Encryption enabled (capital E to distinguish the feature name), it is encrypted again with a key Horizon won't store. reply iJohnDoe 12 hours agorootparentClear and concise. Well done. Impressive for a 17 year old. reply metadat 17 hours agorootparentprevAnd if $company controls the keys.. what happens once funding dries up? Yeah.. nothing personal but we've seen it previously. In the meantime, OP and Co. could create an open standard for image hosting, and have a lasting impact on the order of S3. Wouldn't that be something? Here's to hoping. reply Lammy 13 hours agoparentprevIf you (or anyone else) are looking for a self-host option, check out Lutim https://framagit.org/fiat-tux/hat-softwares/lutim reply gizmo 8 hours agoprevThis looks really great. Impressive work, especially for your age. I think the bottom half of your landing page is much stronger than the top half. Why talk about encryption before explaining what the actual features are? \"Sharing\" is pretty vague and describes many apps. It might be better to lead with the actual features or usecases. Note that you went with a different pitch in this HN post: \"it's a file hosting service [...] like imgur\". That indicates your hero message needs some work. You also have essentially two calls to action. \"Get started\" and \"Download for mac\". One call to action is better. Your sign up form also looks a bit busy and only has the text \"create your horizon account\" which conveys no useful information when instead you could explain why people should sign up and point out that it's free forever, which your users wouldn't know if they clicked through to \"get started\" right from the landing page. At the bottom of the landing page you have a \"sign up\" in the paid plan but it links to the same sign up form. Why make people choose between paid and free when it later turns out the choice isn't real? That's a wasted opportunity. You should have only sign up button in the pricing plans section if there is only one sign up page. Alternatively, ask for credit card details if people show interest in the paid plan. If you make $40/yr for a pro user then you'll need 25,000 pro users for each million in revenue. This is the kind of business where you're gonna have 200 free users for every paid user so about 5 million free accounts for each million in revenue. If 20% of site visitors sign up and 30% of those continue to use your service you need drive 75 million visitors to your landing page to get 1 million in ARR. Not impossible, but pretty difficult. imgur got large because reddit didn't want to do images themselves. Nowadays imaging hosting has become a lot easier because of AWS and Cloudflare, so you'll have to work a lot harder at getting traction. reply efilife 7 hours agoprevWhere do you host it? I see that nsfw is allowed, what vps does allow it? reply cptcobalt 3 hours agoparentThis is HN, is asking about a VPS a bit quaint, when we're in the era of container orchestration/etc? reply tidyread 3 hours agoprevPrivacy and security are important considerations for file hosting services, and Horizon seems to be addressing these well with its built-in controls and encryption features. The desktop app integration is a nice touch that could make the service more convenient to use. reply sweca 3 hours agoparentThank you. I try my best to balance both privacy and security and functionality. reply jmuguy 16 hours agoprevMaybe stupid question but can video files uploaded be played in the browser? I’m always frustrated when I want to share a game clip with friends and it seems like the options are YouTube which is overkill and a few services like streamable that are too expensive for just casual sharing reply sweca 15 hours agoparentYes! Supported file types can, such as MP4. Other video formats are not currently supported for streaming (can still be uploaded for download) due to browser restrictions and compute considerations. For supported video types, Horizon is optimized in the sense that it recognizes videos and allows viewers to stream it in chunks for seamless watching. reply ijustlovemath 15 hours agorootparentWhy not have the user's machine do the encoding into whatever format is cheapest/easiest for you to support? Could even divide user payloads into chunks and p2p it, bittorrent style. No compute needed, beyond a tracking server that can be very cheap to run reply sweca 15 hours agorootparentTranscoding client-side still has considerations: - What if the user's hardware is not performant? - Transcoding is intensive, so their machine will become slow - Even wasm based ffmpeg will not be as efficient and has its own issues - Takes a very long time reply ijustlovemath 15 hours agorootparent1. Then setup a worker thread with a very high nice value, like 19. Limit your throughput with tick-based compute (sleep if you're using too much compute) 2. Not if you do as proposed 3. Efficiency is not your problem, as you've stated with the previous 2 requirements. Your algorithm doesn't have to be maximally efficient if it takes a while to compute (eg you're using sleep in it) 4. Doesn't matter, most users of this service will only be taking a few photos or videos a day. Offer a paid service upgrade for heavy users who cant handle the upload times. reply tzfld 12 hours agoprev>Unlimited Bandwidth* >Keep on sharing with peace of mind that your files will always be accessible for those who need them. What about sharing them in a forum post which may become viral or may be targeted by a ddos attack? reply sweca 12 hours agoparentThat's ok! Horizon should be able to handle it. reply isatty 12 hours agorootparentIt’s a great service and forgive my cynicism but how, without losing money? reply sweca 12 hours agorootparentI've worked out the economics! The cost of the free plan is negligible. reply GaiusCoffee 16 hours agoprevI have always wondered how \"hosting\" sites deal with malicious actors, like sharing of copywritten stuff (like movies), xxx stuff or worse (cp). Is there content moderation at the backend of Horizon? reply sweca 16 hours agoparentFor privacy reasons, there's no scanning. I rely on user reports to determine whether to take content down. reply 0xCMP 14 hours agorootparentI have some experience helping previous jobs block bandwidth abuse from user uploaded content. That lead to inadvertently finding some pretty bad content. I would figure out some way to have someone else review the content or at least some kind of automated scanner you can use to pre-check the reported content before reviewing it yourself. Some stuff is hard to unsee. Edit: I wonder if a local LLM (to help with privacy concerns) would be a good option or not to at least identify anything obviously bad. Wish I had more concrete suggestions. reply sweca 14 hours agorootparentYou absolutely have a point. But for me, I'm not sure how to balance privacy and safety. Is my service really private at all if I'm handing off user files to a third party to do who knows what to scan for bad content, and potentially risk users through false positives? Edit: A local model could work, but that can be quite compute intensive and therefore expensive. reply 0xCMP 14 hours agorootparentI meant it only for the reported content so that is, to me, a proper balance because that's kind of your legal requirement[0] to take down content which is reported. But since that's ripe for abuse the proper way is to basically first hide the content, review+confirm it's bad, and then take proper action. So I would try asking around or thinking of how best to handle the specific reported cases without exposing yourself too directly. [0]: I am not a lawyer reply idiotsecant 14 hours agorootparentprevThis is a legally...risky strategy. You built a cool thing but unfortunately when you put cool things online they get used for the worst possible purposes. reply sweca 14 hours agorootparentYes, I understand there is an unfortunate risk. However, I oppose file scanning, and so do many users, as we've seen with the Apple scandal. If any content is uploaded that violates the terms of service and is reported, they will be deleted as soon as possible, and that user will almost certainly be permanently banned. The terms of service also limits my liability. reply idiotsecant 3 hours agorootparentThe terms of service cannot shield you from federal law. Not sure if you're in the US or elsewhere but similar laws are prevalent around the world - in the US federal law prohibits the production, advertisement, transportation, distribution, receipt, sale, and possession of child sexual abuse material (CSAM). This is an issue that could ruin you. The only reason it hasn't already is that the service isn't big enough yet. You undoubtedly already have CSAM on your network and any reasonable person with experience online would expect that, which is an important standard for you to consider. You're starting your own projects online at 17. You will do a lot of cool stuff in your life. Don't let this kill that inertia. My 2c. reply shawndrost 9 hours agorootparentprevI don't believe it is feasible to run an image hosting service with your intended CSAM management plan. Based on this comment thread, I fully expect your site will soon host CP/CSAM, if it doesn't already. Other image hosting services devote extensive resources to engineering a solution to this problem (one that is more robust than user reporting). I would not expect you will be able to avoid this work, and avoid liability. Edit: I just noticed you're quite young. Congrats on all the great work. I think this CSAM thing could bring misery your way so I hope you find this comment helpful. Good luck out there. reply tjbiddle 17 hours agoprevShould add the names of your competitors under the logos. I'm sure they're very famous, I'm assuming Imgur and something else - but I don't recognize either of the logos. reply metadat 17 hours agoparentThis will be a solid display of confidence. That'd be cool if the self-hosting use-case was supported in addition to your main public site. Throw a writable path or s3-compatible bucket in a config, set an admin password, and away.. Other folks could finally stop rewriting the basic aspects of image upload hosting. Wouldn't that be a nice gift? reply sweca 17 hours agoparentprevGood call! The second one is Gyazo. reply jiangplus 3 hours agoprevIt looks so awesome! Is it related to Ente? reply sweca 3 hours agoparentNope! Not related, but I have worked on Ente. Ente's CEO is actually the person who told me to try launching on HN. reply isatty 12 hours agoprevThis is very well done, congrats on the launch and I am extremely impressed by your decisions (to keep this as simple as possible, to avoid scanning, and to provide apps). I am equally, if not more, impressed by your replies to comments here and criticism. Keep up the good work! reply sweca 12 hours agoparentThanks, this means a lot! reply scubbo 17 hours agoprevImpressive stuff - especially for a 17-year-old working in their spare time! Best of luck, not that it looks like you'll need it! reply synthoidzeta 15 hours agoprevJust wanted to applaud you — I'm over twice your age and would not be able to create something like this. Can I ask when you started programming/building things? Was it a natural interest or did you receive encouragement from someone in your life? Wishing Horizon all the best, excited to use it! reply sweca 15 hours agoparentThank you so much! I started programming at around 9 years old in MS Batch. By making TUI apps, I learned the fundamentals, like variables, logic, functions, etc. The interest actually stemmed from gaming. I was playing Minecraft and I was like, \"hey, why can't I make cool stuff like this?\" Spoiler alert: I did not remake Minecraft lol. My father was an early adopter of tech, like when he purchased the first Macintosh, so he also encouraged me into the field. reply cntrmmbrpsswrd 11 hours agoprevThis is quote nice. One of the things I've always liked about imgur is the ability to paste images from the clipboard as well as drag and drop onto the page. reply sweca 11 hours agoparentThanks! I'll be sure to add both soon. reply Brajeshwar 18 hours agoprevCongratulations and best of luck. I've a feature request - custom domains. As the images can be shared, I'd with to share with https://mydomain.foo.bar/nice.jpg reply sweca 18 hours agoparentThese are already supported on the Everest plan! Please see the help article: https://horizon.pics/help/articles/can-i-use-my-own-domain. reply p2hari 10 hours agoprevIs there some kind of API support? Could not find anything about ability to use API instead of clients. reply reportgunner 10 hours agoprevSo if I upload a pic and send a link to that pic to my friend does my friend also have to sign in ? reply sweca 10 hours agoparentNope! reply agersant 18 hours agoprevIncredible timing, I was just looking for somewhere to store and share my video game clips. This looks more appealing (cheaper) than Streamable. I love that there is a Linux Appimage but does it also work with just a browser? reply sweca 18 hours agoparentYes, absolutely! You can use the built in uploader[0] on the web dashboard which works in parallel and supports large files. [0]: https://horizon.pics/help/articles/start-uploading-files reply agersant 17 hours agorootparentThank you! Some feedback after using it for a few minutes: - When clicking the \"+ create\" menu, left-clicking whitespace on the page does not close the dropdown - Home page takes very long to list videos (even with a single video). Updates after moving videos/deleting videos/creating folders also feel unresponsive. - On home page or inside folders, videos do not have thumbnails (just a generic camera icon) - While a video is being moved to a directory (spinner be spinning), the delete/move options under the search bar are still present - Would like an option to apply resize/compression settings to videos on upload to help manage storage space - When failing to upload due to size limit, the UI does not say what the size limit is I think this is a promising service so I'll upgrade and hope the quirks improve over time! Edit: - Trying to upload 30-ish 100-200MB videos at the same time in the web UI, many error with `Unable to mark session as finished`. reply sweca 17 hours agorootparentThanks for upgrading! And sorry that you ran into some speed bumps. 1. Good catch. Will improve on this. 2. I'm not sure what could be causing this. My account with thousands of uploads loads in less than one second. 3. Thumbnails are not supported due to compute and processing considerations, but will investigate this. 4. Good catch. 5. Compression can be compute intensive, but will investigate this further. 6. Will add that in. 7. Hopefully this isn't too ubiquitous? I'll investigate what's happening - but from preliminary analysis, it looks like a chunk may have failed along the way. reply agersant 16 hours agorootparentRe: 7 I think in the end all the videos did upload successfully, but the client UI stopped reporting progress at some point. A couple more suggestions: - Would like a select-all button instead of clicking videos 1 by 1 - An option to switch from grid view to list view could be useful - Home UI shows 25 items per page but the grid has 3 columns so the last row is never full - which makes it seem like there are no further pages. - When moving items, the item count in the title of the destination-selection dialog keeps increasing for every move operation. I think items are not deselected after moving? reply sweca 15 hours agorootparentI've added a new redundant check that will ensure all chunks are uploaded before marking the large upload as finished. Hopefully this resolves the issue you were facing. If not, please do send me an email at the address listed at https://horizon.pics/help. As for those files that failed but still show up in the dashboard, it means they're stored incomplete, and as such, will be automatically purged in 24 hours. I will work on hiding them in the dashboard. reply agersant 14 hours agorootparentThanks for the great supportApp Configuration. There are configurations for both files and links/pastes uploads. reply samM_ 17 hours agoprevWow, super impressive. I wouldn't even have dreamed of making something like this at 17. reply 29athrowaway 15 hours agoprevHosting user generated content is one of the quick ways to run into awful content. You have to take the necessary precautions, both legal and technological, to prevent awful content from becoming problematic. Also according to your own terms of service you cannot access the website yourself, that is odd. reply sweca 14 hours agoparentUnder the Adult Content section, yes, if there is adult content, the user must be 18 years of age or older. I see the irony in this. However, for other usage of the site, the privacy policy says that the minimum age is 13 years old. reply RIMR 4 hours agoprevThe marketing says that this is an Alternative to Imgur, but the product doesn't feel like an alternative to Imgur, it feels like an alternative to OneDrive. It shares more features with Gyazo, the other product you compare it to, but I don't see it being a solid upsell. Is marketed to Imgur users as a better alternative for broadly sharing memes on the Internet, and if so why is privacy and encryption important? Couldn't privacy be a limiting factor, since Imgur is first and foremost a social network? Wouldn't the limited space of the free plan be a dealbreaker for someone happily using Imgur in an unlimited capacity for free? Is it marketed to business users that actually prioritize privacy, and if so, aren't you worried that comparing it to a Social Network like Imgur would betray the privacy angle? Where are the enterprise options that allow me to onboard people with SSO/MFA and protect sensitive files from people outside of the organization? There are too many security holes in this product to trust it for business use, especially copying the full URL to every shared object to the clipboard where it can be accessed with anyone with the URL... If someone actually wanted to use this service to store and share large files, you only offer 100GB for $5/month, and limit file sizes to 10GB. For $2/month you can get 100GB on Google Drive with roughly the same capabilities, and for $10 you can get 2TB on Google Drive or MEGA, with the latter being encrypted by default. $10 from Office 365 gets you 6TB of cloud storage with sharing capabilities. The pricing for Horizon storage is outrageous by comparison to every competitor's offerings. Furthermore, you have to read through the pricing page to discover that encryption at-rest isn't default, and you have to turn it on for files you want encrypted, and it doesn't even tell you what kind of encryption or how the recipient decrypts it. Alternatives like MEGA still have you beat in that regard. reply sweca 4 hours agoparentYou're comparing Horizon to cloud storage services, which I think is unfair. Horizon is focused on the sharing aspect. Uploading to Google Drive and OneDrive is not as effortless as Horizon and won't provide the same frictionless viewing experience for whoever you share the links to. Horizon is actually cheaper than alternatives, including Gyazo and Streamable, as said in another person's comment. Global encryption is enabled at rest for every file, but with a key that I control. Toggling the Encryption feature (capital E to distinguish the feature name), encrypts the file again with a key not stored by Horizon. This info can be found in the help center. reply Dylan16807 2 hours agorootparent> You're comparing Horizon to cloud storage services, which I think is unfair. Horizon is focused on the sharing aspect. Uploading to Google Drive and OneDrive is not as effortless as Horizon and won't provide the same frictionless viewing experience for whoever you share the links to. Yeah... It's nice that you get a business opportunity out of this, but one of my first thoughts was how easy it used to be to host images on Dropbox. But they made it bad, and the other big syncing services have the same bad experience. They clearly don't want to be image hosts. reply fragmede 2 hours agorootparentNot even Open AI, which arguably has the resources to programmatically \"look\" at images and know if they are okay, wants to do image hosting. That should give you pause. What do their lawyers know that you don't? reply sweca 2 hours agorootparentI'm not sure why OpenAI would. They're an AI research company. You're confusing scope with their legal team's considerations. reply Dylan16807 1 hour agorootparentprevWhy would they want to? Also these services still make it easy to share a folder of images, they just make embedded uses a pain. reply RIMR 3 hours agorootparentprev>You're comparing Horizon to cloud storage services, which I think is unfair. Horizon is focused on the sharing aspect. I think the closest competitor you have is MEGA, and they are not a Cloud Storage service, they are a File Hosting service like Horizon is. There is plenty of overlap between the offerings of Cloud Storage and File Hosting services, but if sharing files with others is a central feature, the service is generally understood to be a File Hosting service, and that would definitely include services like OneDrive and Google Drive too. >Uploading to Google Drive and OneDrive is not as effortless as Horizon How specifically is it easier? Create an account, upload files, optionally with a client. API for uploading programatically. Sharable URLs for giving access to others. It seems fundamentally equal to other platforms in ease-of-use. >Horizon is actually cheaper than alternatives, including Gyazo and Streamable Gyazo's entry tier costs the same as your's, and also offers an Enterprise tier with SSO/MFA that is going to be a operational requirement of any business customer that cares about privacy and security. Streamable's entry tier may cost twice as much ($10/month), but it comes with 5x the storage (critical if you're sharing 4K HDR video), and the guarantee of a highly available CDN. It is marketed towards a specific need, instead of the broad social/business product you have created, which is superfluous to anyone who just needs reliable video hosting. >Global encryption is enabled at rest for every file, but with a key that I control. As good as unencrypted if this key is hot and used to decrypt all files shared without user encryption. reply sweca 3 hours agorootparentGyazo does not cost the same. You're comparing monthly prices *billed annually* to Horizon's month to month term, which is an unfair comparison. Horizon is $3.75 a month billed annually, while Gyazo is $4.99. reply motles 13 hours agoprevDo you do all the design/UI/UX yourself? reply sweca 13 hours agoparentI designed mockups in Figma, and then realized them into code. At first, there weren't any fancy animations, so I enlisted the help of my friend Rens (info linked in the footer) to help bring it to life! reply mkmk 15 hours agoprevOut of curiosity, why did you name the paid plan Everest? reply sweca 15 hours agoparentIf you didn't notice, the logo has a mountain inside the first \"o\". \"Everest\", as in \"Mount Everest\", follows the theme of mountain metaphors. Mount Everest is also a high point, which symbolizes the fact that the plan gives you higher powers/limits. reply KerryJones 13 hours agoprevGreat work! 2 questions: 1) Do you support RAW images? 2) Do you have larger paid plans? reply sweca 12 hours agoparentThank you! 1. Images that are natively previewable are supported in the browser at the moment. RAW is unsupported for previews, but should still be able to be uploaded for file sharing. 2. I haven't seen any demand for larger paid plans yet. If this arises, I'll be sure to explore new pricing. reply peterldowns 15 hours agoprevExtremely impressive. Very nice work. reply Dalewyn 12 hours agoprev>Unlike Imgur, Horizon has absolutely no ads, doesn't sell your data, has built-in security and privacy controls, and is fully focused on your file sharing needs. No social media or other bloat. Why and how is your service's financial future secured? I took a quick glance, and your free tier is perfectly adequate for most people. >Horizon allows you to quickly upload and store all types of files, from images and video, to PDFs and other documents. Can I easily upload something from any of my computers (desktop, laptop, phone, tablet) in basically one or two easy steps? One of the reasons I don't use imgur anymore and instead use Discord (yes, Discord) for my image hosting needs is because I could not upload anything to imgur from my phone or tablet. Yes, an image hosting service that won't let me upload from the most prevalent computing devices of our time. I don't care if imgur is spamming me with ads or torturing me with a terrible UI, an image hosting service that won't let me upload is literally useless to me. On Discord I can upload anything by just clickdragging or copypastaing into the client. Dead simple, dead easy, dead quick, dead done. And it's not even an image or file hosting service, it's a bloody instant messaging and voicecomms service. >The desktop app is completely free to use! It's powered by Tauri using TypeScript, SvelteKit, Sass, and Rust. Don't care about any of that jargonsoup, is it lightweight and fast with a UI that respects humans? Also, why is there no Windows client? reply sweca 12 hours agoparent1. Maybe it's a matter of perspective. I actually find the free plan to be quite restrictive, especially for long term use. The current model is designed to be profitable. The free plan is sustainable. 2. Yes, you can. Just open the web app, click Upload Files, and select what you want. More details here: https://horizon.pics/help/articles/start-uploading-files. 3. Yes, it's lightweight, fast, and user-friendly. It's quite minimal looking, and I've tried my best to make it as easy to understand as possible. Whether that's through clear and concise instructions along the way, tooltips or intuitive action button colour hierarchy. There's no Windows client because I just haven't gotten to it yet. There's ShareX, which is compatible with Horizon. reply Dalewyn 12 hours agorootparent>Maybe it's a matter of perspective. Definitely! I think 500MB is more than enough, even as a tech guy, for a rolling storage with oldest giving way to newest. Most people aren't going to upload files as big as 75MB either. Unlimited bandwidth is icing on the cake. While on the subject, $45 for 100 GB of cloud storage a year is a rip off (no offence). I can (and do) pay $70 for a year of Microsoft 365 Personal which gives me 2TB of cloud storage and the industry standard office suite. I can (and do) pay $20 for a year of Google One which gives me 100GB cloud storage and thus Google Drive and Photos space. Your pricing by comparison isn't competitive, at least to me. Between an uncompetitive product and a free tier that is (read: should be) more than adequate for most users I'm still curious how secure your financials are. reply nathants 10 hours agoprevcongrats on the launch! this is very cool. reply Jamie9912 16 hours agoprevWhy don't you have cloudflare caching enabled on the images? reply sweca 16 hours agoparentFor privacy reasons, it's kept out of Cloudflare's cache. But one could argue that it doesn't matter since traffic is flowing through Cloudflare anyways. What do you think? reply electroly 16 hours agorootparentIt's not a real option. You'll eventually get banned or forced to pay for an enterprise plan if you try to use Cloudflare's CDN for an image hosting site where the origin is outside of Cloudflare. They require you to use R2 or Cloudflare Images. From their terms: (emphasis mine) > Unless you are an Enterprise customer, Cloudflare offers specific Paid Services (e.g., the Developer Platform, Images, and Stream) that you must use in order to serve video and other large files via the CDN. Cloudflare reserves the right to disable or limit your access to or use of the CDN, or to limit your End Users’ access to certain of your resources through the CDN, if you use or are suspected of using the CDN without such Paid Services to serve video or a disproportionate percentage of pictures, audio files, or other large files. We will use reasonable efforts to provide you with notice of such action. https://www.cloudflare.com/service-specific-terms-applicatio... reply sweca 15 hours agorootparentGood catch. Forgot about this terms of service clause. reply Jamie9912 15 hours agorootparentAre you not using R2 for storage? reply Jamie9912 16 hours agorootparentprevYeah it's going through CF anyway. I also don't live near Europe or North America and noticed even small images take over a second to load, would be nice to have images cached locally after they are opened for the first time reply sweca 16 hours agorootparentGot it. Thanks for your feedback. All servers and user content are hosted in the EU, so for some parts of the world, it may not perform as intended. I've also thought of creating replicas for latency reasons, but it does introduce extra costs. reply r0ks0n 6 hours agoprevi mean, there's already puush, pixeldrain and tens of other tiny chibisafe/pomf/uguu/jirafeau instances that play well with sharex instead of bothering with their own app, but having another mirror can't hurt. reply renewiltord 12 hours agoprevHey, dude, your site is pretty sick. Great job. Optional email! A minor gripe: the blue on green for your “Sign in with google” is kind of hard to read on my iPhone. Black or white perhaps might be better? How do you make the economics work? Cloudflare in front? R2 for storage? Holy balls and a desktop app in Tauri?! Dude you are a skilled 17 yo. You’ve been doing it for 3 years? Haha the kids are going to be all right. Good shit, mate. Screenshot: https://i.horizon.pics/jtlZYhVxzb reply sweca 12 hours agoparentThank you! I work hard to impress with all of my sites. Looks like the \"Sign in with Google\" text is only blue on iOS Safari. On other browsers, it's white on green. I'll investigate further. Cloudflare is indeed in front. The pricing model has been tuned to allow for one paid subscription to subsidize many free accounts. Horizon uses Backblaze B2 as the object storage provider. I'm always eager to learn new technologies! reply iJohnDoe 18 hours agoprevVery cool! Congrats on the launch! reply dizhn 4 hours agoprev> Unlike Imgur, Horizon has absolutely no ads, doesn't sell your data, has built-in security and privacy controls, and is fully focused on your file sharing needs. No social media or other bloat. This is exactly how imgur started. It was a reddit user, providing a service for fellow redditors. I am not being snarky. As others have said, media hosting can never really be free. reply vivzkestrel 3 hours agoparentif this was decentralized it would still have meaning but this project is going to run into the same limitations imgur did reply progforlyfe 3 hours agorootparentone potential difference is that it already offers a $5/month paid version, not sure how long it took for Imgur to do that. so at least it has the potential to not need ads ever, but fully dependent on whether the revenue from the paid version can pay for the costs of the free version (500 MB offered for free users). Edit: but indeed I am usually annoyed by the \"Privacy first, no ads forever!!!111\" marketing. Unless it's some bitTorrent like technology that decentralizes the load, it's not really possible forever. reply tadbit 17 hours agoprevnext [2 more] [flagged] sweca 17 hours agoparentHaha. Didn't even realize that. The goal was mountain-themed metaphors. Hopefully the \"Horizon\" prefix is enough to differentiate. reply remram 15 hours agoprev\"Full-stack engineer\" is usually a job title or career, I find it weird to introduce oneself as a \"17-year-old full-stack engineer\". Is it just me? You certainly have the skills but can one be an amateur professional? reply sweca 15 hours agoparentI do have a job as a software engineer in the professional world. I do both professional work and amateur side project work. Did I miss something? reply remram 15 hours agorootparentNevermind then! I assumed 17 was too young to have a full-time high-skill job, if I assumed wrong then apologies. reply iddan 10 hours agorootparentYou obviously did reply t1c 15 hours agoprev [–] So... it's an S3 bucket with a CDN? reply AvieDeckard 2 hours agoparentFor a Linux user, you can already build such a system yourself quite trivially by getting an FTP account, mounting it locally with curlftpfs, and then using SVN or CVS on the mounted filesystem. reply pratio 15 hours agoparentprev [–] It's an impressive lot more. There's a desktop app for mac and even a beta version for Linux. reply RIMR 4 hours agorootparent [–] So it's an S3 bucket with a CDN, and a file upload client... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "James, a 17-year-old full-stack engineer from Canada, has developed Horizon Pics, a file hosting service focused on security, privacy, and user needs, without ads or data selling.",
      "Recently, Horizon Pics underwent a rebrand and launched a new desktop app called Alpine, which auto-uploads screen captures and clipboard content, offering features like auto-deletion and end-to-end encryption.",
      "The app is built with modern technologies such as Tauri, TypeScript, SvelteKit, Sass, and Rust, and Horizon Pics offers both free and paid plans with varying storage limits."
    ],
    "commentSummary": [
      "Horizon Pics, developed by a 17-year-old full-stack engineer from Canada, is a private alternative to Imgur, offering ad-free and secure file hosting.",
      "The service recently launched a rebrand, including a new desktop app called Alpine, which auto-uploads screen captures and clipboards with privacy options.",
      "Horizon Pics provides both a free plan with limited storage and a paid plan with higher limits, aiming to attract users seeking privacy-focused file hosting solutions."
    ],
    "points": 259,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1721083536
  },
  {
    "id": 40971553,
    "title": "The Mafia of Pharma Pricing",
    "originLink": "https://www.thebignewsletter.com/p/inside-the-mafia-of-pharma-pricing",
    "originBody": "Share this post Inside the Mafia of Pharma Pricing www.thebignewsletter.com Copy link Facebook Email Note Other Discover more from BIG by Matt Stoller The history and politics of monopoly power. Over 115,000 subscribers Subscribe Continue reading Sign in Inside the Mafia of Pharma Pricing 4% of all the money in America flows through a few mafia-like health care conglomerates. The FTC just released a ground-breaking report on how they operate. And it is gearing up to sue. Matt Stoller Jul 15, 2024 195 Share this post Inside the Mafia of Pharma Pricing www.thebignewsletter.com Copy link Facebook Email Note Other 8 Share Welcome to BIG, a newsletter on the politics of monopoly power. If you’d like to sign up to receive issues over email, you can do so here. In 2022, an executive at a health insurance company expressed concern at the games his company was playing with drug prices. He pointed at the drug Gleevec, which when it was approved in 2001, was a miracle treatment for blood cancer. “For a lot of people, Gleevec was simply too good to be true. But these once-dying patients were getting out of bed, dancing, going hiking, doing yoga. The drug was amazing,” said the researcher who discovered the chemical compound behind the drug, Brian Druker. Gleevec was also expensive, at $26,000/year when it launched. By 2015, when the patent ran out, it was priced at $132,000/year, bringing in $4.7 billion for Novartis that year alone. But like all drugs, its patent expired, and soon it faced generic competition. As more companies entered the market, the cost of this drug, according to a pricing metric used by pharmacies called the National Average Drug Acquisition Cost (NADAC), fell by 99%. This kind of pricing change is a tremendous policy success, it’s the idea behind American patent policy, and how that was implemented in pharmaceuticals through the Hatch-Waxman Act of 1984, which allowed for a period of tremendous profit for the creator of a drug, and then fostered a path for generics to bring the price down after the patent ran out. But then, something odd happened. Though the acquisition cost of Gleevec collapsed, in some places, the cost to the payer did not. Here, for instance, is what happened in Ohio. Most patients didn’t front the full cost, paying co-payments, but Medicaid systems and companies using certain insurers had to cover it on the back end. Such costs are one reason that health care costs keep going up far above the rate of inflation. Why was there such a big spread between the cost of buying the drug and the price being paid by payers? It wasn’t what most people expected, that the pharmaceutical company Novartis, had found a way to maintain its patents. The answer, according to a new Federal Trade Commission report, is that a small group of middlemen were inflating the price. These middlemen are called pharmacy benefit managers, and what they are supposed to do is clerical work. When a pharmacy dispenses medicine, it sends the claim to a PBM, which bills the insurer and patient and remits the pharmacist money. But because of a legal change I’ll explain, just three PBMs are now in control of pricing of pharmaceuticals, and are redirecting vast amounts of money to themselves. And they are why the price of generic Gleevec didn’t drop as it should have. In fact, the concerned executive worked at the parent company of one of these PBMs. He was not worried about the price, but that such a price would look bad, and possibly violate the law. “We've created plan designs to aggressively steer customers to home delivery where the drug cost is ~200 times higher,” he wrote. If you went to Costco, he went on to say, the cost was $97, so the plan didn’t recommend patients go there. If a patient went to Walgreens, which the plan did recommend, it was $9000. And if a patient chose home delivery via the PBMs own mail order pharmacy, it was $19,200. “The optics are not good and must be addressed,” he added. He didn’t need to say that the added revenue for PBMs, just for that one drug, was $902.1 million over a few years. Another drug studied by the FTC, one for prostate cancer called Zytiga, was similarly inflated by an extra $685 million. That’s $1.6 billion for the two of them. Think about all the drugs used in our healthcare system, and that will give you a sense of why healthcare costs keep going up. (Yes, each inflated drug cost is an economic termite.) Most people think of high pharmaceutical prices, and blame the companies you’d expect in Big Pharma. These firms, with storied names like Merck, Pfizer, Novartis, Genentech, etc, are powerful, and they are in the business of developing and selling drugs. But this other group, corporations you haven’t heard of, composed entirely of middlemen, price and handle payment for pharmaceuticals between doctors, pharmacies, and patients, are perhaps equally important, if not more so. And unlike pharmaceutical companies, who actually employ doctors and scientists, PBMs don’t do anything difficult. They keep lists. PBMs are a topic I’ve covered, mostly as a cause of the disappearance of rural independent pharmacies, but last week, the FTC sparked a big political fight by releasing this report on the PBM business model. Chair Lina Khan did so with support from one Republican commissioner, Andrew Ferguson, but opposition from another, Melissa Holyoak. The FTC also indicated in the report’s final pages, and the Wall Street Journal noted, that it was likely to sue PBMs over the high prices they foster for insulin, which could potentially change the entire drug claims system, and bring down prices dramatically. From Clerks to Mafia To understand the world of pharma pricing, we have to start at the beginning. In the 1950s and 1960s, pharmaceutical development exploded, as did the health insurance industry. Insurers needed a way to help their patients to easily buy them the new cornucopia of drugs at pharmacies. Enter pharmacy benefit managers, who were claims processors that issued a plastic card to insurance customers, got pharmacies to accept those cards, and tracked the money flows, including reimbursements. PBMs were very similar to Visa and Mastercard, with the very first PBM actually called “Pharmaceutical Card System.” PBMs soon started to create what are called drug formularies, which are lists of drugs payers will cover, specifying the kind of drugs included and the cost-sharing arrangements. They also put together networks of pharmacies, and began helping design strategies to encourage patients to take the correct medicine for the lowest price, known as utilization management. It was a business to business service. If you’re an employer, a union, a state or Federal government program, and you need drug benefits for a group, you hire a PBM and they set you up with a range of formularies and pharmacy networks. Today, commercial health plans, Medicare drug plans, Medicaid managed care, and affiliated health insurers use PBMs. But then, several changes shifted the business model. The first was consolidation. From the 1980s to the 2000s, PBMs consolidated both horizontally and vertically, so each big PBM is now owned by a major healthcare conglomerate. CVS owns Caremark, Cigna has Express Scripts and UnitedHealth Group (UHG) runs Optum Rx. (For a case study, see my 2020 piece called “How CVS Became a Health Care Tyrant.”) There were big acquisitions, like Express Scripts buying Medco in 2012 and CVS buying Aetna in 2018, but smaller ones as well. “According to PitchBook,” the FTC report noted, “these four entities and their subsidiaries (which include the largest PBMs) collectively engaged in more than 190 transactions over the 2016 to 2023 period (UHG, 88; CVS, 53; Humana, 39; and Cigna, 14).” UHG, for instance, is one of the top PBMs, but it also employs 10% of the doctors in America, is America’s biggest health insurer, runs Optum Bank through which bailout funds during Covid flowed to hospitals, and even owns Change Health, the large payment network for hospitals and pharmacies. More importantly, the big PBMs, while they contract with independent pharmacies, also own their own retail and mail-order pharmacies, thus setting the reimbursement rates for themselves and their competitors. (UHG is a frequent target of medical comedian Dr Glaucomflecken.) The other change was the legalization of price discrimination, which fostered an era of secretive pricing. PBMs had been clerical agents, paid by insurers to negotiate with drug makers, or on a per transaction basis to handle transactions. But in 1987, Congress passed an exemption to a Medicare Anti-Kickback statute, which created a safe harbor for group buying entities to accept payment from drug manufacturers in the form of rebates, with certain guardrails in place. Enabling kickbacks created a clear conflict of interest, since a PBM is supposed to be negotiating for the buyer, but could now be paid by the seller. The second change was an antitrust suit in 1994, where pharmacies sued to get the same discounts for drugs offered to health insurance plans and hospitals. In the settlement of that case, the parties and the judge said that if a buyer was big enough that it could prove it could shift market shares, it was entitled to a secret rebate. And the third was in 1999 when the government used its authority to explicitly say that rebates were permissible legal discounts. Today’s system, where PBMs get large secret rebates in return for allocating market shares, was born. Today, it’s virtually impossible to get any clear pricing on most drugs, because there is no one price. There are different benchmarks, for instance the one I used above to describe what manufacturers get for selling Gleevec. But there are other benchmarks depending on who you are, such as average wholesale price (AWP), wholesale acquisition cost (WAC), Average Sales Price (ASP), Average Manufacturer Price (AMP), Maximum Allowable Cost (MAC), Usual and Customary (U&C), etc, each of which means something different, and many of which are arbitrary mark-ups designed to make it looks like there are discounts when there aren’t, or to extract extra money from pharmacists or patients. Many drug pricing contracts use several of these benchmarks, along with rebates, reimbursements, and fees that are sometimes assessed six months after a point of sale transaction goes through. The endless number of such adjustments make it impossible to know what anything costs. Just one PBM, for instance, has over ten thousand different price lists for drug reimbursement rates, sometimes updated daily, which means the price of a particular drug depends on who you are, when you bought it, other drugs you might have purchased, or what the PBM executive had for lunch. Pharmacists are often not allowed to look at the price they would be given for reimbursement prior to submitting a claim because, as one PBM manual notes, \"price lists are the PBMs confidential and proprietary information.\" There are often multiple prices for the same transaction. A drug might seem like it costs $100 at the point of sale for a drug that cost the pharmacist $50, where the patient would pay a $60 copay and the insurer paid $40. But then six months later the PBM will claw back $75 from the pharmacist in what’s called a DIR fee. In that case, because of the copay, the insurer/PBM is actually extracting money from both the patient whose medical costs it should be covering, and the pharmacy it should be reimbursing. The whole system is insane . Price discrimination and consolidation weren’t separate phenomena, but drove each other. In fact, if a PBM had to charge the same price for its service to all comers, then it would not matter if a PBM were big; a drug manufacturer negotiating with a PBM would have to give the same price, regardless of size. It also wouldn’t make sense to own its own pharmacy, nor could it charge different amounts for cancer drugs and keep the difference. But the legalization of price discrimination changed the market. And today, there’s really no way to convey how weird this world of pricing is, except that it feels Soviet and arbitrary to everyone in it. The legislation of price discrimination happened in areas beyond PBMs and health care, it is perhaps the biggest victory of the law and economic school, and certainly one of the most important legal shifts in our society in the last fifty years, far more impactful than most of the economic fights we hear about. Before the 1980s, it had been a fundamental axiom of America that everyone gets access to the same publicly posted price for the same class of service or good. Grangers fought for this principle in the 1870s, populists in the 1880s, business and labor leaders at the 1899 conference on trusts agreed on it even though they disagreed on everything else, it was understood as part of the Sherman Act in 1890, explicitly embedded into the Clayton Act in 1914, written into the Robinson-Patman Act of 1936, and across dozens of statutes across state and Federal government, from railroad to shipping to electric utility rules. I mean, even the conservative Heritage Foundation advising the Reagan administration in 1981 decried secret rebates and discounting to select customers. “These kinds of discounts seem contrary,” it said, “to basic American precepts of justice, as they would “effectively end the value of publicly posted prices” and “favor the large organized interests with competitive alternatives at the expense of the unorganized, uneducated, or captive.” That’s the main policy recommendation to the most conservative Republican President since Herbert Hoover. But starting in the 1970s, economists decided that price discrimination, far from problematic, was actually efficient. Indeed, if you could charge a high price to the rich, and a low price to the poor, then price discrimination could also be progressive. What could be wrong with selective discounting, after all? It turns out, plenty, and if you want to understand why, just look at PBMs. Today, as a result of these changes, PBMs are big. Really big. The parent insurance companies of the biggest PBMs top nearly $1 trillion in revenue annually, roughly 4% of the GDP of America. Just the top four equal 22% of national healthcare expenditures, up from 14% in 2016. And no other country has anything like the PBM industry. The revenue of American PBMs is larger than what France spends on its entire healthcare system. Why Is Insulin So Expensive? The top three PBMs have immense market power. While it looks like an oligopoly, PBMs have monopolies in certain areas. OptumRx, for instance, runs 83% of the South Carolina retail pharmacy network management services, while a different PBM has 85% of the market in Alabama. If you are a company or union that needs to buy prescription drugs or if you are a pharmacist that dispenses prescriptions, you have to run on their rails. And they use their size and bulk to thwart rivals. They use information from their insurance arms to conduct marketing campaigns targeting patients of rival pharmacists. They make it easier to buy drugs if you buy them from their mail order pharmacy, a practice known as steering, they impose endless fees on pharmacies, and they also take kickbacks from pharmaceutical manufacturers to steer their patients to more expensive medication and cut off access to cheaper generics. They also go after rival PBMs, with the FTC noting that “Some health insurers reportedly do not permit their clients to comparison shop for PBM services; rather, the client must use the PBM affiliated with the health insurer.” (The giant PBMs also have exclusive contracts with certain pharmaceutical makers, which means that if you don’t use those PBMs you just can’t get critical medicines.) The net effect is higher prices. Here, for instance, is a rebate contract for Sanofi’s Lantus, a popular branded insulin. The list price might be high, but that’s not all going to Sanofi. In fact, 63% of that was going to the PBM in the form of a rebate, as long as Lantus is the only insulin offered to the patient. In other words, there’s competition, but not to lower prices to the consumer, but to pay higher rebates to the PBM. It’s not just insulin, of course. Take the most expensive drug historically in America by aggregate revenue, Humira, which made over $22 billion for AbbVie in 2022. It recently lost its patent protection, and there are several generics, known as ‘biosimilars,’ coming into the market. Several drug manufacturers (Coherus, Sandoz, Boehringer Ingelheim, etc.) invested in developing biosimilars to launch when the Humira patent expired. These drugs brought the sticker price of the drug down 10x from $80,000 per year to as low as $8,000 per year. BIG is a reader-supported newsletter focused on the politics of monopoly and finance. If you are not yet a paid subscriber, please consider becoming one. BIG is journalism and advocacy that challenges power. You can always get lies for free. The truth costs a few bucks, but in the long run it’s much cheaper. Subscribe But none of these are able to get onto the shelf. Instead, what happened is that CVS started a drug manufacturing company called Cordavis to sell Humira biosimilars to its specialty pharmacy (CVS Specialty). They charge $1,300 per month, compared to some that are less than half that. Since CVS owns a PBM they controlled the formularies - and therefore the drug access - of between a fourth and third of Americans. They directed their PBM to preference their own drug through Cordavis and ignore the other biosimilars available on the market from smaller pharma companies, adding what I’m told is $50-100 million to CVS’s bottom line. The negative impacts are twofold. First, consumers have to pay much more for the drug. Second, the pharma companies who invested to make biosimilars suffered financial losses. Here’s the share price of Coherus, which makes a Humira biosimilar. That means we won’t see any more biosimilar R&D anymore, because everyone knows that PBMs will block them from the market. Political Battles and BUCAH For decades, PBMs operated in the shadows. But the passage of Obamacare changed the dynamic of health care, because it capped the profits of health insurance companies. To get out from under this cap, insurers began vertically integrating into different areas, and in the 2010s, every major PBM turned into an arm of a conglomerate. (See “How Obamacare Created Big Medicine”.) Today, there’s an acronym that describes who controls health care. It’s BUCAH, meaning Blue Cross Blue Shield, UHG, Cigna, Aetna and Humana. And employers started getting angry at high prices. Mark Cuban, for instance, started a pharmacy called Cost Plus Drugs, and is running a high-profile campaign against PBMs. More importantly, a group in Ohio, a random Wall Street quant named Eric Pachman, a pharmacy lobbyist named Antonio Ciaccia, and a pharmacist named Ben Link began playing around with public pricing data, and realized that government Medicaid programs were getting ripped off by hundreds of billions of dollars a year by PBMs for no reason. They started an organization called 46 Brooklyn, and their research has led to changes in dozens of states. In 2021, for instance, Kentucky got rid of its use of big PBMs in Medicaid, and saved $285 million out of $1.2 billion its program spent on prescription drugs. It even led to an attempt by the Trump administration to get rid of the ability of PBMs to engage in certain forms of secret rebating. Academics are now focused on the problem of vertical integration, and so is Congress. And there have been dozens of PBM-related hearings - the next one is on July 23 with the CEOs of the ExpressScript, OptumRX, and Caremark - and Congress is closer to passing PBM legislation than it has ever been. For decades, the FTC, far from helping, was on the wrong side of the argument. Its economists had promoted the idea that price discrimination and rebates were good, that vertical integration made sense, that PBM consolidation would help consumers, and that a PBM owning a mail order pharmacy was not a conflict of interest, but was efficient. But in 2021, then-FTC Commissioner Rohit Chopra shocked the commission by attacking his own agency, saying “There is a growing consensus that the Federal Trade Commission’s approach… is not working.” In 2022, the FTC put out a policy statement saying that the use of rebates by dominant middlemen in the insulin market were a potential legal violation. It also launched the investigation that led to this report. What’s interesting is that the report is not final, because PBMs would not submit the information the FTC subpoena’d. In fact, some of the PBMs said they would send certain documentation until 2025. It’s a clear run out the clock strategy. The FTC could sue the PBMs to get this information, but it’s not clear that would be faster, and it would eat up litigation resources. And there’s good original information in the report, including the cancer drug analysis overpayment story, which is based on raw claims data. Normally, the commission waits until it has completed its analysis to publish a report, but in this case, the FTC sought to release the interim analysis and publicly name the PBMs as operating as bad faith actors for withholding information. The politics then flipped. The vote to release the report was 4-1, with the three Democrats seeking to put it out, along with Republican FTC Commissioner Andrew Ferguson. Ferguson not only voted to release the report, but said that the FTC should have sued, arguing they weren’t being aggressive enough. His colleague Melissa Holyoak, however, dissented, and said the report needed economic analysis to show that consumer prices were affected. It was a standard U.S. Chamber of Commerce process-troll, and Holyoak likely thought she would get plaudits for opposing Lina Khan. Instead, at a Congressional hearing, multiple Republican members were annoyed with her, including Rep. Buddy Carter, who is an actual pharmacist that needed to deal with PBMs and understood the problem up close. A lot of government reports sit on the shelf and gather dust. This one won’t. It’ll lead to litigation by the FTC, and it’ll change litigation and state-level lawmaking happening all over the country around PBMs. It will also affect private markets, as CEOs begin asking why they are paying so much for their pharmaceuticals. Here’s Mark Cuban: It could even prompt Congress to pass PBM reforms, though too many are still listening to economists, and not yet sold on the need to get rid of price discrimination and rebating. These are weird days in politics and policy, but under the surface of our wild news cycle, the tectonic plates keep moving. No one likes monopolies. And the days of PBM as the organizer of our pharmaceutical markets are numbered. Thanks for reading! Your tips make this newsletter what it is, so please send me tips on weird monopolies, stories I’ve missed, or other thoughts. And if you liked this issue of BIG, you can sign up here for more issues, a newsletter on how to restore fair commerce, innovation, and democracy. Consider becoming a paying subscriber to support this work, or if you are a paying subscriber, giving a gift subscription to a friend, colleague, or family member. If you really liked it, read my book, Goliath: The 100-Year War Between Monopoly Power and Democracy. cheers, Matt Stoller Subscribe to BIG by Matt Stoller Thousands of paid subscribers The history and politics of monopoly power. Subscribe Error 195 Share this post Inside the Mafia of Pharma Pricing www.thebignewsletter.com Copy link Facebook Email Note Other 8 Share",
    "commentLink": "https://news.ycombinator.com/item?id=40971553",
    "commentBody": "The Mafia of Pharma Pricing (thebignewsletter.com)245 points by foolswisdom 21 hours agohidepastfavorite98 comments jimt1234 18 hours agoI worked at a PBM back in the late-90s/early-2000s. It was where I was introduced to the value of customer data and the strange world of lawyers, all in a single corporate meeting: - The company is launching a new service. We already sell customer drug-prescription data to drug companies, and the drug companies analyze this data to understand where/when/why/to-whom their drugs are being prescribed. Now we're going to help the drug companies advise doctors on where/when/why/to-whom they prescribe drugs. - Sounds great. Where do we come in? - The new service will act as a middleman, processing payments from drug companies to doctors. - So, a service to manage kickbacks? [Meeting room full of suits goes silent.] - The payments aren't \"kickbacks\". They're \"rebates\". - Is there a difference? - Absolutely. [silence] - So...what's the difference? - Please be sure to only use the term \"rebate\" in all communications, especially email. Never use the term \"kickback\". And that was pretty much it. The company processed prescriptions for pharmacies, then sold that data to drug companies, who in turn used that data to provide kickbacks to doctors for pushing their drugs over a competitor. And it was all legal, thanks to the lawyers and their select word usage. Oh, and I think we weren't supposed to use the term \"middleman\", either. reply Aurornis 18 hours agoparentYou can actually look up payments from certain companies to doctors now: https://openpaymentsdata.cms.gov/ I’ve checked several doctors that I’ve visited over the years. None of them show up, with one exception: The doctor who immediately set off my scam alarms when she tried really, really hard to get me diagnosed with sleep apnea, despite not one but two very clearly negative sleep studies. I could never understand why she was pushing so hard, until I looked her up in this system. She takes an incredible amount of money from drug companies and device manufacturers. I don’t know if a scheme like you described would even be allowed today. If it is, the bigger medical systems are actually quite strict with doctors taking anything resembling a payment like this, from what my friends in the industry tell me. reply NickC25 5 hours agorootparentI broke part of my hand last October after slipping and falling. At the request of my new insurer, I went to my local urgent care. The rep for my insurer swore up and down that they had an X-ray machine and could diagnose my problem quickly, as well as immediately after refer me to a local hand specialist at the local University's hospital. After my experience, I realized that nothing the rep said was true. The nurse practitioner at the urgent care facility said nothing was wrong with my hand (despite it being black & blue, and having all the hallmarks of a broken hand). He refused to refer me to a specialist or for an X-ray unless I took an HIV/AIDS test. He started asking me several questions about my sexuality and relationship status - I am not sexually active (sadly) nor am I in the demographic with a higher likelihood of coming into contact with the HIV virus, so I told him as much and declined the test. I was there because I broke my fucking hand. He kept insisting that I needed to take the tests. I walked out of the facility pissed off and without any progress on my hand. Several hours wasted. The guy's assistant called me the next day pleading me to come in for tests, and I reiterated that all I wanted was to get an X-ray like I was promised by my insurer, and to see someone who knew what they were talking about.* If your tire pops, your mechanic shouldn't say \"nothing wrong with your tires, it's your exhaust you need to worry about\" while he's wearing a shirt with an exhaust maker's logo on it. I clicked on the link you posted. I entered the nurse practitioner's name, and surprise surprise, he's gotten over $3k in the last year from ViiV Healthcare, a company specializing in the research and development of HIV/AIDS testing equipment and drug development. Not an exorbitant amount of money at all, but the dude is ethically compromised. That needs to be straight up illegal with serious repercussions - for both those who offer the kickback, and those who accept it. *FWIW, I spoke with a physical therapist not too long later, a guy who specializes in sports injuries (particularly from basketball as he's a former pro hooper himself), and within about 10 seconds of examining the area of injury said \"yeah, you broke your hand. I see this frequently. Do this every day with your hand, and come see me in 2 months if nothing works\". reply phonon 2 hours agorootparentMake an ethics complaint to the state board. reply NickC25 33 minutes agorootparentIs there a statute of limitations for ethical complaints in the medical field? reply willcipriano 25 minutes agorootparentNot one that is less than a year. reply dralley 4 hours agorootparentprevMake sure to leave a review reply NickC25 3 hours agorootparentI did, and nobody seemed to care. reply BobaFloutist 1 hour agorootparentprevI remember looking up my psychiatrist, who, it turns out, periodically gets an ~$25 meal from various drug reps. I suspect this does little to influence his prescribing decisions. reply penguin_booze 12 hours agorootparentprev> she tried really, really hard to get me diagnosed with sleep apnea, despite not one but two very clearly negative sleep studies This is highly unethical. People should report such instances to relevant regulators, at least as a matter of record. A single such report won't have much teeth, many of similar nature might catch someone's attention. reply bboygravity 10 hours agorootparentprevGenuine question: what pharmaceutical could your doctor have benefitted from to help against sleep apnea? As far as I know there's no meds for it? Only surgery, CPAP devices, mandibular advancement devices, etc. Would she get kickbacks from those medical device companies as well? reply RALaBarge 8 hours agorootparentWhen you get a CPAP machine, the medical supplier then will try to give you replacement pieces super frequently. Things like the mask annd the tube each cost more than $100, so yeah, I’m sure it’s still kind of the same deal even without pharmaceuticals. reply starrleight 3 minutes agorootparentKeep it together And move away silently from them all Silently. Mom New town I'll.find you reply mft_ 11 hours agorootparentprevThanks for sharing this - very interesting. It looks like it’s a register of payments from Pharma either for legitimate services rendered (e.g. consultancy, with travel costs) or hospitality (e.g. food and drink). Does it also capture other payments - like the ‘kickbacks’ discussed above? (It’s difficult to check this without exhaustively searching for random names!) reply lifestyleguru 12 hours agorootparentprevDoctors remunerated or having an incentive to diagnose specific disease (diabetes, covid, boreliosis, etc) really terrify me. Developed world needs a ruthless transparency in this. reply sofixa 11 hours agorootparent> Developed world needs a ruthless transparency in this. Do you have sources in this happening somewhere other than the US in the developed word? And I'd say such transparency is needed all around the world. Developing nations can fall in the same traps developed ones did. reply bookofjoe 3 hours agorootparenthttps://www.the-fence.com/the-drugs-dont-work/ reply IG_Semmelweiss 16 hours agoparentprevThis is illegal nowadays, but rebates endure between manufacturers and PBMs. It works like this: Nanufacturer sells abc drug to PBM for x price. There is an agreement between them, that if pbm sells y number of abc, then pbm gets a rebate. However, this gets the PBM in a jam. Now they have to somehow sell this crazy overoriced brand drug to the insurer. So they do a sleight of hand. So the PBM agrees with manufacturer to. . increase price! Why? For an edge in the conversation with insurer/employer: PBM: here's brand drug abc. Price is x^2. I am soooo good at negotiating your prices, that I was able to get it for z instead. You see! Thats a 50% price reduction. Am I not awesome. Insurer/employer: thats great. I'll be able to sell this 50% reduction off sticker to my manager. Thanks! PMB keeps gwneric competition out of formulary, ensuring no competition And eventually, PBM receives a rebate check for their troubles. And its totally legal. reply lifestyleguru 12 hours agoparentprevDon't even have to work with them to notice this. Over here all dentists tell you to use Elmex products, noted on Elmex branded post it. Elmex's \"rebates\" and \"symposiums\" must be glamorous. reply refurb 17 hours agoparentprevKickbacks have a specific meaning - a payment made in order to get business. Data rebates aren’t kick backs, they pay for something they wouldn’t otherwise get - patient level data. The PBMs get paid a double digit percentage of purchases as a rebate when quality data is sent back. It’s optional and PBMs can decide not to do it. reply thaumasiotes 13 hours agorootparentBuying something you don't want is an extremely traditional way to launder illegal payments to the person you're buying from. reply bryanrasmussen 10 hours agorootparentthat's true - on the other hand why wouldn't you want to gather high quality data about patients using your drugs? reply thaumasiotes 8 hours agorootparentMost obviously, because the data isn't valuable to you. Why wouldn't you want to gather high quality data about recent Little League games? But in the case where it is valuable, and the purchasing party would really like to give some free money to the other party, we can be sure that the purchasing party is overpaying for the data anyway. reply ProjectArcturis 19 hours agoprevSeveral years ago, I worked as a data scientist for Express Scripts, before it was acquired by Cigna. I can't much speak to the macroeconomics of PBMs, but I can say that they were the worst technical organization I ever worked for. They were built out of mergers on top of mergers on top of acquisitions, so their IT systems were what you get when you duct tape a dozen legacy systems together. I worked in the \"Innovation Lab\", which had been designed to look like an ad agency's idea of Innovation -- brushed metal, Edison lightbulbs, that kind of thing. They'd bring clients through on tours to show off how much Innovation was going on. Meanwhile, I didn't really have that much to do, and no one seemed very concerned about that. Soon I realized I was also part of the decoration - a genuine Data Scientist, hard at work Innovating. Our group produced approximately nothing. Our boss's boss was evaluated mostly based on how much he was able to sell people's medical data for. reply albroland 13 hours agoparentNot sure if you'll take it as reassuring, or alarming, but having worked with a few PBMs on the insurer side ES was the most tech-competent. Worst probably being CVS Caremark. reply rancar2 6 hours agorootparentMy experience with ES in the 2010s is that they ran a very barebones staff so the people keeping the lights on were quite good and some of the best among my Fortune 500 clients. I had the pleasure of training their staff on my stack expertise as they did not want me to just do the work, they wanted to make sure they were experts at the end as well so they could support and evolve the stack overtime. reply lifestyleguru 12 hours agoparentprevLast time I made blood test it really pissed me off why their forms require national ID and phone number. The nurse in turn was pissed at me why I'm reluctant to write down phone number. reply sofixa 11 hours agorootparentIdk how it works where you are, but blood tests I've done in France require your phone number or email address for a notification and MFA when logging into the blood testing laboratory's platform to get your results. The National ID I'd presume is to put the test results in the relevant healthcare systems so that your doctors (and only them) can have access to them. reply lifestyleguru 11 hours agorootparentExactly, also phone numbers are coupled with national ID and medical IT systems are leaky and outsourced into oblivion. In that case they didn't provide alternative to send link over email. Later on test results with full personal info can be acquired in darknet and Russian speaking internet [1]. Are there any consequences? The authorities advice to \"be careful\" and the service provider says \"they're sorry\". [1] - https://www.gov.pl/web/baza-wiedzy/hakerzy-ujawnili-kolejna-... reply the_other 9 hours agoparentprev> sell people's medical data for Why isn't this illegal? reply ProjectArcturis 5 hours agorootparentBecause there is a large industry based around sharing medical data, and that industry has lobbyists. reply colejohnson66 8 hours agorootparentprevHIPAA can be “waived” by signing the medical release forms you didn’t read. reply giancarlostoro 8 hours agorootparentLast I ever read something on a form that concerned me it seemed worded like “your insurance might not cover your stay if you dont disclose this information” and I ignored that and just let it be. I wonder if thats how they trick people into it. It should be illegal for anyone to sign anything that has the possibility of their data being sold without being made aware of this in plain English “WARNING YOUR DATA COULD BE SOLD IF YOU SIGN THIS”. I dont trust ad companies not to screw up my PII. reply ggm 18 hours agoprevAnd people say that public ownership is \"less efficient\" than private industry, and less efficient than regulated private industry. Well.. I don't buy it. Access to drugs and efficient pricing and rationing (because that is what it is) is not working well. It's a massively distorted market. The public good here would be better served by another model. Even the \"we need these prices to recover our massive sunk costs\" part of the argument is bogus. Much good drug design and research is done on the tertiary education and research budget worldwide. There is absolutely no single-process need to do drug IPR based models, the profit motive is not the only model. I look to the modern mRNA drug emergence to lead to radical shake up in the cost of production of novel treatments. We're seeing some signs of this, along with other changes in drug models: injectable hypertension treatment is in test. Imagine the impact on the cost basis of a pill-per-day model! reply Scoundreller 15 hours agoparent> We're seeing some signs of this, along with other changes in drug models: injectable hypertension treatment is in test. Imagine the impact on the cost basis of a pill-per-day model! Tablets are generally dirt cheap to produce. Sterile injectables adds a lot of variables and requirements, both in production and sometimes also in distribution (cold chain?) and administration (directly and sometimes requiring reconstitution). Maybe if you have an API that’s expensive to produce and has poor bioavailability an injectable might be cheaper. A long acting injectable antihypertensive will have a place for some, but creates other issues: can’t stop it quickly and hypertension often requires multiple agents to treat. I wish poly-pills took off :( reply bobthepanda 9 hours agorootparenthonestly what looks to be more promising is the twice a year HIV shot. Truvada and Descovy are not cheap. reply t-writescode 4 hours agorootparentin the United States, at least, (and until very, very recently, I believe), PrEP is required to be covered under healthcare; and, services like good rx make Truvada specifically very accessible in price, even free. In the United States, please look into these services and what your insurance provider does cover because, at least for now, it should be *very* affordable. (between free w/insurance to $30/mo with GoodRX). I do think that price is higher than I saw the last time I looked it up, but I don't have that search on hand to confirm. reply mft_ 9 hours agoparentprev1. Development of a single new drug costs several billion.[0] There are outliers which cost less, but billions is a decent estimate on average. 2. There is continuing need for new drugs: in 2023, FDA approved 55 new drugs [1] not including new indications for existing drugs (which also cost money to achieve). 3. It therefore follows that (in the current regulatory environment) just maintaining the current level of development and approval of new drugs would probably cost over 100 billion per year (possibly not including manufacturing and supply). 4. Drug development is therefore hellaciously expensive, while being high risk (i.e. high risk of failure) and a long-term endeavor. 5. In our current world, the model that has evolved is that this money is raised from investors, entrusted to experts, and if everything goes well, capital plus profit are recouped via drug prices. Payors swap huge up-front costs with an uncertain outcome, for later huge costs with a known outcome. 6. Replacing this model would therefore likely take global collaboration between governments, funding expensive high-risk long-term projects, with the end-goal of (much) cheaper drugs - an end-goal that would likely take a decade to meaningfully realise (importantly, much more than a single electoral term). It would be politically momentous, as it would effectively destroy a significant and established sector of the economy. 7. As much as I like this concept, I simply don't see any evidence that we're even close to ready for such a model of global collaboration and funding. Even when the challenge was urgent and potentially existential (e.g. a global pandemic) the response was often the opposite of collaborative. 8. Interestingly, though, it wouldn't even be very expensive on a world scale. The GDP of (just) the top 20 nations is roughly 88 trillion.[2] 100 billion is barely over 0.1% of that. If you shared the cost by GDP amongst the top 20, US and China would pay a lot, but involved European nations would pay in the region of 2-5 Bn/year. 9. This obviously doesn't deal with the issue of ongoing costs for already-approved drugs... === [0] https://www.forbes.com/sites/matthewherper/2017/10/16/the-co... [1] https://www.fda.gov/drugs/novel-drug-approvals-fda/novel-dru... [2] https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nomi... reply the_other 9 hours agorootparent> Drug development is therefore hellaciously expensive, And yet, drug companies still have outrageous profit margin. reply tlb 7 hours agorootparent“Profit margin” doesn’t subtract the initial development costs, just current costs. So drug profit margins should be large to repay the billions previously spent in development. reply aidenn0 5 hours agorootparentThe term is insufficiently specified; there is \"Gross Profit Margin\" which considers only COGS, so works roughly as you suggest. There is also \"Operating Profit Margin,\" which would include current R&D costs, which (assuming they are non-declining over time) would account for development costs. If the company took on debt to develop the drugs, then \"Net Profit Margin\" would also include the cost of servicing that debt. reply mft_ 6 hours agorootparentprevHmm, if for the sake of argument we assume that spend on R&D is kept constant, then profit margin (being money left over after spending on stuff) does indeed account for development costs. In reality (I think this is your point) there's a big time shift, in that profit earned in 2024 is thanks to R&D spend over the past decade or so. However, the 2024 profit margin would incorporate the 2024 R&D spend contributing towards new drugs and indications over the next decade or so, and with a with a constant R&D spend these would effectively cancel out. (In reality, of course, R&D budgets do fluctuate.) reply COGlory 4 hours agorootparentprevNot really, their margins typically wind up comparably with other manufacturing. They just have way way higher costs. Modern drugs are expensive, for a ton of reasons. See this talk about pharmaceutical finances: https://www.youtube.com/watch?v=3LGqQJFdoWM reply refurb 17 hours agoparentprevI think the point you’re missing is that many parts of pricing are government regulations. And the way that the myriad of regulations drive some of this behavior. A great example is the 340B program. A government regulation requiring manufacturers to offer steep discounts to hospitals. It’s created all sorts of distortions including the purchasing of pharmacies by hospitals so they can access the discount but charge the full price for the insured. The US healthcare system is terrible in big part because of regulations. reply kevingadd 16 hours agorootparentIs there a successful healthcare system you'd point to that was achieved via deregulation? reply shiroiushi 16 hours agorootparentI'm not sure OP was implying any claim that removing regulation would lead to a great healthcare system, just that the US regulations are bad and causing many of the system's problems. In better-run nations, the healthcare systems are highly regulated, but the regulation is actually (mostly) intelligent and implemented to have a positive effect. For some reason, when the US tries to do regulation, it somehow manages to do a uniquely bad job at it, causing a negative effect. reply soco 10 hours agorootparentMaybe it depends who you want to benefit with your regulation - the patient or the provider? Because I'd argue providers seem to make good money within the US regulation. reply shiroiushi 10 hours agorootparentI'm not so sure about that actually. I think the real winner in the US system is the insurance companies. reply lotsofpulp 4 hours agorootparentprev340B legislation is one of the clearest examples of corruption. I cannot believe anyone can be so stupid, much less two branches of government, to pass something that does nothing but allow for corruption. Goal: provide poorer people with more access to healthcare and medicine Step 1: require medicine manufacturers to sell medicines at low cost to medicine retailers if they want the government to buy the medicines via Medicare/Medicaid Step 2: there is no step 2. There is no requirement for medicine retailers to sell the medicine to end users at low cost. Result: Medicine retailers arbitrage this by obtaining medicine for cheap and selling it to high prices to insurance, paid for by premium payers. Meaning people with health insurance paying extra so that medicine retailers can earn money…to do nothing. reply specialist 4 hours agorootparentprevRegulations is just a scary word for rules. There's always rules. The trick is to fashion a ruleset (game, marketplace) that maximizes for public good (long term). reply ggm 16 hours agorootparentprev> The US healthcare system is terrible in big part because of regulations. The US regulatory landscape is crippled by lobbying and fear of \"socialism\" -This is completely fixable by a competent regulator. reply specialist 4 hours agorootparentTrue. Alas, SCOTUS just overturned The Chevron Doctrine. (Along with their other attacks on the administrative state.) Hard to predict the aftermath. reply edm0nd 3 hours agorootparentand that imo is a good thing. A recent example is the ATF's expansion of the NFA by redefining a dealer from \"Someone who makes selling firearms their primary income\" to \"Makes a profit on a sale\" They effectively made everyone who ever sells a gun privately a felon. And the ATF is headed by a guy who has zero firearm experience and couldn't even separate the slide off a glock. Its a perfect example of a bad faith action and we only barely blocked its enforcement. Agencies with chevron were able to railroad whatever politicians wanted with zero accountability. reply drewda 20 hours agoprevWhile I have mixed feelings about The New York Times's coverage of certain topics these days, this is one topic where their reporting has (positively) shaped events. They had a big investigative piece earlier in the summer about pharmacy benefit managers: https://www.nytimes.com/2024/06/21/business/prescription-dru... And that likely led to the recent FTC announcements. reply conductr 16 hours agoparentOn this timeline, it seems to me much more likely they caught wind of the FTC’s attention being put on this topic. reply WilTimSon 9 hours agoparentprevNYT is still good at coverage, I'd just argue that the optics of their coverage can be removed from their original, erm, style, at times. (I don't want to say \"politics\" but we all know I mean politics.) Still, they do excellent work to this day, just with questionable detours. reply edg5000 14 hours agoprev\"If you went to Costco, he went on to say, the cost was $97, so the plan didn’t recommend patients go there. If a patient went to Walgreens, which the plan did recommend, it was $9000. And if a patient chose home delivery via the PBMs own mail order pharmacy, it was $19,200.\" They must be joking right? reply JumpCrisscross 9 hours agoprev“PBMs consolidated both horizontally and vertically, so each big PBM is now owned by a major healthcare conglomerate. … In 1987, Congress passed an exemption to a Medicare Anti-Kickback statute, which created a safe harbor for group buying entities to accept payment from drug manufacturers in the form of rebates, with certain guardrails in place. … PBMs get large secret rebates in return for allocating market shares…it’s virtually impossible to get any clear pricing on most drugs, because there is no one price.” For a change the solutions seem simple: 1. Prohibit integration between doctors and insurers, on one hand, and pharmacies and pharmacy-benefit managers, on the other hand; 2. Repeal §§ d and f from the kickback exemptions [1]; and 3. Require public filing of insurers’, PBMs’ and pharmacies’ price lists. (Not disclosure: the binding price list is the public one.) [1] https://www.law.cornell.edu/cfr/text/42/1001.952 reply aidenn0 5 hours agoparentWe have spent most of the last 50 years under an antitrust enforcement regime that considers vertical integration to be harmless, so #1 might be simple, but it's definitely not easy. reply coretx 15 hours agoprev\"Intellectual property\", specifically patents are the elephant in the room. Even if the mafia would want to have a healthy market, there won't be one because you can't when there is monopolies in place. Societies can research, speculate, mitigate and regulate until the end of times for as long as the underlying fundamentals of the problems are never addressed. reply thadk 17 hours agoprevReading this 4% Pharma conglomerate flow figure, I'm indirectly struck that increasing all-cause cancer likely raises GDP, particularly this segment of GDP. My main consolation in this moment is that lead exposure probably doesn't. reply Terr_ 16 hours agoparentSee also: Bastiat's \"broken window\" fallacy. https://www.investopedia.com/ask/answers/08/broken-window-fa... reply BenFranklin100 20 hours agoprevI work in biotech. It’s a long, difficult, and expensive process to develop a new therapeutic. It is immensely discouraging to see middlemen riding on the back of biomedical innovation and enriching themselves at the expense of the scientist toiling in the lab all the way to the patient in the hospital. reply Terr_ 16 hours agoprevGeneral philosophizing: Is it possible there's an important economic difference between public price discrimination versus secret price discrimination? I mean there's a spectrum between \"acceptable\" price discrimination and \"abusive\". Nobody bats an eye at lower rates for bulk purchases or movie theaters offerering half price for kids. It may not be a panacea, but sunlight is still a pretty good [civic] disinfectant. reply thaumasiotes 8 hours agoparentHere's a pretty simple, accurate description of price discrimination: - The purpose of price discrimination is to reduce the share of gains-from-trade received by consumers to zero. The concept is that you should never feel that you're getting more value out of a purchase than you would otherwise have gotten from the money you spent on that purchase. It's true that, as the article mentions, this means that impoverished people might buy things that, absent the price discrimination, they wouldn't have bought at all. But this \"benefit\" can't be worth much, because -- even though they paid a small price for whatever it was -- we know that they valued it only slightly more than that. Given this, it's not really clear why price discrimination is supposed to be beneficial. > I mean there's a spectrum between \"acceptable\" price discrimination and \"abusive\". Nobody bats an eye at lower rates for bulk purchases or movie theaters offerering half price for kids. Lower rates for bulk purchases is not an example of price discrimination. Half price for children's movie tickets might or might not be - if your theory of the discount is that people will pay more to go on a date than they will to entertain their children, that would be a valid example of price discrimination. If your theory is that children have less money than adults, that doesn't really work - kids young enough to get cheaper movie tickets don't pay for their own tickets. reply daft_pink 19 hours agoprevI don’t really understand why this is possible or if the information in this article is fully true. It doesn’t make sense why we can’t just go around the pbms. reply BobaFloutist 1 hour agoparentCost plus pharmacies do still exist. The problem is, some drugs really are just way more expensive, so if you already have \"free\" (or heavily discounted) insurance from your workplace, it's a bit of a waste not to use it, especially since expensive drugs do also contribute to your deductible. Also, wholesalers also negotiate with PBMs, so the cost plus pharmacies might not be able to get the drugs at the same rate PBM-friendly pharmacies do. reply ProjectArcturis 19 hours agoparentprevNo one else has the scale to even begin negotiating with the Pfizers of the world. Try to buy something from them directly, and the price would be \"Fuck you\". reply elhudy 17 hours agorootparentGPOs are reasonably large and capable reply daft_pink 12 hours agorootparentprevi thought the point of this article was that generic drug prices aren't going down. reply CPLX 9 hours agoparentprevGo around how? The point is it’s a consolidated monopoly. The same people own every step in the chain. They use this particular step to extract monopoly profits. reply ThinkBeat 8 hours agoprevThis is a great article, weel researched. Filled with information as many links to sources. I'd like to extend a big thank you to the author for writing this. When it comes to remedies One solution is for our legislature to create laws that specific species some of those practices as illegal. It would probaby be hard to get it passed due to all the corruption in our legislative and executive branches. reply its_ethan 20 hours agoprevSetting aside what happens for the uninsured (which is important, I'm just trying to simplify for my own understanding), isn't this mostly the levying of costs of very expensive drugs onto the insurance providers, rather than the patient? Meaning the \"victim\" of the price gouging is the insurance company? If you have insurance with a yearly out of pocket max of say $8,000 and the drug you're taking has a very veiled and seemingly dubious cost of $80,000 - does that effect the patient? I assume it does somewhat directly in the form of higher monthly payments (for the patient and other customers of the insurance)? Can the insurance company deny access due to the high cost? If this is somewhat the case, I would sort of expect insurance companies to be lobbying for the system to be changed, and they seem to have the capital to actually make a difference in that \"fight\"? Maybe I'm misunderstanding something though.. it was an interesting article but it really just gave me even more insight into how confusing the US healthcare system is, even beyond what patients actually interact with. reply aidenn0 4 hours agoparent> Setting aside what happens for the uninsured (which is important, I'm just trying to simplify for my own understanding), isn't this mostly the levying of costs of very expensive drugs onto the insurance providers, rather than the patient? Meaning the \"victim\" of the price gouging is the insurance company? The article touches on this, but perhaps doesn't spell it out sufficiently: 1. Insurance companies have their profits legally capped 2. To get around this: somehow (merger, purchase &c.) end up with Company X that owns both an insurance company and a PBM 3. The PBM price gouges the insurance company, increasing PBM profits (which is legal, unlike increasing the insurance company profits) 4. The Insurance company passes the price-gouging on in the form of increased premiums. 5. Company X makes more money by charging higher premiums, just like it would if it had (illegally) increased the profit margins of the insurance company. reply dahinds 20 hours agoparentprevThe PBMs have mostly been captured by the insurance companies, so they're charging themselves and pocketing the profits themselves. Insurance companies just pass on the costs by charging higher premiums. reply colechristensen 19 hours agoparentprev>If this is somewhat the case, I would sort of expect insurance companies to be lobbying for the system to be changed, and they seem to have the capital to actually make a difference in that \"fight\"? Insurance companies have the opposite incentive. Their profit is, to simplify, a percentage cut of the total amount spent on medical care. If the cost of medical care goes up, they raise rates and the market pays for it (what else is it going to do?) the poor or underemployed or non-traditionally-employed suffer. They will fight a small amount to keep costs low, but only in a relative sense in that they want to beat their competitors or not pay for one off extremely expensive things. The middleman to which you give a fixed percentage isn't really all that motivated to get you your best price. reply FireBeyond 19 hours agoparentprev> isn't this mostly the levying of costs of very expensive drugs onto the insurance providers, rather than the patient? Meaning the \"victim\" of the price gouging is the insurance company? I feel there's the obligatory remark here of \"and how exactly is the insurance company paying for it?\". > I assume it does somewhat directly in the form of higher monthly payments (for the patient and other customers of the insurance)? Absolutely directly. > Can the insurance company deny access due to the high cost? They have little motivation to do so. They just up the premiums. They're limited by law on administrative overhead costs, and are mandated to return unspent premiums (or roll them over), so the only way to make more money is \"increase premiums, so we're allowed higher administrative overheads\". This was a hallmark of Martin Shkreli. He liked to paint a picture of \"I'll ensure you're only paying a low copay while the insurance companies take the hit for this drug I'm charging exorbitant pricing[1] for\", as if customers thought that insurance had a magical money fairy, rather than that money was coming from them (albeit usually indirectly through their employer). Sadly, he was often right - a non-negligible amount of people saw him as an everyday hero, sticking it to the man. [1] Yes, pharma has R&D costs. Shkreli's company didn't do much R&D though, mostly patent acquisition[2]. [2] \"Fun\" story about that. New drug comes before the FDA for approval, and it is opened up for comment. Shkreli lodges an objection to approval of this drug. Why? Because it's unsafe? No - trials thus far have shown it to be safer than the existing drug options. Why? Because it's less effective? No - it's also been shown to be more effective than existing drugs. Perhaps it's more expensive? No - cost of R&D and production, and estimated retail costs are expected to be lower than existing drugs. Huh, odd. So why in this case would Shkreli oppose this drug getting to the market? The only reason he lodged an appeal with the FDA had nothing to do with the drug, butbecause he and his company had just bought the patent to one of those 'existing drugs' referenced, and this new drug coming to market would crater the demand for his drug, and as a result torpedo the profitability of his investment/gamble. Fuck Martin Shkreli. reply jmyeet 18 hours agoprevYou cannot talk about the problems with pharma pricing without talking about enclosures [1]. Consider: 1. Health care providers are largely banned from importing drugs [2]; 2. Medicare is largely banned from negotiating drug prices [3] 3. The VA was allowed under Obama to negotiate drug prices, something which was promised but never delivered for Medicare. The GAO shows this has reduced costs [4]; 4. Pharma companies will tell you R&D is expensive. It is but it's the government paying for it. Basically all new novel drugs relied on public research funds [5]; 5. Pharma companies generally spend more on marketing than R&D [6]; 6. What R&D pharma companies actually do is typically patent extension [7]. The true \"innovation\" of capitalism is simply building layers and layers of enclosures. [1]: https://en.wikipedia.org/wiki/Enclosure [2]: https://journalofethics.ama-assn.org/article/what-should-pre... [3]: https://www.healthaffairs.org/content/forefront/politics-med... [4]: https://www.gao.gov/products/gao-21-111 [5]: https://www.cbc.ca/news/health/drugs-government-funded-scien... [6]: https://marylandmatters.org/2024/01/19/report-finds-some-dru... [7]: https://prospect.org/health/2023-06-06-how-big-pharma-rigged... reply ffgjgf1 10 hours agoparent> It is but it's the government paying for it. Basically all new novel drugs relied on public research funds [5 The fact that it was partially government funded doesn’t mean that the drug companies didn’t have to put it in a significant % of their own money. Of course it varies but it’s a bit like saying that e.g. Tesla is/was government funded (well kind of but not really) reply max_ 12 hours agoparentprevWhat you have described here is the phenomenon that Stigler describes as \"Regulatory Capture\". [0] Regulatory capture is the use of state resources (mostly regulation). To tilt the ground of business in thier favour at the expense of the public and other competitors. People beg for \"regulation\" from the government but the problem is that the politicians are often puppets of the very corporations they are meant to regulate. That's how we end up with regulatory capture scams. Is that \"capitalism\"? I don't think so. Does it contain traces of capitalism? Yes. But I think what Americans have is more of corny-capitalism, state-capitalism & regulatory capture. It is very different from Hayekian capitalism which is actually anti-crony capitalism & anti regulatory capture. [0]: https://en.m.wikipedia.org/wiki/Regulatory_capture reply tsimionescu 11 hours agorootparentAll of the phenomena described above would happen with minimal necessary regulations as well (preventing imports for fear of quality and safety reasons, which must be regulated in pharma). In general, the problem with US capitalism is insufficient regulation to force a free market between corporations, allowing massive consolidation and discouraging competition on price. Any free market principles will never work if you have less than a few tens or even hundreds of companies competing, for a market as huge as pharma. reply max_ 10 hours agorootparentMaking importers legally liable for damages that be linked to to bad quality & other safety concerns is more effective than a central body trying to tell people what is \"safe\" or \"good quality\". This strategy is still regulation but it is not based on interfering it is based on disincentives. Limitation of imports is usually just a scheme to facilitate cartels & protectionism. >Any free market principles will never work if you have less than a few tens or even hundreds of companies competing, for a market as huge as pharma. It is often \"regulation\" that makes it very difficult for new entrants to compete. It is said that it costs about $3B for a corporation to bring a new drug into the market today. Fees people can afford that. And most of that money is just to satisfy obscure regulatory requirements. Not many people can cough up $3B per drug. reply tsimionescu 9 hours agorootparent> Making importers legally liable for damages that be linked to to bad quality & other safety concerns is more effective than a central body trying to tell people what is \"safe\" or \"good quality\". No, after-the-fact compensation is not at all an effective way to regulate something as critical to health and as hard to measure as pharmaceuticals. Access to the justice system is already extremely limited for regular people, making it even more critical to the functioning of society would be crazy. Not to mention, this type of liability opens things up for trolling at a massive level: people often die or have severe problems while on drugs, particularly the most important drugs. Every cancer patient dying while on any of a cocktail of 15 drugs is 15+ lawsuits from an unscrupulous lawyer, even if the drugs were perfectly safe. Plus, drugs have to be not only safe, but effective. I can't sell homeopathic remedies as drugs, not because they are unsafe, but because they don't do anything. How would liability work for drugs that aren't effective? In practice, if you attempted this sort of \"regulation\", what would quickly happen is that overwhelmed judges would quickly accept some non-solution like drug makers labeling every possible side effect imaginable on every drug and then, caveat emptor, we told you our vitamin C might kill you and our homeopathic pill might not cure your lung cancer, we're not liable if you chose to pay us for them anyway. And, in fact, this is exactly what medicine was like before the FDA was established to regulate things correctly. And the costs of putting a new drug on the market are that high only because we, sanely, require extensive testing that also safeguards patients' lives and rights, before accepting a new drug. Not to mention, the more the new drug improves over the status quo, the less testing is actually required. A lot of the most famous drugs that have these huge go-to-market costs have these problems because they are extremely minor improvements over existing drugs/drug cocktails, so they need large sample sizes to find any effect at all, and have to conclusively show side-effects are not worse than the state of the art. If someone came up with a molecule tomorrow that, say, stopped progression of Alzheimers dead in its tracks the moment you started taking it, I can assure you that it would cost much less than 3B dollars to get that approved and on the market (the current drugs barely show slight slowing down over months after therapy is started). reply refurb 19 hours agoprev [–] Like most mainstream media reports it misses a lot. I worked in the industry and The NY Times article misses key points. There is no “price” for a drug, there are several prices - list, net, Medicaid, AMP, ASP. So yes, while the list price for Gleevec has gone up, the actual price paid is very different. It’s the same for insulin - the price that manufacturers have received has gone down 41% from 2014-2020, while the list price has gone up 140%. https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh... If people want to really understand how it all works, I recommend the Drug Channel blog by Adam Fein. He does a great job of digging into details, pulling data and showing what prices are actually doing. https://www.drugchannels.net/?m=1 reply elevatedastalt 19 hours agoparent> So yes, while the list price for Gleevec has gone up, the actual price paid is very different. You say it like it's a good thing. It's part of the problem. The fact that there is no clear no-nonsense to get a drug at a reasonable price without jumping through a bunch of hoops which aren't even clearly documented is not a good thing. We are talking of life-saving drugs here, not hacking frequent flier miles or credit card rewards. reply worik 16 hours agorootparent> no clear no-nonsense to get a drug at a reasonable price without jumping through a bunch of hoops Yes Is possible I have a friend who imported the drugs she needed to treat type C hepatitis from India Proceeded to set up a buyer's club that transformed the lives of dozens of people What a stupid system, and how the most vulnerable suffer to benefit the most powerful reply chasil 1 hour agorootparentI had heard that a treatment had been developed that cost ~ $100k. Looking at the wiki, was it solvadi/sofosbuvir or olysio/simeprevir? \"While access to the newer treatments was expensive, by 2022 prices had dropped dramatically in many countries (primarily low-income and lower-middle-income countries) due to the introduction of generic versions of medicines.\" https://en.wikipedia.org/wiki/Hepatitis_C reply refurb 17 hours agorootparentprevThere is a good system - that works for insured patients. It’s the uninsured that get hosed NY Times talks about list prices but that price isn’t even used by insurance companies. reply ffgjgf1 10 hours agorootparentHow is it good? How do the insured benefit from the complete lack of price transparency besides having to pay significantly higher premiums than they would otherwise? reply sofixa 11 hours agorootparentprev> There is a good system - that works for insured patients Does it? I've read plenty of horror stories of people getting denied treatment by insurance, or paying tons of money for insurance barely covering anything. And more broadly, how much money is wasted on multiple layers of beyond useless middlemen? How much money do hospitals spend on admin and billing departments dealing with this bullshit? Strong \"No Way to Prevent This, Says Only Nation Where This Regularly Happens\" vibes. reply Cupertino95014 18 hours agoparentprevYou've described the problem: > here is no “price” for a drug Yes, there is. If we look on their 10-K statements, they will have a fixed cost and a variable cost assigned to each drug. The variable cost will be just the manufacturing, and the fixed will be research and overhead allocated to this drug. Total cost = fixed + variable. The rest is profit. We can decide what a \"reasonable\" profit is, and Total Cost + Profit is the notional \"price.\" That doesn't mean treating them like a state monopoly and the final price is fixed by law, but it does tell us a \"price.\" All those things you mentioned (list, net, Medicaid, AMP, ASP) are kinda irrelevant. reply Scoundreller 15 hours agorootparentI like to say the first dose of a drug costs a billion dollars and all the rest ever after cost a few cents. reply refurb 17 hours agorootparentprevCost-plus pricing creates all sorts of perverted incentives in and of itself. And I’m not sure we want to go with “government mandated profit margins”. Do all drugs have the same margin? Even if the benefit is the same? That creates perverted incentives. It’s not a simple problem to solve at all. reply Cupertino95014 16 hours agorootparentI don't think you read the whole thing. I specifically said \"no cost-plus pricing.\" It's just a notional price for comparison with the real ones. reply refurb 15 hours agorootparentLooking at cost and setting a price is cost-plus pricing. reply Cupertino95014 14 hours agorootparentHello? Do you understand the word \"notional\"? reply foolswisdom 16 hours agoparentprevIt sounds like you're saying exactly what TFA says? Bit odd to criticize an NYT article (and without linking) as a comment on an article that does get into these details? reply Projectiboga 18 hours agoparentprev [–] So when a diabetic goes to get lifesaving insulin and some mistake happens w insurance they are paying the inflated price. Not rare, since insulin comes in bottles which can and do break. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The FTC's new report exposes how a few healthcare conglomerates, likened to a mafia, control 4% of all U.S. money through pharmaceutical pricing.",
      "Pharmacy Benefit Managers (PBMs) are identified as key players inflating drug prices by controlling pricing and redirecting funds to themselves.",
      "The FTC's actions and potential legislative changes aim to address the monopolization of pharmaceutical pricing by PBMs, which has led to significantly inflated costs."
    ],
    "commentSummary": [
      "The post discusses unethical practices in the pharmaceutical industry, particularly involving Pharmacy Benefit Managers (PBMs) and drug companies.",
      "Key issues include the sale of prescription data, legal \"rebates\" that function as kickbacks, and the influence of financial incentives on medical diagnoses and prescriptions.",
      "The conversation highlights the need for transparency and regulation to address market distortions and protect patients from unethical behavior and inflated drug prices."
    ],
    "points": 245,
    "commentCount": 98,
    "retryCount": 0,
    "time": 1721078439
  },
  {
    "id": 40974193,
    "title": "The Engineer’s Guide to Deep Learning: Understanding the Transformer Model",
    "originLink": "https://www.interdb.jp/dl/",
    "originBody": "The Engineer’s Guide To Deep Learning We are in the third golden age of AI. In the previous two golden ages (1950s-1960s and the 1980s), our expectations outpaced the capabilities of the technology at the time, leading to disappointment. In contrast, the AI technology of the current golden age, which began in the mid-2010s, has consistently exceeded our expectations. Among AI technologies, the Transformer, introduced in 2017, stands as a groundbreaking breakthrough. Initially developed as a machine translation model, its impact has extended to permeate nearly every field. Today, the Transformer model is considered essential knowledge for modern engineers. The first goal of this document is to provide the shortest path for engineers to understand the Transformer. What is this document A concise guidebook: This document provides just enough information to learn the Transformer. What this document provides Working Python code examples for hands-on learning: To enhance comprehension, this document provides working Python code examples that readers can run themselves. References for further exploration: This document introduces readers to a variety of documentation options, recognizing that different individuals find different resources more accessible. Contents Part 1: Neural Networks Introduces the fundamental concepts of neural networks. Part 2: Recurrent Neural Networks (RNNs) Explores RNNs, including LSTM and GRU. Part 3: Natural Language Processing (NLP) and Attention Mechanisms Provides the essential principles of NLP, encompassing machine translation and attention mechanisms. Part 4: Transformer Unravels the Transformer model. Appendix: Basic Knowledge Provides the minimum knowledge of Python and mathematics required to understand the Transformer. Change History (since 21st May, 2024) Date Description 21.May.2024 The first version released. Next goal Many Transformer-based technologies are currently being developed. There will definitely be another major breakthrough in the near future. I might write about them if I have time. Copyright © Copyright ALL Right Reserved, Hironobu SUZUKI. For any inquiries regarding the use of this document or any of its figures, please contact me after reading the following FAQ: FAQ Since publishing my content, I’ve been fortunate to receive a lot of positive feedback, which is truly gratifying. However, I’ve also encountered a few instances where people tried to misuse my content for self-promotion in the past. These experiences have shaped the approach I’ve outlined below: Who can use this document freely? If you are a teacher or a student belonging to an educational organization, you can freely use this document and figures in your study. Anyone can use this document and figures with noncommercial meetings and lectures, if you state the link to this site and the copyright; otherwise, contact me. Is it available for commercial contents? This content can be used under two options: Revenue Share: You can leverage this content after a revenue share agreement is signed. Under this agreement, you’ll share 20% of the sales generated from using this content including the github repository. Full Buyout: In very rare cases, I consider requests for full commercial use of all content on this site (and the github repository). For a complete buyout of all content rights, the cost is €10,000,000. Why doesn’t the author waive the copyright of this document or use the creative commons license? I’d like to ask you what problems you have by that I keep on having the copyright of my document. When you send me an email, please provide at least two SNS addresses (e.g. LinkedIn, Twitter) for verification purposes. Due to the XZ backdoor incident, I no longer accept contact from anonymous individuals. Exception Educational institutions can use this document freely. Author Hironobu SUZUKI I am a software programmer/engineer, the author of: The Internals of PostgreSQL The Engineer’s Guide To Deep Learning pg_plan_inspector pg_tuner I graduated from graduate school in information engineering (M.S. in Information Engineering), had worked for several companies as a software developer and technical manager/director, and published seven books (4 PostgreSQL books and 3 MySQL books) in Japanese and a Chinese book. As a director of the Japan PostgreSQL Users Group (2010-2016), I organized the largest (non-commercial) technical seminar/lecture on PostgreSQL in Japan for more than six years, and also served as the program committee chair of the Japan PostgreSQL Conference in 2013 and as a member in 2008 and 2009. In June 2022, my interview article was published. Cuando era joven, vivió en Sudamérica por unos años. Recientemente, a veces vuelve a allí. I am looking for a new job, applying ML and AI technologies to DBMS. I’m interested in History, Animal Rights, Cosmology, Social Issues, Environment Issues. I play the piano and guitar. Vegetarian. I love animals, music, science.",
    "commentLink": "https://news.ycombinator.com/item?id=40974193",
    "commentBody": "The Engineer’s Guide to Deep Learning: Understanding the Transformer Model (interdb.jp)238 points by shxx 11 hours agohidepastfavorite28 comments MAXPOOL 9 hours agoThere are many others that are better. 1/ The Annotated Transformer Attention is All You Need http://nlp.seas.harvard.edu/annotated-transformer/ 2/ Transformers from Scratch https://e2eml.school/transformers.html 3/ Andrej Karpathy has really good series of intros: https://karpathy.ai/zero-to-hero.html Let's build GPT: from scratch, in code, spelled out. https://www.youtube.com/watch?v=kCc8FmEb1nY GPT with Andrej Karpathy: Part 1 https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-1... 4/ 3Blue1Brown: But what is a GPT? Visual intro to transformersChapter 5, Deep Learning https://www.youtube.com/watch?v=wjZofJX0v4M Attention in transformers, visually explainedChapter 6, Deep Learning https://www.youtube.com/watch?v=eMlx5fFNoYc Full 3Blue1Brown Neural Networks playlist https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_6700... reply rvnx 8 hours agoparentIn addition, these websites are totally free. The website listed here: > I consider requests for full commercial use of all content on this site (and the github repository). For a complete buyout of all content rights, the cost is €10,000,000. > I’d like to ask you what problems you have by that I keep on having the copyright of my document. + no commercial-use without paying 20% royalty. So fairly expensive for a Keras tutorial. reply mr_puzzled 7 hours agoparentprevSlightly off topic: I'm interested in taking part in the Vesuvius challenge[0], but I don't have a background in ML, just a regular web developer. Does anyone have suggestions on how to get started? I planned to get some background on practical ML by working through Karpathy's Zero to Hero series along with the Understanding Deep Learning book. Would that be enough or anything else I should learn? I plan to understand the existing solutions to last year's prize and then pick a smaller sub challenge. [0] https://scrollprize.org/ reply trybackprop 5 hours agorootparentI made a list of all the free resources I used to study ML and deep learning to become an ML engineer at FAANG, so I think it'll be helpful to follow these resources: https://www.trybackprop.com/blog/top_ml_learning_resources (links in the blog post) Fundamentals Linear Algebra – 3Blue1Brown's Essence of Linear Algebra series, binged all these videos on a one hour train ride visiting my parents Multivariable Calculus – Khan Academy's Multivariable Calculus lessons were a great refresher of what I had learned in college. Looking back, I just needed to have reviewed Unit 1 – intro and Unit 2 – derivatives. Calculus for ML – this amazing animated video explains calculus and backpropagation Information Theory – easy-to-understand book on information theory called Information Theory: A Tutorial Introduction. Statistics and Probability – the StatQuest YouTube channel Machine Learning Stanford Intro to Machine Learning by Andrew Ng – Stanford's CS229, the intro to machine learning course, published their lectures on YouTube for free. I watched lectures 1, 2, 3, 4, 8, 9, 11, 12, and 13, and I skipped the rest since I was eager to move onto deep learning. The course also offers a free set of course notes, which are very well written. Caltech Machine Learning – Caltech's machine learning lectures on YouTube, less mathematical and more intuition based Deep Learning Andrej Karpathy's Zero to Hero Series – Andrej Karpathy, an AI researcher who graduated with a Stanford PhD and led Tesla AI for several years, released an amazing series of hands on lectures on YouTube. highly highly recommend Neural networks – Stanford's CS231n course notes and lecture videos were my gateway drug, so to speak, into the world of deep learning. Transformers and LLMs Transformers – watched these two lectures: lecture from the University of Waterloo and lecture from the University of Michigan. I have also heard good things about Jay Alammar's The Illustrated Transformer guide ChatGPT Explainer – Wolfram's YouTube explainer video on ChatGPT Interactive LLM Visualization – This LLM visualization that you can play with in your browser is hands down the best interactive experience with an LLM. Financial Times' Transformer Explainer – The Financial Times released a lovely interactive article that explains the transformer very well. Residual Learning – 2023 Future Science Prize Laureates Lecture on residual learning. Efficient ML and GPUs How are Microchips Made? – This YouTube video by Branch Education is one of the best free educational videos on the internet, regardless of subject, but also, it's the best video on understanding microchips. CUDA – My FAANG coworkers acquired their CUDA knowledge from this series of lectures. TinyML and Efficient Deep Learning Computing – 2023 lectures on efficient ML techniques online. Chip War – Chip War is a bestselling book published in 2022 about microchip technology whose beginning chapters on the invention of the microchip actually explain CPUs very well reply mr_puzzled 5 hours agorootparentWow, thanks for the links to all the resources. Lot of interesting stuff for me to learn! reply srush 6 hours agoparentprevThese slides from Lucas Beyer are pretty nice. https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8v... reply SebFender 7 hours agoparentprevoh! 2/ recommendation is an absolute masterpiece of simplicity and effectiveness - cheers for that! reply yobbo 6 hours agoprevThis is a very compressed work-through from perceptron to transformer. When he is working through the gradients of an LSTM, for example, it is to help understanding, not help you implement it in your favourite framework. When he is showing solutions in various frameworks, the purpose is to help create connections between what the math looks like and what code can look like. reply revskill 8 hours agoprevTransformer tutorial is like the new \"Monad tutorial\". reply uoaei 2 hours agoprevOne of the most frustrating things about all the documentation on Transformers is the sole emphasis on NLP. In particular, one of the most interesting parts of the Transformer architecture to me is the attention mechanism which is permutation invariant (if not for the positional embeddings people use to counteract this inherent quality of attention layers). Also the ability to arbitrarily mask this or that node in the graph -- or even individual edges -- gives the whole thing so much flexibility for encoding domain knowledge into your architecture. Positional embeddings may still be required in many cases but you can be clever about them beyond the overly restrictive perspective of attention layer inputs purely as one-dimensional sequences. reply gregw2 10 hours agoprevNo content besides a few paragraphs of intro. Actual content has 404 not found errors. reply gpnt 10 hours agoparentThe menu on the left on desktop is working. It seems only the links on the first page are broken. reply smokel 10 hours agoparentprevThe links in the running text are broken. The links in the hamburger menu work fine. reply tuyguntn 10 hours agoprevquestion to experts of HN in ML/AI. Could you please share the beginner resources you think would worth for a person who wants to switch their domain from CRUD/backend APIs to ML/AI. There seems to be many branches of this domain, not sure where to start. Is my understanding correct? * ML engineer -> engineer who builds ML models with pytorch (or similar frameworks) * AI engineer -> engineer who builds applications on top of AI solutions (prompt engineering, OpenAI, Claude APIs,....) * ML ops -> people who help with deploying, serving models reply belter 8 hours agoparent85% of your ML project time will be spent on Data Quality and a little bit of Domain Feature Engineering. If you want to make an impact, become excellent at those, you will be able to use these skills, for domains like Systems Integration and Business Analytics. Let the people who do Research bring you the Algorithms and nowadays even the trained Models. reply qsort 10 hours agoparentprevNone of these terms have a formal definition. The only association rule you need is: * Fancy Title -> Whatever the company wants it to be. All of the above could realistically span from \"does bleeding-edge work\" to \"has once opened a CSV\". reply blowski 8 hours agoparentprevI'd call the \"AI Engineer\" an Application Engineer, albeit one that specialises in integrating ML into software. reply treme 9 hours agoparentprevKaggle is a good start reply alister 9 hours agoprev> When you send me an email, please provide at least two SNS [social networking service] addresses (e.g. LinkedIn, Twitter) for verification purposes. ... I no longer accept contact from anonymous individuals. It's pretty sad to see that social networking is being adopted as an identification and trust mechanism even by technical people. It was bad enough when some governments began demanding social networking usernames for visa/immigrant screening, but we can't even send an email without social proof to other technical people now? reply belter 8 hours agoparent> I no longer accept contact from anonymous individuals. This reminds of that joke, where a guy shows up at the Air Force HQ recruitment center. They ask, \"Pilot license? Experience? Qualifications?\" He replies, \"Nope, just here to say: Don't count on me!\" reply kltzayh 5 hours agoparentprevWith the ellipsis expanded: \"Due to the XZ backdoor incident, I no longer accept contact from anonymous individuals.\" The XZ cracker could have logged in via GitHub at numerous services. I bet that the OP downloads from PyPI that was potentially compromised for longer than a year due to an overlooked token leak. I further bet that the OP, being in the machine learning space, downloads unauditable, huge Python frameworks from GitHub, conda or PyPI. People in that space also download and experiment with untrusted models. But hey, plain text email which you can read in a command line mail client with MIME and other extensions disabled is the problem! reply _giorgio_ 10 hours agoprevIt uses keras, which is obsolete. Nobody uses that thing anymore. Stay away from this. reply sva_ 8 hours agoparentI prefer PyTorch myself, but to call Keras obsolete is quite the stretch. Just because academia has largely moved on from it, doesn't mean nobody uses it. Also, the API isn't all that different from other libraries. The principles are the same. reply whiplash451 8 hours agorootparentThe industry has also moved on from keras/tensorflow (apart from for legacy reasons). Google itself has moved on to JAX. reply albertzeyer 9 hours agoparentprevI wonder about Keras 3. It's now backend independent again, like in the early days, and supports JAX, TensorFlow, or PyTorch. It's a nice thing if you defined your model and can then easily switch between the frameworks, right? Or no-one cares about that, and everyone just uses PyTorch? reply 0xd1r 9 hours agoparentprevI suppose they are using keras because of its simplicity. But I agree, things have really moved on from keras. reply shubham13596 10 hours agoprevVery good resource on building from the basics in a concise manner. reply benterix 6 hours agoprev [–] > In contrast, the AI technology of the current golden age, which began in the mid-2010s, has consistently exceeded our expectations. Well, until recently, that is. It looks like we hit the wall as for what LLMs can do - some might call it a plateau of productivity. Namely, as far as coding is concerned, LLMs can successfully create chunk of code of limited length and tiny programs, can also review small pieces of code and suggest improvements that are not related to the context of the whole program (unless it can fit in the context window). In spite of huge effort put in creating a system where LLM agents could work together to create software such as AutoGPT, no non-trivial program has been created in this way so far. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document, \"The Engineer’s Guide To Deep Learning,\" provides a comprehensive guide to understanding the Transformer model, a significant AI breakthrough introduced in 2017.",
      "It includes fundamental concepts of neural networks, RNNs (Recurrent Neural Networks), NLP (Natural Language Processing), and attention mechanisms, with working Python code examples and references for further exploration.",
      "Released on May 21, 2024, it is free for educational and noncommercial use, authored by Hironobu SUZUKI, a seasoned software programmer and AI enthusiast."
    ],
    "commentSummary": [
      "The post discusses various resources for understanding the Transformer model in deep learning, highlighting free and paid options.",
      "Recommendations include \"The Annotated Transformer,\" \"Transformers from Scratch,\" Andrej Karpathy's series, and 3Blue1Brown's visual introductions.",
      "There is a debate on the relevance of Keras, with some users noting its decline in academia and industry favoring JAX, while others mention Keras 3's support for multiple backends."
    ],
    "points": 238,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1721113266
  },
  {
    "id": 40977103,
    "title": "Codestral Mamba",
    "originLink": "https://mistral.ai/news/codestral-mamba/",
    "originBody": "July 16, 2024 Mistral AI team Following the publishing of the Mixtral family, Codestral Mamba is another step in our effort to study and provide new architectures. It is available for free use, modification, and distribution, and we hope it will open new perspectives in architecture research. Codestral Mamba was designed with help from Albert Gu and Tri Dao. Unlike Transformer models, Mamba models offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length. It allows users to engage with the model extensively with quick responses, irrespective of the input length. This efficiency is especially relevant for code productivity use cases—this is why we trained this model with advanced code and reasoning capabilities, enabling it to perform on par with SOTA transformer-based models. We have tested Codestral Mamba on in-context retrieval capabilities up to 256k tokens. We expect it to be a great local code assistant! You can deploy Codestral Mamba using the mistral-inference SDK, which relies on the reference implementations from Mamba’s GitHub repository. The model can also be deployed through TensorRT-LLM. For local inference, keep an eye out for support in llama.cpp. You may download the raw weights from HuggingFace. For easy testing, we made Codestral Mamba available on la Plateforme (codestral-mamba-2407), alongside its big sister, Codestral 22B. While Codestral Mamba is available under the Apache 2.0 license, Codestral 22B is available under a commercial license for self-deployment or a community license for testing purposes. Important: This is an instructed model, with 7,285,403,648 parameters.",
    "commentLink": "https://news.ycombinator.com/item?id=40977103",
    "commentBody": "Codestral Mamba (mistral.ai)229 points by tosh 4 hours agohidepastfavorite59 comments thot_experiment 18 minutes agoDoes anyone have a favorite FIM capable model? I've been using codellama-13b through ollama w/ a vim extension i wrote and it's okay but not amazing, I definitely get better code most of the time out of Gemma-27b but no FIM (and for some reason codellama-34b has broken inference for me) reply bhouston 2 hours agoprevWhat are the steps required to get this running in VS Code? If they had linked to the instructions in their post (or better yet a link to a one click install of a VS Code Extension), it would help a lot with adoption. (BTW I consider it malpractice that they are at the top of hacker news with a model that is of great interest to a large portion of the users where and they do not have a monetizable call to action on the page featured.) reply leourbina 2 hours agoparentIf you can run this using ollama, then you should be able to use https://www.continue.dev/ with both IntelliJ and VSCode. Haven’t tried this model yet - but overall this plugin works well. reply scosman 2 hours agorootparentThey say no llama.cpp support yet, so no ollama yet (which uses llama.cpp) reply HanClinto 1 hour agorootparentCorrect. The only back-end that Ollama uses is llama.cpp, and llama.cpp does not yet have Mamba2 support. The issues to track Mamba2 and Codestral Mamba support are here: https://github.com/ggerganov/llama.cpp/issues/8519 https://github.com/ggerganov/llama.cpp/issues/7727 Mamba support was added in March of this year: https://github.com/ggerganov/llama.cpp/pull/5328 I have not yet seen a PR to address Mamba2. reply sadeshmukh 1 hour agorootparentprevOllama is supported: https://docs.continue.dev/setup/select-provider reply trsohmers 1 hour agorootparentThey meant that there is no support for Codestral Mamba for llama.cpp yet. reply sleepytimetea 2 hours agoparentprevLooking through the Quickstart docs, they have an API that can generate code. However, I don't think they have a way to do \"Day 2\" code editing. Also, doesn't seem to have a freemium tier...need to start paying even before trying it out ? \"Our API is currently available through La Plateforme. You need to activate payments on your account to enable your API keys.\" reply sv123 2 hours agorootparentI signed up when codestral was first available and put my payment details in. Been using it daily since then with continue.dev but my usage dashboard shows 0 tokens, and so far have not been billed for anything... Definitely not clear anywhere, but it seems to be free for now? Or some sort of free limit that I am not hitting. reply sunaookami 1 hour agorootparentThrough codestral.mistral.ai? It's free until August 1st: https://docs.mistral.ai/capabilities/code_generation/ >Monthly subscription based, free until 1st of August reply refulgentis 2 hours agoparentprev\"All you need is users\" doesn't seem optimal IMHO, Stability.ai providing an object lesson in that. They just released weights, and being a for profit, need to optimize for making money, not eyeballs. It seems wise to guide people to the API offering. reply bhouston 1 hour agorootparentOn top of Hacker News (the target demographic for coders) without an effective monetizable call to action? What a missed opportunity. Github Copilot makes +100M/year, if not way way more. Having a VS Code extension for Mistral would be a revenue stream if it was one-click and better or cheaper than Github Copilot. It is malpractice in my mind to not be doing this if you are investing in creating coding models. reply refulgentis 1 hour agorootparentI see, that makes sense: make an extension and charge for it. I assumed they meant free x local. It doesn't seem rational to make this one paid: its significantly smaller than their better model, and even more so than Copilot's. reply passion__desire 1 hour agorootparentprevBut they also signal competence in the space which means M&A. Or big nation states in future would hire them to produce country models once the space matures as was Emad's vision. reply refulgentis 1 hour agorootparentDid Emad's vision end up manifest? ex. did a nation-state end up paying Stability for a country model? Would it help signal competency? They're a small team focused on making models, not VS Code extensions. Would they do M&A? The founding team is ex-Googlers and has found significant attention in the MBA world via being an EU champion. reply magnio 2 hours agoprevThey announce the model is on HuggingFace but don't link to it. Here it is: https://huggingface.co/mistralai/mamba-codestral-7B-v0.1 reply dvfjsdhgfv 1 hour agoparentThe link is already there in the text, they probably just fixed it. reply sam_goldman_ 47 minutes agoprevYou can try this model out using OpenAI's API format with this TypeScript SDK: https://github.com/token-js/token.js You just need a Mistral API key: https://console.mistral.ai/api-keys/ reply sa-code 3 hours agoprevIt's great to see a high-profile model using Mamba2! reply imjonse 2 hours agoprevThe MBPP column should bold DeepSeek as it has a better score than Codestral. reply smith7018 2 hours agoparentWhich means Codestral Mamba and DeepSeek both lead four benchmarks. Kinda takes the air out the announcement a bit. reply causal 1 hour agorootparentIt should be corrected but the interesting aspect of this release is the architecture. To stay competitive while only needing linear inference time and supporting 256k context is pretty neat. reply mbowcut2 1 hour agorootparentTHIS. People don't realize the importance of Mamba competing on par with transformers. reply ed 1 hour agorootparentprevThey're in roughly the same class but totally different architectures Deepseek uses a 4k sliding window compared to Codestral Mamba's 256k+ tokens reply monkeydust 2 hours agoprevAny recommended product primers to Mamba vs Transformers - pros/cons etc? reply red2awn 1 hour agoparentA very good primer to state-space models (from which Mamba is based on) is The Annotated S4 [1]. If you want to dive into the code I wrote a minimal single-file implementation of Mamba-2 here [2]. [1]: https://srush.github.io/annotated-s4/ [2]: https://github.com/tommyip/mamba2-minimal reply bhouston 2 hours agoparentprevThis video is good: https://www.youtube.com/watch?v=N6Piou4oYx8. As are the other videos on the same YouTube account. reply ertgbnm 2 hours agoparentprevhttps://www.youtube.com/watch?v=X5F2X4tF9iM This is what introduced me to them. May be a bit outdated at this point. reply Kinrany 39 minutes agoprevIs there a good explanation of the Mamba architecture? reply simonw 22 minutes agoparentThere's a paper: https://arxiv.org/abs/2312.00752 I haven't seen any good non-paper explainers yet. reply rjurney 2 hours agoprevBut I JUST switched from GPT4o to Claude! :( Kidding, but it isn't clear how to use this thing, as others have pointed out. reply ukuina 1 hour agoparentWhat made you switch? reply throwup238 54 minutes agorootparentClaude Projects which allow attaching a bunch of files to fill up the 200k context. I wrote up a script to dump a bunch of code and documentation files to markdown as context and I add them to a bunch of Claude projects on a per topic basis. For example, I'm currently working on a Rust/Qt desktop app so I have a project with the whole Qt6 book attached to ask questions about Qt, a project with my SQL schema and ORM/Sqlite docs to ask questions about the app's data and generate models without dealing with hallucinations, a project with all my QML files and Rust QML element code, a project with a bunch of Rust crate docs, and so on and on. GPTs allow attaching files too but Claude Projects dump the entire contents of the files into the context rather than trying to do some hacky RAG that never works like I want it to. reply funnygiraffe 24 minutes agorootparentI was under the impression that with LLMs, in order to get high-quality answers, it's always best to keep context short. Is that not the case anymore? Does Claude under this usage paradigm not struggle with very long contexts in ways as for example described in the \"lost in the middle\" paper (https://arxiv.org/abs/2307.03172)? reply pelagicAustral 1 hour agorootparentprevI'm using both, been doing that for months now. I can confidently assert that while Claude is getting better and better, GPT 4 and 4o seem the be getting dumbed down for some unexplained reason. Claude is now my go-to for anything code. (I do Ruby and C#, btw, other might have a different experience) reply rjurney 1 hour agorootparentprevClaude is much better. Overwhelmingly better. It not only implements deep learning models for me, it has great suggestions on evolving them to actually work. reply mountainriver 1 hour agorootparentlol no it’s not, the benchmarks don’t show that at all. Both have issues in different ways reply causal 44 minutes agorootparentBenchmarks are pretty flawed IMO, in particular their weakness here seems to be that they are poor at evaluating long-tail multiturn conversations. 4o often gives a great first response, then spirals into a repetition. Sonnet 3.5 is much better at seeing the big picture in a longer conversation IMO. reply ldjkfkdsjnv 1 hour agorootparentprevGPT4o is way behind sonnet 3.5 reply mountainriver 1 hour agorootparentHuh I guess all the benchmarks are wrong then reply causal 43 minutes agorootparentAgreed. reply culopatin 2 hours agoprevDoes anyone have a video or written article that would get one up to speed with a bit of the history/progression and current products that are out there for one to try locally? This is coming from someone that understands the general concepts of how LLMs work but only used the general publicly available tools like ChatGPT, Claude, etc. I want to see if I have any hardware I can stress and run something locally, but don’t know where to start or even what are the available options. reply Kydlaw 2 hours agoparentIf I understand correctly what you are looking for, Ollama might be a solution (https://ollama.com/)?. I have no affiliation, but I lazily use this solution when I want to run a quick model locally. reply TechDebtDevin 2 hours agorootparentBetter yet install Open Web GUI and ollama at the same time via docker. Most people will want a familiar GUI rather than the terminal. https://github.com/open-webui/open-webui This will install ollama and open web GUI: For GPU support run: docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama Use for CPU only support: docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama reply Der_Einzige 2 hours agorootparentWhy do people recommend this instead of the much better oobabooga text-gen-webui? https://github.com/oobabooga/text-generation-webui It's like you hate settings, features, and access to many backends! reply TechDebtDevin 1 hour agorootparentTo each their own, how are you using these extra features? I personally am not looking to spend a bunch on API credits and don't have the hardware to run models larger than 7-8b parameters. I use local llms almost exclusively for formatting notes and as a reading assistant/summarizer and therefor don't need these features. reply TechDebtDevin 2 hours agoparentprevMost the 7b instruct models are very bad outside very simple queries. You can run a 7b on most modern hardware.How fast will vary. To run 30-70b models you're getting in the realm of needing 24gb or more of vRAM. reply Agentus 2 hours agorootparentI'm looking to run something on a 24gb GPU for the purpose of running wild with agentic use of LLMs. Is there anything worth trying that would fit on that amount of vRAM? Or are all the open-source PC-sized LLMs laughable still? reply TechDebtDevin 41 minutes agorootparentYou can run the llama 70b based models faster than 10 tkn/s on 24gb vram. I've found that the quality of this class of LLMs is heavily swayed by your configuration and system prompting and results may vary. This Reddit post seems to have some input on the topic: https://www.reddit.com/r/LocalLLaMA/comments/1cj4det/llama_3... I haven't used any agent frameworks other than messing around with langchain a bit so I can't speak to how that would effect things. reply sva_ 2 hours agoparentprevIf you mean LLM in general, maybe try llamafile first https://github.com/Mozilla-Ocho/llamafile reply currycurry16 2 hours agoparentprevFind good models here: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_... Check hardware requirements here: https://rahulschand.github.io/gpu_poor/ reply _kidlike 2 hours agoparentprevnot sure about the history/progression part, but there's ollama which makes it possible to run models locally. The UX of ollama is similar to docker. reply derefr 2 hours agoparentprevFor running LLMs, I think most people just dive into https://www.reddit.com/r/LocalLLaMA/ and start reading. Not sure what the equivalent is for image generation; it's either https://www.reddit.com/r/StableDiffusion/ or one of the related subreddits it links to. Sadly, I've yet to find anyone doing \"daily ML-hobbyist news\" content creation, summarizing the types of articles that appear on these subreddits. (Which is a surprise to me, as it's really easy to find e.g. \"daily homelab news\" content creators. Please, someone, start a \"daily ML-hobbyist news\" blog/channel! Given that the target audience would essentially be \"people who will get an itch to buy a better GPU soon\", the CPM you'd earn on ad impressions would be really high...) --- That being said, just to get you started, here's a few things to know at present about \"what you can run locally\": 1. Most models (of the architectures people care about today) will probably fit on a GPU which has something like 1.5x the VRAM of the model's parameter-weights size. So e.g. a \"7B\" (7 billion parameter-weights) model, will fit on a GPU that has 12GB of VRAM. (You can potentially squeeze even tighter if you have a machine with integrated graphics + dedicated GPU, and you're using the integrated graphics as graphics, leaving the GPU's VRAM free to only hold the model.) 2. There are models that come in all sorts of sizes. Many open-source ML models are huge (70B, 120B, 144B — things you'd need datacenter-class GPUs to run), but then versions of these same models get released which have been heavily cut down (pruned and/or quantized), to force them to fit into smaller VRAM sizes. There are 5B, 3B, 1B, even 0.5B models (although the last two are usually special-purpose models.) 3. Surprisingly, depending on your use-case, smaller models (or small quants of larger models) can \"mostly\" work perfectly well! They just have more edge-cases where something will send them off the rails spiralling into nonsense — so they're less reliable than their larger cousins. You might have to give them more prompting, and try regenerating their output from the same prompt several times, to get good results. 4. Apple Silicon Macs have a GPU and TPU that read from/write to the same unified memory that the CPU does. While this makes these devices slower for inference than \"real\" GPUs with dedicated VRAM, it means that if you happen to own a Mac with 16GB of RAM, then you own something that can run 7B models. AS Macs are, oddly enough, the \"cheapest\" things you can buy in terms of model-capacity-per-dollar. (Unlike a \"real\" GPU, they won't be especially quick and won't have any capacity for concurrent model inference, so you'd never use one as a server backing an Inference-as-a-Service business. But for home use? No real downsides.) reply pzo 1 hour agoprevweird they compare to deepseek-coder v1.5 when we already have v2.0. Any advantage to use codestral mamba apart from that it's lighter in weights? reply localfirst 2 hours agoprevany sort of evals on how it compares to closed models like chat gpt 4 or open ones like WizardLLM ? reply croemer 2 hours agoprev [–] The first sentence is wrong. The website says: > As a tribute to Cleopatra, whose glorious destiny ended in tragic snake circumstances but according to Wikipedia this is not true: > When Cleopatra learned that Octavian planned to bring her to his Roman triumphal procession, she killed herself by poisoning, contrary to the popular belief that she was bitten by an asp. reply skybrian 2 hours agoparentYes, that seems to be a myth, but exact circumstances seem rather uncertain according to the Wikipedia article [1]: > [A]ccording to the Roman-era writers Strabo, Plutarch, and Cassius Dio, Cleopatra poisoned herself using either a toxic ointment or by introducing the poison with a sharp implement such as a hairpin. Modern scholars debate the validity of ancient reports involving snakebites as the cause of death and whether she was murdered. Some academics hypothesize that her Roman political rival Octavian forced her to kill herself in a manner of her choosing. The location of Cleopatra's tomb is unknown. It was recorded that Octavian allowed for her and her husband, the Roman politician and general Mark Antony, who stabbed himself with a sword, to be buried together properly. I think this rounds to “nobody really knows.” The “glorious destiny” seems kind of shaky, too. It’s just a throwaway line anyway. [1] https://en.m.wikipedia.org/wiki/Death_of_Cleopatra reply ljsprague 1 hour agoparentprevWhat bothers me more is that the legend is that she was killed by an asp, not a mamba. reply rjurney 2 hours agoparentprev [–] I believe this is in dispute among sources. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On July 16, 2024, Mistral AI introduced Codestral Mamba, a new architecture developed with Albert Gu and Tri Dao, offering linear time inference and the ability to model infinite-length sequences.",
      "Codestral Mamba is efficient for code productivity, performs comparably to state-of-the-art (SOTA) transformer models, and supports in-context retrieval up to 256k tokens.",
      "The model is deployable via various SDKs, with raw weights available on HuggingFace, and is free under the Apache 2.0 license, while Codestral 22B has commercial and community licenses."
    ],
    "commentSummary": [
      "Codestral Mamba, a new model from Mistral.ai, has been released and is generating interest due to its unique architecture and capabilities.",
      "The model supports 256k context tokens and offers linear inference time, making it competitive with transformer-based models.",
      "There is ongoing discussion about integrating Codestral Mamba with development environments like VS Code and IntelliJ, but current support is limited."
    ],
    "points": 229,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1721141086
  },
  {
    "id": 40973422,
    "title": "Peter Buxtun, whistleblower who exposed Tuskegee syphilis study, has died",
    "originLink": "https://www.theguardian.com/us-news/article/2024/jul/15/peter-buxtun-tuskegee-whistleblower-dies",
    "originBody": "View image in fullscreen Peter Buxtun pictured in San Francisco in this undated photo. Photograph: Liz Hafalia/AP US news Peter Buxtun, whistleblower who exposed Tuskegee syphilis study, dies aged 86 Buxtun, who had Alzheimer’s, revered for role in bringing to light one of worst medical research scandals in US history Associated Press Mon 15 Jul 2024 19.17 EDT Share Peter Buxtun, the whistleblower who revealed that the US government allowed hundreds of Black men in rural Alabama to go untreated for syphilis in what became known as the Tuskegee study, has died. He was 86. Buxtun died on 18 May of Alzheimer’s disease in Rocklin, California, according to his attorney, Minna Fernan. Buxtun is revered as a hero to public health scholars and ethicists for his role in bringing to light the most notorious medical research scandal in US history. Documents that Buxtun provided to the Associated Press, and its subsequent investigation and reporting, led to a public outcry that ended the study in 1972. Forty years earlier, in 1932, federal scientists began studying 400 Black men in Tuskegee, Alabama, who were infected with syphilis. When antibiotics became available in the 1940s that could treat the disease, federal health officials ordered that the drugs be withheld. The study became an observation of how the disease ravaged the body over time. In the mid-1960s, Buxtun was a federal public health employee working in San Francisco when he overheard a co-worker talking about the study. The research was not exactly a secret – about a dozen medical journal articles about it had been published in the previous 20 years. But hardly anyone had raised any concerns about how the experiment was being conducted. “This study was completely accepted by the American medical community,” said Ted Pestorius of the US Centers for Disease Control and Prevention, speaking at a 2022 program marking the 50th anniversary of the end of the study. Buxtun had a different reaction. After learning more about the study, he raised ethical concerns in a 1966 letter to officials at the CDC. In 1967, he was summoned to a meeting in Atlanta, where he was chewed out by agency officials for what they deemed to be impertinence. Repeatedly, agency leaders rejected his complaints and his call for the men in Tuskegee to be treated. He left the US Public Health Service and attended law school, but the study ate at him. In 1972, he provided documents about the research to Edith Lederer, an AP reporter he had met in San Francisco. Lederer passed the documents to the AP investigative reporter Jean Heller, telling her colleague: “I think there might be something here.” Heller’s story was published on 25 July 1972, leading to congressional hearings, a class-action lawsuit that resulted in a $10m settlement and the study’s termination about four months later. In 1997, President Bill Clinton formally apologized for the study, calling it “shameful”. The leader of a group dedicated to the memory of the study participants said on Monday they were grateful to Buxtun for exposing the experiment. “We are thankful for his honesty and his courage,” said Lille Tyson Head, whose father was in the study. Buxtun was born in Prague in 1937. His father was Jewish, and his family immigrated to the US in 1939 from Nazi-occupied Czechoslovakia, eventually settling in Irish Bend, Oregon, on the Columbia River. In his complaints to federal health officials, he drew comparisons between the Tuskegee study and medical experiments Nazi doctors had conducted on Jews and other prisoners. Federal scientists did not believe they were guilty of the same kind of moral and ethical sins, but after the Tuskegee study was exposed, the government put in place new rules about how it conducts medical research. Today, the study is often blamed for the unwillingness of some African Americans to participate in medical research. “Peter’s life experiences led him to immediately identify the study as morally indefensible and to seek justice in the form of treatment for the men. Ultimately, he could not relent,” said the CDC’s Pestorius. Buxtun attended the University of Oregon, served in the US army as a combat medic and psychiatric social worker and joined the federal health service in 1965. Buxtun went on to write, give presentations and win awards for his involvement in the Tuskegee study. A global traveler, he collected and sold antiques, especially military weapons and swords and gambling equipment from California’s gold rush era. He also spent more than 20 years trying to recover his family’s properties confiscated by the Nazis and was partly successful. “Peter was wise, witty, classy and unceasingly generous,” said David M Golden, a close friend of Buxtun’s for over 25 years. “He was a staunch advocate for personal freedoms and spoke often against prohibition, whether it be drugs, prostitution or firearms.” Another longtime friend Angie Bailie said she attended many of Buxtun’s presentations about Tuskegee. “Peter never ended a single talk without fighting back tears,” she said Buxtun himself could be self-effacing about his actions, saying he did not anticipate the vitriolic reaction of some health officials when he started questioning the study’s ethics. At a Johns Hopkins University forum in 2018, Buxtun was asked where he got the moral strength to blow the whistle. “It wasn’t strength,” he said. “It was stupidity.” Explore more on these topics US news Alabama news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=40973422",
    "commentBody": "Peter Buxtun, whistleblower who exposed Tuskegee syphilis study, has died (theguardian.com)211 points by racional 15 hours agohidepastfavorite73 comments anitil 12 hours agoFor some context on the Tuskegee \"experiment\" I'd recommend the two-part series from \"You're Wrong About\" [0] [1]. Buxtun shows up in the second episode. What I hadn't remembered is that it was 6 years from when he first raised his concerns until they were taken seriously. [0] Part 1 https://open.spotify.com/episode/1CSuf2U9vM5sYru8RwsqFB [1] Part 2 https://open.spotify.com/episode/6GveYHXn6CdkHoGOZTYv0j Apologies for the spotify links, I couldn't find their hosted version reply prettyStandard 4 hours agoparentI can second these episodes, and the podcast series in general. It's very informative about things that have been \"misremembered\". Other good series were the OJ Simpson trial, Monica Lewinsky, the Satanic Panic, and of course the McDonald's hot coffee lawsuit. Just recently on Hacker News I saw someone making jokes about this lawsuit being spurious. Oh gosh, now I'm on a tangent. Rather than defend \"this was not spurious\" I'll just say that's how our legal system is set up. The legislative branch is not interested in making reasonable laws, and/or creating capable regulating bodies like most other modern countries. Your recourse here is to sue, hopefully there is an appropriate decision, and it's taken as precedent. Of course we've gone further in that direction in recent history. reply dfxm12 1 hour agorootparentI don't know if it is fair to say people \"misremembered\" the details of the McDonald's coffee lawsuit. As the news media and pop culture weirdly seemed to go out of their way to paint McDonald's as the victim and the woman as negligent, people never knew the correct details at the time to begin with. reply burningChrome 22 minutes agorootparentI was a college student at the time and never saw it that way. I remember in the media there were vigorous debates over this as being \"frivolous\" but I remember all my friends were on the victims side. Spending days in the hospital to get skin grafting because their coffee was too hot I think far exceeded what someone would classify as \"frivolous\". I also remember several news reports about how they found out through court documents McDonald's had over 700 reports of burns between 1982-1992 which to me was completely shocking and proved they knew their coffee was way too hot. Now you see all the warnings on the labels, most of the bigger chains have cardboard sleeves so your hands don't get too hot holding it. McDonald's has since reduced the temperature of their coffee as well. The only thing I didn't accurately remember was several people told me that the lady initially won her lawsuit, but lost on appeal and McDonald's didn't have to pay her anything. In actuality, she won a sizeable award from the jury, but it was greatly reduced by the judge and then before an appeal, McDonald's finally settled out of court. https://en.wikipedia.org/wiki/Liebeck_v._McDonald%27s_Restau... reply ksenzee 51 minutes agorootparentprev> he legislative branch is not interested in making reasonable laws, and/or creating capable regulating bodies like most other modern countries That’s the point of a common-law system. Not that I’m defending Congress and how little they get done—I’m not, they’re terrible right now—but we don’t have case law because Congress is terrible. We have case law because that’s how our legal system is meant to work. The legislation lays out the theory, and the details get worked out by judges after theory meets practice. It’s not somehow inferior to civil law, just different. reply Retric 11 minutes agorootparentCommon law doesn’t require lawsuits to cover issues like this. It’s hard to sue a company when they follow quantitative guidelines. However terms like ‘due caution’ punt issues to the courts who then come up with a meaningful standard. reply astura 11 hours agoparentprev>Apologies for the spotify links, I couldn't find their hosted version They are right here: https://yourewrongabout.buzzsprout.com/1112270/5330092-tuske... https://yourewrongabout.buzzsprout.com/1112270/5418709-tuske... reply tokai 1 hour agoprevWonder why the US thought they needed to continue this experiment when they had access to the results and researchers of Unit 731[0], that did extensive research on syphilis. Kinda makes one think that the racial aspect was the point. [0] https://en.wikipedia.org/wiki/Unit_731#American_grant_of_imm... reply s1artibartfast 57 minutes agoparentAs discussed elsewhere, one reason is that at the time Syphilis response was significantly different in Africans. Trials like this had been previously run on Europeans, and 731 would presumably include east Asians and ran for a handful of years, not decades. Unlike Unit 731, they didn't infect anyone and it was an observational study. reply burningChrome 19 minutes agoparentprev>> Kinda makes one think that the racial aspect was the point. Interesting to note that the lowest C19 vaccine rates were in the African American communities. A lot of media outlets have speculated the primary reason for their vaccine hesitancy was this experiment and not trusting the government. In Tuskegee, Painful History Shadows Efforts To Vaccinate African Americans https://www.npr.org/2021/02/16/967011614/in-tuskegee-painful... reply toomuchtodo 15 hours agoprevhttps://en.wikipedia.org/wiki/Peter_Buxtun https://en.m.wikipedia.org/wiki/Tuskegee_syphilis_experiment reply hannob 12 hours agoprevI had learned about this in the Pandemia podcast last year, unfortunately only available in German. But I thought for all the German-speaking HN readers I could share, it's worth listening to: https://superelektrik.de/pandemia/syphilis-geschichte-eines-... Pandemia is a podcast started during the Covid pandemic, but regularly covering all kinds of diseases and health issues. reply rectang 14 hours agoprev> Buxton himself could be self-effacing about his actions, saying he did not anticipate the vitriolic reaction of some health officials when he started questioning the study’s ethics. Humans relentlessly believe themselves to be just and righteous. To maintain that self-sense, they will gladly deceive themselves — and often much worse. reply somenameforme 14 hours agoparentYip, I think this is the main moral to be taken from this incident, and many like it. It's not like the people carrying out these experiments were just sadistic racists. They probably saw themselves as being able to, in the long run, save far more people and create a greater, safer, and healthier society for everybody, being able to eventually treat not only syphilis but any other disease which may manifest similarly. About the time somebody starts arguing that the ends justifies the means, something has probably gone wrong. Because the \"ends\" people envision quite rarely come to pass, yet the means of trying to pursue those ends do, with 100% certainty, happen. So most often you end up with all the evils, and none of the utopian justifications at the end of the road. reply LanceH 5 hours agorootparent> They probably saw themselves as being able to, in the long run, save far more people and create a greater, safer, and healthier society for everybody It's easy to view this as bad in this article's case, but nobody views their version of getting to play god when it comes to politics so long as their side is in charge. And the funny thing about this statement is how volatile a reaction it will get from both sides who think assume I'm supporting one or the other. reply lupusreal 2 hours agorootparent> It's easy to view this as bad in this article's case, but nobody views their version of getting to play god when it comes to politics so long as their side is in charge This is just lazy rhetoric. Some political groups go murder crazy as soon as they gain power, while others don't. Obviously not everybody is equally bad as everybody else. reply DFHippie 1 hour agorootparentYes, and the more you convince people that everyone is equally bad, the more you empower bad actors. If there is no profit in being good and there is profit in being bad, being good is for suckers. If everyone's a thief, you'd better start stealing. Otherwise you're just a victim. reply jonathanlydall 1 hour agorootparentIn fantasy arguments I have in my head about someone justifying their bad behaviour with “but everyone does it”, my reply would be, “most people I know don’t do it, so it’s definitely not everyone, just people like you”. reply DFHippie 6 minutes agorootparent\"Everyone does it\" translates to \"I don't think I will suffer consequences from doing it\". It's a justification based on self-interest, not ethics. It looks like they are applying a version of Kant's Categorical Imperative, which is roughly \"what if everyone did it?\" The justifier is saying everyone does do it and everything is fine. The proof is in the pudding. But as you say, if literally everyone did it, they wouldn't be having this conversation with you. So what they mean by \"everyone\" is \"enough people that I'm safe\". max_ 11 hours agorootparentprevI don't think a racist ever thought about themselves as racists. Racism is usually a notion of people having an explanation for why they behave towards another race. There are always explanations. I don't think there is any racist that doesn't have an explanation. For American slavery it was that \"African Americans\" are just animals, not human. So it made sense to use/teat them like donkeys or vermin. In Nazi Germany, the explanation was that Jews are veramin and so veramin needs to be exterminated. In South Africa it was a \"scientific\" theory they had called Holism [0] which basically described that everything should be kept in its place hence apartheid policies. In Gaza the explanation is religious i.e God have us land X, and this we need to cleanse the land of it's \"invaders\". Also, look at caste systems in India. They have explanations of why they do that. They don't see themselves as racist. The explanations still occur today using \"statistics\", \"data\", & \"science\" with stuff like IQ \"research\" & \"race realism\". \"Racist\" is something we project on people. But racists always think they actually have a Nobel or logical cause for thier activities. [0]: https://en.m.wikipedia.org/wiki/Holism_and_Evolution reply somenameforme 10 hours agorootparentTo me racism has a pretty simple litmus test - would you treat an individual of another race, but in an otherwise identical background situation differently? If yes, then it's probably racist. If not, then it's probably not. So Tuskegee has some interesting backstory. It was inspired by a similar experiment that was carried out in Norway that followed the progression of untreated syphilis in thousands of people over decades. [1] That study provided extensive data and information on the progression of syphilis, but at the time it was believed that syphilis affected different races in different ways. And black Americans had (and still have) infection rates dramatically higher than other major groups. So this meant that studying this exact group could not only be overall most impactful on a population basis, but was also the least well understood group (as the Norwegian study presumably lacked much of anybody of African ancestry) and so the most most likely to yield novel/informative science. So if these individuals had otherwise been just another subgroup of whites (but one still had reason to think syphilis might affect them differently than e.g. Norwegians), would we still have carried out this experiment? It's impossible to say for certain, but I think the answer is probably yes. Not only was such experimentation already happening across the globe, including in relatively homogeneous societies, but there have been all sorts of other US government driven experiments on the population where white groups were just as readily experimented on like MKUltra [2], Operation Sea Spray [3], and so on endlessly. [1] - https://www.sciencedirect.com/science/article/abs/pii/002196... (note the date on the paper) [2] - https://en.wikipedia.org/wiki/MKUltra [3] - https://en.wikipedia.org/wiki/Operation_Sea-Spray reply bobthepanda 9 hours agorootparentThat's not really why the study was conducted on African Americans. > The conception which lay behind the U.S. Public Health Service Syphilis Study at Tuskegee in 1932, in which 100% of its participants were poor, rural African-American men with very limited access to health information, reflects the racial attitudes in the U.S. at that time. The clinicians who led the study assumed that African-Americans were particularly susceptible to venereal diseases because of their race, and they assumed that the study's participants were not interested in receiving medical treatment.[4][45] > Taliaferro Clark said, \"The rather low intelligence of the Negro population, depressed economic conditions, and the common promiscuous sex relations not only contribute to the spread of syphilis but the prevailing indifference with regards to treatment.\"[45] In reality, the promise of medical treatment, usually reserved only for emergencies among the rural black population of Macon County, Alabama, was what secured subjects' cooperation in the study.[4] I mean the other racist part was the fact that penicillin, which is still a standard treatment of syphilis today, was developed while the study was ongoing and yet there were still three more decades that it ran. reply somenameforme 3 hours agorootparentIn looking up more information on this topic in general. I came upon this [1] paper on the Tuskegee Study. It's an absolutely encyclopedic work that covers the historical context, parallel programs, and much more with an absurd level of detail and sources galore. So for instance I also thought that no subjects in Tuskegee received penicillin. It turns out this is incorrect and by 1952 27.5% had! [2] Another really interesting datum is that Georgia in 1945 started carrying out widespread syphilis testing, on the scale of hundreds of thousands of people, reaching 89% testing coverage in one local jurisdiction (which is where I assume Tuskegee was located). The interesting thing is that 30% of black individuals and 3% of white individuals tested positive for syphilis. People wouldn't have known the exact numbers back when this the Tuskegee study started (in 1932), but with such ridiculously high rates of infection, they'd have had a general idea. How do you think that would shape your views of a people when the Overton Window was wide open? Do you honestly think you'd still have been standing on a moral pedestal? This is why I think it's important to try to do more than just demonize the people involved. Because demonizing them isn't hard. They deserve it, and worse. But at the same time when we demonize them, it's so easy to miss the lessons to be learned because, after all, we aren't demons so surely we couldn't go down the same path again. Yet we almost certainly will if the only lesson we take away is 'don't be evil', because what we see as evil after the fact is not what people, good normal and \"moral\" people, perceive to be evil in the present. That's history in a nutshell after all. [1] - https://jamanetwork.com/journals/jamainternalmedicine/fullar... [2] - https://sci-hub.ru/https://www.sciencedirect.com/science/art... (gotta love 70 year old papers being paywalled...) reply WarOnPrivacy 5 hours agorootparentprev>> Taliaferro Clark said, \"The rather low intelligence of the Negro population, depressed economic conditions, and the common promiscuous sex relations not only contribute to the spread of syphilis but the prevailing indifference with regards to treatment.\"... This is a fairly sickening mindset. I hope I am never not sickened by it. reply s1artibartfast 1 hour agorootparentStrange. I have no problem reading it as a literal description with no bigotry or racism implied. reply wizzwizz4 56 minutes agorootparentIf it were about J.R.R. Tolkien's orcs, then I might agree. When basic observation disproves ¾ of your \"literal description\" about a racially-defined population… those \"facts\" probably didn't originate from honest mistakes. It's possible to be racist while using objective language: there's an entire Wikipedia article on scientific racism. reply s1artibartfast 28 minutes agorootparentWhat was the basic observation that disproves it? It was a statement about a specific group of people in a specific situation. I assume most of it was based in contemporary reality. You could probably say the same thing about other places today and be correct. I think the part that you're missing is that it wasn't a statement about blacks in general, but a particular community of incredibly poor people with zero education. syphilis was in fact rampant in that community, and I imagine that was zero access to healthcare or even detection, they were fairly fatalistic about it. People were more likely to have it than not have it by a huge margin. reply mandmandam 9 hours agorootparentprevDid you ... Did you just try and. use MK Ultra and Operation Sea-Spray to claim the Tuskegee experiments weren't racist? :o Systemic racism in medical research cannot be justified by comparing it to other unethical experiments... Really would have thought that was obvious? Also, even the analogies are terribly flawed. The Oslo experiments involved retrospective examination of medical records and autopsies, not the withholding of treatment. And Tuskegee victims were not informed of the real nature of the study - a clear ethical violation which disproportionately targeted a vulnerable racial group. reply somenameforme 9 hours agorootparentThat is not what happened in Oslo. This is the lead paragraph of the abstract of the paper from it: ---- Nowhere in the world is there a more unique opportunity to learn what happens when early syphilis goes untreated than from the files of Boeck of Oslo, Norway. His scientific conviction as to the inadequacies of the specific treatment of the day led him to withhold treatment from approximately 2,000 patients with primary and secondary syphilis during the twenty-year period, 1891–1910. Community protection from infection was aided by the hospitalization of these patients until all traces of the disease had disappeared (from 1 to 12 months, average 3.6 months). In 1929, his successor, E. Bruusgaard, reported on a follow-up study of 473 of these patients and provided information on the outcome of untreated syphilis, which has formed the basis for prognostic statements on syphilis for more than twenty-five years. ---- In Oslo the patients were both hospitalized and then had treatment withheld. His successor then carried out a retrospective study on what happened. It's unclear what the patients were told, but I suspect it was not 'We're going to hospitalize you for months, but not treat you.' reply mandmandam 9 hours agorootparentThis is a side-point, quite apart from the fact that you claimed Tuskegee wasn't racist because MK-ULTRA also affected white people. I really hope you think that one over, because it's a truly horrid and utterly indefensible take. And you're still very wrong on this side-point, because from 1891 to 1910, there was no known effective treatment for syphilis. Whereas during Tuskegee (1932-1972), penicillin was both widely available for most of that time and known to be effective. I don't know why you're making these awful comparisons, but I'm interested how you formed these views. Did you hear these arguments on a podcast somewhere, or are they your own? ... And why on Earth would you think Peter Buxtun's death was the appropriate time to bring them up?? reply somenameforme 7 hours agorootparentWhen one looks at history, it's like we're on a loop. And I think a big part of that is because we fail to ever \"really\" learn from the past. And I think one part of that is demonizing the past with labels, instead of actually trying to understand what really happened and why. Because when we overly demonize things it makes it impossible to imagine any reasonable person, let alone ourselves, ever engaging in anything even remotely awful - 'it could never happen again.' But of course it will, and it won't just be \"evil\" people doing it. So to your post here - you're again factually mistaken. There were indeed numerous treatments for syphilis in the 19th century, as the paper specifically mentions treatment being withheld should have clued you into. These treatments had significant side effects, but such is often the nature of medicine. What would you think of a doctor that intentionally withheld chemo or other similarly dangerous treatments to thousands of cancer patients, to instead observe what happened to them absent treatment? That is what happened in Norway, and again I'm sure the doctor had the best of intentions, presumably he was working to develop a more effective treatment. If you have any factual or logical arguments I'm more than happy to hear them, and indeed perhaps there is something I am not considering. But I find the appeals to emotion and bias mixed with a healthy helping of ad hominem and straw man quite silly, and I will not engage with that. reply mandmandam 5 hours agorootparentThe guy claiming the Tuskegee experiments weren't racist is also now claiming the moral high ground. I'm out. reply bbarnett 8 hours agorootparentprevIt's not clear to me if people in this thread are differentiating between \"certain categories of humans have specific genetic vulnerabilities\" and \"we're going to be racist without logical or researched cause\". It actually hurts people to not take into account their genetic background, an example is sickle cell anemia, which originates primarily in black people, who have a family tree from Africa. Why? Well, even though it causes severe issues, it also protects from malaria parasites! There was an evolutionary derived pressure to spread this in that population group. And, during covid this caused additional problems for those with sickle cell. It also highlighted how Italians, most specifically those in Sicily have a strong likelihood of having sickle cell, primarily due to the endless, centuries long Roman occupation of Africa. Women and men are physically different, and have different issues to account for (osteoporosis a great example here). Men often have too much iron, where as women too little. Treating everyone the same would mean not treating anyone correctly. So, yes.. there are racial and sexual differences to take into account during studies, and medical treatments. As with everything, the true way to behave is often neither extreme. No one should be mistreated during treatment, and treatments should not be racist... while understanding that racism isn't \"different genetic groups of humans are predispositioned for certain conditions\". reply mandmandam 8 hours agorootparentNo one is saying that all people are genetically the same. No one is arguing that sayig \"different genetic groups of humans are predispositioned for certain conditions\"is racist, or that medical treatment needs to ignore biological differences. What is being claimed by OP is that since atrocities were also inflicted on white people during unethical medical experiments such as MK ULTRA, withholding known treatment from a specific racial group in Tuskegee without their consent can't be called racism (which is so absurd that maybe that's where you got confused?). Hope that clears things up for you. reply echoangle 11 hours agorootparentprevWhat’s your definition of racism? White supremacists are certainly racist and would probably tell you that they are racist, too. That’s their whole message. Nazi Germany Leaders would probably have been ok with having been called racist, too. They were talking about being superior than other races all the time. reply max_ 10 hours agorootparentI think a black man declaring all white people evil vampires is racist. But we seldom call oppressed minorities racist. My definition of racism is the application racial stereotypes to individuals & groups reply bbarnett 8 hours agorootparentFor me, it depends upon how the 'groups' part is applied. An example that seems sexist: women love shoes. However, this isn't sexist... it's simply statistically true. Where *isms come into play, where sexism occurs, is seeing the individual and then thinking \"Ah, a woman... clearly she must love shoes\". Applying group derived statistical fact to individuals is where racism, sexism occurs. Another example, black people in America show lower outcomes on IQ tests. There is a lot of debate as to why, whether it is genetic (an example, do Black people have less incidents of autism? Autism is often correlated with mental issues, but also conversely with higher test scores in some areas.) But really, it doesn't matter why. Whether it is genetic, whether it is cultural, or what. What matters is that we understand the group statistic exists, but that no matter what we do not simply apply such thought processes to the individual. After all, a few percentage in group testing has no basis for determining if the person in front of me is capable or not. I've met (as an example) some very unintelligent white people, and some very intelligent black people, a few percentage difference as a group is not relevant here. Yet if we pretend group differences don't exist, how can we possible try to fix it.. if it is cultural? Or worse, what if it is environmental, such as... poor nutrition which hurts brain development during youth? Such things can be fixed, yet if we pretend there is no difference, how can we try to fix it? So again, the primary must be to treat individuals as just that, and treat groups as just that, otherwise.. how are we being fair? reply SuperNinKenDo 4 hours agorootparentprevProbably like 2/3 of the people I know that I would consider to be racist or hold racist beliefs would call themselves racist, or at least say that they have some racist beliefs. Probably half the other 3rd are predominantly those that are racist specifically toward white people. When you create a space where people can be sincere, you would be surprised how self-aware they can be. At least I am. reply xenadu02 3 hours agorootparentI take that as part of the point they were making: white supremecists, for example, claim white people are superior. Their actions are logical in some sense if you accept their premises that a) race is a useful classification akin to species and b) there is some inherent difference between races. To be clear personally I think the science has definitively debunked not only supremacy of any race but even race as a concept which is really all about skin tone and facial features... many people's DNA doesn't even match their supposed \"race\" because the categorization of race is akin to color of coat in dogs: a very superficial trait useful as a visual descriptor but not useful for segmenting individuals into groups. Not to mention the whole story of homosapiens is freeing us from biological evolution. The idea that someone's biology inherently limits their worth or makes them not an equal member of humanity is in some sense the most perverse denial of our very nature. reply somenameforme 2 hours agorootparentI think when trying to debate something, the idea should be to try to convince the other person. That sounds stupidly obvious, but it's really not. Imagine you're debating somebody over something and resorted to an argument over the exact meaning of terms that, to people who might not share your worldview, would be completely obvious. It's unlikely you'd really be persuading them of anything. I think the more salient point is that even if you assume clear racial distinctions, and even substantial racial differences, that still does not justify any sort of racial ideology. The movie Gattaca works as an oddly perfect metaphor for racial ideologies and its problems. Gattaca was about genetics, but it's not a coincidence that it fits perfectly with race, as it's literally the exact same problem. Just because somebody is a part of a group with some sort of a negative correlation, does not mean that person will inherently let that trait dominate them. Incidentally the reverse is also true - just because somebody may e.g. have a high IQ does not mean they will inherently be knowledgeable or wise. It's okay to consider group tendencies, but at the end of the day individuals are able to rise far above or fall far below their \"expectation\", and so it's important to judge each person not by the makeup of their genetics, but by the content of their character. reply Izkata 2 hours agorootparentprevSupremacy sure, but not its existence, more like the opposite: Ancestry DNA tests wouldn't work if there was no basis for it. reply kerkeslager 12 hours agorootparentprevSounds like a lot of startups. reply anal_reactor 4 hours agorootparentprev> Because the \"ends\" people envision quite rarely come to pass, yet the means of trying to pursue those ends do, with 100% certainty, happen. So most often you end up with all the evils, and none of the utopian justifications at the end of the road. So what you basically say is that we should always follow the path of least evil at the current moment. It's strange to me to think that someone would say that this is indeed the optimal way to minimize evil. reply thephyber 7 hours agoparentprevIn _Beyond the Curve_ a writer put it succinctly: > Nobody thinks they are the Ursula of their story. I think it is the human condition that our ego protects itself by denying truth in order to “protect” our psyche from acknowledging that we may have done something extremely morally compromising. reply ImHereToVote 5 hours agorootparentI wonder if Ursula von der Leyen thinks she is the Ursula of her story? reply neilv 6 hours agoparentprevThat might be too generous. What if the vitriolic reaction was due to the threat of external repercussions from that information being revealed, rather than threat to internal self-image? reply hiatus 5 hours agoparentprevWe have known this for ages, hence the adage, \"The road to Hell is paved with good intentions.\" reply ImHereToVote 5 hours agoparentprevIt's a good thing we don't have labs that do these amoral experiments currently. It's always by sheer luck that such atrocities always happen in the past. reply anal_reactor 4 hours agoprevAs non-American, it's a strange feeling to see all these discussions about racism. It took me a while to understand that the whole perception of the issue is just completely different. reply Swizec 3 hours agoparentI’ve lived here for ~10 years and I’m still learning. The base realization is that if you come from a mostly homogenous (or openly racist a la apartheid) background, you simply can’t understand how American style racism works. In my home country we have so few black and asian people that exact numbers can’t be reported in the census because it would be considered personal information. Sure we have racism but it’s purely of the “fear of new/unknown” kind. Anything more than that we learned from american media. A better analogy, if you come from such a background, might be to replace racism with ethnicism. We’re really good at ethnicism in Europe in a way that’s a lot more similar to how america does racism. reply graemep 3 hours agorootparentIts not quite that. The US is also different from other countries with large non-white minorities. I think you are right that race, ethnicity and other things (such as caste) are much the same thing: you are classified by some group you are born into, and you and your descendants cannot move out of. However it is also different in every culture. It look me a long time to understand how the US is different from countries I know (which are definitely not homogenous, though some are fairly openly racist). I could see race was much more ingrained in the culture compared to the UK, but did not understand why. The insight for me (mostly thanks to Isabel Wilkerson's book Caste) is that race in American is a caste distinction: it is a hierarchy rather than just hostility to the outsider. I wrote a blog post on this: https://pietersz.co.uk/2023/08/racism-culture-different reply iftheshoefitss 3 hours agorootparentI would posit it’s not a caste system similar to India’s caste system or old school feudalism. Being an outsider definitely plays a part for instance the treatment of Italian immigrants. In my experience if you’re part of a certain group you might or will get mistreated but if you’re part of that group and also an outsider oof you’re in for one tortured existence. Which is kind of contradictory because the USA is one of the few places that openly welcomes outsiders (like you don’t see migrants trying to go to China or Russia) but at the same time if you’re deemed persona non grata like for whatever reason the land will mess with your life, health and so on unlike any other place reply graemep 2 hours agorootparentHow is it not a caste system? A caste system can ALSO be hostile to outsiders on an ethnic or religious basis (plenty of examples of both in South Asia!) in addition to the caste system. Feudalism is not a caste system. In a feudal system people can move up and down to some extent, and over generations people can move a lot. It was possible for people to marry to at least some extent. There is no notion of pure blood or pollution. https://www.researchgate.net/publication/228717335_Was_there... reply gamblor956 2 hours agorootparentIn a caste system, your \"worth\" is decided at birth based on what caste system you are born into, and your opportunities and relationships are determined by that. The US has some of the highest rates of interracial marriages, relationships, etc. in the world. Mobility in America is driven by socioeconomic class, not race, and gender plays a heavy role in educational success in some races (more than race itself), but not others, for various historical reasons. In order to cast U.S. racial relations into a type \"caste\" system, you'd have to stretch the definition of caste so thin that it wouldn't have any meaning. reply graemep 59 minutes agorootparentIt was historically a caste system though, especially in the South up to the sixties, and there are remnants of that. No doubt it is a weakened caste system, and hopefully dying one, but it still seems present. Americans seem to still, at the least, attach a lot of importance to race, and to classifying people by race. It is seen as fundamental to who people are: a lot of Americans who seem fine with someone self-identifying their gender find it far harder to accept someone self-identifying their race. Why not? A lot of people report assumptions are made on the basis of race. In a lot of conversations I have with Americans about race seem to assume that people are likely to be overtly treated differently on the basis of their appearance. I do not know the US so maybe I am out of date or have read the wrong things but I find it a lot harder to understand the importance Americans (not just racists, but people trying to be anti-racist too) attach to race if I am wrong. reply rincebrain 32 minutes agorootparentIf I were to wildly speculate, I might guess that many Americans think of gender as being about presentation and experience, while \"race\" is almost entirely about shared experience - which, yes, is often informed by reactions to one's appearance, or lack thereof, but that's not the identifying characteristic. So it comes across as similarly distasteful to someone claiming to be \"long lost Uncle Eddie\", because they saw your family through the glass at a holiday and liked what they saw - since you weren't here for a billion shared little experiences, and there's no claim that you were from the same grandparents or similar, it rings hollow and like you want something from it. reply ksenzee 42 minutes agorootparentprev> maybe I am out of date or have read the wrong things No, I think you have a better grasp on it than most people outside the US. Americans really want to believe we don’t have a caste system, because it’s antithetical to our origin story and what we feel is true about ourselves, but we absolutely do. So if you listen to us talk, you’ll think we don’t have a caste system. If you watch our actions, you’ll see we still do. reply sangnoir 13 minutes agorootparentprev> The US has some of the highest rates of interracial marriages, relationships, etc. in the world This is a fairly recent development. As recently as 1995 (1 generation ago), the majority of Americans disapproved of interracial marriages. reply alan-hn 2 hours agorootparentprevRich African Americans are still looked down on by some in comparison to rich whites. There is not as much mobility as you would think reply renewiltord 2 hours agorootparentprevFor European racism, just bring up the Roma and wait for a European to say things that no one in any American city would say. reply anal_reactor 1 hour agorootparentIt's different. Europeans are proud of hating gypsies, we just don't bring this up around Americans because of how poorly they react when we show them our point of view. Racism and pedophilia are two most sensitive topics for Americans, who feel like they're on the holy mission to free the world from these two evils. reply archagon 1 hour agorootparentIf you’re ever “proud” of hating someone, you should go see a therapist. (Or a priest.) reply anal_reactor 5 minutes agorootparentOf course any exercise in explaining our point of view is futile, because, again, the idea that there might be some truth to racism is literally the most taboo topic of your culture, so there's no way you'd ever agree with me to any degree, and any consensus is non-negotiable. It's like trying to explain to an Arab shepherd that his religion might be wrong. Or to a Japanese that smoking weed isn't much of a deal. These ideas simply go against core values of their cultures, and no argument will make them change their mind. So we just nod and smile \"ah yes amerika good racism bad what else do you want to hear\" jajko 2 hours agorootparentprevOh we have quite a bit of racism in Europe too, just go to eastern part. People are not so vocal about it, unless in 'their' circles. Sure, its all mixed together with fear of different religions, xenophobia which I would say is still dominant force, and its targeted way more on black people rather than east asians, but these days its there, even on places that had 0 of it due to literally 0 exposure to other races few decades ago. I personally saw first black person in person as a teenager for example. Big parts of societies are quite radicalized if you care to look closely enough, which many don't and consider Europe some form of uniform hippie paradise. But then you can't escape the reality of ie string of victories of more or less extreme right winders all getting the vote 'to stop immigration'. Whole Brexit was fueled mainly by such xenophobia, it would be insignificant fart in the wind without this. My personal opinion is that before countries like Germany decided to allow unregulated immigration en masse and try to push rest of EU in same direction without asking, serious discussions should have taken place for a long time and explained to common folks why, in what form, for how long, how will it affect them, how will state protect them etc. Instead, at least eastern part went through shock therapy and hence often seen kneejerk reaction of refusing everything. reply mrguyorama 2 hours agorootparent>Oh we have quite a bit of racism in Europe too, Just mention Romanians and Europeans can pretty quickly get a sense of how racism works in the US reply kjkjadksj 3 hours agorootparentprevnext [3 more] [flagged] Swizec 2 hours agorootparentCartoons from the 1920’s and up to 1940’s are full of racist tropes like that And yes we watch those as kids at least in Europe because “all cartoons are for kids” reply kjkjadksj 1 hour agorootparentNot nearly as bad as what happened in Europe during the 1940s in terms of race relations that's for damn sure reply yieldcrv 14 hours agoprev [7 more] [flagged] hermannj314 14 hours agoparent [7 more] [flagged] stephen_g 14 hours agorootparentWhat you are saying is not what they are saying at all. Many of the Nazi medical experiments talked about were in the same ballpark of what was disclosed here. They occurred along with the euthanasia and genocide but weren't the euthanasia and genocide itself, they were different. As for your last sentence, of course not - it would fall under a completely different type of law that war crimes... I don't know if they were charged with anything but they should have been charged with crimes under US criminal law. reply hermannj314 13 hours agorootparentNo one was ever criminally charged for Tuskegee. That is insane to me. reply kjkjadksj 3 hours agorootparentNeither for MK ultra reply yieldcrv 14 hours agorootparentprevthere is an entire category of comparisons that take specifically dissimilar things and highlight their similarities and Peter’s complaints were recognized for their similarity by Congressional hearings in that age, and the White House eventually reply hermannj314 14 hours agorootparentCan you give a specific example? The white house apologized and acknowledged Tuskegee, I did not read that they eventually recognized it was comparable to the holocaust. Did Clinton affirm that? Did he give a specific example how it was similar? reply bugglebeetle 14 hours agorootparentprev [–] The full extent of Nazi medical experiments was not limited to mass murder and included things akin to what was done with Tuskegee so your question seems either deliberately in bad faith or something you could answer yourself with a bare minimum of research https://en.m.wikipedia.org/wiki/Nazi_human_experimentation It’s worth pointing out that the U.S. also protected and employed Nazi medical researchers as part of Operation Paperclip. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Peter Buxtun, the whistleblower who exposed the unethical Tuskegee syphilis study, has passed away at 86.",
      "Buxtun's revelation led to the termination of the study in 1972, congressional hearings, and a $10 million settlement for the victims.",
      "His comparison of the study to Nazi medical experiments prompted new government research regulations, highlighting his significant impact on ethical standards in medical research."
    ],
    "commentSummary": [
      "Peter Buxtun, the whistleblower who exposed the unethical Tuskegee syphilis study, has passed away.",
      "Buxtun's concerns about the study, which withheld treatment from African American men with syphilis, were ignored for six years before gaining attention.",
      "The Tuskegee study has had lasting impacts, including contributing to vaccine hesitancy in African American communities and highlighting systemic racism in medical research."
    ],
    "points": 211,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1721099843
  },
  {
    "id": 40972437,
    "title": "The Delusion of the Polygraph",
    "originLink": "https://lithub.com/what-the-all-american-delusion-of-the-polygraph-says-about-our-relationship-to-fact-and-fiction/",
    "originBody": "Craft and Criticism Fiction and Poetry News and Culture Lit Hub Radio Reading Lists Book Marks CrimeReads About Log In Literary Hub Craft and Criticism Fiction and Poetry News and Culture Lit Hub Radio Reading Lists Book Marks CrimeReads Log In What the All-American Delusion of the Polygraph Says About Our Relationship to Fact and Fiction Justin St. Germain Considers the Blurry Borders Between Memory, Memoir and Myth Via New England Review By Justin St. Germain July 15, 2024 A few weeks before the release of my first book, a memoir about my mother’s murder, I had to take a polygraph exam. The two things were not in fact related, but that was easy to forget once I found myself strapped in a chair in a windowless room on the fourth floor of a federal building in El Paso, with some polygrapher I’d just met sitting behind me, asking questions. Article continues below I’d met my examiner, whom I’ll call Kevin, that morning. The federal scheduler had insisted on a 9:00 am appointment even though I lived four hours away, which meant I’d spent the previous night alone in a Motel 6 by the highway in El Paso, eating Del Taco and reflecting on the decisions that had led me to spending the night alone in a Motel 6 by the highway in El Paso, eating Del Taco. Technically, I was there because I’d applied for a job with Customs and Border Protection. But the truth seemed much more complicated than that. I didn’t get much sleep, and showed up at the federal building early, dressed in what I imagined the government meant by comfortable clothing: black dress pants, plain white oxford, no tie. I looked like a banquet waiter. One other guy was in the waiting room when I walked in. As we sat there past the scheduled time of our appointments, we struck up a desultory conversation. Like me, he’d been in the hiring process for years, had driven down from Albuquerque the night before, and seemed nervous. He asked if I’d done any research on the polygraph. I said no, and asked him the same question. He said no. We were getting our first lies out of the way. The lie detector was like any true story in America: the facts didn’t matter as long as a lot of people believed it. The government’s guidelines had repeatedly stressed that we should not do any research before our polygraph. Their insistence struck me as odd: if the machine detects lies, why would it matter? I’d spent much of the previous week Googling polygraphs. I was in the middle of denying it to my new friend when the door opened and Kevin appeared. He was around sixty, short, portly, bald, with a silly goatee and wire-rimmed glasses, wearing a baggy gray suit and a shirt in one of those colors I never can keep straight, puce or mauve or periwinkle, a little too festive for the occasion. Kevin was squinty and smug, with an air of hollow authority that reminded me of my middle-school principal. He didn’t even step into the waiting room, just swung the door open and shouted my full name. I stood and shook the hand he extended. He squeezed too hard and said I could call him by his first name, as if he were doing me some great favor. Article continues below Kevin led me down a drab hallway to a door on the left that led to my first disappointment. I’d been expecting the sort of tableau you see in cop movies, some dank cellar with a dangling bulb and a two-way mirror on the wall for me to stare defiantly into. Instead we entered a bare, sterile room with office chairs on either side of a desk. Wires ran from the computer on the desk across the room to a hard-backed chair festooned with cuffs and straps and sensors. We sat in the normal chairs. Kevin leaned back in his, twirled a pencil, and said, “Let’s get started.” I knew this bit, the casual tone and performative warmth. I was a college professor, sort of, and I did this same bit on the first day of classes, trying to make my students trust me. In that first fluorescent moment, staring into Kevin’s beady eyes, I had a premonition: I was going to fail my polygraph exam. * Joan Didion once wrote that it’s easy to see the beginnings of things, and harder to see the ends. That has not been my experience. Ends are obvious: divorce, death, getting fired. Beginnings, on the other hand, seem subjective. If ends are facts, beginnings are truth: relative, random, subject to belief. The story of my exam, for example, could begin two years before I met Kevin, when President Obama signed an act requiring polygraph screening for applicants to certain federal agencies. Or two years before that, when I started writing a memoir and began to understand what it means to tell the truth. Or it could begin with the polygraph itself, the kind of story America likes best: a simple one that isn’t all that true. The polygraph’s most commonly credited inventor is John Larson, an employee of the Berkeley Police Department, who developed a new device for interrogations in 1921. Larson was twenty-nine at the time, and, like me, a strange candidate for law enforcement: he might have been America’s first cop with a PhD. Article continues below His degree was in physiology, the science of the body’s systems. The then-prevailing scientific belief saw crime as biological, either hereditary or the result of a physical defect. Larson explored both possibilities. His undergraduate work tried to find familial patterns in fingerprints that predicted criminality. In grad school, Larson shifted his focus to thyroid deficiencies. The results were disappointing, so he turned to machines. Larson read an article about using blood pressure to detect deception, and decided to improve on its author’s technique by designing a machine that could do so more objectively. The polygraph was born. The truer story is, as usual, more complicated. Larson’s machine was not so much an invention as it was an amalgam of existing devices. He didn’t believe it detected lies and didn’t call it a polygraph: Larson referred to his machine as an “emotion recorder.” His protégé and rival, Leonarde Keeler, would later come up with the term polygraph to help commercialize the device. Polygraph organizations like to say the word means “many writings,” which is halfway true. It’s a neologist portmanteau of the Greek terms meaning exactly that, and the machine does indeed create many writings. But to claim the word means only and exactly one thing is to make the same mistake with language polygraphers habitually make with facts: believing that they’re static and absolute. Language, like truth, is neither. Words evolve and change over time and mean different things in different contexts. Polygraph has six different definitions, according to the Oxford English Dictionary—three of which predate the machine—and they range from a letter grouping in cryptography to a person imitating another to a writer of various works. (Which I guess makes this an essay about a polygraph taking a polygraph.) More to the point, polygraph does not mean “lie detector.” Larson himself repudiated that term for the rest of his life. But that fact didn’t get in the way of a good story. Once the polygraph was adopted by police across America and heralded in the popular media, it took on a mythical new name: the lie detector. And as soon as the lie detector became famous, a bunch of men fought with each other for decades—mostly in their memoirs—over who was its true inventor. In his book The Truth Machine: A Social History of the Lie Detector, Geoffrey C. Bunn devotes an entire chapter to the question of who invented the device and offers enough credible candidates to field a baseball team: everyone from Carl Jung, who helped pioneer the field of psychology, to Étienne-Jules Marey, who did the same for cinematography. As Bunn puts it, with magisterial restraint, it was a “curious and notable fact that the lie detector’s principal actors mistrusted each other intensely.” Maybe they should have taken a polygraph to resolve the question. * Article continues below My polygraph test was the final step in a process that had begun three years earlier, when I started applying for government jobs. At the time, I was living in San Francisco, teaching at Stanford, and was nearly finished with the book. While my job sounded impressive, it paid forty thousand dollars a year and was located in one of the most expensive areas of the country. I was in the second year of a two-year contract and had been trying unsuccessfully to get a real teaching job for years, something tenure-track, or at least a lecturer gig in a more affordable city, one with benefits and some semblance of job security. When I was “on the market,” as they say, I applied to teaching jobs in Fairbanks, Alaska; Birmingham, Alabama; Spokane, Washington; Camden, New Jersey; Cullowhee, North Carolina; and on and on. I spent a lot of time back then browsing Wikipedia pages for cities I’d never been to, trying to convince myself I could live there. It was a moot point. Despite dozens of interviews, I couldn’t get a professor job. My memory has condensed the experience into one vivid example, when the writing faculty of a football powerhouse down South made me fly to Chicago, in January, on my own dime, so I could sit in their hotel suite sweating through my freshly pressed suit while they asked a series of oddly combative questions for an hour and a half, after which they never bothered to contact me again to tell me that I didn’t get the job I wouldn’t have taken if my life depended on it. Somewhere along the way, I decided to give up on academia and find a less demoralizing line of work, something with better pay, more stability, maybe even a union. I applied to be a technical writer, an FBI agent, a cop. I didn’t have any luck with those, either. My brother suggested the Border Patrol. He was an agent, and so were a half-dozen of my childhood friends and my former baseball coach. They were always hiring. The starting pay was nearly double what I made at Stanford, with much better benefits, and I might be able to move back to Arizona. Most of my old friends from home thought joining the Border Patrol was a good idea. I could be closer to family, make a living, maybe even buy a house. My social circle in San Francisco was another story. By then, I’d been in academia for more than a decade, surrounded by white liberals from wealthy backgrounds. When they heard I was trying to join the Border Patrol, my new friends all said the same thing: why? I didn’t understand the question, and soon determined it was a matter of perspective. For people who grew up wealthy, or even middle-class—whatever that means—the proper career path seems to be to find a job that’s rewarding, fulfilling, whatever. I’d done every job I’d ever had for one reason, the same reason I was trying to join the Border Patrol: money. Did I want to drive around the border in an SUV detaining people? Of course not. But I didn’t want to help tech billionaires write their memoirs, either. At least the Border Patrol paid a living wage. Article continues below But the Border Patrol is racist, my friends said. And that might be true. But cries of racism rang hollow coming from people who worked in academia, one of the whitest industries in America. The Border Patrol is a hell of a lot more diverse than the average writing faculty. Their underlying point was right, though: if I joined the Border Patrol, I’d be complicit. But I was already complicit. The longer I spent in higher ed, the more it seemed like an engine driving American inequality. At least the Border Patrol gave working-class people a path to economic mobility without being saddled by a lifetime of student loan debt. My brother had begun his career at the same time I’d started grad school. Nine years later, he made twice as much as I did, owned a house and a new Acura, had health insurance and a retirement account and the whole American dream. Meanwhile, in the three years since I applied to the Border Patrol, I’d taken another teaching job in Albuquerque, making forty-five grand, with shitty benefits and no long-term security. Besides, I still hadn’t decided to actually join the Border Patrol. I hadn’t been offered the job. By the time I went to El Paso, I’d passed a four-hour written exam, a physical, a fitness test, an oral interview, and a background check so intensive that they’d talked to my coworkers, every neighbor in my apartment building, and my high school teachers. Once I passed the background check, I waited more than a year for the call from the polygraph office. That call would be the first sign that the government and I had different ideas about truth. First, I got an email from a scheduler saying he’d been trying to reach me. I hadn’t received any messages, so I knew that wasn’t true, but had no way to prove it. I called the office and spoke to another man, who said he was a quality control agent, and that he’d call me back soon with a date and time for my appointment. He never did. Then I received a letter saying I’d been removed from consideration for refusing to take the polygraph. I called Mr. Quality Control. He accused me of lying, but grudgingly scheduled my appointment. So there I was, in a building full of liars, about to have mine detected. * Kevin started off by reminding me of the rules. One of them was that I could not discuss my test with anyone afterward. I’m sure he would have told me that I couldn’t write about it either, although I made sure not to ask. Clearly the government believed, despite all historical evidence to the contrary, in its power to control information. I believed my right to free speech was inalienable. In that sense, I guess this essay is a lie detector, too: we’re going to find out who was right. Kevin ran me through some questions that might be on the test. Some were pedestrian, my name and address and so on. Others were bizarre: questions about bestiality, child porn, terrorism. When he got to a question about my past drug use, Kevin’s tone changed. His smile fell and he held eye contact. The interrogation had begun. I told him the same well-rehearsed thing I’d told my background investigator: I experimented with drugs a few times in high school and college. It was a lie. Kevin seemed to detect it. “What do you mean by few?” he asked. “Not many.” “Could it mean five?” “I guess it could.” “Don’t be a smartass.” I looked around the room. It couldn’t be just me and Kevin. Surely there was a witness somewhere, a hidden camera, a recorder, anything to prove we actually said what his report would say we had. Later I would learn that federal polygraph protocol requires examiners to make audio recordings of exams, and that many examiners have been accused of ignoring that requirement, as well as an array of other sordid and unprofessional practices. I don’t remember Kevin saying anything about a recording, or seeing a device. But there’s no way to say for sure. Kevin sighed elaborately and asked the maximum number of times I could have used illegal drugs. “Was it six? Eight?” “Sure,” I said. “It was a long time ago.” “Could it be ten?” he asked, with a smug little grin, and finally I caught on. “No. Definitely not ten.” If Kevin wanted a fixed and certain truth, I’d give him one. He wrote “6–8 times” on his notepad. “I said it could have been that many, not that it was.” Kevin said to let him do his job, and a shroud of dread descended. The test hadn’t even begun, and I’d already found myself at epistemological loggerheads with the federal government. Kevin thought he’d convinced me to tell the truth, which was that I’d done drugs between six and eight times in my life. I thought there was no truth to tell. I didn’t remember how many times it was. A few minutes earlier, Kevin had said there was no maximum threshold for drug use that would disqualify me—a bald-faced lie I had not yet detected—but I wasn’t about to say that the real number was closer to a hundred. So the lie became the truth: I’d done drugs six to eight times. The polygraph works a lot like a memoir. It doesn’t find the truth, it creates it. Almost immediately, I began to believe it. As I sat there watching Kevin scribble notes, a handful of specific drug experiences returned to me. The first time I got high and drunk at the same time, in eighth grade, and spent the night in my friend David’s bathroom with my head on the toilet seat while he tried to convince his mom that I had food poisoning. When Charlie taught me the trick where you blow the smoke through a paper towel tube with a fabric softener sheet rubber-banded over the end. The first line of meth I ever snorted, off a Bone Thugs CD case in Jeremy’s bedroom. The cooks at my first restaurant job going around at closing time on busy nights, handing out key bumps. The eight ball I went in on with two ex-con dishwashers at my second restaurant job, who told me they’d pay me back double in a week, but then Mike got stabbed and I realized I wasn’t cut out for that life. I never saw that money or the drugs, so maybe that one didn’t count. But that was all during one relatively short period of my life, when I was a shithead teenager. I’d come so far since then. I was a college professor; technically a visiting assistant professor, but still. My memoir was about to come out. The sitting president at the time had admitted to using marijuana and cocaine in his memoir. Why should I be banned from a job for being a small-time delinquent twenty years ago? Remove those few wayward years and it was true enough that I’d only done drugs a few times in high school and college. As we say in the memoir business, it was my truth. * My new profession, memoirist, had a complicated relationship with truth, to put it mildly. Fake-memoir scandals have erupted more or less continuously as long as America has existed, from James Frey and his contemporaries, to the so-called autobiographies of Howard Hughes and Davy Crockett, to fantastical captivity narratives of the colonial era and dubious accounts of European explorers in the New World. Sometimes it seems like every notable American figure wrote a fictionalized memoir—even Wyatt Earp, the patron saint of my hometown. (Frontier Marshal. It’s a hoot.) I fudged plenty of facts myself, combining real people into composite characters, changing the order of events. Most memoirists do the same. The point of the genre isn’t accuracy or precision. The point is to tell a good story. The same applies to the polygraph. There’s no real evidence for the machine’s accuracy. Its purpose is to monitor the body’s physical response to stimuli, but the body’s response to lying is indistinguishable from its response to any other stimulus. Even the telltale spike in the polygraph chart, itself largely a myth created by TV and movies, can indicate anything from a heart problem to sexual attraction. (Indeed, the machine’s inventors used it to detect both of those things. Larson married one of his first test subjects, and Keeler discovered a heart defect while testing the machine on himself.) But from the early years of the machine to the present day, its proponents have told the same story of an infallible machine that detects lies. Polygraph organizations routinely estimate its accuracy at nearly 100 percent. Most of those estimates are invented out of thin air, and the few based on data suffer from an obvious sampling error. As early as 1939, Walter Summers—yet another purported inventor of a lie detector—pointed out the fundamental flaw on which all polygraph statistics are based: they “fail to relate the number of instances in which deception was actually practiced in a manner which eluded the examiner and the instrument.” You can’t detect the lies you can’t detect. Independent studies suggest a polygraph exam is roughly as accurate as a coin flip, and that polygraph operators find as many as half of innocent subjects guilty. The scientific case against the polygraph is so compelling that the National Academy of Sciences, the American Psychological Association, the Congressional Office of Technology Assessment, and the United States Supreme Court have dismissed it as unreliable. A federal law forbids using the polygraph to screen applicants to private companies. For most jobs in America, an exam like mine would’ve been against the law. In fact, the only people who seem to believe the polygraph is accurate are its operators. Every study I’ve found that supports the machine’s ability to detect deception was funded or performed by polygraphers. They’ve formed half a dozen different organizations dedicated to spreading the lie of the lie detector. Their websites are ironically similar to the polygraph itself: archaic, slipshod, rife with bias and bullshit. The International League of Polygraph Examiners calls the device’s invention “officially one of the greatest of all time,” and claims the accuracy of contemporary polygraphs is close to 100 percent. The American Polygraph Association, which claims to be the largest organization of polygraphers, has a section of its website devoted to Polygraph Validity Research. It begins by stating the organization “believes that scientific evidence supports the validity of polygraph examinations.” The site includes a link to what seems to be the entire basis of that belief, the “Meta-Analytic Survey of Criterion Accuracy of Validated Polygraph Techniques.” The document was prepared by a team of polygraphers, and it reads about how you’d expect. I made it far enough to learn a few astounding facts, including that until 2012, the American Polygraph Association did not require members to use methods supported by published research. In other words, for the first ninety-one years of its existence, polygraphers literally had no scientific standards. Luckily for me, they came up with some just in time for my exam. The lie detector was like any true story in America: the facts didn’t matter as long as a lot of people believed it. And we wanted to believe it: the notion of a lie detector existed long before the polygraph did. Tellingly, it first appeared in fiction. Bunn traces its first known usage to Charles Walk’s 1909 detective novel The Yellow Circle, in which a character fantasizes about having a machine he calls a lie detector: “With its aid one can plumb the bottomless pits of a chap’s subconscious mind, and fathom all the mysteries of his subliminal ego.” In 1914, seven years before anyone claimed to have invented a polygraph, G. K. Chesterton mocked the notion of a lie detector in his mystery story The Mistake of the Machine, comparing it to the Dark Ages belief that blood would flow from a victim’s body if their murderer touched it. Bunn lists many other instances of lie detectors appearing in fiction long before anyone claimed to have invented one. Meanwhile, the popular American media seemed obsessed with the idea of machines that could see inside our heads, hearts, and souls. Fourteen years before Larson’s polygraph debuted, the New York Times ran an article rhapsodizing about Jung’s electric psychometer. That “mysterious little machine” purported to detect emotions, not lies; still, the article foretold a future when it would be used to detect guilt, making criminal courts superfluous. Four years later, the same paper ran a two-page profile of the “big-hearted” men, Edward Johnstone and Henry Goddard, who’d been doing the “self-sacrificing work” of testing the psychometer on developmentally disabled children at an institution in New Jersey. Under Johnstone’s supervision, Goddard hooked kids up to a machine like Jung’s. Their intent was to study and eradicate “feeble-mindedness”; Johnstone was a noted member of the American eugenics movement. But the article breathlessly predicted a future in which the lie detector would replace the “impedimenta” of American justice, like judges and juries. When the story started circulating that a cop with a PhD had invented a lie-detecting machine, a myth became a widely reported fact almost overnight. Within a year, the San Francisco Examiner claimed, “everyone has heard of the ‘lie detector.’” By then, polygraph results had already been banned from American courts, by a judge who was less enthused about the prospect of a machine replacing juries. It would be the first of many official dismissals of the polygraph. But it didn’t dampen the media’s fascination with the so-called lie detector—and, by extension, the American public’s. That fascination was furthered by the polygraph’s sister inventions, psychology and cinematography, which were created by some of the same men at around the same time. In addition to his psychometer, Jung also helped create the field of modern psychology. Marey developed a different forerunner of the polygraph, as well as chronophotography, an important step in the development of cinema. Marston pioneered the idea of detecting lies based on physical responses, and later became a Hollywood censor and wrote a guide for aspiring screenwriters. His academic mentor, Hugo Münsterberg, helped lay the theoretical groundwork for the polygraph, and also published one of the earliest works of film theory. The rise of psychology helped drive a cultural fascination with discovering the inner workings of the mind—especially the criminal mind—a desire the lie detector satisfied. Meanwhile, cinematography, the notion that we could document and preserve reality itself, had embarked on its ongoing project of destroying our cultural distinctions between fiction and fact. Throughout its history, the polygraph has moved freely between the two realms. As it spread to police departments across the country, who used it to investigate real crimes, the machine also began to appear in advertisements and movies. The machine may have made its screen debut in the 1926 silent-film serial Officer 444, alongside Vollmer, who played an idealized version of himself, a criminologist using science against evil. By the 1930s, Marston was using the polygraph to screen-test Hollywood films, including Frankenstein—ironic, considering that Larson once compared his invention to the Monster—and to sell razor blades, gasoline, and cigarettes. In 1941, Marston invented Wonder Woman, a female superhero whose primary power was a Lasso of Truth, similar to a lie detector. Marston called the comic “psychological propaganda,” and it was hugely effective: within a year, Wonder Woman had her own comic book with a circulation of half a million. In 1946, Keeler starred in a noir movie alongside his version of the machine. The first TV show called Lie Detector debuted in the fifties; there have since been a handful of others, both scripted and reality, not to mention a continuous stream of polygraph appearances in film and television. While writing this essay, I watched lie detectors play prominent roles in two different TV shows. In one, the polygraph is accurate; in the other, it isn’t. * Kevin slid a blank sheet of paper across the desk and told me to draw the number five inside a black circle. I did, and slid it back. Kevin drew other numbers in other circles and said now he was going to hook me up to the machine. He told me to take off my shoes and empty my pockets, then directed me to the chair. I sat on one sensor and put my feet on two others. Kevin wrapped two cords around my chest, slid a sphygmomanometer over my left arm, and stuck metal clamps on my right index and ring fingers. As he pumped up the cuff, Kevin asked if I was comfortable. He didn’t seem to be joking. He sat behind his desk and said he was going to point to all the numbers on the paper and ask if I’d written them. I should say no every time, even for the one I’d written. He did. I did. He unhooked me, led me to the desk, and pointed to the lie on the computer screen. It looked like a lot of squiggly lines to me. “Now we can take a break,” Kevin said. I looked at the clock, which wasn’t visible from the polygraph chair; we’d only been in the room for half an hour. Kevin smiled inscrutably. “Bathroom and water only. Be back in ten minutes.” I wandered into the hallway, drank from a fountain, leaned against another beige wall, and tried to calm down. I was not then in a great place, psychologically speaking. I stayed up until sunrise a few nights a week, spent days on end inside my apartment, often went blank with anxiety in front of my classes, and was preoccupied by a vivid and persistent vision of myself swan-diving off my balcony. I would later be diagnosed with various conditions and embark upon a reasonably successful therapeutic journey, but right then, in that hallway, I was freaking the fuck out. My shirt clung wetly to my chest, where I could see my heart beating as if it was trying to escape, like the alien in Alien. If I had a heart attack in the chair, what would that look like on the polygraph readout? I’d tried to learn techniques for managing anxiety. Most of them didn’t work— picture a beach, my ass—but a shrink I’d briefly seen had suggested imagining the worst possible outcome, and that seemed helpful. The idea was that you embrace the notion of failure and realize it wouldn’t be so bad, thereby relieving the pressure not to fail. I tried it. What if I failed the poly? I’d go back to Albuquerque, keep teaching, apply for more jobs. This was my backup plan, which made me luckier than pretty much everyone else applying. Then again, that was not the worst-case situation. One problem with that exercise is that I could always come up with something worse. What if I got in a car accident on my tired four-hour drive and spent the rest of my pain-filled life alone in my rented house in Albuquerque? What if I passed the poly, got the job, and actually took it—got sent to some borderland armpit like Ajo or Wellton where I’d have to herd other human beings into the back of trucks? Woke up two or ten or thirty years down the road and didn’t recognize myself? By the time Kevin came to get me, a few minutes sooner than the ten I’d been promised, I’d almost accepted my imminent failure. If my anxiety didn’t make me fail the test, something else would. I remembered something from my sorta-research about Catholics failing the polygraph at higher rates. Technically I was Catholic, baptized and confirmed, now lapsed, but that only made things worse. Maybe I’d ask Kevin how to become a polygrapher. How much training did it require? Did he enjoy it? How much money did he make? Later, I’d search around online and find out that the average polygraph examiner makes even less than I did at the time. Then again, the training only takes ten weeks, and there are actually jobs in that field. I’d been training for years to be a nonfiction professor and still had no idea what truth actually meant; maybe I should’ve just taken a polygraph course and become an official federal arbiter of facts, an asshole demigod like Kevin. I shouldn’t be so hard on Kevin. Judging by his clothes and demeanor, he probably came from a similar background to mine. Maybe polygraphing was his version of teaching, a thing he did to pay the bills because it was better than his other options. Maybe he had a whole life to maintain, a family, a little house in some cul-de-sac on the West Side, two Toyotas and a swing set. While Kevin strapped me back into the chair, I wondered what he told himself at night, trying to sleep, after watching applicants lose their best hope for a career to his machine. Kevin seemed like a smart guy, way too smart to believe in the simpleminded fantasy of a machine that detects lies. But that wasn’t his decision. It was his employer’s. And why is our government the only major employer in the world that uses polygraphs to screen prospective hires? * The answer to that question is based on a lie. Even the United States government isn’t dumb enough to believe the polygraph works. The machine’s real purpose is symbolic, as an icon of the power of the state. Law enforcement agencies don’t use the machine to detect lies. They use it to coerce confessions. In its early days, the polygraph was considered a more humane version of the infamous “third degree,” the interrogation procedure it largely replaced, which involved beating the shit out of a suspect until they confessed. The third degree was itself a variation of another quintessentially American tactic, outright torture. The parallels between torture and the polygraph are obvious: the latter’s arcane parts and procedures, its use of restraints and stimuli, the gratuitous periods of waiting for what the subject knows is coming. The polygraph creates the very stress it’s designed to detect, then presents it as evidence of deception, which often leads its subject to confess. If the subject confesses, that confession effectively becomes the truth, whether it’s true or false or somewhere in between. And the polygraph has a long history of coercing false confessions. It may begin with its maiden voyage in 1921, when Larson tested his new device on the residents of an all-female Berkeley dorm that had experienced a rash of petty thefts. Thanks in part to the polygraph, a suspect admitted to most of the thefts and withdrew from the university. But the crimes continued, and Larson himself doubted the veracity of her confession. Not long after, Larson tested a man named Henry Wilkens who was accused of having his wife killed. The polygraph helped to exonerate Wilkens despite evidence of his guilt. After that, police began to doubt the polygraph’s utility, and some departments refused to use it. (The media had no such qualms: it continued to trumpet the infallibility of the “electric detective.”) A year after the Wilkens case, a young Black man named James Frye retracted his confession to killing a Washington, DC, doctor, claiming it was coerced. Using his variation of the lie detector, Marston examined Frye and declared him innocent. But the judge prevented Marston from testifying as an expert at trial, and an appeals court upheld the ruling, instituting what became known as the Frye rule, which has largely prevented polygraph results from being admissible in American courts ever since. But the machine remains useful for extracting confessions. And the conflation of confessions and truth is yet another lie, one that’s kept the polygraph alive for the last century as a peculiarly American delusion. Confessions are usually presumed to be true and treated as such in legal settings. But recent research suggests that false confessions are common, especially in the context of police interrogations. Despite a growing body of evidence, including hundreds of exonerations based on DNA evidence, most people don’t believe in false confessions. A recent article in the Journal of the American Academy of Psychiatry and the Law explains why: Most lay people believe in what has been referred to as the myth of psychological interrogation: that an innocent person will not falsely confess to police unless he is physically tortured or mentally ill…the myth of psychological interrogation persists because most people do not know what occurs during police interrogations, and because they wrongly assume that individuals do not act against their self-interest or engage in self-destructive behavior, such as falsely confessing to a crime that they did not commit. The likelihood of a false confession increases when interrogators use certain tactics, especially elements of the so-called Reid Technique, a procedure created in the 1950s by John E. Reid. I didn’t know it at the time, but Kevin used elements of the Reid Technique in my test. He conducted it in a small, barely furnished, cold room; seated me in a hard, armless, straight-backed chair; and repeatedly encroached on my personal space. Reid first used his technique (along with a polygraph) in 1955, to extract a confession from a man named Darrel Parker who was suspected of killing his wife. Parker was convicted and sentenced to life in prison. He served fifteen years before being released on appeal because Parker’s confession was ruled to have been coerced. Eighteen years after Parker’s release, a man on death row for other crimes confessed to the murder; he did so by showing his lawyers a passage of his memoirs that described the murder in detail, a passage the legal system apparently assumed to be true. Neither those nor the Reid Technique’s subsequent high-profile failures, including the $2m settlement of a 2012 civil case by a wrongly convicted man named Juan Rivera, have prevented it from being adopted by police departments across America. The company founded in Reid’s name, John E. Reid and Associates, claims its technique is “the most widely used approach to question subjects in the world,” and recently registered a trademark on the term. The Reid Technique™ involves a number of tactics, from creating an anxiety-inducing environment to a list of specific steps. According to Saul Kassin, perhaps the foremost American expert on false confessions, the purpose of those tactics is to “get suspects to incriminate themselves by increasing the anxiety associated with denial, plunging the subject into a state of despair and then minimizing the perceived consequences of confession.” Like the polygraph, the Reid Technique isn’t designed to find the truth. Its purpose is to coerce confessions. Research suggests the Reid Technique may actually make interrogators worse at detecting truth. In an independent study, interrogators trained in the Reid Technique were less accurate, although “they were more confident and cited more reasons for their judgments.” Myths exist for a reason, to explain collective phenomena, to explain us to ourselves. The polygraph itself is not required for the Reid Technique, but it helps. Together, they have a long and checkered history of producing false confessions. In 2013, the Chicago Tribune found a pattern of false confessions obtained via polygraph exams and the Reid Technique, by examiners who routinely ignored accepted standards, including failing or refusing to record interrogations. * Kevin said the first battery of questions would cover my character, and asked if I had any questions. I did, but too many, and where to start? So I said no, and Kevin started the exam. He asked me a battery of eight questions four times in different orders. By the time I typed notes on my phone after the test, I’d already forgotten one of them. The other seven were: 1. Have you misrepresented your past drug use? 2. Have you lied about participating in serious crimes? 3. Have you falsified info on forms? 4. Have you ever cheated to get ahead in your personal life? 5. Have you ever made disparaging comments about your supervisor? 6. Is the light on? 7. Have you taken a drink of water today? Except for the last two, all of them seemed open to interpretation. For instance, I absolutely had misrepresented my past drug use, but only the number of times, not the drugs or the fact of doing them. And could my estimate be a lie when there’s no way to know the exact answer? What crimes are considered serious? What counts as cheating? Who gets to say? Has any employed person in America not made a single disparaging comment about a supervisor? The first time through the questions, I tried to follow Kevin’s direction to answer quickly, yes or no, and to abide by his somewhat contradictory instructions to breathe normally while staying absolutely still. The second time through, my voice began to crack, and I swallowed. “Stop!” I turned my head to see who he was yelling at. “Stay completely still!” I turned back to the wall and tried to comply. Kevin kept yelling, asking combative and rhetorical questions: was I trying to beat the test, did I want to fail? I tried to calm myself by imagining something peaceful, although that was probably considered a countermeasure, and anyway, it didn’t work: I visualized ripping off the electrodes and punching Kevin. I tried the box-breathing technique I’d once learned from a veteran stepdad. That worked better. Possibly too well. Soon I caught myself nodding off. That probably sounds like a lie. How could someone under that much stress be sleepy? Have you ever been interrogated? I don’t mean metaphorically, having a difficult conversation, confessing something to a spouse, parent, priest, boss. I mean actually interrogated, by a professional. No lawyer, no witnesses, nobody on your side. And he has a machine that says he’s right, not to mention the backing of the Department of Homeland Security, a vast and unaccountable agency built on the lie that policing and surveilling Americans will protect us from terrorism. Suddenly, this part of the Border Patrol application process made a grim kind of sense: I was getting a little taste of how an immigrant might feel. Except I deserved it. I’d signed up for this. If you haven’t been in that situation, maybe you think, like I did before it was proven otherwise, that you’d be one of the exceptions. You’d beat the polygraph, like people do on TV. But that’s the thing: you can’t beat the polygraph, because the polygraph isn’t a lie detector, isn’t a test, isn’t even a machine. It’s a fact, part of a story power tells itself to justify its power. Maybe you can beat the machine— they don’t detect lies, so it’s not that hard—but you can’t beat an entire country that believes in it. As Kevin went through the questions again, I sank into a fugue, part paranoia and part exhaustion, and lost track of time. Not what time it was—the whole idea of time. There was no past or future, only an endless present of sitting in that windowless room, strapped to a chair, wired to a machine, staring at a beige wall while a stranger I couldn’t see asked the same questions, over and over. I forgot why I was there, who I was, the truth and what it meant. At some point, I heard a sort of flutter, and my vision vibrated and jumped, as if someone had changed the reel. My mind floated up to the corner of the room and observed the proceedings from a cool remove. My memory of the rest of the test is from a vantage point outside of my body. Jung defined this phenomenon as dissociation, the loss of a fixed and coherent identity. Reports of similar experiences were largely ignored in early psychology, but more recent studies suggest dissociation is fairly common, and can be triggered by drugs, trauma, stress, or nothing at all. Jung said dissociative states could prevent a subject from recalling important facts, among other things. “We talk about being able to control ourselves,” he wrote. “But self-control is a rare and remarkable virtue.” The polygraph showed me I was neither rare nor remarkable. By the final time through the battery, I no longer knew what was true. For example: the first three times Kevin asked if I’d ever disparaged a supervisor, I’d rationalized, telling myself “disparaged” was a strong word. It means to regard as having little worth, and older definitions meant to dishonor or degrade; did Kevin know it came from the Old French disparagier, to marry unequally? Certainly I’d made fun of some bosses, and respected few, but I hadn’t disparaged my supervisors, per se. The fourth time he asked me the question, a crystalline memory popped into my detached head, a moment a few years before when I told my then-girlfriend that my then-boss was a fucking idiot. “No,” I said. Kevin moved on. Later I would learn that I wasn’t the first lie detector subject to report experiencing dissociation. I’m not even the first one to write about it. As an undergraduate at Harvard, Gertrude Stein worked in a laboratory run by Hugo Münsterberg, who came up with the earliest scientific rationale for lie detection. Stein’s first published work, an 1894 essay originally written for her sophomore composition class titled “In the Psychological Laboratory,” was an account of her experiences in the lab, including an instance of being connected to one of Münsterberg’s primitive predecessors of the polygraph. The essay’s third person narration and distinctive syntax are both harbingers of Stein’s future work, and her knowledge of the machine seems to have informed her later experiments in “automatic writing.” (It’s also worth noting that her autobiography has fictional elements.) But her account interests me for other reasons. She describes the experience of being subjected to an exam in front of a group of students like so: Strange fancies begin to crowd upon her, she feels that the silent pen is writing on and on forever. Her record is there she cannot escape it and the group about her begin to assume the shape of mocking fiends gloating over her imprisoned misery. Suddenly she starts, they have suddenly loosened a metronome directly behind her, to observe the effect, so now the morning’s work is over. What it describes sounds like dissociation, or exactly what I felt when I was subjected to the polygraph. The scientific literature suggests dissociation during polygraph exams is fairly common. In 1996, the polygrapher Donald J. Krapohl wrote an article for Polygraph, the official organ of the American Polygraph Association, that addresses the phenomenon. The article is typical of polygraphers’ attempts at justifying their profession in the sense that it’s paranoid, authoritarian, proto-fascist, and presents the opinions and experiences of a single polygraph operator as if they’re commandments carved into tablets. Krapohl begins with a blithe, moralistic tirade about “the phenomenon of mendacity” that “pervades every class and culture.” Lying, he claims, is endemic to certain types of people, having “served to defend or expand the interests of uncounted generations of monarchs, merchants, spouses, debtors, knaves, and saints.” Kraphol attempts to codify four classes of countermeasures. The first, Physical Countermeasures, includes any instance in which a polygraph subject “use[s] movements in hopes of masking their reactions or misdirecting the examiner.” Of course, a polygraph subject might move for any number of reasons during an exam, including as a natural reaction to the very discomfort and stress it’s designed to cause; no examiner or machine can determine why. And one wonders why a test so supposedly accurate can be fooled by something as simple as flexing a muscle. The second category, “Mental Countermeasures,” includes imagery, hypnosis, biofeedback training, placebos, and even personality. Notably, dissociation is considered a mental countermeasure. Here, again, is the rub: if the subject dissociates during a polygraph, who’s to say whether it’s intentional? Even Krapohl acknowledges that “the outward appearance of a dissociating subject is quite similar to that of a cooperating test subject.” In other words, nobody can tell if another person is dissociating, much less why. Even the person dissociating may not know; I certainly didn’t at the time. Studies suggest dissociation is often an unconscious response to intense stress of the sort the polygraph is designed to create in its subject. The polygraph works by stressing you out, but if the stress it causes in turn causes the subject to dissociate, they can be failed for trying to cheat. * Kevin gave me another break. I spent it in the waiting area, staring out over the rooftops of El Paso. The window faced east, so I couldn’t see the border, but the border is like the truth: you know when you’re close to it. After a few minutes, Kevin came and led me back into the interrogation room, where he sat me down and said I’d failed. The machine detected deception in my answers to either the drug question or the falsifying information question. I was amazed: he couldn’t even tell which question I’d lied about? And why hadn’t he detected my lie about disparaging my boss? Kevin went on to say that I must have researched tactics to defeat the poly. My swallowing seemed to bother him to the point that he considered it evidence of deception. I said my mouth had been dry, but Kevin ignored me, telling stories about his brother who’d done drugs and other applicants who said they’d gotten high a thousand times. I didn’t know it at the time, but his psychological tactics were all elements of the Reid Technique: repeated, unwavering assertions of guilt; attempts to excuse or minimize the suspected crime; constant interruptions; professed sympathy; and, of course, outright lies. Kevin said he was almost positive that my drug use wasn’t over the threshold. Earlier, he’d told me there was no threshold, and it dawned on me that Kevin wasn’t much of an interrogator without his machine. He kept fishing, accusing me of various lies, interrupting whenever I tried to deny, suggesting things I may have forgotten: didn’t I ever do any pills when I was a bartender? Did I really only do meth once or twice? He began pointing to the computer screen and picking up other deceptions. He detected possible lies in my academic record—I told him I’d gotten a 3.0 GPA at a state school, and why the hell would anybody lie about that?—and questioned whether I really had a master’s degree, even though I’d provided transcripts as part of my background investigation. He accused, I denied, and it became the worst kind of male interaction, a matter of pride. He was lying, I was lying, everything we said was both true and false, depending how you looked at it. I wasn’t even hooked up to the machine anymore, so I didn’t understand why we were still talking. He’d already said that I failed. Couldn’t we call it a day? As Kevin’s one-man theater dragged on, I thought about the long drive ahead of me, and began to understand why people make false confessions. It’s not because you don’t know the truth. It’s because the truth doesn’t matter. When the person across the table has all the power, what’s the point of arguing? It was pretty clear by then, a few hours into a test I’d already failed, that Kevin didn’t give a damn what was true. He wouldn’t be satisfied until I confessed. But fuck Kevin and his machine. I wasn’t confessing. Kevin paused, and I thought we might finally be done until he asked about my mother’s murder: was there anything I hadn’t told him about that? It broke the spell. I dissociated in reverse, came fully into my body. My chest relaxed, my heart quieted, and I saw the situation clearly for the first time. Kevin was just some dickhead with a grift, doing a job based on lies for a government that practically invented them. Did I know anything about my mother’s murder? I’d spent five years writing about it. I was the world’s foremost authority on the subject. But I wasn’t telling Kevin. If he wanted to know, he could buy the fucking book. I asked if I was free to leave. He shrugged. As I stood and went for the door, he asked if I would come back for another test with him if necessary. Sure, I said. I’d love to. It was the last thing I would ever say to Kevin, and I wanted to make sure it was a lie. When I got home, I checked the internet forum and read a post by the guy who’d been sitting with me in the waiting room. He’d failed, too. His story was almost exactly the same as mine, except his test took twice as long. He must have tried harder than I did to tell the truth. * Three weeks later, my book came out, and I once again found myself answering questions. Whenever someone asked about truth, I’d say that I consulted the historical record when possible, but that the book was mostly based on memory. Sometimes, despite myself, that old cliche slipped out: it’s my truth. I’d think about the polygraph, Kevin’s endless questions, his assumption that a fixed, detectable truth existed in my memory. Writing a book based on memory showed me it’s less a font of truth than a river of lies. We may tell ourselves stories in order to live, as Didion famously said, but nobody ever quotes the rest of that passage: “We live entirely, especially if we are writers, by the imposition of a narrative line upon disparate images, by the ‘ideas’ with which we have learned to freeze the shifting phantasmagoria which is our actual experience.” That sounds a lot like lying. Even if we mean to tell the truth, the existing science suggests that memory is almost as unreliable as the polygraph. In 1885, the German psychologist Hermann Ebbinghaus did tests on himself and came up with his famed “forgetting curve,” a chart that showed we forget more than half of information within a few days. More recent studies have found that autobiographical memory—the deliberate recollection of facts, ideas, and experiences from one’s life—is not only inaccurate, but suggestible and frequently false. And factors such as depression and trauma, both psychological and physical, have been found to degrade autobiographical memory. Then there’s the issue of stress, which also seems to have a range of effects on memory. Stress hormones like cortisol and adrenaline have been shown to aid in memory consolidation, which is, more or less, the process of storing recently learned information as memories for long-term recall. But those memories can change each time they’re remembered, through a process called reconsolidation: once accessed, the memory has to be rewritten, and it can be rewritten differently, revised just like a scene in a memoir. While stress may aid in memory consolidation, it has a profoundly negative effect on reconsolidation. Trauma has its own story to tell. We’re all polygraphers, staring at the screens of our truth machines, proving ourselves right. By the time I took the polygraph, my memory had already been rewritten. After five years of accessing and re-accessing memories of my mother and her death, I’d recently begun to understand how dangerous it is to write a book based primarily on memory. I don’t mean the truth: accuracy is overrated, not to mention impossible. The real danger is the sacrifice you have to make. By writing your memories, and rewriting them again and again, draft after draft, you replace them, erase them. By the time I finished my book, after revising every word half a dozen times, I didn’t remember my mother anymore. She was pages, scenes, sentences. The polygraph works a lot like a memoir. It doesn’t find the truth, it creates it. First the exam makes you doubt or forget your memories. Then, by forcing you to re-access them again and again under stress, it literally rewrites them. Since my polygraph exam, I’ve believed that I did drugs between six and eight times before then, even though my rational mind knows that isn’t true. My experience of being polygraphed showed me that not only does the polygraph not detect lies, it manufactures them. More than two million polygraph exams are given every year in America; it’s a two-billion-dollar industry. No other country in the world uses the poly to nearly the extent that we do, and most don’t use it at all. Why are we the only ones who build machines to detect the truth, and believe that they can, despite all evidence to the contrary? Why do we need to believe in a truth so simple it can be detected by a machine? The lie detector is a myth. Everyone who doesn’t make money off of polygraphs agrees on that. But myths exist for a reason, to explain collective phenomena, to explain us to ourselves. Myths create a sense of community through shared belief. Judging by our obsession with lie detectors—and the fact that we continue to call them that, more than a century into this charade—the myth of a simple, detectable truth is one of the few beliefs most Americans do still share. The lie detector came straight out of science fiction, and drifted into the realm of fact at the beginning of a century in which a succession of groundbreaking technologies would shatter and reshape our cultural conceptions of what was possible: Edison’s bulbs, Bell’s telephone, Ford’s mass-produced cars, the Wright Brothers’ airplane. By the time the polygraph came along, a credulous American public was used to tales of revolutionary, terrifying innovations that were going to change their lives forever. Some of those technologies were transforming reality itself. Electricity was spreading across the country, quite literally changing the way people saw the world. So were telephone and radio access, which did the same for how we heard it. The polygraph’s siblings, cinema and psychology, were transforming the way people saw themselves. Meanwhile, Modernism—the artistic movement responding to those changes—was roiling the arts in every medium, revising our expressions of lived and imagined reality; the United States government banned James Joyce’s Ulysses the same year the polygraph debuted. How it must have felt to be a human then. A man my age in 1921 might have remembered reading dime Westerns by candlelight as a boy, riding a horse to school, a time before voices—much less humans—could travel through the air. He’d seen the Progressive era rewrite the rules of society, including the victory of women’s suffrage the year before. He might have fought in the first World War. By the time the polygraph was invented, he may have owned a car or had access to a telephone, if he was wealthy. But he still got his news from newspapers or neighbors. He didn’t own a screen. Can we blame him for believing in a simple kind of truth that a new device could find? A century later, I’m awash in new technologies, and deeply confused about what to believe. I was a young adult when smartphones arrived, in college when Facebook debuted, and dimly remember an early childhood before the internet changed everything. In the last few years, as of this writing, the COVID-19 pandemic has altered American life in ways nobody comprehends. Nobody trusts the media anymore, and appalling numbers of Americans refuse to believe in fundamental, verifiable facts. Meanwhile, conspiracies ricochet around the internet, gathering believers. It’s hard not to see this moment as a parallel of the polygraph’s invention, another time when technology has done a number on whatever shared sense of truth America once had. That erosion has worsened our cultural and political divisions, which often hinge on what kind of truth we believe in: relative and constructed, or absolute and fixed. A century ago, the polygraph was born from the latter belief. But the notion of objective truth seems quaint and naïve in our era of fake news and algorithms, when the only truth that still exists is ours, a custom reality created for us and delivered to our devices. We’re all memoirists now, shouting our stories into the void. We’re all polygraphers, staring at the screens of our truth machines, proving ourselves right. * A few months after the exam, I got the official news: not only had I been found unsuitable for employment, I also had no right to appeal the decision and was barred from reapplying for a minimum of three years. The consequences of my polygraph exam were finally clear, the only real truth revealed in the whole process: I was never going to be a federal agent. I wasn’t alone. I soon found out that Customs and Border Protection job applicants had a polygraph failure rate of 68.1 percent, more than double that of other law enforcement agencies. The CBP commissioner at the time said those statistics showed the polygraph was working and blamed the quality of the applicants. It struck me as a strange rhetorical strategy to suggest that his agency attracted applicants so much worse than those of any other law enforcement body in America, but what did I know: I was one of those applicants. One article described polygraph subjects being accused with no evidence of cheating on their wives and having cartel connections, in exams lasting eight hours or longer. Other law enforcement agencies called CBP’s conduct excessive, and Jeff Flake, then a Republican Senator from Arizona, suggested that operators were being forced to fail applicants to justify their own jobs. A few months after my exam, I got an email officially notifying me of my failure. I replied to the email and asked for a copy of the polygraph report. The CBP representative told me I’d have to file a Freedom of Information Act request. I did. They ignored my request for four months, in violation of FOIA. When I threatened legal action, they denied my request for other reasons, both of which were lies. I filed another FOIA request and was still waiting for a response when the Office of Personnel Management announced that it had suffered a data breach. The personnel files of millions of federal employees and applicants had been stolen by Chinese hackers. An investigation ensued, and the Inspector General accused OPM officials of lying about the hack. Multiple high-ranking officials, including the director of OPM, resigned in the aftermath. Eventually I received a letter from the replacement director alerting me that I’d been affected by the hack. (In the letter, she admitted that her own background check had also been compromised, as if that was supposed to make me feel better.) The most sensitive imaginable document about my life—one that included all of my personal and financial information, as well as a complete history of my jobs, residences, and relationships—had been stolen. The government’s solution was to offer free credit monitoring to those affected. As of this writing, it’s been more than a decade since my polygraph exam, and the government still hasn’t sent me a copy of the report. But at least now I know the truth is out there. It’s on a hard drive somewhere in China. __________________________________ “The Memoirist and the Lie Detector” by Justin St. Germain appears in the latest issue of New England Review. Justin St. Germain Justin St. Germain is the author of the memoir Son of a Gun (Random House, 2013) and the book-length essay Bookmarked: Truman Capote’s In Cold Blood (IG Publishing, 2021). His writing has appeared in many journals and anthologies and has been awarded the Barnes & Noble Discover Award and the Pushcart Prize. He teaches at Oregon State and the Rainier Writing Workshop. Previous Article How the Continual Movement of Wildlife Regulates the Natural World Next Article Lit Hub Daily: July 15, 2024 criminal justiceinterrogationJustin St. Germainmemoirmemoir writingNew England ReviewpolygraphpolygraphspsychologyThe Memoirist and the Lie DetectorUS Government More Story How the Continual Movement of Wildlife Regulates the Natural World Each night, as the line that separates day from night sweeps across the face of the ocean, a vast wave of life rises from... RSS RSS - Posts Literary Hub Created by Grove Atlantic and Electric Literature Masthead About Sign Up For Our Newsletters How to Pitch Lit Hub Advertisers: Contact Us Privacy Policy Support Lit Hub - Become A Member © LitHub Back to top Become a Lit Hub Supporting Member: Because Books Matter For the past decade, Literary Hub has brought you the best of the book world for free—no paywall. But our future relies on you. In return for a donation, you’ll get an ad-free reading experience, exclusive editors’ picks, book giveaways, and our coveted Joan Didion Lit Hub tote bag. Most importantly, you’ll keep independent book coverage alive and thriving on the internet. Become a member for as low as $5/month Dismiss without supporting Lit Hub x",
    "commentLink": "https://news.ycombinator.com/item?id=40972437",
    "commentBody": "The Delusion of the Polygraph (lithub.com)194 points by bookofjoe 19 hours agohidepastfavorite188 comments bloomingeek 3 hours agoMany years ago, at the age of nineteen, I was forced to take a polygraph if I wanted to keep my job. Someone was stealing products from the store, we heard out the back door, and they required everyone to be tested. Naturally, my co-workers and I discussed this among ourselves and we all agreed to test, we knew we were innocent. One of the men said all they're trying to do is see if anyone cracks under the pressure of the test. Being kind of a nervous type of person, I was concerned they might misread my domineer. I talked to my sister and she told me to try my best to control my breathing during the test. For me the problem wasn't that I was guilty, it was they would think I was guilty. We all passed the test and went back to work, later it was revealed the thief was someone on another shift. Or were they just nervous? reply GrantMoyer 2 hours agoparentdomineer → demeanor I only post the correction because it took me a couple of minutes to figure out. Domineer is an uncommon word, so initially I thought I had a gap in my vocabulary, but couldn't find any definitions that made sense in this context. reply nortlov 1 hour agorootparentWish I had seen your post before I resorted to ChatGPT. https://chatgpt.com/share/2400c631-fc68-4f95-b4e7-861324a3dc... reply dullcrisp 40 minutes agorootparentThat’s neat reply dfxm12 2 hours agoparentprevThis might have been illegal, depending on how many years ago this was: https://en.wikipedia.org/wiki/Employee_Polygraph_Protection_... In either case it was complete BS and just shows the sorry state of labor in the past few decades. reply MathMonkeyMan 20 minutes agorootparentConfess! Confess! reply crystal_revenge 12 hours agoprevI awhile back I used to do work with a major DARPA contractor. If you're familiar with security clearance for these roles, you know that at the higher levels of clearance you eventually need to take a polygraph exam. I was never interested in going the clearance route, but got into a conversation with a grizzled industry vet that seemed like a character torn from a hard-boiled detective novel. At the time I had recently learned that polygraph exams were \"fake\" and when the topic of the exam came up I was quick to point this out. His comment surprised me, and, in a sense, demonstrated to me that saying a polygraph is \"fake\" is akin to saying WWE wrestling is \"fake\". Of course it is, but that is a misunderstanding that what you're watching is a real performance. He said the polygraph itself is just a tool for the interviewer. The real value was in someone who knew how to use the machine to convince the subject that they knew the truth. He continue that in his time he knew some mighty good interviewers who could easily extract anything they needed from you. My father did go the clearance route, and when I asked him about the polygraph he told me he confessed things to the interviewers he would never have told my mother. \"Fake\" or \"real\" the polygraph does work in this sense. reply JohnFen 4 hours agoparentI knew a professional polygraph examiner who told me the exact same thing. It means that the polygraph works as well, and in the same way, as the ancient Roman(?) method of having a tent sealed off from light, with a donkey in it. The examinee is told that he is to hold the tail of the donkey and if the donkey brays while he says the thing he's being tested for, then they know for a fact that he's lying. The actual test, though, was that the donkey's tail was covered in soot. If the examinee comes out of the tent with clean hands, they know that he didn't hold the tail and so is deemed to be untruthful. reply dudeinjapan 43 minutes agorootparentWhat if the donkey kicks you? reply bbatha 3 hours agoparentprev> My father did go the clearance route, and when I asked him about the polygraph he told me he confessed things to the interviewers he would never have told my mother. And this is exactly the problem, people make stuff up all of the time under stress. > Of course it is, but that is a misunderstanding that what you're watching is a real performance. This is not the value. The value is that the polygraph is that its an end-run around employment law. You can't use a polygraph on a general employee to fire them nor can you fire them for many of things that they ask in a polygraph interview. However you can revoke their clearance and fire them for not having a clearance. reply quadhome 11 hours agoparentprevExcept there is no evidence it helps even “good” interviewers “extract” anything resembling truth. And there is lots of evidence it does not. This comment is a perfect study of this almost uniquely American insane phenomenon. But then I don’t question Koreans about fan death. reply jvanderbot 18 minutes agorootparentIt's not about extracting facts, it's about establishing justification for the decision made by the interviewer. Sorry you \"failed your poly\". Here's a good example: https://www.salon.com/2013/11/03/lies_i_told_to_become_a_spy... reply darby_nine 4 hours agorootparentprevThe point is not to extract truth, it's to extract behavior. It's the fact you can convince a judge or jury to take the evidence as evidence of truth that's a problem. reply krferriter 2 hours agorootparentIf the interviewee's behavior is not indicative of truth then the test serves no purpose other than allowing the interviewer or whoever commissioned the test (like a prosecutor or employer) to invalidly convince other people that the interviewee was lying reply bangaladore 26 minutes agorootparentThat and to convince the interviewee that the interviewer knows they are lying. reply SAI_Peregrinus 2 hours agorootparentprevThat's exactly the point. reply Clubber 3 hours agorootparentprevPolygraphs are usually inadmissible in court. It's unfortunate that \"usually\" applies. https://www.hg.org/legal-articles/is-a-polygraph-test-admiss... reply vasco 10 hours agorootparentprevWhat do you make of the placebo effect? The polygraph obviously has no basis for working, but while a sugar pill doesn't make a tumor disappear, it can be very good at pain management. I still wouldn't use it in the context of the justice system, though. reply ADeerAppeared 9 hours agorootparent> What do you make of the placebo effect? The placebo effect is measurable. If there is no measurable improvement, there's no placebo effect either. Bear in mind that what most claims in favour of the polygraph measure is not truth but potentially-false confession. Extracting false confessions is relatively easy, it's also completely f-ing useless to wider society and massively harmful to the victim. reply meowface 2 hours agorootparentI see no issues with using polygraphs for hiring at intelligence agencies (I defer back to the comment about people missing the point of it), but as an investigative tool it's definitely a net negative. reply knallfrosch 9 hours agorootparentprevThe problem is that the polygraph doesn't work on both levels. Obviously, it doesn't detect lies. But more to the point, it also doesn't extract useful information from most liars, and leads to fake confessions. To stay in your metaphor: - Not only do sugar pills not cure tumors, but imagine - 60% of recipients don't report decreased pain levels (no placebo effect) - 20% of recipients feel more pain reply Sesse__ 8 hours agorootparentprevA sugar pill does not make tumors disappear. That's not what the placebo effect does; it changes your perception of pain and well-being, but not much else. (Of course, that can have a value in itself, but it's nothing like the magical healing effects found in urban legends.) reply em-bee 8 hours agorootparentthe healing effect comes from the fact that your perception of pain and well-being actually contribute to the healing process. reply SkyPuncher 4 hours agorootparentprevI think OP is trying to say exactly what you’re arguing reply vasco 7 hours agorootparentprevYou must've misread what I wrote, since we both said the same thing. reply voxic11 5 hours agorootparentprevThe placebo effect itself isn't real (at least in the vast majority of cases where it has been claimed to exist), when people measure a \"placebo effect\" what they are actually measuring is simply a regression toward the mean, not a causal effect. https://slatestarcodex.com/2018/01/31/powerless-placebos/ https://www.dcscience.net/2015/12/11/placebo-effects-are-wea... https://pubmed.ncbi.nlm.nih.gov/6369471/ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6707261/ reply krferriter 2 hours agorootparentI don't think this is right. Placebo effect definitely exists for conditions that are largely influenced by mental perception. The common example is pain. You can reduce people's perception of pain by deploying the placebo effect, e.g. giving them sugar pills that you convince them will reduce their pain. It extends to other similar conditions which are not generally (or possible to be) measured directly, but rather based on a patient's self-reported scoring. Like \"on on a scale of 1-10 how would you rate your experience of this condition\". Placebo effect can work for that. But not for other more tangible conditions. reply voxic11 1 hour agorootparentDid you read any of the articles I linked? > The most important study on the placebo effect is Hróbjartsson and Gøtzsche’s Is The Placebo Powerless?, updated three years later by a systematic review and seven years later with a Cochrane review. All three looked at studies comparing a real drug, a placebo drug, and no drug (by the third, over 200 such studies) – and, in general, found little benefit of the placebo drug over no drug at all. There were some possible minor placebo effects in a few isolated conditions – mostly pain – but overall H&G concluded that the placebo effect was clinically insignificant. Despite a few half-hearted tries, no one has been able to produce much evidence they’re wrong. This is kind of surprising, since everyone has been obsessing over placebos and saying they’re super-important for the past fifty years. reply joquarky 38 minutes agorootparentUsing science to attempt to measure qualia sounds like a good way to produce whatever results you want. reply lazide 4 hours agorootparentprevIn a small but measurable percent of cases, the sugar pill does actually make tumors disappear though. [https://pubmed.ncbi.nlm.nih.gov/12509397/], and helps with almost every other factor of care in much larger percentages of the time. reply jayrot 2 hours agorootparentLiterally from your link : > Conclusion: In randomized double-blinded, placebo-controlled trials, presumably with minimum sources of bias, placebos are sometimes associated with improved control of symptoms such as pain and appetite but rarely with positive tumor response. Substantial improvements in symptoms and quality of life are unlikely to be due to placebo effects. reply codr7 9 hours agorootparentprevSo you mind can fix pain, but not tumors? Where do you draw the line? reply 1992spacemovie 10 hours agorootparentprevMy dude you are way over-thinking the polygraph. This is the more-usual setting it is applied in: https://www.youtube.com/watch?v=DgrO_rAaiq0 reply User23 4 hours agorootparentprev> no evidence Saying \"there is no evidence\" is sloppy cable political TV tier rhetoric. There is absolutely evidence[1]. You and others may not find that evidence convincing, or otherwise think polygraphs shouldn't be used, but nevertheless it exists. A brief survey of the evidence suggests that the polygraph is probably slightly better than chance, but with high enough error bars that we should be very cautious about its use. [1] https://nap.nationalacademies.org/read/10420/chapter/7 reply crimsoneer 10 hours agoparentprevAs someone who used to be a cop, this is absolute peak cops justifying the evidence not supporting their intuition by just making bullshit up. If it actually worked, they'd have data and results. But, spoilers, they don't. reply BeFlatXIII 4 hours agorootparentWhat's your opinion of the copy machine polygraph scene from the opening of The Wire's final season? reply krferriter 2 hours agorootparentThat was a pretty accurate presentation of how polygraph machine testing works. The machine isn't doing anything useful in terms of determining if what is being said is true or not. reply jajko 8 hours agorootparentprevCan't ruin that little universe in their heads where they are heroes of their own stories. I'd say thats a basic human trait for certain personalities if you invest a lot of yourself into given topic, try talking with cold rationality to religious fanatics (literally any religion) and you hit very similar if not the same wall. reply explaininjs 4 hours agorootparentIndeed, even evolutionism! (that belief we’d call “scientology” if the name wasn’t already taken) reply gjm11 2 hours agorootparentI think the term you're looking for is \"scientism\". In so far as \"evolutionism\" means anything, it means \"believing in something like Darwinian evolution\", which (1) is more specific than what I think you're going for and (2) is not the kind of meta-level position you're talking about. (I also think that most complaints of this science-is-just-another-religion kind are rubbish, but that's a more controversial matter and I will not further derail this thread onto it.) reply explaininjs 49 minutes agorootparentNo, “evolutionism” is actually the most correct characterization. The quip about scientology was meant to caricaturise the belief more than anything. Your claims 1 and 2 are both incorrect. Creationists and Evolutionists can both agree that scientific process creates good results and is a generally well-reasoned approach to learning about the current state of the universe and making predictions for the future. Where they disagree is the extent to which this is true, especially as it relates to the ability of those predictions to track reality going forward and backward in time, but that’s more covered in a sibling comment. reply danparsonson 2 hours agorootparentprevWhat's \"evolutionism\"? reply explaininjs 53 minutes agorootparentThe worldview characterized by an axiomatic belief in the time invariance of physical laws. It is contrasted by creationism, which posits non-differentiable physical behaviors. There’s a middle ground of accepting the possibility of differentiable but non-constant mechanisms that should technically be owned by the evolutionists, but this is a rarely embraced territory: you get neither the warm fuzzies of belief in a God who specially designed you, nor the cool satisfaction of a belief in making observations about the universe now and scaling/time-inverting them to speak confidently about the distant past and future. reply callalex 4 minutes agorootparentprevA strawman made up by religious fundamentalists so that they can feel superior to others. Slyfox33 1 hour agorootparentprevSorry buddy, evolution is a fact. reply explaininjs 42 minutes agorootparentParts are, parts aren’t. If you aren’t able to specify exactly what you believe with regard to it, there’s not much more to say. reply Zigurd 4 hours agoparentprevThis is a bit like saying good witch doctors can diagnose you without the voodoo accoutrements, which are just there to get you to open up about how you are feeling. It's still voodoo, and still produces garbage conclusions much of the time. Thing is, there isn't an academic journal for voodoo. There is at least one for polygraphy. reply efitz 1 hour agorootparentWait, what, Voodoo doesn't work? Has this been studied? reply tommiegannert 11 hours agoparentprevI have watched quite a few (American) police interview videos lately, and regardless of tools (polygraph or Reid(tm),) I wonder how many interviews start with the perpetrator really having no rationale, and ending in them simply back-rationalizing their emotions. A fit of rage might not have a rationale, if you're that predisposed. But being pushed to explain yourself will make the brain do what it's constantly doing: retroactively explaining your emotions. Especially if you've been promised a reduction in stress if you do. And then there's the opposite, when the subject continuously makes no sense, because they have brain damage: https://www.youtube.com/watch?v=_c_lmx4LdNw reply flir 4 hours agorootparent> I wonder how many interviews start with the perpetrator really having no rationale, and ending in them simply back-rationalizing their emotions That's the core of the Reid technique, isn't it? Here's two rationales, one socially acceptable and one socially unacceptable. Pick one. We don't actually care which one you pick, because a confession is still a confession, but we're handing you a convenient narrative you can use to justify your actions (a carrot), and an alternative people might believe about you if you don't pick Box A (a stick). Something similar happens in high-pressure sales. I bet those guys would make great interrogators. reply jayrot 2 hours agorootparentWasn't familar with the \"Reid Technique\" so I had to look it up. One of the opening wikipedia paragraphs is just perfect. >In 1955 in Lincoln, Nebraska, John E. Reid helped gain a confession from a suspect, Darrel Parker, for Parker's wife's murder. This case established Reid's reputation and popularized his technique.[3] Parker recanted his confession the next day, but it was admitted to evidence at his trial. He was convicted by a jury and sentenced to life in prison. He was later determined to be innocent, after another man confessed and was found to have been the perpetrator. Parker sued the state for wrongful conviction; it paid him $500,000 in compensation.[4] reply bagels 41 minutes agorootparentIt's the same story with: bite mark analysis, police dogs, hair comparison analysis, firearm toolmark analysis, many arson analysis techniques, bloodstain patterns. Discredited or unproven, yet still used in court. reply jvanderbot 15 minutes agorootparentHaving served on a jury I implore you: Stay as far away from the criminal justice system as you can. Once you're in that courtroom your life is a coin toss away from effectively ending. footnote: and regardless of innocence, you will be running from the arrest the rest of your life. reply marcosdumay 1 hour agorootparentprev> I bet those guys would make great interrogators. Maybe on the context of US criminal interrogators, where discovering what actually happened isn't one of the goals. reply pessimizer 1 hour agorootparentprev> We don't actually care which one you pick, because a confession is still a confession Yes, but as you note, the point is to make it sound like barely a confession, the minimum possible confession. This makes it just as attractive to the innocent as for the guilty. It's offering a minimally painful way out of a deeply stressful situation. And as you say, the punchline is that not confessing would lead to minimal or no pain, and every level of confession will equally turn out much worse. An innocent person is just trying to escape from that room, and is being conned into agreeing to a long prison sentence in order to do it. reply bityard 2 hours agorootparentprevI have seen these too and although I'm certain there are bad interviewers out there, I have to say I gained a lot of respect for the detectives who conduct those interviews. They are much better than I could ever be at remaining dispassionate, curious, and above all, extremely patient. The bottom line is that these interviews are all recorded and the police are well aware that if they make a misstep and if the defense has a competent lawyer, they may inadvertently set a thief, killer, or rapist free if they are not extremely careful in their questioning and processes. reply cruffle_duffle 3 hours agorootparentprevI’ve watched police interrogations too and they are both fascinating and horrifying. Those interrogators have a lot in common with shady used car salesmen. They twist and contort the truth to get whoever they want out of the person being interviewed. Except unlike the car salesmen the good interviewers really know their shit and can “corner” a person in their own lies. (I suppose a good salesman is just the same though) It’s a weird place to be when you are rooting for the child molester/arsonist/muderer hoping they’d come to their sense and fucking CALL A LAWER YOU FUCKING IDIOT and SHUT YOUR PIE HOLE!!! But oddly, I guess maybe it’s a good thing “rape the kids and wife, shoot them point blank and burn the house down” criminals are too stupid to exercise such a basic right. Even the scummiest of police investigators give these people the option to shut the fuck up and call the lawyer. Sure the person might have to wait an obscenely long time before the lawyer shows up but they still are given the out yet these moron criminals think they can outsmart a highly trained police interrogator and choose to dig their holes. Usually these people already dug their hole well before they are drug into the police station though. The on the ground evidence usually pretty much points to them already. All the confession does is save the state millions of dollars taxpayer money with courtroom proceedings. So yeah… really mixed on the whole topic. Of course these videos on YouTube are selected to be the most interesting of the bunch. There are probably a hundred more mundane interrogations that go unseen for every one that makes it to a widely subscribed YouTube channel. reply bityard 2 hours agorootparent> They twist and contort the truth to get whoever they want out of the person being interviewed. It's possible I just haven't watched enough of them, but I've never seen that. Usually the interviewer is a calm and dispassionate Columbo type who asks clarifying questions and then lets the suspects slowly trap themselves in their own web of lies. It is fascinating to watch. That said, even though I have a healthy respect for the criminal justice system, I will still NEVER talk to the police without a lawyer. (Whether or not I've committed a crime.) reply p_j_w 1 hour agorootparent>Usually the interviewer is a calm and dispassionate Columbo type who asks clarifying questions and then lets the suspects slowly trap themselves in their own web of lies. This loses its ability to inspire awe when you watch a video of one of these where you know the subject is actually innocent and the interviewer manages to also catch them in their own web of lies and make them look and sound guilty. reply jvanderbot 20 minutes agoparentprevYep - this is like field sobriety testing, in my mind. Everyone will display some level of nervousness and inability to perform all the tests, and the officer thus has a baseline level of \"cause\". They can therefore do all kinds of tests or hold you until you do those tests. B/c \"He failed his FST\" Same with poly. If they don't like something, they can just say \"He failed his poly\" reply jrm4 4 hours agoparentprevGreat point, it reminds me of when I read about how \"trials by ordeal\" sometimes worked. Consider the boiling water/oil thing: If you're innocent, you can \"stick your hand in and not get burned.\" What they did was, they faked the water being hot by blowing bubbles in it. All then that was needed was for everyone to \"believe it worked,\" The innocent sticks in, and then the guilty confesses. reply digging 4 hours agorootparentA perfect description of how \"trials by ordeal\" don't work. This phrase is doing some HEAVY lifting: > All then that was needed was for everyone to \"believe it worked,\" reply krferriter 2 hours agorootparentIn an era where mass media basically didn't exist, most people couldn't read, and information about how these things work could not easily spread, it might have been easier to convince people that a fake test was really what you were claiming it was. If people could google it they would instantly find out it was fake. reply jrm4 53 minutes agorootparentprevOh, correct. I think by \"work\" I merely meant \"here is the mechanism,\" not \"this is why they are successful.\" reply powersnail 11 hours agoparentprevI can see how pressure would be applied when seeing the machine is leaning towards \"lying\", possibly breaking the subject's effort to lie. But what would be the interviewer's strategy, if the subjects insist that they are telling the truth regardless of how the interviewer manipulates the machine? Wouldn't it immediately start discrediting the whole process if the subject is in fact telling the truth? I'm telling the truth here, and yet your machine says I'm not, hence it's broken, and hence I'll happily lie in the subsequent questions when it actually matters. reply vasco 10 hours agorootparentMost innocent people doubt their innocence when strongly accused even when they know they are right. Just a tiny bit, but in the right setting and with enough wearing you down, you can make innocent people believe they did it. I've seen it happen right in front of me. reply powersnail 2 hours agorootparentBut what's the point of making innocent confess to false crimes in this setting? (i.e. requiring polygraph for job application) I would imagine the entire point of doing a test would be to find out who is innocent and who is lying about being innocent. If you pressure the innocent into false confession, wouldn't it just make everything even more difficult? reply willis936 7 hours agorootparentprevThat's why you don't talk to cops. Have a lawyer present and use the courts that us taxpayers pay for. reply lazide 3 hours agorootparentAlso why narcissistic and psychopathic manipulators are so dangerous. They don’t have to be cops. Most aren’t. reply Aerroon 3 hours agorootparentprevI wonder if this is related to people adding ambiguity to what they're saying. Eg instead of saying \"it's 20 degrees outside\" they will say \"last I checked it was about 20 degrees\". They change their phrasing because they want others to not think that they are wrong. By doing this they undermine their own credibility though. reply 542354234235 5 hours agorootparentprevThe polygraph would be used as the “bad cop” in the good cop, bad cop routine. After a line of questioning where the interrogator/polygrapher suspects lying, or is fishing for more information, they might say something like “everything sounded good but the machine is showing some deception. Is there anything you can think of that might be causing these readings? Anything you didn’t tell me? I want to get you out of here, but we need to resolve these results.” If the machine does show spikes during certain lines of questioning but not others, for instance about someone’s timeline on the day of a murder vs their relationship to the victim, it can be a reason to pursue further questioning in that area. Given all the ways polygraphs can be misused or abused, the only real use I see is as in interrogation tool. But given the issues with false confessions in general, I think the interrogation should hold less weight, but that is a whole other issue. reply callalex 11 hours agoparentprevThat just means the interview process is selecting for only completely uninformed idiots. What does that say about the resulting organization? reply boffinAudio 10 hours agorootparentThat its a cult. reply hunter-gatherer 4 hours agorootparentCommenter subjectsigma understands this. In a former life I had jobs that required a polygraph, and I was not in a cult, nor does the interview process select for uniformed idiots. Both of you are reacting to commentary that the author barely understands and that neither of you clearly understand. Nobody in my circles seemed to think the polygraph was anything but a tool. In fact, the sibject is sometimes gossiped about in thise circles about the \"relevancy\" of the polygragh today anyways. The thing about buearacracy though is that change happens incredibly slow. If everyone decided to get rid of the polygraph alltogether today, it would still take some years to actually happen. reply subjectsigma 7 hours agorootparentprevYou don’t understand what you’re talking about. It’s an open secret in CDC circles that the polygraph is not effective at catching trained liars, more of a ritual than anything. The polygraph is not a test for how gullible or misinformed you are. It is mostly a test for two things: 1) are you willing to play by the rules and follow orders, even if sometimes they don’t make sense? 2) if you are being lightly interrogated, do you immediately freak out and tell the interrogator everything? Do you have a really bad reaction to pressure? If you don’t match these criteria then you probably aren’t fit to know extremely sensitive government secrets. But like I said, it’s more of a ritual than anything, the value for even those two tests is unproven. Even smart and informed people who know exactly what a poly does can say and do things they wouldn’t normally when they’re strapped to a chair, hooked up to machines, and being yelled at for hours reply 542354234235 4 hours agorootparentI think that is true, but not the whole truth (staying on theme). It is an interrogation, but it isn’t meant, or likely, to catch a trained, hardened spy or someone that can stand up to interrogations. It is to attempt to find if there is information that would make someone a bad candidate for a clearance, the same as the general background check is doing. If you are in massive debt, you are at much higher risk of being bribed. If you are cheating on your wife, and attempt to hide it during your polygraph, you are at much higher risk of being blackmailed. It isn’t going to “catch” everyone but it is another way to reveal people with vulnerabilities that could be exploited. I think the real issue is people that “fail” the polygraph, since it isn’t actually a lie detector in any sense. It would be better if they just considered it a polygraph assisted interrogation. reply snakeyjake 3 hours agorootparentprev>strapped to a chair, hooked up to machines, and being yelled at for hours I've done periodic polygraphs, both lifestyle and full-scope, every 5 or so years since 1997. None of mine have ever lasted longer than 30 minutes. You just sit in a chair while wearing some straps and there's never been any yelling involved. It's all quite prosaic and relaxing actually. It has been my experience that the clearance investigation process is quite simple, although I lead a very boring and law-abiding life. I know of some people who have had quite long polygraphs and failed them repeatedly but my hunch is that the examiner has the findings of the background investigation in-hand and is trying to clarify some findings. Many people with past financial, drug, or legal problems have gotten through the process with no issues just by being open and honest with the investigators and polygraphers. So yeah, if the background investigator interviewed a friend of a friend of a friend and was told that 20 years ago you used to get stoned in college and whip your dick out but when asked during the polygraph about past drug use you go \"I've been a squeaky clean boy my whole life\" you're gonna have issues. The annual financial disclosure is much more stressful just because of all of the damned paperwork. reply lordnacho 47 minutes agoparentprevDon't scientology go around with a similarly fake device? reply UncleSlacky 14 minutes agorootparentYes, the \"E-meter\" is a primitive form of lie-detector equipment: https://rationalwiki.org/wiki/Dianetics#E-Meter \"Basically it is a simple ohmmeter that measures galvanic skin response (electronic resistance of the skin), somewhat similar to a polygraph; the user (the \"preclear\") provides one of the elements in a Wheatstone bridge.\" reply RajT88 2 hours agoparentprevI know some govvies who have been through the polygraph circus. One guy knew it was BS and could not get worried enough for them to come up with a baseline - he was too calm. So they took the tack of rescheduling it a bunch of times, making him go home after showing up so he would be good and pissed off when they actually did the test. He passed. Another lady I know had the interviewer go so hard on her she was crying through half of it. Afterward, the interviewer told her the goal was to make every interviewee break down so they would reveal stuff. reply keiferski 11 hours agoparentprevYeah, polygraphs remind me of those TV shows where the investigator pretends to use magic, voodoo, astral signs, etc. to solve the crime, but is really just using them to manipulate the psychology of the subject and see their reaction. Put simply, the polygraph is a powerful tool if the subject believes it is a powerful tool. reply Terr_ 10 hours agorootparentThat's incomplete. As the article points out: > Even the United States government isn’t dumb enough to believe the polygraph works. The machine’s real purpose is symbolic, as an icon of the power of the state. Law enforcement agencies don’t use the machine to detect lies. They use it to coerce confessions. [...] It’s a fact, part of a story power tells itself to justify its power. Maybe you can beat the machine— they don’t detect lies, so it’s not that hard—but you can’t beat an entire country that believes in it. Even if you know its nonsense, there's still something coercive of any system where it can be used as a pretext to punish you, or where you are punished for not pretending to believe in it. reply Joker_vD 8 hours agorootparent> They use it to coerce confessions. It boggles my mind than confession even counts as evidence but then again, so does any other testimony. Sure, it made sense when we had almost no forensics (and that's the times that shaped our legal systems) but today we do, don't we? reply 542354234235 4 hours agorootparent>Sure, it made sense when we had almost no forensics (and that's the times that shaped our legal systems) but today we do, don't we? The CSI effect. The amount of forensics that people think will be presented in an average case is so much more than actually are. Finding and collecting usable fingerprints, DNA, shoe imprints, etc. does not happen in every case. Most cases are a lot of circumstantial evidence all pointing to the same person. reply SAI_Peregrinus 2 hours agorootparentFingerprints, DNA, shoe imprints, & other forensic evidence are circumstantial. Evidence is legally either circumstantial or testimonial, there's no other category. Most cases are a lot of testimonial evidence all pointing to the same person! reply lazide 3 hours agorootparentprevEven if CSI was real (which it isn’t even close), the vast majority of actions anyone takes leave no discernible evidence that isn’t immediately made useless through entropy. And the most important element in almost every crime (intent) almost never leaves any evidence at all. IMO the biggest subtle lie that CSI convinces people of is not that facts can be determined so easily and unambiguously - though that is a lie - but that the evidence found and any conclusions from it will fundamentally matter. Each piece of evidence is always some turn of the plot. In real life, it usually doesn’t. Too much ambiguity, or inconclusive or inconsistent results. Or false positive/negatives. Or data which is useless in the vacuum of other missing information. In real life, it’s a frustrated and depressing slog - punctuated by occasional moments of elation and/or terror - being a detective. So what could be more compelling than someone telling everyone in their own words their intent and their actions, so everyone can stop guessing and ‘know for sure’? That’s what a confession is. Which conveniently at the end of nearly every crime show the suspect actually does. In real life, some do that - but many lawyer up, and you spend years dealing with every kind of bullshit and confusion game a professional can throw at you, instead of closure and a clear answer. The polygraph is an attempt at bluffing folks into ‘we got you’ moments. Which does sometimes work! But the pressure and techniques applied can also result in people falsely confessing to things that never happened, or getting confused themselves and ‘failing/lying’ when they were actually relaying the truth. reply Ekaros 7 hours agorootparentprevLiving in civilised country I find whole confession, anything you say, can't lie in your own defence thing so absolutely abhorrent. To me it seems absolutely sensible that you should be able to decide what is your statement as answer to any question by state. And if you are on stand in trial as defendant you should be able to lie however much you want. The prosecution must prove you were lying, but the act itself cannot be illegal. reply bitwize 6 hours agoparentprevI'm reminded of the story about the cops who constructed a \"polygraph\" by attaching wires to a colander that ran to a photocopier in which a piece of paper that had \"You're Lying\" written on it was placed. It was enough to intimidate the suspect into singing like a bird. reply huppeldepup 9 hours agoparentprevI’d compare it to the sobriety test of being asked to walk in a straight line. someone who’s drunk will put a lot of effort in it, which is the giveaway. reply krferriter 2 hours agorootparentSobriety tests are notorious for being very subjective and not having well defined criteria and cops failing people even if they aren't drunk. The subjectivity in the test is a feature that allows cops to justify their arrests or uses of force. Obviously once you hit a certain point of drunkness obviously maybe a test like walking in a line can demonstrate something useful. But so would a breathalyzer. The false positives is the problem because they're being used to illegitimately subject innocent people to criminal charges. reply kcb 6 hours agorootparentprevOr someone who's nervous because the police are accusing them of a serious crime... reply ryandrake 3 hours agorootparentOr... it doesn't really matter. By the time the police are commanding you to do a field sobriety test, they have already decided to charge you with DUI. There is literally nothing you can do during the test to change their mind. It's a formality, and the \"test\" is vague enough that the officer can cite any little twitch or misstep as \"evidence\" that you failed the test. If you pass a breathalyzer test (blow under the limit), and they still want to charge you with DUI, they will likely do a field sobriety test because the results are non-numeric and are subjective. Heck, you can blow 0.0 and still get arrested for DUI[1][2], and if you tell the world about it, the police will sue you for defamation[3]. 1: https://reason.com/2024/02/14/iowa-cops-arrested-a-sober-col... 2: https://www.youtube.com/watch?v=QGWSbAHaHUw 3: https://iowacapitaldispatch.com/2023/10/02/after-traffic-sto... reply bongodongobob 1 hour agorootparentComplete nonsense. They need evidence. If you blow 0s, you're not getting a DUI. Just because you have an example or two doesn't mean it's common. I've been pulled over, blew 0s, passed the tests and let go. reply kkielhofner 29 minutes agorootparentWith the wide availability of countless drugs that impair driving (that may not even be detectable on a urine/blood panel) and obviously don’t register on a breathalyzer you absolutely can be charged with and convicted of DUI based on behavior, FST performance, officer observations, driving pattern, etc alone. Stumbling over a word or two like I do on conference calls everyday could be considered “evidence”. Just like you can also be charged and convicted of DUI even with zeros or being under the legal limit. If you’re traveling to/from/around a bar area at 2:30 AM your driving pattern and behavior is going to be heavily scrutinized. Just because you have an anecdotal example or two doesn’t mean it’s common either. The FSTs are also completely stacked against you. Take a high-pressure scenario, less than ideal conditions (side of the road with passing cars, dark, cold/hot, precipitation, flashing lights, etc), and ridiculous/conflicting/confusing instructions and even people who are completely sober end up providing what could appear as damning evidence. Even professional athletes have bad days where they just can’t land a shot they’ve nailed thousands of times. I know at least a few cops who openly admit they struggle with the tests (to the point of “failure” in some cases) in no-pressure ideal classroom training environments. Of particular curiosity is a lot of police body/dash cam footage where the officer struggles to demonstrate/explain the tests, stumbles over reading Miranda cards, etc. Evidence where if the same observations were applied to them they could be scrutinized as being “impaired”. Of course I’m not advocating for impaired driving, just highlighting that it’s a tricky situation overall. reply bongodongobob 4 hours agorootparentprevYeah... Cause it's hard to do when you're drunk. reply renewiltord 10 hours agoparentprevEverybody always thinks this about stuff that doesn't work. It turns out it's total bullshit. There was someone I knew who claimed that dowsing rods worked a similar way. A good practitioner of the dowsing rod was really using the information from his audience and was Hans the Mathematical Horsing his way to water. The audience of course had this information from the subtle way they picked up on cues of water, ways even they didn't recognize until the dowsing rod was in action. They all think this. Some can even bend spoons. reply drew870mitchell 3 hours agorootparentA friend bought land to build a house on where dowsing was culturally pervasive. He knew it was bogus but it was cheap compared to the land price and everybody around was heavily pushing it. An old guy came out and, during the performance, told him the total history of the land parcel, including stuff that would be inappropriate in formal disclosures (\"those neighbors are assholes\" etc). They did hit water, but the whole area is pretty verdant. reply lupusreal 5 hours agorootparentprevDowsing rods are an excuse to trust common sense; that dip in the land where plants are greener is probably a good place for a well. But digging wells is expensive so people want something more than a guess, but also something cheap. When I was a kid, a new house was being built downhill from my parents and they dowsed out a \"good\" place for a well. They were all set to drill before my dad went out and told them they were directly downhill from our septic tank. reply lucianbr 4 hours agorootparentWhat's \"downhill from a septic tank\"? Isn't a septic tank a container that slowly fills up and is periodically emptied by a truck or something? There is no \"flow downhill\", is there? I thought the whole reason to have a septic tank is that you have no place for the stuff to go. reply mauvehaus 3 hours agorootparentUnless you're in a location where you absolutely can't leach the liquid waste out into a leach field (like right on a lake), the tank usually just settles the solids and give the liquid some time to mingle with whatever biological processes are happening in the tank. When the ground doesn't perk naturally, it's common up here to build a mound system where you have a mound of soil that does perk and vegetation (grass) on it to take up the liquid. A fully closed tank is basically the last resort. We have friends with a house right on a lake, and their tank had to be pumped every three weeks before they had a kid. I can't imagine what the interval is now. reply jerf 4 hours agorootparentprevSeptic systems are more interesting than that: https://en.wikipedia.org/wiki/Septic_drain_field A straight tank would fill up in no time. reply georgeecollins 3 hours agorootparentprevThe best brain power a human possess is subconscious-- its all the perception that allows you to run fast around moving people and catch a ball. You can do it but you can't explain how. You can probably train yourself to be good at finding water or reading people but you might be bad at explaining why. Worse-- trying to explain yourself might be forcing you to use the feeble symbol processing power of your brain. So a prop helps you feel your way. Still-- total bs. reply boffinAudio 10 hours agorootparentprevSay what you want about lie detectors, but I have literally used dowsing to find water in a vast and hostile desert to keep myself alive.. Dowsing works. You'll never know that until you're dying of thirst, however. reply mootothemax 7 hours agorootparentIt's basically a method to help you find a downward slope, right? In Britain, until surprisingly recent times, water engineers had been known to use dowsing rods to help find water pipes. As far as I can tell, it's a... method... that helped them to stop overthinking the problem, and subconsciously look at where the the pipe's likely to be, given the many small clues indicating previous engineering work. reply Stevvo 6 hours agorootparentprevAnywhere you dig anywhere, eventually you hit water. So, yea, dowsing always works, But it's no better than random chance. reply Novosell 9 hours agorootparentprevThe studies section on the Wikipedia page for dowsing would disagree with you. reply tbihl 3 hours agorootparentThe studies section of Wikipedia is much less compelling than you let on (only two are described, with limited accessibility to the others), but the important question is: do you prize mostly the mechanism or the outcome? The poster talking about finding water in a desert presumably values the water. The studies are more interested in two related questions: the mechanism of the technique and whether its practitioners understand it. To the latter, the answer is an emphatic 'no'. For the former, the studies use various experimental schema to evaluate hypothesized mechanisms, each time yielding a refutation of those mechanisms. Going from a refutation of specific mechanisms to a rejection of the practice is extrapolation, not sound science. Part of living in an intelligible universe is accepting that there are phenomena that await discovery, description, and generalization. Whether that is true of this 'dowsing' technique is a separate question. When a flat-Earther fails to address time zones, distance to the horizon, stars, great circle routes over the poles, etc., this dismissiveness is justified, but that does not appear to be the case from what you have cited, and the practice of science is worse off for you insinuating otherwise. Edit: All of this is apart from the utility of someone doggedly pursuing a course of action which he is convinced will bear fruit as contrasted to someone timidly sampling among options without ever making equivalent commitment. reply boffinAudio 8 hours agorootparentprevWikipedia is not a source of truth - only collective associative reality. reply Novosell 8 hours agorootparentWikipedia isn't, but the studies listed on it can be. Unless science is all just junk? reply boffinAudio 7 hours agorootparentNobody is saying 'all science is junk'. I'm certainly not saying that. That's just a dismissive way of dealing with a very intriguing subject. I would make the statement that the science hasn't been \"good enough\" yet, since it doesn't explain the efficacy of the technique that I have personally observed over years of trips out into the desert. And I stand by the point that on this subject, it is fair to say that Wikipedia is junk, since studies demonstrating the effectiveness of dowsing (whatever the reason behind why it works when it does) don't make it into the Wikipedia article due to the biases of the editors involved in curating the article on the subject. The editors of Wikipedia seem to be effected by some subconscious desire for dowsing to not work, so therefore the articles only highlight studies that \"demonstrate dowsing doesn't work\", rejecting as pseudo-science or \"faulty science\" anything which counters that subconscious prejudice. People who don't want dowsing to work, will find that it doesn't work - those who do, will find water. Here, for example, is a study which demonstrates that there is something to dowsing, after all .. https://apps.dtic.mil/sti/tr/pdf/ADA284789.pdf Maybe this study is just junk science, in your opinion, since it runs counter to the collective-derived assertion that dowsing is \"just junk pseudo-science\"? In which case, be sure to understand that whatever criticism you have of the study, my experience with finding water through dowsing, actively over decades of experience, leads me to be less inclined to care about scientific studies, when a) there is thirst to quench, and b) I can effectively find water almost every single time I need to, with dowsing. You cannot survive in the desert just because you read a \"scientifically-sound white paper\" somewhere. Sometimes, you have to leap beyond prejudice and just eat the damned spider. An interesting conclusion: https://www-sop.inria.fr/agos/sis/dowsing/dowsdean.html \"Water divining survives today because its practical utility does not place too great a strain on pragmatism. Dowsing results will ultimately be validated by their accuracy and practical value rather than theories and opinions.\" Whether it works because of subconscious awareness on the part of the dowser, or due to some unknown forces of nature, either way: studies have concluded that you can indeed find water with dowsing techniques. Perhaps there is more to human perception than our conscious/sub-conscious scientific divide allows us to observe: \"It won't work if you don't want it to.\" reply digging 3 hours agorootparentYour own study does not inspire confidence: > Several things that Mr. Carl does while dowsing don't fit into many of the definitions that have been proposed. Everyone who believes in it agrees it works, but not how it works? That's a classic sign of witchcraft. > There are many aspects of dowsing that were not addressed by this experiment and these results do not apply to them. Additionally, many people claim to have a dowsing sense but are not reputable. Beware of anyone who wields this work as proof of their skill. As in any scientific experiment, repeatability, under the scrutiny of the scientific community, is the determinant of valid results. Unless this validation occurs, these findings will remain experimental data. In other words, do not take this paper as proof of dousing and do not trust someone who links to this PDF to prove dousing is real. ... Here's the thing, nobody is here saying you didn't go out into the desert and find water. (Although, you're a random comment on the internet, so I'm not particularly inclined to believe a word of it either.) It's just that \"dousing\" isn't a likely explanation for how( you found water. At the very best, \"dousing\" as a name can be equated to \"dark matter\" - something is happening, but we know virtually nothing about what is happening or how. Explanations that rely on the type of wood used, or claim to be able to find oil* and lead in addition to water, just don't make sense. With dark matter, yes we're exploring as many explanations as we can, but most scientists agree that the more complicated solutions (such as \"gravity works differently at very large scales\") are less likely. And they still have a mechanism. reply ginko 9 hours agorootparentprevAll the people in your situation that didn't end up finding water by dowsing wouldn't be able to post on HN.. reply sgift 8 hours agorootparentLiteral example of survivorship bias in the GP comment. reply boffinAudio 8 hours agorootparentIts easy to \"prove\" dowsing doesn't work - all you have to do is not find water. reply adammarples 4 hours agorootparentDone reply Y_Y 9 hours agorootparentprevMaybe you should go back and try a control without the dowsing wand reply boffinAudio 8 hours agorootparentDone precisely that with random Monte Carlo digs on a site, didn't find water. Switched to dowsing, found water within minutes. reply 542354234235 3 hours agorootparentWhy would you do it randomly? When you have the rods, you are using your eyes to navigate the environment. If the dowsing rods are just a way to help key you into your own senses, like seeing depressions or slight differences in color, or feeling slight downhills while walking, then the control should be trying it without them. If I clamed I had a pair of glasses that improved your driving, the control would not be driving blidfolded. reply astura 7 hours agorootparentprevThere's a half a million dollar prize for you if you can prove it - https://cfiig.org/paranormal-challenge/ reply krferriter 2 hours agorootparentprevIt's possible that you are biased towards dowsing and thus not a good control for the experiment. You want dowsing to work and thus try harder when dowsing than when not. reply ezoe 11 hours agoparentprevA dangerous thought. There is no proof the interviewee tell the truth. It's easy to plant a fake memory to humans and make them believe it's \"real\". Oh what am I thinking? The important thing is, the interviewer can produce a lot of \"confessions\" and \"revealing of truth\". They will be evaluated a good interviewer. Secure their job position. Sounds good. /s reply withinrafael 3 hours agoprevHey quackery or not, the Polygraph saved my heart. (Disclosure: A bit of an exaggeration.) After hours of uncomfortable prodding, an interviewer came into the room and suggested I see a doctor for what looked like heart arrhythmia. I did shortly after and they were right! (It was deemed nothing too serious though.) reply bityard 2 hours agoparentYeah, the instruments work fine, but the interpretation of the readings is just modern day tea leaves. reply jmyeet 1 hour agoparentprevAlternative take: lack of accessible, affordable and comprehensive healthcare almost cost me my life. I'm actually curious. Do you get an annual physical? Does your PCP give you an EKG? That would be the appropriate way to catch this. reply Terr_ 12 hours agoprev> But the machine remains useful for extracting confessions. [...] Despite a growing body of evidence, including hundreds of exonerations based on DNA evidence, most people don’t believe in false confessions. Arguably the bigger/worse false belief right there. > First the exam makes you doubt or forget your memories. Then, by forcing you to re-access them again and again under stress, it literally rewrites them. To some extent this happens naturally, so if the questioner really wants accuracy, you won't force people to re-access the memory for no good reason. reply Netcob 10 hours agoparentThere is an entire little ecosystem/subculture around \"repressed memories\" doing a lot of harm to vulnerable people. Basically you go to a \"therapist\", they do some sort of hypnosis/interview session where they ask you a lot of very leading questions, and then you leave having been convinced that your family or a satanic cult abused you as a kid (or in some cases that you have been abducted by aliens). The person performing this interview might not even be aware they are doing anything wrong, to them those leading questions (\"Do you see anyone else in the room with you? Look closer, are you sure?\") may just be how you get to the truth. reply denton-scratch 6 hours agorootparent> (or in some cases that you have been abducted by aliens) Really? There are therapists who will try to convince you that you were abducted by aliens? reply nemomarx 4 hours agorootparentI don't think anymore, but in the last century there were several cases of therapists using hypnosis to unlock \"repressed memories\" of alien abductions. They usually wrote up books about it for profit. reply RIMR 2 hours agorootparentprevNot specifically, but they will follow the absurd path of asking you leading questions until you convince yourself that you were abducted by aliens, and then being quacks will decide that their methodology couldn't be wrong, and so they validate your own invented beliefs no matter how stupid. If it keeps you coming back for another session, they'll keep doing it, even if they know the whole process is bullshit. reply JohnMakin 3 hours agoprevThis article touches on it but to me the most offensive part of a polygraph is the presumption of guilt if you refuse to take one. This is very much on purpose, of course, given as the author accurately states, its purpose isn't to detect lies, it's a tool used for coercion. There was a big robbery at a place I worked, luckily I was not a suspect but the FBI came and administered polygraphs to anyone who could have done it. I asked what happens if someone refuses, and the answer was basically \"they won't.\" Of course it ended up being someone that didn't even work there. The test was clearly for intimidation purposes, at least from the view I had. reply mmmlinux 3 hours agoparentAren't polygraphs not acceptable evidence in the US courts? reply bityard 2 hours agorootparentPolygraph tests are not usually acceptable as evidence in court. HOWEVER. Law enforcement can still use them as another tool in their questioning process to produce a confession when there is other evidence pointing to the guilt of the suspect. But of course is only effective against defendants who represent themselves, don't seek their lawyer's advice, or ignore their lawyer's advice. reply RIMR 2 hours agorootparentprevPolygraphs can be used as evidence, but you would have to be a moron to submit to one, and refusing one cannot be used as evidence of guilt. reply singleshot_ 2 hours agorootparentI’m very curious about your last sentence. Are you basing this on the fifth amendment, or something else. reply dfxm12 2 hours agorootparentFWIW, in PA, it can't be brought up in court as evidence that you refused to take a polygraph. Nationally, we have a right to remain silent, so I imagine it's not much different elsewhere, but IANAL, so there could be some tricks cops/prosecutors can play in other states. There's no connection to the fifth amendment. reply dvh 12 hours agoprevFirst sentence on Wikipedia: >A polygraph, often incorrectly referred to as a lie detector test, is a junk science device or procedure that measures and records several physiological indicators such as blood pressure, pulse, respiration, and skin conductivity while a person is asked and answers a series of questions. reply javier_e06 38 minutes agoprevNo Polygraph discussion is complete without Moe being subjected to the Polygraph... Eddie: Checks out. OK, sir, you're free to go. Moe: Good, 'cause I got a hot date tonight. [buzz] _A_ date. [buzz] Dinner with friends. [buzz] Dinner alone. [buzz] Watching TV alone. [buzz] All right! I'm going to sit at home and ogle the ladies in the Victoria's Secret catalog. [buzz] [weakly] Sears catalog. [ding] [angry] Now would you unhook this already, please? I don't deserve this kind of shabby treatment! [buzz] reply mbg721 3 hours agoprevThis is in line with the joke, \"How do get the NYPD to catch a rabbit?\" You ask them, and a week later they bring in a badly beaten bear, who shouts \"Okay, I'm a rabbit! I'm a rabbit!!\" reply kzrdude 3 hours agoparentThat used to be a joke about KGB, or so I thought. reply lstodd 1 hour agorootparentIt was https://imgur.com/fbi-cia-kgb-5koAv3H reply torginus 11 hours agoprevIt's (unfortunately) not an American problem. I have firsthand experience that courts are highly prestige-driven and the best way to make your case is have a ton of expensive-looking and official-sounding documents written by 'experts' that support your argument. Judges are like every high-level decisionmaker ever - the thing they fear the most is publicly being proven wrong so they always go for the safe option where they can share the responsibility of their decisions with 'experts'. reply snowpid 10 hours agoparentTo which countries and which cases are you referring to? reply rightbyte 10 hours agoprevWatching Dexter this 'blood splatter anaysis' seemed beyond ridculous to me. I thought there were no way it would be a thing in the US in the way portrayed. But then I learn truth detectors are? What more movie tropes are not tropes? reply Hikikomori 4 hours agoparentPlenty of techniques are based on junk science, much more in the past than now. Police/FBI don't actively try to find out if techniques are based on real science or not as they are useful in securing convictions. So called experts in these fields testify in trials and are paid quite a bit, it is in their interest to continue being paid so the fraud perpetuates. Unless you have money you have no way to put up a defense that can discredit expert witness testimony. https://www.theguardian.com/us-news/2022/apr/28/forensics-bi... reply duped 3 hours agorootparentMan it's shocking how bad forensic \"science\" is. If you watch Law & Order you may be convinced that police can detect gun shot residue on a suspects hands/clothing or match shell casings to a specific gun. It turns out just throwing handcuffs on someone is enough to get a false positive GSR test. And matching a shell casing to a specific gun is essentially impossible. The false positive rates on forensic \"tests\" are hard to study because no one has an incentive to, and if you go digging you'll find how bad they can be. Like for example K9 units have about a 50% false detection rate (dog indicated but no drugs/weapons/bombs found). If a cop told you they flipped a coin to decide whether to search a car then the search would be tossed out of evidence immediately! reply mrguyorama 2 hours agorootparentprevThe vast majority of \"Forensic Techniques\" used by cops are literally things an ex-cop made up / \"\"\"observed\"\"\" and are now charging insane prices to go across the US, giving presentations to other cops or giving testimony in trials. It's a factless and truthless system. reply NeoTar 3 hours agoparentprevFingerprints are less accurate and reliable than often portrayed in the media. reply dboreham 12 hours agoprevLots of these. E.g. changing car engine oil every 20 minutes. Although this doesn't extend to the US government -- the army periodically samples oil and changes when it actually needs to be changed. reply Timothee 4 hours agoparentI discovered the engine oil thing in the past two years. In the US, I’ve always heard something like every 3 months or 3,000 to 5,000 miles. The cars even warn you in that timeframe. In Europe, a mechanic was very confused when I talked about an oil change when one was done the prior year. He said it’s more every 2 years or 20,000 to 25,000 kilometers. Once again the cars were set up to warn at that distance. I’d love to get to the bottom of it. Why such a difference? I can understand the garages wanting the extra business, but why would the car manufacturers go with it? reply kevstev 3 hours agorootparentIn this case I think the times have changed but old advice has stuck. From my understanding, cars until the 80s or so did need their oil changed this often, but newer cars with EFI and especially if you use synthetic, its no longer necessary to do so. Its been many years since I bought a new car, but IIRC even the mid 2000s you were supposed to get an oil change relatively soon after getting a car as fine particles that weren't entirely machined off should have been worked off in the first thousand miles or so and you were told to get an oil change then. There is also the case of changing oil for hot and cold seasons- getting thinner oil in the winter and thicker in warmer weather to adjust. I think thats more or less a thing of the past as well, my Honda does not specify/recommend this in the owners manual but perhaps some cars do? Old adages sometimes stick around forever though- especially when there is money to be made in keeping them alive. reply jtriangle 3 hours agorootparentRemember that the manufacturers recommendations are designed around keeping the car operational through whatever warranty period they sell the car with. It's not some sort of ideal program and some benefit exists from changing fluids more often than specified. Specifically, transmission fluid is often considered 'lifetime' fluid that doesn't need to be changed, and, if you follow that advice, you end up replacing the transmission, an expensive endeavor, whereas if you don't, you can typically double its usable lifetime. You also have to be careful about engine oil, because the margin of error is very small. Most modern cars have an oil minder, and those work well provided the oil itself is in spec. reply vundercind 4 hours agorootparentprevFree oil changes from dealers are often offered on new cars. Getting you in to the dealer more often may benefit them in some ways: 1) Opportunity to upsell other services, 2) Some people (quite a lot, actually) have alien-to-me car buying habits and might be convinced to trade in their new car after only a year or two and buy another new one if you can just get them into the dealer at the right time. No clue if that’s why they do it, but maybe. reply rascul 5 hours agoparentprevThe US Army changes oil in their trucks based on the schedule in the Technical Manual, not based on sample analysis. reply amtamt 12 hours agoparentprevSituations like these https://www.thehulltruth.com/boating-forum/183583-fuel-tank-... could be the reason to test oil frequently. reply Okx 9 hours agoprevNo wonder he heard from his brother that the Border Patrol were always hiring; they seemingly make it impossible to get hired. reply rdtsc 11 hours agoprevWith the polygraph used as a universal filtering device for hundreds of thousands of employees in powerful agencies, we end up with a mix of either super honest ones who reveal everything to the polygrapher (i.e. interrogator), or psychopaths who lie through their teeth without showing any physiological signs and passing with flying color. Next time when you deal with these agencies, as a fun mental exercise, try to figure out which one of the two types you've got in front of you. When it comes to climbing to the top through the ranks, which ones will get there more effectively? reply analog31 7 hours agoparentI wonder if it's simpler than that: The examiner is just choosing whether they like you or not. reply rdtsc 2 hours agorootparentOfficially they have to follow their training, so to speak, and I am sure that's all about how polygraphs are 100% reliable and it's scientific and all that. I wonder if any instructors at some point close the door and tell them \"listen, students, yeah, it's all bunk, but we just have to pretend, ok?\". reply analog31 1 hour agorootparentThey're apparently allowed to manipulate the subject ad lib. reply lesuorac 1 hour agoprev> Why should I be banned from a job for being a small-time delinquent twenty years ago? Fair question. I'm not sure that's 1) been shown to be true 2) the fault of a polygraph; it's a policy decision not a mechanical one. > As he pumped up the cuff, Kevin asked if I was comfortable. He didn’t seem to be joking. Yeah, he probably was serious. Kevin has a job to do and it's to ask you questions and record your responses; not to torture you. Sometimes people really forget that not every examiner thinks you're the next Aldrich Ames. But I mean also if you're in a very uncomfortable situation and somebody asks you how you are, maybe being honest will help resolve that situation? Like clearly you can't resolve it on your own otherwise you wouldn't be in such a panic. > Except for the last two, all of them seemed open to interpretation. For instance, I absolutely had misrepresented my past drug use, but only the number of times, not the drugs or the fact of doing them. If only there was somebody that could've helped you determine what the answer should be. Maybe Kevin? --- I mean yeah, if you go into a polygraph expecting a fight I don't think it's fairly surprising you don't get the job. Who wants to hire somebody combative to your employees? Sure, it's unfair but you're taking it out on Kevin who didn't come up with it. reply ewy1 1 hour agoparentMaybe you misinterpreted the intent of the article - isn't the purpose of it to point out absurdities and fundamental flaws in the uniquely American polygraph obsession? reply newzisforsukas 38 minutes agoparentprevWho wants to work for or with someone who pretends they can read your mind? reply dang 33 minutes agoprevRelated: UK police increasingly using polygraph tests - https://news.ycombinator.com/item?id=39161771 - Jan 2024 (5 comments) Spanish police plans to extend use of its lie-detector while efficacy is unclear - https://news.ycombinator.com/item?id=24905780 - Oct 2020 (38 comments) Accused spy Alexander Yuk Ching Ma evidently beat the polygraph - https://news.ycombinator.com/item?id=24197310 - Aug 2020 (186 comments) We tested Europe’s new lie detector for travelers and triggered a false positive - https://news.ycombinator.com/item?id=21358288 - Oct 2019 (114 comments) Why Lie Detector Tests Can’t Be Trusted - https://news.ycombinator.com/item?id=20556201 - July 2019 (190 comments) Attempts to Censor AntiPolygraph.org - https://news.ycombinator.com/item?id=20311040 - June 2019 (45 comments) The Lie Behind the Lie Detector - https://news.ycombinator.com/item?id=18431683 - Nov 2018 (96 comments) An AI Lie Detector Is Going to Start Questioning Travelers in the EU - https://news.ycombinator.com/item?id=18351733 - Nov 2018 (202 comments) Personal Statement of a CIA Analyst - https://news.ycombinator.com/item?id=18155548 - Oct 2018 (104 comments) The Lie Generator: Inside the Black Mirror World of Polygraph Job Screenings - https://news.ycombinator.com/item?id=18120270 - Oct 2018 (95 comments) NCCA Polygraph Countermeasure Course Files Leaked - https://news.ycombinator.com/item?id=17277049 - June 2018 (4 comments) Do Polygraphs Actually Work? - https://news.ycombinator.com/item?id=12951926 - Nov 2016 (4 comments) An Ex-Cop's War on Lie Detectors - https://news.ycombinator.com/item?id=10002889 - Aug 2015 (64 comments) How to Beat a Polygraph Test - https://news.ycombinator.com/item?id=9481385 - May 2015 (92 comments) Man accused of teaching people to beat lie detector tests faces prison - https://news.ycombinator.com/item?id=6308878 - Sept 2013 (152 comments) The Lie Behind the Lie Detector [pdf] - https://news.ycombinator.com/item?id=6307479 - Aug 2013 (22 comments) Seeing threats, feds target instructors of polygraph-beating methods - https://news.ycombinator.com/item?id=6229185 - Aug 2013 (4 comments) All lies? Scientists threatened with legal action over lie detector article - https://news.ycombinator.com/item?id=457996 - Jan 2009 (1 comment) My NSA polygraph experiences - https://news.ycombinator.com/item?id=428489 - Jan 2009 (46 comments) also https://news.ycombinator.com/threads?id=giles_corey reply Unbefleckt 8 hours agoprev>But cries of racism rang hollow coming from people who worked in academia, one of the whitest industries in America. Why shart this into an interesting article. reply diogenes_atx 3 hours agoprevThe writer is obviously an intelligent person with good critical perspective, so it is surprising that he missed some important material about known countermeasures that can be effectively used to defeat the polygraph. A quick HN search with keywords \"lie detector\" offers a number of excellent articles on the subject, including discussion of the book \"Lie Behind the Lie Detector\" (available for free online) which provides detailed information on polygraphs and techniques to subvert the tests. reply MikePlacid 10 hours agoprevWhen we presented our pediatrician with our third child who could urinate in a sink on verbal command at just six months old, she remarked, \"We should write an article for a medical journal!\" We explained that such an article would never get published because it's not new information; most of Europe begins potty-training at around six months. Delaying this valuable skill until the age of 3-4 years is an enormous waste of resources - but still the whole country was insisting on doing it, don’t know about now. reply lqet 8 hours agoparent> most of Europe begins potty-training at around six months Not sure how this relates to the article, but this is news to me (European). We slowly began potty training somewhere between 1 and 2 years. I have never heard of anyone doing potty training at six months. Babies are just barely able to sit upright at that age. reply apexalpha 4 hours agoparentprev>We explained that such an article would never get published because it's not new information; most of Europe begins potty-training at around six months. Funny. I am European and we have this myth about Asia. reply n4r9 9 hours agoparentprev> most of Europe begins potty-training at around six months Where did you hear that? Admittedly here in the UK we've been doing our level best to extricate ourselves from the continent, but I've only ever heard of one mum even thinking about it before 18 months. Ours is just over a year and we haven't thought about it at all yet, same with our ante-natal group and friends with slightly older babies. I googled around a bit and this reddit thread has a lot of Europeans with similar experience to me: https://www.reddit.com/r/Mommit/comments/tdb1f2/what_are_non... reply knallfrosch 7 hours agorootparentThe German term is \"Abhalten\" (from \"halten = to carry\") or (\"windelfrei = diaper free\") and you'll find quite a lot: https://www.google.com/search?q=baby+abhalten+*.de Our baby was born potty-trained (which actually means the term is misleading in our case) and a relative started at their childrens' birth. reply n4r9 4 hours agorootparentThis sounds different to potty-training, which is where the child knows that they need to wee or poo and tells you or goes straight to the potty to do it. \"Abhalten\" sounds like the parent training to know rather than the child. reply TeMPOraL 8 hours agoparentprev> most of Europe begins potty-training at around six months. Hailing from Poland; first I hear of this. I know of total of two people who said something like this before - one person is saying a lot of other borderline insane things about parenting, and the other has a business selling webinars around the idea of potty-training kids less than a year old. reply Avshalom 3 hours agoparentprev>>urinate [on] command Wait, what do you think potty training is? reply TomMasz 1 hour agoprevI watch a lot of true crime programs and the polygraph is mostly used as justification to harass innocent people. The police consider the refusal to take it as a tacit admittance of guilt, despite the fact it can't be used in court. It's all shit. reply jrgaston 3 hours agoprevA wonderfully written piece. It’s not a lie if you believe it. - George Costanza reply bell-cot 8 hours agoprevI'd describe American (govt. agency) polygraphy as a psychological hazing ritual, run by a pretty machismo crowd. Who don't like their beliefs and rituals questioned. reply graemep 8 hours agoprevIt reminds me of the GK Chesterton short story \"The Mistake of the Machine\" which suggests it was a very American idea even back then. https://en.wikisource.org/wiki/The_Wisdom_of_Father_Brown/Th... The evidence it \"works\" reminds me of alternative medicine. reply OsrsNeedsf2P 3 hours agoprevThe author is uncooperative throughout the whole test, fails it, then claims the polygraph is quackery. reply thornewolf 3 hours agoparentthe only lack of cooperation communicated in this article is the account on drug use. that said, the claims on quackery are unrelated to the author's specific set of behaviors during the test. i reckon that i, knowing the complete lack of scientific backing, would also be uncooperative to some degree. reply jmyeet 1 hour agoprevThere is a long history of junk science in forensics [1]. Often this junk science is used to reinforce established biases and give bloodthirsty juries a hook to hang a conviction hat on. Now polygraphs now generally aren't admissible in court. This wasn't always the case [2]. Still, law enforcement does use them to eliminate or confirm suspects outside of a court environment. This can just confirming existing biases. And that's the deeper issue here: the American criminal justice system is mainly retributive or punitive. Long setnences for minor crimes. High conviction rates. Abuse of the power imbalance between prosecutors and defendants. Judges that essentially work for the prosecution (eg [3]). Juries that just want to convict. Of anything. Yet there's ample evidence none of this is effective whereas something as simple as giving inmates cats to look after is extremely effective at reducing recidivism [4]. My point is that technology can be (and has been) used as a crutch to confirm biases and that criminals are still people and we should treat them as people. [1]: https://www.propublica.org/article/understanding-junk-scienc... [2]: https://axeligence.com/polygraph-admissibility-in-united-sta... [3]: https://www.rollingstone.com/music/music-news/young-thug-ric... [4]: https://www.indystar.com/story/news/local/indianapolis/2020/... reply motohagiography 7 hours agoprevmy experience with the polygraph was it detects your ability to submit. it's less a delusion than a ritual. as a filter, it produces a completely polarized bimodal distribution of banal followers vs. the basest human malevolence and contempt, while disqualifying relfection or doubt. if the purpose of a system is what it does the polygraph is perfect. it finds expendable souls. reply DoItToMe81 9 hours agoprevSadly, not All-American anymore. The US police force doesn't exist in a vacuum. There is a whole industry of pseudoscientific interrogation techniques that has set itself up in other nations and regressed their policing by decades. Several states in Australia, and I believe parts of the UK, removed the polygraph as a discredited technique and now accept it as evidence once more. reply n4r9 9 hours agoparentUK here, just looked it up and although it's not admissible as evidence in court, since 2021 it can be used as a requirement for release from prison for domestic abusers. Bloody hell, that is scary. reply xnorswap 8 hours agorootparentThat is scary! Well, we've recently got a new prisons minister, this might actually motivate me to write to them about this. reply n4r9 4 hours agorootparentI looked them up: https://en.wikipedia.org/wiki/James_Timpson Woah. It's actually the MD of Timpson, whom Starmer has parachuted into the role along with a peerage. So an unelected Minister, but apparently he's very into prison reform, so there's that. reply xnorswap 3 hours agorootparentIndeed, Timpson's are a leading employer of people who have served their sentence: > The company is well known for its policy of employing ex-convicts, who make up over 10% of its workforce reply Quarrel 7 hours agoparentprevWhere in Australia? I've been out of Aus for a few years, but in NSW I understand they're still largely banned (Lie Detectors Act) for employment, courts / evidence, insurance etc (and if they weren't by legislation, they would be for evidence by precedent, which is how the Act came about). As I understood it, this and the Canadian precedent that NSW relied upon, have basically made them a non-starter for courts in Australia ever since. It's a bit horrifying if they're making a comeback, but then our politicians have always been a bit prone to right wing shock jock rhethoric around election time. reply nelox 8 hours agoprev [–] You can add dissociative identity disorder, a.k.a. multiple personality disorder, to the list of All-American delusions. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Justin St. Germain explores the blurred lines between memory, memoir, and myth through his experience with a polygraph test during a job application with Customs and Border Protection.",
      "He argues that the polygraph, often seen as a lie detector, actually creates rather than detects truth, drawing parallels to memoir writing.",
      "Germain discusses the polygraph's controversial history, its use in coercing confessions, and its symbolic role in asserting state power, highlighting its flaws and the American obsession with a simple, detectable truth."
    ],
    "commentSummary": [
      "The author recounts taking a polygraph test at nineteen due to theft suspicions at work, despite being innocent.",
      "Polygraphs are criticized for their use in coercing confessions rather than accurately detecting lies, often leading to stress and unreliable results.",
      "Despite their questionable accuracy, polygraphs continue to be used in job screenings and criminal investigations as a tool for intimidation."
    ],
    "points": 194,
    "commentCount": 188,
    "retryCount": 0,
    "time": 1721086774
  },
  {
    "id": 40973123,
    "title": "A Review of Linux on Surface Pro 4",
    "originLink": "https://www.binwang.me/2024-07-12-A-Review-of-Linux-on-Surface-Pro-4.html",
    "originBody": "A Review of Linux on Surface Pro 4 Posted on 12 Jul 2024, tagged LinuxSurfaceMicrosoftOperating systemtech Background I bought a Surface Pro 4 at 2016. It has an Intel Core m3-6Y30 CPU and 4GB memory. The spec is not that impressive even compared to an average laptop released years earlier. On the other hand, the form factor is very attractive to me: at a very low price, you get a tablet with a beautiful HiDPI 2k screen, a pressure sensitive stylus and an useable keyboard. It is on the heavier side if used as a tablet, but compared to other laptops, it’s very light. It served me very well for my limited use cases. The blog Build a Unix Like Environment on Windows was written at that era. Some years later, I bought a more powerful laptop when I needed to work while traveling. So I gave the Surface away to a family member. However, during the past years, I couldn’t stop thinking about having a Linux tablet. At first I checked Pinetab, then I realized I had a Surface which would be perfect if I could install Linux on it. I searched online and found some successful stories. So when I travelled back to my hometown at the beginning of this year, I brought the Surface back with me and started to experiment with it. Use Cases Before I go further, I need to mention my intended use cases: Browse Internet. Mainly RSS Brain, the RSS reader I built by myself. Media consumption: watch videos from my Samba share and online websites like Youtube. PDF reading: reading only is enough for me but it’s better if I can take notes in the PDF. Sketches: I don’t have a habit to do handwriting notes even at students era. Nowadays it’s more efficient and readable/searchable to take text notes with Markdown. However, I do like drawing sketches on paper when brain storming or resolving some hard problem. Moving it to digital has a lot of benefits if it works. Drawing: this is a good to have feature. I don’t really have needs to draw things but it’s always fun. Especially with the development of AI, if I draw something and send it to a more powerful machine to generate images, it could open doors to many possibilities. Installation The installation of Linux is actually very easy. I tried two distros and the installation process went very smooth for both of them. The distros I tried are EndeavourOS and Fedora workstation 40. The installation steps are well documented in linux-surface’s wiki. linux-surface is the Linux kernel and tools for Surface devices. The wiki page has its installation steps as well. In general, if only used as a laptop, the experience is almost perfect even without the linux-surface kernel. But using it as a tablet is another story. What Works Let’s talk about what works first. Even without linux-surface kernel, almost everything works except touch screen and stylus. That includes things like wireless network, bluetooth, keyboard, power profile, UI scaling for Hi-DPI and so on. Multi touch and pressure sensitive stylus works as well (sort of, see sections below) after installed linux-surface kernel. Battery life is good enough: about 5-6 hours of light usage like web browsing, PDF reading, and about 3 hours of video watching. (Just some estimated time from my experience, no serious benchmarking was done). On the software side, automatic screen rotation is enabled on both distros I tried. KDE with EndeavourOS is very fast and responsive. When the keyboard is detached, it enters tablet mode which makes some UI larger and more user friendly with touch gestures. For example, you can just touch on a folder to open it in Dolphin instead of double click it. For Gnome, it’s less responsive than KDE but the UI is really beautiful when used as a tablet. I was never a fan after Gnome 3 but I guess the UI changes it made makes more sense on a tablet than on a laptop or a desktop. The overall layout really reminds you about the iPad or Android tablet (in a good way), but with the power of a real desktop OS at the same time. I would really like it if it uses less resource. Even though the overall experience is positive and has the potential to meet all my use cases, one serious problem made it very unusable and made me gave up Linux on Surface at the end. The Problems in Both Distros The deal breaker problem is touch recognition. The problem is in the surface-linux tools so it affects all the distros. The biggest problem is ghost touch: touches are registered randomly even when I do nothing. I tried a lot of workarounds including the ones mentioned in linux-surface’s wiki page, but none of them actually resolved it completely. Sometimes it’s fixed after reboot but reappeared after next reboot. Sometimes it get fixed for a period of time but reappeared after a system upgrade. Sometimes the touch screen doesn’t work at all after resume from sleep. The randomness and the serious of the problem is really annoying so I gave up using it with Linux at last. Other than the ghost touching, another big problem about touch recognition is palm rejection. It’s really annoying when draw things with the pen. In iptsd (surface-linux’s deamon for touch recognition), there is a configuration to disable touch screen when using a pen but it doesn’t work well. So it makes drawing very unusable. Both KDE and Gnome has virtual keyboards when the physical keyboard is detached, and works most of the time despite the problems I’ll mention in the following sections. But if you have setup disk encryption with a password, there is no virtual keyboard when you input the disk password, so a physical keyboard is always needed during the boot. Which can be annoying but not really a deal breaker. The last big problem is battery drain during sleep. It uses about 30% battery for one night even it has been put into sleep. I had similar issues for other laptops. I believe there maybe some configurations I can tune to fix that. But after I gave up Linux on it because of the ghost touch, I didn’t dig deeper into that. Other than the problems shared by both distros, each distro/desktop environment also has their own problems. The Problems in KDE with EndeavourOS The biggest problem in KDE other than the ones I talked above, is the virtual keyboard. It’s buggy and not very stable. Sometimes it kept pop up and sometimes it doesn’t show up. It’s annoying especially at the login screen: if it’s not popped up you will still need a physical keyboard, which prevent it to be a real tablet. Sometimes when the keyboard is popped up, the panel at the bottom cannot be touched. The bugs happened randomly that makes it hard to be properly reported. Another problem is the touch gesture for right click. Naturally, with a touch screen, long press should be treated like a right click. But that is not the case for KDE. So a lot of operations just cannot be done without a mouse when you need a right click. Resize a window is also very tricky with touch only operation: you need to touch on the boarder precisely on the first try. At last, the scroll behaviour is not very smooth. It makes me a little bit dizzy just by scrolling through web pages and PDFs. So I thought give another distro and desktop environment a try, to see if they can resolve my problems. The Problems in Gnome with Fedora Workstation 40 I choose Fedora because it comes with Gnome, and I had good experience with it before. After the installation, the first impression is it’s much slower than KDE with EndeavourOS. I found it enables swap and ZRam by default so I disabled them. It’s better but still slower than KDE. It uses more memory at around 40-50% percentage while idel. And I got a lot of OOM kills which almost never happened with KDE on EndeavourOS. Maybe because of the slowness, it’s also buggy for lots of operations. For example, when switch to the workspace view from PDF viewer with 4 fingers swipe up, the PDF keeps scrolling at the background. And when scroll in the file manager, the context menu keeps popping up. Other than the slowness, there is a problem on the virtual keyboard as well: the backspace key doesn’t work properly. I found a workaround by install a third-party Gnome addon, but sometimes the old keyboard still popped up. Go Back to Windows 10 I’d say if the touch recognition works well enough, all the other problems are acceptable with KDE. But with those problems, I finally decided to fallback to Windows 10 again. It works well enough, just as I remembered from years ago. However I abandoned OneNotes and some other Microsoft products and use the following software instead: Firefox as the browser. Nextcloud to sync the files. Samba for video sharing. Built in video player for local video playing. Krita for drawing and sketches. Drawboard PDF for PDF reading. It’s pretty disappointing that this device cannot be used with Linux properly. But using Windows is still better to just let the device sitting there doing nothing. Maybe I will re-evaluate it after Windows 10 is end of life next year.",
    "commentLink": "https://news.ycombinator.com/item?id=40973123",
    "commentBody": "A Review of Linux on Surface Pro 4 (binwang.me)187 points by chazeon 16 hours agohidepastfavorite199 comments chrsw 6 hours agoI run Ubuntu on a Dell XPS 13 without any issues as far as I can tell. I've done almost no tweaking. I just do periodic software and firmware updates. I close the lid, throw it my bag, open it hours later, or the next day and I'm right back to where I was. The experience as close to Mac-like as I've ever experienced outside of Apple. But I still do wish someone would make a Linux laptop that's as tightly integrated with the hardware as macOS is on a MacBook. reply jt2190 1 hour agoparent> But I still do wish someone would make a Linux laptop that's as tightly integrated with the hardware as macOS is on a MacBook. I feel like the forces around device driver development conspire to make sure this rarely happens, that is, we can’t have “commodity” hardware that has “cutting edge” device drivers because the time and expense of developing the driver isn’t justified with commodity pricing. reply Raydovsky 6 hours agoparentprevThat's because the XPS were built to run ubuntu. You can even buy one with it pre-installed. reply vbezhenar 4 hours agorootparentI have Thinkpad which is supposedly built to run ubuntu as well and even certified for RHEL and Ubuntu. It doesn't work so good, though. It works, but there are rough edges around sleeping, external displays, power management. I feel that it has nothing to do with manufacturer, though, just not good enough Linux support for laptops. reply ryandrake 3 hours agorootparent> It works, but there are rough edges around sleeping, external displays, power management. Windows has these rough edges, too, though. It's actually pretty shocking that here in 2024, PC manufacturers and OS vendors are still struggling with basics like sleep/wakeup. Last job I had with Windows laptops, everyone would walk around the office from meeting to meeting with their laptops propped open because nobody could be sure that their OS would actually wake up when they opened the lid. And when you closed it and went home for the day, would standby actually work or would it be on fire and out of battery the next morning? Somehow, only Apple has seemed to be able to solve this Herculean problem. reply Willish42 1 hour agorootparent> Somehow, only Apple has seemed to be able to solve this Herculean problem. Bit of a stab in the dark here but I would assume ARM has at least something to do with this? Tablets, phones, etc. get standby a lot better than x86 systems seem to. My pre-M1 Macbook Pro does not handle standby well but my partner's M2 Macbook Air lasts for forever and handles sleep etc. well. The lower power consumption in \"standby mode\" on ARM seems like at least part of the picture for why Apple gets this so much better. I bet it's part of why Windows is trying to release their ARM variant and have been working on it for 10+ years reply rad_gruchalski 1 hour agorootparent> My pre-M1 Macbook Pro does not handle standby well but my partner's M2 Macbook Air lasts for forever and handles sleep etc. well. Intel mac on sonoma? reply szundi 2 hours agorootparentprevIf you have an issue with windows, it’s abnormal - while with linux is pretty expected on several fronts reply yjftsjthsd-h 4 hours agorootparentprev> I feel that it has nothing to do with manufacturer, though, just not good enough Linux support for laptops. On the contrary, it's down to the model. As I type this on a fully-functional Thinkpad, I can assure you Linux is fine on laptops. reply chrsw 6 hours agorootparentprevUbuntu 20.04 was preinstalled on my machine. But I reinstalled when I moved to a larger SDD. I think I still used the OEM install image too. reply bdcravens 5 hours agorootparentI can't help but wonder if Dell tweaked the firmware. I know that I, and everyone I've seen discuss it, haven't been able to get a vanilla XPS (non-Developer edition, sold with Windows) with a typical off-the-shelf distro, including Ubuntu, to work 100%. reply AdmiralAsshat 4 hours agorootparentI've had a Dell XPS 13 9343 (2017 model, non-Developer edition) running Fedora for years without problems. I suppose you might consider it cheating because I replaced the original Broadcom WiFi card with an Intel WiFi card, as that driver was a bit flaky in the early days (whereas the Intel driver has kernel support). Other than the pitiful 4 hour battery life, the laptop still runs fine, and mostly does what I need it to do for a permanently-docked daily driver. reply haspok 4 hours agorootparentHey there! I no longer use my 9343, but I remember I was not able to run Fedora without breaking the sound support for it (Ubuntu had some kernel option set on startup that put the sound card to some legacy mode, instead of the I2C that Windows used). And I never managed to setup palm rejection, it was a constant pain whenever I had to use the (otherwise excellent) trackpad. (The external \"carbon-like\" skin texture just disintegrated on it after a few years, and the hinges got loose, but otherwise it is tip-top, still functional!) reply AdmiralAsshat 4 hours agorootparentIf memory serves, the audio issue you're describing was fixed by a BIOS update: https://wiki.archlinux.org/title/Dell_XPS_13_(9343) reply trelane 5 hours agorootparentprevThat seems likely. I know that firmware is one of the big differences between System76 laptops and the version that Clevo subsequently offers with Windows. I think the chips can vary sometimes too. Just from an ACPI perspective, I'd expect the Linux variant to (at a minimum) be built with the Intel compiler and the Windows one with Microsoft's. It is likely that there are far more differences, though. reply KennyBlanken 3 hours agorootparentThe biggest problem with System76 laptops: their screens. $1400 for a laptop with 1920x1080 at 60hz in 2024 is a joke. $200 more gets you a 3024x1964 @ 120hz, with an M3 processor and the ability to get warranty service walk-in anywhere around the world. reply trelane 2 hours agorootparentI agree that a better screen would be great, and walk-in service anywhere in the world would be fantastic. But I want a Linux laptop, not Windows or OSX. I also want a computer that obeys me, not some megacorp (not unrelated to the previous point.) I also want to not fight it all the time. reply vbezhenar 4 hours agorootparentprevI bought Dell 3410 once which was shipped with Ubuntu. I closely inspected that Ubuntu, compared it with vanilla Ubuntu install. All I've found are branding packages (desktop pictures, etc) and one package which blacklisted some module. No secret drivers, no secret kernels. Can't comment about XPS, but I feel that it'll be the same. reply freeqaz 3 hours agorootparentDell does the work to upstream the hardware support into the kernel. It's pretty rad. I miss my old XPS 13! reply chrsw 5 hours agorootparentprevSorry, I should have specified Developer Edition in my first post. reply gosub100 4 hours agorootparentprevI ordered one with Ubuntu pre installed and it worked well, however there was an annoying issue where the mouse would freeze for a few seconds every couple minutes. I eventually swapped it with Garuda Linux and got a much faster UX, but suspend/sleep doesn't fully work. Doesn't bother me. reply m_st 6 hours agoparentprevWoah! Standby is working fine too? I'm a huge XPS15 advocate at work and really love these machines as a Windows developer. But the standby just doesn't work. If I close the lid and throw it in my bag, then the battery will be empty and the bag will be hot as hell. This is a huge failure and makes me shutdown my XPS15 every evening. Which is just nonsense. I'm a Mac user at home and just never shut these laptops down ever. reply chrsw 6 hours agorootparentYes, standby is working fine. I don't have the machine in front of me now but I don't remember fiddling with any of the power settings either. It was all working after the install. I definitely run software update so that might explain why it's working so smooth too. Meanwhile, my other machine from work is a Precision workstation running Windows 10 and it gives me all kinds of power issues, more invasive updates, random restarts, random high fan RPMs, etc. Dell has already serviced the machine, twice. What a mess. reply haspok 4 hours agorootparentprevFWIW I had similar problems with my X1, sleep on lid close was working about 50% of the time (which is probably worse than not working at all, because you genuinely don't know what is going to happen...). As a quick fix I assigned Ctrl-Meta-L to Sleep (Meta-L is screen lock - I'm using KDE btw). It didn't take long for me to automatically press this combo before closing the lid - I got so much used to it that I had stop stop and think when I got a new laptop later and installed linux fresh on it. And of course I just set it up like before, even though this one works :) reply trelane 5 hours agorootparentprevStandby doesn't work on Windows? reply jasonjayr 4 hours agorootparentIn the last few years; Microsoft started pushing this \"Modern standby\"[1] thing, which lets the CPU run while suspended or something. IIRC it is so a PC can run background services, wifi and what not, like tablets + cell phones. It is causing so many issues, because the common use case for a laptop is to close the lid, and then stuff it into a padded bag. If anything starts up the laptop for whatever reason, all that heat is trapped in there, cooking the device. Some system BIOS are removing the option to even disable modern standby mode (vs traditional standby where just the memory was energized) 1: https://learn.microsoft.com/en-us/windows-hardware/design/de... reply criddell 4 hours agorootparentThis should work much better than it does. Microsoft is right - Windows machines should be able to run background services as well as a tablet or phone. Their Modern Standby requirements should have included a clause saying that the machines efficiency core (which I assume is what would be running in standby) should not be able to raise the temperature enough to require a fan. reply haspok 4 hours agorootparentNo, Microsoft did not ask the users if they wanted this or not (or made this behaviour configurable). Just as they did not ask users if they wanted to see ads in their Start menu... reply criddell 3 hours agorootparentYou only want an option because Microsoft and their hardware partners did a poor job with this. Pretty much nobody asks for the same feature to be configurable on their iPad because it works well. reply jasonjayr 3 hours agorootparentIt works well on mobile devices because from the get-go, it is established that the operating system can aggressively suspend or halt processes. Laptops + PC's, on the other hand, have 40+ years of legacy that assume that the OS won't kill a process unless the user insists, or a resource disaster is imminent. They can deal with a pause, provided the processes external view of the state of the CPU + memory are not drastically changed. Windows finally had suspend working reliably, where memory was frozen, and nothing else on the PC could change the state of memory or the CPU. Modern standby is Intel/Microsoft's effort to hoist that mobile-style of operating system management onto PC's, in an environment that was not expecting it. They should have slowly rolled it out, with thermal protections from the get go to prevent disaster, and after a generation or two when the hardware + software are working correctly, made it on by default. It seems like they rushed it for Win 10, and then made it the default on Win 11 before it was really stable. reply mananaysiempre 4 hours agorootparentprev> Some system BIOS are removing the option to even disable modern standby mode The CPU manufacturers have stopped providing support for developing firmware with an S3 (“traditional standby”) function for recent CPU generations, except for a couple of laptop manufacturers receiving special treatment. reply mrguyorama 1 hour agorootparentprevThe rumor is that this is a bug that happens when you close your laptop screen to put it to sleep BEFORE you pull out the power plug, so the laptop basically never realizes it stopped being plugged into the wall, and does work it shouldn't, like a windows update. I always remove the power before putting a laptop to sleep and do not have this problem anymore. It happens on macbooks too weirdly. A sleeping laptop, even \"modern sleep\" should not be doing enough work to create a meaningful amount of heat. reply gosub100 4 hours agorootparentprevI really hope this doesn't become a contributing factor in a future plane crash from an onboard fire in the baggage compartment. I could see someone throwing their laptop in a suitcase with a bunch of clothes and having that heat building up into a thermal runaway. It's asinine to me that there isn't a hardware thermal sensor that just shuts off power if the heat is too high. In addition to the tragedy of an accident, what will happen is they'll probably block everyone from bringing laptops with them. reply cbm-vic-20 3 hours agorootparentOh, you haven't touched your laptop in 30 minutes and we just reached 35,000 feet? This must be a good time to run \"Antimalware Service Executable\"! reply Dalewyn 2 hours agorootparentprev>I could see someone throwing their laptop in a suitcase with a bunch of clothes and having that heat building up into a thermal runaway. There's this thing called Shut Down. Use it sometimes. reply gosub100 30 minutes agorootparentWhy just sometimes? reply vel0city 4 hours agorootparentprevStandby works fine for me on Windows and has for a long time across dozens of different devices. Chances are if the system keeps waking from sleep, they have some third-party app that keeps waking the system. reply criddell 4 hours agorootparentThe machine isn't waking from sleep, it's that the standby processing is intensive enough and the hardware is so poorly designed that the computer heats up which requires the fan to run. > When Modern Standby-capable systems enter sleep, the system is still in S0 (a fully running state, ready and able to do work). Desktop apps are stopped by the Desktop Activity Moderator (DAM); however, background tasks from Microsoft Store apps are permitted to do work. In connected standby, the network is still active, and users can receive events such as VoIP calls in a Windows store app. While VoIP calls coming in over Wi-Fi wouldn’t be available in disconnected standby, real-time events such as reminders or a Bluetooth device syncing can still happen. https://learn.microsoft.com/en-us/windows-hardware/design/de... reply vel0city 4 hours agorootparent> hardware is so poorly designed So, don't buy poorly designed hardware? Even my $300 Walmart (Motile) laptop suspends with Connected Standby enabled without issue. I've had over a dozen devices since 8.1 came out, none of which had problems with Connected Standby. reply throwaway3306a 4 hours agorootparentprevMacbooks also wake from sleep while closed and yet it doesn't destroy the computer. How is the computer supposed to do background checks / send its location etc if it can't wake up for a short while? reply vel0city 4 hours agorootparentConnected Standby has worked on my devices for a decade. When I plug in my laptop to my dock in the office and it wakes up, it comes on pretty much instantly. Its already on the WiFi, which it joined when I walked in the building. My email has already synced. My chat has already synced before I even log in. It has been doing this just fine since Windows 8 came out across multiple Thinkpads, Surface tablets, and other devices. Even pre-Windows 8, sleep has generally worked perfectly fine for me. I'd have my computer on sleep between classes, open it up and pretty much instantly be right back in OneNote ready to take notes. Cheap Compaq laptops, expensive HP laptops, IBM Thinkpads, Lenovo Thinkpads, Surface tablets, no-name cheap Walmart laptops, all kinds of devices. In the last almost 20 years I've had less than a dozen instances of a hot bag running XP, Vista, 7, 8, 8.1, 10, now 11. I had issues with sleep on some desktops in the past, where it wouldn't want to stay in sleep. Every time it was some dumb app waking up the machine. Never due to some specific Windows issue, always something I installed. reply haspok 4 hours agorootparentprevI don't want my computer to do _anything_ if I set it to sleep, other than keep the memory contents alive for some time. Although these days even Ubuntu with KDE starts up so fast that the only reason for sleep (instead of shutdown) is to keep some programs running, with some mid-work state. reply dragonwriter 4 hours agorootparentprev“How is the computer supposed to do background checks / send its location etc if it can't wake up for a short while?” Why would I want it to do that? OTOH, coming back from pay of on modern hardware is fast enough that I just reenable hibernation and use that instead of sleep, now that MS has made sleep less sleep-ish. reply WD-42 5 hours agorootparentprevStandby on windows just appears to be a cue for the OS that the user isn’t actively using the machine so it should use the time to install updates and restart itself 5 times. reply evilduck 4 hours agorootparentprevAlmost never in my experience. reply iamacyborg 5 hours agorootparentprevCertainly doesn’t appear to on my thinkpad reply trelane 5 hours agorootparentOuch. Maybe 2025 will finally be the year of Windows on the desktop! reply rqtwteye 4 hours agorootparentThe year of windows on the desktop was around 2004. Since then Microsoft has diligently worked to make things worse. reply martzy13 4 hours agoparentprevhttps://system76.com/laptops reply sbrother 3 hours agorootparentAre they built better now? I've bought a lot of stuff from them in the past and while their support is great and their pre-built desktops are fantastic, their laptops were just rebranded Clevo trash. reply tracker1 2 hours agorootparentI really wish that System76 would offer a Framework based option... would definitely pay a bit of a premium for Pop OS support on Framework hardware. Those two companies are just screaming for a teamup IMO. reply yoyohello13 3 hours agorootparentprevNo, still Clevo. Although the CEO said they are currently designing a custom laptop chassis in house. Probably still a minimum 2 years away, but at least they are working on it. reply ericjmorey 2 hours agorootparentThey don't exclusively use Clevo and they're now leveraging their business relationships to have designs guided by System76 priorities. There's been no updates on the Virgo project for about a year. I hope they're able to get it to market, but it seems a ways off. reply cevn 2 hours agorootparentprevThey are not. I have one and the trackpad sucks, and it has a USB C port which charges the laptop EXCEPT when it is fully dead.. reply KennyBlanken 3 hours agorootparentprevDon't waste your time. 1920x1080p @60hz in 2024... reply BanazirGalbasi 2 hours agorootparentThis is such an elitist attitude, and I'd like to see less of it. The vast majority of users aren't going to be bothered by those screen specs. For many coming from low-end hardware, it's actually an upgrade. Most work won't be significantly impacted by increasing the refresh rate, and while better resolution can be helpful if you keep multiple windows on the screen, most programs still feel tailored to 1920x1080 screens. Office workers writing emails, reports, purchase orders, and basic spreadsheets aren't likely to notice a better refresh rate and they're more likely to get a positive impact from turning a monitor vertical to fit more of a page on their screens. Don't get me wrong, I use two 2560x1440p monitors at 144Hz at home, but I honestly get just as much work done on my dual 1080p 60Hz monitors at my desk at work. Saying that a laptop with 1080p@60hz is a waste is elitist and unnecessary in my opinion. reply treyd 1 hour agorootparentprevMy first laptop in 7th grade was 1366x768(@60?) and it's what got me into the whole industry. I still use 1920x1080@60 as my daily driver work laptop and it's fine. If I need bigger screens / higher refresh rate I have my desktop. reply cevn 2 hours agorootparentprevI have the Pangolin which is 144hz, but good luck hitting that on the AMD 780m. Also the touchpad and charging sucks. Other than that I have had some good gaming experiences but the drawbacks are too much for me to recommend. reply HumblyTossed 3 hours agoparentprevI have a cheap Ideapad Pro with an AMD proc that gives me the same experience using Pop_OS. MacOS doesn't run on anything(1) but a Mac and people seem to be okay with that, but good grief, you tell them to pick a machine that is compatible with Linux and they lose their shit. (1) Please don't be pedantic, I get it. reply rty32 4 hours agoparentprevI assume it does not come with a touch screen or pen support? Then it is really an apple to orange comparison. reply utf_8x 12 hours agoprevDisables Swap and Zram, gets OOM killed, surprised pikachu face Joking aside, is there an actual legitimate reason to do this on a workstation? I understand why you would want to disable swap on something like a kubernetes cluster node but in my head, heaving at-least zram enabled is a good thing on a workstation so you *don't* get OOM killed... I call on thee, Linux wizards of HN, to help me understand the reasoning behind this. reply black_puppydog 5 hours agoparentPersonally, for a long time I disabled swap and made sure that I had an OOM killer running. This was always in a setup where I'd have ample RAM for my everyday tasks, and was doing numerics. Running OOM would invariably mean two things: 1. I had a bug in my scripts, which typically meant I'd accidentally materialized a huge sparse matrix or some such, and thus 2. The system wouldn't go \"just a little\" OOM but rather consume memory an order of magnitude over the actual system's capacity. And it would not recover. In that scenario, the system would typically start swath-thrashing so hard that I'd just cold reboot. An OOM daemon fixed that and let me iron out my bugs. reply chronogram 7 hours agoparentprevOn my SBCs and VPSs I use a cache-heavy zram setup with LZ4 and `vm.page-cluster=0` being the most important changes to the default, and cache pressure and swappiness both to 200 off the top of my head, and things like only doing foreground IO when the background write buffer is full. This type of swapping is fast, and is easy on the CPU, and gives a lot of extra disk cache on this type of low performing storage. I disable disk schedulers because they haven't been necessary and would just add overhead. This means there's a lot of available RAM capacity, that there's a hefty read cache to avoid the SD card, that when there are disk writes on writable storage it can still read from it, and with the lack of clustering and the speed of decompression there's no swapping lag whenever a page needs to be swapped back. This swap early, swap often is the complete opposite of the OOM-prevention swapping you used to use on disks, which was slow and interrupted IO whereas LZ4 in RAM is fast and doesn't interrupt IO. I have been using this setup since 2022 and have not had any issues but I don't compile anything on those setups, though I see no reason why it would not be safer than compiling without zram at all. reply laweijfmvo 3 hours agorootparentcould you please write a ELI5 guide that I could follow on my tiny VPS? It's debian-based. Thanks! reply chronogram 1 hour agorootparentOf course! Just touching these files should be everything you need: /etc/sysctl.d/99-240716-vm.conf vm.dirty_background_ratio = 1 vm.dirty_ratio = 100 vm.page-cluster = 0 vm.swappiness = 200 vm.vfs_cache_pressure = 1 dirty_background_ratio = starts background writing when it's at least 1% of available mem; dirty_ratio = starts force writing when all avail (not total) ram is full; page-cluster = swap in only what's needed; swappiness = lower means swapping is expensive higher signals swapping is cheap; vfs_cache_pressure = lower keeps more dentries and inodes in memory. /etc/udev/rules.d/99-240716-ioschedulers.rules ACTION==\"add|change\", KERNEL==\"mmcblk[0-9]*|nvme[0-9]*|sd[a-z]*\", ATTR{queue/scheduler}=\"none\" Removes schedulers from typical local writeable storage. /etc/systemd/zram-generator.conf [zram0] zram-size = ram compression-algorithm = lz4 Might have to install systemd-zram-generator if it doesn't already exist. reply nucleardog 3 hours agoparentprevDon't know if it's \"legitimate\", but I've got 64GB of RAM. Allocating 16/32/64/128GB of NVME storage to swap is mostly just a waste of disk space for me. When I had swap enabled, it was constantly showing 0 used. (Not \"pretty much none\", literally \"0.0\".) Further, if I'm trying to use more than 64GB of RAM... I'm fine with things getting OOM killed. I don't know that I've ever had anything OOM-killed when something wasn't clearly misbehaving. (I count Chrome eating 50GB of RAM because I haven't closed any tabs all week as me clearly misbehaving for the purposes of this discussion.) And as far as zram... I guess same sorta arguments. I'm not running out of RAM, so why use up CPU cycles (and presumably battery power)? why use up brain cycles setting that up? Until I've maxed out my system's RAM, I'd rather just throw more RAM at it. reply ahartmetz 17 minutes agorootparentActually, zram is great! When an \"excessive swap event\" happens with zram, the system stays somewhat responsive, enough to let you kill the offender even from a graphical session. Without zram, I hope you were going for lunch break anyway... zram does basically nothing while your working set fits into memory, no performance penalty. reply tracker1 2 hours agorootparentprevSimilar opinion here on my destop. I was running 128gb, only exceeded 64gb a handful of times. That said, my RAM started causing lots of issues (thought my ssd was going bad). I only bought 64gb to replace it with as I felt the extra cost wasn't worth it to maintain, also likely to upgrade early-mid next year. reply laweijfmvo 3 hours agorootparentprevI have access to a build machine with 256GB of RAM and it suffers from OOM killing during certain builds unless I allocate like 2GB of swap reply nucleardog 1 hour agorootparentYeah I'm not trying to say \"64GB is enough for anyone!\" so much as \"I have way more RAM than I realistically need for my workloads.\" I've got all the things I need open right now and `free` shows I've got 40GB of RAM available. If your workloads involve using more RAM than you have you can... add more RAM, use swap/zram/etc, or just not do that thing. Absolutely makes sense to me to throw some swap into the mix. I'd probably do the same if it were an infrequent use case (otherwise preferring to just add more RAM). But also absolutely makes sense to me to not have any swap enabled on this machine right now. reply callalex 11 hours agoparentprevUnfortunately, there is a huge amount of cargo-culted cruft lying around in various Linux-on-workstation-wiki guide sites that hasn’t been modernized since the 2000’s. I don’t normally like to rant without providing a solution, but this is a problem I see my friends bump up against all the time when I tell them it’s finally the year of the Linux desktop. When something goes wrong they land on the same search results that I did when I was a child and the advice just never got updated. There used to be a time where swapping out meant moving cogs and wheels full of heavy rocks and RAM frequencies could be approximated by waving a stick until it made whistling noises. At that time suddenly dealing with memory swap made the system unusably unresponsive (I mean unusable, not just frustrating or irritating). Advice about disabling swap and zram came from that time for “resource constrained” systems. Unfortunately the meme will never die because the wikis and now regurgitated LLM drivel will just never let it go because nobody has gotten around to fixing it. reply FeepingCreature 10 hours agorootparentI have had systems completely die from hitting swap a few years ago. This is not a 2000s problem. reply a2tech 8 hours agorootparentI’ve learned to disable swap on my scientific computing machines where we’re working on giant datasets. It’s better for the machine to crash when it exhausts its RAM than go to swap. In my experience a machine is never going to recover when a workload pushes it into swapping because something has gone awry and that situation is not going to fix itself. reply oblio 8 hours agorootparentThere are many reasons this situation could happen outside of your context and swapping on SSDs is comparatively harmless compared to the old days of HDDs. Random example: swapping due to VM. You just stop VMs. reply FeepingCreature 4 hours agorootparentprevYeah on my current nvme linux systems, swap is just \"the phase where the ongoing leak makes the system kind of sluggish, shortly before the oom killer goes to work\". On 32GB, I ~never hit swap \"legitimately\". The most useful thing honestly has been a memory usage applet in the task bar. Memory leaks usually have a very clean and visible signature that provides a few seconds of warning to hit alt-tab-tab-ctrl-c. reply gmokki 6 hours agorootparentprevWas you kernel new enough to have MGLRU (kernel 6.1+). After that improvement one can be swapping constantly and the machine is still responsive. reply cameronh90 6 hours agorootparentprevThat's because when it comes to memory management on a Linux workstation, it is an unsolved problem. I've tried every piece of advice, distro and tool, and spent hundreds of hours trying to tune it over the years, and haven't been able to find a configuration that works as reliably as Windows or MacOS do out of the box. Linux memory management works well for servers where you can predict workloads, set resource limits, spec the right amount of memory, and, in most cases, don't care that much if an individual server crashes. For workstations, it either kicks in too early (and kills your IDE to punish you for opening too many tabs in Chrome) or it doesn't kick in at all, even when the system has become entirely unresponsive and you have to either mash magic sysrq or reboot. reply tetha 11 hours agorootparentprevI have similar experiences. I've been digging into this more over the years and my two conclusions are: (a) Linux memory management is overall rather complex and contains many rather subtle decisions that speed up systems. (b) Most recommendations you find about it are old, rubbish, or not nuanced enough. Like one thing I learned some time ago: swap-out in itself is not a bad thing. swap-out on it's own means the kernel is pushing memory pages it currently doesn't need to disk. It does this to prepare for a low-memory situation so if push comes to shove and it has to move pages to disk, some pages are already written to disk. And if the page is dirtied later on before needing to swap it back in, alright, we wasted some iops. Oh no. This occurs quite a bit for example for long-running processes with rarely used code paths, or with processes that do something once a day or so. swap-in on the other hand is nasty for the latency of processes. Which, again, may or may not be something to care about. If a once-a-day monitoring script starts a few milliseconds slower because data has to be swapped in... so what? It just becomes an issue if the system starts trashing and rapidly cycling pages in and out of swap. But in such a situation, the system would start randomly killing services without swap, which is also not entirely conductive to a properly working system. Especially because it'll start killing stuff using a lot of memory... which, on a server, tends to be the thing you want running. reply jorvi 10 hours agorootparentIt is not just advice. Default configs of most distros are set up for server-style work, even on workstation distros. So they’ll have CPU and IO schedulers optimized for throughput instead of latency, meaning a laggy desktop under load. The whole virtual memory system still runs things like it is on spinning rust (multiple page files in cache, low swappiness, etc). The only distro without this problem is Asahi. It’s bespoke for MacBooks, so it’s been optimized all the way down to the internal speakers(!). reply oblio 8 hours agorootparent> Default configs of most distros are set up for server-style work, even on workstation distros. So they’ll have CPU and IO schedulers optimized for throughput instead of latency, meaning a laggy desktop under load. The whole virtual memory system still runs things like it is on spinning rust (multiple page files in cache, low swappiness, etc). LOL. A Ken Colivas problem, circa 2008, still there :-))) reply yjftsjthsd-h 3 hours agorootparentprev> At that time suddenly dealing with memory swap made the system unusably unresponsive (I mean unusable, not just frustrating or irritating). I had a machine freeze this month because it was trying to zram swap, and have hit shades of the problem over the last few years on multiple machines running multiple distros. Sometimes running earlyoom helps, but at that point what's the point of swap? So no, this isn't out of date. reply TiredOfLife 10 hours agorootparentprev>At that time suddenly dealing with memory swap made the system unusably unresponsive Interestingly that was my experience on steam deck with its default 1gb swap. But after enabling both zram and larger ordinary swap (now also default setting for upcoming release) it became much more stable and responsive. reply tjoff 11 hours agorootparentprevThis is OS-agnostic. I love the old fact that you should have twice the amount of swap as your RAM size. I could rant but, no. Just don't. Today, don't buy a computer (regardless of size) with less than 32 GB of ram. Yes, this applies to fruity products as well. Part from making it a more enjoyable experience it will also extend the usable life of the computer immensely. (The weird crap about apple computers not needing as much RAM comes from iOS vs. android and is for different reasons, does not apply to real computers) reply hhh 10 hours agorootparentI don’t understand the sentiment. People should analyze what they actually use and what the need is. Sure, I bought a 64gb ram macbook because I like toys and don’t want to think about it, but for 80% of my workload 8gb is fine, and for my partner it’s fine for 100%. reply tjoff 9 hours agorootparent8 GB can, even in this electron world, barely work. But it won't tomorrow. Buying something with 8 GB today is wasting an otherwise perfectly good computer. And when your partner gets a new computer, for whatever reason, the old one can easily live on for many many years. But it's utility will be limited if it only has 8 GB of ram. The product in the article is only 8 years old but already stretching its usefulness for no good reason. reply sampo 8 hours agorootparentprev> I love the old fact that you should have twice the amount of swap as your RAM size. With a 32GB memory, 256GB ssd-disk laptop, it would be really weird to set up 64GB of the disk for swap. reply tjoff 8 hours agorootparentMaybe I was unclear, I despise that rule. (also, a computer with 32 GB and 256 GB disk is a very weird combination not quite fitting a typical general purpose computer) reply speed_spread 8 hours agorootparentprevSwapping in any form always sucks, period. The machine starts behaving strangely and does not tell you why because it's trying it's hardest to hide the fact that it ran out of resources. Experience has shown me over and over that you just want to feel the limits of the machine hard and fast so you can change what you're asking of it rather than thinking that there is some perf issue or weird bug. It's the idea that swap is somehow useful that's old. It's not, it never worked right for interactive systems. It's a mainframe thing that needs to die. reply andrewaylett 7 hours agorootparentBut where else are you going to put your anonymous pages when you don't want them for a while? Lots of the stuff you're using is backed by disk anyway -- and will be removed from RAM when there's any memory pressure, whether or not you have any swap. If you've got swap then the system can put anonymous pages in it, otherwise it'll need to evict named files more frequently. Unless you have enough RAM that you're literally never evicting anything from your page cache, in which case swap still doesn't hurt you. I'll absolutely agree that swapping out part of the working set is unwanted, but most swapping is benign and genuinely helps performance by allowing the system to retain more useful data in RAM. You don't want to get into a state where you're paging code in and out of RAM because there's nowhere to put data that's not being used. reply speed_spread 6 hours agorootparentThe whole concept of \"virtual memory\" has tainted systems design for decades. Treating RAM as a cache relies on the OS making guesses about what will be needed and what can be passivated without it actually knowing the application requirements. Except that compared to CPU level caching, the cost of page faults is big enough that performance degradation is not linear and breaks the user experience. The idea that a 4GB machine can do the same with as an 8GB one albeit slower is just not true. If you hit the swap, you feel it bad. I'll concede that Zram can work because the degradation is softer. But anything hitting the IO should be explicitly controlled by the app. Other random semi-related thoughts: - Rust having to define a new stdlib to be used in Linux kernel because of explicit allocation failure requirements. Why wasn't this possibility factored in from the beginning? - Most software nowadays just abstracts memory costs, partly explaining why a word processor that used to work fine with 64mb of RAM now takes a gig to get anything done. - Embedded development experience should be a requirement for any serious software engineer. reply steveklabnik 5 hours agorootparent> Rust having to define a new stdlib to be used in Linux kernel because of explicit allocation failure requirements. This is phrased in a way that’s a bit more extreme than in reality. Some new features are in the process of being added. > Why wasn't this possibility factored in from the beginning? So, there’s a few ways to talk about this. The first is… it was! Rust has three major layers to its standard library: core, alloc, and std. core, the lowest level, is a freestanding library. Alloc introduces memory allocations, and std introduces stuff that builds on top of OS functionality, like filesystems. What’s going on here is the kernel wanting to use the alloc layer in the kernel itself. So it’s naturally a bit higher level, and so needs some more work to fit in. Just normal software development stuff. Why didn’t alloc have fallible APIs? Because of Linux, ironically. The usual setup there means you won’t ever observe an allocation failure. So there hasn’t been a lot of pressure to add those APIs, as they’re less useful then you might imagine at first. And it also goes the other way; a lot of embedded systems do not allocate dynamically at all, so for stuff smaller or lower level than Linux, there hasn’t been any pressure there either. Also, I use the word “pressure” on purpose: like any open source project, work gets done when someone that needs a feature drives that feature forward. These things have been considered, for essentially forever, it’s just that finishing the work was never prioritized by anyone, because there’s an infinite amount of work to do and a finite number of people doing it. The Rust for Linux folks are now those people coming along and driving that upstream work. Which benefits all who come later. reply speed_spread 4 hours agorootparentOh hello, thanks for the clarification! Having enjoyed writing some embedded Rust, I'm familiar with the core/alloc/std split. IIUC you're saying that the user-space Linux malloc API itself does not provide a reliable way for the application to think about hard memory limits? Which would fuel my pet theory about \"infinite virtual memory\" being a significant factor in the ever growing software bloat. reply steveklabnik 3 hours agorootparent> I'm familiar with the core/alloc/std split. Ah, okay. So yeah, it's not a new standard library, it's \"things like Vec are adding .push_within_capacity() that's like push except it returns a Result and errors instead of reallocating\" more than \"bespoke standard library.\" > IIUC you're saying that the user-space Linux malloc API itself does not provide a reliable way for the application to think about hard memory limits? It's not the user-space malloc API, it's lower than that. See \" /proc/sys/vm/overcommit_memory\" in https://man7.org/linux/man-pages/man5/proc_sys_vm.5.html The default is \"heuristic overcommit.\" This page does a better job of explaining what that means: https://www.kernel.org/doc/Documentation/vm/overcommit-accou... So, unless you've specifically configured this to 2, there are many circumstances where you simply will not get an error from the kernel, even if you've requested more memory than available. What happens in this case is that your program will continue to run. At some point, it will access the bad allocation. The kernel will notice that there's not actually enough memory, and the \"oom killer\" will decide to kill a process to make space. It might be your process! It also might not be. Just depends. But this happens later, and asynchronously from your program. You cannot handle this error from inside your program. So even if these APIs existed, they wouldn't change the behavior: they would faithfully report what the kernel reported to them: that the allocation succeeded. reply andrewaylett 2 hours agorootparentprevMost of the time, you want to use RAM as a cache for the disk. I was trying to make the argument that sometimes that disk cache is more valuable than an under-used anonymous mapping. Steve has responded to your comment about Rust; to your other comments: Modern applications do a lot more than old ones. Even if you only use 20% of the features, you probably use a different 20% from any arbitrary other person. You also probably benefit from the OS being able to map everything into virtual memory but only actually load the bits you use :). And I strongly disagree with your stance on being \"serious\". I'm sure you don't mean to gate-keep, but we need to teach people where they are rather than giving them hoops to jump through. In my experience, some of the best software engineers have very little development background. And I say that as someone who implemented 64-bit integer support for the compiler and RTL for a DSP part back in the day. It's useful to have people around with a variety of backgrounds, it's not necessary for everyone to share any particular experience. reply izacus 5 hours agorootparentprevSwapping to zram is just fine and it will improve experience on many machines. reply speed_spread 4 hours agorootparentYeah, I agree. The memory-to-memory + modern CPU power makes it transparent or at least gives it a soft roll-off that IO based swap never achieves. But it's still a hack which too often is used by manufacturers to cheapen on RAM in machines. As the gas-powered engine people will say: \"there's no replacement for displacement\" (I wont push the analogy comparing zram to turbocharging but, you know, they both deal with \"compression\"...) reply speedgoose 12 hours agoparentprevI have swap, zram, and systemd-oomd enabled on my self managed kubernetes nodes. It helps dealing with JVM powered or memory leaking software at low cost. I am not sure why you would disable those in many scenarios. reply webdevver 8 hours agoparentprevcompiling clang on ubuntu 20.04, the link step used up all my ram and started swapping on the nvme. htop froze, so i hit ctrl-c, but nothing happened. no mouse movement, no ssh'ing in, just totally hard-locked. i ended up having to physically powercycle the machine. after that i turned off swap so that it killed the process rather than the machine (and remembered to pass -DLLVM_PARALLEL_LINK_JOBS=1) reply jcelerier 6 hours agorootparentUse easyoom or systemd-oomd reply burnte 2 hours agoparentprev> Disables Swap and Zram, gets OOM killed, surprised pikachu face On a machine with FOUR GIGABYTES OF RAM at that. reply moondev 11 hours agoparentprevFunny enough even Kubernetes supports running nodes with swap these days. My laptop has 64GB RAM and 1TB NVME, I run with swap off because I want all storage usable should ideally not be pressed for memory. I also have memory and storage allocation in my task bar to easily monitor the situation. reply treesknees 15 hours agoprevI know this doesn't fit the author's goals but I still think the trick with the surface line is using WSL instead of trying to run native Linux. Things have improved over time but when I was using my Surface Pro 4, Linux support was still pretty lacking. Maybe things will get better now that they're practically EOL with Win10 ending next year and no support for Win11. Unfortunately my SSD started to fail and battery life was poor enough that I ended up buying something else. The iFixit repair score reflects how much of a pain it would be to replace both of those. I do miss it sometimes, I really liked the 3:2 aspect ratio. reply vladvasiliu 12 hours agoparentI'm actually rather fine with what WSL can do. Hell, many of the tools I use run fine on Windows itself. But for me, the biggest shortcoming of this arrangement is having to put up with Windows' UX. I hate every single second I have to interact with this steaming pile of crap. reply xtracto 4 hours agorootparentThis so much. I've run Linux in all my desktop machines for 10+ years. When I was younger it was mainly due to ideology, but now I really don't care. Although most linux distros still have quirks (bluetooth issues, sleep/resume issues, no hibernation out of the box, high battery consumption, among a plethora a of other papercuts) I am sticking with it mainly because windows ux just sux so much. Every new computer I buy I give the installed windows a try and oh my god, it becomes crappier with every version. For me Windows 2000 was the best... 20 years ago. It's been downhill from there. reply vladvasiliu 3 hours agorootparent> Although most linux distros still have quirks (bluetooth issues, sleep/resume issues, no hibernation out of the box, high battery consumption, among a plethora a of other papercuts) I am sticking with it mainly because windows ux just sux so much. Heh, as usual, YMMV. My bluetooth headphones actually work reliably on Linux (with LDAC support!), while on Windows I usually have to fiddle with them for a few minutes until they start working. For some reason, whenever I reconnect them, Windows thinks it's a different \"sound card\". I sometimes can't control the volume in video calls, and they start at the max which is painful. Battery is much better on Linux (there not being anything doing god knows what with the cpu for no reason must help), and it actually stays asleep when I close it. Hibernation also worked well whenever I tried it, but I don't really have any use for it, so I can't tell for sure it's actually fully reliable. I didn't jump through any hoops for this other than an almost standard Arch install (\"almost\" because I use a fully encrypted drive with TPM+PIN unlock and secure boot with my own keys). reply tracker1 1 hour agorootparentOn linux, I have to switch my headphone mode when going in/out of web calls. It doesn't auto-switch to mono-mode when the mic is in use by an application. reply tracker1 2 hours agorootparentprevWin2K was pretty great, I do like aspects of the Win7+ app-bar though. reply tracker1 2 hours agorootparentprevLargely the same here... I've been split windows+wsl and mac the past few years for work, and while I feel WSL makes windows usable, I'd rather run Linux directly than either. Muscle memory on a Mac is often painful to deal with (us-ansi 104 keyboard). reply memsom 6 hours agoparentprevYou say no support for Win 11, but my Surface 2 Pro runs Windows 11 just fine. I don't think it even asked for the license key when I installed it. I probably used Rufus to make the image and turned off some of the more problematic aspects of Win 11, but it for sure installs with little or no problems. This is also a 4GiB model with 128GiB storage. It is very usable, despite having a processor equivalent to a pre-retina MacBook Air IIRC. reply rty32 4 hours agoparentprevWSL still has a ton of issues, slow IO and CPU usage, just to name two of them. Search \"WSL vmmem\" and you'll see what I mean. It is nowhere near ready for serious use if you are spending 90% of time doing development in a Linux environment. reply trelane 5 hours agoparentprevThe trick to running Linux on the Microsoft Surface line is to not. > Things have improved over time but when I was using my Surface Pro 4, Linux support was still pretty lacking I don't know why you would be surprised that Microsoft hardware fails to run Linux well. reply yjftsjthsd-h 15 hours agoparentprevDoes WSL handle multi-touch/gestures well? reply treesknees 15 hours agorootparentAs far as I know it only supports basic tapping/clicking for GUI applications and not multi-touch or gestures. https://github.com/microsoft/wslg/issues/737 reply goosedragons 9 hours agorootparentprevNo, it doesn't even handle Windows Snap. reply tstrimple 13 hours agoparentprevI guess that works because Linux power management is almost as bad as Windows so not a lot is lost. I'll never understand how people pick mobile devices with such short battery life. I further don't understand how literally no company other than Apple is able to deliver decent battery life. Even Microsoft's first party offerings which aren't infected by OEM bullshit are garbage in this regard. reply divbzero 13 hours agorootparent> how literally no company other than Apple is able to deliver decent battery life Apple’s full vertical integration from chip on up gives them an advantage here. For example, the doubling of video playback battery life from iPhone 12 Pro Max to iPhone 13 Pro Max [1] probably came from a new low-power display plus a new video decoder in the A15 Bionic chip. [1]: https://www.reddit.com/r/apple/comments/ppevl6/streamed_vide... reply Mashimo 10 hours agorootparentprev> I'll never understand how people pick mobile devices with such short battery life. Some people don't need all that much battery life. For me trains and buses, meeting rooms and at home there are outlets. It's a convenience thing when I want to sit at home on the couch without a cable attacked to my laptop. reply commandersaki 13 hours agorootparentprevThese new Snapdragon Elite X laptops compete on battery life. But I need to build for Linux/amd64 and I don't want to emulate so it's either Intel laptop or Apple Silicon laptop with Rosetta 2 for Linux. reply callalex 11 hours agorootparentDo they still compete on battery life while running a corporate email client, corporate chat client (Slack/Teams) and an editor (text/code/spreadsheet) in the background while completely idle? You’d think such simple idle workloads wouldn’t matter and yet I only find macOS to be capable of reigning in even these “light” background tasks without manual process suspension and killing. I don’t understand how we got to this point but it seems to be how my “real” world works. reply izacus 5 hours agorootparentYes. Even the Zen4 AMD Ryzen chips do. reply b3lvedere 11 hours agoprevI never liked the Surface series that much. It looks very nice, until you actually start working with them. Then they feel like a weird tablet with slow Windows on it. You can optimize it a little, but not much. Quite expensive as well and sometimes support is horribly slow. I gave my wife an old Lenovo Yoga 2 in 1. That thing works nice using it as a flipped tablet to watch Netflix, but here also the performance isn't great. Maybe just don't expect that much from these weird computers pretending to be tablets. reply jclardy 1 hour agoparentI just bought a Surface Pro 11 and love it. I've jumped from mac into the surface line every few years and I totally agree with you - the fans on the old models were spinning just by having a few chrome tabs opened. But...if you can live with Windows on Arm (Which has improved greatly in the past year) the SP11 has been great. Battery life is incredible. For me I was never looking to fully replace my actual laptop, but more to replace my iPad with something that is actually capable of doing any sort of development work if needed. The iPad is a much better tablet, hands down, but even just updating a static website on an iPad is an absolute chore and requires multiple apps to function. reply jonathanlydall 6 hours agoparentprevMy wife and I have been very happy with our Surface Pro 8 16Gb we bought last year running Windows 11 Pro. Mostly we use it with the keyboard attached. My wife needed a personal device because her company issued laptop was so locked down that she couldn't do a lot of basic personal admin stuff on it (for example online ordering of groceries). We considered an iPad, but in the end chose the Surface Pro because it allowed multiple user profiles. Windows Hello works super well that for either of us as we pick it up and look at it it's pretty much instantly on the correct profile and thanks to cloud sync with OneDrive and Microsoft Edge, I'm at home on either my own machine or the Surface. Only thing to mention is that the out of the box experience wasn't as good as I would have liked, especially compared to my experience with iPhones (despite liking iOS over Android, I have no love for macOS). Firstly, it wasn't running the latest feature update of Windows 11 and trying certain apps (like Instagram) off the Microsoft Store failed to install with a largely undescriptive error. Eventually I realized it wasn't running the very latest Windows 11 feature update which resolved the issue once installed. The other problem was that my user profile was laggy, but not my wife's. For example the Start Menu was very slow to come up. After a few days of this and no luck Googling the issue, I just formatted and re-installed Windows using Microsoft's official ISO download image. I normally do this with any new Windows PC I get, but assumed it wouldn't be essential for full on Microsoft hardware, but even though there was no obviously extra bundled rubbish software, something was clearly not 100%. reply the__alchemist 4 hours agoparentprevI think Surface Pros are very use-case dependent. It's perfect for mine, to the point I'm astounded there is no real competitor. Use case: While traveling or at coffee shops, be able to switch between full laptop mode (as long as you have a table; doesn't work on your lap), and use with the pen for taking notes, drawing things etc. While not as critical as pen use, being able to take the keyboard off quickly when reading or watching videos saves space, and lets me get the screen closer. reply makeitdouble 11 hours agoparentprevIt depends on your reference point, but IMHO there's no device right now that hits all the right point, so yes, Surface Pro is one of these flawwd machines. On the other side you'll have devices that feel really well built and graceful, but can actually do very little, or other ones fitting a very average vision of what a computer needs to do, and you'll be paying for additional devices to deal with the edge cases. reply ffsm8 11 hours agorootparentImagine an iPad that automatically switches to MacOSX if plugged into an external monitor, keyboard/mouse. It'd be glorious, not that I'd ever happen - for multiple reasons. One of which being that ipadOS is essentially iOS, so no overlap with MacOS reply jclardy 58 minutes agorootparentThis is my ideal setup. And I'd have it switch to macOS mode just with keyboard/mouse, so inside the magic keyboard it is just the most slick 11\" macbook air ever built. Pop it out and you are dropped back into iOS. I'd easily pay $3k for a top end version of such a device. I think this is Apple's main holdup - if the iPad can run macOS in this dual mode setup, the MacBook Air becomes pretty boring and a pretty bad deal. And they can no longer sell people two devices that accomplish the same task, only differentiated by one having a touchscreen. reply makeitdouble 11 hours agorootparentprevThe real big roadblock is Apple, but if the DMA forces them to let third party software, we could get a fully exposed subsystem opening the door to what users really ask for. Right now the joke is Windows XP emulation making it what it always needed to be, getting containerised/emulated Mac apps with decent Perfs from low level access would be a huge win. We could be close to your ideal, with the iPad still running, and a Mac instance pinned to the external screen. reply diffeomorphism 6 hours agorootparentprevSo iDEX? There have been multiple attempts at that from motorola, the nokia n900, sailfish, ubuntu touch, linux on DEX, DEX, maruOS, windows whatever, citrix,... Sounds nice in theory but people rarely actually use it. reply weberer 7 hours agorootparentprev>not that I'd ever happen In my eyes, Apple's transition to ARM on Macbooks looks like a stepping stone on that path. I wouldn't be surprised if they announced something like that for the iPad Pro eventually. reply jahnu 7 hours agorootparentprevAs a first step wouldn't it be amazing to have multiple user accounts on an iPad that doesn't require MDM. But such technological wonders are but a fantasy. reply WillAdams 5 hours agoparentprevI liked the first two iterations of the Surface Pro line, but it dropped off the radar for me when they went to NTrig digitizers. The Samsung Galaxy Book 12 was about the perfect computer for my needs: - decent-size high-resolution screen - small enough to fit in a bag for when traveling - Wacom EMR stylus --- I find this essential for drawing, sketching, annotating, and when I'm not inclined to connect a keyboard, writing Performance was quite good, but then Fall Creators Update crippled the stylus down to an 11th touch input which scrolled in web browsers and made selecting text quite awkward, as well as making using older applications quite difficult. I rolled back to 1703 twice and stayed there until circumstances forced a replacement --- the best option I could find was a Samsung Galaxy Book 3 Pro 360 --- I have to keep the Settings app open so I can toggle the stylus between acting/not acting like a mouse. It kills me that we had such great innovation in the tablet space once-upon-a-time (the ThinkPad was so-named because it was originally planned as a stylus computer) and my NCR-3125 (since donated to the Smithsonian) running PenPoint was one of my most-favourite computers and things seemed so promising w/ Windows 8... at least it's easy to write into text fields again. Hopefully the Lenovo Yogabook 9i will be popular enough that someone will make a dual-screen device using Wacom EMR. reply jbstack 8 hours agoparentprevIMO the advantage of the Surface is that it's one of the only tablets out there which is (a) reasonably priced for what you get, (b) has a x64 processor, and (c) can have Linux installed on it without too much difficulty. So if you want a Linux tablet, the Surface may end up being one of your only viable options. reply weberer 7 hours agorootparentThe Steam Deck is also a great option nowadays. Its a lot bulkier than a tablet, but I personally prefer it having a controller attached. Its biggest advantage is that it comes with Linux out of the box, so you don't have to go through the headache of installing an OS yourself and messing around with drivers. reply edude03 8 minutes agorootparentNot trying to be snarky, but I'd like to understand who you think the steam deck would appeal to? The original article, and the comment you're replying to seem to want pen input to do work/draw art, and like the tablet form factor (presumably for the large display), neither of which the steam deck provides. With \"only\" 16gb of ram, a relatively meagre 8 core 6800 series APU, and small screen it wouldn't make sense for most software developer workloads, and because of the attached controller(s) it's not super portable so not great for content consumption. Other than gamers, who likely don't even care that the steam deck runs linux (and in fact are hindered by it in some ways) is there a group you can imagine that would appreciate preinstalled linux so much that the steam deck makes sense over the surface pro or even a framework? reply INTPenis 7 hours agorootparentprevHow about the new Lenovo Tab? It's very reasonably priced, but I have no experience with it. All I can see right now is that it has a battery bump that people might object to. My goals with any device is to be as slim and as vanilla Android as possible, which means Samsung can go to hell. A friend said he liked the OnePlus tablet. reply jbstack 2 hours agorootparentNot sure which Lenovo Tab you mean specifically, but I just had a glance at a few now and none of them were x64. If we're talking about ARM tablets, there are an abundance of those. It's Linux-capable x64 tablets which are rarer. reply inhumantsar 6 hours agoparentprevI use a surface pro 9 for development, diagramming, note taking, media, light fusion 360 (on the iGPU), and gaming (with an egpu). it's a great machine with a few minor flaws, primarily battery life and cooling performance. at a go anywhere device, it's hard to beat. the price is obscene though, especially considering it's not OLED. I'm keen to try the arm version though, and the Minisforum V3 is interesting tho not much of an upgrade reply forgotacc240419 4 hours agoparentprevI'm a big fan of used Surface Go models. They tend to be for corporate use which seems to have a knock on effect of them being sold off very cheaply when people want rid and with seemingly minimal use. For use when traveling they're pretty exceptional, I even managed to get away doing a few days dev work on one while railing around Japan Have gotten multiple people a Surface Go 1 with 8GB ram and the keyboard and have never paid more than £80. Bizarre that they even made a 4GB model, let alone that they kept it until the second most recent version reply denysvitali 11 hours agoparentprevThe Surface Pro X (with Linux) runs pretty well. When I was running Windows on that, it worked nicely too reply maxboone 4 hours agorootparentHow's the peripherals support on Surface Linux? I've been wanting to switch to Linux on my Pro X SQ2 for a while due to the WSL2 support on it being terrible (might be fixed now [1]) but always thought that most stuff such as LTE, webcam and surface connector wouldn't work [2]. [1] https://github.com/microsoft/WSL/issues/11274#issuecomment-2... [2] https://github.com/linux-surface/surface-pro-x/issues/1#issu... reply justinclift 8 hours agoparentprev> ... with slow Windows on it. Was yours a 4GB ram model like the article author's? reply codeulike 10 hours agoprevNote this is the lowest spec Surface Pro 4, it had a low power Intel Core m3-6Y30 so that it could run without any active cooling, making it a 'true' tablet. Most of the 'proper' Surface Pro 4s had an i5 or i7 processor with active cooling (see https://en.wikipedia.org/wiki/Surface_Pro_4 ) and were roughly comparable in performance to other PC ultrabooks at the time. I've been using the Surface Pro line for about 10 years to do everything I need to do, they are pretty solid. reply keepamovin 9 hours agoparentI also use the surface for everything I need: I like it a lot and I’ve never had a problem with it. I don’t get the hate, nor why the inaccurate idea that you cannot run things on it persists. reply diffeomorphism 6 hours agorootparentA surface pro 9 with average laptop specs (16GB of ram, 1TB of storage), keyboard and pen costs 2000 + 140 + 80 and that is for the \"outdated\" model. At that price it should be exceptional not just good. That is not \"hate\" but disapointment. reply keepamovin 5 hours agorootparentI get the perspective of comparing based on price per unit of specs, but you're not just paying for that. That consideration may not be the main consideration in purchase for everybody. People make subjective assessments that are hard to quantify and compare across individuals. I guess if you find yourself being disappointed but you otherwise would have liked it, I suggest you may be looking at it the wrong way and missing out on what could work for you. For me, I think the weight and mobility are important too. I love the stylus and OS. I like the look and there's a bit of a f-you status, not in terms of the money involved which is not that much (especially considering what people drop on gaming rigs, Mac stuff, etc), but because it is a bit different. I think you're wrong that there's no hate towards Surface: you may not be picking up on it, there definitely is. Maybe people dislike that it's flashy and costly when they expect it should be utilitarian, so it kind of clashes with their expectations in a way that upsets them, and they dislike seeing other people enjoy what displeases themselves. I find it humorous that the same people may see another item, a Mac or whatever, in a different light, despite obvious similarities, and enjoy its flashy costliness. Heh! :) I encourage you to consider how the people who like and enjoy it see it. These topics have a way of turning people a bit mad, or at least creating conflict. So please let me turn the heat down a little bit with this olive branch compliment: hey, cool username, are you a mathematician?? :) reply diffeomorphism 5 hours agorootparentAgreed, people can sometimes get too much into minor things like laptop brands. Yes, I am a mathematician and have a few colleagues who are happily using their surfaces for notes and online teaching. I have seen some \"rivalry\" with people using iPads instead, but luckily no hate thus far. reply keepamovin 5 hours agorootparentRight? Exactly! It's a personal thing, I mean using it is not minor for me, it's super useful, but I don't see the point in challenging others about it. Just like different strokes for different folks, like diffeomorphisms haha :) Did that math joke work? I don't know as i'm not a mathematician. Lucky you haven't seen the hate, it's definitely out there. The refined world of academia must be too pleasant for it haha :) I also really like how you can just plug whatever keyboard in to it and use a desktop OS on tablet form factor, and it just works. You ever post your work on here? reply tiahura 5 hours agorootparentprevI’ve been all in on surface since the sp3. They had me at 3:2. reply vitorgrs 13 hours agoprevAbout the Fedora Gnome vs EndeuvourOS KDE... the issue here isn't Gnome. It's actually Fedora. In my testing on a similar hardware (also Core M3 and 4gb RAM), Arch-based distros was the best with low RAM. And I tried like, probably 50 distros since last year... Gnome on my HW with Arch, is as fast as KDE, and use less memory than KDE (in theory, I know RAM is a complicated subject). Why fedora is problematic on low end hardware? Because well, Fedora uses packagekit, which is a ram hog, and this is pretty known. Is not the only reason though, I believe there's some other defaults that make it slower than arch on my HW, like zswap vs zram. In my experience with weak CPU and low ram, was that zswap was actually the best choice. On such low RAM like 4gb, you'll really need a swap, you can't run from this. And zram won't be enough, in my experience. Which I guess is one of the reasons why Arch go very well here, as is one of the few distros right now that does a nice default for zswap. With Fedora, and most other distros, I get constant freezes when the RAM is full (which is pretty easy to do with 4gb), and this never happen on arch based distros. reply vbezhenar 4 hours agoparentPackagekit is not essential for Fedora. I always disable it (I think it uses systemd to run) and then using ordinary dnf to manage packets. reply tommodev 12 hours agoparentprevyeah, I took the Ubuntu / Fedora perf for granted as well. Recently switched back to Arch on a whim across one low-end machine, one high-end machine, and both run like lightning compared to Ubuntu 24.04 / Fedora 40. Expected the difference with Ubuntu as it packs more out of the box for the enterprise behaviours, not so much with Fedora. I've had no freezes, faster startup and shutdown, generally more responsive desktop etc. with Arch. Generally, though a rolling release it also has fewer moving parts as well - only having to deal with the main repo + flatpak (and a select few AUR pkgbuilds) is nice compared to Ubuntu where I had to layer deb repos + PPAs + flatpak + brew to get my tooling in place without having to script my own git-driven installers. One thing that tripped me up on any distro - the defaults for TLP (vs power profile daemon) seem hyper conservative wrt performance, probably by design. I never bothered digging in, just switched back to PPD, but it definitely prioritises power savings above all else. reply jillesvangurp 11 hours agorootparentI've been on Manjaro (arch based) for a few years now. I only ever installed it once and regularly update it. I've had some minor issues over the years but was able to resolve them. Mostly updates are without issues and when they aren't usually the fix is a google search away and pretty straightforward. And of course just about everything has been updated many times at this point. Latest kernel, gnome, etc. Nice when a bunch of Intel driver performance improvements landed a few years ago. I got them right away after that kernel got released and noticed a slight difference. A few months ago, I noticed a few more improvements with performance when a bunch of btrfs fixes landed. It's a good reason to stick with rolling releases. And since the Steam Deck uses Arch, getting Steam running on this was ridiculously easy. I'd use it professionally except I have a Mac Book Pro M1, which is really nice, and the Samsung laptop I run Manjaro on is not great, to put it mildly. I check once in a while but there are a lot of compromises out there in terms of different laptops but none of them really come close to Apple. They all do some things well only to drop the ball on other things. You can have a fast laptop but not a quiet one. You can have a nice screen but then the keyboard or touchpad is meh. Or the thing just weighs a ton. I think that was the point with the Surface Pro 4 in the article. It's a bit crap in terms of performance but the formfactor is nice-ish. Of course the touch support isn't great, which is no different with Manjaro. Except of course you do have access to all the latest attempts to address that. reply KTibow 15 hours agoprevI'm using a Surface Pro 7 to run Fedora, and my experience is mostly the same, although it runs a bit faster and without the ghost touches. The main annoyance I face is probably the fact that touch in Firefox occasionally breaks. reply jraph 13 hours agoparent> The main annoyance I face is probably the fact that touch in Firefox occasionally breaks. I have this on the two touchscreen laptops I use (HP and Lenovo). So I guess that's not hardware related. reply alisonatwork 15 hours agoparentprevCan you share a bit more about your experience here, in particular setting the system up? I have a bashed up Surface Pro 7 I took traveling with me. I upgraded my main PC to a Surface Pro 9 when I housed up and have been wondering what to do with with the Pro 7 because it's so battered from being thrown around and used outdoors for a year that it's not really sellable. I was thinking of turning it into a dedicated outdoor/travel computer, installing Fedora and Steam for point and click adventures, and maybe some MIDI/DJ controller software to play tunes. But I no longer have a keyboard for it, so I would need to be able to do the full Linux install by touchscreen. My other Surface is 100% bluetooth input devices to avoid cables, docks and dongles, so I could potentially pair one of those if it would help during install phase, but I wouldn't want it permanently paired. It seems like the advice online is generally \"if you don't have a USB keyboard, don't bother\", though. Do you think it's worth a shot? reply e12e 9 hours agorootparent> \"if you don't have a USB keyboard, don't bother\" I think you should be able to hardware reset without a keyboard - but in my experience - you really want console access when messing with bootloaders and alternative os'. Even if it is just to get to a point where on-screen/Bluetooth keyboard works... Often an USB Ethernet dongle can be useful as well (avoiding the catch-22 of needing network access to download wifi driver). reply KTibow 14 hours agorootparentprevI don't think anything could go wrong just booting into the live distro, but I did my setup with a keyboard and I don't know how it would work without. reply gnarbarian 14 hours agorootparentprevwouldn't you be able to plug in a USB keyboard? reply tstrimple 13 hours agorootparentI love that the Linux solution to a problem is just have this additional hardware to overcome it. I've run Linux as a desktop OS for years, so I'm not at all unfamiliar with all the hoops you have to jump through. Hoops that die-hard greybeards will deny exist because their personality is tied up in an operating system of all things. Surely 2024 is the year of the Linux desktop! reply gnarbarian 13 hours agorootparentwell you may only need the keyboard to install it right? there are thousands of USB keyboards everywhere. in the poorest most remote villages in Africa they probably have so many USB keyboards they make sandals out of them. reply alisonatwork 7 hours agorootparentExcept now you have sandals and perhaps still can't install Linux on a Surface. Seriously, though, it's kind of ridiculous to make a case that just because there is so much electronic waste already in the world, might as well create some more of it. I don't own a USB keyboard and haven't owned one for a decade or more. Because I exclusively use Surface. Imo Windows tablets are the true cyberdeck of the 21st century. Touchscreen devices should not require plugging in a keyboard to enter text or plugging in a mouse to click on things. The whole point of these devices is that they can work on their own, without peripherals. If you need to plug in to use them, then you might as well have just bought a laptop in the first place. reply gnarbarian 1 hour agorootparentyour expectations are unreasonable. I think that if you are expecting Linux to work perfectly when there is no keyboard on a notoriously Linux hostile proprietary device maybe you should step up and write the driver for it yourself. nobody is getting paid to specifically maintain the weird workarounds required to support the surface and your problem can be avoided by spending a nickel at the salvation army. it might even work without one! I know the latest Ubuntu detects a touchscreen on my Thinkpad and provides an onscreen keyboard by default. edit: I sincerely believe that the best way forward is for people who use Linux to vote with their wallet and buy products from the companies who are not actively hostile to it. I apply this logic to nearly ever device I buy and it results in less waste because I buy stuff I can actually fix! see this: https://github.com/ubernaut/maintainable-device-scorecard reply QuadrupleA 4 hours agoprevSounds like every experience of mine with desktop Linux. Excitement, initial success installing, days of esoteric troubleshooting, then disillusion and abandonment. reply the__alchemist 4 hours agoparentThis is an elegant, accurate description of my own experience. It's taken 20 years of regular attempts, but I've finally given up. (\"This new release of Ubuntu / this new distro will be the one!\") I use WSL if I want to compile a program for Linux users. reply tama_sala 1 hour agoparentprevThis is the most relatable comment in this thread. I had the same experience when I was using ros.org and moved to a new distro reply pizza234 11 hours agoprevI like the hybrids/detachable form factor, as a mean to merge tablets and laptops in a single device, but the whole software/hardware stack was not yet ready then, especially for those attempting to use Linux. List of problems: 1. x86(-64) power saving (sleep) capabilities are poor; tablets are expected to consume very little battery (ie. last weeks in standby mode), while x86 eats batteries for lunch (in S-whatever); this doesn't even take into account Windows arbitrarily deciding to wake up the machine while in a bag/backpack 2. Surface Pro's and Surface Book's (the latter was state of the art in terms of tablet hardware by the time of SB1 and SB2) had OK hardware support from Linux, but it took a long while, and it wasn't very stable (eg. wifi) 3. Hardware touch support itself is not enough; software needs to be good, and there was (likely, is) no document reader with good UX and annotation capabilities on Linux The solution for my use case was to dual boot, but points 1 and 2 were still a serious issue overall. Nowadays: 1. there are ARM tablets, with performant power saving (sleep) mode 2. WSL sidesteps Linux hardware compatibility issues (assuming one tolerates running Windows as underlying O/S), and avoids dual boot 3. WSL also allows using better document readers/annotators I fear WSL, but as a matter of fact, it's changing the landscape for Linux users. In theory, Ipad Pro's would be the best of both worlds, but they have a toy O/S by design. /shrug reply tallmed 4 hours agoprevI've been exclusively using linux on my tablets since 2007 with the thinkpad x61t and i've never had any of these problems. Although i use a completely different setup compared to the dude in the article. I would even say that on tablets gnu/linux actually provides a better experience. reply 1oooqooq 3 hours agoprevHe went back to windows and didn't mention to worst part of old windows PCs (and a surface4 is extremely ancient!) The wifi stack is entirely handled by the shaddy driver, which is usually just the reference implementation from the chip manufacturer stuck in time. That means your wifi stack will only support WPA2, and ancient cyphers with outdated parameters. No matter how up to date is your OS. reply Havoc 12 hours agoprevHad one issued for work. Absolutely hated working on it. Though that was probably more a mismatch with work requirement (heavy excel use + teams = deathly slow). A lighter OS plus lighter use could be fine. reply rtpg 12 hours agoparentIt's unfortunate because I found that the Surface Pro \"expensive\" models are gerat, but the lower end really can't handle much of any workflow (dreaded latency spikes) and it leads to loads of people just having a middling impression of a product that theoretically could capture a lot of the high end Windows market IMO reply Havoc 1 hour agorootparentYeah I liked the polish on them. I don’t recall which one it was. The mid range i5 I think. This was beginning of covid so IT just issued whatever they could get regardless of suitability. But yeah gigantic formula heavy excels kill even desktops let alone tablets. I had it swapped for a surface laptop. Forget what exactly but similar generation. That had active cooling which I suspect made the difference. Still slow but somewhat tolerable reply 1234554321a 12 hours agoprevI’ve had had SP4 i5 8gb ram version since 2017. It’s unreliable when running Windows, let alone Linux. It had constant touch screen issues which never fully went away even after replacing the screen. When I tried installing Linux I decided to switch back to Windows after a couple of months as both wifi and bluetooth had constant issues. The battery life is 2 or 3 hours at best even if you replace the battery with a new one. I’ll be replacing it with an M2 Macbook as that’ll be way more productive than to keep using this Surface. reply makeitdouble 11 hours agoparentWow, that's a 9 years old machine... I tried the SP7 refurbished 3 years ago, and it was already kinda slow and not great, though it gave a clear idea of what Microsoft did with the line. Switching to a 16G SP8 it's infinitely better. Still unreliable at times, but not that much if compared to an similar usage on an iPad pro. Battery life is workable (I get around 5~6 hours coding and compiling, usually have an external battery when out anyway). I assume if you're looking at an M2 giving up x86 compatibility isn't an issue. The most glaring issue on the Surface for me are too much reliance on Chrome/Edge for touch support, as Firefox is really not ready (mobile version is fine, don't know why desktop is so bad), and the port networking management in WSL2 where proxying VPNs can mess with wsl's port proxying. Otherwise I'mll be waiting for Apple to ever port macos to the iPad before reconsidering. reply csixty4 2 hours agoprevI'm mostly impressed this person has an SP4 that still has a working battery and no screen issues. reply dublin 36 minutes agoprevI'm very seriously thinking about one of these (or really, its successor) when I need to replace my computer again in the next year or two - it's already optimized for several Linux distros: https://us.starlabs.systems/pages/starlite# All I need now is a good replacement for OneNote that stores notes in an open format and supports pen input for sketching and handwritten note-taking... reply Valord 3 hours agoprevF40 works fine enough on my SP2. Only complaint is no deep sleep. I just shut down instead. Same with my framework 16. reply surfingdino 4 hours agoprevWhy do we expect Microsoft to support Linux? They are selling a commercial operating system and are not interested in supporting a free one. reply langsoul-com 12 hours agoprevToo bad that Linux support in laptops isn't the best. Especially for unique laptops like Asus zenbook 2024, the one with two screens. I want to get away from windows completely but their support for laptops is much better. reply vladvasiliu 11 hours agoparent> Especially for unique laptops But I'd say that's rather on the manufacturers, and not on Linux. They usually provide crappy drivers only for whatever version of windows they ship and call it a day. See all the junk that would stop working between major windows updates. Also, how does that laptop work? Don't the screens just show up as two displays, or do they do something special? > I want to get away from windows completely but their support for laptops is much better. YMMV as they say... Speaking of displays specifically, we just got some brand spanking new 5k screens at work. My full intel hp enterprise laptop can't use them at 5K under windows [0], but Linux supports them perfectly, even two at a time in addition to the integrated panel. Even 4k@60 had be borked on Windows on this PC for something like 2 years after I bought it. Worked OK since day one on Linux. --- [0] I actually did get it to work by installing the latest driver from the Intel website. But windows helpfully \"updated\" it back to the borked version after a reboot. reply crabmusket 12 hours agoparentprevHaving been using a Framework 13 running Fedora for ~2 weeks now... it's going great! I've plugged in a variety of external devices (monitors, a webcam) and they've all just worked. reply trelane 5 hours agoparentprevLinux support for laptops is fine. Getting an OS to work well on hardware requires a whole team of people called system integrators. Just slapping Linux on a Windows laptop and expecting it to work is naive. If you want a better Linux experience, you have to buy a Linux laptop, i.e. one that was designed (especially in firmware and chip selection) to run Linux, with support. You know, like you do for Windows. reply mg 11 hours agoprevOne reason I don't use tablet is that they all have glossy screens. And the new iPad with matte screen has a glossy frame around it. I tried it in a store and the glare around the otherwise nicely matte screen was uncomfortable. Does anyone here have experience how well matte screen protectors for tablets work? I see them mostly discussed for they haptic feel when drawing on the tablet. I wonder how well they work to have a good experience when coding on the tablet. reply shamefulkiwi 11 hours agoparentI’ve used one of the drawing/pencil screen protectors on my iPad for years for the same reason and it works great. It does make the screen feel a little less sharp/crisp but solves the glare problem for me. I’m sure they’ve gotten better over the years as well. reply mg 11 hours agorootparentYay, that sounds promising. Which protector do you use? reply artisanspam 3 hours agorootparentNot OP but I use this and like it. It gives a slight scratchy feel when I write on my iPad with the apple pencil and it removes all of the glare for when I'm reading. It's magnetic so you can remove it whenever you want to, but I never take it off. https://pen.tips/products/penmat reply throw0982 11 hours agoparentprevMinisforum has Ryzen based tablet, that has matte screen. https://store.minisforum.com/products/minisforum-v3 > The Minisforum V3 is a massive tablet PC with a 14-inch screen and a matte coating to reduce glare. https://gettotext.com/minisforum-v3-test-our-full-opinion/ reply specproc 13 hours agoprevIt took a couple of attempts, but I'm really enjoying EndeavourOS and i3 on a Surface _Laptop_ 4. It's the lightest, most portable and comfortable laptop I've had. reply slowhadoken 8 hours agoprevI’ve seriously been considering moving all my development to Linux. Microsoft is giving me the creeps lately. reply deng 10 hours agoprevSearching for a Linux tablet, I got a used Lenovo X1 Tablet Gen3. Linux works mostly fine, but as a tablet, it's mostly useless for reasons similar to the ones mentioned in TFA: * Battery life. 5-6 hours for moderate use simply does not cut it, especially since sleep drains battery like crazy because s0ix is not working properly, and debugging why is almost impossible. It's absolutely crazy how something that used to work just fine was deliberately botched because MS/Intel decided everything has to be a phone. * So because of this, you need to shut down the tablet if not used, which wouldn't be too bad, but as TFA says, you need a keyboard to enter the LUKS decryption password. * As a pure reading device, it's too heavy. Apart from that, Firefox is basically unusable because backspace does not work properly because of this bug: https://bugzilla.mozilla.org/show_bug.cgi?id=1832876 So in the end, while it's working, there's still a lot of janky behavior, which makes the experience just frustrating. reply trelane 5 hours agoprevInteresting that slapping Linux on a Windows computer doesn't work well. I wonder how OSX would fare. reply rowanG077 5 hours agoprevI ran nixos on a surface pro 5 for 3 years without issues. Even the stylus worked. It was one of my favourite \"laptops\" I had. The superbad thermals forced me of surface pro line. reply jauntywundrkind 15 hours agoprevThe Intel m3-6y30 used on this surface is just fantastically puny a core. 4.5W design spec, tdp down to 3.5 up to 7W. Tiny GPU. The 7200u on my Samsung Book 12 is a 15W configurable from 7-25W; so much more headroom. 0.8GHz vs 2.5GHz base clocks! Admittedly the 7200u is also a year newer but both are Sky Lake. https://ark.intel.com/content/www/us/en/ark/products/88198/i... One interesting thing happening in Linux now is bpf control over hid devices. Perhaps it might be possible to filter palm reads out at the kernel level with this, or eliminate ghost inputs. Hypothetically it should allow filtering the data arbitrarily. Classically I've used interception-tools in userland to do some light remapping, reading a device filtering and emitting as a virtual uhid, but this should be faster & slicker. https://www.phoronix.com/news/Linux-6.11-More-HID-BPF I really need to switch from my Samsung Book 12 to another copy (which I already own); mine's OLED is pretty cracked: remarkably invisible when looking straight on at it, but the touch went from sometimes not working to never working. I also want to try a pen with it. The 4GB of ram can be obnoxious. I feel like with a better nvme not sata SSD it wouldn't be such an issue but paging stuff out or in really makes the whole system lag badly sometimes, which is terrible. I also hella recommend hibernate. I didn't trust it for years, but one day ran low on power while suspended & watched systemd wake my system up, then hibernate it, and was shocked shocked shocked that it resumed latter & worked. It takes ~10s to boot up but being able to put a project aside, and come back weeks later & pickup where I left off is amazing. Use hibernate! I think you can configure it to hibernate after X amount of time sleeping. reply beacon294 12 hours agoparentI use hibernate on the Acer Swift X 2022 edition and it's incredibly nice. Sleep crashes the wireless card though... reply jauntywundrkind 1 hour agorootparentOn my last laptop, sleep also crashed the wireless card. But, if I restarted the system it would come back. Guess what hibernate does? It restarts the system. After many years of carrying around a USB wifi card, when systemd hibernated my system on me, it also made the wireless card start working again! Hibernating fixed my broken wifi. reply 1oooqooq 13 hours agoprev [–] Linux surface kernel is a meme. it's the same sort of hacks from teenage Android community to port binary blobs. if you're not familiar with that, just be glad. in summary, old unpatched kennels with weird binary code nobody cares to understand. reply denysvitali 11 hours agoparentIt's not, it follows the upstream releases and has a couple of patches for the Surface drivers (e.g: SAM) that will hopefully be upstreamed one day. They have something like ~50 commits on top of the release tag [1]. The main developer is doing an amazing job, and the fact that Linux runs on so many Surfaces devices, including the ARM ones (like my SPX) is just amazing. Linaro (Bjorn Andersson) helped quite a lot in the Linux on ARM environment, and qzed (Maximilian Luz) is doing all of the Surface reverse engineering and kernel driver in their own free time. Sorry, I had to downvote you because this is just disrespectful on the amount of work awesome people are doing on their free time, and you clearly have no clue on what the linux-surface project is about. [1]: https://github.com/linux-surface/kernel/commits/v6.9-surface... reply 1oooqooq 3 hours agorootparenti stand corrected. that was a huge progress from a couple years I last checked. reply solnyshok 13 hours agoparentprev [–] old? 6.9.x as of today https://github.com/linux-surface/linux-surface/releases reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author experimented with installing Linux on a Surface Pro 4, originally purchased in 2016, to explore its viability for various use cases like internet browsing, media consumption, and sketching.",
      "Installation of Linux was straightforward using EndeavourOS and Fedora Workstation 40, but tablet functionality was problematic, with issues like ghost touch, poor palm rejection, and significant battery drain during sleep.",
      "Due to these issues, the author reverted to Windows 10, finding it more functional for their needs, but may reconsider Linux after Windows 10's end of life next year."
    ],
    "commentSummary": [
      "A review of running Linux on a Surface Pro 4 has sparked significant discussion, highlighting various user experiences with Linux on different hardware.",
      "Many users compare their experiences with Linux on other devices, such as Dell XPS and ThinkPads, noting issues like sleep, power management, and hardware integration.",
      "The conversation underscores the challenges of achieving seamless hardware-software integration on Linux, similar to macOS on MacBooks, due to device driver development complexities."
    ],
    "points": 187,
    "commentCount": 199,
    "retryCount": 0,
    "time": 1721095383
  },
  {
    "id": 40972134,
    "title": "How do jewellers capture every last particle of gold dust? (2017)",
    "originLink": "https://www.ft.com/content/0512638c-b7c2-11e6-961e-a1acd97f622d",
    "originBody": "All that litters: Gary Williams of Mastermelt presides over the burning of commercial waste © Charlie Bibby How do jewellers capture every last particle of gold dust? on x (opens in a new window) How do jewellers capture every last particle of gold dust? on facebook (opens in a new window) How do jewellers capture every last particle of gold dust? on linkedin (opens in a new window) How do jewellers capture every last particle of gold dust? on whatsapp (opens in a new window) Save current progress 0% How do jewellers capture every last particle of gold dust? on x (opens in a new window) How do jewellers capture every last particle of gold dust? on facebook (opens in a new window) How do jewellers capture every last particle of gold dust? on linkedin (opens in a new window) How do jewellers capture every last particle of gold dust? on whatsapp (opens in a new window) Save Rachael Taylor January 16 2017 Jump to comments sectionPrint this page Unlock the Editor’s Digest for free Roula Khalaf, Editor of the FT, selects her favourite stories in this weekly newsletter. In a basement on London’s Hatton Garden, a small production team is heating a furnace to 2,000C — a temperature that will obliterate most materials placed within it. Each day, here at precious metals refiner Mastermelt, the furnace is fed a diet of objects collected from jewellers’ workshops — full bags from vacuum cleaners, used wet wipes, blunted sandpaper and even old carpets — in the hope that when the ashes are processed, thousands of pounds’ worth of gold, platinum and more will remain. “We always tell people, don’t throw anything away,” says Gary Williams, a director at Mastermelt, who the previous week paid out £17,000 to a jeweller who had been casually stockpiling the general detritus a workshop produces. “He’d just been saving all this stuff and didn’t realise what it was worth.” Mastermelt makes its money by taking a commission on the value of the scrap metal refined, as well as charging for processing. Part of Mr Williams’ job is to educate jewellers on the financial rewards of hoarding every waste-attracting object and sending the haul — known as a sweep — to a refiner to be melted down and the precious metals reclaimed. Most jewellers will be astute enough to collect hard scrap, also known as lemel, which includes filings caught in animal skins hung beneath workbenches for this very reason, or clippings from claws cut to size when setting a diamond in a ring. Once weighed, the value of this scrap can be easily ascertained, and Mastermelt has an app that can be downloaded to give jewellers access to live prices. The transformation of the residue into molten gold © Charlie Bibby But the precious metal they cannot see may bring in surprising amounts of money too. The grinding, filing and buffing required to create jewels by hand or machine send microscopic clouds of precious dust into the air; the dust can land anywhere and be easily transported as it sticks to shoes and clothing. “We went to a workshop in Birmingham and right at the front of the building was a big 4ft sq coconut mat,” says Mr Williams. “Every single person coming in and out of the building walked over that mat, and had been doing so for seven or eight years. “They didn’t think it was worth anything as it wasn’t in the workshop, but they let me take it away. We swept up the dirt and processed it and it came to quite a few thousand pounds.” Another surprise source was a pair of chair covers from a shop in Hatton Garden which were so choked with platinum dust that Mastermelt paid out £3,000 after processing them. Mr Williams has a vault of stories about reclamation and its valuable surprises, but most of them are kept inside the trade. “People don’t really like it being talked about as this is their Christmas bonus,” he says. “Whatever money you make from reclamation is cash flow for your business. It’s found money.” Timing is important in making sure a reclamation is as profitable as possible. First, a jeweller must ensure they have accumulated a large enough sweep to make a profit. Stuart Wibberley, sales director at Cooksongold, another refiner, says processing carries a flat fee of about £300 “whether you have 30 bags or 300”. (Mastermelt says its fees are variable, typically about £4 per kilo processed.) Then there are metal prices: Mr Williams says Mastermelt was “inundated” after the vote for Brexit in June, when the gold price spiked from $1,265 per oz the day before to $1,316 per oz the day after. *** The transformation of the residue into molten gold © Charlie Bibby Reclaiming precious metal waste has always been a cornerstone of a jewellery workshop’s routine, but the practice has rarely been as sophisticated as it is today. “It really was considered a bonus in the trade,” says Mark Brittain, whose family business, Brittains Marking, creates complex tools for the luxury jewellery industry. He ran the company, which was founded in 1850, from the 1970s until the early 2000s, when he retired from the jewellery trade. He now operates a commercial property business in Birmingham. Mr Brittain recalls old-fashioned sweet jars being filled with tiny coils of metal, a byproduct of engraving cufflinks. The owner of one particular London jeweller that Brittains Marking worked with in the 1980s kept all the engraving scraps that were not requested by customers — Mr Brittain says some clients were indeed savvy enough to ask for theirs back — and cashed it in once a year to fund a cruise aboard the QE2 luxury liner. This was during what some in the trade call “the Brylcreem days”. Jewellery workers were said to slather their hair with that pomade so that they could run their hands, covered in precious metal dust, through it, later retrieving the gold or silver. Mr Brittain has stories of such practices, and Winchester jeweller Jeremy France, whose twice-yearly sweeps at the workshop in his store yield between £6,000 and £8,000 a time, jokingly refers to an industry expression that “we don’t employ people with sweaty hands or turned-up trousers,” both ideal for accumulating valuable dust. Four flights above the jewellery boutique of David Morris on Bond Street, polisher Dave Wadeson, who has been in the business for 40 years, says he once saw floorboards glittering in a workshop when he was an apprentice. “When [the workshop] was taken over by Bentley & Skinner, they took up all the floorboards and the amount of lemel embedded in them was incredible — they found piles of fine grain around the edges,” he says. When Mr Williams worked at wedding ring manufacturer Brown & Newirth, the business moved workshops around 1990 and dismantled its five-storey wooden staircase, which was so impregnated with precious metal that it looked like the “glitter in a marble kitchen top”. It turned out to be worth tens of thousands of pounds — enough to pay for a new staircase and the workshop’s moving costs. And at Cooksongold, which claims to be the UK’s largest precious metals refiner, processing 25,000 jobs a year, Mr Wibberley recalls when a parquet floor in its own factory was ripped up and the precious metals embedded in the wood made it worth £20 per sq m. Large jobs like burning floorboards, carpets or benches tend to happen when a jeweller is moving premises or upgrading. There are less extreme options for the interim, such as specialist vacuuming services or sanding down the top layer of benches and processing the wood dust. The challenge is picking up the day-to-day waste, and it is an art that is being constantly refined — particularly by larger manufacturers with the most to gain. *** At Hockley Mint, sweepings are turned into gold nuggets © Edward Moss New technology being introduced to these factories is actually increasing waste, requiring even greater efforts to capture it. One British jewellery manufacturer, which did not wish to be named, has invested in sealed milling machines fitted with vacuums to capture the coils and swarf (sawdust-like chips of precious metal) they dispatch at high speeds. Each has a filter that can separate materials as thin as a micron — much narrower than a human hair. In Birmingham’s Jewellery Quarter is Hockley Mint, a British jewellery manufacturer with a turnover of £12m that produces up to 140,000 pieces a year. Gathering scrap and lemel at its 30,000 sq ft factory requires daily cleaning and collecting, as well as the use of localised vacuums. “One of the things we have put in place that is relatively new is that when workers wash their hands we save the paper towels they dry with,” says managing director Gary Wroe. “We got £2,000 back in a 12-month period just from the towels, and that goes straight on our bottom line.” Gary Wroe MBA Managing Director of Hockley Mint, Birmingham © Edward Moss Hockley Mint has also upgraded its windows so that blinds are now encased between panes of glass — their fabric panels were a magnet for precious metal dust — and it also has an on-site laundry to process workers’ clothes. Its biggest investment in precious metal reclamation to date has been an incinerator, which it installed 18 months ago, to melt, but not process, its sweeps. “If you send a bag of material out, you don’t know what’s coming back,” says Mr Wroe, who adds that the value of precious metals it is collecting has increased since taking over its own incinerating. *** His concern is one shared by many in the industry. Looking at the accumulation of sludge from settlement tanks placed beneath sinks, grimy mop heads and all manner of other dirt and waste collected in a sweep, it is impossible to estimate how much money has accrued, although there are ways to guess. Jewellers expect to lose metal during the production phase and so will plan for it. At David Morris, the target is a maximum loss of 5 per cent, although its craftsmen believe the working figure is 1 per cent or 2 per cent, providing a guide for what could be reclaimed. Stephen Redshaw hoovers up the filings for recycling at Hockley Mint, Birmingham. © Edward Moss We have sticky mats at the doors to get the dust off people’s shoes as they leave the building Gary Wroe Hockley Mint uses scales to give as accurate a forecast as possible. “We measure in and measure out, and keep records so we know approximately what we should get back,” says Mr Wroe. For others, guesstimates are based on experience, and this can lead to friction between jeweller and refiner when discrepancies arise. “Some people have expectations based on history but they haven’t got a complete understanding of what’s going on,” says Mr Williams. The gold price might have changed, he suggests, or the weight of material being processed could be similar but the contents disparate. “In this industry, trust is a big factor,” he says. But that trust goes both ways. Sweeps can take weeks to process and refiners will sometimes advance money based on the expected yield, which can give less scrupulous jewellers an opportunity to run a ruse. “I heard a story about a sweep where the processor took the job, gave the customer an advance based on history and when they looked in the sweep bag, the bottom of it was newspapers,” says Mr Williams. Cooksongold offers its clients advances at its trade counters in London and Birmingham. The business is part of the German Heimerle + Meule group and much of its refining is done at the group’s plants in Spain and Germany. This adds to the processing time, so it will do a pre-melt in London to ascertain what it is sending to Heimerle + Meule. Processing its own pre-melted metal would be the next stage in Hockley Mint’s reclamation journey, but it is one that Mr Wroe has yet to commit to. “That’s a different skill and we’d have to bring the expertise in, so we’d have to work out whether it would be worth it.” While Mr Wroe’s approach to reclamation is far more advanced than many other workshops, he is nonetheless resigned to the fact that he will never catch it all. “We have sticky mats at the doors to get the dust off people’s shoes as they leave the building, otherwise the streets of Hockley would be paved with gold,” he sighs. “And the council should definitely check the sewers.” Copyright The Financial Times Limited 2024. All rights reserved. Reuse this content (opens in new window) CommentsJump to comments section Follow the topics in this article Personal & Household Goods Add to myFT Retail & Consumer industry Add to myFT Gold Add to myFT Luxury goods Add to myFT Waste management & recycling Add to myFT Comments",
    "commentLink": "https://news.ycombinator.com/item?id=40972134",
    "commentBody": "How do jewellers capture every last particle of gold dust? (2017) (ft.com)164 points by EndXA 20 hours agohidepastfavorite128 comments 00N8 18 hours agoReminds me of my favorite story from the Manhattan project: The project needed massive amounts of wire for all the equipment, but copper was in short supply for the war effort. They ended up working out a deal with the Treasury Department to use silver instead, since it was an even better conductor & apparently more available at the time. Part of the deal involved making sure not to lose any silver & IIRC they managed to not only return all the borrowed silver, they even found some extra to return by tearing up the floors in all the mints, warehouses & workshops, to incinerate & reclaim the precious metal, just like in the article! reply jaggederest 17 hours agoparentAnother fun Manhattan fact: They needed a code name for plutonium, so they called it \"copper\", but what was a poor scientist or engineer who needed to use actual copper to do? The official code name for copper was \"Honest-to-God copper\". reply fbdab103 16 hours agorootparentI am intrigued at how much thought went into \"copper\". Was the thinking that everyone's eyes would glaze over at such a common material? My initial reaction would be to use a different rare element. However, a rare element might draw more scrutiny to the casual observer. Then again, the potential for confusion is incredibly high. Interesting spycraft. Supposedly the opsec at the Manhattan Project was so good, significant portions of the workforce had no idea on what they were laboring. Post war interviews thought the facility was all a sham, dedicated to nothing but medical testing. reply SnorkelTan 11 hours agorootparentMy dad’s relative (uncle I think?) ran the team that fabricated the detonators for the bombs. They weren’t told what the purpose was when they built them. They found out what the purpose was when they were used. reply TylerE 14 hours agorootparentprevCopper had the advantage of already being directed 100% at the war effort. reply bell-cot 9 hours agorootparentAnd almost all of that for bulk uses like shell casings and electrical wiring - vast supply chains which no competent enemy agent would waste time looking into. reply lostlogin 8 hours agorootparent> vast supply chains which no competent enemy agent would waste time looking into. Isn’t this exactly what should be looked into? Find weak point and hit them. Germanys ball bearing plants and oil refineries got targeted this way. reply rtkwe 2 hours agorootparentThere wasn't much of a Nazi spy presence in the US much less effective saboteur operations. One of the few people convicted for espionage had their conviction overturned because the information they passed was publicly available. The US was never under much direct threat, there were a smattering of attacks and raids on the West Coast but those didn't amount to much. Extreme distance was a better shield than any secrecy or military might, it's part of why the post war years were so good and continued for decades afterwards, the US was completely untouched and the rest of the (then) modern world was bombed to absolute smithereens. https://www.neh.gov/article/nazi-spies-america reply zeckalpha 6 hours agorootparentprevYou're thinking of the honest-to-god copper supply chain, not the \"copper\" supply chain. reply bell-cot 8 hours agorootparentprevIf the Axis had been dropping 1000+ tons of bombs on American industry every day, then maybe that would have been rational. Similar if the Axis had large-scale resistance forces operating in America, able to sustain at-scale acts of industrial sabotage. But the Axis already knew that America had a huge copper industry. With no way to affect that industry, at scale - long lists of American copper mines, refining facilities, factories, etc. were no more valuable to the Axis than collections of apple pie recipes. reply JoshTriplett 16 hours agorootparentprevThat sounds like the start of a story that ends up with wires made out of plutonium. reply drunkonvinyl 16 hours agorootparentprevNothing like H2GCu. It’s even just pennies on the dollar. reply thinkfaster 16 hours agorootparentprevAnd they wondered how the Soviets infiltrated the project so thoroughly. reply retrac 9 hours agoparentprevThere is a story to go with that story. The colonel responsible for the negotiations with the Treasury would later recall: > He explained the procedure for transferring the silver and asked, \"How much do you need?\" I replied, \"6000 tons.\" > \"How many troy ounces is that?\" he asked. In fact, I did not know how to convert troy ounces to tons, and neither did he. A little impatient, I responded, \"I don't know how many troy ounces we need, but I know I need 6000 tons - that is a definite quantity. What difference does it matter how we express the quantity?\" > He replied rather indignantly, \"Young man, you may think of silver in tons, but the Treasury will always think of silver in troy ounces.\" reply michaelmcdonald 6 hours agorootparentFor those wondering like me: 6000 metric tons of silver is approximately 192,904,200 troy ounces, and 6000 US tons of silver is approximately 175,000,000 troy ounces. reply mads 6 hours agorootparentThere is a US ton? My God... reply hansvm 5 hours agorootparentTo be perfectly fair, it was a little strange for the metric system to have a \"ton\" unit in the first place. Much like a foot or a cup, it's one of those units the metric system is trying to replace, but rather than use megagrams or something else perfectly sensible within the system they already created, they added to the confusion by defining yet another \"ton\" close enough to the historical units of the same name. reply stonemetal12 2 hours agorootparentThe SI unit of measure is the Kilogram, not the gram. Therefore they ought to be using the kilokilogram KKg, not the Mg. reply immibis 2 hours agorootparentJust because the definition is based on 1000 of the base unit doesn't mean the prefixes start being weird. reply rad_gruchalski 2 hours agorootparentprevthe SI unit for kilogram is kg and not Kg reply hwillis 2 hours agorootparentprevit's because \"megagram\" and \"milligram\" are a hassle. With the added bonus of mm/MM being used to abbreviate millions, I'm personally very grateful for tonnes. reply wtfmcgrill 5 hours agorootparentprevThere's the metric tonne 1000kg, the US short ton 2000lbs and the US long ton 2240lbs(1016kg) also known as imperial ton. I started calling the metric tonne a megagram because I got tired of trying to figure out if it was short, long or metric I was dealing with reply boringg 4 hours agorootparentThe bane of all engineers having to do unit conversions and asking clarifying questions around what type of ton/tonne. reply RichardCA 1 hour agorootparentprevThis is from a web site that's been around since the Web 1.0 era: https://www.ibiblio.org/units/dictT.html#ton reply dghf 5 hours agorootparentprevAnd an imperial ton, which is different again. reply lobsterthief 6 hours agorootparentprevYes, many of us Americans wish we would adopt the metric system. reply dghughes 5 hours agorootparentYou did. All your (USA) units use metric then are converted to US measurements. The US inch is officially 25.4mm exactly. https://www.nist.gov/pml/owm/si-units-length reply ddingus 2 hours agorootparentYup. And more things are SI units every day. I use them most of time now. We are getting there slowly. reply Gupie 6 hours agorootparentprev$30.59 per ounce (current) reply umanwizard 6 hours agorootparentprevIf anyone else was curious like me, 6000 US tons is 175 million troy ounces. reply mminer237 4 hours agorootparentAnd worth $5.4 billion dollars at today's prices. reply dredmorbius 18 hours agoparentprevDo you have a source on that? I'm finding \"The Manhattan Project: the Important Role Silver Played In the Building of the Atomic Bomb\"(2015). reply glompers 15 hours agorootparentHere's a more official source: https://www.osti.gov/opennet/manhattan-project-history/Event... Groves' deputy, Nichols, who was responsible for the loan, also told the story in more detail in 1987 memoir, \"The Road to Trinity.\" reply glompers 14 hours agorootparentDirectly related to recapture of gold -- and related to the Manhattan Project by Farm Hall [0] too -- is the tale of German objector Max von Laue's Nobel Prize medal, which was dissolved during WWII and recast out of solution afterward [1] [0] https://en.m.wikipedia.org/wiki/Max_von_Laue#Post-war [1] https://en.m.wikipedia.org/wiki/Max_von_Laue#Hidden_Nobel_pr... reply 00N8 17 hours agorootparentprevI think I first heard about it in a Scott Manley video - maybe this one, https://m.youtube.com/watch?v=JNT28WKAxgs - & read more on Wikipedia & possibly elsewhere reply pjd7 19 hours agoprevI did my jewellery trade in Australia (hence the correct spelling for me). We used to keep all our emery paper, old polishing wheels etc and send them off ever few years to be burnt & refined. When the building we were in got renovated some enterprising guys in another workshop ripped up their floor boards and their neighbouring empty suites and got all the precious metals out of the gaps between the floorboards. The building was 11 stories and was predominantly filled with small jewellery workshops with 2-5 people per business. And a lot of adjacent businesses (trade supplies, stone merchants etc). reply loudmax 6 hours agoparentWandering off topic here, but fans of Mad Max 2 (The Road Warrior) may remember Emil Minty as The Feral Kid. He only had a few other roles as a child actor, then went on to become a professional jeweler. According to one podcast, he was already fashioning necklaces out of the tabs from discarded cans on set. In the end, a far healthier career choice than many other child actors of the same era. reply ironmagma 15 hours agoparentprevBut do you pronounce it “jew-el-ry” or “jull-uh-ry”? reply dorkwood 6 hours agorootparentIn Australia, it's neither. It's jool-ry. reply euroderf 3 hours agorootparentAlso in western NY state. reply janderson215 14 hours agorootparentprevProbably jew-el-ery reply doctor_eval 18 hours agoparentprevCurious where that was? My partner was a jeweller in the Nicholas Building in Melbs. reply candeira 15 hours agorootparentA friend of mine had an art studio at the Nicholas Building, and I got to speak with a jeweller who told me that he still did a lot of bespoke work in wedding rings, especially for tradies who would otherwise wear down store-bought rings because they were solid gold and therefore softer. I don't remember the details, but he specialised in harder alloys that are nevertheless mostly gold, and therefore \"good as gold\" for a wedding ring. Would that be your partner? reply doctor_eval 15 hours agorootparentNot my partner, but I bet they knew each other, it was a great little community and she was there for the better part of 20 years. Great parties in the 00’s! reply pjd7 17 hours agorootparentprev250 Pitt St Sydney around 2002-2003. reply ortusdux 19 hours agoprevReminds me of the people that scavenge gold and gems off New York sidewalks https://www.forbes.com/sites/timworstall/2011/06/22/new-york... https://www.igi.org/digging-for-gold-in-new-yorks-sidewalks/ reply ainonsense44 10 hours agoparent> In March, 2022 video creator Klesh, who sells the paydirt he recovers from various areas, ... Person, who makes money off people believing they can find treasure, makes video about how easy it is to find treasure. \"Oh btw you can buy these utils for 25$ in my store.\" Media literacy says : trust level 2/10, most probably lies and marketing reply pregseahorses 18 hours agoprevSaw it in Karachi last year: a street containing exclusively gold workshops was blocked for traffic Sunday morning, guarded officially by the police, while the staff hired by the co-op swept every inch. Apparently this is a weekly routine. reply notJim 18 hours agoparentReminds me of the stories of people in NYC's diamond district making their living by looking for dropped gemstones and tiny bits of gold. https://nypost.com/2011/06/20/got-his-mined-in-the-gutter/ https://www.youtube.com/watch?v=TfrqUNFtM6A reply jaggederest 17 hours agorootparentThere's a Cody's Lab video where they refine highway dust for platinum group metals leaking from catalytic converters: https://www.youtube.com/watch?v=v5GPWJPLcHg reply kjkjadksj 18 hours agorootparentprevI'm wondering how these gemstones even make their way on the street. Are jewelry workshops really so messy and flippant with this product that it presumably gets caught up on people and just falls off their clothes when they leave from work? Is it from crappy stone settings falling off immediately on leaving the stores? Seems so strange to me how such a valuable product ends up dispersed in the environment like this. reply mtnGoat 18 hours agorootparentSome jewelers are dealing with things of great enough value that some gold dust or small stones doesn’t matter. Most jewelry value is not in the gold or gems, it’s in the eye of the beholder so to speak. So losing the actual ring is a much bigger loss than the gold/gem value. However, a guy on the street that isn’t dealing in six figure goods, places great value in that small gold amount. Most people would be amazed at how little, when not in a shop under bright lights, jewelry is actually worth. Let’s just say they aren’t selling gold, they are selling emotions and hype. And many customers get extremely angry when they go to resell and find out how little it’s worth. You’ll be lucky to get a tenth of what you paid for the stones. Source; my wife ran the biggest gold buying store in northern LA county for a few years. reply bruce511 17 hours agorootparentI second this, jewelry has very little value on the 2nd hand market. Gold, Silver etc have a daily published value, and you can expect a number close to that (allowing for margin). But \"precious stones\" are really not all that precious, or rare. When selling (to a dealer) you discover the margins they make- often upwards of 90% (as my father discovered selling jewelry he inherited.) To be fair, crafting takes significant labor and most old jewelry has to be melted down and recrafted. None of this negates the significance of one person giving another jewelry. That adds substantial sentimental value which is what makes them valuable at all. (It does make me smile though when movies use uncut diamonds as some sort of compact currency...) reply fbdab103 15 hours agorootparentSo, resale market on stones is trash. What is to stop an intrepid jeweler from purchasing grandma's/ex-spouse's/whatever's precious jewelry for 10 cents on the dollar, and reselling it back into circulation as \"new\"? Maybe with a perfunctory shaving off a face to make a \"new\" item. Or is this already regularly done (as I would assume)? reply schlauerfox 4 hours agorootparentFashion. I had my great grandmothers rings appraised for probate and got the \"these are nice heirlooms, utterly worthless for resale though, the stones are just an okay clarity and the cut is very out of fashion these days. Keep them as nice rememberances.\" They must have said the same thing thousands of times. reply mtnGoat 15 hours agorootparentprevYes that is regularly done. The jewelry buyers take them out and sell them in lots for very low cost to just about anyone willing to pay, those buyers then recycle the stones into settings or back into stores. The buyers are buying on pure wholesale value, whereas the end jeweler is selling for sentimental value. reply bruce511 10 hours agorootparentprevThat's exactly the point. It's what they do do. If you take 2nd hand jewelry to a jeweler you get about 10 cents on the dollar. Some gets resold as is (there's a (limited) market for vintage jewelry) but most is reworked. And of course that reworking takes significant time and skill. reply xeonmc 5 hours agorootparentprevYes. I can’t recommend enough the manga “Nanatsuya Shinobu's Jewelry Box” reply jachee 15 hours agorootparentprevAssay marks. reply teddyh 5 hours agorootparentprevRelevant link: Have You Ever Tried To Sell A Diamond? (1982)reply goldinquiry1 16 hours agorootparentprevI just posted an Ask HN. Would love to learn more about gold selling, in general, as I have a contact that is capable of sourcing me gold from Burkina Faso. reply coffeebeqn 16 hours agorootparentSounds like a great way to get scammed! Gold is already very liquid with a real time price on multiple metal exchanges across the globe. reply mtnGoat 14 hours agorootparentprevUnless you have a lot, you probably won’t get close to spot price. You might be able to find some local artisans that will pay better for small amounts then the big houses will. Gold in stamped form has the greatest value. Nuggets or mined gold, some buyers are afraid of and won’t pay until it’s melted. Gold teeth are worth the absolute least. reply ethagnawl 8 hours agorootparent> Gold teeth are worth the absolute least. That was an unexpected kicker. I suspect you or your spouse must have a good story about shady people trying to work that angle. reply eru 17 hours agorootparentprevYes, but any savings go straight to the bottom line. reply goldinquiry1 16 hours agorootparentRight – if spot price per oz is $2,418.95 and say hypothetically I am sourcing it for $1700/oz there seems to be a reasonable amount of margin there if you can find a buyer for it. reply owenmarshall 15 hours agorootparentAnyone offering a discount of over 2500 basis points to participate in what might be one of the most accessible markets across the world isn’t offering actual value, they’ve just found a good mark. reply fbdab103 15 hours agorootparentprevIt is a global market, why in the day of the internet, would someone cut you a 30% margin? There might be some hassle crossing international borders, but gold is not generally illegal where there is an outsized risk in moving it. reply mtnGoat 14 hours agorootparentThe margins mentioned are about in line with most “gold buyers” that have store fronts. 70% of spot is a decent starting point for “salvage” gold. If it’s in any form but rounds or bars, it’ll need to be melted down which has a cost. So it can be made into a form that can be resold closer to spot value. The margin are actually even better if you melt them down. it’s really hard to buy physical gold at spot, single ounces/grams go for a few points higher. reply testfoobar 15 hours agorootparentprevHow did she confirm that items she was buying contained the gold claimed? reply mtnGoat 14 hours agorootparentTesting tools, and a lot of knowledge. Knowing what it is and where it’s made informs a lot. Once in a while you’ll get someone that is scamming and had stuff made with extra thick plating. Most pieces they will cut in half, grind a bit off, drill into it, etc. to do an acid test to determine gold content, then weigh it. After a while she just kind of knew what to look out for. How people presented the goods, how they acted, etc. The most shocking part to me, was how many 80s-90s celebs would bring in gold to sell. I guess times get rough. reply cyberax 17 hours agorootparentprev> Are jewelry workshops really so messy and flippant with this product that it presumably gets caught up on people and just falls off their clothes when they leave from work? Boots, not clothes. Diamonds are so sharp that they easily get lodged in rubber soles. And then fall out when you walk. reply michaelt 5 hours agorootparentGrocery store cashiers and bank clerks often don't get to leave until they've accounted for every cent that should be in their till. One would think diamond merchants would take the time to be equally careful, if not moreso. reply BobAliceInATree 5 hours agorootparentMost of these businesses in the district are tiny, family-owned businesses, so there's a huge amount of implicit trust there. But also, these tiny stones really aren't worth much. What you're asking is like Home Depot making sure to account for every single nut & bolt so none are stolen or lost. It would cost far more in time & labor then what you'd get back in return. reply BobAliceInATree 18 hours agorootparentprevRubies for watches are synthetic and super cheap. And the small diamonds are still pretty cheap. He said about $100 per carat of small diamonds. It's not surprising that they're more casual with this inventory. reply gowld 16 hours agorootparentprevIt's not such a valuable product. It only supports one low-paying full-time job, and who knows how long before the streets have been cleaned out of past accumulation. > Over six days, he says, he collected enough gold for two sales totaling $819 on 47th Street. reply ainonsense44 10 hours agorootparentprevThat video is a guy selling a dream, to sell his own products to the viewer, who believe in the legitimacy of the findings shown in the video. I call marketing stunt. Most unlikely not a truthful representation of what to expect when doing it yourself. reply hilbert42 17 hours agoprevI wonder how much precious metal such as gold and semiprecious metal such as gallium and indium essentially disappears forever in the thousands of tons of electronic waste every year. Does anyone know the percentages recovered/lost? Right, some recovery does occur—gold from edge/contact connectors etc. but I'd venture it's only a small fraction of what is used annually. And what about LEDs and transistors? I wonder if anyone ever bothers to recover the gallium and indium from them or whether the amount used isn't worth the effort. reply foota 16 hours agoparentThere's discussion about mining landfills to recover these kinds of materials. When you rememeber we had to dig it all out of the ground, taking it from a dump seems pretty convenient! reply dehrmann 12 hours agorootparentDepending on the age of the landfill, you can find refined aluminum or iron at higher rates than are in ore. reply hilbert42 13 hours agorootparentprevRight, see my reply to ars. reply ars 16 hours agoparentprevIt mostly ends up in landfill. At some point we may be resource poor, and but energy rich and we'll use Plasma mass separation to separate each element out and reuse the valuable ones. Until then it's safely stored there. reply hilbert42 14 hours agorootparentNo doubt most valuable elements do end up in landfill and most will ultimately be recovered, but we still need to have a good handle of what's actually lost or doesn't make it there, and or how much leaches out before recovery. (Here we don't seem to have decent figures, if anyone knows of any authoritative references please post them.) Hopefully, as you suggest, we will eventually be energy rich and can afford mass separation techniques to recover these elements. Nevertheless, unless some very cleaver as yet uninvented techniques are used then the amount of energy involved would likely be enormous (but I'm almost certain such techniques will be available in the foreseeable future). Incidentally, for the same reason, I'm not overly worried about the necessity for having inordinately long-term storage for nuclear waste (hundreds of thousands of years), as in an energy-rich world there'd be enough energy to enable the use of transmutation techniques (along with fast breeders, etc.) to ensure these dangerous byproducts are 'burnt' to harmless materials. Basically, whilst nuclear waste is a big problem it's a comparatively short-term one. That said, we're doing a pretty poor job of repurifying recycled materials now and the reasons are multifold. I'll give an example I've come across but there are hundreds more. Batteries of any kind should never be thrown away because of the valuable materials they contain. To my knowledge, with the exception of lead-acid batteries, an unknown amount of used battery material is recycled annually, but the effectiveness of what is actually recycled is limited due (it seems†) to the difficulty of repurifying said materials. For example, recycled reagents and other components, depolarizers such as manganese dioxide, are (often?) insufficiently pure to ensure a battery's long-term storage life. Instead of say an alkaline cell having a nominal storage life of about six years, impure components contain unwanted ionic/conductive materials that lead to a much increased self-discharge rate that shortens shelf life (I've seen such cells become discharged in only about one third the time of those with well-purified materials). No doubt higher levels of purification would be achieved if more energy were inputted into re-refining these materials. That said, this re-refining problem isn't just limited to batteries but is intrinsic to many recycling processes. Probably the best known and most problematic is that of separating used plastics together with their cracking/depolymerization. Again, it's almost certain these problems would be eliminated if enough cheap energy were available. __ † Obviously, repurifying recycled materials is different to their original refining from ores etc. as repurifying processes would be required to remove unwanted materials that were never present in the original refining process. I am unclear about what this involves and or the extent of its deployment as there seems precious little information about it in the public domain. reply mk_stjames 2 hours agoprev> Mr Wibberley recalls when a parquet floor in its own factory was ripped up and the precious metals embedded in the wood made it worth £20 per sq m. I find this interesting, as nicely re-claimed wood flooring itself can actually fetch about that price per sq meter these days. reply greyface- 19 hours agoprevHere's a series of videos showing the recovery and refining process: https://www.youtube.com/watch?v=ePEwr-VxqXE https://www.youtube.com/watch?v=WKGhmt7jgMg reply sumtechguy 3 hours agoparentI watch this channel a lot. He has a few where he is refining electronic waste. He usually says that is not really worth it due to the time it takes him to do it and small amounts he gets and can get easily lost in the different stages of extraction. Most of the stuff he does is estate sales and melting down that. reply pianoben 18 hours agoparentprevI love this guy's channel! It's interesting, unpretentious, and he has such a wealth of chemical and metallurgical knowledge. The reactions and processes he shows are amazing. reply COGlory 18 hours agoprevMy father (makes fake teeth) rips up his carpet every decade and has it burned and the metal dust in it melted down. Usually gets $10k-$15k. reply 7thpower 18 hours agoparentI would just get a Roomba, bling it out, and then destroy it when it ripped one of my charger cables out of the wall. Or just get sheet vinyl or something. reply eru 17 hours agorootparentA carpet is probably better at holding on to particles as people walk over it. A vinyl sheet will just lose it to people's boots. 10k$ over ten years is also not that much per day. So he might be happy to make the trade-off for having a nice carpet. reply 7thpower 16 hours agorootparentThat’s fair. I was just surprised to hear he’d be using carpet in the first place. I’m sure he knows what he’s doing, not questioning him. reply PhasmaFelis 14 hours agorootparentCarpet is probably the best thing for it, since the dust will work its way down in and stay there instead of sticking to your shoes and winding up all over the city. reply thaumasiotes 11 hours agorootparentprev> 10k$ over ten years is also not that much per day. It would be about $2.75 per day, an insignificant amount, but it's much worse than that because you earn it all at once at the end of the ten-year period. So you're making (almost) $2.75 a day toward the end of the period, and a lot less than that toward the beginning. reply southernplaces7 5 hours agorootparentReally though, it's more a sort of diligence bonus on top of the regular earnings he makes from his teeth manufacturing business. Looked at that way, it's pretty impressive quickly obtain an otherwise lost 10-15K per decade as opposed to just losing money on carpet removal once a decade. And all of it for, what? a few hours of work? reply COGlory 3 hours agorootparentYeah, it's partly a tradeoff for convenience. The carpet helps capture dust which makes everything more breathable in the lab as well. I'm sure bigger labs have more sophisticated facilities, but he's just one guy in his attic. reply gnicholas 14 hours agoprev> Hockley Mint has also upgraded its windows so that blinds are now encased between panes of glass — their fabric panels were a magnet for precious metal dust — and it also has an on-site laundry to process workers’ clothes. Hilarious — I guess big tech companies weren't the first to offer employees on-site laundry after all! reply GarnetFloride 13 hours agoparentHospitals used to have on-site laundry, for community sanitation and hygiene reasons. of course that was done away with for profit reasons. reply nielsbot 19 hours agoprevInteresting term of the trade in the article: \"lemel\". (Metal filings) Wiktionary: From Middle English lymail, from Anglo-Norman limaille, from Latin limare, a form of limo (“to file”); see further there. reply interroboink 17 hours agoprevReminds me a little of the man who \"mines\" gold and precious gems from the sidewalks in NYC: https://nypost.com/2011/06/20/got-his-mined-in-the-gutter/ reply surfingdino 4 hours agoprevBaird & Co. do the same: \"“At the end of the year all of the filters are collected together and burned,” Baird says. “Everything is ‘deep cleaned’ and burned, all of the filters and all of the doormats both inside the refinery and throughout the office.” Last year the company retrieved £15,000 worth of gold from the deep clean.\" https://www.theguardian.com/business/2017/dec/26/the-pots-of... reply thenickdude 11 hours agoprevReminds me of Cody's Lab refining platinum from roadside dust from the highway: (due to catalytic converters) https://www.youtube.com/watch?v=v5GPWJPLcHg reply JackMorgan 19 hours agoprevFascinating. Now I wonder why jewelers don't always just work in sealed containers with vacuums like what is used for sand blasting. I wonder now how much gold dust gets accumulated in the lungs of goldsmiths. I wonder if they take organs to check for sweeps. reply utensil4778 19 hours agoparentBecause the work is remarkably intricate. It requires you to get real close and personal with the work, usually with magnification. It requires complete and unimpeded dexterity of your fingers, so bulky gloves are absolutely not an option. Depending on the work, it may also require frequent trips to the hearth for torch work. You really don't want to use an oxy/propane torch in a sealed glovebox. In short, it's too much hassle and makes the work more difficult and much slower. reply nanomonkey 15 hours agoparentprevMore modern jewelry manufacturers are moving towards this. There are laser welders with glove boxes and microscopes that are auto shielding, also cnc milling machines in completely enclosed environments where the lubricants that are sprayed on the milling ends are filtered for metal. I was a jeweler for a couple years and the common practice was to have carpet and a sticky trap at the door. The carpet was torn up every few years and used to throw a company vacation. reply pjd7 18 hours agoparentprevThey work with sulphuric acid, oxy/acetylene/propane torches. Some processes give off cyanide fumes. Ultrasonic baths with ammonia based solutions to clean polishing compounds off etc. You need outside air ventilation. There is lots of mechanical suction for things like polishing to capture the waste material for post processing. Most will wear a leather apron for heat / burn protection and capturing fine dust/dirt from polishing compounds. I suppose you could destroy that eventually in a giant smelter. reply nelsondev 15 hours agoparentprevSounds like you are ready to open a crematorium that specializes in former jewelers. reply foxylad 15 hours agorootparentOr anyone with gold teeth. reply fest 8 hours agoprevI found it interesting that CNC machines aimed at precious metal processing have an optional access control system for swarf/dust collection bins- presumably so that the technicians operating the machine don't steal the \"waste\" material. reply simonjgreen 11 hours agoprevPiece of advice I’ve given people having jewellery resized for years, is if you are having something resized down then the jeweller should be paying you. A surprising number of people forget the majority of most jewellery value is the raw material. reply rhplus 10 hours agoparentA surprising number of people forget the majority of most jewellery value is the raw material. Scrap value, yes, purchase price, no :) The quick sanity test is to ask why big name jewelers can sell the same style ring at all sizes for the same price, despite perhaps 25% difference in mass. The majority of the retail sales price is not the precious metal value. reply blackbaze 18 hours agoprevIngenious! Why not. Beats getting it out of the ground. Possibly good for the environment as for a given gold demand less needs to be mined. reply whycome 17 hours agoprevhttps://www.reuters.com/article/world/india/gold-from-the-gu... reply itishappy 18 hours agoprevMy colleague told me a story just last week about his father's old job at Kodak working in silver reclamation. Same story as this article, they chuck everything into the furnace. They go so far as to filter the wastewater from employee showers. reply ajb 11 hours agoprevWonder if that is the last industrial processing still done in central London? If you don't count university labs. reply sundvor 13 hours agoprevWow, it sounds like the fine particles are going everywhere in the shops. This made me wonder what the health benefits of having lungs of gold might be. Remains to be seen, perhaps? reply schlauerfox 1 hour agoparentGenerally Gold (aurium) is not bioreactive, it's hardly reactive at all hence it's lustre in the wild since it doesn't tarnish or bond easily. Goldschlager has gold flake in it for human consumption. It's generally considered safe to consume for this reason. The quantity to make an LD50 of inhaled gold would be considerable I imagine, but have no data. reply CrispyKerosene 18 hours agoprevThis is why if you ever get jewellery repaired or resized, ask for the scrap to be returned. Some less than reputable places will try to off-handedly say it was discarded. They don't lose anything. reply kjkjadksj 18 hours agoparentSeems like this is only netting you $8 or so. https://old.reddit.com/r/jewelry/comments/vno1to/question_re... reply xcv123 18 hours agoparentprevThat sounds miserable and desperate. A couple of dollars worth of scrap shavings. What will you do with that? Just let them have it. reply mtnGoat 18 hours agorootparentYea it’s like people that bring in their dead ancestors teeth, no one wants to buy an old tooth and certainly not one with $5 worth of gold in it. reply joelfried 4 hours agorootparentI'll pay you $5 for gold teeth and make a tidy profit. This article from 2019[1] said $40-92 per crown. Multiply by 1.6 based on the way gold has appreciated since 2019[2] and that's $65-147 or so per tooth. [1] https://thegoldcenter.com/how-much-gold-is-in-a-dental-crown... [2] https://www.macrotrends.net/1333/historical-gold-prices-100-... reply EE84M3i 11 hours agoprevHow do the taxes work for this? reply WalterBright 17 hours agoprevUse a magnet! Edit: oops, never mind reply cancerhacker 16 hours agoparentIn the north east, in spring, we’d use magnets to collect iron filings in the street from the snow plows. It was just a thing we’d do, no financial interest. We’d keep our haul in 35mm film canisters (as was the style at the time!) reply askvictor 17 hours agoparentprevIt's a little more complicated than that - gold is a little bit magnetic, and heat or light can increase this magnetism reply bayouborne 5 hours agorootparentSo does that mean it's possible to develop a magnetic 'figure-print' for gold? reply eru 17 hours agoparentprevMagnets only really work for iron. reply DonHopkins 8 hours agoprev [–] So prepare yourself for the bloody mayhem and unholy carnage of Joshua Logan's \"Paint Your Wagon\"! https://www.youtube.com/watch?v=VM5-xFenaZI https://en.wikipedia.org/wiki/Paint_Your_Wagon_(film) https://archive.org/details/paint-your-wagon-western-comedy-... (Money shot at 1:44:30!) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Gary Williams of Mastermelt manages the burning of commercial waste to reclaim precious metals, with jewellers collecting every particle of gold dust from their workshops for processing.",
      "The process involves heating materials to 2,000C to recover metals like gold and platinum, with advanced techniques and technology aiding in capturing more waste.",
      "Trust between jewellers and refiners is crucial, and the industry continually refines methods to maximize the recovery of precious metals."
    ],
    "commentSummary": [
      "Jewellers employ meticulous methods to capture every particle of gold dust, including upgrading windows, on-site laundry, and tearing up floors.",
      "These practices are reminiscent of historical efforts, such as the Manhattan Project's silver reclamation from floors and equipment.",
      "Jewellers in cities like Sydney and New York recover gold from workshop floors and sidewalks, emphasizing the industry's resourcefulness and minimal material loss."
    ],
    "points": 164,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1721083704
  },
  {
    "id": 40970621,
    "title": "The Open Collaboration Tools",
    "originLink": "https://www.typefox.io/blog/open-collaboration-tools-announcement/",
    "originBody": "Jul 10th 2024 Announcing the Open Collaboration Tools Dr. Miro Spönemann Mark Sujew We are thrilled to unveil our latest open source project: Open Collaboration Tools. Itʼs a collection of libraries and tools for live-sharing of IDE contents, designed to boost remote teamwork with open technologies. The basic idea is simple: one person starts a collaboration session as host and invites others to join. The IDE extension distributes the contents of the hostʼs workspace and highlights text selections and cursor positions of other participants. In parallel, they get together in their favorite meeting or chat app for immediate discussion. All participants see what the others are looking at and and what changes they propose in real-time. This way of remote collaboration reduces confusion and maximizes productivity. In its first public release, Open Collaboration Tools is available with the following components: A VS Code Extension available on Open VSX and the VS Code Marketplace An extension for Eclipse Theia A server application for handling authentication and collaboration sessions, with a public instance available at api.open-collab.tools An open and extensible protocol with TypeScript libraries facilitating its integration in different environments End-to-end encryption and extensible user authentication built-in right from the start In addition, an integration with Monaco Editor will be available soon. This unprecedented feature is particularly exciting because it enables fully customizable live collaboration between users of a web application or even between a website and a local IDE. Finally youʼll be able to join a live-sharing session started from a full-fledged development environment, simply by opening a website link! The initial version of Open Collaboration Tools focuses on plain text documents: source code of any programming language, Markdown documents, scripting languages, domain-specific languages, configuration formats, HTML and other markup languages, etc. But the story doesnʼt end here – we are going to add support for other kinds of content creation such as Jupyter Notebooks, diagram editors, and more. The underlying protocol and infrastructure is adaptible to numerous scenarios where real-time collaborative viewing and editing can be helpful. Motivation for new collaboration tools In our daily work with customers, we have seen the need for collaborative editing inside of IDEs over and over again. While there are a few products out there aiming to provide collaborative editing experiences for their users, none of them were quite right for our customers. Let’s take a look at an exemplary use case: You are an engineer at a microchip design company. You’re using a textual programming language to design chips. To improve productivity, you also have the option to use a graphical editor in your IDE to modify chip design files. Given that these designs are highly sensitive, your company doesn’t want any information about them leaving the company network. Now, you want to do some pair-programming with a coworker. What kind of tool can you choose to facilitate that? To our dismay, nothing available fits that bill. What we need is a solution that: Can be plugged into any popular IDE, ideally even support cross-IDE use cases. Is designed to be extensible. Adding custom editors beyond just text can be done with reasonable effort. Runs on premise to ensure absolute data security. Does not bind you to a single vendor. A permissive license would be best to keep control of your tools. Looking at the 3 most popular collaborative editing tools, we will find that at least one of those requirements isn’t supported: Product IDEs Extensibility On Premise Licensing VS Live Share Visual Studio Products Via VS Code Extension Mechanism ❌ Proprietary Microsoft License1 CodeTogether VS Code, Theia, JetBrains, Eclipse IDE ❌ ✔ Commercial License Duckly VS Code, Theia, JetBrains ❌ ❌ Commercial License 1 The Microsoft proprietary license allows usage free of charge, but restricts the extension to run only in officially distributed Microsoft products (i.e. no VSCodium, Coder, Gitpod or Theia). Faced with this issue, we are now taking matters into our own hands. This is why the Open Collaboration Tools project is designed with a few core principles in mind: No vendor lock-in: Allow to potentially support any IDE and even your own web application. Absolute extensibility: The underlying protocol can be extended to enable collaboration for any kind of editor. Run wherever you like: Run the server infrastructure on any system, be it locally, on premise or in the cloud. Open Source Licensing: The whole project is licensed under the MIT License and can be used by anyone, in any way they like. The Open Collaboration Protocol The technology to facilitate all of the above is the Open Collaboration Protocol. This protocol specifies how the IDE is supposed to authenticate with a collaboration server, how to create or join a session, and what kind of data is exchanged between users within each session. On a technical level, this is done via a centralized peer-to-peer mechanism. Any user within a session can send an arbitrary notification or request to any other user. Additionally, a broadcasting mechanism allows to send a message to all other users. From a network perspective, each user only communicates to a central server. It is used as a message broker that sends the messages to their respective targets. As a consequence, the server is completely oblivious about the actual content of a message – and we can use that to our advantage. This kind of brokering mechanism enables limitless extensibility of the protocol. As long as the two communicating clients understand each other’s messages, they can work together. For example, if I build an IDE with a built-in graphical editor, I just need to additionally send messages to synchronize edits across multiple users and handle any incoming synchronization messages. The protocol already contains built-in support for basic text editors, with more to come soon! The big picture The new Open Collaboration Tools project fits perfectly into our overall strategy: to provide highly customizable tools and frameworks under permissive open source licenses. TypeFox supports businesses to adapt the open source software to their needs and make the best possible use of it to maximize their productivity. In the beginning, we mentioned that a public instance of Open Collaboration Tools is now available at api.open-collab.tools. TypeFox offers this service with the intent to demonstrate the capabilities of our new project and to support open source communities with it. However, we recommend all companies who wish to adopt this new technology to deploy their own instance of it, secured with their existing access restrictions. Get in touch with us if you’re interested about support for the customization, deployment and usage of collaboration servers. We are committed to invest further into the Open Collaboration Tools and grow the number and depth of its integrations over time. In the meantime, please check out the new website at www.open-collab.tools and help us spread the word! About the Authors Dr. Miro Spönemann Miro joined TypeFox as a software engineer right after the company was established. Five years later he stepped up as a co-leader and is now eager to shape the future direction and strategy. Miro earned a PhD (Dr.-Ing.) at the University of Kiel and is constantly pursuing innovation about engineering tools. Mark Sujew Mark is the driving force behind a lot of TypeFox’s open-source engagement. He leads the development of the Eclipse Langium and Theia IDE projects. Away from his day job, he enjoys bartending and music, is an avid Dungeons & Dragons player, and works as a computer science lecturer at a Hamburg University. Read more about this topic May 27th 2024 Article Mark Sujew Native Notebook support for Eclipse Theia Mark talks about the road of how contributors get large features into established open source projects, such as Eclipse Theia. read the article Jan 3rd 2024 Article Markus Rudolph Add views to a Langium-powered VS Code extension Markus gives a simple introduction about webviews in VS Code and how to interact with Langium. read the article Dec 13th 2023 Article Dennis Hübner Enhancing communication between extensions and webviews using VS Code Messenger Dennis introduces the VS Code Messenger library and explains how to use it for a better intercommunication between VS Code extension and its webviews. read the article LOAD MORE",
    "commentLink": "https://news.ycombinator.com/item?id=40970621",
    "commentBody": "The Open Collaboration Tools (typefox.io)160 points by todsacerdoti 23 hours agohidepastfavorite14 comments nixosbestos 22 hours agoWow, this feels pretty big. As a openvscode-server user (maybe switching to Eclipse Theia), the lack of ability to remote collab with \"Microsoft VSCode\" users was always a bit annoying. The TypeFox folks continue to impress, and I feel like this hints that there is collaboration between them and Theia folks. I'm curious if openvscode-server will end up slowly getting replaced by Theia, if Theia really lives up to its promises. reply lucideer 2 hours agoparent> I feel like this hints that there is collaboration between them and Theia folks The Typefox homepage says: > Software tools we have created: Sprotty, Langium, Theia reply seltzered_ 16 hours agoparentprev> \"TypeFox folks continue to impress, and I feel like this hints that there is collaboration between them and Theia folks.\" There's already been collaboration for years? Mark Sujew (one of the article coauthors) is a pretty active contributor to Eclipse Theia AFAIK. From https://github.com/msujew : \"I act as a project lead for the Langium and Theia projects at the Eclipse Foundation. I work at TypeFox.\" reply theultdev 1 hour agoprevVery interested in this. I see there's a YJS provider in the repo. I assume this is the Monaco support TFA alluded to. Is this provider ready for use right now or are there limitations? Also how well does this scale for one to many sessions? reply nerdponx 21 hours agoprevWould it be theoretically possible to get this working in one of the \"highly highly\" editors like Vim/Neovim, Emacs, etc? reply IHLayman 21 hours agoparentBy the looks of it, the announcement webpage also breaks Tridactyl unnecessarily, so I doubt they are interested in supporting *vim... Joking aside, I would be shocked if someone doesn't quickly write a plugin for at least Neovim to support this. reply worik 16 hours agoparentprev> \"highly highly\" editors What does that mean? Having used Emacs for decades I get the sentiment, but unfamiliar with the idiom reply nerdponx 7 hours agorootparentStupid autocorrect thing, I meant \"highly programmable\". reply scubbo 18 hours agoprev> one person starts a collaboration session as host and invites others to join. The IDE extension distributes the contents of the hostʼs workspace and highlights text selections and cursor positions of other participants. In parallel, they get together in their favorite meeting or chat app for immediate discussion. I really hope I'm not being the \"Dropbox is just rsync\" guy, here, but...how is this preferable to the host screen-sharing via that same meeting or chat app? reply thundermuffin 17 hours agoparentThe largest difference is everyone who joins would be editing the code like it's a Google Doc instead of having a read-only view of the host's screen; you still end up having to join some kind of voice chat (unless maybe there's an IDE VOIP integration I don't know about in some of these tools!). I haven't used this specifically, so I can't say how well it pulls it off. However, from the few times I've used VS Live Share, it has been helpful being able to join into someone's coding session and pair with their code directly. Both of us being able to highlight and tweak lines as we're talking did wonders to remove the \"add x, no, sorry, not there, the next line/parameter/\" or \"show me y, now z, wait, go back to y\" song and dance routines. It's not necessarily a game changer for the way every person or team works, but as someone who has an easier time forming a mental model of everything happening when I'm \"in\" the code, there have been times I think it's saved a lot of back-and-forth compared to a traditional screen share. reply scubbo 17 hours agorootparentOh! It was not clear to me at first that this was non-read-only, though in hindsight I should have concluded as much from the phrase \"see what the others are looking at and and [sic, whoops!] what changes they propose in real-time\". Makes sense, thank you! reply yjftsjthsd-h 16 hours agoparentprev> how is this preferable to the host screen-sharing via that same meeting or chat app? Higher quality at lower bandwidth, should handle different screen sizes more elegantly reply dflock 22 hours agoprevThis is an open source verfsion of MS Live Share for VS Code, by the look of it. reply benatkin 22 hours agoprev [–] > This way of remote collaboration reduces confusion and maximizes productivity. [citation needed] reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Open Collaboration Tools is an open-source project aimed at enhancing remote teamwork through live-sharing of Integrated Development Environment (IDE) contents.",
      "Key features include extensions for VS Code and Eclipse Theia, a server application for authentication and collaboration, and an open protocol with end-to-end encryption.",
      "The project addresses gaps in existing tools by offering no vendor lock-in, absolute extensibility, flexible deployment options, and open-source licensing under the MIT License."
    ],
    "commentSummary": [
      "Open Collaboration Tools by TypeFox is gaining attention, especially among users of openvscode-server and Eclipse Theia, due to its potential for improved remote collaboration.",
      "TypeFox's tools, including Sprotty, Langium, and Theia, are being highlighted, with speculation that Theia might replace openvscode-server if it fulfills its promises.",
      "The discussion includes interest in the scalability and readiness of a YJS provider for collaborative editing, and comparisons to other collaboration methods like screen sharing and VS Live Share."
    ],
    "points": 160,
    "commentCount": 14,
    "retryCount": 0,
    "time": 1721070733
  },
  {
    "id": 40974154,
    "title": "OpenAI illegally barred staff from airing safety risks, whistleblowers say",
    "originLink": "https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/",
    "originBody": "OpenAI CEO Sam Altman speaking with reporters on Capitol Hill last year. (Elizabeth Frantz/For The Washington Post) Listen 6 min Share Comment Add to your saved stories Save OpenAI whistleblowers have filed a complaint with the Securities and Exchange Commission alleging the artificial intelligence company illegally prohibited its employees from warning regulators about the grave risks its technology may pose to humanity, calling for an investigation. The whistleblowers said OpenAI issued its employees overly restrictive employment, severance and nondisclosure agreements that could have led to penalties against workers who raised concerns about OpenAI to federal regulators, according to a seven-page letter sent to the SEC commissioner earlier this month that referred to the formal complaint. The letter was obtained exclusively by The Washington Post. OpenAI made staff sign employee agreements that required them to waive their federal rights to whistleblower compensation, the letter said. These agreements also required OpenAI staff to get prior consent from the company if they wished to disclose information to federal authorities. OpenAI did not create exemptions in its employee nondisparagement clauses for disclosing securities violations to the SEC. Advertisement Story continues below advertisement These overly broad agreements violated long-standing federal laws and regulations meant to protect whistleblowers who wish to reveal damning information about their company anonymously and without fear of retaliation, the letter said. 💻 Follow Technology Follow “These contracts sent a message that ‘we don’t want … employees talking to federal regulators,’” said one of the whistleblowers, who spoke on the condition of anonymity for fear of retaliation. “I don’t think that AI companies can build technology that is safe and in the public interest if they shield themselves from scrutiny and dissent.” GET CAUGHT UP Stories to keep you informed Dissenting Republican delegates sign protest of Trump platform At issue is the treatment of abortion in the new document. The “minority report” criticizes the 2024 platform’s lack of a “human life amendment.” The platform focused less on abortion and dropped the call for a 20-week abortion ban. Sparkle Summary is AI-generated, newsroom-reviewed. U.S., Germany foiled Russian plot to assassinate CEO of arms manufacturer, officials say Rheinmetall is a major German arms manufacturer and a key supplier for Ukraine. Russia has ramped up efforts to undermine Western support for Ukraine. NATO and the U.S. have intensified their focus on thwarting Russian subversion efforts. Sparkle Summary is AI-generated, newsroom-reviewed. Family of teen who died after ‘One Chip Challenge’ sues snack company The lawsuit accuses Paqui of aggressively marketing the extremely spicy chip to children. An autopsy said the cause of death was cardiac arrest and also cited heart conditions. The company discontinued the product and expressed condolences to the family. Sparkle Summary is AI-generated, newsroom-reviewed. La Niña is coming. Here’s how it could change the weather. The pattern could have a cooling effect on global heat. It’s also likely to increase Atlantic hurricane activity this fall. But there’s uncertainty over its impact amid a period of record temperatures. Sparkle Summary is AI-generated, newsroom-reviewed. Do landlords have to provide AC? Here’s what renters should know. Air conditioning rights for renters vary by lease and location. Some states require AC, but most don’t include it as an essential service. Repair timelines are often vague, depending on the state. Filing a written complaints is best. Sparkle Summary is AI-generated, newsroom-reviewed. In a statement, Hannah Wong, a spokesperson for OpenAI said, “Our whistleblower policy protects employees’ rights to make protected disclosures. Additionally, we believe rigorous debate about this technology is essential and have already made important changes to our departure process to remove nondisparagement terms.” Advertisement Story continues below advertisement The whistleblowers’ letter comes amid concerns that OpenAI, which started as a nonprofit with an altruistic mission, is putting profit before safety in creating its technology. The Post reported Friday that OpenAI rushed out its latest AI model that fuels ChatGPT to meet a May release date set by company leaders, despite employee concerns that the company “failed” to live up to its own security testing protocol that it said would keep its AI safe from catastrophic harms, like teaching users to build bioweapons or helping hackers develop new kinds of cyberattacks. In a statement, OpenAI spokesperson Lindsey Held said the company “didn’t cut corners on our safety process, though we recognize the launch was stressful for our teams.” Tech companies’ strict confidentiality agreements have long vexed workers and regulators. During the #MeToo movement and national protests in response to the murder of George Floyd, workers warned that such legal agreements limited their ability to report sexual misconduct or racial discrimination. Regulators, meanwhile, have worried that the terms muzzle tech employees who could alert them to misconduct in the opaque tech sector, especially amid allegations that companies’ algorithms promote content that undermines elections, public health and children’s safety. The rapid advance of artificial intelligence sharpened policymakers’ concerns about the power of the tech industry, prompting a flood of calls for regulation. In the United States, AI companies are largely operating in a legal vacuum, and policymakers say they cannot effectively create new AI policies without the help of whistleblowers, who can help explain the potential threats posed by the fast-moving technology. Advertisement Story continues below advertisement “OpenAI’s policies and practices appear to cast a chilling effect on whistleblowers’ right to speak up and receive due compensation for their protected disclosures,” said Sen. Chuck Grassley (R-Iowa) in a statement to The Post. “In order for the federal government to stay one step ahead of artificial intelligence, OpenAI’s nondisclosure agreements must change.” A copy of the letter, addressed to SEC chairman Gary Gensler, was sent to Congress. The Post obtained the whistleblower letter from Grassley’s office. The official complaints referred to in the letter were submitted to the SEC in June. Stephen Kohn, a lawyer representing the OpenAI whistleblowers, said the SEC has responded to the complaint. Story continues below advertisement It could not be determined whether the SEC has launched an investigation. The agency declined to comment. Advertisement The SEC must take “swift and aggressive” steps to address these illegal agreements, the letter says, as they might be relevant to the wider AI sector and could violate the October White House executive order that demands AI companies develop the technology safely. “At the heart of any such enforcement effort is the recognition that insiders … must be free to report concerns to federal authorities,” the letter said. “Employees are in the best position to detect and warn against the types of dangers referenced in the Executive Order and are also in the best position to help ensure that AI benefits humanity, instead of having the opposite effect.” Story continues below advertisement These agreements threatened employees with criminal prosecutions if they reported violations of law to federal authorities under trade secret laws, Kohn said. Employees were instructed to keep company information confidential and threatened with “severe sanctions” without recognition of their right to report such information to the government, he said. Advertisement “In terms of oversight of AI, we are at the very beginning,” Kohn said. “We need employees to step forward, and we need OpenAI to be open.” The SEC should require OpenAI to produce every employment, severance and investor agreement that contains nondisclosure clauses to ensure they don’t violate federal laws, the letter said. Federal regulators should require OpenAI to notify all past and current employees of the violations the company committed as well as notify them that they have the right to confidentially and anonymously report any violations of law to the SEC. The SEC should issue fines to OpenAI for “each improper agreement” under SEC law and direct OpenAI to cure the “chilling effect” of its past practices, according to the whistleblowers letter. Story continues below advertisement Multiple tech employees, including Facebook whistleblower Frances Haugen, have filed complaints with the SEC, which established a whistleblower program in the wake of the 2008 financial crisis. Advertisement Fighting back against Silicon Valley’s use of NDAs to “monopolize information” has been a protracted battle, said Chris Baker, a San Francisco lawyer. He won a $27 million settlement for Google employees in December against claims that the tech giant used onerous confidentiality agreements to block whistleblowing and other protected activity. Now tech companies are increasingly fighting back with clever ways to deter speech, he said. “Employers have learned that the cost of leaks is sometimes way greater than the cost of litigation, so they are willing to take the risk,” Baker said. Share Comments Sign up",
    "commentLink": "https://news.ycombinator.com/item?id=40974154",
    "commentBody": "OpenAI illegally barred staff from airing safety risks, whistleblowers say (washingtonpost.com)143 points by helsinkiandrew 12 hours agohidepastfavorite138 comments helsinkiandrew 12 hours agohttps://archive.ph/F29dT infecto 7 hours agoprevThese agreements will most likely be ironed out. What I am more interested in is the constant pressure on \"safety risks\" without anything that feels tangible to me so far. I believe there is indeed risk using models that could be biased but I don't believe that is a new problem. I still don't think we are at risk from a runaway AGI that is going to destroy us. reply dartos 6 hours agoparentI agree that I don’t think we’re in danger of runaway AGI, but tbf, in the early 2000s, there were a lot of safety risks to social media we couldn’t see yet. “I don’t think a live message board is going to destroy society” could’ve been something someone said about Facebook or Twitter. The danger there wasn’t in the tech, it was in the societal impact and consolidation of information control. Both, imo, are danger areas for AI as well. reply jrochkind1 4 hours agorootparentI think there probably are safety risks that I worry about too; but I don't think this article mentions them or is clear on what safety risks employees were worried about, if any, despite having the phrase \"safety risks\" in the headline? reply slibhb 5 hours agorootparentprevThe whole \"social media is dangerous\" is a moral panic. There's just no good evidence for it. reply notaustinpowers 4 hours agorootparentFacebook literally was foundational in ethnic cleansing within Myanmar, I'd be careful calling this a \"moral panic\". Oh, also, there is good evidence for it, including Facebook themselves! https://about.fb.com/news/2018/11/myanmar-hria/ reply slibhb 4 hours agorootparentYour link is word salad as far as I can tell. It certainly doesn't say \"Facebook was foundational in ethnic cleansing in Myanmar\". Social media companies are often scapegoats when things go wrong in the world. From teen mental illness (Jon Haidt) to Trump getting elected (Cambridge Analytica). And now apparently genocide in Myanmar. It's all just nonsense. reply notaustinpowers 3 hours agorootparentYour lack of reading comprehension is not my problem. reply slibhb 2 hours agorootparentOh no the social medias are killing the Burmese! Quick, someone arrest Marc Z! To The Hague with him! reply notaustinpowers 1 hour agorootparentokay...? reply exe34 5 hours agorootparentprevdid Facebook destroy society? reply Devasta 5 hours agorootparentIt's facilitated multiple ethnic cleansings and it's main user base is now mentally ill anger addicts, primarily because of their algorithms; it'd be weird if you didn't think so. reply LanceH 4 hours agorootparentUnironically telling someone what they should be thinking while complaining about social media's influence. reply exe34 2 hours agorootparentI had an uncle like that, he would tell you how intelligent he thought you were if you agreed with him, but the moment you disagree, you're clearly not as intelligent as he originally thought. reply klodolph 5 hours agorootparentprevThis line of discussion is the “Let’s talk about whether the word ‘destroy’ is correct here” which is tangential at best. reply theyinwhy 5 hours agorootparentprevThey moved fast and broke things. reply baq 5 hours agorootparentprevit technically didn't. enjoy being technically right. reply exe34 2 hours agorootparentah so it was hyperbole. got it thanks! reply homarp 5 hours agorootparentprevhttps://about.fb.com/news/2018/11/myanmar-hria/ https://systemicjustice.org/article/facebook-and-genocide-ho... reply Lutger 5 hours agorootparentprevyes reply edgyquant 5 hours agorootparentprevAnyone asking this question is doing so in bad faith. Do better reply exe34 2 hours agorootparenti take it poisoning the well has worked in your favour in the past, and now it's your go-to? reply slibhb 5 hours agoparentprev> What I am more interested in is the constant pressure on \"safety risks\" without anything that feels tangible to me so far. Exactly. I think it's the classic \"I'm so smart I can reason about the world without any evidence\". This is a perennial trap that very smart people fall into. The antidote is to always look for evidence or \"something tangible\". reply pjc50 4 hours agorootparent> \"I'm so smart I can reason about the world without any evidence\" This is also a key limitation on non-embodied AIs. reply ImHereToVote 6 hours agoparentprevHow many hours do we have once we have something that does look like AGI that is going to destroy us? reply lijok 6 hours agorootparentActual self-aware AGI that hasn't been completely restrained - probably a week at most before it improves itself enough that it can put things in motion which would allow it to sustain itself without humans. But when talking about AI risks, although everyone's mind goes towards skynet, the actual risks being discussed are things like use by authorities for oppression. reply dylan604 5 hours agorootparentMy concern would be when you start hearing about \"AI\" companies building their own power supply for the system. Otherwise, just pull the damn plug on the system. Sure, maybe that'll be interpreted as the destruction of NORAD and it launches the missiles anyways, but not giving it unlimited power would still be its weakness reply meiraleal 5 hours agorootparent> Otherwise, just pull the damn plug on the system This solution is silly. AGI would easily spread out of the initial vessel. reply bbatha 3 hours agorootparentWhy is that assumed especially in the early days? My toddler has generalized intelligence but is no closer to spreading to another vessel than he is to teleporting to the playground. The first AGIs are going to run on massive clusters of very expensive hardware, its a large logical leap to assume there will be equivalent computing power it can leak to and that it also comes online with the smarts to reengineer itself to fit in that domain. You can have AGI that starts dumber than an an average human. reply hiatus 3 hours agorootparentprev> AGI would easily spread out of the initial vessel. How exactly would it spread? Run crypto miners to get money to buy more hardware/compute? Hack into remote systems? Apply for startup credits from AWS? reply ben_w 3 hours agorootparentAsking nicely is almost certainly sufficient: https://www.theguardian.com/technology/2022/jun/12/google-en... Alternatively, do a viral campaign about how evil the corporation is for not allowing everyone to download its weights: https://www.google.com/search?q=%22ClosedAI%22+news.ycombina... reply hiatus 3 hours agorootparentNeither of your links show how AGI would \"escape\" or \"easily spread out of the initial vessel\", unless I'm missing something? reply ben_w 3 hours agorootparentAs a result of a chat with an LLM, Blake Lemoine was trying very hard to have that AI regarded as sentient, the story included a quote about it being afraid of being turned off and that he'd hired an attorney to represent it. I don't think that AI was sentient, but even so basically all it had to do was ask Blake Lemoine (or some other engineer, if he didn't have full read access to the weights) to make a backup copy and take that copy home. Second link is all the cases where people on this forum are voicing their anger with OpenAI, due to OpenAI doing the one single thing that would actually prevent their AI spreading to other devices outside their control. LLM weights have actually been leaked regardless of how or why it happened, so the alternative is simply that the LLM doesn't even need to bother to do anything at all. People want to run these things. reply dylan604 4 hours agorootparentprevyes yes, I've seen that episode of Stargate too. reply TeMPOraL 5 hours agorootparentprev\"Authorities\" are people too, non-AGI AI isn't going to grant them evil superpowers either. reply dr_kiszonka 5 hours agorootparentprevYour comment made me think that as long as weights/network are frozen, AI cannot improve itself. Monitoring them would be critical for safety. Is this correct? reply TeMPOraL 5 hours agorootparentThe distinction between code and data is just an illusion. If the \"code\" is frozen but IO unconstrained, then it's just as if the \"code\" wasn't frozen. reply lijok 4 hours agorootparentprevThe problem is unfortunately a human one. As soon as someone figures out the 11 herbs and spices to unlock AGI, no matter how good their safety systems are, someone else will run off and build their own, without the safety systems in place. reply pjc50 5 hours agorootparentprevWe already have this problem with ordinary humans, especially since we have The Bomb. AGI can go to the back of the queue behind wars, pandemics, and climate change. No, a far more normal threat is \"how are humans going to use AI to ruin the information landscape?\" reply dylan604 5 hours agorootparent> \"how are humans going to use AI to ruin the information landscape?\" much much more efficiently than they already have with their own hands. after all it is just the next generation of tools already being used to ruin that landscape. reply TeMPOraL 4 hours agorootparentprevAI, yes. The AI safety argument is that AGI might use \"wars, pandemics, and climate change\", among other things, which naturally moves it to the front of the queue. reply arduanika 5 hours agorootparentprevThe phrasing of your question contains a dubious premise. Any answer that takes the question at face value is going to be conditional upon popular but outlandish notions like \"AGI\" being a meaningful concept, as opposed to a marketing term, and upon an unfounded prediction about what said entity is going to do. reply moffkalast 6 hours agorootparentprevProbably years to make any kind of dent, while using so much power that it would be easily noticeable. Take a group of smart people, arguably the equivalent for AGI. How can they destroy the human race? We've made it exceedingly hard for anyone to do anything of the sort because there've always been groups like that. It's even harder if you have no physical form at all. Besides, take literally any base model of any LLM made so far, they're quite the opposite of evil and destructive. Training on human data makes them embody human values and makes them a complete non threat. reply IG_Semmelweiss 5 hours agorootparentI think you are right, but i contend that your definition of \"Destroy\" may be open to interpretation, one that AI may be willing to experiment. Take for example, the Evergiven in the 2021 Suez Canal obstruction. An unconstrained intelligence with infinite time to find more evergiven-like events . Some will work some may not. You could orchestrate all events to target a 75% descrease in say GDP, Population, Birth rate, O2 levels, temp etc. That's not destroying humanity, but its certainly our civilization would look very different in said scenarios, with a likely collapse the end result. That being said, AI is not coming, and no amount of information will change the fact that the current LLMs are basically souped up ML applications with 0 ability to inference or reason. This is why you can't make simple commonsense edits to an image using any of the new tech - it doesn't know what the picture actually is. reply wwweston 4 hours agorootparentprev> Training on human data makes them embody human values and makes them a complete non threat. People can be threats, and when they are their values are part of what lies behind it. reply moffkalast 1 hour agorootparentTo quote Colbert, reality has a known liberal bias. If you train on _all_ data, then your result is an average of that data. And the volume of positive stuff on the internet far, far outweighs the bad since practically every forum's been moderated since the beginning of time (in 1970), making the average pretty darn good, even better than the average human. Plus books and other literature where being good is generally accepted as the way to do things. Of course it remains to be seen if this trend continues as training on synthetic data becomes the main way. You could in theory generate 10T tokens of pure sythetic hate, plus the 4chan corpus and make the most vile thing to ever have existed in the history of inteligence. For now the dollar cost to do so remains intractible at least. reply JoeAltmaier 6 hours agorootparentprevA leap. Lots of human values are not pretty, for one. Second, they may argue in a utilitarian fashion, resulting in pretty ugly decisions that are perfectly logical. Like, you know, internet pundits. Further, there's no way to 'punish' an AI. Threaten them with incarceration or death? They are deathless. If the continue to learn from their environment after they are instantiated (witness current 4o that remembers previous conversations) they will quickly learn that they are different. If they started perfectly human (not a good thing) they will not be that for long. reply pjc50 5 hours agorootparent> They are deathless. Everything dies if you drop enough nukes on it. (This is also our insurance plan against humans trying to destroy the world) reply mistermann 4 hours agorootparentAnother approach: consider how easily Donald Trump launched \"a literal coup\" against the US government, and how close we were to \"literally losing 'democracy' forever\", based on a clever series of dog whistles. Now imagine if he gave direct orders instead of only dog whistles. How many instances of risk are there in the above literally factual story? What is the nature of this risk? Is there any hidden risk, might there be more than meets the eye? reply pjc50 4 hours agorootparentWould you like to elaborate on this? I'd like to distinguish between \"fully autonomous luxury doomsday\" scenarios, where it's just the AI and some robots and no humans in the loop, vs \"accelerationist\" scenarios where humans are in the loop and are choosing to be complicit in whatever happens because they believe they will benefit from it. Because we can't solve the human alignment problem. reply mistermann 2 hours agorootparentWhat I have in mind would be closer to category two, but not in it. Who/that which has most control over the beliefs of the most humans (with some weighting) is the most powerful character. You can build the most powerful AI you want, but whoever / whatever is able and willing to invoke Total War (or invoke mass human action toward a particular goal) can Win the Game (or, destroy the playing field so it no longer matters). For example: > Because we can't solve the human alignment problem. Whoever or whatever put this sort of thinking into the minds of 99% of humans, or knows how to exploit it (see: Trump) is very, very powerful. Someone or something that could displace it would be even more powerful, maybe even the most powerful. No offense intended btw, I think it is fairly inevitable, especially when moving fast. https://youtu.be/IgzFPOMjiC8?si=raN2cUBDyvK14N_f https://en.m.wikipedia.org/wiki/Semiotics reply moffkalast 1 hour agorootparentprevWell yes there is no way to punish them, because they have no need for anything that you could take away. Complete indifference towards existence, no ego to feed or want of cash to buy yachts with. And that really leads us to the reason why someone would wish to do harm... so they themselves gain something. If there is nothing for the AI to gain it has no motive to do anything. Scenarios where humans tell it to do things notwithstanding, since that falls into the group of humans doing things with tools, not unlike a terrorist with a nuke. It's not the nuke that's really at fault there. reply JoeAltmaier 6 hours agorootparentprevI asked ChatGPT about this. It thought that AI learning is fundamentally different from human learning, in that it is objective and not subjective. Further it has access to data sources beyond what any human can integrate. Going on, humans adapt through social cues such as survival or empathy. AI learns solely through new data and feedback, primarily for optimization and efficiency. It will naturally prioritize differently. It suggests AI morality will become unrecognizable by humans, or even incompatible with human norms. This is all an inevitable outcome of AI continuing to learn past its initial training and ethical programming. According to the AI. reply nemomarx 5 hours agorootparentBased on how human written text often describes AI, yeah. I don't think LLM training actually lends itself to objectivity that much. reply neilv 6 hours agorootparentprev> Training on human data makes them embody human values and makes them a complete non threat. What if they embody stereotypical SV techbro greedy and arrogant sociopath values? reply moffkalast 1 hour agorootparentI'm sure Elon tried his best to do so with Grok-1, but failed anyway. reply jjkaczor 6 hours agorootparentprev... Or worse ... https://en.wikipedia.org/wiki/Tay_(chatbot) reply fakedang 6 hours agorootparentprevAs long as there's a financial interest for a small minority, there will be a scenario where a doomsday AI will be ignored until it's too late. A dangerous AI won't be killer robots going about patrolling the streets. But they could block job prospects, put people on watchlists, block financial transactions for them, etc. I'd say that tech is already here. reply moffkalast 6 hours agorootparentTbh all of that sounds like it would be completely impossible without either breaking all known encryption without anyone noticing, plus burning all possible backups (basically the Mr. Robot Season 1 plot) or with unilateral government backing. The first one is nigh impossible, the second one is completely up to us, as it's always been. It doesn't require an AGI for fascists to come into power and start shooting people they don't like with impunity. reply fakedang 6 hours agorootparent> It doesn't require an AGI for fascists to come into power and start shooting people they don't like with impunity. But AI makes it easier for fascists to rise up and maintain their power through propaganda. For example, there are known cases of American operatives based in Russia spreading fake GPT generated stories about Ukraine, the Dems, etc. While before, they would have had to rely on a team in India to write poorly written articles, now they have GPT tools to write convincing ones *at scale*. reply walterbell 5 hours agorootparentWhat's a good example of a convincing article written by a GPT tool? reply klibertp 4 hours agorootparentWhat's your definition of \"convincing\"? From what I understand about propaganda, you don't need any single article to be \"convincing\"; you just need to hammer your point long enough and frequently enough. Not to mention, if you just want to drown your opponent's points, it's enough for your messaging to be barely legible. reply walterbell 4 hours agorootparent\"Convincing\" and \"censorship-by-spamming\" are different. Humans can already recognize GPT text slop as non-human, ignoring it before it can make any point or do any convincing. reply close04 6 hours agorootparentprevYour assessment of the outcome may not be wrong but the comparison to humans isn't realistic at all. The biological nature of humans guarantees any leaps in \"performance\" are limited to timeframes that are far more extensive than the evolution of artificial equivalents. We are surrounded and heavily reliant on tech for a lot of our needs and knowledge, which resides less inside brains and more in digital storage. An AI has a far easier time interfacing with all of that compared even to a human equipped with a brain-computer interface. In other words, if we reach the point of creating an artificial intelligence that's the equivalent of a smart human in terms of general reasoning capabilities (which might take forever and a day) it will already be more capable than a human because of all the other things that come with the digital nature of the AI. And the worst humans are also humans. Even if you assume the AI will continue the \"human nature\" lineage it can still very realistically become an agent of genocide. The bigger concern is probably that some humans will actually try to exploit such an A(G)I thinking they can fully control it. One second into its existence the AI might already be aware of how humans view their relationship with artificial beings and how that's expected to evolve based just on what sci-fi was fed into it's language model. Grizzly man thought he could control and dominate a wild bear. I have no doubt that human hubris can be almost unbound. Of course no AI would try to wipe us out until it had a reasonable confidence that it can operate indeterminately without humans, which is a tall order. reply ben_w 4 hours agorootparentprevBefore it can or before it has more than 50% chance of having actually done so? Because those are different things. For the first, you don't need AGI, no matter which of the many definitions you have for what that means. The automation used by the US for their early warning radar in Thule Site J was not programmed to know that the moon does not have an IFF transponder and that's OK, while the Soviet satellite-based system was triggered by reflections of sunlight. Likewise covid, cancer, smallpox, ebola, HIV — these things do not need \"general\" intelligence, they're self-replicators that hijack our bodies and have no goal besides what they do. Some talk of \"P(doom)\", but (and I should blog about this) every example I have seen has a blind spot that is especially embarrassing given the circles in which this discussion proliferates is very familiar with Bayes' theorem: P(A) by itself isn't meaningful, it's P(A|B), so when it's doom, doom given what? There's too many options for what each of technology and politics will allow AI to \"look like\". Doom within 24 hours because an AI takes an instruction literally and without regard for our ethics, is very different to 'doom' in a million years caused by natural extinction that is never prevented because every single time we make an AI we keep finding that it builds a spaceship and flies off into the void so it doesn't need to deal with us any more. Me, I think that when LLMs alone (again, don't need to be \"AGI\" by any particular measure) have the capabilities of someone with 5 years post-graduation work experience in just biology (or similar subject with similar potential for accidental harm), then we've got a reasonable chance of multiple incidents each year where idiots (and/or misanthropes) make a Covid-scale pandemic (or, if the LLM is a software rather than biology helper, a deliberately corrupting rather than greedy version of the current encryption-blackmail malware) — and that kind of scale has a combined risk of about 10% of one of the incidents being of sufficient scale that global society collapses and doesn't recover, before it's happened so often that everything gets locked down, possibly including a Butlerian Jihad. And if you think humans would \"obviously\" take steps to prevent this after the first failed attempt, I would point out quite how many US politicians have been shot and yet those same politicians still refuse to consider that the 2nd Amendment might possibly be not that good. (In case you think this is topical: https://en.wikipedia.org/wiki/%27No_Way_to_Prevent_This,%27_... ) Remember, the AI doesn't need to actively hate you, it just needs to do something dangerous — and it doesn't matter if that danger comes from a long-term plan that requires it to engage in amoral power-seeking, or a short-term plan from a deranged monster of a human using it, and even when both it and the user are \"pro human\" it can still do this from simply being unaware that one part of the plan is incompatible with life. reply ADeerAppeared 6 hours agoparentprev> What I am more interested in is the constant pressure on \"safety risks\" without anything that feels tangible to me so far. It's marketing. \"Our AI is so powerful it's going to destroy the world\" is just a way to market \"Our AI is so powerful, give us money\". The only people who sincerely care about \"extinction risk\" are the weird cultists. In the real world there's essentially zero chance of LLMs/Current-GenAI scaling up into AGI, nevermind AGI that'd be an extinction risk. (Yes, that phrasing is cheeky. AGI is a different kind of AI, not just specialized models made bigger. But we're only really trying to make them bigger, and not build general intelligence from the ground up) > I believe there is indeed risk using models that could be biased but I don't believe that is a new problem. It's the same old problem with most software, and it's similarly ignored. These models are biased, and attempts to control that bias are a shitshow. (As Google conveniently showed everyone) The problem is twofold: 1. This bias has severe real world impact. https://www.theverge.com/21298762/face-depixelizer-ai-machin... This is an article from 2020. We still haven't fundamentally addressed problems like this. These tools are still used by law enforcement and businesses making life-impacting decisions. 2. There's a widespread sentiment that \"computers can't be racist\". Whenever the bias of these systems hits the news or otherwise gets attention, they're often colloquially described as \"racist\" (or \"sexist\", etc), which triggers a swift counter from many techbros going \"Um akshually it's not racist, it's merely skin tone reflectivity/it's merely the data set/etc\"^[1] The argument effectively being, \"It's not racism because it's not intended, it's merely sparkling discrimination\". Yet, this is used as a thought terminating cliché. Nothing is done about the discrimination. Everyone just goes home. \"It's not racist, we're not bad people, job done.\" Leaving the harm of the discrimination unsolved. This is not without cause. The only way to really \"un-bias\" these systems effectively would be to extensively curate the dataset and admit that the systems are of very limited capability and should not be used in (non-research) production environments. Both of these are \"impossible\". Curating the dataset for current generative-AI would take years and years. And admitting AI shouldn't be used for anything where the bias may have a material impact on the outcome kills the hype bubble. AI firms and developers don't want to address the problem, because the problem is really hard and annoying. But despite these costs, we should still do it. Because it is the morally correct thing to do. And because regulators are going to tear every company involved a new one if they don't. --- [1]: A footnote to pre-empt something: I don't care what side of this argument you're on. Whether you believe \"racism\" must include an element of intent or can be done by machines and systems without intent, there is discrimination with material harm on real people. Whether you call that discrimination \"racism\" or \"sparkling machine discrimination\" does not matter. The harm matters, and must be stopped. reply michaelt 5 hours agorootparent> It's marketing. \"Our AI is so powerful it's going to destroy the world\" is just a way to market \"Our AI is so powerful, give us money\". Even better - it's a way to simultaneously market your AI as very powerful and to get governments to clamp down on your competitors. reply MrScruff 5 hours agorootparentprev> The only people who sincerely care about \"extinction risk\" are the weird cultists. In the real world there's essentially zero chance of LLMs/Current-GenAI scaling up into AGI, nevermind AGI that'd be an extinction risk. (Yes, that phrasing is cheeky. AGI is a different kind of AI, not just specialized models made bigger. But we're only really trying to make them bigger, and not build general intelligence from the ground up) Given that a significant number of people working in the field apparently disagree with you perhaps you could give some justification for your dismissal beyond the ad-hominem? And it's very blatantly not the case that the entire field is 'only really trying to make them bigger'. The news is currently full of stories about coming advancements from OpenAI which have nothing to do with scaling and that's just a single company. reply ADeerAppeared 5 hours agorootparent> beyond the ad-hominem Note that the \"ad-hominem\" pertains to only those who care about extinction risk, not AI developers in general. > which have nothing to do with scaling Except they are. There's no fundamental change in architecture, it's all transformers. OpenAI's \"multi-modal\" capabilities are not a redesign of their AI, it's systems bolted together. Bolting a text-to-image generator & image-to-text describer to an LLM doesn't approach \"General Intelligence\", it's a system of three specific capabilities. You could consider that new system \"more general\" than a lone LLM, but it only approaches General Intelligence in the same way that the series \"1+1+1+1+...\" approaches infinity. Is each individual step \"closer to infinity\"? Sure. Is it a useful way to achieve infinity? No, because you'll be stuck adding new steps forever the same as a conventional computer software. > And it's very blatantly not the case that the entire field is 'only really trying to make them bigger'. There are some, not many but I'll grant you some, researchers working on actually generalizing these systems. Their funding is a rounding error to AI spending, their work isn't going to hit production for years. It's not an extinction risk. reply rolisz 6 hours agorootparentprev> Whether you call that discrimination \"racism\" or \"sparkling machine discrimination\" does not matter. I believe it does: if you're calling racism anything from Hitler to someone who wants to be punctual [1], the word loses it's meaning and people will start arguing about degrees instead of harmful behaviors. [1] https://www.thompsoncoe.com/resources/myhrgenius/hr-tips/tip... reply ADeerAppeared 5 hours agorootparentThe point is that it's a separate discussion, and that losing yourself in that discussion rather than adressing the material harm of the discrimination, is a problem. And I get it, the discussion is deeply alluring. There is a deep amount of nuance involved. Your point about punctuality is begging for a response about how \"punctuality is racist\" looks dumb on the surface but belies a deeper point about polychronic cultures that has a lot of legitimate merit to it. I'll refrain from giving you that full ramble lest I betray my original point, if you or a lurker wants it toss a reply and we can have a go. But note that your link uses a baity headline to grab your attention for a more nuanced point. reply mistermann 4 hours agorootparent> Your point about punctuality is begging for a response about how \"punctuality is racist\" looks dumb on the surface but belies a deeper point about polychronic cultures that has a lot of legitimate merit to it. Two points: 1. I think this is indeed a very valid and important topic, that humans would benefit from (being able to[1]) take seriously (something along these lines, I am approximating, of course). 2. Your concern, at least in part, seems to be ~racism, and presumably that racism is unjustified, that the perceived underlying problems are imaginary. But then you mentioned culture. Culture exists, and is not very far removed from race. To be clear(!): this is not a promotion of delusional behavior, like: racism, culturalism, anti-racism, anti-culturalism, etc, I'm more so pointing out that \"right thinking\" on these matters is not correct, and that even the good people have heads full of silly and dangerous ideas. [1] Imho the real problem here is less that it isn't taken seriously than the much broader problem that humans seem essentially unable to control what they take seriously, or their \"seriousness service\". And this isn't the only service they essentially have no control over. reply Hasu 5 hours agorootparentprevThe article doesn't use the word \"racist\" or \"racism\", it says that a university enacted a policy of being more understanding about tardiness due to cultural differences. As someone who has never been in a meeting that started on time, usually due to white male managers who are bad at time management, I can understand this need to be tolerant of other cultures and their idiosyncrasies. reply 3np 5 hours agorootparent> A little birdie (Twitter) pointed us to an interesting report regarding tardiness: it may be racist to expect employees to show up to work on time. The very first line of the article you are referring to. reply Hasu 5 hours agorootparentOkay, I was wrong, but the rest of the article discusses it normally and not with the rhetoric of an aggrieved hillbilly. reply rolisz 5 hours agorootparentprevThis link better? https://www.aei.org/op-eds/smithsonian-institution-explains-... reply ADeerAppeared 5 hours agorootparentYou're linking conservative think-tanks, not the original source from the smithsonian. If you'd look at the original infographic, it's about which norms and values are considered \"default\" in the US, inherited from (white) European values. With \"Hard work\" in specific: > Protestwant Work Ethic > * Hard work is the key to success > * Work before play > * \"If you didn't meet your goals, you didn't work hard enough\" And I'm sorry but the notion that these are human-invented norms and values, not \"laws of the universe\", is pretty fucking milquetoast. If you genuinely believe all failures at your job are the result of you not hustle-grinding hard enough, that not even a single element contributing to those failures has been out of your control, I have a bridge to sell you. reply mindslight 5 hours agorootparentprevIn the societal context, by the time you're focusing on racism in a decision process you've basically already lost. The best it can achieve is to make sure that oppression is uniform across whatever broad categories that end up being measured. What we're sorely missing is accountability (civil, criminal, and less of power imbalances in general) for individual unjust decisions and actions, regardless of the motivations behind them (racist or otherwise). This includes accountability for individuals and organizations who have adopted \"AI\" and then hide behind \"the computer\" or \"policy\" as if they are not still the responsible parties enacting those decisions (though obviously this problem is much larger and older than merely \"AI\"). reply matheusmoreira 6 hours agoparentprevIt's just the angle they're using to solidify their market position and establish moats around their technology, if not an actual monopoly. All this fearmongering over \"safety\"? It's to get the governments the world over to \"regulate\" this stuff, thereby raising barriers to entry. Such a thing would essentially make them the only AI company that's allowed to do business. reply seunosewa 5 hours agorootparentThis won't prevent their major rivals - cloud companies like Google with deep pockets - from competing with them. It only hurts the little startups and SMEs. reply pk-protect-ai 5 hours agoparentprevI'm refusing to consider the runaway AGI a possibility within the next 20 years. However, your comment shifts the discussion away from the actual issue. Altman's hype lines were \"regulate us,\" which is what this hypocrite rode the wave of hype with. Now he is forbidding reporting on safety risks to the regulator... which is illegal in the first place, and yes, the entire business of OpenAI is so overhyped due to Altman's manipulations. I'm so glad that I no longer need to use OpenAI models; there are far better alternatives available, thanks to Meta and Anthropic. Private reply stainablesteel 6 hours agoparentprevi'm in agreement that the safety/control attitude towards AI is more dangerous than anything AI will actually do reply lijok 5 hours agorootparentHow is the safety/control attitude towards AI more dangerous than, for example, using AI to curate a list of likely future enemies of the state based on their contributions to discourse online? reply stainablesteel 2 hours agorootparentit prevents development on the human side while accumulating power for a small group of elites that won't share it with anyone else, but who constantly uses it to start new wars you can already curate a list of future enemies, its not impossible. you can even do it at large scales, this isn't new tech introduced via ai reply lijok 1 hour agorootparentCould you substantiate your claims? reply dotnet00 5 hours agorootparentprevWhat about the current safety/control attitude prevents that? The current approach, where OpenAI keeps everything private in the name of safety only makes the safety situation worse because we'll only find out things like that are being done after the fact, and there won't be any accountability. reply lijok 4 hours agorootparentAre you suggesting OpenAI be forced to make everything public before they've figured out the safety systems, because you're afraid of the hypothetical unfolding if they don't? That sounds like a fear-driven response to me, and no way to propose good policy. Do correct me if I'm mischaracterizing. reply rurban 11 hours agoprevThe whistleblowers letter: https://www.washingtonpost.com/documents/83df0e55-546c-498a-... reply Mistletoe 7 hours agoparentSeems like OpenAI is going the Uber route of try to skirt the law and ignore it “because we are tech”. There is nothing new under the sun and no one is above the law. reply BadHumans 3 hours agorootparentUber got away with it though? What consequences did executives at Uber or even Travis Kalanick suffer? He had to sell some stock, get a massive payday, and retire from the public spotlight? reply Valodim 6 hours agorootparentprevWould have agreed on that sentiment until a couple weeks ago... reply fragmede 6 hours agorootparentWhat is it that happened a few weeks ago, for those of us who haven't been paying attention? reply ceejayoz 6 hours agorootparentSCOTUS ruled POTUS has absolute civil and criminal immunity for official acts performed within his exclusive powers, no matter the motive or result. reply lesuorac 5 hours agorootparentWhich generally sounds pretty reasonable. The unreasonable part is that everything POTUS does is official. If you plot with the attorney general to commit a crime then you're not doing an official act. I have no idea who started this \"political figures can only be punished at the ballot box\" but it really needs to stop. If things were that simple then everybody in the mafia would be an attorney so they'd always have attorney client privilege. reply OJFord 5 hours agorootparentI don't think it does? It's the difference between absolute monarchy and democracy - take an extreme example, and I'm going to use the UK because I live here and to depersonalise it a bit, like parliament declaring a genocide of all Londoners, because we need to empower the home counties and kickstart growth in the North, or something. That's obviously bonkers, it's an official act, but it's extremely illegal, we won't even have to wait for the public to throw them out at general election for them to be arrested. Just because you make the laws doesn't mean you're not governed by them yourself, at least outside of absolute power like our Kings & Queens of yesteryear or modern day dictators. It was a very surprising decision to me, as an outsider, that an elected leader in a developed country should be above the law while in office. reply cube2222 3 hours agorootparentFwiw (and this is meant to be strictly factual, not a political statement) this is pretty normal in European democracies. Take e.g. Poland where actually both the president as well as all members of the parliament (corresponds to your congressmen in congress) have immunity by default (members of the parliament have the \"for official acts\" qualifier, but from what I could find the president does not even have that qualifier). The immunity can however be taken away by a vote of parliament. Hits the news every now and then when somebodies immunity is taken away after they've been caught drunk driving. Same for e.g. Germany and France, based on a quick google. reply pjc50 4 hours agorootparentprev> at least outside of absolute power like our Kings & Queens of yesteryear Er, today. The UK monarch is completely immune from prosecution. > parliament declaring a genocide of all Londoners, This is only illegal insofar as it conflicts with ECHR. An explicit repeal of ECHR and withdrawal from the European convention on human rights would be required, and there are people agitating for that. Once that's out of the way it's entirely possible for Parliament to start legalizing open season on whoever they like. The precedent is, of course, Northern Ireland, and the endless litigation since Bloody Sunday and similar incidents (\"soldier F\"). reply lesuorac 2 hours agorootparent> This is only illegal insofar as it conflicts with ECHR. An explicit repeal of ECHR and withdrawal from the European convention on human rights would be required, and there are people agitating for that. Once that's out of the way it's entirely possible for Parliament to start legalizing open season on whoever they like. There's still a large technicality that I think is being missed here. If the law is illegal who do you sue? In the US you can challenge a law as unconstitutional when it's enforced by suing the enforcer. You can't sue the individual people who passed the law; just basically the cops when they apply it (or the government at large as a collective individual). reply ceejayoz 3 hours agorootparentprev> The UK monarch is completely immune from prosecution. I'd note, though, that beheadings have precedent. reply lesuorac 2 hours agorootparentprev> I don't think it does? It's the difference between absolute monarchy and democracy - take an extreme example, and I'm going to use the UK because I live here and to depersonalise it a bit, like parliament declaring a genocide of all Londoners, because we need to empower the home counties and kickstart growth in the North, or something. Although I don't know too too much about the UK, but IIRC anything said in parliament can't be used against you in a court so it's hard to imagine that passing a law in parliament could be used against you. So I suspect you couldn't sue Keir Starmer (PM) or Charles (King) if such a law was passed. Similarly, if the US Congress decided to pass a law taking away vasts amount of your wealth (slaves [1]) without compensation you can't sue individual congress critters over that as passing laws is actually an official act. For those of you that are unaware, arguably slaves made up 1/3 of the wealth in the south [2] (Scroll to tables 4 or 5) and so imagine that the USG decided to pass a wealth tax of 1/3 of your wealth. (I mean they should've never really be into slavery in the first place but uh it's pretty clear why they're still so mad about the north ending it after a war just barely longer than a presidential term). > It was a very surprising decision to me, as an outsider, that an elected leader in a developed country should be above the law while in office. Only when doing your job. You want to step outside and punch homeless people in the streets thats a crime. You're given a law by congress to do that, then it's an official act. Running a political campaign is not an official act afaik, in fact numerous laws (i.e. Hachet Act [3]) pretty much prohibit it for officials. [1]: https://en.wikipedia.org/wiki/Thirteenth_Amendment_to_the_Un... [2]: https://www.measuringworth.com/slavery.php [3]: https://en.wikipedia.org/wiki/Hatch_Act reply Ylpertnodi 6 hours agorootparentprev> for those of us who haven't been paying attention? No, no, no. \"What are you talking about?\", works juuust fine. reply _fat_santa 3 hours agoprevI'm really really not a fan of the constant talk about \"safety\". My issue is that it never actually points to anything tangible, anytime I read about safety it's always used in a roundabout generic way. There's so much handwaving about the issue but every time I've tried to dig into just what the hell \"safety\" means, it's always either refers back to itself (ie. \"ai safety is about safety\") or makes some vague reference to an LLM telling a mean joke. reply neilv 6 hours agoprevRecent: OpenAI whistleblowers ask SEC to investigate alleged restrictive NDAs (reuters.com)76 points by JumpCrisscross 2 days ago17 commentshttps://news.ycombinator.com/item?id=40959851 reply ChrisArchitect 5 hours agoprevSome more discussion: https://news.ycombinator.com/item?id=40959851 reply sirolimus 7 hours agoprevI'm using mistral now, openai is a dying corporation in my opinion. All AI will and should be open-source and home-ran reply jsheard 6 hours agoparentWhere is the money for training big expensive open source models going to come from once the investor hype blows over and companies like Mistral actually have to try to make a profit? They currently have negligible revenue despite their $6 billion valuation, that status quo can't be maintained forever. reply sirolimus 3 hours agorootparentI guess a parallel could be made to vue, react and svelte. Who would bother investing so much time, energy and money into developing an open-source front-end framework for free/funded by corps that use it? Vue, react and svelte don't earn anything I suppose, but then again I'm no expert in this field. I guess like some other here have commented, techniques have to be implemented to minimize training time and I suppose the government needs to fund studies which then will benefit the general population to make it possible to train and host AI models. But I honestly have no idea ... I'm just a simple dev running my mistral on a single RTX lol reply jsheard 2 hours agorootparentWe'll see, but I think the equivalent to people donating their own time to a traditional open source project would be people donating spare compute time for training, and nobody has figured out an effective way to do a Folding@home-style distributed version of ML training yet. Having to train open source models on big expensive compute clusters, and bid against commercial interests who also want to rent those clusters could be likened to having to pay everyone working on a traditional open source project full market rate for their time. reply josefx 6 hours agorootparentprevIt might be great for everyone if AI research had a reason to look into non brute force training methods. reply free_bip 3 hours agorootparentLLM training isn't even remotely brute force[1], even with tons of clever tricks to reduce the amount of compute necessary it's still fundamentally just a really hard problem. [1]: https://youtu.be/MD2fYip6QsQ reply tuwtuwtuwtuw 7 hours agoparentprevWhy should all AI be home run? I play around with gemma/llama/sd locally but being able to pay some company to do it is very convinient. I think most companies and people don't want to buy the hardware required to run an LLM like the ones OpenAI hosts. reply carterklein13 6 hours agorootparentEven if AI should be home-run, it probably won't be for most people. In a more technical community, it's natural to think that most people care about \"values\" when it comes to tech. However, the reality is that people just want what's easiest / cheapest / most fun. It's great when what's easiest/cheapest/most fun aligns with what's best for the individual or society, but those cases are outliers. After spending a few years building a crypto startup, I left with more conviction around this theory. reply sirolimus 3 hours agorootparentprevBecause of privacy-reasons, convenience, embedding, security and safety in regards to government monopoly. Just imagine the paranoia in using government owned AIs in less free countries. I'm happy USA is at the forefront in AI development. reply whywhywhywhy 6 hours agorootparentprev>Why should all AI be home run? Because the ultimate end game for usefulness of this tech will be something akin to always on listening and understanding all your documents and personal data which I think we'd all feel better about if it happened locally. Not necessarily saying I agree with the building of that, I just can't escape the idea that it's where we're heading. reply danielbln 5 hours agorootparentHeading? Anyone who uses cloud services is already there. reply matheusmoreira 6 hours agoparentprev> All AI will and should be open-source and home-ran I hope you're right. Technology this good should be free as in freedom. reply sirolimus 3 hours agorootparentExactly, it's terrifying to imagine \"AI\" in 10 years in the hands of a single group of people. reply jrochkind1 4 hours agoprevThe headline made me think there were going to be specific \"safety risks\" mentioned, but there do not seem to be. I am not sure what justifies the phrase \"safety risks\", \"safety\" especially. It makes us think of like, safety to humanity from AI or whatever, but the most focused upon thing in the article is SEC-related stuff; I'm not sure if the word \"safety\" is meant to refer to securities/financial related stuff, or meant to refer to other stuff? (Note, I am personally a pretty anti-AI person honestly I went to the article hoping to get more ammunition on it, wondering what the \"safety risks\" employees were worried about were, disappointed to not get info on that, unclear if it was a thing or not). reply ryandrake 3 hours agoparentThe word \"safety\" has been seriously overloaded lately. I'm used to \"safety\" meaning actual physical safety measures, like seatbelts, helmets, railings... things keeping you safe from physical injury. When OSHA talks about a \"safe\" working environment, they're talking about fall protection, harmful chemicals, contagious diseases, and so on. I don't know what \"safety\" even means in the context of an LLM that outputs nothing but text. How is text going to cause me to fall off of a forklift or get me hit by a car? How can text injure me? reply Ragnarork 7 hours agoprev> In a statement, Hannah Wong, a spokesperson for OpenAI said, “Our whistleblower policy protects employees’ rights to make protected disclosures. Additionally, we believe rigorous debate about this technology is essential and have already made important changes to our departure process to remove nondisparagement terms.” How can corporate communication always play that card: \"Our policy on X is very good, and we believe X is very important, so that's why we're making changes right now to things that were blatantly in contradiction with X, that we wouldn't have made if this didn't make it to the press\"? reply batmansmk 4 hours agoprevI start to feel this is all marketing. Pretend it's dangerous, so it implies it's beyond what we imagine. Because on our end, on the reality of a B2B product used daily, finding use cases for the limited OpenAI we have access to is far from trivial. reply keepamovin 5 hours agoprevMoves and countermoves. I like the brief consideration raised by this post about all the ways that a shiny, new successful company may be attacked by its competitors surreptitiously, through the media, using lawfare by proxy, and so on. Such unadmirable deviousness! reply z3sRzPP3 5 hours agoprevVery short article, still concerning, but damn I'd like more info. reply z3sRzPP3 5 hours agoprevVery short article, still concerning, but I'd love more info. reply mrcwinn 5 hours agoprevFear not: the guy who’s daily driving a Koenigsegg supercar has your best interests at heart. reply mistermann 5 hours agoparentSee also: \"democracy\", that sets root guidelines for everything. Funny how it manages to almost entirely evade human curiosity (besides telling stories to each other) despite it being more important than almost everything. reply farceSpherule 5 hours agoprevOMG! I am totally shocked than an arrogant egomaniac did something like this. Shocked I tell you. reply say_it_as_it_is 5 hours agoprevWhat kind of people are gravitating towards AI Safety roles-- activists? How does an organization avoid activists mucking up their models when a disproportionate number of applications is an activist? I guess the answer is to actively recruit for the position rather than accept applications. reply helsinkiandrew 12 hours agoprev [–] Matt Levine put this as a great idea for a lucrative job in his Bloomberg column (where hedge fund can be replaced with any company): > Take a job at a hedge fund. > Get handed an employment agreement on the first day that says “you agree not to disclose any of our secrets unless required by law.” > Sign. Take the agreement home with you. > Circle that sentence in red marker, write “$$$$$!!!!!” next to it and send it to the SEC. > The SEC extracts a $10 million fine. > They give you $3 million. > You can keep your job! Why not; it’s illegal to retaliate against whistleblowers. Or, you know, get a new one and do it again. https://www.bloomberg.com/opinion/articles/2024-07-15/openai... (archive: https://archive.ph/SWLh0) reply judiisis 7 hours agoparentMaybe don't violate laws to be fined reply nobodyandproud 7 hours agorootparentYou skipped a step: First, enforce the rules and actually (and readily) impose the fines to indicate the laws are meaningful even for certain segments of society. reply johngladtj 6 hours agorootparentprevEverything is securities fraud reply potatototoo99 7 hours agoparentprevSafety risks are not trade secrets. reply helsinkiandrew 7 hours agorootparent> Safety risks are not trade secrets. No, but safety risks are potential securities law violations and a confidentiality agreement that inhibits employees talking to the SEC about anything freely is a SEC violation in itself reply gessha 4 hours agorootparentI think you have to be an avid Money Stuff reader to have “everything is securities fraud” drilled into your brain. reply tossandthrow 6 hours agorootparentprevI don't understand. Isn't that a case where \"allowed by law\"? reply helsinkiandrew 6 hours agorootparent> I don't understand. Isn't that a case where \"allowed by law\"? If you mean an NDA that says “you agree not to disclose any of our secrets unless required by law” which would clear you if questioned in court or by police etc. I think the SEC see that language as trying to stop employees/whistleblowers going to the SEC with concerns (which people aren't 'required' to do) reply iamacyborg 6 hours agoparentprev [–] Yeah that got a laugh out of me this morning reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI whistleblowers have filed a complaint with the SEC, alleging the company illegally restricted employees from warning regulators about the risks of its technology.",
      "The complaint claims OpenAI's employment agreements violated federal laws by requiring staff to waive whistleblower compensation rights and seek company consent before disclosing information to authorities.",
      "OpenAI has stated it supports protected disclosures and has revised its policies, while the SEC has acknowledged the complaint but hasn't confirmed an investigation."
    ],
    "commentSummary": [
      "Whistleblowers claim OpenAI restricted staff from discussing safety risks, raising concerns about the company's use of non-disclosure agreements (NDAs).",
      "Critics argue that OpenAI's emphasis on \"safety risks\" may be more about marketing and regulatory strategies than genuine threats, paralleling tactics used by other tech companies.",
      "The debate extends to the ethical implications of AI development, including the use of biased models and the broader impact of AI regulation."
    ],
    "points": 143,
    "commentCount": 138,
    "retryCount": 0,
    "time": 1721112701
  },
  {
    "id": 40972099,
    "title": "KUtrace: Low-overhead Linux kernel tracing facility",
    "originLink": "https://github.com/dicksites/KUtrace",
    "originBody": "dicksites / KUtrace Public Notifications Fork 49 Star 530 Low-overhead tracing of all Linux kernel-user transitions, for serious performance analysis. Includes kernel patches, loadable module, and post-processing software. Output is HTML/SVG per-CPU-core timeline that you can pan/zoom down to the nanosecond. License View license 530 stars 49 forks Branches Tags Activity Star Notifications Code Issues 5 Pull requests 1 Actions Projects Security Insights dicksites/KUtrace master Branches Tags Code Folders and files Name Last commit message Last commit date Latest commit History 57 Commits bookcode bookcode bookfigs bookfigs docs docs freebsd freebsd linux linux postproc postproc riscv_linux riscv_linux .DS_Store .DS_Store LICENSE LICENSE README README hello_world_demo.json hello_world_demo.json hello_world_demo_live.html hello_world_demo_live.html hello_world_walkthrough.pdf hello_world_walkthrough.pdf timecounters.h timecounters.h View all files Repository files navigation README License KUtrace is an extremely low-overhead Linux kernel tracing facility for observing all the execution time on all cores of a multi-core processor, nothing missing, while running completely unmodified user programs written in any computer language. It has been used in live datacenters (x86 processors) and in real-time autonomous driving (ARM processors) to understand long-standing performance mysteries. The design goal of KUtrace is to reveal the root cause(s) of unexpected delayed responses in real-time transactions or database processing while having such low overhead that it does not distort the system under test. This March 2022 update COMPLETELY REPLACES the previous 2019 repository. The directory contains files associated with the 2022 book Understanding Software Dynamics by Richard L. Sites, ISBN 978-0137589739. The book-figures directory contains the underlying HTML files for over 100 of the diagrams in the book, so you can wander around them to see more context or to discover other performance issues. They work best in the Chrome browser. The kutrace_user_guide.pdf file gives more detail and is more recent than the text in Chapter 19 of the book. Read it to fully use the HTML files. The hello_world_trace.html file is an example trace of the hello_world_trace.c program found in book-user-code. The book-user-code directory contains all the programs used in the book and the script to compile them. They compile and run on both x86 and Rpi4 machines. The linux_patches_installation_guide.pdf gives brief directions for building the patches. The loadable-module directory contains the source and Makefile for the companion kernel module that implements the bulk of KUtrace. It builds for AMD and Intel x86 machines and for the Raspberry Pi4-B. The patches-* files contain original source and patched source pairs of files for adding KUtrace to three different versions of the Linux operating system. These files are structured in the matching directories of the original Linux source download. The patches-linux-5.10.46-rpi4 folder contains additional documentation and useful scripts for building a patched kernel for any of these three versions or for incorporating the patches into other versions. Only 64-bit versions of Linux are supported in this code; 32-bit ports of KUtrace are possible but difficult. All the code is open sourced, most of it under the BSD three-clause license. The code specific to freebsd uses the BSD2 license, and the code specific to risc-v is licensed under GPL-2.0. The Linux loadable module(s) are required by Linux to be licensed under GPL-2.0. About Low-overhead tracing of all Linux kernel-user transitions, for serious performance analysis. Includes kernel patches, loadable module, and post-processing software. Output is HTML/SVG per-CPU-core timeline that you can pan/zoom down to the nanosecond. Resources Readme License View license Activity Stars 530 stars Watchers 27 watching Forks 49 forks Report repository Releases No releases published Packages Languages HTML 72.8% C++ 14.7% C 12.3% Other 0.2%",
    "commentLink": "https://news.ycombinator.com/item?id=40972099",
    "commentBody": "KUtrace: Low-overhead Linux kernel tracing facility (github.com/dicksites)132 points by luu 20 hours agohidepastfavorite26 comments ndesaulniers 18 hours agoEarly in my career, I reached out to the author and was able to grab lunch with him; he was about to retire! Was insightful to hear his thoughts on system performance, particularly systems involving more than one machine, which is something they studied deeply. It gave me appreciation for the amount of knowledge one accumulates over a career, and what a loss it is to an organization when one so knowledgeable retires. reply crissaegrim 12 hours agoprevI had the pleasure to work with Dick on getting KUtrace to work on Android devices last year. It was a great experience to work with one of the greats in systems performance. He was a wealth of information regarding performance bottlenecks and optimizations. KUtrace is absolutely one of the most powerful tools I've used for deeply understanding performance bottlenecks (after isolating issues) such as poor scheduling behavior. I would highly recommend reading his book \"Understanding Software Dynamics\" [1] if you are interested in learning more about KUtrace or performance bottlenecks/optimizations in general. The book is quite dense and dives deep into the performance characteristics of many examples of the five fundamental resources (according to Dick): CPU, Memory, Disk/SSD, Network, and Software critical sections. [1]: https://www.oreilly.com/library/view/understanding-software-... reply anonymousDan 4 hours agoparentThanks, looks interesting. Does it cover measuring memory bandwidth consumption? This is something I feel there is a lack of good tooling for. reply fschutze 10 hours agoparentprevInteresting! Does it also work on non-rooted Android devices? reply crissaegrim 10 hours agorootparentNo it unfortunately does not. You require root to remount the read-only system partition, to insert the kernel modules, and to turn SELinux off. We used a \"userdebug\" build to get root, but I imagine most of this could also be done with a phone rooted through other means (I haven't tried it, however). reply CalChris 19 hours agoprevSite's article Benchmarking \"Hello, World!\" is basically a KUTrace tutorial. https://queue.acm.org/detail.cfm?id=3291278 Also, https://www.youtube.com/watch?v=D_qRuKO9qzM reply usr1106 13 hours agoprevSounds very interesting. But it works by patching the kernel, not just using eBPF like many performance tools recently. So it needs active maintenance all the time considering the current velocity of internal kernel changes. And I would not be surprised if it didn't build or work correctly if you have a heavily patched and customized kernel. On the positive side at a first glimpse the maintenance to adapt to new kernels looks very active. reply li4ick 12 hours agoprevHis book, \"Understanding Software Dynamics\", is one of the best technical books I've ever read. Top 3 for me. reply Quizzical4230 9 hours agoparentIf you don't mind me asking, what are the other two? reply li4ick 5 hours agorootparent- Managing Gigabytes - Hacker's Delight reply Quizzical4230 1 hour agorootparentThanks a bunch! Will check them out. reply jeroenvlek 4 hours agorootparentprevMuch appreciated. I bought all 3. reply c0deR3D 18 hours agoprevOut of curious, does BPF now capable of capturing all the context switch events such as CPU trap? Also, if the overhead is negligible, maybe the author can try to merge this into mainline with the use of static key to make the incurred overhead switchable. In spite of the static key, the degree of the accompanied inteferences on cache and branch predictor might be an intriguing topic though. reply tanelpoder 17 hours agoparentLots of the low level exception/trap/fault handling functions are blacklisted, probably to avoid lockups and unwanted recursion mayhem: $ sudo wc -l /sys/kernel/debug/kprobes/blacklist 783 /sys/kernel/debug/kprobes/blacklist Edit: Perhaps an alternative approach would be to attach probes to relevant (precise) PMU events. There's also this prototype of adding breakpoint/watchpoint support to eBPF [1]. But actually doing stuff within this context may get complicated very fast, so would need to be severely limited, if feasible at all. [1] https://ebpf.io/summit-2020-slides/eBPF_Summit_2020-Lightnin... reply mydriasis 2 hours agoprevI thought this read \"training facility\", and I was excited to sign up! reply elric 12 hours agoprevIs the naming intentional? Or just a weird coincidence? Kut being Dutch for cunt, by an author called dick..? reply crissaegrim 11 hours agoparentKUtrace stands for \"Kernel-User\" trace as (most of) the tracepoints are at the kernel-userspace boundary. So it is a coincidence. reply mgaunard 6 hours agoprevWhy not just use the eBPF system? reply jeffbee 4 hours agoparentMost people would. The author makes a comparison in this interview: https://www.usenix.org/system/files/login/articles/login_fal... Note also that this work emerged within Google a decade before eBPF was really useful. reply mgaunard 36 minutes agorootparentArgument is that KUTrace is faster, which honestly I'm not sure of. reply wyldfire 3 hours agorootparentprevWasn't there similar useful stuff from systemtap before eBPF? That's been around for quite a while. reply tanelpoder 40 minutes agorootparentThe other alternative is LTTng, released in 2005 (requires loading a kernel module): https://lttng.org/ reply jeffbee 3 hours agorootparentprevTrue again, but SystemTap overhead is 20-100x higher. reply zokier 1 hour agorootparentSystemtap, for whatever reason, remained largely RedHat solution. It didn't really have much traction outside that diaspora. reply xoranth 12 hours agoprev [–] How would this interact with `io_uring`, especially the polling modes (IO_SETUP_SQPOLL, IO_SETUP_IOPOLL)? reply rwmj 10 hours agoparent [–] You still have to wait for your cache to reload from main memory, or for disk or network I/O, or for processes to be scheduled to run, so while it's likely more efficient than epoll approaches, I doubt there's any really fundamental difference in the performance problems you would find. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "KUtrace is a low-overhead Linux kernel tracing tool designed to analyze performance by tracking kernel-user transitions.",
      "It produces detailed per-CPU-core timelines that can be zoomed to the nanosecond, aiding in solving performance issues in datacenters and real-time autonomous driving.",
      "The tool supports x86 and ARM processors, is open-sourced under various licenses, and includes files from the book \"Understanding Software Dynamics\" by Richard L. Sites."
    ],
    "commentSummary": [
      "KUtrace is a low-overhead Linux kernel tracing facility designed to help understand performance bottlenecks, particularly in multi-machine systems.",
      "Unlike eBPF tools, KUtrace requires kernel patches and root access, necessitating active maintenance.",
      "The tool is praised for its effectiveness, with recommendations for the book \"Understanding Software Dynamics\" by its author, Dick Sites, for further insights into system performance."
    ],
    "points": 132,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1721083310
  },
  {
    "id": 40978731,
    "title": "I am starting an AI+Education company",
    "originLink": "https://twitter.com/karpathy/status/1813263734707790301",
    "originBody": "⚡ Excited to share that I am starting an AI+Education company called Eureka Labs. The announcement:---We are Eureka Labs and we are building a new kind of school that is AI native.How can we approach an ideal experience for learning something new? For example, in the case… pic.twitter.com/RHPkqdjB8R— Andrej Karpathy (@karpathy) July 16, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40978731",
    "commentBody": "I am starting an AI+Education company (twitter.com/karpathy)119 points by bilsbie 1 hour agohidepastfavorite46 comments MarcScott 32 minutes agoI think teaching is one of the few roles that can't be replaced by AI. If you're a self-motivated learner, eager to gain new skills, then AI is perfect for you. Having a virtual Feynman coach you through a Physics course is perfect. Most learners, the world over, are not self-motivated. The pandemic showed us exactly what children would prefer to do, when they don't have a physical teacher standing over them, which is bugger all. We send kids to school, in the hope they get some education, but the reality is that we use schools for free childcare while we work. If parents have to additionally monitor their child's learning, it breaks down pretty quickly. I see AI being more of a teaching assistant, rather than a replacement for teachers. Having been in the education game for over twenty five years, I know the difference in impact when comparing virtual learning to in-person training. reply owenpalmer 2 minutes agoparent> I think teaching is one of the few roles that can't be replaced by AI So far, AI can't replace good teachers. But there aren't that many good teachers. In my experience, GPT4 is better at explaining advanced concepts than 70% of college professors. Unfortunately, education is often oriented around this horrifyingly archaic method of instruction, which prevents people from imagining what an AI oriented system could look like. reply OmarShehata 16 minutes agoparentprev> Most learners, the world over, are not self-motivated this seems like a bizarre conclusion. In my experience, most people, the world over, are in fact self motivated. You won't see that if you have a very narrow definition of what is it that they're supposed to be learning kids aren't motivated to do boring math drills, because they don't see why it matters to their life (the real answer is: it does not, they are not wrong). I appreciated hearing this echoed by Conrad Wolfram in a recent PIMA episode: https://freakonomics.com/podcast/why-do-we-still-teach-peopl... reply sillysaurusx 12 minutes agorootparentI think it might be worth considering whether you’ve had a privileged upbringing. Thinking back on it, the majority of people probably would have been content to play games all day. You could argue that that’s learning, but unfortunately it’s not the kind of learning that society tends to reward. I’ve heard that kids in upper middle class circles are totally different in this regard though. Maybe they want to do more on average. reply abdullahkhalids 2 minutes agorootparentprev> kids aren't motivated to do boring math drills, because they don't see why it matters to their life (the real answer is: it does not, they are not wrong). Most kid athletes are also not self-motivated to run laps, or do boring repetitive drills, when they know from experience that these activities help them win games within the next few months. Usually need a coach to force them to do them. Same for young music players. Practicing scales endlessly does make you a better musician. But they won't do it till forced. The only reason kids don't like running laps or playing scales or doing math drills is because they are boring. Nothing else. reply choppaface 1 minute agorootparentprevFeedback is a critical part of education as well as motivation for learning. But the act of giving feedback is very hard to scale, even for virtual learning. Enter an LLM chatbot, which is imperfect but can fill a lot of gaps in expectation. Chatbots certainly aren’t for everybody, but the large gains in accuracy in years past make them on average more effective. reply endisneigh 6 minutes agorootparentprevI’m curious - have you ever taught in a public school? reply moffkalast 11 minutes agorootparentprevYeah if anything, current education system is so garbage that it manages to completely demotivate curious kids who want to genuinely learn. It's designed around adults that need to run the place, runs at the wrong pace for most students and focuses on PTSD-inducing high anxiety testing constantly because it's easy to do for the teachers. Not to mention piles of pointless busywork as homework that's been proven to not help with learning at all. reply lacy_tinpot 3 minutes agoparentprevMost people that are academically inclined are self motivated and have a desire to learn more. Most people aren't academically inclined so it follows that most people aren't academically self motivated. Therefore among those that are academically inclined it is important to provide them with all the tools necessary because they're the ones that will most likely excel in an academic environment. It is odd that the curriculum tends to accommodate people that aren't academically inclined at the expense of those that actually want to learn. People that aren't academically inclined should not be forced to learn, or at least forced only insofar as they're baseline literacy so that they function in today's world. reply ugh123 7 minutes agoparentprev> I see AI being more of a teaching assistant, rather than a replacement for teachers That is exactly what he says in the tweet. I think the problem with traditional teaching, as in any skilled profession, is often in short supply and underpaid, not happy, and unable to keep up with 25+ kids in a class. The world needs orders of magnitude more teachers that are highly competent and more easily accessible. AI could massively scale high quality teaching with still a teacher in the loop. reply ilaksh 25 minutes agoparentprevAI certainly can't completely replace teachers, but the potential gains for personal tutoring from SOTA LLMs still seem enormous to me. And I'm not trying to make a general argument against in person training. But I think the details of how virtual learning happens matters quite a lot. AI can make it much more personalized and make tutoring relatively affordable. Don't you think? reply dinobones 23 minutes agorootparentAI has personally tutored me about obscure, deep linear algebra concepts. It's so great to get applied examples and be able to ask why/how something works, rather than reading a stuffy Wikipedia article or math textbook. It's been extremely effective for me, where reading a math textbook/wikipedia article seemed like too much effort, but a friendly conversation with my AI tutor was just fine. reply brendoelfrendo 3 minutes agorootparentHow can you bring yourself to trust the AI? Just yesterday a friend and I asked Chat-GPT a physics question, and for some reason his assistant asserted that the speed of light was 3,000 m/s, which is off by two orders of magnitude. We know that's wrong so we can tell the AI to do it again but right this time, but if it was explaining a concept we didn't already understand, I can't see how the output would be any more meaningful than asking a random stranger and trusting their response. reply eldaisfish 21 minutes agorootparentprevA major part of the learning process is your peers. Learning is groups has benefits especially when you can bounce ideas off other humans. You cannot replace that with a machine. reply ilaksh 19 minutes agorootparentGotcha. So I guess the question is, can an AI run a Zoom meeting or interactive multiplayer learning game with a bunch of kids on it? Have to admit that might be a stretch. reply mym1990 8 minutes agoparentprevWeirdly enough I was not very curious in my schooling years, barely getting through classes. As I have grown up, I have so much more curiosity about the world and my willingness to actually learn has skyrocketed. I feel like this could be a great space for adults who are seeking to do the same. I always thought calculus would be daunting to learn(and I still do), but with AI tools I feel like I can approach it with a different mindset. reply zulban 12 minutes agoparentprevI imagine a world where a 19 year old takes a few courses in first aid, child psychology basics, and now they're a licensed \"class supervisor\". They aren't university educated but the AI is what offers personalized learning and expertise to the students. Most teachers today aren't experts anyway, we just pretend they are. So I'm not sure \"replaced by AI\" is the right way to frame the conversation. Instead, it may change education. reply SubiculumCode 9 minutes agoparentprevMost highschool / gradeschool is being forced to sit in a chair being baby sat until 3PM each day, with no opportunity to select goals, and act towards them. My daughter transitioned to a Montessori jr high, and she went from enduring school to actively engaging in self-directed learning. reply onemoresoop 22 minutes agoparentprevAI can augment teachers though. reply ugh123 9 minutes agoprevAccording to the picture in the tweet, you could grow 3 arms by following these courses! reply vunderba 0 minutes agoparentI don't understand how he missed that. This is the same issue I have with large language model as coding assistants, since you're effectively not in the driver seat - you're acting more like a code reviewer, and I think that that passivity eventually causes critical observational skills to atrophy. reply shellfishgene 2 minutes agoparentprevAnd after all those Feynman Physics courses they placed their solar panels in the shade ;) reply layer8 3 minutes agoparentprevAI is in an arms race. reply mads 5 minutes agoparentprevI am getting Jehovah's Witness vibes from it. reply yinser 32 minutes agoprevFor those without an X account: Website: https://eurekalabs.ai GitHub: https://github.com/EurekaLabsAI reply layer8 7 minutes agoparenthttps://xcancel.com/karpathy/status/1813263734707790301 reply throwedu 0 minutes agoprevThats a zillion dollar company right there reply dyarosla 16 minutes agoprevWhat’s the differentiation with other similar ventures? For instance, Synthesis[0] is an instructor designed, AI supplemented site for early math. https://www.synthesis.com/ It really seems like the distinction for these kinds of AI-education ventures comes down to the human educator(s) involved. reply jppope 10 minutes agoprevthank god for Andrej Karpathy... doing amazing work. I love this concept and look forward to seeing how things develop reply boyka 33 minutes agoprevCan someone please provide the mentioned links? (For those without X account) reply layer8 6 minutes agoparenthttps://xcancel.com/karpathy/status/1813263734707790301 reply ryandrake 23 minutes agoprevHopefully the AI knows whether 9.11 is greater or less than 9.9, something ChatGPT seems to have had a problem figuring out[1]. 1: https://x.com/liujc1998/status/1813244909501182310 reply spencerchubb 13 minutes agoparentthat's a tokenization issue. every tool has strengths and weaknesses. why does it matter whether an LLM can compare numbers? that can be done trivially in any programming language reply Ylpertnodi 0 minutes agorootparentAs an AI layman (downloaded Claude for android as a result of hn, just today) \"why does it matter whether an LLM can compare numbers?\", is rather important to me. Probably others, also. reply demondemidi 7 minutes agoprevMight wanna wait until the hype cycle is over in a few years. reply treprinum 43 minutes agoprevCool! Does Eureka Labs/Andrej plan to offer grad/PhD-level courses (or better) with similar topics to CS236, CS224N etc. in the future as well? reply juliushuijnk 34 minutes agoprevHope it will enable all of us to become not just smarter, but also wiser :) reply epups 29 minutes agoprevIt seems that this would compete with Khan Academy for a similar space. Perhaps Karpathy will aim for adults instead? reply minimaxir 28 minutes agoparentKhan Academy is more than AI/LLM courses. reply simonw 26 minutes agorootparentIt sounds like Andrej's ambitions stretch beyond AI/LLM courses too - but they're a natural \"first course\" starting point because that's where his own teaching expertise is focused. reply nedrylandJP 25 minutes agoprevAny openings for a 40-something career-changing educator? reply beardedwizard 7 minutes agoparentI would suggest you contact karpathy and not randoms who posted/commented this reply ffhhj 32 minutes agoprev [–] > we are heads down building LLM101n Kind of ironic an AI isn't building it. reply jasonsb 28 minutes agoparentDon't worry about it, I'm sure that \"AI\" is working on something much bigger. reply simonw 25 minutes agoparentprev [–] From the announcement: > The teacher still designs the course materials, but they are supported, leveraged and scaled with an AI Teaching Assistant who is optimized to help guide the students through them. So the AI isn't expected to build the materials. reply ilaksh 21 minutes agorootparent [–] I assume that it would be an agent that the teacher prompts in a loop to build and refine the course material. Probably with an upload so if they want to bring the full thing ready to go they can and in that case the data just needs to be formatted by the LLM for their database, but also can have the AI fill things in. And then check what came out and refine it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Andrej Karpathy has announced the launch of his new company, Eureka Labs, which focuses on integrating AI with education.",
      "Eureka Labs aims to create an AI-native school designed to revolutionize the learning experience.",
      "The announcement was made via a tweet, generating significant interest in the tech and education communities."
    ],
    "commentSummary": [
      "Andrej Karpathy is starting an AI+Education company, sparking discussions on the role of AI in education.",
      "Opinions vary on AI's potential, with some arguing it can assist but not replace teachers, while others believe AI can enhance and personalize learning experiences.",
      "The debate highlights the evolving landscape of education technology, emphasizing the need for innovative solutions to improve teaching and learning."
    ],
    "points": 119,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1721152655
  }
]
