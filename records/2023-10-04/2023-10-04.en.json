[
  {
    "id": 37752632,
    "title": "Running Stable Diffusion XL 1.0 in 298MB of RAM",
    "originLink": "https://github.com/vitoplantamura/OnnxStream/tree/846da873570a737b49154e8f835704264864b0fe",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up vitoplantamura / OnnxStream Public Notifications Fork 45 Star 1.2k Code Issues 10 Pull requests 1 Actions Projects Security Insights vitoplantamura/OnnxStream 846da87357 1 branch 1 tag Go to file Code Latest commit vitoplantamura Added support for Stable Diffusion XL 1.0 Base 846da87 Git stats 7 commits Files Type Name Latest commit message Commit time assets Added support for Stable Diffusion XL 1.0 Base onnx2txt onnx2txt: if both \"value_info\" and \"initializer\" are present, take \"i… src Added support for Stable Diffusion XL 1.0 Base .gitignore onnx2txt: if both \"value_info\" and \"initializer\" are present, take \"i… LICENSE Initial commit README.md Added support for Stable Diffusion XL 1.0 Base README.md 📣 UPDATE (OCTOBER 2023) 📣 Added support for Stable Diffusion XL 1.0 Base! And it runs on a RPI Zero 2! Please see the section below 👇 OnnxStream The challenge is to run Stable Diffusion 1.5, which includes a large transformer model with almost 1 billion parameters, on a Raspberry Pi Zero 2, which is a microcomputer with 512MB of RAM, without adding more swap space and without offloading intermediate results on disk. The recommended minimum RAM/VRAM for Stable Diffusion is typically 8GB. Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream. OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from WeightsProvider. A WeightsProvider specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom WeightsProvider can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word \"Stream\" in \"OnnxStream\"). Two default WeightsProviders are available: DiskNoCache and DiskPrefetch. OnnxStream can consume even 55x less memory than OnnxRuntime while being only 0.5-2x slower (on CPU, see the Performance section below). Stable Diffusion 1.5 These images were generated by the Stable Diffusion example implementation included in this repo, using OnnxStream, at different precisions of the VAE decoder. The VAE decoder is the only model of Stable Diffusion that could not fit into the RAM of the Raspberry Pi Zero 2 in single or half precision. This is caused by the presence of residual connections and very big tensors and convolutions in the model. The only solution was static quantization (8 bit). The third image was generated by my RPI Zero 2 in about 3 hours 1.5 hours (using the MAX_SPEED option when compiling). The first image was generated on my PC using the same latents generated by the RPI Zero 2, for comparison: VAE decoder in W16A16 precision: VAE decoder in W8A32 precision: VAE decoder in W8A8 precision, generated by my RPI Zero 2 in about 3 hours 1.5 hours (using the MAX_SPEED option when compiling): Stable Diffusion XL 1.0 (base) The OnnxStream Stable Diffusion example implementation now supports SDXL 1.0 (without the Refiner). The ONNX files were exported from the SDXL 1.0 implementation of the Hugging Face's Diffusers library (version 0.19.3). SDXL 1.0 is significantly more computationally expensive than SD 1.5. The most significant difference is the ability to generate 1024x1024 images instead of 512x512. To give you an idea, generating a 10-steps image with HF's Diffusers takes 26 minutes on my 12-core PC with 32GB of RAM. The minimum recommended VRAM for SDXL is typically 12GB. OnnxStream can run SDXL 1.0 in less than 300MB of RAM and therefore is able to run it comfortably on a RPI Zero 2, without adding more swap space and without writing anything to disk during inference. Generating a 10-steps image takes about 11 hours on my RPI Zero 2. SDXL Specific Optimizations The same set of optimizations for SD 1.5 has been used for SDXL 1.0, but with the following differences. As for the UNET model, in order to make it run in less than 300MB of RAM on the RPI Zero 2, UINT8 dynamic quantization is used, but limited to a specific subset of large intermediate tensors. The situation for the VAE decoder is more complex than for SD 1.5. SDXL 1.0's VAE decoder is 4x the size of SD 1.5's, and consumes 4.4GB of RAM when run with OnnxStream in FP32 precision. In the case of SD 1.5 the VAE decoder is statically quantized (UINT8 precision) and this is enough to reduce RAM consumption to 260MB. Instead, the SDXL 1.0's VAE decoder overflows when run with FP16 arithmetic and the numerical ranges of its activations are too large to get good quality images with UINT8 quantization. So we are stuck with a model that consumes 4.4GB of RAM, which cannot be run in FP16 precision and which cannot be quantized in UINT8 precision. (NOTE: there is at least one solution to the FP16 problem, but I have not investigated further since even running the VAE decoder in FP16 precision, the total memory consumed would be divided by 2, so the model would ultimately consume 2.2GB instead of 4.4GB, which is still way too much for the RPI Zero 2) The inspiration for the solution came from the implementation of the VAE decoder of the Hugging Face's Diffusers library, i.e. using tiled decoding. The final result is absolutely indistinguishable from an image decoded by the full decoder and in this way it is possible to reduce RAM memory consumption from 4.4GB to 298MB! The idea is simple. The result of the diffusion process is a tensor with shape (1,4,128,128). The idea is to split this tensor into 5x5 (therefore 25) overlapping tensors with shape (1,4,32,32) and to decode these tensors separately. Each of these tensors is overlapped by 25% on the tile to its left and the one above. The decoding result is a tensor with shape (1,3,256,256) which is then appropriately blended into the final image. For example, this is an image generated by the tiled decoder with blending manually turned off in the code. You can clearly see the tiles in the image: While this is the same image with blending turned on. This is the final result: This is another image generated by my RPI Zero 2 in about 11 hours: (10 steps, Euler Ancestral) Features of OnnxStream Inference engine decoupled from the WeightsProvider WeightsProvider can be DiskNoCache, DiskPrefetch or custom Attention slicing Dynamic quantization (8 bit unsigned, asymmetric, percentile) Static quantization (W8A8 unsigned, asymmetric, percentile) Easy calibration of a quantized model FP16 support (with or without FP16 arithmetic) 25 ONNX operators implemented (the most common) Operations executed sequentially but all operators are multithreaded Single implementation file + header file XNNPACK calls wrapped in the XnnPack class (for future replacement) OnnxStream depends on XNNPACK for some (accelerated) primitives: MatMul, Convolution, element-wise Add/Sub/Mul/Div, Sigmoid and Softmax. Performance Stable Diffusion consists of three models: a text encoder (672 operations and 123 million parameters), the UNET model (2050 operations and 854 million parameters) and the VAE decoder (276 operations and 49 million parameters). Assuming that the batch size is equal to 1, a full image generation with 10 steps, which yields good results (with the Euler Ancestral scheduler), requires 2 runs of the text encoder, 20 (i.e. 2*10) runs of the UNET model and 1 run of the VAE decoder. This table shows the various inference times of the three models of Stable Diffusion, together with the memory consumption (i.e. the Peak Working Set Size in Windows or the Maximum Resident Set Size in Linux). Model / Library 1st run 2nd run 3rd run FP16 UNET / OnnxStream 0.133 GB - 18.2 secs 0.133 GB - 18.7 secs 0.133 GB - 19.8 secs FP16 UNET / OnnxRuntime 5.085 GB - 12.8 secs 7.353 GB - 7.28 secs 7.353 GB - 7.96 secs FP32 Text Enc / OnnxStream 0.147 GB - 1.26 secs 0.147 GB - 1.19 secs 0.147 GB - 1.19 secs FP32 Text Enc / OnnxRuntime 0.641 GB - 1.02 secs 0.641 GB - 0.06 secs 0.641 GB - 0.07 secs FP32 VAE Dec / OnnxStream 1.004 GB - 20.9 secs 1.004 GB - 20.6 secs 1.004 GB - 21.2 secs FP32 VAE Dec / OnnxRuntime 1.330 GB - 11.2 secs 2.026 GB - 10.1 secs 2.026 GB - 11.1 secs In the case of the UNET model (when run in FP16 precision, with FP16 arithmetic enabled in OnnxStream), OnnxStream can consume even 55x less memory than OnnxRuntime while being 0.5-2x slower. Notes: The first run for OnnxRuntime is a warm up inference, since its InferenceSession is created before the first run and reused for all the subsequent runs. No such thing as a warm up exists for OnnxStream since it is purely eager by design (however subsequent runs can benefit from the caching of the weights files by the OS). At the moment OnnxStream doesn't support inputs with a batch size != 1, unlike OnnxRuntime, which can greatly speed up the whole diffusion process using a batch size = 2 when running the UNET model. In my tests, changing OnnxRuntime's SessionOptions (like EnableCpuMemArena and ExecutionMode) produces no significant difference in the results. Performance of OnnxRuntime is very similar to that of NCNN (the other framework I evaluated), both in terms of memory consumption and inference time. I'll include NCNN benchmarks in the future, if useful. Tests were run on my development machine: Windows Server 2019, 16GB RAM, 8750H cpu (AVX2), 970 EVO Plus SSD, 8 virtual cores on VMWare. Attention Slicing and Quantization The use of \"attention slicing\" when running the UNET model and the use of W8A8 quantization for the VAE decoder were crucial in reducing memory consumption to a level that allowed execution on a RPI Zero 2. While there is a lot of information on the internet about quantizing neural networks, little can be found about \"attention slicing\". The idea is simple: the goal is to avoid materializing the full Q @ K^T matrix when calculating the scaled dot-product attention of the various multi-head attentions in the UNET model. With an attention head count of 8 in the UNET model, Q has a shape of (8,4096,40), while K^T has a shape of (8,40,4096): so the result of the first MatMul has a final shape of (8,4096,4096), which is a 512MB tensor (in FP32 precision): The solution is to split Q vertically and then to proceed with the attention operations normally on each chunk of Q. Q_sliced has a shape of (1,x,40), where x is 4096 (in this case) divided by onnxstream::Model::m_attention_fused_ops_parts (which has a default value of 2, but can be customized). This simple trick allows to lower the overall consumed memory of the UNET model from 1.1GB to 300MB (when the model is run in FP32 precision). A possible alternative, certainly more efficient, would be to use FlashAttention, however FlashAttention would require writing a custom kernel for each supported architecture (AVX, NEON etc), bypassing XnnPack in our case. How OnnxStream works This code can run a model defined in the path_to_model_folder/model.txt: (all the model operations are defined in the model.txt text file; OnnxStream expects to find all the weights files in that same folder, as a series of .bin files) #include \"onnxstream.h\" using namespace onnxstream; int main() { Model model; // // Optional parameters that can be set on the Model object: // // model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider) // model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model) // model.write_range_data( ... ); // writes a range data file (useful after calibration) // model.m_range_data_calibrate = true; // calibrates the model // model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision) // model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference // model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models) // model.m_fuse_ops_in_attention = true; // enables attention slicing // model.m_attention_fused_ops_parts = ... ; // see the \"Attention Slicing\" section above // model.read_file(\"path_to_model_folder/model.txt\"); tensor_vector data; ... // fill the tensor_vector with the tensor data. \"tensor_vector\" is just an alias to a std::vector with a custom allocator. Tensor t; t.m_name = \"input\"; t.m_shape = { 1, 4, 64, 64 }; t.set_vector(std::move(data)); model.push_tensor(std::move(t)); model.run(); auto& result = model.m_data[0].get_vector(); ... // process the result: \"result\" is a reference to the first result of the inference (a tensor_vector as well). return 0; } The model.txt file contains all the model operations in ASCII format, as exported from the original ONNX file. Each line corresponds to an operation: for example this line represents a convolution in a quantized model: Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1 In order to export the model.txt file and its weights (as a series of .bin files) from an ONNX file for use in OnnxStream, a notebook (with a single cell) is provided (onnx2txt.ipynb). Some things must be considered when exporting a Pytorch nn.Module (in our case) to ONNX for use in OnnxStream: When calling torch.onnx.export, dynamic_axes should be left empty, since OnnxStream doesn't support inputs with a dynamic shape. It is strongly recommended to run the excellent ONNX Simplifier on the exported ONNX file before its conversion to a model.txt file. How to Build the Stable Diffusion example on Linux/Mac/Windows/Termux Windows only: start the following command prompt: Visual Studio Tools > x64 Native Tools Command Prompt. Mac only: make sure to install cmake: brew install cmake. First you need to build XNNPACK. Since the function prototypes of XnnPack can change at any time, I've included a git checkout that ensures correct compilation of OnnxStream with a compatible version of XnnPack at the time of writing: git clone https://github.com/google/XNNPACK.git cd XNNPACK git rev-list -n 1 --before=\"2023-06-27 00:00\" master git checkoutmkdir build cd build cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF .. cmake --build . --config Release Then you can build the Stable Diffusion example.is for example /home/vito/Desktop/XNNPACK or C:\\Projects\\SD\\XNNPACK (on Windows): git clone https://github.com/vitoplantamura/OnnxStream.git cd OnnxStream cd src mkdir build cd build cmake -DMAX_SPEED=ON -DXNNPACK_DIR= .. cmake --build . --config Release Important: the MAX_SPEED option allows to increase performance by about 10% in Windows, but by more than 50% on the Raspberry Pi. This option consumes much more memory at build time and the produced executable may not work (as was the case with Termux in my tests). So in case of problems, the first attempt to make is to set MAX_SPEED to OFF. Now you can run the Stable Diffusion example. In the case of Stable Diffusion 1.5, the weights for the example can be downloaded from the Releases of this repo (about 2GB). In the case of Stable Diffusion XL 1.0 Base, the weights can be downloaded from Hugging Face (about 8GB): git lfs install git clone --depth=1 https://huggingface.co/vitoplantamura/stable-diffusion-xl-base-1.0-onnxstream These are the command line options of the Stable Diffusion example: --xl Runs Stable Diffusion XL 1.0 instead of Stable Diffusion 1.5. --models-path Sets the folder containing the Stable Diffusion models. --ops-printf During inference, writes the current operation to stdout. --output Sets the output PNG file. --decode-latents Skips the diffusion, and decodes the specified latents file. --prompt Sets the positive prompt. --neg-prompt Sets the negative prompt. --steps Sets the number of diffusion steps. --save-latents After the diffusion, saves the latents in the specified file. --decoder-calibrate (ONLY SD 1.5) Calibrates the quantized version of the VAE decoder. --decoder-fp16 (ONLY SD 1.5) During inference, uses the FP16 version of the VAE decoder. --not-tiled (ONLY SDXL 1.0) Don't use the tiled VAE decoder. --rpi Configures the models to run on a Raspberry Pi. --rpi-lowmem (ONLY SDXL 1.0) Configures the models to run on a Raspberry Pi Zero 2. Credits The Stable Diffusion implementation in sd.cpp is based on this project, which in turn is based on this project by @EdVince. The original code was modified in order to use OnnxStream instead of NCNN. About Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2 (or in 298MB of RAM) Topics raspberry-pi machine-learning onnx stable-diffusion Resources Readme License View license Activity Stars 1.2k stars Watchers 17 watching Forks 45 forks Report repository Releases 1 win-x64-with-weights Latest Languages C++ 96.0% Jupyter Notebook 3.5% CMake 0.5% Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37752632",
    "commentBody": "Running Stable Diffusion XL 1.0 in 298MB of RAMHacker NewspastloginRunning Stable Diffusion XL 1.0 in 298MB of RAM (github.com/vitoplantamura) 465 points by Robin89 19 hours ago| hidepastfavorite142 comments omneity 18 hours agoFascinating. The money quote:\"OnnxStream can consume even 55x less memory than OnnxRuntime while being only 0.5-2x slower\"The trade-off between (V)RAM use and inference time sounds like it could be advantageous in some scenarios, and not just when RAM is constrained like in the RPi case.I actually wonder if this weight unloading approach can be used to handle larger batch sizes in the same amount of RAM, in effect increasing throughput massively at the cost of latency. reply Mockapapella 17 hours agoparentI want this for LLMs. Having that much less of a memory footprint would allow us to put more models on a GPU at a time, and assuming the clock could keep up it could more than make up for the loss in inference speed per individual model reply SigmundurM 18 hours agoparentprev\"0.5-2x slower\" must be a typo on their part right? If something is 0.5x slower, then it is 2x faster.I assume they meant to say \"1.5-2x slower\". reply dahart 18 hours agorootparentMaybe they meant 50%-200% slower, in which case the x-factor range would really be 1.5x to 3x? reply hinkley 15 hours agorootparent200% slower is 3x as long, yes. reply dr_dshiv 14 hours agorootparentI love making fun of people that don’t understand percentages… wait, wat? reply froggit 14 hours agorootparent117. 472% of grade school students are unable to readily convert between fractions and percentages.38.157% of informally provided statistics are made up on the spot under the assumption nobody will actually check. reply Lewton 1 hour agorootparent9% of people consists entirely of shoulders, elbows and knees reply MR4D 13 hours agorootparentprevYou have a typo - it’s actually 83.157% reply hinkley 14 hours agorootparentprevWho is the intended butt of your joke here?And explain to me why it isn&#x27;t you? reply CookieCrisp 14 hours agorootparentI am pretty sure they intended the butt of the joke to be intentionally themselves reply jychang 14 hours agorootparentprevHe’s being self deprecating, yes replyvitoplantamura 5 hours agorootparentprevhi,I&#x27;m the author.I have never questioned the clarity of that sentence, at least until today :-)By \"0.5x\" I mean \"0.5 times or 0.5 multiplied by the reference time\" where \"reference time\" is the inference time of OnnxRuntime. So I&#x27;m actually meaning \"50%\".I think the expression \"0.5x slower\", taken by itself, could be misleading, but in the context of the original sentence it becomes clearer (\"while being only 0.5-2x slower\", the \"only\" is important here!!!).But I think the general context of that sentence defines its meaning. I am referring to the fact that under no circumstances could my project, with its premises, be in any scenario even a single millisecond faster than OnnxRuntime. Then in the second paragraph of the README the goal of the project is stated, which is precisely to trade off inference time for RAM usage! Obviously combined with the fact that the performance data is clearly reported and that I repeat several times that the generation of a single image takes hours or even dozens of hours.However, given the possible misunderstanding, I will correct the sentence in the next few days. reply nicolaslegland 1 hour agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wikipedia:Two_times_does_not_m... reply robertlagrant 3 hours agorootparentprevI still don&#x27;t understand what you mean. 0.5 multiplied by the reference runtime is faster.Do you mean it increases the runtime by 50%-200%? reply vitoplantamura 2 hours agorootparentBy \"0.5x slower\" I mean \"50% slower\" :-) reply croes 16 hours agorootparentprevIt depends. What do you think if I say it&#x27;s 1x slower?Is it as fast as the original or does it take twice as long? reply emi2k01 16 hours agorootparentTwice as long. As fast as the original would be \"0x slower\" or \"1x as fast\".People should just use duration instead of speed as you did at the end: \"takes twice as long\", \"takes 1&#x2F;3 of the time...\" reply lcnPylGDnU4H9OF 11 hours agorootparent> As fast as the original would be \"0x slower\" or \"1x as fast\".I have played a fair number of incremental games and this quickly became a pet peeve of mine. So many will say things like \"2x more\" and it will actually be \"2x as much\". Fortunately, I don&#x27;t recall any which actually switch between the meanings but it&#x27;s so commonly a guessing game until I figure it out. reply maxlin 17 hours agorootparentprevCommunication is hard. If they said \"Takes 50% to 200% more time\" it would have been clearer reply froggit 12 hours agorootparentWhat? No, that&#x27;s confusing enough it&#x27;s almost hostile. The fact thst your math is wrong is proof enough. 50% to 200% more time is 1.5x to 3x slower.I don&#x27;t like how it was worded by the author. But all you&#x27;ve done is essentially invert the wording while making the math MORE difficult in the process. reply wayfinder 12 hours agorootparentUmm what? I hear “50% more time” all the time.50% to 200% is 0.5x slower to 2x slower.People seem to be confusing “% slower&#x2F;more time” vs “% of current time” reply bee_rider 9 hours agorootparentprev1x slower would be 2x as slow, I think, is how people are interpreting it.I think they are correct but it is easy enough to misinterpret that it is not a good way to phrase things. reply monocasa 16 hours agoparentprevFrom my (albeit naive) reading, it doesn&#x27;t appear that that they&#x27;ve reduced the amount of memory bandwidth required, simply the size of the working set required.Since inference is generally memory bandwidth bound once you reach the level of &#x27;does this model even fit in the given system&#x27;, I&#x27;d imagine that this technique wouldn&#x27;t help much for greater throughpit via larger batch sizes. Just one instance is probably already saturating the memory controller.Maybe it&#x27;d help on the training side though? reply omneity 14 hours agorootparentThat&#x27;s true. But assuming the required memory bandwidth is not already maxed out by this, there might still be a narrow but workable \"Goldilocks zone\" for this technique to be useful. reply vinkelhake 18 hours agoprev11 hours remind me of doing raytracing on my Amiga 500 back in the day. It was definitely an overnight job for the \"final\" render. reply somat 14 hours agoparentHeh sometimes I am still doing that. modern bidirectional raytracers can do some interesting tricks. and I wanted to see caustics(the bright lines in pools). but caustics despite being bright are actually statistically rare. to get good caustics you have to unbound the render engine and just let it cook overnight.And the end result, a single image of a mediocre scene by a poor artist with amazing caustics. I won&#x27;t be quitting my day job. reply hinkley 15 hours agoparentprevDoing that low quality render first because you&#x27;d rather waste an hour being right than all night being wrong.That was about when I decided I needed other hobbies. Right before that happened some brilliant soul put out a tool that would render your scene in OpenGL so you could look at it first. I don&#x27;t think that would run on your Amiga but it (barely) ran on my machine. reply qingcharles 16 hours agoparentprevHa. Same on my 286. Set up povray, go to bed, see image before school in the morning. reply _joel 14 hours agoparentprevSame, (albeit a little later) with dodgy copy of 3DSMAX on a 386. reply Archelaos 12 hours agoparentprevIt reminds me of doing Mandelbrot fractals on my C64. Debugging my code was really hard. reply HappyDaoDude 9 hours agorootparentI am still amazed by seeing Fractals rendered in real time. My Core 2 Duo can do the initial renders at about 1080p resolution in about a second or two. Something that would have taken hours to do on an Amiga in the 80&#x27;s IF you even had the memory for that kind of storage. reply anthk 2 hours agorootparentI did 640x480 fractals at crazy speeds with Xaos on an AMD Athon, which was and is pretty well optimized. reply julienchastang 13 hours agoprevI&#x27;ve been using Stable Diffusion on a MBP via invoke.ai. Are there recommendations for better parameterization of SD? I can never match the quality of the images I find on the internet even when using the same prompt and (seemingly) the same knobs (e.g., same Model like Euler A, etc). [edited for clarification] reply brucethemoose2 13 hours agoparentThis is the best I&#x27;ve tried so far, but no mac support I don&#x27;t think. Its a feature packed fork of Fooocus, which was developed by the orginal ControlNet dev. The quality you can get from small prompts is mind boggling:https:&#x2F;&#x2F;github.com&#x2F;MoonRide303&#x2F;Fooocus-MREFor base SD 1.5, I use Volta, because its fast: https:&#x2F;&#x2F;github.com&#x2F;VoltaML&#x2F;voltaML-fast-stable-diffusion&#x2F;com...Really good SD 1.5 image quality comes from gratuitous use of finetunes, LORAs, controlnet and other augmentations. So you can, say, trace a base image for structure, specify prompting in certain areas of the image and so on. InvokeAI is actually quite feature packed, and has lots of these augmentations hidden in the nodes UI, but Volta and other UIs also expose them more directly. reply IKantRead 13 hours agoparentprevAre you using custom weights? I&#x27;m assuming you are but there is a major difference between using the default RunwayML 1.5 weights and using a model finetuned for a specific purpose.Generally the trade off is that any of the impressive finetuned models are far less generalizable then the default weights, but in practice this is not a big deal and the results can be a substantial improvement. reply doublebind 13 hours agoparentprevI have the same experience with Invoke.ai or MochiDiffusion in the MBP M1. I can only match the quality of other images with Automatic1111 (https:&#x2F;&#x2F;github.com&#x2F;AUTOMATIC1111&#x2F;stable-diffusion-webui).You’ll need more time and memory compared to Invoke or an Nvidia graphics card, but it’s not that bad: 1-2 s&#x2F;it for an image in standard 512x768px quality, 14-20 s&#x2F;it for an image in high 1024x1536px quality (Hires Fix). reply sbierwagen 13 hours agoparentprevDo they specify it&#x27;s straight from the generator? The process videos I&#x27;ve seen start with \"a girl standing in a green field\" and then an hour plus of inpainting to fix hands, pose, etc. reply liuliu 9 hours agoparentprevDraw Things added CUDA compatibility seed mode allows you to match NVDIA card generated images on Mac. reply lawlessone 18 hours agoprevThis would be really cool to have running embedded in a digital photo frame or wall painting. reply changelink 40 minutes agoparentI&#x27;ve built this a while back, using a previous version that runs Stable Diffusion on a raspberry pi zero 2 w:https:&#x2F;&#x2F;hackaday.com&#x2F;2023&#x2F;09&#x2F;19&#x2F;e-paper-news-feed-illustrate...https:&#x2F;&#x2F;github.com&#x2F;rvdveen&#x2F;epaper-slow-generative-art&#x2F; reply isoprophlex 17 hours agoparentprevI&#x27;m building exactly that with an eink display atm. Sadly, i can&#x27;t seem to be able to build the XNNPACK stuff on my pi zero 2W in the repo... reply chasd00 16 hours agorootparentthat&#x27;s an awesome idea, do you have a link to more information? reply isoprophlex 16 hours agorootparentIll write it up once it&#x27;s done and post here, if it gains traction you might see it haha.In all seriousness I can give a brief overview:- I&#x27;ll probably offload the image generation to the 5 year old intel nuc I already have as a home automation server, comfyUI in CPU mode takes 20-30 mins for a generation. Ideally it&#x27;s all self contained on the Pi but that might be beyond me, skill wise.- prompts are composed by taking time of day, season, special occasions (birthdays, xmas etc); adding random subjects from a long manually curated list; then asking gpt4 to creatively remix the prompt for variety- i have an inky impression 7.3 inch 7 color eink display and a raspberry zero stuck onto it. Right now it&#x27;ll simply download new images from the NUC every once in a while- i like wood and i dislike the jagged 3d printer aesthetic so I&#x27;ll create a frame from laser cut plywood by designing some stackable svg shapes in inkscape and sending those to a laser cutterIt works right now, functionally.Considering that I&#x27;m painstakingly writing this on a phone with a sleeping 3 week old baby on my chest it&#x27;ll be while before i have the energy to make it look like something you&#x27;d hang on your wall reply ionwake 8 hours agorootparentSounds awesome what’s your Twitter ? reply eigenvalue 18 hours agoparentprevGreat idea, where every 10 hours or so it would refresh with a new image it created itself (perhaps based on a theme supplied by the user). reply amelius 18 hours agoparentprevnext [13 more] Not very environment-friendly, though. reply agilob 18 hours agorootparentDude, we are literally using single use plastic bottles to store water for a few weeks in them. reply BHSPitMonkey 12 hours agorootparentIf we&#x27;re doing one bad thing already, we may as well do a hundred! reply amelius 17 hours agorootparentprevWe are literally using LED lighting because it saves energy over conventional light bulbs.And now we&#x27;re going to put a screen on the wall that we don&#x27;t even look at 99% of the time? reply dopidopHN 17 hours agorootparentprevNothing is stopping you to buy a reusable bottle. reply lawlessone 18 hours agorootparentprevCompared to what though?I think it might be friendlier in some aspects than fetching an image from a server running the big the models.And you don&#x27;t have to worry about service disruptions or api keysAn e-ink display doing it should only use energy when refreshes. And you could minimize refreshes to once a day, week, etcLess friendly than a photo of course. reply naillo 17 hours agorootparentprevWatch 5 seconds of a tv show with your big tv and you&#x27;ve spent that environmental cost reply aftbit 18 hours agorootparentprevWhy do you say that? The energy usage of inference? I would guess that the embodied energy of the digital photo frame is probably higher. reply numpad0 17 hours agorootparentprevThat&#x27;s where $1999 color E Ink display comes into play. reply lawlessone 17 hours agorootparentThere are cheaper.. but it might require dithering.. reply nathanfig 18 hours agorootparentprev2.5A 5V is not much power, and it would use considerably less when idling. reply notjtrig 17 hours agorootparent125 watt hours for a raspberry pi to generate an image in 10 hours compared to 7 watt hours for a 440W PC to run for 1 minute. reply lawlessone 17 hours agorootparentIs this for a regular Pi . The OP post is using a Pi Zero 2.That is big though replymadduci 18 hours agoprevAmazing feat, but of course takes forever to generate an image (in the Readme states 11 hours) reply bluetwo 9 hours agoparentYep. I&#x27;ll never need or use this implementation, but the tricks used will make it to other tools, which will be great. reply esskay 11 hours agoparentprevIt&#x27;d be interesting to see what the cost and power equivilence would be compared to a higher end method. I.e the time, cost (including all hardware required) and power taken to generate 100 images using 100 individual Pi Zero 2&#x27;s (doesnt even need to be a W) vs something like an average mid-tier PC.I&#x27;d assume the pc would still likely win.Something like a Pi 4 or 5 may be a better benchmark than the Zero 2 as I get the impression its been used more for the challenge than practicality. reply gpt5 4 hours agorootparentA GPU can produce an image in about 1 second. reply esskay 3 hours agorootparentThat depends on the GPU, I&#x27;m not talking mid-high end (e.g RTX level), just your average &#x27;basic&#x27; GPU. reply hinkley 14 hours agoparentprevOn a raspberry Pi. Zero 2. reply samr71 18 hours agoprevSo this should be it for trying to regulate stable diffusion type tech, right? If these models and their inference infra can be shrunk down to be runnable on a PS2, it doesn&#x27;t seem like it&#x27;s possible to stop this tech without a totalitarian surveillance state (and barely even then!). reply Drakim 17 hours agoparentThe war on general computing has been ongoing but not made enough inroads to stop people from owning general computing devices (yet) reply bloaf 16 hours agorootparentIndeed, the death knell could be tolling not for regulation of ai but for general purposes computers. In AI we have four horsemen: copyright infringement, illegal pornography, fake news generation, and democratization of capabilities that large companies would rather monetize. reply fragmede 15 hours agorootparentGiven the proliferation of illegal downloads (I can get a bad cam rip of the Barbie movie on release weekend just fine, plus a VPN would protect me from DCMA takedowns), and illegal pornography (just ask a torrent tracker for the fappening), and the proliferation of fake news (esp on eg, Facebook) despite a lack of it needing to be ML model generated, and companies and OSS in the space doing the democratizing and releasing complete model weights, and not just lone individuals trying to do the work in isolation, (aka stability.ai), are they really four horsemen, or four kids on miniature ponys? reply IKantRead 13 hours agorootparentprevI try to bring up as often as possible in conversation that nearly all the progress we&#x27;re seeing in terms of usability and performance is precisely because of the open source support for these models.Especially because these tools are so popular outside of the developer community, I think it&#x27;s worth really beating into peoples minds that without open source AI would be in a much worse place overall. reply pphysch 16 hours agorootparentprevThat is virtually impossible because Turing-complete systems are everywhere reply yowlingcat 11 hours agorootparentI wonder if there&#x27;s an analogy to be made here to DRM. In theory, yes, DRM shouldn&#x27;t be possible, but in practice, manufacturers have been able to hobble hardware acceleration behind trusted computing model. Often, they do a poor job and it gets cracked (as with HDCP [1], and UWP [2]).The question in my head is whether the failures in their approaches are due to a flaw in the implementation (in which case it&#x27;s practically possible to do what they&#x27;re trying to do although they haven&#x27;t figured out a way to do it), or whether it&#x27;s fundamentally impossible. With DRM and content, there&#x27;s always the analog hole, and if you have physical control over the device, there&#x27;s always a way to crack the software and the hardware if need be. My questions are whether:a) this is a workable analogy (I think it&#x27;s imperfect because Gen AI and DRM are kinda different beasts)b) even if it was, is there real way to limit Gen AI at a hardware level (I think that&#x27;s also hard because as long as you can do hardware accelerated matmul it&#x27;s basically opening up the equivalent of the analog hole towards semi-turing completeness which is also hardware accelerated)I imagine someone has thought through this more deeply than me and would be curious what they think.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;High-bandwidth_Digital_Content...[2] https:&#x2F;&#x2F;techaeris.com&#x2F;2018&#x2F;02&#x2F;18&#x2F;microsoft-uwp-protection-cr... reply xcdzvyn 7 hours agorootparentYeah I think it&#x27;s fair to assume DRM will be a never-ending cat and mouse between developers and end-users.Netflix for example can implement any DRM tech they want -- ultimately they&#x27;re putting a picture on my screen, and it&#x27;s impossible to stop me from extracting it. reply Drakim 14 hours agorootparentprevJust like how making weed illegal is virtually impossible because anybody can grow marijuana in their backyard.How many regular people would risk owning turning-complete devices that can run unauthorized software if it would net you jail time if caught? Lots of countries are already itching towards banning VPN, corpo needs be damned.Especially now that the iPhone has shown having a device that can only run approved legal software covers a lot of people&#x27;s everyday needs. reply pphysch 11 hours agorootparentI&#x27;m more referring to the fact that stuff like PowerPoint and Minecraft and who knows what are Turing-complete, albeit with awful performance.Theoretically, you can have a totally owned device managed by Big Brother, yet generate AI smut with a general purpose CPU built in PowerPoint.How do you possibly regulate that? reply Drakim 2 hours agorootparent> How do you possibly regulate that?The government could send an order to the software developer to patch out that turning completeness, and ban the software if it&#x27;s not complied.I get what you mean, it&#x27;s never possible to 100% limit things. But if you limit things 98% so that the general public does not have access that&#x27;s more than enough for authoritarian purposes. reply FloatArtifact 15 hours agorootparentprevCan you explain that context a little bit of Turing complete? reply dartos 15 hours agorootparentYou can’t regulate the ownership of computing devices.It’s too generic. There are too many of them. reply lodovic 13 hours agorootparentThey could ban and phase out systems with unsecure bootloaders. That would go a long way. Many vendors have already locked down their boot process. reply pmarreck 15 hours agorootparentprevThis is more than a little melodramatic.https:&#x2F;&#x2F;frame.work&#x2F; and the https:&#x2F;&#x2F;mntre.com&#x2F; MNT Reform: Exist reply codetrotter 15 hours agorootparentIf my country decides to ban the ownership of general purpose computers for individual persons, they would order the customs service to stop import of any computer hardware that enabled general purpose computing. Now I would not be able to have any computer shipped to me from outside my country, so I could no longer buy from either of those vendors you linked.Furthermore, it also would mean that I would not be able to bring any personal computers with me when I travel to other countries. I like to travel, and I like to bring my computers when I do.Next, it would also be dangerous to try to buy computers locally within the borders of the country. The seller might be an informant of the police, or even a LEO doing a sting operation.And then next you have to worry about the computers you already have. If you decide to keep the computers that you had since before, after it is made illegal to own them, you will have problems even if you keep them hidden and only use them at home. Other people know about your computers. Some of those people will definitely tip off the authorities about the fact that you are known to have computers.Let’s hope it never goes as far like this :( reply mr_toad 10 hours agorootparentPeople would take the CPUs out of other devices and use them. A consumer grade router has most of the hardware you need to make a general purpose computer. reply cmeacham98 13 hours agorootparentprevThis is a slippery slope to the extreme.What country outside of North Korea has banned the ownership of general purpose computers, or even considered&#x2F;tried to? reply pmarreck 13 hours agorootparentprevBanning the import of personal computers would be absolutely disastrous for any possible economy anywhere. reply phh 18 hours agoparentprevSo this should be it for trying to regulate theft, right? If you can open a window without any tool other than your own body. It doesn&#x27;t seem like it&#x27;s possible to stop thefts without a totalitarian surveillance state (and barely event then!).Or same can be said about media \"piracy\". Or ransomwares.States have forever regulated things that are not possible to enforce purely technically. reply tavavex 18 hours agorootparentBut theft is quite a different thing, is it not? It&#x27;s a physical act that someone can be caught engaging in - be it by another person, a guard or a security camera. Sure, the \"barrier for entry\" to commit it is low, but retailers et al. are doing as much as they can to raise it.Piracy most often isn&#x27;t treated as a criminal matter, but a civil one - few countries punish piracy severely, but companies are allowed to sue the pirate.I agree with OP in principle - regulating generative AI use would be way harder than piracy or whatever, especially since all of it can be done purely locally and millions of people already have the software downloaded. And that&#x27;s not getting into the reasoning behind a ban - piracy and similar \"digital crimes\" are banned because they directly harm someone, while someone launching Stable Diffusion on their PC doesn&#x27;t do much of anything. reply ethbr1 16 hours agorootparent> few countries punish piracy severely, but companies are allowed to sue the pirate.UNCLOS, Part VII, Section 1, Article 100 https:&#x2F;&#x2F;www.un.org&#x2F;depts&#x2F;los&#x2F;convention_agreements&#x2F;texts&#x2F;unc...>> Duty to cooperate in the repression of piracy>> All States shall cooperate to the fullest possible extent in the repression of piracy on the high seas or in any other place outside the jurisdiction of any State.We could have just added \"private computer\" to the definition of piracy, and it largely would have applied.>> Definition of piracy>> Piracy consists of any of the following acts:>> (a) any illegal acts of violence or detention, or any act of depredation, committed for private ends by the crew or the passengers of a private ship or a private aircraft, and directed [...] on the high seas, against another ship or aircraft, or against persons or property on board such ship or aircraft; reply tavavex 16 hours agorootparent..What? Digital piracy has absolutely no logical or legal connections to naval piracy, except for sharing the same name.No sane person could ever implement anything like this. This is like saying that we could \"just\" add the word \"digital\" to the laws prohibiting murder to make playing GTA illegal. reply ethbr1 15 hours agorootparentAn extra-territorial crimeMostly committed by private citizens in pursuit of profitThat all nations of the world have an interest in suppressing to encourage free trade that economically benefits themBut which some countries at various times have a geopolitical interest in supporting... you&#x27;re right, they have no logical or legal connections at all. reply tavavex 15 hours agorootparentYou could tie essentially any two crimes by assigning more broad descriptors to them that&#x27;d boil down to \"this is what countries want to discourage\". Not to mention, half of this is just wrong - digital piracy most often isn&#x27;t extraterritorial (it very much falls under the jurisdiction of where the piracy took place), and most individuals pirate for personal needs, not profit.The point stands - no jurisdiction that I know of treats digital piracy similarly to naval piracy, and there is no strong argument in favor of doing so. reply ethbr1 11 hours agorootparent> digital piracy most often isn&#x27;t extraterritorial (it very much falls under the jurisdiction of where the piracy took place)The canonical eBay&#x2F;PayPal fraud from eastern Europe example?> most individuals pirate for personal needs, not profit.But most piracy is done by individuals in pursuit of profit, not for personal need. replyShrigmaMale 18 hours agorootparentprevno, this is a lousy analogy because there is a clear harm to others in the case of theft. we&#x27;ve tried regulating other difficult to regulate things where the harm is unclear or indirect (drugs being a good example) to no avail.your piracy example is better. consider that it&#x27;s the rise of more convenient options (netflix and spotify) not some effective policy that curtailed the prevalence of piracy. reply JimDabell 17 hours agorootparent> consider that it&#x27;s the rise of more convenient options (netflix and spotify) not some effective policy that curtailed the prevalence of piracy.The turning point was earlier than Netflix or Spotify – it was the iTunes Store. It was such a dramatic shift, people labelled Steve Jobs as “the man who persuaded the world to pay for content”.https:&#x2F;&#x2F;www.theguardian.com&#x2F;media&#x2F;organgrinder&#x2F;2011&#x2F;aug&#x2F;28&#x2F;s... reply xcdzvyn 7 hours agorootparentprevYes, it is impossible to stop theft. reply leothecool 16 hours agorootparentprevTheft has a clearance rate of only 15%. Sounds like we already stopped trying to regulate most theft, in practice. reply dragonwriter 14 hours agorootparent“Trying to regulate” and “succeeding in enforcing regulations” aren&#x27;t the same thing.In fact, a low clearance rate can be evidence of trying to regulate far beyond one&#x27;s capacity to consistently enforce; if you weren&#x27;t trying to regulate very hard, it would be much easier to have a high clearance rate for violations of what regulations you do have. reply AnthonyMouse 16 hours agoparentprev> If these models and their inference infra can be shrunk down to be runnable on a PS2, it doesn&#x27;t seem like it&#x27;s possible to stop this tech without a totalitarian surveillance state (and barely even then!).The original requirement for these is 16GB of RAM, which can be had for less than $20. They run much faster on a GPU, which can be had for less than $200. Millions of ordinary people already have both of these things. reply skyyler 15 hours agoparentprevThe PS2 only had 32 MB of ram. Even the PS3 only had 256 MB.I know it was a bit of a funny hyperbolic example, but you&#x27;d need to shrink this down way further to run it on a PS2. reply natdempk 15 hours agoparentprevI thought most of the regulatory efforts were focused on training runs getting bigger and bigger rather than generation with existing models. Is there regulation you’re aware of around use of models? reply jayd16 16 hours agoparentprevCopyright infringement is quite cheap as well. Ease and illegality are tangential. You&#x27;d still stop commercial acts even if it&#x27;s impossible to fully stop something.That said, I don&#x27;t think blanket regulation is all that likely anyhow. reply cortesoft 15 hours agoparentprevWhat sort of regulations on the tech are you talking about? It really depends on what you are trying to do whether you can or not. reply bmacho 17 hours agoparentprevNot a surveillance state, but a stop on producing new, high performant chips should be enough. reply Nevermark 16 hours agoprevImpressive!Verily, the era is nigh wherein even lamps and toasters shall brim with surpassing sagacity.After exposure to this field for many years, the last decade was stunning.I say “was”, because the speedup in the last 6-18 months has been another thing altogether.I am not concerned with what we will be able to two years hence, but with how much faster progress will be. And then again, and again. reply riskable 15 hours agoparentOoh! A toaster that takes a prompt and generates that image on your toast! The GPU heat could be harnessed to actually toast the toast.Let&#x27;s make a startup! reply mkaic 12 hours agorootparentWe&#x27;re extremely proud to announce that ToasterDream has raised $323M in Series A funds, and we look forward to many years of exciting developments ahead. As the CEO I&#x27;d like to personally assure our loyal customers that taking this funding will not compromise the quality of our goods and services — rather, quite the opposite! In fact, in just the first month since we secured this funding with our investors, we have already managed to create an entirely new product! It&#x27;s called ToasterDream Ultra, and allows users to toast up to 8 images simultaneously for just $5.99&#x2F;month... reply readyplayernull 9 hours agorootparent$5.99! But please make those toast dream catridges bigger so we don&#x27;t have to replace them each week, my trashcan has been complaining about it! reply loondri 10 hours agoprevThe trade-off between memory usage and inference time uncovers a potential flaw in prioritizing resource efficiency over performance.This would deter real-time or near real-time applications where latency is a critical factor.Also, the confusion over the phrase \"0.5-2x slower\" highlights a possible lack of clarity in communication within the community, which would hinder the accurate assessment and adoption of such optimizations in practice. reply josephg 10 hours agoparentYou might be making some good points, but it took me about 3 attempts to understand your comment.For example:> Also, the confusion over the phrase \"0.5-2x slower\" highlights a possible lack of clarity in communication within the community, which would hinder the accurate assessment and adoption of such optimizations in practice.Maybe instead:> The phrase \"0.5-2x slower\" is confusing. You might get more adoption if the language was more clear. reply scrpl 11 hours agoprevThis is insane! 11 hours or not, I didn&#x27;t expect SD could ever run on hardware like Pi Zero. reply orangepurple 18 hours agoprevI can&#x27;t wait for Stable Diffusion for Windows 3.1 reply pizzaknife 17 hours agoparent.&#x2F;lifts eyebrows suggestively reply __mharrison__ 7 hours agoprevWhen is someone going to run this on the blockchain...(ducks) reply worldmerge 18 hours agoprevThis is so cool!!! Nice job on it! reply rcarmo 18 hours agoprevI wonder if this could be accelerated with the Pi&#x27;s onboard GPU somehow. reply brucethemoose2 13 hours agoparentIs not not already using it? I thought ONNX had a GPU runtime the Pi could use. reply ngcc_hk 9 hours agoprevIs there any summary of the minimum requirement to run and generate key open source model? Wonder to get 24g MacBook Air m2 or still have to use Intel and N… Gpu for this kind of text and image AI … for learning the technology mainly… reply maxlin 17 hours agoprev> This is another image generated by my RPI Zero 2 in about 11 hoursSo pointless. I love it reply naillo 17 hours agoparentCalculator next reply symisc_devel 18 hours agoprev [25 more] [flagged] dang 14 hours agoparentIt looks like your account has been using HN primarily for promotion. This is against HN&#x27;s rules - see https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html:\"Please don&#x27;t use HN primarily for promotion. It&#x27;s ok to post your own stuff part of the time, but the primary use of the site should be for curiosity.\" reply esskay 11 hours agoparentprevA bit of advice...stop. Blatent self promotion of commercial products is a hard no here. We dont want it, and its against the rules. Delete this, and the other posts before they get deleted for you along with account closure. reply habibur 14 hours agoparentprevBefore you waste your time, this is a commercial product and you need to pay $30 to buy their model to run it. reply aftbit 17 hours agoparentprevWhat does it mean to be \"partially uncensored\"? reply Filligree 18 hours agoparentprevOkay... what&#x27;s the downside? reply biomcgary 17 hours agorootparentAlso $29 to get pre-trained model assets to run code. reply smusamashah 16 hours agorootparentWhy does this one needs pretrained models? Can&#x27;t we use any of the thousands of already available ones? reply brucethemoose2 13 hours agorootparentThese are mostly Stable Diffusion architecture models, but its not the only game in town. reply TeddyDD 15 hours agorootparentprevHard to tell since there is zero documentation in regard to models. reply refulgentis 18 hours agorootparentprevIn terms of, what&#x27;s the tradeoff for the time decrease?Apples to oranges, they&#x27;re comparing 11 hours on a Raspberry Pi Zero to:- 10 seconds on Intel i7-13700- 3 seconds on Intel i9-9990XE- 5 seconds on Ryzen 9-5900XAdditionally, the 2048 is accomplished by using RealESRGAN to 2x, which isn&#x27;t close to what a native 2048 diffuser&#x27;s quality would be.It does look interesting and is an achievement, in terms of, it&#x27;s hard to write this stuff from scratch, much less in pure C++ without relying on GPU. reply Filligree 17 hours agorootparentAh. I use RealESRGAN (or one of its descendants, rather) as a first pass upscaler before high-resolution diffusion. If you skip the diffusion step, of course it&#x27;ll be faster. reply leonidasv 11 hours agorootparentprevUnrelated, but now I&#x27;m curious about how much would it take on RPis 4 and 5. reply refulgentis 10 hours agorootparentyeah me too...I&#x27;ve been very negative about the edge, it got overhyped with the romanticization of local LLMs, but there&#x27;s a bunch of stuff coming together at the same time...Raspberry Pi 5...Mistral 7B Orca is my 20th try of a local LLM...and the first time it handled simple conversation with RAG. And new diffusion, even every 2 hours, is a credible product, arguing about power consumption aside... reply omneity 13 hours agoparentprevI see you&#x27;re opting for AGPL on a codebase that is designed to be embedded as a library. Genuine question, what kind of user did you have in mind when you decided on this license? reply smusamashah 16 hours agoparentprevAre those 2048 x 2048 images still sensible? SD 1.5 is best used at 512x512 and may produce sensible images upto 768. It generates monstrosities above that. Similarly SD XL is good upto 1024. reply orbital-decay 15 hours agorootparentThese are limitations of a single text-to-image gen, which is the least interesting way to use those models. When guided by a previous low-res generation, it won&#x27;t fall apart at arbitrary resolutions, that&#x27;s how all diffusion upscalers work. Just don&#x27;t expect being able to fit every detail in one pass, use multiple ones (that&#x27;s how detailers work). reply dragonwriter 14 hours agorootparentprev> Are those 2048 x 2048 images still sensible? SD 1.5 is best used at 512x512 and may produce sensible images upto 768. It generates monstrosities above that. Similarly SD XL is good upto 1024.You can do significantly higher resolutions with various tricks like tiled diffusion, which is also a memory efficiency hack. (The stable-diffusion-webui tiled diffusion extension uses 2560×1280 direct [no upscale step] generation with an SD 1.5-based model as one of its examples.) reply smusamashah 10 hours agorootparentUp scaling the image in chunk creates loads of semantic issues. For example, bottom of tree might look further in the mountains but it&#x27;s top will be near you. You don&#x27;t see problems like these in non scaled images. reply dragonwriter 9 hours agorootparent> Up scaling the image in chunk creates loads of semantic issues.No, tiled upscaling generally does not have that problem significantly (compared to direct generation at native model-supported size, which doesn&#x27;t completely avoid that kind of issue), since the composition on that level is set before the upscale (direct tiled generation does, if you aren’t using something like controlnet to avoid it.)> You don’t see problems like these in non scaled images.You actually occasionally do, but its fairly rare. reply orbital-decay 9 hours agorootparentprevIt&#x27;s conditioned on the lowres input, so if it doesn&#x27;t have semantic discontinuites it doesn&#x27;t happen. It will eventually happen if you continue doing this indefinitely, but with reasonable size to tile ratio (saySimilarly SD XL is good upto 1024.I don&#x27;t think that&#x27;s right. SD xl is good starting from 1024. Anything lower generates a useless mess. reply dragonwriter 9 hours agorootparentSDXL native trained resolution for 1:1 aspect ratio is 1024x1024 like SD 1.5’s is 512x512. Like SD 1.5, you can go a bit below or above that without too much problem; unlike SD 1.5, SDXL also has significant training in a fairly wide set of other resolutions (ranging from 2048x512 to 512x2048) with approximately 1 mebipixel resolution, and they can be treated as starting points as easily as 1024x1024 can. I think SDXL has a narrower (proportionate) range of viable resolutions around its starting points, but that’s offset but having more than one “starting point”. reply diimdeep 17 hours agoparentprev [–] Maybe next time shamelessly mention that you sell models for $29 and there is no instructions to convert from vanilla SD. reply refulgentis 16 hours agorootparent [–] I can&#x27;t believe this is still the top comment. I wish I didn&#x27;t edit down my reply, shoulda just said \"this is stupid, you&#x27;re comparing your desktop to a raspberry pi\"ONNX streaming is way cooler and more impressive than another commercial wrapper around SD. Doesn&#x27;t deserve this. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The repository \"vitoplantamura/OnnxStream\" features an open-source inference library, OnnxStream, designed to limit memory usage during machine learning model execution.",
      "OnnxStream enables the execution of Stable Diffusion models on devices with limited RAM, like a Raspberry Pi Zero 2, through dynamic and static quantization, an optimization method yielding lower memory usage than OnnxRuntime.",
      "The repository provides example implementations, performance benchmarks, and detailed features of the Stable Diffusion tool, along with credit to original projects, version info, resource details, and associated programming languages."
    ],
    "commentSummary": [
      "The post focuses on the performance of the Stable Diffusion XL 1.0 software, specifically highlighting the memory usage and speed trade-off.",
      "Users' experiences and recommendations regarding the software, particularly its use for image generation, are extensively discussed, with certain concerns about its memory usage, inference time, and hardware requirements.",
      "The discussion also reveals issues related to the regulation of such emerging technologies and differentiates between theft and piracy, alongside news about the availability of SD models and the ONNX streaming component."
    ],
    "points": 465,
    "commentCount": 142,
    "retryCount": 0,
    "time": 1696344190
  },
  {
    "id": 37756656,
    "title": "Debunking NIST's calculation of the Kyber-512 security level",
    "originLink": "https://blog.cr.yp.to/20231003-countcorrectly.html",
    "originBody": "The cr.yp.to blog Older (Access-J): 2023.06.09: Turbo Boost: How to perpetuate security problems. #overclocking #performancehype #power #timing #hertzbleed #riskmanagement #environment Table of contents (Access-I for index page) 2023.10.03: The inability to count correctly: Debunking NIST's calculation of the Kyber-512 security level. #nist #addition #multiplication #ntru #kyber #fiasco [Sidney Harris cartoon used with permission. Copyright holder: ScienceCartoonsPlus.com.] Quick, what's 240 plus 240? It's 280, right? No, obviously not. 40 plus 40 is 80, and 240 times 240 is 280, but 240 plus 240 is only 241. Take a deep breath and relax. When cryptographers are analyzing the security of cryptographic systems, of course they don't make stupid mistakes such as multiplying numbers that should have been added. If such an error somehow managed to appear, of course it would immediately be caught by the robust procedures that cryptographers follow to thoroughly review security analyses. Furthermore, in the context of standardization processes such as the NIST Post-Quantum Cryptography Standardization Project (NISTPQC), of course the review procedures are even more stringent. The only way for the security claims for modern cryptographic standards to turn out to fail would be because of some unpredictable new discovery revolutionizing the field. Oops, wait, maybe not. In 2022, NIST announced plans to standardize a particular cryptosystem, Kyber-512. As justification, NIST issued claims regarding the security level of Kyber-512. In 2023, NIST issued a draft standard for Kyber-512. NIST's underlying calculation of the security level was a severe and indefensible miscalculation. NIST's primary error is exposed in this blog post, and boils down to nonsensically multiplying two costs that should have been added. How did such a serious error slip past NIST's review process? Do we dismiss this as an isolated incident? Or do we conclude that something is fundamentally broken in the procedures that NIST is following? Discovering the secret workings of NISTPQC. I filed a FOIA request \"NSA, NIST, and post-quantum cryptography\" in March 2022. NIST stonewalled, in violation of the law. Civil-rights firm Loevy & Loevy filed a lawsuit on my behalf. That lawsuit has been gradually revealing secret NIST documents, shedding some light on what was actually going on behind the scenes, including much heavier NSA involvement than indicated by NIST's public narrative. Compare, for example, the following documents: A public 2014 document says that its author is \"Post Quantum Cryptography Team, National Institute of Standards and Technology (NIST), pqc@nist.gov\". A secret 2016 document listed the actual pqc@nist.gov team members, with more NSA people (Nick Gajcowski; David Hubbard; Daniel Kirkwood; Brad Lackey; Laurie Law; John McVey; Mark Motley; Scott Simon; Jerry Solinas; David Tuller) than NIST people. (Another Department of Defense representative on the list was Jacob Farinholt, Naval Surface Warfare Center, US Navy. I'm not sure about Evan Bullock.) Another secret 2016 document shows that NSA's Scott Simon was scheduled to visit NIST on 12 January 2016. Another secret 2016 document shows that NIST's \"next meeting with the NSA PQC folks\" was scheduled for 26 January 2016. Another secret 2016 document shows that Michael Groves from NSA's UK partner was scheduled to visit NIST on 2 February 2016. Another secret 2016 document lists Colin Whorlow from NSA's UK partner as someone that NIST visited in 2016, in particular discussing \"confidence and developments for each of the primary PQC families\". A public 2020 document says \"Engagement with community and stakeholders. This includes feedback we received from many, including the NSA. We keep everyone out of our internal standardization meetings and the decision process. The feedback received (from the NSA) did not change any of our decisions ... NIST encouraged the NSA to provide comments publicly. NIST alone makes the PQC standardization decisions, based on publicly available information, and stands by those decisions\". I filed a new FOIA request in January 2023, after NIST issued its claims regarding the security level of Kyber-512. NIST again stonewalled. Loevy & Loevy has now filed a new lawsuit regarding that FOIA request. Public material regarding Kyber-512 already shows how NIST multiplied costs that should have been added, how NIST sabotaged public review of this calculation, and how important this calculation was for NIST's narrative of Kyber outperforming NTRU, filling a critical gap left by other steps that NIST took to promote the same narrative. This blog post goes carefully through the details. Alice and Bob paint a fence. At this point you might be thinking something like this: \"Sorry, no, it's not plausible that anyone could have mixed up a formula saying 2x+2y with a formula saying 2x+y, whatever the motivations might have been.\" As a starting point for understanding what happened, think about schoolchildren in math class facing a word problem: There is a fence to paint. Alice would take 120 minutes to paint the fence. Bob would take 240 minutes to paint the fence. How long would it take Alice and Bob to paint the fence together? The approved answer in school says that Alice paints 1/120 of the fence per minute, and Bob paints 1/240 of the fence per minute, so together they paint 1/120 + 1/240 = 1/80 of the fence per minute, so it takes them 80 minutes to paint the fence. The real answer could be more complicated because of second-order effects. Probably Alice and Bob working together are getting less tired than Alice or Bob working alone for longer would have. In the opposite direction, maybe there's a slowdown because Alice and Bob enjoy each other's company and pause for a coffee. Schoolchildren often give answers such as 240 − 120 = 120, or 120 + 240 = 360, or (120 + 240)/2 = 180. These children are just manipulating numbers, not thinking through what the numbers mean. Two disciplines for catching errors. In later years of education, physics classes teach students a type-checking discipline of tracking units with each number. Here are examples of calculations following this discipline: Dividing \"1 fence\" by \"120 min\" gives \"0.00833 fence/min\". Adding \"0.00833 fence/min\" to \"0.00417 fence/min\" gives \"0.01250 fence/min\". Taking the reciprocal gives \"80.0 min/fence\". The same discipline wouldn't let you add, for example, \"1 fence\" to \"120 min\": the units don't match. This discipline avoids many basic errors. On the other hand, it still allows, e.g., the mistake of adding \"120 min\" to \"240 min\" to obtain \"360 min\". What catches this mistake is a discipline stronger than tracking units: namely, tracking semantics. The numbers have meanings. They're quantitative features of real objects. For example, 80 minutes is the total time for Alice and Bob to paint the fence when Alice is painting part of the fence and Bob is painting part of the fence. That's what the question asked us to calculate. A different question would be the total time for Alice to paint the fence and then for Bob to repaint the same fence. This would be 120 minutes plus 240 minutes. Yet another question would be the total time for Alice to paint the fence, and then for Bob to wait for the coat of paint to dry, and then for Bob to apply a second coat. Answering this would require more information, namely the waiting time. All of these questions make sense. They pass type-checking. But their semantics are different. Alice and Bob tally the costs of an attack. Alice and Bob have finished painting and are now discussing the merits of different encryption systems. They'd like to make sure that breaking whichever system they pick is at least as hard as searching for an AES-128 key. They've agreed that searching for an AES-128 key is slightly above 2140 bit operations. Alice and Bob are broadcasting their discussion for anyone who's interested. Let's listen to what they're saying: Alice: \"Hmmm, there are a bunch of sources saying that the XYZZY attack algorithm uses 280 iterations to break this particular cryptosystem. It's worrisome that this number is so low. What else do we know about the cost of the attack?\" Bob: \"I found a source saying that there are actually extra factors in the iteration count, and estimating that the XYZZY attack uses 295 iterations.\" Alice: \"Here's another source looking at the details of the computations inside each iteration, and estimating that those computations cost 225 bit operations.\" Bob: \"There's also a gigantic array being accessed. Here's a source estimating that the memory access inside each iteration is as expensive as 235 bit operations.\" Alice: \"Okay, let's review. The best estimate available for the total cost of each iteration in the XYZZY attack is around 235 bit operations. A tiny part of that is 225 bit operations for computation. The main cost is the equivalent of 235 bit operations for the memory access.\" Bob: \"Agreed. Multiplying 295 iterations by 235 bit operations per iteration gives us a total of 2130 bit operations. Doesn't meet the security target.\" Alice: \"Right, that's a thousand times easier than AES-128 key search. Let's move on to the next cryptosystem.\" How to botch the tally of costs. Imagine a government agency that has also been looking at this particular cryptosystem, but with one critical difference: the agency is desperate to say that this cryptosystem is okay. How does the agency deal with the XYZZY attack? One answer is to aim for a lower security goal, hyping the cost of carrying out 2130 bit operations. For comparison, Bitcoin mining did only about 2111 bit operations in 2022. (\"Only\"!) But let's assume that the agency has promised the world that it will reach at least the AES-128 security level. What does the agency do? Here's an idea. For the costs per iteration, instead of adding 225 for computation to 235 for memory access, how about multiplying 225 for computation by 235 for memory access? The product is 260. Multiplying this by 295 iterations gives 2155, solidly above 2143. Problem solved! How discipline catches the error. Alice and Bob are correctly tracking the semantics of each number. The agency isn't. The total attack cost is the number of iterations times the cost per iteration. Each iteration incurs cost for computation, estimated as 225 bit operations, and cost for memory access, estimated to be as expensive as 235 bit operations. The agency's multiplication of these two costs makes no sense, and produces a claimed per-iteration cost that's millions of times larger than the properly estimated per-iteration cost. This multiplication is so glaringly wrong that it doesn't even pass physics-style type-checking. Specifically, multiplying \"225 bitops/iter\" by \"235 bitops/iter\" doesn't give \"260 bitops/iter\". It gives \"260 bitops2/iter2\". Multiplying further by \"295 iter\" doesn't give \"2155 bitops\"; it gives \"2155 bitops2/iter\". Agency desperation strikes back. How can the agency phrase this nonsensical calculation of a severely inflated security estimate in a way that will pass superficial review? The goal here is for the 155 to sound as if it's simply putting together numbers from existing sources. For example: Here's a source estimating an iteration count of 295. Here's a source estimating 225 bit operations per iteration. Here's a source estimating that accounting for memory multiplies costs by 235. 95 plus 25 plus 35 is 155, solidly above 143. The deception here is in the third step, the step that leaps from cost 225 per iteration to cost 260 per iteration. How many readers are going to check the third source and see that it was actually estimating cost 235 per iteration? Streamlining the marketing. The wrong calculation sounds even simpler if there's a previous source that has already put the 295 and the 225 together: Here's a source estimating 2120 bit operations. Here's a source estimating that accounting for memory multiplies costs by 235. 120 plus 35 is 155, solidly above 143. At this point the agency has completely suppressed any mention of iterations, despite the central role of iterations in the attack and in any competent analysis of the attack. How many readers are going to check both sources, see that the second source estimates cost 235 per iteration, and see that the iteration count in the first source is far below 2120? Kyber's limited selection of security levels. You might be thinking something like this: \"Okay, sure, I see how it would be possible for a desperate agency to replace cost addition with a nonsensical multiplication, replacing 2130 with a fake 2155, while at the same time making this hard for people to see. But why would anyone have wanted to play this risky game? If Kyber-512 was around 2130, and the target was a little above 2140, why didn't they just bump up the parameters to 10% higher security, something like Kyber-576?\" This is an obvious question given that RSA and ECC and (to take some post-quantum examples) McEliece and NTRU naturally support whatever size you want. A long, long time ago, I wrote fast software for the NSA/NIST P-224 elliptic curve, and then found a better curve at that security level, namely y2 = x3 + 7530x2 + x mod 2226−5. But then I decided that bumping the size up to 2255−19 would be much more comfortable, so I did. Kyber is different. You can't just bump up Kyber's parameters to 10% higher security: Kyber-576 doesn't exist. If you want something stronger than Kyber-512 then you have to increase the \"dimension\" by 50%, jumping all the way up to Kyber-768. If you want something stronger than Kyber-768 then you have to jump all the way up to Kyber-1024. If you want something stronger than Kyber-1024 then, sorry, tough luck. One of the \"unique advantages of Kyber\" specifically advertised in the official Kyber documentation is that implementing a \"dimension-256 NTT\" handles \"all parameter sets\" for Kyber (emphasis in original). This \"NTT\" isn't something optional for Kyber implementors; it's baked into the structure of Kyber's public keys and ciphertexts. Using dimensions that aren't multiples of 256 would require changing the core Kyber design. The same Kyber \"advantage\" also means that going beyond 1024 would lead to performance issues and, more importantly, security issues surrounding occasional \"decryption failures\" forced by the prime baked into the NTT. Avoiding this would again require changing the core Kyber design. For comparison, NTRU options targeting higher security levels—including simple proofs that there are no decryption failures—are readily available. For example, one of the NTRU Prime options is sntrup1277. But let's assume that NIST doesn't care about Kyber's limitations at the high end. Let's instead focus on the low end, specifically on applications that have limited sizes for public keys and/or ciphertexts and thus can't use the highest available security levels. An application limited to 1KB can't use Kyber-768 (1184-byte public keys, 1088-byte ciphertexts). The highest-security Kyber option for that application is Kyber-512 (800-byte keys, 768-byte ciphertexts). The same application obtains higher security with NTRU, according to a security-estimation mechanism called \"Core-SVP\". For example, the application can use sntrup653 (994-byte keys, 897-byte ciphertexts), where the Core-SVP security estimate is 2129, or NTRU-677 (ntruhps2048677, 931-byte keys, 931-byte ciphertexts), where Core-SVP is 2145, while the current version of Kyber-512, starting with the round-3 version from 2020, has Core-SVP just 2118. Is this \"Core-SVP\" something I made up to make Kyber look bad? Absolutely not: Core-SVP is the security-estimation mechanism that was chosen by the Kyber team to estimate security levels in its round-1 and round-2 submissions. The mechanism was introduced by Kyber's predecessor, NewHope. In 2020, after I expressed skepticism about whether Core-SVP \"gets the right ordering of security levels\", NIST stated that \"we feel that the CoreSVP metric does indicate which lattice schemes are being more and less aggressive in setting their parameters\". NIST's official round-2 report in 2020 used Core-SVP for comparisons. The original definition of Core-SVP assigns 2112 to the round-3 version of Kyber-512. Round-3 Kyber switched to a new definition of Core-SVP that increases Kyber's Core-SVP (without changing anything for NTRU). This blog post has bigger fish to fry, so let's blindly accept Kyber's claim that the new definition is better, meaning that Kyber-512 has Core-SVP 2118. That's still clearly worse than the 2129 for sntrup653 and the 2145 for NTRU-677. It's not that Kyber's competitors always beat Kyber in size-security tradeoffs. For example, if an application instead has a limit of 1184 bytes, then it can use Kyber-768, which has Core-SVP 2181, while ntruhps needs 1230 bytes to reach Core-SVP 2179. But Kyber's competitors often beat Kyber in size-security tradeoffs. Throwing away Kyber-512, leaving just Kyber-768 and Kyber-1024, means that Kyber has nothing as small as the 931 bytes for NTRU-677. The normal way for scientists to present quantitative tradeoffs is with scatterplots, such as Figure 3.5 in my 2019 paper \"Visualizing size-security tradeoffs for lattice-based encryption\". The particular scatterplot shown here is Figure 7.3 in the 2021 paper \"Risks of lattice KEMs\" from the NTRU Prime Risk-Management Team. The vertical axis is the Core-SVP security estimate, and the horizontal axis is ciphertext bytes. The scatterplot shows that Kyber has a higher Core-SVP than NTRU for applications with a size limit of, e.g., 768 bytes or 1088 bytes. But NTRU has a higher Core-SVP than Kyber for applications with a size limit of, e.g., 700 bytes or 1024 bytes or 2048 bytes. Kyber has nothing as small as the 699-byte option for NTRU. Kyber also has nothing as strong as the 1842-byte option for NTRU. NTRU is also trivially capable of adding further options between and beyond what's shown in the graph, whereas for Kyber this is more problematic. Official evaluation criteria for the competition. NIST had issued an official call for post-quantum proposals in 2016. One of the evaluation criteria in the call was as follows: Assuming good overall security and performance, schemes with greater flexibility will meet the needs of more users than less flexible schemes, and therefore, are preferable. One of the official examples given for \"flexibility\" was that it is “straightforward to customize the scheme's parameters to meet a range of security targets and performance goals\". The call proposed five broad security \"categories\", and said that submitters could specify even more than five parameter sets to demonstrate flexibility: Submitters may also provide more than one parameter set in the same category, in order to demonstrate how parameters can be tuned to offer better performance or higher security margins. In 2020, NIST eliminated NewHope. One of the reasons stated in the aforementioned round-2 report was that \"KYBER naturally supports a category 3 security strength parameter set, whereas NewHope does not\". NewHope offered only NewHope-512 and NewHope-1024. Imagine Kyber similarly offering only Kyber-768 and Kyber-1024, acknowledging that Kyber-512 doesn't meet the minimum security level specified by NIST. It's then very easy to see how limited Kyber's flexibility is compared to NTRU's broader, denser spectrum of security levels. How, then, would NIST argue that Kyber is the best option? One answer is that the evaluation criteria say more flexibility is preferable only assuming \"good overall security and performance\". But how would NIST argue that NTRU doesn't have \"good overall security and performance\"? Regarding the security of Kyber and NTRU, NIST's official 2022 selection report says that NIST is \"confident in the security that each provides\". The report describes MLWE, the problem inside Kyber, as \"marginally more convincing\" than the problem inside NTRU. There's much more that could and should have been said about the security comparison between Kyber and NTRU: Kyber's use of modules, despite being portrayed as purely having a (marginal) security benefit, also introduces extra subfields into the cryptosystem structure, creating security risks analogous to the risks of taking extra subfields in pre-quantum DH. Fewer extra subfields appear in NTRU (depending on parameters) than in Kyber. NTRU Prime completely avoids extra subfields. Kyber's QROM IND-CCA2 proof assuming MLWE hardness is much looser than NTRU's QROM IND-CCA2 proof assuming hardness of the problem inside NTRU. In other words, even under the assumption that MLWE is as strong as the problem inside NTRU, Kyber could be much weaker than NTRU. NIST could have told people to use NTRU shortly after its deadline for NISTPQC input in 2021. Instead it delayed for three quarters of a year to carry out patent negotiations, and ended up telling people to wait for its Kyber patent license to activate in 2024, giving away three years of user data to attackers. Picking Kyber was doing obvious damage to security given the patent situation. The situation isn't that NTRU avoids every security risk of Kyber. A careful comparison finds mathematical security risks in both directions. Maybe there's a way to argue that the mathematical security risks for NTRU should be given higher weight than the mathematical security risks for Kyber. But the immediate choice that NIST was facing in 2021 between NTRU and Kyber, assuming that the attackers currently recording user data will have quantum computers in the future, was between the security risks of NTRU and the guaranteed security failure of not yet deploying anything. The call for submissions said \"NIST believes it is critical that this process leads to cryptographic standards that can be freely implemented in security technologies and products\". Nothing else in the call was labeled as \"critical\". How could NIST ignore the damage that it was doing in not going ahead with NTRU? NIST knew it didn't have a patent license signed for Kyber yet, let alone an activated patent license. Anyway, let's get back to the question of how NIST might be able to argue that NTRU doesn't have \"good overall security and performance\". A report saying that NIST is \"confident in the security that each provides\" is obviously not claiming that NTRU doesn't have \"good overall security\". What about performance? The same selection report admits that \"the overall performance of any of these KEMs would be acceptable for general-use applications\". If the objective is to use performance differences as a deciding factor between two acceptable options, let's see how Kyber would stack up without Kyber-512: Kyber-768 and Kyber-1024 provide size-security tradeoffs that NTRU doesn't match. NTRU-677 and NTRU-1229 provide size-security tradeoffs that Kyber doesn't match. Even more options are already implemented for NTRU Prime. The smallest options are from NTRU, not Kyber. The highest-security options are from NTRU, not Kyber. This is a solid case for eliminating Kyber in favor of NTRU, given NIST's declaration that there can be only one. (If NIST thought that performance differences at this scale matter, and if the best performance comes from Kyber at some security levels and NTRU at other security levels, then why wasn't NIST allowing both? Answer: The movie says there can be only one! STOP ASKING QUESTIONS!) Tilting the competition, part 1: ignoring NTRU's extra flexibility. Keeping Kyber-512 changes the competition. Having three options, Kyber-512 and Kyber-768 and Kyber-1024, looks a lot better than having just two. There are four NTRU circles in the first scatterplot above, namely NTRU-509 and NTRU-677 and NTRU-821 and NTRU-1229. But NTRU-821 isn't a winner, and earlier in NISTPQC there wasn't an NTRU-1229. Wait a minute. The NTRU literature has always made clear that NTRU supports many more options. For example, here's a scatterplot from John Schanck's 2018 paper \"A comparison of NTRU variants\". There are a huge number of dots; each dot is showing another NTRU option. One of the bizarre twists in NISTPQC was the following announcement from NIST in 2020: \"NIST believes that too many parameter sets make evaluation and analysis more difficult.\" I asked various questions about this, starting as follows: How many is \"too many\"? How did flexibility, which was portrayed as purely positive in the call for proposals, turn into a bad thing for NIST? The call for proposals explicitly allowed multiple parameter sets per category, never suggesting that this would be penalized! NIST's latest report complains about NewHope's lack of flexibility to use dimensions strictly between 512 and 1024. If a submission team is thinking \"Aha, Kyber similarly suffers from its lack of flexibility to target security levels strictly between maybe-2128 and maybe-2192, and we can clearly show this to NIST by selecting parameter sets at several intermediate security levels\", then isn't this something NIST should be interested in, rather than discouraging by making submitters worry that this is \"too many parameter sets\"? NIST never replied. Think about what this is like for submitters trying to figure out what to do: The official evaluation criteria say flexibility is good. A high-profile submission has just been eliminated, in part for having only two parameter sets. So, okay, implement more parameter sets to demonstrate flexibility. But, yikes, NIST is suddenly going out of its way to criticize \"too many\" parameter sets. They won't say what \"too many\" means and where this criticism came from. NTRU Prime moved up to selecting six sntrup parameter sets (plus six ntrulpr parameter sets, which, compared to sntrup, have larger ciphertexts but smaller public keys), enough that the flexibility advantage over Kyber should have been impossible to ignore. NIST ignored it. Tilting the competition, part 2: exaggerating and hyping key-generation costs. For Intel's recent Golden Cove microarchitecture (the \"performance\" cores in Alder Lake CPUs), https://bench.cr.yp.to reports that Kyber-512 takes 25829 cycles for encapsulation and 20847 cycles for decapsulation, while NTRU-509 takes just 15759 cycles for encapsulation and 25134 cycles for decapsulation. The total cycle count for handling a ciphertext, the total of encapsulation and decapsulation, is 13% smaller for NTRU-509 than for Kyber-512. NTRU-509 also beats Kyber-512 in ciphertext size. NTRU-509 is the leftmost dot in the first scatterplot above, meaning smallest ciphertexts. On the other hand, NTRU-509 takes 112866 cycles for key generation while Kyber-512 takes only 17777 cycles. The total of key generation plus encapsulation plus decapsulation is more than twice as large for NTRU-509 as for Kyber-512. When some factors favor one option and some factors favor another option, someone objectively searching for the best option will think about what weight to put on each factor. Here are three reasons that a careful performance analysis will put very low weight on Kyber's key-generation speedup: There's overwhelming evidence that these cycle counts are far less important than byte counts. A useful rule of thumb is that sending or receiving a byte has similar cost to 1000 cycles; see Section 6.6 of the aforementioned paper \"Risks of lattice KEMs\". Sending a key, receiving a key, sending a ciphertext, and receiving a ciphertext involves thousands of bytes, similar cost to millions of cycles. All of these KEMs are designed to allow a key to be reused for many ciphertexts. If an application actually cares about the cost of key generation then this reuse is an obvious step to take. NIST's official evaluation criteria already acknowledged the possibility that \"applications can cache public keys, or otherwise avoid transmitting them frequently\". Many applications are naturally reusing keys in any case. Even in the extreme case of an application that structurally has to use a new key for each ciphertext, there's a trick due to Montgomery that makes NTRU key generation much faster. Billy Bob Brumley, Ming-Shing Chen, Nicola Tuveri, and I have a paper \"OpenSSLNTRU: Faster post-quantum TLS key exchange\" at USENIX Security 2022 giving a web-browsing demo on top of TLS 1.3 using sntrup761 with Montgomery's trick for key generation. We already had the paper and code online in 2021, before NIST's deadline for input regarding NISTPQC decisions. In other words: If an average key is used for just 100 ciphertexts then Kyber-512 saving 95089 Golden Cove cycles in key generation is of similar importance to changing ciphertext size by a fraction of a byte; 6x less important than NTRU-509 saving 5783 cycles per ciphertext; and not what will happen in applications trying to optimize key-generation time, since in NTRU's case those applications will use Montgomery's trick. With this in mind, let's look at the \"Kyber vs NTRU vs Saber\" slide from NIST's March 2022 talk \"The beginning of the end: the first NIST PQC standards\". The eye is immediately drawn to the larger red bars on the right. NTRU appears in two of the groups of bars, in both cases with clearly larger bars, meaning worse performance. The main message NIST is communicating here is that NTRU costs strikingly more than Kyber and Saber. Only a small part of the audience will go to the effort of checking the numbers and seeing how NIST manipulated the choices in its presentation to favor Kyber over NTRU: The graph gives 100% weight to key generation, utterly failing to account for key reuse. The graph also utterly fails to account for Montgomery's trick. The graph does include some recognition of communication costs, but even here NIST couldn't resist tweaking the numbers: \"1000*(PK+CT)\" counts Alice's cost while omitting Bob's cost. Regarding the last point: 1000 is just a rule of thumb. NIST could have posted a rationale for a proposal to use 500 and asked for public comments. But it didn't. NIST's secret October 2021 Kyber-SABER-NTRU comparison claimed, without citation, that I had said 1000*(PK+CT) was reasonable. Compare this to what I had actually written in 2019 about the costs of sending and receiving a ciphertext, after various NTRU Prime documents had given examples backing up the first sentence: Typically sending or receiving a byte costs at least three orders of magnitude more than a clock cycle. Taking bytes+cycles/1000 for sntrup4591761 gives 1047+45 = 1092 for the sender, 1047+94 = 1141 for the receiver, which is better than 1248 no matter how few cycles you use. The numbers here account for Alice sending a 1047-byte sntrup4591761 ciphertext and Bob receiving a 1047-byte ciphertext, on top of about 45000 Haswell cycles for Alice's enc and about 94000 cycles for Bob's dec (which was later sped up a lot, but this barely matters next to the ciphertext sizes). See also the more detailed NTRU examples in Section 6.6 of \"Risks of lattice KEMs\", filed before NIST's deadline for input at the end of October 2021. NIST's secret comparison continued by saying \"David suggests 2000?\", apparently referring to a secret performance comparison in 2020 where NIST used \"bandwidth cost of 2000 cycles/byte\". Evidently NIST was considering multiple options for this number. Maybe more FOIA results will shed more light on how exactly NIST ended up with a NIST-fabricated option that—quelle surprise!—is better for Kyber. As for key reuse, NIST might try to defend itself by saying, look, there's a separate PK+CT bandwidth graph on the left, which for these KEMs is visually close to a 2000*CT+enc+dec graph. However: NIST chose to deemphasize the bandwidth graph by using thinner red bars for it. The graph isn't invisible, so together the two graphs don't give exactly 100% weight to key-generation time. But a key used for 100 ciphertexts incurs 1 keygen, 100 enc, and 100 dec, meaning only 1% weight for key-generation time, which is very far from the weight conveyed by NIST's slide. NIST chose to use smaller (and non-log) vertical scales for the bandwidth graph. This further deemphasizes that graph and makes it hard for the audience to notice the size advantage of NTRU-509 (699-byte keys and 699-byte ciphertexts) over Kyber-512 (800-byte keys and 768-byte ciphertexts). NTRU-509's savings of 170 bytes in key+ciphertext size compared to Kyber-512 is comparable to saving 340000 cycles in total for Alice and Bob. This easily outweighs the cost of NTRU-509 key generation, even in the extreme case of one ciphertext per key, even without Montgomery's trick, even if one rewinds a decade from Alder Lake to Haswell. In short, NTRU-509's size advantage is more important than Kyber-512's keygen-time advantage. But NIST chose to give more vertical space to Kyber's keygen-time advantage than to NTRU-509's size advantage. NIST applied a discretization attack to both graphs to conceal the security advantages of the larger NTRU options. If NIST had provided an honest size-vs.-Core-SVP scatterplot, then readers would have seen that NTRU-677 has much higher Core-SVP than Kyber-512 and much better size than Kyber-768. NIST would never have been able to get away with its claim that NTRU has \"somewhat larger public keys and ciphertexts\" than Kyber: a scatterplot immediately shows that, no, this depends on the target security level, with NTRU smaller at some security levels and Kyber smaller at others. Instead NIST started with the options in Core-SVP order and then grouped the options according to \"category\". Because of this grouping, the options look like they have some arbitrary order within each \"category\". People looking at the graph have no idea that NTRU's placement farther to the right in each \"category\" reflects NTRU's higher security levels. A different choice of \"category\" cutoffs would have reversed the visual comparison. As for the failure to account for Montgomery's trick, NIST might try to defend itself by saying that the OpenSSLNTRU software focused on NTRU Prime, so NIST's only choice was to presume that there's no speedup for NTRU beyond NTRU Prime. In fact, the OpenSSLNTRU paper had already explained why there will be about a 2x speedup. Tilting the competition, part 3: concealing the fact that NTRU offers the highest security levels. The official call for submissions in 2016 recommended focusing on \"categories 1, 2 and/or 3\". See below for a full quote. The call also recommended that submitters \"specify some other level of security that demonstrates the ability of their cryptosystem to scale up beyond category 3\". NTRU (and NTRU Prime) did this, specifying parameters across a wide range of security levels. See, e.g., the 2018 scatterplot shown above. In the aforementioned round-2 report from 2020, NIST suddenly said that it \"strongly encourages the submitters to provide at least one parameter set that meets category 5\", complained that \"the NTRU submission lacks a category 5 parameter set proposal\" when the costs of memory are ignored, and complained that NTRU Prime provided \"a narrower range of CoreSVP values than other lattice submissions targeting security strengths 1, 3, and 5\". This wasn't following the official evaluation criteria. NIST was retroactively changing \"recommend\" to \"strongly encourage\", was retroactively changing \"beyond category 3\" to \"category 5\", and was ignoring all of the existing documentation of NTRU's flexibility. Submissions that provided \"category 4\", or provided higher security within \"category 3\", were fully meeting the recommendation in the official evaluation criteria: Submitters may also provide more than one parameter set in the same category, in order to demonstrate how parameters can be tuned to offer better performance or higher security margins. NIST recommends that submitters primarily focus on parameters meeting the requirements for categories 1, 2 and/or 3, since these are likely to provide sufficient security for the foreseeable future. To hedge against future breakthroughs in cryptanalysis or computing technology, NIST also recommends that submitters provide at least one parameter set that provides a substantially higher level of security, above category 3. Submitters can try to meet the requirements of categories 4 or 5, or they can specify some other level of security that demonstrates the ability of their cryptosystem to scale up beyond category 3. But, in 2020, NIST wasn't even trying to follow the official evaluation criteria. It was inventing new evaluation criteria, with no warning, and retroactively applying those criteria to criticize the NTRU and NTRU Prime submissions. Unsurprisingly, those submissions responded with software for higher security levels: NTRU responded with reference implementations of NTRU-1229 and NTRU-HRSS-1373. The NTRU team didn't provide optimized implementations (maybe it ran out of time, which is NIST's fault for not having asked for category 5 in the official call four years earlier), but it reported that NTRU-1229 has Core-SVP 2301 and that NTRU-HRSS-1373 has Core-SVP 2310. Both of these are solidly above Kyber-1024's 2254. NTRU Prime responded with reference and optimized implementations of various options, such as sntrup1277 and ntrulpr1277, which have Core-SVP 2270 and 2271 respectively, again above anything Kyber offers. (There's a code generator automatically producing all of the official NTRU Prime implementations; the generator is easily extensible to cover further parameter sets.) After insisting on higher security levels (and adopting Core-SVP) in its 2020 round-2 report, NIST praised NTRU for responding with higher security levels (as measured by Core-SVP) than Kyber, right? Of course not. NIST concealed the fact that NTRU was offering higher security levels than Kyber: NIST's big graph doesn't show any NTRU options in the top \"category\". (The cover story writes itself: The NTRU submission didn't provide optimized software for the new options! Reporting reference speeds would have been unfair! NIST is just trying to protect readers from being misled!) For readers who go to the effort of looking at the small graph, the discretization attack makes NTRU's higher security levels look just like Kyber's lower security levels. Readers looking at NIST's graphs are left with the impression that NTRU is less flexible than Kyber and, in particular, has more trouble reaching high security levels. This is exactly the opposite of the facts. Tilting the competition, part 4: throwing away the highest-performance option. NTRU-1229 and NTRU-HRSS-1373 aren't the only options that NIST excluded from its big graph. Let's again look at the low end, the top-performance end, where NIST chose to exclude NTRU-509. Optimized NTRU-509 software was already available. If NIST had included NTRU-509 in the big graph then that graph would have shown NTRU-509 as the best performer, better than Kyber-512. Accounting for key reuse would have further favored NTRU-509. Accounting for Montgomery's trick would have further favored NTRU-509. Upgrading from Haswell would have further favored NTRU-509. Counting 1000 cycles per byte for Alice and for Bob would have further favored NTRU-509. But NIST simply removed NTRU-509 from the big graph, making NTRU look strictly worse than Kyber in that graph. NIST went even further in its subsequent report selecting Kyber for standardization: the report didn't show NTRU-509 in any of the figures or tables. The report's descriptions of Kyber's performance were visibly more positive than its descriptions of NTRU's performance, as illustrated by NIST's claim that NTRU has \"somewhat larger public keys and ciphertexts\" than Kyber. How does NIST stop people from quickly spotting the errors in this \"somewhat larger public keys and ciphertexts\" claim? A discretization attack easily hides the fact that NTRU has smaller sizes than Kyber at intermediate security levels, but it doesn't hide NTRU-509 being smaller than Kyber-512. NIST's narrative also relied on kicking out NTRU-509. How can NIST justify throwing NTRU-509 away? The only possible answer is claiming that NTRU-509 doesn't reach the minimum allowed NISTPQC security level, the security level of AES-128. But, at the same time, NIST is including Kyber-512, so NIST is claiming that Kyber-512 does reach the security level of AES-128. NTRU-509 has Core-SVP 2106, just 6 bits below Kyber-512's original Core-SVP (2112) or 12 bits below Kyber-512's revised Core-SVP (2118). Evidently NIST is claiming that AES-128 is inside this narrow margin: in other words, that NTRU-509 has slightly lower security than AES-128 while Kyber-512 has slightly higher security than AES-128. Let's take a moment to admire how spectacularly fragile this is: If some effect slightly increases lattice security levels compared to what NIST is claiming, then NTRU-509 is back in the game, outperforming all of the Kyber options. If some effect slightly reduces lattice security levels compared to what NIST is claiming, then Kyber-512 is gone, and NTRU-677 outperforms all of the Kyber options. If security levels are measured in a way that just manages to have Kyber-512 retained while NTRU-509 isn't retained, then NTRU's superior flexibility still provides the highest security level and wins at various intermediate levels, but Kyber wins at other intermediate levels and provides the highest performance level, so putting enough weight on the highest performance level favors Kyber. See how important it is for Kyber-512 to reach the AES-128 security level? Without that, Kyber is in big trouble: NTRU provides the highest level of security and the highest level of performance and the best flexibility. The chaos beyond Core-SVP. How is Kyber-512 supposed to reach the AES-128 security level if AES-128 needs more than 2140 bit operations to break while the Core-SVP security estimate for Kyber-512 is only 2118? This question was briefly addressed in the round-1 Kyber submission in 2017. That submission said that the 2017 version of Kyber-512 had Core-SVP 2112, falling short of the target by 30 bits, but gave a five-line list of reasons that \"it seems clear\" that Kyber-512 has at least 30 bits more security than Core-SVP indicates. The round-2 Kyber submission in 2019 made the same claim regarding the 2019 version of Kyber-512. In 2020, I disproved the stated rationale. To summarize: Kyber's argument that it was gaining security from \"the additional rounding noise (the LWR problem, see [13, 8]), i.e. the deterministic, uniformly distributed noise introduced in ciphertexts via [rounding]\" was simply wrong. Attackers could freely target Kyber's keys, and the keys didn't have any rounding. Kyber's argument that it was gaining security from the \"additional cost of sieving with asymptotically subexponential complexity\" was unfounded and probably wrong: as far as I could tell (and as far as we know today), the actual asymptotics are subexponentially faster than Core-SVP, not subexponentially slower. It was still plausible that the costs for specific sizes such as Kyber-512 were higher than Core-SVP, but this required an analysis that Kyber hadn't carried out. Kyber's arguments that it was gaining security from \"the (polynomial) number of calls to the SVP oracle that are required to solve the MLWE problem\" and \"the gate count required for one 'operation' \" were plausible, but didn't seem to be enough to rescue Kyber-512 without further help. Kyber's argument that it was gaining security from \"the cost of access into exponentially large memory\" was plausible as a matter of real-world attack costs. NTRU Prime had already proposed a particular way to quantify this cost. However, the official call for submissions had asked for a security level of at least 2143 \"classical gates\" without regard to memory-access costs. So this argument was useless for rescuing Kyber-512: it wasn't what the official evaluation criteria were asking for. To try to rescue Kyber-512, the round-3 Kyber submission changed Kyber-512 and (as noted above) redefined Core-SVP to obtain Core-SVP 2118 rather than 2112; took (without credit) my preliminary analysis of the gaps between Core-SVP and reality; added further numerical estimates regarding the gaps and the \"known unknowns\"; concluded that this preliminary analysis gave a 32-bit range of security estimates, specifically 151 bits plus or minus 16 \"in either direction\"; and claimed that dropping to 135 wouldn't be \"catastrophic, in particular given the massive memory requirements that are ignored in the gate-count metric\". The memory argument again wasn't relevant, given the official evaluation criteria asking for 2143 \"classical gates\". Kyber-512 wasn't claiming to require 2143 \"classical gates\" to break; it was claiming some undetermined number between 2135 and 2167. Various papers then appeared claiming to cut further bits out of lattice security in various ways, such as a 2022 paper reporting an order-of-magnitude speedup from tweaking the \"BKZ\" layer inside attacks. Many of the papers made the analyses of lattice security even more complicated and even less stable than before. For example, for one line of \"dual attacks\": there's an Asiacrypt paper and a paper from Israel's Matzov organization with complicated analyses claiming to reduce Kyber-512's 151 to 137; but then there's a Crypto paper \"Does the dual-sieve attack on learning with errors even work?\" giving the impression that, no, this whole line of attacks fails; but then the actual contents of the Crypto paper are merely saying that there's a \"presumably significant\" change in the improvements without quantifying the change; but then there's a new paper \"A remark on the independence heuristic in the dual attack\" that sounds like it's helping quantify the change; but that paper still doesn't get all the way to claiming any particular attack cost for Kyber-512; but then there's another new paper \"Rigorous foundations for dual attacks in coding theory\" that, for a dual attack against an analogous low-rate decoding problem, says that a \"slight modification of this algorithm\" avoids the issue raised in the Crypto paper; but that paper doesn't analyze what the idea means for lattices; but then there's another new paper \"Provable dual attacks on learning with errors\" that says it proves the correctness of a simplified dual attack for lattices; but that paper also doesn't quantify consequences for Kyber-512. And this is just one small piece of a giant unholy mess that some cryptographers say we should trust. How, back in 2022, did NIST end up concluding that Kyber-512 is as hard to break as AES-128? Time to look at some quotes. I'll go through the quotes in two parts: first, looking at what NIST said its notion of hardness was; second, going line by line through what NIST said about Kyber-512's security level. NIST rescuing Kyber-512, part 1: manipulating the qualification criteria. In the call for submissions, it was crystal clear that cryptosystems had to be at least as hard to break as AES-128 in every \"potentially relevant\" cost metric: Each category will be defined by a comparatively easy-to-analyze reference primitive, whose security will serve as a floor for a wide variety of metrics that NIST deems potentially relevant to practical security. ... In order for a cryptosystem to satisfy one of the above security requirements, any attack must require computational resources comparable to or greater than the stated threshold, with respect to all metrics that NIST deems to be potentially relevant to practical security. (Emphasis in original.) The call commented on the \"classical gates\" to break AES-128 etc. Obviously \"classical gates\" were a \"potentially relevant\" cost metric. What exactly is this metric? The literature defines many different gate sets. NIST dodged years of requests to define exactly which gates it was including as \"classical gates\". NIST's 2022 selection report finally pinned down one part of this, allowing \"each one-bit memory read or write\" as a cost-1 gate. Here's an illustration of how important definitions of cost metrics are: Kyber's security analysis relies on an Asiacrypt 2020 paper for counting the number of \"gates\" inside the most important attack step inside \"primal\" attacks. Tung Chou and I have a new paper \"CryptAttackTester: formalizing attack analyses\" including an appendix that, for Kyber-512, cuts almost 10 bits out of the \"gate\" count for the \"primary optimisation target\" in the Asiacrypt 2020 paper, exploiting the fact that the Asiacrypt 2020 paper counts a memory-access \"gate\" as cost 1. (The Asiacrypt 2020 paper also relies on this; it's not a typo in that paper.) The same appendix also disproves the claim that an \"optimal\" AES-128 key search requires 2143 \"gates\", but the reduction in AES-128 \"gate\" counts isn't as large as the reduction in Kyber-512 \"gate\" counts. Keep this in mind if you hear people claiming that the costs of lattice attacks have been thoroughly analyzed. Anyway, being able to access arbitrarily large amounts of memory for cost 1 isn't realistic: the actual costs of data communication grow with distance. But NIST said in 2020 that anyone proposing a replacement metric \"must at minimum convince NIST that the metric meets the following criteria\", which \"seems to us like a fairly tall order\": \"The value of the proposed metric can be accurately measured (or at least lower bounded) for all known attacks (accurately mere means at least as accurately as for gate count.)\" \"We can be reasonably confident that all known attacks have been optimized with respect to the proposed metric. (at least as confident as we currently are for gate count.)\" \"The proposed metric will more accurately reflect the real-world feasibility of implementing attacks with future technology than gate count -- in particular, in cases where gate count underestimates the real-world difficulty of an attack relative to the attacks on AES or SHA3 that define the security strength categories.\" \"The proposed metric will not replace these underestimates with overestimates.\" There have been no announcements on the NISTPQC mailing list of anyone claiming to have met these minimum criteria, never mind the question of whether such a claim could survive public scrutiny. Recall that NIST excluded NTRU-509 from the figures and tables in its selection report, the report announcing the selection of Kyber over NTRU. If you look for the report's explanation of why NIST excluded NTRU-509, you'll find the following quote: The submission specification uses both local and non-local cost models for determining the security category of their parameter sets. For a more direct comparison with the other KEM finalists, the assignment of security categories according to the non-local cost model is appropriate. This is what NIST used for NTRU in the figures and tables in this report. The underlying definition of \"local\" accounts for long-distance communication costs, whereas \"non-local\" allows accessing arbitrarily large amounts of memory for free. Everything I've been describing from NIST above, and more, sounds consistent with the official call for submissions asking for 2143 \"classical gates\", not counting the costs of memory access: To try to avoid overestimating security levels, NIST was insisting on counting just bit operations for computation, ignoring the costs of communication. In response to the round-1 NTRU Prime submission, which provided a detailed rationale for including the costs of memory access, NIST complained in its round-1 report that the submission \"uses a cost model for lattice attacks with higher complexity than many of the other lattice-based candidates\". (NTRU Prime started reporting Core-SVP in round 2.) In its round-2 report, as noted above, NIST complained that \"the NTRU submission lacks a category 5 parameter set proposal\" when memory-access costs are ignored. In its selection report, NIST kicked out NTRU-509 because NTRU-509's \"category 1\" claim relied on a \"local cost model\", i.e., accounting for memory-access costs; see above for the full quote. With this in mind, consider the fact that NIST was including Kyber-512 in its figures and tables in the same report. This must mean that NIST was claiming that breaking Kyber-512 takes at least 2143 bit operations, without accounting for memory-access costs, right? Nope. NIST doesn't ask Kyber to meet the same criteria as other submissions. In November 2022, NIST announced a list of parameter sets that it was \"planning\" to standardize, including Kyber-512. NIST's announcement avoided claiming that Kyber requires as many \"classical gates\" to break as AES-128. The announcement specifically acknowledged the possibility of Kyber being \"a few bits\" below (while omitting the possibility of Kyber being many more bits below): It is clear that in the gate-count metric it is a very close call and that in this metric the pre-quantum security of Kyber-512 may be a few bits below the one of AES-128. Instead the announcement relied on accounting for \"realistic memory access costs\" to claim that Kyber-512 qualified for \"category 1\": ... the best known attacks against Kyber-512 require huge amounts of memory and the real attack cost will need to take the cost of (access to) memory into account. This cost is not easy to calculate, as it depends on the memory access patterns of the lattice algorithms used for cryptanalysis, as well as the physical properties of the memory hardware. Nonetheless, barring major improvements in cryptanalysis, it seems unlikely that the cost of memory access will ever become small enough to cause Kyber-512 to fall below category 1 security, in realistic models of security that take these costs into account. We acknowledge there can be different views on our current view to include Kyber-512. As a point of clarification: in this email, we refer to parameter sets based on the claimed security strength category where those parameter sets are most recently specified, irrespective of whether those parameter sets actually meet their claimed security level. That said, our current assessment is that, when realistic memory access costs of known attacks are taken into account, all the parameter sets we plan to standardize do, in fact, meet their claimed security strength categories. (Emphasis added.) So NIST used a \"non-local\" free-memory metric to kick out NTRU-509, but used a memory-access-is-expensive metric to claim that Kyber-512 qualifies for \"category 1\". Can anyone tell me how these two things make sense together? (As a side note, NIST subsequently stated that its 2022 selection report was merely reflecting \"the submitters' claimed security categories\" and that the report was making no \"assertions about whether or not the parameter sets actually provided the claimed level of security\". How does NIST reconcile this with the report kicking out NTRU-509 while keeping Kyber-512? Both of those submissions were claiming to achieve \"category 1\" given memory-access costs.) For anyone who cares about reviewability of security analyses, NIST's sudden switch to accounting for Kyber's memory-access costs should be setting off alarm bells. None of the official Kyber security analyses had tried to quantify the effects of memory on security levels. The Kyber documentation had merely pointed at memory as supposedly saving the day in case there weren't enough \"gates\". In the absence of an analysis, how exactly was NIST concluding that memory-access costs were enough to close the gap? NIST rescuing Kyber-512, part 2: NIST's botched security analysis. In early December 2022, I asked how NIST was arriving at its conclusion that Kyber-512 was as hard to break as AES-128. NIST followed up with an explanation on 7 December 2022. I'll refer to this explanation as \"NIST's botched security analysis of Kyber-512\"; for brevity, \"NISTBS\". One of the complications in NISTBS is that it considers a large space of scenarios, with analysis steps mixed into comments on the likelihood of each scenario. Even worse, NISTBS doesn't give any confirming end-to-end examples of the tallies obtained in each particular scenario. So a security reviewer has to trace carefully through each step of NISTBS. Here's one example of a scenario from within the space that NISTBS specifies. I'll call this \"scenario X\" for future reference. Scenario X makes the following three assumptions: Assume accuracy of 2137 from the most recent attack paper taken into account (Matzov) regarding the number of \"gates\". (This is a number specifically mentioned in NISTBS as a starting point; see below. NISTBS also considers the more complicated possibility of this estimate being invalid.) Assume this isn't affected by the \"known unknowns\". (This is a possibility specifically mentioned in NISTBS; see below. NISTBS also considers the more complicated possibility of the security level being affected by the \"known unknowns\".) Assume accuracy of the RAM-cost model in the NTRU Prime documentation. (This is one of two sources that NISTBS repeatedly points to and calculates numbers on the basis of. NISTBS also considers other possibilities for the RAM cost.) Obviously the quantitative conclusions of NISTBS vary depending on the exact assumptions. Considering scenario X is simpler than considering the full space of scenarios. I'll use scenario X as an example below. Without further ado, here's every line of NISTBS, NIST's botched security analysis of Kyber-512. We can elaborate a little bit further on our reasoning leading to our current assessment that Kyber512 likely meets NIST category I (similar considerations apply to the other parameter sets we plan to standardize for lattice-based schemes.) This is a preliminary statement regarding the importance of the calculations at hand. See below for the calculations. That said, beyond this message, we don’t think further elaboration of our current position will be helpful. While we did consult among ourselves and with the Kyber team, I filed a formal complaint in December 2022 regarding NIST's lack of transparency for its investigation of Kyber-512's security level. As noted above, I filed a new FOIA request in January 2023. it’s basically just our considered opinion based on the same publicly available information everyone else has access to. This is not true. NISTBS starts from, e.g., the Matzov paper's 2137 estimate for \"gates\", but then goes beyond this in quantifying the impact of memory costs, something the Matzov paper definitely did not do. What we'll see later is how NISTBS botches its own calculations starting from the Matzov number. The point of this thread is to seek a broader range of perspectives on whether our current plan to standardize Kyber512 is a good one, and a long back and forth between us and a single researcher does not serve that purpose. Public review of NIST's security evaluations requires transparency and clarity regarding those evaluations. It is not appropriate for NIST to be asking for a range of perspectives while concealing information. An open and transparent process would involve less \"back and forth\" than the process that NIST chose. Here's how we see the situation: In April this year, “Report on the Security of LWE” was published by MATZOV (https://zenodo.org/record/6412487#.Y4-V53bMKUk), describing an attack, assessed in the RAM model to bring some parameter sets, including Kyber512, slightly below their claimed security strength categories. This is the most recent attack paper mentioned in NISTBS. That's why my definition of scenario X says \"the most recent attack paper taken into account (Matzov)\". It's surprising that NISTBS doesn't mention any of the newer attack papers. NIST had hypothesized that there are no \"major improvements in cryptanalysis\" (see full quote above), but this doesn't justify ignoring the improvements that have already been published! Anyway, given that NISTBS is calculating security levels starting from the Matzov paper, let's look carefully at those calculations. \"Assessed in the RAM model\" appears to be referring to the Matzov paper counting the number of \"gates\". As a side note, \"the\" RAM model is ambiguous; the literature defines many different RAM models, and many different sets of \"gates\", as noted above. In particular, the report estimates the cost of attacking Kyber512 using a classical lattice attack to be 2137 bit operations, which is less than the approximately 2143 bit operations required to classically attack AES-128. NISTBS takes this 137 as the foundation of various calculations below. This doesn't mean NISTBS is saying Kyber-512 is broken in 2137 \"gates\". NISTBS is saying that Matzov estimated 137, and then NISTBS is calculating various consequences of the 137. If the 137 is inaccurate then the details of the NISTBS calculations (see below) go up or down accordingly. For purposes of putting together the sources available, the simplest case to consider is that 2137 accurately counts the number of \"gates\". Scenario X explicitly assumes this. However, like previous lattice attacks, the MATZOV attack is based on sieving techniques, which require a large amount of (apparently unstructured) access to a very large memory. In announcing its plans to standardize Kyber-512, NIST had said that \"the best known attacks against Kyber-512 require huge amounts of memory\"; here NISTBS is reiterating this. The RAM model ignores the cost of this memory access, Indeed, the \"gate\" counts in question ignore the cost of memory access. and while the science of comparing the cost of memory access to other costs involved in a large cryptanalytic attack is not as mature as we would like, it seems overwhelmingly likely that, in any realistic accounting of memory access costs, these will significantly exceed the costs that are assessed by the RAM model for lattice sieving. Here are three obvious examples of quantitative questions raised by this part of NISTBS. Quantification is essential for cryptographic security review. First, what exactly does \"significantly\" mean in this context? Second, how does NISTBS reach its \"overwhelmingly likely ... significantly exceed\" conclusion? Third, how does NISTBS get from \"significantly exceed\" to its conclusion that having Kyber-512 fall short of AES-128 is \"unlikely\"? (Assuming no \"major improvements in cryptanalysis\".) NISTBS does eventually get to some quantified calculations; see below. The largest practical implementation of sieving techniques we know of, described in detail in “Advanced Lattice Sieving on GPUs, with Tensor Cores” by Ducas, Stevens, and van Woerden (https://eprint.iacr.org/2021/141), was forced by memory access limitations, to adopt settings for bucket size, that would be suboptimal according to the RAM model. Something else unclear from this part of NISTBS is whether \"bucket size ... suboptimal\" is supposed to imply NIST's \"significantly\" claim regarding \"costs\", and, from there, NIST's claim that it's \"unlikely\" for Kyber-512 to be easier to break than AES-128. It should be noted that, increasing the scale of the instances being attacked to near cryptographic scale would probably require extensive hardware optimization, e.g. by using special purpose ASICs, and these techniques, being generally acknowledged to be less effective against memory-intensive tasks, would likely make memory access even more of a bottleneck. Qualitatively, this is a reasonable summary of what the literature on point is saying. However, at this point the reader still doesn't know how NISTBS gets from this to the claim that Kyber-512 is \"unlikely\" to be below the AES-128 security level. Additionally, This is where NISTBS transitions into quantification. While the Kyber, Dilithium, and Falcon teams did not give a quantitative assessment of the practical cost of memory access during sieving against cryptographic parameters, assessments by the NTRU and NTRUprime teams gave estimates that would suggest the cost of sieving against category 1 parameters, in models that account for the cost of memory access, is something like 20 to 40 bits of security more than would be suggested by the RAM model. Finally some numbers to work with! See below for how NISTBS uses these numbers. As a side note, NIST seems to have very low confidence in the numbers it's citing, saying not just \"estimates\" but also \"suggest\" and \"something like\". But the question I want to focus on here is not how confident NIST is in the sources that it cites. The question is simply what security level NISTBS is calculating for Kyber-512 starting from the sources it cites. Scenario X explicitly assumes accuracy of one of the two sources that NISTBS cites, specifically NTRU Prime. In context, this choice of source is favorable to Kyber: NISTBS points to NTRU Prime as giving Kyber a 40-bit bonus, and points to NTRU as giving Kyber only a 20-bit bonus. (For NTRU’s estimates see section 6.3 of the round 3 specification document available at https://ntru.org/index.shtml . For NTRUprime’s estimates see section 6.11 of https://ntruprime.cr.yp.to/nist/ntruprime-20201007.pdf . Scenario X specifically assumes \"accuracy of the RAM-cost model in the NTRU Prime documentation\", one of the two sources that NISTBS relies upon for its quantification. See below for the numbers that NISTBS obtains from this source. The Kyber spec (available at https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf) discusses, but does not quantify, memory access costs in section 5.3 (Q6)) Indeed, what's cited here doesn't quantify this. So let's keep going with the numbers that NISTBS obtains from other sources. Taking Matzov's estimates of the attack cost to be accurate, This is exactly what scenario X is assuming. Of course, NISTBS also considers other possibilities, but, as an illustrative example, let's follow through what NISTBS obtains from this assumption. only 6 bits of security from memory access costs are required for Kyber512 to meet category 1, Indeed, 137 is \"only\" 6 bits short of the 143 goal. NIST wants to find 6 bits of security that it can credit to Kyber-512, plus so much security margin that it can claim not to be worried about the \"known unknowns\" etc. The point of NISTBS is to argue that the costs of memory do the job. so in this case Kyber512 would meet category 1 even if the NTRU and NTRUprime submission significantly overestimate the cost of memory access in lattice sieving algorithms. Here NIST is finding more than its desired 6 bits of security, by giving Kyber the aforementioned \"20 to 40 bits\" coming from \"assessments by the NTRU and NTRUprime teams\" of the extra costs coming from memory access. For example, if NTRU says 20 and if this is accurate, then NISTBS is calculating a security level of 137+20 = 157, safely above 143. (Again, this is explicitly assuming accuracy of the 137 in the first place.) As another example, if NTRU Prime says 40 and if this is accurate, then NISTBS is calculating a security level of 137+40 = 177, even farther above 143. (Once again assuming accuracy of the 137.) See how simple this calculation is? NISTBS points to its sources as saying that there are actually \"20 to 40 bits of security more than would be suggested by the RAM model\" (in NIST's words). So NISTBS adds 20 or 40 to Matzov's 137, giving 157 or 177. NIST says that even if those sources have \"significantly\" overestimated the memory-access cost then Kyber-512 is still okay. To figure out what NIST means by \"significant\" here, simply work backwards from NIST's desired conclusion: if \"20 bits\" is overestimated by as many as 14 bits, then that still leaves 20−14 bits, covering the desired 6 bits. Anyway, Scenario X simply assumes accuracy of the NTRU Prime RAM-cost model. Further, since about 5 of the 14 claimed bits of security by Matzov involved speedups to local computations in AllPairSearch (as described by section 6 of the MATZOV paper), it is likely that Kyber512 would not be brought below category 1 by the MATZOV attack, as long as state of the art lattice cryptanalyses prior to the MATZOV paper were bottlenecked by memory at all. It's of course correct that if there's a bottleneck then speeding up computations outside the bottleneck has little impact. See below for how NIST seems to be using this to claim even more security. However, we acknowledge there is some additional uncertainty in the exact complexity of the MATZOV attack (and all other sieving-based lattice attacks) due to the known-unknowns Dan alludes to (described with quantitative estimates in section 5.3 of the Kyber spec.) Three reasons that it might be possible to beat Matzov's 2137 \"gates\" are (1) inaccuracies in Matzov's analysis (of course, these could also point the other way), (2) missing optimizations covered by the \"known unknowns\", and (3) missing optimizations beyond the \"known unknowns\". Here NIST is pointing to #2. As a side note, it's disturbing to not see NIST accounting for #1 and #3. NIST explicitly assumed that there are no \"major\" improvements in cryptanalysis; but some of its scenarios have Kyber with very few bits of security margin, and closing those wouldn't require \"major\" improvements. Scenario X skips this complication: it explicitly assumes that the 137 is accurate, and that there are no improvements from the \"known unknowns\". Nonetheless, even taking the most paranoid values for these known-unknowns (16 bits of security loss), This is what the Kyber documentation says is the worst case, yes. the cost of memory access and/or algorithmically making memory access local, would still need to be less than what both the NTRU and NTRUPrime submissions assume. I found this puzzling when I first saw it: if we take 137, and then subtract a hypothesized 16, then we need to find 22 bits, which is less than the 40 that NISTBS mentioned but not less than the 20. What's going on? The best explanation I could come up with is that NIST thinks the 16 overlap the 5 bits that NISTBS mentioned above from Matzov, so NIST is actually taking 137−16+5, meaning that NIST has to find only 17 bits, and then the 20 that NISTBS attributes to NTRU is enough (at least if we disregard the uncertainties conveyed by \"estimate\" and \"suggest\" and \"something like\"). Again, Scenario X simply assumes that the 137 is accurate, with no speedups from the \"known unknowns\", so this complication doesn't arise for that scenario. The low end estimate of approximately 20 bits (from the NTRU submission) is based on a conjecture by Ducas that a fully local implementation of the BGJ1 sieving algorithm is possible. Here NIST is pointing to a reason to ask whether the NTRU model is too low. Scenario X explicitly takes the NTRU Prime model, which doesn't trigger this particular issue. So, in the case that all known-unknowns take on the most paranoid values, this would either require a sieving algorithm with local memory access that is much better than any such published algorithm, and in fact better than any that has been conjectured (at least as far as we are aware), This is summarizing the NISTBS calculations from the perspective of what algorithmic improvements would be required to break NIST's conclusions. This isn't relevant to scenario X. or it would require the approximately 40 bits of additional security quoted as the \"real cost of memory access\" by the NTRUprime submission to be a massive overestimate. This is summarizing the NISTBS calculations from the perspective of what modeling errors would be required to break NIST's conclusions. It's concerning to observe deviations between what NISTBS attributes to its source here and what the source actually says. For example, the source says that it's estimating the cost of memory access, whereas NIST incorrectly makes it sound as if the source is mislabeling an estimate as a fact. Furthermore, contrary to what NISTBS's \"quoted as\" claim leads readers to believe, the \"40 bits\" that NISTBS claims as memory overhead is not a quote from what the source says on this topic. Presumably NIST obtained 40 in the following easy way: look at the security-level table on page 103 of the source; observe that pre-quantum sieving for sntrup653 at the top is listed as 169 and 129 for \"real\" and \"free\" respectively; subtract the 129 from the 169. In any event, a lot of things would have to go wrong simultaneously to push the real-world classical cost of known attacks against Kyber512 below category 1, which is why we don't think it's terribly likely. This is going beyond the per-scenario calculations into an overall probability conclusion. As a final note, known quantum speedups for lattice sieving are much less effective than Grover’s algorithm for brute force key search, so in the likely scenario where the limiting attack on AES128 is Grover’s algorithm, this would further increase the security margin of Kyber512 over AES128 in practice. This is yet another complication, and one with several unquantified steps. It's also blatantly inconsistent with earlier comments from NIST on the impact of Grover's algorithm. For example, in email dated 11 Sep 2017 13:48:59 +0000 to pqc-forum@nist.gov (before the list moved to Google), NIST wrote that \"even if we assume the sort of quantum technology often suggested to be possible in 15 years (e.g. ~1GW power requirement and a few hours to factor a 2048 bit number), current technology can still do brute force search cheaper than Grover’s algorithm\". Where are the numbers backing up NIST's new claim that Grover's algorithm is \"likely\" the top threat? Surely NIST agrees that pre-quantum metrics are at least \"potentially\" relevant to the practical security of Kyber-512. Consequently, under the official evaluation criteria, NIST can't use post-quantum metrics as a way to rescue Kyber-512 if Kyber-512 is easier to break than AES-128 in the pre-quantum metrics. I'll focus below on how NISTBS botched its calculation of the pre-quantum Kyber-512 security level. What the underlying numbers actually mean. Core-SVP is a rough estimate for the number of iterations in a particular type of lattice attack. Each iteration involves large-scale memory access and computation. Let's look at how the latest versions of the documentation for two submissions, NTRU Prime and Kyber, convert their estimates for the number of iterations into larger security-level estimates. (Note that both of the documents in question are from 2020, so the numbers don't include subsequent attack improvements.) NTRU Prime focuses on the cost of memory access. In particular, for the important task of sorting N small items, a two-dimensional circuit of area essentially N needs time essentially N1/2, whereas a circuit of the same area running for the same time can carry out essentially N3/2 bit operations. To put these two types of costs on the same scale, the NTRU Prime documentation estimates \"the cost of each access to a bit within N bits of memory as the cost of N0.5/25 bit operations\", and explains how the 25 comes from analyzing energy numbers reported by Intel. As a concrete example: The NTRU Prime documentation reports Core-SVP 2129 for sntrup653, meaning a rough estimate of 2129 iterations. The documentation also reports a rough estimate that memory accesses cost, in total, the equivalent of 2169 bit operations for sntrup653. This comes from combining the N0.5/25 formula with estimates for N, for the number of iterations, and for the number of bits accessed inside each iteration. For comparison, recall that Kyber-512 says Core-SVP 2118. A rough estimate for the cost of memory accesses in this Kyber-512 attack is the equivalent of 2154 bit operations. This might sound similar to the Kyber documentation estimating 2151 bit operations (\"gates\"). But the 2151 estimate in the Kyber documentation isn't an estimate of the bit-operation equivalent of memory access. It's ignoring memory access. It's instead considering the number of bit operations used inside the attack's computations, and estimating that this number is somewhere between 2135 and 2167, given the \"known unknowns\". Agency desperation, revisited. With the meaning of the numbers in mind, let's briefly summarize how NISTBS tries to use computations and memory to push up the claimed security level of Kyber-512: Start with 118 bits of security for Core-SVP. Indeed, Core-SVP estimates 2118 iterations, at least with the round-3 Kyber redefinition of Core-SVP. Add 33 bits of security, giving Kyber-512's claimed 151 bits of security, to account for the bit operations used in computations. Yes, the Kyber-512 documentation has a preliminary estimate of 2151 bit operations. Oh, oops, Kyber says this could be 16 bits too high, and Matzov says it reached 137, and maybe these could be combined, and there are other attack papers too? That's okay: memory will come to the rescue! Will it? Quantification needed. Add \"40 bits of additional security\" (NIST's words) supposedly estimated by NTRU Prime, turning Matzov's 137 bits of security into 177 bits of security. This is where NISTBS goes horribly wrong. The calculation here doesn't even pass basic type-checking. Yes, there's a 240 in NTRU Prime for sntrup653, but that's 240 bitops/iter. Multiplying this by Matzov's bitops, and portraying the result as bitops, is nonsense from NIST. Whatever the cost is for computation per iteration, you have to add that to the cost for memory access per iteration. Multiplying is wrong. In the typical case of both numbers being considerably above 1, multiplying the numbers—which is exactly what NISTBS is doing when it says \"40 bits of security more than would be suggested by the RAM model\" and \"40 bits of additional security\"—gives an embarrassing, indefensible overestimate of attack costs. To finish this NISTBS recap, let's briefly summarize the happy conclusions that NISTBS draws: Look at how much security margin we have here! The critical point is that, starting from 137, \"only 6 bits of security from memory access costs are required for Kyber512 to meet category 1\" (NIST's words). So we don't have to worry about a few bits here and there, such as the possibility of 137 being too high. We can even get away with replacing 40 bits of NTRU Prime with an attacker-optimistic 20 bits of security from NTRU, since that gives 157 bits of security. Still way above 143! Surely we aren't going to lose all 16 bits from the \"known unknowns\". To summarize, \"a lot of things would have to go wrong simultaneously to push the real-world classical cost of known attacks against Kyber512 below category 1, which is why we don't think it's terribly likely\" (NIST's words). Yeah, sounds great, except that it's all based on a botched calculation. How easy it is to catch the error. This blog post is aimed at people who want to understand the whole picture of what's going on here. But imagine that you're looking at NISTBS without knowing any of this. How quickly can you see that NISTBS is wrong? I think the fastest answer is the following simple sanity check. If Kyber estimates that the computations in breaking Kyber-512 cost between 2135 and 2167 bit operations, and NTRU Prime estimates that the memory accesses in breaking sntrup653 (which seems harder to break than Kyber-512) cost the equivalent of 2169 bit operations, and attacks then improve by a factor 214, how can NIST end up estimating that breaking Kyber-512 costs 2177 bit operations? This doesn't tell you where NIST went wrong, but there's a more basic trick that works for that. See where NISTBS is claiming that the NTRU Prime documentation estimates \"40 bits of security more than would be suggested by the RAM model\" (NIST's words), without giving a full quote from the NTRU Prime documentation? I'm one of the NTRU Prime submitters. I already knew that this NISTBS claim was false: it's misattributing NIST's wishful thinking to the NTRU Prime documentation. But say you're reading this claim without knowing in advance that it's false. How do you figure out that it's false? Here's a hard answer and an easy answer: Hard answer: Follow NISTBS's pointer to Section 6.11 of the documentation. That section starts on page 68, ends on page 70, doesn't say \"40\", and doesn't say \"the RAM model\". You can read through all the formulas and comments, try to match it up to the NISTBS claim, and see that nothing matches. Easy answer: As soon as you observe that this citation is hard to check, simply ask for clarification regarding what exactly the citation is referring to. Honest authors will be happy to clarify. As a followup, let's imagine that NIST responds by saying \"We calculated the 40 by subtracting 129 from 169 on the top row of Table 2\". NIST is then implicitly claiming that the 129 is an example of calculating security in \"the RAM model\". How do you figure out that this implicit claim is false? This followup similarly has a hard answer and an easy answer: Hard answer: Read through enough material about what NIST calls \"the RAM model\" to see that this doesn't match the definition of the 129 in the source document. Easy answer: Simply ask for clarification of what exactly the rest of the citation, the part attributing something about \"the RAM model\" to the NTRU Prime documentation, is referring to. Honest authors will again be happy to clarify. Asking questions is the normal scientific process for rapidly reaching clarity—and rapidly fixing errors. For the particular error at hand, it takes very few rounds to pinpoint the discrepancy: the 2129 in the source document for sntrup653 is Core-SVP, not a gate count in what NIST calls \"the RAM model\". Of course, this clarification process doesn't work when an agency decides to dodge clarification questions, for example because it doesn't want errors to be fixed. The research that would be needed for a correct calculation. To fix NIST's calculation, one needs to carefully distinguish two different effects: Kyber-512's preliminary estimate of security being 33 bits above Core-SVP (151 vs. 118) comes partially from estimating the number of bit operations inside the computations in an iteration inside a \"primal\" attack; see the Asiacrypt 2020 paper mentioned above. The cost for computation per iteration has to be added to the cost for memory access per iteration. Multiplying these costs, as NIST did, is exactly the central mistake highlighted in this blog post. On the other hand, the estimate comes partially from saying that there's an outer loop increasing the number of iterations compared to Core-SVP. Multiplying the new iteration count by the cost of memory access per iteration makes perfect sense. Quantifying these effects requires tracing carefully through hundreds of pages of papers on state-of-the-art lattice attacks (not just rewriting the Asiacrypt 2020 paper) to see what would happen if costs of memory access were included. What makes this really tough is that a change of cost metric also forces reoptimization of the entire stack of attack subroutines, along with all applicable parameters. Consider, as one of many examples, the choice between low-memory \"enumeration\" and high-memory \"sieving\" as a subroutine inside BKZ. The Kyber documentation uses cost metrics that ignore the cost of memory access to conclude that enumeration is less efficient than sieving. If NIST is suddenly saying that memory access makes sieving slower than obviously there's a gap in the Kyber analysis. Where's the recalculation that accounts for the cost of memory access, and for the large recent improvements in enumeration? Shortly after Matzov's attack appeared in April 2022, I had sent a message to the NISTPQC mailing list summarizing the complicated analysis that needed to be done. I took, as an example, a less Kyber-favorable scenario in which the \"known unknowns\" reduce 137 to 121, and I said that simply multiplying the bit-operation count by 240 would be wrong: Does accounting for real RAM costs close the gap between 2121.5 and 2143? One might think that, sure, this is covered by the 240 mentioned above: Kyber-512 previously had security 240*2135.5 = 2175.5, so a 32.5-bit security margin, and the new paper is reducing this to an 18.5-bit security margin: i.e., the new paper is merely cutting out 40% of the Kyber security margin, rather than breaking Kyber outright. But let's look more closely at the numbers. As a preliminary point, round-3 Kyber-512 is starting from Core-SVP just 2112 and revised-Core-SVP just 2118, with exponent 87% and 91% of 129 respectively, so the obvious estimate is about 236 instead of 240. Furthermore, this 236 is accounting for the energy cost of accesses to a giant RAM array, while it's clear that many of the bits of security beyond Core-SVP claimed in the round-3 Kyber security analysis are coming from accounting for the cost of local bit operations. These effects don't multiply; they add! Internally, Core-SVP is starting from estimates of the number of \"operations\" inside sieving. It makes sense to say that the attacker needs to pay for the large-scale memory access inside each \"operation\". It also makes sense to say that the attacker needs to pay for all the bit operations inside each \"operation\". But the local bit operations are an asymptotically irrelevant extra cost on top of the memory access, and the best bet is that they don't make much difference for Kyber-512. The real cost of this type of algorithm is, at a large scale, driven primarily by data motion, not by local computation. ... So I don't see how current knowledge can justify suggesting that the costs of RAM rescue Kyber-512 from the new attack. It seems entirely possible that the real costs of this Kyber-512 attack are considerably below the costs of a brute-force AES-128 attack. Deciding this one way or the other will require much more serious analysis of attack costs. An agency desperate to rescue Kyber-512 will take note of the first part of what I had written: great, memory-access costs bump Kyber's security level up by 40 bits, giving us a healthy security margin! The agency won't listen to the subsequent part saying that, no, this calculation is garbage. The agency won't even listen to the preliminary adjustment of 40 to 36: we have a healthy security margin, why worry about a few bits here and there? Meanwhile, if there's something that sounds like a few bits favoring Kyber-512, then the desperate agency happily takes note of that, as the following example illustrates. The fact that the cost of memory access in each iteration adds to the cost of computation in each iteration, rather than multiplying, has a silver lining for defenders: in the common situation of memory access being dominant, improvements in the cost of computation per iteration make little difference in total cost. I mentioned this in my April 2022 message regarding the Matzov paper: The new paper seems to have some local speedups to the sieving inner loop, which similarly should be presumed to make little difference next to the memory-access bottleneck, but my understanding is that this is under half of the bits of security loss that the paper is reporting. Now look at this from the perspective of the desperate agency. Aha, some bits of the Matzov speedup are computation speedups that won't matter next to memory access! As long as we're willing to switch to counting memory access, this effect downgrades the Matzov speedup, which sounds good for Kyber-512! Sure enough, NISTBS says that \"about 5 of the 14 claimed bits of security by Matzov involved speedups to local computations\", and portrays this as a \"further\" reason for confidence in Kyber-512, beyond the \"40 bits of additional security\" supposedly produced by memory access. This is double-counting the silver lining. Multiplying the 240 cost of memory access per iteration by Matzov's 2137 bit operations is already assuming (implicitly and incorrectly) that every bit operation has its own iteration, giving 2137 iterations. This leaves no room for multiplying by a \"further\" 25. The estimated 25 is actually on a completely different axis: it's an estimate for the Matzov-vs.-previous speedup ratio in one metric divided by the Matzov-vs.-previous speedup ratio in another metric. NIST rescuing Kyber-512, part 3: dodging clarification requests. When NISTBS appeared in December 2022, I looked through and saw that NISTBS was multiplying, rather than adding, the cost of memory access per iteration and the cost of computation per iteration, despite my having already pointed out in April 2022 that this was wrong. But, hmmm, NIST didn't write NISTBS in a verification-friendly way. In particular, as noted above, NIST didn't include any examples of confirming tallies. It seemed perfectly clear that NIST was adding \"40 bits of additional security\" to 137 in scenario X. But NIST didn't bother saying, yes, the security level is 177 in that scenario. NIST also didn't make clear where exactly it was getting the 40 from. When I find mistakes in security analyses, the authors usually say \"Thanks for catching the mistake!\"—except in lattice-based cryptography, where the authors usually claim that they meant something different from what they had written. This continual evasion is a serious disincentive to security review. If there was any way that I could have misunderstood what NISTBS was saying, then I wanted to know that at the outset, before doing the work of writing up an explanation of the error. So I posted a short clarification question. Specifically, I spelled out scenario X and asked whether, in that scenario, I was \"correctly gathering that you're calculating the Kyber-512 security level as 2177 (i.e., 34 bits of security margin compared to 2143 for AES-128), where this 177 comes from the above 137 plus 40, where 40 comes from 169 minus 129 on page 103 of the NTRU Prime documentation, specifically 'real' minus 'free' for pre-quantum sieving for sntrup653\". I was expecting a prompt answer saying \"Yes, for that specific scenario we're calculating 177 bits of security, and we're getting the 40 from the 169 and 129 that you mentioned.\" What actually happened is that NIST didn't reply. Seriously? NIST picks a risky, bleeding-edge cryptosystem to standardize for users worldwide, and then doesn't even bother answering clarification questions about what NIST claims the security level is? I mentioned above that I filed a formal complaint regarding the lack of transparency. Here's what the complaint said: NIST has publicly claimed that Kyber-512 is as difficult to break as AES-128 (see, e.g., page 8 and Figure 1 of NISTIR 8413 claiming that Kyber-512 is \"category 1\"), at least by known attacks. As you know, this is the minimum security level allowed by the official evaluation criteria for the NIST Post-Quantum Cryptography Standardization Project. However, NIST has concealed many details of the investigation that led to this claim. NIST admits that \"we did consult among ourselves and with the Kyber team\"; NIST still has not published those communications. I have been trying to review the details of NIST's work on this topic. NIST's lack of transparency makes this review process unnecessarily difficult. Some information was released by Dr. Moody and Dr. Perlner in response to my requests, but this information is (1) incomplete and (2) unclear. My email dated 8 Dec 2022 03:10:06 +0100 consisted of an \"am I correctly gathering\" clarification question that could have been immediately answered with a simple \"Yes, that's correct\" if my understanding of NIST's calculations was correct; but there was no reply, so presumably NIST actually meant something else. Surely the communications that NIST is concealing shed light on how NIST actually reached the above claim. I am writing to file a formal complaint regarding NIST's failure to promptly and publicly disclose full details of its investigation of the security of Kyber-512. This investigation should have been carried out transparently from the outset, allowing prompt correction of any errors that NIST failed to detect. The fact that NIST was still concealing the details in July 2022 prevented the public from seeing how NIST arrived at NISTIR 8413's claims on the topic. The fact that NIST is continuing to conceal the details today seems inexplicable except as part of NIST trying to limit public review of NIST's security evaluations. Please acknowledge receipt of this message, and please publish full details of NIST's investigation of the security of Kyber-512. I escalated the complaint to NIST's Matthew Scholl on 20 January 2023. Scholl didn't reply. The public still hasn't seen the details of NIST's consultations \"among ourselves and with the Kyber team\" regarding Kyber-512. Maybe Scholl was sending internal email: \"Why is djb asking about this? Did we screw something up again?\" Maybe NIST looked again at my April 2022 message, realized how badly it had botched its Kyber-512 security analysis, and then decided that it could get away with being obstructionist rather than admitting the error. Or maybe NIST, still struggling to catch up on post-quantum cryptography, simply hasn't had time to figure out the meaning of the numbers that it's multiplying to obtain its claims regarding Kyber-512. But this doesn't explain what happened next, namely NIST spending more time dodging clarification questions than it would have spent simply answering the questions. The same day that I escalated my non-transparency complaint to Scholl, I publicly noted NIST's non-responsiveness, and asked if anyone saw another way to interpret NIST's calculations: In the absence of such clarity, reviewers have to worry that putting NIST's stated components together in what seems to be the obvious way, and then doing the work to disprove what NIST appears to be claiming about the security margin, will lead to a response claiming that, no, NIST meant something else. It's natural to ask for clarification. ... I've again gone through NIST's 7 December email, and again concluded that for this scenario NIST is claiming 34 bits in the way spelled out below. Is there any way I could be missing something here? Does anyone see another way to interpret NIST's calculations? NIST dodged, replying that NIST's email \"speaks for itself\". Well, yes, I think NISTBS speaks for itself, and is very clearly adding the \"40 bits of additional security\" to the 137 postulated in scenario X, obtaining 177 in that scenario, i.e., 34 bits more than NIST's 143 target. I was simply asking for NIST to confirm that, yes, in that scenario you take the 137 from Matzov, and add the \"40 bits of additional security\", giving 177 bits of security. NIST also tried to shift attention to the question of \"whether or not our current plan to standardize Kyber512 is a good one\", while downplaying the question of whether NIST had correctly calculated the Kyber-512 security level: While reviewers are free, as a fun exercise, to attempt to \"disprove what NIST appears to be claiming about the security margin,\" the results of this exercise would not be particularly useful to the standardization process. Seriously? NIST kicks out NTRU-509 as supposedly being easier to break than AES-128, keeps Kyber-512 as supposedly being as hard to break as AES-128, repeatedly, inside its rationale for selecting Kyber, points to Kyber-512's efficiency, says it's planning to standardize Kyber-512 as supposedly being as hard to break as AES-128, and then claims that disproving NIST's Kyber-512 security-level calculation wouldn't be useful input? I replied, starting with again asking for clarification: I think I understand what NIST is claiming in that message regarding the quantitative Kyber security level. I think that my clarification question (focusing on one example, much shorter than NIST's message) is identifying the obvious interpretation. But then why hasn't NIST simply said \"Yes, that's correct\" in response? If the interpretation I've identified differs from what NIST meant, can NIST please simply say what the difference is, so that security reviewers don't have to spend time on the quantitative security claims that NIST currently seems to be making? I also commented on the notion that this wouldn't be useful input: If Kyber-512 doesn't meet the minimum security level allowed by the official call for submissions to the NIST Post-Quantum Cryptography Standardization Project then Kyber-512 should not be standardized. NIST's evaluation of the Kyber-512 security level---after various attack advances newer than the latest version of the Kyber submission---depends explicitly on NIST's calculations of the impact of memory costs. With all due respect, is it so hard to imagine that NIST has botched those calculations? If NIST is so sure that it got the whole sequence of calculations right, why is it so resistant to clarification questions that will help reviewers check and confirm that NIST got this right? If NIST isn't sure, doesn't that make public review even more important? In any case, there's a strong public interest in having NIST's security evaluations clearly and promptly explained, to maximize the chance of having errors corrected before bad decisions are set into stone. NIST dodged again: It would be helpful to redirect discussion to 1) The question of whether Kyber512 is as hard to break as AES128, (which is a scientific question that cannot be settled by NIST pronouncements) 2) The related question of whether Kyber512 should be standardized, (which is a question where NIST will ultimately need to make a definitive decision, but thus far we have only signaled we are leaning towards yes.) With this in mind, I would like to note that the technical point on which Dan has asked for clarification is effectively \"how much additional security does Kyber512 get on account of memory access costs, according to the NTRUprime submission's memory cost model?\" Surely Dan, being on the NTRUPrime team, is in a better position to answer this question than us. Seriously? NIST takes NTRU Prime's smallest bitops/iter number, slightly screws up by failing to downscale that number from sntrup653 to Kyber-512, massively screws up by multiplying that number by Matzov's 2137 bitops, claims on this basis that \"a lot of things would have to go wrong simultaneously to push the real-world classical cost of known attacks against Kyber512 below category 1\", and then says that any questions should be addressed to the NTRU Prime team? Even if NIST didn't understand by this point that it had screwed up, it certainly knew that NISTBS was stating conclusions about the Kyber-512 security level relative to AES-128, and those conclusions were not in the source documents that NISTBS was citing. Those conclusions were the result of calculations announced by NIST. It's completely inappropriate for NIST to be trying to deflect clarification questions about those calculations. Chris Peikert had entered the discussion in the meantime to issue blanket denials that NIST was claiming any particular number of bits of security. Of course, Peikert didn't propose an alternative interpretation of NIST's words \"40 bits of additional security\". I posted a line-by-line dissection of NISTBS, very similar to the line-by-line dissection shown above, and asked if anyone could see any alternative interpretation: If anyone sees any way that I could be misunderstanding the details of NIST's posting, please pinpoint which step is at issue and what the alternative interpretation of NIST's calculation is supposed to be. There was no reply. Perhaps NIST will now claim that, when it wrote \"40 bits of additional security\", it actually meant something different from, um, 40 bits of additional security. But then why didn't NIST promptly answer my first question by saying that, n",
    "commentLink": "https://news.ycombinator.com/item?id=37756656",
    "commentBody": "Debunking NIST&#x27;s calculation of the Kyber-512 security levelHacker NewspastloginDebunking NIST&#x27;s calculation of the Kyber-512 security level (cr.yp.to) 373 points by bumbledraven 14 hours ago| hidepastfavorite154 comments tptacek 12 hours agoAn important detail you really want to understand before reading this is that NIST (and NSA) didn&#x27;t come up with these algorithms; they refereed a competition, in which most of the analysis was done by competitors and other academics. The Kyber team was Roberto Avanzi, Joppe Bos, Léo Ducas, Eike Kiltz, Tancrède Lepoint, Vadim Lyubashevsky, John M. Schanck, Gregor Seiler, Damien Stehlé, and also Peter Schwabe, a collaborator of Bernstein&#x27;s. reply kpdemetriou 11 hours agoparentAbsolutely, but NIST ultimately choose the winners, giving them the option to pick (non-obviously) weak&#x2F;weaker algorithms. Historically only the winners are adopted. Look at the AES competition - how often do you see Serpent being mentioned, despite it having a larger security margin than Rijndael by most accounts? reply lucb1e 10 hours agorootparent> Historically only the winners are adopted. Look at the AES competitionOften, yes. But also consider the SHA-3 competition.BLAKE2 seems more widely used than what was chosen for SHA-3 (Keccak). What was submitted for the SHA-3 competition was BLAKE1 (it didn&#x27;t have a number back then but I think this is clearer) so it&#x27;s not like NIST said that Keccak is better than BLAKE2, they only said it&#x27;s better than BLAKE1 (per their requirements, which are unlikely to align with your requirements because of the heavy weighing of speed-in-hardware), but still this is an example of a widely used algorithm that is not standardized.> how often do you see Serpent being mentioned, despite it having a larger security margin than RijndaelThe goal of an encryption algorithm is not only to be secure. Sure, that has to be a given: nobody is going to use a broken algorithm when given a choice. But when you have two secure options, the more efficient one is the one to choose. You could use a 32k RSA key just to be sure, or a 4k RSA key which (to the best of my knowledge) everyone considers safe until quantum. (After quantum, you need something like a 1TB key, as djb humorously proposed.)Wikipedia article on Serpent: \"The 32 rounds mean that Serpent has a higher security margin than Rijndael; however, Rijndael with 10 rounds is faster and easier to implement for small blocks.\"I don&#x27;t know that nobody talks about Serpent solely because it was not chosen as winner. It may just be that Rijndael with 256-bit keys is universally considered secure and is more efficient at doing its job. reply kpdemetriou 9 hours agorootparentRe: BLAKE2, I&#x27;m not sure it&#x27;s fair to say that BLAKE2 is more widely used overall. But I do agree BLAKE2 is a bit of an outlier in terms of adoption. I think part of the reason is that SHA2 remains the go-to option, else I&#x27;d expect the ecosystem to consolidate around SHA3.Re: Serpent, there are many things to unpack here but, in summary, you don&#x27;t know a priori how large of a security margin you need (given the primary function of a cipher, you want to pick the conservative option), efficiency concerns become much less relevant with hardware-accelerated implementations and years of Moore&#x27;s law performance uplifts, low-power devices can take advantage of much lighter algorithms than Rijndael OR Serpent, ease of implementation does not equal ease of correct&#x2F;secure implementation vis-a-vis side channel attacks, and certainly if Serpent was chosen you wouldn&#x27;t see Rijndael talked about much. reply pclmulqdq 9 hours agorootparentBlake2 also uses a very SHA2-like construction (a HAIFA construction, which is based on Merkle-Damgard). I believe this was the main reason SHA3 was chosen to be something completely different (a sponge construction). If SHA2 was found to be insecure, Blake2 would be at more risk of also being broken than Keccak.Speculatively, if SHA2 is broken without breaking Merkle-Damgard hashes in general, Blake2&#x2F;3 could well become SHA4. reply lucb1e 8 hours agorootparentprev> I&#x27;m not sure it&#x27;s fair to say that BLAKE2 is more widely used overallAh, maybe my experience is biased then. I keep coming across BLAKE2 implementations, but rarely hear so much as people considering to use SHA-3 somewhere. If anyone has actual numbers on this, that would be interesting.It would be good if SHA-3 is being used because then chip makers have a reason to bake it into their hardware, which is exactly where the biggest gain over SHA-2 is expected. If that happens, and all else being equal (no cracks appear in Keccak), I&#x27;d be surprised if BLAKE2 remains as popular!> you don&#x27;t know a priori how large of a security margin you needTrue, so this can be argued to be an educated guess at first. But then confidence increases over time. It seems to be expected that, more than 20 years later, people aren&#x27;t considering Serpent anymore. Is it because it wasn&#x27;t chosen as AES? Certainly partially, but BLAKE2 (I&#x27;ll admit it does seem like an outlier) likely will still be talked about in the future so standardization is not the only factor.I didn&#x27;t see actual benchmarks, but Serpent sounds at least three times slower than Rijndael for, by now, no tangible benefit. What would be interesting is if there were AES competitors that are also fully unbroken and are more efficient than Rijndael, or easier to implement, etc. reply EGreg 3 hours agorootparentI wish chip makers would bake the elliptic curve used in Bitcoin and Ethereum (secp256k) as well, instead of the entire industry coalescing around secp256r, which many suspect was somehow weaker (since its parameters are some weird large number X instead of a hard-to-game number like 15, leading some to believe that the first X-1 candidates were tried and X was found to be weaker).The real reason I would have liked that to be the case is so that one could use WebAuthn and Subtle Web Crypto to sign things and have the blockchain natively verify the signature.As it is, I am hoping that EVM will roll out a precompiled signature verifier for secp256r, which is on the roadmap — they say! reply arjvik 2 hours agorootparentThere are a few different on-chain implementations of secp256r1 signature verification for use with passkeys, my favorite of which is demoed at https:&#x2F;&#x2F;p256.alembic.techWork is also being done on SNARK-based cross-curve signature verificationBut I fully agree, especially with the growing popularity of account abstraction, the EVM desperately needs a secp256r1 precompile! reply adastra22 9 hours agorootparentprevSHA-3 is vastly more widely used than BLAKE. reply zahllos 10 hours agorootparentprevI fully admit to having a weak spot for Serpent - it is self-bitslicing (see the submission package or the linux kernel tree), which in hindsight makes constant time software easier to write, and it was faster in hardware even when measured at the time, which is where we have ended up putting AES anyway (e.g. AES-NI etc).BUT. On security margins, you could argue the Serpent designers were too conservative: https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;1492 It is also true that cryptanalytic attacks appear to fare slightly better against AES than Serpent. What does this mean? A brute force attack has the same number of operations as the claimed security level, say, 2^128 for 128-bit. An attack is something better than this: fewer operations. All of the attacks we know about achieve slightly less than this security level - which is nonetheless still impossible to do - but that comes at a cost: they need an infeasible amount of memory. In terms of numbers: 9000 TB to reduce 2^128 to 2^126 against full-round AES according to a quick check of wikipedia. For reference, the lightweight crypto competition considered 2^112 to be sufficient margin. 2^126 is still impossible.In practice, the difference between Serpent and AES in terms of cryptanalytic security is meaningless. It is not an example of NIST picking a weaker algorithm deliberately, or I would argue, even unintentionally. It (AES) was faster when implemented in software for the 32-bit world that seemed to be the PC market at the time. reply kpdemetriou 10 hours agorootparentImplemented correctly, I agree the difference in security margin may not be too important. Otherwise, Serpent is more resistant to timing attacks. Weaknesses in implementation are as important as weaknesses in design.Regardless, the comparison wasn&#x27;t intended to argue for a meaningful difference in security margin, but to show that that the winner of the competition, well, wins (in adoption). reply bsder 8 hours agorootparentprev> BUT. On security margins, you could argue the Serpent designers were too conservative: https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2019&#x2F;1492Thanks for digging that paper out again. It is really telling that AES only gets a bit of a bump (10-30%) while the other ones gain like 2x or more.I was about to comment that the competitors to AES were definitely too conservative, and it bit them because of how much slower it made them in software and larger in hardware. reply kevin_thibedeau 8 hours agorootparentprevBlowfish has a continuing existence as the basis for bcrypt. reply tptacek 7 hours agorootparentIt works as a password hash for reasons having in part to do with why it isn’t a great general purpose cipher. reply bonzini 4 hours agorootparentCan you expand, or link to an explanation? reply aidenn0 4 hours agorootparentBlowfish has an unusually slow key-setup phase. Slowness is an advantage for password hashes, since it makes offline attacks harder. replywnevets 12 hours agoparentprevCorrect me if I&#x27;m wrong, everything is also being done out in the open for everyone to see. The NIST aren&#x27;t using some secret analysis to make any recommendations. reply tux3 12 hours agorootparentTeams of cryptographers submit several proposals (and break each other&#x27;s proposals). These people are well respected, largely independent, and assumed honest. Some of the mailing lists provided by NIST where cryptographers collaborated to review each other&#x27;s work are publicNIST may or may not consort with your friendly local neighborhood NSA people, who are bright and talented contributors in their own right. That&#x27;s simply in addition to reading the same mailing listsAt the end, NIST gets to pick a winner and explain their reasonning. What influenced the decision is surely a combination of things, some of which may be internal or private discussions reply mike_d 5 hours agorootparent> NIST may or may not consort with your friendly local neighborhood NSA peopleIt is worth noting that while breaking codes is a big part of the NSA&#x27;s job, they also have a massive organization (NSA Cybersecurity, but I prefer the old name Information Assurance) that works to protect US and allied systems and cryptographic applications.In the balance, weakening American standards does little to help with foreign collection. Their efforts would be much better spent injecting into the GOST process (Russia and friends) or State Cryptography Administration (China and friends). reply justinclift 4 hours agorootparent> In the balance, weakening American standards does little to help with foreign collection.While that makes logical sense, the previous actions of the NSA has demonstrated they&#x27;re not a logical actor in regards to this stuff, or that there&#x27;s more going on. reply nvy 10 hours agorootparentprevI was under the impression that only fools trust NIST after DUAL_EC_whatsit.Is that not the case? reply evil-olive 8 hours agorootparentfrom the article:> I filed a FOIA request \"NSA, NIST, and post-quantum cryptography\" in March 2022. NIST stonewalled, in violation of the law. Civil-rights firm Loevy & Loevy filed a lawsuit on my behalf.> That lawsuit has been gradually revealing secret NIST documents, shedding some light on what was actually going on behind the scenes, including much heavier NSA involvement than indicated by NIST&#x27;s public narrative.even if I had never heard of DUAL_EC_whatsit, there&#x27;s enough here to make me mistrust NIST. reply ethbr1 9 hours agorootparentprevYou mean ANSI&#x2F;ISO&#x2F;NIST and Dual_EC_DRBG, that everyone suspected had a backdoor before it was included as one of multiple options? https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Dual_EC_DRBG#Timeline_of_Dua...Or the s-boxes in DES, that the NSA suggested to IBM + NIST&#x27;s predecessor, so as to be resistant to then-not-widely-known differential cryptanalysis? https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20120106042939&#x2F;http:&#x2F;&#x2F;securespee... reply sneak 6 hours agorootparentOne of those things happened after 9&#x2F;11, and one of those things happened before.There is a widely held belief that the US IC changed fundamentally in terms of their regard for their own raison d’etre that day. reply ethbr1 4 hours agorootparentIt&#x27;ll be curious, looking back from the near future, what prompted the next fundamental change.I&#x27;d like to think the US is in the midst of that now, with the Afghan withdrawal and Ukraine war. reply nvy 7 hours agorootparentprev>You mean ANSI&#x2F;ISO&#x2F;NIST and Dual_EC_DRBG, that everyone suspected had a backdoor before it was included as one of multiple options?What&#x27;s this snarky reply supposed to imply?>Or the s-boxes in DES, that the NSA suggested to IBM + NIST&#x27;s predecessor, so as to be resistant to then-not-widely-known differential cryptanalysis?In your rush to write this oh-so-pithy comment, did you somehow get mixed up and think that when I wrote \"Dual EC\", I somehow meant s-boxes? reply bonzini 4 hours agorootparentHe&#x2F;she means that there have been good things coming out of the NSA&#x2F;NIST collaborations (another example is SHA0->SHA1, introducing a \"mysterious\" left shift that made SHA1 much stronger), and the bad ones are caught quickly. reply Ar-Curunir 9 hours agorootparentprevThings have changed quite a bit since then. reply logifail 21 minutes agorootparentprev> everything is also being done out in the open for everyone to seeWell, everything apart from the secret stuff:\"I filed a FOIA request \"NSA, NIST, and post-quantum cryptography\" in March 2022. NIST stonewalled, in violation of the law. Civil-rights firm Loevy & Loevy filed a lawsuit on my behalf.That lawsuit has been gradually revealing secret NIST documents, shedding some light on what was actually going on behind the scenes, including much heavier NSA involvement than indicated by NIST&#x27;s public narrative\" reply codr7 11 hours agorootparentprevMy rule of thumb in these situations is always: if they could, they would.I&#x27;ve seen enough blatant disregard for humanity to assume any kind of honesty in the powers that were. reply dmix 5 hours agorootparentI&#x27;m sure the NSA 9-5ers justify weakening standards processes by the fact it&#x27;s still secure enough to be useful for citizens and some gov orgs but flawed enough to help themselves when it matters at x point in the future.No one can say they pushed some useless or overtly backdoored encryption. That&#x27;s rarely how Intel agencies work. It&#x27;s also not how they need to work to maintain their effectiveness indefinitely.When the CIA is trying to recruit for HUMINT if they can get claws into anything whether it&#x27;s a business conference that has a 0.1% chance they&#x27;ll meet some pliable young but likely future industry insider that may or may not turn into a valuable source then they&#x27;ll show up to every single year to that conference. It&#x27;s a matter of working every angle you can get.They aren&#x27;t short of people, time, or money. And in security tiny holes in a dam turn into torrents of water all the time.The fact NIST is having non public backroom meetings with NSA, concealing NSA employee paper authors, generating a long series of coincidental situations preferencing one system, and stonewalling FIOAs from reputable individuals. IDK, if was a counter intelligence officer in charge of detecting foreign IC work I&#x27;d be super suspicious of anything sold as safe and open from that org. reply pclmulqdq 12 hours agorootparentprevThere is a final standardization step where NIST selects constants, and this is done without always consulting with the research team. Presumably, these are usually random, but the ones chosen for the Dual-EC DRBG algorithm seem to have been compromised. SHA-3 also had some suspicious constants&#x2F;padding, but that wasn&#x27;t shown to be vulnerable yet. reply tptacek 11 hours agorootparentThe problem with Dual EC isn&#x27;t the sketchy \"constants\", but rather the structure of the construction, which is a random number generator that works by doing a public key transformation on its state. Imagine CTR-DRBG, but standardized with a constant AES key. You don&#x27;t so much wonder about the provenance of the key so much as wonder why the fuck there&#x27;s a key there at all.I don&#x27;t know of any cryptographer or cryptography engineer that takes the SHA3 innuendo seriously. Do you?Additional backstory that might be helpful here: about 10 years ago, Bernstein invested a pretty significant amount of time on a research project designed to illustrate that \"nothing up my sleeves\" numbers, like constants formed from digits of pi, e, etc, could be used to backdoor standards. When we&#x27;re talking about people&#x27;s ability to cast doubt on standards, we should keep in mind that the paragon of that idea believes it to be true of pi.I&#x27;m fine with that, for what it&#x27;s worth. Cryptography standards are a force for evil. You can just reject the whole enterprise of standardizing cryptography of any sort, and instead work directly from reference designs from cryptographers. That&#x27;s more or less how Chapoly came to be, though it&#x27;s standardized now. reply pclmulqdq 11 hours agorootparentI do know a few cryptographers who were suspicious of SHA-3 when it came out, but after some napkin math and no obvious hole was found, they were fine with it. The actual goal of that extra padding was to get extra one bits in the input to avoid possible pathological cases.My understanding of the Dual-EC problem may be different than yours. As I understand it, the construction is such that if you choose the two constants randomly, it&#x27;s fine, but if you derived them from a known secret, the output was predictable for anyone who knows the secret. The NIST did not provide proof that the constants used were chosen randomly.Random choice would be equivalent to encrypting with a public key corresponding to an unknown private key, while the current situation has some doubt about whether the private key is known or not. reply hovav 9 hours agorootparentEven with a verifiably random key, Dual EC is still unacceptable.First, because its output has unacceptable biases [1,2].Second, because its presence allows an attacker to create a difficult-to-detect backdoor simply by replacing the key, as apparently happened with Juniper NetScreen devices [3,4].--- [1] Kristian Gjøsteen, Comments on Dual-EC-DRBG&#x2F;NIST SP 800-90, draft December 2005. Online: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20110525081912&#x2F;https:&#x2F;&#x2F;www.math....[2] Berry Schoenmakers and Andrey Sidorenko, Cryptanalysis of the Dual Elliptic Curve Pseudorandom Generator, May 2006. Online: https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2006&#x2F;190.pdf[3] Stephen Checkoway, Jacob Maskiewicz, Christina Garman, Joshua Fried, Shaanan Cohney, Matthew Green, Nadia Heninger, Ralf-Philipp Weinmann, Eric Rescorla, and Hovav Shacham, A Systematic Analysis of the Juniper Dual EC Incident, October 2016. Online: https:&#x2F;&#x2F;www.cs.utexas.edu&#x2F;~hovav&#x2F;dist&#x2F;juniper.pdf[4] Ben Buchanan, The Hacker and the State, chapter 3, Building a Backdoor. Harvard University Press, February 2020. reply tptacek 11 hours agorootparentprevWho were those cryptographers? reply adgjlsfhk1 10 hours agorootparentthe NSA reply pclmulqdq 9 hours agorootparentThis is closer to the correct answer than anyone who writes publicly about cryptography. replytptacek 12 hours agorootparentprevYou don&#x27;t really know, but you can be reasonably sure that they didn&#x27;t sabotage the submissions themselves. reply thadt 11 hours agoprevThe unfortunate reality of this is that while he may be right, it is difficult to classify the responses (or non-response) from the NIST people as deceptive vs just not wanting to engage with someone coming from such an adversarial position. NIST is staffed by normal people who probably view aggressively worded requests for clarification in the same way that most of us have probably fielded aggressively worded bug reports.Adding accusatory hyperbolic statements like: \"You exposed three years of user data to attackers by telling people to use Kyber starting when your patent license activates in 2024, rather than telling people to use NTRU starting in 2021!\" doesn&#x27;t help. Besides the fact that nobody is deploying standalone PQ for some time, there were several alternatives that NIST could have suggested in 2021. How about SIKE? That one was pretty nice until it was broken last year.Unfortunately, NIST doesn&#x27;t have a sterling reputation in this area, but if we&#x27;re going to cast shade on the algorithm and process, a succinct breakdown of why, along with a smoking gun or two would be great. Pages and pages of email analysis, comparison to (only) one other submission, and accusations that everyone is just stalling so data can be vacuumed up because it is completely unprotected makes it harder to take seriously. If Kyber-512 is actually this risky, then it deserves to be communicated clearly. reply lucb1e 9 hours agoparentThis is 100% in line my reading of the submission.Also noting that the page contains seventeen thousand words. That many words of harry potter take an average person 70 minutes to read. This text is no harry potter: it&#x27;s chock-full of numbers, things to consider, and words and phrasings to weigh (like when quoting NIST), so you&#x27;re not going to read it as fast as an average book, if you know enough about PQC to understand the text in the first place.I even got nerdsniped near the beginning into clicking on \"That lawsuit has been graduallysecret NIST documents, shedding some light on what was actually going on behind the scenes\". That page (linked by the word ) is another 54000 words. Unaware, due to not having a scroll bar on mobile (my fault, I know), I started skimming it linearly to see what those revelations might be. Nothing really materialized. At some point I caught on that I seemed to have enrolled for a PhD research project and closed that tab to continue reading the original page...Most HN readers, who are often smart and highly technical but in various fields, cannot reasonably weigh and interpret the techobabble evidence for \"nist=bad\". Being in an adjacent field, I would guess that I understand more than the average reader, but still don&#x27;t feel qualified to judge this material without really giving it a thorough read. The page reasonably gives context and explains acronyms, but there&#x27;s just so much of it that I can&#x27;t imagine anyone who doesn&#x27;t already know would want to bother with it. Not everyone understanding a submission is okay, but this is about accusations, and that makes me feel like it is not a good submission for HN. reply hn_throwaway_99 9 hours agoparentprevEdit: Just realized the author is djb, Daniel Bernstein, which I guess is semi-ironic for me because I was recently praising him on HN for an old, well-read blog post on ipv6. Thus, I guess I may take back a bit of what I said below, or least perhaps it would be better to say that I can better understand the adversarial tone given djb&#x27;s history with NIST recommendations (more info at https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Daniel_J._Bernstein#Cryptograp...).> The unfortunate reality of this is that while he may be right, it is difficult to classify the responses (or non-response) from the NIST people as deceptive vs just not wanting to engage with someone coming from such an adversarial position.Couldn&#x27;t agree with this more. I don&#x27;t like to harp on form over substance, but in this case the form of this blog post was so bad I had difficulty evaluating whether the substance was worthwhile. I&#x27;m not in the field of cryptography, so I&#x27;m not qualified to assess on the merits, but my thoughts reading this were:1. All the unnecessary snark and disparagement made me extremely wary of the message. It seemed like he was making good points, but the overall tone was similar to those YouTube \"WhaT ThE ElITe DoN&#x27;T WanT YoU TO KnoW!!\" videos. Frankly, the author just sounds like kind of an asshole, even if he is right.2. Did anyone actually read this whole thing?? I know people love to harp on \"the Internet has killed our attention spans\", and that may be true, but the flip side is we&#x27;re bombarded with so much info now that I take a very judicious approach to where I&#x27;ll spend my time. On that point, if you&#x27;re writing a blog post, the relevant details and \"executive summary\" if you will should be in the first couple paragraphs, then put the meandering, wandering diary after. Don&#x27;t expect a full read if important tidbits are hidden like Where&#x27;s Waldo in your meandering diary. reply aidenn0 4 hours agorootparentI read the whole thing because of who the author was.The executive summary is above the fold:Take a deep breath and relax. When cryptographers are analyzing the security of cryptographic systems, of course they don&#x27;t make stupid mistakes such as multiplying numbers that should have been added.If such an error somehow managed to appear, of course it would immediately be caught by the robust procedures that cryptographers follow to thoroughly review security analyses.Furthermore, in the context of standardization processes such as the NIST Post-Quantum Cryptography Standardization Project (NISTPQC), of course the review procedures are even more stringent.The only way for the security claims for modern cryptographic standards to turn out to fail would be because of some unpredictable new discovery revolutionizing the field.Oops, wait, maybe not. In 2022, NIST announced plans to standardize a particular cryptosystem, Kyber-512. As justification, NIST issued claims regarding the security level of Kyber-512. In 2023, NIST issued a draft standard for Kyber-512.NIST&#x27;s underlying calculation of the security level was a severe and indefensible miscalculation. NIST&#x27;s primary error is exposed in this blog post, and boils down to nonsensically multiplying two costs that should have been added.How did such a serious error slip past NIST&#x27;s review process? Do we dismiss this as an isolated incident? Or do we conclude that something is fundamentally broken in the procedures that NIST is following? reply lucb1e 9 hours agorootparentprev> I know people love to harp on \"the Internet has killed our attention spans\"Not just that. Give your parent or grandparent a 75-page booklet to read, full of accusations and snark, and let&#x27;s say it&#x27;s about something they care about and actually impacts their lives (maybe a local government agency, idk). What are the odds they are going to read that A-Z versus waiting for a summary or call-to-action to be put out? The latter can be expected to happen if there is actually something worthwhile in there.This is objectively too long for casual reading, nothing to do with anyone&#x27;s attention span.(The 75-page estimate is based on: (1) a proficient reader doing about a page per minute in most books that I know of, so pages==minutes; (2) the submission being 17.6k words; (3) average reading speed is ~250 wpm, resulting in 17.6e3&#x2F;250=70 minutes; (4) this is not an easy text, it has lots of acronyms and numbers, so conservatively pad to 75.) reply viraptor 8 hours agorootparentprevEven worse, I expected to find a part when he reports it and includes the responses&#x2F;follow-up from that... But this is the first time it&#x27;s published a far as I understand? Did I miss it in the wall of text? Or is it really a huge initial writeup that may end up with someone responding \"oh, we did mess up, didn&#x27;t we? Let&#x27;s think how to deal with that.\" reply aidenn0 3 hours agorootparentIt&#x27;s in there.He first raised the issue in April 2022.Then in December 2022 he asked about the evaluation of Kyber&#x27;s security and they posted this[1], which included a 2^40 multiple that he wasn&#x27;t sure where it came from; if it came from where he thought it did (bogus math on numbers from a paper DJB himself coauthored), then that was troubling.There was no response, so a few weeks later he posted his assumptions and asked if anyone else could come up with another possible explanation for what the NIST e-mail was assuming.This did get a response[2], the main thrust of which was:> While reviewers are free, as a fun exercise, to attempt to \"disprove what NIST _appears_ to be claiming about the security margin,\" the results of this exercise would not be particularly useful to the standardization process. NIST&#x27;s prior assertions and their interpretation are not relevant to the question of whether people believe that it is a good idea to standardize Kyber512.After further prodding the response[3] was essentially a rather polite version of \"You&#x27;re the scientist and it&#x27;s your model, why don&#x27;t you tell us?\" Which DJB considers dodging his question of \"How did you get these numbers?\"At this point DJB posts[4] a dissection of the December 2022 e-mail, which is similar to the middle quarter of TFA.1: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu...2: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu...3: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu...4: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu... reply viraptor 3 hours agorootparentThank you. Now that&#x27;s a readable summary! reply cycomanic 3 hours agoparentprevThat&#x27;s pretty selective quoting of the issues. He even says himself that the waiting for the patent is one of the minor issues.The many questions he asks is why did they repeatedly change the evaluation criteria after the fact, presented results in a misleading ways, and made basic calculation errors (remember these guys are experts). All these in favor of one algorithm.Now to someone like me this points to the fact that they really wanted that algorithm to be the standard. If we add to that the fact that there was significantly more NSA involvement than indicated and that they did their best to hide this, leads me to be extremely skeptical of the standard. reply kurikuri 5 hours agoparentprev> If Kyber-512 is actually this risky, then it deserves to be communicated clearly.The statement djb seems to be making: It is not known if Kyber-512 is as cryptographically strong as AES-128 by the definitions provided by NIST.This is an issue because these algorithms will be embedded within hardware soon.> Besides the fact that nobody is deploying standalone PQ for some timeNow that an implementation has been chosen to be standardized, hardware vendors are likely to start designing blocks that can more efficiently compute the FIPS 203 standard (if they haven&#x27;t already designed a few to begin with).Given that the standard&#x27;s expected publication is in 2024, and the 1-2 year review timeline for NIST CMVP review on FIPS modules, I wouldn&#x27;t be surprised to see a FIPS 140-3 Hardware Module with ML-KEM (Kyber-etc.) by mid 2026.> a succinct breakdown of whyThe issue seems to be his statement from [1]: \"However, NIST didn&#x27;t give any clear end-to-end statements that Kyber-512 has N bits of security margin in scenario X for clearly specified (N,X).\"djb succinctly outlines the \"scenario X\" he referred to in [2], in which he only needs a yes or no answer. He is literally asking the people who should know and be able to discuss the matter, who would have the technical background to discuss this matter. He had received no response, which is why he had posted [1].NIST&#x27;s reply in [3] is a dismissal of [1] without a discussion of the security itself. The frustrating part for me to read was the second paragraph: \"The email you cited (https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu...), speaks for itself. NIST continues to be interested in people&#x27;s opinions on whether or not our current plan to standardize Kyber512 is a good one. While reviewers are free, as a fun exercise, to attempt to \"disprove what NIST _appears_ to be claiming about the security margin,\" the results of this exercise would not be particularly useful to the standardization process. NIST&#x27;s prior assertions and their interpretation are not relevant to the question of whether people believe that it is a good idea to standardize Kyber512.\"If NIST views the reviewers&#x27; claims about security to be \"not particularly useful to the standardization process,\" (and remember: the reviewers are themselves cryptographers) then why should the public trust the standard at all?> a smoking gun or two would be greatThere wouldn&#x27;t be a smoking gun because the lack of clarification is the issue at hand. If they could explain how they calculated the security strength of Kyber-512, then this would be a different issue.The current 3rd party estimates of Kyber-512&#x27;s security strength (which is a nebulous term...) puts it below the original requirements, so clarification or justification seems necessary.[1]: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu...[2]: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu...[3]: https:&#x2F;&#x2F;groups.google.com&#x2F;a&#x2F;list.nist.gov&#x2F;g&#x2F;pqc-forum&#x2F;c&#x2F;4MBu... reply codeflo 12 hours agoprevThat&#x27;s more of a diary than an article -- jargony, disorganized, running in circles, very hard to follow. But the information might be important regardless. There&#x27;s a strong implication that NIST with help of the NSA intentionally standardized on a weak algorithm.We all know that&#x27;s possible.But can someone who follows some of this stuff more closely explain what the play would be? I always assumed that weakening public cryptography in such a way is a risky bet, because you can&#x27;t be sure that an attacker doesn&#x27;t independently find out what you know. You can keep a secret backdoor key (that was the accusation when they released Dual_EC_DRBG), but you can&#x27;t really hide mathematical results.Why would they be willing to risk that here? reply Exoristos 11 hours agoparentYou&#x27;re making an assumption that the NSA cares about the efficacy of cryptography for other people. Why would they care about that? reply aleph_minus_one 9 hours agorootparent> You&#x27;re making an assumption that the NSA cares about the efficacy of cryptography for other people. Why would they care about that?Hypothesis 1: because the NSA sees evidence that more efficient cryptographic algorithms are easier to crack for them.To give some weak evidence for this: if you need brute force to crack the cipher (or hash function), a more efficient algorithm need less computation power to crack.Hypothesis 2: A more efficient algorithm is likely to become applied in more areas than a less efficient one (think of smartcards or microcontrollers). So if the NSA finds a weakness or is capable of introducing a backdoor in it, it can decrypt a lot more data from more areas. reply giantrobot 10 hours agorootparentprevBecause the NSA has equally well funded adversaries that would love to find a back door to the NIST standards the whole of the US government uses. Even if the highest levels of the military and government use secret squirrel super cryptography the rest is using NIST standards. It&#x27;s all the boring parts of government that deposits paychecks and runs the badge readers to their offices. reply affinepplan 8 hours agorootparentprevit&#x27;s in the national security interest of the United States to have its industries use high-quality cryptosee: colonial oil pipeline hack reply defrost 8 hours agorootparentIt&#x27;s in the national security interest of the United States to have its industries use robust security practices.Industries with secure fences that are regularly patrolled are entirely different to industries with partial coverage by unpatrolled rusty fences and a freestanding door frame that has a titanium unpickable lock.Passwords get compromised that&#x27;s a fact.How the single employee password that got breached was obtained is still (AFAIK) a mystery - but this will always happen ... given many employess, at least one will eventually make a mistake.After that, the VPN had no multifactor authentication, the network had no internal honey subnets, canary accounts, sanity checks, etc.High-quality crypto alone does not make for secure systems.And systems can be secure with lower quality crypto if the systems are robust. reply technion 6 hours agorootparentprevI feel that examples argues the opposite.It&#x27;s not entirely known how every step of that attack went down, but \"breaking low quality crypto\" hasn&#x27;t factored into any incident write up I&#x27;ve ever seen.However, nearly all ransomware uses rsa. Therefore in this particular case, high quality crypto caused harm.(To state the obvious, I&#x27;m not advocating for bad crypto, just discussing this case). reply aaomidi 12 hours agoparentprev> Why would they be willing to risk that here?Certain types of attacks basically make it so you need to have a specific private key to act as a backdoor. That&#x27;s the current guess on what may be happening with the NIST ECC curves.If so, this can be effectively a US-only backdoor for a long, long time. reply tptacek 12 hours agorootparentI don&#x27;t believe that is anybody&#x27;s guess on what may be happening with the NIST ECC curves. Ordinarily, when people on HN say things like this, they&#x27;re confusing Dual EC, a public key random number generator, known to be backdoored, with the NIST curve standards. reply dfox 11 hours agorootparentThe issue with the NIST curves is that they were generated from a PRNG with some kind of completely random seed. The conspiracy theory there is that the seed was selected such as to make the curve exploitable for NSA and NSA only. Choosing such a seed is somewhat harder than complete break of the hash function (IIRC SHA-2) used in the PRNG that was used to derive the curve.On the other hand, there is a lot of reasons to use elliptic curve that was intentionally designed, so, DJB&#x27;s designs. And well, in 2009 I would not imagine that the kinds of stuff that DJB publishes will end up being TLS1.3. reply tptacek 11 hours agorootparentIt&#x27;s very unlikely the seeds were random, and they weren&#x27;t even ostensibly generated from a PRNG, as I understand it. Rather, they were passed through SHA1 (remember: this is the 1990s), as a means to destroy any possible structure in the original seed. The actual seeds themselves aren&#x27;t my story to tell, but are a story that other people are talking about. For my part, I&#x27;ll just point again to Koblitz and Menenzes on the actual cryptographic problems with the NIST P-curve seed conspiracy:https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2015&#x2F;1018.pdf reply f33d5173 11 hours agorootparentA hash function is a (CS)PRNG. It has the key property, namely of being indistinguishable from randomness while being generated deterministically. reply tptacek 11 hours agorootparentIn fact, `echo \"This is my seed\"openssl sha -sha256` is not really a CSPRNG. Hash functions are the bases of many PRNGs. But I think you&#x27;re abusing an ambiguity with the word \"random\" here. At any rate: we should be clear now on the point being made about the P-curve seeds. reply Ar-Curunir 9 hours agorootparentprevThat is not true. There is no such requirement for a hash function. reply ethbr1 8 hours agorootparentThread is talking about cryptographic hash functions, given the context reply Ar-Curunir 4 hours agorootparentYes, they don’t output random looking things necessarily. For example a hash function could be collision resistant but not pre image resistant, or vice versa. There’s much more nuance in these definitions. replyaaomidi 12 hours agorootparentprevYeah I&#x27;ve noticed people mixing them up. They happened around the same time, so I can excuse it a bit.The problem with the NIST ECC curves are that we still do not know where the heck that seed came from and why that seed specifically. reply tptacek 12 hours agorootparentSee Koblitz and Menenzes:https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2015&#x2F;1018.pdf reply api 11 hours agorootparentprevAlso: if the NIST ECC curves actually are backdoored then why would the NSA need to try to push a backdoored random number generator? Just exploit the already-backdoored curves. reply PeterisP 11 hours agorootparentRedundancy, so if one backdoor is closed&#x2F;fixed&#x2F;avoided, you still have more. reply barsonme 7 hours agorootparentprevNo, it’s really not. Ask Neal Koblitz. reply manonthewall 11 hours agoparentprevIt&#x27;s all far too conspiratorial for me. Just show me the math as to why it&#x27;s broken, I don&#x27;t need a conspiratorial mind map drawing speculative lines between various topics. Do an appendix or two for that. reply Nine99 3 hours agorootparentThere&#x27;s nothing conspiratorial about the post, why not read the article? The math error is described in line 2, the actual error about two screens down, highlighted in red. reply perihelions 13 hours agoprevRelated thread from last year, with 443 comments:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32360533 (\"NSA, NIST, and post-quantum crypto: my second lawsuit against the US government (cr.yp.to)\") reply sneak 6 hours agoprev> Discovering the secret workings of NISTPQC. I filed a FOIA request \"NSA, NIST, and post-quantum cryptography\" in March 2022. NIST stonewalled, in violation of the law. Civil-rights firm Loevy & Loevy filed a lawsuit on my behalf.As much as I generally loathe djb personally, professionally he will always have my support as he’s been consistently willing to take the federal government to task in court. It brings me great joy to see he’s still at it. reply SoylentOrange 5 hours agoparentWhy do you dislike him personally? reply jcranmer 12 hours agoprevNotwithstanding DJB&#x27;s importance to cryptography, and the fact that I&#x27;m ignorant of a large number of details here, there was a point where he lost a lot of credibility with me.Specifically, when he gets to the graphs, he says \"NIST chose to deemphasize the bandwidth graph by using thinner red bars for it.\" That is just not proven by his evidence, and there is a very plausible explanation for it. The graph that has the thinner bars is a bar chart that has more data points than the other graph. Open up your favorite charting application, and observe the difference in a graph that has 12 data points versus one with 9... of course the one with 12 data points has thinner lines! At this point, it feels quite strongly to me that he is trying to interpret every action in the most malicious way possible.In the next bullet point, he complains that they&#x27;re not using a log scale for the graph... where everything is in the same order of magnitude. That doesn&#x27;t sound like a good use case for log scale, and I&#x27;m having a hard time trying to figure out why it might be justified in this case.Knowing that DJB was involved in NTRU, it&#x27;s a little hard to shake the feeling that a lot of this is DJB just being salty about losing the competition. reply ziddoap 12 hours agoparent>At this point, it feels quite strongly to me that he is trying to interpret every action in the most malicious way possible.Given the long and detailed history of various governments and government agencies purposefully attempting to limit the public from accessing strong cryptography, I tend to agree with the \"assume malice by default\" approach here. Assuming anything else, to me at least, seems pretty naive. reply djur 10 hours agorootparentThere&#x27;s a meaningful difference between assuming an actor is malicious or untrustworthy and going out of your way to provide the maximally malicious interpretation of each of their actions. As a matter of rhetoric, the latter tends to give the impression of a personal vendetta. reply mandevil 7 hours agorootparentprevEh, it goes both ways. Back in the 1970&#x27;s and 1980&#x27;s there was a whole lot of suspicion about changes that the NSA made to DES S-boxes with limited explanation- was it a backdoor in some way? Then in 1989 white hats \"discovered\" differential cryptography, and realized that the changes that were made to the algorithm actually protected it from a then-unknown (to the general public) cryptographic attack. Differential cryptography worked beautifully on some other popular cryptosystems of the era, e.g. the FEAL-4 cipher could be broken with just 8 plaintext examples, while DES offered protection up to 2^47 chosen plaintexts.The actual way that the NSA had tried to limit DES was to cap its key length at 48 bits, figuring that their advantage in computing power would let them brute force it when no one else could. (NIST compromised between the NSA&#x27;s desire for 48 and the rest of the world&#x27;s desire for 64, which was why DES had the always bizarre 56 bit key.) So sometimes they strengthen it, sometimes they weaken it, and so I&#x27;m not sure it appropriate to presume malice. reply deadbeeves 5 hours agorootparent>So sometimes they strengthen it, sometimes they weaken it, and so I&#x27;m not sure it appropriate to presume malice.If you had a dog that sometimes licked you and sometimes bit you, would you let it sleep with you?Neither NSA nor NIST can be trusted. They brought this on themselves. reply Ar-Curunir 9 hours agorootparentprevDJB has lost a ton of credibility already within the non-government cryptography community for his frankly unhinged rants on the PQC mailing list.If you read his posts there, it’s hard not to come away with the impression that he’s just upset his favourite scheme wasn’t chosen. reply ethbr1 8 hours agorootparentStare into randomness for long enough, and you&#x27;ll see something staring back. There&#x27;s a reason I didn&#x27;t go pure-math reply DangitBobby 11 hours agoparentprevIf you continue reading, you&#x27;ll find that they aren&#x27;t responding to requests for clarification on their hand-waving computations. Suspicion is definitely warranted. reply pbsd 10 hours agoparentprevFWIW, there are two NTRUs: the original one, which had no djb involvement, and NTRU Prime, which does. reply kevincox 12 minutes agorootparentYeah. It does honestly sound like he looked at the options and decided that this one was the best, then he started contributing. reply aaomidi 12 hours agoparentprev> Knowing that DJB was involved in NTRU, it&#x27;s a little hard to shake the feeling that a lot of this is DJB just being salty about losing the competition.There isn&#x27;t a lot of people in the world with the technical know-how for cryptography. It&#x27;s clear that competitors in this space are going to be reviewing eachothers work. reply tptacek 12 hours agorootparentYes, that was the premise of the competition, and was in fact what happened. reply pnpnp 12 hours agorootparentprevSure, but this was just a weird thing to hone in on. reply neonate 12 hours agoprevhttp:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231003195013&#x2F;https:&#x2F;&#x2F;blog.cr.yp...https:&#x2F;&#x2F;archive.ph&#x2F;NrOG6 JoachimS 3 hours agoprevMy takeaway (impression) from the DJB post is that the evaluation by the NISTPQC seems not to provide algorithms with a firm level of security. That the evaluation is not clear cut, and not provide a good, conservative lower bound for the security provided by the algorithms selected. reply loondri 10 hours agoprevThe NIST standardization process, appears to have a grey area particularly around the selection of constants.The skepticism around standardization, advocating instead for direct adoption from cryptographers, sheds light on potential shortcomings in the current system.There is definitely a need for a more transparent or open scrutiny in algorithm standardization to ensure security objectives are met. reply 0xDEAFBEAD 4 hours agoprevAssuming djb is correct and the current process is broken... is trying to expose it and then fix it through FOIA requests really the best approach?If your codebase is hairy enough, and the problem to be solved is fundamentally fairly simple, sometimes it&#x27;s better to rewrite than refactor. Doubly so if you believe a clever adversary has attempted to insert a subtle backdoor or bugdoor.What would a better crypto selection process look like? I like the idea of incorporating \"skin in the game\" somehow... for example, the cryptographer who designs the scheme could wager some cash that it won&#x27;t be broken within a particular timeframe. Perhaps a philanthropist could offer a large cash prize to anyone who&#x27;s able to break the winning algorithm. Etc. reply capitainenemo 3 hours agoprevMinor typo. \"How can NIST justify throwing NIST-509 away?\" should be \"How can NIST justify throwing NTRU-509 away?\" reply 1vuio0pswjnm7 4 hours agoprev\"Security is supposed to be job #1. So I recommend eliminating Kyber-512.\" reply greggsy 11 hours agoprevIt would be interesting to see Signal Sciences response to this Bernstein’s post reply barsonme 6 hours agoparentWho is Signal Sciences? reply greggsy 3 hours agorootparentActually, I meant Open Whisper, the company behind Signal.Got my wires crossed. reply 10g1k 9 hours agoprevRelated note: Government employees (including military, intel) are just people, and worse, bureaucrats. They aren&#x27;t magical wizards who can all do amazing things with mathematics and witchcraft. If they were good at what they do, they wouldn&#x27;t need ever increasing funding and projects to fix things. reply ethbr1 8 hours agoparentCryptanalysis and encryption are somewhat of an exception to this. There are some extremely smart people who work in these areas for the government, precisely because funding and application is on a different scale. reply lazide 5 hours agorootparentVery few folks except the gov’t have real existential need for best in breed crypto, frankly. reply jeffrallen 13 hours agoprevSomething I&#x27;ve learned from a career of watching cryptographer flame wars: Don&#x27;t bet against Bernstein, and don&#x27;t trust NIST. reply fefe23 12 hours agoprevIf you have never heard of Bernstein, this may look like mad ramblings of a proto-Unabomber railing against THE MAN trying to oppress us.However, this man is one of the foremost cryptographers in the world, he has basically single-handedly killed US government crypto export restrictions back in the days, and (not least of all because of Snowden) we know that the NSA really is trying to sabotage cryptography.Also, he basically founded the field of post-quantum cryptography.Is NIST trying to derail his work by standardizing crappy algorithms with the help of the NSA? Who knows. But to me it does smell like that.Bernstein has a history of being right, and NIST and the NSA have a history of sabotaging cryptographic standards (google Dual_EC_DRBG if you don&#x27;t know the story). reply zahllos 12 hours agoparentThis comment is factually incorrect on a number of levels.1) single-handedly killed US government crypto export restrictions - Bernstein certainly litigated, but was not the sole actor in this fight. For example, Phil Zimmerman, the author of PGP, published the source code of PGP as a book to work around US export laws, which undoubtedly helped highlight the futility of labelling open source software as a munition: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pretty_Good_Privacy#Criminal_i...2) Bernstein \"founded\" the field of post quantum cryptography: Uh. Ok. That&#x27;s not how academia works. Bernstein was certainly an organiser of the first international workshop on post quantum cryptography, but that&#x27;s not the same as inventing a field. Many of the primitives that are now candidates were being published long before this, McEliece being one of the oldest, but even Atjai&#x27;s lattice reductions go back to &#x27;97.3) The dual_ec rng was backdoored (previously read was and is fishy, poor wording on my part), but nobody at the time wanted NIST to standardize it because it was a _poor PRNG anyway_: slow and unnecessarily complicated. Here is a patent from Scott Vanstone on using DUAL_EC for \"key escrow\" which is another way of saying \"backdoor\": https:&#x2F;&#x2F;patentimages.storage.googleapis.com&#x2F;32&#x2F;9b&#x2F;73&#x2F;fe5401e... - filed in 2006. In case you don&#x27;t know Scott Vanstone, he&#x27;s the founder of Certicom. So at least one person noticed. This was mentioned in a blog post as a result of the Snowden leaks working out how the backdoor happened: https:&#x2F;&#x2F;blog.0xbadc0de.be&#x2F;archives&#x2F;155NSA have been caught in a poor attempt to sabotage a standard that nobody with half a brain would use. On the other hand NSA also designed SHA-2, which you are likely using right now, and I&#x27;m not aware of anyone with major concerns about it. When I say NSA designed it, I don&#x27;t mean \"input for a crypto competition\" - a team from the NSA literally designed it and NIST standardized it, which is not the case for SHA-3, AES or the current PQC process.DJB is a good cryptographer, better than me for sure. But he&#x27;s not the only one - and some very smart, non-NSA, non-US-citizen cryptographers were involved in the design of Kyber, Dilithium, Falcon etc. reply tptacek 11 hours agorootparentDual EC is virtually certain to be a backdoor.I had the same take on Dual EC prior to Snowden. The big revelation with Snowden wasn&#x27;t NSA involvement in Dual EC, but rather that (1) NSA had intervened to get Dual EC defaulted-on in RSA&#x27;s BSAFE library, which was in the late 1990s the commercial standard for public key crypto, and (2) that major vendors of networking equipment were --- in defiance of all reason --- using BSAFE rather than vetted open-source cryptography libraries.DJB probably did invent the term \"post-quantum cryptography\". For whatever that&#x27;s worth. reply zahllos 11 hours agorootparentDualEC: agree. Wanted to point out that it was a poor PRNG _anyway_ and point out that the NSA&#x27;s attempt at backdooring the RNG wasn&#x27;t that great - as you say, RSA BSAFE used it and it made no sense. We could also point out they went after the RNG rather than the algorithm directly, which is a less obvious strategy.I&#x27;ll believe he invented the term - I have a 2009 book so-named for which he was an editor surveying non-DLP&#x2F;non-RSA algorithms. Still, the idea that he&#x27;s \"the only one who can produce the good algorithms\" and literally everyone else on the pqc list (even if we subtract all the NIST people) is wrong is bonkers. reply ziddoap 11 hours agorootparentWhile I agree with a lot of what you have said,>Still, the idea that he&#x27;s \"the only one who can produce the good algorithms\"The parent post did not, at all, make the claim that Bernstein is the only one. reply zahllos 9 hours agorootparentNo, true, the post did not explicitly state this. However the post did suggest that NIST is specifically out to get him and take a swipe at the other candidates:> Is NIST trying to derail his work by standardizing crappy algorithms with the help of the NSA? Who knows. But to me it does smell like that.\"Crappy\" algorithms that were designed by well-regarded cryptographers, none of whom work for NIST or the NSA, many of whom are not US nationals. reply cycomanic 2 hours agorootparentThe evidence seems to at least point to NIST trying to get selected one specific algorithm selected.How else do you explain the after-the-fact changing of evaluation criteria (all favoring one algorithm) and the weird calculation error (which as I understand the text didn&#x27;t come from the Kyber designers but the evaluation committee)?Add to that the lack of transparency in particular why not follow the FOI requests ? and the much more significant involvement of NSA employees in the process (contrary to their own statement). Shouldn&#x27;t that make everyone very suspicious? reply pigeonhole123 6 hours agorootparentprevBeing incompetent is possible even if you’re not a US national. replykpdemetriou 11 hours agoparentprevBernstein is often right, despite the controversy around the Gimli permutation.In this particular case it&#x27;s worth noting that neither BSI (Germany) nor NLNCSA (The Netherlands) recommend Kyber.Unfortunately, alternative algorithms are more difficult to work with due to their large key sizes among other factors, but it&#x27;s a price worth paying. At Backbone we&#x27;ve opted not to go down the easy route. reply throw0101a 11 hours agoparentprev> If you have never heard of Bernstein, this may look like mad ramblings of a proto-Unabomber railing against THE MAN trying to oppress us.> However, this man is one of the foremost cryptographers in the world […]It&#x27;s possible to be both (not saying Bernstein is).Plenty of smart folks have &#x27;jumped the shark&#x27; intellectually: Ted Kaczynski, the Unabomber, was very talented in mathematics before he went off the deep end. reply akira2501 9 hours agorootparent> Plenty of smart folks have &#x27;jumped the shark&#x27; intellectually: Ted Kaczynski, the Unabomber, was very talented in mathematics before he went off the deep end.Kaczynski dropped out of society to live in a cabin alone at 29. He delivered his first bomb at 35. I&#x27;m not sure this is a reasonable comparison to invoke in any way whatsoever.When DJB starts posting about the downfall of modern society from his remote cabin in Montana, perhaps, but as far as I know he&#x27;s still an active professor working from within the University system. reply lazide 5 hours agorootparentWhile kaczynski was clearly unhinged, and I frankly don’t see how sending mail bombs did anything helpful towards solving the problems he addressed (or that his proposed solution would necessarily be better than ‘the disease’), I dare anyone to read his manifesto and say he was wrong.If DJB is unhinged but similarly insightful about a crypto algo, I think we’d all be better off. Assuming he lays off the mailbombs anyway. reply pigeonhole123 6 hours agorootparentprevThere was a smart guy once who went crazy. We should assume smart people are crazy. reply apendleton 5 hours agorootparentThat&#x27;s not the claim. The claim is \"because we know smart people have gone crazy, we know being smart and being crazy are not mutually exclusive, so someone being smart isn&#x27;t disqualified from also being crazy.\" Which seems obviously true. reply pigeonhole123 4 hours agorootparentAnd not useful reply lazide 10 hours agorootparentprevDue to likely CIA sponsored mental abuse (MKULTRA), absurdly. reply zdw 9 hours agoparentprevThis also skips his pioneering work into microservice architecture, as exemplified by the structure of qmail, djbdns, and daemontools. reply ignoramous 12 hours agoparentprevAn interesting set of comments (by tptacek) from a thread in 2022 (I wonder if they still hold the same opinion in light of this latest post on NIST-PQC by djb):> The point isn&#x27;t that NIST is trustworthy. The point is that the PQC finalist teams are comprised of academic cryptographers from around the world with unimpeachable reputations, and it&#x27;s ludicrous to suggest that NSA could have compromised them. The whole point of the competition structure is that you don&#x27;t simply have to trust NIST; the competitors (and cryptographers who aren&#x27;t even entrants in the contest) are peer reviewing each other, and NIST is refereeing.> What Bernstein is counting on here is that his cheering section doesn&#x27;t know the names of any cryptographers besides \"djb\", Bruce Schneier, and maybe, just maybe, Joan Daemen. If they knew anything about who the PQC team members were, they&#x27;d shoot milk out their nose at the suggestion that NSA had suborned backdoors from them. What&#x27;s upsetting is that he knows this, and he knows you don&#x27;t know this, and he&#x27;s exploiting that.---> I spent almost 2 decades as a Daniel Bernstein ultra-fan --- he&#x27;s a hometown hero, and also someone whose work was extremely important to me professionally in the 1990s, and, to me at least, he has always been kind and cheerful... I know what it&#x27;s like to be in the situation of (a) deeply admiring Bernstein and (b) only really paying attention to one cryptographer in the world (Bernstein).> But talk to a bunch of other cryptographers --- and, also, learn about the work a lot of other cryptographers are doing --- and you&#x27;re going to hear stories. I&#x27;m not going to say Bernstein has a bad reputation; for one thing, I&#x27;m not qualified to say that, and for another I don&#x27;t think \"bad\" is the right word. So I&#x27;ll put it this way: Bernstein has a fucked up reputation in his field. I am not at all happy to say that, but it&#x27;s true.---> What&#x27;s annoying is that [Bernstein is] usually right, and sometimes even right in important new ways. But he runs the ball way past the end zone. Almost everybody in the field agrees with the core things he&#x27;s saying, but almost nobody wants to get on board with his wild-eyed theories of how the suboptimal status quo is actually a product of the Lizard People.(https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32365259, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32368598, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32365679) reply mort96 12 hours agorootparentI don&#x27;t think the \"these finalist teams are trustworthy\" argument is completely watertight. If the US wanted to make the world completely trust and embrace subtly-broken cryptography, a pretty solid way to do that would be to make competition where a whole bunch of great, independent teams of cryptography researchers can submit their algorithms, then have a team of excellent NSA cryptographers analyze them and pick an algorithm with a subtle flaw that others haven&#x27;t discovered. Alternatively, NIST or the NSA would just to plant one person on one of the teams, and I&#x27;m sure they could figure out some clever way to subtly break their team&#x27;s algorithm in a way that&#x27;s really hard to notice. With the first option, no participant in the competition has to that there&#x27;s any foul play. In the second, only a single participant has to know.Of course I&#x27;m not saying that either of those things happened, nor that they would be easy to accomplish. Hell, maybe they&#x27;re literally impossible and I just don&#x27;t understand enough cryptography to know why. Maybe the NIST truly has our best interest at heart this time. I&#x27;m just saying that, to me, it doesn&#x27;t seem impossible for the NIST to ensure that the winner of their cryptography contests is an algorithm that&#x27;s subtly broken. And given that there&#x27;s even a slight possibility, maybe distrusting the NIST recommendations isn&#x27;t a bad idea. They do after all have a history of trying to make the world adopt subtly broken cryptography. reply tptacek 12 hours agorootparentIf the NSA has back-pocketed exploits on the LWE submission from the CRYSTALS authors, it&#x27;s not likely that a purely academic competition would have fared better. The CRYSTALS authors are extraordinarily well-regarded. This is quite a bank-shot theory of OPSEC from NSA. reply mort96 11 hours agorootparentIt&#x27;s true that nothing is 100% safe. And to some degree, that makes the argument problematic; regardless of what happened, one could construct a way for US government to mess with things. If you had competition of the world&#x27;s leading academic cryptographers with a winner selected by popular vote among peers, how do you know that the US hasn&#x27;t just influenced enough cryptographers to push a subtly broken algorithm?But we must also recognize a difference in degree. In a competition where the US has no official influence over the result, there has to be a huge conspiracy to affect which algorithm is chosen. But in the competition which actually happened, they may potentially just need a single plant on one of the strong teams, and if that plant is successful in introducing subtle brokenness into the algorithm without anyone noticing, the NIST can just declare that team&#x27;s algorithm as the winner.I think it&#x27;s perfectly reasonable to dismiss this possibility. I also think it&#x27;s reasonable to recognize the extreme untrustworthiness of the NIST and decide to not trust them if there&#x27;s even a conceivable way that they might&#x27;ve messed with the outcome of their competition. I really can&#x27;t know what the right choice is. reply tptacek 11 hours agorootparentThat&#x27;s an argument that would prove too much. If you believe NSA can corrupt academic cryptographers, then you might as well give up on all of cryptography; whatever construction you settle on as trustworthy, they could have sabotaged through the authors. Who&#x27;s to say they didn&#x27;t do that to Bernstein directly? If I&#x27;d been suborned by NSA, I&#x27;d be writing posts like this too! reply mort96 11 hours agorootparentYou&#x27;re still not recognizing the difference between corrupting a single academic cryptographer and corrupting a whole bunch of academic cryptographers. This isn&#x27;t so black and white.For what it&#x27;s worth, I do think the US government could corrupt academic cryptographers. If I was an academic cryptographer, and someone from the US government told me to do something immoral or else they would, say, kill my family, and they gave me reason to believe the threat was genuine, I&#x27;m not so sure I wouldn&#x27;t have done what they told me. And I know this sounds like spy movie shit, but this is the US government.One last thing though, if you&#x27;re giving me the black and white choice between blindly trusting the outcome of a US government cryptography standard competition or distrusting the field of cryptography altogether, I choose the latter. reply tptacek 11 hours agorootparentAs long as we&#x27;re clear that your concern involves spy movie shit, and not mathematics or computer science, I&#x27;m pretty comfortable with where we&#x27;ve landed. reply mort96 11 hours agorootparentIf your argument is: “assuming the US government wouldn’t be able to make someone act against their will and stay silent about it, the NIST recommendation is trustworthy”, I’m certainly more inclined to distrust this recommendation than I was before this conversation.Note that the “forcing someone to comply” thing was just meant as one possibility among many, I don’t see why you completely dismiss the idea of someone who’s good at cryptography being in on the US’s mission to intercept people’s communications. I mean the NSA seems to be full of those kinds of people. You also dismiss the possibility that they just … picked the algorithm that they thought they could break after analysing it, with no participant being in on anything. But I get the feeling that you’re not really interested in engaging with this topic anymore, so I’ll leave it at that. It’s already late here. reply acer4666 8 hours agorootparentprevWhy would you use mathematics or computer science to ascertain whether someone has been corrupted by a government agency? reply thadt 9 hours agorootparentprevIt&#x27;s an interesting thought, but then you would need those cryptographers to not only stay quiet about it, but also spend a good chunk of the next part of their lives selling the lie.Secrets are hard to keep at scale. Trying to do it with coercion, to a group of people who&#x27;s entire field of study is covert communication, seems like an unenviable prospect. reply pigeonhole123 6 hours agorootparentprevThis of course means we should ignore reasonable criticism of the contestants in this contest. reply tptacek 6 hours agorootparentNo, it doesn&#x27;t. reply tptacek 12 hours agorootparentprevI hope he finds all sorts of crazy documents from his FOIA thing. FOIA lawsuits are a very normal part of the process (I&#x27;ve had the same lawyers pry loose stuff from my local municipality). I would bet real money against the prospect of him finding anything that shakes the confidence of practicing cryptography engineers in these standards. Many of the CRYSTALS team members are quite well regarded. reply NovemberWhiskey 12 hours agoparentprev>If you have never heard of Bernstein, this may look like mad ramblings of a proto-Unabomber railing against THE MAN trying to oppress us.Can I point out that Ted Kaczynski was also actually a mathematical prodigy, having been accepted into Harvard on a scholarship at 16? reply skeaker 12 hours agorootparentIf you want, sure, but I think the reason he was mentioned with a negative connotation might be more to do with the murders he committed. reply Ar-Curunir 9 hours agoparentprevBernstein did not “found” the field of PQC. He wasn’t even doing cryptography when this field was founded!Also, the schemes he’s railing against are also the work of top cryptographers in the space. reply nonrandomstring 12 hours agoprevLove the narrative style of this writing second-guessing the erroneous thought processes. Are they deceptive? Who knows.What worries me is that it&#x27;s neither malice nor incompetence, but that a new darker force has entered our world even at those tables with the highest stakes.... dispassion and indifference.It&#x27;s hard to get good people these days. A lot of people stopped caring. Even amongst the young and eager. Whether it&#x27;s climate change, the world economic situation, declining education, post pandemic brain-fog, defeat in the face of AI, chemicals in the water.... everywhere I sense a shrug of slacking off, lying low, soft quitting, and generally fewer fucks are given all round.Maybe that&#x27;s just my own fatigue, but in security we have to vigilant all the time and there&#x27;s only so much energy humans can bring to that. That&#x27;s why I worry that we will lose against AI. Not because it&#x27;s smarter, but because it doesn&#x27;t have to _care_, whereas we do. reply r3trohack3r 12 hours agoparentBad systems beat good people.There are a lot of symptoms to distract yourself with. Focus on the game instead.A society full of good people will sort out the rest. reply bdamm 11 hours agorootparentThis apathy is an interesting phenomenon, let&#x27;s not ignore it. The Internet has brought us a wealth of knowledge but it has also shown us how truly chaotic the world really is. And negativity is a profitable way to drive engagement, so damn near everyone can see how problematic our society is. And when the algorithm finds something you care to be sad about, it will show you more, more, and ever more all the way into depression.This is the lasting legacy of the Internet, now. Not freedom for all to seek and learn, but freedom for the negativity engines to seek out your brain and suck you into personal obliteration.A society of good people? Nobody really cares any more. And I do agree with the gp; if you look, you can see it everywhere. What is this going to become? Collective helplessness as we eek out what little bits of personal fulfillment we can get in between endless tragedy and tantalizing promise? reply lobochrome 10 hours agoparentprevThe car is on fire and there is no driver at the wheel. reply nonrandomstring 10 hours agorootparentBollocks to the car. It&#x27;s the rest of us innocent pedestrians who need to take cover. :) reply nmitchko 13 hours agoprev [–] Unfortunately, the NSA & NIST most likely is recommending a quantum-proof security that they&#x27;ve developed cryptanalysis against, either through high q-bit proprietary technology or specialized de-latticing algorithms .The NSA is very good at math, so I&#x27;m be thoroughly surprised if this analysis was error by mistake rather than error through intent. reply jrochkind1 13 hours agoparentThe NSA also has a mission-based interest in _breaking_ other people&#x27;s crypto though, which is generally known.Which is generally known, so I&#x27;m surprised by your argument. Even if the NSA knows more than they are telling us, this doesn&#x27;t result in most of us feeling less worried, as their ends may not be strengthening the public&#x27;s cryptography! reply cosmojg 12 hours agorootparentIsn&#x27;t that what the person you&#x27;re replying to said? reply aaomidi 12 hours agorootparentprevYes: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dual_EC_DRBGAlso, we still to this day do not know where the seed for P256 and P384 came from. And we&#x27;re using that everywhere. There is a non-zero chance that the NSA basically has a backdoor for all NIST ECC curves, and no one actually seems to care. reply sweis 12 hours agorootparentNIST P-256 curve seed came from the X9.62 specification drafted in 1997. It was provided by an NSA employee, Jerry Solinas, as an example seed among many other seeds, including those provided by Certicom. Read this for more details: https:&#x2F;&#x2F;eprint.iacr.org&#x2F;2015&#x2F;1018 reply brohee 11 hours agorootparentprevOr you find it somewhat credible but still use them because fending off the NSA is not something you want to spend energy on, and you are confident in the fact that NSA think no one else can find the backdoor. reply garba_dlm 13 hours agorootparentprevI just find it sad that it&#x27;s things like these that make it impossible for the layman to figure out what is going on with, for example, Mochizuki&#x27;s new stuffI have no reason to doubt that a lot of math has been made more difficult than necessary just because it is known to give a subtle military advantage in some cases, but this isn&#x27;t new; reply sweis 12 hours agoparentprev\"High q-bit proprietary technology\" and \"specialized de-latticing algorithms\" are made up terms that nobody uses. reply tptacek 11 hours agorootparentI&#x27;m stuck on trying to work out what it would mean to de-lattice something. Would that transform a lattice basis into a standard vector space basis in R or something, or, like MOV, would it send the whole lattice to an element of some prime extension field?In my mind&#x27;s eye, it&#x27;s cooler: it&#x27;s like, you render the ciphertext as a raster image, and then \"de-lattice\" it to reveal the underlying plaintext, scanline by scanline. reply garba_dlm 11 hours agorootparenti&#x27;m still working on understanding lattices betterbut i can imagine, based on my own ignorance, creativity, and lack of correct understanding, would be some kind of factorization.as I think while trying to better know what&#x27;s a lattice, I imagine a lattice like a coordinate pair, but instead of each coordinate existing on a line, they exist on a binary tree (or some other directed graph explored from a root outwards without cycles)which means you have two such binary-trees (not necessarily binary, but it&#x27;s just easier to work with them seemingly)and then you combine these into ONE lattice. so then, to de-lattice means to recover the binary trees.but when I say binary tree I&#x27;m thinking about rational numbers (because stern broccott trees) reply tptacek 11 hours agorootparentA lattice is like a vector space, but with exclusively integer coefficients. It&#x27;s not a coordinate pair. If you think of vectors as coordinate pairs, a vector space is a (possibly unbounded) set of coordinate pairs. If you haven&#x27;t done any linear algebra, a decent intuition would be mathematical objects like \"the even numbers\" or \"the odd numbers\", but substituting vectors (fixed-sized tuples of numbers) for scalars. reply20after4 10 hours agorootparentprevJust bounce a graviton particle beam of the main deflector dish. reply tptacek 12 hours agoparentprev\"Specialized de-latticing algorithms\"? reply bsder 8 hours agoparentprev [–] > through high q-bit proprietary technologySomebody would leak or steal that as it would be a GIGANTIC leap forward in our engineering skill at the quantum level.Getting more than a handful of qubits to stay coherent and not collapse into noise is a huge research problem right now, and progress has been practically non-existent for almost a decade. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post raises concerns about the allegedly inaccurate calculations and lack of transparency in the evaluation of the Kyber-512 cryptosystem by NIST (National Institute of Standards and Technology).",
      "NIST's review process is questioned in favor of Kyber-512, involving the NSA (National Security Agency), leading to controversy and calls for clearer transparency and understanding.",
      "The author highlights the potential advantages of alternative encryption systems like NTRU and criticizes potential bias and alleged flaws in NIST's security assessments."
    ],
    "commentSummary": [
      "The central focus of discussions is on the credibility and trustworthiness of the National Institute of Standards and Technology (NIST) in selecting cryptographic algorithms.",
      "There are speculations about possible interference from the NSA, giving rise to potential backdoors or compromised algorithms.",
      "The necessity for transparency, clear communication, and independent evaluation in algorithm standardization is underlined, highlighting the importance of these aspects in maintaining trust."
    ],
    "points": 373,
    "commentCount": 154,
    "retryCount": 0,
    "time": 1696362587
  },
  {
    "id": 37752366,
    "title": "Pgroll: zero-downtime, reversible schema migrations for Postgres",
    "originLink": "https://xata.io/blog/pgroll-schema-migrations-postgres",
    "originBody": "Features About Roadmap Pricing Blog Careers Docs Sign in Start free Introducing pgroll: zero-downtime, reversible, schema migrations for Postgres We are excited to ship the first version of pgroll, a command line tool that offers safe and reversible schema migrations for PostgreSQL Written by Carlos Pérez-Aradros Herce Published on October 2, 2023 # Schema migrations are painful Database schema migrations can be a double-edged sword. They are essential for keeping our systems up to date and in sync with evolving application requirements, but often come bundled with a set of challenges that can leave even the most seasoned developers and database administrators scratching their heads (or banging them on the keyboard). Breaking changes: One of the fundamental issues plaguing schema migrations is the potential for breaking changes. Altering the database schema can have far-reaching consequences, causing disruptions and errors in applications that depend on it. Multiple steps: Database migrations are rarely a one-and-done affair. They often involve a series of intricate steps that need to be executed meticulously. Managing these multiple steps is usually not part of the team deployment workflow, and can quickly become a logistical nightmare (e.g. It can take 6 steps to rename a column without downtime)! Unexpected database locks: Traditional migration methods can cause unexpected database locks, bringing services to a grinding halt and causing application downtime. No easy rollbacks: In the world of schema migrations, the safety net is often full of holes. Rolling back to a previous state in the event of a migration gone awry is rarely straightforward and is frequently a risky endeavor. Due to these issues, many developers choose to avoid complex migrations and only make additive changes. This leads to the accumulation of technical debt in the database schema, such as orphaned columns or missing constraints. # Introducing pgroll At Xata, we use Postgres for our internal systems and to host our users' data. As a result, we need to perform migrations for both our internal development and from the Xata product itself. We believe evolving your Postgres schema can be a considerably better experience: Migrations should not entail risks Migrations should be easy to define, easy to execute Migrations should be part of the normal deployment workflow (continuous delivery) Migrations should be easily & quickly reversible Migrations should not require special orchestration This is why we created pgroll: https://github.com/xataio/pgroll pgroll is an open source command-line tool for performing schema migrations against Postgres databases. Designed on these principles, pgroll allows users to define schema migrations using a high-level JSON format and then takes care of executing them. These are some of the key features: Migrations are defined in a high-level JSON format: Simple definition, allowing for richer operations information on top of Postgres DDL statements (CREATE, ALTER, etc.) Keep two versions of the schema (previous and next) accessible at the same time during the whole migration process: Previous versions of the applications will still work while the migration is happening, taking risk and pressure away from the deployment process. Instant rollbacks: Since the previous version of the schema is kept alive, a rollback basically means canceling the migration; the previous schema never went away! Zero downtime: All operations are implemented to ensure that Postgres won’t lock data access to the table while the schema changes are happening. # How does pgroll work? pgroll uses the expand and contract pattern to evolve the database schema, automating its whole lifecycle behind an easy-to-use command line interface. Previous and new versions of the schema are made available as “virtual” schemas on top of the Postgres physical one. By leveraging table views pointing to the right columns, pgroll is able to expose new parts of the schema and hide the old parts before safely removing them after the migration is completed. pgroll multiple active versions, client applications rollout As discussed in a previous blog post, ensuring Postgres locks on tables (i.e. ACCESS EXCLUSIVE) while executing DDL statements don’t end up in data access blocking is possible. pgroll implements all migration operations using the right techniques to avoid this situation, so you don’t need to think about it. Backfilling data is also a big part of performing backwards-compatible schema changes. pgroll takes care of performing automatic backfills when they are needed, abstracting the problem away while keeping things transparent. Let’s show how pgroll works using an example. For instance, a typically complex migration would be to update a column to add a NOT NULL constraint to an existing column, while still allowing existing client applications to work without changes, providing time for devs to update them after the migration as part of their normal workflow. Other similar migrations that would typically result in breaking changes on the schema could be renaming a column, adding or removing constraints, setting UNIQUE... all of them supported by pgroll. pgroll virtual schemas during a migration This defines the migration for setting a column as NOT NULL using pgroll. Additional operations can be included in the same migration, but we'll focus on this one for simplicity: Copy { \"name\": \"review_not_null\", \"operations\": [ { \"alter_column\": { \"table\": \"reviews\", \"column\": \"review\", \"not_null\": true, \"up\": \"(SELECT CASE WHEN review IS NULL THEN product || ' is good' ELSE review END)\", \"down\": \"review\" } } ] } This is executed by running this simple command: Copy pgroll start review_not_null.json pgroll will perform all the necessary steps to make this change available without disrupting existing client applications (the expand step from the expand/contract pattern). In this particular example, it will: Create a new column as a copy of the review column that respects the NOT NULL constraint. Add the NOT NULL constraint to the new column, using the NOT VALID clause to ensure that data access doesn’t block due to this statement, avoiding unexpected downtime. Backfill all existing values from the old column into the new one, upgrading the data in the cases where the value was NULL, based on the user-defined function. Of course, backfill happens in batches, avoiding excessively large updates that could also block the database. Set up a trigger function so new inserts & updates get automatically copied between the new and the old columns. Keep the old column working at all times, ensuring the migration can be rolled back instantly if needed. Create a new view of the schema that hides the old column and promotes the new one in its place, exposing this way the new version of the schema while the old one keeps working. Typically, all of these steps would require manual execution to achieve a complex update like this. However, pgroll streamlines the process by performing all of them with a single command ✨. Once you are happy with the new version of the schema and all clients have been updated to use it, the old schema will no longer be accessed. It’s time to complete the migration (contract the schema) to get rid of the old column & triggers: Copy pgroll complete For more complex examples, there is a wide range of schema migration samples in our examples and docs. # What’s next? Today, we are rolling out the first version of pgroll and we are looking forward to your feedback! We plan to continue developing pgroll and exploring how it can make it easier and safer for Xata users to evolve their schema. If you have any suggestions or questions, please open an issue in our GitHub repo, reach out to us on Discord or follow us on X / Twitter. We'd love to hear from you and keep you up to date with the latest progress on pgroll. Let’s start rolling! Start free, pay as you grow Xata provides the best free plan in the industry. It is production ready by default and doesn't pause or cool-down. Take your time to build your business and upgrade when you're ready to scale. Start free Explore all plans FREE PLAN INCLUDES 10 database branches High availability Point-in-time recovery 15 GB data storage 15 GB search engine storage 2 GB file attachments 250 AI queries per month Copyright © 2023 Xatabase Inc. All rights reserved. PRODUCT Roadmap Feature requests Pricing Status AI solutions File attachments COMPANY About Careers Blog RSS CONTACT Email Support Discord Twitter GitHub LEGAL Cookies Security Terms Privacy policy",
    "commentLink": "https://news.ycombinator.com/item?id=37752366",
    "commentBody": "Pgroll: zero-downtime, reversible schema migrations for PostgresHacker NewspastloginPgroll: zero-downtime, reversible schema migrations for Postgres (xata.io) 303 points by ksec 19 hours ago| hidepastfavorite140 comments dboreham 17 hours agoFor those curious, as I was, how this works, beyond the details in the readme and blog post, note that \"schema\" in this context is both a loose term (we changed the schema, old schema, new schema) AND a concrete thing in PostgreSQL[0]. It&#x27;s helpful to know that pgroll implements the first one (let&#x27;s change the schema) using the second one (make Schema objects in PG [1]). The magic is in creating different views in different Schemas that map appropriately to underlying tables (in a third Schema).Presumably (didn&#x27;t see this mentioned in the docs yet) the trick is that you re-deploy the app with a new connection string referencing the new schema (as in [3]), while an old app deployment can keep referencing the old schema with its connection string.Hopefully I got that right.[0] https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;ddl-schemas.html [1] https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;blob&#x2F;main&#x2F;pkg&#x2F;roll&#x2F;execute.... [3] https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;67525360 reply surjection 16 hours agoparentYou&#x27;re right. I wish schema wasn&#x27;t such an overloaded term :)In order to access either the old or new version of the schema, applications should configure the Postgres `search_path`[0] which determines which schema and hence which views of the underlying tables they see.This is touched on in the documentation here[1], but could do with further expansion.[0] - https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;ddl-schemas.html#DDL... [1] - https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;blob&#x2F;main&#x2F;docs&#x2F;README.md#cl... reply dkubb 13 hours agorootparentI mentally change schema to namespace when thinking about the postgresql feature. reply guffins 13 hours agorootparentYou’re not alone. That’s also how PostgreSQL itself thinks about schemas! https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;catalog-pg-namespace... reply fabianlindfors 16 hours agoparentprevYou got it right! I wrote a blog post a few years back about how this technique works for anyone curious: https:&#x2F;&#x2F;fabianlindfors.se&#x2F;blog&#x2F;schema-migrations-in-postgres... reply candiddevmike 18 hours agoprevHelp me understand the value of undoable migrations. I&#x27;ve always operated under \"the only path is forward\" and you release a new version that fixes the issue or create a new migration that does some kind of partial rollback if necessary. Once a migration is live, naively rolling things back seems like you&#x27;re asking for problems.I also only perform migrations as part of app init, not separately. reply exekias 18 hours agoparentI believe this is one of the reasons why migrations become scary in many cases. If something goes wrong \"the only path is forward\". Also, rolling out new versions of the application means either breaking the previous versions (with some instances still running) or doing the migration in several steps.We believe there is a better way, they way pgroll works, you can start a migration, and keep the old & new schemas working for as long as you need to rollout your app. If the new version of the app&#x2F;schema doesn&#x27;t behave as you were expecting, you only need to rollback the commit and undo the migration. pgroll guarantees that the previous version is still working during the whole process.There is a graph in the readme depicting this concept:https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;blob&#x2F;main&#x2F;docs&#x2F;img&#x2F;migratio... reply pcthrowaway 13 hours agorootparentI recently had to do a migration on a timescale hypertable where a \"schema\" was migrated for a table which had jsonb columns containing arrays of arrays of numbers to a new table containing the same data as two-dimensional postgres arrays of numeric[][] data (better storage characteristics)Our workflow was something like:1) Create the new hypertable2) Create after insert trigger on first table to insert transformed data from first table into second table, and delete from first table (this ensured applications can continue running using first schema&#x2F;table, without any new data being added to first table after migration)3) Iterate over first table in time-bucketed batches using a plpgsql block to move chunks of data from first table to second table.Would pgroll enable a similar workflow? I guess I&#x27;m curious if the way pgroll works would similarly create a trigger to allow apps to work with the initial schema as a stopgap... I guess pgroll would perform the whole migration as a series of column updates on a single table, but I&#x27;m unclear on whether it attempts to migrate all data in one step (potentially locking the table for longer periods?) while also allowing applications using the old schema to continue working so there is no downtime as changes are rolled out.Has pgroll been tested with timescaledb at all? reply exekias 13 hours agorootparentTo do this with pgroll I would use an alter_column migration, changing the type: https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;tree&#x2F;main&#x2F;docs#change-type, this would:1) Create a new column with the desired type (numeric[][] in your case) 2) Backfill it from the original one, executing the up function to do the casting and any required transformation 3) Install a trigger to execute the up function for every new insert&#x2F;update happening in the old schema version 4) After complete, remove the old column, as it&#x27;s no longer needed in the new version of the schemaBackfills are executed in batches, you can check how that works here: https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;blob&#x2F;main&#x2F;pkg&#x2F;migrations&#x2F;ba...I don&#x27;t think any of us has tested pgroll against timescaledb but I would love to know about the results if anyone does! reply vlovich123 12 hours agorootparentIs my understanding correct that the need to copy columns makes starting a migration potentially extremely expensive on a large database? reply surjection 12 hours agorootparentYes, for those pgroll migrations that require a new column + backfill, starting the migration can be expensive.Backfills are done in fixed size batches to avoid long lived row locks, but the operation can still be expensive in terms of time and potentially I&#x2F;O. Options to control the rate of backfilling could be a useful addition here but they aren&#x27;t present yet. reply claytonjy 11 hours agorootparentprevThis is almost exactly how I did a similar migration, also in Timescale. I used PL&#x2F;pgSQL and sqitch, did you use a migration tool? reply pcthrowaway 7 hours agorootparentNo, this was all done in handwritten .sql scripts. I don&#x27;t think it matters too much in this case, but we&#x27;re using Rust and the sqlx cli for driving the migrations, but that basically just runs the sql migration scripts reply dot5xdev 16 hours agorootparentprev> If the new version of the app&#x2F;schema doesn&#x27;t behave as you were expecting, you only need to rollback the commit and undo the migration.If I delete a \"last_name\" column, apply the migration, and then decide I shouldn&#x27;t have deleted users&#x27; last names. Do I get that data back? reply indigo945 16 hours agorootparentJust from my understanding from having read the linked website: yes, you do.\"Applying the migration\" doesn&#x27;t actually do anything to the table, it just creates a new schema containing views over the old one, where the view for the table whose column you deleted hides the column. You can then try if your app still works when using accessing this schema instead of the old one. If you&#x27;re happy, you can \"complete\" the migration, at which point only the table structure actually gets altered in a non-reversible way. reply exekias 15 hours agorootparent^ this is exactly how it works :) reply javaunsafe2019 15 hours agorootparentBut if it works like that aren’t there schema migration paths that are changing the actual content of a column and are then not undoable? reply surjection 13 hours agorootparentAny pgroll operations[0] that require a change to an existing column, such as adding a constraint, will create a new copy of the column and backfill it using &#x27;up&#x27; SQL defined in the migration and apply the change to that new column.There are no operations that will modify the data of an existing column in-place, as this would violate the invariant that the old schema must remain usable alongside the new one.[0] - https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;tree&#x2F;main&#x2F;docs#operations-r... reply darkwater 12 hours agorootparentMaybe this is explained somewhere in the docs but I&#x27;m lazy: how does it cope with possible performance issue in highly trafficked tables? Can you somehow control the backfill speed if it&#x27;s taking up too much I&#x2F;O? reply surjection 12 hours agorootparentThere&#x27;s nothing about this in the docs :)Backfills are done in fixed size batches to avoid taking long-lived row locks on many rows but there is nothing in place to control the overall rate of backfilling.This would certainly be a nice feature to add soon though. reply pcthrowaway 13 hours agorootparentprevAlso, if the data isn&#x27;t deleted couldn&#x27;t this lead to database bloat? reply surjection 13 hours agorootparentThe bloat incurred by the extra column is certainly present while the migration is in progress (ie after it&#x27;s been started with `pgroll start` but before running `pgroll complete`).Once the migration is completed any extra columns are dropped. replymst 15 hours agorootparentprevApologies for the off-topic-ness, but no matter where I&#x27;ve tried putting the mouse focus on the post, Up&#x2F;Down don&#x27;t work to scroll (but PgUp&#x2F;PgDown are fine).(I very much appreciate the effort to provide tooling that puts all these things together, btw) reply exekias 15 hours agorootparentThanks for reporting! we will look into it reply candiddevmike 17 hours agorootparentprevThat&#x27;s great that pgroll does this, but the heavy lifting for supporting this comes at a huge cost on the application side, IMO. reply surjection 17 hours agorootparentDo you mean the extra configuration required to make applications use the correct version of the database schema, or something else? reply candiddevmike 17 hours agorootparentYea, keeping your application consistent with two different schema versions. And I&#x27;m not saying from a blue&#x2F;green standpoint, from whatever pgroll does instead so when the rollback happens you don&#x27;t lose data. reply surjection 16 hours agorootparentI don&#x27;t see the need to keep your application consistent with both schema versions. During a migration pgroll exposes two Postgres schema - one for the old version of the database schema and another for the new one. The old version of the application can be ignorant of the new schema and the new version of the application can be ignorant of the old.pgroll (or rather the database triggers that it creates along with the up and down SQL defined in the migration) does the work to ensure that data written by the old applications is visible to the new and vice-versa.A rollback in pgroll then only requires dropping the schema that contains the new version of the views on the underlying tables and any new versions of columns that were created to support them. reply matsemann 16 hours agorootparentprevEh, isn&#x27;t this making it easier on the application side? Today, when I make a change in my app that needs a migration, I need multiple steps to make sure I don&#x27;t break old instances of the app still running. With this it looks like one can avoid all that? reply exekias 15 hours agorootparentThat is what this project is trying to achieve. By allowing your client apps to access both the old and the new schema at the same time, you can have the old instances of your application working while the new ones (using the new schema) get deployed.They can work in parallel for a while until you complete the rollout and call the migration as done. replycontravariant 18 hours agoparentprevI don&#x27;t think &#x27;undoable&#x27; is the clearest description, the crux is this:> Keep two versions of the schema (previous and next) accessible at the same time during the whole migration processThis has some obvious advantages. Like you said you can&#x27;t easily roll back once a migration is fully live, but it helps a lot if you can cancel a migration once it turns out it doesn&#x27;t work. reply tudorg 18 hours agorootparentYes, that&#x27;s exactly it. I generally agree with \"always move forward\" but if rolling back is as easy as dropping the \"new\" view, that makes it a lot less scary. reply candiddevmike 17 hours agorootparentprevIsn&#x27;t that what transactions are for? reply vanviegen 13 hours agorootparentThat wouldn&#x27;t allow you to partially roll out your new code base (depending on the altered schema), nor to easily revert such a roll out. reply contravariant 17 hours agorootparentprevThat would be hard to coordinate if your application is even slightly complicated. reply candiddevmike 13 hours agorootparentNot really...- Start migration, create transaction and locks- Do migration things- Do some SELECTs, INSERTS, or w&#x2F;e in your code to validate the schema matches your model.- End transaction or rollback if failureNone of this is free, but it&#x27;s by no means hard IMO. I do something similar in Go: https:&#x2F;&#x2F;github.com&#x2F;candiddev&#x2F;shared&#x2F;blob&#x2F;main&#x2F;go&#x2F;postgresql&#x2F;... reply ttfkam 13 hours agorootparent> Start migration, create transaction and locksIf you do that on a table or two with millions of rows, you&#x27;ve effectively locked your database from all other access, even SELECTs.ALTER statements typically grab exclusive locks, blocking both writes and reads. Chain a couple together and you&#x27;re looking at high CPU, queued queries, and an unresponsive database for hours.Once you realize what&#x27;s happening and kill the migration in a panic, it may take tens of minutes or more for the DB to revert back to its initial state, much to everyone&#x27;s impatient horror.In simple programs and databases, everything is possible and quick. Complex programs and large databases disabuse you of your naïveté with a quickness.I love Postgres&#x27;s transactional DDL, but as the article&#x27;s mention of renaming columns highlights, one transaction simply isn&#x27;t enough for the job. reply candiddevmike 13 hours agorootparentThe locks I was referring to were more around advisory locks to prevent multiple migrations from running. For the locks you mention, all migrations create some kind of table lock typically, it&#x27;s up to the dev to make them as small as necessary. I don&#x27;t follow your strawman. reply ttfkam 11 hours agorootparentThe locks I was referring to were a direct result of many (most?) ALTER statements. Multiple clients trying to migrate isn&#x27;t even in this equation. Just one client migrating can ruin your day with a single ALTER statement given a large enough database.What&#x27;s the biggest table you&#x27;ve ever had to modify?Adding a column with a default is easy. What about deleting a column from a table with 10s of millions of rows? Renaming a column? Changing a column&#x27;s type? Adding a CHECK constraint? Adding a NOT NULL? Adding a foreign key constraint that someone accidentally removed in the previous migration while data keeps flowing?Those concerns aren&#x27;t about your explicit advisory locks. Not by a long shot.https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;explicit-locking.htm... reply bananapub 12 hours agorootparentprev> I don&#x27;t follow your strawman.how large are the DB migrations you&#x27;ve done? your experience is seemingly completely different to mine. replyNullabillity 18 hours agoparentprevDuring development I&#x27;ll often keep rolling the same change back and forth (with minor changes), rather than recreate the database from scratch&#x2F;a backup each time. reply bityard 17 hours agoparentprevIf your organization requires change management (as many are contractually obligated to), then you don&#x27;t have much of a choice. Every change needs to be tested, every change needs a way to fully and completely roll back the change, and the rollback has to be tested.Additionally, the people executing the change are not necessarily those who have developed the change. They need two big buttons: Do and Undo. If the change fails or breaks something, they hit the Undo button and tell the developers about it and by the way, here are the logs, please go reproduce it on the test system and try again.I know this is not \"devops,\" but it&#x27;s still how a lot of high-availability software deployments work, particularly when hampered by bureaucratically-imposed processes.Finally, database schema changes are a fine way to irreversibly munge your data if you are not careful. (This goes beyond SQL.) If that happens, there is no such thing as a path forward, the only way to save the ship is to restore from your most recent backup. reply nobleach 11 hours agoparentprevA couple of places I&#x27;ve worked lived like this. We used the free version of Flyway with Spring Boot and Quarkus. We got by really well \"rolling forward\". My next gig used Sequalize and MySQL. Aside from hating those technologies, I hated that false sense of security baked into \"oh well, we can always roll back\" reply fabianlindfors 18 hours agoprevCongrats to the lovely Xata team on another great launch! I&#x27;m the creator of Reshape [0], a similar tool which inspired pgroll, and seeing the concept taken even further is really exciting.[0] https:&#x2F;&#x2F;github.com&#x2F;fabianlindfors&#x2F;reshape reply michaeldejong 14 hours agoprevVery cool! Congratulations to the authors on the release! I&#x27;m the author of a similar (zero-downtime migration) tool for PG called QuantumDB[0]. It was the first (to my knowledge at least) tool to support foreign keys, by creating table copies (keeping those in sync using triggers), and exposing multiple schemas through a custom database driver. I never got to production-ready version unfortunately, but I&#x27;m happy this one did. I&#x27;m seeing a lot of familiar concepts, and it looks well thought out.[0] https:&#x2F;&#x2F;github.com&#x2F;quantumdb&#x2F;quantumdb reply menthe 16 hours agoprevBeen reading the code.. very tidy. However the Complete step (e.g. in op_set_notnull.go) renames the temporary column name to the proper column name on the underlying table.. but while the docs describe the view on the new schema getting updated to refer to the now renamed underlying column, I do not seem to find the step where it happens? Also, shouldn&#x27;t those two steps be in a transaction to ensure no failed queries in between - otherwise that&#x27;s enough to be qualified as downtime ihmo? Quite dubious to see that `createView` is only called once, on `Start`, and that there doesn&#x27;t seem to be locks or transactions.Unless obviously the view has magic to use either column name based on what&#x27;s available on the underlying, but I did not see that either on `createView`. reply surjection 16 hours agoparentThere is no code to do this because it&#x27;s actually a nice feature of postgres - if the underlying column is renamed, the pgroll views that depend on that column are updated automatically as part of the same transaction. reply menthe 16 hours agorootparentVery cool, thank you! reply GRBurst 15 hours agoprevSo during migration both schemas are valid if I understood correctly?! It would be awesome if \"during migration\" could be lifted to a point where it is possible to keep both schemas (old and new) for as long as I want and do migrations&#x2F;transformation of incoming request (like queries) on the fly. Then I could map my different api version to different schemas and these on the fly transformation would be able to take care of the rest in many scenarios :-) reply exekias 15 hours agoparentThis is actually the case, old and new schemas are available and working until you complete the migration, and you can run this step whenever you want.The aim is not to deal with conditional logic in the app dealing with both schemas, but having an old version of the app linked to the old schema and the new one using the other. reply GRBurst 12 hours agorootparentso if I want to sunset my api version X in 1 year for whatever reason and I am able to support an old schema X, which api X maps to, for that time period without any hassle (and not only during migration), this would be a much bigger feature &#x2F; USP for me then everything else mentioned. I am really curious to look deeper into this :-) reply chrisweekly 15 hours agoparentprevI had the same thought; eager to see if anyone can explain why this wouldn&#x27;t work, or (better) how they&#x27;re already doing this today. reply jedberg 17 hours agoprevThis is very cool! Schema migrations have always been tough and fraught with peril.That being said, I&#x27;ve always found that doing the migrations within the app is safer, because rollbacks are tied to the code. For example to change a constraint, we add a new column with the new constraint, and then change the code to read from the new column and old column, take the new value if it exists, otherwise use the old value, perform the operation, then write the new value to the new column and old column. We do this for a while and then do a background migration of the data at the same time, slow enough to not overload the database. At some point the new column is fully populated, and then we can put a new version of the code that only reads the new column. Then you check your stats to make sure there are no reads from the old column, and then you can delete the old one whenever you want, which is a very fast operation. Then you get your space back when you do your vacuum (protip: make sure you have solid vacuuming schedules!).What are some use cases where you&#x27;d want to migrate the schema without also migrating the code at the same time? reply skrebbel 16 hours agoparentTo my reading that’s exactly what this is intended for.You do “pgroll start”, let it run, and then when that’s done you deploy your new code. Then when you’re confident the new code is fine, you do “pgroll complete”. If at any time you realize you got it wrong, you rollback the code and then you do “pgroll rollback” and it’s af if nothing happened (but data changes that went through the new code and schema are still around, if the change you made allows). reply jedberg 16 hours agorootparentRight, but the difference is that this is done on the backend with no app awareness. Doing it in the app ties the app logic to the database schema. Using pgroll would allow the database and app to get out of sync. reply surjection 15 hours agorootparentAllowing the database and the application to be out of sync (to +1&#x2F;-1 versions) is really the point of pgroll though.pgroll presents two versions of the database schema, to be used by the current and vNext versions of the app while syncing data between the two.An old version of the app can continue to access the old version of the schema until such time as all instances of the application are gracefully shut down. At the same time the new versions of the app can be deployed and run against the new schema. reply jedberg 15 hours agorootparentBut then how do avoid errors introduced due to the lack of app awareness? For example the old app can keep writing data with the old schema, whereas the new version makes assumptions based on the new schema, which could be broken by the new data in the old schema that don&#x27;t follow the same assumptions.Seems like a dangerous way to introduce very hard to find data inconsistencies. reply ris 15 hours agorootparent> For example the old app can keep writing data with the old schema, whereas the new version makes assumptions based on the new schemaTo do safe migrations it&#x27;s a good idea to avoid rolling out versions that start making assumptions about the new schema until the migration is done and dusted.This does of course start to create weird territory where migrations cease to become trivially \"stackable\" if you want to retain the whole zero-downtime thing, but this is true for a lot of \"graceful update\" patterns in computing (in fact I&#x27;ve not seen it solved properly before). reply jedberg 14 hours agorootparent> To do safe migrations it&#x27;s a good idea to avoid rolling out versions that start making assumptions about the new schema until the migration is done and dusted.Or make the app do the migration as part of its normal operation, so you have total control over it. :)I think we&#x27;re saying the same thing, I&#x27;m just suggesting it&#x27;s safer to do it in the app itself, because then app changes and database changes are tied together instead of divorced from each other.If you only have one or two people working on the code that interacts with the table that is being changed, both methods are pretty much the same. But if you have a large team or want to be able to rollback better, then doing it all in the app code feels like the better way to go for me. reply exekias 14 hours agorootparentprevFor migrations altering columns (for instance adding a constraint), data gets upgraded&#x2F;downgraded between old and new versions trough the up&#x2F;down functions. These are defined by the migration. They work like this:- For rows inserted&#x2F;updated through the old version of the schema, the up function gets executed, copying the resulting value into the new column - For rows inserted&#x2F;updated through the new version, the down function gets executed, copying the resulting value into the old columnFor instance, you can test that up function works before releasing the new app relying on it, just by checking that the values present in the new schema are correct. reply jedberg 14 hours agorootparentWhat happens if you write into the old table with data that violates the constraint the new schema adds? Does the up function fail? reply surjection 13 hours agorootparentAn example would make this more concrete.This migration[0] adds a CHECK constraint to a column.When the migration is started, a new column with the constraint is created and values from the old column are backfilled using the &#x27;up&#x27; SQL from the migration. The &#x27;up&#x27; SQL rewrites values that don&#x27;t meet the constraint so that they do.The same &#x27;up&#x27; SQL is used &#x27;on the fly&#x27; as data is written to the old schema by applications - the &#x27;up&#x27; SQL is used (as part of a trigger) to copy data into the new column, rewriting as necessary to ensure the constraint on the new column is met.As the sibling comment makes clear, it is currently the migration author&#x27;s responsibility to ensure that the &#x27;up&#x27; SQL really does rewrite values so that they meet the constraint.[0] - https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;blob&#x2F;main&#x2F;examples&#x2F;22_add_c... reply skrebbel 4 hours agorootparentIf I may be so free, I read your docs quite carefully last night and it wasn’t immediately clear to me that “up” and “down” are automagically run to keep both the old and the new data in sync, while a migration is pending. To my reading this is a spectacular feature because it solves the “the only way is forward” problem (where you can’t actually ever roll back a migration or risk losing &#x2F; making inconsistent some recent data) for many kinds of migrations.May I suggest spelling this out more explicitly in the docs? As far as I can tell it’s only mentioned by example halfway down the tutorial. To me it seems like one of pgroll’s true killer features, worth more coverage :-) reply exekias 14 hours agorootparentprevYes, this is the case as of today. It&#x27;s important to get the up&#x2F;down functions right.The good thing is that this will probably be detected during the start phase, as data would fail to be backfilled.We are thinking of ideas to improve this in the longer term, for example: * Having a dry-run mode where you check that the data present in the DB will stay valid after applying the up function * Optionally allow for this to happen, but \"quarantine\" the affected rows so they are no longer available in the new schema. replyhosh 17 hours agoparentprevI wasn&#x27;t able to see the blog article (because it 404 by the time I am looking at this). I&#x27;m considering introducing this to the eng team I am a part of, because multiple teams and multiple projects often touch the same db.Anyone know how well this works with very large datasets? The backfill sounds like it would take a while to do.Does this Go binary need to be continuously running, or does it keep track of migration state in the database? reply tudorg 17 hours agorootparent> Anyone know how well this works with very large datasets? The backfill sounds like it would take a while to do.It can take a long time, yes. It&#x27;s somehow similar in that regard with, for example, gh-ost for Mysql that also does backfills. The advantage of Postgres here, is that backfill is required for fewer migration types, and pgroll only does backfills when needed.> Does this Go binary need to be continuously running, or does it keep track of migration state in the database?The latter, you only run the Go binary when doing schema changes. reply Yeroc 14 hours agorootparentHow can you properly plan for eg. disk storage requirements etc. Does the tool calculate that upfront via some sort of dry-run mode? For companies with larger datasets this would be a rather important consideration. Also, those backfills will generate a lot of network traffic in clustered environments. reply exekias 14 hours agorootparentThis is a good point, I believe we can look into trying to estimate storage needs or timings before a migration. It definitely looks like a nice to have. replynojvek 6 hours agoprevThere is no magic bullet to database migration unless your app is deployed with the database itself and can atomically switch to new version as a DDL change is committed, or you take down the whole ship in maintenance mode and bring it back up once app + DB is migrated.The only way to do it without downtime is to make the app forward compatible before making DB changes, and make it backward compatible before undoing the DB changes.There will always be a short period where the app version and DB version will be out of sync. reply menthe 17 hours agoprevZero-downtime, undoable, schema migrations for Postgres... But definite downtime and undone blog post...> Page not found> We&#x27;re sorry, but the page you requested could not be found. This could be because the page has been moved or deleted. We&#x27;re tracking these errors so we can fix them in the future. reply alexf_19 17 hours agoparentThis should be back up now: https:&#x2F;&#x2F;xata.io&#x2F;blog&#x2F;pgroll-schema-migrations-postgresThat&#x27;s what we get for trying to fix some text ;-) reply hosh 17 hours agoparentprevI did find this: https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll reply tudorg 17 hours agoparentprevSorry about it, small glitch with our blog as we tried to fix something in the title and accidentally broke the metadata for it :) reply csmattryder 15 hours agorootparentWhen can we expect blogroll to be released?? reply alexf_19 14 hours agorootparentWell played, well played. reply ris 10 hours agoprevThis does look awesome, though I think I&#x27;d need a lot of trust in it before I went down this route. It seems to take a pretty drastic approach and it&#x27;s unclear how complex a schema it would be able to handle. e.g. how well would its use of triggers to synchronize fields work with a schema that itself uses triggers in various ways? I can imagine some weird interactions that could take place.I&#x27;d also be a bit nervous that any of these tricks may themselves cause (perhaps temporary) performance problems. reply aeyes 18 hours agoprevThis looks very nice indeed but I see a few possible problems which I have seen with pg_repack which might apply to this approach as well:You can&#x27;t change table names unless you take a lock. How exactly do you switch the original table to be a view pointing to the original table? The docs don&#x27;t go into detail how this is done exactly, I&#x27;ll check the code later.It looks like the tool maintains two copies of the table but how exactly this copy process is done isn&#x27;t explained. A potential issue is that you need to have disk space and I&#x2F;O capacity available to support this.The copy table + trigger approach might not work for databases of significant size. For example I have seen instances with >50k qps on a table where it is not possible to run pg_repack because it never catches up and it also doesn&#x27;t ever manage to take the lock which is needed to switch to the new table. This can be simulated with overlapping long running queries. reply exekias 18 hours agoparent> This looks very nice indeed but I see a few possible problems which I have seen with pg_repack which might apply to this approach as well:Thank you for your input! I&#x27;m one of the pgroll authors :)> You can&#x27;t change table names unless you take a lock. How exactly do you switch the original table to be a view pointing to the original table? The docs don&#x27;t go into detail how this is done exactly, I&#x27;ll check the code later.pgroll only performs operations requiring a short lock, like renaming a table. It sets a lock timeout for these operations (500ms by default), to ensure we avoid lock contention if other operations are taking place. We plan to add an automatic retry mechanism for these timeouts so there is no need for manual intervention.One cool thing about views is that they will automatically get updated when you rename a table&#x2F;column, so the view keeps working after the rename.> It looks like the tool maintains two copies of the table but how exactly this copy process is done isn&#x27;t explained. A potential issue is that you need to have disk space and I&#x2F;O capacity available to support this. > The copy table + trigger approach might not work for databases of significant size. For example I have seen instances with >50k qps on a table where it is not possible to run pg_repack because it never catches up and it also doesn&#x27;t ever manage to take the lock which is needed to switch to the new table. This can be simulated with overlapping long running queries.pgroll doesn&#x27;t really copy full tables, but individual columns when needed (for instance when there is a constraint change). It is true that I&#x2F;O can become an issue, backfilling is batched but the system should have enough capacity for it to happen. There are some opportunities to monitor I&#x2F;O and throttle backfilling based on it. reply cpursley 17 hours agoprevWhat I&#x27;d love to see is state-based migrations similar to what Prisma offers - but that can handle, views, functions, and complex logic that references other things - and have it be smart enough to change those as well. Or at least walk you through any dependent changes. I&#x27;d pay for that. reply evanelias 14 hours agoparentFor things like stored procs, triggers, and views, there&#x27;s a lot of vendor-specific (e.g. Postgres vs MySQL vs SQL Server) edge cases in syntax, introspection, and operational best practices. That&#x27;s true of tables too of course, but at least the introspection part tends to be fully functional for tables in all major database systems. For other object types, introspection can be half-baked and things can get painful in general. It&#x27;s much harder to design a generic declarative tool which works across multiple DBs without making sacrifices in expressiveness, safety, and ergonomics.So most likely you&#x27;re going to want a Postgres-specific tool for this, but I&#x27;m not sure one exists yet that handles everything you&#x27;re looking for here.I&#x27;m the author of a product called Skeema which does handle all this (tables, procs&#x2F;funcs, views, triggers) for MySQL and MariaDB, and in my opinion this is an area where MySQL&#x2F;MariaDB&#x27;s relative simplicity -- in things like e.g. lack of transactional DDL -- actually makes this problem easier to solve there. For example Skeema explicitly doesn&#x27;t handle data migrations because you can&#x27;t atomically combine schema changes and data changes in MySQL&#x2F;MariaDB in the first place.btw when describing&#x2F;searching for this, I always say \"declarative\" and never \"state-based\". \"Declarative\" is consistent with terminology used by other infra-as-code such as Terraform and Kubernetes. The main places I see calling it \"state-based\" are marketing blog posts from commercial schema management tools using an imperative migration approach (Liquibase, Bytebase, etc). To me it feels like they say \"state-based\" in order to make the declarative competition seem more strange&#x2F;foreign... reply cpursley 10 hours agorootparentI’m 100% in on Postgres but what you describe sounds awesome. reply peter_l_downs 10 hours agorootparentIf you’re seriously willing to pay money for this, send me an email — I’m considering implementing this in my pgmigrate tool but not sure if it’s worth the development time. I have all the groundwork done, probably achievable in about 10 hours of concerted effort. reply t1mmen 16 hours agoparentprevI’ve looked everywhere for this in NodeJS & adjacent stacks; almost all migration tools seem to focus on tables, columns and rows. None seem to deal with views, functions, triggers.I only got back into Postgres this year, after almost a decade away from SQL. It’s kind of bizarre to me that the migration tooling is still at the stage where a 1 line change to eg a Postgres function requires a the whole function to be dropped and re-created?I understand this is needed at the db level, but surely a “definition” that generates the final migration is doable; it would make such a huge difference in code reviews and to understand how a function&#x2F;etc changed over time.Am I just looking in the wrong place? Does this exist? If not, how come? Is it really that hard to do? reply peter_l_downs 16 hours agorootparentI believe Migra can generate those changes for you via diffing, not sure how well it handles dependent views&#x2F;functions&#x2F;etc reply t1mmen 15 hours agorootparentThanks! I’ll give it a look (their docs are offline atm)DrizzleKit and several others do this for table changes, but nothing I’ve found (possibly excluding Flyway and other Java options) do views&#x2F;functions&#x2F;etc. reply peter_l_downs 10 hours agorootparentThe migra maintainer has abandoned it for greener pastures. If you’re interested in sponsoring an alternative, send me an email. My pgmigrate tool has all the groundwork necessary to make this possible but I have held off implementing this because I am not personally interested in using it. reply Guillaume86 12 hours agoparentprevSSDT for mssql can do it, I have my schema as SQL files in the repo, the tooling can diff and migrate between schema versions. reply tryithard 13 hours agoprevCool, We use liquibase I wonder how this compares to it?Also, how do you handle the back filling on columns, how you make sure you don&#x27;t miss any data before dropping the old column? reply exekias 13 hours agoparentI don&#x27;t know much about Liquidbase, but I believe it doesn&#x27;t support accessing both the old and the new schema versions at the same time? (I could be wrong here)Backfilling happens in batches, we use the PK of the table to update all rows, a trigger is also installed so any new insert&#x2F;update executes the backfill mechanism to update any new column.More details can be found here: https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgroll&#x2F;blob&#x2F;main&#x2F;pkg&#x2F;migrations&#x2F;ba... reply kakoni 16 hours agoprevUsing this thread to ask; Is there some simple sql based migration tool for psql? reply a8m 13 hours agoparentCheck out: https:&#x2F;&#x2F;github.com&#x2F;ariga&#x2F;atlas. It’s like Terraform for databases.(I&#x27;m one of the authors of this project). reply nextaccountic 11 hours agorootparentForr postgres, how does the schema diffing aspect compare to migra?https:&#x2F;&#x2F;github.com&#x2F;djrobstep&#x2F;migraI&#x27;m asking because, although migra is excellent and there are multiple migrations tools based on it (at least https:&#x2F;&#x2F;github.com&#x2F;bikeshedder&#x2F;tusker and https:&#x2F;&#x2F;github.com&#x2F;blainehansen&#x2F;postgres_migrator), issues are piling up but development seem to be slowing down reply rotemtam 4 hours agorootparentHey,I&#x27;m @a8m&#x27;s co-founder and also a maintainer of Atlas. I don&#x27;t have an exact feature comparision but Atlas (and esp the Postgres driver) are heavily maintained with advanced features like views and materialized views (functions and procedures coming up, followed by triggers and more).What features specifically are you missing in migra? Perhaps we can prioritize them. Also feel free to join our Discord https:&#x2F;&#x2F;discord.gg&#x2F;zZ6sWVg6NT reply totallywrong 5 hours agoprevThis looks amazing, thank you! reply jmccarthy 16 hours agoprevThank you for the project! I did a brief scan for an explanation of the PG 14+ version constraint. Which 14-specific features are you relying on? reply exekias 15 hours agoparentWe use a few statements that are not supported by previous versions. For instance `CREATE OR REPLACE TRIGGER`.Supporting previous versions would be possible, but we went for the most recent ones to reduce complexity. reply tudorg 19 hours agoprevhey HN, repo is here: https:&#x2F;&#x2F;github.com&#x2F;xataio&#x2F;pgrollWould love to hear your thoughts! reply gvkhna 18 hours agoparentGreat to see more innovation in this space! How does this compare to?https:&#x2F;&#x2F;github.com&#x2F;shayonj&#x2F;pg-osc reply exekias 18 hours agorootparentHi there, I&#x27;m one of the pgroll authors :)I could be mistaken here, but I believe that pg-osc and pgroll use similar approaches to ensuring no locking or how backfilling happens.While pg-osc uses a shadow table and switches to it at the end of the process, pgroll creates shadow columns within the existing table and leverages views to expose old and new versions of the schema at the same time. Having both versions available means you can deploy the new version of the client app in parallel to the old one, and perform an instant rollback if needed. reply gvkhna 7 hours agorootparentThanks for the reply, a write up on pros&#x2F;cons in these approaches would be fantastic. I have no clue which is better but I believe pgosc is heavily inspired by github&#x2F;gh-ost, their tool for online schema change for mysql. reply brycethornton 17 hours agorootparentprevDoes pgroll have any process to address table bloat after the migration? One of the (many) nice things about pg-osc is that it results in a fresh new table without bloat. reply surjection 17 hours agorootparentAnother pgroll author here :)I&#x27;m not very familiar with pg-osc, but migrations with pgroll are a two phase process - an &#x27;in progress&#x27; phase, during which both old and new versions of the schema are accessible to client applications, and a &#x27;complete&#x27; phase after which only the latest version of the schema is available.To support the &#x27;in progress&#x27; phase, some migrations (such as adding a constraint) require creating a new column and backfilling data into it. Triggers are also created to keep both old and new columns in sync. So during this phase there is &#x27;bloat&#x27; in the table in the sense that this extra column and the triggers are present.Once completed however, the old version of this column is dropped from the table along with any triggers so there there is no bloat left behind after the migration is done. reply brycethornton 16 hours agorootparentThanks for the reply. My question was specifically about the MVCC feature that creates new rows for updates like this. If you&#x27;re backfilling data into a new column then you&#x27;ll likely end up creating new rows for the entire table and the space for the old rows will be marked for re-use via auto-vacuuming. Anyway, bloat like this is a big pain for me when make migrations on huge tables. It doesn&#x27;t sound like this type of bloat cleanup is a goal for pgroll. Regardless, it&#x27;s always great to have more options in this space. Thanks for your work! reply pritambaral 16 hours agorootparentprev> ... so there there is no bloat left behind after the migration is done.This is only true after all rows are rewritten after the old column is dropped. In standard, unmodified Postgres, DROP COLUMN does not rewrite existing tuples. replynwhnwh 18 hours agoparentprevIs it possible to make it work using SQL only in the future?Also, what about if the user can just maintain one schema file (no migrations), and the lib figures out the change and applies it? reply surjection 18 hours agorootparentHi, one of the authors of pgroll here.Migrations are JSON format as opposed to pure SQL for at least a couple of reasons:1. The need to define up and down SQL scripts that are run to backfill a new column with values from an old column (eg when adding a constraint).2. Each of the supported operation types is careful to sequence operations in such a way to avoid taking long-lived locks (eg, initially creating constraints as NOT VALID). A pure SQL solution would push this kind of responsibility onto migration authors.A state-based approach to infer migrations based on schema diffs is out of scope for pgroll for now but could be something to consider in future. reply aseering 17 hours agorootparentThanks for releasing this tool! I actually interpreted the question differently: Rather than manipulating in SQL, would you consider exposing it as something like a stored procedure? Could still take in JSON to describe the schema change, and would presumably execute multiple transactions under the hood. But this would mean I can invoke a migration from my existing code rather than needing something out-of-band, I can use PG’s existing authn&#x2F;authz in a simple way, etc. reply gorkish 18 hours agorootparentprev> Also, what about if the user can just maintain one schema file (no migrations), and the lib figures out the change and applies it?Because that only solves the DDL issues and not the DML. It is still useful though.I use a schema comparison tool that does exactly this to assist in building my migration and rollback plans, but when simply comparing two schemas there is no way to tell the difference (for example) between a column rename and a drop column&#x2F;add column. The tooling provides a great scaffold and saves a ton of time. reply potamic 17 hours agoparentprevThe link to the introductory blog post here appears to be brokenhttps:&#x2F;&#x2F;xata.io&#x2F;blog&#x2F;pgroll-schema-migrations-postgres reply aschleck 19 hours agoparentprevCool stuff! Do you have any thoughts about how this compares to https:&#x2F;&#x2F;github.com&#x2F;fabianlindfors&#x2F;reshape? reply tudorg 18 hours agorootparentGreat question! Reshape was definitely a source of inspiration, and in fact, our first PoC version was based on it.We decided to start a new project for a couple of reasons. First, we preferred it to have it in Go, so we can integrated it easier in Xata. And we wanted to push it further, based on our experience, to also deal with constraints (with Reshape constraints are shared between versions). reply Already__Taken 16 hours agoprevthe 3 schemas is new to me and a cool idea but, how about a migration that removes a column into a few linking tables. what does that look like? I&#x27;ve changed note table colmun into a note-type and it&#x27;s attached ID so you can add a note to anything. initially with notes from the original table colmun. how does that undo reply exekias 15 hours agoparentI didn&#x27;t fully understand the change that you are explaining, but in general having old & new schemas working relies on the migration definition having the proper `up` & `down` functions defined. These are postgres functions so you can fit any logic in them to ensure that a column deleted in the new schema gets properly backfilled to the old one.I wonder if there will be cases where this gets too complex or otherwise hits any limits. It may be possible that for those cases a raw SQL migration is required. reply ps256 16 hours agoprevHow do people typically do migrations in production anyway - what tools are used? what are the best practices? reply peter_l_downs 16 hours agoparentFlyway is generally the most popular. Django&#x2F;alembic in the python world.I think most migration tools are missing a lot of features that would make them safer and easier to use. Modern teams merge a lot and deploy frequently, but most migration tools are built for a db admin manually running commands.I wrote a migration cli tool (and golang library) that I think is more suited to modern workflows, if you check it out I’d appreciate any feedback!https:&#x2F;&#x2F;github.com&#x2F;peterldowns&#x2F;pgmigrate reply claytonjy 14 hours agoparentprevI used sqitch in a past job and loved it, but I had to basically implement what pgroll does automatically in order to ensure smooth migrations, and I hit plenty of issues along the way. Learned a lot, but pgroll looks much friendlier. reply lazyant 16 hours agoparentprevFlyway and Liquibase are typical migration tools.Best practice is to have your new code backwards-compatible with the old schema so you can do blue-green deployments. reply meowtimemania 16 hours agoparentprevAt my job we do database migrations 1 week and then deploy code the next. That way if any problems happen in either, it’s easier to roll back. reply jonny_eh 17 hours agoprevIn one of the diagrams: \"Rename column lastname\", did they mean \"add column lastname\"? reply tudorg 17 hours agoparentAh yes, good catch! Fixing.. reply moltar 11 hours agoprevAre there any plans for an abstraction over JSON? reply sidcool 17 hours agoprevWhy does a migration tool need to be subscription based? One time fee should be enough. reply exekias 17 hours agoparentHi, one of the authors here!The tool is open source and doesn&#x27;t require any subscription :) reply happytoexplain 17 hours agorootparentTo add context for others:Reading the homepage as a total DB noob, all signs point to this product being a CLI tool. However, when you get to the bottom, you see that it is subscription-based with a free tier. Reading the feature list (\"availability zones\", \"storage\", etc), it sounds like this is in fact a service with a CLI frontend? But again, this is a layman perspective, and some clarity from the authors (or other commenters) might help.Update: I think I see what happened. Xata is a serverless DB service, and this is a tool they wrote that can be used independently of their service (I assume). The subscription options presented to the user look like they are related to this CLI tool, but they are in fact for the broader Xeta service. reply tudorg 17 hours agorootparent> Update: I think I see what happened. Xata is a serverless DB service, and this is a tool they wrote that can be used independently of their service (I assume). The subscription options presented to the user look like they are related to this CLI tool, but they are in fact for the broader Xeta service.Yes, that&#x27;s right. Xata is a Postgres-based service and we&#x27;re working on exposing the Postgres DB directly and unrestricted to our users. As part of this, we&#x27;re also open-sourcing parts of the platform. We&#x27;ll have more such open source projects soon.This is not exactly the case with pgroll, as its approach is different from what we do today in Xata, but we&#x27;ll be incorporating pgroll in Xata soon. reply canadiantim 12 hours agoprevAny chance of doing something similar with sqlite? reply tibanne 18 hours agoprevIf I&#x27;m using alembic for schema migrations already, how do I make use of this? reply surjection 17 hours agoparentWe are looking to build integrations with other tools but for now isn&#x27;t recommended to use pgroll alongside another migration tool.To try out pgroll on a database with an existing schema (whether created by hand or by another migration tool), you should be able to have pgroll infer the schema when you run your first migration.You could try this out in a staging&#x2F;test environment by following the docs to create your first migration with pgroll. The resulting schema will then contain views for all your existing tables that were created with alembic. Subsequent migrations could then be created with pgroll.It would be great to try this out and get some feedback on how easy it is to make this switch; it may be that the schema inference is incomplete in some way. reply tlarkworthy 12 hours agoprevHow would I fit this into an alembic workflow? reply pierat 18 hours agoprev\"Undoable\" in this case means \"possible to undo\", not irreversible! reply wccrawford 18 hours agoparentIt wouldn&#x27;t mean \"irreversible\" regardless. It would mean \"not possible to do\" (ie impossible) or \"possible to undo\". reply bitslayer 18 hours agoparentprevThat is a funny word. I am a big proponent of in-word dashes, which in this case could help clarify. It is \"undo-able\", not \"un-doable\". reply murkt 18 hours agoparentprevYeah, needs a dash there. Undo-able. reply exekias 18 hours agoparentprevThank you for noticing this! We are looking into changing the wording :) reply jandrese 18 hours agoparentprevThanks, I was wondering why they were advertising the process to be impossible. A better term would have been \"reversible\". reply canadiantim 18 hours agoparentprevSeems like very unfortunate wording then as irreversible is exactly what I thought reply hamilyon2 18 hours agoparentprev\"With easy rollback\", because rollback is well understood reply systems 18 hours agoparentprevyes, horrible choice of word, i read it to mean one way schema migration (but zero downtime so maybe worth the risk)i think he should use the obvious word if this is what it means: reversible reply jakswa 13 hours agoprevif you&#x27;re in rails, then in my experience you just add `safe-pg-migrations` gem and call it a day :D reply 1ba9115454 16 hours agoprev [–] I didn&#x27;t look too deep at this as soon as it said JSON. I was gone.I&#x27;ve been using dbmate which uses SQL and works really well. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Xata introduced the first iteration of pgroll, a command line utility promoting secure and reversible schema migrations for PostgreSQL.",
      "pgroll mitigates database schema migration risks by enabling the definition of migrations using JSON format, sustaining dual schema versions during migration, permitting instant rollbacks, and ensuring zero downtime.",
      "Xata is committed to further developing pgroll; this tool automates the complete lifecycle of schema changes and offers a user-friendly command line interface."
    ],
    "commentSummary": [
      "The primary focus of the discussion is pgroll, a tool utilized for zero-downtime, reversible schema migrations in PostgreSQL databases, allowing schema changes while maintaining access to both old and new versions.",
      "The performance of pgroll and its capability to handle complex schemas are key points of concern in the comparison.",
      "Other popular migration tools like Flyway and Liquibase are mentioned for comparison purposes."
    ],
    "points": 302,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1696342856
  },
  {
    "id": 37750859,
    "title": "Hey, computer, make me a font",
    "originLink": "https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font",
    "originBody": "SerCe's blog Home Blog Talks By Sergey Tselovalnikov on 02 October 2023 Hey, Computer, Make Me a Font This is a story of my journey learning to build generative ML models from scratch and teaching a computer to create fonts in the process. Yes, genuine true type fonts, with a capital-only set of glyphs. The model takes a font description as an input, and produces a font file as an output. I named the project 'FontoGen'. Here are a few examples of fonts generated by the FontoGen model: bold, sans THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG? italic, serif THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG! techno, sci-fi, extrabold THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG. If you want to generate your very own font, head to the GitHub project, clone it, and don’t forget to leave a star. Then download the weights from Huggingface, and follow the instructions here. And if you want to learn the full story, keep reading. I'm gonna go build my own theme park — Bender Bending Rodríguez Intro At the beginning of 2023, when AI started creating ripples across the internet, like many others I became very interested in the topic. I was sucked into the world of making memes with Stable Diffusion, training LoRAs on my friends’ faces, and fine-tuning text-to-speech models to mimic famous voices. At some point, I started looking at text-to-SVG generation which, as it turned out, is a much harder task compared to raster-based text-to-image generation. Not only is the format itself quite complex, it also allows for representing the exact same shape in many different ways. As I was interested in learning how to build a generative ML model from scratch, this became my weekend project. The Idea As I began exploring different ways to generate SVGs, I came across the IconShop2 paper which achieved pretty impressive results. It took me some time to reproduce them by building a model based on the description in the paper. After finally achieving close-enough results, I realised that the process of generating fonts could be similar to the process of generating SVGs, and started working on the project. The final result Compared to SVG images, fonts are both easier and harder to generate. The easier part is that fonts don’t have the colour component present in colourful SVG images. However, the harder part is that a single font consists of many glyphs, and all glyphs in a font must maintain stylistic consistency. Maintaining consistency turned out to be a significant challenge which I'll describe in more detail below. The Model Architecture Inspired by the SVG generation approach described in the IconShop paper, the model is a sequence-to-sequence model trained on sequences that consist of text embeddings followed by font embeddings. Input Sequence Text Embeddings To produce text embeddings, I used a pre-trained BERT encoder model, which helps to capture the \"meaning\" of the prompt. The text sequence is limited to 16 tokens, which in BERT’s case roughly corresponds to the same number of words. While the text prompt could potentially be longer, memory constraints were a significant concern for my single-GPU setup. So, all textual font descriptions present in the dataset were summarised to a set of a few keywords with the help of OpenAI’s GPT-3. Font Embeddings In order to produce font embeddings, the fonts first need to be converted to a sequence of tokens similar to how text is tokenised with the BERT tokeniser. In this project, I’ve only considered the glyph shapes and ignored the width, height, offset, and other useful metadata present in the font files. Each glyph was downsampled to 150x150 and normalised. I found that the 150x150 dimension preserves font features with minimal glyph deformation, which was more pronounced at lower resolutions. I used Python’s fonttools to parse font files which can conveniently process each glyph as a sequence of curves, lines, and move commands, where each command can be followed by zero or more points. I decided to limit the glyph set to the following glyphs to get a minimal usable font. ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!? The final model vocabulary needed to represent 22547 different tokens: 40 glyphs, 5 line path operations: moveTo, lineTo, qCurveTo, curveTo, closePath, 2 tokens to represent EOS (end of sequence) and PAD (padding), 150^2 = 22500 different points. An example font token sequence. The token sequence is then converted into an embedding vector using learnable embedding matrices. Additionally, as proposed in the SkexGen1 paper, separate matrices were used specifically for x and y coordinates. And the final step was to apply positional embeddings. Transformer The model is an autoregressive encoder-only transformer consisting of 16 layers and 8 blocks. The model’s dimension is 512, resulting in a total of 73.7 million parameters.NameTypeParams ----------------------------------- 0modelFontogen73.7 M ----------------------------------- 73.7 M Trainable params 0 Non-trainable params 73.7 M Total params 294.728 Total estimated model params size (MB) I computed the loss using simple cross-entropy and disregarded the padding token. Attention Every time a part of the glyph is generated, several factors influence the decision on which token comes next. First, the model prompt affects the glyph’s shape. Next, the model needs to consider all previously generated tokens for that glyph. Finally, it needs to take into account all other glyphs generated so far to ensure consistency in style. When doing initial experiments with only a handful of glyphs, I started with full attention. However, as the sequence length increased, this approach became impractical, prompting a shift to sparse attention. After exploring various options, I settled on BigBird3 attention. This approach supports both global attention, to focus on the initial prompt, and window attention, which observes N previous tokens, capturing the style of several preceding glyphs. BigBird attention Given that a single glyph can have a variable number of tokens, I set the attention mechanism to consider at least the 3 preceding glyphs. While most of the time, the approach has been successful at preserving the overall font style, in some complex cases, the style would slowly drift into unrecoverable mess. Calligraphy is hard Training To train the model, I assembled a dataset of 71k distinct fonts. 60% of all fonts only had a vague category assigned to them, while 20% fonts were accompanied by longer descriptions, so the descriptions were condensed to a few keywords using GPT-3.5. Additionally, I included 15% fonts where the prompt only contained the font's name, and the remaining 5% of the dataset had an empty textual description assigned to them to ensure that the model is capable of generating fonts with no prompt at all. Due to large memory requirements, my Nvidia 4090 with 24G of VRAM could only fit two font sequences in a single batch, and I’d often observe gradient explosions. Using gradient accumulation and gradient clipping helped to resolve the issue. The model was trained for 50 epochs which took 127 hours. I restarted training once after 36 epochs, and kept training for another 14 epochs with reduced gradient accumulation. The training was stopped when the validation loss showed very little improvements. Validation loss of the first 36 epochs Validation loss of the remaining 14 epochs Chasing Performance Achieving good training performance was critical since I was training on a single GPU, and training took a significant amount of time. In the initial iteration, I processed font files and textual descriptions directly within the model on each step. While this codebase structure streamlined prototyping, it meant that the same tasks had to be repeated over and over again, making the training process slower. Additionally, having BERT loaded in memory meant that it would take up precious VRAM. By shifting as much as possible to the dataset preprocessing stage, I achieved a threefold performance boost. Originally, the model relied on huggingface's transformers. Migrating the code to xformers4 gave a very visible boost in speed and memory usage. Instead Of Conclusion I achieved what I set out to do – I learned how to build a generative transformer model, and built a project that's capable of generating fonts as a side effect. But there are so many things that I still haven't tried. For example, what if the model could be integrated into the existing font editors so that the font designer only creates a single glyph A, and all other glyphs are generated by the model. Or maybe the font editor could suggest the control points for bézier curves as they're being drawn! The horizon is vast, and there's much left to explore. If you've read this article, and you think that I've overlooked something obvious, there's a good chance I did! I'm always keen to learn more, so please reach out and let me know what you think. Thank you to Paul Tune for answering many questions I had about building transformer models. References SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers Big Bird: Transformers for Longer Sequences xFormers: A modular and hackable Transformer modelling library Discuss on Twitter Hacker News Subscribe I'll be sending an email every time I publish a new post. Subscribe Or, subscribe with RSS.",
    "commentLink": "https://news.ycombinator.com/item?id=37750859",
    "commentBody": "Hey, computer, make me a fontHacker NewspastloginHey, computer, make me a font (serce.me) 296 points by pavanyara 21 hours ago| hidepastfavorite129 comments lachlan_gray 19 hours agoI found a few months ago that the gpt-4 code interpreter is capable of converting a black and white png of a glyph to an svghttps:&#x2F;&#x2F;twitter.com&#x2F;lfegray&#x2F;status&#x2F;1678787763905126400It would be cool to combine a script like the one gpt-4 gave me with an image generation model to generate fonts. The approach from this blog post is way more interesting though.On a separate note it reminds me of this suckerpinch video :) maybe we can finally get uppestcase and lowestcase fontshttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=HLRdruqQfRk reply alana314 17 hours agoparentThat&#x27;s amazing. One of my favorite things to do with copilot is to comment something like \"&#x2F;&#x2F;white arrow pointing right\" and then start \" Saves me time searching for the right SVG and digging through free but really paid image sites.FWIW, Google&#x27;s Material Design Icons and The Noun Project are decent sources of high quality, actually-free SVG icons:* https:&#x2F;&#x2F;fonts.google.com&#x2F;icons (Apache license)* https:&#x2F;&#x2F;thenounproject.com&#x2F; (CC-BY) reply speps 16 hours agorootparentprevAnd it saves you having to credit anyone, win-win! reply jbc1 12 hours agorootparentAwful lot of sites have icons on them. I can&#x27;t recall ever seeing icon credit. Copilot is like a year old. reply spookie 4 hours agorootparentThat&#x27;s not a good argument, I&#x27;m sorry. A lot of people spent time and effort designing and creating something; credit, or reference is the least one could do.If it&#x27;s not done, even in a comment inside the HTML... well, it would be nice if it were :) reply jbc1 4 hours agorootparentNot a good argument for what? reply spookie 2 hours agorootparentFor dismissing crediting someone for their work based on an appeal for the majority replytoddmorey 17 hours agorootparentprevThis is such a good idea. Not sure why svg code escaped my mind as something copilot would be good at. reply pphysch 11 hours agorootparentIn general, copilots are a massive boon to \"boilerplatey\", simple syntax languages from XML&#x2F;HTML to Go. reply bambax 19 hours agoparentprevThe author says he achieved text-to-SVG generation but doesn&#x27;t point to a code repository for it... It would be super interesting (or does gpt-4 do it natively?)That said, I&#x27;m not sure that you need GPT-4 for outlining a BW image and making a path out of it; Corel Draw did that well, over 25 years ago?So yes, another approach to what the author is doing, would be to generate font bitmaps using any of the leading image generators, and then vectorize the bitmaps. Less straightforward and precise, but probably simpler. reply SerCe 13 hours agorootparentHi, I am the author. For text-to-SVG, check out IconShop [1]. It was the paper that I tried to reproduce results from initially. In the paper, there is a comparison of their approach against using GPT-4 [2].Using vectorisation tools like potrace, is indeed a much more popular approach, and there are quite a few papers generating fonts this way. The most recent I believe is DualVector [3]. But I tried to approach the problem from another angle.[1]: https:&#x2F;&#x2F;icon-shop.github.io[2]: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.14400.pdf[3]: https:&#x2F;&#x2F;openaccess.thecvf.com&#x2F;content&#x2F;CVPR2023&#x2F;html&#x2F;Liu_Dual... reply bambax 3 hours agorootparentThank you for the follow-up! Much appreciated! reply simonbw 15 hours agorootparentprevChatGPT&#x2F;GPT-4 does it natively. You can say \"Please generate me an SVG image of a unicorn\" and it will spit out the SVG code. reply cjaybo 11 hours agorootparentHere’s my stupid question of the day:Would you mind to explain what you mean by “native” in this context? reply tough 11 hours agorootparentNot using a -plugin- probably reply specproc 16 hours agoparentprevThank you for sharing that suckerpinch, enjoyed watching that immensely reply logicallee 19 hours agoparentprev>I found a few months ago that the gpt-4 code interpreter is capable of converting a black and white png of a glyph to an svg:) Easy there, let&#x27;s not make all the naysayers who say it only just predicts plausible words sweat.Your phrasing almost makes it sound like you&#x27;re sharing a clear example of it analyzing and completing a complex task correctly, while perfectly understanding what it&#x27;s doing.Perhaps we should say it only just predicted words that are plausible responses to someone asking to do that, while also predicting plausible words someone might say in response to an error message along the way. It might not actually be doing any converting, just predicting words and tokens without really doing anything.My favorite part of its predictive capabilities is how it is able to predict the other half of a conversation that literally goes \"didn&#x27;t work, try again\", \"didn&#x27;t work, try again\", \"still didn&#x27;t work, try again\", \"all right you finally fixed it good job\" - without even telling it why it didn&#x27;t work or quoting the error message. Somehow it is still able to predict the other half of the conversation so that it ends up with \"finally, good job!\"Who knew that to get results that look like it knows what it&#x27;s doing, it&#x27;s enough to predict what could make someone say that!We are truly living in the golden age of statistical prediction that does not involve any degree of thinking, analysis, or understanding.Truly our age of applied statistics is going better than anyone could have, er, \"predicted\". :) reply wizzwizz4 12 hours agorootparent> Your phrasing almost makes it sound like you&#x27;re sharing a clear example of it analyzing and completing a complex task correctly, while perfectly understanding what it&#x27;s doing.OpenAI has hardcoded (or heavily overfit) several special-purpose functions into their ChatGPT systems. In the past few months, they&#x27;ve integrated other special-purpose models, so their tools can do more than just predictive text (e.g. image recognition).GPT can do limited verbal reasoning, whatever else can do image recognition, but that does not mean the combined system can do visual reasoning. There&#x27;s no mechanism by which it would (unless you specifically create one, but that&#x27;s not trivial and doesn&#x27;t generalise).> Who knew that to get results that look like it knows what it&#x27;s doing, it&#x27;s enough to predict what could make someone say that!Everyone. Some call it “specification gaming” or “reward hacking”, and we&#x27;ve known about it for a long time. It&#x27;s a really obvious concept if you have a good mental model of reinforcement learning. https:&#x2F;&#x2F;doi.org&#x2F;10.1162%2Fartl_a_00319 is a fun example.> We are truly living in the golden age of statistical prediction that does not involve any degree of thinking, analysis, or understanding.This is a straw argument. I can&#x27;t speak for anyone else, but my criticisms are mainly of people seeing some thinking-like, analysis-like or understanding-like behaviour, and assuming that it is human-like thinking, analysis or understanding, while ignoring other hypotheses (some of which make successful advance predictions in a way the “it&#x27;s doing what humans do!” models don&#x27;t).I will note: the people being the most loudly exuberant about ChatGPT&#x27;s vast intelligence seem to view it as a tool. If I were faced with an opaque box, inside which was a being capable of general-purpose problem solving, conversation, and original thought, my first reaction would not be “I can use this for my own ends”. I am glad that I have seen nothing to convince me that ChatGPT is such a being, and I have theoretical arguments that ChatGPT probably won&#x27;t ever be such a being, but if you genuinely think this technology has the potential to produce such a being, you have an ethical responsibility. reply logicallee 9 hours agorootparent>>statistical prediction that does not involve any degree of thinking, analysis, or understanding.>This is a straw argument.People say that it does not understand anything, that it just predicts text as though it does.However, I believe they&#x27;re mistaken. I find that it clearly understands things.What do you think? Do you think it understands you when you speak to it? Can it do problem solving or original thought in your opinion? My own anwer is: \"100% it understands me, and 100% yes it can do problem solving and original thought - maybe not world class scientist level but to an impressive extent.\"Clearly it just has a few thinking \"moments\", it can&#x27;t spend hours extensively tackling a problem the way a human can, and it doesn&#x27;t have a memory, nor can plan nor execute large projects by itself or anything like that.But it can, as you say, do \"limited verbal reasoning\", and that is incredibly impressive. reply ddingus 8 hours agorootparentThat mirrors my experiences. One thing in particular that I find notable is to ask it a question after it has made an effort.\"Does this really answer the question\"Or, describe how the information you provided will reach the goal in a possible, practical way.It does understand basics, like me, you, others. And it can handle logic expressions to a degree I find notable. replymartincmartin 17 hours agoprevDouglas Hofstader, the author of Godel Escher Bach, thought the task of creating fonts could only be solved with general AI.https:&#x2F;&#x2F;www.m-u-l-t-i-p-l-i-c-i-t-y.org&#x2F;media&#x2F;pdf&#x2F;Metafont-M...The Letter Spirit project aims to model artistic creativity by designing stylistically uniform \"gridfonts\" (typefaces limited to a grid). reply gwern 16 hours agoparentI read that a while ago and thought that it was interesting: Hofstadter was right that it would require much more general approaches than Knuth&#x27;s approach of &#x27;think very hard and tweak a hand-engineered knob&#x27;, because that&#x27;s how all the past VAE&#x2F;GAN&#x2F;RNN work on typography-related stuff has worked.As for the broader question of whether such approaches are general AI, well, that&#x27;s a bullet Hofstadter is increasingly willing to bite, as upset as it makes him: https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;kAmgdEjq2eYQkB5PP&#x2F;douglas-ho... reply svat 13 hours agorootparentHofstadter&#x27;s article is very interesting and delightful (as is typical of him). But as a response to Knuth&#x27;s article it&#x27;s basically reacting to a straw-man or misunderstanding: by \"a metafont\" in \"The Concept of a Meta-Font\"[1] Knuth simply meant a common description of many related fonts in a family (like the Computer Modern family where different font sizes, bold, italics, sans-serif, typewriter style etc are all generated from common code and tweakable knobs) — this is a consciously chosen and designed family. But when he joked about> The idea of a meta-font should now be clear. But what good is it? The ability to manipulate lots of parameters may be interesting and fun, but does anybody really need a 6⅐-point font that is one fourth of the way between Baskerville and Helvetica?Hofstadter ran with it, imagining Knuth to mean a single universal \"metafont\" from which every single font can be achieved by suitable tweaking of knobs. This is of course nonsense.Knuth wrote a (little-known or referenced) short response in the same journal&#x27;s Vol. 17 No. 4 (1983): Volume 17.4 (p 412, or in the PDF page 89 of 96 at https:&#x2F;&#x2F;journals.uc.edu&#x2F;index.php&#x2F;vl&#x2F;issue&#x2F;view&#x2F;364&#x2F;183) [from the tone I imagine him very annoyed :-)]:> I never meant to imply that all typefaces could usefully be combined into one single meta-font, not even if consideration is restricted to book faces. For example, […] Meanwhile, I&#x27;m pleased to see that my article has stimulated people to have other ideas, even if those ideas have little or no connection with the main point I was trying to make. Misunderstandings of meta-fonts may well prove to be more important than my own simple observations in the long run.Returning to the thread a bit, all these “write code to draw an image” systems—like Metafont&#x2F;MetaPost, Asymptote, TikZ (and also I guess DOT&#x2F;Graphviz, Mermaid, nomnoml, …)—are IMO interesting as a way for those who think in language &#x2F; symbols &#x2F; concepts to do visual stuff (and vice-versa to some extent), and also (along Knuth&#x27;s lines) “truly understand” shapes by translating them into precise descriptions. Metafont was never going to become popular expecting font designers to write code (and the fact that hand-writing SVG is a negligible fraction of usage makes sense), but now that LLMs can help translate back-and-forth, it&#x27;s going to be interesting to see if we ever get to “understanding” shapes.[1]: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20220629082019&#x2F;https:&#x2F;&#x2F;s3-us-wes... &#x2F; https:&#x2F;&#x2F;journals.uc.edu&#x2F;index.php&#x2F;vl&#x2F;article&#x2F;view&#x2F;5329&#x2F;4193 reply adastra22 12 hours agoparentprevWell, GPT is a general AI. reply Jack000 12 hours agoprevI think this approach isn&#x27;t ideal because you&#x27;re representing pixels as 150x150 unique bins. With only 71k fonts it&#x27;s likely a lot of these bins are never used, especially at the corners. Since you&#x27;re quantizing anyways, you might as well use a convnet then trace the output, which would better take advantage of the 2d nature of the pixel data.This kind of reminds me of dalle-1 where the image is represented as 256 image tokens then generated one token at a time. That approach is the most direct way to adapt a causal-LM architecture but it clearly didn&#x27;t make a lot of sense because images don&#x27;t have a natural top-down-left-right order.For vector graphics, the closest analogous concept to pixel-wise convolution would be the Minkowski sum. I wonder if a Minkowski sum-based diffusion model would work for svg images. reply SerCe 10 hours agoparentThank you for the suggestion. A couple of ML engineers with whom I&#x27;ve spoken after publishing the blog also suggested that I should try representing x and y coordinates as separate tokens. reply briandw 12 hours agoparentprevHow would the Minkowski sum be used in the diffusion model? Is the idea to look at the Minkowski sum of the prediction and label? reply Jack000 10 hours agorootparentIn pixel space a convnet uses pixel-wise convolutions and a pixel-kernel. If you represent a vector image as a polygon, the direct equivalent to a convolution would be the Minkowski sum of the vector image and a polygon-kernel.You could start off with a random polygon and the reverse diffusion process would slowly turn it into a text glyph. reply philipwhiuk 19 hours agoprev> To train the model, I assembled a dataset of 71k distinct fonts.I give it a week before Monotype sues your face off. reply ballenf 19 hours agoparentCopyright around fonts may not support such a suit in the same as way as works of art.Wikipedia says: \"In the United States, the shapes of typefaces are not eligible for copyright but may be protected by design patent (although it is rarely applied for, the first US design patent that was ever awarded was for a typeface).[1]\"So just scanning the rendered font (as opposed to the code that generates it), may be harder to stop than scanning of artwork.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intellectual_property_protecti... reply virtue3 17 hours agorootparentThe interesting thing about law is that even if the law doesn&#x27;t absolutely protect you; the person that you believe infringing on your work for free had better be prepared to pony up lawyer fees to defend their work.I think this would be one of the few times where I think that&#x27;s useful. Typefaces take a lot of time and consideration and work to create so just blanket ripping off that work because we all take them for granted is kind of bullshit.I have conflicting thoughts about this.But at the end of the day if you only trained on open fonts they just aren&#x27;t generally as good and the output should be not generally as good as opposed to training on nicer fonts that you technically don&#x27;t have the rights to (but no one thoughts of this being an issue at the time of design patents &#x2F; etc ).But we&#x27;re now in the world where we will pay money to compute an AI model to design fonts instead of just paying designers to design fonts. The race to the bottom is accelerating at an alarming rate. reply jddj 18 hours agorootparentprev> may be harder to stop than scanning of artwork.Which has not been particularly easy to stop either reply bmitc 7 hours agorootparentprevGeneral question. What is the end game of all this? Corporations that have the money and other means are just wildly burning electricity while sucking up the world&#x27;s art and knowledge work.How can anyone possibly protect their work in this no holds barred environment? reply yellow_postit 17 hours agoparentprevFont law is almost as complex and fascinating as Tree law. Given how complex font licensing can be, a generative use case that produces usable fonts would be a huge threat to the foundaries and I expect they will be very litigious, just as Getty and others are in the image space. reply dwaltrip 16 hours agorootparentTree law? Please say more, sounds interesting reply 123pie123 11 hours agorootparentpossibly this? https:&#x2F;&#x2F;www.atlasobscura.com&#x2F;articles&#x2F;tree-law-is-a-gnarly-t.......“It’s never about the trees,” Bonapart says. “The trees often serve as lightning rods for other issues that are the psychological underpinning of a dispute that people might have with each other.” reply xyzelement 10 hours agorootparentFunny - literally doing with this right now - a tree that crosses my boundary with a neighbor dropped a branch on another neighbor&#x27;s house. reply BeFlatXIII 13 hours agoparentprevThat&#x27;s why it&#x27;s so important for the weights to be released to the public ASAP. Even when the original is sued, they can still be passed around in torrents for hobbyists and third-world businessmen to enjoy. reply mock-possum 15 hours agoparentprevNot this agin &#x2F;eyerollIt’s not illegal for a human to look through 71,000 fonts and then creat their own. It can’t be illegal for a human to use a robot to look through the fonts for them. reply raincole 9 hours agorootparent> it can&#x27;t be...Oh boy it totally can. I&#x27;m not saying it it, but it totally can.I feel many people (at least on HN) treat laws like programming. There are some similarities, but programmers like to eliminate arbitrary special cases, while lawmakers love to create arbitrary special cases. It&#x27;s like their only job, really. reply ddingus 8 hours agorootparentIn fact, that failure to grok lawmakers, lies at the heart of many poor decisions made. reply ChristianGeek 11 hours agorootparentprevIt depends on exactly what is learned from looking through them. If you end up copying shapes and segments then there are possible grounds for a lawsuit. If you’re able to determine the rules to make a good font from your analysis, however, then nothing is stopping you from applying them. reply yklcs 17 hours agoprevI’ve tried out some work on generating vector fonts too, in the format of Bezier curves and a seq2seq model. The problem was that fonts outputted by ML models were imprecise. Lines were not perfectly parallel, corners were at 89°, and curves were kinked. It’s not too difficult to get fonts that look good enough, but the imperfections are glaring as fonts are normally perfectly precise. These imperfections are evident in OP’s output too, and in my opinion make these types of models unusable for actual typesetting.A 1% error in a raster output would be pixel colors being slightly off, but a 89° corner in a vector image is immediately noticeable, making this a hard problem to solve. I haven’t looked into this problem too much since, but I’m interested to hear about possible solutions and reading material. reply waterheater 15 hours agoparentWithout changing the fundamental learning process, one could conceivably introduce a \"post-production\" step, where you tighten up the output according to a set of pre-defined rules (e.g., if an angle is 89 degrees, adjust the angle to 90).Of course, changing the learning process would be best. One idea which comes to mind is finding a way to embed relationships into the ML training system itself (e.g., output no angles other than 90 degrees or some predefined set). Such an approach is a type of contraint-based ML, where the ML agent identifies a solution given certain constraints on the output. In my experience, the right approach to accomplish this goal is using factor graphs. reply Rantenki 15 hours agoprevOK, that&#x27;s cool, but those fonts are all terrible. The serifs are all different sizes and shapes, sometimes on the same letter. The kerning looks like a random walk. The stroke widths are all over the place, and&#x2F;or the hinting is busted.Now, that said, it&#x27;s pretty amazing that this works at all, but it&#x27;ll take some pretty specific training on a model to get something that can compete with a human made font that&#x27;s curated for good usability _and_ aesthetics.Sadly, we&#x27;ll also probably see adoption of these kinds of fonts (along with graphic design, illustration, songwriting, screenwriting, etc)... because \"meh, good enough\" combined with some Dunning-Kruger.TL;DR: Thanks, I hate it. reply jeron 15 hours agoparentI don&#x27;t think any self respecting graphic designer would use these fonts in its current state but it&#x27;s a cool proof of concept and could be improved upon to a more usable state reply BoorishBears 15 hours agoparentprev> Sadly, we&#x27;ll also probably see adoption of these kinds of fonts (along with graphic design, illustration, songwriting, screenwriting, etc)... because \"meh, good enough\" combined with some Dunning-Kruger.Ironic bringing up Dunning-Kruger as you treat generic RLHF as a \"pretty specific training\" and make sweeping declarations about how people will use AI as if the current SOTA of several of the tasks you just mentioned didn&#x27;t come from not settling for \"meh, good enough\" and instead applying the \"pretty specific training\" you alluded to (see Midjourney) reply Rantenki 9 hours agorootparentI never mentioned reinforcement learning, and my DK statement was completely around using flawed fonts for graphic design, etc.My partner _is_ a professional graphic designer, and we _have_ seen some pretty terrible client graphics that came out of Midjourney. They&#x27;re amazing for what they are, but it&#x27;s very difficult to get something out of it that competes with a professional illustrator, even ignoring the whole copyrighted content in the model issue. reply BoorishBears 8 hours agorootparentReinforcement learning from human feeedback is the training you&#x27;re referring to, you just don&#x27;t realize it.RLHF is why 2 years ago \"They&#x27;re amazing for what they are\" would have been \"They&#x27;re so hideous no one in their right mind would use them\", and why in 2 years that too will be some weaker form of argument.There&#x27;s no special knowledge needed to know \"I like X over Y\": RLHF allows a model to turn that into guidance at a scale that&#x27;s never been possible before. reply scarygliders 20 hours agoprevHmmm. The model is a ckpt instead of a safetensor.Pondering on whether to keep proceeding trying this out or not...EDIT: a scan with picklescan[0] found nothing.. exciting.[0] https:&#x2F;&#x2F;github.com&#x2F;mmaitre314&#x2F;picklescan reply 7moritz7 20 hours agoparentHaven&#x27;t seen a single maliscious ckpt file so far. Sure, there is a possibility, but huggingface scans pickled weights automatically so the likelihood of someone using that site to spread malware in this form is super low reply scarygliders 19 hours agorootparentI&#x27;ve never spotted one in the wild either, but, y&#x27;know, I like to not be the one who first finds one out... the bad way. ;) reply artursapek 19 hours agorootparentprev“pickled weights”?serious question, how on Earth should someone like me, who has completely missed the last 12 months of AI development, catch up with the state of the art? reply omneity 18 hours agorootparentTwo separate terms here, pickling is a serialization method for Python objects (unrelated to AI per se).Read more here: https:&#x2F;&#x2F;docs.python.org&#x2F;3&#x2F;library&#x2F;pickle.htmlThen \"weights\" is just referring to a model&#x27;s weights, a specific instance of a python object that can be pickled. reply simbolit 19 hours agorootparentprevI suppose you being here means that you are already fluent in some programming languages. If so, I would start here:Conway & Miles - Machine Learning for Hackers: Case Studies and Algorithms to Get You StartedOnce you read and understood this, I&#x27;d do an online course... reply artursapek 12 hours agorootparentthank you reply scarygliders 19 hours agorootparentprevJust know that the .ckpt format has more or less been replaced by .safetensors these days.tl;dr .ckpt files can contain Python pickles containing runnable Python code, which means a Bad Guy could create a .ckpt model containing malicious python code. Basically. reply eurekin 20 hours agoparentprevproxmox&#x2F;virtualbox&#x2F;qemu + throwaway vm reply scarygliders 20 hours agorootparentQuite, I was thinking about doing so.I just scanned it with picklescan, which found nothing malicious. I just updated my original reply. reply PaulHoule 20 hours agoprevKinda funny how it works well at this whereas diffusion models go to die when it comes to drawing text but of course it works in a completely different manner. reply TheRealPomax 19 hours agoparentThere&#x27;s a huge difference between \"pictures of letters\" and \"writing text\" though. Ask stable diffusion to write text and it&#x27;ll generate hilarious weird-looking results. But, ask it to generate individual letters (e.g. \"Show me an ornate uppercase letter b\") and it&#x27;ll do that for you with (mostly) no problems. reply ilaksh 17 hours agoparentprevSDXL can do text kind of. Also isn&#x27;t DALLE-3 a diffusion model?But yeah overall diffusion has not generally been able to do it at all before. reply gwern 16 hours agorootparent> But yeah overall diffusion has not generally been able to do it at all before.Imagen&#x2F;Parti were doing text just fine long before DALL-E 3 was announced. GANs were also learning some text in the earlier runup (even ProGAN was doing striking &#x27;moon runes&#x27; - amusingly, they were complete gibberish because it did mirroring data augmentation). reply scarygliders 19 hours agoprevOkay I can&#x27;t try it out anyway. \"Blocksparse is not available: the current GPU does not expose Tensor cores\"My \"best\" GPU is an RTX 2070 Super, Turing architecture.I&#x27;ve seen similar messages when using stable-diffusion... either with -webui or with automatic, can&#x27;t exactly remember, but they both run fine on that RTX 2070 Super, so I can only guess that they revert to some other method than Blocksparse on seeing that it doesn&#x27;t support Turing. Or something. I haven&#x27;t looked into how they deal with it.I&#x27;ve submitted an Issue [0] for it. I don&#x27;t have enough knowledge to know if there&#x27;s some way of saying \"don&#x27;t use Blocksparse\" for fontogen.[0] https:&#x2F;&#x2F;github.com&#x2F;SerCeMan&#x2F;fontogen&#x2F;issues&#x2F;2 reply logdahl 20 hours agoprevCool! Now generate &#x27;upper-uppercase&#x27; and see what happens :^) reply matsemann 19 hours agoparentI think this is a reference to \"Uppestcase and Lowestcase Letters\", a submission a while back about someone training a ML model to generate lowercase&#x2F;uppercase letters, and used it to uppercase letters already in uppercase. Quite fun https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26667852 reply TheRealPomax 19 hours agoprevNeat! Does it have prompt capabilities for things like FVAR, GSUB, and GPOS? E.g. \"okay now include a many-to-one ligature that turns the word &#x27;chicken&#x27; into an emoji of a chicken in the same style\" or \"now make a second, sans-serif, robotic style and add an axis called interpol that varies the font from the style we just made to this new style\"? reply simbolit 19 hours agoparentNot OP, but the answer is \"no\".What exactly made you suspect such abilities? reply TheRealPomax 19 hours agorootparentOdd phrasing, but: the part where I&#x27;ve worked on OpenType parsing for decades and love seeing people with a passion for digital typefaces make new and creative tools in that space. Typically folks don&#x27;t stop working on a cool tool after they write a blog post, they&#x27;re still refining and extending, so you never know how far someone is trying to take a tool without asking them. reply simbolit 19 hours agorootparentSo, if I understand you correctly, it was less a question \"does it do x?\" and more an indirect form of \"hey OP, would be cool if it did X\" ? reply TheRealPomax 19 hours agorootparentThis is not the place for starting a discussion about whether context and subtext should be implicit or explicit in written English. That&#x27;s what https:&#x2F;&#x2F;philosophy.stackexchange.com is for. reply simbolit 18 hours agorootparentI don&#x27;t want to start a discussion, I wanted to know if I misread your original comment, and whether you meant something different from what I thought you meant.From your answer, tho very indirect, I now suspect that I did misunderstand your initial comment, and answering your question was missing the point.That is all I wanted (after at first wanting to be helpful). reply TheRealPomax 17 hours agorootparentFair enough, I thought you were trolling, but you&#x27;ve made it clear you weren&#x27;t. I wrote my comment as a question that would hopefully engage the author on the capabilities (both concrete, as well as hypothetical) of this approach to font generation. replylawlessone 19 hours agoprevThis is interesting but i think generating the next letter from the letters before may not be the best way to do it. As you mentioned they degrade with each letter.Maybe creating one long image of a whole font would work better.edit: in the above am misunderstanding what is happening here.But i still think there must be another way to structure this so the attention mechanism doesn&#x27;t have to work so hard. reply jrmg 18 hours agoparentSince the first three letters are good, and generated only with the context of the preceding letters, shouldn’t just using the first three (instead of the preceding three) as context for every other one be good enough? reply dleeftink 20 hours agoprevAlthough I would be sad to see the handcrafting that goes into designing custom fonts go, some iterations down the line a model like this would greatly aid tedious glyph alignment and consistency tasks when designing CJK, hiragana, katakana and kanji fonts. Inspiring stuff. reply california-og 20 hours agoparentI think that would be ideal. The &#x27;killer&#x27; feature would be: Handcraft a set of control characters, like the letters in \"handglove\" and then let AI generate the rest. Designing a typeface is fun, until you need to add support for multiple languages and need to make 800+ characters. Or, maybe there is a nice (open source) font, that is unfortunately missing some characters you really need: let AI generate them. reply matsemann 19 hours agoparentprevIt&#x27;s already so that writing on computers is quite US-centric (at least English-centric). While this might help on some of the shortcomings, I&#x27;m also a bit afraid it will make it so that even more focus is put only on the US part, and the rest of the world get a \"good enough\" implementation made by AI that kinda erases some heritage. reply dleeftink 19 hours agorootparentMaybe, but the Latin font market is quite saturated, whereas the CJK space has ample opportunity for innovating and is likely even in need of it, cf. [0][1][2][0]: https:&#x2F;&#x2F;qz.com&#x2F;522079&#x2F;the-long-incredibly-tortuous-and-fasci...[1]: https:&#x2F;&#x2F;fonts.google.com&#x2F;knowledge&#x2F;type_in_china_japan_and_k...[2]: https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;14573813 reply mastersummoner 15 hours agoprevPoof! You&#x27;re a font. reply paulcnichols 17 hours agoprevInevitable in a good way. Keep going! There&#x27;s gold here. reply gigglesupstairs 17 hours agoprev\"Fucking Hell\" - first thing I yelled to myself when I saw that headlineKudos for the project, of course, but it just saddens me a bit more. Nothing is sacred anymore. reply layer8 16 hours agoparentI mean, looking at the kerning of the second example in particular, there’s still a lot to be done. And something like “extend this latin-1 font to all scripts of the BMP so that is looks stylistically consistent and, within that constraint, the glyphs and their combinations look natural and readable for native readers of each script, assuming Japanese readers for the Han characters” is probably still way off. reply chefandy 15 hours agorootparentJust like all visual generative AI, it gets the first 95% but doesn&#x27;t get the last 5% that takes 95% of the time. Kerning pairs on typefaces take an incredible amount of human time. Years of full-time work for a large type family. After all these years, even Adobe can&#x27;t perfectly automate kerning because making letters look right next to each other isn&#x27;t (obviously) formulaic. Maybe generative AI will nip it in the bud? Certainly hasn&#x27;t so far, but maybe it will. Obviously in monospaced fonts, like that last one, kerning isn&#x27;t an issue.More correctable in these models would be the balance between the letterforms. Surely if there was some kind of prompt you could tell it to not make those Ms in that bold serif font to be obnoxiously wide?Either way, as of now, what this gets us is exactly 90% less useful and probably of lower quality than the stuff you can get for free on dafont.com. I know it will progress, but I imagine the best use case for generative AI and font creation for commercially viable fonts would be to give roughs glyphs to fill out a large character set as an aid for a professional type designer.And surely there will be a chorus of people insisting that it doesn&#x27;t matter. Well, you&#x27;re wrong. If you blindly showed people a headline, book, poster or whatever with properly kerned type and then one without, they will see how much more polished the properly kerned page is, even if they couldn&#x27;t tell you specifically why. In a lot of situations, that really, really matters, even to people who haven&#x27;t developed the ability to point out the differences. reply waterheater 15 hours agoparentprevLet&#x27;s assume the technology will eventually work.What if you had a \"personal font\"? Sure, you have a user name, but what if you had a custom-generated font which communicates your personality to other people on the Internet? The font could be on a spectrum between static (generated once and reused indefinitely) and dynamic (continuous online learning of personal information causes an adjustment of the font).I&#x27;m just making up an example here, but say you&#x27;re feeling sad, and your smart technology figures out you&#x27;re feeling sad. When you send a text message to family, then your personal font takes on \"sad\" characteristics. reply notatoad 7 hours agoparentprevwere fonts ever sacred? monotype has made a whole business off of making helvetica and times new roman alternatives that are basically indistinguishable to the originals but don&#x27;t require licencing fees.that seems like exactly the sort of business that deserves to be taken over by AI. reply rogerclark 16 hours agoparentprevIt&#x27;s so depressing to think that this is what people want. reply tpmoney 13 hours agorootparentWhat is that? The ability to quickly and easily generate creative or expressive pieces of computer wizardry without first having to delve into the depths of esoteric knowledge? Of course people want that. It turns out you can’t specialize in everything, but sometimes you just want to be able to make something good enough without having to engage the services of an expert in the field.No this might not be the most beautiful font with the most perfect kerning or optimized code. But if it’s functional enough for the person who requests it, that should be good enough shouldn’t it? Most things people are printing on their 3d printers aren’t high quality designed parts either. Plenty of scientists and accountants have scripts and code that would make most developers cringe, but if it’s good enough then why be bothered?The ability of people to make things with tools that they otherwise never would have been able to make before without dedicating months or years of time they may not have is awesome and we should be excited for it. I’ve watched 70 year old grandmothers learn to make little home movies of their grandkids in iMovie. No they weren’t doing “real” film editing and certainly weren’t learning any skills that would transfer to avid or Final Cut. And so what? That home movie cut together with a minimum of skill and a whole lot of technology hiding the esoterica was probably more meaningful and joy inducing for that woman than most blockbuster cinematics produced by the best minds. reply rogerclark 8 hours agorootparentDo you wholeheartedly believe this and can you truly not understand why others might be uncomfortable with the direction of generative AI? Defending your viewpoint makes sense, but I don&#x27;t buy the idea that the other stance is completely incomprehensible.The grandma doesn&#x27;t need generative fonts. She already has fonts that ship with her phone and computer. She already has iMovie for her grandkid videos. Does she need the ability to generate videos of political figures saying things they didn&#x27;t say?Obviously, what concerns us is a race to the bottom for the value of all human output. reply nvy 17 hours agoparentprev>Nothing is sacred anymore.If it can be specified, it can be automated. reply gigglesupstairs 16 hours agorootparentI know, I know. I am not disputing the technicalities. reply mock-possum 15 hours agoparentprev‘Anymore’ haYou don’t actually believe anything was ever sacred to being with do you? reply colesantiago 16 hours agoparentprev> Kudos for the project, of course, but it just saddens me a bit more. Nothing is sacred anymore.Why does this sadden you?I&#x27;m quite happy everything is being done by AI, time will be freed for other things that are more important.Manual font making will not go away though and now anyone can make their own fonts for free. reply rogerclark 16 hours agorootparentYou don&#x27;t know that the time will be freed for other things that are more important. We don&#x27;t know for sure how this is all going to work out at all.And people who make fonts, create art, and write prose generally do these things because they like doing them, not because they&#x27;re forced to. These technologies aren&#x27;t automating drudgery, they&#x27;re automating things that give people&#x27;s lives meaning. What&#x27;s the endgame here exactly? reply nvy 14 hours agorootparent>What&#x27;s the endgame here exactly?All of us paying a set of subscriptions to the FAANGs, for literally every aspect of our lives. reply aatd86 13 hours agorootparentWith what money once everyone is out of a job? reply colesantiago 16 hours agorootparentprevTime will be freed, ChatGPT, DALL-E, Midjourney and Stable Diffusion has collectively saved countless people billions of hours of time and this will do the same.The big font makers no longer have a hold on extremely pricey fonts that are inaccessible, the general endgame is most software is going free and open source thanks to AI, and that is a good thing. reply rogerclark 16 hours agorootparentCreatives will have their hopes and dreams stripped away so that artless and tasteless software engineers can type words into a box and instantly get exactly what they want, with no surprises, no feelings, and no economic upsides for anyone else. A beautiful future indeed. reply axus 16 hours agorootparentWon&#x27;t the creatives be able to type the software specification into a box, and add functionality to their endeavor without needing programmers? I&#x27;m not sure that the process and the paycheck are more important than the final artifact. reply ori_b 14 hours agorootparentWhy would we need people to have endeavors? That sounds like automatable drudgery.Won&#x27;t the AIs be able to infer what would best maximize engagement for their owners and type the specifications necessary to create whatever the entity running them would want their users to consume? reply morph12 13 hours agorootparentWhy would we need people? Seems like a pointless bottleneck in the pursuit of efficiency. reply BeFlatXIII 13 hours agorootparentprevDesigning cereal boxes is not authentic human experssion. reply rogerclark 11 hours agorootparentYou must not know any designers. Pretty much everyone I know would consider that to be pretty fun – this is exactly the kind of thing artistic kids say they want to do when they grow up. And most of us would rather get paid to design cereal boxes than to do many other things, and almost everyone would rather do it than to not get paid at all. reply morph12 16 hours agorootparentprevTime for what? reply ori_b 14 hours agorootparentprevGenuine question: What do you think is more important that won&#x27;t eventually be done with AI? reply euroderf 16 hours agoprevHas anyone tried using an LLM to make a font based on their handwriting ?EDIT: There&#x27;s a couple (IIRC) of online services that offer this. reply OnlyMortal 16 hours agoparentIf it was my handwriting, it wouldn’t be popular.Perhaps a cursive font might be good though I’m pretty sure one exists.An expert system might be able to join up the letters in cursive and make intentional mistakes to give it the character of natural handwriting? reply rogual 16 hours agoprev> THE QUICK BROWN FOX JUMPS OVER THE LAZY DOGIt&#x27;s \"a\" quick brown fox, otherwise the sentence has no \"a\". reply jnosCo 16 hours agoparentThere is an \"a\" in \"lazy\"? reply rogual 16 hours agorootparentWell, I&#x27;m stupid. reply jameshart 15 hours agorootparentThe usual mistake people make in reciting this is to say the fox jumped over the lazy dog, causing them to omit an ‘s’ from the sentence.Making sure it’s ‘the’ lazy dog rather than ‘a’ lazy dog is actually important if you care about completing the lowercase alphabet, as without it there’s only an uppercase ‘T’. reply SamBam 16 hours agorootparentprevIndeed.But it&#x27;s worth having \"a lazy\" anyway to avoid the repetetive \"the.\" reply dave78 16 hours agoparentprevHuh, I never gave that sentence much thought, and I guess I never realized it conveniently covered the whole alphabet. It makes so much more sense now! reply sgerenser 10 hours agorootparentI liked the one that Apple used to use in the MacOS 7&#x2F;8 era: “How razorback-jumping frogs level six piqued gymnasts!” (Might be slightly off but its unique enough that I remembered it at least 20 years after it disappeared from MacOS). reply lordfrito 16 hours agoparentprevLazy? reply dexsst 19 hours agoprevI used to make some fonts for rare, non Latin alphabets like the Orkhon script by hand using a Paint-like freewar, it was fun reply Nevermark 18 hours agoprevIn honor of all the times he pressed his hands into his eyes (and myself doing the same thing):I present: “Perplexed” by Nilsa. [0]I have a print in my office, in lieu of a mirror.[0] https:&#x2F;&#x2F;www.sargentsfineart.com&#x2F;img&#x2F;nisla&#x2F;all&#x2F;nisla-perplexe... reply RugnirViking 20 hours agoprevOoh I have to try this out when I get home, looks like the weights are under 1GB too reply matt3210 10 hours agoprevGranted... You are now a font! reply boffinAudio 20 hours agoprevI&#x27;ve long had a project in mind involving the various typefaces of the signage around the city of Vienna, which I find very inspiring in many cases.The idea is to just take a picture of every different typeface I can find, attached to the local buildings at street level.There are some truly wonderful typefaces out there, on signage dating back to last century, and I find the aesthetics often quite appealing.With this tool, could I take a collection of the various typefaces I&#x27;ve captured, and get it to complete the font, such that a sign that only has a few of the required characters could be &#x27;completed&#x27; in the same style?Because if so, I&#x27;m going to start taking more pictures of Vienna&#x27;s wonderful types .. reply chris_st 19 hours agoparentEven if you never get around to using the photos, I think it would be a wonderful service to take the photos and put them up somewhere for non-Vienna residents to enjoy. reply boffinAudio 19 hours agorootparentOh, definitely .. but first I must amass an archive worthy of it .. reply simbolit 19 hours agoparentprevWith this tool: no.With a next-gen tool: if you do some pre-processing on the images, quite possibly. reply kleiba 15 hours agoprevObligatory xkcd reference: https:&#x2F;&#x2F;xkcd.com&#x2F;1015&#x2F; reply nitrofurano 20 hours agoprevlots of kernings misfits ftw reply tabtab 18 hours agoparentStop your bigotry of Kernians. reply andybak 17 hours agoprevEveryone knows that AIs can&#x27;t draw sans... reply itsyaboi 18 hours agoprev [–] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=a8K6QUPmv8Q replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author recounts their experience of developing generative Machine Learning models to teach a computer to create fonts, in a project called 'FontoGen'.",
      "The article provides an in-depth look at the challenges of generating fonts and explains the model structure, involving the use of pre-trained BERT and font embeddings.",
      "The post concludes with the author's enthusiasm about the project's potential and the prospects of future applications in the realm of artificial intelligence and font generation."
    ],
    "commentSummary": [
      "The conversation is centered on the application of AI, particularly GPT-4 code interpreter, to transform black and white glyphs into SVG to produce fonts.",
      "Participants hold varied viewpoints on the potential and restrictions of AI-generated fonts, coupled with the prospective effects on the design industry including copyright concerns.",
      "Apprehensions about the undermining of craftsmanship and the potential reduction in the value of human work are counterbalanced by enthusiasm for the opportunities and accessibility provided by AI in font construction."
    ],
    "points": 296,
    "commentCount": 129,
    "retryCount": 0,
    "time": 1696335453
  },
  {
    "id": 37759871,
    "title": "BBC gives up on Threads, sticks with Mastodon",
    "originLink": "https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon",
    "originBody": "Darnell (Seize The Day) BBC Gives Up On Threads (By Instagram), Sticks With Mastodon October 2, 2023 So numerous brands are giving up on Threads by Instagram, allegedly due to lack of engagement (ironically, most of them are still using X, formally known as Twitter). What makes this news more interesting is the fact that the British Broadcasting Corporation (BBC) has abandoned its Threads account but is still maintaining its self-hosted Mastodon accounts online. The National Football League hasn't posted anything in six weeks, before the start of the regular season. This is the nation's most popular sports league, and it has completely abandoned Meta's new platform. Even with its 1.9 million followers. Among news publishers, the British Broadcasting Corporation (BBC) stopped postingto Threads 11 weeks ago, not long after the launch. CBS News hasn't posted in five weeks. (KTLA5) Earlier in the year (at the end of July), the BBC announced that they were engaging in a six-month experiment with Mastodon, & even self-hosted their content on their custom domain: Social.bbc (how cool is that!). While accounts like @BBC5Live@social.bbc & @BBCRadio4@social.bbc remain relatively active, @bbc@threads.net appears abandoned, despite boasting far higher engagement levels on Meta’s Twitter rival (now called X). But why would the BBC abandon Threads while maintaining a presence on Mastodon as well as other social media platforms‽ There are three possible theories: BBC might be weary of posting content to Threads after Meta geo-banned Canadian news outlets after Canadian legislators passed a “news tax” law (note: the law is silly, but I digress). BBC prefers to control its social narrative, & will redirect followers from Threads to their Mastodon accounts once Threads joins the Fediverse. BBC thinks Threads is a waste of time & resources as Threads lacks a public API to help automate posting news to their account. Could the BBC eventually shut down their Mastodon account by the time 2024 arrives‽ Sure. But I suspect that with X dying, Threads stagnating, & Meta raging, the BBC might keep their Mastodon account active to avoid having their voice limited online. 👨🏾💻 by Darnell Clayton 🔛 @darnell@darnell.day 🕺🏾 Follow my adventures upon: 👉🏾 @darnell@one.darnell.one 🐘 (Mastodon) 👉🏾 @darnell@darnell.moe 🦁 (Misskey) 👉🏾 @darnell@threads.net 🧵 (Threads) 🦹🏾♂ Other digital havens: 👉🏾 @darnell@darnell.app 📸 (Pixelfed) 👉🏾 @darnelltv@darnell.tv 👨🏾💻 (WordPress) 👉🏾 @darnell@counter.social 🥷🏾 (Counter Social) published with write.as",
    "commentLink": "https://news.ycombinator.com/item?id=37759871",
    "commentBody": "BBC gives up on Threads, sticks with MastodonHacker NewspastloginBBC gives up on Threads, sticks with Mastodon (darnell.day) 292 points by cmrdporcupine 8 hours ago| hidepastfavorite155 comments perfectstorm 5 hours agoI lost interest in Threads once their product guy said news is not a priority for them. One of the main reasons I use Twitter to catch up on world events from trustworthy sources.Also earthquakes. Living in the California, I&#x27;m actually very impressed with Twitter&#x27;s trending searches. I can always find few people tweeting about earthquake within 10seconds of an earthquake. reply threeseed 5 hours agoparentIf you listen to Zuckerberg&#x27;s interview with Alex Heath it&#x27;s clear what they meant.They are interested in breaking news, sports etc. They are just trying to avoid the platform turning into the polarising mess that Twitter is right now. And so they&#x27;ve been trying to position the app as more holistic than just politics and news.Hashtags and trending searches are next on their roadmap. And I suspect that there is some technical work on their part to upgrade the underlying Instagram platform to support more real-time updates to the feed. Once that happens news will inevitably become a bigger part of Threads. reply pfist 4 hours agorootparentI like the UI design of Threads and I’m trying to enjoy the app, but it keeps flooding my feed with desperate, attention-seeking women in suggestive clothing and positions. I never have this problem on X. reply somedude895 3 hours agorootparentI don‘t use Instagram often, but I recently met a girl irl who wanted to connect. I open the search in the app and it instantly recommends exclusively girls showing off their bodies, even though I only follow friends otherwise. It was quite embarrassing. reply te_chris 3 hours agorootparentStop looking at thirsty girls on the gram then. In my experience it shows you more of what you look at. reply dividedbyzero 2 hours agorootparentIt&#x27;s showing me a lot of luxury handbag influencers that obviously target a very different demographic. I&#x27;m absolutely sure I never looked for anything like that (I follow mostly science&#x2F;scientist accounts and comics and I&#x27;m male). I also keep getting ads for commercial pilot insurance and I&#x27;m definitely not in that demographic either. Instagram&#x27;s recommendation model seems to work well if you fit a common categorization well (women interested in luxury handbags perhaps), and utterly break down when you don&#x27;t. I&#x27;m a software engineer, I don&#x27;t need specialty carts to move Boeing aircraft engines. reply JD557 2 hours agorootparentprevAs a very casual Instagram user, I had a similar experience to the parent comment (not so much now).From what I can tell, this is the way Instagram recommendations treat cold start users. It was only when I started liking stuff that they went away. reply CM30 1 hour agorootparentprevSame here. The algorithm seems to be really bad at recommending relevant content on the &#x27;for you&#x27; style timeline, and barely seems to take my likes and follows into account at all... reply Lolaccount 1 hour agorootparentprevYou mean, Twitter.I had the opposite experience. I never had this problem either, until all the firings happened.After that, either someone wasn&#x27;t looking after the thirst traps, or they actively changed the throttle of them so they kept appearing.That was about a month before I left that cess pit. reply andreygrehov 3 hours agorootparentprev> They are just trying to avoid the platform turning into the polarising mess that Twitter is right now.What if that is the actual state of the country and Twitter is just reflecting it? reply spookie 2 hours agorootparentThe real world is a lot saner, and beautiful reply sph 2 hours agorootparentI am more and more convinced that the Internet is a net negative for discourse and human socialisation, and no one is really exploring this possibility. We are all trying to make the social media experiment work and I am horrified by its result. The more we tweak and add to it (upvotes, user moderators, likes, retweets, the Algorithm), the worse it gets.We&#x27;re so used to the fast pace of modern science that we forget cultural evolution sometimes takes hundreds of years. What if we are 500 years before enough Internet philosophers have taught us how to behave online? reply robertlagrant 1 hour agorootparent> I am more and more convinced that the Internet is a net negative for discourse and human socialisation, and no one is really exploring this possibility.A huge amount has been written and spoken about this. reply sph 1 hour agorootparentApparently, not enough, or not forceful enough for a contingent of people to decide to quit the Internet for good. Where are the modern day anti-Internet luddites? Talking about it on a blog or, worse, posting about it on Twitter doesn&#x27;t count, sorry. reply robertlagrant 1 hour agorootparent> Where are the modern day anti-Internet luddites?They probably don&#x27;t use the Internet. reply Maken 1 hour agorootparentprevYou don&#x27;t go outside too much, don&#x27;t you? reply leosanchez 1 hour agorootparentYou don&#x27;t read Twitter comments do you? reply goatlover 3 hours agorootparentprevAnd what if Twitter is partly causing it (along with other media). reply somenameforme 57 minutes agorootparentYou can also go the other way. There&#x27;s always going to be the possibility of a chicken&#x2F;egg type question, but it seems clear that a lot of the insanity and division started to happen about the time people started trying to control (and started caring so much about) what other people say and think. It just feels like there was this sudden flip of a switch where suddenly there was this idea that if you control &#x27;the message&#x27;, you can control what people think, and a certain group of people vigorously pursued this.It&#x27;s not hard to see how this is going to drive people into deeply divided groups. You immediately end up with an in-group and an out-group, both with good reason to avoid the other. And then in this division, both groups characterization of the other drifts further and further from reality, further cementing such divisions. This site [1] demonstrating the scale of the perception gap among various groups is quite telling. I don&#x27;t think it was like this, at all, not that long ago.[1] - https:&#x2F;&#x2F;perceptiongap.us&#x2F; reply nonrandomstring 2 minutes agorootparent> It just feels like there was this sudden flip of a switch where suddenly there was this idea that if you control &#x27;the message&#x27;, you can control what people think, and a certain group of people vigorously pursued this.I locate this moment quite precisely. It was when either Richard Perle or Donald Rumsfeld responded to a journalist&#x27;s live TV question about &#x27;the reality&#x27; in Iraq II, and said \"We control reality\". That was when narrative paradigm took a dark turn in the West and I see the PNAC as taking internal psyops to a whole new, overt level.All politicians _know_ this, but you never _say it_ !!Bush, Cheney, Rumsfeld, Wolfowitz, Perle and company publicly burned the precious veneer of good faith.They didn&#x27;t just want to construct the Iraq War, they needed to rub our noses in the blatant cheek of it.That was the birth of \"post truth\" for me.After that it naturally followed that every organisation, business, or individual could legitimately construct their own \"narrative reality\" through social media or whatever, without any regard for facts. malermeister 3 hours agorootparentprevIt&#x27;s both, in a feedback loop from hell. reply basisword 1 hour agorootparentprev>> They are interested in breaking news, sports etc. They are just trying to avoid the platform turning into the polarising mess that Twitter is right now.They&#x27;re right to do this too. I enjoyed Threads drama free for a few days. The first time I saw a post from a journalist - it was drama. Complaining about how important news is and how it&#x27;s morally wrong of Meta to not be prioritising it. So obviously self-serving too. reply bagels 1 hour agorootparentThe irony, people whine when shown rage bait, people whine when not shown rage bait. reply sph 2 hours agorootparentprevThey want to avoid recreating the polarising mess that is Twitter... by copying Twitter wholesale? reply DoesntMatter22 5 hours agorootparentprevThreads is dead at this point realistically reply threeseed 5 hours agorootparentNot according to Zuckerberg [1]. He was happy with DAU numbers and planned to commit to the app long-term.Threads is contributing a huge amount of behavioural data to Instagram ad profiles so it may never need to run ads in order to provide significant value to the company. That&#x27;s going to place X at a disadvantage over the long term.[1] https:&#x2F;&#x2F;www.theverge.com&#x2F;23889057&#x2F;mark-zuckerberg-meta-ai-el... reply marcus_holmes 4 hours agorootparentBut this is standard CEO-speak. He will remain completely happy with the app, committed to it going forward, and a personal power-user of it. Right up until the moment it&#x27;s abandoned completely with no warning. reply TheAceOfHearts 5 hours agorootparentprevInstagram is also embedding Threads posts directly, with a link to open them in the app. I&#x27;ve gotten a few prompts while using Instagram to remind me to open Threads.However, every time I open the app it&#x27;s basically a ghost town. I personally don&#x27;t know anyone using it, so I just see a bunch of brand and celebrity posts.Threads feels more like a platform for brands than for regular people. reply Gigachad 4 hours agorootparentAlmost everyone I know is using Bluesky now. I’ve yet to see anyone using threads. reply vsnf 2 hours agorootparentMust be nice to live in a bubble where Bluesky isn&#x27;t gated behind a waitlist and your entire circle can move wholesale with ease. reply numpad0 13 minutes agorootparentWaitlist is a distraction. Ask for a code from core(\"terminally online\") Twitter users. I got mine on Nostr from someone not a furry. Perhaps you can ask around in the Fediverse. reply Gigachad 1 hour agorootparentprevAccess to invite codes is abundant in the furry community. Can just ask anyone you know for one. reply fortyseven 4 hours agorootparentprevNot sure where this optimism comes from, but alright. reply threeseed 4 hours agorootparentBecause this is Meta we are talking about. They know how to build social networks.And they have the cash to keep Threads going for decades whereas X is in a financial hole. Linda is meeting with creditors [1] who are freaking out over the drop in valuation and she doesn&#x27;t even know whether the company plans to be advertising or subscription driven.[1] https:&#x2F;&#x2F;www.ft.com&#x2F;content&#x2F;474b30e9-a726-4571-8475-9adabbbc8... reply yjftsjthsd-h 4 hours agorootparent> Because this is Meta we are talking about. They know how to build social networks.Meta, the company currently named after their attempted pivot to a VR social network that utterly bombed? I&#x27;ll certainly grant that they built one social network, but it is less obvious to me that they can repeat it on command. reply brnt 4 hours agorootparentprev> Because this is Meta we are talking about. They know how to build social networks.Not really. Zuckerburg got lucky once, the rest was purchased. reply rafark 3 hours agorootparentYou have to have a talent for that though. How many companies have made terrible purchases?Like him or hate him, Zuckerberg’s strategies have been great for his company. I don’t know about his VR business, but he’s like a Midas king when it comes to social media. Everything social media related he’s touched has turned into gold. Instagram and WhatsApp were already kind of big, but they’re were not the ubiquitous behemoths they are today.Threads could be his first big fail though. reply marcus_holmes 3 hours agorootparentBeware survivor bias. How many acquisitions has Facebook&#x2F;Meta made? (answer: a lot). How many are social behemoths? (answer: 2). We don&#x27;t know how many he tried to make successful and failed at, because we don&#x27;t hear of them, or don&#x27;t remember them.The whole Oculus mess does seem to indicate he doesn&#x27;t have a Midas touch. And let&#x27;s not forget the metaverse, which isn&#x27;t going at all well. reply threeseed 1 hour agorootparentThey have 5 successful social platforms:Facebook, Instagram, WhatsApp, Workplace, Threads.The latter has reached 130m+ users after all. reply marcus_holmes 1 hour agorootparentNever heard of Workplace before (unless that&#x27;s the metaverse thing, in which case it&#x27;s clearly not successful). And \"threads is successful\" is dubious: the key point of the article is that it isn&#x27;t. reply numpad0 25 minutes agorootparentWorkplace is private Facebook for large orgs. So just Facebook. Doesn&#x27;t seem like a total flop, though.I&#x27;d rather think that Facebook had been consistently successful in catering its cohort. That targeted cohort might just have too little overlap with users in market for Twitter&#x2F;Mastodon&#x2F;Threads, but as far as Facebook&#x2F;Instagram(post-acquisition) are concerned the users seem satisfied with Meta-run social media to me.charcircuit 21 minutes agorootparentprev>the rest was purchasedInstagram had 30 million users at the time of acquisition. Scaling that to billions of users is not trivial nor is it inevitable. reply brnt 13 minutes agorootparentWhatsApp, another Meta purchase, begs to differ. antupis 3 hours agorootparentprevalso it might be enough now if they dominate some small niches at start and grow from there. Instagram started with mood pictures about coffee, tiktok was first about dancing videos etc. Zuckberg knows actual numbers and we don&#x27;t. reply raverbashing 2 hours agorootparentprevAccording to Zuck their \"metaverse\" is not dead neither, but I guess it&#x27;s filled of legless people wandering around aimlessly reply grishka 3 hours agoparentprev> news is not a priority for themHuh? That&#x27;s a strange requirement. Somehow every social media platform eventually devolves into having news, meme pages and other faceless inhuman entities you seemingly can&#x27;t escape.I, for one, desperately want the opposite — a platform where organizations of any kind are not welcome. Only people and their personal accounts. The fediverse is kinda like that. Yes, BBC does have their own Mastodon server, but they don&#x27;t have many followers and there are no algorithms to force-feed you out-of-network content. reply simongray 2 hours agorootparentMastodon is quite different. It really filters a lot of noise away (e.g. Xwitter always has ~20 pointless meme gif comments below every viral post) and it self-selects what I would consider interesting people. The algorithm-centred platforms tend to congregate around celebrities&#x2F;influencers over time, while Mastodon is much more egalitarian and people-focused. reply hackernewds 2 hours agorootparentprevNews is not as monetizable as something short-form such as Reels reply CM30 1 hour agoprevGiven that Threads seems to suppress posts linking to other websites, and the audience there isn&#x27;t particularly interested in news, I&#x27;m not surprised the BBC has given up on it. The fact the algorithm recommends you everything but what you&#x27;re interested in and engagement is questionable even for larger accounts is just the icing on the cake.But I&#x27;m honestly not sure any of these alternative platforms is going to replace Twitter for news anytime soon. BlueSky feels a lot like Threads activity wise; decent to mediocre engagement for popular accounts and barely any for regular folks&#x2F;new users, and Mastodon has high engagement, but a far more technical, niche audience.And neither really care much more about news than Threads does. Probably because of all the toxicity that news posts tend to bring over on Twitter&#x2F;X, and these services acting as an escape from that... reply nirui 6 hours agoprev> What makes this news more interesting is the fact that the British Broadcasting Corporation (BBC) has abandoned its Threads account but is still maintaining its self-hosted Mastodon accounts online.The title made it sound like BBC have picked a side, but the reality could be that they still got a few more months left in their experiment with Mastodon. Maybe it&#x27;s still too early to tell anything concrete at this time. reply robga 4 hours agoparentDefinitely. BBC News Labs has a drawer full of “Completed” projects as seen at https:&#x2F;&#x2F;bbcnewslabs.co.uk&#x2F;projects&#x2F; reply M2Ys4U 3 hours agorootparentNews Labs isn&#x27;t running the Mastodon trial, it&#x27;s a different team in BBC R&D with a different remit reply hgrtegwerh 6 hours agoparentprevthreads supposedly intends to eventually federate with activitypub, so it&#x27;s entirely possible the bbc is just betting on the one with the more active users, assuming eventually threads will pull in their feeds anyway. reply dmix 5 hours agorootparentI highly doubt BBC&#x27;s social media managers are making long term bets on smaller social media networks eventually investing in activitypub. IRL they probably got little engagement vs the time they were investing so decided to just keep at least one Twitter style feed around so they don&#x27;t completely abandon the concept of \"microblogging\"... as the vast, vast bulk of the journalism industry continues to use Twitter and that idea isn&#x27;t dying any time soon. Having a backup comms platform is deemed valuable enough even if it doesn&#x27;t drive high business value.Edit: others have pointed out they still use X so I guess the whole point is moot reply shapefrog 1 hour agorootparentAt the scale of BBC&#x27;s social media managers I hope they are not on the x.com website typing in updates and clicking post.They should be using custom software and be able to redirect &#x2F; duplicate the distribution of their news to different or multiple outlets with little more effort than a few lines of code. reply vr46 53 minutes agoprevI wish more social media companies would give up on the EU, it&#x27;s so peaceful here. I&#x27;ve boxed myself into will-follow-back corner on Twitter so every time I open it I feel like one of the chimps at the beginning of 28 Days Later with the rage virus.goes off to have a nice cup of tea and waits for all this to blow over reply ThinkBeat 4 hours agoprevI do fully agree that there are philosophical and ethical problems with TwitterX, and I would like to see it fail however the rumors of the demise of Twitter &#x2F; X are exaggerated.It is a massive and difficult job to pull regular users away from what they are comfortable with X &#x2F; Twitter to a completely new platform without any real end user benefit. In fact, it lacks a lot of features users may like on TwitterX reply CM30 1 hour agoparentI think Twitter&#x2F;X charging money might be what kills it off. If you need to pay to post&#x2F;view anything, then about 95% of the population will say \"sod this, I&#x27;m out of here\".I&#x27;ll be very surprised if Musk actually does go through with that promise&#x2F;idea (even someone like this knows how network effects work, right?), but if he does, I&#x27;d say that would be the tipping point. reply lolc 1 hour agorootparentAs somebody with only second-hand exposure to Twitter, it feels like they&#x27;re actively driving it to irrelevance. All I hear for a year now is that they turn off interfaces that enabled people to integrate with them.Just last week a customer called because the Twitter embed on their site didn&#x27;t work anymore. When we said we&#x27;d investigate, they said not to bother and just replace it with a link. reply exodust 1 hour agoparentprev> \"I do fully agree that there are philosophical and ethical problems with TwitterX, and I would like to see it fail...\"Agree with who? The article mentions nothing of the sort. It seems you have your own philosophical and ethical problems. Agreeing with ghosts, and wishing failure on others. reply nicky0 1 hour agoparentprevNow I&#x27;m curious to know know what you think the philiosphical and ethical problems with X are. reply phatfish 1 hour agorootparentProbably that owner is a Billionaire online bully with a large following of acolytes ready to brigade anyone he feels like.And that Twitter&#x2F;X has now been login-walled so anyone that wants more than an embedded Tweet link has to make an account. Which means organisations like the BBC which use it are now implicitly supporting Musk. reply AdamN 38 minutes agorootparentprevSupporting Musk just to view tweets (or whatever they are called) is a big moral compromise imho.I get that if you are a news or social media source you need the views so it&#x27;s work with him or lose money but that&#x27;s not the case for many other people.I left months ago and think it was the right decision for many reasons. reply proactivesvcs 42 minutes agorootparentprevI don&#x27;t think there are any philosophical problems with twitter: it&#x27;s run by someone who is a transphobe and gives aid and comfort to Nazis (I believe this qualifies as \"being a Nazi\"). How can anyone need any more reason to refuse it? reply a1o 8 minutes agoprevAnyone here in BlueSky? Still trying to figure that one out. reply nocoiner 7 hours agoprevI get an involuntary shudder of PTSD at seeing “Threads” and “BBC” in the same sentence. reply Lio 18 minutes agoparentFor me it was When the Wind Blows[1].A cutesy animated cartoon film about an old couple dieting the affects of nuclear fallout ...things were different in the 80s.1. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;When_the_Wind_Blows_(1986_film... reply dmbche 7 hours agoparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Threads_(1984_film)Because of this? reply firecall 7 hours agorootparentBecause of that! reply firecall 7 hours agoparentprevI&#x27;m there with you man.Every time this comes up I point out that Cold War, and specifically Threads related PTSD, is a very real thing for UK Gen X! reply seoulmetro 5 hours agorootparentEquating watching a scary drama in 1984 with something as severe as PTSD and saying it&#x27;s a very real thing for an entire generation is over the top. Baby boomers, sure they dealt with the cold war pretty realistically. reply marcus_holmes 4 hours agorootparentUK-born Gen X here. We entirely expected to be nuked before we were 40. If millenials can have PTSD from climate change [0], then we had PTSD from the cold war.[0] https:&#x2F;&#x2F;www.theguardian.com&#x2F;environment&#x2F;2021&#x2F;apr&#x2F;20&#x2F;climate-... reply seoulmetro 4 hours agorootparentThey can&#x27;t have PTSD from climate change.Unless you want PTSD to just mean \"uncomfort\". In which case, everyone has PTSD. But then what does disorder mean if it&#x27;s normal? reply marcus_holmes 3 hours agorootparentI agree, but apparently PTSD from climate change is a thing. reply nocoiner 4 hours agorootparentprevI dunno. Try watching it. It’s one of the few movies I’ve seen that I’d call literally “life changing” and certainly convinced me that nuclear war is the most immoral act our species is (currently) capable of undertaking. reply beowulfey 4 hours agorootparentI can relate to this a little. I had a similar experience (maybe not quite as strong, but still) when I first read On the Beach by Nevil Shute reply nocoiner 1 hour agorootparentI first read that back in high school and it is legitimately one of my favorite books - I re-read it every few years.I think in my area it was pretty broadly assigned reading at the time - lots of other high schools in the US assigned it - though no idea if that’s still the case (the book certainly isn’t perfect - the female characters are incredibly one-dimensional) but I hope it is still commonly read.I see On the Beach as being Threads-Lite. On the Beach is certainly upsetting but wow, does Threads show a more upsetting version of similar dynamics. Plus none of the characters of Threads are as uncomplicatedly virtuous as the characters of On the Beach. reply seoulmetro 4 hours agorootparentprevI&#x27;ve seen it. I&#x27;ve also seen some scary movies that make me scared of the dark, but that&#x27;s not PTSD. reply sundvor 5 hours agorootparentprevNot quite \"boomer\" but vividly recall air alert sirens being played for evacuation exercises to seek shelters in Norway when I was a child of I guess 6 or 7 in the late 70s.\"Better dead than red.\" That&#x27;s another one I vividly remember circulating, a bit later (early 80s).The threat of nuclear war was very real. I guess we got lucky in the end. reply andsoitis 5 hours agoprevThe BBC is very much active on X, such as https:&#x2F;&#x2F;twitter.com&#x2F;BBCWorld reply rvz 3 hours agoparentThe BBC is also actively using more of their accounts on X with the last time they posted is less than an hour ago on https:&#x2F;&#x2F;twitter.com&#x2F;BBCSport, https:&#x2F;&#x2F;twitter.com&#x2F;BBCWorld and https:&#x2F;&#x2F;twitter.com&#x2F;BBCBreaking.On threads their last post on BBC Sports was 12 hours ago: https:&#x2F;&#x2F;www.threads.net&#x2F;@bbcsportSo in fact, they are more active on Twitter &#x2F; X than on the rest of the social networks.But surely that somehow means &#x27;X is dying&#x27; &#x2F;s reply iudqnolq 2 hours agorootparentCan you see many replies by the BBC? Genuine question, I just get a Twitter frontend error when I open the replies tab. (In Musk&#x27;s defense, I got this sort of error frequently pre-acquisition).If it&#x27;s mostly top-level posts that could just be the same script pumping out content on all their platforms. reply marban 6 hours agoprevLast time I checked Threads was still not available in Europe so that&#x27;d be another curtailment. reply prmoustache 2 hours agoparentIsn&#x27;t threads what appeared as channels notifications in whatsapp anyway? I haven&#x27;t tested subscribing to any account but I totally have that. reply threeseed 5 hours agoparentprevEU has already read the riot act to X on disinformation.So we could see Threads approved at the same time X is banned. reply troupo 4 hours agorootparentNo one banned Threads in the EU.The way Threads is implemented and is working is illegal in the EU, so Meta decided not to launch it in the EU until they figure out how to circumvent the laws. reply jillesvangurp 1 hour agorootparentI think the history of that is that Meta at the last minute decided that they couldn&#x27;t launch Threads in the EU and that that sealed the faith of Threads leading to its now increasingly obvious failure in the market. It looks like a poorly planned and executed rush job by Meta that is now failing.The reason the BBC is ditching Threads is because it has no traction worth talking about and because they are a European company and a European launch of Threads seems to be not happening. Mastodon is apparently good enough that they are not giving up on that one for now. Probably also really cheap for them to support it as they can self-host it. And given how critical they are of Musk, they need an alternative to X even if they still have to use it. reply Moldoteck 3 hours agorootparentprevThey just should provide login with own mail reply troupo 3 hours agorootparentIt&#x27;s more than that: Meta is also in hot water because of handling of user data, behavior advertising, and using data from its seemingly separate apps across Facebook for tracking and advertising.Threads runs afoul of many provisions in Digital Markets Act and possibly GDPR, too. replydusted 1 hour agoprev> (ironically, most of them are still using X, formally known as Twitter).Dunno, feels like the only people leaving X was the people who didn&#x27;t use X anyway..I&#x27;ve been using it with the same frequency as before Musik took over, and to be honest, I&#x27;ve not noticed anything different in the stuff I see&#x2F;find&#x2F;interact. reply __alexs 1 hour agoparentI followed 3000 people and read and posted most days. Haven&#x27;t even opened the app since the logo changed.Most of the people I interacted with are now on Mastodon. I guess we just have different circles. reply rsynnott 36 minutes agoparentprevI was a pretty heavy Twitter user since 2007 (back when it had visible sequential numeric IDs, mine was six figures) and gave up a couple of months after the coming of Naughty Old Mr Car. For me his war on journalists in late 2022 was the last straw (I did return once for a silly joke in December when uttering the dread word &#x27;Mastodon&#x27; was briefly banned), but even if that hadn&#x27;t bothered me, the end of third party clients a month or so afterwards and the promotion of replies from gormless bluetick idiots above all else in the same timeframe would&#x27;ve killed it for me.That said, I&#x27;m happy enough with Mastodon now; it feels a lot more like old (early-10s) Twitter.I&#x27;m keeping the account, for now, on the basis that he might at some point get bored and go back to his imaginary car tunnels, and sell the remains of Twitter to someone sensible (presumably Automattic, which collects mortally wounded social networks). reply nightfly 1 hour agoparentprevI now have stuff shoved at me on the home screen that I don&#x27;t know who they are, don&#x27;t want to see their stuff, or both. I&#x27;m only ever on Twitter to see stuff from a small handful of people so it&#x27;s very noticeable. reply kmlx 1 hour agoparentprevi completely stopped using it since july. the quality of the experience has been atrocious for me. reply preisschild 51 minutes agoparentprev> I&#x27;ve not noticed anything different in the stuff I see&#x2F;find&#x2F;interact.I definitely did. Russian disinformation from random accounts, such as \"Catturd\", show up on my home page. reply mellosouls 6 hours agoprevThis seems also intended to be read as the BBC deprecating its presence on X, which alternative-universe situation is implied by the wording of the title and anti-X stance of the writer generally; obviously that&#x27;s not the case, however much it might be wished here. reply mvdtnz 6 hours agoparentX is not mentioned in the title and the very first sentence of the article says that the BBC accounts are still on X&#x2F;Twitter. What gives you the impression that the intention was for this to be read as the contrary? reply rlt 5 hours agorootparentI absolutely read the title as the BBC choosing a single platform. If not why wouldn’t it say “BBC gives up on Threads, sticks with Mastodon and X”? reply davisoneee 2 hours agorootparentI think it could much more fairly be read as \"...out of alternate sources for real-time short-form communication, BBC has given up on Threads, but is sticking with Mastodon\".Twitter&#x2F;X is the &#x27;default&#x27; and the assumption that media will be on it...while Threads and Mastodon are the _alternatives_. This article, to me, is just suggesting that BBC has dropped _one of the alternatives_. reply Hamuko 5 hours agorootparentprevWouldn&#x27;t it then say \"BBC gives up on Threads and X, sticks with Mastodon\"? reply lmm 4 hours agorootparentMaybe they haven&#x27;t memorized which platforms the BBC is currently on and assume it was never on X. reply mellosouls 5 hours agorootparentprevBecause \"X is not mentioned in the title\" and the \"anti-X stance of the writer generally\". reply rsynnott 30 minutes agorootparentHold on, _not mentioning Twitter in the title_ implies that they&#x27;re abandoning Twitter? How do you figure that? The title also doesn&#x27;t mention chairs or toilets, but one assumes the BBC will still be using those. reply mvdtnz 5 hours agorootparentprevHow could you possibly know the stance of the writer generally without even having read the first sentence? reply mellosouls 2 hours agorootparentI read the entire article, including the dismissal of X, the implied misleading message discussed, and the links at the bottom to the authors missives which have practically every platform except X.Less of the bumptious tone please. reply davisoneee 2 hours agorootparentThey MAY have an X bias, but I think a much more generous interpretation is simply that they are highlighting which _alternative_ platforms BBC have given up on.Threads and Mastodon are both _alternative_ choices, with Twitter being the main real-time shortform source. TFA highlights that BBC is giving up on threads, but not on alternatives in general. reply spiderice 3 hours agorootparentprevPossibly because they are familiar with the writer reply numpad0 3 hours agorootparentprevs&#x2F;X&#x2F;Tesla&#x2F;g reply spartanatreyu 5 hours agoprevUnfortunately, BBC doesn&#x27;t seem to have world news on their own mastodon instance.Without that, I don&#x27;t think they&#x27;re going to get worthwhile numbers from mastodon... reply robga 4 hours agoparentI imagine it is only a matter of time before they do have a world news account. Meanwhile, there are plenty of mirror accounts (rss posters) of their headlines on Mastodon.And summarisers like https:&#x2F;&#x2F;mastodon.bot&#x2F;@bbcnews reply shapefrog 4 hours agoparentprevWhy haven&#x27;t they at least taken the twitter feeds and cross posted? Or spit the beeb rss feed out via mastodon?The \"experiment\" seems to be little more than spinning up a server. reply jwmoz 56 minutes agoprevI immediately stopped using it after they spammed random accounts to the timeline, even after following steps to stop it. Not a good look. reply arpowers 5 hours agoprevX dying? It seems like it gets better every day reply timeon 3 hours agoparentBetter maybe for logged-in users. Without account it is hit or miss. In my case, I do not have account and it still shows just error page. reply usr1106 39 minutes agorootparentI have an account, but have not been logged in for a long time. I don&#x27;t fear missing a lot.Or maybe I don&#x27;t have an account anymore. Didn&#x27;t Musk announce that inactive accounts will be deleted? reply fragmede 2 hours agoprevOne problem is the lack of desktop client. Instagram gets away with not really having a website, but for Threads, that&#x27;s paramount. reply insin 1 hour agoparentThey do have a web version now, but if you try to log in you get hit with an &#x27;install the app to continue\" wall.Nice try, Zuck. reply notatoad 2 hours agoparentprevthreads has had a webapp with pretty much feature parity to the mobile app for about a month now.instagram has also had a webapp with pretty much feature parity to the mobile app for many years, so saying they don&#x27;t really have a website is pretty weird. reply helsinkiandrew 1 hour agoparentprevInstagram&#x27;s website is fairly good and just about fully featured isn&#x27;t it? reply oliv5900 1 hour agorootparentIf watching a reel in a 3\" box on a 27\" monitor is fully featured, then ya sure. reply Theodores 21 minutes agorootparentprevNot at all. It might as well be written in 1999 with tables. They could make the web version use big monitors to show beautiful photos in a responsive way, but they cannot be bothered.They know who their customers are and their customers do not have time for computers. reply CM30 2 hours agoparentprevThey do actually have a desktop client&#x2F;website now. reply scythe 6 hours agoprevI&#x27;m not terribly familiar with these ecosystems, but for a news broadcaster it seems like a massive advantage that Mastodon is highly accessible to a non-logged-in user. reply jkaljundi 3 hours agoprevSince Threads is still available only in selected countries - most importantly blocking Europe - it can&#x27;t be taken seriously. reply peanuty1 3 hours agoparentIt&#x27;s not even popular in the US. reply samcat116 7 hours agoprevIn theory this won’t matter in a couple months when Threads gets ActivityPub support. Makes sense from a branding perspective reply greesil 7 hours agoparentEmbrace, extend, enshitten reply TaylorAlexander 7 hours agorootparentI guess I don&#x27;t know the ins and outs of this but I imagine if there is any issue, mastodon server administrators will ban the threads servers from federating to their server, and then users can live happily in ignorance of whatever Threads is doing, unless they elect to join or migrate to a server which does federate it. reply greesil 7 hours agorootparentAnd I&#x27;m sure Usenet is just fine for the people who are still on it. For the mass market, it might be a different story. It&#x27;s not like the masses are piling onto mastodon atm so maybe it&#x27;ll always be niche unless someone finds a way to make a buck with it. reply dmix 5 hours agorootparentAnd maybe Signal will win the chat wars considering how much morally &#x2F; intellectually better everyone feels using it reply toastal 4 hours agorootparentI hope not. Signal accounts require a phone number and an Android or iOS primary device or you can’t use the service.There are already good, decentralized options that don’t require a phone at all. The time-tested XMPP+OMEMO has the characteristics I need while having the hardware requirements of a toaster compared to other options. reply Moldoteck 3 hours agorootparentprevYeah suuure... They have problems with transfering backups from one platfor to another(you can&#x27;t officially transfer history from say android to desktop&#x2F;ios) and this issue is from at least 2015... reply ikt 2 hours agorootparentprevI think he means it&#x27;s not an issue even if Mastodon does remain niche, we&#x27;re quite happy here how it is, Fosstodon went invite only recently because it felt like the server was becoming too generalised reply ShadowBanThis01 7 hours agorootparentprevAnd thus we see the sham of Mastodon, where people are supposed to be able to go for freedom from censorship and jagoff moderators... but where swaths of content or users are banned. reply greesil 6 hours agorootparentJagoff moderators are as old, or older, than the internet my friend. reply simbolit 6 hours agorootparentprevOnce you understand the subtlety, it is no longer \"the sham of mastodon\":ActivityPub, the protocol, is the freedom you describe. But individual servers, using the mastodon software and activitypub protocol, they may or may not, depending on server admins.If you don&#x27;t like content bans or user bans, you have three options:(1) find a mastodon server with the same values re freedom&#x2F;censorship as you have.(2) set up such a server yourself(3) don&#x27;t use mastodon&#x2F;ap reply dmix 5 hours agorootparentMaybe you have those options but anyone outside of the technically savvy (aka the mom test) is not engaging in a system where they are forced into that situation as a rule of using the platform. I&#x27;m not recommending a site to my mom where I have to explain why she needs to sign up for a 2nd or 3rd time with a user&#x2F;pw to access the content she already invested time&#x2F;energy in finding&#x2F;following because it got delisted.The early popular services on Mastodon 100% eagerly enforce hyper moderation...so yeah, basically the only large active community there is going to be a very in-group subset of technical users, with some niche services offering broader access.Despite the best possible opportunity coming about with Twitter nothing in the year since then has indicated otherwise. reply kstrauser 5 hours agorootparentStop getting your mom signed up on awful, unmoderated servers, then.I manage a Mastodon instance. When I disconnect from another server, I explain the reason to the users on my instance. I&#x27;ve never once disconnected because I disagreed with another server&#x27;s politics. I have cut off servers that post literal Nazi content (like swastika flags, extreme antisemitism, etc.), content that&#x27;s illegal in the USA where I live, and other reasons that go far beyond \"things I disagree with\". I&#x27;ll continue to do so. And I get new users signing up because they appreciate that I disconnect from that filth.And from what my other admin friends say, that seems to be the standard practice. Sure, there are some instances that disconnect much more quickly, as is their right. Most don&#x27;t. The only servers that regularly get cut off from the rest of the fediverse are the ones hosting truly vile content, and by that, I don&#x27;t mean \"this person has people in a political party I don&#x27;t like\", but like their public timeline is full of people making &#x27;kill all the black people&#x27; \"jokes\". Darned if I&#x27;ll tell someone they&#x27;re wrong for cutting the link to some of the truly, genuinely abhorrent instances out there. reply troupo 4 hours agorootparent> Stop getting your mom signed up on awful, unmoderated servers, then.Ah yes, because it&#x27;s so easy to figure out which of the thousands of Mastodon servers is not awful and is properly moderated. Or has any guarantees that it will be properly moderated in the future. reply ikt 2 hours agorootparent> https:&#x2F;&#x2F;joinmastodon.org&#x2F;> Join mastodon.socialThere you go :) reply spookie 2 hours agorootparentprevThere&#x27;s a big blue button on their homepage leading you to the general instance. You pick that. Instances don&#x27;t matter for most people. If you aren&#x27;t most people, you&#x27;ll understand what that means by using the platform.As for moderation, Mastodon does have plenty of moderation and federation allows for responsibility to be divided among server&#x2F;instances owners. If anything, it scales better than a centralised alternative. reply lmm 4 hours agorootparentprev> ActivityPub, the protocol, is the freedom you describe.Disagree. ActivityPub, the protocol, is oriented around a world of first- and second-class citizens (server operators and users), and very much nudges you towards living under the thumb of a local petty tyrant. Yes, you notionally have a way out (although in practice if the cabal (TINC) decides to defederate you there&#x27;s not much you can do), but defaults and practices matter. reply spookie 2 hours agorootparentPlease, do enlighten us on the instances that are defederated. reply ShadowBanThis01 4 hours agorootparentprevI chose #3, after going so far as to set up an account on an instance. A couple other major issues killed it for me; the biggest being this bizarre, rabid hatred in the Mastodon community of being able to SEARCH content. I&#x27;m mystified that a tech-savvy segment of the population wants to publish material online and then \"hide\" it. I mean... how is that not monumentally ignorant?And then there&#x27;s the implication that you&#x27;re supposed to choose a server based on some \"interests\" or \"values.\" No. My interests change. This is supposed to be a global medium where ANY topics are discussed. Why should I be asked to pigeonhole myself in a special-interests community? Yes, I realize that some instances are \"general,\" and content is propagated across instances, but I think this whole idea is dumb from the outset and creates confusion amongst the general public.This goes back to the lack of search, too. If I&#x27;m interested in a topic, I expect to be able to search for it.And finally I don&#x27;t think your #1 is a solution, because the server I choose might be banned by other segments of the network and now I&#x27;m not reaching anyone. So... I still say this \"freedom\" is a sham. reply ikt 2 hours agorootparent> And then there&#x27;s the implication that you&#x27;re supposed to choose a server based on some \"interests\" or \"values.\" No.That&#x27;s an easy fix:Go to https:&#x2F;&#x2F;joinmastodon.org&#x2F; click Join mastodon.social> searchYou might like the latest release:https:&#x2F;&#x2F;blog.joinmastodon.org&#x2F;2023&#x2F;09&#x2F;mastodon-4.2&#x2F;> In this version we overhauled search. reply M2Ys4U 3 hours agorootparentprevThe recent 4.2.0 release has introduced an opt-in search feature, so that&#x27;s changing.About half of Mastodon instances are on 4.2 now (according to FediDB), and it looks like future releases will prompt existing users to check whether they want to opt-in. replyspookie 2 hours agorootparentprevEh, don&#x27;t worry, there are plenty of instances that won&#x27;t federate with Threads or other select instances. You have the choice to remain among the people you want.And before someone mentions \"that defeats its purpose\", it does not. The purpose is for the user to act on its own agency and be wherever it feels right. Federation also enables that. reply zmmmmm 6 hours agorootparentprevif the alternative is moving straight to enshitten ..... i guess i&#x27;ll take it? Seems to be basically how technology progresses these days. We just have to keep forking from the point before enshitten to build the next thing each time. reply sp332 5 hours agoparentprevIs there an actual timeline for that? reply pmarreck 6 hours agoprevI can&#x27;t stand that Instagram doesn&#x27;t let you copy and paste (or save) content out of it, and I also can&#x27;t stand when Threads also doesn&#x27;t let you do the same thing. In the Threads app, you literally can&#x27;t just select the text of a thread to paste elsewhere. To hell with that. reply donatj 6 hours agoparentI don&#x27;t know what you&#x27;re using, but modern Android will let you copy text and images out of basically anything from the \"Select\" button on the bottom of the app switcher reply lazycouchpotato 6 hours agorootparentIf you don&#x27;t happen to have that on your Android phone, Copy [1] works well.[1] https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.weberdo.ap... reply threeseed 5 hours agoparentprevOn iOS at least you can copy text from Threads. Just long press and select Copy. reply senectus1 3 hours agoprevI still feel that the barrier to entry for mastodon is way too high. it needs to be made simpler. reply raverbashing 2 hours agoprev [–] Zuck tried to create the Disneyland of short updatesBut guess what, nobody cares about the bland and corporatized stuff for too long replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Several brands such as the British Broadcasting Corporation (BBC), the National Football League, and CBS News have discontinued using Threads by Instagram due to poor user engagement.",
      "The BBC is still maintaining its self-hosted Mastodon accounts despite this change, indicating a preference for platforms that offer greater control and flexibility.",
      "The BBC's decision could be influenced by concerns over Instagram parent company Meta's geo-ban policies and the absence of a public API for automated posts on Threads."
    ],
    "commentSummary": [
      "The BBC has decided to drop their news-specific feature, Threads, on its app, shifting focus to Mastodon due to dissatisfaction from users, such as unwanted content recommendations on Instagram.",
      "Social media's harmful influence on discourse and difficulties in building successful social networks are highlighted in the article, underlining the uncertainty over news credibility on alternative platforms like Bluesky and Mastodon.",
      "Threads usage on Instagram for gaining valuable behavioural data and the depiction of the Cold War's traumatic effects in the film \"Threads\" are also mentioned."
    ],
    "points": 290,
    "commentCount": 154,
    "retryCount": 0,
    "time": 1696383423
  },
  {
    "id": 37751140,
    "title": "Detroit man steals 800 gallons using Bluetooth to hack gas pumps at station",
    "originLink": "https://www.fox2detroit.com/news/detroit-man-steals-800-gallons-using-bluetooth-to-hack-gas-pumps-at-station",
    "originBody": "Live Weather Things to Do Sports Email Contests Moneysaver More Expand / Collapse search Detroit man steals 800 gallons using Bluetooth to hack gas pumps at station By Jessica Dupnack and David Komer Published September 29, 2023 12:00AM Crime and Public Safety FOX 2 Detroit Detroit man steals 800 gallons of gas using Bluetooth to hack gas pump Some gas station owners are falling victim to a sophisticated scam. DETROIT (FOX 2) - Some gas station owners are falling victim to a sophisticated scam. Scammers are using cellphone's Bluetooth option to hack the pump - and get it for free. \"I wish it would go back to $1.99 - it’s almost $4,\" said Tywanna Coleman. \"I get why people are doing it but it’s still not right.\" Locate Almost Anyone By Entering Their Name (This Is Addicting!) TruthFinderSponsored Paying at the pump is for chumps - when you can get gas for free - and illegal, but it didn’t stop a Detroit man from stealing almost 800 gallons of gas at the Shell at Eight Mile and Wyoming. \"They just open the pump for them automatically,\" said Mo, the gas station owner. FOX 2: \"And then cars just keep coming up and filling up?\" \"Yeah they meet up with them and tell them to come over here,\" he said. When Mo says \"open the pump\" the thief – overrides the system, basically hacking in using a Bluetooth connection from his phone, as a kind of remote. Then, it’s a free-for-all. FOX 2: \"How many gallons have they got from you guys? \"From us, about 800,\" Mo said. That’s just shy of $3,000. And when the clerks inside try to stop it - they can’t. \"Every time we push Pump Three stop, it wasn’t doing anything,\" he said. \"We have to shut off the whole pumps - we have emergency stops.\" The station happens to be a Detroit police-patrolled \"Project Green Light\" gas station. With that comes surveillance video of a suspect who investigators are actively looking for. But it’s not just one guy, and this maneuver is not new, just re-surfacing. Like at a Speedway station Downriver – in Riverview this month. In that case, they used a bait-and-switch. One guy distracted the clerk with a Cash App problem inside, while the other hacked the pump. They got away with $54 in gas. Mo says his clerks are on high alert – if they see a big backup at a pump, or someone hanging around a little too long, they’ll call cops to check it out. \"We can only do so much,\" he said. Locate Almost Anyone By Entering Their Name (This Is Addicting!) Find Detailed Public Record Information About Someone. Search Records Quickly. Search Any Name. TruthFinderSponsored Can Dental Implants for Seniors Be Paid For By Medicare? (See How) StuffAnsweredSponsored Puppy Refuses To Leave Railway Police Turns Pale When They Realize What He Is Hiding Crowdy FanSponsored Top Vet Begs Americans: \"Stop Feeding This Meat To Your Dogs\" Most dog owners don't know this... Ultimate Pet NutritionSponsored Seniors Getting First Class for Price of Economy Online Shopping ToolsSponsored Expert Says This Drugstore Wrinkle Cream Is Actually Worth It BrunchesNCrunchesSponsored Housekeeper Had No Idea She Was Being Filmed - What Owner Captured Was Shocking Science PickerSponsored UAW negotiations View More More than 200 UAW workers laid off at GM Toledo plant amid strike Ford's latest offer includes 20% pay raise, elimination of Tiers, no job losses with EV plants UAW strike update: Ford, GM lay off more workers Michigan strip club has special offer to help UAW workers on strike UAW Strike inflicts nearly $4 billion in losses, analysis shows Top videos video Light rain and cloudy conditions can't dampen Detroit Fireworks fun video Hart Plaza 2023 Detroit Fireworks fans enjoyed the show video Four people shot in Ypsilanti, two of them teens video Elderly Detroit woman stuck inside home from large driveway sinkhole video Belle Isle fans weather the rain and clouds, and take in the 2023 Detroit Fireworks Trending Detroit street racer shot by police after hitting officer while trying to speed away Life-saving rescue of electrocuted boy by Warren police officers: 'I gotta get him' Detroit man steals 800 gallons using Bluetooth to hack gas pumps at station Detroit to open completed Riverwalk in October that connects Mt. Elliott and Gabriel Richard parks Invasive plant Hydrilla discovered for first time in Michigan News Local National Hall of Shame Let It Rip Crimestoppers The Interview Business Things to Do FOX News Sunday Live LiveNOW from FOX FOX Soul Stream FOX 2 FOX LOCAL FOX 2 News app Weather Closings Weather App Weather Alerts Traffic Airport Delays Fox Weather Mornings The Nine The Noon Mug Contest Health Works Cooking School Nosh with Josh Get Fit Money Saver Doctor is In Weekend Mornings Politics Let It Rip Gretchen Whitmer Joe Biden Sports First & North Lions Wolverines Spartans Pistons Tigers Red Wings Entertainment FOX Shows CriticLEE Speaking TV Listings Contests About Us FOX 2 Staff Contact Us Work at FOX 2 FOX 2 News App Job Shop Community FCC Public File FCC Applications Closed Captions Problem Solvers Resources Contests Wallside Windows Weather Quiz Gardner White Dream Team Mug Contest Exposed Money Personal Finance COVID-19 & The Economy Business Stock Market Small Business Regional News Chicago News - FOX 32 Chicago Milwaukee News - FOX 6 News Minneapolis News - FOX 9 facebook twitter instagram email About UsNew Privacy PolicyTerms of ServiceYour Privacy ChoicesWork at FOX 2FCC Public FileEEO Public File This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX Television Stations",
    "commentLink": "https://news.ycombinator.com/item?id=37751140",
    "commentBody": "Detroit man steals 800 gallons using Bluetooth to hack gas pumps at stationHacker NewspastloginDetroit man steals 800 gallons using Bluetooth to hack gas pumps at station (fox2detroit.com) 282 points by mikece 21 hours ago| hidepastfavorite376 comments kotaKat 20 hours agoIs it actually Bluetooth, or did they get their hands on keys or remotes for the dispenser?The dispensers have constant real-time communication to the forecourt controller and the attendant inside. What are they showing when this \"hack\" happens? Are the attackers taking the RS485 line down (which would show the pump offline immediately inside) and forcing the pump to manually dispense?I&#x27;d kill to see some more actual information than this. I am not aware of a single pump on the market with Bluetooth right now but I do remember some IR-based remotes for some old Wayne pumps. reply obituary_latte 19 hours agoparentIn the article one of the attendants says \"I hit stop on Pump 3 and nothing happens\" so they are at least seeing the pump is on and running. Whatever is happening must be either locking the attendant out or is leaving the attendants machine in a bad state with the pump running switch \"on\" so to speak. I wonder if it&#x27;s actually NFC. It&#x27;d be interesting to see what a Flipper Zero would come up with. reply yellow_postit 17 hours agorootparentI can imagine a lot of Flipper Zero owners heading out to explore pumps now. reply SoftTalker 15 hours agorootparentprevAgree, sounds like some kind of poorly-protected function has been discovered that&#x27;s putting the pump into a \"maintenance\" or \"test\" mode where it will dispense fuel without a prepay having been completed. reply wyldfire 12 hours agorootparent> \"Every time we push Pump Three stop, it wasn’t doing anything,\" he said. \"We have to shut off the whole pumps - we have emergency stops.\"Whew - sanity prevails! When I read the part about not stopping, I was a bit worried that the e-stop somehow was not a simple mechanical failsafe. reply Kye 20 hours agoparentprevI&#x27;ve seen people getting ready for a day of working on yards fill up 30 different things each on multiple trucks. The quantity might not stand out, and the attendant(s) might not be allowed to intervene if they think something looks suspicious. A \"better to lose some money than risk losing a longtime customer over a mistake or an attendant getting shot playing hero\" policy makes a lot of sense. reply Fatnino 12 hours agorootparentMost pumps I see will only dispense 75 dollars and then cut off. In one case it was 100 and then cut off. reply tomjakubowski 11 hours agorootparent$75 at California&#x27;s gas prices right now doesn&#x27;t go very far! reply Razengan 19 hours agorootparentprevnext [66 more] > “an attendant getting shot playing hero\" policy makes a lot of senseIn the post-apocalypse or some other lawless society perhaps reply t-3 19 hours agorootparentnext [–]> In the post-apocalypse or some other lawless society perhaps > Detroit man steals 800 gallons > DetroitJokes aside, it&#x27;s common policy pretty much everywhere for cashiers and other employees who are not specifically security personnel to never confront shoplifters or other thieves, always comply with robbers, etc. Insurance covers material losses as long as reasonable precautions are taken. reply ibejoeb 19 hours agorootparentDetroit has really come up in the past 5 years. I recommend folks visit and check it out. Motor City&#x27;s downtown actually has decent public transit via bus and light rail, and it&#x27;s very walkable* during the nice season. Lots of entertainment.* oh but watch out for bird&#x2F;lime&#x2F;lyft&#x2F;uber scooters literally everywhere and everyone&#x27;s drunk reply rainclouds 18 hours agorootparentDetroit downtown is nice, the Lions win games, what alternate reality is this? reply red-iron-pine 18 hours agorootparentpendulum swings back it seems reply gitonup 16 hours agorootparentprevDetroit hasn&#x27;t, downtown has.Downtown has improved wildly since when even when I was growing up near there in the 80s&#x2F;90s, but people don&#x27;t realize how large Detroit is. It&#x27;s nearly 140 square miles. Downtown is, charitably, 2sqmi, and that&#x27;s including all the way up to Wayne State. The light rail is the People Mover loop downtown plus a trolley up Woodward, within that range.The theft mentioned in the article occurred at 8 Mile and Wyoming, which is a 20 minute highway drive from Campus Martius in no traffic. \"Clean Downtown\" that happened when the city hosted the super bowl and other related efforts have very lopsidedly focused on the downtown area. reply henvic 18 hours agorootparentprevI&#x27;m really into driving and all, but I&#x27;ve been in Detroit a year ago and it sucked. The only good thing about it was the Henry Ford Museum and the fact that Canada is just across the border. reply gs17 18 hours agorootparentYou probably don&#x27;t know how it used to be (which is also very different from my grandfather&#x27;s idea of \"how it used to be\"). I was there last year, but I was also there for 25 of the past 30 years and the change from when I was a kid is startling. \"I recommend folks visit and check it out\" is not something someone would have said about Detroit unironically for a long while. reply RandallBrown 17 hours agorootparentprevThe Henry Ford Museum isn&#x27;t in Detroit... reply pests 16 hours agorootparentIt&#x27;s metro detroit and less than 5 minutes from Detroit itself. reply rootsudo 17 hours agorootparentprevI want to believe, but I also remember 2007-2014 $1 houses, GM bankrupcy and such - is it really different? reply ibejoeb 17 hours agorootparentIt&#x27;s different, and it&#x27;s not like it&#x27;s without its problems. Of course there are still depressed neighborhoods, but there are a ton of development projects going up, lots of new restaurants, bars, nightlife venues, and arts and sports programming. By \"coming up\" I mean on the upswing, and you can definitely see the difference from a few years back. reply queuebert 18 hours agorootparentprevExactly. That theft is only ~50 nano-Enrons. Not worth a life. reply FeteCommuniste 16 hours agorootparentLove \"Enron\" as a unit of theft, haha. Did you just make that up? reply queuebert 9 hours agorootparentYes. I&#x27;m in favor of comparing blue collar crime in units of white collar crime so that punishments are proportionate. reply doubled112 14 hours agorootparentprevBack when I was a gas station attendant, my manager made it very clear that if you&#x27;re being robbed, the only correct answer was to offer them smokes and help loading or bagging.This was not Detroit, or even the USA. Think slightly North. reply qingcharles 16 hours agorootparentprevI personally know of an incident where Walmart shelf stacker chased shoplifter of $5 hat out of store and got into a tangle with the getaway driver&#x27;s car and they ended up in a coma for three months which apparently cost Walmart almost $2m in medical bills. reply SilasX 18 hours agorootparentprevMaybe a nitpick, but I think it would be more accurate to say that it has, in recent years, become policy to not confront thieves, even verbally and non-violently because so many of them are now willing to escalate to lethal force on the most trivial of provocations, effectively giving them a heckler&#x27;s veto[1] on any measure except locking them up or asking the police if they&#x27;ll find time to arrest.I know in my grocery store training c. 1998 they definitely told us to give verbal warnings to thieves that we saw what they did, but now, even that much is dangerous.>Insurance covers material losses as long as reasonable precautions are taken.No one would get insurance to cover small, regular losses. And even so, insurance isn&#x27;t some magic pot of gold that restores the losses that lead to closure, it just means the impact is absorbed over everyone. It&#x27;s still there.[1] Recent discussion of the concept here https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37688954 reply jghn 17 hours agorootparentWhen I worked at a grocery store in the early 90s we were instructed to not intervene. So the trend isn’t as linear as you propose reply chupasaurus 16 hours agorootparentThere&#x27;s a video [0] back from 1984 or 1985 with a small up-and-coming band Metallica stealing from a store.[0] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XnPXn19Li6A reply bagels 14 hours agorootparentNapster: never forget reply SoftTalker 15 hours agorootparentprev\"insurance\" for small losses == higher prices for everyone. Some amount of \"shrinkage\" whether due to shoplifting, employee theft, or unknown reasons is built into the economics of a retail store. reply CPLX 17 hours agorootparentprevWhere did this insurance myth come from?Why do people think there&#x27;s some kind of insurance that has anything to do with people shoplifting from a standard retail store?Like that&#x27;s not a thing. There&#x27;s insurance for catastrophic losses and out of the ordinary stuff but there&#x27;s not an insurance policy that&#x27;s paying for people stealing from gas pumps, or grabbing a TV from an electronic store and running out the door. reply ryathal 16 hours agorootparentIt&#x27;s about insuring employees, not the items. Businesses have insurance so that if a worker is hurt during work, they don&#x27;t have to directly pay the related bills. That company will assume the risk that an employee helping a customer put a tv on a cart results in injury, but they absolutely won&#x27;t assume the risk of confronting a potential thief. reply autoexec 15 hours agorootparentStores like Walmart and Winn-Dixie used to take out life insurance policies on all their low level employees (\"dead peasant\" policies) so that every employee who died got them a payout anyway. I wonder if that&#x27;s changed their policies. reply trimethylpurine 17 hours agorootparentprevOf course it&#x27;s covered. You can insure anything (with very few exceptions protected by law for obvious reasons). An individual case is likely not worth claiming when considering the deductible. 800 gallons might be. But, it can definitely be covered. reply CPLX 16 hours agorootparentYou actually can’t insure everything. And you definitely can’t insure everything for less than the cost of just suffering the loss.There’s no viable or common policies that cover run of the mill retail shrinkage. It’s not a thing. Stores just have to eat it.Before you argue this point try to consider how such a policy could possibly work.You just total up what you think shoplifting is costing and tell the insurance company and they give you money? What business model is going on there exactly? How does the overall market work, where does the money come from that is reimbursing you for those losses? reply bluGill 16 hours agorootparentYou could insure that though. The cost of the policy would be higher than the cost of just eating the losses, but there are insurance companies willing to write the policy if you are stupid enough to pay for it. For things that happen all the time you are better off self-insuring it - that is eating the loss - than getting a policy. Insurance makes sense for rare events that are high enough loss that you would not want to self insure. Any common loss you should self insure, and the same for small losses. Only for larger losses that are not common are you better off with a pool of people who also might have that loss but probably won&#x27;t - odds are one of you will but not all so you all limit your costs. reply CPLX 15 hours agorootparentYes that&#x27;s my point.And since literally nobody wants a policy that reliably and predictably costs more than not having the policy it&#x27;s not a thing. And since it&#x27;s not a thing anyone wants insurers don&#x27;t really have it.Yes somewhere there&#x27;s like someone that might have something similar for sale, sort of. But the point of the comment is that in general businesses don&#x27;t have it.I constantly see people say that retailers don&#x27;t confront shoplifters because insurance will just cover it and it&#x27;s not worth it. It&#x27;s true they often don&#x27;t, but not for that reason, since it&#x27;s basically not ever insured short of a more extraordinary event. reply bluGill 11 hours agorootparentThere are special insurance companies that will insure anything legal if you pay the price which they will figure out. As you say it wouldn&#x27;t be cost effective, but it still exists if you want it anyway. reply trimethylpurine 16 hours agorootparentprevRisk is calculated on a per incident basis. Deductibles prevent frivolous claims. It&#x27;s pretty simple really. There&#x27;s no need to downvote just because you don&#x27;t understand something. That&#x27;s just petty. reply CPLX 16 hours agorootparentYou can’t insure repeated ordinary course of business losses that you have almost total control over and are a core part of your business function.You didn’t answer my question because it’s impossible to come up with a reasonable answer.In insurance the cost of losses is borne by people who did not suffer loss.So the cost of your car crash is borne by someone who didn’t have a crash. But they’re OK with it because they don’t know if and when it’ll happen to them. And an accident is a major loss, so they’d rather pay a little every month to avoid the uncertainty.But shoplifting is a week in and week out thing. If you have a shoplifting policy that’s reliably more expensive than the shoplifting why would you keep it?You wouldn’t because it’s not a thing.You can insure for major losses, robberies and burglaries and so on. But no retail stores can’t and don’t insure for things like someone driving off with a tank of gas, or pocketing an iPad, or a steak, or something. It’s not an insurable risk. reply otteromkram 16 hours agorootparentprevIf you make a claim, then your rates can go up because you&#x27;re now a higher risk.This is one aspect of insurance aimed at discouraging fraud. reply CPLX 15 hours agorootparentIt has nothing to do with fraud.Even if every business is completely and scrupulously honest this kind of insurance policy is completely unworkable because it&#x27;s not an insurable risk, it&#x27;s just a cost of doing business.It&#x27;s like trying to buy insurance to cover your monthly cost of airline travel. Like it&#x27;s not a thing, that doesn&#x27;t make sense. reply soderfoo 9 hours agorootparentprevAgreed. On the finance side, typically each store has a shrink allowance as a % of sales. The account is adjusted following the periodic inventory audit (usually annually). It rolls up to cost of goods sold so it is reflected in the margin.So if shrink was forecast at 0.75% of sales but actually came in lower the store would see better margins on their financials. reply iepathos 18 hours agorootparentprevOr you know... just the united states today. Gas station attendants get shot here without them even trying to play hero. At least a few every year. reply trimethylpurine 16 hours agorootparentMurder rate in the UK is roughly the same. It&#x27;s lowest in countries with low immigration rates per capita. Considering the lax immigration process in the US the crime rate is exceptionally low. reply ed_elliott_asc 16 hours agorootparentAre you sure? I had us murder rate as about 6 per 100k and Uk at 11 per million so much less than us.[1] https:&#x2F;&#x2F;www.macrotrends.net&#x2F;countries&#x2F;USA&#x2F;united-states&#x2F;murd....[2] https:&#x2F;&#x2F;www.ons.gov.uk&#x2F;peoplepopulationandcommunity&#x2F;crimeand.... reply webnrrd2k 16 hours agorootparentprevyour statement that \"the murder rate... is lowest in countries with low immigration rates per capita\" is largely bullshit. Even the laziest of googling \"crime and immigration rates per capita\" shows that.At best, the relationship is weak, and not well understood. reply ClumsyPilot 13 hours agorootparentprevA better correlation - Lower inequality countries have much less violent crime. reply tshaddox 19 hours agorootparentprevI’m not sure what you mean. It sounds like you’re implying that in the ideal lawful society, the gas station clerk is supposed to be the one defending the gas station against theft? reply mwigdahl 15 hours agorootparentYes. It is part of the job requirement and the essential character of a good wage slave for an hourly, minimum-wage employee to lay down their life with joy and zeal in defense of their employer&#x27;s profits and property. reply sdwr 19 hours agorootparentprevIn an ideal lawful society, a disagreement over gas won&#x27;t escalate into a violent confrontation. So the attendent can intervene, because they aren&#x27;t in physical danger. reply c22 17 hours agorootparentOf course in this sort of society no one is stealing gas in the first place, so the attendant has no reason to intervene. reply xp84 8 hours agorootparentA robbery there would go something like: (Note: for some reason this seems to work best in a New Zealand accent)“Listen up, sir. I intend to steal the contents of that till. Be a good pal and hand it over.”“I’m afraid not. As you know, I’m duty-bound to protect this till, and I’m not about to hand it over to you.”“I’m afraid I must insist. I intend to take it, all of it. Don’t make this uncomfortable. I don’t want to use harsh language, but I’m serious.”“I’m sorry, but it’s simply out of the question. There’s policies and regulations. I can’t just give this money out to you. If we did that, do you know how many people would be in here robbing us every day? There wouldn’t really be a gas station at all, would there? May as well shut the whole thing down then.”sigh “No, I’m sorry. I should have expected you weren’t about to give me the cash. I’ll leave. Obviously I’m not about to use physical force, and it’s clear you’re not budging on this one.” reply tshaddox 12 hours agorootparentprevYeah, if we&#x27;re dismissing the possibility of direct physical violence with the wave of a hand, why not wave the hand a second time and dismiss the possibility of gasoline theft? reply bratgpttamer 18 hours agorootparentprevThis is SOP among most chain stores&#x27; loss prevention policies.Sub out \"not worth an employees&#x27; life\" with \"not worth an expensive assault&#x2F;false imprisonment lawsuit\" if you&#x27;re extra cynical. reply hnburnsy 17 hours agorootparentMaybe fear of litigation from a liability lawsuit if the employee is injured or killed. reply 14 19 hours agorootparentprevHere in British Columbia, Grants law was passed requiring all gas purchases to be prepaid after a gas station worker named Grant was ran over and killed as he tried to stop someone attempting to gas and dash. Back then gas was much cheaper so someone died over like $50. Doesn’t need to be a lawless society just one where someone makes a stupid choice. And we have lots of stupid people out there. reply StrictDabbler 19 hours agorootparent\"The Law was named for Grant De Patie, a gas station worker who was killed in 2005 by an underage drunken driver ... Grant was dragged 7.5 kilometers before his body was dislodged from the white Chrysler LeBaron.\" reply cj 18 hours agorootparent\"Grant was writing down the license plate number as Darnell ran over him while stealing $12 worth of gasoline.\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Grant%27s_Law#:~:text=Grant%20.... reply HumblyTossed 18 hours agorootparentprevYou can&#x27;t honestly feel the attendant should run out there and try and stop a thief. You just can&#x27;t. reply shadowgovt 19 hours agorootparentprevIt&#x27;s pretty standard policy in most retail places.Stories abound of staff at Walmart and Target being fired for attempting to enforce store shoplifting policies (https:&#x2F;&#x2F;www.ksl.com&#x2F;article&#x2F;14319284&#x2F;4-walmart-employees-fir...). Broadly speaking, the risk of injury to other employees and customers, the risk of lawsuit if their staff misinterpret the law in the moment and overstep the authority each state gives them (a patchwork of law that stores are not interested in training its staff on), and mostly, the risk of brand harm if these stores gain a reputation for being a place where fights break out between employees and the public, regardless of reason, are far costlier than the opportunity cost of being unable to sell lossage.Instead, companies just track lossage and sales, and if a given neighborhood proves to have too many shoplifters, they pull up stakes and close the store. reply anonylizard 18 hours agorootparentIts not necessarily good that they just losses happen and close up store.I guess it gives room for small business owners, who are willing to shield themselves behind bullet barriers, willing to defend their own property with guns, and don&#x27;t care whatsoever about bad PR. reply c22 17 hours agorootparentI think it has to get really bad before they pull the store (a store takes a lot of money to install).Usually they just offset the losses by raising prices. reply jstarfish 14 hours agorootparentMy local big-box stores have taken to putting half the store&#x27;s inventory in locked glass cabinets. Started with liquor, then detergent, socks, electric sawblades, car batteries, regular batteries, flashlights, bike parts, medicine, Monster drinks...if it&#x27;s something you&#x27;ll need to survive the apocalypse, it&#x27;s behind glass.It&#x27;s unshoppable; nobody is ever around to unlock them. reply xp84 8 hours agorootparentYeah, I feel like policies like the locked racks were thought up before the great decrease in retail staffing levels. At current staffing levels, a lot of places are operating on what would have been considered a light skeleton crew, all the time. I bet there were more workers on the graveyard shift of a 24-hour Walmart 20 years ago, than there are at most stores during the day today (not counting pickers for delivery&#x2F;pickup orders). reply bluGill 15 hours agorootparentprevThey watch trends. If the store isn&#x27;t making money - losses is only a part of this - they will not invest. The building and parking lot is only expected to last 20 years or so - they will stop maintaining them to milk out more profit until they fall apart and then pull up shop. Stores that are making money get remodeled and the parking lot repaved. reply shadowgovt 15 hours agorootparentPrecisely. And when everyone knows that the commercial lifeblood of a town is so hinged on multiple factors outside people&#x27;s control, well... Might as well get those underpants while they&#x27;re still in town, right? If you don&#x27;t steal them and then Walmart pulls up stakes next month anyway because it makes more sense to invest in Tallahassee than your poor neck of the woods, you&#x27;ll just feel like a sucker. reply evanmoran 16 hours agorootparentprevYes, but it’s 100% that bad and they are closing stores because of it: https:&#x2F;&#x2F;www.kiro7.com&#x2F;news&#x2F;local&#x2F;90-target-workers-be-displa... reply qingcharles 16 hours agorootparentThe Walmart next to me closed due to rampant looting, and it was the only real store in a several mile radius.I was in there once and people were just taking stuff off the shelves and walking straight out with it like it wasn&#x27;t shit. reply Dylan16807 15 hours agorootparentThat&#x27;s very strange. If it&#x27;s happening enough you can pay a cop to come stand there, can&#x27;t you? reply qingcharles 7 hours agorootparentThere was a cop SUV parked outside the door and I never saw them do anything. replyshadowgovt 15 hours agorootparentprevNot to mention that small business owners can sometimes change dynamics, depending on why people steal.It is, sometimes, one thing if it&#x27;s Susan&#x27;s shop (you know Susan; she lives in the apartment down on the corner of Fourth and Mission, stealing from Susan would be like stealing from your own aunt) and another if it&#x27;s Walmart (you will never meet Sam Walton in your life, or any of the management of that company. There isn&#x27;t even a franchisee). reply bumby 19 hours agorootparentprevYou understand you took that quote out of context right?I think the OP meant:>better to lose some money than risk losing a longtime customer...[as a] policy makes a lot of sense reply rmilejczz 18 hours agorootparentprevOr Florida reply mmd45 20 hours agoparentprevmaybe its an nfc hack. pretty scary how wide open it is&#x2F;was: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=eV76vObO2IM reply nekoashide 13 hours agoparentprevBluetooth controlled? The device sure, for all we know it&#x27;s just kicking over the dispenser. The clerk isn&#x27;t looking but I&#x27;m sure if they look at the pump on the register it&#x27;s counting.It&#x27;s still all serial based comms, but unless the attendant is trained or aware they might just think someone is pumping. It will show up on the daily report when the dispenser report doesn&#x27;t match the sales report. reply Affric 13 hours agoparentprevIf the attack were better understood by people who didn’t perform the hack then it wouldn’t have happened.I don’t think any engineer in the last 20 years would put IR on the thing. At the same time I wonder “who would put Bluetooth on the embedded controller for such a machine?” reply yetanotherloss 4 hours agorootparentWith so many multifunction SOC embedded controllers out there it&#x27;s possible some one just didn&#x27;t cut the lines to the radio built into the board that was selected for other reasons. Would not surprise me to see a lot of control boards that expected firmware or dip switches to disable unneeded functionality versus having a different line for similar boards if they weren&#x27;t being made to a specific end customer anyway. reply amatecha 9 hours agoprevThe source article is so poor it&#x27;s mind-boggling. Is this the kind of \"news\" people actually spend their time reading? My IQ has temporarily dropped from subjecting myself to that.There&#x27;s no substantiation that Bluetooth was used, nor that the perpetrator(s) is&#x2F;are from Detroit. The most detail I&#x27;ve found, including quote from Detroit PD, is here: https:&#x2F;&#x2F;www.detroitnews.com&#x2F;story&#x2F;news&#x2F;local&#x2F;detroit-city&#x2F;20...Reading between the lines, it sounds like one person performed the hack, and numerous people then proceeded to fill up for free -- knowingly or otherwise. There&#x27;s no specific assertion anywhere that one particular individual stole 800 gallons of fuel. Of course that doesn&#x27;t stop dozens of news sites all over the web from reporting inaccurate \"information\". reply throwaway2037 1 hour agoparentYet another garbage click-baity article on HN. Look at the news source: Fox News local. reply mrweasel 20 hours agoprevThe article seems a little light on details. I don&#x27;t expect them to explain exactly how it works, but is this a bug&#x2F;feature in the pump design?If you can just roll up to the pump and communicate with it over Bluetooth, then I&#x27;d argue that it&#x27;s a design flaw and the manufacturer should be held accountable as this seems like a gross laps in security. reply bgroins 19 hours agoparentThe article is poorly written. \"Scammers are using cellphone&#x27;s Bluetooth option to hack the pump - and get it for free.\" is all the detail you get. My cellphone&#x27;s Bluetooth doesn&#x27;t have that option. reply jiscariot 14 hours agorootparentThis sentence doesn&#x27;t know if it&#x27;s in the local paper or at a slam poetry reading:\"Paying at the pump is for chumps - when you can get gas for free - and illegal, but it didn’t stop a Detroit man from stealing almost 800 gallons of gas at the Shell at Eight Mile and Wyoming.\" reply jacamera 14 hours agorootparentI felt like I was having a stroke multiple times while trying to get through this article. reply extraduder_ire 15 hours agorootparentprevIf you&#x27;re on android, there&#x27;s a bunch of bluetooth-serial apps that let you send more or less whatever raw data you want.On iOS, there&#x27;s nrf connect. Which is slightly more limited, but can still do BLE. I think there&#x27;s other bluetooth-serial apps for iphones, but apple has a stick up their butt about bluetooth-classic not working super good unless the other device has a \"made for i\" chip&#x2F;cert in it.If either of these don&#x27;t work well enough, ESP32s can be had forOn iOS, there&#x27;s nrf connect. Which is slightly more limited, but can still do BLE. I think there&#x27;s other bluetooth-serial apps for iphones, but apple has a stick up their butt about bluetooth-classic not working super good unless the other device has a \"made for i\" chip&#x2F;cert in it.Is BLE why \"modern\" iphones don&#x27;t have issues with android&#x2F;3rd party devices&#x2F;accessories? I remember a long time ago when an iPhone meant you could only share bluetooth with other Apple devices. My iPhone today doesn&#x27;t seem to be like that. reply pineconewarrior 17 hours agorootparentprevIt&#x27;s a local news piece so it&#x27;s not meant for a technical audience. That&#x27;s why they lead with the typical \"random bystander\" quotation. reply spelunker 18 hours agorootparentpreviPhone 15 feature obviously, time to upgrade! reply dylan604 18 hours agorootparentprevYou have to enable the DevTools, but that requires a $99 annual fee. If you can&#x27;t afford gas, it&#x27;s not likely you can afford that fee too. So it&#x27;s kind of a perfect catch-22 reply extraduder_ire 15 hours agorootparentWhy would you have to enable devtools? Also, with just an unpaid icloud account, you can build, sign, and install apps to devices with an expiry time of one week. reply monkpit 18 hours agorootparentprevIf you steal gas you can afford lots of things, so this logic doesn’t really track.Also, you assume that someone wouldn’t just sell the gas they stole, which is probably very much worth the small investment in tools. reply adolph 15 hours agorootparentIt&#x27;s better to just return it to a different gas station. Selling gas is more difficult than you might think because of rampant dilution&#x2F;adulteration in the secondary market. Can&#x27;t just go door to door saying \"Hey I bought too much gas\" or \"My car only takes 93 octane\" -- nobody trusts anyone anymore.When you make a return without a receipt you generally just get store credit instead of cash. If you don&#x27;t need some chips and tallboys, you can typically sell the credit to someone behind the station for .3 to .4 of the face value. That isn&#x27;t great but since the gaming machines only take cash its a solid way to get liquid. reply thatguy0900 12 hours agorootparentHow could you return gas to a gas station? Why&#x2F;how would they take it back, especially with no receipt? reply adolph 12 hours agorootparentJust act normal and ask for the manager. Like the parent comment noted \"probably very much worth the small investment in tools.\" Be sure to get one of the fancy black rubber hoses instead of the green ones so things look more professional. replyAntipode 18 hours agoparentprevYes it&#x27;s a clear design flaw but it&#x27;s still theft. It&#x27;s not like you&#x27;re legally allowed to rob a store if the front door lock isn&#x27;t installed properly and doesn&#x27;t work. reply theultdev 17 hours agorootparentNo we punish the companies now, not the thieves, see Kia and Hyundai. reply mrweasel 17 hours agorootparentI think the point of the Kia and Hyundai cases is that customers had some expectation that the locks would actually provide a higher level of security than they actually did. Otherwise why even bother having locks? Saying that \"stealing is illegal and we should pushing the thieves not the companies for failing to provide security\" completely negates the point of having any security industry.If you provide a product that is intended to offer a certain level of security you should get punished when your product fails in a spectacular manor. reply declan_roberts 17 hours agorootparentWeird how the level of protection the Kia and Hyundai locks provide change depending on whether your local law enforcement agencies are effectively able to deter theft? reply thatguy0900 12 hours agorootparentTo some extent, but the TikTok craze was just crashing it, which happened to my parents. The car made it all of 30 ft and they hit a tree. Honestly not sure what local police can do to stop that. reply vuln 17 hours agorootparentprevGlass breaks very easily. Locks don’t matter. reply callalex 17 hours agorootparentYou can start a car by breaking glass? That’s even more impressive than the hack in the article! reply vuln 17 hours agorootparentIs an ignition a lock? How about the wiring to the ignition…?It would be one thing if Kia and Hyundai advertised “anti theft” via an immobilizer then didn’t actually put one in. They just left it out because the company is being cheap. The cars being sold with this vuln are cheap. You get what you pay for. reply anaccountforme 16 hours agorootparentModels affected by this extend well into the 30k range. This is not just affecting the cheapest possible vehicle someone can buy and we&#x27;re running into people not setting expectations accordingly. Equally cheap vehicles from other manufacturers are not nearly this easy to steal, particularly by completely inexperienced thieves. reply olyjohn 12 hours agorootparentHave you seen how to steal a Honda built between 1972 and 2000? You get a key from any other Honda, and jiggle it, and off you go. reply vuln 16 hours agorootparentprevYes 30k is cheap when the Average cost of a new car is 48k.https:&#x2F;&#x2F;www.moneygeek.com&#x2F;insurance&#x2F;auto&#x2F;average-price-of-a-...This is even worse when vehicles from both brands have absolutely terrible resale value and depreciation. reply anaccountforme 14 hours agorootparentCheaper than the mean perhaps, but again not bottom of the barrel for many of these models. However, even if you want to keep focusing on these being &#x27;cheap&#x27; vehicles, it STILL stands out in the industry as highly unusual. Even back in 2015, 96% of vehicles from other manufacturers had these devices (https:&#x2F;&#x2F;www.iihs.org&#x2F;media&#x2F;0e14ba17-a3c2-4375-8e66-081df9101...). It&#x27;s a pretty easy argument to make that consumers would not be looking specifically for a feature that is near universal elsewhere. It was an abnormality and Hyundai&#x2F;Kia will, unsurprisingly, pay for it in the long run. reply smsm42 14 hours agorootparentprev30k car is definitely not \"cheap\". That statistics is skewed by 1) high-end cars (Bill Gates walks into a bar, everybody&#x27;s average income doubles) 2) the fact that people that can&#x27;t afford expensive cars just buy used ones. That doesn&#x27;t make the new ones less expensive in any way. reply kyubey 13 hours agorootparentprevStating \"You get what you pay for\" is a strange way to both blame the victims & attempt to absolve multi-billion dollar corporations for not including a standardized piece of security equipment, when you consider that every major manufacturer sells vehicles firmly in the same price bracket - up to half the price of your reported average (we love to forget about outliers, don&#x27;t we) that have this equipment.Pedantry about what security was advertised or not will not make a compelling case. What a strange thing to try to argue. reply olyjohn 12 hours agorootparentNobody even knows what an immobilizer is or how it works. They put key in car and go. People don&#x27;t even read the manuals that come with the car. Most people don&#x27;t even read signs along the freeways while they&#x27;re driving.This kind of willful ignorance about cars goes so far that insurance companies willfully chose to insure these cars against theft, and claimed they had no idea that the cars didn&#x27;t have immobilizers. And now they are suing the manufacturers. It&#x27;s the insurance company&#x27;s job to evaluate risk. All they had to do was read the fucking manual and if it didn&#x27;t say something in there, ask. reply Dylan16807 16 hours agorootparentprevIt&#x27;s not expensive enough to be worth leaving out in basically any car.Basic security shouldn&#x27;t be a premium feature. reply vuln 16 hours agorootparent> It&#x27;s not expensive enough to be worth leaving out in basically any car.> Basic security shouldn&#x27;t be a premium feature.That’s your opinion presented as a fact. reply dontlaugh 12 hours agorootparentIt’s the law in most countries. It is a fact, averaged out globally. reply Dylan16807 15 hours agorootparentprevIt is, which is pretty normal for opinion posts.But your argument that this is okay is also an opinion. reply vuln 15 hours agorootparentI’m stating the manufacturer did not advertise this security feature, nor did any of the purchasers really care until their cars started getting stolen and used to cause mayhem.You stated a basic security feature such as an immobilizer should be installed by default because it’s cheap.Am I missing something? reply Dylan16807 15 hours agorootparentThe implication of \"it would be one thing\" and \"you get what you pay for\" is that this product offering is acceptable. Which is an opinion.If there was no implication intended, then \"it would be one thing if X\" would be a tautology. Which wouldn&#x27;t make sense as a post.And then in contrast, my opinion is that this product offering is not acceptable. replytnel77 17 hours agorootparentprevWhy not both? People shouldn’t steal cars and companies should probably be held accountable for gross negligence. reply turquoisevar 16 hours agorootparentprevThis is not a zero sum game.You can punish both.Especially considering immobilizers have been mandatory in the EU since the ‘90s and cost a manufacturer less than $1.This is not some state of the art high tech high cost mitigation. reply sib 15 hours agorootparentWell, then I suppose you could rightfully punish the manufacturers for cars sold in Europe that don&#x27;t have them. But, you should probably not punish manufacturers for cars sold in the US that don&#x27;t have them until you make it illegal to sell cars without them in the US... reply turquoisevar 14 hours agorootparentThere’s the concept of negligence.If you distill it down, it essentially comes down to not doing things that no law explicitly requires (lest you end up with walls and walls of law books simply describing very specific matters), but can be reasonably expected of you to have done.What’s reasonable depends from time period to time period and the norms that exist, nevertheless the concept exists.A good example would be you owning a house with a balcony in an unincorporated area where no law exists that sets requirements on structural integrity of such things.You’re aware that the balcony might be a bit iffy and it has been decades since last it was checked out by a professional.You have guests over, you don’t bother telling anyone to stay away from the balcony. Some of your guests walk underneath the balcony, or worse, they go out onto the balcony to have a smoke.The balcony collapses and your guests get injured or die.You’re on the hook for negligence. Specifically negligence that resulted in tort in this case.Hell, in some places, depending on the prosecutors in the area and the laws on the books, you might even be criminally liable for negligent homicide.My comment on the EU and the regulations there isn’t to imply that the laws there apply to here in the US, rather it’s used (and could be used in real life) as a way to argue that the solution is inexpensive and not implementing it could be considered negligent because there is an easy solution available due to that regulation. reply sib 14 hours agorootparentSure, negligence is a very useful concept.But what we&#x27;re talking about here is that there are \"bad guys\" (tm) who stopped by your party and used a chain saw to cut the supports under the balcony, which would not otherwise have fallen down. But you could have prevented this if you had reinforced the supports with steel posts.The Kias and Hyundais didn&#x27;t steal themselves. Let&#x27;s put the blame where it lies, on the f**ing criminals. reply more_corn 15 hours agorootparentprevAre you really taking the side of the idiots who made a modern car you can Hotwire in 45s with no special tools? And then refused to issue a recall and fix the problem? reply nekoashide 13 hours agoparentprevThe pump itself is the least secure piece of technology in existence. They keys to open them are common, the communication is all serial based and hasn&#x27;t changed at all in 40+ years.It wasn&#x27;t more than a decade ago that I was swapping 3des pinpads out. If you had really old pump all you needed was a small screw driver to stop the meters from turning after you popped the front panel off. reply ohduran 20 hours agoparentprevyeah, I opened it just in case they explained how it&#x27;s done. For research purposes of course. reply JudasGoat 12 hours agoparentprevhttps:&#x2F;&#x2F;www.directindustry.com&#x2F;prod&#x2F;levtech-service-producti... reply userbinator 20 hours agoprevWhat are the pumps even using Bluetooth for?I suspect it might&#x27;ve been something like a default feature of the embedded PC that they didn&#x27;t bother to disable, so you can \"plug in\" a BT keyboard &#x2F; mouse and take control of the system. reply wildzzz 18 hours agoparentGas pump hacks in the past have been related to opening up the cabinet and adjusting the flow sensor to lower the sensitivity so that you pay for way less gas than what actually came out of the dispenser. These sensors have an interface between them and the controller so it could be possible to add some MITM device that can edit the flow reading to something much lower or none at all while gas continues to come out of the dispenser. Add Bluetooth to the device and you can turn it on and off at will so that no law-abiding citizen complains about the pump supposedly putting out less gas than what they need. These pumps aren&#x27;t inspected frequently so if the replica tamper sticker looks good, the pump puts out the right amount during a state inspection, and you aren&#x27;t so greedy that the owner notices, you might get away with the scam for a long time. 800 gal is a lot of gas and could have easily filled tanks for at least 50 cars. This guy could have slowly stolen 800 gal over the course of a year and probably no one would have noticed. reply at-fates-hands 16 hours agorootparentThis happened back in the 90&#x27;s so I&#x27;m sure things have changed a bit since then.One of the guys I went to high school with worked the swing shift at a gas station in our town. The shift was 11pm-11am. It was a small town so the gas station rarely had any business after midnight. I guess the guy got bored and started reading all the manuals the manager had in his office.To this day, I&#x27;m not even sure how he did it, but he managed to change the price of gas&#x2F;gallon on two of the four pumps down to 5 cents a gallon. Then he proceeded to call all his friends and family to come fill up for almost nothing. He reverted it back sometime around 5 or 6am. It took the station a few months to figure out what had happened, but by then, we both had graduated and were on to college. I remember running into him at a bar when I was home and he told me the whole story.When I worked there (before he was working the night shift) I remember there was a box on the floor by the cash register which displayed the price of gas. It had a sizable chunk of cables going in and out of the box, along with some buttons on top near the display. The manager at the time told explicitly not to fuck around with that box, its a federal crime and I will go to jail. I&#x27;m guessing the guy figured out how to change the price from that box.To this day, I still have no idea how he did it. reply bluGill 15 hours agorootparentThe manager always has power to change the price of gas. At the big chains the manager is required to drive around and note the price of all the other stations and then apply that to some other formula to decide what price to mark. If any station doesn&#x27;t everyone in town figures out where the low price is and that station runs out of fuel and has to pay for an expensive emergency delivery. This is why stations in the same area all have the same price.Your story is from the 1990s, just based on the state of technology we can assume that the pump prices were set at a computer inside the station that wasn&#x27;t networked. The computer probably had a modem that called headquarters once a day to upload information, but the manager directly edited the prices when there was need to change prices.Note that the manager drives around to look at the posted prices on all stations around. The manager is not allowed to talk to the managers of each station as this would violate federal monopoly laws (this does happen all the time, but legally they cannot do it and that is policy any chain will have - they may or may not ignore a manager ignoring the law depending on if they think they would be caught). reply SoftTalker 15 hours agorootparentWhy drive around? They could just call, pretending to be a random person, and ask \"what is your gas price today?\" reply bluGill 13 hours agorootparentBecause the phone company as a log of who called who, and if the feds see that record they will assume illegal collusion. No sane franchise will allow a phone call in their policies as that policy is allowed in court as evidence they are talking to the other station. If you drive and write down prices that is the easiest way to stay legal.I&#x27;m sure gas stations do illegal collusion all the time in some towns. However it is illegal and so at least some station managers will not do it and all company policies will say drive. reply TedDoesntTalk 14 hours agorootparentprev... or use the GasBuddy app reply peteradio 18 hours agorootparentprevThat sounds plausible but the bluetooth is barely part of making the scheme work, in your scenario they directly modified the hardware. reply seanalltogether 18 hours agoparentprevAs someone who works in IOT, my guess is this is a new feature for servicing. Letting a technician communicate with appliances, machines, etc.. over bluetooth using their phones is becoming more and more popular and the security around these features is designed to be as simple as possible to avoid calls to tech support. Getting access could be as simple as entering the service companies id. It&#x27;s like how heavy machinery drivers just leave the keys in their vehicle cause it&#x27;s easier to share and they assume no one else will try to use it. reply bluGill 15 hours agorootparentHeavy machinery all has the same key unless the customer paid for the option to have different keys. Most modern stuff has a GPS attached and some form of cell connection: the machine won&#x27;t start if the machine isn&#x27;t in the geographical area it is supposed to be. If you have the key you can start anything. reply 34679 20 hours agoparentprevThey could be keeping logs of unique identifiers to sell to location data brokers. reply ta1243 19 hours agorootparentI&#x27;ve been looking at my unifi SSID log recently, although most of my APs are in a secure area (there&#x27;s a ring of steel), clearly some are close enough to the road to pick up a fair few cars over the previous 24 hours -- 114 Audi_MMI_nnnn APs (and a few like \"Frazer&#x27;s Audi\", quite a few \"My Skoda&#x2F;SEAT&#x2F;VW 1234\" too reply JKCalhoun 18 hours agorootparentStill war driving in the 21st Century. I love it. reply c22 15 hours agorootparentThis sounds like reverse war-driving. reply xur17 14 hours agorootparentprevIf you buy a cheap ($20) software defined radio, you can listen to the TPMS sensors in cars, and use this for tracking. reply pizzaknife 18 hours agorootparentprevI both love and worry about this reply ta1243 17 hours agorootparentBest one I saw was an SSID of \"C:\\\\Virus`567*76.exe\"No jndi hotspots reply user_7832 12 hours agorootparentI&#x27;m a little disappointed to see people forgot about little Bobby tables again :&#x27;( reply tivert 19 hours agorootparentprev> They could be keeping logs of unique identifiers to sell to location data brokers.Not to say no one did that, but it seems pretty stupid to integrate that into the pump itself (as well as extra trouble).Seems like if someone wanted to do that, it would be far easier to those companies to just ship a Raspberry Pi-sized device and bolt it to the side&#x2F;top of the pump, or put it up in the canopy with the lights. reply Aurornis 18 hours agorootparentprevThe article says thieves are connecting to the system as clients and manipulating settings of the pump.A data gathering system wouldn&#x27;t need client connections nor would it realistically even be connected to the pump controls.They&#x27;re just accessing some Bluetooth service interface for the pump. reply spicybright 20 hours agorootparentprevNot wrong. Retail stores do that with wifi (and probably bluetooth now) all the time. reply gymbeaux 19 hours agorootparentOn a related note, I recently went out of town and received several spam calls from phone numbers in that area. Some app on my phone seems to be selling my location data to these spam callers. reply brookst 19 hours agorootparentprevIs that still a thing in the era of randomized MAC addresses? reply asynchronous 20 hours agoparentprevThis seems like a likely answer. reply cozzyd 20 hours agoparentprevMaybe it&#x27;s the cashier&#x27;s computer inside the station? reply CyberDildonics 19 hours agorootparentBluetooth does not have the range to do that. reply rtkwe 19 hours agorootparentRF range is down to how good your antennas are and the local environment rather than hard limits. With a good environment I can pretty easily get about as far away as pumps are from the building without losing connection. PHone and headsets are generally using tiny antennas to fit their form factor so they fall off quick. reply CyberDildonics 17 hours agorootparentBluetooth is 2.45 ghz.Bluetooth® technology supports transmit powers from -20 dBm (0.01 mW) to +20 dBm (100 mW). reply hinkley 16 hours agorootparentAnd yet Bluetooth 5 claims &#x27;up to&#x27; 240 meters.I wouldn&#x27;t base a national franchise&#x27;s business on what BT 5.0&#x27;s &#x27;up to&#x27; states, but it&#x27;s not impossible that they would rely on at least 50 meters working. I can think of very few gas stations that have a separation from pump to cashier of more than that and most of them are truck stops. reply cozzyd 16 hours agorootparentprevNothing is stopping you from using a high gain antenna though (well, other than regulations, but if you&#x27;re stealing has you presumably don&#x27;t care about FCC violations...) reply CyberDildonics 15 hours agorootparentOriginally you were talking about bluetooth being there for the cashier to work the pumps, now you&#x27;re talking about what the thief is doing, which are two different things. reply cozzyd 15 hours agorootparentI probably wasn&#x27;t clear... I meant perhaps they connected a bluetooth mouse or something to a cashier&#x27;s computer and gave themselves free gas. reply CyberDildonics 13 hours agorootparentI didn&#x27;t get that at all, but that&#x27;s an interesting idea. I&#x27;m not sure how the rest would work, you would have to see the screen.With a keyboard I guess you could use a sequence to pop up certain windows and run certain commands. replyRetric 20 hours agoprevMeanwhile your license plate is captured on camera. This is identical to someone hitting pay inside then driving off.Edit: Looks like pump then pay is more regional than I assumed. Yes it’s still a thing in some parts of the US in 2023. reply pbmonster 20 hours agoparentThe interesting question is how often they notice, and whether the pump keeps logs. Because not everybody is stupid enough to invite their entire neighborhood to a free-for-all.Just drive up, unlock pump, fill up, lock pump, leave.I would not be surprised if the pump doesn&#x27;t have a time-stamped log of Bluetooth service function calls. And even if it does, how long can you run this scam before they notice missing inventory from their underground tanks? How long do they keep CCTV footage? reply ethbr1 20 hours agorootparentThey could correlate by absence of a matching receipt.Strip out every timestamp correlated with a sale.Strip out no object at a pump.Scan through the remainder. reply pbmonster 19 hours agorootparentYou&#x27;re talking about running image detection on the video footage and correlate with payment system receipts?You could probably hack that together in python in 15 minutes, but I doubt there&#x27;s commercial software offering that functionality. Which means nobody (outside a digital forensics firm) is doing that, and certainly not for $60 worth of gas.And still, that requires the station even noticing missing inventory. I really wonder how long people have been using that hack, you&#x27;d probably check for leaks in the underground tank before checking whether your pumps have been compromised... reply dylan604 18 hours agorootparent>You&#x27;re talking about running image detection on the video footage and correlate with payment system receipts?Or, do what has been done since the beginning of video recordings used in investigations...have a human sit in front of the screen scanning the footage. reply ClumsyPilot 13 hours agorootparentYou are potentially paying a human to watch mutple days of video footage to detect $60 of gas. Even at minimum wage, that could be a loss.And then you find out the car was a white Toyota corolla and it&#x27;s missing a number plate, now what? reply dylan604 13 hours agorootparentYou&#x27;re advocating paying a dev at a significantly much higher hourly rate to develop something that will find Ryan Gosling stole the gas from Santa&#x27;s sleigh with a pattern of snakes painted on it. Now what? reply bluGill 15 hours agorootparentprevIf you can make such a program in 1 week there are plenty of franchises willing to pay for it. It doesn&#x27;t have to be very good, even if 50% of what you show is a false positive and you only catch 20% of all crimes you say that is still more than enough for a human to look through everything you flag and get the police involved. While it probably directly costs more than eating the loss from these crimes, you only need to catch a few cases to be worth it. reply micromacrofoot 20 hours agorootparentprevLike most crimes, that seems like a lot of assumptions to risk a felony charge reply brookst 19 hours agorootparentIt’s possible that, like a lot of criminals, the decision to proceed was not based on a complete and well-formed risk analysis. reply micromacrofoot 18 hours agorootparentof course, but I&#x27;m responding to a risk analysis reply Aurornis 18 hours agoparentprev> Meanwhile your license plate is captured on cameraIf the thieves are going to such lengths as to hack the pumps, they&#x27;re likely taking the 60 seconds necessary to tape over their license plate.This is an extreme premeditated attack, not someone running off with gas on a whim. reply sethhochberg 18 hours agorootparentHell, if you&#x27;re doing it all with Bluetooth anyways, get one of those electronic license plate cover devices that have become a plague with all of the worst drivers in every town who try and avoid tolls and red light cameras... reply pierat 15 hours agorootparentHoly shit, you weren&#x27;t joking. Search Scamazon for \"license plate flipper\"https:&#x2F;&#x2F;www.amazon.com&#x2F;Universal-Electric-Rust-Proof-Weather...Verified Purchaser: \"I like that I don&#x27;t have to pay for tolls\" reply adolph 15 hours agorootparentprevThat sounds expensive&#x2F;complicated compared to the ubiquitous paper plate.How Texas Paper Tags Became a $200M Criminal Enterprise: https:&#x2F;&#x2F;www.nbcdfw.com&#x2F;investigations&#x2F;how-texas-paper-tags-b... reply rtkwe 20 hours agoparentprevMy memory is that pay inside was a forced prepay option? My entire adult driving time has been after the advent of card readers being on basically every gas pump so I&#x27;m going off vague childhood memories of going in to pay for my parents. reply Retric 20 hours agorootparentDepends on the station but pump then pay is still a thing, but seems to be regional as this post from 2023 shows.https:&#x2F;&#x2F;www.cleverdude.com&#x2F;content&#x2F;4-things-you-need-to-know... reply gadders 20 hours agorootparentprevPump then pay is the default in the UK, but card readers on pumps are becoming more common. reply mhandley 19 hours agorootparentAnd in most places, what they lose on fuel theft, they more than make up on sales in the petrol station shop which is where they really make their money, so they have little incentive to change. reply jstarfish 14 hours agorootparentprevWhen I still had a motorcycle (early 2000s), I&#x27;d find a lot of rural stations didn&#x27;t have modern tech on the pumps, and there were a lot of stations that still allowed pump-then-pay.Some stations just have shitty tech. A station near me fails open if you initiate a credit card transaction, take the pump off the hook, and then abort the transaction. reply mikece 20 hours agoparentprevExcept that hitting \"Pay Inside\" probably activates the \"get high res photos\" function because that&#x27;s a known fraud profile. If the pump is tricked into giving gasoline for free how long does it take for the gas station employee(s) to notice something went wrong? reply jjkaczor 20 hours agorootparentInteresting - in Canada, if you want to \"Pay Inside\", you have to go in and actually pay - they authorize the maximum transaction, and it unlocks the pump and sets it to a maximum amount. If you pump less than that amount, you pay only for what you use. But, OTOH - Canada seems a bit ahead of the US when it comes to Interac&#x2F;debit-card transactions in general (... but also a little \"behind\" Europe and AU&#x2F;NZ...) reply mikece 20 hours agorootparentA lot of places in the US are either pay by credit card at the pump or pre-pay inside only. I&#x27;ve also seen where \"pay inside after\" only works if you leave your driver&#x27;s license with the attendant inside before the pump is activated. reply throwbadubadu 20 hours agorootparentHuh, interesting.. the usual default in most of Europe is pump then pay, with nothing required beforehand, they have your license plate on video anyway? reply salad-tycoon 20 hours agorootparentIt used to be like this in the US too. Now we have “upgraded customer interactive digital displays” that stream ads, TikTok, “cheddar news” , and propaganda into your brain while you pump and I haven’t seen a pump then pay in some years. reply opello 19 hours agorootparentI recently learned that you can mute some of these! I&#x27;ve had success with the top right or second from the top on the right button near the screen. No reason not to try them all if you&#x27;re a captive audience anyway... reply fuzzzerd 17 hours agorootparentI spam all the buttons on any pump to attempt to mute it, and like you have had success with top right, or second from top on the right having the highest success. In my area, I&#x27;m only about 1&#x2F;4 in being able to mute it, but when it works, usually top right. reply olyjohn 16 hours agorootparentI wonder if I could just get a powerful electromagnet and use it to rip the coil out of the speaker. Would be a benefit to society. reply willcipriano 19 hours agorootparentprevYou can&#x27;t buy a toothbrush without calling a attendant where I live, no way they let you pump $100 of gas without paying first. Pay after was the norm when I was a kid though. reply registeredcorn 19 hours agorootparentprevSide note: If you&#x27;re at a pump with those stupid TV things, try and hit...right side, second button from the top. This usually activates mute. Some people go to the effort of putting little stickers indicating where the mute button is. Other times you can tell which one it is if the button is heavily scuffed. If that button doesn&#x27;t work, just try pressing each of the other ones on there. In my own experience, it doesn&#x27;t hurt to try. I think one time the left side, bottom button worked too.Whatever the case, I find those infernal things to not only be obnoxious, but a potential safety risk. I mean, you have a customer literally in the process of handling large quantities of dangerous, liquid flammable material which emits fumes that can be ignited by a spark, with potentially inadequate controls in place to stop it killing people. So, let&#x27;s distract them with shrill, unwanted blather-boxes showing distracting imagery. I make a point of trying to avoid gas stations that have those stupid things installed precisely for that reason - I don&#x27;t trust that location to put my safety above them making $.0002 from some shady ad network.I just can&#x27;t stand that there is such a concerted effort for this war on silence. Just give me a few minutes of simple contemplation. reply ajford 17 hours agorootparentJust a personal anecdote, but miss-timing your button-press sometimes results in a car-wash being added to your pump charge.I was trying to hit skip or cancel or something (this was in the Before Times, barely remember what it was asking) and what was the skip button became the \"Yes, I&#x27;d like a car wash\" button as the next ad started. I always suspected this was a bit of Dark UI level trickery, but could also have just been unfortunate button selection between two separate ads.Since then, I just avoid those buttons when ads are playing lest they get me again! reply mindslight 18 hours agorootparentprevIt would be better to put some tape over the speaker grille to cut the volume by a good amount, since it would help the next person and generally improve the place. Hostile defaults and having to repeatedly do some action to \"opt out\" means the shitheads essentially still win modulo an illusion of choice, like so many modern dynamics. reply salad-tycoon 16 hours agorootparentWhile I fully support this if user hostile then hostile user idea, I remembering hearing stories about people putting Biden stickers on pumps getting charged with crimes. Apparently any sticker or even magnet could be argued to be an attempt at changing its pumping mechanism. So maybe just be discrete. reply mindslight 16 hours agorootparentThere&#x27;s always the possibility of malicious arrest&#x2F;charging&#x2F;prosecuting when bucking the authoritarian order of just accepting that it&#x27;s proper and just for shit to be rolled downhill. Funny how there&#x27;s never any charges against hostile pump owners for assault, disturbing the peace, possible stalking [0], etc. The answer is to be aware of the vanishingly small possible downside, do it regardless, and then be personable to mitigate the damage if you do end up drawing aggro.(Not that anti or pro Biden stickers terribly upset that status quo. The problem is having upset the wrong person with a modicum of power)[0] If the nuisance pump is tied into the commercial surveillance databases for choosing what content to blast. I&#x27;ve no idea if we&#x27;re there yet. replysoco 19 hours agorootparentprevUnattended&#x2F;night stations still work like this: you pre-authorize your card then pump to get charged. reply Loughla 19 hours agorootparentprevMost rural areas away from interstate highways can still pay after you&#x27;ve pumped, without needing to leave your license or anything else. reply ziddoap 20 hours agorootparentprev>in Canada, if you want to \"Pay Inside\", you have to go in and actually pay - they authorize the maximum transaction, and it unlocks the pump and sets it to a maximum amount. I&#x27;m in a good-sized city, and every gas station I&#x27;ve been to lets you pay inside after filling up. The only ones that require pre-pay (either by going inside as you described, paying for a max amount, or by CC) are the farthest pumps away from the store. reply pizzaknife 18 hours agorootparentthis is insane to me. I have not been to a single station in the continental united states in the last 7 years where \"pay inside\" does not equate to \"pay first\"That being said - perhaps its an \"attendant\" state? Several states have laws which restrict motorists from pumping their own gas, is this the scenario which you are talking to?edit: grammar reply matsemann 16 hours agorootparentLaws saying you can&#x27;t pump your own gas? Having to prepay inside? Wow, I&#x27;m flabbergasted, and it&#x27;s a bit funny you&#x27;re thinking it&#x27;s insane the other way around. I guess it&#x27;s what you&#x27;re used to.When tanking before paying, I also used to get into the car and move it so that someone else could fill while I was inside(it would start a new transaction when you put the nossle back), probably to you look like a dash from the bill. Then walk inside saying \"hundred and fifty something on pump 3 is mine\".But I haven&#x27;t seen a pump not accepting card in what, 20 years, here? So haven&#x27;t used anything else for ages (and now got an electric car anyways) reply ziddoap 18 hours agorootparentprevI&#x27;m in Canada, not the US. But no, no attendant.You fill up for $X, then walk into the store and say \"I filled up on pump 3\", then you pay. reply pizzaknife 18 hours agorootparentthank you! This pattern essentially disappeared from the US with the exception of perhaps the very smallest and remote locations (public accessible) seemingly since the late 1990s (I do remember this pattern as you have described, just not in the last 20years) reply pests 15 hours agorootparentprev> Several states have laws which restrict motorists from pumping their own gasJust one now. Oregon just starting allowing it last month so New Jersey is the only one left. reply aceofspades19 16 hours agorootparentprevI&#x27;m in Canada and I have never seen a pump or gas station in the last 10+ years that allow you to pay after filling up. I had thought all the stations have changed since incidents like this: https:&#x2F;&#x2F;www.theglobeandmail.com&#x2F;news&#x2F;national&#x2F;attendant-drag... reply aembleton 7 hours agorootparentOne in Yahk, BC let me pay after filling up last week. In fact, that was the only option there. reply ziddoap 14 hours agorootparentprevPerhaps only BC made the switch? Not sure. But I filled up this morning and paid after the fact at PetroCan, so it&#x27;s definitely not a Canada-wide thing. reply ajford 17 hours agorootparentprevThat was normal in Texas ~15-20yrs ago. But in the intervening time, I&#x27;ve seen that become less common. I&#x27;ve run into places that won&#x27;t let you pay after and will instead just ask you to prepay (full transaction) and come back in to get the difference refunded.But I&#x27;ve also ran into plenty of places that if you hit \"pay inside\" on the pump will simply just turn the pump on, or will call you via the pump intercom (is that common everywhere?) and tell you they&#x27;re turning the pump on for a max of XX gallons and to prepay or use the card at the pump if that&#x27;s not enough. reply bluGill 15 hours agorootparentThe pump intercom is done because then you know someone inside actually saw you and is expecting you. If you drive off without paying they are much more likely to notice and call the police in time for the police to catch you. Or at least that is what they want you to think. reply ajford 14 hours agorootparentYeah, I figured the intercom call was to get you thinking that they&#x27;re watching closely, even though they&#x27;re usually too busy with other customers to really be watching someone on the chance they drive off. reply dghlsakjg 19 hours agorootparentprevAs an American living in Canada: that’s exactly how it works on both sides of the border. reply cactusplant7374 19 hours agorootparentprevIt’s more efficient to pay after you’ve completely filled your tank. I guess there is a trust issue, which is why it isn’t common in the US anymore. reply salawat 18 hours agorootparentNope! It&#x27;s actually because there&#x27;s a financial standard transaction flow for things like pumps!-Confirm account: super small test move to confirm liveness of endpoint-Place hold: (locks $⁷5 worth of funds attached to an account) Finalize transaction-Finalize transaction for final amount-Release holdThis is actually a really troublesome thing for people who have trouble keeping their account balances positive, because you can end up locking them out of further financial activity until the hold is expired&#x2F;released.It&#x27;s one of those things you don&#x27;t really think about but are all over the place, and are a big part of making the world turn the way it does. reply sethhochberg 17 hours agorootparentSimilar problem for people this close to the financial edge: saying \"I want exactly $23.46 worth of gas\" because thats all the cash they have, a penny more wouldn&#x27;t fly. Much easier and safer to ask the attendant to preload the pump with that amount and run it down to zero than to try and get as close as you can with the trigger without going over. reply complianceowl 19 hours agorootparentprevThis comment is kind of confusing; maybe others have had a different experience, but I live in the Midwest and what you&#x27;re speaking of has been the norm for like the last decade. I&#x27;ve been driving for 13+ years and don&#x27;t have any memories of being able to pump gas without first paying at the pump, or paying inside and my pump automatically stopping at the amount I paid for. reply bluGill 15 hours agorootparentI starting driving in 1990 - back then there was no pay at the pump so the norm everywhere was to pump gas then go inside and wait in line to pay. You knew you were in a bad neighborhood if you had to pay for your gas before you pumped it (you probably already knew you were in a bad neighborhood, but this confirmed it). About 15 years ago stations started to realize that nearly everyone was paying at the pump even if they were going to come inside for something else - which made it much more likely anyone not paying at the pump was trying to steel gasoline and so they went to pay first as few were inconvenienced.I do not miss at all standing in line behind someone who couldn&#x27;t figure out which lottery ticket they wanted to buy. reply londons_explore 20 hours agorootparentprevPresumably they find out within a week when they reorder gas and the filling truck has to fill substantially more than expected.They probably start investigating for a fuel leak, pull the data from all the pumps, and then realise the number of gallons pumped and the number of gallons paid for don&#x27;t match up, and then check the CCTV. reply djtango 20 hours agorootparentDepends really. Most businesses will assume some amount of shrinkage. If you fly beneath the radar eg only hit the station once a month or every other month. You might get away with it. reply jareklupinski 19 hours agorootparent> Most businesses will assume some amount of shrinkagein the gas trade, they use the term \"evaporation\" ;P reply zmgsabst 20 hours agorootparentprevCompletely agree.Also, volume is huge.If a gas station averages 1 car at all times across a month, you’re talking 400,000+ gal. Even just 1 hour of pumping each day is 18,000 gal.Are they really going to bother if one person swipes 20 gal a month? reply brookst 19 hours agorootparentNot to mention the margin of error. Do fuel delivery quantities really exactly align with sold gas, to within a gallon or so? Or is there natural variation from each transaction being +&#x2F;- $0.01, plus evaporation, plus any other rounding&#x2F;physics considerations? reply jacquesm 19 hours agorootparentprevHere in NL it&#x27;s &#x27;pay inside&#x27; or &#x27;pump then pay&#x27; by default unless you&#x27;re in a border region where there are a lot of scammers. And judging by some recent foreign trips pretty much the same happens in other countries in the North of Europe. reply Retric 20 hours agorootparentprevYea it’s less automated but this bit: “when the clerks inside try to stop it - they can’t.” suggests it’s very obvious. reply c22 15 hours agoparentprevAround here I&#x27;ve been noticing an alarming number of vehicles being driven around with deeply expired tabs, dubious bits of paper tacked in the license plate area, or frequently no plates at all ever since the pandemic. Depending on where you live this just might not be enforced very much at the moment. reply pard68 20 hours agoparentprevSame man. I don&#x27;t know of a single pump within 40 miles of me that even has a card reader. It&#x27;s all pump-then-pay. reply gambiting 20 hours agoparentprev>>Edit: Looks like pump then pay is more regional than I assumed.The only place I have ever seen the requirement to pay first was in some tourist resorts in Spain, I guess to prevent theft. Weirdest thing ever, it&#x27;s like....lady I have no idea how much fuel this rental car will take, how can I possibly prepay you. reply adra 17 hours agorootparentAll the credit cards have a reserved amount they can pre-allocate so that once the final bill is due, you&#x27;re required to pay at most X dollars (the max that you chose at the pump). It&#x27;s fraud proof. Like it or not, it&#x27;ll likely be deployed everywhere eventually because it should be able to cut down on fraudulent purchases significantly. reply gambiting 15 hours agorootparent....is it common that people fill up and drive away without paying where you live? I have literally never heard of that happening, ever - I guess if you tried you wouldn&#x27;t get very far, police would be called and they&#x27;d find you quickly with the reg plate. reply gruez 10 hours agorootparent>I have literally never heard of that happening, everHow would you hear about it? Unless you&#x27;re friends with a gas station owner&#x2F;employee, how would you know? The amount is small enough that it&#x27;ll never show up in local media, just like the local teenager shoplifting wouldn&#x27;t show up on local media. reply bluGill 15 hours agorootparentprevWhat do you mean by common. The vast majority of people do not do this every. However even in the best areas someone will do it (I&#x27;m guessing a few times per month). In really bad areas it happens several times per hour. Generally the police are good at finding who did it when this happens, but only if the station notices in the first place - they don&#x27;t always. reply ziddoap 20 hours agoparentprevIt&#x27;s not exactly difficult to obfuscate a license plate for the few minutes needed to fill up. reply Mister_Snuggles 19 hours agoparentprevIn Alberta, it&#x27;s required by law to pay before you pump. This happened after an attendant was killed during a gas-and-dash robbery. reply coin 17 hours agoparentprev> Meanwhile your license plate is captured on cameraDepends on if local police and district attorneys actually acts on it reply behringer 20 hours agoparentprevIn my state they sure as heck don&#x27;t let you pay after pumping! That&#x27;s only really a thing in small towns reply NoMoreNicksLeft 19 hours agoparentprevLast week I had some idle time, and I was looking up whether they sold multi-color e-ink displays in a 6\"x12\" size.My state doesn&#x27;t even emboss plates anymore. If I did mount one and kept my real tag number on it, could anyone even tell? I&#x27;m boring as shit, so I don&#x27;t know why I&#x27;d have a James Bond gadget on my car...> Looks like pump then pay is more regional than I assumed. Yes it’s still a thing in some parts of the US in 2023.I haven&#x27;t seen this in forever. Even the shitty little gas stations where I grew up, I thought those were all upgraded ages ago. The EPA tends to frown on tanks that have been in the ground a long time, and since that&#x27;s the biggest cost, they tend to get the electronics upgraded too so that it&#x27;s pay at the pump. No idea why anyone with cash wouldn&#x27;t be forced to pay first. reply yomlica8 19 hours agorootparentThere was a post on here where some one mentioned a lot of people just install bike racks to block the plate and said Amazon sells lots of plate hiding stuff. I&#x27;d definitely seen plastic covers that obviously made the plate very hard to read in the wild but I&#x27;d never really thought about using different kinds of luggage racks to obscure plates.Of course, my state used a bunch of bad paint on license plates for a number of years so you could just peel off all the paint removing any contrast from a legit plate and you&#x27;d look no different than most people&#x27;s unreadable plates anyway. reply Retric 14 hours agorootparentWhich is all well and good until someone aims a camera to capture the VIN number near your drivers side dash.That’s the thing it’s easy to say something is a perfect crime, it’s harder to prove. reply jehb 20 hours agoprevThe headline says \"Detroit man\" but the article goes on to provide zero indication that the thief is a man or from Detroit. reply mdip 20 hours agoparentWelcome to Southeast Michigan.For whatever reason, everyone within 25 miles of Detroit \"lives in Detroit\" even though a whole lot of us might visit the city less than once a year.The media will usually say \"Metro Detroit\" when referring to someone who doesn&#x27;t actually live in the city[0] of Detroit and given the location of the gas station, it&#x27;s probably an actual person who lives in the city.[0] In which case it could be someone living in one of three different counties. reply havblue 18 hours agorootparentI always got the impression that it&#x27;s based on not wanting to say you&#x27;re from \"Detroit\". You say you&#x27;re from Royal Oak or whatever. People get confused. You hold up your palm and point to the lower right corner. People are still confused that you just pointed to Detroit... reply hinkley 16 hours agorootparentprevMeanwhile people living within 40 miles of Chicago &#x27;live in Chicago&#x27;. Some insist on it themselves, others are put into that box by the rest of the Midwest. reply bluGill 15 hours agorootparentprevEither you live in Detroit or you live in the UP. Sincerely everyone in the US who doesn&#x27;t live in Michigan. reply hospitalJail 19 hours agorootparentprevOdds are its some techbro that lives in Oakland country, went down to some pump in Detroit where people DGAF, and did their exploit.Also, I just call it south east Michigan. Detroit is subordinate to Oakland County now. Maybe we should call it Oakland-Macomb for better accuracy. reply jan_Sate 20 hours agoprevThat&#x27;s exactly why it&#x27;s a bad idea to use IoT on everything! Most companies working on this kind of product doesn&#x27;t give a shit about security. reply Aurornis 18 hours agoparentI doubt it&#x27;s IoT anything.It&#x27;s more likely that someone left a Bluetooth interface open on the service&#x2F;control interface. Could be even be something used for normal service operations that wasn&#x27;t properly secured. reply Obscurity4340 20 hours agoparentprevJust nice to see it used against someone other than the little people for once reply mdip 20 hours agorootparentI used to know a family who ran a gas station in my city. I was surprised to learn they made almost nothing from the sale of fuel. The actual profit -- to them -- was in the convenience store goods they sold.I&#x27;m not sure who eats the cost of the theft, but I&#x27;m guessing it&#x27;s the same people that make almost no money from the sale of the fuel. These were not wealthy people; they were working obscene hours and barely scraping by. reply gs17 18 hours agorootparentFor anyone interested, Freakonomics has an \"Economics of Everyday Things\" podcast with a good episode on gas stations: https:&#x2F;&#x2F;freakonomics.com&#x2F;podcast&#x2F;gas-stations&#x2F;They said they get $0.30 per gallon which mostly goes toward the other overhead, leaving $0.07 of profit per gallon. reply jacquesm 19 hours agorootparentprevGas stations - especially in rural areas - are usually hanging by a very thin thread and if they go bust they leave the area underserved. I don&#x27;t know what your definition for &#x27;little people&#x27; is but gas station owners would definitely qualify in my reading of that term. They tend to make less than minimum wage. If they make any money at all it is usually from store items, and that&#x27;s one of the reasons &#x27;pump then pay&#x27; is a thing in those places, it is an attempt to get the customer into the store. reply huytersd 20 hours agorootparentprevIt’s stealing from gas pump owners who are still very much the little people. reply jjkaczor 20 hours agorootparentDepends on if it is a direct corporate owned location, or franchised - but yes, I would argue that it hits many of the average small&#x2F;medium business owners.Things will be \"interesting\" over the next 10-20 with the switch to electrification - yet another entire sector that may no longer be viable for small&#x2F;medium independent owners. (but, time marches on - we don&#x27;t see many people lamenting the \"buggy whip manufacturers\" from days of old) reply asynchronous 20 hours agorootparentI actually think electric is one thing that could be squarely profitable for the small guy- think a small lot with solar panels or micro hydro and a few charging stations invested for much less than the cost of a house. reply greendave 19 hours agorootparentPeople often have an unrealistic notion of what solar can do. A single 50kW charger would require 125 400w panels minimum - around 3000 square feet of area - and still only be able to provide max current for a few hours of the day. And 50kW is still slower than most people will tolerate if they’re not at home&#x2F;work. reply rtkwe 19 hours agorootparentprevFast chargers pull huge amounts of electricity requiring significant electrical connections and infrastructure that&#x27;s pretty expensive to build up. You could provide level 1 or 2 chargers maybe on a DIY but even a single fast charger is going to need a LOT of solar and batteries to provide round the clock service. reply TremendousJudge 17 hours agorootparentAs other people mentioned, the money of owning a gas station comes mostly from the convenience store attached to it. This isn&#x27;t going away, quite the opposite. \"Fast\" charging is quite slow by gas station standards, which means more time for the drivers to go in and shop. reply rtkwe 14 hours agorootparentIf you can afford to setup a charger fast enough to bring people in. They&#x27;re not going to stop and browse your candy isle for 3 hours while your low wattage level 2 charger trickles 30 mph of charge into their batteries unless that&#x27;s the only option available. Faster charging, even staying within the bounds of AC Level 2 charging, requires beefy connections to the electrical grid to support more than one or two charging points.Many places will make the transition but you&#x27;ll also need to change your interior to dedicate more space to people waiting while charging. reply bluGill 15 hours agorootparentprevSort of. Most people will charge at home, so there will be less of that style of store with chargers. Along highways and in busy rural areas you will still find them - for travelers and truckers, but overall there is less need.Though there is still need for a store close to home where I can get a bit of milk quick, my morning donuts, and all the other things. I hope such stores move closer to where people live - suburban people should experience the big city convenience of walking to such places and when they don&#x27;t have a hope of competing for drivers they may as well move closer to where people live. (if zoning allows!) replymattgeo_j 20 hours agorootparentprevLast I heard, gas station \"owners\" make such a minuscule amount of money from the gas pumps and really rely on what they sell reply gottorf 20 hours agorootparentBarely making a profit on gas sales doesn&#x27;t mean you&#x27;re OK losing $3,000 of gas. reply quesera 19 hours agorootparentIt&#x27;s actually worse.Having a zero-margin product stolen, means that your loss is 100% of the sale price. reply JKCalhoun 18 hours agorootparentprevThen they have cause to sue the bastards that sold them the pump equipment. reply justrealist 18 hours agorootparentSociety actually works a lot better if every possible way to steal from your neighbor isn&#x27;t considered fair game. reply JKCalhoun 17 hours agorootparentAt what point is it negligence on the part of the manufacturer? reply justrealist 17 hours agorootparentSociety works better if every exploitable loophole is not considered a valid excuse to steal from your neighbor. reply Dylan16807 16 hours agorootparentYou&#x27;re talking past each other. reply justrealist 16 hours agorootparentThrough, not past. reply Dylan16807 15 hours agorootparentI have no idea what that means.Unless you mean \"talking through\" in the sense of teaching a lesson, because you&#x27;re not. Your argument does not invalidate what they are saying, or bring any notable information to light. You merely started a waste-of-time tangent. Theft bad, everyone knows that. reply justrealist 14 hours agorootparentThey are are trying to weaken the societal obligation of not stealing by punting the obligation on equipment manufacturers to make things hard to steal.It is cowardly and weak to abuse the legal system to compensate for societal problems by shifting blame onto easy targets. I have no interest in entertaining that line of reasoning. reply Dylan16807 14 hours agorootparentIt&#x27;s not punting to go after negligence. And saying the goal is to weaken obligations against stealing, not even a side effect? You&#x27;re throwing up quite a strawman there.And a manufacturer is not an \"easy target\" at all. If suing a big manufacturer is easy in some circumstance, it means they screwed up badly. reply justrealist 12 hours agorootparentIt&#x27;s shifting responsibility.And yes, it is an easy target, because the goal is to find someone to yell at; nobody cares about winning lawsuits. It&#x27;s about DAs finding a scapegoat to yell at, to avoid the fact that they have failed to disincentivize crime.It&#x27;s not negligence because nobody should be under any obligation to defend their property against theft. It&#x27;s the job of the state to punish crime. reply Dylan16807 11 hours agorootparent> It&#x27;s shifting responsibility.If you design a dispenser that dispenses at the wrong time, you have some responsibility.> And yes, it is an easy target, because the goal is to find someone to yell at; nobody cares about winning lawsuits.If you&#x27;re not going to win, you lose even more money. Why would you file it?> It&#x27;s about DAs finding a scapegoat to yell at,That has nothing to do with lawsuits.> to avoid the fact that they have failed to disincentivize crime.It&#x27;s not possible to go after every little thing.> It&#x27;s not negligence because nobody should be under any obligation to defend their property against theft.I&#x27;d better never have you watch my property, if you think there&#x27;s no obligation at all to defend things.Leaving valuable items in the middle of the sidewalk is negligent, even though it&#x27;s a crime to take it. reply justrealist 9 hours agorootparentDo you believe women bear responsibility as victims of sexual violence if they put themselves in unsafe circumstances? reply Dylan16807 5 hours agorootparentPeople are different from property. Jesus what are you doing.Also the correct analogy for the lawsuit would be leaving someone else to unsafe circumstances. Perhaps a vendor that sold the woman pepper spray that didn&#x27;t spray. Would that vendor be blameless because a crime occurred and only the criminal gets blame?Edit: Though if you really want an in-depth answer to your question, I would say that just by being an autonomous human being you are self-protecting in a way that meets the bar. Your items can&#x27;t do the same, so you need to apply protections to them if they&#x27;re in a place with easy public access. reply c22 5 hours agorootparentprevShould women choose to put themselves in unsafe circumstances just to prove a point about how they ought to be able to? reply prmoustache 20 hours agorootparentprevSome would argue they are complicit of a giant crime against humanity and environment. reply gottorf 20 hours agorootparentI mean, it&#x27;s not like the thieves were stealing gas to put it back in the ground. reply dahfizz 20 hours agorootparentprevand stealing gasoline helps the situation? reply prmoustache 14 hours agorootparentDid I write that? reply gowld 20 hours agorootparentprevThe gas station owners aren&#x27;t the criminals. The people who built suburbs are. reply throw__away7391 15 hours agorootparentPlease try to make substantive comments, not wild flames reply throw__away7391 20 hours agorootparentprevCool, so theft is morally justified depending on whom is being stolen from. Also, burning 800 gallons of fuel will release over two tons of carbon into the atmosphere, but I guess we just blame the oil companies for that too.This is where we are at now, the ultimate luxury, everyone has complete moral immunity for life for absolutely anything they do so long as someone else has more money than them, never mind if they are in the top 10% of the world&#x27;s wealthy overall. reply dahfizz 20 hours agorootparentSeriously, it feels like all morality is situational now. People have no principles.Something as straightforward as \"stealing is wrong\" doesn&#x27;t apply. \"Stealing from people I don&#x27;t like is fine\". reply ryandrake 18 hours agorootparentNot that I agree that stealing is acceptable (it&#x27;s not), but this is expected and what happens deep into class warfare when inequality skyrockets and hopelessness sets in. People do what they can to make a buck, and justify it as evening out a perceived imbalance. We should expect to see more and more of this as more and more economic gains are captured by the rich, squeezing more of the middle class into poverty. reply huytersd 10 hours agorootparentprevThe massive wealth inequality has a role to play. People are willing to accept people having more money than them but 10 people owning 50% of the wealth while the rest are literally suffering through multiple jobs and still live paycheck to paycheck, that’s when people think of revolting. What we’re seeing now is the initial stages of a soft revolution where ideas are introduced to the larger public and reactions are gauged. Things aren’t bad enough for an actual revolution but these are the first signs of the growing discontent. reply AngryData 10 hours agorootparentprevMorality has always depended on circumstances. When life gets hard, ideals held about morality or ethics tend get looser and looser until they fly right out the window. reply JKCalhoun 18 hours agorootparentprev> everyone has complete moral immunity for life for absolutely anything they do so long as someone else has more money than them, never mind if they are in the top 10% of the world&#x27;s wealthy overall.I&#x27;ve never seen anyone introduce the idea of \"wealth absolutism\". From your comment though I&#x27;m not sure if you would allow the bottom 10% to feel complete moral immunity. reply gowld 20 hours agorootparentprevPlease try to make substantive comments, not wild flames. reply thsksbd 20 hours agorootparentprevI understand you&#x27;re sentiment, however, never mind the fact that gas station owners are barely scraping by, theft is theft. We cannot win by debating ourselves because instead of dismantling the security state (that&#x27;s what iot is), the result will be a doubling down (secure iot, install backdoors, take registration, etc) reply 1234letshaveatw 20 hours agorootparentprevSuper nice to see. I am so looking forward to living in a society that continues to devolve to the point that thievery is generally accepted. Oh btw, give me your wallet reply tgv 18 hours agorootparentThe thought behind it might be: perhaps now someone with influence will pay attention. At least, that was my interpretation. reply networkchad 20 hours agorootparentprevMost of these stations are owner-operated. They usually aren’t owned by BigCo. reply mdip 20 hours agoparentprevYou&#x27;re mostly right. I think several do give a shit about security but they both lack the knowledge of what it takes to reasonably secure something like this and are somewhat powerless&#x2F;misled into believing they don&#x27;t need to care.The first problem is that companies that add wireless&#x2F;IoT capabilities to devices have nothing to do with their implementation&#x2F;security beyond writing a cheque. I used to work for a company that created IoT devices for huge brands[0]. They make a toothbrush, a lightbulb or a vacuum cleaner. They&#x27;re good at that, but they bring in someone else to either \"wholesale create an IoT product\" or collaborate on its creation where the \"IoT pieces\" (and their security) were farmed out to us.Of course, we didn&#x27;t have much to do with the creation of the product, either. We (frequently) used a white-label Chinese product -- often, but not always, explicitly chosen by the manufacturer. In fact, the large brands often insist on a specific Chinese provider for long-term maintenance&#x2F;cost savings[1].We receive a \"dev board\" and device that consists of an ESP8266&#x2F;ESP32 bolted onto whatever it&#x27;s IoT-ing, and a cloud service (hosted in the country of origin; almost always China). The developers involved write a mobile app and some firmware that interacts with the cloud web service -- which we have no control over&#x2F;cannot work around or suggest additional security layers -- and we integrate it. It creates an impenetrable SEP field[2].Companies who care about security care about it up to some industry defined standard that has some form of acronym (the names of which escape me these days) which involve, basically, filling out a form or two attesting (truthfully) to the requirements but that serve only to shield those involved in the process from responsibility beyond whatever the acronym-defining organization thinks to ask. It involved passing a lot of those questions off to the third-party who runs the cloud service, who -- surprise, surprise -- know what boxes to check to receive payment.[0] I love my former employer so I&#x27;m leaving the brands off, but at least one product involved was popular enough that my parents received one as a gift for Christmas ... it was fun explaining that some code I had written was running on their phone.[1] The pattern (one that we didn&#x27;t advocate) went \"hire us to build the initial, user-friendly, polished version of the app\" then pass it onto said OEM to maintain&#x2F;update it between hardware versions. To the extent that the first version could be secured, it is (until it&#x27;s not). They don&#x27;t trust the OEM to write the initial version, but once it&#x27;s out there, augmenting it costs 1&#x2F;10th the price and that&#x27;s about all their willing to pay after the initial product is created.[2] Someone Else&#x27;s Problem (Douglas Adams -- Hitchhiker&#x27;s Guide to the Galaxy). reply ChrisClark 20 hours agoparentprevIoT, the S stands for security. reply checker 20 hours agorootparentAnd the I stands for Internet. I guess we&#x27;re considering local BT networks to be \"the internet\" now.Perhaps the BT connection bridged the device to the internet, but we have no indication of that and the internet was used as the vector. It could have been a local wireless software update interface so maintenance doesn&#x27;t have to open up each pump to apply the patch. reply 83 more comments... Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A man from Detroit has been utilizing Bluetooth technology to remotely access and control gas pumps, enabling him to steal gas without having to pay.",
      "Gas station owners have been significantly impacted, with losses amounting to almost $3,000 from this hack. Although a suspect is captured in the surveillance footage, the man remains at large.",
      "Similar incidents have previously happened, leading to heightened alertness from gas station clerks who are advised to involve the police at any sign of suspicious activity."
    ],
    "commentSummary": [
      "The post is a comprehensive discussion surrounding various security concerns at gas stations, including hacking, theft, and the efficacy of locks.",
      "It delves into subjects like insurance coverage for theft-related losses, responsibilities of gas station clerks, and payment methods.",
      "Also highlighted are the moral and security implications of theft as well as the myriad challenges gas station owners face."
    ],
    "points": 282,
    "commentCount": 376,
    "retryCount": 0,
    "time": 1696337257
  },
  {
    "id": 37753442,
    "title": "Graph Mining Library",
    "originLink": "https://github.com/google/graph-mining",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up google / graph-mining Public Notifications Fork 12 Star 424 Code Issues 1 Pull requests Actions Projects Security Insights google/graph-mining main 1 branch 0 tags Go to file Code Latest commit Laxman Dhulipala Remove double-license and update README.md c1c54b4 Git stats 8 commits Files Type Name Latest commit message Commit time docs Boilerplate for new Google open source project in_memory Remove double-license and update README.md utils Remove double-license and update README.md .bazelrc V0 of graph-mining repo BUILD.oss V0 of graph-mining repo CONTRIBUTING.md V0 of graph-mining repo LICENSE Boilerplate for new Google open source project README.md Remove double-license and update README.md WORKSPACE.bazel V0 of graph-mining repo README.md The Graph Mining Library This project includes some of Google's Graph Mining tools, namely in-memory clustering. Our tools can be used for solving data mining and machine learning problems that either inherently have a graph structure or can be formalized as graph problems. For questions/comments, please create an issue on this repository. About No description, website, or topics provided. Resources Readme License Apache-2.0 license Code of conduct Code of conduct Security policy Security policy Activity Stars 424 stars Watchers 11 watching Forks 12 forks Report repository Releases No releases published Packages No packages published Languages C++ 93.4% Starlark 6.2% C 0.4% Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37753442",
    "commentBody": "Graph Mining LibraryHacker NewspastloginGraph Mining Library (github.com/google) 275 points by zuzatm 18 hours ago| hidepastfavorite96 comments esafak 15 hours agoGraph mining was \"so hot right now\" ten years ago. Remember GraphX (https:&#x2F;&#x2F;spark.apache.org&#x2F;graphx&#x2F;) and GraphLab (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GraphLab) ? Or graph databases?I guess it coincided with the social network phenomenon. Much more recently geometric learning (ML on graphs and other structures) shone, until LLMs stole their thunder. I still think geometric learning has a lot of life left in it, and I would like to see it gain popularity. reply PaulHoule 14 hours agoparentThere are \"graph databases\" which see graphs as a universal approach to data, see RDF and SPARQL and numerous pretenders. For that matter, think of a C program where the master data structure is a graph of pointers. In a graph like that there is usually a huge number of different edge types such as \"is married to\", \"has yearly average temperature\", ...Then there are \"graph algorithms\" such as PageRank, graph centrality, and such. In a lot of those cases there is one edge type or a small number of edge cases.There are some generic algorithms you can apply to graphs with many typerd edges edges such as the magic SPARQL pattern ?s1 ?p ?o . ?s2 ?p ?o .which finds ?s1 and ?s2 that share a relationship ?p with some ?o and is the basis for a similarity metric between ?s1 and ?s2. Then there are the cases that you pick out nodes with some specific ?p and apply some graph algorithm to those.The thing about graphs is, in general, they are amorphous and could have any structure (or lack of structure) at all which can be a disaster from a memory latency perspective. Specific graphs usually do have some structure with some locality. There was a time I was using that magic SPARQL pattern and wrote a program that would have taken 100 years to run and then repacked the data structures and discovered an approximation that let me run the calculation in 20 minutes.Thus practitioners tend to be skeptical about general purpose graph processing libraries as you may very have a problem that I could code up a special-purpose answer to in less time than you&#x27;ll spend fighting with the build system for that thing that runs 1000x faster.----If you really want to be fashionable though, arXiv today is just crammed with papers about \"graph neural networks\" that never seem to get hyped elsewhere. YOShInOn has made me a long queue of GNN papers to look at but I&#x27;ve only skimmed a few. A lot of articles say they can be applied to the text analysis problems I do but they don’t seem to really perform better than the system YOShInOn and I use so I haven’t been in a hurry to get into them. reply Someone 13 hours agorootparent> a universal approach to data, see RDF and SPARQL and numerous pretenders. For that matter, think of a C program where the master data structure is a graph of pointers.A graph of typed pointers. As you likely know, the basic element of RDF is not “foo has a relationship with bar”, but “foo has a relationship with bar of type baz”.Also, the types themselves can be part of relationships as in “baz has a relationship with quux of type foobar”> The thing about graphs is, in general, they are amorphous and could have any structure (or lack of structure) at all which can be a disaster from a memory latency perspectiveBut that’s an implementation detail ;-)In theory, the engine you use to store the graph could automatically optimize memory layout for both the data and the types of query that are run on it.Practice is different.> Thus practitioners tend to be skeptical about general purpose graph processing librariesI am, too. I think the thing they’re mostly good for is producing PhD’s, both on the theory of querying them, ignoring performance, and on improving performance of implementations. reply PaulHoule 12 hours agorootparentFunny, the core table of salesforce.com is triples but they got a patent circa 2000 on a system that builds indexes and materializes views based on query profiling so the performance is good (w&#x2F; gold plated hardware). That patent is one reason why graph databases sucked for a long time.Now the Lotus notes patents have been long expired so I’d like to see some graph database based products that can do what Notes did 30 years ago but it is lost technology like the pyramids, stonehenge and how to make HTML form applications without React. reply throwaway290 5 hours agorootparentprev> foo has a relationship with bar of type bazNope, \"of type baz\" is not required. reply hobofan 1 hour agorootparentDepends on the perspective. The predicate will always be an IRI. The object will either be an IRI or a literal, and all literals in RDF (as of RDF 1.1) are typed , though serialization formats like Turtle work with implied types.There is also the option of blank nodes for objects, though in almost all implementations they are stand-ins for anonymous IRIs, so in some sense or another almost anything has \"a\" type. reply esafak 13 hours agorootparentprev1. Graph algorithms like the ones you mentioned are processed not by graph databases like Neo4j, but graph processing libraries like the titular Google library.2. Geometric learning is the broader category that subsumes graph neural networks.https:&#x2F;&#x2F;geometricdeeplearning.com&#x2F; reply PaulHoule 13 hours agorootparentDepends, some graph databases have some support for graph algorithms.I’ll also say I think graph algorithms are overrated, I mean you know the diameter of some graph: who cares? Physicists (like me back in the day) are notorious for plotting some statistics on log-log paper, seeing that the points sorta kinda fall on a line if you squint and decide that three of the points are really bug splats and then yelling “UNIVERSIALITY” and sending it to Physical Review E but the only thing that is universal is that none of them have ever heard of a statistical estimator or a significance test for power law distributions. Node 7741 is the “most central” node, but does that make a difference? Maybe if you kill the top 1% central nodes that will disrupt the terrorist network but for most of us I don’t see high quality insights coming out of graph algorithms most of the time. reply esafak 13 hours agorootparent> Physicists (like me back in the day) are notorious for plotting some statistics on log-log paper...For people who&#x27;ve missed it: So You Think You Have a Power Law — Well Isn&#x27;t That Special? (http:&#x2F;&#x2F;bactra.org&#x2F;weblog&#x2F;491.html) :) reply reaperman 14 hours agoparentprevI still use NetworkX a lot when a problem is best solved with graph analysis, I really enjoy the DevEx of that package. reply afandian 16 hours agoprevCan someone with familiarity with Bazel give any clues how to build? `bazel build` does something, but I end up with `bazel-build` and `bazel-build` with no obvious build artefacts. reply elteto 15 hours agoparentIn bazel &#x2F;&#x2F;... is the equivalent of the &#x27;all&#x27; target in make: bazel build &#x2F;&#x2F;... bazel test &#x2F;&#x2F;... bazel query &#x2F;&#x2F;...The last one should list all targets (from what I remember). reply afandian 15 hours agorootparentThanks! That last one lists 84 results. None looks obviously like &#x27;main&#x27;. Trying a random one: bazel run &#x2F;&#x2F;in_memory&#x2F;clustering:graph ERROR: Cannot run target &#x2F;&#x2F;in_memory&#x2F;clustering:graphI&#x27;m going to wait until someone updates the readme I think! reply tfsh 15 hours agorootparentconsidering the repo doesn&#x27;t contain a cc_binary build rule, I&#x27;m inclined to believe there&#x27;s no demo, the easiest way to get started (if you want to play around from scratch) would be to add a cc_binary, point that to a main.cpp file which depends on the library targets you want, e.g \"&#x2F;&#x2F;in_memory&#x2F;clustering:graph\" and ensure there&#x27;s sufficient visibility from the targets. reply hashar 14 hours agorootparentprev`bazel run` is for a rule that has been marked `executable = True` and there is no such rule in the repository.If you `bazel build &#x2F;&#x2F;...`, you should get the compiled libs under `bazel-out&#x2F;*fastbuild&#x2F;bin&#x2F;`. reply elteto 15 hours agorootparentprevThen most likely this is meant to be used primarily as a library. You should wait until they open source the tests (soon, per another commenter). Those will be runnable targets. reply simpleladle 9 hours agoparentprevJust to follow up on the above replies, you could also just build a single package. For example, you could build asynchronous_union_find with `bazel build &#x2F;&#x2F;in_memory&#x2F;connected_components:asynchronous_union_find`. (This isn&#x27;t very useful outside of the context of a cc_binary rule.)This in turn allows you to only build and use the &#x27;package&#x27; you care about without having to build the whole repo in other projects. Continuing on the above example, if you only wanted to use the asynchronous_union_find.h header file in your project, somewhere in your WORKSPACE file, you add the graph-mining library using a git_repository rule (see WORKSPACE.bazel for examples), and in a cc_library rule in a BUILD file inside your project, you can add a `@graph-mining&#x2F;&#x2F;in_memory&#x2F;connected_components:asynchronous_union_find`. Then you can include it as a header elsewhere. Building your project then only builds that package and its dependencies, and not the entire graph-mining library. reply Keyframe 13 hours agoparentprevInteresting. I always had in back of my mind this notion that I ought to check bazel one of these days. So, one of these days is then today. In order to install bazel, recommended way seems to be to install bazelisk first and just rename that to bazel and move it somewhere on the path like &#x2F;usr&#x2F;local&#x2F;bin&#x2F;bazel.. fine. Now when I run query it warned me about JDK.. huh. Now when I run build it errored and failed due to missing JAVA with \"WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.\". Ok, I&#x27;m not using Java - let&#x27;s check which Java JDK&#x2F;JRE to use these days and after few minutes of googling I&#x27;m not up for it anymore and that, ladies and gentlemen, is where this day is then up for another day after all. Pathetic how cargo and even npm&#x2F;yarm spoiled us.edit: thanks to https:&#x2F;&#x2F;sdkman.io&#x2F; it&#x27;s up and running. It wasn&#x27;t _that_ bad after all. reply emmanueloga_ 9 hours agoprevFor those wanting to play with graphs and ML I was browsing the arangodb docs recently and I saw that it includes integrations to various graph libraries and machine learning frameworks [1]. I also saw a few jupyter notebooks dealing with machine learning from graphs [2].Integrations include:* NetworkX -- https:&#x2F;&#x2F;networkx.org&#x2F;* DeepGraphLibrary -- https:&#x2F;&#x2F;www.dgl.ai&#x2F;* cuGraph (Rapids.ai Graph) -- https:&#x2F;&#x2F;docs.rapids.ai&#x2F;api&#x2F;cugraph&#x2F;stable&#x2F;* PyG (PyTorch Geometric) -- https:&#x2F;&#x2F;pytorch-geometric.readthedocs.io&#x2F;en&#x2F;latest&#x2F;--1: https:&#x2F;&#x2F;docs.arangodb.com&#x2F;3.11&#x2F;data-science&#x2F;adapters&#x2F;2: https:&#x2F;&#x2F;github.com&#x2F;arangodb&#x2F;interactive_tutorials#machine-le... reply itissid 10 hours agoprevNoob Q: Would this library be a (good?) candidate to be integrated with a wrappers&#x2F;extension libraries to have all the graph based clustering algorithms in one place(assuming they are not already)?Or do(better?) frameworks for the same function as this code already exist(maybe networkx?)? reply sbrother 17 hours agoprevI might be (very) far behind the times, but does this have any relationship with Pregel? reply cmckn 10 hours agoparentPregel is a distributed graph processing system, this (AFAICT) is a library for working with graphs in-memory on a single computer. reply pharmakom 13 hours agoprevCan someone explain what this library might be useful for? reply oddthink 13 hours agoparentClustering. I used the correlation clusterer from here for a problem that I could represent as a graph of nodes with similarity measures (this data looks like this other data) and strong repelling features (this data is known to be different from this other, so never merge them). reply zekenie 17 hours agoprevsome examples would be super helpful! reply specproc 16 hours agoparentDocumentation of any sort would be super helpful. reply zuzatm 11 hours agoparentprevIt&#x27;s coming! Check again in 12 hours, I believe it should be up then! reply nologic01 12 hours agoprevGraph algorithms cry out for some standardization. Think blas and lapack. reply bigbillheck 12 hours agoparentConsider: https:&#x2F;&#x2F;graphblas.org reply nologic01 11 hours agorootparentI wonder how much overlap this new project with graphblas and older graph libraries like boost::graph https:&#x2F;&#x2F;www.boost.org&#x2F;doc&#x2F;libs&#x2F;1_83_0&#x2F;libs&#x2F;graph&#x2F;doc&#x2F; reply whitten 17 hours agoprevGithub says it is C, C++, and Starland.What is Starland ? reply Laremere 17 hours agoparentIt&#x27;s Starlark, the language for configuring the build system Bazel. Bazel is the open source port of Google&#x27;s internal build system, Blaze. Starlark is a subset of Python. reply Terr_ 13 hours agorootparentThis list of corporate project name associations makes me wonder where Galactus comes in. :Phttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=y8OnoxKotPQ reply ashout33 17 hours agoparentprevif I had to guess, that is a typo and should be starlark, which is the language used for bazel build files. bazel is the build system they use reply jefftk 17 hours agorootparentGithub says \"Starlark 6.2%\", so it looks like whitten&#x27;s typo, not GitHub&#x27;s. reply nolok 17 hours agorootparentprevOn which keyboard layout is rk into nd a typo ... reply mcpeepants 17 hours agorootparenton any layout operated by a human, who may at times type the wrong word entirely reply macintux 16 hours agorootparent…and&#x2F;or fall victim to autocorrect. reply bsimpson 16 hours agorootparentprev\"STARLAND VOCAL BAND? THEY SUCK!\" reply xxpor 18 hours agoprevI was hoping this would mine literal stats graphs for anomaly detection reply supriyo-biswas 8 hours agoparentThat’s relatively easy, see https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Interquartile_range reply lanstin 17 hours agoparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Graph_theoryIt&#x27;s interesting and deceptively simple at first. reply blitzar 17 hours agoparentprevI think they use the word \"graph\" to mean a different thing to what I use the word for. reply tomrod 18 hours agoprevThis is a big deal, I think. I&#x27;m guessing it&#x27;s not widely used internally anymore if they are open sourcing it. What is used instead? reply simonw 17 hours agoparentI don&#x27;t think \"not widely used internally anymore\" is a common rationale for open sourcing something.Generally I&#x27;d expect companies to open source things when it&#x27;s proven itself internally and they want to reap the benefits of open source:- Make internal engineers happy - engineers like having their code released outside the bounds of their company- Prestige, which can help with hiring- External contributions (not even code necessarily, just feedback from people who are using it can be amazingly useful for improving the software)- Ability to hire people in the future who already know important parts of your technical stack, and don&#x27;t need internal training on it- Externally produced resources that help people learn how to use the software (tutorials, community discussion forums etc)If the software is no longer used internally, open sourcing it is MORE expensive - first you have to get through the whole process of ensuring the code you are releasing can be released (lots of auditing, legal reviews etc), and then you&#x27;ll have to deal with a flow of support requests which you can either deal with (expensive, especially if your internal knowledge of the tool is atrophying) or ignore (expensive to your reputation). reply palata 16 hours agorootparentIMO, you forget one important point: control.If your open source project&#x2F;protocol is the most popular, and you have the governance over it, then you decide where it goes. Chromium is open source, but Google controls it, and everyone who depends on it has to follow. If Chromium was not open source, maybe Firefox would be more popular, and Google would not have control over that.> or ignore (expensive to your reputation).I don&#x27;t think that anything is expensive for Google. They can do whatever they want. reply mlinhares 17 hours agorootparentprevGoogle has done this before so it&#x27;s not surprising people would think that. reply tomrod 17 hours agorootparentTo be fair, I can often be uninformed with regards to some of the smaller movements of techcos. reply wilsynet 17 hours agoparentprevAs merely two examples, both gRPC and Kubernetes are important to Google, and yet Google opened sourced them. \"No longer used\" is not the criteria Google uses to make their software OSS.FYI, I work at Google. reply jefftk 17 hours agorootparentGoogle Wave is the only counterexample I can think of, where it was \"we&#x27;re deprecating this project, but releasing it as open source\". reply bsimpson 16 hours agorootparentI don&#x27;t think Google generally opensources _products_ - either it always is open source (Android) or never is (web apps). I can&#x27;t think of an example where a product was closed source, released as open source, and continually maintained.Open source at Google generally takes the form of libraries rather than products. Often, that&#x27;s something that an individual engineer is working on, and it&#x27;s easier to open source than get the copyright reassigned (since Google by default owns any code you write). There are also libraries that are open sourced for business reasons - e.g. SDKs. You can tell the difference, because most individually-driven libraries contain the copy \"Not an official Google product\" in the README. reply progval 14 hours agorootparent> I can&#x27;t think of an example where a product was closed source, released as open source, and continually maintained.I found one after some searching: Nomulus. https:&#x2F;&#x2F;opensource.googleblog.com&#x2F;2016&#x2F;10&#x2F;introducing-nomulu... reply PaulHoule 15 hours agorootparentprevI&#x27;d say both of those are actively harmful products (like PFOS or cigarettes) that hurt Google&#x27;s competition by being open sourced. Google wrecked their own productivity, the least they could do was wreck everybody else&#x27;s. reply dieortin 14 hours agorootparentAnd why would any of those be harmful? Care to elaborate? reply PaulHoule 13 hours agorootparentThey take a process a small team could complete quickly with high quality and low cost maintenance and turn it into a process a huge team completes slowly with poor quality and high maintenance cost. Google can afford this because of huge profits from their advertising monopoly that they don’t know how to spend.Go look at the manuals for IBM&#x27;s Parallel Sysplex for mainframes and compare the simplicity of that to K8S for instance.Or for that matter look at DCOM and the family of systems which Microsoft built around it which are truly atrocious but look like a model of simplicity compared to gRPC. (At least Don Box wrote a really great book about COM that showed people in the Microsoftsphere how to write good documentation.)Or for that matter try doing something with AWS, Azure or some off-brand cloud and Google Cloud from zero (no account) and time yourself with a stopwatch. Well, it will be a stopwatch for AWS but you will probably need a calendar for Google Cloud. reply tomrod 17 hours agorootparentprevThanks for clarifying reply nivekney 17 hours agoparentprevIt&#x27;s based on ParlayLib, which is for shared-memory multicore machines. Highly suspect that they moved the algorithms on to distributed systems. reply spankalee 16 hours agoparentprevMost projects try not to open something that&#x27;s not going to be maintained. If they do it&#x27;s usually rather loudly called out in the readme. reply meneer_oke 14 hours agoparentprevWould it be possible to explain why it&#x27;s big deal. reply dllthomas 16 hours agoprevDoes it accept graphviz? reply charcircuit 18 hours agoprevMost of these files have a double license header. reply ldhulipala 17 hours agoparentThanks for pointing this out (fixed now). reply specproc 16 hours agorootparentIf you&#x27;re working on this repo, can we plz haz docs? reply ldhulipala 16 hours agorootparentThanks, yes, this is on the list of TODOs! (also, to open-source the tests) reply xw3098 15 hours agorootparentIt would be nice to have a bit of documentation on what makes this library special as well. It’s a significant time investment to learn a library like this one well. So some information on why one should choose this over, say, http:&#x2F;&#x2F;snap.stanford.edu&#x2F; Would be very helpful. reply specproc 16 hours agorootparentprevThank you kindly! reply choppaface 14 hours agorootparentprevDid you release it without docs so that you could add it to your Perf packet? replyponyous 16 hours agoprevNo idea where is the hype coming from, who is actually upvoting this? 0 Docs, 0 examples, 0 explanation of how is it useful.Is \"Graph Mining\" so ubiquitous that people know what this is all about? reply ldhulipala 16 hours agoparentWe are updating the README to be more descriptive; in the meantime, please see https:&#x2F;&#x2F;gm-neurips-2020.github.io&#x2F; or https:&#x2F;&#x2F;research.google&#x2F;teams&#x2F;graph-mining&#x2F; reply bafe 14 hours agoparentprevIt was hyped some years ago. There are plenty of legitimate applications of graphs, perhaps the library offers well optimized implementation of important algorithms. But the past hype around all things \"graph\" was not rational. As always, you can&#x27;t solve all problems with a graph as you can&#x27;t with a neural network or with any other structure&#x2F;algorithm reply MarkMarine 16 hours agoprevWhew. Lots of complaints from people who probably will never need to use this code.If you need docs just read the .h files, they have extensive comments. I’m sure they’ll add them or maybe, just maybe, you could write some to contribute.This would have made some of my previous work much easier, it’s really nice to see google open source this. reply riku_iki 13 hours agoparent> If you need docs just read the .h filescurious if this is typical dev experience inside google.. reply dekhn 13 hours agorootparentI think in most cases, back when I worked there, I would have instead searched the monorepo for targets that depended on this library (an easy lookup), and look at how they used it.Some code libraries had excellent docs (recordio, sstable, mapreduce). But yes, reading the header file was often the best place to start. reply MarkMarine 13 hours agorootparentprevI’m not at google so I’ve got no idea.Reading the code, especially the header files, seems to be pretty standard as far as what I see in non-open source code. So, it’s been my typical dev experience, I’d say if you’re somewhere that has gleaming, easy to understand docs that are actually up to date with the code you all have too much time on your hands, but I serially work at startups that are running to market. reply riku_iki 13 hours agorootparentHeader file gives you a view into some narrow window of the system, API, pipeline, and you probably have no idea which header files are important and which are part of some internal implementation.10 mins spent on readme with some high level details is investment with 100x return for lib users. reply helsinki 11 hours agoparentprevThe .proto files are the documentation everyone is looking for. reply ls612 15 hours agoparentprevI think it’s that it’s not at all obvious how to even build the damn thing so at least a little bit of readme would have been nice. I agree with the sentiment this looks like a super cool tool. reply PaulHoule 15 hours agorootparentIt says you&#x27;re supposed to leave a ticket if you have questions or comments... A README file isn&#x27;t much to ask for. reply MarkMarine 14 hours agorootparentI’m not saying it’s too much to ask for, but also, when you’re doing distributed in memory graph mining (which means you’ve got an application with a big enough graph that you need to do this, and the technical expertise to need the algorithms in this open source package) maybe it’s expected that you can read the bazel files and bazel docs yourself and figure it out.Or just write a make file and cut all the bazel build optimization out.They don’t put instructions on how to start a F1 car inside the cockpit, you don’t hop into a fighter jet and look for the push to start easy button, it’s expected that when you’re at that level you bring expertise. reply PaulHoule 14 hours agorootparentYeah, and somebody who is that smart can probably pack their data structures efficiently and find an approximation to do the job on a macbook pro that people with too many resources need a 1000 machine cluster to do. And get the coding and the computation done in the time that the C++ compiler is still chewing on the headers of the bloated library. (At times I’ve been that guy.)But seriously, there is such a thing as industrialization. Google is notorious though for hiring 180 IQ people and getting them to perform at a 35 IQ level because there the documentation makes no sense, a procedure which should be done in one step really takes 300, fighting with C++, etc. They can afford to do it because it is just so profitable to help that guy who shows up on every. single. video. roll. who says “you can’t lose weight by exercising”, “you can’t lose weight by cutting carbs” who links to some video that drones on hours and hours and signs you up for some subscription you can never cancel to sell scam supplements.Shame on them.BTW, with high-complexity software there is enough question that you got it working right that you expect a significant process of testing that it works for your application. For instance if you got a hydrocode for simulating the explosion of a nuclear weapon you would not take for granted that you had built it properly and were using it properly until you&#x27;d done a lot of validation work. A system like that isn&#x27;t a product unless it comes with that validation suite. The same is true for automated trading software (gonna hook it up straight to the market without validation? hope you have $100M to burn!)... now there was that time a really famous CS professor e-mailed me a short C program that was said to do something remarkable that crashed before it even got into main() which did teach me a thing about C but that&#x27;s not what a professional programmer does. reply MarkMarine 13 hours agorootparentI agree with all of this.Its just frustrating that all the comments about an interesting library seem to be customer service complaints from people who never need to reach for this library. I was hoping for a real discussion, something I could learn from. reply PaulHoule 12 hours agorootparentReally though an open source product has not really been released until there is documentation walking through setting it up and doing some simple thing with it. As it is I am really not so sure what it is, what kind of hardware it can run on, etc. Do you really think it got 117 Github stars from people who were qualified to evaluate it?(I’d consider myself qualified to evaluate it.. If I put two weeks into futzing with it.)Every open source release I’ve done that’s been successful has involved me spending almost as much time in documentation, packaging and fit-and-finish work as I did getting working it well enough for me. It’s why I dread the thought of an open source YOShInOn as much as I get asked for it.Sometimes though it is just a bitch. I have some image curation projects and was thinking of setting up some “booru” software and found there wasn’t much out there that was easy to install because there are so many moving parts and figured I’d go for the motherf4r of them all because at least the docker compose here is finitehttps:&#x2F;&#x2F;github.com&#x2F;danbooru&#x2F;danboorueven if it means downloading 345TB of images over my DSL connection. replycorentin88 17 hours agoprevInteresting fact: the first commit is 2 years old and is entitled \"Boilerplate for new Google open source project\".Either they rewrite git history or it took about 2 years to get approval on making this repo public. reply j2kun 17 hours agoparentThe code has an internal analogue, and the tooling lets you choose whether to export the entire git history or squash it. They may have chosen the former, in which case it could just be 2 years to migrate and rework the code to be ready for open sourcing. In that time I imagine there were four reorgs and countless priority shifts :) reply spankalee 16 hours agoparentprevIf you know you want to open source a project eventually, it&#x27;s easier if you start it in the open source part of the internal repo with all the licensing and headers in place. Open sourcing existing code is harder because you need to review that it hasn&#x27;t used something that can&#x27;t be opened.So probably they just started the project two years ago, had aspiration to open source, and finally just did now. Some teams might publish earlier, some like to wait until it&#x27;s had enough internal usage to prove it out. reply progval 15 hours agoparentprevFWIW they had already pushed that commit four months ago: https:&#x2F;&#x2F;archive.softwareheritage.org&#x2F;browse&#x2F;snapshot&#x2F;bd01717... reply hiddencost 16 hours agoparentprevThat could be the template it was cloned from reply thfuran 17 hours agoparentprevNow that&#x27;s bureaucracy. reply numpad0 15 hours agorootparentI&#x27;d agree if last commit was 2 years ago. reply 0x6461188A 11 hours agoprevHow is this usable. I see no documentation. There is a docs folder but all it contains is a code of conduct. reply mathisfun123 11 hours agoparentI&#x27;m not trying to be snarky but have you considered reading the code? Like I&#x27;ll be honest I can&#x27;t remember the last time I looked at docs at all instead of reading the code itself. reply n3150n 10 hours agorootparentAre you for real? I&#x27;m also not trying to be snarky but...$ cat $(find . -type fgrep -vE LICENSE\\|README\\|BUILD\\|bazel\\|git\\|docs)sort -uwc -l8360 unique lines scattered across more than 100 files. Good luck deciphering that in a single day!By the way, the first issue in the repo is a \"Request for a more verbose README\", which I agree with. reply jshobrook 8 hours agorootparentYou could try using something like Adrenaline https:&#x2F;&#x2F;www.useadrenaline.com&#x2F; I built it exactly for this use case :) reply mathisfun123 9 hours agorootparentprevmy guy what exactly are you expecting here? this is free as in beer code (apache license). no one is forcing you to use this and no one is asking anything of you for using it. i fully support people releasing their code (that took enormous amounts of blood sweat tears to get working) absolutely however they want to. if i&#x27;m interestd enough i&#x27;ll figure it out and thank them.so as i see it you have like three options if you are unhappy with that:1. close the tab2. dig into the impl and learn as you go3. do 2 but also write docsi just really believe i&#x27;ve covered literally all the cases that any reasonable (not whiney, not entitled) person would concede.> the first issue in the repo is a \"Request for a more verbose README\", which I agree with.posted today - do you think it might have something to do with this post we find ourselves convening on? i.e. no one was so bothered about a lack of docs until now?edit:i forgot actually something else you could do: email the author and ask nicely for some tips. reply otteromkram 8 hours agoparentprevThis header file has lots of commentary.https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;graph-mining&#x2F;blob&#x2F;main&#x2F;in_memory&#x2F;c...This, too:https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;graph-mining&#x2F;blob&#x2F;main&#x2F;in_memory&#x2F;s...Same with most of the other files.How is it usable? It&#x27;s usable if you want to find date within lots and lots of data efficiently. That&#x27;s kinda Google&#x27;s thing. :-D reply PaulHoule 15 hours agoprev [–] Towards the end of my relationship with a business partner, he was really impressed with a graph processing library released by Intel (because it was Intel), while my thoughts were \"ho hum, this looks like it was done by a student\" (like a student who got a C-, not a A student) and thought about how much I liked my really crude graph processing scripts in Pig that were crazy fast because they used compressed data structures and well-chosen algorithms. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"graph-mining\" is an open-source project developed by Google, hosted on GitHub, offering tools for graph mining, including in-memory clustering.",
      "The toolkit finds usefulness in data mining and machine learning applications that need graph structures.",
      "Accompanied with documentation, a code of conduct, and a security policy, the project is licensed under the Apache-2.0 license."
    ],
    "commentSummary": [
      "Google has open-sourced a new graph mining library, drawing attention and discussion due to the lack of documentation or examples.",
      "The conversation revolves around the importance of having sufficient documentation and the challenges of deciphering the library's usage without it.",
      "The discussion also involves comparison with other graph processing libraries and mentions applying to Y Combinator's program."
    ],
    "points": 275,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1696347967
  },
  {
    "id": 37755648,
    "title": "Amazon used algorithm to test how much it could raise prices: FTC",
    "originLink": "https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706",
    "originBody": "SKIP TO MAIN CONTENT SKIP TO SEARCH Skip to... Select DJIA Futures33234 points with a 0.10%▲ S&P 500 F4265.75 points with a 0.02%▲ Stoxx 600441.90 points with a 0.27%▲ U.S. 10 Yr-1/32 with a 4.813%▼ Crude Oil88.20 points with a 1.15%▼ Euro1.0500 points with a 0.33%▲ Subscribe Sign In SPECIAL OFFER English Edition Print Edition Video Audio Latest Headlines More World Business U.S. Politics Economy Tech Finance Opinion Arts & Culture Lifestyle Real Estate Personal Finance Health Science Style Sports WSJ NEWS EXCLUSIVE RETAIL Amazon Used Secret ‘Project Nessie’ Algorithm to Raise Prices The strategy, as described in redacted parts of FTC lawsuit, is part of agency’s case that Amazon has outsize influence on consumer prices By Dana Mattioli Updated Oct. 3, 2023 4:54 pm ET Share Resize 231 Listen (2 min) Since Lina Khan became Federal Trade Commission chair in 2021, she’s taken on Meta, Microsoft, and Amazon, and that’s made her a lightning rod for controversy. WSJ breaks down the battles she’s picked and why she’s willing to lose. Photo illustration: Xingpei Shen Amazon.com AMZN -3.66% decrease; red down pointing triangle used an algorithm code-named “Project Nessie” to test how much it could raise prices in a way that competitors would follow, according to redacted portions of the Federal Trade Commission’s monopoly lawsuit against the company. Copyright ©2023 Dow Jones & Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8 Already a WSJ subscriber? Sign in To continue reading, choose an option below Create Your Free Account Register now to read this article for free. Register Now or Unlimited Access Subscribe to WSJ Today Just $1/Week for 1 year Unlimited access to world-class journalism Daily puzzles and crosswords Exclusive podcasts and newsletters You can cancel any time. Subscribe Now Already a subscriber? Sign In What to Read Next SPONSORED OFFERS TURBOTAX: Save up to $15 with TurboTax coupon 2023 THE MOTLEY FOOL: Epic Bundle - 3x Expert Stock Recommendations H&R BLOCK TAX: 15% OFF DIY Online Tax Filing ServicesH&R Block Coupon TOP RESUME: Top Resume Coupon: 10% Off professional resume writing EBAY: Save 25% on designer items using this eBay coupon GROUPON: Up to $50 off any order with Groupon promo code MOST POPULAR NEWS Tougher Return-to-Office Policies Are No Remedy for Half-Empty Buildings Wall Street Thinks America’s Homes Are Overvalued Ecuador Was a Retirement Paradise for Americans. Then the Drug Gangs Arrived. Kevin McCarthy Ousted as House Speaker in Historic Vote Millennials on Better Track for Retirement Than Boomers and Gen X MOST POPULAR OPINION Opinion: AOC Redefines the Term ‘Rich’ Opinion: Donald Trump’s Fraud Trial in New York Opinion: Republicans Cut Off Their Own Heads Opinion: ‘No Guardrails’ Foretold Today’s Breakdown 30 Years Ago Opinion: Jamaal Bowman and Matt Gaetz Are Alarmingly Similar RECOMMENDED VIDEOS The Wall Street Journal English Edition Subscribe NowSign In BACK TO TOP« WSJ Membership Buy Side Exclusives Subscription Options Why Subscribe? Corporate Subscriptions WSJ Higher Education Program WSJ High School Program Public Library Program WSJ Live Commercial Partnerships Customer Service Customer Center Contact Us Cancel My Subscription Tools & Features Newsletters & Alerts Guides Topics My News RSS Feeds Video Center Watchlist Podcasts Visual Stories Ads Advertise Commercial Real Estate Ads Place a Classified Ad Sell Your Business Sell Your Home Recruitment & Career Ads Coupons Digital Self Service More About Us Content Partnerships Corrections Jobs at WSJ News Archive Register for Free Reprints & Licensing Buy Issues WSJ Shop Facebook Twitter Instagram YouTube Podcasts Snapchat Google Play App Store Dow Jones Products Barron'sBigChartsDow Jones NewswiresFactivaFinancial NewsMansion GlobalMarketWatchRisk & ComplianceBuy Side from WSJWSJ ProWSJ VideoWSJ Wine Privacy NoticeCookie NoticeDo Not Sell or Share My Personal InformationLimit the Use of My Sensitive Personal InformationCopyright PolicyData PolicySubscriber Agreement & Terms of UseYour Ad ChoicesAccessibility Copyright ©2023 Dow Jones & Company, Inc. All Rights Reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=37755648",
    "commentBody": "Amazon used algorithm to test how much it could raise prices: FTCHacker NewspastloginAmazon used algorithm to test how much it could raise prices: FTC (wsj.com) 259 points by bookofjoe 15 hours ago| hidepastfavorite229 comments bookofjoe 15 hours agohttps:&#x2F;&#x2F;archive.ph&#x2F;NWu1D AMZNCommentary 11 hours agoprevThe pricing system was more archaic and manually driven than I expected. Vendor Managers were asked to review pending major price drops and raises. Later, a team in India&#x2F;Pakistan would be the first line of defense, then the VM would be asked to review in edge cases.We could easily manually lower the price, however raising it was very difficult and required managerial approval.One thing I felt was anti-competitive was price matching Costco at the \"each.\" This would result in absurd 2-day shipping prices that could not possibly be profitable. e.g Costco sells a 24 pack of soap for $1 per soap bar. We would price match the individual soap bar such that it was a $1 delivered to the customer&#x27;s door.France caught on to this \"pro\" customer behaviour and is implementing laws requiring minimum shipping charges so that E-commerce platforms can&#x27;t use \"free\" shipping in a predatory pricing manner. reply granzymes 9 hours agoparentRequiring companies to charge more for shipping doesn&#x27;t seem very pro-consumer to me. reply geraldwhen 9 hours agorootparentPredatory pricing is classic anti competitive behavior.The soap will no longer be $1 when competitors are driven out of business. reply granzymes 9 hours agorootparentHow exactly are you planning to drive out all competitors in the market for soap? reply drekipus 9 hours agorootparentIf it makes you happy we can add in \".. except for soaps\" in the ruling... &#x2F;sFor a real life example, the major grocery chain in Australia (Woolworths) did this with milk, selling their brand for $1 a litre, where everyone else had to do $2+ just to break even.Effectively selling it at a loss so that they get market dominance. They make up the loss on other products in their big chain.I think there needs to be a break-up between \"the retailer\" and their home brand being sold there, because it means the profits made on selling the competition works to the home brand advantage. reply granzymes 9 hours agorootparentAnd in Australia today is Woolworths a monopoly, or are there still other companies in the market selling milk?I think I can guess. reply drekipus 8 hours agorootparentThe consumer protection body got involved as well as a lot of negative media attention.They had to raise their prices so they don&#x27;t become a monopoly.Did you guess \"there needs to be market regulation and monitoring?\" Because: Bingo! You got it! reply granzymes 8 hours agorootparentReally? Because the reporting I found says that the competition authority cleared the market participants of any wrongdoing in the \"milk wars\".>[Chairman of the Australian Competition and Consumer Commission] Samuel said the major impact of discounted milk prices appears to have been a reduction in the supermarkets’ profit margins on house brand milk, rather than a gain.https:&#x2F;&#x2F;www.smartcompany.com.au&#x2F;startupsmart&#x2F;advice&#x2F;startups... reply drekipus 4 hours agorootparentHow is that different to what I said?> \"Effectively selling it at a loss\"Sure, maybe it wasn&#x27;t a total loss, just less profit. reply mullingitover 8 hours agorootparentprevWhen your competitors are dumping, can’t you just buy their goods and resell them at cost until they go broke? reply DecoPerson 8 hours agorootparentNot without potentially creating a scandal.“IGA coordinates anti-consumer buyout of Woolworths milk”“Why is IGA skimming your milk?”“Woolworths vs IGA — Calf scalps bull’s milk” reply nickpp 1 hour agorootparentprevThat is exactly what Dow Chemical did at the beginning of the 20th century to break the German bromine cartel.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Herbert_Henry_Dow#Breaking_a_m...No government intervention necessary! reply scarface_74 9 hours agorootparentprevEvery retailer has loss leaders. reply barrkel 8 hours agorootparentIllegal in California... reply scarface_74 7 hours agorootparentThree conditions must be met.https:&#x2F;&#x2F;www.upcounsel.com&#x2F;lectl-california-antitrust-law> Loss Leader Sales> The Act also bars \"loss leader\" sales, defined as sales (1) below cost, (2) to induce, promote or encourage the purchase of other merchandise, and (3) with the intent to injure competitors or destroy competition. Business and Professions Code 17044.If that were not the case, cell carriers couldn’t sell phones below cost to sell service plans and the entire “razor vs razor blades” model would be illegal.That also means that game console sales would be illegal. reply jjeaff 5 hours agorootparenthow would you prove intent to injure a competitor? wouldn&#x27;t all attempts to outcompete a competitor be considered an attempt to injure them?there is no way Costco isn&#x27;t losing money on their $4.99 chickens or $1.50 hot dog and drink. and I assume the point is to get you in the door so you will buy other stuff from them and not Amazon or Walmart. reply scarface_74 5 hours agorootparentYou mean legislators pass laws that are only for show and have no basis in reality? I’m shocked! reply kalleboo 8 hours agorootparentprevLoss leaders are illegal in several countries reply scarface_74 7 hours agorootparentWell since this a US lawsuit… reply adt2bt 9 hours agorootparentprevBy selling soap for a loss for longer than the competitors can stomach. If you have 10 businesses making $100M&#x2F;yr, you can lose $1B&#x2F;yr on soap (by selling a $2 bar for $1) and get a ton of customers who buy your cheaper soap. Eventually, other basic soap sellers will either need to match your prices and take their own losses to match, or hold steady hoping you&#x27;ll fold.Eventually, they are either sold to Amazon or fold, and Amazon can increase the price to $2.20&#x2F;bar and mint another $100M&#x2F;year for the next industry to attack with $1.1B. Rinse & repeat and eventually the customer is charged some percentage more for the same product once the competition is kowtowed. reply WalterBright 7 hours agorootparentWhen they raise prices again to cover for the losses, another soap company appears.\"Predatory\" pricing is not sustainable. reply altcognito 6 hours agorootparentIt is possible that large scale soap companies will be deterred due to previous anticompetitive behavior reply afavour 6 hours agorootparentprevThat is surely a vast simplification. By sheer operating scale Amazon can ship items at a cost far lower than any brand new “soap company” could. There’s no way you could be competitive on such a low priced item. reply nickpp 1 hour agorootparentAnd still countless competitors do exist. They must find some other ways to compete, because your point is valid. It&#x27;s almost like the human ingenuity knows no bounds... reply tuatoru 9 hours agorootparentprevYeah, it might be easier to crash the price of Unilever and then buy it.1. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Unilever> It is the largest producer of soap in the world,[3] and its products are available in over 190 countries.[4] reply uhura 9 hours agorootparentprevI believe the general strategy for this kind of product is to drive CX conversion. Amazon usually offers auto repurchase too, so if you are going to buy once and Amazon is cheaper, the second time too..., by the third you might actually decide to setup a auto purchase from time to time. if they discount 5 cents from the best competitor initially and in the long run they start to price 10 above, they use this to finance similar strategy to convert more CX and then they keep a more strong market position. In the end you don&#x27;t push your competitors out of the market, but consolidate a very strong consumer base. reply BlueTemplar 2 hours agorootparent\"CX\" ? reply scarface_74 9 hours agorootparentprevYes, because Amazon is going to drive out every single competitor including those who physically sell commodity goods in stores out of business - including the much larger Walmart.Is everyone forgetting just how small Amazon is when you consider all of retail? reply eganist 8 hours agorootparent> Is everyone forgetting just how small Amazon is when you consider all of retail?https:&#x2F;&#x2F;www.statista.com&#x2F;statistics&#x2F;274255&#x2F;market-share-of-t...June 2022, 37.8% of all ecommerce market share.Not sure why you&#x27;re comparing to physical retail considering the antitrust practices alleged are in the ecommerce space. And even if we were to compare all retail in the US, Amazon is likely to overtake Walmart across all retail come next year. https:&#x2F;&#x2F;www.ascentialedge.com&#x2F;press&#x2F;ecommerce-shakes-top-5-r... reply scarface_74 7 hours agorootparentSince when was 37.8 percent a monopoly?And it was also predicted by analysts that Windows Phone would overtake the iPhone by 2017https:&#x2F;&#x2F;www.computerworld.com&#x2F;article&#x2F;2473666&#x2F;windows-phone-... reply eganist 7 hours agorootparent> Since when was 37.8 percent a monopoly?https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;wex&#x2F;monopoly \"For instance, the term monopoly may be referring to instances where: [...] There are many buyers or sellers, but one actor has enough market share to dictate prices (near monopolies)\"> And it was also predicted by analysts that Windows Phone would overtake the iPhone by 2017https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Straw_man reply scarface_74 7 hours agorootparentIt’s not a straw man. It’s showing you that “what analysts predict” is completely irrelevant.And is Amazon raising prices?And how is 1&#x2F;3 of the market enough to dictate prices? reply afavour 6 hours agorootparentOf course it’s a straw man. Suggesting that every single “analyst prediction” has the same value is plainly silly. reply scarface_74 6 hours agorootparentWhat “value” do you think it has? Do you think it would be probative in a court? Have you looked at the track record of the prediction?It’s “plain silly” to cite a prediction as evidence. replygranzymes 7 hours agorootparentprevThe FTC doesn&#x27;t get to determine the relevant antitrust market. Part of Amazon&#x27;s defense will be pointing out that consumers buy soap on Amazon and in physical stores alike. It will be up to a judge to decide. reply scarface_74 7 hours agorootparentAnd the government has lost every single case it brought against BigTech in the last 4-5 years replyslively 9 hours agorootparentprevNeither is abusing monopoly power. So there’s trade offs. reply wardedVibe 7 hours agorootparentprevat best it&#x27;s requiring people who don&#x27;t care about the shipping time to subsidize those who do. reply WalterBright 7 hours agoparentprevThe problem with pricing things below cost is you lose a lot of money. reply BlueTemplar 2 hours agoparentprevIt&#x27;s probably worth noting that France has recently allowed again to sell fuel below cost (at least temporarily)... which I guess isn&#x27;t particularly surprising, since it has had been led for 6 years now by one of these far-right neoliberals.Which I guess is a great example of the unidimensional labeling being ridiculous in politics : the anti-government, pro-monopoly, anti-free-market top right being radically different from the pro-free-market, anti-monopoly bottom right.(IIRC Marx said that the bottom right were useful idiots for the top right ? But this is probably his failure at seeing history as something cyclical, I very much doubt that both the current situation can continue for long and that it spells doom for capitalism in general.) reply granzymes 14 hours agoprevFrom the book “The Winner Sells All” on Project Nessie:>The focus on matching Walmart on price also created some issues, like when Amazon&#x27;s pricing tool would repeatedly lower the price on an item to match its competitor, leading to what insiders dubbed a death spiral. Amazon created a specialized team to try to determine how and when to decide that its pricing tool should pull back and no longer match Walmart&#x27;s lowest price on a given item, but instead match the next-lowest price from a competitor. The initiative was called Project Nessie. In the end, the program was scrapped when it was determined that the tool did not lead to more profitable outcomes.This sounds like a trends&#x2F;spike monitoring algorithm that Amazon used to figure out when to give up trying to price match the very lowest competing offer. reply 1vuio0pswjnm7 9 hours agoprev\"The focus on matching Walmart on price also created some issues, like when Amazon&#x27;s pricing tool would repeatedly lower the price on an item to match its competitor, leading to what insiders dubbed a death spiral. Amazon created a specialized team to try to determine how and when to decide that its pricing tool should pull back and no longer match Walmarts lowest price on a given item, but instead match the next-lowest price from a competitor. The initiative was called Project Nessie. In the end, the program was scrapped when it was determined that the tool did not lead to more profitable outcomes, according to Amazon spokesperson Jordan Deagle.\"https:&#x2F;&#x2F;www.amazon.com&#x2F;Winner-Sells-All-Walmart-Wallets&#x2F;dp&#x2F;B... reply dakial1 8 hours agoparentIs this book any good? I was wondering if I&#x27;ll learn anything insightful or just inside stories... reply yftsui 2 hours agorootparentDoesn’t seem to be inside stories after reading the snippet above, “made up” inside stories I guess? reply 1vuio0pswjnm7 2 hours agorootparentprevJust stories. No analysis. reply cm2012 13 hours agoprevIt&#x27;s pretty crazy to me that the FTC targeted Amazon in particular because they had a bone to pick, have done massive discovery, and just gone fishing with that discovery to find anything that sounds vaguely bad.I think this is probably normal for large gov agencies and prosecutions in general, but it just shows how arbitrary and political this stuff really is. reply ameister14 8 hours agoparentWell, fortunately that&#x27;s not what happened and not how this works.You don&#x27;t get to the discovery stage of a lawsuit without a legitimate claim, especially not against a company like Amazon. It&#x27;s just not realistic for the FTC to decide to investigate and immediately be granted access to everything. A \"fishing expedition\" wouldn&#x27;t be likely to make it past a judge. Amazon has excellent counsel.That said, unfortunately for Amazon, they are also built for an antitrust model that has been a corrupt perversion of the Clayton act. It&#x27;s one Lena Khan, especially, has been trying to change since before she was in office. As a result, some of what they were doing in the open was in violation of the act - it just wouldn&#x27;t have been successfully prosecuted 5 years ago. reply pkilgore 11 hours agoparentprevWidely rationalized as the government has finite resources, so there&#x27;s a better deterrence effect of going after prominent entities and persons vs. randomly selecting bad actors.IMO this is good. The biggest and most elite among us should be held to a higher standard than a random guy on the street or a small business, not a lower one.Westley Snipes, Donald Trump, Hunter Biden, Amazon, Tesla, ExxonMobile, Amgen. I sorta want everyone and anything in that class of society on their best behavior at all times, lest the reins of government fall to a Party with reasons to make an example out of them. Unlike most, they have the resources to make it a fair fight too. reply randomcarbloke 2 hours agoparentprevAfter their failures with meta, ms, and Google it was the next one to try. reply hk__2 14 hours agoprevSo they used a super-secret algorithm to… raise prices? That’s all?When you’re the leader in your category, it’s quite obvious that competitors are aligned on your prices, and that if you raise them they would raise them as well. At a previous company our main competitor fixed most of its prices to 1 cent below ours, so when we raised the prices they followed us as well. reply phkahler 13 hours agoparent>> At a previous company our main competitor fixed most of its prices to 1 cent below ours, so when we raised the prices they followed us as well.So theoretically if you both raised prices too much arbitrarily, a third player might enter the market at a lower price. How realistic is that? I think any newcomer would be well aware of your ability to lower prices to compete, and a new player probably has costs just to enter the game so it&#x27;d be a bit risky. reply bawolff 8 hours agoprevSo? Is this supposed to be a sin?Amazon is a huge company, of course they are going to use computers to analyze how to price items. reply paulddraper 15 hours agoprev> The algorithm helped Amazon improve its profits on items across shopping categories, and because of the power the company has in e-commerce, led competitors to raise their prices and charge customers more...In instances where competitors didn’t raise their prices to Amazon’s level, the algorithm—which is no longer in use—automatically returned the item to its normal price point.> The company also used Nessie on what employees saw as a promotional spiral, where Amazon would match a discounted price from a competitor, such as Target.com, and other competitors would follow, lowering their prices. When Target ended its sale, Amazon and the other competitors would remain locked at the low price because they were still matching each other...Soooooo....it used the algorithm to raise prices, but also lower them?Anyone want to try their hand at an actual headline? reply vngzs 14 hours agoparentCory Doctorow disagrees (1) that the consumer welfare theory of monopolies is valid and (2) that monopolies which dominate industries are economically desirable [0]. I found it interesting to realize that most modern antitrust theory - that consumer harm is the only reasonable basis for government intervention in monopolies - appears to lack historical basis in the law. For instance, the consumer welfare standard severely downplays the importance of free and fair competition among businesses as a means for class advancement.> This is the “consumer welfare” standard, a theory as economically bankrupt as it is historically unsupportable. Let’s be clear here: The plain language of America’s antitrust laws make it very clear that Congress wanted to block monopolies because it worried about the concentration of corporate power, not just the abuse of that power. This is inarguable: Think of John Sherman stalking the floor of the Senate, railing against autocrats of trade, declaiming that “we should not endure a King over the production, transportation, and sale of the necessaries of life.” These are not the statements of a man who liked most monopolies and merely sought to restrain the occasional monopolist who lost sight of his duty to make life better for the public.[0]: https:&#x2F;&#x2F;archive.ph&#x2F;aTv47 reply secabeen 13 hours agorootparent> I found it interesting to realize that most modern antitrust theory - that consumer harm is the only reasonable basis for government intervention in monopolies - appears to lack historical basis in the law. For instance, the consumer welfare standard severely downplays the importance of free and fair competition among businesses as a means for class advancement.This feel similar to the myth of shareholder primacy. In both cases, the \"agreed on standard\" has little to no basis in the law, but happens to enrich the powerful and well connected over the little man.https:&#x2F;&#x2F;corpgov.law.harvard.edu&#x2F;2019&#x2F;08&#x2F;22&#x2F;so-long-to-shareh... reply username332211 12 hours agorootparentIf you look into the actual historical basis of shareholder primacy, you&#x27;ll find everything is a bit more complicated than you think it is. Shareholder primacy has an old idea, coming from a the 1919 court case Dodge v. Ford Motor Co. The facts of the case were as follows:1. The Dodge brothers - minority shareholders in Ford Motors, wanted to set up their own automotive company. They planned to use the money from Ford&#x27;s dividends to support their company.2. Henry Ford would have liked to keep his monopoly on affordable cars. He cancelled the dividend and claimed he&#x27;s going to spend the money on improving society (by selling the Model T even cheaper) as a thinly veiled excuse.3. The courts in Michigan saw what&#x27;s happening and forced Ford to issue dividends.As you can see in that case case, it was the lack of shareholder primacy that enriched the powerful. In the last 20 centuries of human history the powerful have always crushed the little man with the excuse they are serving the public interest. Beware of anything that empowers powerful individuals (like corporate CEOs and directors) to take arbitrary actions in the name of \"social responsibility\". reply mulmen 9 hours agorootparent> In the last 20 centuries of human history the powerful have always crushed the little man with the excuse they are serving the public interest.Was this not happening before the estimated birth of Jesus of Nazareth? reply username332211 3 hours agorootparentI was thinking more of the \"people&#x27;s\" party of the Roman republic which replaced electoral government with divine monarchy in 27 BC. It&#x27;s a rather fragrant example of what I&#x27;m describing as well as the first one I know of, but I doubt it&#x27;s the first time it actually happened.Christianity couldn&#x27;t have been a tool of the rich and powerful in the first century, because it only began acquiring powerful followers around the third century crisis. reply svachalek 10 hours agorootparentprevHow did Ford cancel the dividend without shares? reply WrongAssumption 8 hours agorootparentThe did have shares. reply paulddraper 12 hours agorootparentprev> myth of shareholder primacyFWIW that&#x27;s just logic.Whether it&#x27;s de jure or de facto, the owners of something will own it to their interests. reply brigadier132 11 hours agorootparentYep, why would anyone own anything that negatively impacts them.If you had 100 dollars would you invest it into a company where the CEO could just take the entire company hostage and potentially make you lose your entire investment? reply campbel 10 hours agorootparentOwnership guarantees some legal rights, but it doesn&#x27;t mean that all value is yours to consume. Employees, partners and customers are all going to fight for the value that is created and will use whatever leverage they have to get it. reply paulddraper 10 hours agorootparentOf course owners don&#x27;t get all of the value. Otherwise, there wouldn&#x27;t be employees or customers.Everyone shares.Company actions maximize company value. Employee actions maximize employee value. Customer actions maximize customer value. reply autoexec 10 hours agorootparentprevSo often it&#x27;s \"buyer beware\" when it comes to consumers, it seems like it should likewise be the job of the investor to be smart about what companies they invest in if they want to avoid a CEO who would run a business into the ground just for funsies. The risk of potentially losing your entire investment is something every investor has to accept as being possible with carefully considered and wise investments making it less likely to occur. reply brigadier132 10 hours agorootparent> So often it&#x27;s \"buyer beware\" when it comes to consumers, it seems like it should likewise be the job of the investor to be smart about what companies they invest in if they want to avoid a CEO who would run a business into the ground just for funsiesIt&#x27;s pretty much like that already, the CEO can run the company into the ground for funsies if they are insane.But if the investor catches the CEO doing this they have the right to sue to prevent it from happening and the legal system and shareholder primacy are what allow for this. Otherwise, what does the concept of ownership even mean if the owner can&#x27;t decide what is done with the business? reply autoexec 10 hours agorootparentEven without a right to sue or shareholder primacy shareholders can get a crazed CEO fired, screw with their pay, etc. reply jonny_eh 10 hours agorootparentprev> Yep, why would anyone own anything that negatively impacts them.That&#x27;s a false dichotomy. You can own something (or a share) and the purpose of it need not be \"return as much value as possible, via buybacks, in the next quarter\". reply brigadier132 10 hours agorootparentUh, so why would I choose to own that? reply feetsoup 3 hours agorootparentTo earn a return on your investment. Maximizing that return in the next quarter by threatening to take a controlling stake in the company falls into this category of course, but it&#x27;s silly to think it is the exclusive mode. The point the other poster is trying to make is that it&#x27;s absurd to pretend the options are &#x27;unilateral control of the company is determined by shares&#x27; and &#x27;shareholders are being negatively impacted by their investment&#x27;. reply scarface_74 8 hours agorootparentprevWell, people buy both Google and Facebook stock where the founders still have a controlling interest and could theoretically do anything they wanted reply fallingknife 11 hours agorootparentprevWhat is the reasonable alternative to shareholder primacy? Removing shareholders as a check on the executives would create an all powerful CEO similar to the structure of tech companies with dual class shares but at all companies. I don&#x27;t think that&#x27;s a good thing.Also, why would investors want to invest in a company where they could not remove a ad CEO that isn&#x27;t delivering returns? They would probably just reinvent shareholder primacy through investment contracts. reply Varriount 11 hours agorootparentDo shareholders actually act as a substantial check on companies though? And it&#x27;s this a check towards positive behavior, or negative behavior (for the company, society, etc.)? reply lazide 11 hours agorootparentA CEO or board of directors that angers the majority of shareholders (or an outspoken minority) doesn’t last long. That correlation is absolutely clear.Which means most CEOs and boards focus on managing shareholders at least as much as anything else they do. reply AgentOrange1234 11 hours agorootparentI agree. Unfortunately the majority of shareholders are rich dudes who just care about their numbers going up. The whole thing is psychopathic. reply lazide 10 hours agorootparentThey certainly wouldn’t be rich dudes for long if they were okay with the numbers going down! reply gruez 9 hours agorootparentprevWhat do you suggest we optimize for instead? \"their numbers going up\" is at least objective. reply scarface_74 8 hours agorootparentprevEveryone who has money in the stock market via their retirement funds care if the stock goes up reply feetsoup 3 hours agorootparentTHEIR numbers going up, is the part you may be overlooking in that. reply fallingknife 10 hours agorootparentprevIf they didn&#x27;t why would anybody be complaining about \"shareholder primacy?\"And it&#x27;s a check towards their own interests. Just like the CEO acts in his own interests. I can&#x27;t for the life of me understand why anyone thinks the CEO is more likely to act in the best interest of employees or society than the shareholders are. reply autoexec 10 hours agorootparent> I can&#x27;t for the life of me understand why anyone thinks the CEO is more likely to act in the best interest of employees or society than the shareholders are.Probably because shareholders can just care about money in the immediate term and don&#x27;t need to actually care about any one company or its long term success. They may never know or even interact with the employees of that company on any meaningful level, and may have investments in several other companies including direct competitors so that if any one company tanks they&#x27;ll still have other investments making them money hand over fist.A CEO, particularly when they&#x27;re a founder, might actually care about the company doing well and may personally know the people working for the company. They might care a lot more about the employees they work closely with on a daily basis as opposed to a shareholder who just watches numbers go up and down while deciding when best to sell. The CEO&#x27;s day to day will change drastically if their company fails. A shareholder whose company does poorly just adjusts what they buys&#x2F;sell the same as any other day. reply fallingknife 10 hours agorootparentA CEOs compensation is mostly stock so that doesn&#x27;t make any sense. But even if that weren&#x27;t true, the idea that shareholders don&#x27;t care about long term prospects or don&#x27;t mind when the company tanks because they have other investments is one of the most outlandish financial takes I have ever heard. reply autoexec 9 hours agorootparentWhen shareholders can trade in nanoseconds to exploit random anomalies in stock prices it&#x27;s harder to imagine them deeply caring about their investments in the long term. Because CEOs don&#x27;t have the option to jump ship at nearing the speed of light, it&#x27;s reasonable to conclude that they might just care a bit more. reply fallingknife 8 hours agorootparentHFTs and day traders inventory accounts for negligible percentage of shares. The vast majority of shares in all publicly traded companies are held by long term investors. Short term holders have no impact on corporate decision making. replypanarky 13 hours agorootparentprevThe internet provider for my home is a true monopoly. I have no other choice. I can&#x27;t click around in my browser, or futz with the cables in the walls and choose a competitive internet provider. I live in an area with moderate population density, so there could be more competition, but the local municipality negotiated exclusive rights for one provider. My provider knows it, they probably paid handsomely for the exclusive rights, and that&#x27;s why they charge $95 a month for unreliable and slow service.When my uncle had a heart attack, he wasn&#x27;t in a position to shop ambulances, emergency departments, surgeons, cardiologists or anesthesiologists. As with internet providers, emergency healthcare is not a competitive market, and that&#x27;s reflected in extortionate pricing.It seems like reducing consumer harm should be the primary objective of regulators. reply eropple 13 hours agorootparent> It seems like reducing consumer harm should be the primary objective of regulators.You&#x27;re not wrong, but the definition of \"consumer harm\" used in practice hinges on short-termism and is price-centered right now. \"This won&#x27;t raise prices...for now, at least\" is, by itself, an alarmingly strong argument in this arena, irrespective of market health or sustainability of competition.The Biden administration has made some steps to counter that, we&#x27;ll see if it&#x27;s really a thing. reply lazide 11 hours agorootparentPersonally I’d love it if companies like Comcast, AT&T, Wells Fargo, BofA, et. al would need to compete enough that they stop being extractive and manipulative assholes at a minimum. Or at least stop being periodic criminal enterprises. Perhaps even fair and honest market participants?I guess I need to cut back on the heroin. reply moneywoes 11 hours agorootparentprevwhy doesn&#x27;t your provider charge $200 a month then?any other examples of local monopolies reply katbyte 8 hours agorootparentBecause people would be unable to afford&#x2F;pay it reply sokoloff 13 hours agorootparentprevhttps:&#x2F;&#x2F;www.starlink.com&#x2F; is not available where you are? reply giobox 12 hours agorootparentHas anyone who ever suggests Starlink as the solution to any given fixed-line ISP problem ever actually used the service in a range of typical homes for a meaningful amount of time?I have the latest dish model, live on a one acre property with a relatively large clearing, and I still struggle with the extremely wide field of view of the clear sky (no trees, hills or other occluding items) which it needs - I simply can’t get 24hrs of uninterrupted service, and performance is extremely variable. This is even after mounting it some 25ft in the air to reduce occlusion issues as much as possible. Starlink needs an unobstructed view right down to surprisingly low on the horizon for 24 hours of solid service, which many, many homes simply won’t be able to provide.Starlink is incredible for what it is, but it’s not magic and becomes very hard to use reliably in urban areas at least with the current dishes and tech stack they are using. In many urban neighbourhoods it will be close to impossible to get a reliable 24 hrs of service unless you can knock down your neighbours homes and trees too.Starlink is best used in situations where there are almost no other options, or as a backup connection to another more reliable one, for most people. It is not a great replacement for almost any fixed line service, if you have that option. Perhaps this changes in future, but people should stop just suggesting starlink in its current form as the panacea to all ISP problems.Occlusion appears to be the single biggest enemy of starlink, and it’s super easily occluded - so much so, the app helps you measure&#x2F;approximate the occlusion in your intended use space via your phones cameras before you buy. reply panarky 12 hours agorootparentprevIn my testing about four months ago, Starlink had between one and seven outages per day lasting more than 10 minutes each. It was not reliable, and it&#x27;s even more costly than the local monopoly.My reply wasn&#x27;t asking for help to find an internet service provider. There are actual monopolists extracting monopoly rents right this minute. They&#x27;re causing genuine consumer harm right now, so stopping this harm should be the priority of regulators.Where there is no ability to access competitive offerings, these actual monopolists are a different league of economic pathology from markets where consumers can access competition by clicking around in their browsers. reply BlueTemplar 2 hours agorootparentYes, Doctorow&#x27;s article talks in the end about the exceptional nature of universal computing which means that competition is always an option.But yours isn&#x27;t a hard case of monopoly either, the main thing preventing you from creating a competing ISP is your local laws, which are failing antitrust to the point one wonders how they ever came to be ?!?(We also shouldn&#x27;t discount the importance of network effects.) reply azinman2 13 hours agorootparentprevIt’s unreliable with cloud coverage, adds latency, and may not match bandwidth with good offerings (for example my uncapped 1GBPS symmetric fiber for $60&#x2F;mo thru Sonic). It also is run by Musk which can be a turnoff for many. reply TehShrike 12 hours agorootparentI&#x27;m using Starlink in Nebraska. It is reliable with cloud coverage. I have only lost connectivity for 5-10 minutes during the fiercest part of the worst thunderstorms. reply paulddraper 12 hours agorootparentprev> I have no other choiceThere are approximately ~1 gazllion wireless internet providers where I live. reply eropple 9 hours agorootparent1) Great! You don&#x27;t live where the previous poster does, apparently.2) For many people, wireless and wireline internet providers are not substitutable. reply paulddraper 6 hours agorootparent#2 what is it about those people that make it that way? reply BurningFrog 10 hours agorootparentprevThe time to shop for emergency services is obviously before you need them. typically though some kind of insurance.Maybe that is not legally possible where you are, but there is nothing inherent about emergency services resulting in extortionate pricing. reply KerrAvon 10 hours agorootparentWhere in the world are you that you get to choose your emergency services provider? Certainly not the USA. reply BlueTemplar 2 hours agorootparentWhile discussion is specifically about the USA, it&#x27;s worth noting that for each of these two problems, there exist plenty of places in the world that don&#x27;t have one or even both of them. reply hn_throwaway_99 13 hours agorootparentprevWas really glad the article you linked mentioned Robert Bork, because it&#x27;s important to realize (a) this shift to the consumer-welfare standard is relatively new in US jurisprudence, like since the late 70s, and (b) if you consider, like I do, that Robert Bork was a Grade-A Turd for numerous reasons (can start by looking into the Saturday Night Massacre), you can see how this reading of the law is well in-tune with lots of other shitty ideas from Bork.For more info: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Antitrust_Paradox reply tshaddox 10 hours agorootparentprevThe theory that consumer harm is the only reasonable basis for government intervention in monopolies seems pretty silly on its face, since many cases of customer harm would be impossible to predict because the counterfactual is by definition unknowable. The whole point (according to Textbook Free Market Econ 101) is that competitors would try to find new ways of producing existing things for cheaper and producing new things that are better than existing things. reply mooreds 12 hours agorootparentprevIf you are interested in a book length version of this argument, I&#x27;d recommend Goliath by Matt Stoller: https:&#x2F;&#x2F;www.simonandschuster.com&#x2F;books&#x2F;Goliath&#x2F;Matt-Stoller&#x2F;... reply cataphract 12 hours agorootparentprevIt&#x27;s well known that the consumer welfare focus only started in the late 70s, thanks in big part to Robert Bork&#x27;s (the one whose SCOTUS nomination famously failed) \"The Antitrust Paradox\". reply gruez 8 hours agorootparentprev[deleted] reply colinsane 8 hours agorootparentthis is silly. antitrust came from the Standard Oil days. the name itself is in reference to Standard&#x27;s novel approach of using a trust as the root of its organizational hierarchy. notoriously, Standard Oil dominated the market by vertically integrating for efficiency, and then telling any competitor in the geographical area they were about to expand into \"either merge with us on really shitty terms, or we&#x27;ll sell at-cost until we kill your business (because at-cost for us is in-the-red for you).\"Standard Oil would be labeled \"pro consumer\" by today&#x27;s crowd: cheap prices, and highly standardized products that you could rely on wherever you travel (like McDonald&#x27;s or Starbucks is today). the notion that antitrust can be boiled down to \"consumer harm\" is new. the history isn&#x27;t vague, and thereby neither is the \"theory\". the legislation, i&#x27;m not sure about, but since it&#x27;s rooted in those two things and vagaries are easily cleared up by consulting the former. reply RecycledEle 11 hours agorootparentprevOnly on HN do I read something like this.You are smarter than me.Thank you for allowing me to be here on HN. reply jawilson2 14 hours agoparentprevIf this were an HFT algo, there would absolutely be an investigation by the SEC. You cannot place orders with the intent to artificially manipulate the price. This is obviously not a regulated exchange, but the end result is similar, so I understand why it is an issue. reply lannisterstark 14 hours agorootparentThis isn&#x27;t HFT, and Amazon isn&#x27;t placing any orders. Going \"I&#x27;m gonna lower my prices so competitors have to do it too to remain competitive\" is not illegal iirc, and I don&#x27;t think it should it be.This part in specific only benefits consumers. reply munk-a 14 hours agorootparentLoss leader pricing when used to force other businesses to close is quite illegal in the US[1] - when used for other reasons the legality varies on a state by state basis in the US... it is illegal in quite a few places. I&#x27;m not particularly familiar with international laws here.1. https:&#x2F;&#x2F;www.ftc.gov&#x2F;advice-guidance&#x2F;competition-guidance&#x2F;gui... reply sokoloff 13 hours agorootparentIs there evidence that Nessie was used to sell items below cost? reply paulddraper 12 hours agorootparentGiven Amazon&#x27;s retail profit margins....the answers must be yes, right? reply chii 10 hours agorootparentThe profit margin includes costs of reinvestment (such as all the capital costs associated with their fleet of planes etc).Per-item margin is the metric you need to use to judge whether they&#x27;re deliberately trying to price out their competitor. reply Schiendelman 11 hours agorootparentprevWhy? reply azemetre 11 hours agorootparentBecause Amazon is using AWS to literally carry every division they have. I don&#x27;t think retail has ever been profitable for them, happy to be proven wrong; but some of their newer ventures in hardware: Alexa, Kindle, Fire TV, Ring, and Echo have all been losers. If each were normal companies they would all likely be going out of business, bankrupted, or chopped up for parts for PE.It&#x27;s no different than Google using ads to subsidize failing ventures in order to gain market share. reply yowlingcat 10 hours agorootparentI think \"literally carry every division they have\" is incorrect; on a margins basis, Ads are significantly more performant than AWS, although AWS wins on volume of profits and still has great margins.Beyond that, I don&#x27;t think this kind of analysis is meaningful because it fails to account for second order effects. For example, even if retail is breakeven on a net basis, it still subsidizes a gigantic fleet of machines used to power retail from which AWS came from. Negotiating in bulk to build data centers with a significantly larger internal customers leads to better unit economics for the AWS side of the build out even if retail is break even on a first order basis or even has losses. The same argument can be made for ads as well. Amazon is great at monetizing infrastructure &#x2F;because&#x2F; they can sell not just significant volumes of the end product, but and the infrastructure used to deliver it.I think you could make the same argument for Google, which is that technically, they should be able to use the second order effects of assets they&#x27;ve needed to put together to run search and ads to sell better infrastructure. Theoretically, this should mean that GCP is king, but it doesn&#x27;t. I&#x27;m not sure there&#x27;s an obvious answer to this question, or even a great clue behind first mover advantage AWS had in cloud. reply munk-a 8 hours agorootparentIt is certainly my gut feeling that Amazon is evil as hell but I think that I&#x27;d really like to see an actual investigation by folks who study this stuff to know whether Amazon was using their pricing predatorily or not. People, especially executives, are dumb - there&#x27;s bound to be an email somewhere about John congratulating Fred about managing to shut down Plug&#x27;s Stuff Hut a small family business if that was the intent and these actions were being made to accomplish this goal.However, I thing that your last point is not relevant. What-about-ism has no place in the law or it necessarily creates a slippery slope. If Google is also able to do this and purposefully did it in a predatory manner to accomplish a market advantage they both need to be legally pursued - and GCP not succeeding doesn&#x27;t necessarily mean that Google didn&#x27;t try underhanded actions to get it to succeed - it either means those actions weren&#x27;t enough or they were incompetent (but still malicious). Either way each case needs to be judged on its own. reply paulddraper 11 hours agorootparentprevFor a long time (less so recently), Amazon retail made no or essentially no profit. reply sokoloff 8 hours agorootparentGross profit is the measure to look at to determine dumping, not net or EBIDTA. replynofunsir 13 hours agorootparentprevThere was a pretty big fight over this not too long ago. reply Jka9rhnDJos 13 hours agorootparentprevIt’s called predatory pricing. Pricing products to eliminate competition is illegal.If you sell Widgets and Sprockets, but you have a competitor that only sells Widgets, you can price of your Widgets so low (on 1-2% margins, for example) that the competitor is unable to compete and goes out of business because you can use Sprocket sales to keep your company in business during that time.Now that the other company is out of business, the price of your Widgets doesn’t matter because you no longer have competition in the market. You’re getting 100% of the potential sales and despite selling on a lower margin, you’re sales volume is now way up making those margins acceptable.You don’t have to worry about making a better Widget, or improving the Widget making process, because you have no competition. And you’ve priced yours so low, no other company can come in and attempt to enter the market because they can’t compete at your volume and margins.If there’s a high-demand material needed to make a Widget, you can put pressure on the producer to lower material prices since you are now their primary customer, or purchase the company that produces it and prevent access to the material.Predatory pricing consolidates market control and can be used to prevent access to the market. Anti-trust laws were designed to prevent this. reply username332211 13 hours agorootparent> Now that the other company is out of business, the price of your Widgets doesn’t matter because you no longer have competition in the market. You’re getting 100% of the potential sales and despite selling on a lower margin, you’re sales volume is now way up making those margins acceptable.Please don&#x27;t re-define words. This is not what&#x27;s normally called predatory pricing. Predatory pricing is supposed to involve a corporation raising prices after destroying it&#x27;s competition. The thing you are describing is nothing more than having a low margin strategy.Is every dropshipper undermining brand-name (high-margin) apparel? reply withinboredom 2 hours agorootparent> Predatory pricing is supposed to involve a corporation raising prices after destroying it&#x27;s competition.Why would you raise prices after? That would just invite competition again. Keep the prices low and competitors away. Maybe raise them to at-cost, but if your Widgets can comfortably cover the cost, then there is no reason to raise prices. reply username332211 53 minutes agorootparent> Why would you raise prices after? That would just invite competition again. Keep the prices low and competitors away.The assumption of predatory pricing is that it&#x27;s not easy for a competitor to just show up. Supply chains would be destroyed, capital equipment scrapped, etc and replacing them would be time consuming and expensive.Sometimes that&#x27;s a reasonable assumption to make, sometimes it&#x27;s not. Even if the assumptions are unreasonable, many CEOs won&#x27;t let mere reason stand in their way.> Maybe raise them to at-cost, but if your Widgets can comfortably cover the cost, then there is no reason to raise prices.So, the Widget-making capital would just sit there producing 0 ROI? Someone&#x27;s gonna object to that. A company pursues market dominance to make money. reply keep_reading 10 hours agorootparentprev> Is every dropshipper undermining brand-name (high-margin) apparel?I don&#x27;t like this analogy because you&#x27;re not comparing items of equal quality. They&#x27;re not fungible. reply username332211 51 minutes agorootparentPerhaps I&#x27;ve let my feeling of decline of quality in brand-name apparel color my argument. Sorry. reply paulddraper 11 hours agorootparentprev> Predatory pricing is supposed to involve a corporation raising prices after destroying it&#x27;s competition.That is not necessary for the definition of predatory pricing. reply epylar 14 hours agorootparentprevAn offer to sell at a particular price is equivalent to an order. reply WillPostForFood 12 hours agorootparentYou should run a sales team where you pay commission on how many offers to sell they make. reply charcircuit 14 hours agorootparentprevPrice discovery isn&#x27;t manipulation. reply FeepingCreature 14 hours agorootparentPrice collusion, however, is. The goal of price discovery is to find the price that the market will bear; it is not to find the price that your so-called competitors will agree to match. reply jjk166 13 hours agorootparentCollusion requires the other party to be in on it. Simply reacting to your competitors&#x27; actions, or lack thereof, is not collusion. reply phkahler 13 hours agorootparent>> Collusion requires the other party to be in on it.The other party is being tested for cooperation. Amazon raises the price and then reverts if competitors don&#x27;t follow suit. They didn&#x27;t revert after seeing sales drop or something, it sounds like they reverted based on a lack of \"cooperation\" from competitors.This is tricky stuff to define wrong doing. What if a company wants to see the going rate for a product and just looks to Amazon to get an idea? You know, because a lot of people will shop at Amazon by default unless there is a reason not to, like saving a bit of money. These kind of algorithms become anti-consumer the more they get automated, but they may seem reasonable on the surface or in isolation.If two algorithms are fine in isolation, but when used together cause overall market prices to rise what should we think of that? In the above example, Amazon would raise their price and the competitor would follow but not quite to the level of Amazon. Then if markets really are competitive (and fast) someone else may step in at a lower price than either, but I don&#x27;t believe a lot of markets are fast or efficient when it comes to lowering prices. reply paulddraper 12 hours agorootparentprev> The goal of price discovery is to find the price that the market will bear; it is not to find the price that your so-called competitors will agree to match.I&#x27;m sorry, huh?Who exactly do you think is participating in this \"market\"? reply axus 14 hours agorootparentprevUnder what conditions should a business be allowed to change their prices?I&#x27;m OK with a computer making the pricing decision, but like HFT it&#x27;s the frequency that leads to problems. It&#x27;s a bit sad that the \"free market\" has to be regulated like this, but beautiful theories break down when you get to the quantum level. reply lotsofpulp 13 hours agorootparent> , but like HFT it&#x27;s the frequency that leads to problems.What problems? replyphkahler 13 hours agoparentprev>> In instances where competitors didn’t raise their prices to Amazon’s level, the algorithm—which is no longer in use—automatically returned the item to its normal price point.This is a form of automated collusion. I&#x27;ll raise my price and if my competitors don&#x27;t follow suit I will lower it again. It only works well for them if the competitors are watching prices too and looking for an opportunity to raise theirs. You know it&#x27;s anti-consumer because of the conditional reversion after watching competitors prices not following.I have a feeling there is currently a LOT of this going on across all industries in the US. We&#x27;ve seen it rent pricing software, now Amazon, and I think fast food and restaurants are dojng something similar. Food has gotten obscenely expensive expensive outside a grocery store. reply scarface_74 8 hours agorootparentDo you think two neighborhood convenience stores are also automatically colluding when they look across the street from one another and raise and low gas prices to match? reply defrost 8 hours agorootparentThat entirely depends on the stores and their ownership.It is not unkown for &#x27;competitors&#x27; to stage cost cutting dramas and pantomine competitive pricing in order to create artifical demand and customer loyalty to either or both scrappy underdogs. reply beambot 12 hours agorootparentprev\"Price matching\" is standard practice in virtually every major retailer... reply losteric 11 hours agorootparentPrice matching refers to lowering prices. That&#x27;s legal, up to the point of predatory pricing.Raising prices then checking if so-called \"competitors\" follow is the illegal part - that&#x27;s automated collusion, definitively illegal.There is grey area around intent&#x2F;motive, but the OP article is clearly on the bad side.(Also, \"standard practice\" doesn&#x27;t mean legal or moral) reply granzymes 10 hours agorootparent>Raising prices then checking if so-called \"competitors\" follow is the illegal part - that&#x27;s automated collusion, definitively illegal.This is completely false.Collusion, legally, requires an agreement between competitors. Tacit collusion is *not* illegal, see Brooke Grp. Ltd. v. Brown & Williamson Tobacco Corp. (1993).Amazon can raise and lower their prices all day, algorithmically or not, as long as they aren&#x27;t calling up their competitors and telling them to raise their prices too. reply codetrotter 14 hours agoparentprev> Anyone want to try their hand at an actual headline?A better headline would be:Amazon Used Secret ‘Project Nessie’ Algorithm to Steer PricesUsing the word “steer” instead of the word “raise”. reply vb-8448 14 hours agoparentprev> Soooooo....it used the algorithm to raise prices, but also lower them?Lowering prices in the short&#x2F;medium term to throw out of the market the competition is a way of rising prices in the long term. reply SideQuark 13 hours agorootparentBy that reasoning no one could lower prices, which is ludicrous. How about showing the lowering was followed by the harm, instead of dreaming of a world where the prices are already raised? reply vb-8448 12 hours agorootparentYou miss the point: lower the prices is good, lowering to kill competition is bad for users and customers.Diapers.com is just an example. reply SideQuark 11 hours agorootparentEvery producer lowers prices trying to kill competition. It&#x27;s a major reason to lower prices, and lowering prices to kill competition is how goods over time become cheaper.I think you misunderstand the reasons prices actually become lower for many goods over time. Or why some producers get replaced over time by more efficient producers. Without this process there&#x27;s be no lower prices or more efficient production over time. reply vb-8448 3 hours agorootparentDuring the diapers war, Amazon was \"on track to lose $100 million over three months in the diapers category alone\"[0].Do you call this \"efficiency\"? How many companies in the world can afford it?[0] https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2020&#x2F;07&#x2F;emails-detail-am... reply azemetre 11 hours agorootparentprevI think the difference is that those other companies don&#x27;t have an \"AWS card\" to play to subsidize their excursions. reply paulddraper 12 hours agorootparentprevWhat if lowering prices has both consequences? reply vb-8448 3 hours agorootparentIn the short term I agree, but in the long term, no way the consumers will benefit from the absence of competition. replycuriousllama 14 hours agoparentprevI read that differently. Ie \"project Nessie raised prices. One place where this was especially impactful was when a competitor temporarily lowered prices forcing Amazon to match, Nessie got them back up quickly\" reply taeric 14 hours agorootparentAnd amusingly, I read it as \"assume 3 parties, Amazon, Target, Y. Target has a sale, so Amazon price matches sale. Y was price matching Amazon. Target ends sale, but Amazon and Y have stuck each other on the lower price.\"That is, it makes it sound like Nessie was intended to bust some broken price matching logic that would allow other competitors to stick Amazon at lower prices due to interactions with other parties. reply gorlilla 14 hours agoparentprevAmazon&#x27;s price-matching algorithm does nefarious things.I&#x27;d argue that the &#x27;lower prices&#x27; just let amazon disperse the cost of markdowns across all of the vendors. Meaning Amazon still benefits from the lower pricing because they get possibly extra sales but only absorb the cost on &#x27;their&#x27; products which were still matched to their &#x27;competition&#x27; who were stuck matching Amazon. reply paulddraper 14 hours agorootparent> price-matching algorithmHalf the stores I walk into have price matching. reply JohnFen 14 hours agoparentprev> Anyone want to try their hand at an actual headline?\"Amazon Used Secret ‘Project Nessie’ Algorithm To Attempt Market Price Manipulation\" reply yonran 14 hours agoparentprevThis is straight out of FTC Chairwoman Lina Khan’s original article Amazon’s Antitrust Paradox https:&#x2F;&#x2F;www.yalelawjournal.org&#x2F;note&#x2F;amazons-antitrust-parado.... Amazon is willing to match any competitor’s price in order to build market share while not triggering predatory pricing antitrust review (which did not consider network effects). But the price is low only as long as the competitor is there. When small competitors finally give up (since they cannot offer the same free shipping variety as Amazon) and drop out of the market, Amazon will raise the price in the long run and capture monopolistic profits. Repeat for each new market that they compete in. reply aga98mtl 11 hours agorootparentThis theorical talk of monopolistic profits does fit the market reality. There are at least a dozen retailers for every type of product Amazon sells.Can you provide an example of a product where Amazon enjoys a monopoly (or even something close to a monopoly)? reply azemetre 11 hours agorootparentWell what is your definition of monopoly? Amazon, like other big tech, like to use their highly profitable BUs (Amazon AWS, Google ads, Meta ads) to prop up other failing BUs. This is extremely anti competitive.We should not be shepherding a new tech baron era, but enforce competition in markets. Competition should be the highest thing we value in an economy. This experiment of consolidations, mergers, and acquisitions since the 80s has been disastrous. We&#x27;re reaching the point where we should break up nearly every multinational conglomerate, they&#x27;ve abused too much and suffered too little. reply timmaxw 7 hours agorootparentyonron&#x27;s comment made a specific claim: \"Amazon will raise the price in the long run and capture monopolistic profits.\"aga98mtl pointed out this isn&#x27;t actually true in practice.It sounds like you&#x27;re describing a different form of potentially anti-competitive behavior, which is not the same as the one yonron accused Amazon of. reply scarface_74 8 hours agorootparentprevWhat definition of “market” would you classify Amazon as a monopoly? reply obblekk 14 hours agorootparentprevThis is important.Not just that amzn algorithmically finds the optimal market clearing price.They may be finding an optimal dumping price to reduce competition and then an optimal profit maximizing price to capture monopoly profits.If true, this would be a clear violation of anti trust law, despite being algorithmically implemented at scale. Unclear if intentionality matters for the law - even if the algorithms learned this behavior implicitly as the solution to a legal objective, it could be illegal. reply scarface_74 8 hours agorootparentprevCan you name one product of note that you can only get from Amazon? reply toasted-subs 13 hours agoparentprev\"Amazon used it&#x27;s influence to control the market\" reply hot_gril 12 hours agoparentprev\"Led competitors to raise their prices and charge customers more\" is serious, yet it&#x27;s glossed over. If Amazon is truly telling competitors to raise prices, that&#x27;s collusion in of itself.The rest of this is just \"Amazon uses pricing algorithm.\" reply ada1981 12 hours agoparentprevAmazon Uses Computers to Aid In Price Discovery; FTC In Uproar. reply unethical_ban 14 hours agoparentprev\"Soooooo....it used the algorithm to manipulate the market price of items by way of their dominance as an e-commerce storefront\" reply shadowgovt 13 hours agoparentprevNormally I&#x27;m the first one out the gate to remind people that the legal standard in the US for monopoly isn&#x27;t size or impact on competition but consumer harm... Amazon could strangle all alternative product channels, but if they do so by legitimately finding a breakthrough that delivers product to people with 10x efficiency, that&#x27;s (a) not actionable in the US and (b) great news, system works as intended, A++ would let winner win again.... but in this case? Amazon has massive access to otherwise-secret price knowledge that they require sellers provide to participate in the marketplace. More than people realize and more than is shown in the UI (I did some work for a company that specialized in providing that data).If they used it to feed this algorithm, there&#x27;s a real solid case that they used their unique market position to make things more expensive for no consumer benefit, and the antitrust case writes itself. reply beauzero 12 hours agoprevThis is common practice with 3rd party&#x2F;used sellers of books. It has been going on since the early 2000&#x27;s and started as an excel spreadsheet that would go out and look up a price for you, match or go a penny lower, or raise prices when there was no competition. It doesn&#x27;t surprise me that Amazon did the same. Why wouldn&#x27;t they do it? reply weq 5 hours agoparentMy mum used todo this for Kmart back in the 90s. Each day should would goto competitors and write down their prices, report back, and the prices in kmart were adjusted. reply jklinger410 12 hours agoprevI continue to not see the difference in this vs physical stores measuring customer behavior and adjusting their pricing. Considering also store brands, which fluctuate pricing based on those behaviors and the pricing of their competitors. reply belval 12 hours agoparentYou and me both, this and the \"AmazonBasics is creating cheaper copies of best selling items\". That&#x27;s literally what Walmart (GreatValue) or Costco (Kirkland), and basically any brick and mortar store has been doing to improve their margins in the last 50 years.As a customer I am not harmed by this, if anything I win because the intermediate sellers between the Chinese factory that actually makes the stuff and me is replaced by Amazon who is willing to accept smaller margins. reply fallingknife 11 hours agorootparentYeah, it&#x27;s a huge win. I have probably saved over $10K in my lifetime by buying store brand generics. reply Spivak 12 hours agoparentprevEvery time issue comes up it&#x27;s abundantly clear that the complaints are due to Amazon doing a thing that&#x27;s like kinda shitty but not abnormal in business mixed with a massive pool of sellers that have zero exposure to traditional retail. You don&#x27;t see business like Nike making a fuss because to them Amazon is just another retail channel that has their own teams managing the relationship and the weirdness isn&#x27;t that different than the other people they sell to.But I also sympathize because I think the eternal September of people stepping into that world producing so many WTFs at the accepted state of things they harmonize is a potentially good vector to push for some change because no one else is going to do it. The people who don&#x27;t get eaten alive have no motivation to upset things lest they inadvertently hurt themselves. reply fallingknife 11 hours agorootparentI guess I don&#x27;t get what change is needed. Most of these Amazon sellers don&#x27;t do any of the following:- manufacturing- marketing &#x2F; advertising- warehousing- shippingSo basically they have outsourced the entire value chain, and then they complain that they are losing almost all of their revenue. Well, yeah, the people doing all of the work are getting most of the money. We don&#x27;t seem to think it&#x27;s an issue when a manufacturer takes 50% of someone&#x27;s revenue, but when a platform that handles marketing&#x2F;sales&#x2F;logistics takes 30% everyone acts like it&#x27;s highway robbery.These sellers remind me of the people who find out I&#x27;m a SWE and want me to work on their \"brilliant idea\" which they somehow think is worth something without putting in any actual work. It&#x27;s not. And the fact that these sellers are able to even make a living acting as a middle man between Alibaba and Amazon at all is honestly a great deal for them. reply huijzer 15 hours agoprevI wonder if things like this happen because Bezos is doing other things or because Bezos has changed his mind on \"Its All About the Long Term\". I suspect the former and that some executive is optimizing for his personal metrics instead of the long term success of the company. reply Someone1234 14 hours agoparentHe said that in 2003 though. Amazon lived that business model for over 15-years of its life. It is now the largest online business in the world, at some point they were going to cash-in on their monopolistic position. reply jsnell 14 hours agoparentprev> Amazon stopped using the algorithm in 2019, some of the people said.The article doesn&#x27;t say when this project started, but given that end date it would have been entirely on Bezos&#x27;s watch reply GaryNumanVevo 7 hours agoprevI&#x27;m in a (paid) Discord server that notifies people when Amazon &#x2F; etc. prices hit a certain discount from the mean. I got a 65&#x27; 4K HDR TV for $300, when the list price was $3000. I always assumed these were marketplace sellers fudging a number when updating a listing, but we experienced some of these \"death spirals\" another comment was talking about in real time. Walmart was notorious for price matching other sites and you&#x27;d be able to get some electronics for a steal for a weekend before an employee caught it. The typical fulfillment rate was around 50%, with most orders being refunded within 24 hours. reply gen220 7 hours agoparentJust curious, for Amazon specifically does this service capture price changes that don&#x27;t appear on https:&#x2F;&#x2F;camelcamelcamel.com? reply GaryNumanVevo 6 hours agorootparentYes, camelx3 will only scan as fast as an hour. The bots they use scan at a minute resolution. Most prices will get fixed (on amazon anyways) within 20 minutes. reply sidechaining 6 hours agoparentprevJust curious. Where can I find such a discord server? reply GaryNumanVevo 6 hours agorootparentIt&#x27;s private, invite only unfortunately reply beenBoutIT 10 hours agoprevIt seems like Amazon could take a cue from the tactics employed by very small shops and start dynamically adjusting prices per item per customer. Strategically lowering prices for some customers and raising prices for others could quickly become more profitable than worrying about the average customer. Amazon&#x27;s already got their system setup like a shell game with multiple pages selling the same item via multiple sub-vendors so placing blame on Amazon would be incredibly difficult if not impossible for anyone outside of Amazon.Artificially deflate prices across the board for some customers while artificially raising prices across the board for others. Some customers could end up paying 4x the Walmart price while others might end up paying 1&#x2F;2 price - there are customers that don&#x27;t care or don&#x27;t pay attention to how much things cost.Jack up prices for the big spender while slashing prices for cheapskates and offer bulk discounts to customers who will lay out more cash for single large purchases. reply rrrrrrrrrrrryan 10 hours agoparentThis would be basically impossible for Amazon to implement without painting a giant target on their back. The FTC would easily consider this deceptive&#x2F;unfair pricing unless Amazon clearly discloses to the customer how the quoted prices are determined. reply beenBoutIT 3 hours agorootparentWith black boxes being what they are, prices in a constant state of flux and the deal where amazon has several &#x27;pages&#x27; for the exact same item with each page having maybe a dozen different prices - how would it ever be possible to determine anything about how amazon does its pricing? reply partiallypro 14 hours agoprevI try not to shop at Amazon, but every time I try to just go to Target or Walmart, the prices are higher. I was looking for some probiotics and lotion just yesterday, felt that was simple enough, so I could just go to one of the two...it was $5 more (each) than on Amazon. reply SoftTalker 14 hours agoparentThe Amazon products are (possibly) counterfeit. I think this is less likely at Target or WalMart (as long as you avoid the third-party sellers on WalMart&#x27;s site -- not sure if Target has those). reply themagician 14 hours agorootparentWalmart and Target are actually much worse. Target is perhaps the worst of all, believe it or not. If a scammer creates a listing for a particular UPC, they own it within the Target system basically forever. Even if you approach Target as the legitimate brand and try to take control of the listing they refuse and then try to essentially extort you. They tell you that you can essentially “stop the bleeding” by signing up with a certain third party vendor that charges thousands a month so you can list on Target. And even if you do, that product listing still remains controlled by someone else. They wanted it to be a kind of “gold rush” where brands would run to sign up for fear of losing the ability to sell their products to third parties. reply consp 12 hours agorootparentWouldn&#x27;t you just order one as the legitimate manufacturer, check if it&#x27;s fake, and the sue the shit out of target and the seller for willfully selling counterfeit product after having been warned about it? I don&#x27;t know about the US but in some places knowingly selling fakes is illegal. reply saganus 13 hours agorootparentprevThat sounds... weird?Not saying it doesn&#x27;t happen, but if this is indeed the case, what is preventing the same scammer from essentially registering all still-unregistered UPC codes to do something akin to domain squatting?(I am not familiar with how Target online works so maybe I am missing something) reply themagician 6 hours agorootparentNothing, really. But you have to actually sell the product. On Target only one person can sell an item. Only one listing for that item can exist. It goes to whoever lists first. reply saganus 5 hours agorootparentThat seems ripe for (more) abuse.Not really sure who comes up with these schemes but... doesn&#x27;t sound well thought out. reply ctvo 11 hours agorootparentprev> The Amazon products are (possibly) counterfeit. I think this is less likely at Target or WalMart (as long as you avoid the third-party sellers on WalMart&#x27;s site -- not sure if Target has those).Do you have any data on the number of counterfeit items? I&#x27;ve been an Amazon customer for 20 years now. I&#x27;ve never had an incident, and we order pretty much every week for basic home goods.You&#x27;re saying that people are selling counterfeit probiotics and lotion on the site now?I&#x27;ve seen this counterfeit claim repeatedly on this site. While I understand Amazon intermingling inventory from different sources makes it a possibility, I&#x27;ve yet to see any data that it&#x27;s a significant issue. Not to the extent that it warrants you telling people their lotion, which is lower priced, is counterfeit and not for numerous other reasons. reply verteu 9 hours agorootparentNot a ton of research on this, but there are some worrying examples:\"According to a recent lawsuit filed by tech giant Apple in October of 2016, roughly 90% of Apple chargers on Amazon, even when labeled as a genuine article, are fake\" [1]\"manufacturers determined 20 of 47 items we purchased from third-party sellers on popular sites were counterfeit\" [2][1] https:&#x2F;&#x2F;larc.cardozo.yu.edu&#x2F;cgi&#x2F;viewcontent.cgi?article=1148... [2] https:&#x2F;&#x2F;www.gao.gov&#x2F;products&#x2F;gao-18-216 reply tbrownaw 8 hours agorootparentprevSometimes I&#x27;ll pick up a cheap SD card or two if what I actually want is just under the free shipping limit. Several and possibly even most are flaky enough that I doubt they&#x27;re genuine. reply TedDoesntTalk 14 hours agorootparentprevI try to buy from the manufacturer these days. Always more expensive but guaranteed (I think?) not to be counterfeit. reply grogenaut 13 hours agorootparentIt&#x27;s kind of crazy to me that the manuf is consistently the most expensive place to buy things. Went to the benchmade factory store a few weeks back, there&#x27;s only one. Their prices were 20% over what sportsmans warehouse a few blocks away had for the same knife.Like someone comes all the way to your one store location and you&#x27;re gouging them on price. Crazy. I didn&#x27;t buy anything at the store, got the knife at the SW.They charge even more online which is also fun. This is on $425 knives so you know there&#x27;s hella headroom. reply aaronax 13 hours agorootparentProbably they are less efficient at retail operations than a retailer, so have higher overhead. Also wouldn&#x27;t want to undercut the retailers, who are providing valuable shelf space and marketing. reply SoftTalker 13 hours agorootparentMore the latter. Why would any retailer carry your product if you&#x27;re going to undercut their price with your own direct consumer sales? reply tomjakubowski 12 hours agorootparentthe retailer can be competitive by providing convenience&#x2F;location, faster shipping speed, etc reply mportela 11 hours agorootparentWhich cost a lot, so they need to get the product for a lower price before reselling. Also, because they are buying wholesale, they usually have the upper hand in the negotiations. It&#x27;s quite common for their contracts to be contingent to the manufacturer not selling the product at the same price point reply hbosch 11 hours agorootparentprevIt&#x27;s not cheap to pay for inventory storage, warehouse workers, logistics, etc. When something like Amazon&#x2F;Walmart&#x2F;other can take on those liabilities for you, the costs come down. reply TedDoesntTalk 13 hours agorootparentprevWhat’s “SW”? reply hhh 12 hours agorootparentSportsmans Warehouse reply mitthrowaway2 14 hours agorootparentprevHow many manufacturers offer direct-to-consumer sales? reply bookofjoe 13 hours agorootparentA lot, I&#x27;ve discovered, and not just giants like Apple. reply smegsicle 14 hours agorootparentprevas long as it&#x27;s not &#x27;fulfilled by amazon&#x27; or whatever and thus commingled with their garbage reply miked85 14 hours agorootparentprevI&#x27;m not sure if WalMart is much better since they have third-party sellers. reply flictonic 13 hours agorootparentBut do they commingle inventory? That&#x27;s the real problem with Amazon, not that fakes are listed on the marketplace, but that you can&#x27;t even trust Amazon as the seller. reply makestuff 11 hours agorootparentAccording to a quick google search Walmart does not commingle inventory. However, I think they will eventually once their tech can support it. Target started commingling inventory so it is only a matter of time before Walmart does. From a logistics standpoint it makes 2-day shipping much easier since you can spread inventory out across the country. reply partiallypro 8 hours agorootparentprevYeah, but on Amazon you can buy directly from that vendor (Nivea or Bayer for example) instead of the 3rd party sellers. Those are going to be legit, it&#x27;s when you start messing with 3rd party sellers that you run into issues. I&#x27;ve gotten counterfeit items before but haven&#x27;t had any come straight from the main vendors. reply InitialLastName 14 hours agoparentprevOnly buy things on Amazon that you would buy off the back of someone&#x27;s truck. You have the same odds of those items being meaningfully subject to the US regulatory, consumer safety, and trademark ecosystem. reply WendyTheWillow 14 hours agoparentprevWalmart&#x27;s online store is actually a marketplace, with Walmart being one of the many sellers (they also do fulfillment by Walmart).So, if you see higher prices, it&#x27;s probably an arbitrage situation. Filter down to just Walmart as the retailer, and you should get the kind of pricing you expect from Walmart, albeit with the selection you would expect from a Walmart. reply partiallypro 8 hours agorootparentI mean physically walking into their store (Walmart and Target) reply bogwog 14 hours agoparentprevThat&#x27;s probably exactly the phenomenon that the FTC is talking about in their complaint. Since Amazon punishes sellers who offer lower prices off of Amazon, sellers are forced to use their Amazon price as their price floor. And since Amazon has huge market share and insanely high seller fees (upwards of 40%), then prices across the entire economy go up. reply tomschwiha 14 hours agoparentprevMost stuff you can buy at AliExpress (or similiar), but it more conventient to have same&#x2F;next day delivery and excellent customer favoring support. A lot of trash items on Amazon though, I try to not buy a lot there, it still happens. reply JohnFen 14 hours agoparentprev> but every time I try to just go to Target or Walmart, the prices are higher.Isn&#x27;t it worth paying a bit more in order to stop doing business with Amazon, though? reply paulddraper 14 hours agoparentprevI haven&#x27;t found that to be the case.But the selection, reviews, and overall experience of Amazon makes it better anyway. reply dieselgate 13 hours agoparentprevSmall price to pay to not buy from amazon in my opinion reply mellosouls 13 hours agoprevBonus points for the actual OP posting an archive link :) reply nonethewiser 9 hours agoprevOK? None of this sounds even remotely bad. reply grogenaut 13 hours agoprevIs it secret if there&#x27;s literally a building named after it?https:&#x2F;&#x2F;www.google.com&#x2F;maps&#x2F;place&#x2F;Amazon+-+Nessie&#x2F;@47.623881... reply yftsui 2 hours agoparentNot a secret, it is even covered in the blog post. https:&#x2F;&#x2F;www.aboutamazon.com&#x2F;news&#x2F;amazon-offices&#x2F;the-surprisi... reply paulddraper 12 hours agoparentprevHm, perhaps it is not the name that is secret but the algorithm itself and its use of it. reply schott12521 13 hours agoparentprevI&#x27;ve walked by &#x2F; inside of this building countless times, never knowing what \"Nessie\" was codename for.. reply pachouli-please 15 hours agoprev [–] Mega-Giant E-Commerce E-Corp Abuses Consumers and Competitors, To Everyone But Their Own Bottom Line&#x27;s Detriment replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Amazon employed a strategy, dubbed \"Project Nessie,\" to explore how much it could elevate prices, hoping competitors would follow suit, as revealed by the Federal Trade Commission (FTC).",
      "This strategy is part of FTC's lawsuit against Amazon, alleging that the tech giant has an undue influence on consumer prices.",
      "The revealing of \"Project Nessie\" highlights the concerns and focus on the potential anticompetitive practices of dominant tech firms."
    ],
    "commentSummary": [
      "The Financial Times revealed that Amazon employs algorithms to test tolerable price increases, but price hikes continue to depend on managerial approval.",
      "There are concerns about Amazon's potential anti-competitive behavior, including price matching, predatory pricing, and potential violation of antitrust practices due to its market dominance.",
      "The article also delves into the roles of shareholders in corporate decision-making, the downsides of monopolies, the trustworthiness of Elon Musk's Starlink, and the issue of counterfeit products on Amazon, including a majority of fake Apple chargers."
    ],
    "points": 259,
    "commentCount": 229,
    "retryCount": 0,
    "time": 1696357402
  },
  {
    "id": 37753292,
    "title": "I applied to 250 jobs and timed how long each one took",
    "originLink": "https://www.careerfair.io/online-maze",
    "originBody": "Hey HN - I timed how long it took me to go through the application process of 250 jobs. Some of my key findings:- On average, it took a bit over two and a half minutes (162 seconds) to apply to a job.- If company size doubles, the application time increases by 5%. If company size increases by a factor of 10, then the app time increases by 20%.- Being a government company is the single largest determinant of a long application, followed closely by aerospace and consulting firms.- The longest application time went to the United States Postal Service (10 minutes and 12 seconds).- On the other hand, It took me just 17 seconds to apply to Renaissance Technologies.- Older ATS like Workday and Taleo make job applications as much as 128% longer.",
    "commentLink": "https://news.ycombinator.com/item?id=37753292",
    "commentBody": "I applied to 250 jobs and timed how long each one tookHacker NewspastloginI applied to 250 jobs and timed how long each one took (careerfair.io) 243 points by shsachdev 18 hours ago| hidepastfavorite199 comments Hey HN - I timed how long it took me to go through the application process of 250 jobs. Some of my key findings:- On average, it took a bit over two and a half minutes (162 seconds) to apply to a job.- If company size doubles, the application time increases by 5%. If company size increases by a factor of 10, then the app time increases by 20%.- Being a government company is the single largest determinant of a long application, followed closely by aerospace and consulting firms.- The longest application time went to the United States Postal Service (10 minutes and 12 seconds).- On the other hand, It took me just 17 seconds to apply to Renaissance Technologies.- Older ATS like Workday and Taleo make job applications as much as 128% longer. lbarrow 17 hours agoI appreciate this guy taking the time to test this stuff out, but... 2.5 minutes per application seems really reasonable and short? Even the Post Office application that took 10 minutes seems fine. I get that you might have to apply to a lot of different jobs, but at that rate in 2-3 hours of focused work, you could apply to nearly a hundred different jobs.The fact that it&#x27;s that easy to apply belies the tone the article takes, which generally bemoans how hard job applications are. But they didn&#x27;t demonstrate that it&#x27;s hard at all! Moreover, the author even says in the beginning that they didn&#x27;t use any of the products that help you, like LinkedIn Easy Apply.Anyway seems interesting but this mostly just confirmed my perception that applying to jobs is pretty easy. Interviews, on the other hand... reply hollander 15 hours agoparentWhen I apply for a job it takes me at least an hour if not more. I adopt my resumé, write the motivation letter. If I spend less then 15 minutes it means I&#x27;m doing it as a requirement from the social benefits agency, for a job that I don&#x27;t want. reply BeetleB 14 hours agorootparent> I adopt my resumé, write the motivation letter.Only to find out that the other side didn&#x27;t read that letter, and most didn&#x27;t read the resume.I know, because I asked. reply Mystrl 13 hours agorootparentFunny story. When I was going the coop workshops in uni they invited in some people from microsoft to talk about what they look for in a cover letter and they straight up said they don&#x27;t read them. reply xormapmap 14 hours agoparentprevThe actual application step might take that long, assuming you have all your supporting documents ready. The trouble is you often need to adapt your CV to the job & write a cover letter at a minimum. Depending how much you care about the job, this might include studying the job description carefully, reading the company website, and maybe even contacting the company to request more information. Then proof-reading and editing everything. It probably takes me at least an hour for most jobs, even if the actual submission of files only took 1 minute in the end. reply xboxnolifes 15 hours agoparentprev2.5 is fine, but 10 feels incredibly long when half the time companies don&#x27;t even both emailing back a rejection. Gives the thought \"since they probably won&#x27;t even see it, why don&#x27;t I skip the 10 and apply to 4 2.5s?\". reply chambored 14 hours agoparentprevThis doesn’t seem to account for finding meaningful opportunities, and in my experience, that has been the more time consuming aspect of the process. reply cluoma 13 hours agoparentprevI also don&#x27;t think 2.5 minutes is unreasonably long or much of a barrier. After spending an hour looking for relevant postings at companies I&#x27;m interested in, another few minutes to actually apply is not a big deal.What is truly annoying though is creating new accounts for each company&#x27;s application portal. reply 1270018080 14 hours agoparentprev2.5 is extremely long if you just filled out the exact same information 2.5 minutes ago and 2.5 minutes before that and… reply whatamidoingyo 16 hours agoparentprev> 2.5 minutes per application seems really reasonable and short?That&#x27;s what I thought as well. I suppose 2.5 minutes feels incredibly long if you&#x27;re spending your day watching 10 second reels on TikTok and Instagram. reply arccy 15 hours agorootparentit&#x27;s long if you don&#x27;t have the connections to get a job through your network and need to sort of spray and pray to even get to a recruiter screen reply 1323portloo 14 hours agorootparentSomething you don&#x27;t want to learn post-graduation: if you don&#x27;t have the connections to get a job, you simply don&#x27;t have a prayer. reply red-iron-pine 13 hours agorootparenti&#x27;ve been hired through online applications. current gig, actually.but of all of my jobs, to include bartending and FedGov, that&#x27;s the only one. the rest came through personal contacts.i guess the military applies, but that&#x27;s not really the same thing. replycushychicken 18 hours agoprevOh goodness, someone else who shares my irrational antipathy for Workday!Workday is such junk from a user facing perspective. It’s a huge pain to index content from. I have to omit a bunch of big semiconductor companies from my job site because it’s fucking impossible to get job data out of Workday.If you work in talent at either NXP or Qualcomm, please God read this essay. You are actively harming your own recruiting prospects by using Workday!I don’t have this problem at all with the more modern ATS’es - Lever, Greenhouse, and Workable all provide nice, easy to consume APIs for this. reply rdedev 17 hours agoparentWorkday is weird. It&#x27;s a chore to apply but things like simplify jobs browser addon makes ituch better. That being said I have more luck getting a response from a workday application than greenhouse or lever. Maybe the harder pathway discourages spam applications reply smnrchrds 9 hours agorootparentWhich add-on is that? Do you happen to have a link? reply rdedev 3 hours agorootparenthttps:&#x2F;&#x2F;simplify.jobs&#x2F;This is the site I mentioned. They provide an addon for auto filling workday applications reply datavirtue 15 hours agorootparentprevI&#x27;ve gone through several workday apps in the last few days and found them all to be easy. reply rdedev 3 hours agorootparentIt&#x27;s easy but just mind numbing reply callalex 17 hours agoparentprevRunning a scraping aggregator and being an end user are not the same thing. reply pineconewarrior 17 hours agorootparentWorkday is not great for applicants either. Maybe the back-end is nice? reply artiscode 14 hours agorootparentIt&#x27;s just aa convoluted. I work for Workday. Having to re-enter your data each and every time is part of the enterprise tenant isolation model. There is no information sharing between tenants, especially PII. The UX suffers because of data privacy. reply cushychicken 17 hours agorootparentprevMaybe the back-end is nice?Lol I’m definitely swiping this next time I throw shade on a website XD reply aaronbrethorst 16 hours agorootparentprevWorkday is awful from every angle. reply dmoy 15 hours agorootparentYea, but companies will keep using it because offloading all HRMS to workday (or similar competitors) is cheaper than doing it all in house. (For companies of a sufficient size. Small enough and it&#x27;s probably not worth it) reply CoastalCoder 17 hours agoparentprevCan you elaborate on the problems with Workday?I hear lots of antipathy about it, but the only problem I&#x27;ve personally noticed is (IIRC) it consistently mis-parses my PDF resume. reply tcmart14 17 hours agorootparentOne of the biggest issues I&#x27;ve had, and I don&#x27;t know how workday looks from their end, when it comes to entering in education and specifically degree. My degree says \"Software Engineering,\" which I never see given in a list. So I usually just pick whatever is closest like \"Computer Science,\" and I always wonder if there is a system that does some correlation between what it put in the box and on the resume. reply notsurenymore 15 hours agorootparentI applied to company using workday recently. The job was a referral and the JD made it explicitly clear I didn’t need a degree. Yet it was a required field.And of course, it wasn’t a free form text field that would allow me to just enter N&#x2F;A or my high school. It was a drop-down&#x2F;search box that had a predefined list of colleges and universities. I ended up just entering some college I attended for two months before dropping out. reply tcmart14 12 hours agorootparentI&#x27;ve also seen that too. The no-free-form entry sucks. I went to a pretty big public school, but for some reason it wasn&#x27;t included in the pre-populated list on one job I was applying to. No clue what the company using it&#x27;s controls look like, but I even checked over big name public universities to see if they were on the pre-populated list and it was surprising how many were missing. Like UNC (University of North Carolina) wasn&#x27;t listed. reply candiddevmike 17 hours agorootparentprevFor me, I refuse to create accounts when applying for jobs. Too much of a pain in the ass. reply datavirtue 14 hours agorootparentprevYeah, they get what they get from the parser. The good resume is uploaded anyway. reply PascLeRasc 15 hours agoparentprevAlso, anyone know a way to automate undoing the all-capsing Workday does with resume data? Sometimes the data makes it to the correct field but it might as well not since I need to retype the whole thing anyway. reply rushils 3 hours agorootparentHey! My company Simplify built a copilot for job seekers that helps with this. We’re also YC backed!https:&#x2F;&#x2F;simplify.jobs&#x2F;copilot reply chias 14 hours agoparentprevWorkday: At Least It&#x27;s Not Oracle reply jddj 18 hours agoprevI overheard a guy at the gym a couple of weeks back saying that he&#x27;s automated (gpt4) the keyword-optimised CV&#x2F;cover letter process to the point where he was able to apply to - I&#x27;m a bit suspect on this bit, but the generation at least is believable - 75 jobs in an hour and a half.We&#x27;ve raised the noise floor significantly.Ps. If you&#x27;re here mate I hope it worked out for you. reply kiltedpanda 17 hours agoparentWe hire on Freelancer sites quite a bit and have noticed that about 50%-80% of applicants now very obviously use ChatGPT or equivalent to apply. Now suddenly every person magically has 8 years experience in a weird esoteric requirement that we&#x27;re looking for.They are currently very easy to spot and the applications go directly into the trash, so the freelancers aren&#x27;t doing themselves any favors using them.It is increasing the noise floor a lot though. reply roamerz 17 hours agorootparent> Now suddenly every person magically has 8 years experience in a weird esoteric requirement that we&#x27;re looking for.That&#x27;s crazy to me. Why would someone put ANYTHING on their resume that is not factually correct? That in the end is a disservice to yourself and to your potential employer, not to mention the pressure it puts on job seekers to push the envelope on embellishment.What I am on the fence about in my own resume is including a skillset that yes I have done but maybe a couple years in the past. Right or wrong I have decided to keep them on their knowing full well that it might create a bit of a challenge for me during the interview process. reply fouric 16 hours agorootparent> Why would someone put ANYTHING on their resume that is not factually correct?Tragedy of the commons and negative externalities. If you&#x27;re applying for a ton of jobs, then lying on your resume comes with potential upsides (you could get a job that you normally wouldn&#x27;t) with very little personal downside (employers don&#x27;t really have a way to share which applicants falsified resume data).Sure, doing this raises the noise level and makes it harder for people who don&#x27;t lie on their resumes (tragedy of the commons), but from an individual perspective, that&#x27;s a negative externality that they don&#x27;t have to care about. reply blitzar 16 hours agorootparentprev> That&#x27;s crazy to me. Why would s ...Because it works... They get the job and the person who wasnt captain of the football, tennis, rowing, lacrose, bowling, sailing, cheerleading, chess and debate teams all at the same time is just some unemployable loser.The real question is why managers and recruiters fall for it - the obvious answer is that they got where they got by inflating their CV and simply assume everyone does it too. reply roamerz 16 hours agorootparentI really hope your obvious answer is not a widespread truth. Seems like a horribly vicious circle. reply sambazi 4 hours agorootparentidk over-promising and under-delivering is called business isn&#x27;t it? reply outworlder 14 hours agorootparentprev> That&#x27;s crazy to me. Why would someone put ANYTHING on their resume that is not factually correct?I have dealt with my fair share of resume embellishments. I _will ask_ questions about anything that you put in your resume. Anything at all. It&#x27;s fair game. That&#x27;s part of my sanity check. Better have a pretty decent answer as to why something is in the resume relatively recently and you can&#x27;t even give me an overview of what it was (I assume people forget details and it&#x27;s fine).However, have you ever seen &#x27;proxy interviews&#x27;? In those cases, it&#x27;s not just a case of &#x27;embellishments&#x27;, the candidate interviewing has zero experience and the resume is not even his. Had this experience a few months ago.People go as far as lip syncing. It&#x27;s horrendous. reply lazide 16 hours agorootparentprevI bet you’re surprised that people lie all the time to women at bars too! reply tuatoru 14 hours agorootparent.. and women have totally unrealistic criteria, too, just like employers.The internet has caused mass insanity. reply lazide 13 hours agorootparentIt’s definitely helped spread it. But honestly, these situations are nothing new. Back in the dot-com crash I remember it all being the same. Even entry level school computer labs were requiring CS PhD’s! And women, well, same. reply tuatoru 13 hours agorootparentEr, we had the internet during the dot-com crash. And before it, even.As I recall, things were not quite so bad in the 1980s. I vaguely recall seeing the odd newspaper article about the intense interest in a job, because it attracted maybe 500 applications. That was newsworthy. reply lazide 13 hours agorootparentIt used to be you had to physically show up, or mail in a form to apply. That tends to put the brakes on mass applications pretty effectively.It’s less the internet, and more folks using the internet&#x2F;apps for processes that used to require in person physical presence. Which has been getting more and more common. reply roamerz 16 hours agorootparentprevHaha.. wait have we met? :DIt would be a fun experiment to have a few shots, update my resume and then read it the next day. reply lazide 16 hours agorootparentHaha, unlikely. I just know the feeling.Another experiment - have a friend update your resume while you take those shots, and then see if you can really object to the results the next day. ;) reply fastback 14 hours agorootparentprevI think it depends a bit on where you are in life.I remember exaggerating a bit when applying for my first job. I was fresh out of university and really needed that job. I spent the following 2 years miserable, I just didn&#x27;t fit in that well.Nowadays I&#x27;m brutally honest with my application and the following interviews. I see it as me choosing where to work rather than the other way around. If they pick my application then I more or less know that I will fit in rather well. It has served me well over the years.I can understand people being desperate and in need of that first job or having to start over due to different reasons though... reply jstarfish 12 hours agorootparent> I can understand people being desperate and in need of that first jobDesperate people do desperate and unpredictable things though. Case in point:> I remember exaggerating a bit when applying for my first job. [...] I spent the following 2 years miserable, I just didn&#x27;t fit in that well.Every now and then I have to investigate employees who seem to spontaneously lose their shit-- aside from one with an alcoholic spouse, so far every single one of them were just in over their head. They don&#x27;t return calls, cozy up to security and ask questions about monitoring tools, check into mental hospitals, suddenly have internet connection issues all the time, lose or destroy their equipment repeatedly, etc. One would hop onto the IT support Slack channel and see what widespread issue was currently impacting others, then claim it was happening to her (and do the same with general&#x2F;social, to see when people were getting sick and with what).I wouldn&#x27;t say it rises to the level of malingering, but it&#x27;s clear they&#x27;re desperately stalling, and it just creates a vortex that sucks them and everyone around them into. Contractual obligations stop being met because they become an entire sideshow and won&#x27;t surrender. My fear is that one might eventually resort to sabotage; the closest we&#x27;ve come was a nonperformer trying to leverage workplace violence allegations against an executive.> Nowadays I&#x27;m brutally honest with my application and the following interviews.This is the way to do it. When you weave a web of lies, you have to maintain all those threads. Pathological liars are always anxious. Honesty makes for a much easier life. reply sokoloff 16 hours agorootparentprev> Why would someone put ANYTHING on their resume that is not factually correct?If their current income is $0 and the income from a successful ruse is minimum several weeks of several thousands of dollars, and the cost per throw is around a dime…It’s not hard to see how the algebra works out. reply red-iron-pine 13 hours agorootparentyou need algebra to figure that out? reply sokoloff 13 hours agorootparentAlgebra that you can do in your head is still algebra. reply jddj 17 hours agorootparentprevSuppose you could have some fun putting made up technology honeypots in there to weed them out. reply tornato7 15 hours agorootparentI once applied to a job that had recommended experience in applied sitting algorithms. I asked the recruiter WTF sitting algorithms are and she told me it was just a test, and it&#x27;s surprising how many candidates will say that they&#x27;ve studied sitting algorithms. It works! reply kiltedpanda 17 hours agorootparentprevThat&#x27;s a great idea, we will do that. reply thaumaturgy 17 hours agorootparentprevOkay, but I have a resume that is a good fit for a lot of different roles but isn&#x27;t seen by any human ever because it doesn&#x27;t include the specific combination of keywords mentioned in the job posting.I am in the process right now of embedding keywords and a \"shadow resume\" invisibly in my resume to get past the stupid filtering software.Employers are doing this to themselves. reply kiltedpanda 17 hours agorootparentSure, tailoring your resume for a specific job is pretty standard. Perhaps an LLM that has access to your resume, along with loads of documentation on what you have previously done would provide for a much better automation process.It could read the job spec, then tailor your application with information that is actually true. Starting off with a company with information that is false is not a great way to start a (hopefully) long relationship.FWIW, we review every application we receive on the freelancer sites and do not use automated filters. reply tuatoru 13 hours agorootparentI sometimes think requiring people to telephone and navigate a voice system to get an application ID before they can apply might help.Tiny barriers can have disproportionate effects. reply ToucanLoucan 17 hours agorootparentprevI mean, big corpos for years have been automating the hiring process to the detriment of their own results. Is it really that surprising applicants are ready to do the same? Now suddenly automation is bad? Hiring managers being butt-hurt that their automated factory farm application process is being inundated with spam generated by other automation is so hilarious to me, this just made my day.You get what you fuckin deserve. If you can&#x27;t be arsed to review applications with people, why should people be arsed to apply in person? reply kiltedpanda 17 hours agorootparentIt isn&#x27;t the use of automation that is the problem. It&#x27;s that in the automated applications we&#x27;re receiving, there is very obviously fraudulent information in there. The applicants are LYING about their experience, in order to match the job specification.ChatGPT just consumes each of the job requirements, and then makes a story about how the applicant has had significant experience in all of those areas. I would prefer not to hire people who lie about their experience to get a job. reply ToucanLoucan 15 hours agorootparent> It isn&#x27;t the use of automation that is the problem. It&#x27;s that in the automated applications we&#x27;re receiving, there is very obviously fraudulent information in there. The applicants are LYING about their experience, in order to match the job specification.Who cares? The specifications half the time include experience that&#x27;s impossible to achieve because the people writing them either are also in turn using automated software and&#x2F;or because they have no idea what they&#x27;re hiring for. reply tuatoru 13 hours agorootparent... just like in dating! reply jddj 17 hours agorootparentprevI wasn&#x27;t making a value judgement, for what it&#x27;s worth. I&#x27;m not sure the commenter that you&#x27;re replying to was either. reply dvngnt_ 17 hours agorootparentprevhave you thought of adding a trap that a LLM might trip on but a human wouldn&#x27;t. reply GuB-42 8 hours agorootparentThe problem is not the LLM, the lies are the problem.I don&#x27;t think an employer would mind a résumé that is factually correct, but edited by a LLM. In the style of \"here is my résumé, emphasize the items that match this job offer, and also, fix my grammar and spelling\".Here, the candidates are using a LLM to invent experience that matches the job offer, making a fake résumé. A human doing it doesn&#x27;t make it better. reply tarek_computer 16 hours agorootparentprevWhat are the most common freelancing sites you use for job postings? reply kiltedpanda 16 hours agorootparentFreelancer.com and Upwork. Although after the torrent of AI powered garbage we received after our latest posting on Freelancer, I&#x27;m not sure we will use that again. reply tarek_computer 16 hours agorootparentThank you. I have side question for you. Sorry, all this thread coincides with another thread about LinkedIn and me getting banned without knowing why (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37748263). As a job recruiter looking for freelancers, do you expect to find the person on LinkedIn as part of the fact checking process? Would you seriously consider alternating sites such as a person’s website or GitHub and would you find the person a suspect if their LinkedIn profile does not exist ? reply kiltedpanda 14 hours agorootparentSorry to hear about your Linkedin profile. Yes, we do look at Linkedin profiles for shortlisted candidates. It helps to establish credibility and for us to get an idea of whether the claimed previous experience is legitimate.We do also ask for Github, but many folks use other repositories. Linkedin is one of the best sources of credibility for us, so I would recommend continuing to try to get it unbanned, or start a new one. replytimmaxw 17 hours agoparentprevI think there&#x27;s a dynamic where:- Desirable job applicants get hired quickly, but people who can&#x27;t get hired stay on the market- People who can&#x27;t get hired will keep applying to more and more jobs- So every new job opening gets flooded with applications from people who couldn&#x27;t get hired elsewhere- Employers don&#x27;t have time to read the flood of applications in detail, so they rely on cheap filters (keywords on resume, where they went to college, did they work at FANG, etc.)- Which makes the process worse for everyoneWhat if there was some way to limit job seekers to e.g. 10 job applications per month, industrywide? Feels like that could cut down the noise and allow employers to consider each individual applicant more carefully.(It would be hard to implement this limit, though. You could do it via data-sharing between Greenhouse, Lever, Workday, etc., but there are huge privacy concerns and it would run into the same sorts of issues as credit reports do.) reply ar_lan 16 hours agorootparent> What if there was some way to limit job seekers to e.g. 10 job applications per month, industrywide? Feels like that could cut down the noise and allow employers to consider each individual applicant more carefully.Beyond the actual difficulty in doing this without a completely centralized hiring process, this feels incredibly immoral. People have families to feed. reply timmaxw 7 hours agorootparent> People have families to feed.People would submit fewer applications, but each individual application would have a higher chance of success, because everyone else would _also_ be submitting fewer applications. The number of job openings is the same either way, so the same number of people get hired in the end, right? reply ar_lan 4 hours agorootparentI don&#x27;t know? I&#x27;m mostly arguing against the industry-wide comment, and thinking from the perspective of someone who has been laid off.I think individual companies should do whatever they want (within legal bounds), but such a big overhaul just seems ripe to screw people over. reply tuatoru 13 hours agorootparentprevrepeat of my comment above: -I sometimes think requiring people to telephone and navigate a voice system to get an application ID before they can apply might help.Tiny barriers can have disproportionate effects. reply ufmace 13 hours agorootparentprevThis, and a few more dynamics, including:- The better applicants usually have several warm leads and often don&#x27;t bother with high-effort application processes, since they have a pretty decent chance of getting hired wherever they apply, and they&#x27;re also not bothering to apply for things they&#x27;re wildly unqualified for- The worse and more desperate applicants have the time and motivation to stick through the most bizarre and convoluted application process until they get kicked out, with no regard to how well they actually match the requirementsSo thus, the more hoops you put into your process to try and stem the tide of the hoards of desperate unqualified applicants, the more you disproportionately screen out the highly qualified applicants who have better things to do and better opportunities to pursue. reply avidiax 12 hours agorootparent> So thus, the more hoops you put into your process to try and stem the tide of the hoards of desperate unqualified applicants, the more you disproportionately screen out the highly qualified applicants who have better things to do and better opportunities to pursue.Exactly. Asking me to upload a resume, and then data entry all the facts from my resume into form fields, just so that a company can not even reject me, basically tells me that I will be expendable drone #88238875 if I get a job there.I feel a little bad that sometimes the recruiter probably fills the form out for me if their process requires is, but at least they actually intend to follow up on me as a lead. reply sokoloff 16 hours agorootparentprevThis is also how 80% of companies can believe they’re hiring mostly top-1% programmers.No, you’re hiring top-1% of applicants, not top-1% of employees! reply BeFlatXIII 13 hours agorootparentAll companies share a common pool of bottom 99% applicants. reply bumby 17 hours agorootparentprevThe shortcut around this problem is for companies to rely on networking&#x2F;recommendations rather than a large net, but that comes with its own downsides. reply milesvp 17 hours agorootparentA video came across my feed recently talking about this in tech. Was talking about how the advice for getting into tech that worked really well 1+ years ago isn&#x27;t as effective today. Basically, the idea was that tech was hiring so voraciously, that they couldn&#x27;t keep up with hiring by relying on networking bringing in enough candidates, so they needed a lot more recruiting efforts to fill the pipeline. This meant, that just grinding leet code and applying was enough to significantly increase the odds of getting hired. Basically the proportion of random application hires to referral hires was high. Today, not so much. I&#x27;d expect to need to jump though a lot more hoops today (not that leet code studying isn&#x27;t it&#x27;s own major hoop) to get hired. And of course, this proportion will shift again with the next major tech hiring boom. reply tamimio 14 hours agoparentprev> If you&#x27;re here mate I hope it worked out for youI hope it didn’t, contaminating the applications like this and automating it will only hurt the ones who’s applying, because now authentic resumes and legit knowledge will be overlooked by other fake auto generated resumes. This try hard wannabe attitude is always going to ruin the experience for everyone else, either in jobs, blog writings, dating, gaming, and basically everywhere. reply jddj 11 hours agorootparentYeah. With the genie being as far out of the bottle as it is though, I opted for empathy for the unemployed guy in this case.I can see your point, absolutely, and the new normal is unfortunate and probably unsustainable.Careers boards are just another early battleground where we&#x27;re at a bit of a wait and see moment as for how we&#x27;re going to deal with all of this cheap, coherent noise. reply willsmith72 14 hours agoparentprevPeople aren&#x27;t as smart as they think.I also automate part of the process, and I think everyone should be or soon will be, but having received many CVS&#x2F;cover letters as well, the ones with too much LLM are glaringly obvious and an easy rejection.These days with companies receiving hundreds of CVs, you only need 1 reason to reject an application. Don&#x27;t make your application so LLMy that you give the hiring manager an easy reason reply hcks 18 hours agoprevProbs took less than 17 seconds to be rejected by rentech too lol reply PaulHoule 16 hours agoparentRentech wouldn&#x27;t be fulfilling their brand promise if they didn&#x27;t have an SLA to reject 90% of job seekers in less than 17 milliseconds.There was that time I was at a conference put on by Sun Microsystems in NYC and asked a question about main memory databases which got me jumped on by somebody from rentech. If I knew to the extent which hell was about to break loose at that job I was working at then, I should have applied for a job at rentech. reply rconti 18 hours agoprevEverything about the Workday UX is an absolute freaking dumpster fire.Just as an example, but one that trips me up a lot, every action seems to be controlled by an orange button (Submit?) way down in the lower right corner of your browser, even if the information you&#x27;re working on is way up in the upper right corner. reply ambicapter 18 hours agoprevTwo and a half minutes to apply to a job doesn&#x27;t seem like a lot of time at all. Do people really think that&#x27;s too much time? reply yamazakiwi 17 hours agoparentIf a person only spends that much time applying, it&#x27;s likely those applications are going in the trash, unless they have highly relevant signals. I change my resume for every company I apply, which is likely not happening. reply miscaccount 18 hours agoparentprevyou generally apply to several jobs across several companies even with referral.based on my experience its > 5 mins most of the time. reply timmaxw 17 hours agorootparentThat still doesn&#x27;t seem like very much time to me. Job hunting involves hours and hours of interviews; five minutes of paperwork is negligible by comparison, right? Are people mass-spamming applications or something? reply pizzaknife 17 hours agorootparent\"Are people mass-spamming applications or something?\"since the dawn of time (1970, jan1) reply tuatoru 13 hours agorootparentWell, in 1970 you could only do about 10 a day, because you needed to type a cover letter with the hiring manager&#x27;s name in it. Job search (newspaper \"help wanted\" ads), envelope stuffing, trips to the post office and so on took time too.Photocopies were low quality and looked obvious and got you into the trash straight away.So mass, but compared to population not very mass.The real start date was Eternal September. reply xboxnolifes 15 hours agorootparentprevIf I was getting hours and hours of interviews, an hour of paperwork would be trivial.When 2&#x2F;3 companies don&#x27;t even bother sending a rejection email, I tend to not bother doing the 20 minute job applications. reply kraftman 17 hours agorootparentprevWhen you see a job and it says &#x27;3000 applicants so far&#x27; or something, this is why reply antisthenes 16 hours agoparentprevThe answer to that depends entirely on the success rate.Over 11 years in the workforce, my success rate with applying to ATS-like platforms on company websites is 0%. And even just getting a rejection is in the neighborhood of 5-10%.So with those numbers do you still think 2.5 minutes is a lot of time? 2.5x500 applications is ~21 hours of applying to jobs.With recruiters on LinkedIn, my success rate is roughly 80% to 1st interview, 60% to second and 20% to getting an offer. reply mkatx 14 hours agorootparentI&#x27;ve reached out to recruiters on LinkedIn, but maybe I&#x27;m not an easy candidate, because they never seem to do much for me. Do I need to do something specific or hit up the right recruiter? reply sumtechguy 14 hours agorootparentYou have to keep rotating them. If they do not happen to have something that matches their current clients they basically dont help you but string you along. reply antisthenes 13 hours agorootparentprevJust for clarity, I have not reached out to any recruiters.My success rate was entirely for recruiters that reached out to me first.(I also paid $400 for an HR consultant to update my resume and LinkedIn profile as a one-time fee, so maybe that helped, but I didn&#x27;t really A&#x2F;B test it before and after) reply mikeyan320 14 hours agoprev2.5 minutes seems short for every application but it&#x27;s literally mindlessly re-entering information that&#x27;s literally already on your resume. It&#x27;s definitely the frustration more than the time IMO...PSA: I built a browser extension that autofills your job application so you don&#x27;t have to manually re-type your resume on every job -> https:&#x2F;&#x2F;simplify.jobs&#x2F;copilot (we&#x27;re also YC backed!) reply billy99k 18 hours agoprevI have found that tailoring my resume to a specific job listing increased my response time significantly. I&#x27;ve seen so many articles about people answering 50+ job ads and getting no response. reply dwaltrip 16 hours agoparentWhat kind of tailoring do you do? reply fckgw 15 hours agorootparentNot the OP but generally you want to re-write your prior job history or anything in your resume to align with and highlight anything in their job description. Match the keywords to make it clear you&#x27;re the perfect candidate. reply billy99k 14 hours agorootparentprevI re-write all of my job descriptions. Usually because I touch so many different technologies, libraries, etc in each job, I can&#x27;t put them all there. I try to tailor it toward the job description. reply thdc 18 hours agoprevWill you follow up with stats and analysis on response rate and further steps in the application&#x2F;evaluation process? I&#x27;d be interested in that. reply PascLeRasc 17 hours agoparentVery likely going to be zero. The only way I can get a chance at a phone screen is to spend at least an hour crafting my resume and cover letter for each position. reply 0xffff2 16 hours agorootparentCan you describe in more detail what this actually means? I can&#x27;t imagine what I would spend an hour doing to my resume that would somehow make it more attractive to a specific employer without outright lying. reply notsurenymore 16 hours agorootparentI’m not sure how accurate it is, but I started to play with jobscan to test my resume against ATS scanning. It was getting knocked for not having uselessly vague keywords like “business solutions” in the resume when they were in the job app, or not listing every single data format I’ve worked with. So now, I’ve just started keyword dumping my resume with everything in a job listing even though it feels stupid.I doubt it matters much though. Even when I get passed the initial resume screen these days, it’s usually followed up with a “we’ve decided to pursue someone else” from the recruiter&#x2F;hiring manager&#x2F;etc. before I can even get to an interview. And that’s for jobs where my resume seems a perfect fit. reply obmelvin 16 hours agorootparentHey, I&#x27;ve actually been wondering about this the last few days given linkedin shows high numbers of applicants for recently posted jobs. I have one main question - are you submitting multi-page resumes? I&#x27;ve been wondering if I should basically be submitting a CV for cold applications with a focus on resume scanning, and have a 1 page resume that I can send when communicating with real people reply notsurenymore 16 hours agorootparentI try to keep my resume to one page, it’s just old advice I love heard since I was a kid. I’m not sure if it’s still good advice though, I’ve seen suggestions that it’s a hold over from before online ATS systems were common.I also try to keep the formatting simple in hopes that the parser has an easier time. I had a previous resume that had a slightly more complex layout that I thing compiled down to tables, but recently I’ve been using one with a simpler linear&#x2F;hierarchical format.I’ve also removed some stuff I used to have on there, such as contributions to open source projects. No one I’ve ever talked to has cared about that stuff, even when the market was easier, but I suspect that’s partly because the OSS stuff I’ve done is in a different domain from my professional career.Right now, most rejections I get at the ATS stage don’t come till 2-3 weeks after the application. reply bluGill 14 hours agorootparentprevYour goal is to have enough keywords that the automated tools pass you onto the hiring manager. You can find these in the job description, so make sure all keywords that even somewhat fit are there (don&#x27;t claim something you don&#x27;t have! - that might get you an interview but will kill your chance even if otherwise you were fine).The real goal is when a human reads your resume they decide to call you in for an interview. That human doesn&#x27;t have time for your whole CV, so even though it might get you to that person, it won&#x27;t get you an interview: forget about the cold applications to a program, you are just wasting the other person&#x27;s time if you get past the machine. If you have a job only apply to jobs you have looked into enough to know they are worth accepting an offer if you get one. If you don&#x27;t have a job you can&#x27;t be as picky - but you have a lot more time to investigate potential companies and design a resume to get their attention.So my advice as someone who might read your resume: read the job ad and then modify your resume to make it fit. Don&#x27;t remove unrelated jobs, but make them a couple lines, while jobs where you did things more inline with what they want get more attention. I only glance at cover letters so I wouldn&#x27;t recommend you spend much time on them. Note that the above is focused on me - others are different but I can only advise how to get my attention: you get to figure out how&#x2F;if it generalizes.Honestly, finding someone worth hiring is hard. I want to know if you can do the job and nothing about the process is very good there. reply bluGill 14 hours agorootparentprevFor starters it means reading the job ad and understanding if you are even a fit for it. You can then take the various keywords in the job description and make sure those are the ones you use and not some variant that HR won&#x27;t realizes is the same thing. Generally you can tell they are looking for someone to do something in particular so you can add a line or two about doing something like that, while taking away some other line they don&#x27;t care about. (In the US we use resume not CV, resumes are support to be short: once you have been around for a few years there will be things you can do that just don&#x27;t fit on the page)If you are just finishing college [or worse looking for an internship] you will have trouble putting enough on a page to attract attention. Once you have been around for a few years though you should be cutting things - and that means there are things potentially relevant you can put back on.It also depends on how focused you are. If you are only interested in OpenGL 1.1 jobs you would cut anything not related that and just have a single resume that you don&#x27;t need to focus. I used openGl 1.1 to make a point: it is obsolete so normally you wouldn&#x27;t put anything about it on a resume - but there is a small chance you would encounter it as a nice to have in an otherwise interesting job (If the job wants someone who knows Vulkan but is 10% maintaining old products: openGl 1.1 might catch their attention even though you have no Vulkan experience) reply remus 14 hours agorootparentprevAnswering as someone who&#x27;s done a little hiring recently, but for me it&#x27;s obvious when someone has put some time and effort in to their application via the cover letter. A good application will demonstrate some understanding of what the role is likely to entail, a bit about the company as a whole and a bit of understanding of the wider market the company works in.Granted this is for a relatively small company in a niche area. Maybe it is different when applying for jobs at big companies. reply barryrandall 13 hours agorootparentprevBack when I thought companies put any thought into their ads, I&#x27;d take each ad, make a list of things the job is looking for, and update my base resume with any relevant experience, making sure that things on the \"looking for\" list were at the top of their respective category.I scaled back a bit once I saw how people actually write job descriptions (search Google until they find something \"close enough\" and post that verbatim). reply jason-phillips 16 hours agorootparentprevIt&#x27;s the thought that counts... reply jborden13 16 hours agorootparentprevMaybe my side project will be accreditive to your process, apologies for the clunky UI - working on it:https:&#x2F;&#x2F;www.kindbuds.ai&#x2F;ght&#x2F;spence reply super256 17 hours agoprevI only apply to companies I really, really want to work for. Might sound a bit kitsch, but I work a lot on my cover letter to show how eager I am to work with them.Of course it could be luck, but so far I wrote only two applications in my life and got both got accepted + for job offer from both (after interviewing).When I tell people about my cover letters some say that HR don’t even read cover letters, but I want to believe otherwise :D reply postcynical 17 hours agoparentI do read them. It’s the easiest way to stand out for candidates. A paragraph about what interests u in this role usually is enough. reply remus 14 hours agorootparentSame here. In my experience a cover letter is the fastest way to tell if someone actually cares about the role enough to do a little research and understand what is involved. reply kevmarsden 17 hours agoprevMost job application flows are suboptimal, but job seekers need to stop maximizing the number of applications they submit and instead create a few really great applications.I typically spend two or three hours writing a cover letter and customizing my resume for each job application. reply gear54rus 17 hours agoparentMy god.. 3 hours for a cover letter, mind sharing an example of one? reply bluGill 14 hours agorootparentCover letter and resume. Which is more reasonable. You should be reading the job ad closely to make sure you match that. Then do some research on the company to make sure you understand what they do - often you can think of some other relevant experience that isn&#x27;t in the ad.Don&#x27;t forget that this isn&#x27;t a hackernews comment. You should take an hour (day is better!) break and then reread and revise everything. Don&#x27;t send the rough draft or even the second draft in - follow the whole writing process they told you in school. Some people will spot a spelling error at the bottom of the page in one second and reject you, so make sure everything is perfect. reply dvngnt_ 17 hours agoparentprevdepends on your career level. reply Inviz 17 hours agoprevI always tell my friends that i apply to 100 jobs, go through interviews with 15 companies, get 5 offers. In my experience as a generalist it&#x27;s possible to find _something_ interesting about the position once you get an interview, there&#x27;s no reason to pre-filter self to a very specific tech stack. As long as they pay! reply jldugger 17 hours agoprevI assume career page quality is related to how important immigrant visas are to staffing, where \"unable to find qualified local labor\" is a criteria. No idea how to quantify that though. reply mr_o47 17 hours agoprevI remember applying to RenTech and getting a rejection email in 3 hours and let me tell you it was quick.Most companies I have applied to would probably take a week or more than a week reply notsurenymore 15 hours agoparentSome years ago I remember applying to a job at a well known US bank. It got to a series of checklist questions, one of which asked if I had a degree, I checked no, and it kicked me out before I could even finish the application.Honestly, I wish more applications were like that. Just let me know sooner rather than later what your filters are. reply lelandfe 15 hours agoparentprevDisney’s rejection for a front end position came a full year after I applied reply sumtechguy 14 hours agorootparentMy record was 2.5 years later for a different company. At that point why bother? reply mr_o47 15 hours agorootparentprevthat&#x27;s insaneI remember applying for a position getting a phone call and then getting rejected afterwards but the funny thing was I got another email from the same company asking about my availability for the interview and I was like Woahh, are you sure you sent the email to the right person because I remember you already rejected me reply ndiddy 15 hours agorootparentThe candidate(s) they wanted probably rejected their offer. reply brightball 16 hours agoparentprevThe only time I reject an application that quickly is if it was completely outside the realm of the job.At least for programmers, I always cast a wide net unless there is some very specific item that has a very steep learning curve. reply ipaddr 16 hours agoparentprevThey are quick. From applying to getting an offer to getting laid off 1.5 seconds. The lawsuit is taking years reply squeaky-clean 16 hours agoprevSeems like even though multiple industries were tried these are all for tech positions, right? I wonder how long it would take for non-tech positions. I remember applying online for a minimum wage job at Burger King around 2010 and the application took about 45 minutes. Not only did you need to upload your resume, you need to enter every detail again in a long web form, and then there was a \"personality test\".Funny sidenote, my manager told me I was the first application he received that got a perfect score on the personality test. The next best applicant in the queue at the time was 85%. The questions were all things like \"If a customer is complaining do you: Smile and listen to their complaint? Or Tell them it&#x27;s fast food and to deal with it?\" or \"A customer accidentally included an extra 5 dollar bill when giving you their payment. You could keep this $5 without anyone knowing, do you keep it?\" Just all very obvious answers and I don&#x27;t understand why you wouldn&#x27;t at least lie. reply suriyaG 18 hours agoprevIt&#x27;ll be much more interesting to know how long the \"behavior assessment\" that you get sent immediately after applying. Having just attended one mind numbing assessment from Citibank, which took more than an hour reply shsachdev 18 hours agoparentmy next article is likely about that :) reply stanford_labrat 15 hours agoparentprevI remember doing these at Blackrock and JP Morgan. Talk about tedious... reply mongol 17 hours agoprevThe worst is is you have to enter all your skills in some CV-builder, and select your choices in tons of select boxes. reply marssaxman 13 hours agoparentI skip all those - any company willing to begin a relationship with potential future coworkers by putting them through such a tedious, time-wasting exercise is not a place I would want to work. reply minitoar 17 hours agoparentprevI just never do this and it’s never been a problem. reply Ilasky 17 hours agoprevThese are some really interesting results that I have felt empirically, but is great to put some data to. One thing this doesn’t take into account is the best practice of tailoring resumes to job descriptions. This is a time-intensive task that does actually improve call back rates. A bit of a self-plug, but I’ve made Resgen[0] to help do this for you.[0] https:&#x2F;&#x2F;resgen.app reply MostlyStable 15 hours agoprevI remain confused by the absense of a sizeable reverse headhunter industry.Headhunters are trying to find a best fit for a company. Why is there no one trying to find a best fit for a candidate? The inherent problem in hiring, on both sides, is search costs.I&#x27;m probably going to be searching for a new job sometime next year. I think that, to avoid the time spent on these appliations, and to ensure a relatively good fit, I&#x27;d be willing to pay quite a bit, or to try and align incentives pay a percentage of the pay increase from my current job for a period of time or something like that.But essentially, it seems like free money on the ground to be able to pay people to solve these search frictions.I realize headhunters exist on the company side, but it really seems like A) there should be more of that and B) there should be a reciprocal industry representing the applicants. reply Spoom 14 hours agoparentCompanies are both more solvent and pay quicker than your typical job seeker (who is usually unemployed). Basically, less risk. reply willsmith72 14 hours agoparentprevIsn&#x27;t that just a recruiter at an independent agency? Reply to LinkedIn messages and even if you don&#x27;t like the role they have at that time, those people are your headhunters. reply ignite2 11 hours agoparentprevThere used to be many of these, but they became mostly scams that preyed on desperate job seekers. By the time I started job hunting, many years back, this industry was viewed about like nigerian princes. No sensible person would deal with them.There are career coaches to be found these days, I&#x27;m not intending to include them in this rant. I&#x27;ve had a few friends use them, and I&#x27;ve talked to one and he was reasonable.As an individual, I&#x27;d avoid anyone that wants a percentage. Specified fee for helping with resume, linkedin profile, etc., fine. reply kraftman 17 hours agoprevIs how long it takes to submit your generic CV a valuable statistic? Is there a benefit for companies to make it easy for you to apply to them?I don&#x27;t think anyone is sitting around going &#x27;We just have no good candidates, if only people could apply in 71 seconds instead of 160, then we&#x27;d finally see the applicant quality increase&#x27; reply Peroni 16 hours agoparent>Is how long it takes to submit your generic CV a valuable statistic?Not at all. If anything, you want people to take their time when applying. In almost every A&#x2F;B test I&#x27;ve ever run, the more questions I introduce into the application process, the quality of applications increases exponentially, the time to filter applications decreases, and the decrease in the total number of applications was barely noticeable. reply amDonezo 17 hours agoprevYeh; am done with “professional careers”.De-urbanizing and moving to a more rural location. Will get a job at a grocery store to avoid all the future dead people in office jobs whose toxic positivity psychosis is leading them to believe they’re leaving behind the most important outputs for the future.Such jobs are not meaningful. They’re just a form of social organization, like religion before us.Signal attenuation due to generational churn comes with loss of experience and an obligation on the future to re-discover and encode that knowledge. We only propagate the species, social norms are not fractal and immutable; nothing we’re doing is preserved. We’re just jumping through elites hoops for bigger paychecks.Been there, done that. It gets just as banal and repetitive as everything else. It’s drug addiction, chasing dopamine highs. Physics will end the species and all this effort will be for naught.Given that there’s little point to being a court jester and dancing for the high court. reply CapstanRoller 13 hours agoparentMust be nice to have enough money to simply buy a house. reply sambazi 3 hours agorootparenttbf the dynamic range here is pretty big.there are usually old&#x2F;unmaintained houses on the edge of nowhere to buy for less than a years rent of a flat in downtown. reply MattPalmer1086 17 hours agoprevI guess this is interesting research, but for me not personally useful.I&#x27;ve had a lot of jobs in my long career, and I have spent a considerable amount of time researching and applying for the few ones I am interested in. I have a pretty good hit rate.Do people really apply for hundreds of jobs? reply notsurenymore 15 hours agoparent> Do people really apply for hundreds of jobs?I’ve done it twice. The first time was when I was 18 or 19 trying to get my first job in the industry. I put in a ton of applications, and while it felt tedious, it only took around three months at the end of the day.The second time was this time. I’ve probably put at least 100 applications in over the past year, and it’s been very hit or miss on whether I get interviews. I don’t bother researching and applying for the few jobs in interested in, because I’m usually not qualified for them. Most of my recent job applications have been exclusively ones that looks like they’re relevant compared to my resume, and some that are different from the work I’ve done in the past, but utilize tangential skills. reply MattPalmer1086 14 hours agorootparentYes, I can see that working early in a career. Now that I think of it, my best friend did exactly that too when he was 19, and he managed to get his foot in the door.For what it&#x27;s worth I think your current strategy of applying for relevant jobs is the right one, particularly if it&#x27;s a difficult market where you are. Tangential skills are also good possibilities. I hope you find something that works for you. reply bluGill 13 hours agorootparentWhen you are fresh out of college you don&#x27;t stand out. A few months ago we had 100 applicants for an entry level programming position. Every applicant was finishing their bachelors (a couple in CS, the rest in computer engineering) in June. They all had done some internship (in a not very relevant area but that is expected) and a class project. Almost all of them were Eagle scouts (until recently scouts only allowed boys so this is indirectly an assessment of boy&#x2F;girl ratio of applicants and thus illegal for me to care about). It was really hard to filter down who to interview because they all look the same and we only had one position. As such spaming your resume is the best bet when fresh from school.When you have been our of school for a while we get a few resumes with interesting experience that can make a few people stand out. Most probably have standout experience, but it doesn&#x27;t come out in the resume. reply MattPalmer1086 13 hours agorootparentMost people are terrible at writing resumes. We used to joke that we would throw half the resumes in the bin immediately on the grounds that we didn&#x27;t want to employ unlucky people!The biggest mistake I see for people with experience is that they describe their role and responsibilities but not what they specifically contributed and why they are awesome.The best advice I was ever given is that your resume is a marketing document. It should contain things you would like to be asked about in an interview. Cut out everything else. reply BeFlatXIII 13 hours agorootparentprev> It was really hard to filter down who to interview because they all look the same and we only had one positionDid you make up some BS metrics, or did you do the honest thing and draw résumés from a hat? reply MattPalmer1086 12 hours agorootparentFrom my experience, when you have too many similar resumes there are no useful metrics you can define. You just have to choose some to interview and bin the rest. reply bluGill 9 hours agorootparentprevThat was my bosses problem. If we have more than 10 résumés he filters them and tells me. He used to be technical so he can filter well enough and I can focus on real work not hiring. replysebbul 13 hours agoprevWhen I was involved in hiring at a major enterprise, our HR recruiting lead advised against reviewing candidates directly from the ATS. The reason? Due to our company&#x27;s reputation as a top workplace, many applicants indiscriminately applied to every role available. Our HR lead essentially served as a human spam filter. reply EarthAmbassador 16 hours agoprevWhy do HR people demand the duplication of efforts including manual entry of the very same information on a resume or LinkedIn profile, two resources that many HR people request. This is abusive, lazy, and easily solvable issue. I don’t bother applying with any company that feels entitled to ware my time. reply Peroni 16 hours agoparent>This is abusive, lazy, and easily solvable issueAgree with the first two but not the last point. The types of companies you&#x27;re referring to are almost always large and not particularly fast moving. Swapping out their archaic applicant tracking system is often a non-starter as it&#x27;s usually embedded in their HRIS and swapping out a HRIS in a large company is a monster task. reply HWR_14 16 hours agoparentprevIt&#x27;s not their time or their budget, so it works out to $0 cost on their spreadsheet. reply euroderf 16 hours agoparentprevA.I., anyone ? reply tamimio 14 hours agoprevNice write up! If anything, this proves that the job application process is a total mess, and of course, when things are chaotic, it opens opportunities to data miners, scammers, and the likes. reply tennisflyi 17 hours agoprevOk. Update us on the average time to get a reply from ghosting to months. reply blitzar 16 hours agoparentAverage time to get a reply = infinity. reply ddgflorida 15 hours agoprevIt&#x27;s been a decade ago, but I remember the nightmare of using Taleo to apply for a job. I always thought I could write something better in about a week. reply 0cf8612b2e1e 15 hours agoprevNow we just need a resource about which companies will ghost you at some point in the process.I can take being rejected. Disappearing into the night is so disrespectful. reply MikeTheRocker 18 hours agoprevI love seeing this kind of detailed analysis of everyday things like this. It always seems to reveal intriguing correlations and insights. reply generic92034 17 hours agoprevI am looking forward for the days when my AI is negotiating with several corporate HR AI wherever I will finally end up. ;) reply Glyptodon 16 hours agoprevNot including things like any custom cover letters, coding take-home, phone screens, resumé tweaks, etc. reply alpb 18 hours agoprevWould you consider opening the spreadsheet to anyone without requiring them to sign up to your mailing list? reply benterix 18 hours agoparentWhy would they make the trouble of doing this experiment and posting it to HN then? reply donretag 15 hours agoprevGreat article. Well done. Have you considered adding an RSS feed for your content? reply shsachdev 15 hours agoparentthanks! I have not but will def consider it reply willsmith72 16 hours agoprevIt would be nice to see the expected salary for each role vs application time too. reply danielovichdk 15 hours agoprevAbsolutely love this post. Thank you for your hard efforts. Great!!! reply polalavik 17 hours agoprevgreat now time how many hours HR phone screening, hiring manager phone screening, take home tests, (multiple) all day onsite interviews take if you want to follow through on like 3 employers. reply m1117 18 hours agoprevThat is job stacking fo&#x27; real! reply newfonewhodis 18 hours agoprev> - The longest application time went to the United States Postal Service (10 minutes and 12 seconds).My spouse is not in tech but in comms. The quickest I know she&#x27;s done an application has been O(hours) primarily due to writing cover letters, personalizing resume and often sending in clips.Having seen that, I find it amusing that we techies (yes me too) get annoyed when we have to provide our Linkedin AND all the information on that page. reply goodcanadian 17 hours agoparentYes . . . I don&#x27;t think I have spent less than 1 hour on any serious job application I have done. It usually takes multiple hours. My spouse is in Academia, and her job applications take days! reply xnorswap 18 hours agoprevThe tldr seems to be that it&#x27;s exceptionally quick and easy to apply for jobs, perhaps too quick and too easy, there&#x27;s essentially no barrier to applying to any and every job advert seen given it apparently takes less than 10 minutes in the \"worst\" case. reply benmo_atx 16 hours agoparentWhich makes the overall experience worse for hiring managers and qualified applicants. reply bluGill 13 hours agorootparentAnd in turn we turn more to automated tools to filter even though we know they are rejecting good people who just didn&#x27;t write some keyword. In turn applicants are writing worse resumes just to ensure they have the right keywords even if they honestly barely have it (NEVER say you have a keyword you don&#x27;t have, but if you just barely have it put it on and prepare to explain how little you know if that really is important) reply pestatije 17 hours agoprevcool...now time how long it takes for a signed contract offer reply 18 hours agoprev[deleted] transcriptase 17 hours agoprev [–] Apply to Procter & Gamble, and as part of the application itself enjoy being put through 1-2 hours of ambiguous personality&#x2F;morality testing and Mensa-style math, memorization, and logic puzzles! With adaptive difficulty!It&#x27;s the most surreal and absurd experience. There are actually prep courses you can pay for just to improve your odds on the P&G testing process just to get someone to actually see your resume. Be warned though: if you don&#x27;t score sufficiently high for the position you&#x27;ve applied to, you&#x27;re banned from applying for any job at the company for a year! reply joe5150 17 hours agoparentMany years ago, I had to complete a long personality inventory AND about 45 minutes of \"what would you do\"-type scenario simulations to apply for a job at...Kmart!Then the manager no-showed when I came in for the interview. reply notsurenymore 15 hours agorootparentRetail applications have gotten lazy recently. I put one in, and they wanted a recorded interview. No one on the other side, I was expected to just awkwardly record myself answering some questions and send it in. reply joe5150 12 hours agorootparentI was asked to do that for a state government job that paid above market, so it&#x27;s not just retail. It was awkward and I would probably decline to do it again unless for some reason I still really wanted the job. reply rgblambda 17 hours agoparentprevIs P&G a particularly great employer that justifies such a bizarre job application? reply HWR_14 16 hours agorootparentIf you want to work in advertising and marketing, yes. If I recall correctly it&#x27;s reputation in those circles is equivalent to FAANG in tech. reply itronitron 16 hours agorootparentI recall seeing a presentation in school by a young brand manager at P&G where they were talking about their excitement of being in charge of sales and marketing of a particular body wash in the southwest region. reply bythreads 16 hours agorootparentprevWhat?, no - not really. Perhaps in fmcg specialty shops you&#x27;d get some creds - but as in tech you&#x27;re primariæy judged on past projects not past x reply lazide 17 hours agorootparentprevThe worse the job, usually the worse the application process. In my experience anyway. reply sokoloff 16 hours agorootparentprevIt’s pretty regionally highly regarded, to the point that my brother-in-law voluntarily resigned to form his own business and people would disbelieve him that he wasn’t fired from P&G (even after knowing that his business is successful). They just assume “no one around here quits P&G so he must have been fired.” reply ingoaf 16 hours agoparentprevAnd then, if you&#x27;re feeling lucky apply to Canonical for a 10 step recruiting fun. reply O_nlogn 16 hours agorootparentCanonical sent me a 30 question \"written interview\" which asked all sorts of personal and irrelevant questions as the first step in their interview process. I ghosted them. reply moneywoes 6 hours agoparentprev [–] are they considered the faang in their domain? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author conducted a study on the time required to apply for 250 jobs, finding that the average application time was 2.7 minutes.",
      "The study indicated that application time tends to increase with company size, and sectors such as government, aerospace, and consulting have lengthy applications.",
      "The author noted that older Applicant Tracking Systems (ATS), like Workday and Taleo, tend to extend the duration of job applications significantly."
    ],
    "commentSummary": [
      "The text enumerates multiple challenges and frustrations encountered in the job application process, including negative experiences with automated systems and lengthy application periods.",
      "It presents a debate over the use of artificial intelligence (AI) in evaluating resumes, with concerns about fraudulent resumes, and discusses the impact of restricting job applications on both applicants and employers.",
      "The text emphasizes the need for customization in resumes and cover letters, sheds light on varying response times from companies, and underscores the necessity for enhancements in hiring procedures."
    ],
    "points": 243,
    "commentCount": 199,
    "retryCount": 0,
    "time": 1696347378
  },
  {
    "id": 37759873,
    "title": "Career Advice (2013)",
    "originLink": "https://moxie.org/2013/01/07/career-advice.html",
    "originBody": "ABOUT LIFE PROJECTS BLOG Career Advice Jan 07, 2013 To my great surprise, young people now somewhat frequently contact me in order to solicit career advice. They are usually in college or highschool, and want to know what the best next steps are for a career in security or software development. This is, honestly, a really complicated question, mostly because I’m usually concerned that the question itself might be the wrong one to be asking. What I want to say, more often than not, is something along the lines of don’t do it; when I got out of highschool and focused on the answer to that same question, it was very nearly one of the biggest mistakes of my life. Since I get these inquiries fairly regularly, I thought I’d write something here that I can use as a sort of canonical starting point for a response. Tyler Durden was wrong, you are your job. In 1971, Dr. Philip Zimbardo conducted a psychological experiment that is now popularly known as the “Stanford Prison Experiment.” He constructed a makeshift simulated prison in the basement of Stanford University’s psychology department, took a group of volunteers who tested as psychologically “normal” with no criminal background, flipped a coin, and assigned half of them to be “prisoners” and half of them to be “prison guards.” Initially, everyone acted awkwardly; the situation was so obviously foreign to their sense of self and their lived experience that they found it strange and even humorous. Fairly quickly, however, the situation eclipsed all of that. The prisoners became divorced from their former identities, and began to act from the place of being a prisoner. They alternated between staging minor protests and ratting each other out, to going on hunger strike and suffering psychological breakdowns. The prison guards also became prison guards, even though they were allowed to return to their normal lives in between their eight hour shifts. They went so far as to invent forms of psychological punishment, create solitary confinement cells, pit prisoners against each other in order to break them, and give long speeches while slowly pacing the hallway and rattling their wooden batons across the bars of the cell doors. Had the coin landed differently, the individual people would have been on the reverse sides of the cell doors, but the roles would have been the same. Things got so out of control that the experiment, originally scheduled for two weeks, had to be ended after six days. The experiment was positioned as a comment on institutions, but the simpler lesson I draw from it as an individual is just be careful what job you take, because your job will change you. This is obviously true for these types of hyperbolic examples — it’s not hard to imagine that a customs agent (who spends 40 hours a week looking suspiciously at people, thinking skeptically about the honesty of what people say, scientifically observing people’s eye movement, and trying to trip people up with confusing questions) might approach a conversation with someone they’ve just met at a party differently than they would have before taking the job. It’s equally easy to imagine that someone who has worked as a prison guard for a decade might approach a romantic relationship differently than someone who has worked as a grief counselor for the same period. But it’s not just cops, prison guards, and customs agents. The context of one’s life defines not just what but how one thinks, and a job tends to dominate the context of one’s life — particularly when that job is considered to be part of a career. Your job will change you. The choices we make. I recently got around to reading Aaron Cometbus’ enjoyable “Cometbus #54, In China With Green Day?!!” One of my favorite quotes was: Some things are like that—they strike you as repugnant for instinctive reasons, probably having to do with your culture and the way you were raised. The French word “gauche” comes to mind, but I preferred the Hebrew word “treyf.” Literally, it means not kosher, but I also use it to describe things like cars, bars, strip clubs, guns, dogs, rock-n-roll, and football games. Things that are treyf, you avoid, not because you hate them per se, but because in avoiding them you keep yourself from becoming like the people you hate. – Aaron Cometbus, “Cometbus #54” The concept resonates with me for the same reasons. Like Aaron, I have my own list of things that are treyf, not because I find them necessarily unenjoyable, but because they add up to something that I ultimately dislike. For instance, whenever I get on an airplane and walk past first class, I inevitably go through a familiar mental process. First, I’m envious. The passengers are already in their comfortable seats, drinking from champagne flutes, contemplating the moment after takeoff when they can recline into their cocoons and watch a movie of their choice on demand. But then, I register who is sitting in those seats. It’s usually almost all predominantly unhealthy looking middle-aged white men, who it is clear from a glance have spent literally hundreds of hours of their lives over the past year in these airplanes. And suddenly, I’m glad that I’m not sitting there. Those seats are treyf for me, not because I don’t envy extra leg room, but because I don’t envy the people sitting in them. There’s a reason the bulk of the first class passengers resemble each other, just as there’s a reason prison guards tend to act the same. I know that by making choices designed to land me in the first class cabin, it would be difficult to avoid also inheriting the dreariness associated with its current occupants. The future is looking back at you. In the context of a career, these concepts make it simple to look into the future. Jobs at software companies are typically advertised in terms of the difficult problems that need solving, the impact the project will have, the benefits the company provides, the playful color of the bean bag chairs. Likewise, jobs in other fields have their own set of metrics that they use to position themselves within their domains. As a young person, though, I think the best thing you can do is to ignore all of that and simply observe the older people working there. They are the future you. Do not think that you will be substantially different. Look carefully at how they spend their time at work and outside of work, because this is also almost certainly how your life will look. It sounds obvious, but it’s amazing how often young people imagine a different projection for themselves. Look at the real people, and you’ll see the honest future for yourself. Be careful not to discover a career before you’ve discovered yourself. This all presupposes we’re starting from a point where considering these questions is a real possibility. Sure, pure objectivity is impossible; after all, society itself also defines the context of our thoughts, and by now it’s way too late to effectively remove ourselves from that. So how can we evaluate what experiences we want for ourselves, when it is experiences themselves which transform our very desires? At the moment when young people are considering their career strategy, they have typically made all of their life choices completely within supporting structures. Even having worked hard to get where they are, and even though things like class and race can mean that some have the cards stacked against them, it’s rare for young people to have substantially departed from supporting frameworks. Highschools have “college counselors” (not “dropout counselors”), scholarships and financial aid packages lead in a single direction, and university overlaps with internships — which then culminates largely in a series of “career fairs.” There is a tremendous amount of support for these decisions, and very little support for making any deviating choices. When we arrive at the ends of these funnels, it’s possible that the direction we’re facing is more a reflection of those structures than it is a reflection of ourselves. Self-determination in a moment like that can’t simply be about making a choice, it has to start with transforming the conditions that constitute our choices. It requires challenging the “self” in “self-determination” by stepping as far outside of those supporting structures as possible, for as long as possible. This is necessarily terrifying. I think a lot about a quote from Alfredo Bonanno, an anarchist and habitual bank robber, on the feeling of leaving prison: The instant you get out of prison you have the sensation that you are leaving something dear to you. Why? Because you know that you are leaving a part of your life inside, because you spent some of your life there which, even if it was under terrible conditions, is still a part of you. And even if you lived it badly and suffered horribly, which is not always the case, it is always better than the nothing that your life is reduced to the moment it disappears. – Alfredo Bonanno, “Locked Up” I know that the most significant and meaningful periods of my life have all been moments that I could have never rationally chosen or even known as possibilities had I not been foolish or lucky enough to step into the nothingness that Alfredo Bonanno writes about. I try to remind myself that if leaving prison is scary, the same is likely true for any genuine process of discovery. There’s no rush to get started early on a never-ending task. Everything before a career has defined beginnings and endings. Elementary school, middle school, junior high, highschool, university. There’s always been a predefined end, and that contributes a lot to making the indignities of those institutions bearable. Once you start working full time, though, it’s just One Long Semester that you’re expected to attend for the rest of your life. So consider caution if you’re overly excited to start down that road as quickly as you can. Other than being forever, it’s not as different from what’s come before as you might imagine. By way of an explanation. This is all just to explain why, when people write me for career advice, I’m as likely to respond with something like “if I were you, I’d hitchhike to Alaska this summer instead.” My career advice usually falls within the framework of doing the absolute minimum amount of work necessary to prevent starvation, and then doing something that’s not about money, completely outside of supporting structures, and not simply a matter of “consuming experience” with the remaining available time. Learn three chords, start a band, and go on tour. Ride a bicycle as far as you can. Go WWOOFing or start a community garden in your neighborhood. Put together a traveling puppet show. Build a drone to engage the emerging drones controlled by domestic police. Do whatever — but make it uncomfortable (like leaving prison!) and make it count. Stay in touch, © 2012 Moxie Marlinspike",
    "commentLink": "https://news.ycombinator.com/item?id=37759873",
    "commentBody": "Career Advice (2013)Hacker NewspastloginCareer Advice (2013) (moxie.org) 191 points by hypertexthero 8 hours ago| hidepastfavorite107 comments farmeroy 5 hours agoI spent my twenties riding freight trains across the US and cycling across Europe and Asia, living off of music. I&#x27;m not sure I would recommend that to anyone who is looking for career advice - Now I&#x27;m trying to build a solid career for myself as a software developer and I have no idea if it is easier with all that behind me or not. While I&#x27;m hoping that the soft skills and self knowledge I may have acquired over the years pay off on this new path, there is no guarantee. But there generally isn&#x27;t. I certainly agree with this interpretation of Treyf and I&#x27;m glad that I have at least Moxie to look up to as freight hopping software engineer who has tried avoiding a lot of the same futures that I have, but it certainly is a path less traveled. reply rubicon33 16 minutes agoparentI wouldn’t necessarily expect that you gained some discrete skills that will help you in software. Probably what you gained is something more valuable: perspective, and peace.Now when you are 45 or 50, rather than a crushing feeling of having wasted your life behind a desk you will hopefully feel happy that you spent your good years living life. reply dumpsterdiver 5 minutes agorootparent45? Wha... these are still our good years! reply hyperthesis 3 hours agoparentprevI think if you&#x27;re engaged with what you do, you will learn and develop more than if not.OTOH there seems to be a developmental window for some specific skills, e.g. phoneme acquisition 6-12 months; (human) language acquisitionFew people regret not working more.For what it&#x27;s worth, working more would build a habit of doing so. A good work ethic would mean that you need a bit less motivation or discipline to get things done, due to it basically being just something you do.It doesn&#x27;t have to be just for some corporation, but also when you&#x27;re trying to build some personal project, especially the ones that might get a bit bigger (writing your own blog engine or static site generator, maybe even a game engine, or planting crops or building a shed for all I care). Sometimes there won&#x27;t be many shortcuts to success, but just boring slog of legwork that needs to be done.In that regard, I definitely regret not working more, because I still need to rely on motivation, which is fleeting, or discipline, which is unpleasant, all just to get through things sometimes, even when I take care of myself in every other way (sleep schedule, nutrition, activities, mental health).As for the whole retirement aspect, sadly I don&#x27;t have answers for that, the state of the economy is concerning sometimes. reply badpun 16 minutes agorootparentprev> Almost everyone regrets not having achieved more.Depends on the culture. This my be true in the US, but most other cultures aren&#x27;t as obsessed with achieving. reply moffkalast 1 hour agorootparentprevEh you gotta curb your expectations with 8 billion people alive trying to out-achieve you every single moment. reply achenet 1 hour agorootparentit&#x27;s not a zero sum game or a competition though.You can make beautiful music, and someone else can create a beautiful choreography for it... their achievement will probably increase your joy, not decrease it. reply moffkalast 45 minutes agorootparentTo some degree yes, but after a while there are so many creators that it&#x27;s just noise and nobody can really stand out unless they&#x27;re truly beyond exceptional. So in that sense it is more of a zero sum game in practice.Take Steam for example. After Greenlight was superseded by Direct and you now only need $100 to get on the platform it&#x27;s practically drowning in games, making for near zero discoverability.But especially in terms of idk, government jobs or certain positions at a specific company it&#x27;s usually a fixed number of seats that will be filled from a large pool of people, making it a completely zero sum game. There can only be one president of a country at a time and a fixed number of them during your lifespan. What are you gonna do, make a new country? reply danielmarkbruce 4 hours agorootparentprevfwiw i think more people struggle with this tradeoff than it seems, and more people question the \"nobody regrets... \" quote. You are far from alone. reply Hendrikto 1 hour agoparentprev> Would be nice to be married now.While the idea and ideal are nice, it often does not work out that way in practice. Lots and lots of unhappy marriages and divorces. reply hypertexthero 8 hours agoprev> As a young person, though, I think the best thing you can do is to ignore all of that and simply observe the older people working there.> They are the future you. Do not think that you will be substantially different. Look carefully at how they spend their time at work and outside of work, because this is also almost certainly how your life will look. It sounds obvious, but it’s amazing how often young people imagine a different projection for themselves.> Look at the real people, and you’ll see the honest future for yourself.…> This is all just to explain why, when people write me for career advice, I’m as likely to respond with something like “if I were you, I’d hitchhike to Alaska this summer instead.”> My career advice usually falls within the framework of doing the absolute minimum amount of work necessary to prevent starvation, and then doing something that’s not about money, completely outside of supporting structures, and not simply a matter of “consuming experience” with the remaining available time. reply bradley13 1 hour agoparentHonestly, that seems really naive. While you probably shouldn&#x27;t work a job you hate, the fact is that money enables a lot of positive life choices. If you work the \"absolute minimum\", you are going to be stressing about a roof over your head, about where your next meal is coming from, about how to pay those unexpected medical expenses, or whatever.Find a job you like that pays decently. Use the money to live a good life. Instead of hitchhiking to Alaska, you can take a plane. reply obruchez 1 hour agorootparentI&#x27;ve read \"Die With Zero\" by Bill Perkins recently.I think Perkins would say in that case that there&#x27;s a time for everything. At 20-25, it&#x27;s maybe a good idea to hitchhike to Alaska. At 45, married, with children and a mortgage, it&#x27;s probably a bad idea. reply hiAndrewQuinn 5 hours agoprevPretty good combined life+career advice I heard a long time ago and followed: If you&#x27;re going to move countries, the best time to do it is right after college.Starting from scratch in an entirely new country, with no connections, no professional ability to speak the native language, and rapidly dwindling cash reserves, was one of the scariest and most thrilling things I&#x27;ve ever done in my life. I recommend it to no one, because if you&#x27;re crazy or motivated enough to actually do it, no man&#x27;s recommendation for or against it is going to stop you. You will become some kind of minor force of nature for a while and you will look back on that time years later and think \"I had no idea I could be that hardcore.\"Sadly this is necessarily the opposite of most of Moxie&#x27;s sentiments. \"Throw yourself into the toughest situations you can find\" is shared between us, but Moxie recs doing so amidst friends and comrades, while I merely remind you can hurl yourself into the ravine to learn to fly. reply bjornsing 14 minutes agoparent> If you&#x27;re going to move countries, the best time to do it is right after college.Perhaps this should be reconsidered in light of remote work? I’m 45 and thinking about moving to another country, mostly for lower cost of living and lower taxes (I’m Swedish). I’m basically a remote freelancer, so the step doesn’t feel huge. reply ImPleadThe5th 1 hour agoparentprevI&#x27;m currently doing it right now (living abroad, working for a startup). I worked back home for a year and a half after college,but once covid cleared up I immediately started applying abroad. Its a great opportunity because not only can one still develop their career, they get to deep dive a new culture and every day feels like a vacation!For me, I actually think it was not too hard (I think it greatly depends on where you move, your appetite for challenge and how much traveling you&#x27;ve done before). The process was confusing and long, but nothing a couple of trips to a consulate and a lot of reading couldn&#x27;t help with.I was lucky enough to have some friends that live in the country I moved to (albeit quite far away). I felt the hardest part was the first 3 months, it was lonely and some days I was bedridden with homesickness. But then the magic happens. You meet new people, you find your footing, you make lifelong memories.After that life just feels like normal life but turbo charged by all the new experiences.The new hard part is deciding if I want to come back or not, I&#x27;m having the time of my life out here! reply CalRobert 5 hours agoparentprevI did it myself, and right after college. It worked out OK, and I know now that making friends in your 20&#x27;s is 10 times easier in your 30&#x27;s, and in your 30&#x27;s is ten times easier than in your 40&#x27;s (though if you have kids in school that can help a little as you meet other kids&#x27; parents)But why not do it _in_ college? reply hiAndrewQuinn 4 hours agorootparentIt&#x27;s pretty hard to move between countries when you&#x27;re in college. The credit transfer situation is pretty tough.People do it, though. That&#x27;s why most of the foreign students at any given college beat the pants off of the locals. I speak from experience :D reply j7ake 3 hours agorootparentOne can do an foreign exchange year during university… reply jezzamon 5 hours agoparentprevHaving done it myself, I would recommend it!Though I suppose it speaks to my personality type that I didn&#x27;t find moving country and leaving everything that scary. I was moving to a job, so didn&#x27;t have the dwindling cash reserves part. reply FlyingSnake 5 hours agoparentprevI did that (twice!) and honestly it was the riskiest and most rewarding experiences of my life. reply Swizec 5 hours agoparentprevI moved countries soon after college with not even a job lined up. From Slovenia straight into SFBA because if you&#x27;re gonna do something that hard, you might as well go all the way.To anyone asking if they should do the same, I say: Don&#x27;t do it.Because if you have to ask, the answer is no. The ones who succeed don&#x27;t ask for permission, they ask for tips about how to get it done. The mind is already made up.Same is true for starting a business. \"Should I start a business&#x2F;move countries&#x2F;other high variance thing?\" --> no. \"How do I overcome ?\" --> advice time!edit:For the downvoters -> this was, for me, the best thing I&#x27;ve ever done. But you really do need a lot of internal resolve to make it work. To give you an example of an unexpected difficulty: I used to practice facial expressions in the mirror because Americans had trouble reading my emotes. Like practicing your accent away, but for your face. reply hiAndrewQuinn 4 hours agorootparentExactly right. It&#x27;s not even close to, say, becoming a US Marine, but they&#x27;re both crazy enough moves that you can&#x27;t just reason yourself into them. You have to be steeled.Godspeed, and I hope Slovenia continues its upward trajectory as well. reply moffkalast 1 hour agorootparentprevAs a fellow Slovenian who&#x27;s also worked with Americans, the last part is both hilarious and real. It&#x27;s just such a weirdly large cultural difference in general behaviour that&#x27;s hard to explain. I would personally find it near impossible to adapt to that always-fake-cheerful attitude that&#x27;s common on the west coast. reply cpach 4 hours agorootparentprevHow did you manage to get a work permit? reply Swizec 3 hours agorootparentHere&#x27;s the story with links to previous steps: https:&#x2F;&#x2F;swizec.com&#x2F;blog&#x2F;how-i-used-indie-hacking-to-sponsor-... reply artiscode 6 hours agoprev> Be careful not to discover a career before you’ve discovered yourselfI feel it&#x27;s possible to re-discover yourself more than once throughout your life. Right after finishing high school I thought that writing code is the real me. Now more than a decade has passed and I&#x27;m trying to find myself again. Our jobs define us, but we&#x27;re in charge of how far we&#x27;re willing to take it. There are engineers who are genuinely passionate about writing code and will happily do so outside of working hours, for free, with no intent for recognition or clout. And then there&#x27;s me and the rest of average Joe&#x27;s - sure our job has defined us, but it hasn&#x27;t consumed us. Be careful about your career choices consuming you. reply kstenerud 6 hours agoprevNo. What the experiment showed is that you can be TEMPORARILY coerced into certain behaviors in abusive environments (especially by authority figures - as was exemplified in the Milgram experiment), but it won&#x27;t change who you actually are.The SPE is really a study in toxic workplaces and abusive relationships.I really wish people would stop quoting the SPE without researching what actually happened and why (and the fact that it was tainted due to Zimbardo&#x27;s direct involvement and direction in order to get the results he wanted).Don&#x27;t worry: You are NOT your job, and your job isn&#x27;t going to change you forever. This is just a scare piece.The only thing you should keep in mind is: If it FEELS wrong to you, go with your gut, because sticking around will be traumatic. reply helloplanets 3 hours agoparentStanford prison experiment has very much been debunked, it does not reflect at all what would naturally happen in such a situation.> These new criticisms include the biased and incomplete collection of data, the extent to which the SPE drew on a prison experiment devised and conducted by students in one of Zimbardo&#x27;s classes 3 months earlier, the fact that the guards received precise instructions regarding the treatment of the prisoners, the fact that the guards were not told they were subjects, and the fact that participants were almost never completely immersed by the situation. [0]> Zimbardo and his assistants ordered the guards to become cruel, and, moreover, that guards and prisoners alike knew what Zimbardo was trying to prove and were eager to help. [1]> In other words, the sadism of the guards and the submissiveness of the prisoners in the experiment didn’t reveal the hidden darkness of the human soul, so much as the willingness of college students to please a professor. [1][0]: https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;31380664&#x2F; [1]: https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;beware-the-epiphany-industrial-c... reply ceceron 30 minutes agorootparentThat&#x27;s why I am so often disgusted with the psychology research ethics. So many experiments are impossible to reproduce (vide recent reproducibility crisis), some (like the Stanford Prison Experiment) are debunked, but it is already too late — the damage is already done. As soon as a controversial research thesis reaches the public opinion (Zimbardo made sure his thesis would become popular), it becomes part of the \"folk wisdom\" and is repeated as a holy truth and a self-fulfilling prophecy. reply saberience 4 hours agoparentprev> Don&#x27;t worry: You are NOT your job, and your job isn&#x27;t going to change you forever. This is just a scare piece.Jobs definitely can and do change people though. For a while I worked for a financial company (and I had previously worked in videogames) and I definitely became a different person there, one whom I didn&#x27;t like. I ended up leaving because I could see myself changing over time and it made me uncomfortable when looking at the \"lifers\" in the company and understanding that was my future. reply Atrine 6 hours agoparentprevThis isn&#x27;t what I took away from it. It&#x27;s basically your environment really dictates a lot of who you are and what you become. From my experience, this is absolutely the case. reply kstenerud 5 hours agorootparentYour environment during your early formative years, yes. By the time you&#x27;re in your mid-late teens, your moral reasoning is already pretty much solidified (and really it&#x27;s more about clan loyalty than morality at that point - although we often conflate the two when it comes to inter-clan conflict). reply matt_daemon 5 hours agorootparentMaybe in an ideal world, but in practically any real world scenario this isn&#x27;t the case. Spend enough time somewhere and the mechanics of that place inevitably influence the way we act, in the workforce and out. reply ipaddr 3 hours agorootparentYou can change the work environment. The ability to influence goes both ways. reply robertlagrant 3 hours agorootparentprevReasoning I think keeps developing with your brain to your mid-20s. reply jxl62 4 hours agoparentprevIf someone gets angry, lashes out, and the following day says sorry I wasn&#x27;t acting like my self the other day.. Why would that be true? Why can&#x27;t it just be that it was a part of you? I don&#x27;t mean when it&#x27;s said casually either.Even without the experiment, it seems odd to me that behaviors and culture you engage with for generally half of your waking hours for several decades has nothing to do with your identity. This posts reads a bit like a Joshua Fluke reaction video. reply yard2010 1 hour agoparentprevNothing lasts forever, and we both know hearts can change reply j7ake 3 hours agoparentprevMost people who go through university and grad school would disagree.They are clearly very different at 18 years old freshman vs 23 graduate, and most would say it is because of their university experience. reply hyperthesis 3 hours agoparentprevMaybe not change you, but become mistaken as to who you are.But who is anyone, anyway? How do you know? Is the old advice is profound or meaningless: Temet Nosce reply AtlasBarfed 5 hours agoparentprevYou don&#x27;t think post college jobs and experiences change you? You don&#x27;t think being a salaryman vs a school teacher versus world travel changes your perspective values and behaviors?But it is healthy to keep this perspective (Tyler durdens assertion) and always try to grow in some way that isn&#x27;t involved with money reply edu 4 hours agorootparentI agree with both. Your job is a pretty big part of your life and, like any experience you have, it shapes you.You’re a complex and constantly evolving human. Your job is a part of you, like it’s your family, friends, education, hobbies, media you consume, ideas, secrets…So, you’re not your job but your job is a big part of you. reply raister 39 minutes agoprev\"...if I were you, I’d hitchike to Alaska this summer instead.\"Read this book first though: \"Into the Wild\", a 1996 non-fiction book written by Jon Krakauer. reply ImPleadThe5th 55 minutes agoprevI love this advice. I think more people need to focus on personal capital and not just career.One of the traps I think this falls into is \"I can either work on my career or be adventurous\". I consider myself a \"work to live\" kind of person but still fall into this trap myself.I believe you can do both, it just takes a lot of effort, planning, and patience (more than a lot of people are willing to put in) to make it work. reply gcanyon 4 hours agoprev> As a young person, though, I think the best thing you can do is to ignore all of that and simply observe the older people working there.> They are the future you. Do not think that you will be substantially different. Look carefully at how they spend their time at work and outside of work, because this is also almost certainly how your life will look. It sounds obvious, but it’s amazing how often young people imagine a different projection for themselves.> Look at the real people, and you’ll see the honest future for yourself.Maybe I&#x27;m the outlier, but this is 100% not true for me. My title, Product Manager, didn&#x27;t exist 30 years ago. I was a developer 20 years ago, and 10 years before that I was managing and supporting a network of 200 Mac Pluses and the like. I&#x27;ve done other things besides, and at no point did the older people where I worked look the way I did years later.I do focus a significant amount of energy on keeping up with the development of technology, and that has changed so much that I couldn&#x27;t help but be different.But I think that applies to everyone working in technology. The developers I work with now are using TypeScript and we organize using Notion. Ten years ago it was PHP and Jira. Ten years before that it was Java and Office docs. Who knows what it will be in ten years, but almost certainly none of those. Two week sprints have been a thing for fifteen years, so maybe that will still be true, but I doubt it.Plan for change. It&#x27;s coming for all of us. reply climatekid 6 hours agoprevThe Stanford prison experiments are pretty well debunked at this point reply foobarqux 6 hours agoparentWere they debunked? I thought that they only had some methodological mistake which made them inconclusive. reply helloplanets 3 hours agorootparent> Zimbardo and his assistants ordered the guards to become cruel, and, moreover, that guards and prisoners alike knew what Zimbardo was trying to prove and were eager to help. [1][1]: https:&#x2F;&#x2F;www.wired.com&#x2F;story&#x2F;beware-the-epiphany-industrial-c... reply caesil 5 hours agorootparentprevI think the main problem is they weren&#x27;t really \"experiments\" in the sense of proving anything meaningful, and serious attempts to do so did not replicate. reply miguelazo 6 hours agoparentprevTrue, but the larger point stands: your job has a huge influence on who you become. And the organization&#x27;s mission and culture also will impact your identity. I have seen a lot of former colleagues make a lot of gross compromises for career advancement over the years. People who wanted to make a positive difference in the world who end up working for weapons manufacturers and the intelligence agency cutouts that create the conditions for our forever wars. reply jppope 5 hours agorootparentStrong disagree. I am reminded of the quote: \"If you don&#x27;t have a plan of your own you&#x27;ll become part of someone else&#x27;s plan.\" Your job has as much of an influence on who you become as you let it have... the contention that you are your job really shows nothing more than a lack of ambition. reply threeseed 6 hours agorootparentprev> create the conditions for our forever warsIf there were no weapons manufacturers or intelligence agencies you would still have wars.Humanity always finds a way to resolve disputes through violence and aggression. reply jatins 4 hours agoprev> They are the future you. Do not think that you will be substantially differentI don&#x27;t remember exactly if it was this post or a similar advice somewhere else but this advice made me leave my job at BigTech back in 2015In hindsight I lost a lot of money because of that :&#x27;) because the company&#x27;s stock 10x&#x27;d at it&#x27;s peak.Overall, I still think it&#x27;s a reasonable advice. The behaviours in these companies are largely driven by incentives and for most people this results in a replicable behaviour (almost like a dog being given treats)Though there will always be outliers.But another thing that struck me as I was reading this was how, based on your state of mind at that point, a simple blog post can make you take significant life decisions. reply fermentation 6 hours agoprevI would love to hitchhike to Alaska, but I’m terrified of what would happen to my career if I did. I worked hard to get a good job, and the situation right now is bad. There are so many people waiting for me to keel over and die because they’re also good engineers who want good jobs. reply grecy 5 hours agoparentI quit my Software engineering job and drove up to Alaska for a whole summer. I continued south after that, all the way to Argentina. It took 2 years, through 17 countries. Paddled with icebergs, poked lava with a stick, climbed a 20,000 foot active volcano, learned Spanish, etc. etc.Adventure I had never dreamed possible, and adventure lifetime is an understatement.When I got back I got the first Software Engineering job I applied for, basically at the same level I was before (junior&#x2F;mid). It was easy to explain the \"gap\" on my resume as working on myself, learning self-reliance, people skills, Spanish, etc. You just have to accept you won&#x27;t climb the ladder as fast as people that stayed, but also you&#x27;ll have two years of massive adventure instead of sitting at a desk every day. reply synergy20 6 hours agoprevdo whatever to make money first, use that money to do what you love.&#x27;follow your dream&#x27; is too risky for most of us and might be misleading for most young people in my opinion.make a living first, and never forget your dream in the process reply tayo42 5 hours agoparentCal Newport has a whole book about this. So good they can&#x27;t ignore you. The idea is what you pointed out, and you&#x27;ll be happier if you just find something to be good at. There&#x27;s more nuance in the book reply jhatemyjob 5 hours agorootparentDitto. I wish I found Cal&#x27;s writings earlier in my life. I followed the line of thinking in OP for 5 years. Since I was following my \"passion\", I wasn&#x27;t making that much money and thus was obsessing over my career day in and day out. Then at one point I decided to sell my soul to big tech. After some time, I realized that I actually don&#x27;t care about my career that much. Ultimately all I want is to increase my hourly rate as much as possible. Making money is a problem to be solved and minimized, not something to be passionate about reply badpun 3 hours agoparentprevAlso, be aware that most people don&#x27;t have a dream or a thing they love. The message about having a dream and \"doing what you love\" is so prevalent in the US culture, that the large number of people whom it does not pertain think that there&#x27;s something wrong with them, and start essentially force themselves to have a passion (which is, obviously, not how it works). reply bad_username 4 hours agoprev> But then, I register who is sitting in those seats. It’s usually almost all predominantly unhealthy looking middle-aged white men, who it is clear from a glance have spent literally hundreds of hours of their lives over the past year in these airplanes.Mentioning people&#x27;s skin color unnecessarily should also be \"treyf\", I think reply pnathan 5 hours agoprevThis is good advice. Looking at the future you _and_ at the idea of taking a leap of faith into the unknown.I am reminded of the Hebrew Lech lecha, which occupies a certain amount of commentary space and has some alignments.There is something to be said for being normie. But also it should be a choice, and moxie points out that it... often isn&#x27;t, really.The spe, btw, I think has gotten much more popularly discredited since 2013. reply hcks 1 hour agoprevThis is all extremely bad advice.To sum it up: work is a long endless grind that will alienate you, therefore therefore left-libertarian flavoured YOLO.Without downplaying the existing structures of social oppression, you may be lucky to live in a country and a time where you have rights.In that case, you need to decide for yourself what you want in life (economically).You need to decide how much money you want, how many hours a week you want to work, how many sabbatical to take and when to retire (these choices can always be made within what the economic environment allows).You can refuse to think about these questions, this how you’ll get trapped working for someone else’s dream. reply badpun 2 minutes agoparentThat&#x27;s exactly right. For example, I decided I prefer to live on $500-$1000 a month to having to work more years. So, I retired as soon as I could afford $1k till the rest of my life. It&#x27;s not a life rich in material goods, but, in my experience, the stuff that brings me the most value is super cheap, and after that it&#x27;s mostly diminishing returns. reply caesil 5 hours agoprevSigh. Bringing up Zimbardo as being insightful about the human condition... I just... I gotta stop reading at that point. reply layla5alive 4 hours agoparentBut... that experiment did show things about the human condition- even if not the things it purported to show... reply abhnv 5 hours agoprevDoes this apply to 30 year olds who&#x27;ve been working for a while and feel lost? reply akaij 3 hours agoparentI would say that is a pretty good time to re-evaluate a lot of things: 20s behind you, slightly matured, more time alone, able to do healthier introspection. At least it was, in my own timeline! I didn&#x27;t begin climbing out of my mess until 30 :) reply JohnMakin 4 hours agoprevThe stanford prison experiment has been thouroughly debunked and nothing should be taken seriously that references it. reply sssilver 5 hours agoprevLoved the piece, but to be frank at the time when I picked software engineering as a career, most older software engineers had had extremely different life trajectories than we do nowadays. reply hcks 1 hour agoparentHow so? reply jasfi 6 hours agoprevMy interpretation when watching Fight Club was that Tyler Durden meant you become your job. But at that point, when you&#x27;re a working adult, you are that job, so you may as well say that you are your job. reply whiddershins 6 hours agoprevI didn’t read the whole article because I stumbled on the Stanford Prison Experiment part.As far as I can tell this experiment was largely debunked, or at best widely questioned and not reproducible.It’s one of those stories that lives on because it’s simultaneously so implausible yet seems to say something believable about human nature.But I suspect it was between 90 and 100% b.s. reply batguano 6 hours agoparentAgree 100%. First of all, this is much more of an “anecdote” or “dinner theater” than an “experiment.“But even more so, when you take a bunch of people and _tell them to play these roles_, why are surprised when they _play the stereotype of those roles?_ reply arduanika 6 hours agoparentprevThat&#x27;s totally fine. You&#x27;re allowed to have your own brown M&Ms. There&#x27;s a lot of thinkpieces out there, and you need some means of deciding which to read. Citing a debunked experiment w&#x2F;o disclaimer is a totally reasonable criteria for you to close one of those hundred tabs.But FWIW, I found the rest of the article pretty good. reply OrderlyTiamat 5 hours agorootparentThank you! I wasn&#x27;t going to read the article for the same reason, but your recommendation changed my mind, and it was actually a good read.It&#x27;s definitely diminished by it&#x27;s non qualified citation of Zimbardo, but there&#x27;s some insight there regardless.Ultimately the point Moxie is making is served by the stanford prison drama play just as well as if it had been an actual experiment. reply graycat 1 hour agoprev(1) What seem to be relatively small efforts, knowledge, etc. can yield big results.(2) Think. Continually try to understand what is going on that has any real chance of affecting you.(3) Get an education, at least a good education at the college level. As silly as this sounds, such an education does provide some discipline in thinking that can be a big advantage. Might apply the advantage just some one afternoon and get big results.(4) For first earnings, will about have to be an employee. So, do that. Live cheaply; save a lot of your money.As an employee, might find that if do some work that is especially good, nearly everyone else in the company will try to get you fired. Some business school courses call this goal subordination.(5) Own something that has a good chance of becoming valuable. One general approach -- real estate. Another approach, a small business that might grow. E.g., at one time could start a simple Web site for romantic matchmaking, as a sole, solo entrepreneur, grow the number of users, and sell out to a large company for $500 million.For owning a business, do start with a collection of the business basics: Maybe (a) Please the customer. (b) Have a barrier to entry, e.g., geographical. (c) Have something new, e.g., technology. (d) Study what successful entrepreneurs have done, not necessarily building Google or Amazon but, maybe, having 10 of the best BBQ restaurants in town. reply foobarqux 6 hours agoprevYou can&#x27;t apply the rule \"look at the future versions of the people who make this choice\" to criticize office drones and then not do the same to \"Alaskan hitchhikers\". It&#x27;s perfectly plausible that most of those people&#x27;s life outcomes are worse than the office drone, bad as that is. Worse, you probably only get to meet the \"best\" of the Alaskan hitchhikers, not a representative sample, many of which are probably in an invisible underclass. reply saloniGquickads 2 hours agoprevThis was extremely useful needed this today! reply throwaway892238 3 hours agoprevThe Stanford Prison Experiment was a fraud, please don&#x27;t listen to rationale based on one unreproduced \"study\".\"The context of one’s life defines not just what but how one thinks, and a job tends to dominate the context of one’s life — particularly when that job is considered to be part of a career. Your job will change you.\"Absolute and complete pseudoscientific bullshit. Everything in life changes you. Every. Thing. Your job, the old man that works at the fruit stand around the corner from your flat, the air you breathe. No one thing in your life is going to determine who you are, because your rational, conscious, decision-making brain is what chooses your path in life. Not some amorphous organization or position that supposedly has control over your cognitive pathways. You change you.Go ahead and chase a career. Will your life be any better or worse than if you didn&#x27;t? There is absolutely no way to know that in advance.You know how you discover yourself? By living your life. By self-reflection, by seeking knowledge, and also by flipping a coin and walking wherever the coin leads.\"Learn three chords, start a band, and go on tour. Ride a bicycle as far as you can. Go WWOOFing or start a community garden in your neighborhood. Put together a traveling puppet show. Build a drone to engage the emerging drones controlled by domestic police. Do whatever — but make it uncomfortable (like leaving prison!) and make it count.\"Or don&#x27;t. It doesn&#x27;t matter. You are going to become who you are going to be, no matter what you do or don&#x27;t do. You don&#x27;t have to be uncomfortable. Nothing you do has to \"count\". It can, if you want. But you don&#x27;t need to live some noble life. Your life doesn&#x27;t have to have meaning. You don&#x27;t have to seek new experiences and become some dynamic, interesting renaissance man. It&#x27;s your life.Here&#x27;s my career advice:If you want a career, keep in mind that a career is basically a marathon, whereupon the end goal of the race is to have a high position, good pay, the respect and reference of your peers, interesting work, personal fulfillment, and a fat retirement account. If that&#x27;s what you want, then most of your actions should be centered around becoming likeable, knowledgeable, useful, and constantly preparing for the next step on the path. If you do it really well, you can climb very fast, but that might work against you. If you do it poorly, you can end up not very far, without much money, or references, or prospects, when you need them the most. Careers are largely a political consideration.That said, even with this weird political focus around work, you can still have a personal life that revolves around whatever you want outside of work. You also have a very wide latitude of what kind of work you do, where you do it, who you do it for, what its results are, how long and how hard you do it, etc. There is a very, very wide spectrum of both career, and life, and you don&#x27;t have to fit into anyone&#x27;s mold. You can even give birth to your own career niche.The point of this comment, just like the point of Moxie&#x27;s post, is to tell you that you have options. You can expand your horizons and do all kinds of things that you aren&#x27;t aware of yet. But also know that, no matter where you go or what you do, you are still just who you are. Life doesn&#x27;t get better or worse based on what you do. It&#x27;s your state of mind that makes life worth living. You are not your job. You are yourself. reply j7ake 2 hours agoparentThis is very succinct while still provides a rich solution to a complex problem. Bravo. reply kwar13 6 hours agoprevThat was amazing to read.> Be careful not to discover a career before you’ve discovered yourself.Words to live by.> There is a tremendous amount of support for these decisions, and very little support for making any deviating choices. reply aranchelk 5 hours agoprev> Tyler Durden was wrong, you are your job.So cringe. reply monological 5 hours agoparentIt’s ok to be cringe. Try expressing yourself sometime instead of always conforming. reply aranchelk 3 hours agorootparent> Try expressing yourselfOkay. The reason I find that heading painful to read is it seems to miss the entire point of the quote. The line is said in the context of a world where everyone already is defining themselves by their jobs, possessions, etc. it’s not descriptive, it’s prescriptive. So it can’t really even be wrong.My memory of the book is it’s fairly philosophical, getting at modernity and the human condition. It’s quite traditionally masculine and violent and I get that that turns a lot of people off. Love it or hate it, quotes of it probably don’t belong in a LinkedIn-style “here’s some advice kids” article.Moreover, it’s just too easy — low hanging fruit, e.g.Tyler Durden said “once you you lose everything you’re free to do anything” but that’s not right because many activities are expensive and you’ll need money.Yuck. reply somsak2 4 hours agorootparentprevIs commenting on HN not a form of self-expression? Looks like the poster has a pretty extensive history of comments and posts. reply drewfis 4 hours agorootparentprevAgreed - you have a good perspective in a sea of internet pessimism. reply haltist 5 hours agoprevLots of good insights and quotes. reply monero-xmr 6 hours agoprevMy advice:- If you don&#x27;t come from money, and your personality desires security and safety, don&#x27;t do risky stuff like wasting your 20&#x27;s hitchhiking and being a ski bum. You should get skills and a career. I came from lower-middle class and have always had the nagging fear of being destitute. I am so much happier that I worked hard in my 20s (80 hours a week hard) so my 30s and 40s were more comfortable.- Your habits chart your trajectory. If you start working out 3x a week, you will continue doing this and be physically fit. If you drink alcohol 3x a week, you will continue to do this and become wrinkled and grumpy. If you get fired from a job 3x a year, you will continue to have this happen, no matter what your excuse is about why it isn&#x27;t your fault you got fired. Try and work on good habits.- You are the company you keep. Cut out negative people who always are complaining and blaming others. Focus on wholesome, happy, well-rounded, good people who have goals that align with yours.- Your best advocate is yourself. How often do you go out of your way to advocate for other individuals? Don&#x27;t be surprised when no one is putting in the extra effort to help you. Don&#x27;t be a victim - your future is in your hands - don&#x27;t blame others or society for your own failings.- Family, friends, health, purpose are all that truly matter.- No matter what your predicament is, someone has it worse, and someone has it better. You can always be valuable to society. Don&#x27;t let your disabilities and frailties stop you from finding a purpose and way to benefit society. reply drewfis 4 hours agoparent> If you start working out 3x a week, you will continue doing this and be physically fit. If you drink alcohol 3x a week, you will continue to do this and become wrinkled and grumpy.What about working out 3x a week and drinking alcohol 3x a week? Potentially that could be a sweet spot for sociability and activity. Compared to doing neither 3x a week, what do you think would be better?Both drinking and exercise mean different things to different people. Running a 10k 3x a week with a glass of wine after would be fine. Blacking out 3x a week off cheap vodka while riding the stationary bike would be a different story. reply badpun 3 hours agorootparentIs the alcohol really necessary? Any social relation that requires alcohol to continue does seem low value. reply achenet 17 minutes agorootparentmaybe not alcohol, but many of my closet friendships, which are probably the greatest sources of joy in my life (maybe romantic relationships come close), were formed around shared intoxication - weed, LSD, 2C-B, MDMA, Ketamine...now obviously the relation has moved beyond that, we&#x27;re very happy hanging out sober, and generally spend the vast majority of our time together sober, but getting intoxicated with someone is actually a good bonding mechanism. reply kaffekaka 2 hours agorootparentprevYou can absolutely drink alcohol by yourself. reply michaelguan666 6 hours agoprevMoxie, this is an incredibly profound perspective on careers and the very essence of life decisions. I wholeheartedly believe that growth often happens in the uncomfortable zones. Thanks for the enlightening read. reply throwaway290 5 hours agoprevSome good points, especially around being influenced by what you do and people around. If a bit simplistic.But I feel like he&#x27;s putting unnecessary pressure by treating first full-time job as \"forever\". Quitting is a thing... reply grecy 5 hours agoprev> “if I were you, I’d hitchhike to Alaska this summer instead.”I agree 100%, best advice there is.After a couple of years working as a Software Engineer, I sold all my stuff and drove my little Jeep up to Alaska to a whole summer of camping, hiking and exploring. I hiked into the Magic Bus (of Into the Wild Fame), paddled with icebergs, and saw more bears, moose, bald eagles, salmon and mountains than I can count.It changed my life, and I&#x27;m extremely, extremely happy I walked away from the golden handcuffs and go away from that desk. Getting perspective and learning how I wanted to spend my life BEFORE I got saddled with a car loan or mortgage was paramount. reply barbs 4 hours agoparentThat sounds fantastic! What are you up to now? reply imchillyb 5 hours agoprevPersonality defines, shapes, and limits personal choice.Those choices are further defined, shaped, and limited by experiences.Companies, organizations, groups and people further refine a personality.Our choices are arrayed by self, formed by both experience and circumstance and then refined by contact with others.We become our work because our personality led us to that original choice. Life then shaped that choice. And, finally, coworkers refine us.We’re molded by this process, fitting ourselves into the form with extreme prejudice. reply mberning 6 hours agoprev [–] It is career advice. Whether it is pretty good is debatable. To suggest that you look at people slightly ahead of you and use that as some kind of magic mirror is just so tragically flawed.First, it supposes that the you of now, inexperienced as you are, can properly evaluate not only what is going on in these older coworkers lives, but that you will also properly appreciate how your own tastes will change between now and then.Secondly, you are looking at pure survivorship bias. You aren&#x27;t going to see the folks that did a 5 year turn doing development and then found some other direction to take their career in. If you are a new hire and look at the 50 year old dev on the team and assume your career might go the same you are taking a very narrow vision of the future. reply quickthrower2 1 hour agoparent [–] I always found late 40s plus developers very nice people in general. Maybe the selection bias of those who didn’t become managers or leave the profession. I agree with you though. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author advises young individuals to ponder carefully on their career choices, insisting jobs can significantly shape their personalities.",
      "They propose observing older professionals in prospective fields can offer valuable insights into one's potential future.",
      "The piece stresses on self-discovery and the need to challenge societal norms when making career decisions, promoting the pursuit of unconventional experiences and valuing personal satisfaction over monetary benefits."
    ],
    "commentSummary": [
      "The author describes their transition from music to software development, emphasizing the merits of gaining perspective, learning from older colleagues, and maintaining a work-life balance.",
      "They question the extent to which environment and job affect identity, citing the Stanford Prison Experiment, and underline the importance of pursuing personal passions and maintaining critical thinking.",
      "The author stresses making choices aligned with one's values and goals, establishing good habits, and achieving personal fulfillment outside of work; they also consider the impact of lifestyle choices, such as alcohol consumption, on overall health."
    ],
    "points": 190,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1696383449
  },
  {
    "id": 37759366,
    "title": "Daisugi, the Japanese technique of growing trees out of other trees (2020)",
    "originLink": "https://www.openculture.com/2020/10/daisugi.html",
    "originBody": "Online Courses Certificates Degrees & Mini-Degrees Audio Books Movies Podcasts K-12 eBooks Languages Donate Daisugi, the 600-Year-Old Japanese Technique of Growing Trees Out of Other Trees, Creating Perfectly Straight Lumber in Architecture, History, NatureOctober 23rd, 2020 8 Comments 41.3k SHARES Facebook Twitter Reddit Image by Wrath of Gnon We’ve all admired the elegance of Japan’s traditional styles of architecture. Their development required the kind of dedicated craftsmanship that takes generations to cultivate — but also, more practically speaking, no small amount of wood. By the 15th century, Japan already faced a shortage of seedlings, as well as land on which to properly cultivate the trees in the first place. Necessity being the mother of invention, this led to the creation of an ingenious solution: daisugi, the growing of additional trees, in effect, out of existing trees — creating, in other words, a kind of giant bonsai. “Written as 台杉 and literally meaning platform cedar, the technique resulted in a tree that resembled an open palm with multiple trees growing out if it, perfectly vertical,” writes Spoon and Tamago’s Johnny Waldman. “Done right, the technique can prevent deforestation and result in perfectly round and straight timber known as taruki, which are used in the roofs of Japanese teahouses.” These teahouses are still prominent in Kyoto, a city still known for its traditional cultural heritage, and not coincidentally where daisugi first developed. “It’s said that it was Kyoto’s preeminent tea master, Sen-no-rikyu, who demanded perfection in the Kitayama cedar during the 16th century,” writes My Modern Met’s Jessica Stewart. At the time “a form of very straight and stylized sukiya-zukuri architecture was high fashion, but there simply weren’t nearly enough raw materials to build these homes for every noble or samurai who wanted one,” says a thread by Twitter account Wrath of Gnon, which includes these and other photos of daisugi in action. “Hence this clever solution of using bonsai techniques on trees.” Aesthetics aside — as far aside as they ever get in Japan, at any rate — “the lumber produced in this method is 140% as flexible as standard cedar and 200% as dense/strong,” making it “absolutely perfect for rafters and roof timber.” And not only is daisugi‘s product straight, slender, and typhoon-resistant, it’s marveled at around the world 600 years later. Of how many forestry techniques can we say the same? via Spoon and Tamago Related Content: The Art & Philosophy of Bonsai This 392-Year-Old Bonsai Tree Survived the Hiroshima Atomic Blast & Still Flourishes Today: The Power of Resilience The Philosophical Appreciation of Rocks in China & Japan: A Short Introduction to an Ancient Tradition The Secret Language of Trees: A Charming Animated Lesson Explains How Trees Share Information with Each Other The Social Lives of Trees: Science Reveals How Trees Mysteriously Talk to Each Other, Work Together & Form Nurturing Families A Digital Animation Compares the Size of Trees: From the 3-Inch Bonsai, to the 300-Foot Sequoia Based in Seoul, Colin Marshall writes and broadcasts on cities, language, and culture. His projects include the Substack newsletter Books on Cities, the book The Stateless City: a Walk through 21st-Century Los Angeles and the video series The City in Cinema. Follow him on Twitter at @colinmarshall, on Facebook, or on Instagram. 41.3k SHARES Facebook Twitter Reddit by Colin MarshallPermalinkComments (8)Support Open Culture We’re hoping to rely on our loyal readers rather than erratic ads. To support Open Culture’s educational mission, please consider making a donation. We accept PayPal, Venmo (@openculture), Patreon and Crypto! Please find all options here. We thank you! Comments (8) You can skip to the end and leave a response. Pinging is currently not allowed. David Tracey says: October 24, 2020 at 10:55 am Interesting take but I’m not buying. Yes trees can be pruned to affect future growth, but not to change their genetic dispositions to be tall or straight or whatever. You can keep pruning for some ongoing desired effect, as I suspect the trees in these pics have been to remove all branches up to the highest level of the canopy. I suppose that could help in ensuring a smooth and straight trunk. But calling something a “giant bonsai” is a puzzler. Isn’t that another word for “tree”? Also it seemed odd to try to grow trees this tall on the weak support of branches as seen in the top picture. Comparing the two pics made me think all the more we’re being fed some guff. Look at the trunks in the two pics to see what I mean. Anyway, just some random first impressions with zero research so if I’m wrong, sorry. I hope the writer continues to find and report on interesting international tree news — always a worthy topic. Reply sdaf says: October 24, 2020 at 10:53 pm It does seem a little bit too perfect, why would the trees grow straight just because they are on other trees? Reply cm83 says: October 27, 2020 at 10:51 am This require a lot of work. You need to carefully prune them every two years (so you can remove all the knots in the wood) for twenty years. Here’s your giant bonsai Reply Ivo says: October 30, 2020 at 5:56 pm Also known as ‘coppicing’ in the West, a common practise … it’s not that unusual! Reply Tamsin says: August 13, 2021 at 5:27 am As Ivo says this is a standard woodland management technique, called coppicing in the west, which has been practiced for hundreds of years to do exactly this – produce straight usable pole size timber which can be harvested over and over again, on a 5 – 15 year rotation time scale, without having to fell the original tree from which the ‘poles’ grow. In my experience its practiced on broadleaf species including commonly hazel, alder, willow, chestnut, ash, and sometimes oak. It’s fallen out of wide scale use sadly with the rise of destructive industrial scale commercial forestry, but is re-emerging as a good sustainable way of managing woodland. I’ve never seen it done on a conifer and it looks fascinating. There’s no magic to ‘buy’! or anything too ‘perfect’ about it – its just what nature does! And its got nothing to do with altering the trees genetics! Prune an apple tree and see what happens – it throws up water shoots – exactly the same reaction by the tree. Reply Sainath Pawar says: March 27, 2022 at 5:39 am This is great technique . The technique shuuld be impliments in India. Thise type of knowledge should be addded in our school and college syllabus and should implement on it . I am trying my best to implement this technique in my nation INDIA 💯💯 Reply Sylvia says: March 29, 2022 at 2:19 am Your comment tells me that you know little about the history of Japan and their well known competition with nature. If you have seen any Bonsai, that is the work of the Japanese. I have been to commercial orchards and often go for the harvest, a have picked an apple weighing 700gramms and had peaches just short of 1kg Please do more research https://www.openculture.com/2020/10/daisugi.html Reply Ray Theron says: June 13, 2023 at 6:51 pm Considering that 67% of Japan’s land surface is even today covered by forests and plantations, and given that Hinoki, Sugi and red pine naturally tend to grow straight and tall, there is no need for daisugi. Not only is it unnecessarily time consuming when there are MILLIONS of normally growing trees available, but it will not produce nearly enough timber. I lived in rural Japan and close to my village there were virtually impenetrable hinoki forests used by two sawmills in the village, sawmills that had been in operation for many generations. As for the notion that the Japanese are “in competition with Nature”, that’s an idea inimical to the Japanese: they live in HARMONY with nature, not in competition with it. Reply Leave a Reply NAME (REQUIRED) EMAIL (REQUIRED) MESSAGE Essentials 1,700 Free Online Courses 200 Online Certificate Programs 100+ Online Degree & Mini-Degree Programs 1,150 Free Movies 1,000 Free Audio Books 150+ Best Podcasts 800 Free eBooks 200 Free Textbooks 300 Free Language Lessons 150 Free Business Courses Free K-12 Education Get Our Daily Email Support Us We're hoping to rely on loyal readers, rather than erratic ads. Please click the Donate button and support Open Culture. You can use Paypal, Venmo, Patreon, even Crypto! We thank you! Free Courses Art & Art History Astronomy Biology Business Chemistry Classics/Ancient World Computer Science Data Science Economics Engineering Environment History Literature Math Philosophy Physics Political Science Psychology Religion Writing & Journalism All 1500 Free Courses 1000+ MOOCs & Certificate Courses Receive our Daily Email FREE UPDATES! GET OUR DAILY EMAIL Get the best cultural and educational resources on the web curated for you in a daily email. We never spam. Unsubscribe at any time. Click Here to sign up for our newsletter FOLLOW ON SOCIAL MEDIA Free Movies 1150 Free Movies Online Free Film Noir Silent Films Documentaries Martial Arts/Kung Fu Animations Free Hitchcock Films Free Charlie Chaplin Free John Wayne Movies Free Tarkovsky Films Free Dziga Vertov Free Oscar Winners Free Language Lessons Arabic Chinese English French German Italian Russian Spanish All Languages Free eBooks 700 Free eBooks Free Philosophy eBooks The Harvard Classics Philip K. Dick Stories Neil Gaiman Stories David Foster Wallace Stories & Essays Hemingway Stories Great Gatsby & Other Fitzgerald Novels HP Lovecraft Edgar Allan Poe Free Alice Munro Stories Jennifer Egan Stories George Saunders Stories Hunter S. Thompson Essays Joan Didion Essays Gabriel Garcia Marquez Stories David Sedaris Stories Stephen King Chomsky Golden Age Comics Free Books by UC Press Life Changing Books Free Audio Books 700 Free Audio Books Free Audio Books: Fiction Free Audio Books: Poetry Free Audio Books: Non-Fiction Free Textbooks 200 Free Textbooks Free Physics Textbooks Free Computer Science Textbooks Free Math Textbooks K-12 Resources Free Books Free Video Lessons Web Resources by Subject Free Language Lessons Quality YouTube Channels Teacher Resources Test Prep All Free Kids Resources Free Art & Images All Art Images & Books The Met The Getty The Rijksmuseum Smithsonian The Guggenheim The Tate The National Gallery The Whitney LA County Museum Stanford University British Library Google Art Project French Revolution Getty Images Guggenheim Art Books Met Art Books Getty Art Books New York Public Library Maps Museum of New Zealand Street Art Smarthistory Rembrandt Van Gogh Coloring Books Free Music All Bach Organ Works All of Bach 80,000 Classical Music Scores Free Classical Music Live Classical Music 9,000 Grateful Dead Concerts Alan Lomax Blues & Folk Archive Writing Tips Hemingway Fitzgerald Stephen King Ray Bradbury William Zinsser Kurt Vonnegut Toni Morrison Edgar Allan Poe Margaret Atwood David Ogilvy Steinbeck Billy Wilder Archive All posts by date Categories Amazon Kindle Animation Apple Architecture Archives Art Astronomy Audio Books Beat & Tweets Biology Books Business Chemistry Coloring Books Comedy Comics/Cartoons Computer Science Creativity Current Affairs Dance Data Deals Design e-books Economics Education English Language Entrepreneurship Environment Fashion Film Finance Food & Drink Games Gender Google Graduation Speech Harvard Health History How to Learn for Free Internet Archive iPad iPhone Jazz K-12 Language Language Lessons Law Letters Libraries Life Literature Magazines Maps Math Media MIT MOOCs Most Popular Museums Music Nature Neuroscience Online Courses Opera Philosophy Photography Physics Podcasts Poetry Politics Pretty Much Pop Productivity Psychology Radio Random Religion Sci Fi Science Software Sports Stanford Technology TED Talks Television Theatre Travel Twitter UC Berkeley Uncategorized Video – Arts & Culture Video – Politics/Society Video – Science Video Games Web/Tech Wikipedia Writing Yale YouTube Great Lectures Michel Foucault Sun Ra at UC Berkeley Richard Feynman Joseph Campbell Carl Sagan Margaret Atwood Jorge Luis Borges Leonard Bernstein Richard Dawkins Buckminster Fuller Walter Kaufmann on Existentialism Jacques Lacan Roland Barthes Nobel Lectures by Writers Toni Morrison Bertrand Russell Oxford Philosophy Lectures About Us Open Culture scours the web for the best educational media. We find the free courses and audio books you need, the language lessons & educational videos you want, and plenty of enlightenment in between. Advertise With Us Great Recordings T.S. Eliot Reads Waste Land Sylvia Plath - Ariel Joyce Reads Ulysses Joyce - Finnegans Wake Patti Smith Reads Virginia Woolf Albert Einstein Charles Bukowski Bill Murray Hemingway Fitzgerald Reads Shakespeare William Faulkner Flannery O'Connor Tolkien - The Hobbit Allen Ginsberg - Howl W.B Yeats Ezra Pound Dylan Thomas Anne Sexton John Cheever David Foster Wallace Subscribe to our Newsletter Book Lists By Neil deGrasse Tyson Ernest Hemingway F. Scott Fitzgerald Allen Ginsberg Patti Smith Brian Eno Henry Miller Christopher Hitchens Joseph Brodsky W.H. Auden Donald Barthelme Carl Sagan David Bowie Samuel Beckett Art Garfunkel Marilyn Monroe Jorge Luis Borges Picks by Female Creatives Syllabi WH Auden David Foster Wallace Donald Barthelme Allen Ginsberg Zadie Smith & Gary Shteyngart Spike Lee Lynda Barry Junot Diaz Favorite Movies Kubrick Kurosawa's 100 Tarantino Scorsese Tarkovsky David Lynch Werner Herzog Woody Allen Wes Anderson Luis Buñuel Roger Ebert Susan Sontag Scorsese Foreign Films Philosophy Films Archives October 2023 September 2023 August 2023 July 2023 June 2023 May 2023 April 2023 March 2023 February 2023 January 2023 December 2022 November 2022 October 2022 September 2022 August 2022 July 2022 June 2022 May 2022 April 2022 March 2022 February 2022 January 2022 December 2021 November 2021 October 2021 September 2021 August 2021 July 2021 June 2021 May 2021 April 2021 March 2021 February 2021 January 2021 December 2020 November 2020 October 2020 September 2020 August 2020 July 2020 June 2020 May 2020 April 2020 March 2020 February 2020 January 2020 December 2019 November 2019 October 2019 September 2019 August 2019 July 2019 June 2019 May 2019 April 2019 March 2019 February 2019 January 2019 December 2018 November 2018 October 2018 September 2018 August 2018 July 2018 June 2018 May 2018 April 2018 March 2018 February 2018 January 2018 December 2017 November 2017 October 2017 September 2017 August 2017 July 2017 June 2017 May 2017 April 2017 March 2017 February 2017 January 2017 December 2016 November 2016 October 2016 September 2016 August 2016 July 2016 June 2016 May 2016 April 2016 March 2016 February 2016 January 2016 December 2015 November 2015 October 2015 September 2015 August 2015 July 2015 June 2015 May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 2010 January 2010 December 2009 November 2009 October 2009 September 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 2006 November 2006 October 2006 September 2006 Search ©2006-2023 Open Culture, LLC. All rights reserved. Home About Us Advertise with Us Copyright Policy Privacy Policy Terms of Use Bio Audio Books Online Courses MOOCs Movies Languages Textbooks eBooks Open Culture was founded by Dan Colman.",
    "commentLink": "https://news.ycombinator.com/item?id=37759366",
    "commentBody": "Daisugi, the Japanese technique of growing trees out of other trees (2020)Hacker NewspastloginDaisugi, the Japanese technique of growing trees out of other trees (2020) (openculture.com) 184 points by quyleanh 9 hours ago| hidepastfavorite72 comments Retric 9 hours agoLooks like a more labor intensive refinement of the much older techniques of copping and pollarding.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coppicinghttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pollarding reply Cthulhu_ 2 hours agoparentCoppicing is used a lot in my neighbourhood (and in the Netherlands as a whole) on willows close to water banks; I don&#x27;t know if the wood is used for anything, but the trees have deep and sturdy roots that stabilize the soil, without the tree growing to a size where it might fall over in a storm or drop a lot of materials in the water. reply culi 8 hours agoparentprevI don&#x27;t think copping and pollarding are \"older\". They&#x27;re just the European (rough) equivalents reply Retric 7 hours agorootparentCoppicing predates writing, pollarding was written about in 1st century BC so it’s at least that old. Daisugi is mentioned as 600 years old so ~1500 years more recent.Daisugi could in theory be significantly older than 600 years, but it’s origins seems well documented. reply charles_f 5 hours agoparentprevArticle literally says> Daisugi (台杉, where sugi refers to Japanese cedar), is a similar Japanese techniquein its intro reply Retric 1 hour agorootparentSimilar as in related, not similar as in identical.All 3 involve how trees grow back after you cut them but the methods are different. Daisugi involves a lot more effort after the initial cutting process, it’s roughly Bonsai + Pollarding. Though you keep a little more of the tree than with Pollarding. reply benj111 2 hours agoparentprevIs it coppicing or grafting?The title suggests the latter, but the article doesn&#x27;t go into details. reply scoot 8 hours agoparentprevIf it&#x27;s more labor intensive is it a refinement? reply manxman 7 hours agorootparentIt’s interesting you say this. There’s something cultural in Japan that optimises the hell out of something even after it becomes obsolete everywhere else and then cannot let go of that investment. Or perhaps the people involved in the process stop questioning whether other processes may be better and then simply iterate upon the path they are already on?Take the buses: They have the most amazing automated machines for giving you change from banknotes and then collecting&#x2F;calculating the correct fare payment in coins. Similar machines exist in some retailers. Try to pay by card and you won’t travel very far.Much of the rest of the world moved to contactless card payments over a decade agoI sort of sympathise on some levels - the investments they made in these ”refinements” are not trivial. I suspect Japan will stick to its legacy processes. IMHO It will succeed as a niche curiosity that outsiders find interesting as the world passes it by.I have to wonder if such curiosity tourism will ever be enough to cover its economic needs. reply anigbrowl 3 hours agorootparentMuch of the rest of the world moved to contactless card payments over a decade agoWhile I fully agree with your summary, I think there&#x27;s an opportunity benefit as well as an opportunity cost, in that they are not turning the country into a digital panopticon for the citizenry. Considering the more collectivist social ethos (compared to many western countries), Japan is in many ways a privacy-maximizing society. reply mailarchis 3 hours agorootparentprevI don&#x27;t think this is a isolated case for Japan but rather an unintended downside of being a pioneering adopter of an innovative technology wave.For instance, Credit Cards are ubiquitous in the US. In India (and am guessing China too) credit cards were and even nor limited to a higher income niche while mobile payments via QR codes (UPI, Wechat) became more prevalent. And I think from an experience and convenience stand point they are better than Credit Cards.Similarly in legacy banking, a large part of the tech infrastructure am guessing still runs on mainframes as banks were early adopters. Applications are still being written in COBOL.Large Organizations &#x2F; Governments benefit from tech adoption but are also slower and more difficult to migrate a new technology when it appears. reply SenHeng 2 hours agorootparentprevYou&#x27;re not wrong, but this is also the country that invented the Felica standard (NFC-F)[0] because regular NFC the rest of the world was using wasn&#x27;t fast enough for the train station turnstiles.0: https:&#x2F;&#x2F;www.sony.net&#x2F;Products&#x2F;felica&#x2F;NFC&#x2F;relation.html reply zztop44 5 hours agorootparentprevSure Japan has some weird anachronisms but everyone pays for buses and trains using contactless transit cards, just like most rich countries. And lots of people just use Apple Wallet on their phone or watch.Likewise, there are many different competing payment systems in daily use (including Western-style contactless card payments and Chinese-style QR code mobile payment networks). There are so many options that cash registers normally have a little poster next to them informing you which of the payment networks are available (it’s often more than 20).Cash is still more widely used than in day Sweden, China or Australia. But if I were to guess I’d say it’s because there are too many available alternatives (no single dominant network) rather than too few. reply manxman 4 hours agorootparentInternational contactless cards don’t work period - you need SUICA or PASMO for the metro. Buses outside of the cities are cash based.I travelled in Hokkaido, Rishiri, Okinawa, Ishigaki, Iriomote etc. They simply did not take cards at all. Also true on small private train lines such as Kanazawa to Uchinada. reply brutusborn 6 hours agorootparentprevI don&#x27;t know how true it is, but I have heard that many large Japanese companies still make significant use of fax machines. This surprised me because of how advanced Japanese industrial automation is.The only example that I think makes sense is the Japanese intelligence services still using paper records. In this context it is advantageous since paper filing cabinets cannot be hacked remotely. reply legulere 6 hours agorootparentGermany does as well reply fhaldridge7 1 hour agorootparentI lived in another country last year and had to contact the German embassy. They told me to write down my problem and fax it to them reply froh 5 hours agorootparentprevthe fax axis powers. reply kaba0 2 hours agorootparentprev‘The West’ also still heavily uses quite outdated tech for the very basis of their banking systems, so I don’t think it’s unique. Though one might argue that is more of a backwards compatibility thing. reply ariasemi 5 hours agorootparentprevLove your comment but I&#x27;m not entirely convinced that it is necessarily a phenomena unique to Japan culture.I mean, isn&#x27;t it the same as the old saying \"if it ain&#x27;t broke...\"?Using your example and this is just a guess, at the time Japan implemented those automated machines there was probably a big push to make the switch as manual handling of change had become a source of stress.The rest of the world didn&#x27;t make that jump until cards were a thing, product of a similar experience.But by that time in Japan, where the problem was already \"solved\", switching to cards was no longer that big of a jump and so there was no incentive (or at least not enough) to worth the effort in changing technologies. reply manxman 3 hours agorootparentFrom what I saw of the mindset I don’t think it works that way there.In London “oyster” contactless cards replaced cash&#x2F;paper tickets for buses and the tube. It was then incremented to contactless bank payment cards as soon as the banks added such cards. This seemed largely to be an internal systems upgrade leveraging existing infrastructure.Japanese suica&#x2F;pasmo contactless payment cards are more like oyster and need topping up. Their bank payment cards are incompatible with international contactless systems. International contactless cards don’t work locally on many payment terminals.Many retailers that support both local and international cards literally have mutiple contactless terminals. Some manage to take a contactless payment but then print a bit of paper for you to sign (total WTF moment!!).But this post was originally about trees. And what I was getting at was the culture reveres preserving and optimising paths after those paths stop making sense or become obsolete. Some of that is very cool to see but I’m not sure if it’s going to be sustainable given their decline. reply pests 4 hours agorootparentprevThis is slightly similar to how some countries skipped landline phones and jumped straight to mobile networks.It makes sense - landlines are on the way out and require a much bigger initial investment to lay lines. Cell networks only require building towers and their backhaul networks.Countries invest in infrastreucture at differerent times. Time of the ages. reply throwaway290 4 hours agorootparentprevThere are downsides to cards, tracking a card with your name makes surveillance too easy compared to cash. reply fomine3 5 hours agorootparentprevIt was just because local bus operators are in poverty to install new system. Now IC payment like Suica, original card, or Visa are being introduced for local buses. reply _rm 3 hours agorootparentprevYou&#x27;re just so much better and smarter and more knowing and above the Japanese! With a 2 week visit was it? Did you manage to keep your nose out of your anus for any portion of that? reply huytersd 8 hours agorootparentprevDepends on what you’re optimizing for. Daisugi looks more aesthetically pleasing to me. reply Retric 7 hours agorootparentprevIt produces straighter wood and looks less harmful to the tree.So it’s a question of trade offs. reply anigbrowl 3 hours agorootparentprevIf the results are more impressive (which they are, by a long way), then yes, absolutely. reply charles_f 5 hours agoprevA fascinating sight in the Pacific northwest forests, especially close to large cities like Vancouver or Seattle, where the lumber industry decimated old growth forests, is big cedars, spruce and fir growing off huge rotten stumps. They&#x27;re nothing like straight and perfectly round, but it&#x27;s very beautiful reply dclowd9901 3 hours agoparentI thought you were going to talk about the octopus tree and its brethren on the coasts there.https:&#x2F;&#x2F;www.atlasobscura.com&#x2F;places&#x2F;octopus-tree-of-oregon reply turminal 9 hours agoprevI&#x27;m not sure calling this a \"technique\" is entirely accurate. It&#x27;s mostly made possible by a unique genetic mutation that some tree happened to develop. All the trees this can be done with are clones of that tree. reply kaba0 2 hours agoparentCopping and pollarding doesn’t require genetic identity. E.g. you can grow pears from the branches of an apple tree (they still have to be somewhat close genetically, which is true here). reply seszett 1 hour agorootparentWell... growing pears on an apple tree is doable but not really practical, pear and apple trees are generally not graft-compatible and they will need specific interstock. It&#x27;s rarely done.But coppicing, pollarding and daisugi have nothing to do with grafting. It&#x27;s just that daisugi works with only one tree and its rooted cuttings, but no grafting is involved. reply dang 9 hours agoprevRelated:Daisugi, the 600-year-old Japanese technique of growing trees out of other trees - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26941631 - April 2021 (171 comments) reply silcoon 8 hours agoprevTrees growing out of other trees is a natural process that occurs in different environments in a natural way. I saw a lot of examples in rainforests. There are trees that survive on top of other trees, or trees that keep expanding themself with large branches that they become a small tree by itself (ficus tree[1]).[1]: https:&#x2F;&#x2F;www.google.com&#x2F;search?q=ficus+tree+palermo reply tigerlily 34 minutes agoparentIs that like, what naturalists call an epiphyte? I&#x27;ve got a little podocarp growing out of a tree fern in a shady part of my backyard. reply tansan 7 hours agoparentprevIsn&#x27;t this only a type of tree? The way you phrased your first sentence make it seem like a common natural occurrence for all trees. reply needle0 9 hours agoprevHow many \"____, the Japanese ____ of ____\" exist? reply Groxx 7 hours agoparentTsundoku will forever be my favorite: https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;world-44981013 reply ycombinete 3 hours agoprevThe cedar becomes a parasite. reply johng 9 hours agoprevWhat an interesting subject. reply artursapek 9 hours agoprevI feel like one could study Japanese culture for a lifetime and never get bored reply smt88 9 hours agoparentWhat culture is this not true of? Perhaps Japanese culture is more fascinating to you because it&#x27;s so different from yours. reply Tadpole9181 8 hours agorootparentI agree the distance makes it even stronger for the Japanese, but as an American I do feel like we have very little identifiably American culture. We&#x27;re so young, spread out, individualistic, and corporatized. It feels a significant amount of what was positive \"American culture\" in the age of Art Deco died. We left behind our Grand Central Stations for strip malls and fentanyl and huge parking lots around square beige buildings. And what clearly identifiable culture does exist aren&#x27;t always pleasant: constant war (both foreign and domestic), gun culture, our views on labor, conservative christianity, anti-intellectualism, etc.So any nation with centuries of culture and tradition catches our eye as something we crave. Whether that&#x27;s France or Mexico or Japan.That isn&#x27;t to say we don&#x27;t have subcultures nestled around, we certainly do. Nor to say other cultures don&#x27;t have serious negatives &#x2F; fragmentation we ignore from afar, they certainly do. But a lot of people I know all express the same longing for a sense of belonging.But a bias for Japanese culture does exist. It&#x27;s better preserved, heavily juxtaposed to ours, has a close relationship with us, and they spend a lot of money to spread it. reply saalweachter 7 hours agorootparentSo there are some measures by which America will never have a culture like other countries.We&#x27;re a young country, by many standards -- just 250 years, by most standards, four centuries if your really push it, only a century or two for most of our cities.We have always been in a state of change and development -- the Eerie Canal was less than fifty years old when the trans-continental railroad was completed. The Pony Express was put out of business by the telegraph within two years. The shopping mall era was maybe three decades. It&#x27;s hard to find any techniques or traditions or enterprises which last more than a generation or two when everything changes so much.We have always existed in the global context. Take pizza -- it&#x27;s hard to find a more American cuisine than pizza. Our cities have their own rivalries and pride over their takes on it. But it started off as Italian pizza after WWII, and we of course immediately re-exported our takes on pizza through the globe, so you can hardly claim it&#x27;s distinctly American.We&#x27;ve always been a fragmented culture. The breaded pork tenderloin sandwich is on the menu at half of the restaurants in Indiana, but virtually unknown outside of it. Mardi gras is a huge thing in New Orleans, but not New York. There&#x27;s barely anything besides fireworks on the fourth of July you can say is truly done throughout the US.We have plenty of culture of course, from jazz clubs to Shaker furniture to apple orchards and county fairs and quilting bees and all the history and artisans you could ask for; it will just never be the same as the culture that develops over hundreds of years in stagnant isolation. reply Tadpole9181 6 hours agorootparentI appreciate that you can more eloquently communicate what I meant to say. Thanks! reply Cthulhu_ 2 hours agorootparentprevThat&#x27;s because the culture that existed was erased by the colonizers. The fact you consider America and its culture is only 250 years old is a great example of that.Native american culture goes back thousands of years, and it would be commendable to develop an interest in it as a lot of people do with the Japanese. reply kortilla 1 minute agorootparent“Native American culture” only goes back thousands of years if you blend all of them together. At that point there isn’t much of a meaningful difference including the culture that came from the early Spanish colonies.It’s not clear to me one tribe that wiped out a bunch of others to become powerful enough to be dominant for a hundred years is any more deserving of interest over Spain. From a rarity perspective sure, but not because they have any more right to the title of “American culture”. kaba0 1 hour agorootparentprevMost Americans are not related to native americans - why should they consider their culture as their own? It is absolutely fascinating, but a few generations back - say - Irish family would be hard-pressed to feel any belonging to it - the same way any conquerer would feel about the conquered. reply cscurmudgeon 8 hours agorootparentprevAny culture&#x27;s citizen will feel the same as you do about their own culture.Immigrants (like me) to the US adore US culture.Some positive examples:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Americana_(culture) reply Tadpole9181 7 hours agorootparentOh, for sure! My point is just that America has a lot of factors that exacerbate the reasons those feelings happen.Americana is a good example, IMO. It&#x27;s more a stereotype used in country songs idealizing a conservative, rural America; but it&#x27;s not something actually practiced or related to by most Americans. Heck, it&#x27;s often actively opposed! reply cscurmudgeon 7 hours agorootparentEvery culture has positive and negative aspects.Several positive aspects of US culture belong to non conservative areas: NYC, Hollywood, NASA, Silicon Valley, etc. Culture is not just ancient low tech practice.American culture is so globally ubiquitous we don’t even recognize it when we do it.E.g, despite animosity between China&#x2F;US, Chinese citizens are fond of US culture (and vice-versa):https:&#x2F;&#x2F;www.cnn.com&#x2F;2013&#x2F;07&#x2F;04&#x2F;world&#x2F;asia&#x2F;china-jackson-hole...Japanese culture likewise has some negative aspects too: grope issue on trains, monoethnic culture where almost no other races can live as real equals, innovation and risk being actively discouraged, etc. reply Tadpole9181 6 hours agorootparent> Several positive aspects of US culture belong to non conservative areas: NYC, Hollywood...Regional cultures aren&#x27;t national culture, I&#x27;d tuck this alongside subcultures in my original comment.> NASA, Silicon ValleyHave internal business cultures, of which the vast majority of Americans have never participated in.> Culture is not just ancient low tech practice.I didn&#x27;t say it was. I&#x27;m also not saying the US doesn&#x27;t have ANY culture. I&#x27;m just meandering on the topic that cultural fetishism can be appealing to Americans because of how our own culture has grown to be and how immature it still is.Japan and France have American cultural fetishists too.> Japanese culture likewise has some negative aspects tooI acknowledged this in my original comment as well. reply hoseja 4 hours agorootparentprevYou&#x27;ve been successfully demotivated. reply culi 8 hours agorootparentprevIt&#x27;s more fascinating because it&#x27;s more well-preserved. There was a conscious effort to do so reply lizardking 8 hours agorootparentprevSome cultures are more interesting to me than others. This seems to be the rule for most people rather than an exception.Are all cultures equally interesting to you? reply justrealist 8 hours agorootparentprevI&#x27;m not a Japanophile but I think it&#x27;s hard to deny there&#x27;s a crispness to their cultural institutions that other cultures don&#x27;t care as much about preserving at high fidelity.Like, when they have traditional festivals, there&#x27;s a pretty real effort at doing them right? It&#x27;s not that the culture itself is \"better\" but they don&#x27;t generally half-ass their public traditions and that makes it more fun to observe.Like I don&#x27;t want to be mean, but for example American Indian culture in a historical context is fascinating, but it&#x27;s really hard to find groups that put effort into preserving cultural institutions at more than a \"phone it in\" level. And that makes it a lot less fun to go out of your way to see&#x2F;learn. reply Klonoar 2 hours agorootparentThat “crispness” wears off if you actually live in the culture for a bit and realize it’s just a different way of doing things. It’s really not particularly special.I’ve been in festivals for Brazilian culture, Taiwanese, etc and they can be just as fulfilling to experience.The real thing to grapple with is that the tech industry in particular has an almost unhealthy exoticism focus on Japan and it colors and elevates any discussion about it beyond any other culture. Seriously - take a shot every time you see a new Japan-focused article hit the front page of this very site. You won’t be drunk but you’ll finish a bottle faster than you’d think. reply Jensson 1 hour agorootparent> take a shot every time you see a new Japan-focused article hit the front page of this very site. You won’t be drunk but you’ll finish a bottle faster than you’d think.Do it for California and you&#x27;d never stop drinking. Stuff gets posted here based on how relevant it is to the tech community, and Japan being the most developed nation that doesn&#x27;t have European origins makes it different and interesting.But still you see a lot of articles about different European countries, China, India etc, I don&#x27;t think Japan is singled out here its just one of the focus points outside of the American coasts that gets the most attention. reply Tadpole9181 7 hours agorootparentprevTo be fair, Native American culture was the victim of a 200-year genocide, is still actively repressed today for corporate interests, and has virtually no healthy, nostalgic population left to actively champion and revive it.There were Native American cities with the population of London, they had (ephemeral, regional) writing and counting systems, etc. But aside from anthropologists and a few survivors, all of that culture is dead and gone. reply defrost 7 hours agorootparentNot entirely despite best efforts to the contrary.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qTJvpfkRRdA reply justrealist 7 hours agorootparentprevThat&#x27;s true and I don&#x27;t mean to cast blame on how we got here.But it&#x27;s disappointing in practice. My wife and kids are tribal members (technically, albeit at long distance) so we stop by Pow Wows sometimes and I feel like there&#x27;s a lot of missed opportunity. Mostly ends up being a place to resell commercial trinkets (of course with some exceptions). reply Tadpole9181 6 hours agorootparentOh, I apologize if I came across as accusatory! That wasn&#x27;t my intention. reply huytersd 5 hours agorootparentprevIndian culture (from India) is another culture that puts a lot of effort into cultural preservation. reply thedailymail 9 hours agoparentprevI&#x27;ve been in Japan for 29 years and, while I can&#x27;t say I&#x27;ve never been bored, I do find new (or very old) things all the time that keep life interesting. reply smegsicle 9 hours agoparentprevjust wait until you hear about other cultures reply lizardking 8 hours agorootparentI don’t understand the reason for this snark (or the down votes). reply smegsicle 7 hours agorootparentnot sure what down votes, and i know that demanding someone take culture seriously feels like ray liotta in his underwear and a hairnet tearfully insisting &#x27;fear me&#x27;, but i imagine there&#x27;s an even richer heritage much closer to home reply ms2718 3 hours agorootparentYou do know “closer home” is relative right? People, who are not Americans, also frequent the internet and this website. reply lizardking 4 hours agorootparentprevGP comment was being down voted at the time of my reply. There’s no objective measure for richness of a culture, so you can’t guarantee anything of the sort. People are into whatever they are into. reply huytersd 5 hours agorootparentprevDo you also like Japanese culture? reply Kouley24 8 hours agoprev [–] sound interesting replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Daisugi is a Japanese method of growing multiple trees from a single one to yield straight lumber, used primarily in architecture.",
      "Open Culture is an online platform providing a wide array of free educational resources and degree programs, aiming to facilitate wider access to cultural and educational materials."
    ],
    "commentSummary": [
      "The article explores the Japanese technique of Daisugi, contrasting it with European techniques.",
      "It deliberates on the transition to contactless card payments in Japan, its limitations, and the continued use of outdated technology.",
      "The discussion highlights the importance of cultural preservation and the global diversity of internet users."
    ],
    "points": 182,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1696378681
  },
  {
    "id": 37752950,
    "title": "Review: Framework Laptop finally gets an AMD Ryzen config–and it’s pretty good",
    "originLink": "https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/",
    "originBody": "SKIP TO MAIN CONTENT BIZ & IT TECH SCIENCE POLICY CARS GAMING & CULTURE STORE FORUMS SUBSCRIBE SIGN IN A DIFFERENT FRAMEWORK — Review: Framework Laptop finally gets an AMD Ryzen config—and it’s pretty good Battery life is a sticking point, but the speed is generally worth it. ANDREW CUNNINGHAM - 10/3/2023, 7:48 AM This is the Framework Laptop 13. We're using the same pictures as a previous review because it's the exact same laptop. Andrew Cunningham And the same keyboard and trackpad. Andrew Cunningham And the same lid. Andrew Cunningham Previous Slide Next Slide SPECS AT A GLANCE: FRAMEWORK LAPTOP 13 (2023) OS Windows 11 22H2 CPU AMD Ryzen 7 7840U (8-cores) RAM 32GB DDR5-5600 (upgradeable) GPU AMD Radeon 780M (integrated) SSD 1TB Western Digital Black SN770 BATTERY 61 WHr DISPLAY 13.5-inch 2256x1504 non-touchscreen in glossy or matte CONNECTIVITY 4x recessed USB-C ports (2x USB 4, 2x USB 3.2) with customizable \"Expansion Card\" dongles, headphone jack PRICE AS TESTED $1,679 pre-built, $1,523 DIY edition with no OS included The Framework Laptop 13 is back again. My third review of this laptop is probably the one that I (and many Framework-curious PC buyers) have been the most interested to test, as the company has finally added an AMD Ryzen option to the repair-friendly portable. Updates to the Intel version of the Framework Laptop have boosted CPU performance, but its graphics performance has been at a standstill since the Framework Laptop originally hit the scene in mid-2021. Even AMD's latest integrated graphics won't make a thin-and-light laptop a replacement for a gaming PC with dedicated graphics, but a bit more GPU power makes the Framework Laptop that much more versatile, making it easier to play games at reasonable resolutions and settings than it is on Intel's aging Iris Xe graphics hardware. Whether you hopped on the Framework train early and have been waiting for a motherboard that felt like a true all-around upgrade or you've been on the fence about buying your first Framework Laptop, the new Ryzen version makes a good case for itself. If you want to order one, there's currently a backlog—all versions are shipping at an unspecified date in \"Q4.\" Meet the Ryzen-powered Framework 13 Enlarge / The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures. Andrew Cunningham I won't spend a lot of time talking about the design of the Framework Laptop 13 again, except to say that it remains a competent ultraportable, and there's nothing that feels dated or clunky about its design now that didn't already feel a little dated and clunky two years ago (the relatively thick display bezel is the main culprit here). Another laptop in this category we generally like, Lenovo's ThinkPad X1 Carbon, has been using the same basic design for years, so it's not like Framework is in danger of falling behind in a chaotic and fast-paced industry. Advertisement The Ryzen version of the mainboard looks mostly identical to the Intel version, given that it needs to fit in all the same cases with all the same connectors. It dropped directly into the same case I've also used for the Intel versions of the Framework Laptop, and moving from Intel to AMD is as easy as it is in a desktop tower with standard parts. Enlarge / The label on the board is one of the few indicators that you're using an AMD board. Andrew Cunningham FURTHER READING Review: Framework Laptop’s 13th-gen Intel upgrade helps fix its battery problem But it wouldn't be a Ryzen system if there weren't a couple of weird, fiddly things about it! All the Intel Framework Laptops have supported the same specifications for all four ports (USB 4 for the 11th-gen, Thunderbolt 4 for the newer ones), allowing you to install the expansion card modules wherever you want them without worrying about the particulars. The Ryzen laptop supports USB 4 in the rear-left and rear-right ports, USB 3.2 and DisplayPort for the front-right slot, and only USB 3.2 on the front-left slot (all four ports support USB-PD for charging, though). Framework also says the rear ports enter a \"high-power mode\" when USB-A modules are connected to them, which can reduce battery life. So yes, the Framework Laptop's ports are still customizable, and you can still have a lot of flexibility when installing expansion modules. But some modules are better fits for specific ports, and you'll have to be a bit more careful about where you put things if you want the best performance and battery life. Page: 1 2 3 4 Next → READER COMMENTS 86 WITH ANDREW CUNNINGHAM Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called Overdue. Advertisement Channel Ars Technica SITREP: F-16 replacement search a signal of F-35 fail? Footage courtesy of Dvids, Boeing, and The United States Navy. SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube Scott Manley Reacts To His Top 1000 YouTube Comments LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments How Forza's Racing AI Uses Neural Networks To Evolve The F-35's next tech upgrade Fighter Pilot Breaks Down Every Button in an F-15 Cockpit Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments Customizing Mini 4WD Racers For High Speeds On A Small Scale MegaBots: Born to Smash Anything in Their Path First Look: Xbox Adaptive Controller Quantum Computing Expert Explains One Concept in 5 Levels of Difficulty Kids versus 80s tech: Game Boy, Vectrex and a stereo system Expert Explains One Concept in 5 Levels of Difficulty - Blockchain Best wearable tech of 2017 The Moov HR Sweat - heart rate monitor in a headbandArs Technica More videos ← PREVIOUS STORY NEXT STORY → Related Stories by Taboola Sponsored Links Doctor Says Slimming Down After 60 Comes Down To This Dr. Kellyann Heart Surgeon Begs Americans: “Stop Doing This To Your Avocados” Gundry MD The Horrifying Truth About CBD Tommy Chong's CBD Historical Figures Who Lived Long Enough To Be Photographed Past Factory How The Perfect Female Body Looked Like 100 Years Ago (and every decade since) Housediver Costco Workers Reveal Things They Avoid From The Store StreetInsider.com CCPA Notice Today on Ars STORE SUBSCRIBE ABOUT US RSS FEEDS VIEW MOBILE SITE CONTACT US STAFF ADVERTISE WITH US REPRINTS NEWSLETTER SIGNUP Join the Ars Orbital Transmission mailing list to get weekly updates delivered to your inbox. Sign me up → CNMN Collection WIRED Media Group © 2023 Condé Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy. Your California Privacy RightsYour Privacy Choices The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices",
    "commentLink": "https://news.ycombinator.com/item?id=37752950",
    "commentBody": "Review: Framework Laptop finally gets an AMD Ryzen config–and it’s pretty goodHacker NewspastloginReview: Framework Laptop finally gets an AMD Ryzen config–and it’s pretty good (arstechnica.com) 164 points by saltysalt 19 hours ago| hidepastfavorite137 comments dogas 16 hours agoInteresting to see their test indicating battery life is worse than the Intel 13th gen variant. Notebookcheck&#x27;s result[1] was vastly different:> Runtimes are longer than the Intel-based configurations by significant margins due to the lower power consumption levels mentioned above. We&#x27;re able to browse the web for over 3 hours longer on our AMD model versus the Core i7-1370P model when both are set to Balanced mode and 150 nit brightness.[1]: https:&#x2F;&#x2F;www.notebookcheck.net&#x2F;Framework-Laptop-13-5-Ryzen-7-... reply soulnothing 14 hours agoparentThere was a note on Reddit that the ports &#x2F; expansion cards were in the wrong way to maximize power efficiency. USB c in back two, USB a in the front.Link to official comment https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;framework&#x2F;comments&#x2F;16ytxjd&#x2F;comment&#x2F;...Direct to knowledge base https:&#x2F;&#x2F;knowledgebase.frame.work&#x2F;en_us&#x2F;expansion-card-functi... reply invalidator 14 hours agorootparentThat&#x27;s an interesting quirk! I thought they were all just USB-C ports internally. Do you know why this matters? reply mananaysiempre 14 hours agorootparentThey are all USB-C as far as the physical interface goes, but their protocol capabilities are different: the rear ones are USB4 (actually full TB4 AFAIU, except Intel refuses to certify AMD systems), the front ones are USB3.2+DP and USB3.2. The fundamental reason seems to be that a “mobile CPU” is more of a SoC, including built-in USB capability, and (unlike the Intels) the AMD ones only do two USB4 ports, which on the Framework are routed to the rear.This, then, is done through USB4-capable retimers, which turned out to draw noticeable power when a USB-A card with a pulldown is connected and idling; but no better retimers could be substituted. The front USB3.2 ports use different retimers and don’t suffer from this. (Why do the Intel motherboards not have this problem? No idea, but if I had to guess, Intel probably makes the retimers for those and is refusing to let them be used with AMD processors.) reply jychang 13 hours agorootparentThat’s pretty cool information.Also this is the type of stuff where Apple would make sure the end user would not have to worry about it. Charitably, they’d put in the money&#x2F;effort to make sure all 4 USB ports are the same (notably, all USB ports on a macbook pro can charge at 100W, which is not a trivial task); or uncharitably, they’d just say fuck it and ship a laptop with only 2 USB ports. reply GloriousKoji 12 hours agorootparent> Also this is the type of stuff where Apple would make sure the end user would not have to worry about itSome of the earlier macbooks with USB-C had overheating problems if the USB-C power cable was connected to any of the ports on the left side. reply viraptor 12 hours agorootparentNot just overheating, but overheating the chips to the level where they put extra performance stress on the kernel for some weird reasons. Basically I was seeing 70% CPU usage plugged in on the left and 20% usage on the right with the same task. MacOS has many issues like that, the \"Apple cares more about users&#x2F;design\" idea is a bad meme at this point. reply AnthonyMouse 9 hours agorootparentIt has been like this for years. Apple does some things well, and there are reasons to like those things, but their following is overzealous to the point of kneeling before what is, fundamentally, still a for-profit corporation.I feel like business schools have failed this generation of management. They look at Apple and see them do something that customers like, but it has a cost to the company, like giving up the ability to track people. So they conclude that it will be an advantage if they defect and make money from ads.Then they see Apple do something customers hate and get away with it and the lesson they take is that they can get away with it too. But they can&#x27;t, because the reason why Apple got away with it was by doing the thing the customers liked. And then they can&#x27;t understand why they can&#x27;t achieve Apple&#x27;s margins.You idiots, do the opposite. Then you can charge Apple&#x27;s margins from the first one and take their market share with the second one. reply KolenCh 4 hours agorootparentprevIIRC this is not a reflection on actual CPU use. Those extra CPU usage is more like throttling. I don&#x27;t remember where I read this though, worth searching about this if you&#x27;re interested. reply mananaysiempre 12 hours agorootparentprevI mean, yes, Apple (even before they get to making their own silicon) can slam a wad of cash on the table and say “make us a better retimer or else” or perhaps even “make us a better SoC or else”. Not a lot of other laptop manufacturers can. (Now that I’m looking, it seems that Intel and Kandou are basically it when it comes to USB4 retimers, and the Kandou ones—whose deficiencies we’re discussing here—only recently arrived on the market.)That said, I believe all Frameworks will accept 100W down any of the four ports, even if the batteries are only designed to charge at 55 or 61W[1] and the rest will have to be consumed by the rest of the system somehow.[1] https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;how-fast-can-the-battery-be-c... reply Aaron2222 11 hours agorootparent> and the rest will have to be consumed by the rest of the system somehowThe wattage figures on a charger are the maximum it can provide. The connected device only draws what it needs. reply soulnothing 14 hours agorootparentprevUpdated parent comment with links. The back 2 are USB 4 while the front are usb 3.2 is one thing.Last I read, the USB a cards don&#x27;t fully enter suspend in the os. I have an 11th gen and I pop out my USB a to save battery when I don&#x27;t need it. The hdmi also used to have that problem but that&#x27;s since been fixed. reply sva_ 15 hours agoparentprevI wonder what magic gets them more than 12 hours out of a 61Wh battery when the average idle power usage is already 6.5w. reply Leimi 16 hours agoprevFramework is really appealing as thinkpads get more and more soldered everywhere. And pricing is surprisingly ok, especially if you target 64gb of RAM and a big ssd that you buy on your own and install yourself. You can almost buy 3 amd framework with those specs for one macbook pro.Only thing that I&#x27;m afraid of is the build quality of the chassis that doesn&#x27;t really seem on par with premium thinkpads and other business laptops. reply tadfisher 15 hours agoparentSoldered LPDDR5x has more bandwidth, lower latency, and consumes less power than the equivalent DDR5 SODIMM. It can also be clocked higher due to signal integrity constraints, and you really want higher clocks with a Ryzen SOC.If you plan on maxing-out the memory, the only advantage of SODIMMs I see are in repairability. That said, I have verified exactly one bad stick of RAM in over 3 decades of computing, so I personally think the downside of soldered RAM is overblown. reply LeonenTheDK 15 hours agorootparentFor me it goes beyond repair, if I sell the device later and the next user wants more RAM, they have the option to do so. Plus bringing ram forward between mainboard upgrades (save for transitions between DDR4, DDR5, and whatever else may come in the future).I remember now too Dell was working on a new memory module for laptops called CAMM, I think it was aiming to bridge the gap between soldered and SODIMMs, but I&#x27;m not sure where its development currently is, nor the real world differences. reply freedomben 15 hours agorootparentYes, I have carried RAM forward to new machines twice now. It&#x27;s a huge benefit to me and worth the tradeoffs. reply dmm 15 hours agorootparentprev> the only advantage of SODIMMs I see are in repairability.Another advantage is that you don&#x27;t have to buy ram from the OEM. I can buy 64GiB of ddr5 forThat said, I have verified exactly one bad stick of RAM in over 3 decades of computing, so I personally think the downside of soldered RAM is overblown.Man I must&#x27;ve been unlucky. I had a dead DDR stick around 10 years back and LPDDR (soldered on a ThinkPad x1) die on me around 6 years ago. Both of them died very early on though and were covered by warranty. reply m-p-3 12 hours agorootparentI had to send some bad RAM under warranty as well. Another upside with socketed RAM is that I was able to buy a small stick of RAM and get my machine going (although with constrained memory) until I received the new pair, which was a minor inconvenience.Having soldered RAM would have meant shipping the entire system back, then find a spare system and the trouble of setting up all my softwares until then. reply tjoff 15 hours agorootparentprev... and the 10x price difference, the waste and crippled hardware, the pathetic pricing options you get. Soldered ram has no place nor reason to exist in sad reality we live in. reply huijzer 15 hours agorootparent> Soldered ram has no place nor reason to exist in sad reality we live in.Doesn’t it have performance benefits since it can be located closer to the processors? reply prmoustache 4 hours agorootparentBut is that performance benefit worth the non replacability&#x2F;upgradability.Looks to me like laptops are fast enough these days and have been for more than a decade. I have 3 laptops, one from 2011, one from 2017 and one from 2019.Even the oldest one is fine for everything. If I had specific tasks that needed more comoutation speed it would be make more sense to use a remote server than replace it. reply huijzer 3 hours agorootparentThat is not my experience at all. My Intel laptop bought new in 2023 took 40 minutes to compile LLVM, would get extremely hot and loud, and the screen would hang multiple times during the process. This laptop’s fan would even spin when watching YouTube. My current M1 chip takes only 20 minutes without fan or freezing screens. I’ve looked it up and the M1 chip requires only half the clock cycles to get data from memory and I suspect that does play a role in the dramatic difference in performance and energy efficiency. reply tjoff 5 hours agorootparentprevSure, but that is one very small consolation. reply topspin 15 hours agorootparentprevSoldering has benefits, but RAM is likely the most important thing that will need upgrading in extended service of a system. reply IshKebab 2 hours agorootparentprevSurely the main benefit is cost? RAM size (and disk size in Apple&#x27;s case) are used for price differentiation. If you can replace them yourself they can&#x27;t charge you through the nose for bigger sizes.On a technical level I think you&#x27;re right. reply thomastjeffery 15 hours agorootparentprev> I personally think the downside of soldered RAM is overblown.It is now, but only because it wasn&#x27;t before. Narratives are slower to change than their subjects, especially in tech.When soldered RAM started to take off, it was usually 1-2GB, which, even at the time, was a painful compromise. Even a lightweight Linux distro running a browser (i.e ChromeOS) will feel the limits of 4GB.Now, most laptops have at least 8GB, which is good enough for most. 16GB is plenty unless you have some specialized workload that actually uses more, like compiling a large codebase or video editing.64GB on a laptop is absurd. Whatever workload you have that needs that much memory should almost definitely run on a remote server anyway. reply alluro2 6 hours agorootparent8GB is hardly good enough for most, with proliferation of Electron apps everywhere and browsers being such memory hogs.I am easily utilizing all of my 32GB just normally running a browser, email client, Slack, database mgmt tool, a couple of light servers (e.g. NodeJS, Redis for local dev), VS Code, Figma and a Git client. I would definitely get 64GB for my next work laptop, and I don&#x27;t consider myself special in my needs in any way as a developer, nor would anything significantly change if I used remote servers.So your generalization might be a bit off. reply prmoustache 4 hours agorootparentI have yet to see a use case where I need and can&#x27;t replace a shitty electron app with a native one or its web equivalent running in a browser tab or window.I have a laptop with 32GB, the only use case where I was maxing out the memory was when I was running a 6 nodes kubernetes cluster in vagrant virtual boxes.And that was kinda stupid because I could have run those VMs in a remote hypervisor. reply layer8 16 hours agoparentprevThe Framework keyboards are not particularly appealing compared to (earlier) ThinkPads, in terms of layout. No full-height cursor keys or top row, too many keys missing one might want, in particular on Windows. Unfortunately the keyboard cutout is vertically so constrained that a third party also can’t do anything about it. reply deepsun 9 hours agorootparentWell, theoretically, a third-party can also change the whole keyboard cover with touchpad, not just the keyboard. Specs are open. reply panick21_ 14 hours agorootparentprevI would love a configuration without a touch pad but with an added ThinkPad style nibble. reply sohkamyung 11 hours agorootparentMaybe somebody could hack the Lenovo Thinkpad TrackPoint Keyboard on to it. reply diggan 16 hours agoparentprev> Only thing that I&#x27;m afraid of is the build quality of the chassis that doesn&#x27;t really seem on par with premium thinkpads and other business laptops.I guess in theory you would be able to manufacture it yourself, if the specification for the chassi is available publicly (which I guess it should as that&#x27;s their whole shtick?). reply jacoblambda 15 hours agorootparentYou definitely could. All the details are available on the FrameworkComputer github org and you could either use what they have as a basis for a chassis or design your own custom chassis that fits the framework parts.https:&#x2F;&#x2F;github.com&#x2F;FrameworkComputer&#x2F;Framework-Laptop-13 reply ksec 8 hours agoparentprev>Only thing that I&#x27;m afraid of is the build quality of the chassis that doesn&#x27;t really seem on par with premium thinkpads and other business laptops.That is something I \"hope\" they could work on, but also aware this may distract them from other much more important things. In a perfect world you could have a laptop that has the chassis of a MacBook but the portability, repairability or Framework. reply rafaelmn 13 hours agoparentprevI wonder if they eventually branch out into boutique quality stuff - since they should be upgradeable for at least an upgrade cycle it shouldn&#x27;t be that big of a deal to have a case&#x2F;keyboard that&#x27;s 3-4x regular price if the quality&#x2F;durability is there. reply rjh29 16 hours agoparentprevI want one but it&#x27;s not quite there yet for me, still 50% more expensive than a thinkpad (let alone a refurb one, which is arguably better for the environment than buying a framework) and it falls behind on keyboard, trackpad, chassis materials and number of ports. They are doing amazing work but it needs a few more iterations.The chassis is made of much cheaper materials, it does not look cheap but it won&#x27;t withstand the abuse of a thinkpad t series or similar. reply Leimi 15 hours agorootparentI&#x27;m curious how it&#x27;s 50% more expensive than (new) thinkpads. Personally I&#x27;m looking for machines where there is at least 64gb soldered, or the possibility to upgrade later. In the thinkpad line that means basically only the X1 now for 13&#x2F;14 inches laptops. And it&#x27;s not cheaper than the framework.I agree about the rest, a few things are not quite there yet, or maybe will never be. But on lots of things it is really refreshing. reply tadfisher 15 hours agorootparentI&#x27;m typing on a P14s Gen 4 (AMD), which has 64gb soldered and a Ryzen 7840 SOC. It also has an OLED screen, which is hit-or-miss depending on your preferences; I value the high resolution and color accuracy, but it&#x27;s not the best choice for battery life. reply Leimi 15 hours agorootparentman, I was convinced this was 32gb max, great to hear!I have the gen 2 which is the most annoying thinkpad I ever had, but well, it&#x27;s still really great compared to other brands. reply mkozlows 16 hours agorootparentprevI don&#x27;t agree. I&#x27;ve owned a ThinkPad T and currently have a Framework. I&#x27;d give the build quality edge to the Framework, even without the repairability (which is obviously better). reply rjh29 14 hours agorootparentThere is some subjectivity but the Framework is plastic+aluminium, glossy and has flex especially in the lid, and photos online show it warping when dropped.The T14s is made of magnesium. It is completely solid, even the keyboard does not flex when typed on. It feels much more premium imo. reply moneywoes 15 hours agorootparentprevwhat thinkpad models are you looking at? i have a t480 and it seems due for upgrade as the battery is pitiful even after upgrading reply rjh29 14 hours agorootparentimo the T14 AMD (or the T14s if you need usb4 and a slightly nicer looking chassis) are the best value in the thinkpad range. reply anotherhue 16 hours agoparentprevI have one, it&#x27;s pretty good. Not as sleek as the MBPs but perfectly functional. reply ploxiln 15 hours agoprevI pre-ordered this, 2nd batch, and have been awaiting the charge&#x2F;shipping confirmation (which was originally scheduled to be last month, some more delay is understandable, but kinda annoying that they didn&#x27;t communicate more about updated expected shipping window for batches 2+).Browsing their forums, I&#x27;m a little disheartened to see that it appears that the Intel 12th gen model has yet to get a stable firmware update - there was a beta bios released last December, windows installer only, and it often fails, and at that point they seem to have just gotten stuck. I totally understand not being able to fix some things that some demanding users keep yelling about, but not being able to make any stable release with any fixes is ... a bit scary. Not releasing the stand-alone EFI or self-booting firmware-update installer variant first is also a head-scratcher.I really had the opposite impression before I recently started digging into their forums. The Intel 11th gen model seems to have gotten a few good firmware updates. They had this whole blog post about how they updated the 12th gen firmware for full Thunderbolt certification of the usb-c ports. ... but that&#x27;s the update that they can&#x27;t actually get a stable release of, for over 9 months now ...I&#x27;m still rooting for them, and hoping this is just one speed-bump.Links: https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;12th-gen-intel-core-bios-3-06... https:&#x2F;&#x2F;frame.work&#x2F;blog&#x2F;framework-laptops-are-now-thunderbol... reply Talinx 13 hours agoparentThey sent out an e mail an hour ago, batch 2 shipments starting later in October. reply opine-at-random 17 hours agoprevI own a framework, and 3&#x2F;4 of the ports are failing. I shilled this machine hard when I got it, but now I have dubious connections to my ext-monitor and my microphone. Sometimes when I&#x27;m moving files to-from my SSD I have intermittent failures. Also the hinge that it comes with is fucking atrocious.I really wanted to love this $1500 product, but frankly; I&#x27;m just going to resuscitate my thinkpad, and get my work to provide a desktop and use x11 forwarding on it. reply tecleandor 17 hours agoparentOut of curiosity, is that the internal ports or the ones in the usb-c expansion bays?Also, where did you buy your laptop ? I know they found some hinges out of spec mid 2022 and where sending replacements to the users ( https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;explainer-lid-rigidity-hinge-... ) reply red369 11 hours agoparentprevJust another random suggestion which could be astray... Is the port issue you&#x27;re having with the 1TB expansion card, or is the SSD issue which you describe a separate issue, i.e. with the internal SSD?If you&#x27;re having an issue with the 1TB expansion card disconnecting, it could be this: https:&#x2F;&#x2F;guides.frame.work&#x2F;Guide&#x2F;1TB+Expansion+Card+Throttlin...As one extra piece of anecdata, I haven&#x27;t had any issues with the ports either, though I have had the expansion card issue.Just in case anyone is wondering about their support, Frame.work were very good about offering to fix the expansion card issue, and my fan-scraping-noise issue, both for no cost. Unfortunately I am in a country they don&#x27;t sell to yet so they can&#x27;t help me yet. They did say that if they later expand to offer sales here, they would still offer a fix, even though outside of the warranty period. reply 83457 16 hours agoparentprevHave you spoken to support about the issue? Sounds like an uncommon problem and could be linked to a single part failing.The hinge is certainly flimsy. reply mkozlows 16 hours agorootparentThe hinge _was_ light on the old ones (deliberately, they wanted it to be easy to open with one hand), but after a lot of feedback like this, they went with a heavier one. I have a 12th gen and a 13th gen (one is my wife&#x27;s), and the new one has a stiffer hinge that doesn&#x27;t move if you aggressively move the laptop.Because it&#x27;s Framework, you can of course buy the heavier hinge and put it on the older models; I haven&#x27;t, because it&#x27;s not a problem for me, but if it&#x27;s one for you, it&#x27;s a fix you can do cheaply. reply wholesomepotato 16 hours agoparentprevSomething&#x27;s wrong with it. Mine is fine.Swap the ports and see if the problems moved with the module, or stay fixed with a port location. Try on a LiveCD with Linux to rule out the OS. If nothing else - probably MB. reply ploxiln 15 hours agoparentprevYou can replace the ports and the hinge, without too much expense, and this is kinda the defining feature of this product line. Did you try replacing the ports and the hinge? reply Gigachad 13 hours agorootparentThe ports and hinge shouldn’t fail within the lifetime of a laptop. Sure occasionally you’ll get a defect, but the average experience should be that nothing needs replacing other than physically damaged parts from misuse. reply nikodunk 16 hours agoparentprevI own a Framework, have been using it daily for 2+ years, and it&#x27;s been a rock-solid workhorse for me - haven&#x27;t dealt with any of the issues that you describe. Sorry to hear! Have you contacted support? reply pengaru 16 hours agoparentprevThis confuses me, I thought the whole point was the Framework&#x27;s repair-friendly?Why are you living with these defects instead of servicing the thing? reply jjcon 16 hours agoparentprevJust chiming in here as more anecdata - my framework has been totally fine aside from the hinge (but support sent for their new one for free). reply jackmott42 17 hours agoparentprevI own a framework, one of the early ones from launch. All of the ports are working. My SSD has no problems, and the hinge has been fine for us, but they upgraded it after launch so newer ones should be better. reply wierd-eye-loser 16 hours agoparentprevI have a framework. My biggest issue has been the touch pad. reply axiologist 15 hours agorootparentThe reason i have no Framework is the lack of a track point in stead&#x2F;addition to a touchpad that can be turned off by default. Basically i have to keep using Thinkpads until another manufacturer sees the light and includes an optional working track point in their keyboards. reply Leimi 14 hours agorootparentSadly I think this is hopeless. The trackpoint is a thing of the past. Modern thinkpad trackpoints are notably not as good as the ones from 10 years ago anyway. I personally use it less and less on my p14s and surprisingly, it&#x27;s not that annoying. reply Forbo 16 hours agorootparentprevI had to vouch for your comment, I&#x27;m guessing because it&#x27;s a new account? Might be something to keep an eye out for in case it continues happening.Edit: Looking at your comment history, maybe that was a mistake. reply wierd-eye-loser 16 hours agorootparentWhat are you talking about? reply mixmastamyk 15 hours agorootparentShitty combative comments. Read the guidelines and shape up, foo&#x27;. replyClaraForm 16 hours agoparentprevMy ports are fine, something funky is up with yours. All 1st Gen hinges are bad, but the 2nd Gen hinge is almost perfect. All speakers are bad, battery life is bad, the fan is just a jet engine some times. The promise of an upgradable laptop is great, but it&#x27;s less great if it&#x27;s an excuse to ship sub-par default parts. If I have to replace most parts to end up with a laptop that makes me happy, it feels ... like I was cheated of the actual initial promise.That said, I am happy with it. And still, I&#x27;m in for the experiment. If I can get one truly great inter-generational mainboard update while retaining most parts, then the \"build\" will have paid for itself. I&#x27;m on a 12th gen mainboard, hoping my next update will be some time around 2026 or 2027. reply albertgoeswoof 15 hours agoprevAre the s3 sleep issues on Linux fixed with this build? From previous reviews the intel builds were losing 30-40% overnight.I want to be able to shut the lid, and open it up a few days later, with instant start, with only a few % off the battery. My Macbook can do this, but it’s not Linux, and not a framework reply aljgz 16 hours agoprevI&#x27;m using a Framework laptop since November 2021, as my everyday laptop. I don&#x27;t treat my laptop with too much care. It has been working really well. I also have an M1 MBP, and an xps 13. They both feel more solid, but I don&#x27;t care. I hate the UX in macos, and I don&#x27;t like how I&#x27;m basically stuck with my choices of ram, hdd, and ports. My only major problem with the framework laptop was the stupid Intel thermal throttling. If only Intel had decided to apply the throttling gradually as the cpu gets closer to the limits (instead of working full speed and then suddenly drop to 400Mhz or even 200 if it reaches a certain temperature, they could lower the max speed over a range of 2-3 degrees, that would work perfectly well: the cpu would settle on a slightly lower frequency than max, and no one would notice, except benchmarks, which were probably the only thing that Intel cared about).My desktop is based on AMD. I&#x27;m much happier. Even with an Nvidia GPU and Linux, I&#x27;m doing amazingly well. Whenever I really need to get a new laptop, Framework with an AMD cpu is my #1 choice. It does not hurt if they manage to add the external GPU support to the 13\" version by that time :D. That would be the ultimate laptop. I&#x27;ll probably even sell my $6000 desktop. For now, I prefer not to carry a larger laptop. reply yread 16 hours agoprevI don&#x27;t like that the keyboard layout isnt customizable. Not everyone likes not having dedicated pgup pgdn buttons reply dmm 16 hours agoparentYou can use ectool to swap keys around: https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;exploring-the-embedded-contro...Someone changed their keyboard to colemak using this technique: https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;changed-my-keyboard-layout-in... reply stavros 14 hours agorootparentOooh thank you for this! I&#x27;ve been struggling with making my Caps Lock into Ctrl&#x2F;Esc. reply PrimeMcFly 4 hours agorootparentThat&#x27;s a super weird remap. reply stavros 4 hours agorootparentWhy? Plenty of people make it into either Ctrl or Esc, why not both? reply PrimeMcFly 3 hours agorootparentI&#x27;ve never heard that many people modify it to be Ctrl or Esc, so that is also weird to me. reply stavros 49 minutes agorootparentYou still have Ctrl mapped to your pinky? You should change that, that&#x27;s no life. replymixmastamyk 14 hours agorootparentprevInteresting, looks like it is the same guy from the \"dunk on our\" Windows Terminal team. :D reply jerf 16 hours agoparentprevButtons are just buttons. Make them do what you want. The majority of buttons on my keyboard don&#x27;t do what their keycap says they do. reply ar_lan 16 hours agoprevHoly moly. A Ryzen 5 on the 13\" with 1 TB SN850x SSD and 32GB RAM is ~$1250. That is insane to me.If I get laid off anytime soon I&#x27;m ordering this immediately. reply thekombustor 15 hours agoparentI&#x27;m not sure I exactly follow your logic - do you think that price is insane(ly high)? or insane(ly low)? If it&#x27;s high, why would you want to order it when you get laid off? reply ar_lan 15 hours agorootparentIt&#x27;s low compared to Apple (which is the only comparison I&#x27;m making).The cheapest I can find that is comparable is an M1 Macbook Air which, for the same price point, comes with 16gb RAM + 256gb storage, or an M2 Macbook Pro w&#x2F; 8gb RAM.\"Insanely low\" is definitely an exaggeration but in the modern day this just seems pretty reasonable for what appears to be decent specs - plus I just really like Framework&#x27;s model. reply kcb 9 hours agorootparentApple, where NAND flash and RAM costs are stuck in 2013. reply neurostimulant 8 hours agorootparentWhy though? Are they using higher endurance NAND with better controller? reply wmf 7 hours agorootparentNope, Apple just likes profit. reply endorphine 14 hours agorootparentprevNot parent poster but my reason is that I use my job&#x27;s workstation as my personal laptop as well; so as to avoid getting yet another laptop (minimalism and all that). reply timbit42 13 hours agoparentprevThe article says Ryzen 7 7840U. reply ar_lan 13 hours agorootparentI don&#x27;t imagine the Ryzen 7&#x27;s benefits being worth the extra cost, as I don&#x27;t think I really need them. The Ryzen 5 should be fine for what I do. reply jfim 17 hours agoprevFor those that would want a dedicated GPU, the 16 inch model has an expansion bay that allows putting in a Radeon GPU. reply INTPenis 16 hours agoprevVery happy to hear that I have some good options when my current Thinkpad finally retires. But I won&#x27;t replace a perfectly good computer just for the sake of it.Got a Thinkpad L13 with Ryzen 5 pro right now. Cheap, but it does the job. And best of all, no qualcomm wifi chipset. Mediatek, it&#x27;s not intel but it works.All USB-c, even 4G and smartcard slot.Funny but the L13 has pretty much the same cons. Battery life is meh, and fans are noisy when I play games on it.It doesn&#x27;t matter, it&#x27;s enough battery for a 4 hour work session on the go, which is more than I need before I need a break. And when I game I have the sound turned up anyways. reply bhasi 16 hours agoparentWhat&#x27;s the issue with Qualcomm&#x27;s WiFi card? reply INTPenis 10 hours agorootparentLinux driver sucks. I accidentally got one with my last computer and it had weak signal, intermittent disconnecting, bluetooth interference, just overall crap.In general the Intel wifi driver is the best but you never see an Intel wifi card with a Ryzen CPU. So mediatek was the best I could find. reply hamandcheese 16 hours agoprevIt&#x27;s very interesting and surprising that battery life is worse than Intel. I wonder if this will get any better with driver&#x2F;firmware updates? reply Rebelgecko 15 hours agoparentI wouldn&#x27;t count on it. My 12th generation Intel Framework has godawful battery life. Framework has apparently been beta testing a BIOS update since 2022 that will improve it (and nrp told me here on HN that it&#x27;s still being worked on as of 6 months ago)... but it&#x27;s not here yet.In the mean time they&#x27;ve released a ton of new products and locked the 12th Gen BIOS threads on their forum (ironically, after talking about how they&#x27;ll try to do a better job communicating the status of firmware updates).I strongly suggest that if you buy one, you base your decision on the hardware and software as they are today, not on future improvements that may never come. If you&#x27;re not willing to accept warts like poor battery life and nonfunctional brightness keys, you&#x27;re better off getting a laptop that doesn&#x27;t have those issues. reply Leimi 16 hours agoparentprevyeah it&#x27;s surprising, notebookcheck also has benchmarks and the results go in favor of amd. reply moneywoes 14 hours agoparentprevdoes the battery vary on windows vs linux? reply linza 17 hours agoprevNot convinced yet of AMD an Linux for mobile devices. Just sent back a T14s Gen 4 with Ryzen I wait for a long time because of power&#x2F;stability issues (S2idle, sleep+wakeup problems, GPU, WiFi, ...).I really hope AMD gets quality on par with Intel in this space. Maybe Framework did a better job than Lenovo, but I&#x27;m done experimenting for (going X1 Gen 11 now). reply binkHN 4 hours agoparentI have a similar Gen 4 machine, but in P14s form, and all is well with the exception of sleep and wake up. I&#x27;m hoping that gets addressed quickly with some BIOS and kernel updates. reply anotherhue 16 hours agoparentprev>Not convinced yet of AMD an Linux for mobile devicesHard to argue with the Steam Deck. reply MikusR 9 hours agorootparentSteamDeck has a dedicated team of developers supporting that particular hardware reply rjh29 16 hours agoparentprevBoth Gen 1 and Gen 3 AMD have been extremely stable with good battery life. The Gen 4 is new so it might take a few months for the kernel team to work out all of the bugs.The X1 Gen 11 is a very premium machine but compared to the T series it&#x27;s very expensive, and the CPU is underclocked. reply sva_ 15 hours agorootparent> The X1 Gen 11 is a very premium machine but compared to the T series it&#x27;s very expensiveOut of curiousity I set up a machine with max specs i7-1370p, 64GB RAM, 2.8K OLED; and the price came out to be $1685. Doesn&#x27;t seem unreasonably expensive if you wait for a sale. reply rjh29 14 hours agorootparentFair enough, it&#x27;s almost double that in the UK. reply PrimeMcFly 4 hours agorootparentSo buy a cheap ticket to the US to buy it and bring it back and it will still be less than double. reply rjh29 47 minutes agorootparentAssuming my time is worth nothing！ replysoperj 16 hours agoparentprevI have a T14 with Ryzen that I&#x27;ve used as a linux laptop from day 1 with the only issue being the wifi card, which I was able to solve on day 2 and haven&#x27;t had an issue with since. reply tlhunter 16 hours agoparentprevMy T14s AMD Gen 1 has been rock solid with Linux reply benoliver999 15 hours agorootparentI have a T495 and it&#x27;s fine. reply beebeepka 16 hours agoparentprevWhat? I have a number of Zen 1&#x2F;2&#x2F;3 laptops and the only one that gave me any trouble on Linux was the Zen 1 one. My wife has been using for 5 years. Everything else has been rock solid with a number of different distributions. reply mixmastamyk 15 hours agoprevYeah, ordered one relatively early and have been waiting a long time. Wish these new platform schedules were better aligned to the school year. :-&#x2F;Been wanting to get away from Intel for a while but seeing how much better the gen 13 iCPUs battery life is, sort of reconsidering. Must have finally found a way to reduce power draw.The reduction in port flexibility partially negates the whole idea, which was a bit of a gimmick in the first place. Oh well, at least the ports are protected from damage.Linux support is often good but AFAIK they still don&#x27;t ship it from the factory, making it second-class for regular folks.2023 and still no mention of ECC RAM! Still not even an option!All in all I like our Frameworks but the company and products still need to grow a bit. reply daft_pink 11 hours agoprevHow about an nvidia gpu config? reply grecy 16 hours agoprevHmm, the battery life comparison graph doesn&#x27;t show the M2 Macbook, but all the other ones do.How does the battery life compare to the M2 Macbook? reply MegaDeKay 7 hours agoparentThis was answered by the Ars Staff in the comments.> Answered your own question. There really aren&#x27;t good actively developed cross-platform battery life tests available, at all. And rolling and maintaining my own, especially one that&#x27;s in any way indicative of real-world performance, isn&#x27;t something I have the resources to do (or the authority to make anyone else on staff use, honestly).https:&#x2F;&#x2F;arstechnica.com&#x2F;civis&#x2F;threads&#x2F;review-framework-lapto... reply acheong08 16 hours agoparentprevCan’t compare. Apple is way too ahead on that front. If only they would allow me to run Linux on their devices… reply ZekeSulastin 15 hours agorootparentI mean, the entire reason the Asahi team are able to do what they do is because Apple isn’t stopping them at all (in the context of laptops&#x2F;Mac Minis at least). It’s just that they’re not doing anything to help them except for maintaining the ability to boot a different OS. reply seszett 15 hours agorootparentprevThey do allow it, it works fine unless you need one of the not yet implemented features (HDMI out, webcam and sound mostly, although the latter can be enabled at your own risks). reply grecy 14 hours agorootparentprev> Can’t compare. Apple is way too ahead on that frontWhat do you mean? The whole point of a comparison is to show what is better in what categories. If one was WAY ahead in performance or price that would be heavily talked about and graphed and compared.It&#x27;s an important point that absolutely should be compared. reply Strs2FillMyDrms 14 hours agoprevAs LL&#x2F;SC type architectures get more and more widespread, I am wondering all the novel ways in which programmers may discover new \"anomalies\" with the old memory model rules and constraints. speculative execution combined with relaxed semantics... and now LL&#x2F;SC widespread bubbleless cache exclusiveness acquirement?In all honestly mathematical operations are still better performed by LOCK prefixed architectures since there is no reason to go to the assembler level to re-read the cache... if all you&#x27;re going to be doing is perform a mathematical operation.LL&#x2F;SC is good for displays&#x2F;publishing operations (losers win all) ... not for graphics or mathematical ops. reply repelsteeltje 18 hours agoprev [–] I like the idea of replaceable and upgradable parts. But I would be totally convinced this platform is also a viable ecosystem if parts were also sourced by suppliers competing with Framework and everyone is profitable&#x2F;sustainable.Wouldn&#x27;t that be great?! reply moelf 18 hours agoparentnothing is stopping others from selling \"Framework-compatible parts\", the CAD for all bays, expansion bays, etc. are open source https:&#x2F;&#x2F;github.com&#x2F;FrameworkComputer reply pipo234 18 hours agorootparentPermission is one. (A pretty important one, at that.)A second is funding, the prospect of becoming profitable. This hurdle, Framework seems to have taken (for now), but it also seems to be stopping competitors (for now).Would be great if something grew from hobbyist &#x2F; enthusiasts. reply Moto7451 17 hours agorootparentCan you elaborate on the permission comment? The designs in their github allow commercial use with attribution. They even link to a Google form for submitting your product to their marketplace. Is there a submarine restriction somewhere? reply pipo234 17 hours agorootparentEhrm. Yes, there is permission. I did not say it imply that there is a restriction there, or did I?!Just meant that so far, the permission has not led to a thriving ecosystem. So beyond permission there must be factors. Like access to money.Not at all frameworks fault, of course. I was trying to argue from perspective of potential competitor what would be holding them back. reply Dylan16807 16 hours agorootparent> Ehrm. Yes, there is permission. I did not say it imply that there is a restriction there, or did I?!Yes, you definitely did. You replied \"Permission is one.\" to \"Nothing is stopping\". reply tlhunter 16 hours agorootparentprevOnce enough people buy frameworks you&#x27;ll be able to find cheap Chinese components on Amazon. reply delfinom 17 hours agorootparentprev>Would be great if something grew from hobbyist &#x2F; enthusiasts.Making PCBs to support modern x86 and even high-end ARM CPUs with DDR5 RAM and also PCI-express is beyond the skillset of a vast majority of hobbyist and enthusiasts. It gets so so far up the creek of high frequency signal engineering under electrical engineering. It&#x27;s not impossible, just very time consuming and costly (between equipment required for testing and PCB manufacturing).But yea all the other stuff in a framework could get hobbyists and enthusiasts making their own elements like the expansion cards and keyboard significantly more easily if there was a need for it. reply jdsully 16 hours agorootparentI did a DDR3 layout, sure there&#x27;s a lot to think about if you want it to work in extreme temperatures or with lots of EMI. But for hobbyist use its essentially get the right trace width for the PCB material using an online impedance calculator, and then length match and match the count of vias for all your traces of the same bus and don&#x27;t exceed the length restrictions. Don&#x27;t forget to have a good ground plane. This will work 99% of the time.Its time consuming and annoying but not rocket science. reply acheong08 16 hours agoparentprev [–] Framework doesn’t ship to China yet and when I got mine shipped via a third party, the customs stole some parts claiming that the laptop lacked certification. Once it’s open to the Chinese market, tons of cheap and low quality parts will flood the market replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Framework Laptop 13 has introduced an AMD Ryzen configuration, offering superior graphics performance compared to its Intel counterpart.",
      "The design of the laptop remains unchanged, and the Ryzen mainboard is compatible with existing Framework Laptop 13 cases, ensuring backward compatibility.",
      "While customization of ports is available with expansion modules, users need to be aware that some modules perform better in specific ports. In particular, battery life could diminish when USB-A modules are connected to certain ports."
    ],
    "commentSummary": [
      "The conversation focuses on the Framework Laptop, its features like the AMD Ryzen processor option, and comparisons with brands like ThinkPad.",
      "Users have reported shorter battery life with the AMD model, potentially due to USB port arrangement, and they debate the merits of soldered versus socketed RAM.",
      "Discussion extends to the prospect of an ecosystem for upgradable parts, the production feasibility of high-performance computer component PCBs, and the feasibility and concerns over shipping Framework laptops to China."
    ],
    "points": 164,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1696345803
  },
  {
    "id": 37750763,
    "title": "Trigger.dev V2 – a Temporal alternative for TypeScript devs",
    "originLink": "https://trigger.dev",
    "originBody": "Back in February, we posted a Show HN about building a “developer-first open source Zapier alternative” (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34610686). This was v1 of Trigger.dev.During the months since, we’ve gathered a lot of feedback from early users and realized that what developers actually wanted was more like an easier-to-use Temporal with integrations.Here’s what we’ve learned so far:- Serverless timeouts make it hard for anyone to write reliable background jobs. So our current product makes that easy for Next.js and other full-stack React frameworks. Long-running server support is coming soon.- We simplified the architecture to make it far easier to self-host. This was the most common comment in our previous Show HN.- We made it much easier to contribute to. You can now add new API integrations for any service we don’t already support. Either publicly (we appreciate PRs) or privately in your existing codebase.We’re open about what we’re building (https:&#x2F;&#x2F;trigger.dev&#x2F;changelog) and what we’re planning on doing next (https:&#x2F;&#x2F;trigger.dev#roadmap) as we believe community feedback ensures that we’re solving real problems.So here’s where we’re at, and where we’re headed: [x] Easy self-hosting. [x] Serverless. Long-running Jobs on your serverless backend. [x] Integration kit. Build your own integrations, or use ours. [x] Bring-Your-Own-Auth. You can now authenticate integrations as your users. [x] Dashboard. View every Task in every Run. [x] Cloud service. No deployment required. [x] React hooks. Easily update your UI with Job progress. [x] React frameworks. Support for Next.js, Astro, Remix, Express. [ ] More frameworks. Support for SvelteKit, Nuxt.js, Fastify, Redwood. [ ] Background functions. Offload long or intense tasks to our infrastructure. [ ] Long-running servers. Use Trigger.dev from your long-running backend. [ ] Polling Triggers. Subscribe to changes without webhooks. [ ] And lots more…I’d love to hear your thoughts on background jobs. Have we missed anything off the list? What should we be building next?https:&#x2F;&#x2F;github.com&#x2F;triggerdotdev&#x2F;trigger.dev",
    "commentLink": "https://news.ycombinator.com/item?id=37750763",
    "commentBody": "Trigger.dev V2 – a Temporal alternative for TypeScript devsHacker NewspastloginTrigger.dev V2 – a Temporal alternative for TypeScript devs (trigger.dev) 164 points by eallam 22 hours ago| hidepastfavorite34 comments Back in February, we posted a Show HN about building a “developer-first open source Zapier alternative” (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34610686). This was v1 of Trigger.dev.During the months since, we’ve gathered a lot of feedback from early users and realized that what developers actually wanted was more like an easier-to-use Temporal with integrations.Here’s what we’ve learned so far:- Serverless timeouts make it hard for anyone to write reliable background jobs. So our current product makes that easy for Next.js and other full-stack React frameworks. Long-running server support is coming soon.- We simplified the architecture to make it far easier to self-host. This was the most common comment in our previous Show HN.- We made it much easier to contribute to. You can now add new API integrations for any service we don’t already support. Either publicly (we appreciate PRs) or privately in your existing codebase.We’re open about what we’re building (https:&#x2F;&#x2F;trigger.dev&#x2F;changelog) and what we’re planning on doing next (https:&#x2F;&#x2F;trigger.dev#roadmap) as we believe community feedback ensures that we’re solving real problems.So here’s where we’re at, and where we’re headed: [x] Easy self-hosting. [x] Serverless. Long-running Jobs on your serverless backend. [x] Integration kit. Build your own integrations, or use ours. [x] Bring-Your-Own-Auth. You can now authenticate integrations as your users. [x] Dashboard. View every Task in every Run. [x] Cloud service. No deployment required. [x] React hooks. Easily update your UI with Job progress. [x] React frameworks. Support for Next.js, Astro, Remix, Express. [ ] More frameworks. Support for SvelteKit, Nuxt.js, Fastify, Redwood. [ ] Background functions. Offload long or intense tasks to our infrastructure. [ ] Long-running servers. Use Trigger.dev from your long-running backend. [ ] Polling Triggers. Subscribe to changes without webhooks. [ ] And lots more…I’d love to hear your thoughts on background jobs. Have we missed anything off the list? What should we be building next?https:&#x2F;&#x2F;github.com&#x2F;triggerdotdev&#x2F;trigger.dev reion 17 hours agoI moved from n8n to trigger.dev a month ago. Maintaining all the workflows in GUI was just too time consuming and difficult. I already use NextJS for my app, so switch to trigger.dev was easy, not maintaining and making changes my workflows in IDE takes about 80% less time. So far I am very happy with my decision to switch. reply louis-lau 18 hours agoprevMy initial thought when opening your website was that this was a job queue, like BullMQ. Not a \"link things together like IFTT&#x2F;Zapier&#x2F;N8N but in code\" platform. I don&#x27;t think I&#x27;d use \"Background Jobs framework\" to describe what you&#x27;re building, as that term has very different associations. Am I right in saying this is like BullMQ, except with integrations? If so, I&#x27;d focus more on the integration part of your marketing. reply matt-aitken 17 hours agoparentThat&#x27;s really interesting. What associations do you have with \"Background Jobs framework\"?Quite a lot of our customers are using us instead of Temporal. Not for linking stuff together use cases. reply nsonha 17 hours agorootparentTemporal calls itself a “Durable execution platform”, which I think emphases the persistence aspect of a message queue. I remember Cadence which is Temporal’s predecessor called itself a “workflow orchestration framework”. I think it’s a better term. reply jusonchan81 16 hours agoprevYou could also use Netflix Conductor which has typescript support and all the features listed here and more. reply corba_v2 14 hours agoparent+1 to this. Conductor is a far better experience. reply adamkaz 21 hours agoprevComing to Next.js from Laravel, being able to hand things off to a long running worker is one of the things I missed the most. We&#x27;re currently handling this with a non-ideal solution and will definitely check Trigger.dev out when you launch that feature. reply matt-aitken 19 hours agoparentGreat, you can subscribe to updates on our discussion here: https:&#x2F;&#x2F;github.com&#x2F;triggerdotdev&#x2F;trigger.dev&#x2F;discussions&#x2F;400We&#x27;ll be updating it as we make progress and open it up for early testers. reply plastic_bag 7 hours agoparentprevMay I ask what made you to switch from Laravel to Next.js? reply alecfong 13 hours agoprevI think this really hits the sweet spot. Been looking for a product just in this space for a while!- Types + IDE > UI&#x2F;WYSIWYG- Integrations- Long running workflows- \"Serverless\" reply chrisesplin 19 hours agoprevI&#x27;ve been using Firebase functions and Firebase function cron jobs for this purpose... but it&#x27;s definitely a roll-your-own situation.This project could go so many different ways and solve a bunch of my problems. reply matt-aitken 18 hours agoparentPart of the inspiration for this was Firebase functions. We used them extensively on a previous project with their Firestore triggers. There&#x27;s a lot to like about them but it was a messy development experience, especially the deployment process (although this was a few years ago). reply thallavajhula 9 hours agoprevThe marketing landing page looks very similar to Vercel&#x27;s homepage.https:&#x2F;&#x2F;i.imgur.com&#x2F;o5Cdldv.png reply Alifatisk 2 hours agoparentMost of these shadcn &#x2F; radix &#x2F; tailwind based sites look similar. reply xwowsersx 9 hours agoparentprevish yeah, only a tad bit more than the way all pages look like other pages these days reply nevodavid10 19 hours agoprevI have been using it for my new project. it&#x27;s really gold, because I don&#x27;t need to run another instance of NestJS for the background jobs. reply knowsuchagency 19 hours agoprevHow does it compare to windmill.dev? reply matt-aitken 19 hours agoparentOur focus is on writing jobs in your existing codebase. That means you use your existing development workflow (code editor, version control, etc) and can access your database and existing code easily.I&#x27;d also say that Windmill, and other similar tools like Airplane, are more focused on internal tools. Building internal admin tools and related workflows.Our users are building some internal tools but also core parts of their product that involve user interaction. reply rubenfiszel 17 hours agorootparentFounder of windmill here.You can, and that&#x27;s the main workflow for most users, use your code editor and version control with windmill, and you can access your database and existing code easily. It works exactly the same way as trigger where you sync and deploy from your existing code repo.Inngest and trigger are event-driven workflow engines while windmill is a more traditional workflow engine (think modern airflow) where the flow&#x2F;graph is defined statically in a low-code UI (although can be generated dynamically) while the steps are code-centric. We also include a UI builder (similar to Retool) and have an heavy focus on running on your own infrastructure (k8s with helm or docker-compose) and include complete observability for heavy jobs (streaming logs in real time) as well as the ability to use hardware acceleration since we use your raw nodes. Windmill also include workers management and is polyglot, You can run typescript, but also go, bash, and python, and write queries for bigquery, snowflake, and postgresql without having to wrap them in typescript.So I would say we are less focused on integration to external APIs and more focused on enterprise use cases, for critical and heavy background jobs that require long and complex workflows (for instance that may require approval steps). But I think you guys are also working on background jobs and I haven&#x27;t seen it so hard to say.Good luck to you all. I think for the people that know well both frameworks, it would be easy to discern when windmill or trigger is a better fit and I agree the use-cases are different. reply zubairov 18 hours agoprevTrigger.dev is awesome, use it every day. A feature list is quite impressive and complete, the only thing on my wish-list is a `wait&#x2F;resume` for long running jobs reply matt-aitken 17 hours agoparentThanks! Do you want the ability to call a Job from inside another Job and wait for the result?We have a discussion about that here, would love to get your input: https:&#x2F;&#x2F;github.com&#x2F;triggerdotdev&#x2F;trigger.dev&#x2F;discussions&#x2F;516 reply tedspare 19 hours agoprevThe hooks are great for returning multiple values&#x2F;progress updates to the UI. I&#x27;ve been using it for chained and&#x2F;or long calls to the OpenAI API. reply bilalq 16 hours agoprevWould love to see an article or post from you guys on how this compares to AWS Step Functions. reply revskill 11 hours agoprevI won&#x27;t use this because of the weird way of framework integration.Just give pure api call, instead of forced convention on file system level.Because i don&#x27;t use any of \"popular\" frameworks, that means i have no integration ? No. reply mikaeln 19 hours agoprevReally excited for this! reply Kalpeshbhalekar 21 hours agoprevSuper interesting tool! Glad to know you folks use SDK to write Jobs to avoid any deployment blockers. reply KRAKRISMOTT 15 hours agoprevYou need stronger support for state management and scheduling across long running tasks.This is your competitor:https:&#x2F;&#x2F;www.inngest.com&#x2F; reply asselinpaul 18 hours agoprevHow does it compare to inngest.com? reply matt-aitken 18 hours agoparentInngest is also an easier to use version of Temporal, for serverless.Currently the major differences are:- Open source: we&#x27;re fully open source and self-hostable with Apache 2 license.- API Integrations: we&#x27;re building first class support for popular APIs. That makes it really easy to subscribe to webhooks, and when you do API calls you get good retrying behaviour, automatically dealing with rate limits, and a great logging experience. You can write your own integrations and contribute them (that would be awesome), or keep them private to your own codebase.- React hooks: often background jobs are related to an action a person has done in your app (end-user or an admin tool). We have hooks so you can very easily show the live status of a run exactly how you want.Very soon- Support for Background Functions – we deploy your code so you can run any length of task. You write the code like any other job in your codebase. Discussion here: https:&#x2F;&#x2F;github.com&#x2F;triggerdotdev&#x2F;trigger.dev&#x2F;discussions&#x2F;400- Support for long-running servers (so you can use if you’re not deploying to serverless). Issue here: https:&#x2F;&#x2F;github.com&#x2F;triggerdotdev&#x2F;trigger.dev&#x2F;issues&#x2F;244 reply madarco 17 hours agorootparentWell done for being completely open source!I&#x27;ll suggest this to my employer and we&#x27;ll probably choose the managed solution, but having the freedom to choose is a great bonus! reply tonyhb 17 hours agoparentprevChiming in as a founder of Inngest.We released our TS SDK a year ago so had quite a head start in this area. The approach in their V2 release makes them more similar to us, though there are still some large differences, including the fundamental architecture, queueing technology, scale, etc. Some differences you&#x27;ll notice as a developer:- Concurrency and parallelism. We allow you to run steps in parallel, configure concurrency (per function, or with sub-groups for custom concurrency limits), and automatically fan jobs in, eg https:&#x2F;&#x2F;www.inngest.com&#x2F;docs&#x2F;functions&#x2F;concurrency and https:&#x2F;&#x2F;www.inngest.com&#x2F;docs&#x2F;guides&#x2F;step-parallelism.This is a pretty big point, as you often don&#x27;t want to run functions sequentially. We&#x27;re also fully event-driven, allowing you to pause workflows and automatically resume when specific events are received (https:&#x2F;&#x2F;www.inngest.com&#x2F;docs&#x2F;reference&#x2F;functions&#x2F;step-wait-f...).We also handle a lot of complexity for you that you&#x27;d have to build yourself:- Rate limiting- Batching, allowing one function to run from eg. 100 events, instead of 1:1 matching of event->function- Streaming, for long-running responses- Auto-cancellation, based off of matching events- Branch deploys for all workflows, regardless of platform- Debounce- And a bunch more such as middleware, error handlers, multi-language support (including zero-downtime live-migrations of long running workflows), fully-offline local development, etc.In general, I think we&#x27;re tackling a similar problem with fundamentally different approaches in events and architecture. Trigger definitely have more in the space of integrations, while our approach is: send us anything, no matter the event or source, and we&#x27;ll work with it.While I don&#x27;t know the Trigger folks I&#x27;m assuming they&#x27;ve seen similar problems as us and it&#x27;s fun to tackle this area, so I&#x27;m looking forward to seeing how they build out their platform in the future and this area evolves :) reply tshaddox 10 hours agoprevIn the TypeScript&#x2F;JavaScript world the only thing called \"Temporal\" that I was aware of is the Stage 3 proposal for an excellent new date and time module:https:&#x2F;&#x2F;tc39.es&#x2F;proposal-temporal&#x2F;docs&#x2F; reply Sheepsteak 10 hours agoparenthttps:&#x2F;&#x2F;temporal.io&#x2F; reply cpursley 10 hours agoprev [–] I have this bookmarked and it looks really well done, but feels like an anti-pattern. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Trigger.dev, an open-source alternative to Zapier, has responded to user feedback by simplifying its architecture for self-hosting and facilitating contribution.",
      "The team plans to extend support towards long-running servers, multiple frameworks, background functions, and polling triggers among others.",
      "The company encourages continued community feedback and maintains transparency regarding their progress."
    ],
    "commentSummary": [
      "Trigger.dev V2 is a newly released open-source alternative to Zapier, offering enhancements in self-hosting, architecture, and API integration.",
      "The tool is lauded for its ease of use, capability to handle long-running jobs, efficient API integrations, and UI updates.",
      "Promising future updates and its structural differences over other platforms has drawn positive feedback, making it a compelling choice for users seeking replacements for platforms like BullMQ, Windmill, and Temporal."
    ],
    "points": 163,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1696334839
  },
  {
    "id": 37756714,
    "title": "Lumber prices down 11% YoY",
    "originLink": "https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html",
    "originBody": "AboutAdvertisingEmail CRTantaWeekly ScheduleSponsored Offers Subscribe via emailTwitterRSS Feed In Depth Analysis: CalculatedRisk Newsletter on Real Estate (Ad Free) Read it here. TUESDAY, OCTOBER 03, 2023 Update: Lumber Prices Down 11% YoY by Calculated Risk on 10/03/2023 02:42:00 PM Here is another monthly update on lumber prices. SPECIAL NOTE: The CME group discontinued the Random Length Lumber Futures contract on May 16th. I've now switched to a new physically-delivered Lumber Futures contract that was started in August 2022. Unfortunately, this impacts long term price comparisons since the new contract was priced about 24% higher than the old random length contract for the period both contracts were available. This graph shows CME random length framing futures through last August (blue), and the new physically-delivered Lumber Futures (LBR) contract starting in August 2022 (Red). LBR is currently at $491.50 per 1000 board feet, down 11% from $550.00 a year ago. Click on graph for larger image. There is somewhat of a seasonal demand for lumber, and lumber prices usually peak in April or May. We didn't see a significant runup in prices this Spring due to the housing slowdown. P.S. Never miss the latest in housing, macroeconomics and finance with my daily round up newsletter. Sign up here. Weekly SchedulePosted by Calculated Risk on 10/03/2023 02:42:00 PM Newer Post Older Post Home Bloomberg 2020: A leading real-estate data junkie is now focused on the impact of the coronavirus LA Times: Blogger keeps finger on pulse of housing market. Business Insider: ... How He Got Everything Right And What's Coming Next Mastodon Last 10 Posts Oct 03 at 9:08 PM Wednesday: ADP Employment, ISM Services Oct 03 at 6:33 PM Vehicles Sales increase to 15.67 million SAAR in September; Up 15% YoY Oct 03 at 2:42 PM Update: Lumber Prices Down 11% YoY Oct 03 at 12:04 PM FHFA’s National Mortgage Database: Outstanding Mortgage Rates, LTV and Credit Scores Oct 03 at 10:00 AM BLS: Job Openings Increased to 9.6 million in August Oct 03 at 8:30 AM CoreLogic: US Annual Home Price Growth Rate Increased in August Oct 02 at 7:19 PM Tuesday: Job Openings, Vehicle Sales Oct 02 at 5:32 PM Energy expenditures as a percentage of PCE Oct 02 at 10:53 AM ICE (Black Knight) Mortgage Monitor: \"Home Prices Set Yet Another Record in August\" Oct 02 at 10:19 AM Construction Spending Increased 0.5% in August In Memoriam: Doris \"Tanta\" Dungey Click here for more on Tanta. Archive Archive Oct 2023 (14) Sep 2023 (126) Aug 2023 (139) Jul 2023 (121) Jun 2023 (129) May 2023 (137) Apr 2023 (129) Mar 2023 (139) Feb 2023 (119) Jan 2023 (129) Dec 2022 (133) Nov 2022 (131) Oct 2022 (134) Sep 2022 (127) Aug 2022 (135) Jul 2022 (132) Jun 2022 (138) May 2022 (136) Apr 2022 (128) Mar 2022 (138) Feb 2022 (127) Jan 2022 (132) Dec 2021 (136) Nov 2021 (150) Oct 2021 (155) Sep 2021 (155) Aug 2021 (184) Jul 2021 (170) Jun 2021 (168) May 2021 (168) Apr 2021 (169) Mar 2021 (179) Feb 2021 (153) Jan 2021 (161) Dec 2020 (155) Nov 2020 (152) Oct 2020 (160) Sep 2020 (158) Aug 2020 (169) Jul 2020 (173) Jun 2020 (169) May 2020 (165) Apr 2020 (164) Mar 2020 (144) Feb 2020 (116) Jan 2020 (118) Dec 2019 (113) Nov 2019 (105) Oct 2019 (124) Sep 2019 (122) Aug 2019 (125) Jul 2019 (125) Jun 2019 (116) May 2019 (124) Apr 2019 (117) Mar 2019 (123) Feb 2019 (108) Jan 2019 (125) Dec 2018 (125) Nov 2018 (122) Oct 2018 (124) Sep 2018 (114) Aug 2018 (127) Jul 2018 (124) Jun 2018 (114) May 2018 (130) Apr 2018 (123) Mar 2018 (128) Feb 2018 (114) Jan 2018 (126) Dec 2017 (123) Nov 2017 (121) Oct 2017 (121) Sep 2017 (116) Aug 2017 (119) Jul 2017 (108) Jun 2017 (116) May 2017 (110) Apr 2017 (111) Mar 2017 (119) Feb 2017 (109) Jan 2017 (108) Dec 2016 (113) Nov 2016 (116) Oct 2016 (118) Sep 2016 (120) Aug 2016 (112) Jul 2016 (111) Jun 2016 (125) May 2016 (111) Apr 2016 (112) Mar 2016 (121) Feb 2016 (114) Jan 2016 (114) Dec 2015 (119) Nov 2015 (117) Oct 2015 (125) Sep 2015 (124) Aug 2015 (103) Jul 2015 (125) Jun 2015 (131) May 2015 (123) Apr 2015 (129) Mar 2015 (133) Feb 2015 (125) Jan 2015 (135) Dec 2014 (134) Nov 2014 (129) Oct 2014 (144) Sep 2014 (127) Aug 2014 (130) Jul 2014 (143) Jun 2014 (131) May 2014 (137) Apr 2014 (139) Mar 2014 (134) Feb 2014 (128) Jan 2014 (141) Dec 2013 (140) Nov 2013 (136) Oct 2013 (145) Sep 2013 (146) Aug 2013 (147) Jul 2013 (151) Jun 2013 (141) May 2013 (150) Apr 2013 (149) Mar 2013 (151) Feb 2013 (133) Jan 2013 (160) Dec 2012 (154) Nov 2012 (157) Oct 2012 (165) Sep 2012 (145) Aug 2012 (161) Jul 2012 (170) Jun 2012 (162) May 2012 (169) Apr 2012 (162) Mar 2012 (162) Feb 2012 (156) Jan 2012 (169) Dec 2011 (157) Nov 2011 (178) Oct 2011 (182) Sep 2011 (170) Aug 2011 (178) Jul 2011 (174) Jun 2011 (157) May 2011 (158) Apr 2011 (164) Mar 2011 (172) Feb 2011 (162) Jan 2011 (177) Dec 2010 (171) Nov 2010 (169) Oct 2010 (182) Sep 2010 (179) Aug 2010 (184) Jul 2010 (190) Jun 2010 (189) May 2010 (198) Apr 2010 (185) Mar 2010 (210) Feb 2010 (195) Jan 2010 (212) Dec 2009 (225) Nov 2009 (209) Oct 2009 (215) Sep 2009 (202) Aug 2009 (230) Jul 2009 (269) Jun 2009 (252) May 2009 (241) Apr 2009 (256) Mar 2009 (254) Feb 2009 (255) Jan 2009 (214) Dec 2008 (204) Nov 2008 (252) Oct 2008 (268) Sep 2008 (304) Aug 2008 (210) Jul 2008 (251) Jun 2008 (206) May 2008 (203) Apr 2008 (202) Mar 2008 (204) Feb 2008 (195) Jan 2008 (212) Dec 2007 (179) Nov 2007 (189) Oct 2007 (179) Sep 2007 (176) Aug 2007 (209) Jul 2007 (155) Jun 2007 (135) May 2007 (106) Apr 2007 (120) Mar 2007 (138) Feb 2007 (77) Jan 2007 (70) Dec 2006 (63) Nov 2006 (70) Oct 2006 (67) Sep 2006 (70) Aug 2006 (61) Jul 2006 (56) Jun 2006 (44) May 2006 (60) Apr 2006 (53) Mar 2006 (45) Feb 2006 (38) Jan 2006 (42) Dec 2005 (46) Nov 2005 (54) Oct 2005 (60) Sep 2005 (46) Aug 2005 (86) Jul 2005 (43) Jun 2005 (47) May 2005 (52) Apr 2005 (39) Mar 2005 (29) Feb 2005 (26) Jan 2005 (12) Econbrowser Quantity Theory and (Broad) Money Demand in Normal and Abnormal Times Business Cycle Indicators at October’s Start Policy Uncertainty over the Last Forty Years (Again) GDP and Nowcasts: Continued Growth in Q3 Business Cycle Indicators, Pre- and Post-Comprehensive Revision Pettis: China Financial Markets China Financial Markets NY Times Upshot Kaiser Permanente Workers Poised to Strike The Americans Most Threatened by Eviction: Young Children Power of Older Women? Extinct G.O.P. Moderates? It’s Time for the Mailbag. A Brief History of Consequential Deaths in Congress Top 10 Hardest and Easiest Spelling Bee Words, Sept. 23-29 The Big Picture Transcript: Gary Cohn 10 Tuesday AM Reads 10 Quotes That Shaped My Investment Philosophy 10 Monday AM Reads 10 Sunday Reads Economic Sites A Dash of Insight Angry Bear Bonddad Blog Christophe Barraud Blog Mish's Global Economist Analysis Oilprice.com Privacy Policy Copyright © 2007 - 2023 CR4RE LLC เว็บไซต์นี้ใช้คุกกี้จาก Google ในการให้บริการและเพื่อวิเคราะห์การเข้าชม จะมีการแชร์ที่อยู่ IP และ User-agent ของคุณกับ Google รวมถึงเมตริกด้านประสิทธิภาพและความปลอดภัยเพื่อรับรองคุณภาพของการบริการ สร้างสถิติการใช้งาน รวมทั้งตรวจจับและรับมือการละเมิดดูข้อมูลเพิ่มเติมตกลง",
    "commentLink": "https://news.ycombinator.com/item?id=37756714",
    "commentBody": "Lumber prices down 11% YoYHacker NewspastloginLumber prices down 11% YoY (calculatedriskblog.com) 162 points by alphabettsy 14 hours ago| hidepastfavorite139 comments tpmx 12 hours agoThe chart in the article does a nice job at illustrating that prices are roughly speaking back to essentially pre-pandemic levels after giant spikes. This seems healthy to me. reply reducesuffering 12 hours agoparentI love the tradingeconomics site for tracking commodities prices. Indeed, lumber is at pre-pandemic prices from 5 years agohttps:&#x2F;&#x2F;tradingeconomics.com&#x2F;commodity&#x2F;lumber reply csomar 1 hour agorootparentIt&#x27;s almost double. There has been some spikes in the past but the mean is like 250-300. reply greggsy 11 hours agorootparentprevCripes what is going on with orange juice.. there’s some seasonal patterns in the last 25 years, but the last year has just kept climbing. reply mcculley 11 hours agorootparentHere in Florida, citrus greening has devastated the groves. reply bigmattystyles 9 hours agorootparentAren’t some growers able to grow in Georgia now, whereas they hadn’t been able to before? I seem to recall something about that. reply hattmall 7 hours agorootparentYes there are citrus groves in Georgia. I don&#x27;t think it&#x27;s that they hadn&#x27;t previously been able to so much as the agricultural dynamics makes it more reasonable now. It may be short lived though as the solar farms seem to be the fastest growing land usage now. reply HankB99 8 hours agorootparentprevI have no idea if that&#x27;s true or not, but I wonder how long it would take to establish orchards further north - time to realize it&#x27;s possible and time to get the trees to production. reply ramesh31 11 hours agorootparentprevhttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Citrus_greening_diseaseSeems this is could be a direct result of warming:\"The Asian form, Ca. L. asiaticus is heat tolerant. This means the greening symptoms can develop at temperatures up to 35 °C. The African form, Ca. L. africanus, and American form, Ca. L. americanus, are heat sensitive, thus symptoms only develop when the temperature is in the range 20–25 °C.\" reply mcculley 10 hours agorootparentI don’t understand how this follows. The Asian form could develop at temperatures less than 35°C also, right? reply ramesh31 10 hours agorootparentPoint being the American form is the issue in Florida. Warmer winters means more time spent at 20–25°C, increasing the overall disease burden. reply eth0up 11 hours agorootparentprevThat... and grubby land grabs, cavalier&#x2F;deranged development (Lennar), and ignorant newcomers who think making a subtropical keystone ecosystem conform to their hell of previous residence is justified by exorbitant checks and brutal disregard. reply greggsy 3 hours agorootparentI’m interested to understand your concerns - is there one issue that you’d be able to expand on? reply hibikir 7 hours agorootparentprevWelcome to climate change: Citrus trees take a while to bear fruit, so uncommon weather patterns lead to bad harvests.Something similar happens with Spanish olives, and with them, olive oil. Most Olive groves are not irrigated, because they historically don&#x27;t need to be, and we&#x27;ve had two very dry years. Production is down to 50% of what it was, and irrigating isn&#x27;t cheap or easy. And again, replanting isn&#x27;t a sensible choice, as the trees don&#x27;t mature in a season. We can plant different kinds of corn and wheat in different places as weather changes: A winter that is a little too short, and too long, and big ag will sell you a different bag to adapt to your updated planting date with minimal yield losses. But not with trees. reply tharkun__ 7 hours agorootparentOr really many if not most fruits where I am. Apple picking? Better be fast. Pears? None left. Plums? You wanted plums?Same with my own cherry trees. Beautiful flowers. Looked like it would be just as good as last year&#x27;s awesome harvest. Then frost at just the wrong time. I had maybe a couple handfuls of cherries. Not even enough on the trees to warrant putting up the bird netting. reply sdwr 7 hours agorootparentprevThanks for reminding me to buy olive oil next time at costco reply rgmerk 9 hours agorootparentprevI&#x27;ve often idly wondered whether there is more speculative trading in orange juice than other commodities because of Trading Places[1][1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Trading_Places reply Tao3300 8 hours agorootparentprevIt&#x27;s the Dukes reply ramesh31 11 hours agorootparentprevClimate change comes at you quick. We&#x27;re well past the point of hypotheticals, and headed down the road of catastrophe. reply reducesuffering 11 hours agorootparentprevNot sure. That was the lone outlier I saw too. Quick internet glance seems to implicate crop disease and Florida hurricanes dropping supplies. reply artursapek 9 hours agoparentprevCrazy to think people were doing construction and remodeling at 3x typical lumber costs reply fragmede 9 hours agorootparentI wonder how much of that remodeling was to accommodate a newly required home office due to WFH. If you really needed to have a separate office (eg kids), you simply had to grit your teeth and bear the cost. reply bluGill 8 hours agorootparentAlso there was little else to spend your money on. It is easy to spend ten thousand on a family vacation and for a couple years you didn&#x27;t take one. Plus all the eating out you didn&#x27;t do, movies you didn&#x27;t go to and whatever else people stopped doing. reply colecut 9 hours agorootparentprevIf you didn&#x27;t have the money, you would make it work...Could do the same even if you did.. reply bkjelden 9 hours agorootparentprevLumber is not that big of an input to total construction cost. reply harambae 7 hours agorootparentRight around 15% on a traditional stick build.Not insignificant but not the majority either. reply londons_explore 9 hours agorootparentprevLabour is the biggest cost of house building.And during a pandemic, that is almost free, since many people are stuck at home with nothing to do. reply dilyevsky 9 hours agorootparentYou best believe it wasn’t free. Especially for smaller jobs like flooring it’s often priced based on material. Everyone I know in construction was making $$$ in pandemic reply vel0city 9 hours agorootparentI think they were talking about DIY. People couldn&#x27;t really travel or do things outside the house so they might start home improvement projects themselves where as you might be more inclined to hire if you&#x27;re busy with outside things. reply flagrant_taco 9 hours agorootparentprevI assume it varied greatly by region, but my experience was that it was extremely difficult to find workers during the pandemic response.It seemed like a combination of fear and restrictions, plus free money at least in the US. I saw hourly rates for physical labor jobs go through the roof, double the pre-response rates and still couldn&#x27;t fill positions. reply sokoloff 8 hours agorootparentGP was suggesting that many people found a supply of labor in the mirror while brushing their teeth. reply artursapek 7 hours agorootparentprevLabor was not free during covid, it was harder to find than ever reply envsubst 9 hours agorootparentprevThese things are always cyclical, but FOMO is a tough emotion to combat. reply WillPostForFood 9 hours agorootparentLet&#x27;s hope being stuck at home for a year is not a cyclical event! That&#x27;s what drove a lot of it, not FOMO. Building a home office, using a kitchen multiple times a day, schooling from home... reply positr0n 5 hours agorootparentprevBuilt an entire house at the peak :(It sucked but plans were in motion pre-covid and waiting a year (or two? Who knew back then) would have meant missing the window that minimized disrupting our kids&#x27; school lives. reply lostlogin 4 hours agorootparentprevMaterial costs are not a lot over half the cost of building (in my experience), and lumbar isn’t all of that.Sometimes timing matters more than other factors. reply onlyrealcuzzo 9 hours agorootparentprevIt was only 3x the cost because people had 3x the excess money. reply huytersd 9 hours agorootparentNonsense. The lumber pipelines were interrupted. It was simple supply and demand unlike all this insane price gouging the grocery stores are pulling off now. reply onlyrealcuzzo 9 hours agorootparentRight - demand was up because people had more money. reply cmrdporcupine 8 hours agorootparentNo, people had more time, DIY projects exploded, and that was compounded additionally by supply chain issues (and closures) like everything else with COVID. reply huytersd 8 hours agorootparentYes this is correct. reply nightfly 9 hours agorootparentprevDemand was up because people had more _time_ reply nxm 8 hours agorootparentAnd money was being printed left and right reply SteveNuts 8 hours agorootparentHow does that end up in the hands of regular people doing renovation or home builds? reply fragmede 7 hours agorootparentI&#x27;m sure some of it is down to LLCs owned in their names that took out PPP loans from which contractors building a home office were paid out. reply ambicapter 7 hours agorootparentWhat fraction do you expect that was? Even just giving a completely unfounded guess. reply fragmede 7 hours agorootparentThe useful question isn&#x27;t that fraction, though I&#x27;d bet it&#x27;s larger than you&#x27;d expect, but how much free money was injected into the economy this way. I&#x27;m sure that&#x27;s a consequential sum, especially if you expand the money we&#x27;re looking at to include corporations paying other corporations inflated rates, coordinated by having execs&#x2F;board members in common. reply metaphor 7 hours agorootparentprevWere you asleep[1] the entire time? And that&#x27;s just what Average Joe was pocketing.[1] https:&#x2F;&#x2F;www.pandemicoversight.gov&#x2F;news&#x2F;articles&#x2F;update-three... reply nightfly 1 hour agorootparentGuess you&#x27;re forgetting the whole \"lots of people not able being able to go to their job and get paid\" thing during this time, right? replyjiveturkey 9 hours agoparentprevIt&#x27;s only healthy if prices have been relatively stable for the 5 years prior to that as well. If prices were on a steady rise and then only back to pre-pandemic, this is losing ground to where they should be.However, in addition, we can see that nominal prices remain stable over multiple 5 year periods. So yes, healthy.Too bad that this very good presentation on a decent blog has such a clickbait headline. I guess that&#x27;s the world we live in. Someone somewhere is going to say this is the death of lumber or other such drama, based on just the title of the post though. reply ggm 11 hours agoprevTreating \"price\" purely as a supply signal, and not as \"value\" or \"worth it\" I think it&#x27;s true the price of lumber worldwide related to supply chain shock, and the consequences for just-in-time dependencies in the building sector.I am a little surprised things like chip and plywood suffered, I guess when you cannot get timber, you go to manufactured wood (which in some circumstances actually has better mechanical strength, but most people think has lower aesthetics although opinions differ on this) and maybe even as a downstream (sometimes, waste+, certainly different cuts&#x2F;grades of timber go in) informed component of construction it \"costs more\" in these times.The whole Price&#x2F;Value&#x2F;Worth thing is really confusing. Timber is amazing. Burying it inside the carcase of a home feels at first glance like \"why not something else\" but the entire tooling of construction crews can be built around \"its wood\" not \"erect this premade steel form\" or \"lower the entire wall with insulation, wires windows and pipes into place\" as they do on &#x27;grand designs&#x27;The problem really is that price isn&#x27;t just a pure supply signal. People treat the ratchet up and down as distinct arcs with their own dynamic. It goes up faster than it comes down unless you know something about the future to make it worth dumping at any price. With wood, there isn&#x27;t much surprise in the future driving price down, only suprises driving price up.\"Building codes to forbid wooden housing due to giant space termites\" wasn&#x27;t on my roadmap for 2024 but you never know... reply greenie_beans 10 hours agoparent> I guess when you cannot get timberi thought timber wasn&#x27;t the bottleneck here, at least not in the US? from what i know, US timber has a glut of supply, something like 40 years worth of inventory.maybe you meant \"lumber\"? reply ggm 10 hours agorootparentYes. I think the fine-grained differences of meaning here matter. Good call.I am less sure why the same thing happened here in Australia because we have a huge logging, processing and distributing system domestically and for export, but we also had massive shortages of framing supplies. I suspect it was the timber&#x2F;lumber distinction, maybe the woods available on-shore weren&#x27;t being directed into framing and this depends on cheaper sources off-shore, which just disappeared.Our cost of construction has gone up so much a lot of builders are going bust. It&#x27;s causing angst in a hot market, house prices and rents out of control, and the government intervening but the industry says it can&#x27;t build that much more than is on the table, its not lack of desire, it&#x27;s lack of capacity in construction overall.Most of the fixes simply increase housing cost inflation. But this glitch on framing wood and related products, was really bad for us. reply fy20 6 hours agorootparentEurope also has a shortage, as it turned out a lot of both timber and lumber came from Russia. This has pushed up demand for imports from North America, which I guess has a knock on effect to other regions.Prices in Europe still haven&#x27;t dropped back to their original levels. In my country pellets for heating were €150 per ton in 2021, €600 per ton (and much worse quality) in 2022, and now are around €300 per ton. reply wlesieutre 12 hours agoprevFor the unfamiliar, a \"board foot\" is a unit of lumber volume representing a square foot of board with 1 inch thickness.So an 8&#x27; long board 12\" wide and 1\" thick is 8 board feet. Or half as wide and twice as thick (6\" x 2\") in the same length is also 8 board feet. You get the idea. reply angry_moose 11 hours agoparentTo elaborate a little more:Board thicknesses are measured in the \"quarter system\" - 4&#x2F;4 (pronounced \"4 quarter\", that is 4 quarters of an inch) is a 1\" thick rough-rough sawn board; 8&#x2F;4 (8 quarter) is a 2\" thick rough board. As this is rough sawn (super rough and unsightly finish), you usually lose at least 1&#x2F;8\" while finishing the board, so a 4&#x2F;4 rough board gives you something around 3&#x2F;4\" finished product.This leads to 5&#x2F;4 being another fairly common dimension, as you can get a 1\" finished board out of it. 6&#x2F;4 and 12&#x2F;4 are somewhat common as well.It might seem like an \"ugly dimension\" compared to something like m^3, but its really built around speed. Watching someone skilled price out an enormous stack of mismatched lumber (width, thickness, and length) in seconds with a lumber rule is always a fascinating process to watch (https:&#x2F;&#x2F;www.popularwoodworking.com&#x2F;techniques&#x2F;the-lumber-rul...).Usually the $&#x2F;board foot increases slightly at higher dimensions as thicker lumber is harder to produce. I got some 4&#x2F;4 black walnut for $12.80&#x2F;bdft last week; 8&#x2F;4 was closer to $16.Edit: This is also why 2x4s are actually 1.5\"x3.5\" - historically they were 2x4 rough sawn lumber, but .5\" is lost in both dimension when producing finished boards you purchase in the store. reply oblio 7 hours agorootparent> It might seem like an \"ugly dimension\" compared to something like m^3, but its really built around speed. Watching someone skilled price out an enormous stack of mismatched lumber (width, thickness, and length) in seconds with a lumber rule is always a fascinating process to watch (https:&#x2F;&#x2F;www.popularwoodworking.com&#x2F;techniques&#x2F;the-lumber-rul...).I imagine there are people all over the world doing the same tricks with metric measurements. reply bluGill 8 hours agorootparentprevHistorically 2x4 never had a standard and different sawmills produced different sizes. I&#x27;ve seen houses built in 1880 where a modern 2x4 fit perfectly, meanwhile a different house built in 1968 had a obvious size difference. reply anarazel 11 hours agorootparentprevIsn&#x27;t the loss for dimensional lumber more due to drying than finishing? reply coryrc 9 hours agorootparentNo, and they rough cut fast closer to 6&#x2F;4 (1.5\") than the original 8&#x2F;4 (2\") they may have originally used, because we don&#x27;t care about the quality as much. Part of that is the wood isn&#x27;t as good. Part is our machinery is now more precise.To get a 1\" finished board you need to start with at least 5&#x2F;4, because you may have some twist in the board and you can flatten alternating sides to make it flat. If you were just running it through a machine without paying attention, your might need to take that same board down to 0.75\" to get it flat.But they aren&#x27;t wasting time like that for framing lumber. But when your lumber is expensive, it&#x27;s worth a little focused labored to use more of the board reply angry_moose 10 hours agorootparentprevThat is some of it and it varies a lot based on the species, though I&#x27;m a lot less knowledgeable there.In the hardwoods I usually work with (walnut&#x2F;maple&#x2F;padauk) its fairly minimal. A nominally 4&#x2F;4 board usually comes in about 1&#x2F;16\" small; which is a lot less than I lose running it through the planar. Some of that could be the sawmill skimping out too and cutting just a bit undersized....I&#x27;m finding quoted numbers between 2-8% volumetric shrinkage (https:&#x2F;&#x2F;www.wood-database.com&#x2F;wood-articles&#x2F;dimensional-shri...), which is still quite a bit less than you find in the small dimension of a 2x4 (2\"-> 1.5%, or ~33%) reply PaulDavisThe1st 10 hours agorootparentprevhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=WaJFudED5FQ reply singleshot_ 11 hours agoparentprevWhen you say one inch wide, do you mean one of the twelve things in a foot, or one of the two things in the width of a 2x4? reply yojo 11 hours agorootparentI believe you’re referring to “nominal” vs “actual” size. For anyone who doesn’t know, a 2x4 is actually ~1.5”x3.5”Dimensional lumber is the original shrinkflator reply derekp7 9 hours agorootparentJust like a quarter pound burger is 1&#x2F;4 lb before cooking. reply BoiledCabbage 10 hours agorootparentprevThe \"smallest dimension is 1\".So in a 10ft long 2x4 it would be the actual size of a 2x4 (1.5\"x3.5\"x10&#x27;).A standard board foot would be (1\"x12\"x12\"), or any other math that makes up the same volume. reply em3rgent0rdr 12 hours agoparentprevmetric system is so much easier: m³ reply jpitz 11 hours agorootparentMetric makes the math easier, but it is much easier for me to relate to 40 board-feet of lumber than to 0.0944 cubic-meters. reply olyjohn 10 hours agorootparentJust like temperature. F is easier for me to relate to. 0 is very cold. 100 is very hot. We like it pretty warm, so 70-72 is great. Also F has a much finer scale for adjustments. I don&#x27;t have to set my thermostat to 22.5 degrees to be comfortable. I don&#x27;t really care about the boiling point or freezing point of water in my day to day life. reply applied_heat 8 hours agorootparentI am quite interested in knowing if it is going to rain or snow, if there might be ice on the road, if my hot tub might freeze if it is not turned on, etc. reply osigurdson 9 hours agorootparentprevI bet you likes miles, feet and country specific gallons too. Enjoy your unit system! reply hartator 12 hours agorootparentprevI don’t think you can have a 1m^3 of wood. Life is not Minecraft. reply tinco 12 hours agorootparentIf you&#x27;re purchasing gross lumber here in The Netherlands it is definitely priced per m3. For example here: https:&#x2F;&#x2F;inlands-hout.nl&#x2F;houtsoorten-en-prijslijst&#x2F;Just like with board feet it doesn&#x27;t say anything about the actual dimensions. Also, I don&#x27;t think it&#x27;s much better as a unit than board feet either, both have very unclear relationship to the actual amounts you need. You&#x27;ll always need to do a pretty complex calculation to figure out how much you&#x27;d need to buy. The pricing is mostly useful as a comparison to look at relative or day to day pricing differences. reply singleshot_ 11 hours agorootparentAnd for what it’s worth, the board foot cost of a 2x4 and the board foot cost of a 2x12 are wildly different. reply Jedd 11 hours agorootparentprevA foot is a (variable) measure of length.A board foot is a measure of volume - with an implicit third dimension.So you could just use the SI unit for volume.A board foot ~= 2.4 litres.This seems a bit silly though, as timber is not fungible in the way you&#x27;d expect a volume of something to be.As noted by sibling comments, you very much can buy timber in cubic metres - a convenience for costing &#x2F; transport - but you&#x27;re not going to buy timber without knowing dimensions of the individual lengths. reply mrcode007 11 hours agorootparentprevUnless you’re making infinitely thin two dimensional planes made of wood you always get a cubic volume of wood. Always. No exceptions. What do you think W x L x H stand for? reply mtmail 11 hours agorootparentprevGermany uses Euro per Festmeter as measure, a Festmeter is 1m^3. Trees are round of course but Festmeter is on the invoices and pricing tables. reply extraduder_ire 7 hours agorootparentprevWould litres make more sense? It is kind of vague though, since you&#x27;d be paying for the cuts you have to make and material you&#x27;ll lose. reply candiddevmike 12 hours agorootparentprevIf you buy logs you probably can reply applied_heat 8 hours agorootparentprevIn forestry road use agreements payments are made based on m^3 of wood hauled in the format of logs reply m463 11 hours agorootparentprevI think you would need old growth for true 1m³ reply Am4TIfIsER0ppos 8 hours agorootparentprevI don&#x27;t think most replies got the idea you mean a cube with sides measuring one meter. reply newhotelowner 8 hours agoprevI track price of the gloves.Box of gloves is very close to pre covid level.Used to be $2&#x2F;box in 2019. now you can get it around $2.5 box. reply metaphor 6 hours agoparent$2.50&#x2F;box today relative to $2.00&#x2F;box in 2019 represents +25% inflation...so nowhere near purported pre-COVID levels.CPI less food and energy[1] over the same period increased +17.3%...so currently priced worse than nominal inflation.Of course, this baseline reference assumes shrinkflation hasn&#x27;t impacted your \"box\" unit.[1] https:&#x2F;&#x2F;fred.stlouisfed.org&#x2F;graph&#x2F;?g=19FTW reply firebones 6 hours agorootparent>> $2.50&#x2F;box today relative to $2.00&#x2F;box in 2019 represents +25% inflation...so nowhere near purported pre-COVID levels.Annualized, that is 5.7% inflation. (1.25 increase ^ 0.25 for 4 years elapsed). So not pre-covid levels, but not nominal 25% annual inflation. Still a little over 2x desired annual growth rate. reply oarfish 6 hours agoparentprev25% inflation over 4 years is still substantial. If the contents are even still the same size. reply firebones 6 hours agorootparentSame size is important. There&#x27;s likely also 12-15% shrinkflation that increases this significantly. reply throw0101a 12 hours agoprevCan&#x27;t wait for another Odd Lots podcast supply chain episode on lumber. Previously (make it a trilogy):* https:&#x2F;&#x2F;omny.fm&#x2F;shows&#x2F;odd-lots&#x2F;why-the-price-of-lumber-has-s...* https:&#x2F;&#x2F;omny.fm&#x2F;shows&#x2F;odd-lots&#x2F;stinson-dean-on-the-lumber-cr... reply bradly 11 hours agoparentIf you want to learn about the lumber industry and how the supply chain works I recommend this podcast: https:&#x2F;&#x2F;www.lumberupdate.com&#x2F; reply somethoughts 11 hours agoprevAlways thought it&#x27;d be cool if there was some economic simulation game (simcity, city skylines, etc.) that was tied to real life commodity markets. reply extraduder_ire 7 hours agoparentThis often backfires on your game design. Like in escape from tarkov, where booming bitcoin prices made any player with GPUs very rich ingame. (also, tying the weather system to moscow weather led to people being annoyed at the constant rain)I&#x27;d be interested to see someone attempt market manipulation IRL to affect their game though. reply beckingz 6 hours agorootparentClassic Tarkov issue. Real bitcoin ruined the economy for a bit. reply m463 11 hours agoparentprevWould that make the game less fun?or would the economy be the fun part of the game, automating and driving down prices?I think games like factorio, the fun comes from automating and being less of a mental burden. reply bluGill 8 hours agorootparentReal world ecconomics doesn&#x27;t act like the simplified models in econ 101. Anyone with significant supply is adjusting how much supply they produce based on expected demand to keep prices where they need them. Sometimes someone chooses a lower price to take business from competition, but that is still a complex analysis. reply somethoughts 11 hours agorootparentprevI was thinking a bit like EVE online or Off World Trading Company but using real life stats. reply devnullbrain 9 hours agorootparentTying EVE online outcomes to real world markets is a guaranteed way to disrupt real world markets. reply pcurve 11 hours agoprevFrom the same blog: vehicle sales are up 15% YoYhttps:&#x2F;&#x2F;www.calculatedriskblog.com&#x2F;2023&#x2F;10&#x2F;vehicles-sales-in...Not necessarily a sign of health, but playing catcup with hisotrical trend. reply ehnto 10 hours agoparentUsed vehicle markets seem to have slowed, after a dramatic and surprising spike during the pandemic. Specialty vehicles in particular.For new cars, there was a dealer&#x2F;scarcity dynamic that seems to have allowed dealers to charge more than RRP, and consumers were paying it. I wonder if that has had an impact. reply balderdash 9 hours agorootparenthttps:&#x2F;&#x2F;site.manheim.com&#x2F;en&#x2F;services&#x2F;consulting&#x2F;used-vehicle...Definitely come down, but still quite elevated reply havnagiggle 7 hours agoprevI&#x27;m sure home insurance prices are going to follow shortly after. You know, because that&#x27;s why they needed to raise prices. Yep, any day now. reply ars 7 hours agoparentConstruction prices are still elevated due to labor shortages, lumber is only a minor component.And don&#x27;t say \"so pay labor more\" because there is a labor shortage in every single industry. If you pay more for job x, job y will also go up, until everyone goes up and then inflation is higher. reply ZoomerCretin 6 hours agorootparentThere is no labor shortage. US prime age employment is far below that of other rich countries. We have a huge amount of slack.And yes, \"just pay more\" is the solution. There is no shortage of Lamborghinis, but there is a shortage at $1000. Machine operators near me are paid $12&#x2F;hour, the equivalent of 2009&#x27;s minimum wage after inflation adjustment.Yes, wages are too low. Yes, it&#x27;s good to pay more money and for everyone to be forced to pay more money for wages. If you want cheap services built on extreme inequality, you&#x27;re free to move to a 2nd or 3rd world country. reply ars 5 hours agorootparent> Yes, it&#x27;s good to pay more money and for everyone to be forced to pay more money for wages.You might not have thought that through - if you actually did that, then everything costs more, and there is no net change in affordability. All you did was cause inflation.If you want to help everyone, without drawbacks, then increased productivity is the way to go. Then things are cheaper. Services are harder to improve though.> There is no labor shortage.Unemployment is historically low, so that&#x27;s not correct. reply oblio 12 hours agoprevAre these changes reflected in any downstream price changes? reply matternous 12 hours agoprevLumber prices seen all over the place when I search for various species of hardwood. Is there a standard online retailer to reference? reply angry_moose 11 hours agoparentNot really because its so regionally dependent - prices can vary dramatically within even 100 miles. I just picked up a bunch of of black walnut for $12.80&#x2F;bdft, but I could get it for around $8 if I drove a couple hours south. reply 0cf8612b2e1e 6 hours agorootparentThat seems surprising nobody has picked up on that arbitrage opportunity.Is it because most lumber volume is performed under commercial contracts? The DIY market is too small to chase? reply notyourwork 8 hours agoprevAnd companies in the business are still using supply chain as an excuse for prices. Recently experienced this when going through quotes for an 800 foot cedar fence. reply user3939382 9 hours agoprevThis could be a leading indicator for a slow down in real estate. reply ZoomerCretin 6 hours agoparentAmazing how counterproductive this is, and how the Federal Reserve refuses to see this.High prices are their own cure. Given high demand and high margins, people will increase production to meet demand. And yet this is not going to happen because interest rate hikes have doubled the price of new construction.High interest rates may lower or slow real estate appreciation in the short term, but they also increase it in the long term by killing production increases. reply chubs 12 hours agoprevI&#x27;m a little concerned that the downturn in that graph coincides with changing from measuring one financial instrument to another. But i dont know enough about those instruments to know if that&#x27;s an issue or not :) reply pnpnp 12 hours agoparentThe price of lumber also was astronomical during Covid. I think it’s somewhat logical to expect a return to a more “normal” price as economics bounce back from a somewhat unprecedented couple years of crazy economics. reply listenallyall 12 hours agoprevSupposedly, wholesale chicken wings are lower than pre-pandemic prices.https:&#x2F;&#x2F;www.nbcnews.com&#x2F;business&#x2F;consumer&#x2F;chicken-wing-price...I&#x27;ve yet to find a restaurant, however, where menu prices for wings aren&#x27;t at least 20% higher than 2019 or so -- often much higher. Similarly, I have a hard time believing that lower lumber prices will reduce new construction costs. reply dessimus 11 hours agoparentNot a lot of people are willing to work in the food service industry in general, nor at 2019 wages specifically, also rents, and real estate skyrocketed. Food costs are generally only about 1&#x2F;3 of menu revenue, with wages and overhead being the other ~2&#x2F;3. Profits are generally very thin in restaurants. reply curiousllama 12 hours agoparentprevRetail prices are generally pretty sticky - if the wholesale price persists, they&#x27;ll come down, but it&#x27;ll take time. reply quadrifoliate 11 hours agorootparentI hope they come down -- I have reduced my restaurant spend considerably since restaurant (even chain) prices went up by 30-50% in my area compared to pre-Covid.Maybe I&#x27;m too price-conscious, but I&#x27;d expect demand to go down over time as the rising interest rates affect daily life. reply Analemma_ 9 hours agorootparentIt&#x27;s not gonna happen. Labor costs determine the price of restaurant meals way more than wholesale food costs do. Those have gone up and probably are not ever coming back down; wages are a one-way ratchet in all but the most extreme circumstances. reply curiousllama 11 hours agorootparentprevYea for sure. This is the recession & deflation folks have been asking for. reply listenallyall 9 hours agorootparentprevI&#x27;m like you, however it sure seems that the proliferation of DoorDash and the rest proves that plenty of people are entirely price-insensitive. Expensive base prices + 20-30% in fees.Personally, I think there&#x27;s a lot of overlap between heavy DD users and people on social media complaining publicly about how tough it is to make ends meet. Just a guess. reply bdcravens 9 hours agoparentprevThe cost of labor to cook those chicken wings has gone up. (about 20% or so since 2020) reply listenallyall 6 hours agorootparentBut labor is, essentially, a fixed cost. Once a cook is working a shift, it costs the restaurant nothing extra (or a very small marginal amount) in labor to cook one additional order of wings. Most restaurants&#x27; problem is not the cost of labor, but the fact that expensive labor is idle much of the time. Lowering prices to increase volume would, in many cases, be a much better strategy, especially for a local restaurant which is starting out at a disadvantage compared to chains, which can purchase much larger volumes of food at lower prices. reply makestuff 11 hours agoparentprevYeah I agree, it will make prices stay flat for a few years, but I highly doubt there will be a massive decrease in construction cost unless we see another 2008 style economic crisis. Although, labor shortages in the construction industry might make still allow prices to stay high. reply superwalker 11 hours agorootparentLabor prices for skilled labor have increased so much that even when you do the “how much is my time worth” calculation as an overpaid engineer, DIY has become the only way to go. It’s nuts, but a fun learning experience. reply yieldcrv 9 hours agoprevThanks JPOW! stay the course, more money to delete reply Nick87633 7 hours agoparentJPOW sold all his stocks at the peak. He is gonna keep rates high until he can buy something at a steep discount, and it ain&#x27;t 2x4s. reply post_break 11 hours agoprevThis means my homeowners insurance will surely go down right? Right…? Of course not. reply pwarner 10 hours agoparentLabor to fix your house is still very much more expensive than before COVID reply adamc 10 hours agorootparentNot doubting you, but why? reply rgmerk 9 hours agorootparentA few factors - labour markets are very tight generally, new housing construction has increased, WFH and hybrid has increased the demand for renovating existing housing to make space for home offices (and improve the amenity of housing more generally). reply bdcravens 9 hours agorootparentConstriction of foreign-born labor as wellhttps:&#x2F;&#x2F;www.nbcnews.com&#x2F;data-graphics&#x2F;tough-immigration-laws... reply Dig1t 8 hours agorootparentThere were almost 200k people who crossed into the US illegally in August. It&#x27;s over 2M people per year now, pouring into the US.https:&#x2F;&#x2F;www.cbp.gov&#x2F;newsroom&#x2F;national-media-release&#x2F;cbp-rele.... reply oblio 7 hours agorootparentYour link doesn&#x27;t mention 2 million per year, plus at least some are recidivists and some more are sent back.It doesn&#x27;t mean 2 million people stay illegally per year. replygigel82 12 hours agoprevThings like plywood (for furniture) didn&#x27;t come down in price yet to pre-pandemic levels; hoping they catch up soon.You used to be able to get 3&#x2F;4 maple 4x8 for ~$50, now it&#x27;s $85 at Home Depot and over $100 from the local lumberyard. reply dugmartin 11 hours agoparentThere was a weird blip of time during the pandemic where cabinet grade plywood was cheaper than CDX sheathing at my local Home Depot. If I was building a house then it would have been cool to sheath it in oak veneer plywood. reply Nick87633 7 hours agorootparentIs oak veneer plywood exposure rated? reply ChatGTP 7 hours agoprev“In the USA”In Asia, for example, they have doubled. reply alexfoo 12 hours agoprev [–] tinlc reply somethoughts 11 hours agoparent [–] there is no lumber cartel? reply alexfoo 1 hour agorootparent [–] Yes, long running joke in the old first days of email spam.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lumber_Cartel replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article reports an 11% decrease in lumber prices compared to the previous year.",
      "It indicates that the transition to a new physically-delivered Lumber Futures contract has influenced long-term price comparisons.",
      "There's a nod to seasonal demand trends in lumber, with prices typically reaching their peak in April or May."
    ],
    "commentSummary": [
      "The article highlights the potential normalized market conditions leading to a decrease in lumber prices following the pandemic-driven surge.",
      "It also delves into the influence of citrus greening disease and climate change on agriculture, which is of interest to industries including construction and food service.",
      "A point of discussion is the high prices of food delivery services and the factors impacting plywood prices, providing multiple perspectives on how these changes affect various industries."
    ],
    "points": 162,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1696362918
  },
  {
    "id": 37751924,
    "title": "Fairphone 5 review: The most uncompromising repairable phone yet",
    "originLink": "https://www.androidpolice.com/fairphone-5-review/",
    "originBody": "ADVERTISEMENT Newsletter Trending Made by Google event Android 14 Google Pixel 8 Pixel Watch 2 One UI 6 Today's best deals PHONES GOOGLE OPERATING SYSTEMS CHROMEBOOKS GADGETS REVIEWS BUYER'S GUIDES MORE Join Our Team Newsletter Advertise With Us Write For Us Share A Tip Home Terms Privacy Copyright Meet The Team How We Test About Us Fact Checking Policy Corrections Policy Ethics Policy Ownership Policy Partnership Disclaimer Contest Rules Copyright © 2023 www.androidpolice.com AD AD AD AD AD AD Home Phones Phones & Accessory Reviews Fairphone 5 review: The most uncompromising repairable phone yet BY MANUEL VONAU PUBLISHED 2 DAYS AGO Fairphone learned all the right lessons from its previous phones Readers like you help support Android Police. When you make a purchase using links on our site, we may earn an affiliate commission. Read More. Fairphone is probably the company most associated with repairable and environmentally friendly phones, no matter how much Apple and Google want to tell you their latest and greatest smartphones are green. The new Fairphone 5 is more than just a repairable phone, though. For the first time, the company's latest smartphone looks like a product that fits the year it was released in, with a modern design and display combining a device that can be easily fully disassembled. WATCH NEXT Play Video The Fairphone 5 certainly manages to turn eyes with its first impressions, but how does it hold up over a longer period of time? I set out to find out just that when preparing to write my full review, so read on to learn if this eco-focused phone is really as good as we all hope it is. Fairphone 5 8 / 10 The Fairphone 5 is the phone to choose if you value repairability and environmental responsibility above everything, but it may not be easy to come by in the US. It's possible that the company will launch it soon, though, maybe in the form of a cooperation with Murena, like it did with the Fairphone 4. SoC Qualcomm QCM 6490 RAM 8GB Storage 256GB Battery 4,200mAh Ports USB-C Operating System Android 13 Front camera 50MP Rear camera 50MP IMX800 wide, 50MP IMX858 ultrawide Connectivity 5G, Wi-Fi 6, Bluetooth 5.2 LE Dimensions 161.60 x 75.83 x 9.6mm Colors Transparent, black, blue Weight 212g Charge speed 30W IP Rating IP55 Price €700, £620 Pros Longest software and hardware support window in the industry Only slightly bigger and bulkier than non-repairable phones Removable battery Fully repairable Cons Worse than average camera performance, particularly in low light Comparatively expensive The processor might not have enough oomph long-term Mediocre vibration motor and speakers See at Fairphone See at Amazon Availability, network, and pricing The Fairphone 5 is only available in Europe for now, available on the company's own website and a few other retailers. You can buy it for €700 or £650, which is roughly $750. That is more expensive than phones with comparable specs, but you're also buying a much longer support window, fairly sourced raw materials, and a repairable handset that introduces some unique design challenges. It’s possible that the Fairphone 5 will make the jump to the US in the future, though. In a collaboration with Murena, the Google-free Android ROM, the Fairphone 4 came to the US market just a few months ago, complete with the usual replacement parts. While it may take Fairphone a while, it's certainly possible that it will bring its new phone to the US sooner than later now that the infrastructure is in place. We can only hope that the company will offer customers the choice if they want to go Google-free or not — the Fairphone 4 is currently only offered without Google apps. Design and display The Fairphone 5 looks a lot like its predecessor, the Fairphone 4. That’s not a bad thing. It shows that Fairphone has found a unique and recognizable design language that works and that makes it instantly recognizable. The Fairphone 5 retains the signature triangular camera setup in the top left of its removable plastic back, the same slightly rounded aluminum frame, and a more sizable bezel than what you would usually see in today’s smartphones. The visible design is not the whole story, though. The Fairphone 5 offers some significant industrial design improvements. While the Fairphone 4 clearly had some design compromises attached to make it repairable, the Fairphone 5 is only slightly bigger, heavier, and bulkier than other 2023 phones. If you’d put an assortment of budget and flagship phones from this year next to each other, you’d have a hard time picking out the Fairphone 5 as something that absolutely doesn’t fit. That in itself is a big achievement and makes me hopeful about future innovation in this space. If Fairphone, a small upstart company, can build a repairable smartphone that doesn’t look like it’s compromised, what’s stopping Apple and Google? (Profits, probably.) The Fairphone 5’s biggest upgrade is its new OLED display, which, combined with the new hole-punch selfie camera, sets a couple of firsts for the company. It gets plenty bright, runs at a speedy 90Hz refresh rate, is protected by Gorilla Glass 5, and at 1224x2770, is as sharp as you'd hope for. I also appreciate the perfectly even bezels at the top and bottom. They may be bigger than what we see on other modern phones, but the fact that they’re even makes them, at the very least, aesthetically pleasing. From the looks of it, the screen is laminated to the glass — this is a compromise when it comes to repairability, as you can’t replace broken glass when the screen itself is still fine. Thankfully, you'll find it a much more immersive viewing experience because of its lamination. Hardware and what’s in the box The rest of the hardware fits right in with the OLED display: The Fairphone 5 offers the specifications of an upper midrange phone. It comes with a 4,200mAh battery that charges at 30W, 256GB of microSD-expandable storage onboard, 8GB of RAM, and a Qualcomm QCM6490 processor. The latter is an outlier in the phone world — don't be surprised if it's the first you're hearing of it. Its a chipset usually applied to industrial or smart home applications, but it offers the big advantage of a longer support window that allows Fairphone to offer up to eight years of software support. It's pretty much equivalent to the Snapdragon 782G in all but name. What truly makes the Fairphone special is the fact that you can fully disassemble it yourself in a matter of minutes, using a standard screwdriver you likely have at home already. Once you pry off the back and remove the hand-removable battery (which you will have to take out whenever you want to access the SIM card or the microSD card, just like in the good old days), you can easily disassemble the phone and replace whatever modules need replacing. Despite the removable back, Fairphone still managed to get an IP55 water and dust resistance certification — something you don’t usually see in electronics you can take apart yourself. IP55 means that it offers basic protection from water and dust, but it’s not rated to be immersed in water. Try not to drop it in a puddle. In the box, you’ll find nothing but the Fairphone 5, along with some warranty information and a quick start guide. Apart from the phone, all contents and the box itself are made exclusively from cardboard. You can use the box to ship in your old phone to have Fairphone recycle it, but that’s about everything you can get out of it. Fairphone argues that you likely have a lot of the necessary accessories like a charger and a cable already lying around, and it doesn’t want to create more e-waste than necessary. If you do need a new cable and charger, the company offers some of their own that you can add to the cart while shopping. Otherwise, there are plenty of great chargers and high-quality USB-C cables we recommend, too. Software and performance The Fairphone 5’s software comes very close to what Google offers on its Pixel phones, minus the company’s specific enhancements like Call Screen, always-on song recognition, and more. It’s basically stock Android with only a handful of extra tweaks, which should make it easy to feel right at home with it. Fairphone entirely relies on Google apps to provide the default software experience, with all the usual suspects like Gmail, Maps, Calendar, Messages, and more on board — no needless duplicates anywhere. The one app that Fairphone itself pre-installs is the My Fairphone app, which offers a quick start guide, information on the company’s extended 5-year warranty, device information, and quick access to help. Something that surprised me, though, was that I found three of my German carrier’s apps pre-installed, too, which might be some form of cooperation with Fairphone. It’s a bummer that they can’t be uninstalled at all, at least while my SIM card is in it. Speaking of the SIM card: Fairphone still insists on displaying your carrier’s name in the top left corner by default, taking away precious space that could be used for useful notifications. Thankfully, this can be easily and quickly disabled under system settings → Display → Network name. 4 Images The setup process, including activating Fairphone's 5-year warranty, is simple I’m also not a big fan of the launcher. It has the Google search bar hard-coded at the top of the first home screen, which puts it in an awkward and hard-to-reach position. If you prefer a less dense app grid, you only have the option to switch from the default 5x5 grid to a 4x4 and a 3x3 option, but no 4x5 option. This makes icons appear comically far apart from each other, and takes away one more row of apps that could easily fit the screen. I also wish the launcher would auto open the keyboard when opening the app drawer, like Pixel phones optionally offer. It’s mostly serviceable other than these points, though, and I’m sure these are all points that might not bother many people at all. Fairphone promises to support the Fairphone 5 for up to eight years, with a potential extension to ten years in the cards. In that time frame, the company wants to provide five OS updates. On top of that, it makes it simple to install a different OS on the phone, like the aforementioned privacy-centric Murena — some units are even sold with it pre-installed. If history is any indicator, it may take Fairphone quite some time to port new Android releases to the Fairphone 5 the older it gets, though the industry-focused processor with its long-term support may help alleviate those problems. 4 Images When it comes to performance, the midrange Snapdragon equivalent does a fine job for anything I throw at it today, be it extensive maps navigation, some light gaming, video streaming, and more. It barely ever stutters or has to take a break to think, and that’s promising for its long-term viability. However, the Fairphone 5 is supposed to last more than eight years. While the processor is good today, I’m not sure if it was a good idea to opt for a midrange processor rather than a high-end one, if only to have that extra performance overhead for the years to come. Think about it — the first Pixel phone arrived just seven years ago, and I’m sure it’s not nearly as usable today as it was in 2016. At the same time, it’s clear that Fairphone learned its lessons about processors. When I reviewed the Fairphone 3, it already exhibited severe performance problems right from its release. I don’t know if there was more optimization done to it to help it last longer, but there is only so much you can do to keep phone running well with underpowered hardware. For what it’s worth, I saw multiple people at IFA still using the Fairphone 3 this year, including a Fairphone PR representative who, I would assume, could have access to a newer Fairphone if they wanted to, and they seemed happy enough with it. If you’re not pushing the boundaries of what mobile computing offers, I’m confident that the Fairphone 5 should last you for a long time, especially considering that Fairphone 3 experience in mind that I had at IFA. I still wish it had a flagship processor, though, just for that extra peace of mind. There is a reason why Apple keeps pushing the processor boundaries on its phones, something that allows it to sell brand-new older phones like the iPhone 13 with little to no performance issues two years after their initial release. Battery life and charging The 4,200mAh battery is also a wonderful throwback to past times. If you don’t like carrying a good old power bank to top up your phone, you can consider simply getting a replacement battery early that you can throw in your bag for backup. This might be necessary in the longer run, since the 4,200mAh capacity it has is on the lower end of the modern spectrum. I used the Fairphone while covering IFA 2023 and when I was traveling on vacation, so I put it through its paces more than I normally would, but I felt that I had to recharge it more often than other phones, particularly compared to some Xiaomi and Honor phones out there. Given that Fairphone’s Android version is barely changed compared to stock Android — without the optimizations certain brands place on battery life that can result in missed notifications — this isn’t a perfectly negative downside. Here, apps run reliably and as expected in the background with timely notifications, something that can’t be said for Xiaomi and Honor. 4 Images The battery life can vary widely depending on whether you're on the go or on Wi-Fi at home The Fairphone 5 allows you to monitor your battery health with an extra section in the battery section. You can see the health in percentage, how many charge/discharge cycles your battery has had so far, and what the full mAh count should be for the battery. There are additional features to keep your battery healthy over a long period of time, like a Battery Protect option that limits the maximum battery life to 80% or a charging mode selector that lets you slow or speed up the charge speed. Camera Fairphone upgraded its camera array to a trio of 50MP cameras front and back, with a 50MP IMX800 primary, a 50MP ultra-wide, and a 50MP selfie camera. On paper, this setup is promising and should be competitive, but it shows that photography and videography has become a software problem more than something that can be fixed through hardware alone. Fairphone’s cameras offer significantly less dynamic range, with highlights easily blown out and shadows barely visible. It’s more in line with how a DSLR would take pictures, which isn’t necessarily a bad thing, but it’s very different from the way other phones handle photography these days. A gallery of sample images straight from the camera You certainly have to accommodate for that when composing a shot. There are also some issues with oversaturation in tungsten lighting conditions, and some discrepancies in color science across the primary camera and the ultrawide lens. The camera still serves well for images that you don't want to print out on a big canvas, and some of the color problems can be mitigated with a bit of editing. Fairphone rolled out updated software in the midst of our review, complete with some improvements to the camera, and I do feel like both the oversaturation problem and HDR performance have both improved. I'm sure things will only get better with more software updates, and maybe we will see a GCam camera mod down the line that takes things even farther. However, as always, I can only judge the phone in the state it currently is in and not based on potential future updates. Overall, the camera is certainly serviceable — impressive, even, considering Fairphone has far fewer engineering resources than Google, Apple, and Samsung. A gallery of sample images straight from the camera The camera software also isn’t as reliable as I would like it to be. For example, Fairphone doesn’t offer an automatic night mode, forcing you to manually enable it whenever you need it instead. The zoom controls are also more finicky than I would like them to be, with a single button letting you cycle between 1x, 2x, and ultrawide that also turns into a zoom slider when you hold and slide on it. Competition The Google Pixel 7, the Fairphone 5, and the Apple iPhone 13 next to each other There isn’t really any comparable repairable phone out there that is sold as broadly as the Fairphone 5, but of course, we can’t look at the handset in isolation. At its £650 price tag, there is plenty of competition. First and foremost, I think of the Google Pixel 7, which offers a better build quality in a smaller body and a much better camera experience. However, Google only gives you five years of security updates and three big Android updates, with one of them — Android 14 — right around the corner today. Google also makes replacing parts much more complicated. If you want a phone with a comparable lifecycle as the Fairphone 5, the iPhone 15 comes closest. Apple is known to support its devices for years to come, so it’s likely you could be able to use the iPhone 15 for the next eight to 10 years if you’re really pushing for it. While the iPhone isn’t self-repairable, Apple offers walk-in stores and mail-in service almost everywhere in the world. Fairphone isn’t only about repairable hardware, though. The Fair in its name stands for its mission to build a fairer supply chain, with living wages for everyone involved building the phone, from engineers over factory workers to miners. The company also sources its materials as fairly as possible, making sure raw materials are either recycled or ethically mined. While Apple may claim that it’s on a path to provide the latter, too, I don’t think that workers in the lower parts of its supply chain are paid nearly as well as those working on the Fairphone. If your environmental and social impact is important for you, Fairphone is virtually peerless in this field. Should you buy it? The Fairphone 5 is a special phone. It shows the industry how repairable and fairly sourced handsets can be created without compromising too much on design and only a little bit on price. At the same time, these are exactly the points that make it hard to recommend for many people. When you buy the Fairphone, you pay a high price for a phone that should last you for a long time, all while it’s unclear how well the hardware will hold up over the next eight years. For our planet and for the sake of social responsibility, the Fairphone 5 is still the best smartphone choice you can make. You'll just have to consider how committed to the cause you really are before you pull the trigger on your purchase. Fairphone 5 The Fairphone 5 is the phone to choose if you value repairability and environmental responsibility above everything, but it may not be easy to come by in the US. It's possible that the company will launch it soon, though, maybe in the form of a cooperation with Murena, like it did with the Fairphone 4. See at Fairphone See at Amazon Subscribe to our newsletter Comments 3 Share Tweet Share Share Share Share Copy Email Link copied to clipboard RELATED TOPICS PHONES REVIEWS ABOUT THE AUTHOR Manuel Vonau • Senior Google Editor (2218 Articles Published) Manuel Vonau is Android Police's Google Editor, with expertise in Android, Chrome, and other Google products — the very core of Android Police’s content. He has been covering tech news and reviewing devices since joining Android Police as a news writer in 2019. He lives in Berlin, Germany. Manuel studied Media and Culture studies in Düsseldorf, finishing his university career with a master's thesis titled \"The Aesthetics of Tech YouTube Channels: Production of Proximity and Authenticity.\" His background gives him a unique perspective on the ever-evolving world of technology and its implications on society. He isn't shy to dig into technical backgrounds and the nitty-gritty developer details, either. Manuel's first steps into the Android world were plagued by issues. After his HTC One S refused to connect to mobile internet despite three warranty repairs, he quickly switched to a Nexus 4, which he considers his true first Android phone. Since then, he has mostly been faithful to the Google phone lineup, though these days, he is also carrying an iPhone in addition to his Pixel phone. This helps him gain perspective on the mobile industry at large and gives him multiple points of reference in his coverage. Outside of work, Manuel enjoys a good film or TV show, loves to travel, and you will find him roaming one of Berlin's many museums, cafés, cinemas, and restaurants occasionally. POLL Will you watch Google's Pixel 8 and Pixel Watch 2 event this week? Yes, I'll be watching! Maybe, I haven't decided yet. It doesn't fit my schedule, but I might watch a replay. No, I'm not interested. Something else (leave a comment). Vote View Results TODAY'S BEST TECH DEALS THE LATEST AP PODCAST Upgrade your car to wireless Android Auto for less with this discounted adapter 15 hours ago Level up your IT know-how with StackSocial's $50 CompTIA course bundle 17 hours ago Best Prime Day power bank deals: Up to $75 off top Anker, Baseus, and more 21 hours ago See More TRENDING NOW How to reset a Fire TV remote Best budget Android phones 2023 Sony Xperia 5 V vs. Xperia 1 V: How similar can two phones be?",
    "commentLink": "https://news.ycombinator.com/item?id=37751924",
    "commentBody": "Fairphone 5 review: The most uncompromising repairable phone yetHacker NewspastloginFairphone 5 review: The most uncompromising repairable phone yet (androidpolice.com) 155 points by raybb 20 hours ago| hidepastfavorite241 comments sigmoid10 19 hours agoThis kind of release schedule totally ruins the entire premise of their products. The Fairphone 3 is barely four years old and you already can&#x27;t get replacement parts for it anymore. And for what? Only because they slightly changed the chassis, instead of offering a new base version with more modern components. On top of that, the 3 will also lose security updates in 12 months. So you will be forced to trash your phone after 5 years tops. Compare that to Apple, who still support their seven year old iPhone 7 with parts and security updates. The problem is that a \"real\" Fairphone, that actually stands up to its values, is a terrible business idea. So no profit oriented company will ever be able to fulfill that promise. reply FredFS456 19 hours agoparentI can find Fairphone 3 (and even Fairphone 2) parts here: https:&#x2F;&#x2F;shop.fairphone.com&#x2F;shop&#x2F;category&#x2F;spare-parts-4&#x2F;page&#x2F;...Is there a specific part you can&#x27;t find? reply corney91 18 hours agorootparentThe bottom module has been out of stock for a while[1], and is also the part that&#x27;s probably going to break first due to the amount of use it gets when charging daily. As someone who needs to put their FP3+ in _just_ the right position for it to charge, I&#x27;m very tempted to buy a new phone so I don&#x27;t have to deal with the anxiety of wondering if it&#x27;s properly charging when I go to bed.Before anyone says: yes, I have cleaned the USB port out. Cutting a triangle off a credit card and using the corner worked the best in my experience.I still would pay for the other \"fair\" aspects of the Fairphone, but until I can actually repair it I&#x27;m not going to consider it a repairable phone.[1] https:&#x2F;&#x2F;forum.fairphone.com&#x2F;t&#x2F;fairphone-3-bottom-module-avai... reply depressedpanda 8 hours agorootparent> Before anyone says: yes, I have cleaned the USB port out. Cutting a triangle off a credit card and using the corner worked the best in my experience.Are you sure you got all of the lint out?I&#x27;m asking, because I thought I had, and my phone still wouldn&#x27;t charge reliably.As a hail Mary (I was ready to buy a new phone anyway) I tried a thin metal needle. I couldn&#x27;t believe the amount of gunk I got out of the port.In my attempts to get it charging I had compressed the lint so hard that none of the other softer tools I&#x27;d tried before could get it out.While I didn&#x27;t try your method specifically, I&#x27;m quite sure a credit card would&#x27;ve be too thick and soft to do it. reply corney91 21 minutes agorootparentHey, thanks for the suggestion. I&#x27;ve tried all sorts, from compressed air (useless) to toothpicks (not quite small enough) to thumbtacks (really hard to do carefully). You&#x27;ve reminded me that the other successful tool was a SIM eject tool I found lying around, but even then it&#x27;s still not reliably charging. It&#x27;s better with the USB cable one way round compared to the other, which makes me wonder if I&#x27;ve managed to scratch it with the thumbtack.The irony is one of the main reasons for buying this phone was I didn&#x27;t want to have to get a new one and transfer everything due to some tiny problem with it. The latest ETA is end of October so I guess I&#x27;ll wait at least that long before giving up... reply elesiuta 16 hours agorootparentprevI had the same issue and found floss picks worked well, but after a while the pins just became too worn. Thankfully my new phone has wireless charging. reply sigmoid10 17 hours agorootparentprevThe important parts (i.e. the ones that actually break easily) have been out of stock forever. reply jstummbillig 18 hours agorootparentprevActually fairly incredible that you can still get spare parts for a 8 year old smart phone (the Fairphone 2). The problem from my POV was, that the hardware specs were already way past their expiration date when I sold mine 3 years ago, even when using rather unexciting apps, making spare parts somewhat obsolete.Are later FPs set up to offer chip upgrades to avoid this dilemma? reply FredFS456 18 hours agorootparentThey&#x27;ve offered camera upgrades before (https:&#x2F;&#x2F;www.fairphone.com&#x2F;en&#x2F;camera-upgrades-for-fairphone-3...) but not chipset. reply wierd-eye-loser 16 hours agorootparentprevThese type of comments miss the point of the argument in that the company is failing at promising an actual fair product. reply Scandiravian 18 hours agoparentprevIt&#x27;s sad that the Fairphone 3 is losing support already, though I think that&#x27;s much more down to upstream vendors of the hardware not providing support for the chipset, not Fairphone themselvesIn terms of the release schedule I think it&#x27;s important to continue to have a \"jumping on point\" for customers who want to switch to a Fairphone. It&#x27;s hard to convince someone to replace a 3-5 year old phone for another 3 year old modelAs long as they keep providing 8+ years of support for newer models I think it&#x27;s the best option, since the company also need to have a continuous revenue stream to exist for the full lifetime of their phones reply freedomben 18 hours agorootparentI agree with you, but the problem for me is, how do I know the Fairphone 5 won&#x27;t suffer the same fate with security updates? reply Scandiravian 18 hours agorootparentAs far as I can recall the fp3 was sold with a promise of five years of software updates and parts. I think they&#x27;re still honouring that and haven&#x27;t backtracked on any such promises for any of their models, so their track record is good in terms of keeping promisesI couldn&#x27;t find any mention that they backtracked on their promises, but if that were to be the case, that would indeed be a very strong disincentive to trust them on their commitment to the fp5 reply pimlottc 19 hours agoparentprevI was sad to see that parts for this new phone are also incompatible with the Fairphone 4 [0]. Granted, they do specically say \"repairable\", not \"upgradeable\", but it does kind of rub me the wrong way.0: https:&#x2F;&#x2F;thesonification.org&#x2F;2023&#x2F;09&#x2F;01&#x2F;fairphone-5-parts-won... reply msk-lywenn 19 hours agoparentprevI&#x27;d like to see a list of compatibility of parts between fairphone versions. I couldn&#x27;t find any Fairphone&#x27;s website. If there is nothing compatible, I really don&#x27;t see the point either. Like if I had a 4 and was only interested in the upgraded camera, why can&#x27;t I just put a camera from the 5 in the 4th? That would be actually game changing. reply Scandiravian 18 hours agorootparentThere were some prototypes like phonebloks (later named project Ara) ten years ago, which sadly was cancelledInterchangeable parts between models is really hard, since your support burden kind of explodes when you have to support several parts for several models, so it would probably not be sustainable for a smaller company like FairphoneWith that said, I remember a Fairphone representative saying they were looking into if they could provide upgrades for the FP5 down the line, as the longer lifespan of this model might makes it economically feasible for themI wouldn&#x27;t base any purchase on that until there&#x27;s an official announcement thoughPersonally I&#x27;m happy enough with the value proposition as it is to be getting myself an FP5 reply TylerE 18 hours agorootparentAlso, modular components are death to the size and power budgets. reply OJFord 19 hours agoparentprevI&#x27;d love to see Framework make a phone. I&#x27;m not sure I agree with your conclusion, it doesn&#x27;t mean not selling anyone anything for years, it just means selling upgraded&#x2F;alternative&#x2F;replacement parts instead? But the desire for it is pretty niche. reply Tade0 18 hours agorootparentSame.But I suspect that if it happened today it would have been detrimental to their ability to remain on the market. reply dustyharddrive 16 hours agoparentprevNitpick: iPhone 7 never got iOS 16 or 17, which means it won’t be fully patched (https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2022&#x2F;10&#x2F;apple-clarifies-secu...).Soon you can also expect Apple to stop providing both service and parts for it (https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT201624). reply RcouF1uZ4gsC 19 hours agoparentprevI have an inherent distrust of for profit business that uses virtuous adjectives:For example, Jessica Alba&#x27;s \"The Honest Company\".In the same way, I am suspicious of \"Fairphone\" as a for profit company. reply NateEag 18 hours agorootparentI have a similar gut reaction to companies using virtuous adjectives in their names.That said, if a company really does value doing things differently, what are they supposed to do?Not say so? Try not to make a profit, guaranteeing they won&#x27;t be able to make a positive impact?I think it&#x27;s important to not let the deceitful marketing in the world poison the well for any possibility of honest marketing.From what I can tell, Fairphone is working pretty hard to earn the name. reply garyrob 18 hours agoprevI must admit I&#x27;m having trouble relating to the strong desire to have a phone that&#x27;s more repairable than an iPhone.There have been a couple times I&#x27;ve had a problem with my iPhone under AppleCare. I&#x27;ve gone to my nearest Apple Store, and pretty quickly walked out with a new iPhone that works perfectly and contains all my data and, for all practical purposes, is exactly as if my old one never broke. That seems pretty repairable to me, on the axes I care about.It does mean I have to pay for AppleCare, but my guess is that on average you&#x27;re not going to get away much more cheaply with years of use of a more \"repairable\" phone.I&#x27;d be interested in seeing counterarguments. I&#x27;m not trying to dispute anyone&#x27;s claims about repairability. I just honestly don&#x27;t get it.[Update: the main thing I&#x27;m getting from responses is that people care about their being low-cost, easily repairable phones for people who can&#x27;t afford iPhones and Apple Care; the enthusiasm is empathy-based. That makes a lot of sense to me. I don&#x27;t know of a source for actually comparing long-term costs, though. Are you sure the repairable phones are actually cheaper to own, long-term? Arguments about the environment make less sense to me since the broken phone is refurbished and used again. I asked, and I got useful answers. Thanks, HN!I&#x27;ve gotten a lot of downvotes for this question, though, which mystifies me... why downvote an honest question? It seems unfriendly and uncharitable.] reply softfalcon 17 hours agoparentIt&#x27;s about making it accessible to everyone, not just you who can afford an iPhone and AppleCare.It creates jobs by letting independent repair shops legitimately fix and repair your devices. Small business is important to our economy. It also creates competition for service. Your AppleCare service is good... for now... how long before Apple becomes like IBM? Maybe it&#x27;ll never happen, but it might!Also, by making the iPhone repairable, it sets a standard (and possible legal backing) to make every device repairable. This applies to different phones, your laptop, your car, the speakers you bought.It also allows someone to buy a part and replace it instead of chucking the whole device when some small part breaks. This helps reduce e-waste and general landfill garbage.It also allows folks who come under tough times to just buy a knock-off battery and have their cousin install it until they get their next pay-cheque and can afford to take their phone in for Apple Care.You&#x27;re thinking only about yourself, what about people less privileged than yourself? What about people who rely on Apple to do the right thing so that their manufacturers follow suit (Android)? You can&#x27;t just trust a company will always be good, sometimes you have to enforce or push for good. reply isykt 15 hours agorootparent>people who can&#x27;t afford iPhones and Apple CareA refurbished iPhone 12 is $449. A brand new iPhone SE is $429, $17.87&#x2F;mo. for 24 months... or $11.91&#x2F;month from a carrier. AppleCare+ with Theft and Loss is $7.50&#x2F;month So you&#x27;re looking at ~$20&#x2F;month for a phone that Apple will replace instantly if it breaks or gets stolen. Is someone who doesn&#x27;t have access to credit really going to be able to afford a cheaper android phone and the tools to repair it and the parts to repair it? reply twiss 12 hours agorootparentFairphone offers a \"Fairphone Easy\" plan (in the Netherlands only, for now) for €14 per month for the 60 months plan (€19 per month for the 36 months plan), which includes repairs etc. So if you&#x27;re planning to keep it longer than 24 months, it&#x27;d be cheaper, yeah. reply raybb 10 hours agorootparentThat looks quite nice. I hope they offer the Rairphone 5 through that plan soon. Right now the page just has the 4 available.https:&#x2F;&#x2F;shop.fairphone.com&#x2F;fairphone-easy reply kube-system 17 hours agorootparentprevI supported a few fleets of smartphones back in the windows mobile and blackberry days, when they all had replaceable batteries.Do you know what people did when the batteries died? They overwhelmingly bought a new phone.Just because something is technically modular doesn&#x27;t mean that:1. other people know how to diagnose the problem2. other people want to spend time or effort repairing the phone3. other people don&#x27;t just want a new phone anywayWhile people on this forum are often power users or technically minded people who like spending mental cycles on their devices, this is not what other people are like. People who are purely users delegate the fixing of their devices to others. reply kibwen 17 hours agorootparent> back in the windows mobile and blackberry daysBack in the day, phones were improving so fast that upgrading often was naturally incentivized. These days, the tech has plateaued and phones are a commodity. How often does the average person replace their fridge? How often does the average person replace their hot water boiler? That&#x27;s how often I want to replace my phone; closer to every ten years than every two years. reply kube-system 17 hours agorootparentTrue, and people still do not repair their own fridge when it breaks. They either call the warranty line, an appliance repair shop, or an appliance store. reply AnthonyMouse 16 hours agorootparentThe device still has to be repairable in order for the repair shop to do it affordably, and the same is true even if you then choose to buy a new one, because an old appliance that can be repaired and resold has a higher trade in value. The lack of repairability costs you money whether you do the repairs yourself or not. reply lotsofpulp 16 hours agorootparentIn most of the US, I would bet that skilled labor willing and able to do the job of going to people’s homes to do appliance repairs will be expensive enough such that any ~$1,000 appliance just gets replaced.My brother in law just called his 3 year old washer’s extended warranty line, and they didn’t even want to verify the issue. Immediately told him they were sending him a check for the amount he purchased the washer for, and all that was left was for him to buy a new one.I imagine the labor itself costs $100 per hour, and including travel time, you are looking at $300 just to diagnose the issue. Then a couple hundred for the part, and if they have to wait for it, come back, that’s another $300. And you are already losing money.I imagine that is why Citibank got rid of their 4 year Costco appliance warranty. reply monocasa 15 hours agorootparentThat&#x27;s for the new stuff that isn&#x27;t designed to be repaired.My circa 1990s freezer broke a couple years ago. First guy quoted us \"not worth repairing\". Second guy did the job for two hours of $100&#x2F;hr labor and a $60 part. Told us that if it were a modern device it wouldn&#x27;t have been worth it because of the labor, but the freezer was old enough that everything that can break was designed to be easily and quickly repaired. reply nuancebydefault 14 hours agorootparentThe unfortunate irony in this story is that repairing stuff usually helps the environment, but in this case probably not, since older freezers consume quite a lot of energy, they were built when electricity was cheaper and using less optimized technology.That being said, I&#x27;m fully in favour of repairable phones, the energy cost is not such an issue for such low power device.I also think that todays dishwashers, washing machines, cooking plates, which are typically thrashed witin 10 years would benefit a lot from repairability if they are built to last. reply monocasa 13 hours agorootparentIt&#x27;s a hard to say, but it&#x27;s generally better to keep something at the consumer level running because of the cost of carbon manufacturing and shipping the product.Additionally I happen to live in an area with a very heavy use of renewables, so the carbon cost is even lower. reply AnthonyMouse 15 hours agorootparentprevBut now you&#x27;re just making the argument for repairing it yourself. How many people could make enough money in even a full day to recover the replacement cost of a $1000 appliance, rather than spending up to that long to fix it themselves?And the parts are only hundreds of dollars because the appliances aren&#x27;t repairable. Obviously if you have to replace a third of the appliance instead of just the bearing because that isn&#x27;t sold as a separate part, repairs will be much less cost effective. reply kube-system 15 hours agorootparentprevMainstream phones today can generally be repaired commercially, even if they are devices criticized as not being self-repairable.> the same is true even if you then choose to buy a new one, because an old appliance that can be repaired and resold has a higher trade in value. The lack of repairability costs you money whether you do the repairs yourself or not.TCO is a complicated subject. A more repairable device does not necessarily have a cheaper TCO. Some of the costs of building and&#x2F;or repairing a device scale linearly, and some scale exponentially. And they tend to depreciate logarithmically. Based solely on TCO, there is an optimum expected lifetime, and it mathematically isn&#x27;t \"as long as possible\" reply AnthonyMouse 14 hours agorootparent> Mainstream phones today can generally be repaired commercially, even if they are devices criticized as not being self-repairable.Doing this raises the labor cost to the point of making it uneconomical in many cases that it wouldn&#x27;t be otherwise.It also raises the parts cost (and the amount of waste), because a failed logic board that has every chip in the device soldered to it is going to cost more to replace than a card with only the bit that failed on it.> Some of the costs of building and&#x2F;or repairing a device scale linearly, and some scale exponentially.Which ones scale exponentially? How come a Fairphone isn&#x27;t dramatically more expensive than an iPhone?> And they tend to depreciate logarithmically.Depreciation on existing devices is precisely because they can&#x27;t be upgraded or repaired. If your phone has a 3G cellular modem in it, it stops being useful as a phone when they shut down the 3G towers -- unless you can replace the modem.Compare this to PC components that don&#x27;t depreciate quickly, like a standard ATX chassis or power supply or monitor. Or an AM4 system board that could have been purchased in 2016 yet supports 2022 CPUs that could viably still be in use for five or ten more years.And a decline in value of a component only makes the repairs more affordable. A Ryzen 7 2700X has lost half its value over 5 years, which only means that if you should need to replace one it costs half as much -- or can be replaced with a faster model in the same system. reply kube-system 14 hours agorootparent>> Some of the costs of building and&#x2F;or repairing a device scale linearly, and some scale exponentially.> Which ones scale exponentially? How come a Fairphone isn&#x27;t dramatically more expensive than an iPhone?Nothing that they&#x27;ve chosen. The Fairphone isn&#x27;t dramatically more expensive than other phones because it really isn&#x27;t dramatically different. They don&#x27;t have any modular parts that aren&#x27;t already available on other phones in various combinations. They are just offering a slightly different box of COTS parts with a promise to support it. This is relatively cheap to do. It still suffers from some of the problems you mention and more: the logic board is all soldered, the modem is not modular, the software support promises are reliant on third parties, etc. reply AnthonyMouse 11 hours agorootparentThey built something which is repairable but not upgradeable. The modular parts are the ones most likely to fail, not the ones most likely to become obsolete. But that doesn&#x27;t seem to have increased the cost by any significant degree, so why doesn&#x27;t everybody else do at least that?Meanwhile many PC laptops do make the upgradeable parts modular. Laptops with modular memory, CPU, wireless etc. are available for less than a Macbook or iPhone. So where is the exponential cost?> the software support promises are reliant on third partiesThis is a \"current vendors are crap\" problem rather than any kind of technological barrier.And that can&#x27;t be the reason that Apple can&#x27;t do it. reply kube-system 6 hours agorootparent> They built something which is repairable but not upgradeable. The modular parts are the ones most likely to fail, not the ones most likely to become obsolete. But that doesn&#x27;t seem to have increased the cost by any significant degree, so why doesn&#x27;t everybody else do at least that?It&#x27;s a few cents, and it makes packaging more difficult. If Samsung shaves a single penny off of each phone they sell, they are more a million dollars more profitable per year.But most mid-to-high end phones do have a significant number of modular parts. Battery, camera, speakers, displays, and sometimes IO are modular on many phones, and repair shops will fix these for customers.> Meanwhile many PC laptops do make the upgradeable parts modular. Laptops with modular memory, CPU, wireless etc. are available for less than a Macbook or iPhone.Modular CPUs are generally not available on laptops anymore, that&#x27;s a thing of the past anymore. Modular memory and wireless are starting to disappear, in part due to latency requirements for the later memory standards, and packaging and power requirements for popular thin-and-light segment devices.If you&#x27;re asking why phones don&#x27;t have modular memory, the answer is simple: they&#x27;re optimized for small size and low power, and adding a connector would compromise on that. Also, it would require inventing a new memory standard since modular LPDDR does not exist.> So where is the exponential cost?You see exponential costs if you look at devices that actually have extended lifetimes. I&#x27;m not talking about one consumer phone compared to another. In the scheme of things, they really all have expected lifetimes around a handful of years. Compare the redundancy and expected lifetime of consumer products to industrial products to aerospace products if you want to see how making something last a long time can be very expensive. If you want to double a product&#x27;s lifetime, you are often looking at 10x the cost, if not more.> This is a \"current vendors are crap\" problem rather than any kind of technological barrier.Well, it&#x27;s not possible to make a phone without relying on vendors for some parts. There are too many specialized parts. Not even Apple has enough resources to in-house it all, and they have been trying. reply coryrc 7 hours agorootparentprev> Doing this raises the labor cost to the point of making it uneconomical in many cases that it wouldn&#x27;t be otherwise.You&#x27;re arguing against the specialization of labor. I don&#x27;t think this is a winning argument. replysoftfalcon 17 hours agorootparentprevI think you&#x27;re applying a metric of well-to-do corporate workers over the needs and wants of folks who are less fortunate and don&#x27;t work for a big company like you clearly did.What about poor folks just trying to keep their phone running and quite literally can&#x27;t afford to buy a new phone because they live paycheck to paycheck?Or lets look at it more self-serving from your corporate perspective. Say, your company is coming under hard times. You need to find places to cut. With repairability you can opt to just replace batteries, no upgrading, save some cash across your fleet of thousands of phones. Your boss is happy and you save money to survive another day.This stuff doesn&#x27;t have to only be for power users, that&#x27;s just how it&#x27;s marketed to you. reply kube-system 17 hours agorootparent> I think you&#x27;re applying a metric of well-to-do corporate workers over the needs and wants of folks who are less fortunate and don&#x27;t work for a big company like you clearly did.And you think the less fortunate are going to buy a niche $730 phone just so they can buy a $105 screen for it later?No, they&#x27;re going down to the Boost&#x2F;Metro&#x2F;Walmart&#x2F;DG and picking up a whole new BLU, TCL or Moto G for less than $100. reply softfalcon 16 hours agorootparentNo. I don&#x27;t think they&#x27;re going to buy a niche phone. I&#x27;m advocating for repair-ability in general. All of my comments have been in response to the original comment I replied to, which was about AppleCare and iPhones.Also, as for poor folks. Many of them buy iPhones. They do this because appearing poor with a cheap phone from Walmart hampers your career trajectory. These folks are essentially \"faking it til you make it\". This isn&#x27;t even wrong, it&#x27;s just a fact of the way society perceives people. Unfortunately, folks look down on you if you don&#x27;t own a Samsung or iPhone and wonder why you have a budget flip phone.These very people are those who would benefit from affordable phone repair. They can easily replace a battery or a screen without paying another $1000+ for an iPhone. This helps them keep up appearances while saving money. I know many, many folks like this. It&#x27;s important we fight for these repair options to help all of us out. reply kube-system 16 hours agorootparentYes, there is no denying that self-service repair would help the people who are willing and able to do so.However, I&#x27;m just saying that market is not incentivized to do so. The BOM cost would be too high and the volume too low for it to be a cheap phone to begin with, so the affordability angle is a non-starter. Cheap high-volume glued-together phones will always be cheaper.If people really can&#x27;t afford a screen replacement at a repair shop, the market will sell them aCheap high-volume glued-together phones will always be cheaper.I appreciate your cost focused perspective on this. I really do. What I&#x27;m trying to argue is that it is necessary to stop doing this. I foresee that just like the EU forcing USB-C into the iPhone, we could also see more repair-ability to combat climate issues, e-waste, etc.The world is going to have a hard pill to swallow, not everything is about profits. Sometimes things need to be done because they are good for us and our society.> If people want an iPhone, then the Fairphone is not a solution to that problem either.I don&#x27;t think the Fairphone is necessarily the solution either. That was never what I was arguing about. I&#x27;d appreciate it if you&#x27;d stop bringing it up. I&#x27;m not talking about the Fairphone. I&#x27;m explicitly talking about why an iPhone user might care about repair-ability. reply kube-system 15 hours agorootparentCost is a good proxy for resources expended. How many devices exactly are thrown away due to damage vs being EOL&#x27;d for other reasons, like vanity, or other obsolescence? Repairability does not equal infinite product lifetime, however it does increase material on the BOM and increase the energy required to manufacture the device. Are you sure that those additional increases in material and energy would be offset by a longer observed lifetime of the device?I mean, I&#x27;m not against repairability laws as a matter of consumer protection, but I think it&#x27;s pure conjecture to jump to the conclusion that it is undoubtably better for the environment.If you add 10% of material to the average device to gain 5% average lifetime, you&#x27;re not decreasing waste. I don&#x27;t think it would add any lifetime personally, as repair and refurbishment shops already know how to open phones that are glued together and there is a huge industry that refurbs these phones already. I doubt screws will increase the average lifetime of the average phone, it&#x27;ll just increase the number of screws in the landfill. reply nuancebydefault 13 hours agorootparentThe thing is, cheap phones are more expensive in the longer term.Legislation should set a minimum on quality, repairability helps on setting quality standards. This goes for many appliances as well as phones.I&#x27;d rather have iron screws in landfills than extra glue. reply AnthonyMouse 16 hours agorootparentprevIf people can&#x27;t be seen with a cheap phone, a $730 Fairphone will satisfy the \"people can see you paid a premium for this\" bar, but then if a piece of it breaks the cost is $105 instead of another $730. reply kube-system 15 hours agorootparent> a $730 Fairphone will satisfy the \"people can see you paid a premium for this\" barDoes it? I don&#x27;t think most people have even heard of it. reply AnthonyMouse 15 hours agorootparentIt looks like it&#x27;s made of quality materials (because it is), if anyone looks it up they can see that it isn&#x27;t a cheap device, if anyone asks you about it you can earn status points by telling them how much you care about the environment etc. reply kube-system 15 hours agorootparentI&#x27;m honestly not intending this to sound snarky, but this sounds like something someone with a green bubble would say. If those things matter to a person&#x27;s audience, they weren&#x27;t the iPhone crowd anyway. reply AnthonyMouse 14 hours agorootparentThat&#x27;s not snark, it&#x27;s pomp.And it&#x27;s the exact reason that people keep wanting Apple to make a repairable phone. Someone else can do it, and do a fine job, and you say \"green bubbles\" and turn up your nose. Well then, where&#x27;s the repairable phone with blue bubbles, pray tell?You&#x27;re dangerously close to an admission that \"the iPhone crowd\" wants the logo as a status symbol and excluding underprivileged aspirants by increasing the cost of entry is the point. reply kube-system 13 hours agorootparent> you say \"green bubbles\" and turn up your nose.I&#x27;m not doing it, I&#x27;m saying that the crowd that cares about iPhone brand image, cares about iPhone brand image.> You&#x27;re dangerously close to an admission that \"the iPhone crowd\" wants the logo as a status symbol and excluding underprivileged aspirants by increasing the cost of entry is the point.I don&#x27;t know why you think I would pretend otherwise. Apple&#x27;s brand image is as a premium product, and Apple has been openly hostile to unauthorized repair for the stated reason that many aftermarket parts are of lower quality than OEM parts. Apple obviously doesn&#x27;t want low quality refurbs floating around and diluting their brand image.e.g.: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT210323 reply AnthonyMouse 12 hours agorootparent> I&#x27;m not doing it, I&#x27;m saying that the crowd that cares about iPhone brand image, cares about iPhone brand image.The company obviously has a bit of a cult following, but the point is that there are crowds who would have no objection to any premium phone whether it be iOS or Android but would put you in the outgroup if you showed up with a flip phone or anything else that costApple&#x27;s brand image is as a premium product, and Apple has been openly hostile to unauthorized repair for the stated reason that many aftermarket parts are of lower quality than OEM parts. Apple obviously doesn&#x27;t want low quality refurbs floating around and diluting their brand image.That is indeed their stated reason, and yet Toyota maintains one of the highest brand ratings for reliability even while anyone can have their Prius repaired at an independent shop with third party parts. reply PrimeMcFly 10 hours agorootparentprev> You&#x27;re dangerously close to an admission that \"the iPhone crowd\" wants the logo as a status symbolThat&#x27;s all it is in the US and has been for a long time.Android was superior for the longest time. I was shocked when I used a friend&#x27;s iphone and saw they couldn&#x27;t create folders to organize apps, didn&#x27;t have widgets, etc.People in the US just want to be in the club, which is where the ridiculous &#x27;green bubble&#x27; snobbery comes from. replynuancebydefault 13 hours agorootparentprevI think it is better to buy a 500 dollar phone with replaceable parts than a sub 100 that either does not function well from start or starts to deteriorate (typically less quality and glued battery or under dimensioned charger) within a year.It does not need to be a 1000 dollar iphone to consist of quality parts. We can wait for 5G to become main stream before requiring it. reply pritambaral 17 hours agorootparentprev> ... fleets of smartphones back in the windows mobile and blackberry days ...In a completely unrelated world, where smartphones were not prohibitively expensive but still expensive enough to be a rare purchase, people in my country repaired old phones by buying new batteries. Those used to cost only a quarter to a half of the price of a new smartphone, and street-side cellphone accessories shops used to carry both original, branded batteries and cheap, knockoff batteries. The knockoffs were only about $10-$20 cheaper, and people were smart enough to know the difference it&#x27;d make. reply iopq 17 hours agorootparentprevBack in blackberry days a new phone would support stuff that wasn&#x27;t even dreamed of on the previous device. Do people not remember the original iPhone didn&#x27;t have an app store?Back then things moved so fast reply kube-system 16 hours agorootparentPoints 1 and 2 still apply. People outside of forums like this don&#x27;t care to fix their own phone. They don&#x27;t care if you can open it with a screwdriver or not, they&#x27;re not going to open it either way. reply nomel 15 hours agorootparentprev> not just you who can afford an iPhone and AppleCare.I’m not sure I understand. An iPhone SE is cheaper than a fairphone, with longer support and far better specs. From a quick search, third party knock off replacement screens and battery replacement kits are cheaper for an iPhone, I assume due to the orders of magnitude difference in scale.The math isn’t working, from my simple perspective. reply softfalcon 10 hours agorootparentThe person was asking about why they should care about repair-ability as an iPhone user. I wasn&#x27;t trying to do a cost-benefit analysis of iPhone vs FairPhone.I personally don&#x27;t really care what FairPhone does. I care about general repair-ability of all tech and what benefits that has for flexibility, affordability, and competition in the economy and for the end user. reply Derbasti 16 hours agoparentprevI had a Fairphone 2 in Germany (where the next Apple Store was four hours away). I dropped it onto the metal blades of our radiator, which shattered the screen. I was due to go on vacation in two days.So I express-ordered the replacement screen from Fairphone, which arrived the next day, case closed. (A similar thing happened with a Pixel 4a a few years later, which had to be mailed in, and took a week.)My wife also had a Fairphone 2, and her headphone jack was a bit flakey. So after a few months we ordered a replacement module, which fixed the problem (for €30!). Afterwards, I emailed Fairphone as this issue had existed from the start. They refunded us the module.And I also fondly remember the updated camera module they sold for the Fairphone 2.Alas, these phones are too big for my tastes, which is why I no longer have one. reply garyrob 16 hours agorootparentAnother helpful response, thanks! I tried to mention it in an update to my original question, but that&#x27;s now closed to editing. reply SECProto 17 hours agoparentprev\"AppleCare+\" costs more than my phone over a 2-3 year period, and it sounds like you still have to pay for screen replacement? reply PrimeMcFly 4 hours agorootparentThe more money people have the more sense seems to go out the window. reply rldjbpin 1 hour agoparentprevregarding the price, it is possible to replicate a third-party solution for a cheaper phone but with some compromise.can buy a budget phone and buy third party damage insurace (unsure how many scenarios they support and real-world experience) instead of this and compare the costs associated.i won&#x27;t be surprised if that makes more sense to more people than not, besides the abysmal software support of course. reply gherkinnn 18 hours agoparentprevAs you just said. You walked out with a new phone. Imagine if you didn’t have to replace an entire phone over a broken triviality. reply biggc 17 hours agorootparentIt’s usually not a new phone, it’s refurbished. Apple will then refurbish the phone you came in with and use it to replace someone else’s broken phone reply danieldk 16 hours agorootparentprevI only had an issue with an iPhone once (yellowing around the edges of the display), I took it to the Apple Store and they replaced the screen. Probably the bulk of repairs (display, battery, shattered glass) don&#x27;t entail a full replacement. reply garyrob 17 hours agorootparentprev> Imagine if you didn’t have to replace an entire phone over a broken triviality.I can imagine it. It wouldn&#x27;t make the slightest difference to my practical experience. And then, as another commenter mentioned, my phone is refurbished and used for someone else. reply fsflover 12 hours agorootparentIt would make a big difference for the used natural resources though. reply graypegg 18 hours agoparentprevAppleCare is a great service. I’ve had the exact same experience!The problem is then also ensuring all other options are artificially worse than AppleCare. Your local “mall guy” is just as capable of swapping out a battery, in theory. reply lucideer 9 hours agoparentprev> I must admit I&#x27;m having trouble relating to the strong desire to have a phone that&#x27;s more repairable than an iPhone.I have a Fairphone for one reason & one reason only: it&#x27;s not repairability, environmentally friendly material sourcing, nor software freedom. It&#x27;s wages. They pay living wages in Fairphone factories.Fwiw, that&#x27;s also the substantive contributor to the price bump. reply comte7092 16 hours agoparentprevNot everyone is close to an Apple Store, especially those outside of the US.There are only 2 stores in all of India, for example. Even in Western Europe, depending on your country, there might not even be one. reply callalex 17 hours agoparentprevIt is insanely expensive. Remember that the median household income in the US is less than $70k before taxes. That means half the people are making less than that, sometimes very significantly less than that. I assume your household makes more than double that, correct?And that’s just the United States, which is very high income compared to most of the world. reply garyrob 17 hours agorootparentYes. I can see that having inexpensive, repairable devices could be good for people who can&#x27;t afford iPhones or AppleCare, whether in the US or elsewhere.What I&#x27;m wondering about is the enthusiasm for them among the HN crowd. Maybe many in the HN crowd are still students, or \"Ramen-profitable\"...? reply callalex 17 hours agorootparentOr posses empathy and want technology to work for everybody. reply Scandiravian 17 hours agorootparentprevI need a phone to be able to perform every day things like 2fa for my banking, just like I need other things like a fridge for my foodI want to the keep the cost per year for these things as low as possible while fulfilling my base requirements. For the phone that&#x27;s no vendor lock-in, cheap repairs, and ability to play YouTube, browse emails, and check the newsThe Fairphone is the best option for me. It does what it needs to and is reasonably cheap per year, so I can spend that money on things I actually care about reply garyrob 15 hours agorootparentNot arguing with you, but asking: have you compared the price of the cheapest iPhone + AppleCare with the price of a corresponding Fairphone plus the cost of parts for likely repairs? reply Scandiravian 14 hours agorootparentYes. The Fairphone wins hands down. The cheapest iPhone has six years of remaining software updates and cost approximately $590 where I live. Thats a little more than $98 per yearApple care+ is approximately $40 per year plus another $80 for each repair due to \"accidents\" (unless it&#x27;s the screen or case in which case it&#x27;s $30)That&#x27;s a minimum cost of $138 per year if I don&#x27;t suffer any malfunctions or accidentsThe Fairphone is approximately $750 with at least 8 years of support. That&#x27;s a minimum cost of $93.75 per year. That means that I&#x27;d have to crack my screen every 18 months or so for the iPhone to be more price competitive. Given that I&#x27;ve never cracked a screen that seems highly unlikelyFor the battery I can replace it every 11 months and still spend less money with the Fairphone (plus I get to keep the old battery as a backup)Any other part breaking is a massive expense on the iPhone. Not including the screen, every spare part for the Fairphone is less than the service fee with apple careIf Fairphone ends up providing 10 years of support for the device, that just pushes the math even more in its favourAnd all that is not even considering that the Fairphone comes with a 5 year warranty which would already cover any non-accident&#x2F;wear+tear malfunctions reply callalex 14 hours agorootparentprevI’m going to butt in and say that yes, I agree with you that Fairphone hasn’t completely succeeded in the goals they have set out. The price is definitely still a problem, but at least they are trying. reply PrimeMcFly 4 hours agorootparentprevI just had a discussion with an Apple fanatic on HN where I tried to explain that to him, in response to him claiming people in the US make more money than most other countries which is why everyone buys luxury iphones. reply oever 17 hours agoparentprev> a new iPhone that works perfectly and contains all my data and, for all practical purposes, is exactly as if my old one never broke.That is a very compelling convenience that all (phone) OS-es should strive for. I try out different phones and operating systems, but syncing the data and settings via a self-hosted device are not convenient yet.`adb backup` will give me an archive but it&#x27;s a deprecated command and so far I&#x27;ve never been able to restore data with it.A system like NixOS is getting the applications in exactly the same version and configuration, but the same rigor is not there for the associated data.Ideally, I&#x27;d be able to run a plug-and-play server at home, link it to my phone and have it take backups without ever being visible until there&#x27;s a problem with running the backups. Then, when the phone needs to be replaced, link it the new phone and continue where the old one left off, including all contacts and contact history. reply twism 17 hours agorootparentWhat? I just got a pixel fold and during the setup it asks to connect my old device (pixel 5). After a few minutes I have a carbon copy of my old device save a few apps I sideloaded. reply oever 16 hours agorootparentThat sounds like a smooth experience. It would be great if FOSS operating systems would work just as well.I&#x27;m assuming you were going from functional pixel 5 to a pixel fold and that both were running a Android with a lot of proprietary software that allows Google to track you so they convince you to buy the products of the customers of their advertising business.It&#x27;s hard for competitors to offer an equally smooth experience.Would the data transfer worked as well if the original phone was broken or stolen? reply eptcyka 17 hours agoparentprevWhy should Apple have a monopoly on repairing a device you own? reply december456 18 hours agoparentprevIts completely the freedom aspect for me. Sure, Apple wont let me down because that would impact their business, but what if they did? Its also a thousand times better to be 100% sure you can modify your electronics without any fear of getting rejected like its vehicle insurance. reply GuB-42 10 hours agoparentprevI actually want a durable phone more than a repairable phone. No need to repair a phone that doesn&#x27;t break. But there is one exception, and that&#x27;s the battery. Phone batteries always fail at some point, they are consumables, for chemical reasons.A user replaceable battery makes it much less of a problem. Even with non-standard form factors (a 18650 is not very practical for a phone), even if the manufacturer drop supports, someone will make a compatible battery. Plus, you get the option of backup batteries.So I don&#x27;t really care about all the fuss about the Fairphone, except with that one feature that was once the norm, and that&#x27;s the removable battery. Hopefully in a few years, it will be the norm again thanks to EU regulations, but I am not holding my breath for it. reply amomchilov 7 hours agoparentprevIt’s a great user experience if you fall into the happy path. If you don’t (water damage on a MacBook, older than 3 year iPhone, …) it’s preposterously expensive and impractical.At that point, you don’t have any third party alternatives, because of all the intentional anti-competitive measures they take to snuff them out. reply Nullabillity 13 hours agoparentprevI replaced the battery of my FairPhone last night.I didn&#x27;t need to make an appointment. I didn&#x27;t need to travel. I didn&#x27;t need to have yet another overpriced subscription. I didn&#x27;t need to do it within business hours. I didn&#x27;t need to get a massive pelican box of equipment shipped to me. I didn&#x27;t need to make any expensive deposits. And I didn&#x27;t need to beg anyone to please please please let me use the thing I bought. reply palata 15 hours agoparentprev> I must admit I&#x27;m having trouble relating to the strong desire to have a phone that&#x27;s more repairable than an iPhone.It&#x27;s good to have competition, and not many Android phones get 5 years of warranty like the new Fairphones.iPhones are great in that they get security updates for a long time, and they can be repaired if you can afford it. Fairphone is just a different, cheaper option. reply iopq 17 hours agoparentprevThe counterargument is that you have to pay for AppleCare. I don&#x27;t pay for anything for my phoneFrom another standpoint is that you&#x27;re adding to a pile of e-waste reply garyrob 16 hours agorootparent\"The counterargument is that you have to pay for AppleCare. I don&#x27;t pay for anything for my phone \"But you have to pay for the new parts, don&#x27;t you? What I&#x27;m saying is that I don&#x27;t know of a head-to-head comparison of long-term maintenance of a Fairphone, including buying parts to fix it, compared to buying the cheapest iPhone and Apple Care.And when they give you a new iPhone, they refurbish the old, (and the \"new\" one they give you is actually refurbished), so I don&#x27;t see how it adds to e-waste. reply iopq 15 hours agorootparentI bought a screen for $70 for an iPhone third-party screen and paid some guy in China $10 to fix it. You probably pay $199 for two years, yet they still charge you $29 for it reply schnuri 18 hours agoparentprevIt’s more about sustainability. reply Klonoar 18 hours agorootparentI mean... no. Apple publishes breakdowns&#x2F;reports of their recycling efforts and it&#x27;s pretty damn good for what it is, and these devices now last a very long time if you don&#x27;t feel like chasing the yearly upgrade train.The selling point of the Fairphone is moreso that it&#x27;s:- Attempting to pay fair wages to workers- Not Apple, for those who just don&#x27;t want an Apple device- More (user) repair-ableThese are all very valid reasons to want to use something like the Fairphone and I am glad it exists, and it might even be my next phone - but to imply that it wins out over the iPhone trade-in-and-get-it-recycled flow is just bonkers. reply comte7092 16 hours agorootparentMore third party repairable as well.>to imply that it wins out over the iPhone trade-in-and-get-it-recycled flow is just bonkers.I made this point in another comment already, but for many people, it isn’t easy to access an Apple Store, and Apple is notoriously stingy with third party authorized repair reply Klonoar 16 hours agorootparent> Phone trade-in-and-get-it-recycled flowI&#x27;m not talking about the Apple Care&#x2F;App Store get-it-fixed flow, I am talking about how when you buy a new iPhone you can quite literally just mail your (hopefully data-cleansed) phone to Apple and they&#x27;ll recycle it properly. If you&#x27;re buying your phone through Apple - which you should do, just because phone carriers are ripoffs - then you generally get a credit back to boot.For the average consumer this is great and encourages sustainable reuse of the core materials throughout the iPhone lifecycle and generally means there is no good reason for iPhones to bloat up landfills or anything. reply AnthonyMouse 15 hours agorootparent> For the average consumer this is great and encourages sustainable reuse of the core materials throughout the iPhone lifecycle and generally means there is no good reason for iPhones to bloat up landfills or anything.But they do though.A lot of this stuff is corporate PR. For example, recycling aluminum is the default -- it costs less than mining it. You don&#x27;t get credit for that, it&#x27;s saving you money and you were going to do it anyway. And the reason it&#x27;s so cheap is that you can source it from things that are basically pure aluminum, like aluminum cans, even though they&#x27;re then making something out of it that has to go through a complicated process to separate the diverse materials from each other again.They have a recycling program for their old devices, but to use it you have to buy a new one. And there&#x27;s a reason for that -- it&#x27;s not otherwise cost effective to do it. It&#x27;s a promotion to drive new sales, because the process is complicated and inefficient.Because they&#x27;re trying to turn the device back into raw materials after gluing and soldering them all together. Which is why they go into the landfill unless someone is subsidizing it.Whereas the best way to \"recycle\" a piece of electronics is to continue using it as a piece of electronics. Allow the memory or storage to be upgraded to extend its usable life. Have modular parts to minimize the materials necessary to replace before it can go back into service after being damaged, and minimize the cost of such repairs to increase the number of repairs that are economical before a new device has to be manufactured. reply Klonoar 12 hours agorootparent> It&#x27;s a promotion to drive new sales, because the process is complicated and inefficient.There is no reason it cannot be both reasons and I find your take overly and needlessly cynical. reply AnthonyMouse 12 hours agorootparentIt isn&#x27;t both because they do PR about building a robot that can disassemble iPhones, but there are only two of them in the world and even if they were run nonstop they could only recycle 1% of the iPhones Apple manufactures in the same amount of time.In the meantime most of the iPhones people trade in are sent to third party \"recyclers\" who don&#x27;t recover nearly as much of the materials and are contractually required to shred the devices without recovering functional parts for reuse, even though that would reduce the amount of ewaste by several fold -- once for the device already on its way for the shredder, and again for each of the devices that could have been repaired from its operational parts.It&#x27;s a cynical take because it&#x27;s a cynical marketing ploy. replyme_jumper 16 hours agorootparentprevBut it&#x27;s not only about the sustainability of the existing phone, Fairphone also tries to be as sustainable as possible with the sourced materials. reply Klonoar 16 hours agorootparent...and Apple doesn&#x27;t?https:&#x2F;&#x2F;www.apple.com&#x2F;environment&#x2F;pdf&#x2F;Apple_Environmental_Pr...I swear it&#x27;s like nobody reads the actual reports that they put out. There is a lot to knock about the Apple&#x2F;iPhone experience but frankly sustainability just isn&#x27;t it.And I will note again, to be clear, that I still think the Fairphone is a good thing. I just don&#x27;t think it should be held as the highest regard when it&#x27;s not. reply PrimeMcFly 4 hours agorootparentprev> and these devices now last a very long time if you don&#x27;t feel like chasing the yearly upgrade train.But they constantly make new devices with incremental improvements and advertise&#x2F;market for people to upgrade when they don&#x27;t need to - and people do exactly that. reply m0llusk 17 hours agoparentprevFor some convenience is a factor. Apple stores tend to be in crowded urban areas or fancy shopping malls. In less central locations small operators and components shipped directly may be competitive offerings. reply PrimeMcFly 10 hours agoparentprev> I must admit I&#x27;m having trouble relating to the strong desire to have a phone that&#x27;s more repairable than an iPhone.It&#x27;s about control. You&#x27;re fine being in a walled garden and letting the company handle every aspect of the device for you.Others don&#x27;t like being locked in and appreciate the freedom to fix or change things on their own. reply Scandiravian 19 hours agoprevI understand why some people don&#x27;t want to pay this amount of money upfront based on the specs alone. From a \"per year\" cost I still think it&#x27;s competitiveIf they really manage to support this device for 10 years, that&#x27;s $75 per year, which I think is relatively cheapMy use-case for a phone does not need a lot of processing power. I use it for messaging, checking emails, watch a bit of YouTube, and catch up on the news. I feel reasonably confident that it will be able to perform those tasks 8-10 years down the lineOn top of that I also appreciate that I can buy a device that is sourced with consideration for workers and the environment. It&#x27;s never good for the environment to produce new devices, but the option to limit the impact is important to me reply truculent 21 minutes agoparentHow does that compare with a used&#x2F;refurbished iPhone? reply queuebert 19 hours agoparentprevIt would be even cheaper comparatively if we were properly taxed for the downstream effects of our waste. reply Scandiravian 18 hours agorootparentAgreed. Though I&#x27;m personally more in favour of adding the cost at the other end. I think companies should be taxed for the cost of restoring the damage their products cause to the environment, as this would encourage competition towards more sustainable practices reply freedomben 18 hours agorootparentPhilosophically, I agree with you completely. But practically, how would we measure that let alone enforce it? reply Scandiravian 18 hours agorootparentI think it&#x27;s still hard to say which policies would be effective. I think CO2-equivalent emission taxes are a good starting pointIt might also be possible to put a recycling fee on each produced device based on the retail price and the amount of damage it would cause to the environment if the device was dumped in the environmentWhen a device is then handed in for recycling the company is refunded the fee minus the cost of to handle it at the recycling facilityOn the downside that could also encourage companies to create products that are replaced before they are no longer usable, though honestly we already have that problem. It could maybe be prevented by lowering the fee based on how long the company commit to supporting the model with software and replacement parts reply queuebert 17 hours agorootparentprevIf we can measure the near infinitesimal gravitational waves from black holes colliding across the universe, surely we can at least get close to measuring this. reply wolverine876 15 hours agorootparentNobody loses money when you measure infinitesimal gravitational waves from black holes colliding across the universe. If it affected climate change, it would be a different story - the conspiracy of astrophysicists who also traffic baby goats, Einstein was a hoax, black holes aren&#x27;t settled science, why don&#x27;t we measure gravity waves in America&#x2F;UK rather than foreign gravity waves, etc. replyTylerE 18 hours agoparentprevIt only makes sense to treat it as \"10 years\" if it&#x27;s actually useable for 10. For me, the specs make it a non-starter on day 1. reply Scandiravian 17 hours agorootparentSure, if your use-case requires a lot of computational power it might be a bad fit, but as I also wrote in my post, my requirements are pretty light, in which case this phone is closer to perfect than any other I&#x27;ve come across reply TylerE 17 hours agorootparentIf your needs are that light, how do justify spending that much on a phone? Why not just get whatever shitty android is on sale for $free.50? reply rakoo 16 hours agorootparentGiving Fairphone money has a much higher chance of turning the whole industry than supporting shitty companies. If it&#x27;s shit you&#x27;re probably going to buy another one next year anyway, and in the end spend a similar amount over 10 years reply Scandiravian 16 hours agorootparentprev- Price wise it&#x27;s still competitive if looking at the cost per year- I don&#x27;t want to spend time and energy having to figure out which phone I should replace it with when the shitty phone dies after 12 months- I care about the working conditions for the people actually assembling my phone- I care about the impact my consumption has on the environment reply charcircuit 18 hours agoparentprev>that&#x27;s $75 per year, I still think it&#x27;s competitiveIt&#x27;s more than what it would be for an iphone reply Scandiravian 17 hours agorootparentThe lowest cost iPhone SE 3rd gen I can find on Apple&#x27;s website is €549 with 7 years of software updatesThe fp5 on Fairphones website is €529 with at least 8 years of software updatesEven if iPhones were marginally cheaper, they carry with them vendor lock-in, poor repairability, and a track record of horrendous working conditions for the workers assembling themTo me it would still be worth a small premium to avoid those drawbacks even if the fp5 was more expensive per year reply nailuj 15 hours agorootparentThe FP5 costs 699€ on the Fairphone website, so the premium is not that small reply Scandiravian 15 hours agorootparentFor that price you also get 4x the storage of the cheapest iPhone SE. For comparable storage the iPhone is €749Comparing the cheapest iPhone with the FP5, the latter is still cheaper per year, even if I never have to replace anything (such as the battery)https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37756346 replyrldjbpin 1 hour agoprevfrom all the discussions on fp5 here, i can definitely say that they are catering to a very tough crowd.it is a shame that they still don&#x27;t seem to have found a rythmn in long-term hardware support after all these years.given how fast the rest of industry moves between the infrequent releases, each new model ends up being so different from the previous one. so it is hard to see a future where they can reuse older parts and hence be more likely to sell them for longer.i had a laptop that was easy to repair, but i could not find its replacement battery within two years of its release, which defeated the purpose of having that ability. if existing userbase are having trust issues with fairphone&#x27;s parts supply long-term, it will be a hard sell for many who can put up with the phone&#x27;s tradeoffs in the name of repairability.i do hope for their success so that at least third parties can support the phone for the near future. maybe it will be there for my next-to-next upgrade. reply bullen 18 hours agoprevWe need vanilla linux, Android is poisoning the well.Pinetab-V is the only portable device that seems to work in the right direction.The GPU driver is still broken though so until then don&#x27;t get your hope up.And I know that phone != tablet... but really soon I will not need my phone. reply jlkuester7 15 hours agoparentMeanwhile in the real world, true convergence seems to mostly break down at the application layer. Many desktop Linux apps are not designed to work on a mobile device. It turns out that the UX in Android apps is purpose build for mobile use. This software gap seems really difficult to overcome. Not impossible, but we are not there yet in terms of mobile Linux devices being useful for the majority of consumers.... (I do think Waydroid offers some interesting opportunities here to bridge the gap!) reply bullen 14 hours agorootparentThis is only a problem if you think the mobile has value.To me it&#x27;s a consume only device.Much rather have a tablet (with a keyboard and mouse for making real world applications, headset for audio) and a Pebble watch for notifications.The iPad still can&#x27;t run MacOS...And then you have Ubuntu Touch. reply xctr94 16 hours agoparentprevI agree, but first we’d need vanilla Linux working very well on laptops. If we can’t fix that, I don’t see how phones will fare better, what with their crazy amount of components and non-OSS compatible vendors. reply bullen 14 hours agorootparentI think specific devices will have more likely positive outcome over time as chips don&#x27;t become more performant = the design wont be obsolete and still sell many years later.Modular devices (laptops are a mix of many modems, displays, sound, disk etc.) have had more edge case bugs because the volume is too low per generation to warrant fixes. reply megous 13 hours agorootparentprevCompared to laptops, smartphones have more sensors, and more complicated camera setup. That&#x27;s about it. They&#x27;re not significantly more complicated than laptops. reply fsflover 15 hours agorootparentprevUsing Linux on laptops for 10+ years without any problem. Just buy it preinstalled. reply teva 17 hours agoparentprevThey sell the F4 with &#x2F;e&#x2F;OS reply dustyharddrive 15 hours agorootparentThat’s an Android ROM and can’t provide the longevity of mainline Linux. reply fsflover 15 hours agoparentprevYou may be interested in GNU&#x2F;Linux phones, Librem 5 and Pinephone. reply bullen 14 hours agorootparentBoth lack OpenGL ES 3! reply fsflover 13 hours agorootparentWould you care to elaborate, which practical consequences it has, apart from poor performance in 3D games? reply bullen 13 hours agorootparentOpenGL (ES) 3 has VAO, which IMO is the last GL feature.You need it to build 3D MMOs that can run on low power devices.Without the MMO things are not going to be fun when KWh > $1... replyctenb 19 hours agoprevNot going to buy, because I anticipate remaining happy with my FP 4 for several years :) reply jlkuester7 15 hours agoparentThis, to me, feels like the most important comment on the page! It is great to develop a sweet new offering, but if one of the main selling points of your product is longevity, repair-ability, and long-term support, then I don&#x27;t want to hear about all the Fairphone fans jumping up to by the next gen. I want a phone that folks are happy to stick with for years!As a side note, for years I used a Nexus 6. Great phone (the battery was getting sketchy, but the rest of the hardware held up well for 7 years of use)! With LineageOS, I was getting a steady stream of software updates. I was finally forced off of the phone when they killed the 3G networks here in the USA (and apparently the Nexus 6 did not support VoLTE....). reply xorcist 18 hours agoprevNo 3.5mm plug? Not for me then.I never want to charge an extra device again. I also never want to find a pair of plugs that fit me, again. It is a mystery to me why this use case is apparently so unique that there is no market for me. reply jlkuester7 15 hours agoparentI also am a big fan of the dedicated 3.5mm! That being said, it is definitely true that I am running out of situations where I can actually use that port on my device... sigh reply sambazi 3 hours agorootparenti tend to use it every time i don&#x27;t want to fiddle with bluetooth reply wilsonnb3 17 hours agoparentprevBecause you can use a USB c adapter if you still want to use wired headphones reply _Algernon_ 16 hours agorootparentI can only imagine the extra strain having a dongle plugged into your phone puts on the port with the phone being in your pocket. Say what you will about 3.5mm, but it is robust in that scenario. I&#x27;ve never had a 3.5mm port break before the cable that is plugged in.USB-C ports wear out within a couple of years from just slight strain on a charging cable.This is not the solution you claim it to be. reply bigstrat2003 17 hours agorootparentprevWhich is an abjectly terrible solution. Dongles can be lost or forgotten, and you can&#x27;t charge your phone while using one (a very big deal if your use case is to hook the phone to your car for a long drive). reply stronglikedan 16 hours agorootparentWhy would you even disconnect your 2-in-1 dongle, that allows you to charge your phone while using the 3.5 mm jack, from your headphones wire to begin with? Or disconnect your second 2-in-1 dongle, which you got because they&#x27;re extremely cheap, from your car adapter? I think it&#x27;s a reasonably good solution for the relatively few people that still use a 3.5 mm jack. reply bigstrat2003 14 hours agorootparentI was not aware that such a thing existed. I&#x27;ve only ever seen dongles which don&#x27;t let you charge. So thanks for the info, that&#x27;s good to know at least.As to why you&#x27;d disconnect - shit happens. Maybe you have an iPhone and the other person using your car has an Android phone, so you each need different dongles. Maybe someone accidentally removed it, who knows? The point is that dongles can, and will, get lost or misplaced so they aren&#x27;t a good solution. reply monocasa 15 hours agorootparentprevThe audio jacks are normally on the top of the phone, the USB is normally on the bottom. Where my wife normally rests her phone doesn&#x27;t allow USBC to be plugged in at the same time. reply Kirby64 14 hours agorootparentI assume GP is referring to 2-in-1 dongles that allow charging and audio out at the same time. You just leave the dongle in the car... and only plug in the USB-C. It handles audio out and charging at the same time, and is one less cable to plug in. It&#x27;s better than plugging in USB-C and 3.5mm into your phone every time. reply monocasa 14 hours agorootparentRight, so with the USBC on the bottom of the phone, it&#x27;s blocked where she prefers to rest her phone. reply Kirby64 13 hours agorootparentIf you need the charge the phone, you need to plug it in. No way around that. Might as well get audio output at the same time from the same cable. reply monocasa 13 hours agorootparentShe doesn&#x27;t need to charge her phone in the car generally; she charges it at night. She simply wants audio out that just works even if I used the car last. reply PrimeMcFly 3 hours agorootparentprevSo she can adapt and change her preferences. replyKirby64 16 hours agorootparentprevHonest question, why are you still using a 3.5mm jack for connecting to car audio? Outside of very old cars that have not been retrofitted with more modern head units, every car that I can think of in the last 10+ years has had Bluetooth as a standard feature. It&#x27;s just far more convenient. reply monocasa 15 hours agorootparentBecause pairing&#x2F;connecting to crappy headunits is a task apparently defined by Satan to cause as much human suffering as possible. reply Kirby64 14 hours agorootparentMy experience does not mirror your anecdote, quite frankly. Sometimes pairing is a pain the first time, but any car I&#x27;ve driven in recent memory has very workable bluetooth that connects successfully every time.I even installed a SUPER cheap head unit in a car like 8 years ago (I literally sorted by cheapest head unit that had radio and bluetooth... it was like $35) and that was essentially bulletproof.The only case where this tends to be a pain in the ass is if you have multiple people attempting to use the bluetooth that are frequently in the car at the same time (e.g., a couple who both use the car, and you alternate who uses the bluetooth). In that case, a dongle that is connected to USB-C power and a 3.5mm aux jack directly seems superior, since you just need to connect a single cable to both charge and run audio. reply monocasa 13 hours agorootparent> The only case where this tends to be a pain in the ass is if you have multiple people attempting to use the bluetooth that are frequently in the car at the same time (e.g., a couple who both use the car, and you alternate who uses the bluetooth).One of many scenarios that I have hit where Bluetooth breaks down.> that case, a dongle that is connected to USB-C power and a 3.5mm aux jack directly seems superior, since you just need to connect a single cable to both charge and run audio.USB is on the bottom generally, headphone jacks are normally on the top. There&#x27;re a lot of places to rest a phone in a car where the headphone jack is available, but the USB is not. reply Kirby64 13 hours agorootparentNot sure what the issue is with just placing the phone upside down in whatever &#x27;resting place&#x27; is needed. Unless you&#x27;re actively using it, in which can presumably you would be holding the phone... so it doesn&#x27;t matter too much if the cable is above or below your phone. reply monocasa 13 hours agorootparentYes, she likes to glance at the screen when she&#x27;s stopped at a red light and can do so safely. replybigstrat2003 14 hours agorootparentprevMy dude, my car doesn&#x27;t even have aux input, let alone Bluetooth. I have to use a cassette adapter (or an FM transmitter) to get audio into my car. Not everyone, or even close to everyone, has a car made in the last 10 years.The nice thing about a headphone jack is it&#x27;s basically universal. It&#x27;s been around for some 40 years now! Anything you want to pipe into most likely can accept it. I have nothing against Bluetooth, but killing off a universal standard for one which is merely common is very short sighted. It&#x27;s a perfect example of why people accuse the tech industry of being out of touch with reality. reply Kirby64 13 hours agorootparentSo... buy an FM transmitter that has Bluetooth as an input? Or replace your head unit with one that has Bluetooth? There are plenty of solutions that exist to solve this problem for very little money. I did this myself for less than $50 in parts on an older car I owned. Choosing a phone purely based on the outdated technology of your car is much more shortsighted in my view. At some point (which, has already happened with 3.5mm jacks largely) the world moves on.Also, Bluetooth IS the universal standard. It&#x27;s in every single car that has been sold recently, and has been for years and years. Hell, I&#x27;d argue Bluetooth has been more universal in phones for longer than 3.5mm jacks. Early phones often had fully functional Bluetooth but had weird proprietary aux jacks (2.5mm, that weird Nokia plug, proprietary USB extensions, etc). reply bigstrat2003 11 hours agorootparentOr, hear me out... I could buy phones which use the universal standard we have had for audio connections for 40 years. I have neither the skills nor the inclination to replace my car stereo, and I can only guess you&#x27;ve never used an FM transmitter if you are seriously suggesting that as an option. They suck, full stop. Meanwhile, I stick to phones which aren&#x27;t designed by people so short-sighted as to think that everyone has Bluetooth available.I genuinely have no idea how you can call Bluetooth a universal standard with a straight face. It&#x27;s not even close. A 3.5mm isn&#x27;t truly universal (little or nothing is), but it&#x27;s as close as you can get. Ditching the latter in favor of the former is absolutely asinine.Seriously man, 10 years ago is not that long in real world terms. It&#x27;s a long time in the tech industry, but the rest of the world simply does not move on at that pace. As an industry, we need to pull our heads from our asses and realize that most people aren&#x27;t techies living in a major city for whom anything invented more than 5 years ago is old, and anything invented more than 10 years ago is positively ancient. That simply is not how the world works. reply Kirby64 10 hours agorootparentWe must be shopping at different stores. My experience with cassette adapters (and I tried quite a few different ones, back in the day) was always muddy, bad audio quality. FM transmitters (good ones anyways) never had that problem.As for Bluetooth as a standard: what, exactly, is not a standard about the audio portion of it? You can take any device that supports Bluetooth audio output and connect it to an audio playback device (speaker, head unit, whatever) and it will just... work. Super old speaker to brand new phone? Plays audio fine. Super old phone to brand new head unit? Same thing.This isn&#x27;t some &#x27;big techy is out of touch with reality&#x27; thing. We just have alternatives that are perfectly practical for edge cases where you absolutely need that aux jack, and the vast majority of consumers have moved on years ago. replydreamcompiler 16 hours agorootparentprevAnd AFAICT, just like in the Lightning days, you still cannot charge the phone through the port and use the wired headphone adapter at the same time, at least without a third-party dongle. reply monocasa 15 hours agorootparentCorrect. Power delivery and analog audio are mutually exclusive. The way the dongles that do charge work is by putting a USB DAC chip in the connector. reply cooperadymas 18 hours agoparentprevThere is a market for you, but it&#x27;s not a very big one. Sony in particular plays to this market, but you can see from their dwindling sales, that it&#x27;s rather niche. reply bigstrat2003 17 hours agorootparentSony would probably have better luck if their phones weren&#x27;t so incredibly expensive. As a fellow headphone jack requirer, I would love to buy a Sony phone. But I can&#x27;t afford one. reply Kirby64 15 hours agorootparentIf you can afford a Fairphone 5, it seems a Sony is within your budget. Sure, they make a $1600 phone, but they also make multiple phones that are much cheaper or similarly priced.Xperia 5 III (what a horrible name): $600Xperia 10 V (again, terrible name): $400Xperia 1 IV: $755 (same price as Fairphone)All 3 have headphone jacks. reply bigstrat2003 14 hours agorootparentI honestly have never seen a Sony phone which was less than $1000. Perhaps I just looked in the wrong places, but that&#x27;s what my comment was based on. I agree that a phone costing $400-700 is not an unreasonably high price, but I just wasn&#x27;t aware of the existence of the models you mentioned. reply abdullahkhalids 14 hours agoparentprevThe other audio problem with the Fairphone 4 is that it ranks quite low on audio tests [1]. It has playback&#x2F;recording scores of about ~90, while mainstream manufacturers have cheaper phones with scores of 130+.[1] https:&#x2F;&#x2F;www.dxomark.com&#x2F;fairphone-4-audio-test&#x2F; reply crazygringo 16 hours agoparentprevIt is a mystery why people hate tangled cords that also get caught on things? More than leaving your pods on a charging puck when you get home? Which takes zero additional effort since you have to put them down somewhere?Nobody&#x27;s arguing with your preferences but it&#x27;s a mystery to me how the convenience of wireless headphones is a mystery to you. reply bigstrat2003 14 hours agorootparentI have no idea what people see in wireless headphones. I have never once gotten the wire tangled, caught on something, anything like that. From my perspective, wireless headphones are \"solving\" a non-existent problem, at the cost of additional headache (charging) and at a higher price. They are strictly worse than wired headphones for my usage. reply crazygringo 12 hours agorootparentWell good for you on being tangle-free!Not all of us are so lucky, sadly. I guess there must be a lot of us that companies are catering to our genuine needs. :) reply PrimeMcFly 3 hours agorootparentprevHeadphone wires have caused me issue son packed subways before, with peoples accessories getting caught in them. Wireless is without a doubt more convenient. reply Foobar8568 19 hours agoprevI&#x27;ve a FP4, I won&#x27;t buy another Fairphone. Repairability is a dubious concept at best, and no hardware upgrade path despite larger design. Also see the current drama with the fingerprint and android update for FP3. Only good thing, non android os look functional. reply MegaThorx 18 hours agoparentI must say this is the biggest issue for me too. A upgrade path for the device (like framework provides for its laptops) is the biggest missing feature for me. reply GuB-42 19 hours agoprevUncompromising? There is no such thing as uncompromising.Its most defining feature, for me, is its removable battery. It that, I would compare it to the Samsung Galaxy XCover 6 Pro.Over the Fairphone 5, the Samsung Xcover has a headphone jack, dual sim and a IP68 rating, it is a rugged phone. Both can use a microSD card. The Fairphone has a better camera, better specs in general (but not best in class), and is more expensive. The Fairphone is generally more repairable, but because it is not rugged, probably more likely to break. The rest is about build quality, which is not easy to judge by the specs. The form factor is similar.Software-wise, the Fairphone is, I think, much better. Longer support, and a more active and better supported community. The XCover 6 Pro doesn&#x27;t even have a subforum on xda-developers.com. It has an unlockable bootloader, so in theory, one could run anything, but if no one cares...All in all, I think both are worthwhile options, they just made different... compromises. reply The_Colonel 19 hours agoparentThe most uncompromising != uncompromising.Just like e. g. the cheapest jet airliner isn&#x27;t necessarily cheap... reply rajeshmr 17 hours agoprevFairphone would easily be a big hit in the Indian Market, i figure. If there are any fairphone folks here, please do consider to bring this phone in the Indian market. I would love to own a Fairphone.At the bare minimum, all smartphones should allow for easy battery replacement like they do in Fairphone. (This was the norm, before the iphones and the androids took over) reply xctr94 16 hours agoparentIt’s 699€ in Europe; not sure about the price in India. But that feels prohibitive for my budget, so I imagine it’s expensive for the average Indian consumer? reply rajeshmr 16 hours agorootparentActually, iphones which are much costlier sell well in India! In fact, all the top-end phones sell well in India.Indian population seeks value, and are pretty smart about their choice of investment. An iphone even though is much costlier in India, has longer os &#x2F; security updates than any budget phone. So over a period of time, this would translate to a higher value for the device and hence the purchasing decision. Usually Indians try to ascertain the price to value ratio (something that Nothing phone ceo has acknowledged publicly) while making these decisions.Definitely Fairphone has the appeal of repairability (which again is valuable for the average Indian) and the longer os &#x2F; security updates are a boon. reply FirmwareBurner 16 hours agorootparentprevEven in Europe that&#x27;s a lot of cheese. People with 700 Eruos to blow on a phone will most lively get a iPhone or Samsung flagship. The only people I know who own an Fairphone are other tech workers who browse HN a lot but that&#x27;s a very small demographic. reply sambazi 3 hours agorootparentthe only ppl i know who own an fairphone are semi tech-literate and not interested in the specs or the price.they are all disappointed with the camera and build quality in general but cherish the fairness. reply mynewacct1 19 hours agoprevDoes the Fairphone support de-Googling? I&#x27;m currently on iOS, but something like this is appealing to me if it can be can potentially be more privacy friendly than iOS and other Android distributions. reply lucideer 19 hours agoparentThe Fairphone company&#x27;s primary focus is on fair hardware: supply-chain ethical materials sourcing & fair factory wages. Software \"fairness\" is very much a secondary focus, so while they&#x27;re open to de-Googling it&#x27;s not something they actively support or commit resources to. e.g. The warranty is temporarily invalidated by installing a de-Googled OS but can be \"restored\" by re-installing Android.They do however partner witht another company - Murena - to offer deGoogled phones[0]. They have a full breakdown of it here[1][0] https:&#x2F;&#x2F;murena.com&#x2F;shop&#x2F;smartphones&#x2F;brand-new&#x2F;murena-fairpho...[1] https:&#x2F;&#x2F;support.fairphone.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;997915455681... reply NewJazz 19 hours agorootparentwhile they&#x27;re open to de-Googling it&#x27;s not something they actively support or commit resources toI am not sure if this is entirely accurate. I agree that degoogled software is not their primary focus, but one Fairphone employee, Luca Weiss, has been pretty active in the postmarketOS community. For example, he submitted the initial port of pmOS to the FP5:https:&#x2F;&#x2F;gitlab.com&#x2F;postmarketOS&#x2F;pmaports&#x2F;-&#x2F;merge_requests&#x2F;43...Perhaps that was in his \"off time\", but just as likely is that Fairphone the company is carving out some resources to porting efforts like that. reply lucideer 18 hours agorootparent> one Fairphone employee> Perhaps that was in his \"off time\"I was more talking about the company&#x27;s primary focus - they may well have (limited) resources working to it (whether it&#x27;s off time or dedicated), but it&#x27;s certainly not one of their top priorities.That said: even the facts that they have (a) multiple pages on their website documenting deGoogling & maintained a list of working alternative OS versions & (b) an active partnership with a 3rd-party offering this service - these indicate that this is at least a priority for the company, just not a top priority. reply boudin 19 hours agoparentprevFairphone phones have a good history of supporting alternative operating systems, like lineageos: https:&#x2F;&#x2F;wiki.lineageos.org&#x2F;devices&#x2F; or &#x2F;e&#x2F; https:&#x2F;&#x2F;doc.e.foundation&#x2F;devices&#x2F;FP4 (which also sells fairphones with their os pre-installed.They document how to unlock the bootloader here: https:&#x2F;&#x2F;support.fairphone.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;104924762388... reply Moldoteck 19 hours agorootparentThey also support Ubuntu touch reply NewJazz 19 hours agoparentprevCalyxOS supports at least the FP4.https:&#x2F;&#x2F;calyxos.org&#x2F;docs&#x2F;guide&#x2F;device-support&#x2F;PostmarketOS is supposedly booting on the FP5.https:&#x2F;&#x2F;wiki.postmarketos.org&#x2F;wiki&#x2F;Fairphone_5_(fairphone-fp...E foundation sells a version of the FP4 that has their degoogled &#x2F;e&#x2F;OS preinstalled.https:&#x2F;&#x2F;e.foundation&#x2F;fairphone-and-e-expand-the-availability...I&#x27;d say the pieces are there to load aftermarket software that is degoogled. reply SahAssar 19 hours agoparentprevYes, but I don&#x27;t know if there are ready made ROMs for the 5 yet. I ran CalyxOS on my 4, and it officially supports running alternate OS:es. reply roshin 16 hours agoprevYikes, the screen is 6.46 inch . I guess this phone is not meant for people who want one handed phones. Oh well, my search for an open phone that can be used with one hand continues. reply poulpy123 17 hours agoprevthe lack of jack and the not great camera is unfortunately a deal breaker, although I&#x27;m not sure I would afford it anyway. but the idea is great and i hope if will find its success reply pcurve 18 hours agoprevCan&#x27;t they charge $2-300 more and put in better chip and camera? reply iopq 15 hours agoparentActually no, because they had to use a Qualcomm QCM6490 industrial SoC to get that long support contracthttps:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2023&#x2F;08&#x2F;fairphone-5-sets-a-n... reply Audiophilip 18 hours agoprevLack of headphone jack is a deal breaker for me, unfortunately. reply tomwheeler 18 hours agoparentSame. I&#x27;d absolutely buy one if they had this basic feature. reply sanitycheck 18 hours agoprevApart from everything else, having to remove the battery to access the microSD card is a weird choice. Android has always been able to handle SD swapping without a restart, and an Android cold start takes quite a long time (unlike the old phones which this design harks back to).Who&#x27;s it for? Those very few Guardian readers that can be lured away from their iPhones by the promise of being eco-friendly. reply Scandiravian 18 hours agoparentI&#x27;m getting one. I&#x27;m currently on a galaxy note 9, which hasn&#x27;t received updates for a year. The hardware is still plenty fast for what I use my phone for, but since I have my banking app on the phone, I don&#x27;t feel comfortable continuing to use it without security patchesThe microphone also broke around a year ago and though it&#x27;s possible to replace it, it&#x27;s not a simple fix, so I&#x27;ve accepted I need to have a headset nearby at all timesThe FP5 wouldn&#x27;t have the same issues. I wouldn&#x27;t have to spend any money on a new phone for almost a decade and if something breaks I can fix it pretty cheaply myselfTo me those are the most important considerations when picking a new phone and there&#x27;s not really any other product that I think is competitive on those parameters reply sneeze-slayer 18 hours agoparentprevIt&#x27;s true, removing the battery is mildly annoying. But I can probably count on one hand the number of times I have added or removed a microSD card from my phone. It&#x27;s just not something I do regularly--Get a new phone, plug it in, and forget. Maybe remove it every few months to back it up to my computer. reply sanitycheck 16 hours agorootparentI pop mine in and out all the time, it&#x27;s been the quickest way to move large files on&#x2F;off the phone ever since Android lost USB mass storage support. MTP & PTP are horrible. reply Kirby64 16 hours agorootparentprevYou&#x27;re missing the use-case for folks that use it for large data transfers for video files. Having to shut down the phone to do that is burdensome. It&#x27;s why the new iPhones with USB-C that can record directly to flash drives are being praised. reply tomwheeler 18 hours agoparentprev> having to remove the battery to access the microSD card is a weird choice.IIRC, my old Samsung Galaxy S5 was like that too. While I would prefer being able to remove the card without removing the battery, in practice it wasn&#x27;t that big of a deal for me since I only swapped out cards every few months. reply SiempreViernes 18 hours agoparentprevWait, are you saying you don&#x27;t want this phone because you can&#x27;t hotswap the sim card untethered? reply daft_pink 11 hours agoprevIt&#x27;s nice and maybe this is an unpopular opinion, but I find that my iPhone never breaks and this is not a big issue as the phone is really well made.Unlike my Apple laptop with a butterfly keyboard..... reply spike_protein 18 hours agoprevThis phone is so sneaky, it has one fake camera just for show! reply tannhaeuser 18 hours agoprevI considered buying a Fairphone last year or the year before, but man was it a brick, it calls into question the whole concept of a mobile device! Why is it that only Apple can produce a reasonably-sized phone, with all Android phones (last I checked) being phablet-sized and not fitting comfortably into a back pocket? Is it in memory of SJ&#x27;s iconic gesture of letting it slide into the pocket, or because Android phones are basically ad consumption devices needing as much screen real estate to even see any content at all between the ads? reply NoraCodes 18 hours agoparent> all Android phones (last I checked) being phablet-sizedMajor manufacturers, Apple included, keep making phones bigger; it&#x27;s frustrating. However, I think it&#x27;s a bit unfair to say that, for instance, the Pixel A-series (which I&#x27;ve used for years) is \"phablet sized\".The current flagship iPhones are 6.7\" (iPhone 15 Pro Max, $1199) and 6.1\" (iPhone 15 Pro, $999). The current flagship Pixel phones are 6.3\" (Pixel 7, $599) and 6.1\" (Pixel 7a, $499).> Android phones are basically ad consumption devicesIt&#x27;s possible that this is a fair criticism of some Android devices, but with YouTube Vanced&#x2F;ReVanced and Firefox with AdBlock Plus, I haven&#x27;t seen an obtrusive ad on my phone in years.There are many things iPhones do better than Android devices, but these two criticisms are basically nonsense.Edit: Apple also has a very small phone in the iPhone SE, but they&#x27;re not continuing to produce those (per latest reports, anyway) so it doesn&#x27;t strike me as a fair comparison. If we include older generations of products, there are tons of small Android phones. reply sambazi 3 hours agorootparent1st-gen SE was the perfect phone imhothe size was just right, althou bezel could be smallerheadphone jack !!1astonishingly robust; had mine for 5y, dropped it at least once a week, children ... never broke anything reply TylerE 18 hours agorootparentprevI don&#x27;t think diagonal screensize is the most fair metric here... bezels have gotten much, much, much smaller.Compared to an iPhone 4, which I think is a lot of people&#x27;s idea of a \"normal\" sized phone, a Pixel 6A 32% longer, and 22% wider. It&#x27;s also 30% heavier. reply NoraCodes 17 hours agorootparentOh, absolutely. I&#x27;m all for building smaller phones. If I could get a Pixel 3a new today, I absolutely would be using that.I&#x27;m just saying that GP is being a bit disingenuous in setting this trend at the feet of Google or manufacturers of Android phones when Apple, too, has been increasing the size of their phones and cutting smaller products.The issue is the duopoly itself. reply NewJazz 18 hours agoparentprevI have no idea, but this is the only group I hear talking about the issue with serious intentions to address it:https:&#x2F;&#x2F;smallandroidphone.com&#x2F;In case you don&#x27;t read to the end of that page...Extrapolating from past models, the Pixel 10 will be roughly the size of California reply miloignis 18 hours agoparentprevIn the spirit of sporting rivalry, I imagine I see far fewer ads on my Android than an iPhone user - GrapheneOS+Firefox+UBlock Origin means no ads in OS, no ads on the web. I don&#x27;t remember the last time I saw an ad on my phone... reply samatman 17 hours agoparentprevNot even Apple sells reasonably-sized phones anymore, I&#x27;m hoping my Mini 13 will last until they start making them again but there&#x27;s no guarantee they ever will. reply RomanPushkin 19 hours agoprev$750? No offense, but I wouldn&#x27;t pay for it more than $200> What truly makes the Fairphone special is the fact that you can fully disassemble it yourself in a matter of minutes, using a standard screwdriver you likely have at home alreadyI disassembled my cheap Xiaomi Poco twice, replaced screen and broken camera. I had no issues with that. The phone itself was damn cheap, and good enough so I don&#x27;t have to worry about breaking it, or thinking about it too much.I wish Fairphone all the best, I am just wondering what the market is. Who is going to buy it for that heck of a price? reply NoboruWataya 18 hours agoparentThe point is their materials are more ethically sourced and workers are treated better than with those other phones (hence the name). It costs money to do that, because you are internalising social costs that other manufacturers are externalising. That explains the price difference and to answer your question the target market is people who care enough about such things to pay it. reply Vinnl 18 hours agorootparentSlight nuance is that that partially explains the price difference, but I&#x27;m fairly sure that it&#x27;s also largely due to just lacking economics of scale, unfortunately.But they are paving the way, showing how things can be done, setting up the required tooling and documentation, and hopefully thereby influencing (legislation for) the big players to improve. In fact, that&#x27;s my main justification for spending the money. reply Levitating 19 hours agoparentprevMy thoughts exactly. Chinese phones have great repairability and come a lot cheaper than this. And advancements have kind of stagnated so it&#x27;s starting to make sense to buy older models. For 700 I don&#x27;t care if it&#x27;s repairable because I could&#x27;ve bought 3 separate phones for that price. reply treme 18 hours agoparentprevtech hipsters with 100k+ income? reply sysadm1n 17 hours agoprevAnyone using a FP as a daily driver? reply funcrush 1 hour agoparentI&#x27;ve been using a FP4 for the last year it is fine as a basic phone however:- I&#x27;ve needed to have it repaired under warranty as some of the mics didn&#x27;t work.- I&#x27;ve needed to get their long life usb cable replaced under warranty. The connectors on the ends seem pretty flimsy and will break over time.- For software updates they tend to be every 1-2 months with bug fixes and security patches, major Android versions take more than a year so when they talk about 5 years of software support it is more like 3 year stretched out to 5. reply lawn 17 hours agoparentprevI use a FP4 and it&#x27;s been great so far.I love the replaceable battery, the close to stock Android and it seems to be built very well (having dropped it many, many times).Mot being Apple, Samsung or Google is a great plus. reply wolverine876 15 hours agorootparentHow have you benefitted from the replaceable battery? I like the idea, but when does it come in handy? reply lawn 2 hours agorootparentWhen I travel I bring a spare battery. Then I don&#x27;t have to keep charging the phone all the time, and sometimes it might not be possible. With a spare that&#x27;s one thing less to worry about.The second thing is battery degradation. After a while the amount of charge will drop, and replacing it with a new and fresh battery feels like giving the phone new life. reply jlkuester7 15 hours agorootparentprevOn almost every phone I have owned, battery degradation is the primary complaint that I have after the first few years of use. Unfortunately, when the battery is just glued to the back of the screen, what should be a cheep and easy fix, has become impossibly risky. Instead, I am left nursing a dying battery to the point where, for one phone, I would carry an external battery pack everywhere since my phone battery could not last the day... reply sufficer 16 hours agoprevI got iphone 15 ads on an android site on an article about this phone lol reply al2o3cr 19 hours agoprevThe whole review is \"well, it kinda sucks, but...\" - like if Stockholm syndrome became a tech writer. reply nsxwolf 19 hours agoparentIt&#x27;s not a good name. It&#x27;s a Dutch company - did they not understand that \"fair\" can mean \"mediocre\" as well as \"just\"? reply cbozeman 19 hours agorootparentIt&#x27;s a great name. It means \"fair\" as in fair trade. As in not buying, or minimizing as much as you can, buying conflict minerals.First off, the people buying this or even considering it, already know what Fair Trade is, to some degree, even if all they know is that, \"It&#x27;s trendy!\"Second, the name isn&#x27;t bad just because someone is ignorant of what it means. Oracle is called Oracle because the Oracle of Delphi was supposed to have exceptional insight to the point of being all-knowing. Just because someone doesn&#x27;t know what the Oracle of Delphi is&#x2F;was, or is aware of the broad usage of the term \"oracle\" doesn&#x27;t mean it&#x27;s a bad name.How many people know Nike is the Greek goddess of victory? Doesn&#x27;t mean it&#x27;s a bad name just because we refuse to educate people properly. reply ctenb 19 hours agorootparentprevI think the name is fine. I don&#x27;t often see it used the way you describe it, and when it does I&#x27;d say it means something like adequate, average or acceptable. Mediocre has a more negative connotation to it. reply nsxwolf 18 hours agorootparentYou&#x27;re right, \"mediocre\" is too negative a connotation. \"Acceptable\" is better. That&#x27;s the feeling the name gives me. A phone that will be just fine. reply tomwheeler 16 hours agorootparentprevIn my experience, most Dutch people—especially those involved with tech—have English proficiency similar to that of a native speaker. reply ReptileMan 18 hours agoprevThe only people that care about that kind of phone usually use flagships. So having sub-flagship specs on flagship price is not a winning offer.One plus had the right idea once. although they also didn&#x27;t manage to get the camera thing going. reply pc_edwin 19 hours agoprev [–] This is why I&#x27;m very excited at the prospect of an OpenAI phone.Without billions in funding, its impossible to build a viable alternative to the android&#x2F;ios duopoly. Heck, its near impossible even with the billion (RIP windows phone) but that where OpenAI software side comes in. reply ceejayoz 19 hours agoparent [–] Why would OpenAI want to make a phone? reply Levitating 19 hours agorootparent [–] AI is really becoming the new meta or crypto. Suddenly everybody is thinking it will change the world and be integrated with every existing technology we have.People have to stop buying into these fabricated tech hypes. reply WillPostForFood 18 hours agorootparentSuddenly everybody is thinking it will change the world and be integrated with every existing technology we have.That&#x27;s an argument against all technological progress. You had a crowd 20 years ago asking why you&#x27;d want a smart phone - a phone should just be great at making phone calls. Somet",
    "originSummary": [
      "Fairphone recently released the Fairphone 5, a smartphone praised for its focus on repairability and environmental responsibility, featuring a modern design, 1224x2770 OLED display, a 4,200mAh battery, 256GB of storage, and 8GB of RAM.",
      "While current availability is limited to Europe, there's potential for a future US launch. Despite receiving criticism for average camera performance and price, it remains the top option for consumers prioritizing environmental impact and social responsibility.",
      "The Fairphone 5 claims to offer a longer support window of up to eight years and can be completely disassembled with a standard screwdriver, indicating its commitment to longevity and repairability."
    ],
    "commentSummary": [
      "The discussion primarily focused on phone repairability, sustainability, and user preferences, with a highlight on Fairphone models. Concerns raised included availability of replacement parts and longevity support for older models.",
      "Important topics covered were the cost-effectiveness of repairs versus new purchases, the environmental impact of electronic waste, the significance of using sustainable materials, and factors like removal of headphone jacks or the application of Bluetooth for audio.",
      "There was a noted admiration for continued operating system and security updates offered by high-end phone models like the iPhone in India, and discussions about potential future technology trends."
    ],
    "points": 155,
    "commentCount": 241,
    "retryCount": 0,
    "time": 1696341037
  }
]
