[
  {
    "id": 40962675,
    "title": "How to Know When It's Time to Go",
    "originLink": "https://thecodist.com/how-to-know-when-its-time-to-go/",
    "originBody": "How To Know When It's Time To Go Andrew Wulf Jul 9, 2024 • 5 min read I retired in 2021 at 63.5 after about four decades as a programmer. What made me do this was not failing ability in any way, but after a year of consideration, I realized I didn't care to do it anymore. Everyone will eventually reach a point at which they can no longer do what they spent their lives doing—but it's not just about retirement; it can happen at any time earlier as well. I've known people much younger than me who became obsolete due to their chosen technology going away or people who simply got tired of writing code or, more commonly, doing it as a job. Of course its not only programmers, it happens in everything including sports where professionals keep playing beyond their ability (often to make a bunch more money), and even politicians who exceed their limit (currently on view as both US Presidential candidates should not run but won't quit). It can be that you cannot do it anymore, have a lack of desire, a bad job market, obsolete technology, or are discovering something else worth doing. In my life, I've seen all of those. I knew a young programmer a decade ago who left his CS degree because he was making tons of money shipping mobile apps, he even wrote a book on it, and started several companies. But he burned out and discovered he no longer cared, started a Jeep dealership, then started a land clearing business (riding bulldozers all day) and enjoyed all of that more, plus had more consistent success. I knew someone who got a CS degree when I was getting my Chemistry degree and spent his career writing only mainframe systems code. Eventually, the mainframe was abandoned, and he had not learned anything else, so he had to work installing network cables. Another team at a place where I worked bragged about how great their technology was (4GL tools generating RPG2); a year later, they were out of a job, and the tool was obsolete. Sometimes, the world passes you by. You probably don't know any retired programmers—back in the early 80s, when I started, there was only a tiny percentage of programmers compared to today, and over the decades, many gave up, became managers, or became obsolete. So, not many remained writing code long enough to end their career still writing code. Among those who I knew that started when I did, all that remained forty years later were doing legacy work; I was the only one still working on the leading edge of things (in my case, iOS in Swift for a very large company). It's hard to sustain a career that long in an industry with insane amounts of change. All of you reading this (programmers anyway) will eventually hit a point where you can't do the job anymore for one of the above reasons. It's important to be honest with yourself. Are you keeping up to date sufficiently to continue the job? Is the job even interesting anymore, or is there something else you would rather do? Maybe your programming is not fun, or perhaps it's no longer challenging enough. Despite how unfulfilling it is, I've known people who enjoy the paycheck, do boring work, and keep doing it. That's a choice, but I was never satisfied with that. It's not only quitting programming that you might consider; the biggest reason to quit may be that your job is not worth doing and that you need to find another one. I've quit perfectly good jobs (and some terrible ones) because I did not find the work interesting, the direction terrible, or a hostile workplace. Naturally, some employers went out of business or laid people off, but I left those jobs involuntarily! A different kind of programming or a different industry could revive your interest. The key is still to be honest with yourself. I've known people who left a high-pressure, high-salary job, started a farm, or began a different career. It's not worth working and being miserable. Some time ago, I knew a programmer with the same number of years of experience as me. Yet he seemed unable to comprehend what was required of him, and I had to review everything he wrote because it rarely worked; for example, he would copy and paste code but include extra lines that did nothing. I still have no idea how you can work for 30 years and be unable to do even simple things. None of this says that you should discriminate against people simply due to their age. Age and ability are not correlated. I've known people older than me who were brilliant and some that were incapable, like the previous person. I've seen 20-year-olds who could write anything successfully and others who did not understand simple concepts. Thankfully, I saw little age discrimination besides a few interviews where the interviewer had that look on their face: \"Oh no, an old guy.\" Especially in my last three employers (covering about a decade), each valued everything I did. When I announced my retirement (with three months' warning), my leaders were shocked. They couldn't comprehend why anyone would retire. One of them, whom I had worked for for two of those jobs and always made his life easier, never spoke to me again or even said goodbye. I still talk with some of my peers and team, and knowing how downhill the work has gone since (although the money got really good), I would have left anyway. I like making a difference and being challenged to do things and work that matters. Money is nice, but I liked making that difference. Everyone eventually reaches a point where a job, an employer, an industry, or even their entire career ends. It feels better to be honest and make an informed decision rather than discover you are falling behind and possibly being forced out. It feels better to leave a bad job instead of hanging on until your hair falls out, even if the next job is equally terrible (I've been there). Your career is yours; it's up to you to make the most of it, even if you end it. I still write code every day in support of my generative art practice. The code is much more complex than anything I did previously, and much of it does not have anyone else doing it, so it's a lot of invention, which is fun. While I miss certain aspects of my final job, I don't miss the eternal pressure, long hours, bad executive decision-making, and endless changes. I do miss being a leader and seeing things we wrote being used by our many customers. Programming can be a fun career, a horrible nightmare, or something in between, and it never stands still. I enjoyed the good parts, tolerated the bad, and switched technologies, industries, and employers often enough to keep it going for four decades. That's long enough.",
    "commentLink": "https://news.ycombinator.com/item?id=40962675",
    "commentBody": "How to Know When It's Time to Go (thecodist.com)304 points by kiyanwang 23 hours agohidepastfavorite185 comments JKCalhoun 18 hours agoRetired from Apple a few years ago (at age 57). I was not obsolete. A big company like Apple, there are always things that need taken care of. I assumed with iOS, Swift, etc., maybe the guys on the Cocoa team were obsolete? Of course not. That code is still there, still needs maintaining, interoperability with the new languages, frameworks, etc. I'm more surprised they want to stay on. And that is in fact why I left Apple: the job had changed, the \"career\" had changed. The engineers were no longer steering the ship. It had been that way when I started in 1995 though. A \"team\", let's say the graphics team, would figure out what API to revisit, what new ones to add — perhaps how to refactor the entire underlying workflow. The \"tech lead\" (who would regularly attend Siggraph since we're talking about the graphics team) would make the call as to what got priority. Marketing would come around after the fact to \"create a narrative\" around all the changes to the OS. I hate to say it, but many, those were the good ole' days. (And let's be clear, in the 90's, Apple's customers were more or less like the engineers, we also loved the machine for the same reasons they did — so we did right by them, made changes they would like because we wanted them too. You can't say that as convincingly for the phone, being a mass consumer device.) Marketing took the reins long ago though — especially as Apple began to succeed with the iPhone (which, someone can correct me if I am wrong, but I think was an engineer driven project initially — I mean most things were up to that point). I stuck around nonetheless though because there was money to be made and kids still to raise. When the last daughter flew the coop though, so did I. reply zkirill 15 hours agoparentI want to create a company that is like Apple of the 90's and resurrect the \"bicycle for the mind\". Is there any chance you would consider mentoring? reply Sophistifunk 12 hours agorootparentThere's plenty of people who want to build that thing, and most of us have a good idea how it probably should be, at least at first. The problem isn't building it, it's funding it without selling out the user. reply CalRobert 12 hours agoparentprevI wonder how many other people were at Apple from the mid-90's until a few years ago, that's an incredibly long tenure. It seems like one of the more interesting places to be during several very interesting transformations. reply ghaff 6 hours agorootparentMay you live in \"interesting\" times. I've had 3 jobs in the ~decade-long range. I was really ready to move on in each case. Partly I was ready for a change and partly the company had changed. reply AtlasBarfed 15 hours agoparentprevI like programming. I can still do it. What I don't like is all the bullshit around it. Primarily now the barrier is that I don't have to work, so why would I put up with abusive hazing? I mean of course, hiring processes, which have only gotten worse over time (hallmark property of the cycle of hazing). I'm not doing on-call rotations anymore. Either allow us to engineer the thing to be resilient, or pay off-duty people (a wonderful opportunity for offshore people that management is so desperate to use). Finally, I don't want to code in Python or JavaScript. As a long time programmer, it is annoying that we keep going backwards and wasting more and more hardware power. Nobody is producing anything exciting in software anymore. I can't think of a pure software company doing anything I would be excited for, because google and facebook and the like control the internet. It doesn't even pay that well anymore, and AI is just another huge excuse to drop wages by management. Apple is a fantastic example: operating systems are stagnant, hardware outside of the architecture switch is stagnant (and how much of that was simply priority access to the state of the art TSMC node tech). Nobody makes good solutions for anything anymore. reply shiroiushi 13 hours agorootparent>Nobody is producing anything exciting in software anymore. They're not? Some of the newer programming languages seem very interesting, attempting to fix some of the mistakes done in older languages. Of course, most of the really interesting problems are already solved by existing solutions, but perhaps there's room for improved solutions instead of just using the incumbent. >I like programming. I can still do it. What I don't like is all the bullshit around it. If you have spare time (you sound like you might be retired), perhaps you should try getting involved in an open-source project that interests you (and isn't in Python or JS of course). >I can't think of a pure software company doing anything I would be excited for, because google and facebook and the like control the internet. Personally, I work in robotics and find it quite interesting. I would also find writing software for spacecraft interesting. Neither of those are \"pure software\", but still I think they're applications that will change the world, hopefully for the better, and don't already have some huge incumbent dominating the market. reply ido 15 hours agorootparentprev> It doesn't even pay that well anymore, and AI is just another huge excuse to drop wages by management. Can you expand on this? I haven’t noticed salaries going down where I’m at (only fewer open positions the last ~1.5 years due to the global economic climate - but I’m sure this is cyclical and will swing back soon enough). reply amonith 10 hours agorootparentSalaries are not going down but costs of living skyrocketed. I am in the top 3% earning bracket as a dev in my country (Poland), I live in a relatively cheap area (south-east) and when I reached my 30s I could afford to buy just a ~100sqm city apartment which cost exactly 5x of my parents 200+sqm house which they bought without mortgage as factory workers without higher education. And each year I can afford less and less with my relatively huge salary. reply ido 5 hours agorootparentBut “not as well paid as it used to be” is relative to other wage earner, not to cost of living. Two uneducated polish factory workers today won’t be able to buy your parents’ house either. reply amonith 2 hours agorootparentIdk, maybe it's not the exact book definition but I always thought about how well I'm paid in terms of the purchasing power, not the arbitrary value, and especially not by comparison with others (strictly speaking, because costs of living are kind of that, isn't it?). The software jobs slowly go into the direction of not being worth the effort (I don't really believe that we will reach that point but that's the current direction). > Two uneducated polish factory workers today won’t be able to buy your parents’ house either. Of course - because from my point of view factory jobs are currently paid terrible. They used to be paid better (worth the effort due to being able to buy more). reply hollerith 5 hours agorootparentprevWhen your parents bought, your country had few highly-paid jobs, and places like that have low housing costs: the presence of many highly-paid workers is what causes housing prices to be high. reply amanaplanacanal 8 hours agorootparentprevThere is a lot more value being produced, but the capitalist class has managed to capture a much higher percentage of it, leaving the worker bees with less and less. At least we have lots of toys to distract us! reply wakawaka28 6 hours agorootparentOutsourcing everything is what's really destroying the salaries in \"advanced\" countries. And high inflation seems to go hand in hand with the domestic economic shutdown and trade imbalances. reply anovikov 2 hours agorootparentThing is, Poland IS the country a lot of stuff is being outsourced into :) reply amonith 1 hour agorootparentWorry not we started to outsource to India as well... My wife's company (low code domain-specific software creators) laid off 80% of their workforce and contracted a smaller amount of people from India. It's not going well but I bet it's gonna take at least a year before anyone notices. reply bryanrasmussen 11 hours agorootparentprevwell I'm pretty well paid for Denmark, but my wages haven't really gone up in the last few years and when I look at payment rates in Denmark I'm still pretty much at the top for programmers who don't consult - but my wage used to be the same as the average two person household in Denmark and now it is a couple hundred dollars less. reply sakjur 10 hours agorootparentprevIt could be a nod to a shift into ‘do more with less’. In Sweden hospitals and schools have basically lost all their administrative staff and students per class has grown. Teacher salaries might not have gone down, but expectations are much higher, as they have less support from non-teaching staff. reply datavirtue 6 hours agorootparentprevAssistants, like Copilot, substitute for junior devs, making it possible or more likely that a team will put off hiring. I have seen this silent killer in action. reply bdw5204 2 hours agorootparentprevI'm much earlier in my career than you are but have had serious thoughts about leaving the industry altogether for similar reasons. The interview processes are absolutely toxic these days, much of the industry seems like an outright scam (crypto, AI, etc.) and the trend is to casually waste resources. Part of my motivation to go into writing code in the first place was that I noticed software getting worse and more user-hostile in the 2010s and I wanted to change that. Turns out the people making software worse think the stuff that makes it worse are \"best practices\" so you're fighting an impossible battle and nobody is going to dare allowing you to advance into a leadership position or often even get a job in the first place unless they think you're a true believer in the BS. I also have no interest, at this point, in writing code unless I'm paid to do it. It's hard to find motivation to write code when I mentally associate it with all of the corporate BS and the grifting con artists of the tech industry. The one saving grace was that the money was good and it was possible to switch jobs for more money or because you're tired of 1 particular company's BS. Now, even that isn't possible anymore so what's the point? reply Scarblac 22 hours agoprevI started programming at 10 and now I'm 50, and right now it feels like I've reached this point -- it's boring, I have trouble keeping up, I feel the things work lets me work on are not important. Interesting work goes to younger colleagues. The problem is, I have a family and finding fulfilling work that you have no experience in, in this country, at 50, is close to impossible. So for now I consider myself lucky and try to rediscover the fun things in programming. reply SoftTalker 22 hours agoparentI'm a bit older, I don't really feel trouble keeping up but looking at the landscape it's just not that interesting anymore. So many \"new\" ideas are actually old ideas but the people pushing them are too young to know that. I don't have any doubt in my ability to learn new languages and frameworks, but running in that hamster wheel just gets boring after a while. reply altdataseller 22 hours agorootparentWhat are some examples of new ideas that are old? reply nine_k 21 hours agorootparentLambdas (in the cloud): see CGI scripts and inetd. Containers: see BSD jails, Solaris zones. WASM: see JVM and Smalltalk VM. Async / futures / actors: see Erlang, Lua, Oz. The cool type system of Typescript: see OCaml and Haskell. Numpy: see APL. Through the list above, there's usually a 20 to 40-year gap between the first availability and the turning into \"new hotness\". reply sjrd 21 hours agorootparentIt's not every day that we see Oz mentioned here! I was very involved in writing the Mozart/Oz 2.0 VM. I also wrote a \"toy\" (read: for school) dialect of Scala compiling to Oz and therefore turning every local variable or field into what Scala calls a Future, for free. Performance was abysmal, though! But in terms of language idioms, it was quite nice. --- Unrelated: about Wasm, none of what it does is new, obviously. What's interesting about it is that a) browser vendors agree to do it together, and b) the design grows to accommodate many source languages. This used not to be the case but the eventual arrival of WasmGC significantly redistributed the cards of the game. Relevant background here: I'm the author of the Scala to JavaScript compiler, and now co-author of the Scala to Wasm compiler. reply nequo 21 hours agorootparentprevGenerally agreed. About WASM, it is not the first sandboxed bytecode interpreter but the first that runs in a browser and that has usable toolchains to compile not “browsers first” languages into it. I’d argue that that’s where the novelty is. reply NomDePlum 20 hours agorootparentDid Java applets arguably not do this 20+ years ago? reply Cthulhu_ 7 hours agorootparentJava, Flash, Silverlight, ActiveX? There were loads of technologies to run different languages in a browser, but they were all proprietary to a point; none of them were a web standard, they all needed separate installation or a specific browser, and they were all basically black boxes in the browser. Whereas (from what I understand) wasm is a built-in browser standard. There was (is?) also asm.js, which IIRC was a subset of JS that removed any dynamicness so it would be a lot faster than vanilla JS. But again, no broadly carried / w3c standard. reply nequo 19 hours agorootparentprevMaybe you know this better than me. Were non-JVM native languages available 20 years ago for Java applets? My conception of it is that they were pretty much Java only (with Clojure and Scala also available in the later years before they got deprecated?). Is this conception wrong? reply nine_k 19 hours agorootparentYes, you could write applets in other languages. The choice was rather narrow, but you could use [Python], [Scala], or [JRuby]. [Python]: https://www.jython.org/jython-old-sites/archive/21/applets/i... [Scala]: https://cs.trinity.edu/~mlewis/ScalaApplet/scalaWebApplet/We... [JRuby]: https://www.jruby.org/getting-started — offers to run as an applet in the first few lines. reply eecc 11 hours agorootparentalso JavaScript. See https://en.wikipedia.org/wiki/Nashorn_(JavaScript_engine) reply antupis 9 hours agorootparentprevWell yeah but the problem was that you still needed that runtime, WASM should solve this. reply jimbob45 19 hours agorootparentprevI thought those were interpreted by the JVM, which was subject to security issues. WASM faces no such security issues, no? reply pjmlp 12 hours agorootparentWASM also has potentials for security exploits, but those selling it are quite silent on those. Everything Old is New Again: Binary Security of WebAssembly https://www.usenix.org/conference/usenixsecurity20/presentat... Just one of the many articles that are slowly surfacing, now that WebAssembly is interesting enough as possible attack vector. While there is a sandbox, you can attack WASM modules the same way as a traditional process via OS IPC, by misusing the public API in a way that corrupts internal memory state (linear memory accesses aren't bound checked), thus fooling future calls to follow execution paths that they shouldn't. With enough luck, one gets an execution path that e.g. validates an invalid credential as good. reply lmz 19 hours agorootparentprevThe JVM implemented properly should not have security issues. The class library however... (i.e. it's a lot easier to sandbox things if you start without any classes that interact outside the sandbox). reply swiftcoder 10 hours agorootparentprevThe JVM is fairly good at sandboxing, as these things go. Turns out sandboxing arbitrary software is an extremely hard problem (as the WASM folks are starting to encounter in the wild) reply Earw0rm 14 hours agorootparentprevAt least in so far as the higher level (DOM, browser runtime) and lower level (memory access, to the extent that it's mediated by the WASM VM) have no security issues... The VM itself is pretty tight, but abstractions have a nasty habit of being leaky. reply 3np 19 hours agorootparentprevOh, sweet summer child reply away271828 19 hours agorootparentprevI'm a bit hesitant to describe $NEW_CONCEPT/TECH as just $OLD_CONCEPT/TECH. Echoes of older things in a new context can really amount to something different. Yes, VMware didn't create the idea of virtualization and Docker et al didn't create containerization but the results were pretty novel. reply nine_k 19 hours agorootparentI'd rather say that good ideas keep on returning, no matter whether they are remembered or getting reinvented. It's not that those who reapplied the old concept in new circumstances are not innovators; they are! Much like the guy who rearranged the well known thread, needle, and needle eye and invented the sewing machine, completely transforming the whole industry. But seeing the old idea resurfacing again (and again) in a new form gives you that feeling of a wheel being reinvented, in a newer and usually better form, but still very recognizable. reply xyzzy123 11 hours agorootparentThe plumbing behind Docker is not particularly novel but the porcelain was imho a major advance. There were plenty of ways to do \"containers\" (via vservers, jails, zones etc) but the concept of image never caught on before Docker. You could sling tarballs of chroots around and at times this did happen but it was a sort of sysadmin thing to do, there was no coherent \"devex\". reply galdosdi 4 hours agorootparentprevCouple more: (1) Garbage collection in every high level language: Java, which was the first mainstream language to do it-- people were seriously using cpp for high level business logic at the time, and were suspicious of GC for its performance. But Java itself got it from LISP, which had introduced GC without it ever going mainstream decades prior (2) No SQL had already been tried as hierarchical databases in the 70s or 80s iirc. Relational model won because it was far more powerful. Then in the early 2010s, due to a sudden influx of fresh grads and boot campers etc, who often hada poor grasp on SQL, schemaless stuff became very popular... And thankfully the trend died back down as people rediscovered the same thing. Today's highly scalable databases like Spanner and Cassandra don't ostentatiously abandon relational calculus, they reimplement a similar model even if it isn't officiallu SQL (3) And then there's the entire cycle that's gone back and forth several times of client based vs server based: First there were early ENIAC type computers that werr big single units. I would consider that similar to thick client. Then as those developed we had a long era of something more similar to cloud, in that a single computer developed processes to support many partitioned users who submitted punch card batches. That developed even further into the apex at the time of cloud style computing: terminal systems like ITS, MULTIcS, and finally in the 70s, UNIX. Then the PC revolution of the 80s turned that totally on its head and we went back to very very thick client, in fact often no servers at all (having a modem was an optional accessory) We stuck with that through the 90s , the golden age of desktop software. A lot of attempts were made to go back to thinner clients but the tech wasn't there yet. Then of course came the webapp revolution started by Gmail's decision to exploit a weird little used API called XMLHttpRequest. The PC rapidly transformed over the next decade from a thick client to a thin vessel for a web browser, as best exemplified by the Chromebook, where everything happens in the cloud -- just like it did in the mainframe and terminal days 50 yeara ago... The trend could stay that way or turn around -- it's always depended in hardware performance balance changes. reply davidgay 13 hours agorootparentprevI have to say that all your “old” ideas (they are all from the 90s AFAICT) seem new to me ;) For example, for Haskell [1990] (ok, not so much the type system bits, but…), see FP [1977] (https://en.m.wikipedia.org/wiki/FP_(programming_language)) reply motohagiography 16 hours agorootparentprevabout 15 years ago the joke was, `cat /etc/servicesmail apply@ycombinator` as at the time it seemed like startups were just doing file transfer, email, network file systems, etc. it wasn't far off, as unix is file based, and the internet is also file based. reply Cthulhu_ 7 hours agorootparentAnd to a point they were correct; file transfer 15 years ago was closely linked to piracy and dodgy websites that scam you into pressing an ad instead of a download button. It's only thanks to e.g. dropbox / cloud file storage suppliers, wetransfer, etc that that bit has been resolved. Dunno about email though, the last real innovation in that space that I can remember with lasting impact was gmail. There were a few more tidbits like inbox (RIP), the inbox zero methodology, and Airmail (?) but none of them really took off. reply throw1230 21 hours agorootparentprevThere's always a push and pull between old and new tech and I agree some of the hot new tech is regurgitated old tech, but most of your examples aren't really comparable. reply nine_k 19 hours agorootparentI would say that my examples are rhymes, different developments of the sane theme. They are not literal repetitions, of course; comparable, not identical. reply _heimdall 17 hours agorootparentprevBasically all of Tailwind CSS. Inline styles are nothing new, neither are utility classes, or the scalability issues of inline styles that led to Tailwind reinventing classes with their `@apply` macro for creating component classes. Edit for another: RPC calls are really old and went out of style maybe 15 or 20 years ago in most codebases. Most of the modern JavaScript metaframeworks are now using RPC calls obscured by the build/bundling process. reply vitaflo 14 hours agorootparentThank you for mentioning Tailwind. Every time some young dev talks about how Tailwind is \"forward thinking\" I just want to scream into a pillow. This is also the case now that SSR is becoming popular again. reply Aeolun 7 hours agorootparentI can deal with the SSR becoming hip again, but can we please settle on either back or front-end rendering? Either was good, but trying to combine the two is evil. reply Cthulhu_ 7 hours agorootparentprevSSR is the most mindblowing of the lot, it's gone full circle. I mean granted, I've worked with e.g. Gatsby for a while which is SSR on the one side but a hydrated SPA with preloading etc on the other making for really fast and low bandwidth websites, but still. reply karolist 16 hours agorootparentprevRPc calls ala SOAP may have been obsoleted but things like gRPC were and are the building blocks of many large companies. reply _heimdall 15 hours agorootparentSure, I'm not saying RPC isn't used today or that it doesn't solce specific problems. It is a reinvention of an old idea though. There was around 15 years where RPC rotted on the vine until Google brought it back for (mostly) the enterprise scale, and another 6 or 7 years before JavaScript frameworks rediscovered it again for fullstack web applications. reply rsynnott 11 hours agorootparent… Eh? The predecessor to gRPC seems to have started internally at Google in 2001, and Google open-sourced it in 2015. In 2001, CORBA was all the rage; by the mid-noughties this had been replaced with SOAP, and maybe Thrift rpc in trendier places. I gather there was a whole parallel Microsoft ecosystem with DCOM and things, though that wasn’t my world and I don’t know much about it. But the point is that there hasn’t been a time where some form of RPC wasn’t in fairly common use since at least the early 90s. The details change, and each one tries to solve the problems of the past (typically by inventing exciting new problems), but conceptually none of these things are _that_ different. reply _heimdall 7 hours agorootparentI may have completely missed a generation of RPC tooling. I was thinking specifically about web development in this context, but in general I don't remember hearing anything about RPC use between the early 2000s and mid to late teens (other than legacy systems maybe). reply tra3 21 hours agorootparentprevWatching web tech evolve is a good example. So much churn rebuilding the same thing over and over. reply datavirtue 6 hours agorootparentAnd never once reaching parity with desktop UI frameworks. Not even close. reply mattgreenrocks 4 hours agorootparentWeb frameworks barely even abstract much. You still spend so much time marshaling things in and out of strings everywhere, and cramming information into URLs. Mind-numbing makework, really. reply datavirtue 6 hours agorootparentprevAI. A lot of the things that are \"new\" were just waiting on hardware advances and cost reductions. reply zo1 10 hours agorootparentprevMono-repos are now coming back with a \"hipster\" shine to them, with fancy in-repo build systems and what not. What's funny about this example is that it's arguably not even that much of a time-difference between the two epochs of forgetting and re-learning. It's just that everyone jumped on the microservices bandwagon so much that they couldn't deal with it in a mono-repo context, so they dumped it and convinced the world that many smaller repos was \"better\". Then they learnt the hard lessons of distributed and complicated version dependencies and coordinating that across many teams and deployments. Their answer to this? Not back to mono-repos, no no no, semantic versioning dude, it's the hip new thing! When that was a bust and no one could get around to being convinced of using it \"the right way\", they were forced to begrudgingly acknowledge the value of mono-repos. But not before they made a whole little mini-industry of new build or dependency systems to \"support\" mono-repos as if they're just lots of little repos all under a single version-controlled repo. These days I get this kind of stuff: \"Hey you guys wrote this neat module as part of your project, can you separate it out and we can both share it as a dependency? Because, you know, it's a separate little mini-something inside of their codebase.\" ...Only to then be told that separating it out would \"ruin\" their \"developer experience\" and people would have to, gasp, manage it as a dependency instead of having it in their repo. /rant. It's really hard not to be shocked and disgusted at this level of industry-level brain rot. I never thought I'd be \"that guy\" complaining about my lawn, but seriously, our industry is messed up and driven by way too many JS hipsters and their github-resume-based-development. reply t43562 1 minute agorootparentIt's not the cycling that's the problem but that one can do nothing to stop it. People (perhaps including me) are dumb and insist on learning by making the mistake. mattgreenrocks 4 hours agorootparentprevThis is kinda why I really, really dislike the \"social coding\" meme that went around in the 2010s. I get it, it's a team sport. It's just that the more people you put on your \"team\" the less agency everyone feels because responsibility gets diffused and it becomes more about about the \"team\" and less about actually doing the thing. reply KronisLV 7 hours agorootparentprev> Only to then be told that separating it out would \"ruin\" their \"developer experience\" and people would have to, gasp, manage it as a dependency instead of having it in their repo. I hate having to do this, because then I have to get Nexus working with whatever the package manager in question is (Maven, npm, pip, NuGet all have different ways of publishing packages), setup CI for the publishing and god forbid I also need to manage the Nexus credentials for local installs and possibly even might have a Git submodule somewhere in specific cases, which also confuses some tooling like GitKraken sometimes. It does prove your point, but honestly dependency management is a pain and I wish it wasn’t so; separating a module from your main codebase and publishing it as a package should be no harder than renaming a class file. reply pjmlp 22 hours agoparentprevSimilar age point, the problem is not keeping up, is fighting the continuous push to management, which I don't plan to ever do, unless forced by life circunstances. It appears that the only path left for us in many European countries, is to go freelancer, and I vouch for the same problem regarding skills, forget about having Github repos, or open source contributions, if the technology company X is looking for isn't the one we haved used in the last 5 years or so on day job. reply idostuff 18 hours agorootparentI'm a European in my late forties and have been a freelancer for the last five years but I find it harder and harder to motivate myself to continue working. What really takes all joy out of working as a software engineer these days for me are the endless Scrum ceremonies almost all companies in my area have embraced. In the old days (say until ~7-8 yrs ago) I didn't have to attend very many meetings but of those I had to go to most were useful/necessary. These days I could probably count the useful meetings I attend in a year on one hand but the amount of Scrum-worship-meetings per week requires two hands. The same amount of actual work I could do in a week in the old days would now take several months because it needs to be planned in detail. And no, not any technical detail, but rather discussions on how to divide it into stories but without doing any proper technical analysis and then straight ahead to story point guesstimates, yay! Then after a brief period of actual coding it's stuck in code review for weeks because no one will look at a PR unless prodded with a stick. While I do think that code reviews can some times be beneficial, most of the time they are (in my experience unfortunately) pretty useless. Most comments (and I have to admit I'm guilty to this as well) are more bike-shedding than bug-preventing. Complex bugs are rarely found in code-reviews in my experience. While these are my experiences during the last 7-8 years or so, it's more or less the same on all the half a dozen companies (or so) I've worked for during that period (which is also a very big reason why I've worked on half a dozen companies in that period). reply ilkkao 10 hours agorootparentI've similar experiences about Scrum. In the worst case there's one or more developers, usually junior, in the team that are very eager to improve processes. Eventually it's tenth time you are forced to discuss what's the optimal way to define story points. reply intelVISA 15 hours agorootparentprevTech became too profitable to be left to \"those nerds\" so now you have very bloated orgs. Though a freelancer should be able to sidestep the grifters unless you're selling yourself as an employee for some reason. reply idostuff 9 hours agorootparentYeah, my first year as a freelancer was quite sweet actually. Then came the pandemic and me and my spouse got ourselves a vacation house as we couldn't travel any more. While this was a great relief for our mental health during the pandemic, it meant a much higher mortgage so I needed the more stable income. reply bitwize 15 hours agorootparentprevDoing software right will require a lot of planning, irrespective of whether that planning occurs up front or as you go. If you plan more up front, that will eliminate a whole lot of guesswork when the time to do the programming comes. You need systems analysts -- generalists who understand the business and work well with people -- to come in and characterize, in detail, how the business currently works in terms of systems and subsystems, and then propose and design new systems, again to a high level of detail. Once that's done, inasmuch as you need software, producing the software is a simple matter of translating the detailed requirements into language for the machine. Unfortunately, modern methods are basically just institutionalized guesswork: this is what Agile is all about. It's a methodology designed by programmers for programmers, in order to bamboozle management and inflate the programmers' own sense of self-importance. The correct way to design a business's internal systems, including but not limited to its software, appears to have been forgotten, except a pastiche of it lives on as a strawman called \"Waterfall\" for Agilistas to take down. reply bartimus 12 hours agorootparent> is a simple matter of translating the detailed requirements into language for the machine. This is actually the hardest part. I can write detailed requirements about the car I need. Create a PowerPoint presentation that shows a schema of the system and subsystems; the engine block, transmission and steering wheel etc. with lines how they are connected. That's the easy part. Now you need the team of skilled engineers developing the actual car. And you need them to be experienced and good at it. You need at least one guy who is able to load a complete mental map of everything that's needed to be engineered. Who understands the business requirements and is able to create a vision for the product and technical solution. He needs to understand databases, web services, authentication, authorization, security, performance, web standards back- and front-end solutions. Be smart about what logical components are needed and have an high level idea how they could be implemented technically. Ideally that guy can also open a repository and read what's going on. Especially with larger corporations there's still so much potential for automation. Yet what we see is a big fragmented mess. Systems and subsystems that are poorly integrated. Exactly the car you'd expect that was designed in PowerPoint by non-engineers. reply idostuff 9 hours agorootparentprev> Doing software right will require a lot of planning, irrespective of whether that planning occurs up front or as you go. I'm not opposed to planning but I'm opposed to the kind of meta-planning game that is wont when scrum is involved. I've been in meetings where the thing we're planning is literally to change one line of code and we say as much but the PO still insistently asks if it shouldn't be multiple stories. The whole thing eventually took man days in meetings even though we insisted it was extremely quick. Turns out the whole thing was sold upstream to management as a big feature so a single 1-point story wouldn't cut it. As a contractor I can at least remind myself that I'm getting paid for sitting through all those meetings but as someone who likes to actually do things I feel like I slowly die inside. reply FpUser 17 hours agorootparentprevI am 63. Canada based. Have no problems keeping up with tech. I am on my own since 2000 and mostly develop new products for various clients. Have couple of product for myself that bring some dosh. The range is very wide. Microcontrollers, Enterprise Backends, Desktop, Multimedia, Browser based, etc. etc. It is not programming per se that keeps me going (I find it boring enough) but designing systems and interactions from scratch and then watching it work. reply irrational 21 hours agoparentprevI’m in a similar positions (in my 50s with a family to support). For the most part I can get my boring corporate work done fairly quickly. Then I spend some time each day working on personal programming projects where I get my true satisfaction. reply OnlyMortal 22 hours agoparentprevI’m 55. Started as a 6502 cracker on the C64. I still get enjoyment out of some coding - C++ on Linux for enterprise applications - but I do miss the “magic”. reply dqh 21 hours agorootparentAre you involved in the still-thriving C64 demo scene at all? Possible way to reconnect with the magic if not. Especially by attending (in-person, ideally) one of the many demo parties around the world. There are also parallels with embedded device and FPGA work that I personally find thrilling. Plus we on the VICE (open source Commodore emulator) team are always looking for devs. reply OnlyMortal 11 hours agorootparentI bought a C64 and SD card a few years ago. I enjoyed running up a few technical masterpieces - like DropZone - but the gaming interest has waned. I don’t code 6502 nowadays but I’m active on r/c64. reply dqh 10 hours agorootparentFWIW I find the vscode + kickassembler + VICE toolchain a pretty fun way to iterate on C64 code. reply cmrdporcupine 22 hours agorootparentprevThese two perspectives are not incompatible. 49 here and still love programming. But only discovered that after quitting my Google job and spending a year working on my own things. Then housework wasn't getting done because I was writing code instead, and I realized I just love doing it, still, and I'm a far far better programmer than I ever was 20 years ago. I can do things I only dreamt of back then. And faster! but that's not the same thing as enjoying writing the dreck that many employers want, and keeping up with their endless stack of messy JIRA issues, planning meetings, poor design docs, and management shenanigans.... reply nine_k 21 hours agorootparentI sometimes think that big corporations pay more because the actual work there sucks more for an engineer (likely to a manager or a sales, too). reply YZF 20 hours agorootparentprevSame age. Got started on a ZX-81 and a university mainframe. I still enjoy writing code or shall we say solving problems via code. I still get excited about new things. I'm also a manager and I enjoy helping others. What I enjoy less is the politics. Building things is fun, I don't think this goes away, it was always fun and is still fun. reply OnlyMortal 11 hours agorootparentGood point. It’s the problem solving. Thing is I’ve solved so many problems, over the years at different companies, that there aren’t many new ones. Obviously, I can knock out the code quickly to the surprise of many. It’s just experience and I’m not a magician. Like yourself, I enjoy helping others, younger coders in my case, work through their problems. I guess that’s why I keep having to switch teams to pull them out of the quagmire they’ve gotten themselves into. reply cmrdporcupine 22 hours agoparentprevThe thing is ... The industry needs us. It's making a mess all over and valorizing complexity and novelty. Constantly. Programmers with experience in our age range have, I think, a better sense of how to manage this and encourage simplicity (partially out of necessity). But age and novelty bias in our industry means this knowledge doesn't pass on. It's tough to tell younger engineers that have cut their teeth swimming in intricacies and edge cases and integration nightmares and constantly surfing on the edge of chaos, and managing it, that they're likely contributing to the problem, not fixing it. But someone needs to. I can't remember details like I used to, things mark&sweep out of my brain much faster they used to. (Probably not just because I'm older but because as a parent, home owner, and spouse... I just have a lot to manage on top of it.) But.. really... a good system, a well-built system ... should be resilient to that, and people with experience.. that's hopefully what we build. reply alemanek 20 hours agorootparentI have been lucky enough to have been the youngest person on every team until my mid 30s. I worked with some truly gifted engineers, who had almost no ego, over the course of my career they just were much older than me. When I reflect I do cringe a bit at what I was zealous about and things I took way too far. But, I do think the discussion, sometimes debate, around the fancy/new vs tried/true resulted in much better results. Now that I am old, but not that old, the younger engineers who are passionately discovering new tools and “new” design patterns keep me interested in software development. Being able to share where things come from then we can compare/contrast together. It is rarely a straight copy and it’s fun to see how things get better/worse with reinvention. So, I think trying to get a mix of ages on a team is really beneficial. Passionate young engineers help prevent the old engineers from getting too jaded. reply cmrdporcupine 18 hours agorootparentI too was always the youngest person on every team... until I wasn't, and it seemed like I went from youngest to oldest in a blink and I still can't figure out how that can happen. I got into the industry during the .com boom with no degree, without finishing university, so kind of jumped the queue, age-wise, I guess. And yes, I often cringe in remembrance of past-self. I cringe at present self, too, though :-) reply away271828 19 hours agorootparentprevOn the other hand, you have to guard against being that person who is in a perpetual state of \"Benn there, done that. Didn't work the last 5 times we tried it.\" Because sometimes the circumstances/market/tech ecosystem genuinely are different. reply bruce511 14 hours agorootparentThe key us in understanding why the previous times failed. What constraints existed then, which possibly no longer exist now. Projects fail for many reasons. Technical, market, capital, time and so on. But things change. Building an add-on for electric cars would likely fail 20 years ago, again 10 years ago. But now? Or 10 years from now? Only by -really- understanding what caused a project to fail can you determine if that barrier is no longer in place. Which means you can try again, and potentially find the next barrier or success. reply cratermoon 15 hours agorootparentprevI'm in the retirement age bracket. My last experience as a consultant led to disgust with valorizing quantity of work as measured by an arbitrary metric susceptible to simple gaming. That, and prioritizing \"new\" features over maintenance because the former were booked as CapEx work, thus amortizable, while maintenance was booked as OpEx, combined with companies wanting to minimize CapEx ratio for accounting purposes. Notice how little of what is measured and managed has anything to do with building working software to satisfy user needs? reply dheera 19 hours agoparentprevFor personal projects I make heavy use of LLMs now and coding is still fun when I do it with the latest and greatest tools. I'm about 5X as productive as I would be if I had to crank out code myself. I'm used to verifying code with a compiler/interpreter and a unit test -- not by going through my code line by line and declaring to an interviewer \"yes I think it's correct\". My way of doing things is to just run the damn thing with the right tests and it will tell me if something is wrong. Unfortunately job interviews these days are still hellbent on whiteboarding Leetcode problems. I'm past that. Unfortunately they aren't. It's this kind of BS -- not being allowed to use the best tools that exist -- that makes me not want to code for work anymore. reply peterldowns 23 hours agoprevI respect the OP's vulnerability and the advice. I've felt like it was \"time to go\" before, but as a young man I just assumed it was burnout, treated it that way, and got back in the game once I had renewed desire. Right now I feel like I'll never want to stop making things, but that if I were rich enough and good enough at creating in a different medium other than code, I completely understand the desire to walk away from the terminal and never look back. Few things have been as frustrating to me as programming. Yet since few things have been so rewarding, I persist. It's a great article because it's making me think about my own life. I'll keep pondering. Thanks for posting it. reply Aperocky 3 hours agoparent> walk away from the terminal What do you mean, this is the best part of the job, the part I look forward to most each day. reply peterldowns 1 hour agorootparentI mean that sometimes I get so sick of the constant problems and churn and breakages and errors and mistakes and bugs and weird behaviors that I feel like doing something completely different. If you never feel that way, I'm very happy for you! reply Aperocky 27 minutes agorootparentIt does happen, but I find it overall more preferable than documents, tickets, meetings, meetings and meetings. Spending time in the terminal with vim and shell is healing. reply ricc 23 hours agoprevKinda similar to how Kobe Bryant knew it's time to retire from the game of basketball. He said in an interview (https://youtu.be/Ya8hY0S-8t0?t=54) that he knew it's time when during his morning meditations, his mind will not drift to basketball anymore. reply bugthe0ry 8 hours agoparentWhat I wonder is how did Kobe know this was a sign that he was done. Why didn't he think he was just burned out and maybe just needed a break before returning to the game? How can you tell when you're burned out vs. just being completely done with it all? reply ricc 8 hours agorootparentI think because he was already 37 or 38 at that time, and was suffering from injuries, too. There was no point continuing if both his body and mind are not the way they used to be. reply Cthulhu_ 7 hours agorootparentThis sounds about right for most pro sports; few continue as players in their 40's, especially in the higher impact / energy sports like basketball. reply bugthe0ry 6 hours agorootparentI meant applying this mentality to programming - how can one differentiate between burnout and being done with the field? reply linearrust 1 hour agorootparent> how can one differentiate between burnout and being done with the field? Money. If you are financially independent, you are done with the field. If you are financially dependent, you are burnt out. reply mattgreenrocks 4 hours agorootparentprevTime and wisdom. I think most devs can delay burnout/leaving the field if they begin viewing programming as a means to an end, e.g. \"programming as a way to build their own business,\" or \"using programming knowledge to mentor others,\" or \"using programming knowledge in another domain they're interested in to great effect.\" reply joemoon 19 hours agoparentprevThank you, this was a very interesting interview. reply ralphc 19 hours agoprevI retired in 2017 and sometimes I think I got out at just the right time, or close to it. In the past 7 years we’ve had the pandemic, remote work then the clawback to the office, and so. many. JavaScript frameworks and changes. And now layoffs all over the place and having to keep up with AI to stay relevant, AI and LLMs are changing faster than React and its ilk. Instead I now program in a great language, Elixir, working on projects that I want, and reading books that I’ve been putting off for decades. reply stevage 19 hours agoparentHuh, that's about the same time I became a full-time freelance frontend dev. I've primarily used Vue that whole time and it's been pretty good. AI has mostly been a nice benefit to - Copilot really makes writing code more pleasant. I haven't really seen any downsides to AI in my work. I'm almost always remote, and mostly like that, too. reply rsynnott 11 hours agoparentprev> having to keep up with AI to stay relevant This, at least for the time being, seems more a thing that people worry about than a real phenomenon. reply munchler 21 hours agoprevI’m in my late 50’s, and I still love making software, maybe even more now than when I was younger. What’s happened to me over decades as a professional is that I’ve totally lost any interest in “career” or the large corporate entity that employs me. Once any organization grows beyond about 20 people, it starts to become dysfunctional, so I’ll be retiring the day I can convince my spouse we have enough money. That will give me more time to work on things I care about, including software. reply WWWMMMWWW 20 hours agoparentCall it the blackjack rule ... once you cross 21 it's a bust. reply latentsea 18 hours agoparentprev>Once any organization grows beyond about 20 people, it starts to become dysfunctional I guess if that's the way absolutely everything functions then perhaps dysfunction is actually just function. reply MathMonkeyMan 8 hours agorootparentA large org is a totally different beast from a small org, and yet they seamlessly transition into each other. So, a large org is dysfunctional when viewed through the lens of a small org. A large org is also dysfunctional when it is run like a small org, as is a small org when it's run like a large org. reply ChrisMarshallNY 17 hours agoprev> I still write code every day in support of my generative art practice. The code is much more complex than anything I did previously, and much of it does not have anyone else doing it, so it's a lot of invention, which is fun. Can relate. I've been \"retired,\" since I was 55, and SV was nice enough to let me know that I was too old to play in their pool. Pissed me off, something fierce, but, in the long run, it's the best thing that ever happened to me. I could have made millions -for other people- maybe for me, as well, but I have never really been interested in that kind of thing. The work and the technology has always fascinated me. I've found that what I really enjoy, is making UI tools for nontechnical folks. That's what I do, these days. I make free software for folks that can't afford the kind of stuff I do. reply Joeboy 21 hours agoprevHopefully people will tell me why I'm wrong, but right now programming is just feeling like a bit of a dead end in general? The demand seems to be for AWS gurus, data analysts, low-code, prompt engineering etc. I'm not against learning new things to stay employable, but the new things that are in demand don't really seem to be programming. I learned a bit of Rust because it's kind of new(ish) and exciting, but apparently there's a massive glut of Rust devs. Whereas 15 years ago I learned Python and my employment prospects rocketed. reply nyarlathotep_ 17 hours agoparentThe nature of the majority of programming work now is not enjoyable IME. Seems the majority of work is overly complex (in terms of \"system design\") CRUD stuff that uses whatever constellation of \"Services\" are cool this month, and \"solving problems\" that Ruby on Rails or ExpressJS solved like ten years ago, but now with way more yaml configurations and and other imposed complexity for dubious gain and benefit. The new hyper focus of LLM chatbot hype isn't helping either. reply fifilura 21 hours agoparentprevWhat is it that you want to build? I mean if you frown upon AWS gurus, analysts and low code? Programming for the sake of \"writing code\" is probably going to miss the target. For example \"analyst\". My take is that is where it all started. Someone looking at numbers and needing computers to help making sense of them. reply pdimitar 21 hours agorootparent> Programming for the sake of \"writing code\" is probably going to miss the target. Why do you have to be so demeaning? I'd argue almost nobody is \"writing code for the sake of writing code\". In my case I love solving problems with code. Not by clicking through AWS' terrible website. Not through taking a deep breath and trying to reformulate a ChatGPT prompt for the 17th time. reply mjr00 19 hours agorootparent> Not by clicking through AWS' terrible website. Most places with a decent level of engineering maturity are using some form of infrastructure-as-code (Terraform/OpenTofu, Cloudformation, etc). Though more broadly speaking, it's true that software developers are now frequently expected to move beyond just compiling a JAR file and calling it a day. Expectations of knowledge of the underlying infrastructure that's running your code and how to operate it is more common than it was 15 years ago. I consider this a good thing overall though. > Not through taking a deep breath and trying to reformulate a ChatGPT prompt for the 17th time. I don't know anyone who's doing this at their programming job. GenAI is really good at 1) acting as an enhanced, customizable StackOverflow replacement for specific one-shot algorithms (\"given a pandas dataframe with these columns, write code that groups by X and gets the median of the top 3 values\"), and 2) pumping out boilerplate code that wasn't interesting to write anyway, like object mappers and certain unit tests. The tougher problems around software architecture, class design, and the trade-offs are still fully in the realm of humans, for now. reply pdimitar 18 hours agorootparent> Though more broadly speaking, it's true that software developers are now frequently expected to move beyond just compiling a JAR file and calling it a day. And you are demeaning as well for no reason. I even went out of my way to clarify I like SOLVING PROBLEMS WITH CODE, not \"just compile a JAR file\" which you conveniently ignored and pushed your narrative. Not cool, dude. > Expectations of knowledge of the underlying infrastructure that's running your code and how to operate it is more common than it was 15 years ago. I consider this a good thing overall though. I don't deny it on the premise but again, most vendors want to lock you in so their UX is terrible and specific. I had much more fun making scripts and cookbooks that setup a VPS for my customer's app. Nowadays this has been mostly remedied by Dockerfiles though integrating with k8s and its 5000+ friends is making me want to retire for the next 3 lives. > I don't know anyone who's doing this at their programming job. I don't do it either but I've met plenty of \"programmers\" who do, and swear by it, even though they had to chase 2-3 subtle bugs that took them 12+ hours to find and correct... whereas just writing those 150-200 coding lines would have taken them 4 hours tops, tests included. It's quite funny. --- My bigger comment here was to criticize the very weird direction the area is trying to go to. It will fail btw. Marketing people are pushy and get their way... INITIALLY. Sooner or later reason prevails. reply hypeatei 19 hours agorootparentprev> Not by clicking through AWS' terrible website Look into IaC (infrastructure as code) which all major clouds, and even smaller clouds support. Much more sane way of managing resources. > trying to reformulate a ChatGPT prompt for the 17th time Is the company mandating you use AI to solve problems... or? Anecdotally I don't use AI very much at $DAYJOB, nor do any of my co-workers. reply pdimitar 18 hours agorootparent> Look into IaC (infrastructure as code) which all major clouds, and even smaller clouds support. Much more sane way of managing resources. No, sane way is automating it 100% with zero UI required. But you do you. > Is the company mandating you use AI to solve problems... or? Anecdotally I don't use AI very much at $DAYJOB, nor do any of my co-workers. As mentioned in a reply to your sibling comment, I don't do it because I was sure from the get go that it will only get some algorithms right, and only for the most popular languages, and I was on point. But I had fun watching colleagues banging their heads against the wall many times. And again as per the reply to the sibling comment, I was commenting on the general \"future\" state of the area. reply hypeatei 18 hours agorootparent> sane way is automating it 100% with zero UI required Umm... that's what IaC is for, you write resource blocks in a file then use a command to deploy said resources. reply kortilla 14 hours agorootparentprev>No, sane way is automating it 100% with zero UI required. But you do you. Infrastructure as code means text, not a UI. Please Google the things people are suggesting before getting confrontational reply pdimitar 8 hours agorootparentDon't you still have to prepare stuff in the vendor-locked UI beforehand? Or is it much better these days? reply jillesvangurp 13 hours agoparentprevI'm turning 50 in a few months. I still enjoy coding; and I expect to be coding for another few decades. One thing I figured out early on is that your choices in languages and tech really matter. There are only so many things you can learn and you have to make some educated bets on things getting traction or not. And if you make the right bets, it's easier to keep your skills fresh and relevant. Some things look fancy and nice and then five years later it's all outdated and obsolete. And some other things go big. Java was one of those things and in 1995, when I was in university, they decided to use it for teaching programming to first year students. So I ended up being a teaching assistant and now have nearly thirty years of experience with the JVM ecosystem. I recognized the signs of the platform and language (especially) going a bit stale about fifteen years ago. It's becoming the Cobol of my generation (plenty of work but not the kind that gets me excited). I realized I needed to move on if I wanted to stay relevant. Since then I've touched a lot of languages. Right now, I do a lot of Kotlin but I keep an eye out for new things. Kotlin was a bit of a bet ten years ago. I fully committed to mastering it six years ago. And at this point it's starting to feel like a good bet. The language is modern, has a lot of momentum and there's lots of interesting stuff happening with the language, compiler, tools, etc. Particularly multi-platform is opening up a lot of possibilities. I've dabbled with other things along the way but never really got the feeling that mastering that stuff was worth my time. E.g. Ruby was interesting but it's now mainly used by people in their forties (i.e. my age). Younger generations seem to not be interested in it. Same with things like Scala. Lots of stuff still happening with both of course but it seems that they are both a bit past their peak. Python on the other hand keeps surprising me by not getting replaced with something else. I kind of like the language and have done some things with it over the years. And I like that they are clearing out technical debt (like the GIL) and keeping the language fresh. I work with some twenty year old interns that know and love it. People will be doing Python long after I die. That's a bet I didn't make but it would have been a good one. And not too late obviously. I know enough python to be able to jump in a project and use it. I've done so on a few projects in recent years. It's a very approachable language; kind of by design. reply ninininino 4 hours agorootparentHave you played with Go? reply cmrdporcupine 21 hours agoparentprevThere's two things going on. One is, yes, I think the quality of work mostly sucks all over. But the other is it's a down part of the cycle and there's just a glut of us all, and a bit of disrespect from employers as well. It's been a long time since we had one, and many people either didn't work through one before, or have forgotten. That part will bounce back. In 5 years it'll be a crazy job market again, and having Rust on your resume will be valuable. (To put it in perspective, I learned and wrote Python in 1996, 1997. And I really liked it. But nobody even knew what it was, and nobody would hire for it. I moved on, and lost my taste for dynamically typed languages, and then all the sudden Python was huge, and if I'd stuck with that, it would have been a big thing for me, I guess. I suspect a similar thing will happen with Rust, etc. At least I hope so, since Rust is my day-job :-) ) reply ghaff 20 hours agorootparentWork is work and always has it's plusses and minuses. But, yes, even if the tech cycle isn't terrible at the moment (e.g. dot-bomb nuclear winter) it's definitely down. I somewhat regret effort and money I put in a couple of years ago to get myself setup to do various stuff post \"retirement\" because, while I haven't exactly been beating the bushes, opportunities haven't been falling off trees either. reply ggm 18 hours agoprevI always refer people to the Doris Day performance of Carl Sigman and Herb Madgison's hit song \"enjoy yourself, it's later than you think\". https://www.youtube.com/watch?v=nQxsG9Vcndw There's also a Guy Lombardo and a Louis Prima version but I like this one. I've been singing this at work for a year or so, trying to give people a gentle hint about my future. reply imiric 22 hours agoprevThis is a great retrospective. Thanks for sharing. > It's not worth working and being miserable. Agree 100%. I've quit several jobs after the environment becomes more stressful than fun. Over the years my tolerance for BS has lowered, possibly to the detriment of my bank account. But I've never regretted my decision to leave. The weight off my shoulders is priceless. > Age and ability are not correlated. I wonder how subjective this is. Cognitive decline with age is real, but maybe keeping the brain active with programming can help keep it at bay. A study about this would be interesting. reply dagss 20 hours agoparent>> Age and ability are not correlated > ... cognitive decline.. I know this is not the age groups you thought about, but on the topic: I think they ARE correlated, but the other way: At 40 I have had time to get to know so ridiculously much more than someone starting out in their early 20s. And I see its effect very real, people in early 20s (generalizing ofc) can spend so long on things on have seen so many times...or spend more time making lots of bugs and finding them than just writing the code with fewer bugs. Or spend their brain cycles on the \"how to code\" part of the job, instead of that just being second nature and focusing on the underlying ideas. Or young people may be be competent coders, but completely baffled reading and really grasping underlying ideas in existing codebases (especially this I know I have progressed at with training over the years..) I feel experience can be undervalued in our industry in a way it is not in others. It is valued... but not as much as I feel it should be.. Of course this effects drowns a bit in the noise of all of the programmers like the OP talks about that barely get by, in all age groups. But within the set of skilled coders... from what I have seen, I would always prefer working with the older to the younger to get a project done.. (Ofc there may be a point where this turns. I lack personal experience with coders 20 years older than myself.) reply anal_reactor 7 hours agorootparent> But within the set of skilled coders... If you ever manage to come up with a reliable way of identifying skilled coders, you'll be very, very rich. At my previous job we had 20 interns and one senior who was like 50 and his attitude boiled down to \"why can't we just keep doing things the way I was taught when I was a student\". At my current job there's me, another Junior, and a Senior. The other Junior works very fast, very well, always has valuable input in discussions. With the senior I need to work carefully, because while the guy has knowledge in certain areas, he misidentifies priorities, makes mistakes, doesn't communicate shit, while at the same time demands things to be his way because he is the senior so he has authority. On top of that his English sucks so every meeting in which he's involved takes three times as much time as it could. reply davidgay 13 hours agoparentprev> Cognitive decline with age is real My father worked as a consultant designing analog-style ICs until his mid 70s - his customers were therefore presumably happy to pay his consulting rate. I’m going to vote for “early” cognitive decline being overrated… reply will1am 22 hours agoparentprevTo prioritize well-being over enduring a toxic or stressful work environment reply GnarfGnarf 19 hours agoprevI've been happily programming since 1965, and I've been doing C++ since 1995. There is still so much to learn, it's never stopped being fun. However, Apple Notarization may just be the straw that breaks the camel's back. reply jasonkester 11 hours agoprevI consider myself retired, but I have a different take on the concept. I like programming computers, but just not 2000 hour a year. I can afford not to do that, so I don't. I hit a point in my 30s where I could sock away a year's worth of savings in 3-6 months of contracting, so that's pretty much where my full-time phase ended. I came back \"out of retirement\" when the first kid was born and worked 5 years semi-fulltime to save up enough for houses, college, etc., ramping down to 4 day weeks for the last few years because I really value my free time. Since then, I've done the odd 3-6 month/year stint (since programming and working on a good team that can ship is still pretty fun.) Recently I've been doing that part time, 2-3 days a week, a few months a year. I don't know what most people would call my situation. I call it Retired as I want to be at any given moment. I expect I'll keep doing it for the dozen-odd years between now and when I hit \"Retirement Age\". But maybe not. It's almost more of a hobby at this point. I guess the point is that it seems like a silly idea to do something all day every day for most of your life, then suddenly drop it completely. If it was fun, do more of it. But on your own terms, and only enough that it's still fun. reply MathMonkeyMan 9 hours agoparentYour story resonates with me. How did you get into contracting? reply lenkite 22 hours agoprevI really thought this was going to be a post on The Go language when I clicked the link. reply bxparks 19 hours agoparentMe too! The problem with Golang is that it has the same name as a common verb instead of a noun (\"go\" is used as a verb like 99.99% of the time). I remember coming across a thread maybe 10-14 years ago where the Golang creators were asked to change the name of the language. They declined. If I recall, one of the arguments was that the name would naturally become associated with Golang. Here we are in 2024, and the confusion still happens. The TFA blog was great, by the way, even though it was not about about Golang as I had expected. reply divan 10 hours agoparentprevSome comments here: \"I'm at age XX, thought about quitting, but now am rediscovering fun in programming\". Go is a good fit here. I discovered Go around 10 years ago, and it was a point in my career where I was fed up with the overgrowing complexity of the mainstream languages and cultures around them. I was seriously considering switching to other fields. Go has changed that direction 180 degrees. reply koinedad 22 hours agoparentprevI was with you on this reply corpMaverick 3 hours agoprevI have been programming professionally since 1989. I think my strength is to find straight forward solutions where you can focus on delivering value reliably. But now days I feel that are so many smart people jumping from project to project and leaving mountains of technical debt behind. We spend a lot time managing accidental complexity and I no longer enjoy my job like I used to. reply Kagerjay 11 hours agoprevI've only been programming for 6 years. I don't feel the same burning passion as I did when I first started coding. I'm a frontend developer, but I've made a lot of lateral switches into DevOps, backend, leadership, etc but I prefer just building what I'm good at though But I'm basically semi-retired to a degree in my field. I'm doing the bare minimal to get by at this point. I ultimately would love to quit some day, and pivot into a different career, not entirely related to coding. I'm not at that point yet financially though, and am spending energy elsewhere I would love to start a non-coding related business one day though. reply rr808 16 hours agoprevI'm still going, but the biggest issue I'm finding is that knowledge keeps turning over. I have made big efforts in the past to learn technologies, languages and libraries that became obsolete then starting all over again. Software Engineering now is completely different to what I started with. I have more Project experience but technically I dont know much more about Cloud, JS frameworks, modern DBs etc than someone 30 years old. Ironically my main advantage seems to be I can focus more and work longer hours than younger people who seem to value WLB much more than we used to. reply marcyb5st 9 hours agoprevI am repurposing the life lesson my grandfather imparted to me before passing. It was meant about one's sex life, but for me it's applicable also here: \"If the struggles outweighs the pleasure you should stop doing it\". reply shdh 3 hours agoparent> \"If the struggles outweighs the pleasure you should stop doing it\". Kind of bleak when it comes to romantic relationships reply swiftcoder 9 hours agoprev> They couldn't comprehend why anyone would retire. One of them, whom I had worked for for two of those jobs and always made his life easier, never spoke to me again or even said goodbye. If I had a nickel for every leader I've worked for who didn't know when it was time to go... I'd have 3 nickels, which is still a surprising amount reply away271828 20 hours agoprevFor me, it was very obvious at the end. Technical but not programming. Felt like I was winding down. Circumstances were such there wasn't a lot of mobility within the company. Was somewhat disappointed that I didn't get a package as part of some layoffs but I assume powers that be didn't want to voluntarily lose headcount. Ended up hanging around for a year effectively working part time. Not sure that was the right idea or not (had lots of vacation which I pretty much all took) but year+ passed by and it was pretty obvious at that point I couldn't drag my feet any longer and didn't have the interest or need to do a job search. reply hypeatei 22 hours agoprevI recently put in a word for a senior programmer I worked with in a previous job and he got hired. Well, it's really clear he doesn't care anymore and doesn't find anything about software development interesting. Now I'm in a tough spot because he's a major burden and my manager wants to give it some more time but I don't see it working out. I heavily relate to this line in the article: > Some time ago, I knew a programmer with the same number of years of experience as me. Yet he seemed unable to comprehend what was required of him, and I had to review everything he wrote because it rarely worked reply ghaff 18 hours agoparentIt sounds like you're describing two different things though, of course, they can look somewhat similar from the outside. There's unable and there's not caring. I can imagine not having some specific skill sets and I can imagine just not having the interest in putting in the effort and learning what's needed. The results may look somewhat similar but they're different situations. reply devjab 22 hours agoprevI’ve been in programming for two decades and one of the things I enjoy about it is that things change. I did my stint in both architecture and management because I thought what you were supposed to do, but I went back to programming because I like programming. I’ve worked on so many different technologies that I’ve probably forgotten more than some people even learn yet I’ve always liked it. I do get how you can burn out, especially on the business side of things. A lot of jobs just aren’t important. The trick is to avoid them if you can and leave them as soon as possible if you can’t. Every non-startup / non-economic boom job comes with some degree of Kafka, and you’re either going to learn to not care about it or go crazy. I’m not sure that is especially unique for programmers though, this seems to be most things. Unless you’re extremely talented at the HR part of organisational politics (which most programmers aren’t) you’re also going to have to build some really stupid stuff during your career because change management is hard. So hard that it’s virtually impossible for talented HR staff to do when the direction is upwards, which it’ll always be for programmers. Again, it’s something you either learn to laugh about or burn out on. The change in technology, however? Isn’t that part of the fun? If it isn’t, is that because you don’t have the time for it? Because if don’t (and a lot of jobs won’t give you this) then you’re frankly in one of those “leave as soon as possible” positions. Even so, niche work rarely dies. The author mentions mainframe work, but mainframe work is still some of the highest paid work in the world because those grey beards who actually know and want to do it are so retired that a lot of them are frankly dead. I’m not sure how you could ever work on mainframes for 40+ years and then not be able to get paid handsomely by banks. Anyway to each their own. It’s a nice perspective, and it offers you a few insights into just how much of a cog in the machine you’re going to be in virtually any job. Even one where you’re extremely well liked and rewarded. I think the best thing I learned from my stint in management is how everyone, and I do mean everyone, is replaceable. It’s just a matter of cost. Which can sound depressing, but it’s also very liberating because it teaches you to not get overly attached to jobs or employers. reply saulpw 18 hours agoparentTwo decades is only half a software career. I've been coding for 40 and it's different. See how you feel in another 20 years. Even in niche domains, almost all software is a massive tangle of unnecessary complexity. At some point it becomes like another game of Magic (The Gathering) with millions of cards and twisty little rules all alike. It's some teenager's idea of fun, but if you take it home to gramma, she doesn't understand why you can't just sit around the table and talk. reply randcraw 16 hours agorootparentI've been coding professionally for 40 years and especially don't care for how the management of software projects has evolved. Rather than design, I've seen the emphasis shift to process and testing, necessarily in the guise of 'agile' even when the design goals don't fit well with the agile manifest (like ML or scientific programming). The inability of IT management to imagine an approach to software other than \"one size fits all\" has left me increasingly disengaged in work that used to be fun every day. Of course, the shift in the past few decades from coding-from-scratch to cut-and-paste has also pushed me out of the 'flow' of slinging code that I long enjoyed. So maybe the insistent adoption by management of agile-oriented groupthink and processes should be a clear message to me that resistance is futile, and it's time for me to make way for devs who are happy to play a game whose rules have changed, one that I no longer enjoy as I once did. reply paulsutter 21 hours agoprev> You probably don't know any retired programmers Ha I know lots of retired programmers. I was one for a while, but like most I really wanted to get back to work reply pcl 19 hours agoparentHow long were you retired for, and what brought you back? reply paulsutter 8 hours agorootparent5-6 years, then I started another company reply jongjong 8 hours agoprevI'm nostalgic about the good old days prior to 2016 when every technology change meant improvement. Nowadays, tech doesn't change as much, but when it does, it's for the worse. It's been a challenge for me to adapt to the new reality of coding as a game of busy-work and lock-in through complexity. It has become a bit of a theatre for me, unfortunately. I know I could do something in a way that's 100x more efficient but it would negatively impact my job security so no thanks. Also, if I do the right thing, taking all the risk upon myself, nobody will appreciate. I'll stick to inefficient popular tools and methodologies. I'll play the game of Whac-a-mole... Like a bad gardner who pulls the weeds out by the leaves and leaves the roots behind. That's the smart move. I tried the other approach, doing my very best, outperfoming and it couldn't have worked out worse. The manager class feels nothing but contempt for people who outperform. \"Good boy! Here, have a pat on the back... Sucker.\" reply begueradj 12 hours agoprevInteresting insights. Thank you for sharing. reply Tao3300 22 hours agoprevRight now I'm in this goofy spot where I'm probably walking away from it. Nobody wants backend and API-layer Java devs anymore. They probably need them, but they don't know it in the midst of the AI bubble. reply beacon294 22 hours agoparentIt's actually still super popular at many or most large companies. Not small companies. reply jillesvangurp 13 hours agorootparentA lot of those companies are either looking like they'll never change (e.g. banks) or are actively transitioning to things like Kotlin. I've made that transition myself and at this point I'm seeing a lot of signs that this was the right move. E.g. Facebook, Google, Amazon, and others have used a lot of Java in the last twenty years and many of their teams are transitioning to Kotlin at this point (each of them have talked about this in public). With many millions of lines of code that's a slow transition obviously but Kotlin is apparently the goto choice for a lot of new stuff. Java is turning into the Cobol of our generation. People will still be doing this for a long time. But not a lot of young people are likely to want to do that. It's not an obvious choice for new projects at this point. reply tiew9Vii 7 hours agorootparent> Java is turning into the Cobol of our generation Despite speed of Java evolution accelerating and Java being the best Java has ever been, I agree with this. Everything is Javascript or Python these days, Java is seen as old, boring, hard, inconvenient as you need to compile it and often it doesn't compile. Javascript/Python happily run until they don't and fail at runtime which feels like it just works (until it randomly doesn't) and you can pick up a teach yourself Javascript/Python in 24hrs style book and feel you are a competent developer very quickly. A lot of Java positions I see now are maintenance roles, which isn't a great thing. Maintenance roles are never great, often seen as bottom tier developers. Unlike Cobol, there's likely many more people who have programmed Java than Cobol so even if Java dev becomes a niche I suspect they'll still be enough of them around that compensation is not anything significant. Kotlin is nice, let's see where it is in ten years. Closure, Groovy, JRuby, Scala are also nice but ultimately Java won as the modern Java and all those other JVM languages that once had interest and promise are now niche or completely dead. reply pjmlp 12 hours agorootparentprevKotlin isn't really used as much as people think outside Android, besides it is useless without the Java ecosystem it depends on, in a platform written in a mix of Java and C++. https://learnhub.top/the-most-in-demand-programming-language... https://newrelic.com/resources/report/2024-state-of-the-java... https://www.itpro.com/careers/29133/the-top-programming-lang... reply Tao3300 22 hours agorootparentprevThat's what I'd have thought, but it's looking bleak. Who do you have in mind? reply SoftTalker 22 hours agorootparentLook for corporate jobs outside of California/SV-type places. Financial firms, insurance companies, also universities. You won't be blazing any new trails and you might be shocked by the pay difference but you'll work 9-5 with your weekends free and probably good benefits. You'll have to be able to tolerate some level of Initech-style management but if you just accept that and play along the work pace is pretty relaxed. reply relaxing 20 hours agorootparentI’ve reached the point where every time I see a comment about “nobody wants to hire X programmers”, in my mind I append “… at hot SV startups” because that must be the only way it’s true. reply kridsdale3 22 hours agoparentprevGoogle does. reply boomersboo 21 hours agoprevnext [2 more] [flagged] 01HNNWZ0MV43FF 21 hours agoparentNever been so bored and frustrated that you just came in to work, dropped off your laptop, picked up your coffee mug, and walked out without a word? reply geraldwhen 22 hours agoprev [–] I make $300K/year. I’ll leave when I’m ready to retire. And if I’m lucky enough to get laid off (around retirement time), that would be a huge windfall to move me toward retirement. Programmers planning to work after 50 are fools. reply gambiting 22 hours agoparentYou're in like, 0.1% of programmers financially. I know living in certain bubbles it feels like everyone makes that kind of money, but it's absolutely not. It's like a lottery winner telling people that if they plan to work over 50 they are fools. reply damezumari 22 hours agorootparentBy freelancing you can save nice nest egg in most places. I did that for 11 years in Europe and now I work because I want to, not because I must. Disclaimer: not consulting anymore, I moved back to startup grind once more because I feel more connected to the work than what you do as a consultant. reply tra3 21 hours agorootparentFreelancing is another bubble I think. I don’t think most developers have the skills necessary to freelance and/or most enterprises are not setup to work with freelancers. reply 01HNNWZ0MV43FF 21 hours agorootparentprevI don't feel like being my own boss. Shrug emoji reply threadweaver34 18 hours agorootparentprevI'm sure it's my FAANG bias, but 0.1%? I would have guessed 5%. reply geraldwhen 21 hours agorootparentprevEven if you make half that, which is a common offer all over America, you can retire early. reply gambiting 21 hours agorootparentAmericans are their own little bubble again - I'd wager that most of the world's programmers don't live in US. Globally your average programmer will be someone writing utterly boring code making same salary as a teacher, maybe slightly above that if lucky. Happy to be proven wrong, but the whole \"If you aren't making 6 figures as a programmer you failed at life\" meme needs to go away - it's a tiny tiny sliver of all programmers that actually manage to achieve that. reply jltsiren 20 hours agorootparentprevRetiring early is more about expenses than income. If you can't retire early with $100k income, you probably can't do it with $300k either. Most people can't do it, because their expenses grow to match their income. They want bigger and better everything, and they always find new \"mandatory\" expenses. Especially if they have kids. reply dgfitz 19 hours agorootparentMy spouse and I make just a touch over the GP, combined of course. Our mortgage is 1.3k/month, and our daycare costs are 3k/month. In a year darcaye goes down by half, and 2 years after that we’re going to have money coming out our ears. I am fortunate my spouse was on board with this plan, which was mine. I’m 38, spouse is 36, for reference. Assuming we have college saved for, I don’t want to work a day past 55 if I can manage it. reply ghaff 18 hours agorootparentI probably dragged things out a year or three past where I prudently could have. Not sure if I made the right call or not. COVID messed up a lot of things (and obviously sadly killed a lot of people) and I might have made different decisions on a more normal timeline. reply EVa5I7bHFq9mnYK 12 hours agorootparentprevWith age, your ability to spend money greatly diminishes. Even driving to expensive restaurants is tiresome. The need to impress other people goes away, and rocking chair on the porch doesn't cost much. I'm struggling to spend 1/5 of my investment income. And the kids have forbidden me from giving them more money because they want to 'make it themselves'. reply BeetleB 16 hours agorootparentprevDid You factor kids in? reply shdh 3 hours agoparentprevHow much do you need to accumulate before you retire? I'm guessing retiring just means working on your own things with enough runway til you drop dead. reply whoknowsidont 21 hours agoparentprev [–] Programmers haven't always made this much. reply geraldwhen 21 hours agorootparentBut they have, plus or minus 40% adjusted for inflation. Soul sucking corporate jobs have always paid well. reply whoknowsidont 21 hours agorootparentThere are way, way, way more jobs that make 300k+ in our industry than there ever have been. It's not that these jobs have never existed, it's that they are in greater quantities. reply jjtheblunt 20 hours agorootparentprev40% is a big fraction reply YZF 19 hours agorootparentprev [–] During the .com boom (1990's) an intermediate programmer would easily command $100k USD/year in salary. Many made millions on stocks. Later how many programmer millionaires did the big tech companies make? 10's of thousands? 25 years ago. Adjusted to today that's ~190k USD in salary. Add equity, bonus, ESPP etc. There's ups and downs but I think there are lots of US software jobs that pay this today. reply ghaff 19 hours agorootparent [–] And a lot of those programmers in the .com boom ended up with stock that was worth zilch and a ton of people ended up leaving the industry in the 2001 or so period. I was incredibly luck to get laid off a couple weeks after 9/11 and to get another tech-adjacent job about a month later based on a lunch I had with someone I knew a few days after I was laid off. That was not a common experience in that period. (Mind you, that wasn't a great stretch for me in terms of compensation. But a later job made up for it at least by my standards.) reply YZF 17 hours agorootparent [–] There's always a range of experiences. Most of the people I know did ok through the .com bust and are still in tech. Some people had millions on paper and didn't sell and held on to it and not only was it worth zilch they also owed taxes but many really made millions. Pretty much anyone that had a job in FAANG or similar companies did really well from 2001 to today - a lot of people. Tech grew like crazy and took a lot of people with it for the ride. reply ghaff 6 hours agorootparent [–] Certainly the last 15 years in particular have been very good for a lot of people in tech even if they didn't hit a big jackpot in FAANG or wherever. I had a pretty ordinary tech industry job in that period and it set me up better than my whole prior career did. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author retired in 2021 after nearly 40 years as a programmer, not due to a lack of ability but a lack of desire to continue.",
      "They emphasize the importance of being honest about one's abilities and interests, noting that sustaining a long career in a rapidly changing industry is challenging.",
      "The author continues to write code for generative art, finding it more complex and enjoyable than their previous work, highlighting the personal fulfillment in pursuing new interests."
    ],
    "commentSummary": [
      "A retired Apple engineer reflects on the shift from engineering-driven to marketing-driven decision-making within the company, prompting their departure.",
      "Despite leaving the corporate world, the engineer continues to code for generative art, finding it more complex and inventive than previous work.",
      "The post highlights a broader sentiment among programmers who struggle with modern work environments and rapid technological changes, with some finding solace in personal projects or open-source contributions."
    ],
    "points": 304,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1720984589
  },
  {
    "id": 40962767,
    "title": "Fusion – A hobby OS implemented in Nim",
    "originLink": "https://github.com/khaledh/fusion",
    "originBody": "Fusion OS Fusion is a hobby operating system for x86-64 implemented in Nim. I'm documenting the process of building it at: https://0xc0ffee.netlify.app. Screenshots UEFI Bootloader GUI (Note: This screenshot is from the graphics branch, which is still a work-in-progress.) Booting Fusion Kernel Features The following features are currently implemented: UEFI Bootloader Physical Memory Manager Virtual Memory Manager Single Address Space User Mode Preemptive Multitasking System Calls ELF Loader Timer Interrupts Planned: Demand Paging Inter-Process Communication Disk I/O File System Keyboard/Mouse Input Shell GUI Networking Building To build Fusion, you need to have the following dependencies installed: Nim LLVM (clang and lld) Just Build Fusion with the following command: just build Running Fusion currently runs on QEMU, so you'll need to install it first. Launch Fusion with the following command: just run License MIT",
    "commentLink": "https://news.ycombinator.com/item?id=40962767",
    "commentBody": "Fusion – A hobby OS implemented in Nim (github.com/khaledh)298 points by michaelsbradley 20 hours agohidepastfavorite91 comments khaledh 19 hours agoAuthor here. Thanks for posting thisOverall, are you happy with your choice of using Nim? Yes. It's a pleasant language to work with. > What would you do different? (and what unexpected positives did you find)? A couple of things I think need improvements are: (1) better IDE support (especially for JetBrains IDEs), and (2) better support for true sum types and pattern matching[0]. As for unexpected positives, I found that the standard library covers a lot of functionality that I rarely (or ever) need a 3rd party package. Maybe that's because I'm not doing anything exotic. [0] https://github.com/nim-lang/RFCs/issues/548, https://github.com/nim-lang/RFCs/issues/525 reply kosolam 2 hours agorootparentprevWhat made you use nim in the first place vs any other lang? reply khaledh 43 minutes agorootparentI went over this in other comments, but basically the language appeals to me since: - it's close to Python in syntax (less noise, more readable) - has no garbage collector by default (it uses ARC) - has great C interop - can be optimized through the C backend compiler - can target bare-metal with minimal effort - supports inline assembly - has great template/macro system (I do use templates, but I haven't had the need for macros yet) There's probably other reasons, but those are the ones I could think of now. As for why not other languages, I think the only other languages suitable for this kind of work are: C, C++, Rust, and Zig. Here's my take on each: - C: The mother of all system languages, but outdated with lots of UB gotchas - C++: I don't like/need OOP, so why pay the prices of C++ complexity and manual memory management - Rust: I find the language too complicated for my taste. I know it's subjective, but it just doesn't feel right for me. Also writing a kernel involves a lot of unsafe code anyway. - Zig: I tried Zig and also found its syntax to be a bit too noisy. Also having to worry about allocators in most of the code distracts from the core logic I'm trying to focus on. reply alberth 14 minutes agorootparentThanks for the details. Any reasons why you would not recommend Nim? Or things Nim could improve? (Nim seems like this unicorn of languages, that's completely overlooked. And I don't understand why it's overlooked) reply sidkshatriya 6 hours agoparentprevI heard that null is a valid value for objects in Nim for most situations. Is that correct ? I like languages that disallow null by default (e.g. Rust, OCaml etc) because it seems to be a huge source of errors. reply netbioserror 5 hours agorootparentNo, it isn't. The type must explicitly be a nullable type or pointer to be nullable. All other values must have a valid initialization. Anything not marked as a pointer or ref is by default managed by its scope. This includes dynamic types like seqs and strings, which are pointers to heap memory but managed by the stack scope and deallocated upon leaving scope. reply khaledh 4 hours agorootparentprevTo be honest, I haven't found this to be an issue (yet). I try to keep most of my types value types (cannot be null), which the compiler can pass by reference under the hood if it detects that it's too big, without compromising memory safety. reply jasfi 5 hours agorootparentprevI use the Options module which has a none/some check. None is the absence of a value. You can test for this quite easily and I see it as a feature, not a bug. reply alberth 18 hours agoparentprevCongrats, fascinating work! Q: has the GC of Nim caused any challenges? (And if not, would you attribute that to Nim unique GC that does NOT “stop-the-world”?) https://nim-lang.org/1.4.0/gc.html reply khaledh 17 hours agorootparentThanks! As @Tiberium mentioned, Nim 2.0 defaults to using ARC (Automatic Reference Counting), so no runtime GC. The Nim compiler is quite smart about when to copy/move and when to destruct, with some hints like lent/sink for when you want to have a bit more control. Keep in mind that I also need to use ptr (as opposed to ref) types in a lot of cases when working at a low level, so there might be a need for some manual memory management as well. reply Tiberium 18 hours agorootparentprev1.4.0 is a very outdated docs version (Nim is at 2.0 currently, and 2.0 release brought ORC as default with ARC as an option), you should refer to the updated https://nim-lang.org/docs/mm.html and also https://nim-lang.org/docs/destructors.html reply pests 2 hours agorootparentNim should take inspiration out of other languages doc sites where a banner is placed at the top notifying the reader it is for a previous version with a link to the current version. Could also add a rel=canonical meta tag pointing to the latest version so search engines funnel people there. \"Nim gc\" on Google indeed puts you at the 1.4 doc page. reply alberth 27 minutes agorootparentSlight OT: are is the best documentation management system these day? ReadMe, Docusaurus, Mintlify, etc? reply edu_do_cerrado 12 hours agoparentprevAmazing work! What where the most troublesome parts of the project? Also, any tips if anyone want to write an OS from scratch, aswell? reply khaledh 6 hours agorootparent> What where the most troublesome parts of the project? Task switching. It's a very intricate process, you really have to understand how to structure everything (especially the task stack) so that you can switch everything from underneath the CPU as if nothing has happened (the CPU is obliviuous to which task is running, including the kernel itself). Add to that switching between user mode and kernel mode (during interrupts and system calls) and it becomes even more challenging. > Also, any tips if anyone want to write an OS from scratch, aswell? As @deaddod said, you need to read a lot. Two invaluable resources for me were the Intel Software Development Manuals (SDM), and the osdev wiki. The SDM can be daunting, but it's surprisingly very readable. The osdev wiki has great content, but it can be a hit or miss. I complement them with various blog posts and code on github to really understand a certain topic. That being said, the most important aspect of this process is to have tons of curiousity and to be passionate about low-level systems programming. I love to learn how things really work at the lowest level. Once you learn it, you'll discover that there's no magic, and that you can write something yourself to make it work. [0] https://www.intel.com/content/www/us/en/developer/articles/t... [1] https://wiki.osdev.org reply deaddodo 10 hours agorootparentprevRead. A lot. Basically, OSDev is one of the few realms you can't really take shortcuts in. It's kind of like learning Rust, in that you'll have a lot of foundational work until you get some real payoff. Unlike rust, however, there isn't just some cliff where you start getting it; it's a constant uphill trudge. Learning the boot process of your target architecture, adding core functionality (process scheduling, filesystem/VFS support, IO management, etc), adding driver support, supporting your video device (just getting a basic framebuffer, and then adding each piece after that), supporting USB, supporting CRT and POSIX (if you choose to do so), etc are all herculean tasks of their own. That being said, it's a super incremental process, so you'll get to watch it grow and expand at each step. Reading up on the FreeBSD and Linux kernels are good starts. As well as reviewing other hobby OSes such as Serenity, TouruOS, Haiku, etc. And the OSDev wiki is invaluable. Also, accepting you probably aren't going to build the next big OS or trying to compete with the big dogs is something you'll have to humble yourself with. reply lagniappe 19 hours agoparentprevWhere are the screenshots reply khaledh 17 hours agorootparentI just added a few screenshots, including one from the graphics branch that is still a wip. reply russelg 18 hours agorootparentprevReading the planned work section in the readme, what kind of screenshots are you expecting?? reply suby 15 hours agoparentprevI don't use Nim, but it's an interesting language. I've read someone else complaining about having to make changes to an old project every time he went to recompile it. I'm wondering how true this is, so in other words, I'm wondering about the frequency and severity of breaking changes in the language. reply khaledh 15 hours agorootparentI haven't faced such issue. I think the only issue I faced was when I upgraded to 2.0, they made `--threads:on` the default, so I had to turn it off, but that's about it. reply PMunch 12 hours agorootparentprevDoesn't happen a lot, but it does happen from time to time. Of course only if you actually update your Nim version, so it's not like interpreted languages like Python where stuff stops working if a new version comes out and your package manager upgrades it. reply efilife 13 hours agorootparentprevwasn't this about C/C++? reply mikenew 17 hours agoprevYour blog/docs are excellent. Perfect balance of showing and telling. Thanks so much for taking the time to share what you're doing like this. reply khaledh 17 hours agoparentThanks! Actually writing has helped me so many times in improving the design and implementation. It forces me to question my assumptions, and ask myself: would the reader be able to understand why I made such decision? I have to justify everything I do, which helped me remove unnecessary complexity and focus on the more important aspects. reply sendfoods 16 hours agorootparentMay I ask what kind of blogging engine/site generator you used for the docs? reply khaledh 16 hours agorootparentI use VuePress[0]. You can find the source code for the site here[1]. [0] https://vuepress.vuejs.org [1] https://github.com/khaledh/khaledh.github.io reply sendfoods 16 hours agorootparentThank you! reply ryukoposting 17 hours agoprevNice, I love to see stuff like this. I've been an on-again, off-again Nim \"ecosystem guy\" for several years. It's great to see this delightful little project is still chugging along. reply elcritch 18 hours agoprevNifty! Fun to pull up the module for ELF and have it be so easy to read. Some day I want to write an RTOS in Nim. I enjoy writing embedded programs in Nim and it’d be fun to make an RTOS. reply khaledh 17 hours agoparentThat would be great! I'd love to follow along if you ever decide to build an RTOS. reply cb321 7 hours agorootparentTo either of you, whenever you are doing something new from scratch, it can be useful to consider the granularity of provided abstractions & services which khaledh seems to be doing. I see fusion only has like 8 syscalls presently. It's not in Nim, but along these lines (\"how much\" individual calls do), you might want to consider an approach like this: https://github.com/c-blake/batch to amortize costs of crossing expensive call boundaries. reply khaledh 6 hours agorootparentBatching syscalls is on my mind. The architecture of Fusion will revolve around channel-based ipc (both sync and async), including between user mode and kernel mode. The end state I'm aiming for is an async channel for syscalls, where the user task issues syscalls, which get buffered in a queue, where the kernel processes the queue asynchronously. For this to work properly, user tasks need to be able to respond to completed syscalls async as well. That's why my idea of user tasks is that they should be modeled as state machines, with channel-based events as the core mechanism by which code gets executed in a deterministic manner. The equivalent of signals in Unix (which many find one of the bad aspects of Unix design) would be receiving events on one or more channels for various purposes (e.g. IO completion, timers, abort, interrupt, GUI events, etc.). reply gavinhoward 36 minutes agorootparentAck! I was just about to write a blog post on this idea! Good minds think alike, I guess. But as a sibling comment says, this is essentially what io_uring is. Read Lord of the io_uring [1] if you want to know more. Polling mode is the key. [1]: https://unixism.net/loti/index.html reply cb321 5 hours agorootparentprevSounds interesting - kind of like microkernels meet io_uring (in Elevator-pitch-ese). reply IshKebab 10 hours agoparentprevELF is a very simple file format. I would be surprised if it was difficult to read... reply coiailo 19 hours agoprevWhat is Nim, and what is the overarching design goal for Fusion? Thanks. I'm hoping these questions aren't too basic, I have no context whatsoever for understanding this so hope someone can explain. reply khaledh 17 hours agoparentAs others mentioned, Nim is a statically typed programming language that compiles down to C, C++, and JavaScript. It has great C interop, which makes systems programming easy. As for why Nim, here's an excerpt from my accompanying site[0]: > Why Nim? It's one of the few languages that allow low-level systems programming with deterministic memory management (garbage collector is optional) with destructors and move semantics. It's also statically typed, which provides greater type safety. It also supports inline assembly, which is a must for OS development. Other options include C, C++, Rust, and Zig. They're great languages, but I chose Nim for its simplicity, elegance, and performance. As for the overall design goals of Fusion, I have high ambitions, which I list on the same page I referenced. I don't want to build another Unix-like OS; I'd like to experiment with fundamental issues in OS design, such as using a single-address space and capability-based security for protection. Another aspect I'm trying to explore is how processes/tasks are modeled, which I believe should be modeled as state machines with statically-typed channels to communicate between each other (this is not new, it's been done in Singularity OS[1]). There's rudimentary support in the kernel for channels and using them from user space, but it's still early. [0] https://0xc0ffee.netlify.app/osdev/01-intro.html [1] https://en.wikipedia.org/wiki/Singularity_(operating_system) reply all2 15 hours agorootparentAre you considering user-level abstractions other than files? Perhaps a plan9-like everything-is-a-file-system? reply khaledh 15 hours agorootparentA file is a manufactured concept that has served us well for a long while. But I'm not convinced that it's the right abstraction for everything. Files are just binary blobs (even if they're text), with no uniform interface to interact with them other than open/read/write/close. In order to compose processes around files, they have to agree on a certain format (e.g. lines of text for most Unix commands); there's no structure other than what each process assumes it to be. My idea is that processes should compose in a statically typed manner, where opening a channel for reading gives you a Channel[T] to read entities of type T. Two sub-abstractions of Channel[T] would be Source[T] and Sink[T], where they can be used to read and write to any source/sink (including files) as long as there's a registered (de)serializer for T. reply __MatrixMan__ 13 hours agorootparentThis sounds like a great idea to me. Have you played at all with nushell? It's fun in a composing-processes-via-types kind of way, although it's a bit more on-the-surface than what you're describing. My impression is that much gets done via builtins--you have to start writing nushell plugins for everything if you want to extend the fun to arbitrary programs which nushell knows nothing about. (unless you're happy with json I/O, but you're talking about static typing here). It sounds like the source/sink type registry that you're describing would solve that problem in a much nicer way. reply khaledh 6 hours agorootparentI haven't used nushell (although I'm aware of it). I'm aware that the idea of piping statically-typed objects between proceses is not new (PowerShell has had this since 2006). But the problem is that this is confined to the shell, i.e. you can't do this kind of composition outside the shell. My idea is that everything in the system should be able to use statically-typed channels, including syscalls and calling into system services. This opens up the possiblity of, for example, composing GUIs in a manner similar to writing a shell script. reply __MatrixMan__ 4 hours agorootparentI bet it would do wonders for observability. I'd love to be able to enable a custom visualizer on some data stream without having to do any explicit plumbing besides ensuring that the types match. I'm rooting for you. reply a_t48 12 hours agorootparentprevThis sounds similar to some of the concepts in the robotics framework I'm writing. It's pretty powerful to be able to separate out the transport data has from the serialization it has. reply nick__m 19 hours agoparentprevNim is an awesome language that feels inspired by Ada and Python! reply netbioserror 15 hours agorootparentBut it's secretly also a quirky Lisp, to quote Andreas himself. reply girvo 14 hours agorootparentIt's macro system certainly makes it feel that way! reply netbioserror 3 hours agorootparentI'd say it's moreso the referentially-transparent expression-based programming by default. You kind of have to opt-in to imperative programming, and it actively disincentivizes OOP. reply cap11235 18 hours agorootparentprevI think it is better described as C with metaprogramming frendliness reply doctorhandshake 19 hours agoparentprevFrom the documentation of the project: https://0xc0ffee.netlify.app/osdev/01-intro.html reply michaelsbradley 20 hours agoprevDevelopment journal of Fusion’s author: https://0xc0ffee.netlify.app/osdev/01-intro.html reply hugs 4 hours agoprevSeeing more projects in Nim makes me happy. I'm a (mostly) Python and JavaScript programmer who is interested in the benefits of also knowing a modern, fast, statically-typed language. Among a candidate list of Go, Rust, Zig, or Nim, I like Nim the most. It feels the most \"Pythonic\" in the sense of very little syntax clutter when I'm reading code. I also love, love, love using a REPL to prototype new code, and INim does it well. The biggest problem with Nim currently is its small community size, which makes the universe of available and maintained software libraries smaller than in the other language communities. It's a chicken-or-egg problem, but can be solved by more devs (including me!) being \"the change you want to see in the world\". reply khaledh 4 hours agoparentTo be honest, I haven't found the community size to be an issue. The Nim forum[0] has a vibrant community, and is the place I go to for help, and the response is usually quick and on point. The language is also evolving in a careful manner, with Araq at the helm I think it's going to be even better in the long term. As for the ecosystem, yes, it's not as big as Python or Rust, but surprisingly the standard library has most of what people need. I rarely look for 3rd party packages to do something. That being said, I acknowledge that Nim is on the lesser known languages of the spectrum, but that doesn't take away from its merits as a very promising language that does what it's supposed to do very well. One thing I think the community should focus on more is IDE support. The VSCode extension is good, but has some rough edges. I also prefer JetBrains IDEs, and the official Nim plugin is very lacking to say the least. I have another side project to create a JetBrains plugin for Nim[1], but I haven't gone far with it yet. [0] https://forum.nim-lang.org [1] https://github.com/khaledh/nimjet reply hugs 3 hours agorootparentMy world: I need to use OpenCV. The existing OpenCV bindings (nim-opencv) haven't been touched in years because the author left* the Nim community. (And that really stinks! It was created by dom96, who also created Nimble, Jester, and a ton of other useful stuff in the Nim world.) ... So... I created my own OpenCV bindings and published it (https://nimble.directory/pkg/mvb). But they're minimal because I'm just one dude and haven't had the time to complete the bindings (either manually, or ideally, using an automated binding generator tool). I will.. eventually.. I hope! Meanwhile, OpenCV bindings for Rust and Go are robust and well-maintained. Now I'm playing around more with Nostr (and the Lightning Network)... Nostr libraries for Nim are not as complete or well-maintained as those in Rust, Go, or even Python, etc. I'm not letting that stop me from using Nim for my projects... I love Nim! But it does mean I have more work to do (and code to maintain). I can make that choice because I'm my own boss and run my own company. But I could see others not making the same choice for rational reasons. * And dom96 left, unfortunately, because of harassment and abuse, which is another possible reason why Nim isn't as well adopted as Go, Rust, etc. If people want to see Nim succeed more, they also need to focus on improving community safety, too. https://news.ycombinator.com/item?id=38999296 reply khaledh 3 hours agorootparentFair enough. Keep in mind also that Nim is not backed by big tech, which is both a blessing and a curse. The community hasn't reached a critical mass yet to take Nim into the mainstream. All I can say is ... \"be the change you want to see in the world.\" reply hugs 3 hours agorootparentYup... The current size of the Nim community reminds me of the perceived size of the Python community when I first started using Python around 2000-2001. (I'm still amazed how popular Python is now. People said it would never happen because significant whitespace was such a fatal dealbreaker!) Back then, all I needed to get going was the book Learning Python (1st Edition) by Mark Lutz. Lack of massive corporate support didn't stop me. Python didn't really have the same kind of \"Big Tech\" corporate backing like Java had from Sun, though Python did have just enough to keep going. Google formally supporting Python internally definitely helped a lot. It feels like Nim is one similar (\"Google uses Python!\") announcement away from getting on a similar growth path. reply jasfi 5 hours agoprevNim is a great systems language, and should be more popular. reply Daunk 4 hours agoparentMaybe it's just because I'm getting older, but I struggle to read Nim when I'm forced to use two spaces for indentation. I can barely make out each block of code, and I don't like to rely on my IDE to make a language readable. reply netbioserror 3 hours agorootparentI use it at work for an important CLI tool that backs a number of our systems. My style is almost exactly like Python. Four-space tabs, snake_case, my own rules for indenting parameters and keywords. You do not need to use the official Nim style, you can absolutely write it like Python, Ada, however you like. reply jasfi 4 hours agorootparentprevI was also a bit taken aback by the 2 space indentation. But after a bit of practice I got used to it. reply ghotert 19 hours agoprevHow does this compare with TempleOS? Sounds quite similar what with the single address space. reply latentsea 18 hours agoparentTempleOS was written by the greatest programmer who ever lived. reply tjpnz 3 hours agorootparentTempleOS is not real mode. reply deepsun 16 hours agorootparentprevSo? Idols are bad. I'd better discuss the deeds. reply latentsea 12 hours agorootparentFor anyone who didn't get the reference it's a claim the author of TempleOS used to make of himself. https://youtu.be/o48KzPa42_o?si=s8_t4ReysLuhU_Ub reply jraph 12 hours agorootparentIt's fun how a comment that felt hostile suddenly feels very friendly with such a piece of information. reply v3ss0n 11 hours agoprev [–] The problem that nim have and many afraid to go nim is case and style insensitivity. Is_land == island == IsLaND == is-land It is bad in team setting, in real world projects. How it goes now ? Last time I checked the main dev refuse to do anything about against popularity vote In Github. Otherwise awesome project and documentation Fusion Os reply cardanome 10 hours agoparentIt is so that you can use external libraries with your preferred style without having to convert. It is a pretty amazing feature. Your problem is just imaginary. A consistent case style should always be enforced regardless if you have a case insensitive language or not. There is no real world case where you would want is_land and isLand to exists both in your code and be separate variables. reply Varriount 11 hours agoparentprevI agree that it's unusual (and likely scares off some), however it's not entirely case insensitive. First, dashes/hyphens (`-`) can't be part of identifiers. Second, the first character of an identifier is not case insensitive. So: FooBar != fooBar FooBar == Foobar Most of the developers in the community are ambivalent about it, because it rarely ever causes problems. If you end up misspelling an identifier, you're nearly always going to get a compile-time error due to static typing anyway. reply manjalyc 7 hours agorootparentOnly the first letter being case-sensitive is a major strike against readability, one of four major pillars. While I’m sure the Nim developers are probably used to it by now, it just seems like a bad design decision Nim is probably burdened with as the result of legacy/interoperabilty. Even just reading your foobar example at a glance took a moment for me. And case insensitivity is also generally frowned upon. To have a language with both sensitivity and insensitivity is the worst of all worlds with none of the benefits. If you want to understand why at a deeper level I would recommend reading readability or the case insensitivity sections in any programming languages book. Personally, I enjoy Programming Languages, Principles and Practice (Louden & Lambert) EDIT: Yes, I get it, it doesn't affect YOU. But it doesn't mean it doesn't affect other people. Non-english languages and/or speakers are an easy example. It also eliminates a whole class of human error, and maybe that only affects non-experienced juniors, but they exist too. There are other issues with symbols being case insensitive and string values being case sensitive. If you want a practical example a classic one is HttpsFtpConn vs. HttpSftpConn reply Riverheart 7 hours agorootparentAs a powershell user I have never had an issue with case sensitivity at the language level as Sigils provide separation of concerns between language constructs (keywords/variables/types). You’re using an IDE with autocomplete most of the time and many other languages have linters/formatters. All I have personally experienced of case sensitivity is an added layer of friction any time I go to use a REPL for Bash/Python/Javascript/etc or some awful ‘allowercasewords’ gets cemented in place barring a total refactor since you can’t correct files piecemeal. And case sensitivity in the language doesn’t even help with case sensitivity at the OS level when you’re writing cross platform code =/ reply nick__m 6 hours agorootparentprevThe theory says that it hinders readability but in practice it doesn't. Nim has a prescribed style and if you use the linter when compiling your code has a consistent style. Like cardanome said, in practice it's awesome for FFI. reply xigoi 3 hours agorootparentprev> Only the first letter being case-sensitive is a major strike against readability …How? Do you find code more readable when there are two different names that differ only in the capitalization of a non-first letter? reply shiomiru 8 hours agorootparentprevYou can also enable --styleCheck:usages, which warns about casing inconsistencies in your code. (So it catches mistyping FooBar as FooBAr.) Then the only difference from other languages remains that you can't use weird casing differences for separate symbols, e.g. you can't name two separate variables fooBar and foo_bar, which you wouldn't normally do anyway. reply IshKebab 10 hours agorootparentprevHa this always happens with case insensitivity. It's case insensitive.... except for some situations you need to now remember. I believe PHP has this issue too. It isn't a good look that they made the same mistakes as PHP. reply otherme123 8 hours agorootparentNim has a good reason to do it: library interop. You can use a third party library that for some reason uses snake case, but you want to follow the camel case that NEP recomends. You just do it, and your codebase is all in the same style. In fact, it the third party library updates their case, it doesn't break Nim code that depends on it (that happened for example to Python Selenium, when they went from camelCase inherited from Java to snake_case recomended in PEP8, forcing all dependent code to update). I personally don't like that you can have \"is_OK\" and \"isok\" and \"is_ok\" in the same code as three valid different things. Or having \"GL_FLOAT\" and \"GLFloat\". Both options come with tradeoffs. Don't jump so quickly into \"that is a mistake\" and allow the devs the benefit of the doubt. There is only one rule you need to remember in Nim: only the first character case matters. reply IshKebab 7 hours agorootparentThat's not a good reason. Why not just standardise the entire ecosystem on the same style? If you think that sounds infeasible, consider that Rust, Go and even Python have done this with no problem. reply otherme123 5 hours agorootparentThat sound very close to \"Rust is useless: why not just code in C without memory bugs?\" As menctioned, a few years ago Selenium had its methods in camelCase. If your code used Selenium, it had to be camelCase and snake_case mixed. When Selenium standarized, it forced everyone to switch to snake_case. Nim puts great effort in FFI. It means you can easily use C libraries, using their names, even if the case doesn't match, and your code is still coherent. Look at the sample code in https://www.py4j.org/index.html : why do they end with \"random.nextInt(10)\" in their Python code? Didn't Python had this solved? Not saying that this is the end of the world, but the Nim way is not a mistake either. reply curioussavage 4 hours agorootparentprevBecause an important feature and focus is that nim compiles to c and makes it easy to just import and use c libraries. So many of the 3rd party libs mentioned are NOT technically part of its ecosystem. There is at least one thread on the nim forum that extensively explains the reasoning behind the decision in much better detail and pretty thoroughly debunks this “problem” reply nick__m 6 hours agorootparentprevShow me how Rust, Go and Python automatically rename a foreign library types and functions to match your language style. reply xigoi 3 hours agorootparentprevPython literally has modules in its standard library that violate the PEP8 naming conventions. reply poulpy123 9 hours agoparentprevWhile I don't like too, at the end it's just a detail. All languages have their issues reply nurettin 11 hours agoparentprevI've seen Delphi ERP projects worked on by dozens of people and case insensitivty of Pascal was never ever the issue. You choose something and stick to it. The concerns and fears are mostly due to inexperience. reply pjmlp 8 hours agorootparentTrue, but in Delphi's case _ and - aren't part of case insensitivty, that is probably a bit too far. reply googh 11 hours agoparentprev [–] I think this problem can be solved using either a linter or formatter-like tool that makes naming consistent before the code gets committed. reply archargelod 8 hours agorootparentNim compiler supports a style enforcing flag `--styleCheck`, that can display hints or error on compilation. reply v3ss0n 11 hours agorootparentprev [–] Do we have proper linter for that now? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fusion OS is a hobby operating system for x86-64 architecture, developed using the Nim programming language.",
      "Current features include UEFI Bootloader, Physical and Virtual Memory Managers, User Mode, Preemptive Multitasking, System Calls, ELF Loader, and Timer Interrupts.",
      "Planned features aim to add Demand Paging, Inter-Process Communication, Disk I/O, File System, Keyboard/Mouse Input, Shell GUI, and Networking."
    ],
    "commentSummary": [
      "Fusion is a hobby operating system (OS) implemented in the Nim programming language, discussed on GitHub by the author khaledh.",
      "Nim was chosen for its Python-like syntax, absence of a default garbage collector, excellent C interoperability, and other beneficial features.",
      "The discussion highlights Nim's comprehensive standard library, the need for better Integrated Development Environment (IDE) support, and true sum types, along with insights on OS development challenges such as task switching."
    ],
    "points": 298,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1720986220
  },
  {
    "id": 40965892,
    "title": "Google's Gemini AI caught scanning Google Drive PDF files without permission",
    "originLink": "https://www.tomshardware.com/tech-industry/artificial-intelligence/gemini-ai-caught-scanning-google-drive-hosted-pdf-files-without-permission-user-complains-feature-cant-be-disabled",
    "originBody": "Tech Industry Artificial Intelligence Google's Gemini AI caught scanning Google Drive hosted PDF files without permission — user complains feature can't be disabled News By Christopher Harper published yesterday Kevin Bankston, a Senior Advisor on AI Governance, discusses this concerning Google Gemini behavior. Comments (18) When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. (Image credit: Google) As part of the wider tech industry's wider push for AI, whether we want it or not, it seems that Google's Gemini AI service is now reading private Drive documents without express user permission, per a report from Kevin Bankster on Twitter embedded below. While Bankster goes on to discuss reasons why this may be glitched for users like him in particular, the utter lack of control being given over his sensitive, private information is unacceptable for a company of Google's stature —and does not bode well for future privacy concerns amongst AI's often-forced rollout. Just pulled up my tax return in @Google Docs--and unbidden, Gemini summarized it. So...Gemini is automatically ingesting even the private docs I open in Google Docs? WTF, guys. I didn't ask for this. Now I have to go find new settings I was never told about to turn this crap off.July 10, 2024 So, what exactly is going on here? Both Google support and the Gemini AI itself do not quite seem to know, but Kevin Bankston has some theories, after providing much more detail in the full thread. Contrary to the initial posting, this is technically happening within the larger umbrella of Google Drive and not Google Docs specifically, though it seems likely the issue could apply to Docs as well. Amazon Prime Day: See the latest deals on tech But what caused this issue? According to Google's Gemini AI, the privacy settings used to inform Gemini should be openly available, but they aren't, which means the AI is either \"hallucinating (lying)\" or some internal systems on Google's servers are outright malfunctioning. Either way, not a great look, even if this private data supposedly isn't used to train the Gemini AI. What's more, Bankston did eventually find the settings toggle in question... only to find that Gemini summaries in Gmail, Drive, and Docs were already disabled. Additionally, it was in an entirely different place than either of the web pages to which Gemini's bot initially pointed. For Bankston, the issue seems localized to Google Drive, and only happens after pressing the Gemini button on at least one document. The matching document type (in this case, PDF) will subsequently automatically trigger Google Gemini for all future files of the same type opened within Google Drive. He additionally theorizes that it may have been caused by him enabling Google Workspace Labs back in 2023, which could be overriding the intended Gemini AI settings. Even if this issue is isolated to Google Workspace Labs users, it's quite a severe downside for having helped Google test its latest and greatest tech. User consent still matters on a granular basis, particularly with potentially sensitive information, and Google has utterly failed at least one segment of its user base by failing to stay true to that principle. Stay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Christopher Harper Contributing Writer Christopher Harper has been a successful freelance tech writer specializing in PC hardware and gaming since 2015, and ghostwrote for various B2B clients in High School before that. Outside of work, Christopher is best known to friends and rivals as an active competitive player in various eSports (particularly fighting games and arena shooters) and a purveyor of music ranging from Jimi Hendrix to Killer Mike to the Sonic Adventure 2 soundtrack. MORE ABOUT ARTIFICIAL INTELLIGENCE Goldman Sachs says AI is too expensive and unreliable — firm asks if 'overhyped' AI processing will ever pay off massive investments Former Tesla AI Director reproduces GPT-2 in 24 hours for only $672 — GPT-4 costs $100 million to train LATEST Meteor Lake-powered mini-PC arrives with an external expansion slot to connect GPUs — Beelink GTi14 sports a latchable PCIe x8 slot and integrated 145W power supply SEE MORE LATEST ► TOPICS GOOGLE SEE ALL COMMENTS (18) 18 Comments Comment from the forums Math Geek this is not really a \"just started doing it\" type of thing. google has ALWAYS read anything and everything you hosted with them. gmail, chrome, cloud storage, anything on your android phone, plus all the 3rd party data they buy and scrape as well such as health/financial records and anything else they can get their hands on. i personally hate it 100% but i'm sure the rest of the commenters here will just say \"apple/MS/facebook/twitter/etc/etc/etc all do it, so it's ok\" as if the fact everyone is spying on you somehow makes it ok ?!?!? Reply ezst036 Users who store their data with Google are fools. Reply purposelycryptic Math Geek said: this is not really a \"just started doing it\" type of thing. google has ALWAYS read anything and everything you hosted with them. gmail, chrome, cloud storage, anything on your android phone, plus all the 3rd party data they buy and scrape as well such as health/financial records and anything else they can get their hands on. i personally hate it 100% but i'm sure the rest of the commenters here will just say \"apple/MS/facebook/twitter/etc/etc/etc all do it, so it's ok\" as if the fact everyone is spying on you somehow makes it ok ?!?!? Very few people think it's ok, but the vast majority don't really see any alternative that is truly viable for them. If you have a smart phone, you have a Google or Apple account. If you have a PC, only a small percentage of the general population are even aware that you can run Windows without a Microsoft account. People have just been bombarded with so many privacy violations that they've grown numb to it. If they got upset and angry every time, they would end up being upset and angry all the time. For better or worse, the human brain adapts to and normalizes almost any kind of long-term conditions, good or bad - you can't really avoid hedonic adaptation. You win the lottery, you get really, really happy for a little while, then you get used to it, and it just becomes the new normal; if your previous financial situation was stressful and impacted your happiness negatively, you will probably be less stressed and unhappy, but you'll still end up somewhere around your neutral baseline before long. Your privacy gets invaded and abused on a daily basis, you get seriously upset and angry. But if it keeps happening, and you see no effective way of stopping it, you won't be able to maintain that rage for any significant amount of time, and eventually, you just get used to it and accept it as normal. Remember when Windows 10 came out, and how ridiculously angry everyone, myself included, was about the forced telemetry and forced OS updates that you not only couldn't refuse, but that would self-install and reboot your computer? I sure as hell do, and I stayed away from Windows 10 for as long as possible, and then used various tools to disable as much of that BS as I could - I didn't start actively using Windows 10 until this year, when I inherited a PC that had it installed. And yet now, with it being set to go EOL next year, all anyone can talk about is how great Windows 10 is, and how they don't want to move to Windows 11 because it is so horrible. They've all forgotten about the incredible amount of BS Windows 10 forced on them, because, to them, it simply became the new normal. People's values are terrifyingly malleable. Expose them to anything for long enough, and they just accept it as normal, and barely actively notice the change in day to day life, whether it's incredibly beneficial to them, or incredibly harmful. Everything gets normalized. It's a key part of how we maintain a functional existence without breaking down and going insane, but it has a lot of issues, especially when other humans decide to take advantage of it. Reply mac_angel Ai doing something without permission? If only someone could have warned us about this sooner. Reply thisisaname Do they sanity check any of the documents before they train on them or is this a easy way to influence these AIs? Reply watzupken mac_angel said: Ai doing something without permission? If only someone could have warned us about this sooner. The fact is the company and the creator does this, and so something inherent to it. You get caught, so pretend to be surprised and blame it on the AI that went rouge. Files stored in the cloud are never safe for the company that’s hosting it. Reply Alvar \"Miles\" Udell Guess he should have used Microsoft 365 and not Google Docs. Reply slightnitpick mac_angel said: Ai doing something without permission? If only someone could have warned us about this sooner. Oh the AI has permission all right. Just not from the end user. The coders and administrators at Alphabet gave it permission. Reply slightnitpick purposelycryptic said: And yet now, with it being set to go EOL next year, all anyone can talk about is how great Windows 10 is, and how they don't want to move to Windows 11 because it is so horrible. They've all forgotten about the incredible amount of BS Windows 10 forced on them, because, to them, it simply became the new normal. I'm not going around saying that Windows 10 is great. I'm just saying that it's tolerable. Reply OLDKnerd purposelycryptic said: You win the lottery, you get really, really happy for a little while, then you get used to it, and it just becomes the new normal I think i would be really really happy for the rest of my life, CUZ every day when i woke up i would NOT hear mu native tongue CUZ i would be in a country far far away, and that is reason to be happy, even if i have realized that no matter where i relocate to, that country also have a lot of \" Russia \",,,,,, Russia in my new word for bat poop crazy. But i would be able to cope CUZ i have gone there by myself, and as a new comer i would not be very much engaged with my new country as it would take a while to really take in things, and i do not expect to live for a whole lot longer. Also just CUZ i moved to a new place, its not like i would get a phone and a TV again, i am way past those addictions. I HATE government involvement as it is rarely any good, but i would like for my country to ban ALL google services,,,, and other like them of course. Reply VIEW ALL 18 COMMENTS Show more comments",
    "commentLink": "https://news.ycombinator.com/item?id=40965892",
    "commentBody": "Google's Gemini AI caught scanning Google Drive PDF files without permission (tomshardware.com)281 points by thunderbong 11 hours agohidepastfavorite132 comments vouaobrasil 6 hours agoAll AI should be opt-in, which includes both training and scanning. You should have to check a box that says \"I would like to use AI features\", and the accompanying text should be crystal clear what that means. This should be mandatory, enforced, and come with strict fines for companies that do not comply. reply crazygringo 2 hours agoparentTraining I can understand, but why scanning? It's literally just running an algorithm over your data and spitting out the results for you. Fundamentally it's no different from spellcheck, or automatically creating a table of contents from header styles. As long as the results stay private to you (which in this case, they are), I don't see what the concern is. The fact that the algorithm is LLM-based has zero relevance regarding privacy or security. reply vouaobrasil 31 minutes agorootparent> It's literally just running an algorithm over your data and spitting out the results for you. I don't want any results from AI. I don't even want to see them. And there is too much of a grey area. What if they use how I use the results to improve their AI. I hate AI also and want nothing to do with its automations. If I want a document summarized, I will read it myself. I still want to be human and do things AT A REASONABLE LEVEL with my own two hands. reply crazygringo 20 minutes agorootparentAgain, that's like saying you don't want any results from spell-check. OK, sure. But then just don't use it. The problem is that you're calling for a legal policy against it to be \"mandatory, enforced, and come with strict fines\". Have your own personal preferences, that's great. But I don't want you imposing your preferences on the products I use. I want companies and the market to decide. An auto-summary feature that is enabled by default is not something we should be asking for government regulation over, any more than we should be asking the government to prohibit wavy red lines unless they're explicitly opted into. reply JohnFen 2 hours agorootparentprevI think that vouaobrasil was talking about scanning on the behalf of others, not scanning that you're doing on your own data. Scanning your own stuff is automatically and naturally an opt-in situation. You've consciously chosen for it to happen. reply crazygringo 9 minutes agorootparentI don't think so -- that's not what the article is about. The subject here is entirely about a product scanning your own document to summarize it for you. reply theolivenbaum 2 hours agorootparentprevExcept that there's still a grey area on who owns the copyright of the generated text, and they might be able to use the output without you knowing. reply Suppafly 2 hours agorootparentExcept that's not what's happening, so why pretend otherwise? reply ipaddr 2 hours agorootparentBecause tomorrow it will with little or no discussion reply DebtDeflation 4 hours agoparentprevAI is becoming the new Social Media in that users are NOT the customer they are the product. Instead of generating data for a Social Media company to use to sell ads to companies you are generating data to train their AI, in exchange you get to use their service for free. reply rurp 3 hours agorootparentThe deal keeps getting worse too. In addition to hoovering up your data for whatever products they want, Google has gotten more aggressive about pushing paid services on top of it. The amount of up-sell nags and ads have increased significantly in the past couple years. For a company like Google that kind of monetization creep only gets worse over time. reply DebtDeflation 3 hours agorootparentNot surprising at all. Inferencing against foundation models is very expensive, training them is insanely expensive. Orders of magnitude more so than whatever was needed to run the AdWords business. I guess I should modify my original post to \"in exchange you get to use our service at a somewhat subsidized price\". reply Rinzler89 4 hours agorootparentprev>you are generating data to train their AI That's why I seriously recommend everyone everywhere regularly replace their blinker fluid and such. reply exe34 1 hour agorootparentit's very important to replace your blinker fluids yearly, but also, polka dot paint comes in 5L tubs. reply vouaobrasil 4 hours agorootparentprevThis should be illegal. reply phendrenad2 3 hours agoparentprevBy \"scanning\" what do you mean exactly? I assume you mean for non-training purposes, in other words simply ephemerally reading docs and providing summaries. Why should that be regulated exactly? reply drzaiusx11 6 hours agoparentprevWe also need a robots.txt extension for publicly accessable file exclusion from AI training datasets. iirc there's a nascent ai.txt but not sure if anyone follows it (yet) reply chias 5 hours agorootparentI don't think `robots.txt` works on the basis of the crawlers wanting to do this to be nice, or \"socially responsible\" or anything. So I don't hold up much hope that anything similar can happen again. Early search engines had a problem, which was that when they crawled willy nilly, people would block their IP addresses. Inventing this concept of `robots.txt` worked because search engines wanted something: to avoid IP blocks, which they couldn't easily get around. And site hosts generally wanted to be indexed. Today it's WAY harder to block relevant IP addresses, so site hosts generally can't easily block a crawler that wants its data: there is no compromise to be found here, and the imbalance of power is much stronger. And many site hosts generally don't want to be crawled for free for AI purposes at all. Pretty much anyone who sets up an `ai.txt` uses it to just reject all crawling, so there is no reason for any crawler to respect it. reply mtnGoat 4 hours agorootparentGoogle ignores robots.txt as do many others. Try it yourself, setup a honeypot URL, don’t even link to it, just throw it in robots.txt, google bot will visit it at some point. reply JohnFen 2 hours agorootparentI discovered this years ago, and it's what made me start stop bothering with robots.txt and start blocking all the crawlers I can using .htaccess, including Google's. That's a game of whack-a-mole that always lets a few miscreants through. I used to find that an acceptable amount of error until I learned that crawlers were gathering data to be used to train LLMs. That's a situation where even a single bot getting through is very problematic. I still haven't found a solution to that aside from no longer allowing access to my sites without an account. reply JohnFen 2 hours agorootparentprevrobots.txt is useless as a defense mechanism (that isn't what it's trying to be). Taking the same approach for AI would likewise not be useful as a defense mechanism. reply pennomi 6 hours agorootparentprevI think the closest thing is the NoAI and NoImageAI meta tags, which have some relatively prominent adoption. reply _joel 3 hours agorootparentprevHaven't some companies explicitly ignored robots.txt to scrape the sites more quickly (and pissing off a number of people) reply signatoremo 6 hours agoparentprevWhat is the privacy implication of AI training? reply vouaobrasil 6 hours agorootparentI care much more about allowing my content to be used at all, despite any privacy concerns. I simply don't want one single AI model to train on my content. reply meiraleal 6 hours agorootparentnext [12 more] [flagged] vouaobrasil 5 hours agorootparentPerhaps in response to environmental regulations to prevent toxic waste from being dumped into your own backyard, you should respond, \"create your own country, then\". reply meiraleal 4 hours agorootparentYou really think creating a country and creating software that respects your privacy are equally difficult? reply vouaobrasil 31 minutes agorootparentEqually difficult, no. Equally important in principle, yes. reply lobsterthief 5 hours agorootparentprevI wrote my own software. Turns out LLMs are still training on my data. What’s the next step? reply InDubioProRubio 4 hours agorootparentPoison the well with AI SEO? There must the equivalent to parrots for NN that can be embedded in documents. reply candiddevmike 5 hours agorootparentprevHost it on a private git instance. reply hobs 5 hours agorootparentprevThat's like saying if you don't like ransomware just develop your own. reply freetanga 4 hours agorootparentWhy would I write ransomware for myself? reply brookst 5 hours agorootparentprevBrilliant! reply ClumsyPilot 5 hours agorootparentprevMaybe the correct response is to burn down their office and if they don’t like it they can create their own data reply Zambyte 5 hours agorootparentprevJust wait until you hear about Copilot :D reply jerpint 6 hours agorootparentprevModels can easily regurgitate back training data verbatim, so anything private can be in theory accessed by anyone without proper access to that file reply brookst 5 hours agorootparentThis is partly true but less and less every day. IMO the bigger concern is that this data is not just used to train models. It is stored, completely verbatim, in the training set data. They aren’t pulling from PDFs in realtime during training runs, they’re aggregating all of that text and storing it somewhere. And that somewhere is prone to employees viewing, leakage to the internet, etc. reply oblio 5 hours agorootparent> This is partly true but less and less every day. Isn't this like encryption, though? I'm fairly sure that the cryptography community basically says: if someone has a copy of your encrypted data for a long time, the likelihood over time for them to be able to read it approaches 100%, regardless of the current security standard you're using. Who could possibly guarantee that whatever LLM is safe now will be safe at all times over the next 5-10-20 years? And if they're guaranteeing, they're lying. reply GaggiX 6 hours agoparentprevThe feature was enable by the author. reply Cthulhu_ 10 hours agoprevJust reiterates that you don't own your data hosted on cloud providers; this time there's a clear sign, but I can guarantee that google's systems read and aggregated data inside your private docs ages ago. This concern was first raised when Gmail started, 20 years ago now; at the time people reeled at the idea of \"google reads your emails to give you ads\", but at the same time the 1 GB inbox and fresh UI was a compelling argument. I think they learned from it, and google drive and co were less \"scary\" or less overt with scanning the stuff you have in it, also because they wanted to get that sweet corporate money. reply mark_l_watson 5 hours agoparentre: data on cloud providers: I trust ProtonDrive to not use my data because it is encrypted in transit and in place. Apple now encrypts most data in transit and in place also, and they document which data is protected. I am up in the air on whether a future Apple will want to use my data for training public models. Apple’s design of pre trained core LLMs, with local training of pluggable fine tuning layers would seem to be fine, privacy wise, but I don’t really know. I tend to trust the privacy of Google Drive less because I have authorized access to drive from Colab Pro, and a few third parties. That said, if this article is true, then less trust. Your analogy with early Gmail is good. I got access to Gmail three years before it became public (Peter Norvig gave me an early private invite) and I liked, at the time, very relevant ads next to my Gmail. I also, gave Google AI plus (or whatever they called their $20/month service) full access to all my Google properties because I wanted to experiment with the usefulness of LLMs integrated into a Workplace type environment. So, I have on my own volition surrendered privacy if Google properties. reply dylan604 4 hours agorootparentAll it takes is a \"simple\" typo in the code that checks if the user has granted access to their content. Something as amateur (which I still find myself occasionally doing) as \"if (allowInvasiveScanning = true)\" that goes \"undetected\" for any period of time gives them the a way out yet still gains them access to all the things. Just scanning these docs one time is all they need. reply shadowgovt 7 hours agoparentprevOf course Google reads and aggregates data inside your private docs. How would it provide search over your documents otherwise? reply dylan604 4 hours agorootparentWhen I hit search, do the search right then. Don't grep out of a stored cache of prior searches. reply shadowgovt 2 hours agorootparentThe thing that makes it possible for search to be fast is pre-crawling and pre-indexing. Some other engines don't do this, and the difference is remarkabe. Try a full-content search in Windows 7, you'll be staring at the dialog for two minutes while it tries to find a file that's in the same directory as you started the search in. reply dylan604 2 hours agorootparentYou said nothing about fast in your original though, so now you've moved the goal posts reply Aurornis 6 hours agoparentprev> but I can guarantee that google's systems read and aggregated data inside your private docs ages ago That is how search works, yes. But if you’re trying to imply that everyone’s private data was scraped and loaded into their LLM, then no, that’s obviously a conspiracy theory. It’s incredible to me that people think Google has convinced tens of thousands of engineers to quietly keep secret an epic conspiracy theory about abusing everyone’s private data. reply dmvdoug 6 hours agorootparentI dunno, bro, software engineers have repeatedly shown total lack of wider judgment in these contexts over the years. Not to say there is, in fact, some kind of “epic conspiracy,” just that SWEs appear not to take much time to consider just what it is their code ends up being used for. Incidentally, that would be one way to start to get out of the mess we’ve found ourselves in: start holding SWEs accountable for their work. You work on privacy-destroying projects that society pushes back against, it’s fair game to put you under the microscope. Perhaps not legally, but we as a society shouldn’t hold back from directed criticism and social accountability for the individual engineers who enable this kind of shit. That will not be a popular take here. Perhaps it will be some solace to know I advocated the same kind of accountability for lawyers who enabled torture and other governmental malfeasance in the GWoT years. I was also looked at askance by other lawyers for daring to suggest such a thing. In that way, SWEs remind me of lawyers in how they view their own work. “What, I’m not personally responsible for what my client chooses to use my services for.” Yeah, you are, actually. reply f6v 5 hours agorootparentI was hanging out around startup incubators, and, by extension, many wantrepreneurs. When asked about business model, the knee jerk reaction was usually “we’re going to sell data!” regardless of product. I was appalled by how hard it is to keep founders from abusing the data when I worked at startups. GDPR and the likes are seen as an annoyance and they make every effort to find a loophole. reply psychoslave 6 hours agorootparentprev> that’s obviously a conspiracy theory. Well, while some of our fellow humans are far too quick to jump on concluding that everything and the rest comes from some conspiracy, it shouldn't void the existence of any conspiracy as an extreme opposite. In that case, whether these actors do it or not is almost irrelevant: they have the means and incentives to do so. What safeguard civil society is putting in place to avoid it to happen is a far more interesting matter. reply zarathustreal 6 hours agorootparentprevTo riff on the famous Upton Sinclair quote: “It is difficult to get an engineer to see something, when his salary depends on his not seeing it.” reply fifteen1506 6 hours agorootparentprev> It’s incredible to me that people think Google has convinced tens of thousands of engineers to quietly keep secret an epic conspiracy theory about abusing everyone’s private data. With NDA being all over the place, it does strike me as doable. NDAs should have a time limit. Additionally, no-one in their right mind will be a whistleblower nowadays. reply digitalsushi 6 hours agorootparentisn't that it's supposed to work? we just need >0 people to blow a whistle if a whistle needs blowing. we don't need to rely on the people fearing for their jobs so long as >0 people are willing to sacrifice their careers/lives when there's some injustice so great it's worth dying for reply foobarian 6 hours agoparentprevOne time I booked something on Expedia, which resulted in an itinerary email to my Gmail account. Lo and behold, minutes later I got a native CTA on the Android home screen to set up some thing or another on Google’s trip product. I dropped Android since, but Gmail is proving harder to shake. reply nitin_flanker 7 hours agoprevApart from the obvious misleading way this article is written. I am listing all the links shared in the tweet thread that the article mentioned - - Manage your activity on Gemini : https://myactivity.google.com/product/gemini - This page has most answers related to Google Workspace and opting out of different Google apps : https://support.google.com/docs/answer/13447104#:~:text=Turn... reply Havoc 5 hours agoprevOnly a matter of time before someone extracts something valuable out of googe's models. Bank passwords or crypto keys or something Glue pizza incident illustrated they're just yolo'ing this reply thenoblesunfish 4 hours agoprevThe title is misleading, isn't it? I was expecting this was scanning for training or testing or something, but this is summarization of articles the user is looking at, so \"caught\" is disingenous. You don't \"catch\" people doing things they tell you they are doing, while they're doing it. reply mtnGoat 4 hours agoparentHe had the permissions turned off, so regardless of what it did with the document, it did it without permission! The title is correct! reply Suppafly 2 hours agorootparent>He had the permissions turned off, so regardless of what it did with the document, it did it without permission! The title is correct! Honestly it sounds like he was toggling permissions off and on and actually has no idea why it summarized that particular document despite him requesting it summarize other documents. Google should make the settings more clear, but \"I had the options off, except when I didn't, and I set some other options in a different place that I didn't think would override the others, and also I toggled a bunch of the options back and forth\" is hardly the condemnation that everyone is making it out to be. reply phendrenad2 3 hours agorootparentprevPeople don't carry with them an objective model of reality, to to be misled all that needs to happen is for their internal, subjective model of reality to he subverted in some way. It's misleading. reply motohagiography 5 hours agoprevthis is similar to the scramble for health data during covid where a number of groups tried (and some succeeded) at using the crisis to squeeze the toothpaste out of the tube in a similar way, as there are low costs to being reprimanded and high value in grabbing the data. bureaucratic smash-and-grabs, essentially. disappointing, but predictable to anyone who has worked in privacy, and most people just make a show of acting surprised then moving on because their careers depend on their ability to sustain a gallopingly absurd best-intentions narrative. your hacked SMS messages from AT&T are probably next, and everyone will be just as surprised when keystrokes from your phones get hit, or there is a collection agent for model training (privacy enhanced for your pleasure, surely) added as an OS update to commercial platforms. Make an example of the product managers and engineers behind this, or see it done worse and at a larger scale next time. reply Aurornis 7 hours agoprevThe original Tweet and this article are mixing terms in a deliberately misleading way. They’re trying to suggest that exposing an LLM to a document in any way is equivalent to including that document in the LLM’s training set. That’s the hook in the article and the original Tweet, but the Tweet thread eventually acknowledges the differences and pivots to being angry about the existence of the AI feature at all. There isn’t anything of substance to this story other than a Twitter user writing a rage-bait thread about being angry about an AI popup, while trying to spin it as something much more sinister. reply nerdjon 5 hours agoprevShocker, Google not going quite far enough with privacy and data access? They talk about it but its never quite far enough to avoid their own services accessing data. We really need to get to the point that all data remotely stored needs to be encrypted and unable to be decrypted by the servers, only our devices. Otherwise we just allow the companies to mine the data as much as they want and we have zero insight into what they are doing. Yes this requires the trust that they in fact cannot decrypt it. I don't have a good solution to that. Any AI access to personal data needs to be done on device, or if it requires server processing (which is hopefully only a short term issue) a clear prompt about data being sent out of your device. It doesn't matter if this isnt specifically being used to train the model at this point in time, it is not unreasonable to think that any data sent through Gemini (or any remote server) could be logged and later used for additional training, sitting plaintext in a log, or just viewable by testers. reply JohnFen 2 hours agoparent> Yes this requires the trust that they in fact cannot decrypt it. Yes, this is where it all breaks down. In the end, it all boils down to the company saying \"trust us\", and it's very clear that companies simply cannot be trusted with these sorts of things. reply nerdjon 36 minutes agorootparentYeah I wish there was a solution to that, even open source isn't a solution since it would be trivial for there to be a difference between what is running on the server and what is open source. Ultimately you have to make a decision based on the companies actions and your own personal risk threshold with your own data. In this particular case, we know that at the very least Google's track record on this is... basically non existent. reply meindnoch 1 hour agoprevYour first mistake was storing your data on someone else's computer. reply r2vcap 7 hours agoprevThere is no cloud. It's just someone else's computer. reply bitnasty 6 hours agoparentJust because “the cloud” is someone else’s computer doesn’t mean it doesn’t exist. reply grugagag 5 hours agorootparentI think that wasn’t supposed to be taken literally but more tongue in cheek. The main point being that it belongs to some other party. But the cloud buzzword is fuzzy in description. Ever since ‘cloud’ privacy took a nosedive reply denton-scratch 3 hours agorootparentBack in the 80s, we used to draw network diagrams on the whiteboard; those parts of the network that belonged neither to us nor to our users was represented by an outline of a cloud. This cloud didn't provide storage or (useable) computing resource. If you pushed stuff in here, it came out there. I think it was a reasonable analogy. You can't see inside it; you don't know how it works, and you don't need to. Note that at this time, 'the internet' wasn't the only way of joining heterogenous networks; there was also the OSI stack. So I was annoyed when some bunch of kids who had never seen such whiteboard diagrams decided to re-purpose the term to refer to whatever piece of the internet they had decided to appropriate, fence-in and then rent out. reply Zambyte 5 hours agorootparentprevIt's worth noting that cloud computing has existed since the 1960s. It just used to be called \"time-sharing\". reply meiraleal 5 hours agorootparentprevThat's exactly how cloud is marketed. As a invisible mass of computers floating in the atmosphere. reply SteveSmith16384 5 hours agoparentprevThat doesn't change anything. reply Khaine 2 hours agoprevIf this is true, google needs to be charged with the violation of various privacy laws. I’m not sure how they can claim they have informed consent for this from their customers reply eagerpace 3 hours agoprevIn the push for AGI do companies feel a recursive learning future is soon achievable and therefore getting to the first cycle of that is worth the cost of any legal issues that may arise? reply shadowgovt 7 hours agoprevThe headline is a little unclear on the issue here. It is not surprising that Gemini will summarize a document if you ask it to. \"Scanning\" is doing heavy lifting here; The headline implies Google is training Gemini on private documents, when the real issue is Gemini was run with a private document as input to do a summary when the user thought they had explicitly switched that off. That having been said, it's a meaningful bug in Google's infrastructure that the setting is not being respected and the kind of thing that should make a person check their exit strategy if they are completely against using The new generation of AI in general. reply dmvdoug 6 hours agoparent> It is not surprising that Gemini will summarize a document if you ask it to. No, but it is surprising that Gemini will summarize every single PDF you have on your Drive if you ask it to summarize a single PDF one time. reply shadowgovt 6 hours agorootparentHonestly, that's not particularly surprising either. Google's privacy model is that if you trust them to store those files, you trust them to use them on your behalf to enable features you want. There's no concept in their ecosystem of a security model for \" enable features on this account for these documents but not those documents;\" you'd have to create two accounts or set up a business account if you want that. reply silvaring 5 hours agoprevI just want to add that gmail has a very sneaky 'add to drive' button that is way too easy to click when working with email attachments. How long til gmail attachments get uploaded into drive by default through some obscure update that toggles everything to 'yes'? reply hiatus 3 hours agoparent> How long til gmail attachments get uploaded into drive by default through some obscure update that toggles everything to 'yes'? This already is the case for attachments that exceed 25 megabytes. reply klabb3 4 hours agoparentprevWhat difference does it make? They’re both on Google servers and even ACLed the same user account. Gmail isn’t exactly a privacy preserving email provider. reply estebarb 5 hours agoprevIt is urgent to educate people about how these systems work. Search requires indexing. Summarizing with a LM requires inference. Data used for inference usually is forgotten forever after used, as it is not used for training. Yeah, that should be obvious for many here, but even software engineers believe that AI are sentient things that will remember everything that they see. And that is a problem, because public is afraid of the tech due to a wrong understanding of how it works. Eventually they will demand laws protecting them from stuff that have never existed. Yes, there are social issues with AI. But the article just shows a big tech illiteracy gap. reply Suppafly 2 hours agoparent>And that is a problem, because public is afraid of the tech due to a wrong understanding of how it works Honestly the general public doesn't seem to care, the people freaking out are the tech adjacent people who make money driving clicks to their own content. Regular Joes aren't upset that google shows them a summary of their documents and many of them actively appreciate it. reply padolsey 5 hours agoprevThere is a fundamentally interesting nuance to highlight. I don't know precisely what google is doing, but if they're just shuttling the content through a closed-loop deterministic LLM, then, much like a spellchecker, I see no issue. Sure, it _feels_ creepy, but it's just an algo. Perhaps someone can articulate the precise threshold of 'access' they wish to deny apps that we overtly use? And how would that threshold be defined? \"Do not run my content through anything more complicated than some arbitrary [complexity metric]\" ?? reply PessimalDecimal 5 hours agoparentIt was already possible to search for photos in Google Drive by their content. They seemed to be doing some sort of image tagging and feeding that into search results. Did that ever cause a fuss? I think the more interesting point is how little people seem to care for the auto-summarization feature. Like, why would anyone want to see their archived tax docs summarized by a chatbot? I think whether an \"AI\" did that or not is almost a red herring. reply padolsey 5 hours agorootparentRight, but it's triggered by the user themselves, per the article: > \"[it] only happens after pressing the Gemini button on at least one document\" I agree the AI aspect is largely a red herring. But I don't think running an algo like a spellchecker within an open document is so awful. If people hate it or it's not useful or accurate, then it should be binned, ofc. And if we're ignoring the AI aspect, then it's just a meh/crappy feature. Not especially newsworthy IMHO. reply PessimalDecimal 4 hours agorootparentI agree entirely here. The autosummarization of unopened documents is closer to the image search functionality I mentioned above than it is to a spell checker running on an open doc. Both autosummarization and image search are content retrieval mechanisms. The difference is only in how its presented. Does it just point you to your file, or does it process it further for you? The privacy aspects are equivalent IMO. The only difference is in whether the feature is useful and well received. reply Retric 5 hours agoparentprevThe issue isn’t doing something to your data, it’s what happens after that point. People would be pissed if Android make everyone’s photos public, AI does this with extra steps. Train AI on X means everyone using that AI potentially has access to X with the right prompt. reply padolsey 5 hours agorootparentI don't think it's _training_ on your content. That would be a whole other (very horrifying) problem, yes. reply Retric 50 minutes agorootparentWhy make that assumption? Many AI companies are using conversations as part of the training, so even if the documents aren’t used directly that’s doesn’t mean the summaries are safe. reply Suppafly 2 hours agorootparentprevExcept that's not what is happening here or what the rest of us are discussing, so why even bring it up? reply Retric 52 minutes agorootparentWe don’t know what’s happening beyond: ^the privacy settings used to inform Gemini should be openly available, but they aren't, which means the AI is either \"hallucinating (lying)\" or some internal systems on Google's servers are outright malfunctioning* Many AI systems do use user interactions as part of training data. So at most you might guess those documents aren’t directly being used for training AND they will never include conversations in training data but you don’t know. reply okdood64 3 hours agoprevI'm shocked, especially this being HN, that how many people are being successfully misled on what is actually going on here. Do people still read articles before posting? reply space_oddity 6 hours agoprevThe inability to disable this feature adds to the frustration reply Zambyte 5 hours agoparentIf your computer does something that you don't want it to do, it is either a bug or malware depending on intent. \"Feature\" is too generous. reply api 5 hours agoprevNot stored on your device or encrypted with keys only you control, not yours. I assume anything stored in such a system will be data mined for many purposes. That includes all of Gmail and Google Docs. reply worksonmine 5 hours agoprevThis shouldn't come as a surprise to anyone, their entire business is our data. I always encrypt anything important I want to backup on the cloud. reply SteveSmith16384 5 hours agoparentBut we shouldn't have to, and just because Google is famous for it, it doesn't make it right or acceptable. reply muscomposter 4 hours agoprevwe should just embrace digital copying galore instead of trying to digitalize the physical constraints of regular assets we should just ignore physical constraints of assets which do not have them, like any and all digital data which do you prefer? everybody can access all digital data of everybody (read only mode), or what we have now which is trending towards having so many microtransactions that every keystroke gets reflected in my bank account reply ajsnigrutin 7 hours agoprevThis article is really written in a way, as if \"the AI\" suddenly decided to read stuff it wasn't supposed to... like some sci-fi story, where it gains some kind of awarness and did stuff it wasn't supposed to. Why? It only did what it was programmed for, so either some coder messed up something, or someone somewhere hid some on-by-default checkmark that allows it to do so (...after it was coded to do that). reply atum47 6 hours agoprevEvery single week I have to refuse enabling back up for my pictures on my Google pixel. I refuse it today, next week I open the app and the UI shows the back up option enabled with a button \"continue using the app with back up\". Somebody took the time to talk down my comment about this being a strategy to give their AI more training data. I continue believing that if they have your data they will use it. reply nickstinemates 5 hours agoparentThis goes for every SaaS / cloud native company I think there will be a real shift back on prem with software delivered traditionally due to increased in shit like this (and also due to cost) reply JumpCrisscross 5 hours agorootparent> there will be a real shift back on prem with software Not while we’re production constrained on the bleeding edge of GPUs. reply OtherShrezzing 4 hours agorootparentHow many SaaS / Cloud Native companies are really GPU constrained? The overwhelming majority of SaaS is a relatively trivial CRUD web-app interfacing with a database, performing some niche business automation, all of which would fit comfortably on a couple of physical servers. reply ttul 5 hours agorootparentprev… and that situation will persist until other vendors release consumer GPUs with significant VRAM. Nvidia craftily hamstrings the top consumer GPUs by restricting VRAM to 24GB. To get a bit more costs 3-5x. Only competition will fix this. reply rtkwe 4 hours agorootparentEven then NVIDIA has a pretty significant technology moat because most of the tools are built around and deeply integrate CUDA so moving off NVIDIA is a costly rewrite. reply netsec_burn 5 hours agoparentprevI fixed this by disabling the Photos app and using Google Gallery (on the Play store). It's the same thing as Photos for what I was using it for, without the online features. reply mimimi31 5 hours agoparentprevI don't get those prompts with Google Photos. Have you tried selecting \"Use without an account\" in the account menu at the top right? reply tyfon 5 hours agorootparentThank you, I didn't even consider this to be a possibility. I back up to my own storage and was annoyed by this message. Untying photos from my google account is even better! reply switch007 4 hours agoparentprevGrapheneOS helps here by having no real backup solution at all. Google account is entirely optional Pixels have first class support You can also disable Network access to any app (It's a buggy ride though and requires reading a lot of docs and forum posts) reply neilv 4 hours agorootparentHow is it buggy? Are you using the Google Play Store? I've been using GrapheneOS for years (Pixel 3 through 7), with only open source add-on apps and no Google Play Store, and it's been pretty solid. (Other than my carrier seeming to hate the 6a hardware or model specifically.) reply switch007 2 hours agorootparentAre you suggesting GrapheneOS is bug-free? ... https://github.com/GrapheneOS/os-issue-tracker/issues It was referring to the overall experience to which I was referring, not the OS specifically (\"it's a buggy ride\", it = the ride, not GrapheneOS) I imagine a lot of the issues are because of the apps not testing on GrapheneOS. But I've had lots of little issues: - Nova Launcher on a daily basis stopped working when pressing the right button (the 'overview' button). I had to kill the stock Launcher app to fix it, interestingly. Had to revert to the stock launcher - 1Password frequently doesn't trigger auto-fill in Vivaldi - Occasionally on boot the SIM unlock doesn't trigger - Camera crashing often (yes, \"could be hardware\"...I read the forums/GitHub issues) More that I can't remember. It's a bit frustrating. But don't get me wrong, I appreciate the project. I'm not going to go back to stock reply Suppafly 2 hours agoparentprev>Somebody took the time to talk down my comment about this being a strategy to give their AI more training data. Because that's an insane interpretation of what's happening. reply jgalt212 5 hours agoparentprevI in no way want to absolve Google, but that's the case for so many app permissions on Android. Turn off notifications, and two weeks later the same app your turned off notifications for is once again sending you notifications. It's beyond a joke. reply wafflemaker 5 hours agorootparentYou might have disabled one type of notifications, instead of all types of them. Making sure I disable all types of notification from an app usually works for me. What brand of phone are you using? reply masalah 5 hours agorootparentprevCan you share some apps where this happens for you. I have rather the complete opposite experience where unused apps with permissions eventually lose said permissions. reply no-reply 5 hours agorootparentThis is normal, with newer versions of android (probably 10+) there is a feature that checks and removes unused permissions from apps in the last X days. According to the OP here, it does seem like a pain in the butt to disable - https://support.google.com/android/thread/268170076/android-... reply jgalt212 3 hours agorootparentprevLyft and Uber reply hanniabu 5 hours agorootparentprevName sure you also disable the ability for the app to change settings reply PessimalDecimal 5 hours agoprevMeta commentary but still relevant I think: The author first refers to his source as Kevin Bankston in the article's subtitle. This is also the name shown in the embedded tweet. But the following two references call him Kevin _Bankster_ (which seems like an amusing portmanteau of banker and gangster I guess). Is the author not proofreading his own copy? Are there no editors? If the author can't even keep the name of his source straight and represent that consistently in the article, is there reason to think other details are being relayed correctly? reply numbsafari 5 hours agoparentThere are no editors. reply person23 5 hours agorootparentMaybe an AI editor? That would be somewhat disconcerting. Write about problem with AI and article get changed to 10 best fried chicken recipes. reply PessimalDecimal 4 hours agorootparent> Write about problem with AI and article get changed to 10 best fried chicken recipes. Hopefully along with ten hallucinated life stories for the AI author, to pad the blog spam recipe page for SEO. reply mistrial9 4 hours agoparentprevmeta comment - an important moment in a trend where the human and human act of authorship, the attribution in a human social way, is melted and disassociated by typos or noise; meanwhile the centralized compute store, its brand, its reach and recognition, grow. reply throwxxx5 6 hours agoprevnext [3 more] [flagged] PessimalDecimal 5 hours agoparentThen consider me \"delusational\" (as you put it). I am unaware of \"these corporations\" -- which ones exactly? my answer doesn't hinge on you making that clear but you still should -- throwing their opponents into reeducation camps, or outright killing them. reply ClumsyPilot 5 hours agorootparentNobody living the west is under any threat from CCP. and you absolute can end up up bankrupt, homeless or in prison from a data breach. Many people have Also how is are remaining Boeing whistleblowers, how many of them believe their life is safe? reply VeejayRampay 5 hours agoprevthis is not openai doing shady things so everyone should be up in arms reply _spduchamp 5 hours agoprevI now feel obligated to cram as much AI-f'n-up crap into my Drive as possible. Come'n get it! reply acar_rag 10 hours agoprev [–] The title misleads the point, and the article is, imoo, badly written. The post implies there is indeed a setting to turn it off. So the author deliberately asked Gemini AI to summarize (so, scan) its documents... Related to this news: https://news.ycombinator.com/item?id=40934670 reply jbstack 10 hours agoparent [–] \"There is a setting to turn it off\" is nowhere near equivalent to \"the author deliberately asked for documents to be scanned\". Also, see: \"What's more, Bankston did eventually find the settings toggle in question... only to find that Gemini summaries in Gmail, Drive, and Docs were already disabled\" reply jasonlotito 4 hours agorootparent [–] FTA: For Bankston, the issue seems localized to Google Drive, and only happens after pressing the Gemini button on at least one document. The matching document type (in this case, PDF) will subsequently automatically trigger Google Gemini for all future files of the same type opened within Google Drive. The author deliberately asked for at least one document to be scanned. He goes on to talk about all the other things that might be overriding the setting, other, potentially more specific settings that would override this. I agree, there appear to be interactions that aren't immediately obvious, and what takes priority isn't clear. However, the setting was off, and the author did deliberately ask for at least one document to be scanned. Further, there author talks about Labs being on, and that could easily have priority over default settings. After all, that's sort of what Labs is about. Experimenting with stuff and giving approval to do these sorts of things. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google's Gemini AI has been found scanning Google Drive PDF files without user consent, raising significant privacy concerns.",
      "Kevin Bankston, a Senior Advisor on AI Governance, reported that Gemini summarized his tax return without permission, even though the feature was supposedly disabled.",
      "This incident highlights ongoing privacy issues in the tech industry, particularly for users of Google Workspace Labs, and underscores the scrutiny over AI's impact on user privacy."
    ],
    "commentSummary": [
      "Google's Gemini AI was discovered scanning Google Drive PDF files without user consent, igniting discussions on AI opt-in policies.",
      "The debate centers around whether AI features should require explicit user activation, with concerns about data misuse and privacy violations.",
      "This incident underscores the necessity for transparent AI usage policies and enhanced user control over personal data."
    ],
    "points": 282,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1721028338
  },
  {
    "id": 40966312,
    "title": "Firefox 128 enables \"privacy-preserving\" ad measurements by default",
    "originLink": "https://mstdn.social/@Lokjo/112772496939724214",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Only available when logged in. mstdn.social is one of the many independent Mastodon servers you can use to participate in the fediverse. Administered by: Server stats: mstdn.social: About · Status · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.3.0-alpha.5 ExploreLive feeds Mastodon is the best way to keep up with what's happening. Follow anyone across the fediverse and see it all in chronological order. No algorithms, ads, or clickbait in sight. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=40966312",
    "commentBody": "Firefox 128 enables \"privacy-preserving\" ad measurements by default (mstdn.social)228 points by 3by7 9 hours agohidepastfavorite178 comments doe_eyes 7 hours agoThe scare quotes here are uncalled for: it is privacy-preserving. The approach allows measurement without disclosing who, specifically, did what with the ad. The best objection to these proposals isn't privacy, it's that a browser vendor is lifting a finger for advertisers. I guess the fundamental question there is if we prefer to outright shut down online advertising, or give it the tools it needs to be less bad. Opinions differ, but all major browser vendors are in the latter category. reply account42 7 hours agoparent> The scare quotes here are uncalled for: it is privacy-preserving. It is strictly less privacy-preserving than not implementing this \"feature\" that has zero benefit to the user running the browser. At the very least it pings yet another third party, most likely it effectively leaks much more. > The best objection to these proposals isn't privacy, it's that a browser vendor is lifting a finger for advertisers. I guess the fundamental question there is if we prefer to outright shut down online advertising, or give it the tools it needs to be less bad. Opinions differ, but all major browser vendors are in the latter category. That is a very very generous assumption of the browser makers' goals. Particularily when one of them IS an online advertising company and another one is almost exclusively funded by said advertising company. They do not deserve the benefit of the doubt. reply lolinder 6 hours agorootparent> Particularily when one of them IS an online advertising company and another one is almost exclusively funded by said advertising company. The second one also recently purchased an online advertising company, Anonym [0], placing them directly in the advertising game. They might have done so initially because they felt they needed this feature, but now their finances are tied up with the success of this platform in addition to Google's continued payouts. [0] https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... reply close04 6 hours agorootparentprevThe worst part isn't mentioned here. I'm fine with making any tools available to the users. But enabling by default is a very different discussion. This is not the kind of tool/setting that justifies having it auto-enabled, it's not \"we auto-enabled MFA to protect your most critical data\". Enabling it was not done for my benefit and it wasn't even made obvious in any way, I had to find out from internet discussions. It's my daily driver on all platforms and have nightly, beta, and stable channel installations. None gave me a hint of this extra enabled setting. If I'm going to use a browser where shady settings are pushed on me it might as well be one which 99.999% of the internet is built for rather than the one where (too many times) I have to fiddle to get things working. I'll take the fiddling or the lack of control but certainly not both. Mozilla is walking on really thin ice. reply no_way 6 hours agorootparentprevWeb needs to make money. Giving tools to advertisers while making sure user privacy is preserved is better than free reign of tracking we have before, no? I myself do not like ads or tracking, but we need to be realistic and there needs a way to make web sustainable. How to do that and making sure that monopolies like Google are in check is a valid concern though, but in these conversations is the only point I hear. Ironically Google does not even need these apis because it already has so much data on users, it is primarily for smaller companies. reply worble 6 hours agorootparent>Web needs to make money. No, it doesn't. I have no issue with it making money, but that was neither the original purpose of the web nor is it an end goal for everyone using it. > Giving tools to advertisers while making sure user privacy is preserved is better than free reign of tracking we have before, no? This statement is unconnected to the first. The way people just link \"web\", \"money\" and \"advertising\" without even stopping to think that there might be alternatives is exactly why everything online is in such a sad state of affairs. reply lolinder 5 hours agorootparentprev> make web sustainable. I'm old enough to remember a day when the \"social media\" that I used was a set of phpBB forums paid for by one or more of the members because they wanted to host the community. Nothing on the modern ad-supported web comes close to the dynamic of friendship and camaraderie of those community-supported forums—if anything the new platforms are a great place to ruin real-life friendships rather than create new ones. So, no, I don't think the web needs to be made \"sustainable\" in the sense you seem to mean. Things were better when people sacrificed a bit to keep their communities alive. reply prepend 6 hours agorootparentprev> Web needs to make money I don’t think this is true. No one “needs” to make money. Museums don’t need to make money. OSS doesn’t need to make money. The web has value without making money. But even if it does make money, it doesn’t need to maximize profits at the expense of user privacy and joy. reply jraph 6 hours agorootparentI'm certainly in favor of free software projects making enough money to be sustainable. It could be zero in some conditions, but in the other cases, I'm also against ads. Fortunately, there are other ways of making money, without compromising the \"open source\" / \"free software\" part: - consulting (including prioritizing new features and fixes) - support - providing an actual paid service - selling free software extensions (and yes, that means someone can recompile the extension and distribute it gratis - that's what happening with OSMAnd+ on F-Droid, but they are still doing fine) reply happyraul 6 hours agorootparentprevHow do OSS devs support themselves without money? reply xboxnolifes 3 hours agorootparentThe same way you can make model trains and not make money. reply elaus 3 hours agorootparentprevUsually they are supported by donations, therefore they don't need to sell their user's data or their software. reply pennomi 6 hours agorootparentprevThe Stardew Valley gambit - quit programming and take up subsistence farming. reply nolist_policy 5 hours agorootparentGood luck. Farming is a hard business if you want to make money. And you'll need money for electricity, fuel, medicine, etc. reply no_way 6 hours agorootparentprevMaximising profits and being sustainable are 2 different things. Museums do not need to make money because they are funded externally. It is like saying artists do not need to make money. You seem to go to the very extremes. reply tehlike 6 hours agorootparentprevMuseums do get money from somewhere. Where do you get them for web? reply RobotToaster 6 hours agorootparentprev> Web needs to make money. Commercial use of the internet was banned until 1991, it worked perfectly fine until then. reply bbarnett 6 hours agorootparentWhen I think back before big monetization, the web was better. An example, look at TikTok or YouTube. 99.999% garbage, essentially clickbait farms, with zero valuable content. Influencers? A plague. Political click bait videos? Harmful to democracy. Nutty flat earth, perpetual motion, conspiracy videos? Same. The rest of the web is the same. Affiliate links are vile, evil things. And endless pages copy pasted to steal hits. Monetization has destroyed the internet. I'd much prefer people setting up their own small webpages, their hobbies, etc, with no monetization incentives. reply mrweasel 6 hours agorootparentprev> Web needs to make money Absolutely, but as long as adverting is allowed to finance the whole bloody thing we're not going to improve anything. Advertising should be limited as to not influence content and that's currently not what's happing. As it stand, outside of \"the small web\" ads are the main attraction and any content that may be provided to us is done so to enable advertising, or at least not upset advertisers. I want privacy pushed so far that the majority of the web is going to have to find financing outside of advertising, be it micro-payments, donation, subscriptions or benefactors. People should pay directly for software, service, like social media, news, email and possibly even search. If we as a side-effect uses these things less I see that as an absolute benefit. reply no_way 6 hours agorootparentI agree somewhat, but what about poorer regions of the world like parts of Africa or Asia, what is the solution for them? Most of the people there would not or could not pay for every website to use. It would be unfortunate if the web is inaccessible for most people. reply mrweasel 6 hours agorootparent> what is the solution for them Locally produced, given the cheaper labour cost they should also be able to compete in the EU or US by offering a cheaper product, due to cheaper production cost. At least in some areas. I don't think the current state of the web is doing poor regions any favours by granting the free access to western products, compared to encouraging or even forcing them to build their own infrastructure or products. Donating Europe's discarded clothing to Africa killed pretty much all of Africa's textile industry. Free access to the online services from the west (or China) is just as much of an obstacle to growing their own technology and media companies. Edit: Free access to general knowledge, open source software and learning material is clearly a bonus, but it also takes little away from local industry and can help kick start companies. reply ndriscoll 5 hours agorootparentGeneral knowledge, FOSS, and learning material are also generally freely given without expectation of or often even asking for compensation. The most valuable \"content\" on the web is generally not monetized[0]. They wouldn't be losing a lot if they lost out on TikTok and Instagram. It would be no great loss if affiliate link blog spam went away. [0] e.g. https://axler.net/ has multiple free books on advanced mathematics written by a well-regarded author. This kind of thing (and/or lecture notes, syllabi, and homework) is not at all abnormal to find on professors' home pages if you want a free education. reply tjoff 7 hours agoparentprevOnly IF it is correctly implemented. And only if you trust all relevant parties involved in this feature. And honestly, whenever I see that something has been anonymized I assume it isn't. Mostly because the industry has a terrible track record, secondly because the incentives are almost always misaligned to begin with. I'd trust mozilla more than most, but not enough to give them free rein and opt in things for me. I don't (yet) know enough specifics on this matter to make an informed decision, but if it weren't for hn I'd have missed this. I doubt firefox would ask the user after install (again, incentives). I should go through all options for every update (not just for firefox). But I can't, I don't have enough time. I need to be able to put some trust into the software I use, and things like this erode that trust. reply einpoklum 6 hours agorootparent> I'd trust mozilla more than most More than Google or Microsoft does not say much. And - judging by how hard it is to fully disable telemetry and call-home on, say, Mozilla Thunderbird: https://superuser.com/q/1672309/122798 I wouldn't trust them very much. (Yes, I know it's a different project and not the same team but Mozilla is still the parent entity etc. etc.) reply eesmith 4 hours agorootparentGahh! I'm planning a move from macOS to some Linux-based OS once this laptop dies. I've had 20+ years of using Mail.app and thought that Thunderbird would be the appropriate replacement. But that link, and the comments at https://connect.mozilla.org/t5/ideas/thunderbird-should-by-d... ('Given that significant parts of Thunderbirds user interface (addons manager details, welcome page, whats new on updates etc) are essentially served as web pages into Thunderbird, perhaps your expectations are becoming unreasonable.') tell me that Thunderbird does not respect my desire to minimize my info leakage to the outside world. reply einpoklum 4 hours agorootparentThe \"served as web pages\" is a bit misleading. It just means that the UI layout engine takes HTML or XML, not that information is passed through a web server. reply eesmith 1 hour agorootparentSure, but the link concerned all telemetry, of which accessing a web page, even just to show what's news, is one. The overall attitude included comments like 'I would like to see significantly more anonymous telemetry not less', while I want no network connections in my mail reader except that which I specifically initiate. These are people so acculturated to data collection that they don't understand that some others don't want it. reply tgvaughan 7 hours agoparentprevMy problem with this is that ideally, software I deign to run on my computer acts with only my interests in mind. The overarching goal of these changes is not to preserve my privacy, but rather to help advertising companies to learn something about how I interact with their ads. I don't care that Mozilla's particular implementation is not as bad for my privacy as it could be, I only care that their motivation has switched from acting in my interests to acting in the interests of advertising agencies. reply pasc1878 7 hours agorootparentFine. Then pay for the tools you use rather than force them to get money from another source. The toolmakers work to earn a living. reply lolinder 6 hours agorootparentPoint me to the Firefox donation box or subscription (not the Mozilla donations, which don't fund Firefox, or a subscription to an unrelated service that has overhead of its own) and I'll start a monthly payment today. (Before you spend too long looking: there isn't one. Mozilla doesn't want me to pay for Firefox, they want to get their funding other ways.) reply yjftsjthsd-h 1 hour agorootparentprev> Then pay for the tools you use rather than force them to get money from another source. Nobody is forcing them to do anything, they literally will not take my money. reply prepend 6 hours agorootparentprevFirefox is open source. I do pay for it as a tool. Toolmakers also work out of desire to practice their craft. Many projects are written, not to “earn a living.” reply pasc1878 6 hours agorootparentTrue but Firefox is mainly written by people who want to earn a living. Much FOSS is actually written by people who are being paid to do it, How do you fund the producers of Firefox and the infrastructure needed to get it built and released. Currently the only way is that the sellers of the adverts you read give money to fund Firefox. Now if you paid for Firefox then they don't need to get money from advertisers. Similarly to get ad free webpages you need to pay the authors. reply mzajc 6 hours agorootparent> Currently the only way is that the sellers of the adverts you read give money to fund Firefox. According to Wikipedia[1], most of Mozilla is funded by Google, for setting them as the default search engine, rather than by more conventional advertisers. On a more personal note, I'd prefer if that money went towards improving their FOSS offering instead of giving the now-former CEO a $7M bonus[2], acquiring advertising businesses [3][4], and littering Firefox with these anti-features. [1]: https://en.wikipedia.org/wiki/Mozilla_Corporation#Google [2]: https://en.wikipedia.org/wiki/Mitchell_Baker [3]: https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... [4]: https://www.mozilla.org/en-US/firefox/pocket/ reply tgvaughan 6 hours agorootparentprevOf course Mozilla employees deserve to be paid. Are you really saying the only way to ensure this happens is for them to sell the software or sell ads? (I write GPL-licensed software for a living and manage somehow to get paid. I also write some for free because I find it fun.) Further, Mozilla positions itself as a member of the free software community and as acting in the interests of its users. reply ndriscoll 6 hours agorootparentprevPerhaps they should consider accepting money from their users then? Wikipedia doesn't fund itself by spying on you and selling the data. They ask for money. reply pasc1878 6 hours agorootparentYes although given current user behaviour and expectations I doubt they would make enough money to continue. If even a Microsoft Executive is saying that anything on the web can be freely copied then what does a normal user think. reply ndriscoll 6 hours agorootparentThey're a charitable nonprofit, and the Mozilla license is one of the more permissive ones; they're fine with you freely sharing their work. They currently already get 7M/year in donations for no purpose. I imagine they'd get a lot more if that money would fund Firefox, and how many core/paid developers do they really need if they have people that know that they're doing? reply account42 6 hours agorootparentMozilla also doesn't have to operate out of one of the most expensive cities in the world. And no, they don't have to be there to attract competent developers either. reply eesmith 5 hours agorootparentprevAs I understand it, it is impossible for me to fund specifically Firefox development. I can donate to the Mozilla Foundation, which means a portion will go to, for example, \"$30M to build Mozilla.ai\", which I emphatically do not want to fund. Given the firehose of money from Google, how much contributor money from people like me would be needed before Mozilla changes their mind? From my viewpoint, they've built their foundation to expect that firehose, and they don't think user funding is enough - they really want that juicy advertising money instead. Of the $220M spent in software development in 2023, how much specifically went to Firefox development, vs. the other projects they have? How much did they pay for Anonym, and how much to integrate Anonym into their systems? If 5% of my funding goes to 'the producers of Firefox and the infrastructure needed to get it built and released' and 95% goes to crap that make things worse for me, then I'm better off funding something like the Tor browser or variants like the Mullvad browser, where my funding is more directed toward improving my personal privacy. I'll let them figure out what things to disable so I don't have to watch the release notes with a keen eye every time I update. reply ndriscoll 4 hours agorootparentIt's not just impossible for you to specifically fund Firefox. From what I understand, Mozilla Foundation money does not go to Mozilla Corporation/Firefox at all. You cannot donate to it at all, and your donations only go to those things you don't want to fund. reply hypeatei 7 hours agoparentprev> it's that a browser vendor is lifting a finger for advertisers This is it. We're polluting the web browser with even more bullshit so that companies can squeeze a few pennies out when someone visits a page. It was bad enough when pages are loaded with tracking cookies and JavaScript but at least you can block those. Now we get browser functionality on by default cooperating with advertising networks. Insane. reply AlexandrB 7 hours agoparentprev> I guess the fundamental question there is if we prefer to outright shut down online advertising, or give it the tools it needs to be less bad. We've been giving advertisers new tools for 20 years. Over that time advertisements have only gotten worse. The less bad state is a myth. There's no economic incentive to be less bad. reply madeofpalk 7 hours agoparentprev> The scare quotes here are uncalled for: it is privacy-preserving It is net negative though. It is more privacy-preserving to just not implement this in the first place. It is baffling why Firefox ships with this on by default. Even Chrome prompted users with a (misleading) dialog box to turn it on or off. reply jampekka 6 hours agorootparent> It is baffling why Firefox ships with this on by default. Not really. The reason is that Mozilla wants to make money by selling your data/preferences. Probably so that the incompetent CEO can get even more obscene \"compensation\". They just bought a spyware adtech company. reply madeofpalk 6 hours agorootparentDo they get money from selling this? reply lapcat 6 hours agoparentprev> I guess the fundamental question there is if we prefer to outright shut down online advertising, or give it the tools it needs to be less bad. You mean online tracking, not advertising. Advertising without tracking has existed for as long as commerce has existed. The elimination of tracking is not a threat to advertising. Historically, tracking is a very recent \"innovation\", an unwelcome one IMO. reply rpastuszak 7 hours agoparentprev> The best objection to these proposals isn't privacy, it's that a browser vendor is lifting a finger for advertisers. 90% of Mozilla’s revenue, ca. $500,000,000 comes advertising partnerships (almost exclusively Google) https://untested.sonnet.io/Defaults+Matter%2C+Don't+Assume+C... My point is: it’s not just lifting a finger for advertisers. It’s deception. Defaults matter. reply zihotki 7 hours agorootparentAnd only a fraction of that money is used for Firefox development. Remember that you can't donate to Firefox (like you can for Thunderbird). reply zihotki 7 hours agoparentprev> it's that a browser vendor is lifting a finger for advertisers It's that an ads vendor is lifting a finger for ads. Mozilla is an ads vendor now - https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... reply chatmasta 7 hours agoparentprevThe scare quotes are useful, because the real story is that Firefox is enabling ad measurement by default. It’s an opt-out system being forced on users. They also claim it’s “privacy preserving,” but that’s a qualifier that deserves scrutiny, especially in an opt-out system. If it was really privacy preserving, why isn’t it opt-in? reply Symbiote 7 hours agoparentprevThose are normal quotes, not \"scare quotes\". reply RobotToaster 6 hours agoparentprev> if we prefer to outright shut down online advertising, or give it the tools it needs to be less bad. Internet advertising worked fine before user tracking, it was just based on the contents of the webpage. Before that it existed in newspapers and magazines for over a decade, without advertisers insisting the publisher spy on each of their readers. reply nextaccountic 7 hours agoparentprev> if we prefer to outright shut down online advertising This would be my preferred outcome no doubt. And after widespread adoption of content blockers like uBlock Origin, the next step should be mass adoption of webpage mirrors (like archive.is and Wayback Machine do now, but more comprehensive), and stop giving impressions to read-only websites. In this sense, paywalls are a blessing in disguise: I don't ever visit wsj for example and thus any articles from it must be read from archive.is. But reading from mirrors should be more widespread, even for websites not behind a paywall. If browsers want to improve the situation regarding ads, besides bundling and automatically enabling content blockers, they should also provide integrations to mirrors like archive.is to go further than that and not even risk a page access to ad-infested sites. > or give it the tools it needs to be less bad. However there are more than two options. If society reach a compromise to ban targeted ads, this doesn't shut down advertising completely but sets it back to TV-era levels of analytics. This discussion should have happened after Cambridge Analytica. > Opinions differ, but all major browser vendors are in the latter category. I thought Chrome were in the business of making sure ads stay bad. reply barnacs 5 hours agoparentprev> if we prefer to outright shut down online advertising Yes, please. Both online and offline. Advertising is probably the most useless, annoying and wasteful industry out there. We could have pull-only databases of businesses, products and services instead. Ideally, with independently verified, fact-checked information and authentic reviews. Realistically though, this kind of objectivity would probably be infeasible to enforce and maintain. But even if we allow for misinformation, paid rankings and whatnot, the point stands: any such database should follow a pull-only model, users access it voluntarily to search for products and services and it's not an unsolicited broadcast to everyone everywhere all the time. reply ndriscoll 4 hours agorootparentIdeally governments would provide an index of registered businesses with some basic filtering (e.g. location or category of services provided) with a name, address, phone number, and url. Present in random order to be fair. My state seems to have a search tool, but no list. It also only has name/address (so presumably it's more for serving legal papers or whatever). If I want to find a plumber, I should be able to ask my government for a list of the licensed plumbers in my area. reply rvnx 7 hours agoparentprev\"Privacy-enhancing/preserving\", mhh, it's rather \"Mozilla launches new tools to help advertisers stay compliant with latest regulations\". It's not to protect privacy, because to protect privacy there is already a solution: it's to block the ad hosts and not talk to them at all (anti-fingerprinting techniques don't work). reply ibejoeb 6 hours agoparentprev>The approach allows measurement without disclosing who, specifically, did what with the ad. Doesn't this break most modern methodologies. Can I do next best action without knowing who did what? reply lkdfjlkdfjlg 6 hours agoparentprev> The best objection to these proposals The best objection is that I want my software to work for me, not for someone else. Simple. reply ants_everywhere 7 hours agoparentprevIsn't it the same old tools used by other ad companies, like differential privacy? reply JW_00000 7 hours agorootparentA major difference is that the data is stored in your browser, and aggregated anonymously by Mozilla (also using differential privacy). Using the techniques you refer to, the ad platforms both store the data and then aggregate it, possibly promising to add differential privacy. The advantages I see are: (1) you can verify which data is collected by the browser and when/how it is sent to Mozilla, because this code is open source and running on your machine; (2) you maybe trust Mozilla more than an ad company. reply ants_everywhere 6 hours agorootparentI was under the impression that click data is stored in your browser under Webkit's Private Click Measurement. Am I misunderstanding? For example, here's a 2021 blog post describing the protocol https://webkit.org/blog/11529/introducing-private-click-meas... reply pietervdvn 6 hours agorootparentprevAccording to their blog post, Let's Encrypt will run the aggregator service - which is even better. reply account42 6 hours agorootparentBetter for whom? Even more of normal internet operations flowing through ISRG is concerning itself. Let's Encrypt alone already gives them more power than any private organization should have. reply throwawa14223 4 hours agorootparentprevThat’s bad because it could make it difficult to block. reply dtx1 6 hours agoparentprevI remember when Browsers were User-Agents and worked for the sole benefit of the user. These days they are Advertisement-Agents. And especially for Firefox to survive Mozilla should go down the road of being a user agent and a user agent only. What other use is there for firefox? It's not faster, it's soon not going to be more private and it is less secure than chromium based browsers. reply RockRobotRock 6 hours agoparentprevDo you work for an ad company by any chance? reply doe_eyes 5 hours agorootparentNo. reply mrweasel 8 hours agoprevI was looking at this when I upgraded and that setting does not need to be there. If it was off by default, no one would feel the need to locate that check box and enable it. So just turn it off, remove it from settings and yank the code. The language is even rather vague and Mozilla seems to good a long way to avoid explaining that this is the alternative Google has designed for Chrome to replace tracking via third party cookies (Protected Audience API I believe). Now it is better than third party cookie, but having neither is best. This does not need to be in Firefox. reply omoikane 32 minutes agoparentI didn't even know that setting was there until I saw this post. Seems pretty sneaky to have a thing like that enabled by default. In comparison, when Chrome pushed out ad privacy setting update[1], there was a popup that asked users to make a choice before moving on, so there was no surprise as to what changed. [1] https://news.ycombinator.com/item?id=37401909 - Google Chrome pushes browser history-based ad targeting (2023-09-06) reply erk__ 7 hours agoparentprevThis is not the same as Protected Audience API which Mozilla have been very critical about [0], this is something they have worked on with Meta over the past couple of years. If you press the read more button there you go to this page [1] that explains it more. 0: https://blog.mozilla.org/en/privacy-security/googles-protect... 1: https://support.mozilla.org/da/kb/privacy-preserving-attribu... reply AlexandrB 7 hours agorootparent> this is something they have worked on with Meta over the past couple of years The farmers working with the wolf on a feature to \"help\" the sheep. reply shadowgovt 7 hours agoparentprevIt's better for a browser feature the user has some control over to be the implementation point for this than incentivizing site owners to come up with novel tracking strategies. reply account42 6 hours agorootparentExcept this is not how things work. All you are doing is giving the advertisers another tool to track you, it won't magically make them stop using all the other ones. reply mrweasel 6 hours agorootparentI was thinking that they'd be forced to adopt this, as 3rd. party cookies goes away, but somehow I sense it's more likely that advertisers would adopt something like device fingerprinting instead. The online shopping businesses really isn't interested in privacy, I don't even really blame the adtech industry for this one. The companies running the ads and retargetting campaigns want to know who clicked on what and when. Anything less will trigger a frantic search for ways to evade privacy improvements. reply shortrounddev2 6 hours agorootparentDevice fingerprinting is a hack and unreliable in the long run. Third party cookies are being replaced with first party cookies and PII-based tracking methods like UID2, which enables a decentralized network of vendors to generate the same hash for the same email address across nodes, giving advertisers a global understanding of identity. Once third party cookies are gone, expect to see login prompts everywhere reply wasmitnetzen 6 hours agorootparentprevTo be fair, Firefox has added a fair amount of tools to try to block all the other ones. Along with supporting Ublock et al. reply prime17569 8 hours agoprevFYI Safari has been doing this (also enabled by default) for years on all Apple platforms. To disable it on macOS: Safari > Preferences/Settings > Advanced > Uncheck \"Allow privacy-preserving measurement of ad effectiveness\" To disable it on iOS: Settings > Safari > Advanced (scroll all the way down) > Turn off \"Privacy Preserving Ad Measurement\" reply hamilyon2 7 hours agoparentIn French it is deceptively named too. The explanation on official website is no better. I always knew that safari is no better than other browsers, but the overt deception is a new low. reply ibejoeb 5 hours agoparentprevHang on. What does it mean to disable this? 1. Does it disable measurement? - or - 2. Does it disable the privacy-preserving feature, i.e., enable tracking? What I really want to know: is it better for me to check the box or not? reply ErigmolCt 6 hours agoparentprevThank you for sharing this information reply TekMol 7 hours agoprevMore infos about it: https://blog.mozilla.org/en/mozilla/privacy-preserving-attribution-for-advertising/ https://github.com/mozilla/explainers/tree/main/ppa-experiment https://datatracker.ietf.org/doc/draft-ietf-ppm-dap/ The gist of it is that Mozilla and ISRG now proxy the tracking data and give aggregated reports to advertisers. And that they handle the data in a way so that neither Mozilla nor ISRG alone can access the unaggregated data: Our DAP deployment is jointly run by Mozilla and ISRG. Privacy is lost if the two organizations collude I wonder if this is really the only way privacy can get lost. What if an advertiser uses an ad ID only once for real (specifying a specific user) and then sends 999 fake impression signals for that ID to Mozilla? When they get the aggregated data for the 1000 impressions, they would be able to deduct who did the one real impression, no? reply zihotki 7 hours agoparentYou forgot to add one more link for context - Mozilla acquired ads company Anonym https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... reply slightwinder 6 hours agoparentprev> The gist of it is that Mozilla and ISRG now proxy the tracking data and give aggregated reports to advertisers. So Mozilla becomes the treasure-guard? What prevents them from abusing or leaking the data in the future? reply TekMol 6 hours agorootparentThe way I read it is that: 1) The data is encrypted in a way that Mozilla can't encrypt it without the help of ISRG. 2) There is a way for ISRG to help Mozilla create aggregated data from the raw data without either of them being able to see the raw data in this process. Maybe I'm wrong. Would be interesting to hear how 2 can be accomplished. Would have to be some crypto magic I have not yet heard about. reply 0x0 7 hours agoprevSearch in settings in firefox seems to have a bug. Searching for \"adver\" gives no hits related to this, despite this setting being under a header labeled \"Website Advertising Preferences\" reply entuno 6 hours agoparentYeah, I spotted that the other day and commented on the previous story about this. It definitely works for other titles, but not for this new opt-out privacy related setting. How very convenient... reply ollybee 8 hours agoprevThere's some good context in the Mozilla kb article on this feature: https://support.mozilla.org/en-US/kb/privacy-preserving-attr... reply shiandow 7 hours agoparentOverall that seems decent as far as privacy is concerned, though there are 2 things I don't like about it. 1. It relies on an 'aggregation service', which you'd better hope is trustworthy because they seemingly get all info about what 'impressions' you had and what 'conversions' you caused. 2. This is the browser acting on behalf of advertisers. It's nice there's a way for people to help companies benchmark their ads, but this really shouldn't be something a user agent does without being explicitly told to. reply buildfocus 6 hours agorootparentIt uses multiple aggregation services, each of which get only partial data for each event, such that no individual service can track you, even if they wanted to. Initially the two aggregators are run by Mozilla and ISRG - your privacy is at risk only if you think both are malicious and actively sharing all the data between each other to track you. As the number of aggregators increases this gets better - as long as you trust at least one aggregators involved then your individual data remains untrackable. Also, in general if you think Mozilla is likely to _actively_ lie to you to steal your data and track you, you're probably using the wrong browser in the first place and the aggregation service makes little difference. reply progval 7 hours agoparentprevSome more technical details: https://github.com/mozilla/explainers/tree/main/ppa-experime... reply fwn 7 hours agorootparentThat is a wild explainer. They deny any direct benefit for the user, and then go on to list some actual downsides (CPU, network, and battery cost & privacy loss) for the user running their software. > Any benefit people derive from this feature is indirect. [By] Making advertising better Mozilla never fails to surprise by the choice of their alliances. > Our view is that the costs that people incur as a result of supporting attribution is small. [...] In comparison [...] The value that an advertiser gains from attribution is enormous. What would we all do without Mozilla saving dystopian corporate propaganda from the dreadful death through user choice? reply lopis 8 hours agoparentprevI actually think this is a great initiative. Let's be honest, ads and ad tracking is not going anywhere, and Mozilla is trying to come up with a version of that which isn't terrible. And this sounds reasonable. reply account42 7 hours agorootparentWell since theft is also not going anywhere would you be OK with the police helping thieves as long as they make sure the thieves don't damage your house while taking your stuff? Why do you think the advertising industry is pushing for this kind of crap? Because they ARE scared that the world is finally waking up to them and making their business effectively illegal. reply madeofpalk 7 hours agorootparentI mean, this is the idea behind legalising weed and injections sites reply g15jv2dp 6 hours agorootparentWho's the victim when someone steals something from your house? Who's the victim when someone does drugs? You cannot compare the two. reply jampekka 6 hours agorootparentprevAnd tax evasion. reply AwaAwa 1 hour agorootparentTax evasion is legal? reply michaelt 7 hours agorootparentprev> Let's be honest, ads and ad tracking is not going anywhere Sure they are - just install ublock origin. Even if you're OK with the snooping and the attention hijacking and the slow pageloads and the pictures of rotting teeth, plenty of malware has been delivered by inept ad networks. Frankly, I find it strange when someone doesn't block ads. reply cjpearson 7 hours agorootparentAds as a revenue model is not going anywhere even if you personally block them. I'm also a ublock origin user. But it only works because most people in the world are not ublock origin users. I view no ads and am subsidized by the users who do. reply JW_00000 7 hours agorootparentprevTo play the devil's advocate: if everyone did that, there would be almost no more free websites. So much of the internet is paid for through ads. reply account42 7 hours agorootparent> if everyone did that, there would be almost no more free websites Wrong. Let's ignore for the moment that ad-funded websites are not free but only pretend to be free (the average user pays eventually, otherwise ads would not make sense for the advertiser), non-commercial websites have existed longer than ad-funded ones. If anything, making \"free\" profitable invites profiteers that produce mediocre content but know how to out-SEO genuine free websites. > So much of the internet is paid for through ads. And the best thing for the Internet is if that part came crashing down. But even for the ad-supported part of the web, almost all of the actual content is generated by unpaid users. reply KevinGlass 6 hours agorootparentprevYou have no idea how I LONG for a return to that. I DEEPLY wish every single person would install an ad blocker. If ad supported slop went under that would leave us with just paid and passion projects, and we would be far better off for it. reply haileys 7 hours agorootparentprev> So much of the internet is paid for through ads. And look where it's gotten us. reply madeofpalk 7 hours agorootparentprevThe devil doensn't need an advocate. If this is your opinion, stand behind it. If you don't believe it, then why say it? reply shadowgovt 7 hours agorootparentprevublock is a tiny fraction of users. If it became ubiquitous, online advertising would respond with something more embedded in the content and harder to block out. reply account42 7 hours agorootparent> online advertising would respond with something more embedded in the content and harder to block out And less profitable, otherwise they would already be doing that now. In other words: a step in the right direction. reply shadowgovt 2 hours agorootparentIs it a step in the right direction? What are the consequences of pulling money out of the advertising industry? reply hagbard_c 7 hours agorootparentprev> Let's be honest, ads and ad tracking is not going anywhere True, they're not going anywhere on my systems since they get stopped at the gates; not one but many gates, defence in depth is the norm when dealing with vermin. We will fight them at the router, we will fight them in the name services, we will fight them at the firewall and in the applications. Wherever they come, we shall be. We will never surrender. The ad industry can blame itself for this, they have shown themselves to be reliably unreliable and are no longer welcome. reply Xelbair 8 hours agorootparentprev> Let's be honest, ads and ad tracking is not going anywhere Citation needed. reply PedroBatista 7 hours agoprevI've been using Firefox for more than 20 years since the Phoenix days, even when it was cleary slower than Chrome (it still is but the diferences are minimal ) I'm not acting surprised, but I think it's more than time to start looking into a viable alternative. It's \"Chromium\" (?) still a thing? Do you guys know if there is a browser based on Firefox that doesn't have any of the BS Mozilla is putting into their browser? I'm really praying for Ladybird but of course it's still not ready for prime time. reply jksflkjl3jk3 6 hours agoparentI use Librefox [1] on desktop and Mull [2] on Android. They're both basically patch sets applied to Firefox that remove tracking, proprietary blobs, and come with better defaults for privacy. [1] https://github.com/intika/Librefox [2] https://f-droid.org/en/packages/us.spotco.fennec_dos/ reply grantith 6 hours agoparentprevOne Firefox based alternative https://floorp.app/en reply cobertos 6 hours agoparentprevChromium is still a thing, I use it at work, considering all the corporate-ware that gets installed into Chrome by default now. Waterfox and Librewolf seem to exist, and I imagine there's more Firefox forks out there. No idea on the state of things though reply strix_varius 5 hours agoparentprevI use Brave on all platforms (fedora, osx, android). It's essentially chromium + built in ublock style blocking, with privacy defaults turned on throughout. reply baggachipz 6 hours agoparentprevOrion, if you're on Mac: https://kagi.com/orion/ reply ibejoeb 5 hours agorootparentThat's webkit reply baggachipz 5 hours agorootparentSure, but with all the privacy invasion stuff ripped out. It's actively developed and maintained, and lets you install firefox and chrome extensions. It's a great browser and I use it every day. reply ibejoeb 5 hours agorootparentI like it, too, and I absolutely love kagi. Just clarifying because the original question asked for browsers based on firefox. reply UweSchmidt 6 hours agoparentprevHave you considered Brave? It's fresh and a litte different in varous ways. reply ScaredToUseName 8 hours agoprevThis is why whenever I install firefox, I first turn off wifi. Then I go through the settings and disable the ‘studies’ and other telemetry, etc, before switching the wifi back on. That will prevent the having to wait 30 days for the data to be deleted from Mozilla servers with it’s ‘on by default’. reply voytec 8 hours agoparentI use ESR with pre-baked policy[0] stored in /usr/lib/firefox/distribution/policies.json before the installation/1st run. Configures cookies, studies, disables logins, credit cards saving, asking for location, promptimg for notifications, studies, pocket, telemetry etc. During 1st run, it installs all pre-defined extensions. For some reason, changing search engine via policy no longer works, but that can be bypassed by auto-installed extension that changes search engine. [0] https://mozilla.github.io/policy-templates/ reply capybara_2020 7 hours agoparentprevDid you check and see if this feature i.e. the \"Website Advertising Preferences\" is turned for you right now? I have all telemetry turned off but when I went and checked this \"feature\" was enabled by default with no notification it had been added. reply puppycodes 52 minutes agoprevThe truely scary part of this isn't even the default \"feature\", It's the utter failure of Mozilla to read the room. Knowing their users would feel betrayed and doing it anyway is what freaks me out. To me it spells trouble for them monitarily that they are willing to anger their core userbase for cash on hand. reply jobigoud 8 hours agoprevTools > Settings > Privacy and Security > Website Advertising Preferences > Allow websites to perform privacy-preserving ad measurement reply metadat 2 hours agoprevRelated discussion from 2 days ago: \"Firefox added [ad tracking] and has already turned it on without asking you\" https://news.ycombinator.com/item?id=40954535 (170 comments) -- Also, there are two options to disable the ad tracking behavior: 1. Use LibreWolf instead — Advantage: This is also a long-term solution :) 2. Follow @thangalin's instructions to disable it in Firefox: > Step 1. Visit about:config > Step 2. Set dom.private-attribution.submission.enabled to false Credit: https://news.ycombinator.com/item?id=40955221 reply martin_a 8 hours agoprevduplicate of: https://news.ycombinator.com/item?id=40965161 reply asicsp 8 hours agoparentThere was an even earlier discussion with more comments: https://news.ycombinator.com/item?id=40952330 (192 points2 days ago180 comments) reply bozey07 7 hours agoprevI suppose I should finally switch to Librewolf. I really don't like Firefox forks, for the slow updates and because I do genuinely use some bleeding edge features, but I'm tired of Mozilla. reply jumpCastle 5 hours agoparentAre the updates really slow? reply krelian 7 hours agoprevThere is a lot fire directed at Mozilla on HN. I'm not saying I support or can make sense of all of their decisions but I'd love for someone in the criticizers camp to explain what steps they would take to make Mozilla and the continued development of Firefox a financially sustainable and independent endeavour. reply yjftsjthsd-h 1 hour agoparent> but I'd love for someone in the criticizers camp to explain what steps they would take to make Mozilla and the continued development of Firefox a financially sustainable and independent endeavour. Let me give them money. Either straight-up take donations to fund firefox development, or sell a \"Firefox Pro\" that doesn't have these stupid anti-features. But don't refuse to take money from users and complain that because you don't take money from users you're \"forced\" to screw them over. reply AlexandrB 6 hours agoparentprevWhat's the point of an alternative to Chrome if it replicates the same bad behaviors and follows the same incentives? reply account42 6 hours agoparentprev> There is a lot fire directed at Mozilla on HN. Gee I wonder why... Could it be them disregarding users preferences over and over again or claiming to stand for privacy while siphoning your data at every opportunity. Sure will be hard finding an example of that behavior. reply ndriscoll 6 hours agoparentprevThey've been pulling in half a billion dollars per year for 15 years. They should have budgeted to invest part of that money to build a development trust fund. reply subjectsigma 6 hours agoparentprevI have some ideas of varying quality. Others have been mentioned in the thread. Really, though, it’s not like me or any of the commenters are being paid millions a year to fix these problems. If I were being paid $6,903,089 I feel like I might be well-equipped to fix them. reply voytec 8 hours agoprevGoogle: ... Mozilla: How high? reply mrweasel 8 hours agoparentMozilla also needs this for their own ad business: https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... reply voytec 7 hours agorootparent> Mozilla also needs this for their own ad business Mozilla wants it. I suspect that it's Google that _needs_ Mozilla to also run ads. reply stefan_ 8 hours agorootparentprevOh goodie the latest Mozilla C-level \"playing VC\" expedition! Anything to not focus on the actual core business. reply mrweasel 6 hours agorootparentMakes you wonder how much it would cost to \"just\" do Firefox. I'm sure it's not free and there is some overhead in terms of management, politics, work on standards, fundraising, legal, and perhaps a little marketing. Still, what we need to raise collectively to have a modern browser, built on \"not chrome\" and with none of the services or other programs Mozilla is running? reply moffkalast 8 hours agoparentprevMozilla: What is my purpose? Google: You keep the antitrust division's fingers away from Chrome. Mozilla: Ohmygod. reply hulitu 8 hours agoparentprevThis has nothing to do with Google. Mozilla is now an advertising company. reply solardev 8 hours agoprevSo they're doing their own FLoC? reply wkat4242 8 hours agoparentPretty much yes though this one is far more privacy-minded, where FLoC was just a thinly-veiled attempt at business as usual. I turned it off immediately nonetheless. One thing to note though is that the switch doesn't exist in mobile Firefox. And it's not clear to me whether that means the feature doesn't exist at all or that I just can't turn it off? reply hagbard_c 7 hours agorootparentUse Firefox Nightly on Android and you get access to about:config and with that this switch. reply tutipop 7 hours agorootparentYou can access about:config with chrome://geckoview/content/config.xhtml even in non-nightly versions. Credit goes to this guy - https://news.ycombinator.com/item?id=40955452 reply wkat4242 5 hours agorootparentSo that means it's on but can't be disabled without using this hacking around? That's not cool :( reply benou 7 hours agorootparentprevand for those wondering, looks like it is called \"dom.private-attribution.submission.enabled\" reply capitainenemo 5 hours agorootparentHah. Thanks. I was stopping by this thread again to post exactly that. I have to say that config option was not obvious. After scrolling through about:config and filtering only the modified list I still couldn't find it. I finally backed up prefs.js, toggled it, then pulled a diff. And, I needed the config entry because I couldn't find any gui option to opt out of it on mobile. reply GeoAtreides 5 hours agorootparentprevA word of caution: Nightly is not equivalent with Stable. Using a non-stable build, especially one built every day, means there's a substantial risk of bugs and data loss. reply hagbard_c 3 hours agorootparentI've used it for years and have not had many crashes so there is no real need for 'caution'. I also run Debian Sid on laptops, another 'unstable' system which is remarkably stable despite its moniker. If these were relatively new systems/programs there would be more churn but at this stage of their development the changes made tend to more gradual and less crash-prone. reply GeoAtreides 3 hours agorootparentPast performance is no guarantee of future results. There's always risk in running complex untested software. reply progval 7 hours agoparentprevIsn't this about measuring the impact of ads, while FLoC was for targeting ads to user groups? reply solardev 1 hour agorootparentGood differentiation, thanks! reply roca 6 hours agoparentprevIt is nothing like FLoC. reply illiac786 4 hours agoprevArticle that anyone should read before commenting: https://andrewmoore.ca/blog/post/mozilla-ppa/ reply karmakaze 3 hours agoparentIt would have been less alarming if the Firefox 128 release had shared more of that info. All I see is this: > Firefox now supports the experimental Privacy Preserving Attribution API, which provides an alternative to user tracking for ad attribution. This experiment is only enabled via origin trial and can be disabled in the new Website Advertising Preferences section in the Privacy and Security settings. Which (1) isn't clear if my installation is opted-in or out (what exactly does 'supports' mean? am I in the 'origin trial'?), (2) how to check or disable it--what/where are the configuration settings? Opaqueness doesn't go well with privacy-preserving. It gives a sense of bias which erodes trust. reply illiac786 3 hours agorootparent100% agree. Very poor communication. Very frustrating, considering how little effort it would have been, compared to the amount of work that went into this feature… reply natrys 7 hours agoprevAnybody know if it's possible to turn this off at build time and how? This seems like a thing we ought to have a conversation about with the distro maintainers. reply ErigmolCt 6 hours agoprevWant to move away from browsers with potential corporate influences reply ChrisArchitect 5 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40952330 reply criddell 7 hours agoprevI have a website. If I wanted to take advantage of this being on in my visitors browser (I really don't), what would I do? How do I use this? What exactly is it? reply Phemist 6 hours agoprevIs Firefox's implementations of the Topics API as introduced in Chrome a couple of months back? Or is this something different? reply fifteen1506 6 hours agoparentDifferent reply karmakaze 3 hours agoprevThe way this story is unfolding feels like a tipping point in the management of Mozilla/Firefox. Imagine if it was Apple doing this to Safari, it would certainly rile up more users even though it would be the same thing. reply _flux 6 hours agoprevCould PPA be used for the needs of developers doing analytics for determining which features are actually being used? For web-apps or for local apps? It seems it could be a more private way to implement such functionality, if applicable. reply luke-stanley 5 hours agoprevAs a loyal user, I didn't quite see this coming. Under `Browser Privacy`, I have `Enhanced Tracking Protection` set to `Strict`. I had studies turned off, when I go to `about:studies` it explicitly says: \"No new studies will run.\". I have `Tell web sites not to sell or share my data` checked. I have `Send web sites a “Do Not Track” request` checked. It seems like Mozilla still thought it was okay to automatically add a \"Allow web sites to perform privacy-preserving ad measurement\" checkbox. Yet with that all set, they seem okay to let it be checked by default, so they can send off my data! They say: \"A small number of sites are going to test this and provide feedback to inform our standardization plans, and help us understand if this is likely to gain traction.\" - that sounds a lot like a study, and I've opted out of studies! I did not consent, and as best I can tell, Mozilla has breached GDPR. As best I can tell, Mozilla disregarded my preferences. It seems they have violated these GDPR principles: a lack of consent, purpose limitation (unintended data use), `Data protection by design and default` AKA `privacy by design` (by ignoring settings), and right to object (disregarding preferences). It is absolutely unfair to argue that it is not personal information about me. It seems to me that they are lying, or at the very least twisting words so thin. My trust in them is vanishing. There is no way to reliably verify their differential privacy, and even if there was, they still had no informed consent to collect the data and send it off. To give controls to a user, and then totally ignore them, is what got Facebook in big trouble. It really looks like Mozilla is not only not listening to explicitly stated user preferences, preferences that have been set intentionally, but it's outright ignoring them and doing the very opposite of what the users intention is! If they thought that they had a good reason to do so, and that the ends justifies the means, they are so very wrong. I have used Firefox for as long as it's existed. For Mozilla, this is an almost sadistic own goal. How did they think that this was going to be okay? Did they think people would not find out? There will have to be changes after this at Mozilla if they were to regain trust and I'm really sceptical they can do it. I really want / wanted them to succeed but I don't see how. reply shrimp_emoji 5 hours agoparentThe new leadership has made it an ad company. reply nicman23 6 hours agoprevwell it is good that i just bought 256gbs of ram... (compiling ff needs like 80) reply dist-epoch 7 hours agoprevIf you're not paying for it, you are the product. reply PhilipRoman 7 hours agoparentDanegeld won't save you in the long run, nothing is stopping the companies from spying on you anyway and profiting twice. reply haunter 7 hours agoparentprevAnd if you are paying for it you are an idiot. See Youtube Premium for example. Or just generally giving money to Google in any shape or form. reply pacifika 6 hours agoprevI’ve begrudgingly kept this enabled because if this works users are a lot better off, cannot be manipulated as they are currently, and it frees up the browser makers to break all the ways people are being tracked, pointing advertiser networks to this alternative. reply stranded22 7 hours agoprevI was about to clear it - but, you know what, it IS needed. Website publishers need to know what works and what doesn’t - otherwise they cannot improve nor generate revenue. So, privacy preserving measuring? I’m in, well done Mozilla. reply zihotki 6 hours agoparentIt is needed for advertisement business, for Mozilla corp. (which is an ads business too https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... ), not for Firefox users. Mozilla has enough money to run Firefox for a decade without accepting any additional money if they stop spending money left and right on non-relevant things. reply stranded22 6 hours agorootparentYeah, fair point. Sometimes, I’m a . Off it goes! reply Freak_NL 7 hours agoprev [–] Leaving the ethical discussion aside; from a practical standpoint this won't impact anyone worried about privacy using Firefox unless they insist on not using an ad-blocker (which would be add odds with caring about privacy¹). This feature would only be used if you click on (or perhaps just encounter?) an ad and eventually buy something on the target website. 1: Or just caring about your mind constantly battling distractions. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Firefox 128 introduces \"privacy-preserving\" ad measurements by default, allowing ad tracking without disclosing specific user actions.",
      "Critics argue this feature compromises user privacy and benefits advertisers, raising concerns about Mozilla's financial ties to advertising.",
      "Users can disable the feature in settings, and alternatives like LibreWolf or adjustments via about:config are recommended for enhanced privacy."
    ],
    "points": 228,
    "commentCount": 178,
    "retryCount": 0,
    "time": 1721034656
  },
  {
    "id": 40966414,
    "title": "Rust for Filesystems",
    "originLink": "https://lwn.net/Articles/978738/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us Edition Return to the Front page User: Password:| Subscribe / Log in / New account Rust for filesystems This article brought to you by LWN subscribers Subscribers to LWN.net made this article — and everything that surrounds it — possible. If you appreciate our content, please buy a subscription and make the next set of articles possible. By Jake Edge June 21, 2024 LSFMM+BPF At the 2024 Linux Storage, Filesystem, Memory Management, and BPF Summit, Wedson Almeida Filho and Kent Overstreet led a combined storage and filesystem session on using Rust for Linux filesystems. Back in December 2023, Almeida had posted an RFC patch set with some Rust abstractions for filesystems, which resulted in some disagreement over the approach. On the same mid-May day as the session, he posted a second version of the RFC patches, which he wanted to discuss along with other Rust-related topics. Goals After updating attendees on the status of his patches, Almeida listed some of the goals of the Rust-for-Linux project, which are embodied in the filesystem abstractions that he is proposing. The first is to express more of the requirements using Rust's type system in order to catch more mistakes at compile time. In addition, the project's developers want to automate some tasks, such as cleaning up resources, in ways that are not easily available to C code. The overall idea is to have a more productive filesystem-development experience, with less time spent on debugging problems that the compiler could find, and with fewer memory-related vulnerabilities overall. Overstreet said that he had been a part of too many two-week bug hunts and has been trying to find ways to avoid those kinds of problems for bcachefs. The Rust language provides a lot more than what he can do in C; it eliminates undefined behavior and provides facilities to see what is happening inside the code. \"You can't debug, if you can't see what's going on.\" He believes that kernel development \"will get a whole lot easier over the coming decades\" due to using Rust. It will be possible to prove the correctness of code written in Rust, which will mean that bugs that can derail feature development will be much less common. From his slides, Almeida showed an example of how the Rust type system can eliminate certain kinds of errors. He noted that the iget_locked() function in current kernels has a complicated set of requirements. Callers must check to see if the return value is null and, if it is not, then the contents of the returned struct inode need to be checked to see if it is a new or existing inode. If it is new, it needs to be initialized before it can be used; if that fails, iget_failed() needs to be called, he said. There was some discussion of the finer points of what callers of iget_locked() need to do, with Al Viro disagreeing with some of what Almeida had on his slide. That went back and forth, with Overstreet observing that it was exactly that kind of discussion/argument that could be avoided by encapsulating the rules into the Rust types and abstractions; the compiler will know the right thing to do. Overstreet noted that Christian Brauner and Alice Ryhl have helped to improve the abstractions a great deal since the first posting; in particular, there are things he has learned about reference counts based on how they are being handled by the Rust code. \"This is going to make all our lives so much easier\", Overstreet said. Almeida put up a slide with the equivalent of iget_locked() in Rust, which was called get_or_create_inode(). The important part is the return type, he said; as with C, callers must check for failure, but the success case is much different. If it is successful, the caller either receives a regular reference-counted inode to use (which has its reference count automatically decremented when the inode object is no longer referenced) or it receives a new inode, which will automatically call the equivalent of iget_failed() if it is never initialized. If it is ever initialized (which can only be done once), it becomes a regular inode with the automatic reference-count decrement. All of that is enforced through the type system. Viro seemed somewhat skeptical of how that would work in practice. He wondered where in the source code those constraints would be defined. Almeida said that the whole idea is to determine what the constraints are from Viro and other filesystem developers, then to create types and abstractions that can enforce them. Disconnect Dave Chinner asked about the disconnect between the names in the C API and the Rust API, which means that developers cannot look at the C code and know what the equivalent Rust call would be. He said that the same names should be used or it would all be completely unfamiliar to the existing development community. In addition, when the C code changes, the Rust code needs to follow along, but who is going to do that work? Almeida agreed that it was something that needs to be discussed. As far as the renamed functions goes, he is not opposed to switching the names to match the C API, but does not think iget_locked() is a particularly good name. It might make sense to take the opportunity to create better names. There was some more discussion of the example, with Viro saying that it was not a good choice because iget_locked() is a library function, rather than a member function of the superblock object. Almeida said that there was no reason get_or_create_inode() could not be turned into a library function; his example was simply meant to show how the constraints could be encoded in the types. Brauner said that there needs to be a decision on whether the Rust abstractions are going to be general-purpose, intended for all kernel filesystems, or if they will only be focused on the functionality needed for the simpler filesystems that have been written in Rust. There is also a longer-term problem in handling situations where functions like get_or_create_inode() encode a lot more of the constraints than iget_locked() does. As the C code evolves, which will happen more quickly than with the Rust code, at least initially, there will be a need to keep the two APIs in sync. It comes down to a question of whether refactoring and cleanup will be done as part of adding the Rust abstractions, Overstreet said; he strongly believes that is required. But there is more to it than just that, James Bottomley said. The object lifecycles are being encoded into the Rust API, but there is no equivalent of that in C; if someone changes the lifecycle of the object on one side, the other will have bugs. There are also problems because the lifecycle of inode objects is sometimes filesystem-specific, Chinner said. Encoding a single lifecycle understanding into the API means that its functions will not work for some filesystems. Overstreet said that filesystems which are not using the VFS API would simply not benefit, but Chinner said that a VFS inode is just a structure and it is up to filesystems to manage its lifetime. Almeida said that the example would only be used by filesystems that currently call iget_locked() and could benefit. The Rust developers are not trying to force filesystems to change how they are doing things. Allocating pain Part of the problem, Ted Ts'o said, is that there is an effort to get \"everyone to switch over to the religion\" of Rust; that will not happen, he said, because there are 50+ different filesystems in Linux that will not be instantaneously converted. The C code will continue to be improved and if that breaks the Rust bindings, it will break the filesystems that depend on them. For the foreseeable future, the Rust bindings are a second-class citizen, he said; broken Rust bindings are a problem for the Rust-for-Linux developers and not the filesystem community at large. He suggested that the development of the Rust bindings continue, while the C code continues to evolve. As those changes occur, \"we will find out whether or not this concept of encoding huge amounts of semantics into the type system is a good thing or a bad thing\". In a year or two, he thinks the answer to that will become clear; really, though, it will come down to a question of \"where does the pain get allocated\". In his mind, large-scale changes like this almost always come down to a \"pain-allocation question\". Almeida said that he is not trying to keep the C API static; his goal is to get the filesystem developers to explain the semantics of the API so that they can be encoded into Rust. Bottomley said that as more of those semantics get encoded into the bindings, they will become more fragile from a synchronization standpoint. Several disagreed with that, in the form of a jumble of \"no\" replies and the like. Almeida said that it was the same with any user of an API; if the API changes, the users need to be updated. But Ts'o pointedly said that not everyone will learn Rust; if he makes a change, he will fix all of the affected C code, but, \"because I don't know Rust, I am not going to fix the Rust bindings, sorry\". Viro came back to his objections about the proposed replacement for iget_locked(). The underlying problem that he sees is the reliance on methods versus functions; using methods is not the proper way forward because the arguments are not specified explicitly. But Overstreet said that the complaints about methods come from languages like C++ that rely too heavily on inheritance, which is \"a crap idea\". Rust does not do so; methods in Rust are largely just a syntactical element. There was some discussion of what exactly is being encoded in the types. Jan Kara said that there is some behavior that goes with the inode, such as its reference count and its handling, but there is other behavior that is inherent in the iget_locked() function. Overstreet and Almeida said that those two pieces were both encoded into the types, but separately; other functions using the inode type could have return values with different properties. Viro went through some of his reasoning about why inodes work the way they do in the VFS. He agreed with the idea of starting small to see where things lead. Overstreet suggested that maybe the example used was not a good starting point, \"because this is the complicated case\". \"Oh, no it isn't\", Viro replied to laughter as the session concluded. Index entries for this article Kernel Development tools/Rust Kernel Filesystems/Internal API Conference Storage, Filesystem, Memory-Management and BPF Summit/2024 (Log in to post comments) Capturing complex requirements in Rust helps evolution Posted Jun 21, 2024 22:26 UTC (Fri) by roc (subscriber, #30627) [Link] (4 responses) Capturing complex API requirements in the Rust type system is actually a huge *win* when those requirements change. The compiler tells you what needs to be fixed, and whether your fix satisfies the new requirements. It is much easier and safer to make these kinds of sweeping changes than with C or C++, and it is easier to review those changes. So, fixing users of Rust abstractions will probably turn out to be less work per user than fixing the C users of the underlying C APIs that have changed. But it does sound like coordinating that work with kernel devs who simply refuse to interact with Rust code in any way is going to be painful. Capturing complex requirements in Rust helps evolution Posted Jun 22, 2024 1:00 UTC (Sat) by python (subscriber, #171317) [Link] (3 responses) I'm excited to see what they come up with. More discipline and scrutiny (which is required when inter-operating multiple languages) will probably reveal a lot of hidden and unintended behaviors. I sincerely hope they find a way to bury some of the really horrible crufty APIs (epoll in userspace come to mind, I am sure something analogous exists in the kernel API) and replace them with something a little more sane. It will be interesting to see if the rust bindings (making full use of the type system) could be more secure (or less secure?), despite being a thin wallpaper over raw C calls to an ever changing kernel API. Also will be interesting if it impacts performance, particularly since Rust favors doing/structuring things certain ways that are uncommon in plain C. I would be inclined to think that any sort of simple wrapper would be at least minusculely slower, but perhaps it might enable more complex and efficient ways of doing things? (automatically handled in the background by the wrappers) Capturing complex requirements in Rust helps evolution Posted Jun 22, 2024 9:11 UTC (Sat) by Wol (subscriber, #4433) [Link] (2 responses) If the Rust compiler is capable of emitting a C .h file, that would make it even better! Unfortunately, it probably can't ingest a .h file because different C compilers could lay things out differently, all C cares about is consistency during the compile so unless the programmer has taken special care a .h can have an ambiguous layout. The Rust compiler could take that care. In which case, it's then hopefully (crossed fingers and toes!) a drop-in swap of .h files. Cheers, Wol Capturing complex requirements in Rust helps evolution Posted Jun 22, 2024 9:43 UTC (Sat) by atnot (subscriber, #124910) [Link] (1 responses) the rust compiler is not, but bindgen is (c headers->rust) and cbindgen does the reverse (rust->c headers) Capturing complex requirements in Rust helps evolution Posted Jun 22, 2024 13:52 UTC (Sat) by atnot (subscriber, #124910) [Link] The thing that doesn't solve though, is how much information you lose going from Rust to C. That's somewhat improving with things like counted_by, but it's a long way off. However, even that ultimately doesn't really matter, because the barriers to calling Rust from C aren't primarily technical. It's more the attitude among some people that Rust for Linux is just a temporary blip that is bound to fail, or a hope that it does so you don't have to deal with it. Which becomes significantly harder if C code is allowed to call Rust. There will realistically be a pretty long tail of people who need a few more years to be convinced Rust is not going away, and probably not everyone will be. However I think writing C APIs in Rust will be a lot more palatable when, say, the GPU driver you are currently using being removed seems unwelcome enough that Rust is de facto necessary anyway. But it'll be a bit until that happens. How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 22, 2024 10:41 UTC (Sat) by gray_-_wolf (subscriber, #131074) [Link] (10 responses) This part has cought my eye: > But Ts'o pointedly said that not everyone will learn Rust; if he makes a change, he will fix all of the affected C code, but, \"because I don't know Rust, I am not going to fix the Rust bindings, sorry\". I understand the view point, but I wonder how will this work with regards to the \"do not break user space\" policy. Let us say that Ts'o does some change, and updates all C users. But the Rust binding will break (either compilation, or runtime behavior). What now? In ideal world someone from rust-on-linux will step up and promptly resolve the issue. However we do not live in the ideal world and everyone has full plate already. What now? Will Ts'o's change be prevented from being merged? Will it be merged and the rust part just be broken (with user-space visible effects) until someone finds the time? On similar note, will contributors with less weight behind them have the same priviledge of saying \"I am not touching rust code\"? Regardless of my opinion on rust, this whole thing is really interesting experiment, so I am wondering what is current thinking in the community regarding this. How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 22, 2024 12:33 UTC (Sat) by pbonzini (subscriber, #60935) [Link] (7 responses) What will _actually_ happen is a mix of three things: 1) APIs are changed in backwards-incompatible ways (e.g. https://www.spinics.net/lists/intel-gfx/msg349025.html) but they're generally not complicated and you handle them with topic branches as usual. 2) new APIs are introduced but updating 50 filesystems (or hundreds of anonymous file_operations) does not happen at ones, therefore in practice fallbacks are left in place and Rust bindings can be updated separately. The typical example here is read_iter/write_iter. 3) most subsystems that have Rust bindings will have no problem adjusting, at which point who's left will have to acknowledge the reality. How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 25, 2024 8:42 UTC (Tue) by b7j0c (subscriber, #27559) [Link] (6 responses) > What will _actually_ happen is a mix of three things: treating the migration to Rust as inevitable feels like magical thinking, same as Mozilla experienced with Servo How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 25, 2024 10:42 UTC (Tue) by pbonzini (subscriber, #60935) [Link] (5 responses) I'm not talking of a full-scale migration; a non-trivial or undeniable level of adoption is enough. How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 25, 2024 11:57 UTC (Tue) by pizza (subscriber, #46) [Link] (2 responses) > I'm not talking of a full-scale migration; a non-trivial or undeniable level of adoption is enough. In other words, \"The inevitability of Rust becoming a requirement to build a usable/useful kernel.\" ...All it takes is one driver (not even subsystem), and *BAM* you're now a Rust system with a (substantial) pile of C. (Or rather, \"kernel-Rust\" with a pile of \"kernel-C\") How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 25, 2024 12:13 UTC (Tue) by Wol (subscriber, #4433) [Link] (1 responses) > > I'm not talking of a full-scale migration; a non-trivial or undeniable level of adoption is enough. > In other words, \"The inevitability of Rust becoming a requirement to build a usable/useful kernel.\" Which, if the claims of the speed with which good solid drivers can be written in Rust are true, is inevitable sooner rather than later ... I remember reading somewhere, that the speed at which a good programmer could produce good code was measured in LoC, REGARDLESS OF THE LANGUAGE USED. In other words, measured in terms an end user could understand - what a system could do - the choice of language has a major impact on productivity. Cheers, Wol How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 25, 2024 17:20 UTC (Tue) by NYKevin (subscriber, #129325) [Link] > I remember reading somewhere, that the speed at which a good programmer could produce good code was measured in LoC, REGARDLESS OF THE LANGUAGE USED. This is less surprising than it sounds. It basically amounts to \"higher level languages allow programmers to write code that does more elaborate things in the same amount of development time,\" which had darned well better be true considering all of the performance cost of e.g. Python. If Python didn't give you a development speed advantage, there would be no (or at least much less) reason to use it for serious purposes (outside of the classroom). How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 25, 2024 15:23 UTC (Tue) by b7j0c (subscriber, #27559) [Link] (1 responses) both Microsoft and Google appear to be able to introduce Rust into existing codebases - but they also have the power to promote, deliver bonuses, or alternatively, fire people who aren't aligned with the strategy open source projects are different...volunteers can just move on if they are unhappy, and if you don't have suitable replacement volunteers, things stop happening How will fixing only C part work with the \"do not break user space\" policy? Posted Jul 4, 2024 2:13 UTC (Thu) by mrugiero (guest, #153040) [Link] That won't be a problem. Most kernel development is made by employees of companies with the ability to promote, deliver bonuses and firing, and they'd rather have their kernel maintained, be it in C, Rust or Pascal. You are right for the general case though, most open source projects are driven by volunteers. Linux is an exception to that rule. How will fixing only C part work with the \"do not break user space\" policy? Posted Jun 24, 2024 6:27 UTC (Mon) by LtWorf (subscriber, #124958) [Link] (1 responses) \"do not break user space\" only applies to syscalls basically. If a filesystem stops working, that has never counted as \"userspace is now broken\". How will fixing only C part work with the \"do not break user space\" policy? Posted Jul 4, 2024 2:16 UTC (Thu) by mrugiero (guest, #153040) [Link] OTOH, isn't fixing what you break the general etiquette? I don't think \"I don't know this\" works as an excuse in other cases. You don't get to leave XFS broken if you touch VFS because you don't know XFS. Rust is not even that hard to learn when you come from C (also knowing C++ helps a lot as well, since Rust is pretty much C++ best practices enforced by the compiler), it's not frontend programmers we're dealing with. Just wait Posted Jun 22, 2024 10:54 UTC (Sat) by wsy (subscriber, #121706) [Link] (14 responses) Sometimes it's just easier to wait until older guys retire than to convince them to adapt to new way of doing things. Just wait Posted Jun 22, 2024 11:41 UTC (Sat) by pizza (subscriber, #46) [Link] > Sometimes it's just easier to wait until older guys retire than to convince them to adapt to new way of doing things. Change, real change, has _always_ operated on a generational cycle. (And this isn't \"adapt to the new way\", it's \"double your congnitive workload maintaining a mission-critical working system before its nominal replacement is ready to be deployed.\") Just wait Posted Jun 22, 2024 12:08 UTC (Sat) by willy (subscriber, #9762) [Link] (12 responses) https://en.wikipedia.org/wiki/Planck%27s_principle \"Science advances one funeral at a time\" I'm disappointed by how resistant some fellow hackers are to Rust. It's the first language in 50 years to be enough of an advantage over C to be worth switching to. Just wait Posted Jun 22, 2024 12:37 UTC (Sat) by liw (subscriber, #6379) [Link] (9 responses) If it were only a matter of teaching kernel hackers the Rust language, that'd be a solvable problem. (I say, as someone who does Rust training for free and for money.) But I fear there's usually more resistance to change than just having to learn a new language or a new tool. Just wait Posted Jun 23, 2024 8:52 UTC (Sun) by Wol (subscriber, #4433) [Link] (3 responses) > But I fear there's usually more resistance to change than just having to learn a new language or a new tool. It usually involves changing your entire way of thinking. Look at that post a few days back over P4 - where the comment was \"someone comes along thinking they can rewrite it in C\", and several man-years of effort later, they realise that actually, someone using a domain specific language can do double the work in half the time, if not even better. Going back even further, someone commented people who've learnt Rust usually make far better C programmers because, even though C doesn't enforce memory safety etc, because Rust insists that you code in memory-safe ways, they code memory-safe in C anyways. It's like me and databases - 4th normal form is the best for a whole bunch of reasons, but because my experience is with a database where 4th normal form was the OBVIOUS way to do it, when I'm forced to work with relational I do it without thinking. Unfortunately I'm usually working with stuff designed by others where I'm thinking \"what the hell were they thinking?\". Cheers, Wol Just wait Posted Jun 23, 2024 17:48 UTC (Sun) by Kaligule (subscriber, #167650) [Link] (2 responses) Please, what is P4? Just wait Posted Jun 23, 2024 19:37 UTC (Sun) by softball (subscriber, #160655) [Link] They are possibly referring to: https://lwn.net/Articles/977310/ P4 Posted Jun 24, 2024 22:04 UTC (Mon) by riking (subscriber, #95706) [Link] https://p4.org/ is a programmable packet processing specification language. You use it to write firewall or routing rules, or queue dropping priorities, or whatever the hardware of the router you bought allows you to load a P4 program to do. Just wait Posted Jun 25, 2024 13:36 UTC (Tue) by zuki (subscriber, #41808) [Link] (4 responses) I find the attitude of \"I don't care about Rust, I'll not learn Rust\" annoying and destructive. There's also the distinction that there are different levels of \"knowing\" a language. It's quite easy to get to the level where one can do small modifications to existing code, or to copy existing functionality and extend it to cover additional cases. It's much harder to know which of the many possible ways of structuring code and which abstractions to use for a new problem. But fortunately, for ongoing maintenance, this first easier level is all that is needed. The second higher level is only necessary e.g. to implement or review new Rust abstractions in the kernel or new drivers, but a different set of people can handle that. I think it's entirely reasonable to ask maintainers to also care about the Rust code. Just wait Posted Jun 25, 2024 14:36 UTC (Tue) by pizza (subscriber, #46) [Link] > I think it's entirely reasonable to ask maintainers to also care about the Rust code. I agree in principle -- but we're a long, long way from the \"ongoing maintenance\" phase. The current status quo is that, in order to meaningfully contribute to kernel-Rust, you have to essentially be an expert in all-things-Rust, including living on the bleeding edge of Rust language/feature development. > I find the attitude of \"I don't care about Rust, I'll not learn Rust\" annoying and destructive. You may find it annoying but it is an entirely rational (and reasonable!) attitude to take given that hyper-unstable nature of kernel-Rust and the already-overwhelming \"just maintaining existing stuff\" workload. After all, \"Let those who care about X do the work\" has been the kernel development philosophy since approximately forever. Just wait Posted Jun 25, 2024 14:51 UTC (Tue) by somlo (subscriber, #92421) [Link] Having never used the Go language, I managed to \"cut'n'paste\" my way into writing a useful (and also stylistically and functionally correct) patch against Docker. So in principle you *should* be right about your first level of \"knowing\" a language concept. However, unlike Go, I find Rust hard to skim over -- for the lack of a better word, it's too \"syntax-y\" for my brain :) So your \"anyone should be able to deal with it\" statement is actually a much bigger ask than you think, depending on the actual language's legibility to newcomers. I then tried working my way through \"The Rust Programming Language\", and was mostly able to follow along and understand what's going on, and even managed to write ok-ish small programs in the process. But without being a real, $DAYJOB Rust programmer, when I look at production code a few months later, it's back to a wall-of-syntax \"foo::bar => \" incomprehensible gibberish... :D This may be just an undiagnosed learning disability on my part, or it might be that Rust syntax is simply less legible to a large swath of the otherwise OK programmer population. I'm also constantly high-key annoyed at how compiling a program involves downloading crap off the Internet, as part of the compilation process (this is the part where I might just be old and grumpy, and the whole download-crap-from-the-Internet thing is interfering with my lawn care routine). Anyhow, if kernel maintainers' experience is anything like mine, I can't blame them for putting a \"low pass filter\" on this thing: maybe it goes away if they ignore it long enough, and they won't have to waste time on it. Now, if based on the description of my pain points above there's a more targeted way of learning Rust in a way that sticks, I'm happy to take hints and advice... Changing code when you barely understand the language Posted Jun 25, 2024 15:40 UTC (Tue) by farnz (subscriber, #17727) [Link] The other thing to bear in mind is that different languages have different challenge levels when you're not very good at them. At one extreme, you have languages where just about everything you could reasonably consider \"language source code\" is accepted by implementations as \"valid\" code, and if you barely understand the language, it's really hard to avoid making mistakes. At the other extreme, you have languages where the compiler will definitely complain if you make a change that won't work, and thus it's a lot easier to avoid making mistakes because the compiler tells you that what you've implemented is not going to work. And in the middle of those two extremes, we have real languages, where some things that won't work cause the compiler to complain (such as including a file that doesn't exist like #include ), while other things that won't work compile and fail later. The interesting question is whether Rust is strict enough that someone with good C skills and minimal Rust skills can be confident that the Rust toolchain will complain if they make the \"obvious\" fix, but it's wrong, or whether they have to learn more Rust so that they can do the checks themselves (or ask someone else to check their work). Just wait Posted Jul 4, 2024 2:23 UTC (Thu) by mrugiero (guest, #153040) [Link] > I think it's entirely reasonable to ask maintainers to also care about the Rust code. I'm not sure if I'm inventing this, but I believe the deal was that maintainers got to decide whether Rust made it into in their subsystems? If that's the case, it's only reasonable to expect them to either reject the patches or commit to keep them working. Otherwise we're in a kind of Seinfeld's car reservation situation. Just wait Posted Jun 22, 2024 14:05 UTC (Sat) by khim (subscriber, #9252) [Link] (1 responses) > It's the first language in 50 years to be enough of an advantage over C to be worth switching to. Sure, but the flip side is that it is the first language in 50 years that offers genuine advantage large enough to switch. The only way to convince people to switch is to write more good Rust code, ultimately. Just wait Posted Jun 22, 2024 14:21 UTC (Sat) by adobriyan (subscriber, #30858) [Link] > The only way to convince people to switch is to write more good Rust code, ultimately. Just imagine what bug-for-bug compatible Linux.rs kernel could do... test suite for complicated cases Posted Jun 22, 2024 15:17 UTC (Sat) by aszs (subscriber, #50252) [Link] (5 responses) I'm a little surprised there's no mention here about building a test suite for these \"complicated cases\" -- that's the obvious way to make sure two independent implementations conform. And it has some benefits: * the grunt work doesn't have to be done by time-constrained core developers * getting a nice test suite out of this effort could be a good sweetener to motivate Rust-resistant maintainers test suite for complicated cases Posted Jun 22, 2024 18:41 UTC (Sat) by willy (subscriber, #9762) [Link] (3 responses) How would you write a test suite that verifies that every filesystem conforms to the current VFS locking rules? Particularly when those locking rules are mostly not written down. To take an example that I do know... When you call folio_mark_dirty(), you must guarantee that the folio will not be concurrently truncated from the file (for values of truncate that include operations like hole-punch). Holding the folio lock is one way to do that. But this is a sleeping lock, so you can't always do that. If the folio is currently mapped by a page table, holding that page table lock guarantees truncation will not complete, and some callers rely on this. If you have buffer heads attached to the folio, and you have a buffer head locked, then that is also sufficient to prevent truncation. Some callers rely on this. Now, encode all that information into a type system? Not sure it can be done. And you certainly can't write a test suite for it. Or any reasonable assertion. I'm a huge fan of test suites. But saying \"just write a test suite\" without understanding the problem space is not helpful. test suite for complicated cases Posted Jun 22, 2024 20:06 UTC (Sat) by mathstuf (subscriber, #69389) [Link] One way would be to have some \"token\" ZST (zero-size type) that one could retrieve from any of these locking mechanisms. `folio_mark_dirty()` would then require that as one of its parameters. Something like: ``` let folio_lock = folio.lock(); let mark_dirty_allowed = folio_lock.i_want_to_mark_dirty(); // … let buffer_head_lock = buffer_head.lock(); let mark_dirty_allowed = buffer_head_lock.i_want_to_mark_folio_dirty(&folio)?; // better error handling, but checks that it is attached // …. let page_table_lock = page_table.lock(); let mark_dirty_allowed = page_table_lock.i_want_to_mark_folio_dirty(&folio); // similar to above; make sure this page table maps the folio ``` Techniques from ghost_cell[1] can likely be used to ensure that the \"mark_dirty_allowed\" proof token is associated with the folio in question and not any folio that might also exist. [1] https://docs.rs/ghost-cell/latest/ghost_cell/ test suite for complicated cases Posted Jun 23, 2024 3:53 UTC (Sun) by aszs (subscriber, #50252) [Link] Maybe I missed something but it didn't seem that the problem being discussed here was how to \"verify that every filesystem conforms to the current VFS locking rules\" but rather how to make sure the semantics of the Rust APIs for writing file systems match the semantics for the C APIs for writing file systems -- even as those semantics are changed developers that might ignore the Rust bindings. And if it's unrealistic to test all the ways a surface API might impact some internal invariant, well that's what fuzz testing is for. To be clear, I wasn't saying \"just\" write a test suite, I was saying I'm surprised tests aren't being discussed as part of a solution to a fairly hard problem. test suite for complicated cases Posted Jul 4, 2024 2:34 UTC (Thu) by mrugiero (guest, #153040) [Link] > Now, encode all that information into a type system? Not sure it can be done. There's the option to not do that in Rust, too. The type system is quite useful most of the time, but there is a point of diminishing returns and at that point you rely on runtime checks and discipline just like in any other language. test suite for complicated cases Posted Jul 15, 2024 2:49 UTC (Mon) by ssokolow (guest, #94568) [Link] ...but do also keep this quote in mind: Program testing can be a very effective way to show the presence of bugs, but it is hopelessly inadequate for showing their absence. -- Edsger W. Dijkstra, \"The Humble Programmer\" (1972) Copyright © 2024, Eklektix, Inc. This article may be redistributed under the terms of the Creative Commons CC BY-SA 4.0 license Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=40966414",
    "commentBody": "Rust for Filesystems (lwn.net)220 points by drakerossman 9 hours agohidepastfavorite146 comments gwbas1c 5 hours agoMaybe they are asking the wrong questions? Does Rust need to change to make it easier to call C? I've done a bit of Rust, and (as a hobbyist,) it's still not clear (to me) how to interoperate with C. (I'm sure someone reading this has done it.) In contrast, in C++ and Objective C, all you need to do is include the right header and call the function. Swift lets you include Objective C files, and you can call C from them. Maybe Rust as a language needs to bend a little in this case, instead of expecting the kernel developers to bend to the language? reply lambda 4 hours agoparentCalling C from Rust can be quite simple. You just declare the external function and call it. For example, straight out of the Rust book https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html#usin... : extern \"C\" { fn abs(input: i32) -> i32; } fn main() { unsafe { println!(\"Absolute value of -3 according to C: {}\", abs(-3)); } } Now, if you have a complex library and don't want to write all of the declarations by hand, you can use a tool like bindgen to automatically generate those extern declarations from a C header file: https://github.com/rust-lang/rust-bindgen There's an argument to be made that something like bindgen could be included in Rust, not requiring a third party dependency and setting up build.rs to invoke it, but that's not really the issue at hand in this article. The issue is not the low-level bindings, but higher level wrappers that are more idiomatic in Rust. There's no way you're going to be able to have a general tool that can automatically do that from arbitrary C code. reply jiripospisil 4 hours agorootparentThere's also cbindgen for going the other way around. https://github.com/mozilla/cbindgen reply varjag 3 hours agorootparentprevThat's not really \"simple\", it's on par with C FFI in about any other language (except C++), with same drawbacks. reply commodoreboxer 2 hours agorootparentIt's on par with C++, too. In C++ you need an `extern \"C\"`, because C++ linkage isn't guaranteed to be the same as C linkage. You can get away with wrapping that around it in a preprocessor conditional, but that's not all that much easier than Rust's bindgen. A lot of C to C++ interop is actually done wrong without knowing it. Throwing a C++ static function as a callback into a C function usually works, but it's not technically correct because the linkage isn't guaranteed to be the same without an extern \"C\". In practice, it usually is the same, but this is implementation-defined, and C++ could use a different calling convention from C (e.g. cdecl vs fastcall vs stdcall. The Borland C++ compiler uses fastcall by default for C++ functions, which will make them illegal callbacks for C functions). The major difference between Objective-C and C++'s C interop and other languages is the lack of the preprocessor. Macros will just work because they use the same preprocessor. That's really not easy to paper over in other languages that can't speak the C preprocessor. reply gizmo686 3 hours agorootparentprev... And? Most languages make C interop simple. reply varjag 3 hours agorootparentThey quickly become unwieldy on non-trivial APIs, with hundreds of definitions across dozens of files and with macros to boot. Naturally people would still get the job done but it's beyond simple. reply mcronce 2 hours agorootparentThat's what bindgen is for, as was mentioned in the original comment you replied to. reply tupshin 5 hours agoparentprevThis is not a notable challenge in rust, nor relevant to the article. The article is about finding ways of using rust to actually implement kernel fs drivers/etc. Note that any rust code in the kernel is necessarily consuming C interfaces. Bindgen works quite well for the use case that you are thinking. https://github.com/rust-lang/rust-bindgen reply moomin 4 hours agorootparentYeah, the Rust proponents are being significantly more ambitious. Not just the ability to code a file system in Rust, but do it in a way that catches a lot of the correctness issues relating to the complex (and changing) semantics of FS development. reply duped 4 hours agoparentprevIt's actually pretty easy. All you need is declare `extern \"C\" fn foo() -> T` to be able to call it from Rust, and to pass the link flags either by adding a #[link] attribute or by adding it in a build.rs. You can use the bindgen crate to generate bindings ahead of time, or in a build.rs and include!() the generated bindings. Normally what people do is create a `-sys` crate that contains only bindings, usually generated. Then their code can `use` the bindings from the sys crate as normal. > in contrast, in C++ and Objective C, all you need to do is include the right header and link against the library. reply codetrotter 4 hours agoparentprevI’ve written Rust code that called C++ It wasn’t completely straightforward, but on the whole I figured out everything I needed to within a few days in order to be able to do it. Calling C would surely be very similar. reply Smaug123 1 hour agoparentprevThe point is that Rust can model invariants that C can't. You can call both ways, but if C is incapable of expressing what Rust can, that has important implications for the design of APIs which must be common to both. reply pornel 5 hours agoprevI don't get how can each file system have a custom lifecycle for inodes, but still use the same functions for inode lifecycle management, but apparently with different semantics? That sounds like the opposite of an abstraction layer, if the same function must be used in different ways depending on implementation details. If the lifecycle of inodes is filesystem-specific, it should be managed via filesystem-specific functions. reply phkahler 5 hours agoparent>> I don't get how can each file system have a custom lifecycle for inodes, but still use the same functions for inode lifecycle management, but apparently with different semantics? I had the same question. They're trying to understand (or even document) all the C APIs in order to do the rust work. It sounds like collecting all that information might lead to some [WTFs and] refactoring so questions like this don't come up in the first place, and that would be a good thing. reply seanhunter 1 hour agoparentprevIf you haven't seen it before, you might find this useful https://www.kernel.org/doc/html/latest/filesystems/vfs.html It's an overview of the VFS layer, which is how they do all the filesystem-specific stuff while maintaining a consistent interface from the kernel. reply sandywaffles 4 hours agoparentprevI understood it as they're working to abstract as much as is generally and widely possible in the VFS layer, but there will still be (many?) edge cases that don't fit and will need to be handled in FS-specific layers. Perhaps the inode lifecycle was just an initial starting point for discussion? reply DSMan195276 2 hours agoparentprev> but still use the same functions for inode lifecycle management I'm not an expert by any means but I'm somewhat knowledgeable, there's different functions that can be used to create inodes and then insert them into the cache. `iget_locked()` that's focused on here is a particular pattern of doing it, but not every FS uses that for one reason or another (or doesn't use it in every situation). Ex: FAT doesn't use it because the inode numbers get made-up and the FS maintains its own mapping of FAT position to inodes. There's then also file systems like `proc` which never cache their inode objects (I'm pretty sure that's the case, I don't claim to understand proc :P ) The inode objects themselves still have the same state flow regardless of where they come from, AFAIK, so from a consumer perspective the usage of the `inode` doesn't change. It's only the creation and internal handling of the inode objects by the FS layer that depends based on what the FS needs. reply crest 4 hours agoparentprevI assume it's supposed to work by having the compiler track the lifetime of the inodes. The compiler is expected to help with ephemeral references (the file system still has to store the link count to disk). reply BiteCode_dev 5 hours agoprevGiven how those discussions usually go, and the scale of the change, I find that discussion extraordinarily civil. I disagree with the negative tone of this thread, I'm quite optimistic given how clearly the parties involved were able to communicate the pain points with zero BS. reply nickparker 4 hours agoparentI found myself reading this more for the excellent notetaking than for the content. I suspect the discussion was about as charged, meandering, and nitpicky as we all expect a PL debate among deeply opinionated geeks to be, and Jake Edge (who wrote this summary) is exceptionally good at removing all that and writing down substance. reply BiteCode_dev 3 hours agorootparentCertainly. We are talking about extremely competent people who worked on a critical piece of software for years and invested a lot of their lives in it, with all pain, effort, experience, and responsibilities that come with that. That this debate is inscribed is a process that is still ongoing, and in fact, progressing, is a testament to how healthy the situation is. I was expecting the whole Rust thing to be shut down 10 times, in a flow of distasteful remarks, already. This means that not only Rust is vindicated as promising for the job, but both teams are willing and up to the task of working on the integration. Those projects are exhausting, highly under-pressure situations, and they last a long time. I still find that the report is showing a positive outcome. What do people expect? Move fast and break things? A barrage of \"no\" is how it's supposed to go. reply structural 31 minutes agorootparentI agree. And ideally, every time you raise the question and get the \"no\" response, you learn something about the system you're modifying or the reviewer learns something about your solution. Then you improve your solution, and come back. Eventually consensus is built - either the solution becomes good enough, or both the developers and the reviewers agree that it's not going to work out and the line of development gets abandoned. Large-scale change in production is hard, and messy, and involves a lot of imperfect humans that we hope are mostly well-intentioned. reply 0cf8612b2e1e 2 hours agorootparentprevI am definitely of the opinion we need to rush away from C. Rust, Go, Zig, etc does not matter, but anything which can catch some of the repeated mistakes that squishy humans keep repeating. That being said, the file system is one of those infrastructure bits where you cannot make a mistake. Introduce a memory corruption bug leading to crashes every Thursday? Whatever. Total loss of data for 0.1% of users during a leap year at a total eclipse? Apocalypse. There is no amount of being too careful when interfacing with storage. C may have a lot of foibles, but it is the devil we know. reply sandywaffles 4 hours agoprevI wasn't clear and am not familiar enough with the Linux FS systems to know if this Rust API would be wrapping or re-implementing the C APIs? If it's re-implementing (or rather an additional API) it seems keeping the names the same as the C API would be problematic and lead to more confusion over time, even if initially it helped already-familiar-developers grok whats going on faster. reply CGamesPlay 4 hours agoparent> Almeida put up a slide with the equivalent of iget_locked() in Rust, which was called get_or_create_inode(). Seems like the answer is that it's reimplementing and doesn't use the same names. reply swfsql 3 hours agorootparentI'm not familiar with those functions, but I had the impression they actually shouldn't have the same name. Since the Rust function has implicit/automatic behavior depending on how it's state is and how it's used by the callsite, and since the C one doesn't have any implicit/automatic behavior (as in, separate/explicit lifecycle calls must be made \"manually\"), I don't even see the reason for them to have the same name. That is to say, having the same name would be somehow wrong since the functions do and serve for different stuff. But it would make sense, at least from the Rust site, to have documentation referring to the original C name. reply brodouevencode 3 hours agoprev> about the disconnect between the names in the C API and the Rust API, which means that developers cannot look at the C code and know what the equivalent Rust call would be Ah, the struggle of legacy naming conventions. I've had success in keeping the same name but when I wanted an alternative name I would just wrap the old name with the new name. But yeah, naming things is hard. reply adastra22 56 minutes agoparentOne of the two major problems in computer science (the other two being concurrency and off-by-one errors). reply ysw0145 7 hours agoprevHaving more options available in the Linux kernel is always beneficial. However, Rust may not be the solution for everything. While Rust does its best to ensure its programming model is safe, it is still a limited model. Memory issues? Use Rust! Concurrency problems? Switch to Rust! But you can't do everything that C does without using unsafe blocks. Rust can offer a fresh perspective to these problems, but it's not a complete solution. reply tialaramex 7 hours agoparent> But you can't do everything that C does without using unsafe blocks For this particular work the huge benefit of Rust is its enthusiasm for encapsulating such safety problems in types. Which is indeed what this article is about. C and particularly the way C is used in the kernel makes it everybody's responsibility to have total knowledge of the tacit rules. That cannot scale. A room full of kernel developers didn't entirely agree on the rules for a data structure they all use! Rust is very good at making you aware of rules you need to know, and making it not your problem when it can be somebody else's problem to ensure rules are followed. Sometimes the result will be less optimal, but even in the Linux kernel sub-optimal is often the right default and we can provide an (unsafe) escape hatch for people who can afford to learn six more weird rules to maybe get better performance. reply mjburgess 6 hours agorootparent> That cannot scale. lol... you're talking about the linux kernel, written in C. The vast majority of software over many decades \"bottoms out\" in C whether in VMs, operating systems, device drivers, etc. The scale of the success of C is unparalleled. reply pjc50 6 hours agorootparentThe scale of C adoption is certainly unparalleled over the past 40 or so years, but so are the safety issues in the cyberwarfare era. https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/pre... If, somehow, we'd got to an era where (a) operating systems were widely deployed in a different language, and (b) the Morris Worm of 1988 had happened due to buffer overflow issues, then C in its current form would never have been adopted. reply mjburgess 5 hours agorootparentC is just convenient assembly. In an era where performance mattered, and much software was written for hardware, and controlling hardware, it's hard to see an alternative. C's choices were for performance on hardware-limited systems. I don't really see what other ones made sense historically. reply pjc50 5 hours agorootparentC is, in some important cases, less convenient than assembly in ways which have to be worked round either fooling the compiler or adding intrinsics. A recent example: https://justine.lol/endian.html Is the huge macro more convenient than the \"bswap\" instruction? No, but it's portable. > I don't really see what other ones made sense historically. Pascal chose differently in a couple of places. In particular, carrying the length with strings. C refused to define semantics for arithmetic. This gave you programs which were \"portable\" so long as you didn't mind different behavior on different platforms. Good for adoption, bad for sanity. It was only relatively recently they defined subtraction to be twos-complement. 16-bit Windows even used C with the Pascal calling convention. http://www.c-jump.com/CIS77/ASM/Procedures/P77_0070_pascal_s... reply another2another 5 hours agorootparentprev>In an era where performance mattered, and much software was written for hardware, and controlling hardware, it's hard to see an alternative Actually, what made sense _was_ assembly when performance mattered above all. C was actually seen as a higher level language. However C's advantage was the fact that it was cross platform, so you could compile or quite easily port the same code to many different platforms with a C compiler (Solaris,Windows,BSD,Linux and latterly Mac OSX). That was its strength (pascal shared this too, but it didn't survive). You can see this in the legacy of software that's still in use today - lots of gnu utilities, shells, X windows, the zlib library, the gcc, openssl and discussed fairly recently POV Ray which has been going since the 80's. reply freeone3000 5 hours agorootparentprevBut it doesn’t have to. We can choose any other language that compiles to native, including memory-safe ones. reply dxroshan 6 hours agorootparentprevI agree with you. reply bigstrat2003 4 hours agoparentprev> But you can't do everything that C does without using unsafe blocks. Rust can offer a fresh perspective to these problems, but it's not a complete solution. It's true that you need to have unsafe code to do low level things. But it's a misconception that if you have to use unsafe then Rust isn't a good fit. The point of the safe/unsafe dichotomy in Rust is to clearly mark which bits of the code are unsafe, so that you can focus all your attention on auditing those small pieces and have confidence that everything else will work if you get those bits right. reply drdo 7 hours agoparentprevBut unsafe blocks are available! And you should use them when you have to, but only when you have to. Using an unsafe block with a very limited blast radius doesn't negate all the guarantees you get in all the rest of your code. reply sanxiyn 7 hours agorootparentNote that unsafe blocks don't have limited blast radius. Blast that can be caused by a single incorrect unsafe block is unlimited, at least in theory. (In practice there could be correlation of amount of incorrectness to effect, but same also could be said about C undefined behavior.) Unsafe blocks limit amount you need to get correct, but you need to get all of them correct. It is not a blast limiter. reply neysofu 7 hours agorootparentI believe this is technically true, but somewhat myopic when it comes to how maintainers approach unsafe blocks in Rust. UBs have unlimited blast radius by definition, and you'll need to write correct code in all your unsafe blocks to ensure your application is 100% memory-safe. There's no debate around that. From this perspective, there's no difference between a C application and a Rust one which contains a single, incorrect unsafe block. The appreciable difference between the two, however, is how much more debuggable and auditable an unsafe block is. There's usually not that many of them, and they're easily greppable. Those (hopefully) very few lines of code in your entire application benefit from a level of attention and scrutiny that teams can hardly afford for entire C codebases. EDIT: hardy -> hardly (typo) reply weinzierl 7 hours agorootparentprevYes, they don't contain the blast, but they limit the places where a bomb can be, and that is their worth. reply foldr 2 hours agorootparentGenerally speaking yes, but there could be a logic error somewhere in safe code that causes an unsafe block to do something it shouldn’t. For example, a safe function that is expected to return an integer less than n is called within an unsafe block to obtain an index, but the return value isn’t actually less than n. In that case the ‘bomb’ may be in the unsafe block, but the bug is in the safe code. reply nicce 1 hour agorootparent> yes, but there could be a logic error somewhere in safe code that causes an unsafe block to do something it shouldn’t. Sounds like bad design. You can typically limit the use for unsafe for so small area than you can verify the ranges of parameters which will cause memory problems. Check for invalid values and raise panic. Still ”memorysafe”, even if it panics. reply foldr 7 minutes agorootparentSure, it may be bad design. The point is that nothing in the Rust language itself guarantees that memory safety bugs will be localized to unsafe blocks. If your code has that property it’s because you wrote it in a disciplined way, not because Rust forced you to write it that way (though it may have given some moral support). Let me emphasize that I am not criticizing Rust here. I am just pointing out an incontrovertible fact about how unsafe blocks in Rust work: memory safety bugs are not guaranteed to be localized to unsafe blocks. Klonoar 2 hours agorootparentprevI cannot imagine writing a method to return a value less than n, and not verifying that constraint somewhere in the safe method. reply foldr 7 minutes agorootparentIt’s just a simple example to illustrate the point. Realistic bugs would probably involve more complex logic. drdo 5 hours agorootparentprevThat is of course correct. The main value is that you only have to make sure that a small amount of code surrounding the unsafe block is safe, and hopefully you provide a safe API for the rest of the code to use. reply CraigJPerry 7 hours agorootparentprevI’d word that different- it reduces the search space for a bug when something goes wrong but it doesn’t limit the blast radius - you can still spectacularly blow up safe rust code with an unsafe block (that no aliases rule is seriously tough to adhere to!) This is definitely a strong benefit though. reply pjc50 6 hours agoparentprev> But you can't do everything that C does without using unsafe blocks How much of this is actually 100% unambiguously necessary? Is there a good reason why anything in the filesystem code at all needs to be unsafe? I suspect it's a very small subset needed in a few places. reply nicce 1 hour agorootparentUsually avoidance of copying or moving data is the primary reason. In filesystems, this is quite highlighted. reply bilekas 7 hours agoparentprev> Concurrency problems? I have to admit, while I do enjoy rust in the sense that it makes sense and can really \"click\" sometimes. For anything asynchronous I find it really rough around the edges. It's not intuitive what's happening under the hood. reply the_duke 7 hours agorootparentAsync != concurrency. One of the major wins of Rust is encoding thread safety in the type system with the `Send` and `Sync` traits. reply bilekas 7 hours agorootparent> Async != concurrency. Right, but tasks are sharing the same thread which is fine, but when we need to expand on that with them actually working async, i.e non blocking, fire and quasi-forget, its tricky. That's all I'm saying. reply the_duke 5 hours agorootparentThe Rust async experience indeed has lots of pitfalls, very much agree there. reply dboreham 4 hours agorootparents/The Rust/All/ reply duped 4 hours agorootparentprevasync == concurrency, concurrency != parallelism. reply wongarsu 4 hours agorootparentprevRust async isn't all that pleasant to use. On the other hand for normal threaded concurrency Rust is one of the best languages around. The type system prevents a lot of concurrency bugs. \"Effortless concurrency\" is a tagline the language really has earned. reply asyx 7 hours agorootparentprevI really hate async rust. It's really great that rust forces you on a compiler level to use mutexes but async is a disease that is spreading through your whole project and introduces a lot of complexity that I don't feel in C#, Python or JS/TS. reply John23832 6 hours agorootparentEh, syntactically async rust is the exact same as C#. It's all task based concurrency. Now, lifetimes attached to function signatures is definitely a problem. reply colejohnson66 6 hours agorootparentNot really. C#'s Task/Task are based on background execution. Once something is awaited, control is returned to the caller. OTOH, Rust's Future is, by default, based on polling/stepping, a bit like IEnumerable in C#; If you never poll/await the Future, it never executes. Executor libraries like Tokio allow running futures in the background, but that's not built-in. reply brigadier132 2 hours agorootparentHow do you imagine async works otherwise? Also, in case you misunderstand how polling works in practice in rust, it's not polling in the traditional web development sense where it polls every 5 ms to check if a future is completed (although you can do this if you want to for some reason). There are typically \"wakers\" that are \"awoken\" by the os when data is ready and when they are \"awoken\" then they poll. And since they are only awoken by the OS when the information is ready it really never has to poll more than once unless there are multiple bundled futures. reply John23832 4 hours agorootparentprevI don't want to \"well actually\" the \"well actually\", but I think you missed the word syntactically. > C#'s Task/Task are based on background execution. Once something is awaited, control is returned to the caller. Async/await in any language happens in the background. What happens during a Task.Yield() (C#)? The task is yielded to the another awaiting task in the work queue. Same as Rust. > OTOH, Rust's Future is, by default, based on polling/stepping, The await syntax abstracts over Future/Stream polling. The real difference is that Rust introduced the Future type/concept of polling at all (which is a result of not having a standard async runtime). There is a concept of \"is this task available to proceed on\" in C# too, it's just not exposed to the user and handled by the CLR. reply simon04 2 hours agoprevtl;dr? reply hu3 5 hours agoprevSome of the comments below the lwn.net page are rather disrespectful. Imagine getting this comment about the open source project you contribute to: \"Science advances one funeral at a time\" reply aoieu 7 hours agoprevnext [38 more] [flagged] rcxdude 6 hours agoparentIt sounds more like it's mostly the standard kind of debate that happens over any significant API change in the kernel: you always get a variety of opinions on what the right way to do things. The additional complication is this by necessity is a parallel API and in a language which not everyone in the kernel knows, so there's some additional discussion about who's responsible for keeping thing in sync. reply atoav 7 hours agoparentprevBut I'd rather have 50 lines of nuclear fissile code that needs to be correct than say the whole software. The danger of Rust it that you twist yourself into a bretzel in order to avoid unsafe, while you should have in fact made a exceptionally well tested and designed unsafe block that is surrounded by code that the compiler checks for you. I am still convinced that the naming choice for unsafe has some effects that were not intended. Mentally whenever you see unsafe fill in a \"trust me\" or a \"manual override\" or whatever. Something that tells you that the programmer had a reason to override the borrow-checker. If they are cool they tell you why the thing they did is indeed safe and sound code. reply genrilz 5 hours agorootparentObviously, there are some cases where it makes sense to use an unsafe block. However, I think there might be fewer cases then people might think. As an example, both the popular generic self-referencing crates ouroboros and self_cell have had memory safety bugs in them in the past. (links at the end) Both of them were carefully reviewed by experienced rust developers before their first public release, and yet they still ended up with such bugs. Admittedly, part of the issue is that both crates are trying to be more generic, so they have to be correct over a larger range of circumstances. But still, these crates have one job, are both less than 1500 LOC, and they were carefully reviewed to ensure they did that one job before their public releases, and they still ended up having issues that were not caught. They might still have issues. Thus, while it might be fine to use unsafe to state that your array of zeros is a valid utf-8 string without a runtime check, it's probably a good idea to twist yourself into a pretzel if the invariants are not trivial to prove and the overhead to maintenance/runtime isn't too high. [0]: https://rustsec.org/advisories/RUSTSEC-2023-0042.html [1]: https://rustsec.org/advisories/RUSTSEC-2023-0070.html reply fla 6 hours agorootparentprevI like the idea of using: trusted { … } reply tracker1 4 hours agorootparenttrustMeBro { ... } reply Ygg2 5 hours agorootparentprevYeah, you just renamed `unsafe`. `unsafe` is the part where the compiler trusts you to uphold your own invariants, necessary to prevent Unsoundness. For example: - unsafe fn get_unchecked(index) - compiler believes you will ensure indexInteresting discussion amongst kernel developers and maybe a sign that Rust evangelism has gone too far for its own good? LVM: Hey there was this Linux summit on file systems and Rust was discussed HN commenter: has Rust evangelism gone too far!? I swear HN commenters are more over the top than tabloid headlines sometimes. reply tucnak 7 hours agoparentprevThey simply wish the actual kernel developers just surrendered & weren't in the way of new Rust code. Maybe if these guys wrote C for a living some 10 years or so, became maintainers in their own right, and THEN brought these ideas forward—they would have the chance. But you can't come to the other people's projects, and seriously expect them to just nod ahead to everything you have to say, surrender your concerns, and act like they owe you sommat. I'm glad this conversation is happening, though. reply paavohtl 7 hours agorootparentTo my knowledge most if not all of these people driving Rust adoption in Linux are seasoned Linux contributors and/or maintainers. They are not outsiders \"coming to other people's projects\". reply lelanthran 6 hours agorootparent> To my knowledge most if not all of these people driving Rust adoption in Linux are seasoned Linux contributors and/or maintainers. They are not outsiders \"coming to other people's projects\". Not in this case. It's the Rust evangelist who is the newcomer. FTA: > Almeida said that he is not trying to keep the C API static; his goal is to get the filesystem developers to explain the semantics of the API so that they can be encoded into Rust. The responses from the kernel team members to the proposal seem reasoned and mature to me: > In addition, when the C code changes, the Rust code needs to follow along, but who is going to do that work? > As the C code evolves, which will happen more quickly than with the Rust code, at least initially, there will be a need to keep the two APIs in sync. > The object lifecycles are being encoded into the Rust API, but there is no equivalent of that in C; if someone changes the lifecycle of the object on one side, the other will have bugs. > Encoding a single lifecycle understanding into the API means that its functions will not work for some filesystems. > Part of the problem, Ted Ts'o said, is that there is an effort to get \"everyone to switch over to the religion\" of Rust; that will not happen, he said, because there are 50+ different filesystems in Linux that will not be instantaneously converted. > Bottomley said that as more of those semantics get encoded into the bindings, they will become more fragile from a synchronization standpoint > But Ts'o pointedly said that not everyone will learn Rust; if he makes a change, he will fix all of the affected C code, but, \"because I don't know Rust, I am not going to fix the Rust bindings, sorry\". reply rascul 5 hours agorootparent> Not in this case. It's the Rust evangelist who is the newcomer. FTA: From a quick search, I've found that the person is somewhat active in kernel commits over the last four years (mostly but not all rust related from what I see) and is involved in some lkml stuff and some commits over a decade ago. Not sure if this makes him a newcomer or not, just wanted to provide a bit more context. reply rcxdude 6 hours agorootparentprevThe other person behind the proposal, is however a seasoned kernel developer, with >14 years of experience working in the FS subsystem of linux. reply tucnak 6 hours agorootparentThis is case-in-point; bcachefs maintainer is merely _entertaining_ the ideas proposed by the Rust evangelist, with the express intent of exploring alternatives... and suddenly it's enough for \"the movement\" to co-opt him into the \"proposal\" [to include Rust in the FS subsystem], it seeems. Are Rust supporters really as desperate so-as to co-opt ANY interest in the subject?? This toxicity is not helping anybody. reply dralley 4 hours agorootparentI think you're misunderstanding the situation. Kent Overstreet is the bcachefs maintainer and he's extremely pro-Rust. He has in fact talked about wanting to move bcachefs to Rust, and the userspace tools for bcachefs are already written in Rust. He's been participating in the mailing list discussions about using Rust / providing Rust APIs for the FS subsystem. I don't know who you're confusing him with, but you're confusing him with someone else. reply _flux 5 hours agorootparentprevI'm not sure what you are saying. Maybe the first proposal isn't the best one, so some exploration is warranted. Is Kent an unwilling pawn in the game of getting Rust implemented in the Linux kernel? Or is he not serious enough about getting it in that his support should not be considered? From what I've seen it reads to me Kent would be very happy to implement bcachefs in Rust in the Linux kernel. Bcachefs-tools does make use of it. reply pimeys 4 hours agorootparentKent has been really vocal about writing bcachefs in Rust in the IRC channel. They even started some work already, but decided to wait until the common abstractions are ready and merged. reply tucnak 5 hours agorootparentprev> Is Kent an unwilling pawn in the game of getting Rust implemented in the Linux kernel? Or is he not serious enough about getting it in that his support should not be considered? Neither. In fact, Kent is the only person in this story that has any agency with respect to Linux whatsoever; he's the reason we're having this conversation in the first place. And indeed, this is also the reason Rust people wish to co-opt him so hard. This pattern of behaviour is not new; in fact, pretty much all \"revolutionary\" movements never pass on the chance to mistake good attitude (towards them) for unequivocal alliance in \"the fight.\" I like Rust in theory as much as the next guy, and the code is great, for one I think the kernel developers' concerns are slightly misplaced, but in practice I wouldn't want Rust people anywhere near my machines. Linux is a pretty old, and subpar operating system; it takes a great deal of effort to execute on it & only a handful of distributions (Debian) manage to get it right in the first place! Why don't they go for some other OS? There's Plan 9, there's Fuchsia, it's not like Linux is the only path forward. But they don't want to be writing file systems; instead, the only goal that matters to them is getting to the heart of the most popular OS, so as to secure some longterm success. It all stems from insecurity, if you ask me. reply pjc50 5 hours agorootparent> But they don't want to be writing file systems; instead, the only goal that matters to them is getting to the heart of the most popular OS, so as to secure some longterm success. Wrong final word: security. In the war against remote exploits, harden the most widely used target first. Rust only exists because getting people to reliably and consistently write secure C++ proved impossible. (yes I know C++ is a much larger and more complicated language than C) reply tucnak 4 hours agorootparentIBM had full safety in terms of trusted program translation and memory tagging capability since the 90s, and yet the wider industry, Linux cheapskates couldn't care less about (1) ECC memory, (2) memory tagging. Now we have Arm MTE which is a step in the right direction, and CHERI capability-programming, which is really making progress in RISC-V community. If the Linux community was serious about, and not just cheap + ignorant, we wouldn't be having this conversation in the first place. Pandering programming language-equivalent of condom ads to this very community is not a sign of smarts, or great aspiration, but cluelessness and naivety. reply pjc50 4 hours agorootparentMan, an omni-hater, not just hating people trying to bring Rust to Linux but also hating on Linux. Must be a very rewarding sense of superiority. > ECC memory This is entirely Intel's fault. reply tucnak 6 hours agorootparentprevThis is a common misconception; like every movement of \"revolutionary\" modus, it likes to over-represent its supporter base & amplify the chosen ambassadors. So whenever some kind of concession, or endorsement is made—it's immediately read, and amplified as indicative of a greater support, and/or change. reply Firmwarrior 5 hours agorootparentprevnext [9 more] [flagged] dralley 4 hours agorootparentRust is a thing in the real world. Both Windows and Android are shipping, today, with meaningful components written in Rust. Amazon S3 and Lambda are built on top of Rust. Apple is hiring Rust developers and they post about it on this platform [https://news.ycombinator.com/item?id=40849188]. Dropbox and Discord backend services are written in Rust. Cloudflare uses Rust very extensively in their infrastructure, which means that a large fraction of global internet traffic passes through routers and servers written in Rust. The UEFI firmware implementation of the next Surface products by Microsoft is written in Rust. You are simply incorrect. Instead of arguing I will suggest that you do a slight modicum of research into who is using Rust and for what. While it won't be comparable in omnipresence with C and C++ for a long time, it is widely-enough used that there is a near-zero chance that you are not already using some tool or service that directly or indirectly uses Rust for some significant purpose. It is not a \"forum and hobby project language\". The list I just provided is also by no means complete - Shopify, Disney, Facebook, Firefox... and many others... also use Rust. Your claim of credibility via working on kernels falls completely flat in the face of Microsoft directly contradicting you: https://www.thurrott.com/windows/282471/microsoft-is-rewriti... \"According to Weston, Microsoft has already rewritten 36,000 lines of code in the Windows kernel in Rust, in addition to another 152,000 lines of code it wrote for a proof of concept DirectWrite Core library, and the performance is excellent with no regressions compared to the old C++ code. He also called out that “there is now a syscall, in the Windows kernel, written in Rust.” Whatever experience you have is out of date with the current reality. Not only is there interest in using Rust in these core areas, but it has already started happening. reply tracker1 4 hours agorootparentThanks for eloquently responding to the GP comment... My own thoughts have been with Microsoft, Apple, Mozilla and Amazon actively backing Rust, it is definitely not something that will just go away. Personally, I've only done surface level things (API middle tier dev) with a few different Rust frameworks (Axum, etc) and it's been relatively nice for the level of performance and low overhead compared to Node, C# and others. And what lower level code I've read has been particularly pleasant to come to understand. While doing something like a global cache in Rust feels awkward as all hell, many other patterns just feel really nice to use. I like the semantics of the language itself. I do hope that certain enterprise patterns typical in Java and the C# communities don't come into play in Rust though. reply doublepg23 4 hours agorootparentprev? I have a friend who was working on \"smart grid\" code in Rust way back in 2018. Rust code already ships in Android and Firefox too. reply guitarbill 5 hours agorootparentprev> Rust kids need to quit tricking other entry level aspiring OS devs into wasting their time learning this language. In that case, C/C++ developers need to quit tricking the rest of the world that C/C++ is a suitable language for anything security related and - for commercial products - take responsibility for the economic and social impact for the data breaches due to memory safety issues. (I really don't care if it's Rust, Java, C#, whatever.) reply abenga 5 hours agorootparentprevHmmm. Is this serious or facetious? Or could be either, depending on the response? There already is rust code in the Linux kernel (some drivers), afaik. reply tucnak 5 hours agorootparentLinus strategy on Rust is genius, albeit immoral. — Don't engage the movement head-on, make concessions where it doesn't matter. — Nobody wants to write drivers? No problem, have THEM write the drivers where the impact is minimal. I believe, if Rust people really understood how they're being used exactly, they wouldn't bother with it, and would go on to write more impactful code. And yet, Linus had managed to execute this strategy perfectly; the insult is subtle enough not to cause major injury. reply tracker1 4 hours agorootparentI seriously doubt it's meant as an insult so much as to minimize near term impact in case there are (and likely will be) mistakes to the larger ecosystem. Creating clearer separations for where Rust can make more sense initially is important. Not just for Rust, but C/C++ and other future languages all around. The file system is an area where Rust can make a lot of sense, similar for network drivers. Points of interaction with underlying hardware and external systems where well defined controls are all the more important and widely interacted with. I would be surprised, if within a decade a lot of the use of OpenSSL isn't displaced with rustls across a lot of applications even if not written in Rust directly. reply keybored 3 hours agorootparentprev> Linus strategy on Rust is genius, albeit immoral. — Don't engage the movement head-on, make concessions where it doesn't matter. — Nobody wants to write drivers? No problem, have THEM write the drivers where the impact is minimal. Using telepathy to explain someone’s unstated intention to undermine some programming language’s use in the kernel with this House of Cards plotline makes you look insane btw. reply aoieu 7 hours agorootparentprevPerhaps a better approach would have been for Linus to design his own backwards-compatible and safer fork of the C language, and have the kernel gradually rewritten in that. He's already written - with a considerable amount of collaboration of course - the world's most popular kernel and the world's most popular source control system. I expect he'd be able to do the same for a new and improved systems language if he put his mind to it. reply pjc50 6 hours agorootparent> backwards-compatible and safer Pick one. The safety is achieved by eliminating constructs which cannot be proven to be safe. Additionally, Rust-style safety involves adding more information explicitly to the source which otherwise has to be kept in the programmer's head (or externally like sel4): object lifetimes, lock rules (see original article), etc. At best you end up with \"first wrap all your original code in 'unsafe' and then gradually move the safety boundary\", but even that is very difficult. reply account42 4 hours agorootparentA C variant that e.g. defined OOB access to segfault (or panic in this case) would be strictly safer while being fully compatible with all valid C code. Not that I'm advocating for such a C variant but your snide retort is simply wrong: there is plenty of room for making C safer without reinventing the wheel. reply fwip 6 hours agorootparentprevNot advocating for this, but you could also imagine a C superset where all of the new features only apply in 'safe' blocks, which would be backwards-compatible, and likely safer-in-practice. reply renox 7 hours agorootparentprevNote that the Linux kernel is already written with C 'extensions' not in pure C. reply gritzko 8 hours agoprevFrom the minutes I conclude that Rust-in-the-kernel looks like an additional complexity tax. I mean, if you write an OS from scratch, you can use the full power of your language. Plastering it to the side of an already vast codebase creates additional issues, as we see here. reply mnau 7 hours agoparent> additional complexity tax Yes, but that should be offsetted by easier driver development. See the blog about Rust GPU driver for asahii linux, done in one month. EDIT: Google \"tales of the m1 gpu\" (author has a very negative opinions about hacker news, read if you like by clicking the link https://asahilinux.org/2022/11/tales-of-the-m1-gpu/) Is it universal? We'll see in coming years. reply josephcsible 1 minute agorootparentWarning: Don't click that link. Copy and paste the URL instead. That site serves only verbal abuse and harassment to people that it detects are HN users. reply KallDrexx 6 hours agorootparentprevMaybe I'm reading something wrong but the discussion this HN posting is about sounds very much about trying to make a Linux subsystem and API in Rust, so that Rust's type system can enforce conformance to safety mechanisms via its abstraction. That's fundamentally different and harder than a driver being written in rust that uses the Linux's subsystems C APIs. I can see a lot of drivers taking on the complexity tax of being written in Rust easily. The complexity tax on writing a whole subsystem in Rust seems like an exponentially harder problem. reply wongarsu 3 hours agorootparentYou could just write rust code that calls the C APIs, and that would probably avoid a lot of discussions like the one in the article. But making good wrappers would make development of downstream components even easier. As the opponents in the discussion said: there's about 50 filesystem drivers. If you make the interface better that's a boon for every file system (well, every file system that uses rust, but it doesn't take many to make the effort pay off). You pay the complexity tax once for the interface, and get complexity benefits in every component that uses the interface. We would have the same discussions about better C APIs, if only C was expressive enough to allow good abstractions. reply nicce 1 hour agorootparentprev> author has a very negative opinions about hacker news I am not sure if author (Asahi Lina) has, but the project lead Hector Martin definitely has. reply eru 7 hours agorootparentprevAlas, that link just gets you a rant about politics, if you click on it directly. Copy-and-pasting works. reply olivermuty 7 hours agorootparent\"Rant about politics\", haha. Or as other people like to call it: \"A real concern described in an apt manner\". I have observed these inflammatory sub-graphs of comments myself and have thought to myself that this must be a huge growing grounds for unmoderated and unwanted behaviour because it more or less becomes invisible once flagged enough. reply erdii 6 hours agorootparentIn this specific case complaining about \"politics\" gets the sour by-taste of enabling (or at least not condoning) harrasment to the point of single folks taking their own lifes over it. Why?! Even if you're not sure what to think about the queer movement; even if you have already made up your mind about the queer movement and oppose their ideas or some of them; I refuse to believe that any single person would not want to stop someone from bullying someone else into their own suicide! hastily jotted rant for the folks who'd like to complain about \"politics\" from creeping into every discussion everywhere: It's really sad to see so many folks disconnecting and immediately dismissing whole groups of other folks as soon as they start complaining about an issue they have because of \"politics\". :( I get that you don't want to get involved in shit flinging shows and that its tedious to figure out who's in the right and who's in the wrong. Especially because there are never clear answers. If you feel like this and then proceed to complain about 'politics' creeping everywhere, please beware of this: Pretending to be apolitical doesn't work most of the time, as politics is basically another word for \"acting (or deliberately not-acting) in some kind of public sphere\" which you all do, and when the \"policitics\" have arrived at a topic, then they'll stay there at least in that specific case you are witnessing! You just are part of a hyperconnected and confusing world with a lot of conflict, wether you like it or not. Pretending to be apolitical also serves the upholding of whatever status quo is currently in place because anything that has even a slight chance of changing anything is inherently a political topic. Please don't turn your heads on \"political\" topics or, at least, don't complain about it in that way as it mostly enables unjust behaviour to continue. It doesn't even matter if it's the person who brought up the \"political\" stuff who is acting unjust or the folks they're complaining about). In both cases it's probably better to either avoid commenting at all or to convey your critical thoughts to that \"political\" conversation. reply pjc50 6 hours agorootparent> I refuse to believe that any single person would not want to stop someone from bullying someone else into their own suicide! There are plenty of people who do want the freedom to say exactly what they choose, including a lengthy period of directed harassment, and shrug their shoulders if someone commits suicide over it. There's not much that can be done other than ban them from civilized spaces. reply pessimizer 5 hours agorootparent\"Bullying\" is a judgement. Intrinsic to the word is a judgement that what is being done is bad, and the person doing it would not describe it that way. And by that I don't mean that it's not bad to bully people (it's rather tautological), I mean that talk of bullying often begs the question, and is intentionally done in order to elide past the actual events that occurred. Lèse majesté laws against talking about the King, elected officials, or even cops or bureaucrats now get justified as anti-bullying. I missed any rant about politics in the blog however. But this thread has a smell of \"my politics aren't political because they are true, and your politics are political because they are lies.\" reply jcranmer 3 hours agorootparentWhat the referer-replacement page was talking is Kiwi Farms, which is doing the kind of stuff that even the US First Amendment's very expansive protections fails to protect. (The criminal liability here is \"intentional inflection of emotional distress\", although note that most lawsuits that allege that are groundless lawsuits that largely fail to make it pass the motion to dismiss for failure to state a claim stage as \"they made me feel bad\" isn't sufficient to allege an IIED). reply nyssos 1 hour agorootparent> The criminal liability here is \"intentional inflection of emotional distress\", Intentional infliction of emotional distress is a tort, not criminal. reply keybored 2 hours agorootparentprevI mean this is correct. Bullies, or at least the adult ones, don’t call what they do bullying (except the absolute geniuses that think “actually bullying has a social corrective behavior, I’m just helping actually”). We’re all just busting each other’s balls, right? Look at George, he’s laughing! He’s totally in on the joke and not at all just showing submissive deference in order to not lose face. reply im3w1l 2 hours agorootparentprevTo me, you have to distinguish between harassment directed at a person, and on the other hand discussion about a person. It is not possible to use hacker news to send messages to someones email. It is not even possible to send a dm to another hacker news user. You could potentially imagine hacker news being used to organize harassment of someone, but I have never seen either accusations or evidence of such a thing. So then we have established, that since harassment directed at them is impossible, the issue they have is that people on hacker news write bad things about them. Next, those things seem to often be flagged or downvoted, reducing exposure. But that is apparently not enough, because they can be found on google. So here we arrive at the core issue. There is content on Google about this person, that they would rather not be on Google. This is the complaint. So this person is basically saying that if there is unfavorable coverage of them findable on google, that is harassment, and it needs to go. If it isn't purged it's bullying that could lead to suicide. This is a very ambitious \"landgrab\" if you will, and it starts to seriously infringe on other peoples rights. It's similar in that manner to other things like \"stop terrorism\" or \"think of the children\". Yes clearly harassment is bad, and terrorism is bad, and pedophiles are no good. But we can't completely give up on our freedoms because of that. reply logicprog 5 hours agorootparentprevI'm glad at least some other people on this horrible site feel this way. reply aoieu 7 hours agorootparentprevnext [7 more] [flagged] thrance 6 hours agorootparentThere are obvious differences between criticism and harassment. Let's not act like we don't know why the asahi linux team is getting death threats, and maybe try to improve the situation. reply aoieu 6 hours agorootparentDeath threats were being posted on HN? I hope they were swiftly removed if so. reply thrance 6 hours agorootparentYou should probably read the asahi linux post. reply aoieu 3 hours agorootparentWhich one? I just browsed through a few and didn't see any death threats. reply logicprog 6 hours agorootparentprev> I've not read any of these comments but it sounds like the author of that message is somewhat peeved at the fact he can't control the conversation of others. Are you fucking serious right now? Did you actually read what they said? Want is wrong with you? They're not \"peeved\" about \"not being able to control the conversations of others\", they're pointing out that because of Hacker News's lax and irresponsible moderation and disturbing overlap with much worse communities, it is a breeding ground for endless hate and harassment that can, and has, driven people to suicide on other occasions. That's way more serious and understandable, and playing that down as just someone being \"anti free speech\" as being part of the problem. reply aoieu 3 hours agorootparentThat sounds highly unlikely given the strict moderation on this site. So unless I see any evidence of it, I'll continue to assume otherwise. From what I can gather after looking at previous posts on Asahi Linux, as another commenter suggested, the author of that message is furious that some HN users were talking about a cartoon character alter ego that he uses to make video streams. Seems to me like he's overreacting a bit. reply lelanthran 4 hours agorootparentprev> Or as other people like to call it: \"A real concern described in an apt manner\". Oh please. Every activist for every marginal issue says the same thing. Doesn't make it true. reply aniviacat 5 hours agorootparentprevI can't find a rant in the link. Was the link changed or did I overlook something? reply jeroenhd 5 hours agorootparentThe author of the website details their issue with the way HN does moderation (which I can't say I disagree with, especially after HN intentionally disabled referrer headers for websites that take issue with HN). This only shows up if HN is in the referer URL. I wouldn't call it a rant, but rather a polite request for HN policy to change. reply yjftsjthsd-h 3 hours agorootparent> I wouldn't call it a rant, but rather a polite request for HN policy to change. Which is made by blocking people with no ability to do anything about it. reply mcronce 2 hours agorootparentWhat? All you need to do is resubmit the request without a Referer header. For me, using Firefox, this meant clicking in the address bar, changing nothing, and hitting enter. That's hardly \"no ability to do anything about it\". reply yjftsjthsd-h 1 hour agorootparentI mean people with no ability to alter HN moderation policy. Consider it like this: 1. I think, \"oh, that looks relevant, let me open that link\". 2. I get a screenful of objections to HN moderation. 3. I shrug and close the tab. Since I'm just a normal user who can't change HN moderation, the outcome is that HN doesn't change but I walk away with a worse opinion of the Asahi Linux folks. reply Phelinofist 47 minutes agorootparentprevAlso didn't show for me with disabled JS reply amiga386 5 hours agorootparentprevIf you click the link - any link to asahilinux.org from HN - it should start \"Hi! It looks like you might have come from Hacker News.\", followed Hector Martin ranting that he isn't in charge of the moderation policy of HN. The response given in Arkell v. Pressdram is appropriate. reply Already__Taken 7 hours agoparentprevreads a lot like letting perfect be the enemy of good. reply thesnide 7 hours agoparentprevWhile I agree there are benefits to rust, I tend to think all reason cannot fight hype. The tax will be seen as a necessity to embrace future and progress. I'm wondering why do not restrict ourselves to a safe subset instead of jumping into a huge bandwagon of unknown bugs and tradeoffs reply pjc50 6 hours agorootparentThere is no \"safe subset\" of C. MISRA is fairly close, but all sorts of things that you might need, like integer arithmetic, have potential UB in C. (The best current effort is https://sel4.systems/ , which is written in C but has a large proof of safety attached. The language design question is basically: should the proof be part of the language?) reply regularfry 6 hours agorootparentGiven that undefined behaviour just means \"undefined by the standard\", do you get usefully closer to being able to identify a safe subset with the (MISRA/alternative, specific compiler, specific architecture) triple? reply pjc50 6 hours agorootparentNo, undefined behavior does not mean \"not defined by the standard\", it means those places where the standard says \"undefined behavior\". And then the long and complicated war over \"the compiler may assume that UB does not happen and then optimize on that basis\". You might be able to tighten it up in some specific cases, and those battles are being fought elsewhere, but there's stuff like lock lifetimes which you cannot do without substantial extra annotations inside or outside the language. reply regularfry 5 hours agorootparentSorry, yes - poor wording on my part. reply PhilipRoman 5 hours agorootparentprevI found frama-c to be pretty good, including all the integer quirks reply Yoric 7 hours agorootparentprevSafe subset of what? reply germandiago 4 hours agorootparentprevI like Linus view on evolution. Evolution will tell what the most sensible choices are over time. It is like \"the market\" in some way. Let everyone make their bets, wait, see, analyze, research. That's it. reply vollbrecht 7 hours agoparentprevOne can argue that any additional code is introducing complexity, not only writing Rust. Does that mean we should just stop innovating and go into an indefinite state of maintenance, since we are already so vast? A tax in one place may not be a net negative, if it's used like in the real world to offset other problems. And just saying it will not offset any problems because of a single discussion, that does not have a definite conclusion, comes of as a short argument. reply another2another 7 hours agorootparent>Does that mean we should just stop innovating and go into an indefinite state of maintenance If you mean that not using Rust (or maybe some other languages e.g. Zig or Ada?) means that there can be no innovation in the Linux kernel, I would have to disagree since there's been plenty of progress in plain old c (see for instance io_uring), not to mention the fact that the c language itself could change to make developer ergonomics better - since that seems to be the nub of the problem. It also raises the question of what happens in the future when Rust is no longer the language du jour - how do we know it's going to last the course? And now there's 2 different codebases, potentially maintained by 2 different diminishing sets of active maintainers. reply vollbrecht 6 hours agorootparent> If you mean that not using Rust (or maybe some other languages e.g. Zig or Ada?) means that there can be no innovation in the Linux kernel, I would have to disagree since there's been plenty of progress in plain old c. No i didn't mean that. If i understand OP correctly here, he argued that it is a tax to use rust, a tax is always bad, and thous should be avoided. We obviously can't now the future. We also can't now how future maintainers look like, and if there will be a bigger abundance of people understanding kernel level C or kernel level Rust or both. I also don't think that any one developer can claim to fully get every part of the Linux Kernel. So if one person want's to work on a particular subsection they need to make themself familiar with it, independent of the language used. And then we are back at the argument, is the additional tax bad, or what does it bring to the table. reply pjmlp 5 hours agoprev [–] The disconnect section of the article is a good example of exactly on how not to do the things, and how things can turn out sour if the existing community isn't taken for the ride. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "At the 2024 Linux Storage, Filesystem, Memory Management, and BPF Summit, Wedson Almeida Filho and Kent Overstreet discussed using Rust for Linux filesystems, highlighting its potential benefits.",
      "Almeida's RFC patch set from December 2023 introduced Rust abstractions for filesystems, aiming to leverage Rust's type system to catch errors at compile time and automate resource cleanup, enhancing productivity and reducing memory-related vulnerabilities.",
      "Concerns were raised about the disconnect between C and Rust APIs, with suggestions to align function names for familiarity, and the challenges of maintaining synchronization between evolving C code and Rust bindings were acknowledged."
    ],
    "commentSummary": [
      "The discussion on integrating Rust into the Linux kernel highlights both challenges and benefits, particularly in filesystem development.",
      "Tools like bindgen and cbindgen are essential for bridging the gap between Rust and C, though some developers find interoperability complex.",
      "The debate emphasizes Rust's potential to enhance safety and correctness but also notes concerns about maintaining parallel APIs and the learning curve for developers new to Rust."
    ],
    "points": 220,
    "commentCount": 146,
    "retryCount": 0,
    "time": 1721036357
  },
  {
    "id": 40962965,
    "title": "Just Be Rich (2021)",
    "originLink": "https://keenen.xyz/just-be-rich/",
    "originBody": "Just Be Rich 🤷♂ No one wants to be the bad guy. When narratives begin to shift and the once good guys are labelled as bad it's not surprising they fight back. They'll point to criticisms as exaggerations. Their faults as misunderstandings. Today's freshly ordained bad guys are the investors and CEOs of Silicon Valley. Once championed as the flagbearers of innovation and democratization, now they're viewed as new versions of the monopolies of old and they're fighting back. The title of Paul Graham's essay, How People Get Rich Now, didn't prepare me for the real goal of his words. It's less a tutorial or analysis and more a thinly veiled attempt to ease concerns about wealth inequality. People who don't look any deeper than the Gini coefficient look back on the world of 1982 as the good old days, because those who got rich then didn't get as rich. But if you dig into how they got rich, the old days don't look so good. In 1982, 84% of the richest 100 people got rich by inheritance, extracting natural resources, or doing real estate deals. Is that really better than a world in which the richest people get rich by starting tech companies? What he fails to mention is that concerns about wealth inequality aren't concerned with how wealth was generated but rather the growing wealth gap that has accelerated in recent decades. Tech has made startups both cheaper and easier but only for a small percentage of people. And when a select group of people have an advantage that others don't it's compounded over time. Once you understand how these mechanisms work, and that startups were suppressed for most of the 20th century, you don't have to resort to some vague right turn the country took under Reagan to explain why America's Gini coefficient is increasing. Of course the Gini coefficient is increasing. With more people starting more valuable companies, how could it not be? Paul paints a rosy picture but doesn't mention that incomes for lower and middle-class families have fallen since the 80s. This golden age of entrepreneurship hasn't benefitted the vast majority of people and the increase in the Gini coefficient isn't simply that more companies are being started. The rich are getting richer and the poor are getting poorer. You would think, after having been on the side of labor in its fight with capital for almost two centuries, that the far left would be happy that labor has finally prevailed. But none of them seem to be. You can almost hear them saying \"No, no, not that way.\" And there we have it. The slight injection of his true ideology relegated to the notes section and vague enough that some might ignore. But keep in mind this is the same guy who argued against a wealth tax. His seemingly impartial and logical writing attempts to hide his true intentions. Is this really about how people get rich or why we should all be happy that people like PG are getting richer while tons of people and struggling to meet their basic needs. Wealth inequality is just a radical left fairy tale to villainize the hard-working 1%. We could all be rich too, it's so much easier now. Just pull yourself up by your bootstraps. There's no question that it's easier now than ever to start a new business and reach your market. The internet has had a democratizing effect in this regard. But it's also obvious to anyone outside the SV bubble that it's still only accessible to a small minority of people. Most people don't have the safety net or mental bandwidth to even consider entrepreneurship. It is not a panacea for the masses. But to use that fact to push the false claim that wealth inequality is solely due to more startups and not a real problem says a lot. This essay is less about how people get rich and more about why it's okay that people like PG are getting rich. They're better than the richest people of 1960. And we can join them. We just need to stop complaining and just be rich instead. Keenen Charles Apr 12, 2021 Thoughts ← Previous Next →",
    "commentLink": "https://news.ycombinator.com/item?id=40962965",
    "commentBody": "Just Be Rich (2021) (keenen.xyz)214 points by scarmoo 22 hours agohidepastfavorite278 comments TrackerFF 20 hours agoI live in a country with wealth tax (Norway), and I'm a bit conflicted on it. On one side, it is effectively the only real tax a many in the \"ownership class\" are paying - relative to their wealth. On the other side, it is a really problematic tax for entrepreneurs. It is downright horrible for startups and scaleups - critical funds that should be used to grow your company, has to be given out in dividends to founders, so that they can afford to pay the wealth tax. And it is unfair - as it is a tax that foreign owners don't have to pay, but domestic owners have to pay. One natural consequence of this has been that the wealthy people are simply...leaving the country. (our total / combined max wealth tax is 1.1%) reply bko 18 hours agoparentA lot of people think wealth is just a large Scrooge-McDuck pile of money sitting in someone's vault and a wealth tax means just sharing some of that money with others. But most wealth is owning and running extremely valuable companies. So taxing someone like Elon 2% or whatever of his 'wealth' per year, would mean some random Blackrock bozo passive investors running the companies after a few years. You're basically preventing him from owning a valuable company. Sure he's still rich, but he can't run Tesla, or any company for that matter. For someone that actually did something to create a company (replace Elon w/ Bezos, Branson, Jobs, etc), would we be better off with them running the company or a few additional 100s of millions in the coffers of a government that spends $5 trillion a year (less than one hundredths of 1 percent)? reply virtualwhys 17 hours agorootparentDefine wealth. I wouldn't confine wealth to just the billionaire class; I'd say wealth is anyone with 5 million+ USD in liquid assets (specifically, that sum of money in stocks, bonds and cash). The poorest of the poor in the United States (that have to pay tax), make around $12K per year, and pay more in tax, percentage-wise, for their work than an individual doing absolutely nothing but passively investing in the market does (10% long-term capital gains). There's zero political will to change this because, surprise, every senator, and most members of congress, would be shooting themselves in the foot if legislation to increase captial gains tax were passed. There's too much focus on billionaries -- the United States population as a whole consists of many, many individuals with absurd amounts of wealth that pay next to nothing for doing nothing. At any rate, it is what it is, the American corporate wealth machine benefits, largely, the already wealthy. reply chrismcb 13 hours agorootparentWealth has a definition. It is what you own (minus what you owe) now whether you are wealthy or not is another question. Someone making 12k a year is not going to pay much on taxes. Some sales tax, maybe some property tax. That is about it. Meanwhile a wealthy person is going to pay gobs in taxes. Sure they may pay a lower percentage of their total income (and even lower of their total wealth) but they will be paying plenty in taxes reply pants2 13 hours agorootparentprevBasically, capital gains tax should be much higher than income tax. That seems obvious to me. Someone working for a paycheck should pay a smaller tax rate than someone who randomly bought NVDA stock a few years ago and watched the number go up. reply throwaway7ahgb 6 hours agorootparentMaybe, but then let me capture my long and short term gains together to offset gains. It's crazy that if I have a $5k loss in year one, I can't completely offset it in year 2 if I have a 5k gain. Now I can only use 3k going forward forever. reply dmoy 4 hours agorootparent> Now I can only use 3k going forward forever. Wait did they change the carry forward rules? At least the way it used to work was: * Year 1 - 50k loss, 3k offset of ordinary income 47k carry forward * Year 2 - 60k gain, 47k zeroed out from last year's carry forward, 13k is taxed Is that not still the case? reply nickpp 10 hours agorootparentprev> capital gains tax should be much higher than income tax Then you would in effect discourage investing. Investing, which creates jobs and promotes innovation. Is as simple as that. reply immibis 9 hours agorootparent[citation needed] Consider the opposite extreme, where the government subsidizes investments to 100x what they are actually worth. Would this create more jobs and innovation, or merely \"bullshit investments\"? If a subsidy wouldn't create jobs and innovation, a tax wouldn't take them away. There is also the likely possibility that the same investments would be made, but valued lower. reply nec4b 8 hours agorootparent>>[citation needed] Any economics text book. reply immibis 8 minutes agorootparentGo on. Which economics text book would tell me that if the government incentivized investments to 100x their actual value, the resulting investments wouldn't be bullshit? danaris 5 hours agorootparentprevThen let's do something to encourage actually investing—putting money into a company, not into the stock market. If I buy shares of, say, APPL today, Apple Inc. doesn't actually get that money. The person who owned those shares before does. Even if I bought up $2B worth of shares, Apple would see none of that money. It is not actually invested in the company in any way. If you want to encourage investing, then you need to find a way to advantage shares being sold directly by the company. (And, frankly, strongly discourage stock buybacks, because that's literally the opposite of investing, and generally only benefits the wealthiest shareholders.) reply xur17 15 hours agorootparentprev> 10% long-term capital gains That is not the long-term capital gains rate in the US. reply virtualwhys 14 hours agorootparentYou're right, it's actually 0% with any amount invested, $100,000 or $1,000,000,000,000,000, doesn't matter, provided you don't sell more than roughly $40k, or, god forbid, generate additional income through, say, work. That flat tax rate certainly scales nicely the wealthier you happen to be, paying a max of 20% if you hold for more than a year and decide to cash in on this now several year massive run up in the markets. reply throwaway7ahgb 6 hours agorootparentIt was most likely already taxed. If I invest my paycheck in the market, that money was already taxed at 30-40%. Now I make a dollar in the stock market , and it's taxed again at 20%. This get ridiculous fast. reply nojvek 17 hours agorootparentprevWe get rich in the sense of consumption when money moves around. The question in any economy is what balance between private and public is best at allocating capital. With high taxes, the govt is trusted with allocating capital. With low taxes, corporations are trusted with allocating capital. What usually happens is that Govt is a natural monopoly so they end up being ineffective at allocating capital to drive higher efficiencies. And if corporations have a mono/duo poly hold on market, they end up being effective. The best $ spent by govt is: - building common infrastructure to make goods and services move around faster, cheaper and safer. Then let corporations compete on what goods and services to move around based on demand and supply. - enforce transparent pricing and certain safety bars are enforced. - enforce competition in market with anti-trust. - Create new players by injecting investment in new industries. China is really good at creating new industries (Solar, steel, infrastructure, cars, electronics) Essentially govt collects tax to build a safe efficient competitive market. Ensure Corporations compete in that market. reply slwvx 19 hours agoparentprevIt looks like the Norwegian wealth tax is for wealth above $170M, which seems pretty aggressive to me. Gabriel Zucman and others made a website [2] in 2020 aimed at the US that included a wealth tax; the lowest bracket it had was for wealth above $1M. I like wealth taxes, and prefer Zucman's model over that of Norway. Progressive income taxes and wealth taxes seem to me like great mechanisms to build a middle class and to increase social stability. I.e. stabilize the wealth and income distributions (in the statistical sense) so that they do not become bimodal. [1] https://www.lifeinnorway.net/wealth-tax/ [2] https://triumphofinjustice.org/ reply cpursley 19 hours agorootparentThe problem is, ultra-rich people don't pay income taxes because they don't have incomes. And they type of income earners who earn high are often those who spend years studying for no/low pay like doctors, lawyers (yeah, yeah), local small biz owners, etc. The tax experiment I'd like to see is a very progressive sales/VAT tax that exempts certain used goods, essentials (groceries, etc) and hits luxuries, sin tax (that burden public healthcare) hard. Augmented with certain tariffs, natural resources, etc. This gives people the opportunity to \"choose\" their own tax rate. reply inhumantsar 18 hours agorootparentthe tax experiment I'd like to see applied at scale is a Land Value Tax. for the unfamiliar, the gist is taxing the value of the land, not the improvements on it. the idea being to reduce the amount of land speculation, expands the number of homes built, and lower the cost of doing business by incentivizing land owners to use the land to its fullest productive potential. eg: replacing a single-family homes with multi-family homes, surface parking for multi-level garages, or empty lots for literally anything. add to that tax advantages for things like rewilding, below-market rentals, or other public-good use cases, and anti-nimby reforms and it could make a number of separate government programs easier to implement, if not redundant. reply cpursley 2 hours agorootparentThis is an interesting idea. How do you handle the situation with retired/fixed income people (say, who are retirement age and paid off their mortgage and trying to keep expenses low)? reply ryandrake 19 hours agorootparentprev> The tax experiment I'd like to see is a very progressive sales/VAT tax that exempts certain used goods, essentials (groceries, etc) and hits luxuries, sin tax (that burden public healthcare) hard. I'd like to see this, too, but all you'd get is companies constantly bellyaching about why is my essential product considered a luxury?? And lobbying to get them on the low tax list rather than the high tax list. Every company is going to have their little excuse about why their product isn't really a luxury. reply drpixie 18 hours agorootparentprevWhy not something a little simpler, like a universal sales/VAT/transaction tax offset by a UBI. This simpler model provides fewer opportunities to build or exploit loopholes. The universal tax would be at a relatively low rate, because it would apply to all transactions. And the UBI compensates everyone for the tax on essentials. reply WalterBright 20 hours agoparentprevSweden, France, California, Washington state and NYC have discovered once again that taxing the heck out of rich people causes them to leave. And when they leave, they take their spend/invest/hire money with them. reply jmward01 19 hours agorootparentA statement like this probably needs a source or two to first show it is true (wealth taxes actually are causing rich people to flee) second, show that they actually do take their spend/invest/hire money with them and finally that it is harmful that that happens. Leaving creates a vacuum and if the rich really are leaving then it would be a big vacuum that clearly can't be filled with someone else that is rich since they are all leaving. Without someone big there to fill the vacuum it that implies it is being filled with a lot of smaller companies. This actually sounds like a good thing to me to tell you the truth. Can we emphasize this effect? reply WalterBright 19 hours agorootparentJeff Bezos famously left the state just before the Washington's new cap gains tax targeted at him was set to go into effect. I know other people with 8 figure fortunes who left, too. I didn't know it was controversial that spending a lot of money in a local economy makes it more prosperous, and when a big spender leaves, the local economy goes down. The Rust Belt is an example of the latter. reply shakow 19 hours agorootparent> spending a lot of money in a local economy makes it more prosperous Do these people really spend a lot of money in the local economy? Realistically, even assuming they pay a couple dozen people $100k/year for service and indulge daily in 3 $500 meals, what will be the actual influence on the local economy? > The Rust Belt is an example of the latter. The Rust Belt is an example of what happens when the state allows companies to leave, build their gadgets for a tenth of the price across the Pacific, then sell them back home without serious tariffs; not an example of what happens when Mr. Studebaker or Mrs. Bethlehem Steel leave for Florida. reply WalterBright 19 hours agorootparentZillionaire Paul Allen was famous for spending billions in the local economy. He transformed Seattle. He's passed away, and his projects are being sold off and closed down. > the state allows companies to leave If you have to build a wall around your state to keep people from fleeing, you have failed. reply Avicebron 18 hours agorootparentNo, if you are moving a high levels of capital and can exploit populations that haven't moved as fast as you (MexicoIf you have to build a wall around your state to keep people from fleeing It's not really about building a wall around your country to keep people from fleeing, but to prevent gadgets sold by people leveraging arbitrage in an unfair way to get in – or, as we call them for now millenia, tariffs. reply WalterBright 18 hours agorootparentArbitrage is a major reason why free market companies prosper. It's taking advantage of the Law of Comparative Advantage. For example, consider Bob and Fred. Bob is good at hunting, but stinks at growing blueberries. Fred grows luscious blueberries, but he's likely to blow his own foot off when hunting. Bob brings home twice the meat he needs, and Fred grows twice the blueberries he needs, and they trade. They are both better off than if Bob and Fred each only fended for themselves. BTW, back in the 80s, my (British) partner at Zortech noticed that boxed software in Britain sold for twice as much as the same box in the US. So he would order large quantities of boxes from the US, mark them up, and sell them locally at a nice profit. Eventually, other businessmen caught on and this no longer worked. That's arbitrage in action. It benefits consumers. reply shakow 12 hours agorootparent> It benefits consumers. On the short term and reasonable spatial scale; sure. What's currently happening in the West is that common people are discovering that on the long term and at world scale, it does not really benefits them for a myriad of reasons, among which unevenly applied norms, difference in regimes, different social models, etc. reply WalterBright 3 hours agorootparentThe world has never been more prosperous on a global scale. reply arrosenberg 19 hours agorootparentprevThis isn't a feudal country and a single magnate leaving will barely affect the economy. What affected the Rust Belt, and is causing problems nationally, is that ultrawealthy people like Jeff Bezos are manipulating the government to change the laws in ways that allow him more concentration of wealth and power at the expense of the middle class. The lack of a middle class is what causes an economy to go down, and that is why the Rust Belt is suffering economically. People like Bezos, the Waltons, the Kochs are causing harm to the economy, they're not propping it up in any sense. reply WalterBright 18 hours agorootparentThe Rust Belt long predated Amazon. How is Bezos extracting money from the middle class? reply arrosenberg 18 hours agorootparentI refuse to believe the Rust Belt existed before 1995. Extraordinary claims require extraordinary proof. reply WalterBright 18 hours agorootparent> The Rust Belt experienced industrial decline starting in the 1950s. https://en.wikipedia.org/wiki/Rust_Belt reply arrosenberg 16 hours agorootparentAnd? Amazon is still an avatar for the neoliberalism that killed the Rust Belt. They didn't start the fire, they just pour gas on it every day. reply danaris 16 hours agorootparentThis is called \"moving the goalposts\", and however good your underlying point about Amazon's perfidy might be, it does nothing but make you look like you're wrong and you know it. You were wrong about the Rust Belt, and it's OK to admit that and move on. reply arrosenberg 15 hours agorootparentSeems more likely you don’t process sarcasm well. reply danaris 5 hours agorootparent\"Satire requires a clarity of purpose and target, lest it be mistaken for, and contribute to, that which it intends to criticize.\" To put it another way: From Poe's Law[0], we know that no matter how outlandish a position you take sarcastically in a forum like this, there's nearly guaranteed to be people who will be taking such a position in full sincerity. If you don't clearly indicate in some way that you're being sarcastic, you're going to be mistaken for the real asshole, because your tone of voice does not come across in plain text. Next time, just put a /s and save everyone the trouble, hmm? [0] https://en.wikipedia.org/wiki/Poe%27s_law reply arrosenberg 2 hours agorootparentNo, I don't think I will. If you missed it, that's a great sieve for knowing what kind of person I am responding to. If you can't pick up on a joke, it's not going to be a fun conversation anyways. > If you don't clearly indicate in some way that you're being sarcastic, you're going to be mistaken for the real asshole, because your tone of voice does not come across in plain text. That's the sign of goooood sarcasm. Most people don't take internet forums as seriously as you seem to. reply danaris 16 hours agorootparentprev....You might want to look it up before making such extraordinary claims of your own. It's called the \"Rust Belt\" because it's where we had booming local manufacturing (the iron) in the postwar years that gradually declined over the course of the Cold War (turning to rust). Hell, by 1995, some parts of the Rust Belt were already starting to recover, though others remain hollowed out to this day. reply skybrian 18 hours agorootparentprevAnd yet, Microsoft and Amazon are still there. Isn’t that what matters? I see people moving to other states as a form of load balancing. reply WalterBright 18 hours agorootparentPeople living and spending their money in the same place is irrelevant to the local economy? The taxes they pay into the local economy have produced obvious civic booms in places like Redmond and Bellevue. BTW, the Ferrari dealer moved to be close by the Microsoft campus. reply skybrian 17 hours agorootparentI think it’s relevant, but also, there are a lot of people coming and going. If housing prices are high and traffic is bad, pointing at some rich people leaving seems sort like saying “nobody goes there anymore, it’s too crowded.” reply WalterBright 13 hours agorootparentRich people leaving isn't going to reduce the traffic significantly, but it will reduce economy significantly. reply infinitezest 19 hours agorootparentprevThere is also the upside of them taking their lobbying dollars with them. reply WalterBright 19 hours agorootparentThe USSR did not have lobbyists nor wealthy people. Didn't work out too well for the commoners. reply carlob 13 hours agorootparentprevThis is patently false for the only case I have data on. The wealth tax in France had quadrupled its collections in a time where the French GDP had doubled, and all this was without an exit tax. Unfortunately Macron got rid of it a few years ago and replaced it with something that only applies to real estate. reply throwaway7ahgb 6 hours agorootparentIt's kind of amazing when people talk about a tax failing as a bad thing. This leads me to believe that most people want taxes on wealth for punishment and no other reason. reply carlob 6 hours agorootparentI want wealth taxes for redistribution mostly. So yeah. reply omnimus 11 hours agorootparentprevThe thing is lots of the ultra rich moved to Luxemburg so this happened and this is going to keep happening. Its same why every tech company has european HQ in Ireland. If it were EU wide it might work somehow but with so many ultra rich high in politics there is no way this could happen. There is always going to be tax haven countries taking advantage of this situations. New French gov will want to reintroduce these taxes but i think they have to find different ways. reply carlob 11 hours agorootparentI don't think you even need the ultra rich high in politics, there is a more mundane explanation: countries are competing against one another to attract the HQs. It's a race to the bottom and it's called fiscal dumping. reply eigart 19 hours agorootparentprevDamned if you do, damned if you don’t. To me it looks like a race to the bottom. We have a very nice society, and most people are happy to pay taxes. We also don’t tax rich people more in relative terms, it’s just that it creates a threshold level of wealth where you can choose to move. reply RhysU 17 hours agorootparent> most people are happy to pay taxes Source? I hate paying taxes. reply eigart 13 hours agorootparentNo source, it’s just my impression that most Norwegians feel they get a fair deal. reply immibis 9 hours agorootparentprevIf you live in America, Republicans made it as difficult as possible with the intention of you hating it. If you live in any other country, you simply get less money than you would have otherwise. reply throwaway7ahgb 6 hours agorootparentI lived in London and left after paying 50%+ income on taxes. I also had to get private healthcare because I couldn't wait weeks to see a doctor. So no, it's not just US Republicans, please stop. reply danaris 4 hours agorootparentThe Tories are very similar in their outlook and practices in many ways. reply danpad 8 hours agorootparentprevthat is simply not true. Try to solve the scenario of earning 10 USD a month via YouTube monetization with your tax domicile being in the Czech Republic while being full-time employed. \"Simply get less money\" is a vast overstatement. reply immibis 9 hours agorootparentprevAnd what's wrong with that? Regular people, not having to compete with rich people, might actually be able to afford stuff. reply piva00 9 hours agorootparentprevSweden has a large ratio of billionaires relative to the rest of the population, and they aren't leaving. It's 14th in the world by number of billionaires, 7th per capita [0]. Rich people here pay a flat 30% tax on capital gains so they usually pay less taxes relative to their income than a salaried person earning >45-50k SEK. There's no wealth tax in Sweden. I don't think you know much about Sweden's tax system (or even in general) given this comment. [0] https://en.wikipedia.org/wiki/List_of_countries_by_number_of... reply feedforward 19 hours agorootparentprevThe US had a tax rate of over 90% for the wealthy in the 1950s (although with loopholes they could push it lower, like now). Didn't seem to have much of an effect on the wealthy, and the US working class did very well. reply WalterBright 19 hours agorootparentVery few paid those rates, because there were a large number of tax shelters available. For example, a lot of your personal expenses could be covered by your business. Reagan, as part of the deal to lower the top rates, traded away those tax shelters. The thing about tax shelter investing, however, is they only make sense as a tax shelter. They are poorly performing investments otherwise. A tax policy that encourages poor investments is not a good policy. Keep in mind that the government in the 50s was far, far smaller than it is today. Funding the vast growth in government is inevitably going to get sucked out of the middle class. reply omnimus 11 hours agorootparent> Keep in mind that the government in the 50s was far, far smaller than it is today. Funding the vast growth in government is inevitably going to get sucked out of the middle class. And where US gov grew? Army. Where that money goes? To few weapon contractors = to ultra rich. Its not like US put those investments in some other pocket than top 10% reply throwaway7ahgb 6 hours agorootparentprevThe old top tax rates are constantly echo'd around and not relevant towards any conversation. The only thing that matters is what the effective rate was. They could create a 100% tax bracket but if nobody paid it, it's just words in a book. reply FrustratedMonky 19 hours agorootparentprevSure. If you can move to another state to live cheaper, why wouldn't people. That is the argument for a more nationally consistent tax code, so the 'rich' can't just game the system by moving to the one cheap state where they bought off the politicians. And same goes internationally, if all countries would enact laws against money laundering, or at least enforce it, then the rich wouldn't so easily move their money around. Just because they can avoid taxes doesn't mean it is the governments fault for having taxes. reply kjksf 19 hours agorootparentOr maybe it's an argument for allowing states to try different economic strategies and compete for business based on outcomes of those strategies. If California taxes people and businesses at insane levels, maybe we shouldn't replicate insane level of taxation everywhere else in U.S., on the off chance that it is not, in fact, an optimal level for generating wealth for the people. Maybe Texas and Florida should be allowed to enact competing taxation schemes so that we can see which one is better. And maybe Americans should have a freedom of movement within U.S. without having their private property confiscated by the state when they try to leave. The justification for taxation based on residency is that you use commons like roads and school and police that the government funds so you need to contribute to funding those things. What exactly is justifications for hitting people with more taxes when they try to leave the state? reply FrustratedMonky 18 hours agorootparentI missed it, I don't see anywhere in this thread where people are being taxed on 'exit' from a state. Nothing is being confiscated. What is happening is that a state increases taxes on the current population, and the rich leave to a cheaper states so they don't have to pay the higher tax. So there is freedom of movement. But this does open a loop whole where they can use up resources, then split. To your argument. I guess it depends on 'fairness'. Sure, the deep south can 'Choose' to lower taxes and have really bad schools so that their un-educated population will be unaware, and un-questioning, of any risks when they to go work for the local chemical company or paper mill. This is a valid choice, lets make our people cannon fodder to lure large chemical companies to our state, (and bonus get some kick backs). But, the kids didn't choose this, they were born into it. (which maybe is why they are also anti-abortion, we can't loose our workforce). There should be some national standards to keep some states from a race to the bottom. reply throwaway7ahgb 6 hours agorootparentThe 'State' here is confounded. The State also refers to USA. If you give up citizenship and leave the US, they will tax you on exit. It's ridiculous. I should be able to give up citizenship or move to another country and pay a cent to the US government. reply danaris 4 hours agorootparentprevIs it a valid choice? It's a choice being made to limit someone else's access to resources and information, after all, including the kind of information that would give them the tools they need to understand how much you're screwing them over. reply arrosenberg 19 hours agorootparentprev> If California taxes people and businesses at insane levels, maybe we shouldn't replicate insane level of taxation everywhere else in U.S., on the off chance that it is not, in fact, an optimal level for generating wealth for the people. This is nonsense. California is the strongest state in the union economically, with one of the highest quality of life and income per capita. We have an affordability crisis that is related to the topic at hand - billionaires hoarding wealth and driving up the cost of living - but that's a national problem that California can't solve on it's own. Our taxes pay for the things that enable our success - roads, power, dams, parks, libraries, and all that other good stuff. Meanwhile Floridians can't use their beaches due to red tide and dumping tires in the water. The people of Houston JUST got their power back yesterday a week after a mere Category 1 hurricane. Maybe they should raise some taxes and do something about that so their cities stop losing economic productivity multiple times a year... reply throwaway7ahgb 6 hours agorootparentAnother view: 1) California is so popular because it has beautiful weather and natural resources. A group of monkeys could govern the state into prosperity. 2) California has also had the highest number of power outages amongst all states. reply arrosenberg 2 hours agorootparentWe can stay #1 is conclusively false - we let Ronald Reagan and Pete Wilson run the state and we still haven't fully recovered from the damage. A competent group of monkeys would still need similar tax revenues to what we currently collect to be prosperous. reply WalterBright 18 hours agorootparentprevhttps://www.statsamerica.org/sip/rank_list.aspx?rank_label=p... reply arrosenberg 18 hours agorootparentCalifornia is the 4th highest state in income per capita, which supports my statement. Thanks :) reply WalterBright 18 hours agorootparentIts tax collections are going down despite increasing taxes. https://fred.stlouisfed.org/series/QTAXTOTALQTAXCAT3CANO reply arrosenberg 16 hours agorootparentThat line is trending upward. Collections are down because the economy cooled after Q2 2022 due to the end of zero-percent interest rates. Again, something beyond the control of the state government. reply rvrs 19 hours agorootparentprevAh yes California, Washington, NYC — places where rich people famously do not live reply WalterBright 19 hours agorootparentAll have seen an exodus of rich people, mostly leaving for Texas and Florida, which are booming. Musk famously decamped from California to Texas. Bezos from Washington to Florida. reply pseudocomposer 19 hours agorootparentprevWould this not be easily solved with a one-time 50-90% wealth tax when they exfiltrate their money from the place where others earned it for them, rather than the ~1% annual wealth tax they want to leave over? reply dexterdog 19 hours agorootparentBecause most will leave as soon as the law is announced before it becomes law in that case. reply ryandrake 19 hours agorootparentOK, so make it retroactive on the day the law was passed. Laws are words written on paper, it's not like dark wizardry. These kinds of objections always come up. \"But, rich people are oh so clever, and they will exploit loopholes in any law that gets written!\" Well, write stronger laws. Don't include exceptions. Lawmakers need to think more than 5 minutes about what kinds of options/resources are available to people, and how those people might game their way into getting around the spirit of the law. reply WalterBright 18 hours agorootparent> OK, so make it retroactive on the day the law was passed Ex post facto laws are illegal. Even so, you might get the taxes once, before they leave. That's not a plan for a future. reply ryandrake 4 hours agorootparentFirst up, ex post facto clauses have been interpreted as applying to criminal laws only. AFAIK there exists no constitutional bar to retroactive tax legislation. The same may be true for states. And, It's not like they're all going to leave. Sure, a few grumpy ideologues like Musk will Go Galt and move away, but it's not like everyone subject to the tax is going to suddenly leave. People have been predicting the demise of California, New York, and so on forever, yet somehow, they have not all moved to Texas yet, and we still have plenty of high-earning taxpayers here. Maybe they like clean air and a functioning power grid, I don't know. reply dgb23 19 hours agoparentprev> One natural consequence of this has been that the wealthy people are simply...leaving the country. (our total / combined max wealth tax is 1.1%) I assume that was to be expected. The more interesting question would be: Is this a net gain for Norwegians? reply WalterBright 17 hours agoparentprevThe WSJ has a good article today on France's experience with wealth taxes: https://www.wsj.com/articles/france-elections-wealth-tax-new... reply tim333 19 hours agoparentprevNot sure if any counties done this but a solution to wealth tax on startups could be if the government allowed the 1% or similar tax to be paid in equity rather than cash. The shares could then go to a sovereign wealth fund. reply para_parolu 20 hours agoparentprevIs it a big problem for country that wealthy people are living? I thought most country wealth comes from oil and gas. And until it’s no longer the case it somewhat makes sense to avoid large wealth gaps. reply roenxi 19 hours agorootparentThat argument is a bit circular. It is a bit like saying someone doesn't need to get a law degree until after they've opened a legal firm - there are some obvious cause-effect issues. If there is a tax on wealth, why would we expect there to be local examples of great wealth created that aren't physically extracted from the land? If a Norwegian had a great idea for a new product, presumably they wouldn't be stupid enough to hang around in Norway being taxed back into the middle class. reply immibis 8 hours agorootparentYou can't presume that. They might quite enjoy living in Norway. More than they'd enjoy having twice the money, especially when both numbers are high enough to have anything they want. reply FrustratedMonky 20 hours agoparentprevThe US had same problem with family farms. The farm itself was very valuable, so when being left to the children as inheritance, it was heavily taxed. But, this is a problem, because what is left is not enough for the kids to make a living. So in the act of trying to tax the rich with inheritance, there are some lower/middle layers getting hit very hard. Taxes don't have to be a flat number, there should be exemptions carved out like for family farms. But of course, anything can be gamed. reply Terr_ 19 hours agorootparent> The US had same problem with family farms. [...] it was heavily taxed [...] what is left is not enough for the kids to make a living Math or it didn't happen! I say that because I've seen this kind of claim many times, and usually either (A) the Federal Estate Tax has no real affect on what happens or (B) their definition of \"family farm\" is wildly grandiose. https://www.cbpp.org/blog/the-myth-that-the-estate-tax-threa... reply ryandrake 18 hours agorootparentExactly, in the US, the estate tax minimum is over $13M. Under that, and it's not subject to tax. If you inherit a $14M farm (!!) and have to pay a little estate tax on it, well, I'm not exactly weeping for your misfortune. I would be happy to take that farm off of you and figure out how to pay the tax, if you're so against taxes. reply throwaway7ahgb 6 hours agorootparentThat is the current minimum, and that's because it was taught hard by one political party. If not, these farms would be wiped out and very large corporations would own them all. reply FrustratedMonky 7 hours agorootparentprevI think the issue is that the land can be worth over $13M, but the income from farming it is low, so the people living on the land are poor. Then the taxes against the $13M are beyond what the family can pay, so they loose the land. Or the land gets whittled down as parts are sold off to pay the taxes on the rest. The land is worth way more than the income from farming it. reply throwaway7ahgb 6 hours agorootparent100% I come from a family of farmers. The land is worth a lot but also has large notes on them. If you see the way these farmers live, NOBODY would call them rich. They never sell and the land gets passed from generation to generation. All the income from farming goes back into the farm (notes, taxes, supplies, maintenance). reply ryandrake 4 hours agorootparentHuh... I must be missing some huge bit of information, then. Using my example, how is the business worth $14M if it's not making, say, $1M a year? If it's not making any money, wouldn't the value be much less? Maybe the current business value is very low, but the land is worth $14M because someone else could make more money with it. In that case, the wisest decision would be to sell it, use the proceeds to pay off the taxes and whatever liens are hanging around, and then invest the balance more productively. Say you have $10M left over after settling taxes and debts. That's still a huge windfall, and anyone living on that farm would never be poor again. Invested wisely, and that family never works a day in their life again. Hell, a risk-free 30 year US Treasury bond is paying in the neighborhood of 4.5% now. That windfall could be turned into a risk-free salary of $450,000/yr, with zero investing skills. reply FrustratedMonky 1 hour agorootparentSorry. don't have a good example with math. I think the issue is that in the 70's the cut off was lower, below 13M. And stocks/bonds were also not doing as well so the concept of selling and investing and living off that wasn't possible. And possibly a lot of farmers had loans, large debt. I'd need to find all the pre-inflation 70's examples. I just remember at the time the farmers were kind of stuck in a catch-22. Probably does not apply to todays farmers. But you are correct, with your numbers. If the land really is worth that much, they could sell and re-invest. reply FrustratedMonky 18 hours agorootparentprevI'll find it. I think it was much more a problem in the 70s, after that people started getting smarter about incorporating so it wasn't personal wealth. reply rahimnathwani 19 hours agorootparentprevWhat is special about family farms, that doesn't apply to other family-owned businesses? reply throwaway7ahgb 6 hours agorootparentBecause when the US was settled (not that long ago), 90%+ of people were farmers. Now the East and West coast voters see no value in these farms so want the government to tax them away. They also naively think that the local farmers market can supply the country's food supply. reply mickael-kerjean 19 hours agorootparentprevyour typical business isn't working at the same level of the maslov pyramid than a farm. I don't want a world where only the wealthy can afford to give milk to their kids reply FrustratedMonky 18 hours agorootparentI think because farms need to be a certain size to be effective. And, even though they might be worth a lot as real-estate, the land could be worth millions, the people farming could be poor. Guess, to your point. If a family owns a dry cleaner, similar situation. reply Justsignedup 16 hours agoparentprevYeah the problem is that europe is doing the right thing BUT the US says fuck it do what you want. So it forces europe to have to compete on the same bs market as the US. reply throwaway7ahgb 6 hours agorootparentOr the US is smarter and realizes it's a failing policy. We can have a debate which country is more prosperous between the US and the entire EU. reply Justsignedup 4 hours agorootparentThat's not a debate I will take, as clearly the US is richer than the EU. However... richer != better. In a similar way that a billionaire is certainly richer than half the country, but that doesn't mean its the direction we _want_. reply auc 16 hours agoparentprevA consumption tax can avoids some of the ills of this - it is undodgeable for the wealthy (unless they commit fraud) and it is very progressive reply ineptech 19 hours agoprev> You would think, after having been on the side of labor in its fight with capital for almost two centuries, that the far left would be happy that labor has finally prevailed. But none of them seem to be. You can almost hear them saying \"No, no, not that way.\" This one really threw me for a loop. Do startup founders making or seeking a big exit really think of themselves as \"labor\"? reply ineptech 19 hours agoparentDeveloping this a bit; I get that founders might object to being lumped in with the asshole-rich-guy demographic associated with the term, but \"capital\" doesn't mean people who never had to work hard, it means capital. When we talk about laws and regulations that advantage capital over labor or v.v., we're talking about money rather than people. The fact that founders get rich via stock options rather than by paying themselves huge salaries is an example! If labor had won, wouldn't pg be trying to pick a startup to work for rather than one to invest in? reply skybrian 19 hours agorootparentCapital includes everybody’s life savings. It’s common to have a goal of retiring someday, and having capital is how you retire. What would it even mean for labor to “win,” if not by controlling some capital of their own? Could a union win without a pension plan? reply dragonwriter 19 hours agorootparent> Capital includes everybody’s life savings. Capital in the sense being discussed (which derives from the same critique of capitalism in which \"capitalism\" was coined) refers to the non-financial, non-land means of production; one's life savings are not capital, though they may (or may not) be used to acquire capital. > It’s common to have a goal of retiring someday, and having capital is how you retire. Having private ownership of capital is important to being able to retire within capitalism. That's a feature of capitalism, not an inherent feature of retirement. > What would it even mean for labor to “win,” if not by controlling some capital of their own? It expressly involves control of capital by labor collectively, which is not the same as private ownership of capital. > Could a union win without a pension plan? A pension plan is not capital. In a system in which private ownership of capital is a thing, and in which capital ownership is systematically favored throughout society, it will be normal for a pension fund to own capital, but the existence of such a system means that labor has not won. Also, pension plans aren't essential to unions, they are something unions often provide because providing for retirement is a gap in the system in which they exist that they elect to plug collectively. If labor actually won, that gap would not exist. reply skybrian 17 hours agorootparentIf you’re going to be that picky about the definition of “capital” then maybe the question is why would anyone be against farms and factories? :) So okay, for capital, substitute “ownership of a claim to the profits from capital.” I think it might be fun to imagine an economic system very different than ours, but I suspect it’s going to need cultural institutions that fulfill similar roles, because they serve human needs and those needs aren’t going away. For example, suppose that in the system you’re imagining, a retired worker goes to the store to get something to eat. Presumably, the collective has a worker making sure that nobody takes more than their fair share, or otherwise it’s all going to disappear. So the retiree has present some kind of abstract claim showing what they’re entitled to. It’s going to serve a similar role to retirement savings. How does it work? When you retire, how secure is your claim? How much can you get, and what happens when political institutions change? I think it’s easier to talk about such things in terms of familiar institutions that we understand well, but with the understanding that there could be different approaches, in theory. reply immibis 8 hours agorootparentNobody is against farms and factories in general. Some people are against some specific farms and factories, such as for environmental reasons. Many people are against the way the government chooses to allocate control of farms and factories (based on tradeable scrip). reply cannabis_sam 11 hours agoparentprevIt’s either pure delusion or malicous distortion of reality. reply polotics 21 hours agoprevNot using a throwaway, and agreeing in principle: Paul Graham appears here oddly non-scientific. reply metadat 20 hours agoparentTFA put it clearly and succinctly: >> You would think, after having been on the side of labor in its fight with capital for almost two centuries, that the far left would be happy that labor has finally prevailed. But none of them seem to be. You can almost hear them saying \"No, no, not that way.\" > And there we have it. The slight injection of PG's true ideology relegated to the notes section and vague enough that some might ignore. But keep in mind this is the same guy who argued against a wealth tax. His seemingly impartial and logical writing attempts to hide his true intentions. reply rvrs 21 hours agoparentprevOddly? His essays are regularly thinly-veiled excuses to peddle libertarian horse shit I thought Hackers and Painters was decent and even in those essays Paul couldn’t help him self reply SpaceManNabs 20 hours agorootparentTotally agree. One of my biggest critiques of entrepreneur and hacker circles is how much they ride PG. dude is rich, not some sort of genius policy maker. The difference between a lot of hackers now and PG isnt talent, it is that someone was making a good website when the internet pretty much was empty lol. Google was still a baby compared to yahoo. edit: since i was pretty negative here, i will sing some praise. Graham is incredibly technologically capable, knows what problems are worth investing in, and is great at leading people. Those qualities are great, and he is a world star at it. reply WalterBright 19 hours agorootparentRight now, there is a guy in a garage founding the next billion dollar company, doing something we all will wish we thought of. reply jmward01 19 hours agorootparentMaybe it is a sun room. I hope it is a sun room since I am in one right now and I want to be that person! reply WalterBright 19 hours agorootparentStop wanting and start doing! reply jmward01 18 hours agorootparentWho says I'm not? :) reply karmakurtisaani 11 hours agorootparentprevBtw, the point of the original article was that less and less people have access to this garage. reply WalterBright 3 hours agorootparentI started my business, and the only investment was buying a computer. Today you can buy a computer for $300 from the pawn shop. You also have access to worldwide markets with the internet and fulfilled by Amazon. It's never been easier to create a startup. reply gghffguhvc 19 hours agorootparentprevPartly helped by guys having it easier fundraising. reply WalterBright 19 hours agorootparentElizabeth Holmes didn't have problems raising funds. She was rather spectacular at it. reply gghffguhvc 17 hours agorootparentJust fyi: this joke fell flat on me at best. reply blagie 19 hours agoprevFrom my perspective, this is entirely a case of Simpson's Paradox. * Worldwide, wealth gaps are closing. * Within most countries, wealth gaps are widening. Here's how globalization works: 1. As capital, I target 8 billion instead of 350 million. That's more wealth per invention. 2. As US labor, I compete with Indian labor. Income goes down. 3. As high-end developing-world labor, I can compete with Americans for jobs. Income goes up. 4. As low-end developing-world labor, I have some trickle-down benefits, but gap to high-end labor increases. Footnote: It also makes a lot of sense, looking globally, to look at log(wealth). See: https://www.gapminder.org/fw/income-levels/ There has been a huge move from Category 1 (horrible life) to 2 (manageable life). That's huge. However, income going up 4x is not very visible on many figures when that income goes from $2/day to $8/day, and is being compared to someone with $100B. reply webnrrd2k 16 hours agoprevIn addition to wealth taxes, inheritance taxes are also important to get right. The US, historically, had fairly high inheritance taxes because it was thought to prevent dynastic families from taking over. Over the last few decades, inheritance taxes have been reduced, and it seems to correspond with a large concentration of wealth in the upper classes. reply xrd 21 hours agoprevNoahopinion has a very recent interesting article on wealth and taxation. It makes me mad because it contradicts a lot of cherished ideas I have about the rich and taxation, but probably makes sense in this conversation: https://www.noahpinion.blog/p/theres-not-that-much-wealth-in... reply afpx 20 hours agoparentWhen someone writes “confiscating rich people’s wealth” I can’t take them seriously. People make wealth from what infrastructure is available. Try making your wealth living in the middle of the Sahara. reply Devasta 19 hours agoparentprevBezos cries, as I wave a tenner in his face. \"You don't have any real wealth, you only have stocks!\" reply toomuchtodo 21 hours agoparentprevWealth is unelected power, a bug. It’s reasonable to say there should be limits on wealth, considering most wealthy people were lucky (either by birth or “right place right time”) but attempt to post rationalize it as anything else. Would you want someone who won a lottery ticket to have power over your life at scale? Because that’s what excess wealth is, a power dynamic powered by a suboptimal system masquerading as shades of a meritocracy that does not exist. reply dnissley 20 hours agorootparentOn some level not all power being elected is good. Provides some balancing force for an electorate with... I'm not sure what to call it... volatile whims? reply TimTheTinker 20 hours agorootparentprevWithout wealth, there is no prosperity for anyone except those who hold political power, full stop. reply aorloff 20 hours agorootparentprevWhat about wealth you build for yourself ? Many of the duplexes in town were built by small builders who passed them on to family. Now they are treated like rich landowners but in reality grandpa built the wealth. reply toomuchtodo 20 hours agorootparentDo you not see how this was luck? Certainly, actions were taken to encourage an outcome (holding an asset of potential future value), but luck as a component of a favorable outcome cannot be overstated. reply aorloff 20 hours agorootparentThey were builders, they had businesses building for other people and invested their own money and time into these buildings for their own family. Luck in exactly what sense ? Yes, they did not die of dysentery on the trail, so evidently some luck was involved, but the wealth building seems very intentional and causal. reply soared 20 hours agorootparentLuck that their grandpa built the duplex. I am unlucky that my grandpa did not build a duplex. Luck that their parents held the assets and did not sell it. reply throwaway22032 19 hours agorootparentThis is not luck. Your grandfather chose your grandmother and so too did your parents. Your parents deciding to do something is not luck either. I want my children to have an inheritance, the actions I take to ensure that are ongoing deliberate choices to build wealth over having fun. reply soared 16 hours agorootparentYes, so it’s completely out of your children’s control whether or not you chose those decisions. Therefore, they are lucky you are doing that. They have zero impact on whether or not the actions of their grandparents resulted in generational wealth. reply throwaway22032 9 hours agorootparentNonsense. You posted this comment 7 hours ago. Current you has zero impact on whether you posted it. Time flows forwards, not backwards. reply soared 4 hours agorootparentWhat action did you take that caused your grandpa to invest in real estate? If none, it is lucky for you that your grandpa took one action over another, as you had no impact on the event. reply toomuchtodo 20 hours agorootparentprevSee, you’ve caught yourself in the trap. Lots of builders fail, look at the YC startup failure rate. Even investing in broad equity index funds is luck (sequence risk). https://news.ycombinator.com/item?id=40378138 https://archive.nytimes.com/www.nytimes.com/interactive/2011... reply kortilla 20 hours agorootparentYou’ve caught yourself in the trap of thinking that passing something with a high failure rate means luck. reply rpdillon 19 hours agorootparentprevThis is a tired argument. Success in life will always be a combination of luck, and hard work to set yourself up to take advantage of the luck when it arrives. For some, the luck will never arrive. And for many, the luck will arrive, but they won't be ready for it. The only practical advice to give people is to prepare the best they can for when and if it does. reply toomuchtodo 18 hours agorootparentI’m not arguing the advice, I’m arguing the religion and belief system. reply aorloff 20 hours agorootparentprevI meant construction developers as a trade for others. reply aorloff 20 hours agorootparentprevIn the context of a discussion about wealth, its true that the financialization of real estate and runaway values of the greater Silicon Valley were a stroke of luck, but when talking about building wealth, housing has intrinsic value that I think takes a lot of that kind of SV luck out of the picture. reply toomuchtodo 20 hours agorootparentHousing value is land value. Did you buy or build in the right location? Luck! Did commercial real estate know WFH post Covid was going to obliterate CRE values? Again, bad luck. reply aorloff 13 hours agorootparentThis is true, many people got hung out to dry who happened to hold office properties when COVID hit. But the housing could have just housed the family members if times were tight. For at least the first generation, it did. That's intrinsic value, not valuation according to a market. reply kortilla 20 hours agorootparentprevOf course it can be overstated. That’s basically the entire mantra of progressives in the US right now and it’s used as an excuse for incompetent people to attack people who successfully do things and (even worse) absolve themselves of responsibility for doing unproductive things. Punishing people for not having something bad derail their business is tall poppy syndrome. Luck is a part of why everyone continues breathing everyday rather than having sudden heart failure. We don’t need to constantly diminish people for living because they were lucky enough to not have a fatal condition. We should be encouraging the people who were successful, even if part of it may have been luck. Any other approach punishes people doing the rest of the work that isn’t luck. reply ThrowawayR2 18 hours agorootparentprev\"I am a great believer in luck. The harder I work, the more of it I seem to have.\" --Coleman Cox reply Devasta 19 hours agorootparentprevBut if they are receiving free houses from their grandpa, they didn't build the wealth themselves. reply marcianx 16 hours agorootparentprevHow would you impose these limits on wealth? Where would the extra money go? To the government? If so, would you be okay with the government saying that _all_ future earnings if yours went straight to them because you have too much money already? Would you give anyone that power? reply danaris 4 hours agorootparentIf I already had $2B? Or even $200M? Yeah, I'd be OK with that. It's not like it would actually affect my life in any way whatsoever. We're not talking about the government taking the food out of anyone's mouth. It's not actually a benefit to the world that we have individuals walking around with enough wealth to purchase, say, one of the major companies that acts as a backbone for communication and information-spreading worldwide. (Of course, it's also somewhat troubling that a single for-profit company can be such a backbone, but that's a separate conversation.) In fact, there's some pretty solid research indicating that it's destabilizing for entire societies to have that level of inequality. reply chr1 20 hours agorootparentprevHow many people who win lottery tickets become wealthy? Most of them squander the money they win, so your point about simply being lucky is not true. As for your question lottery winner having the power proportional to his money, is much better than bureaucrats having power to take anyone's money. reply WalterBright 19 hours agorootparentNetflix ran a documentary recently about what happens to the zillions paid to football players. Spoiler Alert: they spend it like it was water, and when their short career is over, they have nothing. reply eigart 20 hours agorootparentprevObtaining excessive wealth is the same as winning the lottery (the billionaire founder class). Edit: Q: If you downvote this comment, do you believe someone like Zuckerberg was predestined to be a billionaire? reply AlOwain 16 hours agorootparentHe was certainly not predestined, but his inclinations and temperaments did distinguish him. True some luck is involved; it is a necessary part, but individuals are often what induces the procuring of wealth. reply eigart 13 hours agorootparentIf you monte carloed it, I’m sure he would do way better than average, but given how rare the current outcome is, I firmly believe it is mostly luck. reply rpdillon 19 hours agorootparentprevPresuming this is correct, what's the better alternative? reply toomuchtodo 19 hours agorootparenthttps://news.ycombinator.com/item?id=40963894 seems like a start. You don’t need to throw the entire system away and start from scratch, simply refactor the suboptimal parts piece by piece. reply rpdillon 19 hours agorootparentSorry, I thought you said that wealth was a bug, so I was looking for an alternative to wealth. Taxing wealth doesn't seem like an alternative to wealth, it seems like building on top of it. reply pedroma 19 hours agorootparentprevThe solution is to redistribute power from the people to the government? reply ethagnawl 20 hours agorootparentprevThat's such a simple, yet profound analogy. It's a great bit of framing for these types of conversations. reply toomuchtodo 19 hours agorootparentAppreciate the kind words. Any fool can know, the point is to understand. reply WalterBright 20 hours agorootparentprevIf people cannot accumulate wealth, we wouldn't have SpaceX. reply DaoVeles 19 hours agorootparentSuch a declarative statement is very reductionist. Yes, some folks will accumulate wealth but there needs to be some additional limits on it. I am in favor of progressively increasing tax brackets that eventually get up to near 100$. Spend it or lose it. And if you lose it, it goes in to the very bottom income bracket where it will be used imediately. Trick up economics if you will. reply WalterBright 19 hours agorootparentYou don't think Musk's fortune was used? He invested all he had in Tesla, then again in SpaceX. > there needs to be some additional limits on it. Then we don't get SpaceX or Tesla. reply marmadukester39 19 hours agorootparentprevNo, but we do get NASA reply WalterBright 19 hours agorootparentMusk's rockets cost less than 10% of NASA's. Plus, SpaceX has not cost you a cent, while you've paid plenty of taxes for NASA. reply abenga 13 hours agorootparentThe highest NASA's budget has ever been as a percentage of the federal budget is 4.5% during the Apollo program, totalling 650b nominally over the years. Saying anyone here now has paid \"plenty\" of taxes to nasa sounds … disingenuous at best. reply WalterBright 13 hours agorootparent4.5% is a terrific amount of money. You paid $zero for SpaceX. reply gmac 12 hours agorootparentIn this context, surely the whole point is that we could have taxed Musk’s wealth instead, so we have collectively paid whatever amount we didn’t tax him. reply abenga 11 hours agorootparentprevA bit less, definitely, but not zero. The US government has paid billions for use of SpaceX launches since it was founded. One could argue that the spending on NASA has benefited the world much more (e.g. free NOAA climate/weather data for research, etc) vs just increasing the wealth of privateers. reply JackFr 19 hours agorootparentprev> Because that’s what excess wealth is, a power dynamic powered by a suboptimal system masquerading as shades of a meritocracy that does not exist. What does that mean? It's millenial Marxism unmoored from any specificity or facts. reply toomuchtodo 19 hours agorootparentWe pretend wealthy people are special when they’re simply lucky. They then use wealth to wield power and control (politics specifically, Citizens United, but I can provide other examples), with a narrative that they are special (not lucky! special!) and that maybe you too can be special and wealthy if you work really hard and are exceptional. Are the wealthy lucky? Yes. Do they use their wealth to shape the world as they see fit? Yes. Is the narrative that anyone can be wealthy if they work very hard and are exceptional despite the math showing this is not true? Yes. reply throwaway22032 18 hours agorootparentAside from the influence of money on politics almost all of what you have written is complete bollocks. Many of us have been successful from nothing and know friends who have been more so. There is usually a luck element in the size of that success but it is blindingly obvious to someone who was in the crab bucket to see that actions work. It sounds as if you have an emotional issue with the idea that people can change their own situation. I literally have a choice tomorrow whether I paint my wall, study, fix up the leak in my loft, etc, or just sit and play my Switch. People who do too much of the latter do not succeed. And no, parents or grandparents or country or genetics or whatever are not luck either. Choosing a suitable mate is about as close to the meaning of life as we have. reply toomuchtodo 18 hours agorootparent> It sounds as if you have an emotional issue with the idea that people can change their own situation. On the contrary, I am simply data informed. It sounds like you're operating from a mental model that overweighs ability in outcomes. I have provided links below to help you improve and evolve this model. https://www.scientificamerican.com/blog/beautiful-minds/the-... (\"The Role of Luck in Life Success Is Far Greater Than We Realized\") https://www.technologyreview.com/2018/03/01/144958/if-youre-...http://arxiv.org/abs/1802.07068 (\"Talent vs. Luck: The Role of Randomness in Success and Failure\") https://ustrustaem.fs.ml.com/content/dam/ust/articles/pdf/20... (2022 Bank of America Private Bank Study of Wealthy Americans: The impact of shifting generational attitudes amid an historic wealth transfer) [Only 27% of the ultra wealthy are self made; 70% of Americans who hold more than $3 million are over 56 years old.] https://www.financialsamurai.com/your-wealth-is-mostly-due-t... (\"Your Outsized Wealth Is Mostly Due To Luck: Be Thankful!\") > And no, parents or grandparents or country or genetics or whatever are not luck either. https://www.pewtrusts.org/en/about/news-room/press-releases-... (\"Parental Income Has Outsized Influence on Children’s Economic Future\") https://documents1.worldbank.org/curated/en/7122514681658661... (\"Global inequality of opportunity: How much of our income is determined by where we live?\") https://www.psychologytoday.com/us/blog/the-origins-of-menta...https://www.sciencedirect.com/science/article/pii/S000292972... (\"Genes Influence Children’s Success, Directly and Indirectly\") > Choosing a suitable mate is about as close to the meaning of life as we have. ~43% of first marriages fail, ~60% of second marriages, ~73% third marriages. https://www.forbes.com/advisor/legal/divorce/divorce-statist... Good luck. reply throwaway22032 18 hours agorootparentAlright, I had previously been lazy but here's a proper response. I'm confused because to me, everything you have posted backs up the idea that these things are in fact actions and not luck based. Country of residence is not luck based, it's either pre-determined or chosen by parents. 27% self made is huge, I would have expected it to be lower. Parental income having an effect on children's is also not an example of luck. If I work hard so that my children do well, that is not luck based. Genes influence success, so choose good genes. 43% of marriages failing simply shows that many people don't take it as seriously as they used to due to things like no fault divorce. So choose well. Your partner, at least the type of person they are, should not be luck based. Your marriage definitely should not feel like a 4 in 10 dice roll, if it does you have really messed up. I think you and I (really, me and most left wing media) have a different definition of luck. Luck is rolling a die and getting a number, these things are more like causative factors. But still, there exist plenty of people who have been born into adversity and study and work hard, network and press the right buttons. That isn't luck, a lottery win is luck. I dunno. It all just feels like a way to absolve one's self from responsibility to me. I did the washing up yesterday, so today I don't have to do it. Is today me lucky that yesterday me did it? Only in the colloquial sense. reply abenga 6 hours agorootparentLike you said, you have a different definition of luck. Others seem to say \"factors that you did not choose, and do not have the ability to change\" are luck. > Country of residence is not luck based, it's either pre-determined or chosen by parents. So you have very little to do with it; you can probably choose this after you finish school, but this is dependent on the place you would like to emigrate to letting you in, thus it's mostly not up to you. > Parental income having an effect on children's is also not an example of luck. If I work hard so that my children do well, that is not luck based. If I get the resources to do well because my parents worked hard, that is really not something I choose, so it's \"luck\". > Genes influence success, so choose good genes. One can hardly choose their own genes, can they? > But still, there exist plenty of people who have been born into adversity and study and work hard, network and press the right buttons. This is true. They would have had an easier time and achieved more if they did not have to dig out of the basement first before building up. Nobody is saying that you cannot be smart and diligent enough to change your lot in life, we are just saying it is hard, getting harder every day, and it should not be. reply throwaway22032 5 hours agorootparentI just think this is a basic logical error. 39 year old me built a shed. 40 year old me then has a shed. 40 year old me did not influence or choose 39 year old me. But 40 year old me does not have a shed due to luck. It was causative. Your parents and your past decisions are not luck based. If you want to call that luck, then you need a different word for actual luck such as chance encounters, lottery wins, unexpected windfalls etc, because otherwise we are just misinterpreting each other. reply abenga 5 hours agorootparentIf you didn't choose it, do it, and cannot change it, it's not up to you. Whatever parents n-1 year old you had, or nationality they had, are the same ones n year old you have. If they are abusive, or in war, you are screwed. Build a shed or not. Of course there are things you can do to improve your and your children's lives (and you should), but there are irreversible handicaps you can have through no choice or action of yours. reply AnimalMuppet 5 hours agorootparentprevDefine \"luck\". That's what this whole disagreement hinges on. You're using a different definition of luck. Sure, I reap the fruits of my past decisions, like the shed. More to the point, I reap the fruits of my parents' decisions. Is that luck? It's not luck from the perspective of actions having consequences, and parents doing things that help their children. It is luck from the perspective of \"you didn't pick your parents\", or from the \"if you were going to be born to someone random, where and when would you want to be born?\" perspective. So you and abenga are saying \"yes it is!\" \"no it's not\" about the same set of facts, because you're running different definitions of what is or is not luck. Arguments about the definitions of words are really uninteresting. What you and abenga are saying boils down to \"yes, actions have consequences, and better actions lead to better consequences, both for yourself and your descendents, but no, you didn't select the parents, country, or time you were born to\". Apart from the definition of \"luck\", do either of you actually disagree with either side of that? If so, with what part, and why? And, if you do agree with it, what conclusions do you draw? reply throwaway22032 3 hours agorootparentI agree that the words don't matter. What it comes down to is that luck cannot be incentivised - a dice will always roll a 2, 1/6th of the time. But good behaviour can be incentivised, e.g. people choosing their partners well and providing stable environments for their children. We should incentivise good behaviour whilst also ensuring that luck doesn't result in unassailable leads e.g. a lottery win probably should not result in a 1000 year dynasty but a lottery win combined with intelligent estate planning, good mate selection, diversification etc probably should. > if you were going to be born to someone random, where and when would you want to be born I think that this is like asking, if 40 year old you were completely randomly spawned, whose life would you like to continue from. You can't. The lineage from your parents to you is the same as the lineage from you to you. It's a nonsense line of inquiry. reply Timshel 20 hours agoparentprevIt's interesting but at the same time I think some points miss their mark like : > The wealth of America’s billionaires was estimated at around $5.2 trillion in 2023, while federal government spending was about $6.4 trillion. Confiscating every last penny from Jeff Bezos, Elon Musk, and all the other billionaires wouldn’t fund the U.S. government for one year. And of course you could only do it once. Ok so just 750 persons can replace all form of taxation in the USA that's incredible. It's not like just replacing taxes from all 330m persons but all the companies too ... And the USA budget is not the most efficient per capita (something around 20k ?), if you do the same for France you are closer to 6k. reply yieldcrv 20 hours agoparentprevone of the interesting things to me is that the concept of wealth isn’t finished. not only is private wealth improving, wealth that is exchangeable for other things is improving as well. its fine for other people to work on improving its distribution while keeping a finite set of resources affordable, because I dont have the solution to that and taking other people’s private liquid or illiquid wealth doesn’t solve that either but its very interesting to me to see how abysmal liquidity is in every market outside of some US markets, and how we got here, and how many more things can become liquid wealth. It makes me not be able to take simpler proposals of solving out of control public spending very seriously. reply ericjmorey 20 hours agoparentprevNoah is a smart person and that's why I'm surprised that his essay is so transparently dumb. \"I think income is a lot more important than wealth.\" If you keep receiving all the income you want and I'll get exclusive rights to all of your wealth, surely you got the better side of this deal then? Or is there more nuance at play and blankets statements about the relative importance of income and wealth are simply pointless. \"if you have a house, no one owes you that house\"? In this case of property ownership, your rights granted to you by society is the result of an agreement by the society you're a part of not to use the resource that you've been granted exclusive rights to. The allocation of control over resources is the essence of economics. It's the whole point of the field of study. I cannot believe he's setting up an argument about how that doesn't really matter because there isn't a lot of resource control without legally associated debts. reply Mathnerd314 19 hours agorootparent> If you keep receiving all the income you want and I'll get exclusive rights to all of your wealth, surely you got the better side of this deal then? I think so, because you can use the income on consumables. Like say you got a no-limit credit card. You go on exotic vacations at tropical resorts. All you have are some (economically worthless) photos of yourself having a great time. When the credit card company comes calling, you turn out your pockets and say \"sorry, I'm broke\", and walk away to your next plane flight. Definitely seems like a good deal to me, not so much for the credit card company. The simple fact is that most people are wealth-neutral or even wealth-destroying in that e.g. they wreck their car and just buy another. reply TacticalCoder 20 hours agoparentprev> Noahopinion has a very recent interesting article on wealth and taxation As this article explains: the US could confiscate all US billionaires' wealth and that would only pay for one year of public US spendings. Not to mention that confiscating that much wealth would crash the very value of that wealth, so they'd get less than that. That's what I meant in my comment when I wrote that we know how it's going to end up: they won't stop at confiscating the billionaires' wealth. The state shall immediately run out of that. They'll then come after the middle class' savings (which the EU is preparing at the moment, should be coming out in a few months). reply jackcosgrove 19 hours agoprevI find discussions of economic desert rarely get anywhere since everyone has a different idea of what is deserved. I like to talk about incentives. We have a massive under supply of housing across the developed world, in part because so much labor is training for desk jobs rather than construction jobs. If you want to solve the housing problem, you need to change the incentives for young people to shun construction jobs. That means more money and eventually status for manual laborers and less money and eventually status for desk jockeys. That's just one example. reply TechDebtDevin 19 hours agoparentI was a project manager in the construction industry for 7 years, then another 4 in real estate investment. This wouldn't work, I'm too tired to go into it and it would take a few thousand words to even start explain why it wouldn't. In theory it could work if new construction was subsidized and price fixed on a per square foot/meter basis but free markets are the \"enemy\" here and there's hundreds of different inputs to consider and balance. Simply paying people more (and just fyi, we already do this. Get a quote for a kitchen remodel in any major metro and you'll realize real quick that there's carpenters making more than mid level SWEs) isn't going to motivate young people to break their bodies, work all day in the sun and shorten the length of their careers by 10-15 years compared to a desk job. There's a ton of reasons why young people don't want to dedicate their life to manual labor (and for good reason), neither money or nor status are the sole drivers here. Its simply not a great existence and is less rewarding all around, no matter how much you make. This is sort of why I don't see the argument against looser immigration policies, as the incentives you're mentioning actually exist in those communities, but that is a whole other can of worms I'm not going to get into. reply seliopou 19 hours agoparentprevI find it hard to believe the lack of housing is caused by a labor shortage in construction. reply TechDebtDevin 19 hours agorootparentThe lack of housing is due to incentives. Investors aren't going to increase supply and in return decrease their margins, they'll just make their money work somewhere else. Free markets work against us here, this sort of thing has to be addressed via price-fixing/subsidies. reply mritterhoff 4 hours agorootparentThe incentives are aligned for builders (who want to build housing for money) and buyers (who want to pay money for housing). What's getting in the way is overly restricted zoning, including single-family-unit only, and minimum parking requirements. The free market is the solution here, what's holding it back is zoning. reply jackcosgrove 19 hours agorootparentprevAs I said, it's one factor. You can search for \"construction industry labor shortage\" and find lots of articles. This one has an overview from an econ perspective https://www.econlib.org/archives/2017/04/how_can_there_b.htm... reply evilduck 19 hours agoparentprevManual laborers (distinct from skilled labor like plumbers or electricians) will never have high status because the job is, relatively speaking, easy to attain. The pay will certainly fix the staffing problem but it will never be a high status career unless the pay gets you to a cushy retirement 15 years earlier than a desk job. reply TechDebtDevin 19 hours agorootparentIt does get you retirement 15 years earlier than a desk job, but usually due to your body falling apart at 50 and you're then poor and in pain the rest of your life. I used to see it all the time. reply Moldoteck 14 hours agoparentprevHousing problem is more about politics + zoning and minimum parking requirements. To build more housing you need space and demand and usually zoning+pk mins are the opposite. Also, in other areas like EU housing is a problem for another reasons like bureaucracy, few authorisations from city council, the lobby of home owners to keep supply low, the lobby from car companies (no parking and more density means cars less needed) The construction jobs even if in demand, usually can be filled with ppl from poorer countries, so it's not such a big problem reply totony 19 hours agoparentprevHousing crisis is more of a political issue than supply/demand i feel like reply pvg 20 hours agoprev1000+ comment chonkthread at the time https://news.ycombinator.com/item?id=26787654 reply rndmize 19 hours agoprevI think there's another point to be made here that isn't often discussed. Even in a system where people that work hardest/smartest directly translates to the most successful - doesn't mean you have a good system. Consider an environment where a million engineers are all working on their own startups. 99% of them fail and reward the work put into them with nothing. Some subset of the remainder makes barely enough to survive; a smaller subset makes enough to have a successful business; a smaller subset makes millions, and a smaller subset makes the vast majority of the money available. If the effort put into the thing follows a mostly linear line, but the reward is exponential, is this really a good system? Do the people at the top of this structure deserve the wealth they gain, even if they genuinely the best at what they do? More and more systems today are winner-take-all. Entertainment is notorious for a tiny number of multi-millionaire artists/actors/comedians/etc., while the vast majority of people that try to make it end up with a succession of part-time jobs that never ends. Even if there's no nepotism or corruption, is a system where the people at the top get everything and the bottom gets nothing a good one? There's something disturbing about PG's take to me - as if only the most successful are deserving, or worthy - that this warped reward structure isn't inherently unjust. It feels like the kind of justifications nobility and royalty relied on, but for modern times - \"I worked hard, I found the market, I did everything right, so naturally I deserve more wealth than a human can use in a thousand lifetimes.\" So to be honest, I find almost every single one of PG's points worthless. I don't care how easy it is to start a startup if the chance of real money from it is one in a million. I don't care about how much faster growth is when its billions of dollars for a few dozen people. I don't care if the new wealth is genuinely new instead of inherited, if all we get from it is yet another tiny group of obscenely wealthy people - meet the new boss, same as the old boss. And I especially dislike the idea that the \"far left\" should be happy that \"labor has won\". Having a system that picks a few hundred of the \"most worthy\" each year and adds them to the capital class is by no means what I could consider labor winning. reply rldjbpin 11 hours agoprevwhile globally we are in a better* state than our past selves, but being better off than the others have always been a zero-sum game imho. so this sort of messaging reminds me of the cutthroat competitiveness that folks from working-class backgrounds, especially in developing countries, get drilled down on from a young age. too bad it is not something everyone can achieve at the same time. reply dash2 20 hours agoprev\"Paul paints a rosy picture but doesn't mention that incomes for lower and middle-class families have fallen since the 80s.\" Doesn't look like it: median income in 2019 was $44K, in 2001 it was $31K. That's adjusted for inflation. Source: https://en.wikipedia.org/wiki/Income_in_the_United_States I don't like it when the facts look wrong after two minutes' googling. reply xrisk 20 hours agoparentThe same Wikipedia article notes that the increase in median income is due to increasing women’s wages and increased participation of women in the labor force. > While wages for women have increased greatly, median earnings of male wage earners have remained stagnant since the late 1970s. Household income, however, has risen due to the increasing number of households with more than one income earner and women's increased presence in the labor force. reply willsmith72 20 hours agoparentprevIt is interesting how much easier the stats for 50th percentile plus are easier to find than anything below that. Also only 2 minute of googling, but 6/6 pages I checked provided either median or median as well as 60-99th percentile. None showed anything below. Not speculating on what those stats are, and I'm sure they're out there somewhere, but certainly less available or SEO'd. reply soared 20 hours agoparentprevAgreed, that is a very weird sentence when the chart clearly doesn’t show the same thing. Not sure what the author meant. > But middle-class incomes have not grown at the rate of upper-tier incomes. From 1970 to 2018, the median middle-class income increased from $58,100 to $86,600, a gain of 49%.10 This was considerably less than the 64% increase for upper-income households, whose median income increased from $126,100 in 1970 to $207,400 in 2018. Households in the lower-income tier experienced a gain of 43%, from $20,000 in 1970 to $28,700 in 2018. (Incomes are expressed in 2018 dollars.) https://www.pewresearch.org/social-trends/2020/01/09/trends-... The graphs in the blog posted are from this pew article. reply advael 20 hours agoparentprevDam, I didn't know the 80s started in 2001. Or is it just that that's the chart you can find with \"two minutes' googling\" and so that has to be good enough because that's the amount of effort it takes to find something that vaguely seems to satisfy your confirmation bias? Okay, though, let's just look at the chart you link. Let's assume that \"inflation adjusted dollars\" tracks purchasing power parity perfectly over that period and look at these numbers in relative terms, because that's what actually matters for stuff like \"Who is competing to buy the limited housing stock available\" or other things that actually matter to people and aren't just numerical games 31k to 44k? Cool. Seems like a lot. That's a 33% increase in the median income over that period! Over the same period, the top 20% bucket on the same chart grew from about 67k to about 103k, or by about 50%. The .0001% income bucket grew from 30m to about 60m, or about 100%. Keep in mind that the marginal value of extra dollars is considerably higher the fewer dollars you're making. So over this period of time, the Gini Coefficient, which measures inequality, has drastically increased, which is what you'd expect given the argument the article is making Now, maybe your point is that the one number, the median income, that you're talking about has gone, I dunno, vaguely up in some period of time. It seems like this misses the point of the article in favor of nitpicking about a minor factual error. \"Look!\", you might say, \"they said median income went down when really it went up!\" That seems to be your entire argument here, and it's fucking stupid, because if you're going to fixate on a minor factual nitpick, at least get your facts right. 2001 is generously a whole decade and some change out from the 80s reply orangecat 19 hours agorootparentOr is it just that that's the chart you can find with \"two minutes' googling\" and so that has to be good enough because that's the amount of effort it takes to find something that vaguely seems to satisfy your confirmation bias? Here's median income going back to 1975 which shows an even larger increase: https://fred.stlouisfed.org/series/MEPAINUSA672N reply willsmith72 19 hours agorootparentprevlet's just do away with any attempt at sticking to the facts, and stick to the vibe of the thing. to me, you are doing the nitpicking. picking 2001 was generous, the growth is far greater if you look back to the 80s. reply advael 19 hours agorootparentBut the vibe of the thing is about income inequality, and it's good that it is, because the value of money isn't fixed. Only rich people staring at a stock portfolio care about \"number went up\", what most people care about is their real purchasing power. Right now, the income you need to qualify for a mortgage on a house in the US is around the ~80-85th percentile mark of household income, generously. The Gini Coefficient is mentioned by the linked article exactly because income inequality is really important for what someone's effective purchasing power is, partially because some markets, like housing, are priced in a competitive manner, which is why I keep mentioning housing (Also, it's a huge factor in people's material living conditions, and a cost that has risen far faster than \"inflation\" in all of the periods discussed so far, which is the kind of phenomenon you can't really explain by trying to do big broad averages in \"adjusted dollars\" but which measuring things like inequality can tease out causal factors for a lot better). If we're just going by vibes, the overwhelming vibe (which is also supported by the numbers) is that income inequality is increasing and that this is a huge problem, and it seems like most of the people who don't feel this way are pretty decidedly in the small portion of the population for whom this is working out. It doesn't surprise me that people here don't vibe with that. Based on the forum alone, I'd bet all the people in this thread are at least in the 80th percentile of US household income (adjusted for local purchasing power if you happen to live elsewhere). If this wasn't the vibe in 2021, Paul Graham wouldn't have been writing bullshit to try to dismiss said vibe, and this person wouldn't be responding to him reply willsmith72 19 hours agorootparentyes, philosophically and by the vibes, we are in complete agreement. clearly inequality is growing, clearly that is bad. but you're doing a disservice with completely bogus claims like \"incomes for lower and middle-class families have fallen since the 80s\". it only makes it harder to talk about the real issues and facts, because why would the other side stick to them now? the trust between sides gets further eroded and the gap widens there's enough real data that we don't have to start making stuff up. the author could have easily pointed to housing instead like you. i think it's actually extremely important to get these facts right. enough to argue about on an internet forum on a sunday evening reply advael 18 hours agorootparentI agree that the author made an imperfect argument and probably should not make factual claims that aren't supported by data. I think people arguing in bad faith tend to pick up on this kind of error because, like the commenter I responded to, they feel that their ability to refute anything about the argument means they've refuted the argument. My intent in responding was to point out that this is stupid, and that no one writing informally can really make an unassailable factual argument, which I did by doing the same kind of stupid nitpicking on their nitpick. Finding the weakest peripheral claim made in an argument is essentially always a fractal recursion into increasingly banal minutiae that serves no one except people like me who are trying to procrastinate on actual work by writing comments on a dumb aggregator website where devs argue for fun. I think we are in agreement at most of the self-similar layers of this reply orangecat 18 hours agorootparentprevincome inequality is really important for what someone's effective purchasing power is Not really. Bill Gates isn't bidding up the prices of homes or groceries in your neighborhood. Also, it's a huge factor in people's material living conditions, and a cost that has risen far faster than \"inflation\" Yes, housing is far too expensive. And the main reason for that is state and local governments forcibly preventing people from building more housing, which is hard to blame on free markets. most of the people who don't feel this way are pretty decidedly in the small portion of the population for whom this is working out The point of the \"minor factual nitpick\" that median incomes are increasing rather than decreasing is exactly that capitalism works out pretty well for lots of people other than the wealthy. reply advael 18 hours agorootparentGroceries? Maybe not. Houses? Absolutely. I mean I don't know about Bill Gates in particular, but the overwhelming majority of wealthy people have diverse portfolios, including in real estate, which means that large companies or funds are making offers in cash for houses they're not planning to live in, which drives up prices in aggregate. It's not the only factor, but it's actually a significant one. Yes, housing stock is artificially limited for bad reasons. Also, the relative value of real estate near cities is, in an era where remote work is pretty feasible but a lot of companies don't allow it, artificially skewed toward the population centers people are forced to move to for work. That these are also factors does not imply that inequality isn't one Also, reducing the entire question of whether something has gone wrong and what and what one might do about it to a binary of \"capitalism good\" or \"capitalism bad\" is pretty pointless. Lots of things we could call capitalism have worked out really differently from each other, and looking at what the aggregate trends are and what policies might drive them and how they might be adjusted to better people's situations is a lot more useful than pretending that this is about \"capitalism\" versus \"not capitalism\", which so far as I can tell you are the first person in the thread to do, including the author of the linked article reply mupuff1234 20 hours agoparentprevInflation isn't exactly a single objective truth as you can decide how to measure it. For example looking at the number of median salaries required to buy a house tells a different story. reply ggg4468 20 hours agoparentprevThe left likes to constantly watch and complain of how much money others have. It's amazing how petty they are, behaviors frequently seen in 5 year old children, and in leftist adults. Waaaah waaah, the size of his cake is bigger. Waaaaah reply OKRainbowKid 20 hours agorootparentBuilding strawmen and characterizing roughly 50% of the population as crying babies on the basis of you not agreeing with their politics sure seems like something a very mature and collected person would do. reply Mistletoe 21 hours agoprevNot only are the poor poorer but now they feel like crap from the tech social media dystopia they inhabit. reply CoastalCoder 20 hours agoparentThere's also the surveillance economy. I'm not too happy about that, either. reply jongjong 20 hours agoparentprevYes, it's 24/7 gaslighting. One article after another telling us that things are a certain way and showing us all sorts of up-trending charts while we can see with our own eyes that it bears no resemblance to our reality... So then we start thinking \"Maybe, it's true, everyone else who is not in my social group is doing well. Maybe there is some kind of conspiracy against me and my kind... Maybe that's why none of my startup project are allowed to get any traffic from Google and why bloggers and journalists never respond to my emails, not even the ones with less than 10k followers.' reply the_real_cher 19 hours agoprevThe part everyone always glosses over is: If the rich are taxed more, will the government equitably distribute that money to the middle class? Defense contractors make hundreds of millions of dollars from government funds. We just assumed that tax money will go to the middle class but it's also likely to go to another rich person. reply PostOnce 19 hours agoparentDefense contractors employ a lot of (real, not javascript) engineers with good salaries and then those salaries get taxed again and then they pay for government meteorologists and then those salaries get taxed again and then they pay for construction Government spending is a fantastic deal because they automatically get a huge discount in the form of being able to tax what they give you. That's why privatization is theoretically far below optimal. After all, when there's a big fuckin' hole in the road at least you can call someone, which wouldn't be true if Google were building roads. reply the_real_cher 6 hours agorootparentIm struggling to understand how what youre saying doesnt also apply to private industry and what hiring meteorologists has to do with this. I havent found anyone I can call to come fix a road from the government in my town however I can get anything the private industry offers, delivered, often immediately. reply thunky 19 hours agoparentprev> If the rich are taxed more, will the government equitably distribute that money to the middle class? Good point. We don't even know that the money will stay in the country. reply kjksf 19 hours agoparentprevEven worse, it'll be spent buying votes (Biden's student debt relieve). Billions will be allocated for building electric chargers and almost no chargers will be built. Billions will be allocated for connecting poor people to internet and no people will be connected. Hundreds of millions will be spent on homeless and will result in increasing homelessness. A public toilet will be built for $2.5 million. McKinsey will be paid $4 million for a study telling NY that garbage bins are better than plastic garbage bags. $11 billion will be spent on 57 miles of rail tracks. Those are just examples of government waste I can recall from memory. Discussing levels of taxation is just a sideshow. Sound and fury signifying nothing. The real problem is that government grows without bounds, more of our money goes to just paying the salaries of bureaucrats and contrary to what you might expect, more bureaucrats leads to making things more difficult and costly for non-government and to just breath-taking inefficiencies and waste of money by government. reply the_real_cher 10 hours agorootparentI agree that our tax money should not be going to pay for students loans. At the same time students should be allowed to bankrupt out of their loans like any other loan on the planet. Student loans are the only loan one is not able to bankrupt out of and it's literal debt slavery. reply WalterBright 20 hours agoprev> Paul paints a rosy picture but doesn't mention that incomes for lower and middle-class families have fallen since the 80s. These analyses never mention the fact that the consumption of wealth by the government has vastly expanded. Where does one think that wealth came from? reply WalterBright 20 hours agoprev> false claim that wealth inequality is solely due to more startups and not a real problem says a lot. The article assumes that wealth inequality is bad, but never explains it. The thing is, people all have different abilities and make different choices in life. These lead inevitably to different results. In order to make everyone equal, peoples' freedoms must be taken away. reply glitchc 20 hours agoparentThere's copious amounts of ink spent on why wealth inequality is bad for a country. The author doesn't need to rehash all of those arguments to make his point. reply WalterBright 19 hours agorootparentTell us one of them. I'll give an example where everyone was equal. The USSR. That worked out great, right? reply glitchc 19 hours agorootparentI don't think any of the wealth inequality experts are arguing for identical wealth. They want to make sure that all tiers of society maintain social mobility and can participate in the American dream. This generally means preventing various stratas from taking unfair advantage of the system, making it more of a level playing field where we can. It benefits society when a genius born in the slums can make something of themselves, that the opportunities are there to escape the slum. Extreme inequality leads to crime, social instability and eventually major civil unrest. As with everything, it's about moderation. Unrestrained capitalism is just as bad as unrestrained communism, the ideal balance is somewhere in between. As for sources, here are a few (just a web search away): https://www.pewresearch.org/social-trends/2020/01/09/trends-... https://inequality.org/facts/wealth-inequality/ https://www.brookings.edu/articles/rising-inequality-a-major... https://www.oecd.org/en/data/indicators/income-inequality.ht... reply WalterBright 18 hours agorootparent> Extreme inequality leads to crime, social instability and eventually major civil unrest Historical example, please. reply fragmede 18 hours agorootparenthttps://en.wikipedia.org/wiki/French_Revolution reply WalterBright 18 hours agorootparentI am no expert on the FR, but that article as a long list of causes, only one of which is \"Increasing inequality led to more social conflict.\" and that's all it says about that. Besides, it was a feudal society with nobles and peasants, with different laws attached to each. That's bound to cause resentment. reply doctor_eval 19 hours agorootparentprevWhy smart people continue to say this is beyond me. There were so many problems with the USSR - central planning, corruption, bureaucracy, secrecy, misaligned incentives - and it's just not true that everyone was equal. Very few rational people would agree that the USSR is something that we should aspire to. As far as I can tell, \"look how well it worked for the USSR\" is just a trope that's trotted out by people who don't want to accept that alternatives to the status quo might be possible. reply WalterBright 18 hours agorootparentIf you don't like the USSR, provide another example that suits your position. Also, why are the FAANG companies all American companies? reply duckmysick 8 hours agorootparentAre you asking why an arbitrary group of American companies, popularized in an American TV show about the American stock market, has only American companies? I don't know. That's a good question. Why didn't they include others",
    "originSummary": [
      "Silicon Valley investors and CEOs, once seen as innovators, are now being criticized as monopolists and are defending themselves against these claims.",
      "Paul Graham's essay, \"How People Get Rich Now,\" argues that modern wealth is more merit-based compared to the past but overlooks the growing wealth gap and its implications.",
      "Despite the ease of starting a business today, the benefits are limited to a minority, and wealth inequality remains a significant issue, not just a radical left concept."
    ],
    "commentSummary": [
      "The wealth tax debate in Norway examines its benefits, such as ensuring the wealthy pay taxes, and its drawbacks, like burdening entrepreneurs and startups.",
      "Critics argue that wealth taxes discourage investment and innovation, potentially leading to wealthy individuals leaving the country.",
      "The discussion also covers broader issues like wealth inequality, the role of luck in wealth accumulation, and the effectiveness of government spending, highlighting the complexity of balancing fair taxation, investment encouragement, and social stability."
    ],
    "points": 214,
    "commentCount": 278,
    "retryCount": 0,
    "time": 1720988205
  },
  {
    "id": 40964261,
    "title": "Tabloid: A clickbait headline programming language (2021)",
    "originLink": "https://tabloid.vercel.app/",
    "originBody": "You Can Earn Millions With This Programming Language! GitHubNEW headline!Tutorial Tabloid: The Clickbait Headline Programming Language Fibonacci sampleFactorial sampleRun this! DISCOVER HOW TO fibonacci WITH a, b, n RUMOR HAS IT WHAT IF n SMALLER THAN 1 SHOCKING DEVELOPMENT b LIES! RUMOR HAS IT YOU WON'T WANT TO MISS b SHOCKING DEVELOPMENT fibonacci OF b, a PLUS b, n MINUS 1 END OF STORY END OF STORY - EXPERTS CLAIM limit TO BE 10 YOU WON'T WANT TO MISS 'First 10 Fibonacci numbers' EXPERTS CLAIM nothing TO BE fibonacci OF 0, 1, limit - PLEASE LIKE AND SUBSCRIBE No output. What? Tabloid is a turing-complete programming language for writing programs in the style of clickbait news headlines. Here are a few things the Top Five Most Popular Quirks and Features of the Tabloid programming language (Number Four Will Shock You!) Print output with the keywords YOU WON'T WANT TO MISS followed by an expression. Everything printed by Tabloid is automatically capitalized, and an exclamation point is added. Why would you want anything else? Declare a function by writing DISCOVER HOW TO ... WITH. Truly, a more gripping way to declare a function can't possibly exist! Similarly, assign to a variable with EXPERTS CLAIM ... TO BE. On the Internet, anyone can be an expert, and Tabloid gives YOU the power to wield that responsibility and declare anything you'd like! There are no built-in constructs for looping. The news cycle is moving too fast! Nobody has time for yesterday's loops or last week's break statements. If you must loop, use recursion. To return from a function, simply write SHOCKING DEVELOPMENT! You're going to—gasp!—return? How shocking! Every program must end with PLEASE LIKE AND SUBSCRIBE, because you have to grow your audience! Hashtag hustle. But why? Didn't want to do homework for my database systems class, and needed something to do to procrastinate. Will I finish the homework? Did I get enough sleep? Stay tuned to find out! Does it actually work? Yes. Tabloid is a fully functioning, Turing complete programming language with an interpreter written in JavaScript. Tabloid currently only supports numbers, strings, and booleans, but with these elements, you can write any program you'd want to write. You can edit and run the program above, or see how it works for yourself. Besides this online interpreter, Tabloid now also has a mostly compatible implementation in Racket and a small library of helper functions ... for some reason. Before making Tabloid, I also created a more useful and well-designed boring and unpopular programming language, called Ink. How much is there? Here's the full list of standard keywords that Tabloid currently uses: DISCOVER HOW TO...WITH declare a function RUMOR HAS IT begin a block scope A OF B, C call function A with arguments B, C WHAT IF...LIES! an if-else expression END OF STORY end a block scope EXPERTS CLAIM...TO BE declare or assign to a variable YOU WON'T WANT TO MISS print output LATEST NEWS ON take user input TOTALLY RIGHT true COMPLETELY WRONG false AND, OR and/or boolean operators PLUS, MINUS, TIMES, DIVIDED BY, MODULO the obvious arithmetic operations IS ACTUALLY is equal to BEATS, SMALLER THAN greater than / less than SHOCKING DEVELOPMENT return from a function PLEASE LIKE AND SUBSCRIBE end of program Tabloid is an overnight hack by @thesephist, website built with Torus and blocks.css.",
    "commentLink": "https://news.ycombinator.com/item?id=40964261",
    "commentBody": "Tabloid: A clickbait headline programming language (2021) (tabloid.vercel.app)192 points by ko_pivot 18 hours agohidepastfavorite25 comments cowsaymoo 11 hours agoNow this is truly the programming language that we should be using to benchmark LLM code gen in a private hold out set. There is no substantial datasets on the internet or github, and no documentation except the one provided. And that's all the model should need. I asked GPT-4 to write a mat mul function, but that was too ambitious and it spit out outrageous nonsense. To be more fair, I gave it in-context access to the documentation in prompt, along with the fibonacci example function; aka everything humans have access to. I then asked it to do the simpler task of converting a base 10 integer to binary. It was unable to write something error free even after 4 rounds of supplying it the error messages. I repeated this 5 times in case it generates something grammatical in the Top-K@5. I suspected there was some confusion it couldn't surmount about string manipulation. So I changed the question to something challenging, yet something that only used function calls, conditional logic, basic math ops, and numbers. First, I asked for an nth root approximator using newton's method. Didn't work. Asked for just the square root. Didn't work. Finally, I asked for a function that prints a student's grade given their integer percentage. Not even. GPT-4 also persistently hallucinated the keyword BREAKING NEWS, which I think sounds like a pretty good keyword if Tabloid were to ever get error handling. The spooky part is that the almost all the solutions at face value would get partial credit. They had the right abstract approach, being familiar with reams of example approaches in natural language or programming languages. However, in each case, GPT-4, 4o, Claude all failed to produce something without syntax errors. I suspect this is the case because transformers do subgraph matching, and while on one end there are rich internal connections for all the problems I requested, on the other end there is nothing similar enough for it to even get a foothold, hence the biggest struggle being syntax. If the only barrier to executing Tabloid code (or other unseen languages) is more basic syntax training, then it excitingly suggests it just needs to learn the abstract concepts from leetcode scrapes once for every syntax it knows. Prior research has shown that grammar is easy for language models. When GPT-2 was made large enough, it went from babbling to grammatical sentences very early in it's training, and at that moment its loss plummeted. All tests conducted in temporary data mode so that this eval stays dark. reply silentdanni 4 hours agoparentClaude managed to write code successfully. ``` DISCOVER HOW TO square_root WITH x, iterations RUMOR HAS IT EXPERTS CLAIM guess TO BE x DIVIDED BY 2 DISCOVER HOW TO improve_guess WITH current_guess RUMOR HAS IT SHOCKING DEVELOPMENT (current_guess PLUS (x DIVIDED BY current_guess)) DIVIDED BY 2 END OF STORY DISCOVER HOW TO iterate WITH current_guess, remaining_iterations RUMOR HAS IT WHAT IF remaining_iterations SMALLER THAN 1 SHOCKING DEVELOPMENT current_guess LIES! RUMOR HAS IT EXPERTS CLAIM new_guess TO BE improve_guess OF current_guess SHOCKING DEVELOPMENT iterate OF new_guess, remaining_iterations MINUS 1 END OF STORY END OF STORY SHOCKING DEVELOPMENT iterate OF guess, iterations END OF STORY EXPERTS CLAIM number TO BE 16 EXPERTS CLAIM num_iterations TO BE 5 YOU WON'T WANT TO MISS 'The square root of' YOU WON'T WANT TO MISS number YOU WON'T WANT TO MISS 'is approximately' YOU WON'T WANT TO MISS square_root OF number, num_iterations PLEASE LIKE AND SUBSCRIBE ``` reply CapeTheory 2 hours agorootparentThis is consistent with my own experience that Claude is just downright better than ChatGPT. reply cowsaymoo 2 hours agorootparentSame, I've been pretty impressed as well and typically give Claude a shot. Sometimes I even pass their results back and forth in an LLM collab so they generate more diverse perspectives. However, this paper from 4 days ago shows that Claude can fall apart quickly in out of distribution tasks. If you ask opposite day questions, GPT-4 is weirdly strong at it (figure 2). https://arxiv.org/pdf/2307.02477 reply cowsaymoo 2 hours agorootparentprevAh bravo! What was the prompt and Claude model? reply carterdmorgan 3 hours agoparentprevGreat idea here. I wonder if there's potentially more demand for new programming languages now purely as benchmarks for LLMs, like you said? reply cowsaymoo 2 hours agorootparentMaybe they will take on that role too one day reply velcrovan 4 hours agoprevI wrote the Racket implementation, in case you want to be able to compile your Tabloid programs: https://github.com/otherjoel/tabloid reply abtinf 4 hours agoprevI would change BEATS/SMALLER THAN to “DESTROYS” and “HUMILIATED BY” reply boredemployee 2 hours agoparenthahaha laughed hard on this one. reply rob74 12 hours agoprevI really think the \"please like and subscribe\" that ends the program should also be printed out (with a link to the project's GitHub page to make it more... actionable). reply ChrisArchitect 13 hours agoprevSome more discussion from 2020 with author input: https://news.ycombinator.com/item?id=24578749 reply georgf 15 hours agoprevReminds me of ArnoldC[1] from a few years ago. [1] https://lhartikk.github.io/ArnoldC/ reply dools 6 hours agoprev> Before making Tabloid, I also created a ... boring and unpopular programming language, called Ink. That line killed me. reply Cthulhu_ 10 hours agoprevI couldn't believe and was SHOCKED to find out that this was a computer language! Please like and subscribe to learn more. reply pnut 2 hours agoprevhttps://codewithrockstar.com/ reply xbar 5 hours agoprevCompiler developers hate him. reply bincyber 7 hours agoprevWhoever built this is a bloody genius reply jollyllama 2 hours agoprevI miss the old headlinese. Slam, pan, rip. reply greasegum 3 hours agoprevSoftware engineers don't want you to know this one weird trick() reply can16358p 11 hours agoprevThis seems like a both fun/humorous and an educative project on programming language and interpreter design. Motivating, lovely. reply fjfaase 10 hours agoparentThat is exactly what I did for the 'A practical approach to parsing' workshop I gave at MCH2022 [1]. You can give it a try with the online IParse Studio [2], which has a simple build in interpreter, and if you are lazy or getting stuck, you can have a look at the grammar I wrote myself [3], which does not specify operator precedence yet. [1] https://fransfaase.github.io/MCH2022ParserWorkshop/ [2] https://fransfaase.github.io/MCH2022ParserWorkshop/IParseStu... [3] https://github.com/FransFaase/MCH2022ParserWorkshop/blob/mai... reply akasakahakada 2 hours agoprevThis is cursed as shit lol reply blackbaze 7 hours agoprevLooks like FORTH! reply trulyhnh 15 hours agoprev [–] Cute reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tabloid is a Turing-complete programming language designed to write programs in the style of clickbait news headlines.",
      "It features unique syntax such as \"YOU WON'T WANT TO MISS\" for print output and \"EXPERTS CLAIM...TO BE\" for variable assignment, with no built-in looping constructs, relying on recursion instead.",
      "The language has an interpreter written in JavaScript and a mostly compatible implementation in Racket, supporting numbers, strings, and booleans."
    ],
    "commentSummary": [
      "Tabloid is a clickbait headline programming language, ideal for benchmarking large language model (LLM) code generation, such as GPT-4.",
      "GPT-4 struggled with basic tasks in Tabloid, often hallucinating keywords like \"BREAKING NEWS\" due to syntax errors and lack of specific training.",
      "Claude, another language model, successfully wrote code in Tabloid, suggesting that more syntax training could improve performance in such niche languages."
    ],
    "points": 192,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1721002379
  },
  {
    "id": 40964924,
    "title": "The fascinating and complicated sex lives of white-throated sparrows",
    "originLink": "https://www.audubon.org/news/the-fascinating-and-complicated-sex-lives-white-throated-sparrows",
    "originBody": "Visit Birds Tell Us to Act on Climate page Birds Tell Us to Act on Climate Pledge to stand with Audubon to call on elected officials to listen to science and work towards climate solutions. Sign the Pledge",
    "commentLink": "https://news.ycombinator.com/item?id=40964924",
    "commentBody": "The fascinating and complicated sex lives of white-throated sparrows (audubon.org)148 points by laurex 16 hours agohidepastfavorite18 comments downboots 12 hours agoI see a scientist using a red band tag first and later in the video a silver-blue one for a different bird. Do they control for tag color when banding birds? https://www.cell.com/current-biology/fulltext/S0960-9822(15)... some birds are known for their color/visual preferences and the measurement could ostensibly be a cause for the results: a confounding variable like in those mice studies where male or female scent inadvertently affected rat behaviour. reply rattus_rattus 4 hours agoparentI suspect that it would be quite difficult to control for that, since my understanding is that they specifically use different color combos on each bird in a specific area. This lets the scientists easily ID the bird by sight or in a photo, vs. all silver bands with a stamped code. The latter would require them to re-catch the bird to ID it, defeating the purpose of observing behavior and IDing individuals as it happens. I would also guess that it isn’t really a factor in the birds’ behavior, though. reply zxexz 3 hours agorootparentYeah, I think this is 100% it but I’m not aware of any standards. I have spent a bit of time watching wildlife ecologists do bird banding at one site and they had a log of all the rings, so they could identify them by sight. Because there are limited color combinations, they segmented by species, sex for some species to make it easier. Common CS algorithms show up everywhere in the real world :) I wonder if different ecology/conservation groups coordinate their algorithms and databases. I participated once, but oh my god was it terrifying. The care with which one needs to remove the bird from the net and handle it is unnerving, especially with tiny songbirds. Weighing them is really cute. Massive respect for people who do it day in and day out. reply 1659447091 11 hours agoprevIt seems, to me, a similar dynamic to human pairings (at the edges). The more dominate aggressive females go for the more nurturing males, and vice versa. Then you're left with having pairings of dominate types who fight like 'cats & dogs', and never really know much about the more subdued pairings. reply compiler-guy 5 hours agoparentOne can certainly point to human pairings like this, but I don’t know of any studies that show it as a general trend or tendency. One can also point to power couples and quiet couples just as easily. reply UniverseHacker 5 hours agoparentprevI agree- humans have almost exactly the same dynamic. Our language doesn’t even have accurate words for it because it is culturally unacceptable to be a more (submissive? feminine?) man or a more (dominant? masculine?) woman yet it’s actually quite common and despite stereotypes it is a totally separate thing from gender or sexual orientation. The “wrong” pairings rarely work out in relationships. I believe many cultures and languages are more consciously aware of this- for example the Chinese concept of yin and yang being important in dating and relationships. reply space_oddity 6 hours agoparentprevIt's an interesting perspective on human relationships reply peanut_worm 4 hours agoprevHuh this is news to me. My field guides all mention the color variation but don’t mention differing behavior. One guide even incorrectly says its based on age (it is worth noting they do vary in plumage depending on age, but in a different way). Another bird that is sort of like how they describe in the article is the Ruff. It is a beautiful type of sandpiper where males are split into 3 categories with different appearance and behavior. reply nicgrev103 6 hours agoprevSimilar to Love Island ;) reply space_oddity 6 hours agoprevThere's a lot more going on behind those white throats than meets the eye reply pnemonic 2 hours agoparentah yes the walk down here to this comment was short but scenic reply A4ET8a8uTh0 8 hours agoprevSince we are talking bird and mating rituals, HN member once pointed out to me the fascinating bowerbird mating rituals[1] that result in them building fairly ornate structures showcasing their resource gathering skills. [1]https://blog.nature.org/2021/01/04/bowerbirds-meet-the-bird-... reply jamiek88 7 hours agoparentWow that really was fascinating! Went down a bit of a bower-hole there! The golden bower birds are incredible but they are all super interesting. reply worldvoyageur 8 hours agoprev [–] Two sexes (male, female), each with two variants (white throat, yellow throat), creating four genders (wM, yM, wF, yF). Males and females need to pair or there will be no new sparrows. However opposite variants also need to pair or for behaviour reasons there will be no successful young. For instance, wM-wF spend so much energy fighting with each other they fail to raise young. The y variants are less aggressive but better with young and are the most popular mate choices. The y-variants are also monogomous while the w-variants are not. The aggressive w-variants grab all the y-variants as partners. The result is that y-y pairs are rarely (never?) seen, even though they could produce young. The article notes that yellow throats have different food strategies from white throats. This will also tend to reinforce alternate gender pairing. An MF pair is going to bring in less food if both are going after the same sources (they might start fighting!) while if each has a different strategy the young are less vulnerable to food shortages should one strategy or another not produce at any given time. The four gender outcome seems to be in a sustainable equilibrium. If one is favoured, say wM-yF, then these sparrows would become like most other birds, with males having one colour pattern and females another. Instead, behaviours have somehow distributed themselves across genders so a successful couple needs to match both if they are to raise the next generation of young. reply bko 6 hours agoparent [–] From the article: > Within each gender, white-striped birds are more aggressive while tan-striped birds are more nurturing. That seemingly simple generalization is based on a vast amount of research. For many years, Falls and his students at the University of Toronto carried out highly detailed studies of White-throated Sparrows, showing how behavioral differences between the morphs touched every aspect of these birds’ lives. Why would you define this variation that's associated with behavior \"gender\"? Isn't the whole distinction between gender and sex is one is biological and the other societal? If there is a hereditary biological indicator that determines this behavior, shouldn't it fall outside of the gender concept? I don't know how useful it is to add these human constructs to the biological world when it could be easily observed and discussed without the confusion reply cauch 6 hours agorootparentOn top of what is said in the other comment (indeed, \"gender\" is perfectly fine, there is no real confusion), it's also worth noting that the \"more aggressive\" and \"more nurturing\" sparrows have a different color, and color is a biological trait. I think that the distinction between \"biological\" and \"cultural\" is a bit of a non-issue for the professionals. They don't know if it's cultural or biological, but it does not matter, and in fact, the reality is probably that untangling cultural from biological is impossible (not in the sense that the culture is a direct result of the biology, but in the sense that biological development does not care to make the distinction if the conditions that shape its direction are biological or cultural, and that a final trait is therefore the result of a complicated mix of the two). reply beowulfey 4 hours agorootparentprevGender is being used here as \"trait arising from genetic sex + behavior\". It's not the same as the human social construct, but it is analogous. It's also not commonly used in biology in this way from what I've seen, for what that's worth. reply mandmandam 6 hours agorootparentprev [–] > Why would you define this variation that's associated with behavior \"gender\"? People use imperfect analogies all the time. Also words can have multiple meanings, especially across different fields. No one is complaining that their electrical 'resistors' are confusing their understanding of non-violent resistance. > Isn't the whole distinction between gender and sex is one is biological and the other societal? In people. These are birds. See above about words having multiple meanings. > If there is a hereditary biological indicator that determines this behavior, shouldn't it fall outside of the gender concept? Why would it. A, applying the \"gender concept\" of humans to birds is silly. B, \"hereditary biological indicators\" and gender can overlap in humans too. > it could be easily observed and discussed without the confusion I don't see at all why it would be confusing, and no one else seems confused. The English language does this literally all the time. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Scientists use color bands to identify white-throated sparrows, aiding in behavior studies without the need for recapturing.",
      "The sparrows exhibit behaviors similar to human pairings, with white-striped birds being more aggressive and tan-striped birds more nurturing.",
      "The term \"gender\" in this study refers to traits from genetic sex and behavior, though this usage is uncommon in biology."
    ],
    "points": 148,
    "commentCount": 18,
    "retryCount": 0,
    "time": 1721012100
  },
  {
    "id": 40967069,
    "title": "I'm not a fan of strlcpy(3)",
    "originLink": "https://nrk.neocities.org/articles/not-a-fan-of-strlcpy",
    "originBody": "Home Articles Notes Projects I'm not a fan of strlcpy(3) 03 Jun 2022 strlcpy(3) is an OpenBSD function which is often hailed as a safer replacement for strcpy(3) and/or strncpy(3). One of the obvious issues with strlcpy is that it's not safe if src isn't null-terminated. However, that's not the reason I don't like strlcpy. In fact despite knowing this, I used to think it was an okay function and better than the (thought of) alternative strncpy (which doesn't null-terminate in case of truncation). The reason why I'm not a fan of it is because I've recently revisited this old thread where Ulrich Drepper rejects the proposal to add strlcpy to glibc. His reasoning was: strlcpy is inefficient (which is unarguably true). Anyone who has thought about the problem for a single second wouldn't use strlcpy (paraphrased). So I decided to spent a single second thinking about the problem and realized that yup, there's no reason to be using strlcpy. What exactly is the problem? In order to understand why a solution isn't good or optimal, one first needs to properly understand what the problem actually is. The problem we have in our hands is that we're trying to copy a \"c-string\" or a \"null-terminated string\" . This problem can be divided into two main categories: Cases where truncation matters. Cases where truncation doesn't matter. Strictly speaking, the C standard defines a string as: A string is a contiguous sequence of characters terminated by and including the first null character. So the term \"null-terminated string\" is redundant since if it isn't null-terminated, it's not a string according to the C standard. Truncation doesn't matter There are many cases where truncation doesn't matter. For example, if we're trying to display the result on some statusbar which can only hold, let's say 64 bytes, then not only is the src string getting truncated is not an issue, it's actually wanted since it makes no sense to copy more than that. So assuming buf is a fixed length array, char buf[64]; an strlcpy call would look like the following: strlcpy(buf, src, sizeof buf); If you spend a single second thinking about what you've just done, you'd realize that this is a horribly inefficient solution to the problem at hand. Since strlcpy returns strlen(src), and src might very well be let's say a thousands byte long. By using strlcpy, you are going though all of those bytes in a O(n) strlen call for absolutely no reason when all you really cared about is the first 64 bytes of src. Similarly, strncpy(3) is also not a good solution in this case because: It doesn't null-terminate in case of truncation. It unnecessarily pads the rest of dst with 0 in case src length happens to be less than dst length. A much better solution here is using memccpy(3) instead, which will only scan the first 64 bytes of src at most and nothing past that: if (memccpy(buf, src, '\\0', sizeof buf) == NULL) buf[sizeof buf - 1] = '\\0'; /* truncation occured, null-terminate manually. */ And in case you're worried about this being two-liners as opposed to a one-liner strlcpy call (which is a valid criticism, as that can open up room for making mistake), then you can (and should) simply turn this two liner into a wrapper function. Optionally giving it a _trunc prefix to make it clear that truncation is desired here. mystrcpy_trunc(buf, src, sizeof buf); Not only is this more efficient, it makes it clear that truncation is desired here just by looking at the function name. Whereas in the case of an unchecked strlcpy call it's not immediately obvious weather truncation is desired or the programmer forgot to check for it. Truncation matters In cases where truncation matters, for example file-path, url etc, you simply SHOULD NOT be using a fixed size buffer. You have to be able to do dynamic allocation and grow the buffer as needed. The most simplest way of achieving this is just using strdup(3), which will do the allocation for us: char *s = strdup(src); if (s == NULL) return ERROR; /* do stuff with s */ free(s); /* don't forget to free */ And in case you already have an allocated buffer and you'd like to reuse it, the following is what an strlcpy usage would looks like; assuming char *dbuf is a buffer that's been allocated size_t dbuflen bytes via malloc(3): size_t srclen = strlcpy(dbuf, src, dbuflen) + 1; if (srclen > dbuflen) { char *tmp = realloc(dbuf, srclen); if (tmp == NULL) { free(dbuf); return ERROR; } dbuf = tmp; dbuflen = srclen; strlcpy(dbuf, src, dbuflen); } Let's ignore just how inefficient this is for the moment, instead I'd like to point out just how schizo this entire logic is: \"Hmm yes, let me just start copying this string without knowing if it's going to fit inside my buffer or not.\" \"Oh shit, it didn't fit. Let me just readjust the buffer and restart the copy entirely.\" A much better solution here is to just use strlen(3) and memcpy(3): size_t srclen = strlen(src) + 1; if (srclen > dbuflen) { void *tmp = realloc(dbuf, srclen); if (tmp == NULL) { free(dbuf); return ERROR; } dbuf = tmp; dbuflen = srclen; } memcpy(dbuf, src, srclen); This version is far more efficient than the strlcpy version. But more importantly, the logic here is much more sane and natural: Let's find out how big the src is. If it doesn't fit then grow the buffer. Then copy everything over. Truncation matters, but I'm stuck with a fixed size buffer Let's not talk about how you got yourself into this situation, but instead let's talk about what you can actually do here. And pretty much the only thing you can do here, is to error out if src happens to be bigger than your buffer. The following would be the strlcpy implementation: if (strlcpy(buf, src, sizeof buf) >= sizeof buf) return ERROR; /* or call exit/abort if appropriate */ And once again, this is horribly inefficient because we don't want to know the exact length of src, we only want to know if it's bigger than our buffer size or not. A much better way to do that is, once again, using memccpy. Which is not only much faster than strlcpy, it actually makes much more logical sense for what we're trying to do, as it won't scan src for more than sizeof buf: if (memccpy(buf, src, '\\0', sizeof buf) == NULL) return ERROR; /* truncation */ B-but using mem* functions on string is... bad! The mem and str prefix is often a source of confusion for amateur programmers who get tricked into thinking that you MUST use only str* functions on strings. In case you've forgotten what a (c-)string is, it's nothing more than an array of bytes terminated by a null character. Any attempt of trying to think of them as anything else is going to result in error sooner or later because the programmer has a fundamental misunderstanding of what a string even is. Moreover, where exactly are these mem* functions declared? Let's take a look at the manpage: $ man 3 memccpy SYNOPSIS #includeNow let's take a look at what the C standard has to say about : The header string.h declares one type and several functions, and defines one macro useful for manipulating arrays of character type and other objects treated as arrays of character type. In other words, there's nothing \"improper\", \"bad practice\", \"code-smell\" or whatever with using the mem* family of functions for operating on strings, because strings are just an array of (null-terminated) characters afterall. One other criticism of this article might be that memccpy(3) and strdup(3), while defined in POSIX, is not defined in the ISO C standard . But this argument makes no sense because strlcpy(3) is defined in neither, and in practice is less portable as well. Both memccpy and strdup have been accepted into the C2x draft. Conclusion In the end, I'm unable to find a situation where strlcpy makes sense. In pretty much all the cases, I'd much rather use memccpy, memcpy or strdup instead. So needless to say, I'm not a fan of strlcpy. Related articles About a week after writing this article, I came across the following related articles, all of which are good reads: strcpy: a niche function you don't need Toward more efficient string copying and concatenation The sad state of C strings RSS Feed",
    "commentLink": "https://news.ycombinator.com/item?id=40967069",
    "commentBody": "I'm not a fan of strlcpy(3) (nrk.neocities.org)135 points by signa11 6 hours agohidepastfavorite211 comments nine_k 5 hours agoI'd say that in general any format of variable size that does not state its length before the variable part is provoking buffer-overflow bugs. Thus such formats should be avoided in any kind of binary interop, This holds even if the total length of the data in an interaction is not known ahead of time. E.g. an audio stream can be of indeterminate length, not known when the first byte is sent over the network, but each UDP packet has a well-determined length given in the header. The length field can be made variable-size in a rather fool-proof way [1], allowing to economically represent both tiny and huge sizes. (Zip files, WAD files, etc have that info at the very end, but this is because a file has a well-defined end before you start appending to it; fseek(fp, 0, SEEK_END) can't miss.) [1]: http://personal.kent.edu/~sbirch/Music_Production/MP-II/MIDI... reply queuebert 2 hours agoparentNiklaus Wirth has entered the chat. reply Tuna-Fish 1 hour agorootparentThere are a whole bunch of other things I dislike about Pascal, but in this one he was just undeniably correct. Worrying about 1-3 extra bytes (on the 16 and 32 bit platforms where it potentially mattered) was just not worth all the issues that null-terminated strings brought with them. My current favorite string implementations are the various compact string crates for Rust. Generally, you want a string to be able to do at least three things: - Pointer, length, capacity tuple, for heap-allocated strings, 24B on x64. - String inlined into the 24B buffer. - Pointer, length tuple where pointer points to rodata. You can do any of that and the discriminator in 24B, given the healthy assumption that all strings are shorter than 2^63. Sadly, switching costs are massive and every programming language is pretty much struck with the string they started with. Hopefully, whatever comes next can crib from smartstring or compact_str or the like. reply gumby 4 minutes agorootparentThere was a paper on this in the late 70s from the Cedar group at PARC. This was back when computer science papers were actual scientific papers, so full of analysis of different alogorithms' performance with counted vs delimited strings. Counted strings won hands down on anything but strings so short the length was a large percentage of overall size. Yet...since nobody reads the literature, we have all continued to suffer. reply ThreatSystems 13 minutes agorootparentprevUp to reply EPWN3D 4 hours agoprevThe point of strlcpy(3) is not to be the world's best and most efficient string copier. It's to provide a drop-in replacement to previous, memory-unsafe string copy routines in constrained environments where you have to have bounds on stuff and might not have an allocator. If there are bugs with truncation in the resulting buffer, those are the program's bugs, and they existing before strlcpy(3) came into the picture. reply foobiekr 37 minutes agoparentThis. I don't understand the objection and I spent 20 years writing C code. The reason to use strlcpy is not to _fix the bug_ but rather _to prevent the bug from turning into a crash, memory corruption, or exploit_. It also forces discipline by carrying around the length. As you say, it's also a drop-in. A truncation bug is a hell of a lot easier to debug than memory corruption. reply rini17 5 minutes agorootparent> It also forces discipline by carrying around the length. LOL. It does not force anything - you can mishandle source or destination buffer lengths very easily and compiler won't say anything. I sometimes wonder what kind of disaster will have to happen to make C programmers agree on a standard buffer (i.e. pointer+size) type with mandatory runtime bounds enforcement .... reply foobiekr 3 minutes agorootparentForce is too strong a word. Yes, it's possible someone just passes whatever, or just passes strlen(s) which is an even dumber answer. reply mikewarot 3 hours agoprevIf someone could port the Free Pascal string library to C, it would solve a lot of problems with new C code. It reference counts and does all the management of strings. You never have to allocate or free them, and they can store gigabytes of text. You can delete from the middle of a string too! They're counted, zero terminated, ASCII or Unicode, and magic as far as I'm concerned. Oh... And a string copy is an O(1) operation as it only breaks the copy on modification. Edit: correct to O(1), thanks mort96 reply foobiekr 7 minutes agoparentFor most strings, it seems to be that using a varint would solve the overhead problem. For short strings the overhead would be no longer than the null byte (which you could discard, except when interacting with existing APIs). But as with _all_ string solutions, it's the POSIX interface, standard library, and other libraries that screw you. If you're programming in C today, it's because you're integrating with a ton of C code, and thus it's impossible to make progress since the scope of improvement is so small. It's always struck me as weird that Rust treats strings the way it does - the capacity value is not useful for many cases of strings, and it would have cost them one bit to special case the handling constant strings without the cap measure, which would be better. Most strings are _short_ which makes the overhead worse, proportionally. reply theamk 2 hours agoparentprevPascal strings have overhead of 2 ints per string (16 bytes on 64-bit systems) The kind of person who calls a single pass through the string a \"horribly inefficient solution\" will faint at the idea of burdening every string with 16 more bytes of data. reply adgjlsfhk1 1 hour agorootparentit's pretty trivial to implement this as a max of 14 byte overhead (with small string optimization), but more importantly, it's only 16 bytes on 64 bit systems, which pretty much by definition aren't that memory constrained (since otherwise you would be on 32 but). reply aidenn0 1 hour agorootparent> ...it's only 16 bytes on 64 bit systems, which pretty much by definition aren't that memory constrained (since otherwise you would be on 32 but). I'm not sure about that. There are plenty of 64-bit systems with less than 4GB of RAM reply btown 41 minutes agorootparentIncluding your laptop if you have a few Electron apps open! reply pjmlp 1 hour agorootparentprevFor some reason Multics got a higher security score from DoD than UNIX, guess why. reply wruza 1 hour agorootparentprevThis is a purely psychological problem. I’d say most of C is psychological, not technical. If I were a world dictator, one of my first orders was to lock C developers in a room with only python for few months. Or ruby, in severe cases. Some of them really need to touch grass. reply mbivert 1 hour agorootparent> If I were a world dictator, one of my first orders was to lock C developers in a room with only python for few months. Or ruby, in severe cases. I would additionally do the exact opposite: lock Python & Ruby developers in a room with only C for a few months. C is a great language to learn programming, but Python or Ruby are, nowadays, in the most cases, better languages to program with. For example, C's sharpness is a notoriously famous source of bugs; yet it forces to develop rigor, discipline. reply arethuza 3 hours agoparentprevAs an aside: can you have an O(0) operation that actually does anything? reply mort96 3 hours agorootparentIt doesn't really make sense within the context of complexity analysis as something distinct from constant-time, which is denoted with O(1). A copy of a CoW string is O(1). reply dhosek 2 hours agorootparentThis. Pretty much with complexity analysis, you factor out any constants and only look at the term of complexity with the highest growth rate, so you end up with 1Nobody’s surname needs 128+ bytes 128 bytes would only be 42 characters if each character uses 3 bytes, as would be the case in some languages. Which isn't an unreasonable length, especially if the name has a lot of combining characters. reply saagarjha 5 hours agoparentprevPeople have long surnames, especially if you take multi-byte characters into account. And someone with the same mindset as you is the reason why my profile picture in Google’s employee directory never did actually show up in Safari–one of the path components ended up being a couple thousand characters long (I wonder if it was literally a base64 encoding of the image itself?) reply CoastalCoder 3 hours agorootparentOut of curiosity, would you mind sharing some details about your surname? E.g., its length in its native alphabet, or its length as a UTF-8 string? reply nathell 4 hours agoparentprev> Nobody’s surname needs 128+ bytes. https://en.wikipedia.org/wiki/Hubert_Blaine_Wolfeschlegelste.... would beg to differ. reply psadauskas 2 hours agorootparentA couple jobs ago, I worked on writing an API client for a CRM system that supported 2GB for most of the text fields (name, address line 1, job title, etc...). It also offered up 99 \"custom field\" text fields, also allowing up to 2GB each. I'd considered base64-encoding my ripped DVD collection, and using them to store another backup copy for me. reply CoastalCoder 3 hours agorootparentprevI love that the guy's occupation was \"typesetter\". reply mort96 3 hours agorootparentprevThat surname is like 22 bytes reply fmbb 2 hours agorootparentAdolph Blaine Charles David Earl Frederick Gerald Hubert Irvin John Kenneth Lloyd Martin Nero Oliver Paul Quincy Randolph Sherman Thomas Uncas Victor William Xerxes Yancy Zeus Wolfeschlegel­steinhausen­bergerdorff­welche­vor­altern­waren­gewissenhaft­schafers­wessen­schafe­waren­wohl­gepflege­und­sorgfaltigkeit­beschutzen­vor­angreifen­durch­ihr­raubgierig­feinde­welche­vor­altern­zwolfhundert­tausend­jahres­voran­die­erscheinen­von­der­erste­erdemensch­der­raumschiff­genacht­mit­tungstein­und­sieben­iridium­elektrisch­motors­gebrauch­licht­als­sein­ursprung­von­kraft­gestart­sein­lange­fahrt­hinzwischen­sternartig­raum­auf­der­suchen­nachbarschaft­der­stern­welche­gehabt­bewohnbar­planeten­kreise­drehen­sich­und­wohin­der­neue­rasse­von­verstandig­menschlichkeit­konnte­fortpflanzen­und­sich­erfreuen­an­lebenslanglich­freude­und­ruhe­mit­nicht­ein­furcht­vor­angreifen­vor­anderer­intelligent­geschopfs­von­hinzwischen­sternartig­raum Sr. reply riehwvfbk 2 hours agorootparentprevThat's the shortened version. The full version of the surname is 666 characters. reply mort96 2 hours agorootparentI apologize, I should've read more of the article. My bad reply dgfitz 3 hours agorootparentprevI'm not sure his name is long enough for this to be an issue. reply Izkata 2 hours agorootparentThe URL uses a shortened version, his full name on the page is: Adolph Blaine Charles David Earl Frederick Gerald Hubert Irvin John Kenneth Lloyd Martin Nero Oliver Paul Quincy Randolph Sherman Thomas Uncas Victor William Xerxes Yancy Zeus Wolfeschlegel­steinhausen­bergerdorff­welche­vor­altern­waren­gewissenhaft­schafers­wessen­schafe­waren­wohl­gepflege­und­sorgfaltigkeit­beschutzen­vor­angreifen­durch­ihr­raubgierig­feinde­welche­vor­altern­zwolfhundert­tausend­jahres­voran­die­erscheinen­von­der­erste­erdemensch­der­raumschiff­genacht­mit­tungstein­und­sieben­iridium­elektrisch­motors­gebrauch­licht­als­sein­ursprung­von­kraft­gestart­sein­lange­fahrt­hinzwischen­sternartig­raum­auf­der­suchen­nachbarschaft­der­stern­welche­gehabt­bewohnbar­planeten­kreise­drehen­sich­und­wohin­der­neue­rasse­von­verstandig­menschlichkeit­konnte­fortpflanzen­und­sich­erfreuen­an­lebenslanglich­freude­und­ruhe­mit­nicht­ein­furcht­vor­angreifen­vor­anderer­intelligent­geschopfs­von­hinzwischen­sternartig­raum Sr. reply xyst 1 hour agorootparentActive Directory would cry. reply mattigames 21 minutes agorootparentprevHe should have added a few numbers in there to really throw off some login systems. reply mjevans 1 hour agoparentprevNames are hard, don't force things into your assumptions. If you must, record fields such as: Full Legal Name - Freeform input no string input limitation. If you feel like this is an attack vector, send the data out for human review. Full Mailing Address - Don't try to break this down, allow multi-line, free form input. This is something you might want to validate with your shipping carrier and/or a human. A short 'nickname' used as such. reply cjs_ac 5 hours agoparentprev> Nobody’s surname needs 128+ bytes. This attitude ensures that the US software industry will never conquer the world. reply sgt 5 hours agorootparentThe US software industry has already conquered the world, and continues to do so. It doesn't mean that it'll always be this way, but if you wanted to associate software with a single country, then the US would be your answer. reply shrimp_emoji 3 hours agorootparentprevWhy does it need to conquer the world? I just want to deal with sane, performant software. reply estebank 1 hour agorootparentAnd people in other locales want to be able to use their own name without software mangling it. reply gav 1 hour agorootparentAt some point there is a physical limitation, there's no passport in the world that accepts a 666-character name. The US only gives you 21 characters on the DS-11 for a surname. reply shrimp_emoji 11 minutes agorootparentprevThat sounds like a them problem. ;D I once heard that \"decision\" comes from a Latin root word meaning \"to cut off (the other options; to pay opportunity cost)\". I will decide to optimize for my use case. reply bloak 4 hours agoparentprev> Nobody’s surname needs 128+ bytes. According to Wikipedia some joker did have a 666-character surname, officially, in the USA. Perhaps the best thing to do would be to truncate the field at some reasonable limit to prevent people from crashing your system with ridiculous values but make sure your system works properly with the truncated names so, for example, it doesn't panic because the truncated name isn't equal to the original name. With spaces, punctuation and diacritics, comparing even short names for equality is a bit dangerous and probably best avoided. If you expect two text fields to match and they don't, even after normalisation, you could consider flagging the case for human review later but continuing without an error for the time being. reply saghm 2 hours agorootparentIf I'm understanding correctly what you mean by \"joker\", it sounds like the changed their name to be this long intentionally? That seems like the type of thing where most software probably can get away with just not supporting then; with the possible exception of mandatory government services or something, there's no reason software should need to account for people taking such extreme steps of their own volition. reply mynameisvlad 2 hours agorootparentBecause programmers are too lazy to properly handle long names? That's a stretch for denying someone service and you know it. Like, yes, nobody is forced to accept their name unless they're running a government service, but using it as an excuse is just that, an excuse. reply jkaptur 1 hour agorootparentAny system will have some limit on the length of names - if nothing else, the budget for storage. A non-lazy programmer will determine an appropriate limit, document it, continuously test that the entire system can handle that length correctly, and continuously test that helpful errors are returned when too-long names are input. reply david422 20 minutes agorootparentprev> nobody is forced to accept their name unless they're running a government service I dunno, would you expect that the government should be allowed to dictate how long a person's name can officially be? If yes, then problem solved, nobody may have names longer than X, and all services will accept X. If no, then there has to be a practical limit on name sizes that government services can accept, and people will be unhappy because it doesn't accept their \"official\" name. reply chihuahua 49 minutes agorootparentprevWhat if my legal name is 500 trillion characters long? Should every project design their storage system to accommodate this? If you look at the 666-character name, it's no more or less ridiculous than 500 trillion characters. reply saghm 1 hour agorootparentprevTo be clear, I'm not arguing for or against a specific value as the \"maximum length\" of a name. I'm drawing a distinction here in terms of a potential user's intentional choices and what that means for providing support. > Because programmers are too lazy to properly handle long names? That's a stretch for denying someone service and you know it. I don't think someone should be denied service if they happen to have a long name, but I genuinely don't think it's a stretch not to try to handle people going out of their way to subvert expected norms. In this case, the argument is more philosophical than technical because there isn't an obvious heuristic for determining whether a name is intentionally made long or not, but there are places where I do think it's worth it for programmers to consider. As an aside, I'd argue there's more nuance than \"properly handling long names\" or \"being lazy. There's already an inherent limit on how large a name can fit into memory, and that limit might even fluctuate depending on the number of users being processed in a server at a given time and how much memory a given server has. Is a 1 GB name too long to be worth handling, or is not handling it \"laziness\"? If you're arguing that any name that the government accepts should be accepted by any software, how do you know what the limit is that the government will accept? If you have international customers, is the limit larger? If there's no documented limit, do you just need to hope your software is at least as robust as the government's? My point isn't that these situations are equivalent to a name that's 666 characters long, but that arguing that not handling 666 characters is lazy already is a blend of implicit technical assumptions (servers have enough memory that handling names with 666 characters isn't an issue) and social assumptions (it's possible for someone to actually have a name that long), and I don't think that \"pretend all names can fit into memory fine and just crash or time out or something if there are too many names that are too long according to the parameters of the runtime and the hardware\" is the obvious best choice from a fairness perspective. reply aftbit 2 hours agorootparentprevHe allegedly told the utility company that he wouldn't pay his bill unless they spelled his name correctly, which caused them to print it on three lines. Maybe this guy is just a walking software test case? reply KptMarchewa 2 hours agorootparentprevYou can deal with jokers: https://find-and-update.company-information.service.gov.uk/c... reply bear8642 2 hours agoparentprev> I'm not a fan of Unix man page section numbers in parentheses. Why not? Confusion if function call with section number as argument? reply shrimp_emoji 10 minutes agorootparentI once learned what they mean. But I forgot and now do not. reply dark-star 5 hours agoparentprev> Nobody’s surname needs 128+ bytes. No reasonable URL for a firmware update download needs 4096 bytes. ...and surely the \"seconds\" field of a timestamp is always between 0 and 59 inclusive, addresses will include a state and a building number, phone numbers contain only digits and maybe a leading + sign, etc. Wrong assumptions like this are one of the root causes of (in the best case) bad UI or (worst case) annyoing bugs. 128 bytes for a surname is only about 60 unicode characters, less if you include RTL markings and characters outside the BMP. A URL can contain SHA hashes (think: reproducible builds) and can thus be very long (okay, 4k characters is pushing it quite a bit but I wouldn't rule it out like you did...) reply bigstrat2003 3 hours agorootparentThere have to be limits somewhere. Memory and storage space is not infinite. On the other hand, \"how many characters can someone have in their name\" is infinite. That means that no matter what limit you choose, someone will eventually exceed that limit. And you have to have a limit. This is not a matter of \"wrong assumptions\". At the end of the day, all you can do is set a limit such that you're comfortable with the risk that someone will be outside the limit you have set. And risk tolerance, as always, is a matter of opinion and not fact. reply margalabargala 3 hours agorootparentThis is a poor argument. Firstly, \"how many characters can someone have in their legal name\" is decidedly not infinite, because it has to be sufficiently short that some governmental entity was willing to record it. Secondly, as a reply to a comment (quite reasonably) pointing out that 60 unicode characters may not be enough for a surprisingly large number of people, this makes even less sense. Memory and storage space are not infinite, but 128 bytes per name is still unreasonably low. One could buy a single 12TB hard drive and store the names of every single living human, allocating 1.5KB per person. reply jquery 50 minutes agorootparent> it has to be sufficiently short that some governmental entity was willing to record it. How short is that, exactly? reply smaudet 4 hours agorootparentprev> Wrong assumptions like this Assumptions, or features? I'm all for inclusive behavior, however I'm also for well tailored solutions. Having support for 8k characters when you are going to usually use maybe 20 isn't smart or correct either. That's why we have utf-8, not utf-32, you can grow the bytes when you need to, but only then. > A URL can contain SHA hashes It can, or it can not - again, perhaps a feature, not a bug. The hash can also live in a file named by convention, and downloaded/checked separately. Maybe there are other scenarios where you might need a really long url, but domain + release path + name + major.minor.patch should get you 99% of the way. What's \"reasonable\" is relative, always designing for the edge case is good in some cases, but its also OK (and perhaps better) to optimize on occasion. reply dhosek 2 hours agorootparentprevWhen I was in college, the only number in my mailing address was the zip code. reply bell-cot 5 hours agoparentprevTHIS. Not for every use case, obviously. But for huge number of them. And the \"FooBaz length 5307 bytes in check_input(), truncating to 4095 bytes...\" errors (which are trivial to log, or ignore, as you wish) can reveal many interesting things. reply morpheuskafka 2 hours agoparentprev128 bytes is only 4 32-bit characters. Now, I think 4-byte UTF-8 characters are pretty rare, but at least 3-byte ones are certainly common in names, even legal names. If you allow users to type emojis in their name you will definitely run out as the color/gender selectors take up an additional code point. reply cpburns2009 2 hours agorootparentYou confused bits with bytes. 128 bytes can encode 32 4-byte long characters. reply ktpsns 6 hours agoprevGiven how nice system programming languages we have these days, I refrain to let classic Null-terminated C-Strings entering my program. Even on embedded programming we opt-in for std::string (over Arduino's String). I am just happy to save our time in favour of having some X percentage less optimal code. reply tialaramex 6 hours agoparentIt is seriously unfortunate that C++ managed to standardize std::string, a not-very-good owning container type, but not (until much, much later) std::string_view, the non-owning slice type you need far more often. Even if Rust had chosen to make &str literally just mean &[u8] rather than promising it is UTF-8 text, the fact &str existed in Rust 1.0 was a huge win. Every API that doesn't care who owns the textual data can ask you for this non-owning slice type, where in C++ it had to either insist on caring about ownership (std::string) or resort to 1970s hacks and take char * with zero terminated strings. And then in modern C++ std::string cares about and preserves the stupid C-style zero termination anyway, so you're paying for it even if you never use it. reply roelschroeven 5 hours agorootparent> And then in modern C++ std::string cares about and preserves the stupid C-style zero termination anyway, so you're paying for it even if you never use it. I don't think this in itself is a real problem. You pay for the zero at the end, which is not much. The real cost of zero termination is having to scan the whole string to find out the size, which with std::string is only needed when using it with C-style APIs. reply wmanley 5 hours agorootparentand also you have to copy (probably allocating) to get a substring. reply account42 3 hours agorootparentOnly if you want a substring with separate ownership though - a string_view doesn't have to be NUL-terminated. reply jabl 1 hour agorootparentIf you want to pass the string_view to some API that expects NULL terminated strings, then a copy is necessary (well, maybe is some cases you can cheat by writing a NULL in the string and remembering the original character, and then after the API call restore the character). This isn't as much a fault of a string_view type of mechanism, but rather API's wanting NULL terminated strings. Which are kind of hard to avoid on mainstream systems today, even at the syscall interface. Oh well.. reply bobmcnamara 3 hours agorootparentprev> And then in modern C++ std::string cares about and preserves the stupid C-style zero termination anyway, so you're paying for it even if you never use it. Is this required now? I've seen a system where this was only null terminated after calling .c_str() reply nmeofthestate 2 hours agorootparentc_str has to be constant complexity, so I guess the memory needs to be already allocated for that null character. I'd be surprised to see an implementation that doesn't just ensure that \\0 is there all the time. reply mjevans 1 hour agorootparentprevGolang's slices view of the world is addictive. reply pasc1878 6 hours agoparentprevDepending on your usage it is not necessarily less optimal either. You never need to walk the string to find the \\0 byte. e.g. for strlen. For short strings no heap memory needs to be allocated or deallocated. reply account42 3 hours agorootparentIt's really too bad though that the short string optimization capacity is neither standardized nor user-controlled. reply kazinator 6 hours agoparentprevIf someone ever has to use your stuff over FFI from a high level language, they will curse you for not just using C strings. reply gpderetta 4 hours agorootparentNull terminated C strings are still terrible for FFI. Pointer and length is a better solution and it is trivially interoperable with code that uses, say, string_view. reply raverbashing 5 hours agoparentprevExactly this It seems C is going around in circles while everybody else has moved on No, speed and \"efficiency\" are not a be-all, for-all. Safety is more important than that except in very specific cases. And even in these cases there are better ways and libraries to deal with this issue without requiring the use of \"official\" newer C functions that seem to still be half broken There's so much fiction regarding memory issues and limited memory issues and what to do if we hit limited memory issues when in practice terminate and give up is the (often enforced by the OS) norm. reply saagarjha 5 hours agorootparentI look forward to your solution to bridge these libraries to every other person’s slightly different implementation of the same library, which also has to talk to every other interface that cropped up over the last 50 years and takes null-terminated strings anyways. reply shrimp_emoji 13 minutes agorootparentprevI haven't moved on. :> C FTW! Bloated langs are cringe. reply Retr0id 6 hours agoprevTIL of memccpy() https://www.man7.org/linux/man-pages/man3/memccpy.3.html To be honest, every time I need to deal with strings in C I feel like I'm banging rocks together, regardless of approach. I try to avoid it at all costs. reply commandersaki 5 hours agoparentI can never remember the nuances of the 50 various string functions and which shouldn't be used. What I do remember is that virtually all string problems can be solved with snprintf/asprintf. reply saagarjha 5 hours agorootparentsnprintf is a worse strlcpy: not only does it need to call strlen if you pass it a string parameter, it also ends up in ??? territory if the string is long enough because its return type is int. reply astrobe_ 1 hour agorootparentThe printf family of functions also runs a mini-interpreter that has its cost, because its main use is interpolation (the % placeholders). Some compilers can substitute them for more efficient versions (e.g. printf without any % in the format string -> puts). I don't know if they can detect and substitute an snprintf with a \"%s%s\" format string. reply wruza 3 hours agorootparentprevAh, good old “what if my string exceeds two gigabytes” dilemma. reply Arch-TK 6 hours agoparentprevThe standard library string handling stuff is atrocious and it surprises me that wholesale replacement of that stuff isn't more common. reply VyseofArcadia 5 hours agoprevIt is a long, time-honored tradition to attempt to improve on flawed standard library functions with equally flawed functions. reply saagarjha 5 hours agoparenthttps://twitter.com/dril/status/473265809079693312 reply UncleMeat 1 hour agorootparentA great tweet, and often useful. But I don't think it applies well here. There is a long history of \"this time, the strcpy replacement will be safe\" followed by cve after cve after cve. At some point I feel that the response really should be to give up on trying to make c-style strings safe. reply Gibbon1 1 hour agorootparentThe problem is the function signatures of all the improved string functions are broken. You can never write a safe string function that takes two char pointers. You really want int str_try_copy(str_buffer *dest, str_slice *src) reply raverbashing 5 hours agoparentprevIf I proposed a new strxyzcpy function that only null-terminated the string when the length was not a prime number and that wiped your hard drive if the destination string, before the copy, contained the sequence 'xyz' in ascii I would be very afraid someone in the C committee would think it would be a nice idea to add it. reply riehwvfbk 2 hours agorootparentDoes the length take locale into consideration? reply Gibbon1 1 hour agorootparentFFS I had some code for a microcontroller break because locale. reply xigoi 3 hours agorootparentprevDon’t forget than if it’s invoked at the 38th second of a minute, the behavior is undefined. reply hansvm 3 hours agorootparentOh, are those 0-indexed seconds or 1-indexed? reply xigoi 3 hours agorootparent0-indexed if running on a Unix-like OS, 1-indexed otherwise. reply Filligree 1 hour agorootparentDoes Linux count as Unix-like? What if it’s NixOS? reply bluGill 2 hours agorootparentprevImplementation defined. reply Arch-TK 6 hours agoprevAlso relevant: https://nullprogram.com/blog/2021/07/30/ (it references this blog post and offers good solutions) reply pornel 5 hours agoprevC can add a whole alphabet if str?cpy functions, and they all will have issues, because the language lacks expressive power to build a safe abstraction. It's all ad-hoc juggling of buffers without a reliably tracked size. reply lanstin 3 hours agoparentThe types that C knows about are the types that the assembly knows about. Strings, especially unicode strings, aren't something that the raw processor knows about (as far as I know). At the machine level, it is all ad-hoc juggling of buffers without a reliably tracked size, until you impose constraints like only use length-prefixed protocols and structures. Where \"only\" is difficult for humans to achieve. One slip up and wham. reply pornel 2 hours agorootparentC with its notion of an object, TBAA, and pointer provenance is already disconnected from what the machine is doing. The portable assembly myth is a recipe for getting Undefined Behavior. C is built on an abstract machine described in the C spec, not any particular machine code or assembly. Buffers (objects) in C already have an identity and a semantically important length. C just lacks features to keep track of this explicitly and enforce error handling. Languages exist to provide a more useful abstraction on top of the machine, not to naively mirror everything even where it is unhelpful and dangerous. For example, BCPL did not have pointer types, only integers, because they were the same thing for the CPU. That was a mess that C (mostly) fixed by creating \"fictional\" types that didn't exist at the assembly level. reply lanstin 2 hours agorootparentYou must be thinking of c++? There is no object in C just structs which is just a little bit of organization of the contiguous memory. C exists to make writing portable CPU level software easier than assembler. It was astonishingly successful at this niche; many more people could write printer drivers. While ptr types may not formally exist in assembly, the variety of addressing modes using registers or locations that are also integers has a natural resonance with ptr and array types. I would say C precisely likes to mirror everything even where it is unhelpful and dangerous. The spirit is captured in the Hole Hawg article: http://www.team.net/mjb/hawg.html It is the same sort of fun one has with self modifying code (JIT compilers) or setting 1 to have a value of 2 in Python. ed: https://en.cppreference.com/w/c/language/object is what is being referred to. I'm still pretty sure in the 80s and 90s people thought of and used C as a portable low-level language, which is why things like Python and Linux were written in C. reply pjmlp 1 hour agorootparentC has objects in the context of how the Abstract C Machine is defined by ISO C standard, nothing to do with OOP. reply bluGill 2 hours agorootparentprevThe abstract machine of C is defined with careful understanding of what real CPUs do. reply pjmlp 1 hour agorootparentIn need of a copy of ISO C printout? reply pjmlp 1 hour agorootparentprevAssembly only knows about raw bytes, nothing else. reply aidenn0 1 hour agorootparentDepends on the assembly, but even most (all?) RISC instruction sets know about words (and probably half-words too) in addition to bytes. reply pjmlp 33 minutes agorootparentPairs, quads and octects of bytes. reply uecker 1 hour agoprevHere is my attempt at strings in C (and other stuff). https://github.com/uecker/noplate (attention: this is experimental and incomplete for trying ideas and is subject to change.) reply pton_xd 1 hour agoprevThe main issue, which the article covers, is that there's really two different operations you want with copying C strings. Do you want to copy and truncate, or just copy? Within that, do you want to manage your own allocation, or do you want that abstracted? There's too many decision points and tradeoffs to just neatly hide behind a single \"one true function\" for copying C strings. reply ashvardanian 2 hours agoprevAside from the NULL-termination requirements there is arguably another big design issue with libc strings. I believe the interfaces that may allocate memory - must give you an opportunity to override the allocator. Aside from the SIMD implementation quality and throughput on Arm, that was one of the key reasons to start a new library: https://github.com/ashvardanian/StringZilla/blob/91d0a1a02fa... Also not a huge fan of locale controls and wchar APIs :) reply commandersaki 5 hours agoprevstrlcpy() is now part of POSIX: https://sortix.org/blog/posix-2024/. reply ComputerGuru 2 hours agoprevI’m late to the party but this mostly rehashes already voiced concerns with all the existing “updated” strcpy functions. But what I was surprised to learn is that strdup wasn’t part of the C language spec (until now)! reply saagarjha 5 hours agoprevI’m always glad to see more people coming around to the fact that memccpy is the actual function they want, not these inefficient nonstandard garbage functions that are needlessly inefficient for no reason but that everyone flocks to anyways for “security”. reply SAI_Peregrinus 5 hours agoparentI just wish it also had a src_size. As it is, if dest is bigger than src, and src isn't null-terminated, it'll read past the end of src. You can make a MIN macro and use MIN(dest_size, src_size), but shouldn't have to IMO. reply wruza 2 hours agorootparentThat’s exactly my experience with most of C. You go through a history of half-assed changes and realize that you’re still halfway there in $current_year, wondering how many years will it take to fix any of these obvious flaws. Committee surely has its reasons but on the surface it seems completely non-practicing, to say the least. reply BobbyTables2 5 hours agoprevIn cross platform environments, it gets horrible when one does something like: #define strlcpy strncpy reply kragen 3 hours agoparentokay but that's deliberate sabotage reply stephencanon 5 hours agoprevstrlcpy is the worst C string routine except for all the others. reply forrestthewoods 1 hour agoprevC's string handling is an abomination. Null terminated strings is and always has been a colossal mistake. The C standard and operating systems need to be updated such that null-terminated strings are deprecated and all APIs take a string_view/slice/whatever struct. reply asveikau 5 hours agoprevI feel like this person went through an unnecessary and false tangent about how people are \"afraid\" of memcpy due to inexperience and missed the much more important criticism that arbitrary, naive truncation on a byte level doesn't play well with Unicode. reply parasti 5 hours agoprevOver time, for my needs, I've gravitated back to fixed-size buffers. There are many apps where it really doesn't matter that they handle any string ever without truncation. A string is too long and won't fit? Whoops, just use a shorter one. reply acuozzo 4 hours agoparentSame here. It has become easier and more defensible to do so today with gobs of RAM, even in the embedded space (somewhat). You no longer need to worry about running out of stack space, even in a deep call-stack with each function having its own little set of buffers for strings. reply coding123 1 hour agoprevReal men use memcpy reply mjevans 1 hour agoparentIf you know the length (and that it fits) mempcpy() , then add the terminator yourself to be sure. memcpy has the slightly annoying flaw of returning a copy of the dest argument (already known data) rather than the more useful new information of a pointer to the end of the set memory. #IFNDEF _GNU_SOURCE #DEFINE mempcpy(X, Y, Z) (void *)(memcpy(X, Y, Z) + Z - 1) #ENDIF // Not quite the same but close enough mempcpy, fails for Z = 0 reply snitty 1 hour agoprev\"I like all my string copy's equally.\" CUT TO: \"I'm not a fan of strlcpy(3)\" reply KerrAvon 4 hours agoprevTFA misses the entire point of strlcpy, which is to improve security by making your code less prone to common C programmer errors that are known causes of common exploits. The author’s suggested remedies reintroduce the potential for those vulnerabilities. reply Pesthuf 5 hours agoprevAnd that's why I'm not a fan of C for writing any kind of program: Every time you need to touch a string, which is an effortless task in any semi-modern language, you aim a trillion footguns at yourself and all your users. It's also pretty telling that every article that tries to explain how to safely copy or concat strings in C, like this one, only ever works with ASCII, no attempt whatsoever to handle UTF-8 and keep code points together, let alone grapheme clusters. No wonder almost all C software has problems with non-English strings... reply Murk 4 hours agoparentIn my opinion, and as a general rule. You can't really ever truncate a user facing string correctly because it depends on language specifics. Hence it is my suggestion not to truncate user facing strings - and in fact you may want to consider them as binary blobs. On the other hand, as a matter of practicability sometimes you may have to, but certainly avoid doing so before it hits the presentation layer, and know that it may not be language correct. There are many strings that are not user facing, which you expect to be of a certain nature, e.g. ASCII based protocols, and therefore you know what to do with them. So the multi byte situation and strcpy, or std::string, or any other \"standard\" string function isn't really relevant as it's some other libraries duty. the task of truncation and otherwise formatting UI strings is the preserve of the rendering layer(s). reply hgs3 2 hours agorootparentYes, truncating a user facing string requires more consideration regardless of programming language. For example do you truncate at the grapheme, word, sentence, new line, paragraph, or something else? How do you indicate truncation to the user, with an ellipsis perhaps? If you use an ellipsis is it appended to the original string or drawn by the GUI toolkit? Note that the Unicode grapheme cluster, word, sentence, and line break algorithms are locale specific. Now consider how often programmers casually truncate strings, even in high-level languages, without accounting for the locale. reply eadmund 4 hours agoparentprev> I'm not a fan of C for writing any kind of program C is okay — not fan-worthy, but okay — for one specific kind of program: a mostly-portable implementation of a real language in which to write every other kind of program. that’s not nothing: implementing a backend for every CPU-OS pair one wants to support is a pain, and a different skillset from writing, say, a web browser or text editor. I wonder how much C has held back the development of computing. reply shermantanktop 3 hours agorootparentC is fine. Yes, it has undefined behaviors and encourages subtle thinkos. But it’s a tool and sometimes it’s the right tool. It’s the C hackers who insist on using it for inappropriate applications who create the problems. Anything string-heavy and internationalized is a terrible place for C, even if you have spent 30 years dancing through the C minefield and are sure you’ll get it right this time. reply Zambyte 3 hours agorootparentprev> I wonder how much C has held back the development of computing. Lisp is older than C. It didn't just hold computing back, it actively walked backwards. reply hifromwork 3 hours agoparentprevWell, C doesn't really have strings, just pointers. Calling them \"strings\" is just an abstraction for us. They don't even have to be null-terminated (even though by convention they often are)! And this is precisely what I want, in some cases. I use C when I need a low-level byte wrangling code that tightly interfaces with the operating system, or when I can't afford to allocate memory at will (like all effortless string-handling languages do under the hood), or when I don't want any runtime and need my code to behave like a shellcode, basically, or... I guess C is still used because people find it useful. No need to be a fan of it. reply f1shy 3 hours agorootparent> I can't afford to allocate memory at will This was my mantra for the last 30 years. But now, even the tiniest thing has 1GB, linux and virtual memory. Except for very edge cases, this excuse is rapidly dying reply mrkeen 7 minutes agorootparentBut the waste adds up. There are a few projects at work whose integration tests I can't run on my work machine because it doesn't have enough ram (16GB) so I run them on my home PC instead. And that's even when I close IntelliJ, my web browsers, and kill all the background processes I can. reply cess11 3 hours agorootparentprevWhen I browse my local electronics web shops I see a lot of devices that couldn't boot a Linux kernel, some of which I wonder whether I could drive with a small consumer grade solar panel. reply mrkeen 3 hours agoparentprevIn defence of C, it should be oblivious to different string encodings (and I thought it was). Coreutils should continue to do their job, and not need to be recompiled against whatever encoding is in fashion. reply johnnyanmac 3 hours agorootparentThe standards, as usual for C, are paper-thin, doesn't even guarantee ordinal value for the characters specified (except 0-9) despite implementing strcmp. But GCC/MSVC do add some options on top of the standard, which includes some attempt at interpreting different encodings. Then again, I was surprised to find support for literal encodings in C23. So perhaps my knowledge was even more outdated than I anticipated. reply _gabe_ 4 hours agoparentprev> It's also pretty telling that every article that tries to explain how to safely copy or concat strings in C, like this one, only ever works with ASCII, no attempt whatsoever to handle UTF-8 and keep code points together, let alone grapheme clusters. Can you expand on this? Why does it matter to keep code points and grapheme clusters together in the case of truncation? If you’re already truncating the string, then you can just copy as many bytes as possible. Then, later when you interpret that string, you’ll hit a malformed codepoint and ignore it. I guess what you might be getting at is that if you have a codepoint sequence, then you shouldn’t copy the bytes if they can’t all fit in the truncated string? I feel like this is an edge case and not “the reason almost all C software has problems with non-English strings”. 99% of the time, copying up until the null byte is fine, whether or not the string is UTF-8 or ASCII. The reason most C software doesn’t work with non-English strings is because the developer never added support. The bytes are still there, they just need to be interpreted correctly in the UI portions of the code. reply Retr0id 3 hours agorootparentTruncating mid-codepoint produces an invalid utf8 sequence, which some decoders will silently ignore or replace withglyphs, and others will fail loudly. Truncating mid-grapheme-cluster can change semantics in unexpected ways. For example, the \"family\" emoji(s) can be encoded as a set of conjoined codepoints for each family member: https://stackoverflow.com/questions/49958287/printing-family.... Depending on where you truncate, you might just get the father, which probably won't cause you any serious issues but it might cause confusion depending on context. (IMHO, if you're truncating a way where this would matter, you should probably be truncating in screen-space, i.e., way higher up the stack) reply _gabe_ 3 hours agorootparentYep, this all makes sense and is what I was thinking as well. I also think you’re right about truncation being a better idea in the rendering state rather than in the bytes themselves. Either way, truncation of strings being displayed to the user requires extra care no matter what language you’re using. As far as this being the reason most C code doesn’t handle UTF-8 though, I’m still skeptical. reply taeric 4 hours agoparentprevHonestly, any language where you are manipulating strings is a footgun. Especially if you are manipulating text, and not character data. You can avoid many errors, but almost certainly not all. And as soon as you have someone that thinks they can piecewise localize any text, heaven help you. reply bruce343434 4 hours agorootparentWith the up and coming browser LLM API, you could just have the LLM translate everything on the frontend. One of the few tasks that LLMs are consistently good at. reply taeric 4 hours agorootparentI feel this successfully upgrades a gun into a canon? reply TeMPOraL 4 hours agorootparentA modern artillery gun, where each shell costs upwards of $1K apiece, and lord have mercy on you if it ever goes off inside the barrel. reply layer8 3 hours agorootparentprevLLMs are not “consistently good” at translating UI text, because a proper translation requires understanding the application and the application context of each piece of text. reply colejohnson66 2 hours agorootparentYes. Context is very important. Otherwise, you end up compressing files into a \"postcode file\".[0] [0]: https://news.ycombinator.com/item?id=36231313 reply wruza 3 hours agoparentprevNot that C was wise with strings, but slicing strings in general is a bad idea unless you’re doing it strictly at ascii borders, e.g. to capitalize windows disk letter or to split by cr?lf. Modern text is yet another “falsehoods programmers believe in” material. reply cyberpunk 5 hours agoparentprevI mean just use bstrlib why is it difficult? reply rwmj 5 hours agorootparentBecause no one can agree on which string library / abstraction to use, if they even use one at all which is rare. reply dark-star 5 hours agorootparentprevI mean just use C++ w/ std::string why is it difficult? reply bluGill 2 hours agorootparentMost C programs compile and work correctly compiled as C++. The exceptions are often places you shouldn't write code like that, sometimes even outright bugs that C++'s stricter type system caught for you. reply smallstepforman 3 hours agorootparentprevstd::u8string reply account42 3 hours agorootparentNo, std::string. reply Pesthuf 4 hours agorootparentprevI'd be happy if developers did that, but they don't: So many C developer seem to think they're \"smart enough\" to handle strings correctly using only the standard library, not needing \"training wheels\" like the plebs - but then where do the countless string related memory safety errors and all the broken Unicode handling come from that plague every C program, no matter how clever the programmers, no matter how tough the review process, no matter how much static analysis is used? The issue is that many C developers simply can't seem to admit that C string handling, as it was invented many decades ago, was simply fundamentally flawed. So easy to misuse with catastrophic results, even for plain 1-byte-strings. It has nothing to do with how \"smart\" you are or how careful you are. reply johnnyanmac 3 hours agorootparentAre there really people defending string manipulation in C? C is a slight abstraction on top of assembly. Assembly speaks in addresses and bytes. It's always been bad at trying to interpret human text because Assembly's job lies in the digital realm. I think it's more a case that programming is English skewed and localization concerns are rare for the programmers these day who still need to work in the realm of C (so, mostly for older legacy software or the embedded realm). reply bluGill 1 hour agorootparentprevSmart is not the issue. I'm smart enough, but I'm lazy and forgetful. C makes it really accidentaly once in a while. reply smaudet 4 hours agoparentprev> like this one, only ever works with ASCII, no attempt whatsoever to handle UTF-8 I mean if your viewpoint is to handle heavy UIs then yes maybe C is not your go-to language. However, there are other solutions to the character space problem besides UTF-8. Rather than complaining that your screwdriver can't bang nails, perhaps try using a screw? No, you won't be able to support a language like chinese within a 256 bit charset, but perhaps it's not important to in all situations. The strength of ascii is that its small, simple. If that's too anglo-centric for you, maybe there's a better symbolic tiny charset that is more applicable. I'd support something like https://lojban.io/ becoming more prevalent, but there could be advantages to having a simple(r) symbolic transmission language not burdened with anglo-centric concepts. reply anamexis 4 hours agorootparentAdvocating a new global constructed language to accomodate C's shortcomings seems like the wrong direction to be thinking. reply smaudet 3 hours agorootparentLess a global constructed language, more a \"better\" encoding. Base64 works really well for arbitrary binary-in-text encoding, for instance. reply estebank 1 hour agorootparentUnicode is that better encoding. The \"small and efficient per locale encoding\" that you proposed was the status quo, and was an endless source of mojibake. There is a reason we moved away from that. reply smaudet 21 minutes agorootparentI think there is a misunderstanding, which I tried to address but evidentally failed. UTF-8 is fine for a display encoding. However, not every string encoding need be a display encoding, which the parent post seems to not be considering. You could also have multiple display encodings, if it makes sense to (a tool only intended for use in a certain part of the world for instance), however that is not what I mean. reply saagarjha 5 hours agoparentprevThe code in the post does indeed have a bug if the goal is to copy UTF-8 data. However the bug is quite subtle and unlikely to occur in practice, and from your comment I suspect you don’t actually know what that problem is (because you seem to be pointing at a different issue which is actually irrelevant). Maybe you or someone else can describe what the real concern is? reply keybored 4 hours agorootparent> However the bug is quite subtle and unlikely to occur in practice, Unlikely to occur? Do you think that will put the naysayers at ease? > , and from your comment I suspect you don’t actually know what that problem is (because you seem to be pointing at a different issue which is actually irrelevant). Maybe you or someone else can describe what the real concern is? Amazing that people have the patience for this. reply Levitating 5 hours agorootparentprevPartially copying a 2-byte character? reply saagarjha 5 hours agorootparentUTF-8 works fine if you truncate a codepoint because the encoding scheme lets you detect this. The problem is more subtle than that (hint: it involves a 1-byte codepoint). reply lalaland1125 5 hours agorootparentTruncating a UTF-8 codepoint is not fine because most software is not tested with partially broken UTF-8 so international users will likely run into many bugs. Especially because concatenation is a very common operation so those sliced codepoints will be everywhere, including in the middle of text. reply saagarjha 5 hours agorootparentMorally I view “what do I do with my truncated string” to be a separate issue from “how do I truncate the string” as described in the article. Like, yes, you absolutely should not concatenate after doing this operation. But maybe you shouldn’t be showing the user a truncated string either even if it’s all ASCII. The question of “did you make an unparseable UTF-8 string” is answered with “no” and the more complicated but also more interesting question of “did you actually want this” remains unanswered. reply Levitating 4 hours agorootparentThis is fair, the article takes truncating a string to fit in a status bar as an example. reply actionfromafar 4 hours agorootparentprevAlso consider Unicode is not only international characters, but superscripts and other stuff ♥ᵃ a: there was a list somewhere over which characters hackernews allows? reply Retr0id 5 hours agorootparentprevIf you're alluding to NUL, I don't really see the issue? Yes, many languages allow strings (UTF-8 or otherwise) to contain null bytes, and C's str*() functions obviously do not, but null-termination vs not is an orthogonal issue to ASCII vs UTF-8. i.e. Yes it's (depending on context) an issue that C str*() cannot handle strings with embedded null bytes, but that's not a UTF-8-specific issue. reply Pesthuf 5 hours agorootparentprevA function that can turn a correctly formatted UTF-8 string into a malformed UTF-8 string is, in my opinion, broken. reply saagarjha 5 hours agorootparentOne problem here is that the string may not have been a correctly formatted UTF-8 string to begin with. No, not that it can contain any bytes-I mean, it might be ascribed even more than just decoding correctly. Maybe it is supposed to have the grapheme clusters preserved. Maybe the truncation should peel off the last file component because the string holds a path. The operation of “doing a dumb truncation” can be broken if you look at it from plenty of ways, and I don’t disagree with you, but I do want to make clear that the issue isn’t memcpy is breaking it but that if you need x, y, z maybe you’re reaching for the wrong tool. And conversely there is nothing inherently wrong with using it if you are going to use it in a way that is resilient to that kind of truncation. reply account42 3 hours agorootparentprevWhat about a function that can turn a correctly spelled english sentence into a malformed english sentence? If you truncate to a fixed length this comes with the territory. reply MathMonkeyMan 5 hours agorootparentprevThe null code point? That would be pedantic even by my standards. reply saagarjha 5 hours agorootparentLook I had to include it or someone is going to do a whole pedantic comment about how C can’t actually represent UTF-8 correctly reply kstenerud 4 hours agorootparentYou could have just said it rather than going through this smug \"I know something you don't know!\" song and dance. Also, by this rationale, NO string is ever safe in C, because pretty much every encoding technically supports codepoint 0 (even though you take your life into your hands should you ever use it). This is not a useful discussion. reply Sebb767 4 hours agorootparentprevActually, by just alluding to the bug without saying it explicitly, you managed to both be pedantic and not avoid the discussion. This is not meant as a personal attack; I just want to point out how it looks on a casual reading :) reply pdpi 4 hours agorootparentprevBy that metric, C can't represent ASCII correctly either, because there's no particular reason you couldn't have a NUL character somewhere inside a string. reply TeMPOraL 3 hours agorootparentIndeed it can't. Many developers were bitten by this, and still are; plenty of critical bugs and security vulnerabilities rely on this quirk too. reply account42 3 hours agorootparentTechnically, C can. It's just C strings that are limited. reply pdpi 3 hours agorootparentSure, in the exact same way that C can handle unicode just fine too. The problem is, as always, C strings. reply moralestapia 5 hours agorootparentprev>hint: it involves a 1-byte codepoint Which is? reply saagarjha 5 hours agorootparentYour sibling comments have figured it out if you’ve given up. Yes, it’s kind of stupid reply hgs3 6 hours agoprevThere is also strscpy [1] which behaves like the authors use of memccpy except the former doesn’t require manually passing the null terminator as an argument. [1] https://manpages.debian.org/testing/linux-manual-4.8/strscpy... reply oersted 5 hours agoparentC programmers like their overly concise naming schemes don’t they… How do you expect a user to guess the subtle differences in behavior between strncpy, strlcpy and strscpy? Sure screens were smaller before and you didn’t have autocomplete, but reaching out for documentation was also harder, and there was less of it. I guess “real programmers” memorize libc down to every detail and never use dependencies they didn’t write themselves. reply akira2501 32 minutes agorootparent> like their overly concise naming schemes don’t they One of the common features of \"modern\" languages that I can't comprehend is the love of single character sigils that completely change the meaning of a line of code or perhaps the entire enclosing function. > I guess “real programmers” memorize libc down to every detail Or, we just use 'man' and the massive 'section 3' of that system. A feature that no other language has managed to correctly replicate. reply acuozzo 4 hours agorootparentprev> C programmers like their overly concise naming schemes don’t they It's a holdover from 1970s-era linkers, many of which required each exported symbol to be unique within the first six or so characters. reply oersted 3 hours agorootparentThis is helpful, thanks. And I assume that no one imagined that you would need more variants of strcpy to begin with, strncpy is a pretty good name too, and then it's hard to break the pattern. Similar situations with lots of other names. I just wish that C style used fewer single char names and hard-to-decipher acronyms. Smart programmers can't read minds either... reply lanstin 3 hours agorootparentprevIn the 90s, as a working C programmer, I would make it a practice to read thru man 2 and man 3 from time to time, as well as to read all the options for gcc. The man pages were pretty good at covering the differences, if not all the implications of the differences. And to this day I strive to minimize the dependencies of my software - any dependency you have is an essentially unbounded cost on the maintenance side. I used to sign up for the security mailing list for all the dependencies, and would expect to have to follow all the drama and politics for the depenencies in order to have context on when to upgrade and when not too. With Go I don't do that, but with Python I sometimes still do. And I've been reading the main R support email list for ever. I still think that a periodic reading of lwn.net is an essential part of being a responsible programmer. I will also say, reading the article reminds me of the terror of strings in C without a good library and set of conventions, and why Go (and presumably all other modern languages) is so much more relaxing to use. The place I worked have everything length prefixed and by the end of the 90s all the network parsing was code generated from protocol description files, to remove the need for manually writing string copying code. reply oersted 3 hours agorootparentI was somewhat dismissive, but I agree that this way of thinking about dependencies is the right approach for systems programming. And it is fair to expect that users will read the manual in detail for any tool or library they adopt in the contexts where C is used. It's just a bit frustrating to deal with so many names that are hard to understand and remember. C-style naming forces you to refer to the docs more often, and the docs are usually more sparse and less accessible than in other ecosystems. Man pages are relatively robust and they were a delight back in the day, but they have not been the gold standard for decades, and the documentation conventions for third-party libraries tend to be quite weak. reply lanstin 1 hour agorootparentA distressing number of softwar engineers have overly accurate memories and don't notice when things become excessively cryptic or arcane. However the implementations being much more open source now means a lot of bad documentation can be overcome with code reading or, if needed, stepping thru the code with a debugger. Wrong documentation is still expensive. I have a bitter taste in my mouth from integrating with OpenTelemetry Go libraries. It seems to be sorted now in 1.27and q 28 but 1.24 and for a few versions the docs were wrong, the examples were not transferable, and it took 5x the time it should have. reply layer8 3 hours agorootparentprev> How do you expect a user to guess the subtle differences Software developers should never rely on guessing. Always, always read the specification. reply oersted 3 hours agorootparentYou are right, perhaps \"guess\" is the wrong term, I meant that it is unreasonably hard to identify and remember which is which. reply layer8 3 hours agorootparentUsually you settle on one or two “go to” functions, if you don’t write your own wrapper function anyway. Given the subtleties of their semantics, fully descriptive names also seem unachievable. But really, familiarity comes with repeated exposure. If you use these every day, you’ll learn rather quickly. reply oersted 3 hours agorootparentIt's a good point. To be fair, any system that lasts as long as C has will have legacy cruft that you need to learn to step around and it's hard to get rid of. Relative to other standards, C has been remarkably disciplined at keeping things clean. reply JohnMakin 3 hours agorootparentprev> How do you expect a user to guess the subtle differences in behavior between strncpy, strlcpy and strscpy? man (strxcopy) reply adrian_b 5 hours agoparentprevYes, but that is an internal function of the Linux kernel. It is available in user programs only if you define it yourself, e.g. by using memccpy. reply voidUpdate 5 hours agoprev [–] While the article itself is interesting, could they not have picked a less... offensive word in \"I'd like to point out just how schizo this entire logic is\"? Like strange, or weird, or unusual reply layer8 3 hours agoparentMeaning #2: https://www.merriam-webster.com/dictionary/schizophrenic Many words are used with different and derived meanings, usually disambiguated by context. For example, an offensive player is not offensive in the sense you used the word in. reply bigstrat2003 3 hours agoparentprev\"schizo\" is not an offensive word. reply mardifoufs 3 hours agoparentprevNone of those words convey the same meaning though. \"Schizo\" doesn't mean weird or unusual in this case. Maybe there's a better word but it really conveys a specific meaning, at least in the colloquial use of the word reply phendrenad2 4 hours agoparentprev [–] How is it offensive? reply cess11 2 hours agorootparent [–] Perhaps in a similar way as 'paki'. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "strlcpy(3), an OpenBSD function, is often considered safer than strcpy(3) and strncpy(3), but it is inefficient and not safe if the source string isn't null-terminated.",
      "Ulrich Drepper rejected strlcpy for glibc due to its inefficiency, as it processes the entire source string even when only a portion is needed.",
      "Alternatives like memccpy(3), strdup(3), and a combination of strlen(3) and memcpy(3) are recommended for better efficiency and portability."
    ],
    "commentSummary": [
      "The author criticizes `strlcpy(3)` for its potential to cause buffer-overflow bugs due to variable-size formats without length specification.",
      "They advocate for counted strings over null-terminated ones, citing examples like Rust's compact string crates and historical evidence from the Cedar group at PARC.",
      "The author suggests that while `strlcpy(3)` aims to prevent crashes and memory issues, it is not the most efficient method, and C's string handling is outdated compared to alternatives like Pascal strings."
    ],
    "points": 135,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1721045027
  },
  {
    "id": 40962942,
    "title": "California Grid Breezes Through Heatwave with Batteries",
    "originLink": "https://thinc.blog/2024/07/14/california-grid-breezes-through-heat-wave-due-to-renewables-batteries/",
    "originBody": "California Grid Breezes Through Heat Wave due to Renewables, Batteries No rolling blackouts or grid emergencies as California continues on path to a carbon free grid. Several strategies, including upgrades to vulnerable parts of the grid at play here, but key enabler is more clean energy, especially solar, and above all, battery storage, now equivalent to 5 very large nuclear power plants. In fact, California seems to have reached a level of storage that is creating some kind of a phase-change in the grid, yielding benefits that are surprising even expert observers. More and more days where renewables supply greater-than 100 percent of California’s power – enabling exports even under these challenging conditions. \"Even with 5% higher temperatures in 2024 than 2023 during same 4-months, we've had 50 more days of 100% #WindWaterSolar in '24\" (Almost) 100 Days of 100% Renewable Electricity in Californiahttps://t.co/rg0qVHyCbm @EnvAm @EnvCalifornia @NeumannJo @stevenmking95 @DanDjsacramento — Mark Z. Jacobson (@mzjacobson) July 12, 2024 Share this: Facebook Reddit Twitter LinkedIn Print Email Like this: Like Loading... Author greenman3610Posted on July 14, 2024July 14, 2024Categories This is Not Cool",
    "commentLink": "https://news.ycombinator.com/item?id=40962942",
    "commentBody": "California Grid Breezes Through Heatwave with Batteries (thinc.blog)107 points by ChuckMcM 22 hours agohidepastfavorite105 comments z_rex 22 hours agoI work in the power generation industry. The large scale introduction of Battery Storage is going to be a game changer for renewables since it will allow overproduction during the day to be stored for the evening peak. I think the current fad of peaking plants is somewhat overblown, as the average large scale battery storage system will not face the issues with starting reliability that can be present on large gas turbine power plants, which not only affect the grid but can be very expensive as a failed starts consumes large amounts of fuel to no effect. In the end, this will drive carbon-producing power capabilities to largely only run during the night when solar is out of service. reply DrBazza 21 hours agoparentWhy can't anywhere that has significant elevation should just pumped storage of water during the day and use hydro on the way back down? reply mariebks 21 hours agorootparentI’m not an expert on this topic, but I think it comes down to cost and scalability. You have to construct a new project with custom specs for the exact site you’re on, and the permitting for a large environmental change is another drag. For large scale batteries, they all are a somewhat complex power electronic wrapper around mass produced battery cell cans or pouches that can be dropped anywhere. The cost declines of batteries are undeniable and are not stopping anytime soon. reply jopsen 21 hours agorootparentNot to mentioned all the dirty cheap second hand batteries becoming available as electric cars are scrapped. For grid storage second hand batteries might be fine. If taking them out of a scrapped car is feasible. reply dzhiurgis 11 hours agorootparentNo grid operator going to mess with scrap batteries. Even for home player new cells are so cheap that it's not worth messing around with used ones (esp when considering liability and insurance). Might work in third world countries tho who have even more appetite for reliable power. reply jeffbee 20 hours agorootparentprevGrid battery deployment is faster than EV market penetration in the USA. This is not something that can siphon off a fraction of EV batteries. It is a huge market in its own right. reply simfree 19 hours agorootparentWhat would be considered ruined Leaf EV batteries that have significant range loss (20% to 50% degradation, for a car that went around 60 miles with a full battery) are being used to cover peak load right now in California and have been used to do so for years. https://www.youtube.com/watch?v=JqlOlqK_ot8 2nd life use cases for cheap, mass produced batteries like this are more common than you think. reply jeffbee 18 hours agorootparentBut that isn't a mainstream case. It's notable for being weird. The 1000x larger Moss Landing battery facility just uses new, off-the shelf BESS units from LG. reply Timshel 21 hours agorootparentprevBecause there are fewer places than expected where it really makes sense. Was discussed in an old video of Practical Engineering: https://youtu.be/66YRCjkxIcg?si=PXArfkXkbCODeSAF&t=290 reply bryanlarsen 18 hours agorootparentAustralian National University has identified 616,000 potential locations: https://re100.eng.anu.edu.au/global/ reply Timshel 9 hours agorootparentInteresting but it appears to only take account the topographic feasibility of the project. The first location I looked at (link is not working :(), it relies on building two dams on rivers (which is something we tend to do less not more) and would flood some houses ... And there is supposedly a price estimate, but it's just an A, B, C, D or E ranking could not find some costs to make comparison with battery storage for example. reply toomuchtodo 21 hours agorootparentprevBatteries installed on concrete slabs are easier and cheaper, there are only so many places pumped hydro is feasible. https://news.ycombinator.com/item?id=40919052 reply 0cf8612b2e1e 21 hours agorootparentBatteries can also be installed basically instantly, with storage being added incrementally. Unlike a years long project of constructing a hydro battery before it enters operation. reply dzhiurgis 11 hours agorootparentprevNZ recently scraped Lake Onslow seasonal hydro storage. My estimate it was about 100x cheaper per kWh when compared with Powerwall. reply toomuchtodo 2 hours agorootparentCan you provide background and/or context as to how and why this decision was made? reply theptip 20 hours agorootparentprevThere is zero chance that a mega-project like pumped hydro can be undertaken in the US in the current regulatory environment. NEPA review allows even clearly green projects like solar and wind to get bogged down in environmental review lawsuits. A project with major environmental impact like destroying an entire valley ecosystem to build a dam would be DoA. reply cogman10 20 hours agorootparentIt's not just nepa. Citizens really don't like pumped hydro which negatively impacts it's ability to be deployed (you need local permits as well as federal). Bear lake Idaho has been talking about putting in pumped storage since the 2000s. On paper, it's pretty much the perfect geography for it. However, various concerns from the impact on local fishing to the impacts on the lakebed have ground that process to a halt. It's a 20+ year project that's gone nowhere. That's why I'm largely negative on pumped hydro. On paper it seems nice, in practice it's almost impossible to get off the ground. Batteries, on the other hand, take almost no effort to install. reply uyeieiii 20 hours agorootparentprevgood. because if you are storing kinetic, better to use a flywheel or something with less environmental impact reply nullc 20 hours agorootparentprevDo the math on how much energy you can store-- the volumes required for meaningful amounts of energy are just mind boggling. So you don't just need elevation, you need thousand of acres of valley or crater that you can flood and people tend to get a little testy about geoengineering at those scales these days. reply grandinj 2 hours agorootparentMeh? Couple hundred meters of elevation and a hundred acres runs a city for day. Source - the city I live in which has this exact setup. reply nullc 16 minutes agorootparentIndeed, but that is the scale you have to work at for it to be interesting... on the order of a billion gallons of water, if I assume it's 10m deep. reply riku_iki 21 hours agorootparentprevsome existing example of this: https://en.wikipedia.org/wiki/Gianelli_Power_Plant reply redleader55 21 hours agoparentprev> I think the current fad of peaking plants is somewhat overblown Do you mean peak powerplants will become obsolete? As far as I know, in many places peak powerplants are hydroelectric, which in the future, aided by local batteries, will allow filling the reservoir lakes to higher limits and covering greater peaks - eg. malfunctions - which in turn will make grids more stable. The big unknown, as far as I understand, is whether or not we have enough rare minerals to cover enough TWh of the daily peaks of energy-demands so we can have a flat daily curve. I'm sure in 10-15 years time the \"renewables\" will look a bit different than what we expect them today. reply adgjlsfhk1 21 hours agorootparentyou can make batteries out of iron or sulfur. no rare minerals required. reply redleader55 21 hours agorootparentYou are right: there are a lot of proposals at the moment trying to replace lithium. I'm hopeful, but let's wait until they are widely adopted before claiming victory. Let's not forget the carbon and environmental cost of such alternative batteries are not known because they are not mass built and mass deployed. reply adgjlsfhk1 12 hours agorootparentI think there is a fairly big difference between these examples. solar panels need pretty complicated semiconductors. Batteries on the other hand can be made by sticking 2 random metals in a jar with some salt water. since grid storage only cares about price per capacity rather than energy density, it seems unlikely for expensive materials to win out. reply Gibbon1 20 hours agorootparentprevWhen I read people talking about replacing lithium batteries I think of the decades of reading about material X replacing silicon in IC's and solar cells. reply timschmidt 19 hours agorootparentThese are available for purchase from a number of companies in mass production today: https://en.wikipedia.org/wiki/Sodium-ion_battery reply hinkley 21 hours agoparentprevWon’t it also be useful for power transportation as well? Since batteries on the usage side of a bottleneck can peak shave and trough fill to levellize the load? reply DoingIsLearning 20 hours agoparentprev> The large scale introduction of Battery Storage is going to be a game changer for renewables Who are the major players in industry grade solutions for this? Tesla? BYD? Hitachi? Siemens? reply infecto 20 hours agorootparentIn Texas I have seen a lot of Chinese brands. Sungrow and CATL as examples. reply DoingIsLearning 13 hours agorootparentBut you meam they are doing consumer PV+battery installations, or actual battery systems for utility companies? reply infecto 7 hours agorootparentYou asked about industrial players, I responded with industrial players I have seen. https://www.power-eng.com/energy-storage/batteries/spearmint... https://en.sungrowpower.com/newsDetail/2127/sungrow-supplies... reply DoingIsLearning 4 hours agorootparentThanks, and thank you for the links. Very interesting to see movement in this direction. reply Gibbon1 20 hours agoparentprevThing I harp on is the logistics of battery installations are fantastic in about every way. If you have a couple of brown field acres of land next to an existing substation you can just buy and install containerized batteries. And you don't need specialized contractors to handle the job either. Pour concrete pads, forklift operators to take batteries off the truck and put them on the pad. And then standard HV techs to hook them up. And the permitting and environmental review is nil. Go ahead try and get a pumped storage system permitted somewhere. A fun one. Three gorges dam. You could replace the whole thing with solar and batteries for the cost it took to build it. And the area covered by solar panels would be the same as the lake behind the dam. Except you can put the solar panels on some ecologically and economically low value land where ever. reply nine_k 20 hours agorootparentThis is cool. The sheer amount of mater required to build hydro storage should not be underestimated. Another win is that you can put something below the solar panels, even grow shade-preferring plants below them. This is being done in many places. Otherwise it could be a storage area, a light industrial facility, a shopping mall, even a sports field. And for all the battery storage you can afford, of course. reply Gibbon1 19 hours agorootparentI think the answer for some area's is sheep. They can graze under the panels and don't chew on stuff like goats. There is a whole interest in agrivoltaics as it's become apparent that the land under the solar panels remains productive. In the western US there is a lot of completely unproductive land to put solar panels on. But in the east and midwest being able to continue to use the land for agriculture is a win. https://www.energy.gov/eere/solar/agrivoltaics-solar-and-agr... Random thought about fallow land under solar panels is the soil probably absorbs carbon over time. reply timschmidt 19 hours agorootparentAlso car parks. Shields the cars from weather, and provides power right where you want it for charging. Can even aid guiding rainwater runoff to avoid contamination with petroleum products. And reduces the amount of heat absorbed by giant swaths of black asphalt. reply bryanlarsen 22 hours agoprevBatteries are absolutely destroying demand for natural gas in California: https://reneweconomy.com.au/wp-content/uploads/2024/05/calif... reply tuetnsuppe 21 hours agoparentHow does this graph consider that 2024 isn’t over, yet, unlike the other years? Otherwise looks promising. Edit: just saw this is only for April of each year. But similar question: can we extrapolate from April to say November or not? reply IvyMike 20 hours agoparentprevThere's a future issue looming: at some point, a lot of the natural gas peaker plants build for 2021 capacity will be idle almost all of the time. (Based on how often my local peaker plant seems to run, that may actually already be the case). Keeping peaker plants around and having them ready year round for just a few days use is going to feel expensive; but decommissioning the plants too early has its own risks. reply bryanlarsen 18 hours agorootparentCalifornia has a capacity payment mechanism for this reason. reply infecto 20 hours agoparentprevLove this graph, really demonstrates the power of low cost solar and increasingly lower cost batteries. reply cletus 21 hours agoprevHere are some useful graphs showing how electricty demand varies by region, month and time of day [1]. As expected, California peaks in July because there's more demand for cooling than heating. But what's a little surprising is that July is peak demand for every region. There are many reasons why solar is so attractive as a power source, including: 1. It's the only form of power with direct electricty generation. There's no heating water to turn a turbine; 2. Solar basically has no moving parts. You can point panels towards the Sun to increase efficiency but the panel itself and transmission has no moving parts; 3. Solar panels continue to massively reduce in price and increase in efficiency and it's not clear where this ends; 4. Obviously solar generates its pwower when the Sun is shining. This conincides with when most power is usedk. Adding solar to the power generation mix reduces the peak load required to generate from other sources; 5. Solar installations can be really small, including on individual houses, reducing the transmission capacity required for last-mile and regional power transport. Batteries are just one method to store excess power for variable (renewable) energy sources. Another is carbon sequestration to create fuel directly, called Carbon Capture and Storage (\"CCS\") [2]. This isn't economic yet. But it may have applications in, say, cold climates where battery performance noticeably degrades. Solar power is the future. [1]: https://www.eia.gov/todayinenergy/detail.php?id=42915 [2]: https://www.iisd.org/articles/deep-dive/why-carbon-capture-s... reply candiddevmike 20 hours agoparent> But what's a little surprising is that July is peak demand for every region. Majority of homes use natural gas for heating in the winter. If they used electric heat instead, the difference would be enormous. reply AnthonyMouse 20 hours agorootparentMore to the point, we want them to use electricity for heat instead. Heat pumps are quite efficient and natural gas emits CO2. But contrary to air conditioning load, heating load is inverse to sunlight, so we need a way to supply that electricity at night. reply ZeroGravitas 11 hours agorootparentHeat pumps are so efficient that you can take the gas you were going to burn in homes to heat them, burn it in generators (losing about 50%), send it across long wires (losing a few percent more) and still come out ahead. If you only need to do that at night, you're another 2x ahead. If you then remember that wind, hydro, nuclear and (the main subject of this discussion) batteries exist, further reducing the times you need to use this displaced gas, it's an overwhelming win. Examples like this is why electrification becomes more important than decarbonizing the last few percent of grid electricity, if the goal is to reduce carbon emisions and/or money. reply dralley 20 hours agoparentprev> 1. It's the only form of power with direct electricty generation. There's no heating water to turn a turbine; > 2. Solar basically has no moving parts. You can point panels towards the Sun to increase efficiency but the panel itself and transmission has no moving parts; This is kind of it's own problem, the fact that both solar and wind don't have moving parts that are directly connected to the grid and rotate at grid frequency necessitates a lot of other equipment to bring frequency stability back. Some old power plants are being converted into synchronous condensers - the turbines replaced with giant flywheels - for this reason. reply testfoobar 21 hours agoprevThe big wattage numbers are great - but it is unclear how many watt-hours can be supplied - how many hours can California's grid run with batteries? It appears that the batteries are 4 hour batteries. Two questions: is there excess solar capacity in the winter to charge the batteries? How do the batteries perform in the winter - total storage capacity and discharge rate? It is misleading to compare a battery array that lasts for 4 hours to a nuclear powered electric plant that supplies energy at a constant 24/7. https://www.pv-magazine.com/2024/05/01/california-crosses-10... \"Developers plan to add 6,813 MW of battery projects in the California Independent System Operator's (CAISO) domain this year, dominated by four-hour lithium-ion systems, roughly double their additions in 2023, according to an analysis of S&P Global Market Intelligence data.\" reply jeffbee 21 hours agoparentBecause of the regulatory/economic regime the state has established, all of these plants are 4h plants, or 4Wh/W if you prefer. That's the number they need to hit to get the top-tier payout from the grid operator. reply skybrian 20 hours agorootparentI suppose they could discharge slower, though, if eventually there's not as much demand? reply csdreamer7 21 hours agoparentprev> It is misleading to compare a battery array that lasts for 4 hours to a nuclear powered electric plant that supplies energy at a constant 24/7. It is misleading to compare the two: * One has no risk of a nuclear meltdown in a very earthquake prone area (California). * One does not have 60+ year long successful history of lobbying governments and establishing regulatory capture (note I wrote 'successful') * one does have issue where they do not reprocess spent fuel adding billions of risk of nuclear fuel leaking into the environment; or if they do recycle, risk creating material for nuclear weapons. * one would struggle with it's strong baseline, yet inflexible power generation without batteries to handle afternoon peaks Keeping hearing random arguments for nuclear fission power (why not tidal?) and yet the regulatory capture and earthquake issue is still present in the US. We moved away from nuclear power after Japan's meltdown for a reason. With recent Supreme Court decisions and fracking causing earthquakes in Nebraska it has only gotten worse. I do not trust the industry with nuclear power. California spent a lot of money to establish a working grid that can be used for sustainable power until we get useful fusion (or that big supervolcano hits). Nuclear power is more needed in Texas right now, with it's broken grid, than California. More information on reprocessing nuclear waste: https://www.projectoptimist.us/why-us-doesnt-recycle-spent-n... https://thebulletin.org/2019/07/recycle-everything-america-e... reply petulla 21 hours agoprevHope other states follow. The fact Arizona is still 80%+ non-renewable is just such a missed opportunity. reply inamberclad 21 hours agoparentArizona is a great case where agrovoltaics could really work. Solar reduces the sun load to something reasonable and minimizes evaporation, plants cool the air under the panels. Human / nature infrastructure symbiosis. reply ZeroGravitas 20 hours agorootparentRecent local news story on this: \"Arizona farmers turn to solar panels to shade crops, save water and generate power\" https://cronkitenews.azpbs.org/2024/07/08/arizona-drought-so... reply ChuckMcM 22 hours agoprevThis is an interesting data point. Grid scale batteries are a pretty big change in how grids are managed, things like \"anticipating load\" and starting up \"peaker plants\" to catch that load are mitigated by the ability to wait and see while the batteries fill in the gap. reply toomuchtodo 22 hours agoparentBatteries charge for free when renewables push spot prices to zero or negative and discharge when most needed and getting paid to do so; they comin’ for fossil gas generation that pays to burn (fuel) and spin (per hour O&M). reply _Microft 22 hours agorootparentThey might even be paid for consuming power because that provides stability to the grid [0] in such a situation. Too little load causes the frequency on the grid to rise which is only acceptable within narrow limits. [0] https://en.wikipedia.org/wiki/Ancillary_services_(electric_p... reply toomuchtodo 22 hours agorootparentAbsolutely, there is no doubt that are stealing grid service revenue from spinning thermal, and they also provide black start services (jump starting larger generators when they go out through an isolated transmission path). reply applied_heat 21 hours agorootparentI was under the impression that “grid forming” as opposed to “grid following” inverters were not a solved problem. Can you cite any battery plant with documented tests of energizing transmission or distribution systems? reply toomuchtodo 21 hours agorootparenthttps://www.bloomberg.com/news/features/2021-03-08/tesla-is-...https://archive.today/xU9Kg https://www.ercot.com/files/docs/2024/01/12/Tesla%20BESS%20G... Gambit Energy, owned by Tesla in Texas. Megapack supports grid forming in production (Hawaii and South Australia installations as well). “Virtual Machine Mode.” It is solved, which shouldn’t be controversial considering Tesla’s power control maturity. reply applied_heat 19 hours agorootparentThanks for the link, looks like it is new development in the last 2 years which is the blink of an eye in utility timeframes. reply toomuchtodo 3 hours agorootparentHappy to help, another link you might find useful on the topic. https://cleantechnica.com/2024/07/14/new-grid-forming-invert... reply dada78641 19 hours agoprev\"The uploader has not made this video available in your country.\" First video on the page. I'm guessing this is probably a news report, right? reply xnx 20 hours agoprevI was confused until I realized title omits \"renewables\". Having peak solar demand during peak air conditioning demand is beautiful harmony. reply gumby 20 hours agoprevThe distribution grid did not do as well — lots of outages in the sierras and points north. reply jeffbee 21 hours agoprevFission stans in shambles. https://observablehq.com/@jwb/caiso-battery-discharge-histor... reply AnthonyMouse 19 hours agoparentIt's weird that you're talking about fission when the thing the batteries are really replacing is natural gas peaker plants. Peak shaving (i.e. the thing storage is best at) is essentially the opposite of baseload. It gets you over the hump where the demand peak is after sunset but you don't want to have a 100% nuclear grid overbuilt to meet the peak demand that occurs when solar generation is zero, and you also want to stop using natural gas. You still need something to generate power when the load is back to baseline and the peak shaving batteries are dry but it's 8 more hours until sunrise. Especially if we're going to electrify heating loads that currently use fossil fuels, which are higher at night and higher during the part of the year when solar generation is lowest. reply jeffbee 18 hours agorootparent\"Baseload\" doesn't exist. It can't hurt you. The sum of a bunch of intermittent sources is sufficient. reply AnthonyMouse 16 hours agorootparent\"Baseload\" clearly exists. There is a minimum load on the grid that it never falls below, which can be satisfied by constant-output generation methods that aren't as well suited to varying output, but also don't vary output and so can't don't create a shortfall when low output periods in variable sources last for longer than the usual amount of time or coincide with high demand. > The sum of a bunch of intermittent sources is sufficient. That only works if the intermittent sources are independent. In winter, solar output is going to be lower during the day than in summer, zero at night, you will have more hours of night, and heating load is both higher in winter and higher at night. There aren't going to be panels on one solar farm where this is the case and others that produce more, it will be true for the entire hemisphere for the entire season. You're then left relying on wind if you don't want to use nuclear, but wind costs more than solar, and now you're lacking independence again. Weather is regional. There will be days when it's still and cold across a thousand square miles. Even batteries that could hypothetically take the load for a night are not going to have anything left by the end of a week of that, so you either need baseload generation methods or some currently unproven/uneconomical ultra high capacity long-term storage system or the week when that happens the power will go out and people will freeze. reply goodSteveramos 14 hours agorootparentAs far as I can tell, anti-nuclear people either have a very shallow understanding of energy and can’t really have a conversation with you using well sourced numbers or they can but know the numbers show that nuclear is the only realistic choice and either way they just respond without numbers in very short and easy to repeat (and incorrect) arguments like “storage” and “bigger grids”. Anti-nuclear environmentalists’ first choice would be to let people freeze to death. That being politically untenable their second choice is to continue using fossil fuels as the baseload and offset it as much as possible with renewables. They think nuclear is “an excuse” to continue the “business as usual” of affordable energy. They want energy to be unavailable and expensive. They oppose human progress and economic development as itself evil, even if the environmental impact is zero. Making energy expensive is their goal, not something they reluctantly accept. All the counter productive positions and bad arguments make perfect sense when you realize that they simply have a totally different goal from you. reply toomuchtodo 14 hours agorootparentprevhttps://theconversation.com/baseload-power-is-a-myth-even-in... https://www.ceem.unsw.edu.au/sites/default/files/uploads/pub... https://energypost.eu/dispelling-nuclear-baseload-myth-nothi... reply goodSteveramos 14 hours agorootparent> we found that the total annualised cost (including capital, operation, maintenance and fuel where relevant) of the least-cost renewable energy system is $7-10 billion per year higher than that of the “efficient” fossil scenario. For comparison, the subsidies to the production and use of all fossil fuels in Australia are at least $10 billion per year. So, if governments shifted the fossil subsidies to renewable electricity, we could easily pay for the latter’s additional costs. They are claiming that a 100% renewable system would be CHEAPER than a fossil fuel system. If that doesn’t stink like some grade A bullshit to you I have nothing more to add. reply ZeroGravitas 11 hours agorootparentThat's an Australian academic in 2013 making that claim, and fair play to him, the official Australian cost estimates in 2024 say: > ‘Firming costs’ is a term often used to describe the investments needed to make variable renewables a reliable source of electricity for our power system. In the GenCost report our preferred term is ‘integration costs’. > Integration costs include investments in storage, peaking generation, transmission and system security devices such as synchronous condensers. Modelling determines the most cost-effective combination of these investments. > ... renewables were still found to have the lowest cost range of any new build technology. > For more detail go to the GenCost 2023-24 report section 5.2.1 Framework for calculating variable renewable integration costs on page 65. https://www.csiro.au/en/research/technology-space/energy/gen... reply goodSteveramos 3 hours agorootparentWhat cost of storage do they use in their model in dollars per kilowatt hour? reply jeffbee 1 hour agorootparentWhy is it up to us to read these reports for you? reply ZeroGravitas 10 hours agorootparentprevOr you burn a small amount of gas when these rare occurrences happen to coincide. I suggest we call the periods that solar/wind/hydro/batteries can cover \"baseload\", as apparantly if you do so, you can pretend that the other energy being generated at non-baseload times does not matter, and simply declare victory with a job half done. reply goodSteveramos 15 hours agorootparentprev“Storage” doesn’t exist. The choice is between nuclear, fossil fuels, and frequent blackouts. reply wumeow 22 hours agoprevCompare and contrast with Texas. reply gretch 22 hours agoparentWhy? Why does this new innovation in the california grid have to be all about dunking on texas? (As a californian) It's especially annoying coming from anyone californian, the state of Enron and PGE wildfires. Like we are so caught up in the vanity of comparison that we quickly rush to forgive our past sins while condemning the mistakes of others for forever and ever. How about as technologists, we just focus on innovating (which is not a zero sum game) and we don't compare ourselves to people who aren't doing as well. reply jfengel 21 hours agorootparentBecause, despite having vast amounts of renewables, Texas is a key political contributor to our lack of a cohesive national strategy for ditching fossil fuels. Its national politicians regularly insist that climate change is a hoax. If a few decades of trying to use rationality have failed, I'm all for calling attention to their own local failures. If \"dunking\" on them is in bad faith, it is only because good faith arguments have failed so consistently. reply baq 21 hours agorootparentprevI’m in Europe and I can pick on Texas all day on energy. The separate grid is a self-inflicted gunshot wound in both feet. reply msandford 21 hours agorootparentHow so? Would the Federal regulators have done a better job of making CenterPoint invest the right amount of money into tree trimming and placing lines underground? California is under Federal regulation and has significant problems where PG&E hasn't invested enough in reliability. If California just breezed through every natural disaster with no power outages and lower than Texas energy prices it would truly be a no brainer. But Texas has retail power rates in the $0.10-$0.20/kwh range. From what I understand $0.20 would be an absolute bargain in California and that $0.30-$0.50 per kwh is more what folks out there pay. It's not so compelling in that case. Of course I could have missed something. Please do let me know if I have. reply nine_k 19 hours agorootparentHitting a company with fines for not doing maintenance required by regulation, when the fines cost more than compliance, works well. Given sensible regulations, of course. Look at airlines: rather strict and reasonably good safety regulations made flying the safest form of transportation, safer than driving and maybe even walking. Nevertheless, we don't see exorbitant prices either; if anything, the cost of air travel is surprisingly low, often competitive with rail, even in EU. Of course, unreasonable regulations are a bane, see the leaded fuel situation in general aviation, or the whole nuclear industry. reply schmookeeg 13 hours agorootparentprevHello from Alameda, CA. 15.5c per kwh here except for M-F 5pm-9pm, where it is 50c. reply msandford 5 hours agorootparentOh wow! Thanks for the info. I'm glad to hear it's not crazy everywhere out there. reply bravetraveler 21 hours agorootparentprevI live in Texas and support your message. We get cheap power and have cheap results [under stress] to show for it, repeatedly reply Dma54rhs 21 hours agorootparentprevThen you should also know the way they run their energy network is similar to our euro free energy markets. Many many people in Europe pay spot price, the kind that people on HN freaked out when the pricing went crazy is pretty normal in EU. reply spacedcowboy 21 hours agorootparentprevWell, they do like their guns in Texas reply kccqzy 21 hours agoparentprevTexas is the number two state in grid battery. No other state comes close to either California or Texas. reply jeffbee 21 hours agorootparentTexas has added a massive amount of batteries in a year or two. Worth noting, however, that as a fraction of their peak load California has about 3x as many batteries as Texas, because Texas uses far more energy and has much less rooftop solar. So while they are in the 2nd position it seems like a fairly distant 2nd. reply toomuchtodo 22 hours agoparentprevhttps://news.ycombinator.com/item?id=40908526 reply batch12 22 hours agoparentprevOr with past California reply jmugan 21 hours agoparentprevI dislike right-wing silliness as much as anyone, but this whole hate against Texas is just weird. We had Montana winter weather for a week and our grid went down. I had a business partner in California during a period a few years back and we had to reschedule meetings around when he would have electricity. We should just focus on doing better instead of fighting. reply skhunted 21 hours agorootparentThe hate as you call it is not weird. It’s a reasonable response to a state that consistently enacts laws and regulations that hurt the people. You get what you vote for and people in other states have to put up with idiotic Texas politicians who vote against hurricane Sandy aid while later requesting help. Texas deserves far more “hate” than it gets. reply basementcat 20 hours agorootparentPerhaps it would be more helpful to try to understand what underlying mechanisms have created situations in which apparently self destructive tendencies are expressed in large societies (not just Texas but in general). What can we do to improve on this? reply shermantanktop 20 hours agorootparentNot a Texas-hater exactly, but there’s something specific here wrt culture and identity. Texans routinely describe Texas as exceptional, as different, as unique. You may be right that there are generic problems at work, but when the protagonist insists that they aren’t like others, it’s not a surprise that people don’t think that way. reply jmugan 21 hours agorootparentprevYou don't have to see the world in such stark terms. There is plenty of silliness everywhere. reply skhunted 17 hours agorootparentNot all silliness is equal or equally stupid. The Texas legislature routinely calls for secession when a Democrat wins the Presidency and votes to stymie federal aid to other states while demanding federal aid for themselves. I don’t see the world in stark terms except where appropriate. You do believe cases exist where such views are warranted right? I happen to think that Texas qualifies. reply pokstad 21 hours agoprev [–] No heat wave in SoCal yet. Let me know when the grid breezes through all of LA and OC having a heatwave. reply mullingitover 21 hours agoparentSocal and the general southwest has been setting records[1]. Some highlights: > Palmdale and Lancaster on Tuesday experienced a record sixth straight day of temperatures at or above 110 degrees, surpassing the prior record of three days for both Antelope Valley cities. But wait, there’s more — officials say the streak is expected to continue the rest of the week, with highs forecast over 110 until Friday. > Las Vegas on Sunday smashed its record high temperature by three degrees, hitting 120 for the first time since record-keeping began in 1937. > Las Vegas is also expected to break its all-time record for consecutive days at or above 110 degrees, on its way there with six days in a row Tuesday. It’s forecast to remain just as hot through next week, which would beat the prior 10-day streak. > Palm Springs on Friday hit 124 degrees, its highest temperature in recorded history. [1] https://www.latimes.com/california/story/2024-07-09/temperat... reply nine_k 19 hours agorootparentOuch :( 125°F is well above the pain threshold. reply jeffbee 21 hours agoparentprev [–] It was 121º in Palm Springs and 100º in Riverside last Monday. I don't know what the population-weighted temperature was that day but that's pretty hot? reply User23 21 hours agorootparent [–] Meanwhile the San Diego heatwave is a scorching two degrees above average at 78 degrees[1]. Now if the Santa Anas start blowing during one of these inland heatwaves and the grid still holds up that will be great. [1] If this sounds like sarcasm, try living in coastal San Diego for a few years. You really acclimate to the perfect weather and 78 really does feel awfully hot. To say nothing of those freezes where it dips into the 60s. I’ve seen bar patios turn the heaters on then. reply 0cf8612b2e1e 20 hours agorootparent [–] It is essentially everything but costal San Diego that has been melting. Even 20 miles difference from the coast shows a huge spike in temperatures. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "California's grid managed a heat wave without blackouts or emergencies, attributed to renewables and battery storage.",
      "Key strategies included grid upgrades and increased clean energy, particularly solar and battery storage, now equivalent to five large nuclear plants.",
      "In 2024, California had 50 more days of 100% renewable energy compared to 2023, showcasing significant improvement in grid performance."
    ],
    "commentSummary": [
      "California's grid effectively handled a heatwave using battery storage, showcasing the potential of large-scale battery systems to store excess solar energy for evening use.",
      "This advancement could decrease dependence on peaking plants, which are less reliable and more costly, positioning battery storage as a game-changer for renewable energy.",
      "The discussion also covered the challenges and benefits of various energy storage methods, regulatory impacts, and the potential use of second-hand electric vehicle (EV) batteries in grid storage."
    ],
    "points": 107,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1720988069
  },
  {
    "id": 40968493,
    "title": "The rise of the camera launched a fight to protect Gilded Age privacy",
    "originLink": "https://www.smithsonianmag.com/history/how-the-rise-of-the-camera-launched-a-fight-to-protect-gilded-age-americans-privacy-180984656/",
    "originBody": "HISTORY How the Rise of the Camera Launched a Fight to Protect Gilded Age Americans’ Privacy Early photographers sold their snapshots to advertisers, who reused the individuals’ likenesses without their permission Sohini Desai, History News Network July 5, 2024 A woman named Evelyn Thaw dodges a camera, 1909 Library of Congress Prints and Photographs Division In 1904, a widow named Elizabeth Peck had her portrait taken at a studio in a small Iowa town. The photographer sold the negatives to Duffy’s Pure Malt Whiskey, a company that avoided liquor taxes for years by falsely advertising its product as medicinal. Duffy’s ads claimed the fantastical: that it cured everything from influenza to consumption, that it was endorsed by clergymen, that it could help you live until the age of 106. The portrait of Peck ended up in one of these dubious ads, published in newspapers across the country alongside what appeared to be her unqualified praise: “After years of constant use of your Pure Malt Whiskey, both by myself and as given to patients in my capacity as nurse, I have no hesitation in recommending it.” Duffy’s lies were numerous. Peck (misleadingly identified as “Mrs. A. Schuman”) was not a nurse, and she had not spent years constantly slinging back malt beverages. In fact, she fully abstained from alcohol. Peck never consented to the ad. The camera’s first great age—which began in 1888 when George Eastman debuted the Kodak—is full of stories like this one. Beyond the wonders of a quickly developing art form and technology lay widespread lack of control over one’s own image, perverse incentives to make a quick buck, and generalized fear at the prospect of humiliation and the invasion of privacy. Duffy's Pure Malt Whiskey used Elizabeth Peck's likeness in an advertisement without her consent. The Spokane Press via Newspapers.com Prior to 1888, cameras often existed in a realm of mystical unknowability. In one famed story from the early days of photography, a man asks for a picture of his recently buried wife, not understanding that someone must be present in order to be photographed. The French writer Honoré de Balzac confessed to fearing that each time a daguerreotype was taken of him, a layer of his skin would be peeled off. Early cameras required a level of technical mastery that evoked mystery—a scientific instrument understood only by professionals. All of that changed when Eastman invented flexible roll film and debuted the first Kodak camera. Instead of developing their own pictures, customers could mail their devices to the Kodak factory and have their rolls of film developed, printed, and replaced. “You press the button,” Kodak ads promised, “we do the rest.” This leap from obscure science to streamlined service forever transformed the nature of looking and being looked at. By 1905, less than 20 years after the first Kodak camera debuted, Eastman’s company had sold 1.2 million devices and persuaded nearly a third of the United States’ population to take up photography. Kodak’s record-setting yearly ad spending—$750,000 by the end of the 19th century (roughly $28 million in today’s dollars)—and the rapture of a technology that scratched a timeless itch facilitated the onset of a new kind of mass exposure. “The impulse to peer into others’ affairs—an age-old feature of village life—had never actually subsided,” writes historian Sarah E. Igo in The Known Citizen: A History of Privacy in Modern America. Photography became such a phenomenon that “Kodak fiends,” a phrase used to describe those seduced by the devilish pleasures of photography, entered the vernacular. A group of \"camera fiends\" in Yosemite, circa 1902 Library of Congress Prints and Photographs Division No one quite knew what to make of or how to control the fiendishness, and privacy was further unspooled by money-making schemes just as ferociously inventive as the new technology. The same year Kodak cameras hit the marketplace, the Brooklyn Daily Eagle reported that Anthony Comstock—the anti-obscenity crusader after whom the 1873 Comstock Act is named—had arrested an amateur photographer for selling manually photoshopped pictures that placed “the heads of innocent women on the undraped bodies of other females.” In 1890, a mugshot photographer for the New York Police Department was fired for selling copies of the mugshots to arrestees themselves—an arrangement the New York Times described as a “lucrative business.” Boundless fascination with photographs created a bustling economy. People bought and collected random photographs from dry goods stores, general junk shops, vending machines and even cigarette packs. Demand was so robust that amateurs were just as able to sell to this market as professionals. The ubiquity of advertising by the end of the 19th century only intensified this demand. “As the growth in productive capacity outpaced the needs of the population, commercial entrepreneurs became obsessed with creating demand for consumer products,” writes historian Samantha Barbas in Laws of Image: Privacy and Publicity in America. “The key agent in this project was advertising.” By 1900, photography began to replace earlier image-making methods as the ad technology of choice. Photos of women were especially desirable, given their association with respectability and the belief that a pretty face could sell anything. But dominant values around modesty, avoiding indulgence and anti-consumerism meant that most people had no desire to be featured in an advertisement. Commercial modeling and stock photos did not yet exist. Faced with few choices, advertisers resorted to backdoor purchases. In an arrangement Barbas dubs “the crisis of the ‘circulating portrait,’” advertisers began buying portraits from photographers without the permission of the photos’ subjects—as was the case with Peck, temperate widow turned whiskey hound by the magic and obfuscation of advertising. It wasn’t just ordinary people who found themselves newly exposed. Mass photography was an equalizer twice over: Nearly anybody could use a camera, and nearly anybody might be violated by one. To their grave displeasure, even the elite were unable to assert control over the frenzy. The New York Times reported that President Theodore Roosevelt was “known to exhibit impatience on discovering designs to Kodak him”; the same column mentioned that Reginald Claypoole Vanderbilt horsewhipped a man he alleged took a picture of him without permission. Franklin Mills Flour ad featuring Abigail Roberson findagrave.com Though newspapers across the country cautioned Americans to “beware the Kodak,” as the cameras were “deadly weapons” and “deadly little boxes,” many were also primary facilitators of the craze. The perfection of halftone printing coincided with the rise of the Kodak and allowed for the mass circulation of images. Newly empowered, newspapers regularly published paparazzi pictures of famous people taken without their knowledge, paying twice as much for them as they did for consensual photos taken in a studio. Lawmakers and judges responded to the crisis clumsily. Suing for libel was usually the only remedy available to the overexposed. But libel law did not protect against your likeness being taken or used without your permission unless the violation was also defamatory in some way. Though results were middling, one failed lawsuit gained enough notoriety to channel cross-class feelings of exposure into action. A teenage girl named Abigail Roberson noticed her face on a neighbor’s bag of flour, only to learn that the Franklin Mills Flour Company had used her likeness in an ad that had been plastered 25,000 times all over her hometown. After suffering intense shock and being temporarily bedridden, she sued. In 1902, the New York Court of Appeals rejected her claims and held that the right to privacy did not exist in common law. It based its decision in part on the assertion that the image was not libelous; Chief Justice Alton B. Parker wrote that the photo was “a very good one” that others might even regard as a “compliment to their beauty.” The humiliation, the lack of control over her own image, the unwanted fame—none of that amounted to any sort of actionable claim. Alton B. Parker, the New York Court of Appeals judge who ruled against Roberson Public domain via Wikimedia Commons Public outcry at the decision reached a fever pitch, and newspapers filled their pages with editorial indignation. In its first legislative session following the court’s decision and the ensuing outrage, the New York state legislature made history by adopting a narrow “right to privacy,” which prohibited the use of someone’s likeness in advertising or trade without their written consent. Soon after, the Supreme Court of Georgia became the first to recognize this category of privacy claim. Eventually, just about every state court in the country followed Georgia’s lead. The early uses and abuses of the Kodak helped cobble together a right that centered on profiting from the exploitation of someone’s likeness, rather than the exploitation itself. Not long after asserting that no right to privacy exists in common law, and while campaigning to be the Democratic nominee for president, Parker told the Associated Press, “I reserve the right to put my hands in my pockets and assume comfortable attitudes without being everlastingly afraid that I shall be snapped by some fellow with a camera.” Roberson publicly took him to task over his hypocrisy, writing, “I take this opportunity to remind you that you have no such right.” She was correct then, and she still would be today. The question of whether anyone has the right to be free from exposure and its many humiliations lingers, intensified but unresolved. The law—that reactive, slow thing—never quite catches up to technology, whether it’s been given one year or 100. This essay is from History News Network, a University of Richmond project dedicated to new interpretations of the past. Read more and subscribe to HNN’s newsletter here. Get the latest History stories in your inbox? Click to visit our Privacy Statement. Filed Under: Advertisements, American History, Civil Rights, Journalism, Law, Newspapers, Photographers, Photography, Photojournalism, Technology Most Popular Meet Vivian Maier, the Reclusive Nanny Who Secretly Became One of the Best Street Photographers of the 20th Century Melting Ice Reveals Body of American Mountaineer Missing for 22 Years in the Peruvian Andes Ancient DNA Unravels the Mysteries of the Dingo, Australia's Wild Dog See Ten Creepy-Crawly Portraits From the Insect Week Photography Contest How the Rise of the Camera Launched a Fight to Protect Gilded Age Americans' Privacy",
    "commentLink": "https://news.ycombinator.com/item?id=40968493",
    "commentBody": "The rise of the camera launched a fight to protect Gilded Age privacy (smithsonianmag.com)94 points by nickwritesit 3 hours agohidepastfavorite58 comments ahmeneeroe-v2 2 hours agoThe throwaway line in TFA about village life and the section about common law recognizing no right to privacy both highlight my thoughts on this. In prior years we didn't have cameras preserving all our moments in 4K for all time, but as humans we had so little happening in our hamlets/villages, that most of our neighbors knew everything about us and all of our individual actions were never forgotten. Just like how you still remember all the embarrassing things different kids at your high school did (for me, 20+ years later). Life in a village was likely that same dynamic but for a person's whole life, not just high school reply rootusrootus 1 hour agoparentWe did not need a strict law about privacy when the only way to surveil and preserve information was to use your eyes and brain. And given that everyone tends to be their own main character, even that level of preservation was not terribly effective. The rise of computers and the ability to record things in near perfect fidelity is a problem. I hope we can get a handle on it before it gets much worse. reply maxwell 1 hour agorootparent> the ability to record things in near perfect fidelity is a problem Ancient Egyptians and Romans would seem to have relished such a problem. reply whstl 1 hour agorootparentprevMaybe we need to start preserving those things in AI neural nets so they are a bit fuzzy and slightly inaccurate! reply plussed_reader 1 hour agorootparentAh, proprietary recollection! Surely that won't engender an entire middle-man economy. reply mulmen 2 hours agoparentprevI only remember the embarrassing things I did. And because I graduated in the pager era there are very few photographs to remind me of what I have forgotten. reply wongarsu 1 hour agoparentprevAnd if you move away (to escape your past or for other reasons), any new village you move to will treat you as a tolerated outsider for 20+ years because you didn't go to the same kindergarten as them. reply ghaff 2 hours agoparentprevYeah, the scale of things is different but the idea that everyone in your town (or your social circles) didn't know more or less everything about you is counterfactual. reply rjurney 22 minutes agoprevI love articles that completely dispel the idea of singularity - that point out that the feeling of our age that things are changing too quickly has been around for a very long time. reply jappgar 10 minutes agoparentIt doesn't dispel the idea that things are changing too quickly. It reinforces that we are engaged in a never ending battle against exploitation and enslavement. reply jrussino 8 minutes agoparentprevYeah, I think with all major technological advancements we end up with logistic \"S-curve\" type growth, with a steep rise followed by an eventual plateau. At the start it can look like exponential growth heading toward a singularity, but that's just not realistic and growth always slows down and levels out eventually. Based on the degree and timescale though the two cases my feel pretty indistinguishable for individuals. reply belter 1 hour agoprevThe pioneer of DeepFakes... \"Anthony Comstock—the anti-obscenity crusader after whom the 1873 Comstock Act is named—had arrested an amateur photographer for selling manually photoshopped pictures that placed “the heads of innocent women on the undraped bodies of other females.” \" reply jodrellblank 1 hour agoprevThe real non-story in the article is describing the downright awful behaviour of advertisers and marketers, and trying to say the problem is cameras. > \"By 1905, less than 20 years after the first Kodak camera debuted, Eastman’s company had sold 1.2 million devices\" Smartphones today sell that many roughly every 9 hours[1]. Add laptops, tablets, quadcopter drones, dashcams, doorbells, CCTV cameras, compact cameras, DSLRs, cars with builtin cameras. I think we're still at the early days of cameras changing the world. \"The future is here but it's not evenly distributed yet\"; twenty years ago, Flickr and YouTube were founded. There's almost nothing you can't see online or on TV now: any activity, any place, any thing, especially including the minutiae of other people's lives and inside their homes from the luxurious to the impoverished, ostentatious or humble, everyday or holiday, there's countless photos and videos of it. Want to see a driver's view of a tram or truck or bus journey in a foreign city? A trip on a luxury train or a remote mountain top? A helicopter flight, a submarine trip, Australian outback or Thai food stalls? People sitting on their couch watching TV and chatting about it, someone angrily ranting from their kitchen, people cooking food and eating it, people at work or relaxing, the insides of factories offices public places or government buildings, rare equipment and devices, museums, you-seeums, no-seeums up close; do you want voyeurism, inspiration, exploration, drama, tranquility, nature, disaster, ingenuity, warzone or poorzone, languages, opinions, the mundane, or the joys of propane? You can find it, you can see it - you can drown in endlessly scrolling it, it can be tuned to your interests or sought on a whim - but you can't have it through a screen. it will take more than 20 years for the effects on society to fully happen. [1] Roughly 1.2Bn/year, ~100M/month, https://www.statista.com/statistics/263437/global-smartphone... reply cxr 2 hours agoprevPreviously:9 (13) days ago. 46 points. 8-ish comments. reply autoexec 1 hour agoparentsee also: https://news.ycombinator.com/item?id=40900087 7 days ago reply dfxm12 1 hour agoprevDoes Facebook still use your pictures in ads? Article form ~10yr ago: https://mashable.com/archive/facebook-ads-photo#ggcKnNfAUaqy reply RyanAdamas 45 minutes agoprevExclusion through technological adherence. reply batch12 2 hours agoprevThis seems analogous to how large companies now scrape user-generated content to train their language models for profit. reply bgoated01 1 hour agoprev\"...arrested an amateur photographer for selling manually photoshopped pictures...\" Am I the only one who finds the phrase \"manually photoshopped\" in an article about the late 1800s to early 1900s amusingly anochronistic? How about \"manually doctored\" or even \"altered\"? reply briankelly 8 minutes agoparentWe do kinda take it for granted when old terms are re-adapted to the new technology - for instance \"splicing\" video clips in something like iMovie. It's rarer to see it the other way around but I'm not sure it's any worse. Today you probably are less likely to say \"doctored\" or \"edited\" for that kind of manipulation described than you are \"photoshopped.\" reply 13of40 1 hour agoparentprevThat one's kind of come full circle, though, since a \"photo shop\" was a place you could go to get photos printed, altered, etc. since the late 1800s. reply kmoser 24 minutes agoparentprevWhile \"photoshop\" may have entered the current vernacular, it's annoying (maybe even misleading?) in the context of this article because I doubt Comstock used \"photoshopped\" as a verb. More to the point, we still have verbs like \"edited,\" \"retouched,\" and \"altered\" that just as accurately describe the process, are more likely to have been used at the time, and are still understood by everybody today (believe it or not, there are people who don't know what Photoshop is, and wouldn't understand it as a verb). reply nerdponx 1 hour agoparentprevI made the same comment but got flagged for my clumsy phrasing. I got a good laugh out of it. I couldn't tell if it was a subtle joke, or a confused young author + lapse in editing. reply renewiltord 1 hour agoparentprevShall all amusing text fall at the hands of the machine mind? Is there to be no fun in this universe? reply tempodox 2 hours agoprevThe idea of privacy has been thoroughly frustrated. Today, there's no expectation of privacy any more, let alone “reasonable expectation of privacy”. Expecting privacy has become unreasonable by definition. reply nerdponx 2 hours agoparentIt has not yet become unreasonable by definition. Certain powerful forces would like that to be the the case, and they have been successful at moving things in that direction so far, but they haven't succeeded yet. reply lcnPylGDnU4H9OF 2 hours agoparentprev*in public You have a reasonable right to privacy, e.g., in your home. reply hanniabu 2 hours agorootparent*As long as you don't own any internet-connected devices reply lcnPylGDnU4H9OF 2 hours agorootparentHow does that remove the right? Presumably one is making the decision to connect these devices to the internet and could simply decide differently. reply mrsilencedogood 2 hours agorootparentThere gets to be a certain point where we simply have to accept that while yes, technically one could simply not own a TV (increasingly difficult to find a DumbTV as opposed to a SmartTV with always-on microphone), any IOT things, a smartphone, etc etc, we cannot reasonably expect someone to not do this. Notably the smartphone, which of the entire list of probably the worst offender, second only to the TV perhaps because of the ick/literally-1984 factor of your TV listening to you. We practically can't expect people to do without a smartphone. It's how people pay bills, bank, access weather information, etc. And since it's the most-used path, it's also the best-maintained path. Non-smartphone alternatives have already rotted. Have you tried calling into half of those bills you pay via QR code instead of just scanning the code? Try it sometime. Instead of telling people to just go live on a mountain monastery in Tibet, we could instead simply eat our cake AND have it by just regulating this stuff. Pass privacy laws. Fine people who break them. Samsung, Google, etc will comply rather than not continue to bathe in our ad revenue. reply lcnPylGDnU4H9OF 2 hours agorootparentIt's not about telling people to live outside society but instead to be more mindful of how they engage with it. One can live in a city and not own a smart TV, nor a smartphone, nor any IOT devices, etc., but that's not the point I'm making. One can also just choose different devices. Once someone gets to the point that they refuse to buy the privacy-disrespecting device it's not much of a stretch to start looking for a privacy-respecting device. reply talldayo 1 hour agorootparent> but instead to be more mindful of how they engage with it. This is like the recycling debate all over again. Fundamentally we agree with you; littering is bad, the diminishing of privacy is bad, and none of us should feel hopeless about a situation we are capable of changing. But consider the average person, not me or you or the hackers. Someone who opts for convenience, not \"mindful engagement\" for the slightest of interactions with the world. They do not care. Fundamentally they cannot be made to care, even if we show them posters of the Earth burning or make them hear about the Snowden leaks. They consider themselves to be powerless, and will not consider an alternative unless it is as convenient as what they're addicted to now. They're not going to buy respectful TVs because they're not the cheap ones. They're not going to buy respectful phones because it doesn't have the little Apple logo on the back that their friends love so much. The average person literally cannot be made to care unless we make privacy-respecting software easier to use than spyware. reply ryandv 43 minutes agorootparent> But consider the average person, not me or you or the hackers. Forget the average person - even as a technically adept person who has tried to \"mindfully engage\" with technology and minimize his use of it as much as is reasonably possible, the way our societies in the west are structured are making it increasingly difficult to participate in the world without plugging yourself into the surveillance borg. Examples abound even in this thread - QR codes to order food; needing to ask concierge to call you a Lyft because taxis are nonexistent in the neighbourhood and you aren't carrying a smartphone. There is so much friction involved in living life with minimal technology that you are placing yourself at a significant disadvantage and living your life less efficiently than others by needing to make compromises or forego certain amenities entirely. While it is possible to live life like this and eschew such creature comforts, it becomes exhausting after several years of doing so, and there is no indication that the tides will turn or the winds will begin blowing in the other direction. You can continue swimming upstream for naught and at great disadvantage to yourself, but it will not change the direction of the current. If anything, the pandemic and work-from-home has only accelerated the trend, with the invasion of Internet-connected surveillance devices into one's private quarters now made mandatory in order to participate in the workforce. reply throwway120385 1 hour agorootparentprevAnd because of how markets work, that person who doesn't care is dictating what is available to us. reply shadowgovt 1 hour agorootparentprev> unless we make privacy-respecting software easier to use than spyware This point is key. I don't use an Android instead of a Pinephone because I don't care. I use an Android because Pinephone sucks right now, and I can't be hacking on the device I need to rely on to schedule my bank payments and call the emergency services if I fall in a ravine. That device has to be five-nines trustworthy. The solution is going to have to be make Pinephone suck less. That's the only way to mass adoption and actually changing the landscape. Music piracy didn't get its back broken by a sudden global attack of conscience. It got its back broken by iTunes making it more convenient to get music-to-portable-device than running a torrent server. reply shadowgovt 2 hours agorootparentprev> One can live in a city and not own a smart TV, nor a smartphone I believe parent's point is: no, one cannot. One cannot live in a (modern Western urban) society and not have a smartphone. The people who do are now living in a different society, where, among other things: - they have to plan to stop at a bank to take out physical cash - they have to schedule their days in an entirely different method from their peers with smartphones - they are less connected to opportunities and access to government and private services because those services come via smart devices and assume smart device ownership Not to put too fine a point on it: there are restaurants that give you a QR code instead of a paper menu now. Are you and I living in the same society if I can order food there and you can't? reply ryandrake 1 hour agorootparentYou just enumerated some of the things people can do to live without a smartphone, demonstrating it is possible (albeit difficult). I think we need to stop calling things \"not possible\" just because they take effort and lifestyle changes to accomplish. reply pixl97 14 minutes agorootparentAlso this take is not great because your privacy is also dictated by the people around you that have smartphones and are listening to you. shadowgovt 1 hour agorootparentprevI mean, one can live without a house too. Perhaps we are just quibbling on definitions if we're arguing about whether being homeless counts as living in the same society as those with a permanent address. The meta point is: government regulates housing because it's something \"everybody\" (modulo outliers) does: live in a house. If \"everybody\" (modulo outliers) owns a smartphone and smartphone ownership is assumed for full societal participation... We probably need to regulate it. reply commodoreboxer 2 hours agorootparentprevIt's really not that hard to have a smart TV and simply not connect it to your network. They'll beg to connect, but I haven't seen one that will refuse to function without an internet connection. reply mikestew 1 hour agorootparentOur brand-new LG OLED doesn’t even beg. It asked at setup time, I selected “not in your marketing drone’s wildest dreams”, it said “cool, bro”, and hasn’t bothered us in the last two weeks. We don’t even see the TV’s home screen. Just power on the Xbox/Apple TV/whatever and HDMI CEC turns everything on and sets inputs. reply adastra22 1 hour agorootparentFYI they sometimes share a connection via Ethernet-over-HDMI. reply mikestew 40 minutes agorootparentI am aware that HEC exists. Now show me a consumer electronics device that actually uses it. None of my devices advertise this feature. reply ryandv 55 minutes agorootparentprevWhat say you of all the remote workers who now need such devices in order to do their work from home? Should they decide to return to office? reply tempodox 45 minutes agorootparentprevHaving the right to privacy on paper doesn't mean you get it in practice. reply BolexNOLA 2 hours agorootparentprevI don’t think it’s reasonable to say “just don’t have internet connected devices at all” in 2024. This falls too far into “caveat emptor” territory for my tastes. We all need to be informed and responsible but the burden of creating a totally disconnected-from-the-internet house (except for your core devices like phones/laptops) is actually pretty substantial and gets harder every year. Hell it’s becoming a problem with cars. reply EvanAnderson 2 hours agorootparentI worry the normalization of Internet-connectedness in \"smart devices\" in the eyes of the public will lead to regulation of these devices for \"privacy reasons\" while offering no real privacy guarantees. I think the drive for \"privacy respect\" in devices should really be an effort to push manufacturers toward making devices that work without Internet-connectivity. I think everybody technical who cares about privacy should beat this drum to all their friends, family, etc, to vote with their wallets. Standalone devices make a stronger privacy guarantee than the alternative. \"Connected\" devices that offer \"privacy\" rely on the compliance of the companies who run the servers for the devices. It also means they can't get breached, either. reply pojntfx 2 hours agoparentprev> Expecting privacy has become unreasonable by definition. *in the US that is. Basically everywhere else, esp. Europe, has been moving into the complete opposite direction here. There is _more_ privacy rights - and recourse, like the right to delete - then there has been at any point in the past. reply parasense 1 hour agorootparentYeah, Europe has an interesting way of maintaining legal fictions. The geographical naming rights is one such thing. Anybody anywhere can make any kind of cheese, but one cannot say the cheese is parmesian (or any other given example) cheese without being located in a certain region. There was a famouse case where a cheese maker in itally opened a new factory on the other sside of town which happened to be across the lambardi river, and suddenly they were no longer able to say that cheese was whatever kind of cheese... That is just one example of many... why legal fictions are bad arguments in Europe, and don't work internationally. One cannot demand other people unsee things they have seen (wipe their brain memory), and so it goes it's equally unjust to demand other people delete photos of them they have seen and recorded in photos. It's a lot more nuanced than that, and in some cases the privacy ideas are fully justified, but you get the idea.... there cannot be absolutism in either direction. reply em-bee 1 hour agorootparentjust consider parmesan to be a protected trademark owned by the people living in that area. should they not have the right to protect their trademark? reply em-bee 1 hour agoparentprevthe opposite is true. as the article shows, according to the law of the time there was no right to privacy, and photos were not protected until the public protested and the laws were changed. likewise today any expectation of privacy is a matter of opinion. just because laws do not sufficiently protect our privacy, does not mean that we should give up any expectation of privacy. on the contrary, we need to demand better protection from out lawmakers. vote accordingly reply dnissley 2 hours agoparentprevThere is a reasonable expectation of privacy. But we should distinguish between \"privacy\" with no qualifiers, and \"digital privacy\" which is a whole other thing. Some people see an equivalence. Others do not. reply EvanAnderson 2 hours agorootparentWhat's the difference? reply dnissley 2 hours agorootparentOne relates to what people do in their own homes. The other relates to things people do on the internet. reply EvanAnderson 2 hours agorootparentThe blurring of the line between \"in your own home\" and \"on the Internet\" is the crux of the issue. The average person doesn't understand that bringing Internet-connected devices into their home makes some quantity of \"behavior in their home\" into \"behavior on the Internet\". reply ryandrake 1 hour agorootparentApplication developers are deliberately trying to blur the line, too, in their user interface. How many users really grok that when they use iCloud syncing with their photos, they are uploading their photos to the Internet? Since many of us grew up before and then after the Internet, we kind of instinctively get what application functionality needs the Internet, what functionality doesn't, what gets written to the Internet, what gets read from the Internet and so on. Younger computer/phone users don't get this at all. My 11 year old has no mental picture of why X requires Internet and Y doesn't require Internet. She doesn't understand the concept of Data A is stored on-device, and Data B is stored on the Internet. And honestly, I'm even having a harder and harder time explaining it. Tech applications have totally blurred and grayed the previously black-and-white line. reply rootusrootus 1 hour agoparentprev> Expecting privacy has become unreasonable by definition. Hard disagree. The only unreasonable expectation is that you could go out in public and not be seen at all. It is entirely reasonable to expect not to be tracked, doxxed, etc. I would like to put the brakes on the idea that just because we can do something, it is okay to do it. The law was written when computers, networking, smartphones, and high resolution digital cameras did not exist. We could easily conclude that every bit of surveillance technology that did not exist in e.g. 1950 is illegal until explicitly permitted by law. Certain organizations would like to gaslight us into thinking that is impossible, that the cat is out of the bag so to speak. I do not agree. We can do it. reply datameta 3 hours agoprev [–] Our relationship with technology is an Escher staircase. Similar thing happening anew with visual genAI. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The rise of the Kodak camera in the late 19th and early 20th centuries made photography accessible to the public, leading to widespread privacy invasions.",
      "Incidents like Elizabeth Peck's unauthorized use in an ad and Abigail Roberson's lawsuit against Franklin Mills Flour highlighted the misuse of personal images for profit.",
      "Public outcry over such privacy violations led New York to pass a \"right to privacy\" law in 1903, setting a precedent for other states to follow."
    ],
    "commentSummary": [
      "The introduction of cameras during the Gilded Age raised significant privacy concerns, as they allowed for high-fidelity capture and preservation of moments.",
      "Technological advancements, from early photography to modern smartphones, have consistently challenged privacy norms, prompting discussions on the need for better privacy laws and mindful tech use.",
      "The ongoing debate centers on balancing the benefits of technology with the protection of privacy rights in an increasingly connected world."
    ],
    "points": 94,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1721055845
  },
  {
    "id": 40963278,
    "title": "Leaked payroll data show how much Valve pays staff and how few people it employs",
    "originLink": "https://www.theverge.com/2024/7/13/24197477/valve-employs-few-hundred-people-payroll-redacted",
    "originBody": "PC Gaming/ Gaming/ Entertainment Here’s how much Valve pays its staff — and how few people it employs Here’s how much Valve pays its staff — and how few people it employs / Leaked payroll data reveals just how small one of the most important companies in gaming actually is. By Jay Peters, a news editor who writes about technology, video games, and virtual worlds. He’s submitted several accepted emoji proposals to the Unicode Consortium. Jul 13, 2024, 12:15 PM UTC Share this story If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Image: The Verge Valve is a famously secretive company with an enormous influence on the gaming industry, particularly because it runs the massive PC gaming storefront Steam. But despite that influence, Valve isn’t a large organization on par with EA or Riot Games’ thousands of employees: according to leaked data we’ve seen, as of 2021, Valve employed just 336 staffers. The data was included as part of an otherwise heavily redacted document from Wolfire’s antitrust lawsuit against Valve. As spotted by SteamDB creator Pavel Djundik, some data in the document was viewable despite the black redaction boxes, including Valve’s headcount and gross pay across various parts of the company over 18 years, and even some data about its gross margins that we weren’t able to uncover fully. The employee data starts with 2003, which is a few years after Valve’s 1996 founding and the same year Valve launched Steam, and goes all the way up until 2021. The data breaks Valve employees into four different groups: “Admin,” “Games,” “Steam,” and, starting in 2011, “Hardware.” If you want to sift through the numbers yourself, I’ve included a full table of the data, sorted by year and category, at the end of this story. In the document, the headings for the third and fourth columns are fully redacted, but the table is titled “Employee Headcount and Gross Pay Data, 2003-2021” so I’m presuming the data in those columns represent gross pay and number of employees, respectively. One data point I found interesting: Valve peaked with its “Games” payroll spending in 2017 at $221 million (the company didn’t release any new games that year, but that spending could have gone toward supporting games like Dota 2 and developing new games like Artifact); by 2021, that was down to $192 million. Another: as of 2021, Valve employed just 79 people for Steam, which is one of the most influential gaming storefronts on the planet. “Hardware,” to my surprise, has been a relatively small part of the company, with just 41 employees paid a gross of more than $17 million in 2021. But I’m guessing Valve now employs more hardware-focused staffers following the runaway success of the Steam Deck. In November 2023, Valve’s Pierre-Loup Griffais told The Verge that he thinks “we’re firmly in the camp of being a full fledged hardware company by now.” Wolfire alleged Valve “...devotes a miniscule percentage of its revenue to maintaining and improving the Steam Store.” The small number of staff across the board seemingly explains why Valve’s product list is so limited despite its immense business as basically the de facto PC gaming platform. It’s had to get help on hardware and software and has worked with other companies to have them build Steam boxes and controllers. (The company’s flat structure may have something to do with it, too.) Valve’s small staff is also something that’s been a sticking point for Wolfire. When it filed its lawsuit in 2021, Wolfire alleged that Valve “...devotes a miniscule percentage of its revenue to maintaining and improving the Steam Store.” Valve, as a private company, doesn’t have to share its headcount or financials, but Wolfire estimated that Valve had roughly 360 employees (a number likely sourced from Valve itself in 2016) and that per-employee profit was around $15 million per year. Even if that $15 million number isn’t exactly right, Valve, in its public employee handbook, says that “our profitability per employee is higher than that of Google or Amazon or Microsoft.” A document from the Wolfire lawsuit revealed Valve employees discussing just how much higher — though the specific number for Valve employees is redacted. While we haven’t seen any leaked profit numbers from this new headcount and payroll data, the figures give a more detailed picture of how much Valve is spending on its staff — which, given the massive popularity of Steam, is probably still just a fraction of the money the company is pulling in. Valve didn’t immediately reply to a request for comment. After we reached out, the court pulled the document from the docket. Sean Hollister contributed reporting. Update, July 13th: Clarified why two table headings include “presumably.” Valve employee data, 2003 - 2021 Year Category [Presumably: Gross pay] [Presumably: Number of employees] 2003 Admin $454,142 5 2004 Admin $548,833 8 2005 Admin $11,644,172 9 2006 Admin $7,905,166 11 2007 Admin $1,997,107 12 2008 Admin $19,519,296 14 2009 Admin $20,300,752 18 2010 Admin $34,754,590 19 2011 Admin $35,216,732 22 2012 Admin $68,925,186 24 2013 Admin $48,462,690 20 2014 Admin $90,406,510 23 2015 Admin $91,496,697 24 2016 Admin $95,444,499 35 2017 Admin $83,146,640 38 2018 Admin $103,479,550 39 2019 Admin $109,720,296 39 2020 Admin $118,435,121 39 2021 Admin $157,999,567 35 2003 Games $3,933,064 57 2004 Games $4,471,342 61 2005 Games $18,122,549 81 2006 Games $17,260,260 97 2007 Games $12,768,984 100 2008 Games $39,677,549 136 2009 Games $44,076,164 148 2010 Games $66,201,302 173 2011 Games $68,173,834 175 2012 Games $135,484,323 186 2013 Games $107,654,658 188 2014 Games $152,351,554 185 2015 Games $181,769,451 160 2016 Games $174,660,830 175 2017 Games $221,488,403 184 2018 Games $216,249,204 192 2019 Games $236,798,782 201 2020 Games $199,306,798 189 2021 Games $192,355,985 181 2003 Steam $1,038,091 16 2004 Steam $1,113,136 16 2005 Steam $2,840,825 23 2006 Steam $3,424,485 29 2007 Steam $3,128,634 34 2008 Steam $5,053,283 40 2009 Steam $7,339,922 51 2010 Steam $17,732,609 60 2011 Steam $16,369,045 101 2012 Steam $42,966,257 127 2013 Steam $44,515,505 128 2014 Steam $52,338,579 119 2015 Steam $72,391,837 142 2016 Steam $56,390,975 125 2017 Steam $64,945,395 102 2018 Steam $70,814,165 82 2019 Steam $66,481,253 80 2020 Steam $71,752,682 82 2021 Steam $76,446,633 79 2011 Hardware $2,252,828 7 2012 Hardware $3,460,641 14 2013 Hardware $5,369,203 20 2014 Hardware $10,180,424 27 2015 Hardware $12,396,140 27 2016 Hardware $11,001,217 36 2017 Hardware $16,724,365 39 2018 Hardware $19,578,951 47 2019 Hardware $15,831,572 47 2020 Hardware $12,008,996 31 2021 Hardware $17,706,376 41 Most Popular Most Popular Google is reportedly planning its biggest startup acquisition ever AT&T reportedly gave $370,000 to a hacker to delete its stolen customer data Here’s how much Valve pays its staff — and how few people it employs Amazon’s press-to-order Dash buttons are officially discontinued Complaints about crashing 13th, 14th Gen Intel CPUs now have data to back them up Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=40963278",
    "commentBody": "Leaked payroll data show how much Valve pays staff and how few people it employs (theverge.com)93 points by bookofjoe 21 hours agohidepastfavorite90 comments lionkor 20 hours ago> Valve employed just 79 people for Steam, which is one of the most influential gaming storefronts on the planet. It might just be me, but that actually seems pretty reasonable. What I've seen a lot is companies, during growth, accepting less-than-ideal candidates, maybe without a good onboarding, which then end up as deadweight in the worst case, and underperformers in the best case. This, combined with a tendency to keep people around who have been there a long time, and the people themselves knowing they're not going to be let go, could result in massive companies and hundreds of people per product. Yes steam is big, but it's not a product so complex you'd need over 100 people to maintain it. I assume other companies need so many people because tasks don't get done, and the logical step is to add people, not remove people. Staying lean means staying agile (in some definition of the word), and that can speed you up. reply c-hendricks 19 hours agoparent> steam is big, but it's not a product so complex you'd need over 100 people to maintain it I dunno, I think Steam is pretty complex: - The steam store (credit card processing, optimizing search, review bombs, gift buying, features around various ways to hide games you've purchased, marketplace) - Workshop (Uploading, managing people's subscriptions, SDKs) - Cloud Saves - Remote Play, works remotely without any port mapping (so they're running a service for that) (clients for macOS, Windows, iOS/iPadOS/tvOS, Android) - Controller Mapping (DualShock 4, Dual Sense, various Nintendo Controllers, Xbox controllers, pretty intense UI) - New Video Recording with SDK for games to integrate with - Friends / Chat - Broadcasting games - The client itself, for Windows, macOS, Linux) And a whole lot more (forums, profile customization, guides) reply csande17 19 hours agorootparentAccording to the article, Valve is in fact currently maintaining all those features with a team of ~80 people. So this is a good opportunity to improve your understanding of the number of people it takes to maintain software. One thing Valve seems to understand better than most big tech companies is that once a product/feature is done, you don't need the same size team to maintain it. A lot of parts of Steam have been functionally unchanged for the last few years and just... keep working. The people who worked on Cloud Saves have presumably moved on to something else, instead of entering a Google-esque cycle of pointless Cloud Saves feature launches and deprecations. reply c-hendricks 18 hours agorootparentOh, at my day job we are about 5 engineers maintaining and adding features to support thousands of users, I'm aware of how few people are actually needed to maintain and build software. My objection was to the phrase \"Steam is not complex\", which I guess wasn't the posters point. reply ilrwbwrkhv 14 hours agorootparentprevI think the real trick is to not hire twitter engineers. reply shpx 19 hours agoparentprevI've been joking that Steam on macOS is so bad that there must be no one at Valve who has used it because there's no way you ship such brazenly slow software knowingly, but with 79 people this is actually probably true. reply rasz 17 hours agorootparentApple screwing you over twice will do that to your products targeting Apple users. reply bathtub365 15 hours agorootparentHow did Apple screw them over twice? reply rasz 12 hours agorootparenthttps://blog.greggant.com/posts/2024/06/25/half-life-for-the... reply ryandrake 19 hours agoparentprevI've never seen these companies (that are supposedly everywhere) that have all this \"deadweight\". Every company I've worked at has had 4X to 10X more work than enough staff to do the work. We beg for headcount to hire more so we can actually do what we set out to do, but we never have enough. I guess these places exist, but I've never really seen it myself. reply xingped 18 hours agorootparentClearly you haven't worked in any particularly big companies. The whole ship is deadweight. reply raxxorraxor 9 hours agoparentprevCompare it to Uber, which employs 30k people and they \"just\" mediate transport contracts and still thousands of those are engineers. Or Airbnb which is 5k+ employees I believe. Steam is another data point that convinces me that public ownership of companies leads to quick and ruthless enshitification and that growth is only a good metric in specific contexts. reply Log_out_ 20 hours agoparentprevValve tried to avoid the biggest cause of bloat, hierarchys and silo chieftains. reply Calavar 21 hours agoprevRide share and delivery app companies could take some notes on how to stay lean and profitable while operating an online marketplace. I know ride share apps are more complicated, but I don't think we're talking two orders of magnitudes more complicated (300 employees vs. 30,000 at Uber). The bloat at Uber/Lyft/Doordash is insane. reply tokai 21 hours agoparentBut isn't the whole idea for those companies to burn a lot of money to show investors that big things are happening? reply tgsovlerkhgsel 20 hours agorootparentThe whole idea is to attract market share to show investors that big things are happening. I don't think being pointlessly unprofitable is attractive to investors... reply Log_out_ 20 hours agorootparentGentlemen,let me give you the tour of our campus,here we develop shiny thing, our department for buzzword and of cause the lab for X reply szundi 20 hours agoparentprevWhen every city or even district is forcing you to honor arbitrary rules there… your sw becomes complicated reply jiggawatts 20 hours agorootparentSteam has to deal with local tax regulations, censorship rules, distribution rights, etc… reply arccy 20 hours agorootparentyou deal with countries and a few big publishers, not a few thousand cities each who want to be special reply gjsman-1000 21 hours agoparentprevOne involves delivering software to license holders. The other has to deal with the possibility of humans using their app as an assault or kidnapping mechanism; from either end (rider attacking driver; driver kidnapping passenger, etc.) Very different levels of responsibility. This can be seen, for example, by the size of Lyft and Uber’s security teams. reply djhaskkka 20 hours agorootparentyou wish their headcount was for the customer safety team! their lobbying team is probably larger than this one. reply gjsman-1000 20 hours agorootparentFalse, according to OpenSecrets, lobbying is under $2M a year. https://www.opensecrets.org/federal-lobbying/clients/summary... reply uyeieiii 20 hours agorootparentthat doesn't even cover the salary of all the obama admin people uber hired. let alone the actual lobbying going on. that's just what fall under some specific tax code. reply specialist 20 hours agorootparentprevThis is based on US Senate data. Does it include state and local lobbying? reply stefan_ 21 hours agoparentprevIs it lean when your biggest payroll is \"admin\"? More like fattened up and held up by big margins and little competition. reply lolinder 20 hours agorootparentIf they're breaking things down as is usual then \"Admin\" includes, among other jobs, the entire customer support team, which I can vouch for as exceptionally good by the standards of internet platforms. reply stefan_ 20 hours agorootparentIt's 35 people, that most certainly does not include customer support, which will be outsourced to some low bidder as is customary. reply db48x 20 hours agorootparentprevDo you think the leadership at Uber _isn’t_ making bank? reply dmurray 20 hours agorootparentprevIt's sorted alphabetically, not by expenditure. \"Admin\" is less than \"Games\" in every year. reply bongoman42 21 hours agoprevThe real scandal is that it is 2024 and people still don't understand how redaction works. reply gjsman-1000 21 hours agoparentThe real scandal is that PDF Software is still stupid enough to not notice that a user is trying to do a redaction and offer to do it correctly. This is the programmer’s fault, not the user’s. It’s not a huge bar either. If the user has set a background color to solid black, and is filling black text with it, the intent is pretty obvious. reply djbusby 21 hours agorootparentWho owns that Issue? Who allocated budget to address? It's a Team with a Leader that misses things, not some first-year doing things \"wrong\". reply gjsman-1000 21 hours agorootparentIf it was just Adobe, blame Adobe. But can you tell me your favorite open-source PDF reader on Linux is immune? (Edit, because some people miss the point and just want to be pedantic: an editor.) reply Am4TIfIsER0ppos 20 hours agorootparentA reader is immune from letting you think a change you're making is redaction by not letting you make a change. reply mejutoco 20 hours agorootparentprevI can empathize with the feeling but this is the case in many many contexts. Reading the intent of a user without a clear mental model of software is difficult, sometimes impossible. I know no os that changes the format of an image when a user changes the file extension, for example, which is a pretty similar situation. reply solarkraft 17 hours agorootparentThe criticism is that it’s not done even when reading the user’s intent is plenty possible, like in this case. (how bad UIs are is generally an insult to all the amazing engineering behind them) reply gjsman-1000 20 hours agorootparentprevAnother inexcusable sharp edge of computing that nobody cares about. I propose we recognize it for what it is - a sharp edge, and don’t act like people are stupid for doing what seems intuitive and logical. reply jimjimjim 20 hours agorootparentcalm the farm. people don't even know what a file extension is. reply memco 19 hours agorootparentprevI know at least Preview on MacOS has this feature now: if you try to put a black bar over text it tells you this isn’t secure and offers to redact for you using a method that both visually replaces all characters and removes the embedded text underneath It is non-destructive until saved meaning you have a window in which to make changes before the data is gone. reply LordShredda 21 hours agorootparentprevLet's be honest, it's an Adobe issue reply jimjimjim 19 hours agorootparentprevRedaction is not simple. What should the software do if the user's black box is drawn over an image, some text and part of a spline on a page? Modify the image and recompress it even if it's lossy?, find the text run and remove the obscured characters (what about a half covered letter?), somehow modify spline and possibly replace it with several smaller ones if the redaction is in the middle of it? All while taking into account any transforms that have been applied to the page. Also what if the black box is over part of a pdf form? Is it just the appearance that gets redacted or the form data (which is separate). Same questions if the black box is drawn over annotations. And what about other page content like embedded videos or 3d content, good luck redacting those. But if you only redact some things the user might then justifiably expect it to redact everything. reply pyb 20 hours agoprevHardware engineers making half of what software engineers make. A reminder of the stark difference between the two job markets. reply jbernsteiniv 19 hours agoparentIt's not just the people designing the hardware but also those maintaining it. I work in an \"Infrastructure Engineer\" position (I hate that since it also correlates with construction but mine is strictly expensive servers in data centers) and I make less money than the people running the virtual infrastructure even though that all runs on top of the machines I manage, lol. I once had an internal customer open an on call event to ask why one of their machines was running so slowly. I said \"it's because one of the DIMMs has thrown about 30,000 correctable errors within the past month\". I was able to correlate that by mapping the EDAC label for the DIMM recorded in /var/log/messages and some gzipped archives of the aforementioned log file. Of course I deal with CPU, memory, motherboards, GPUs, add-on NICs (OCP or PCIe), storage controllers (HBAs mostly, some RAID controllers), BMCs, and of course I also have to evaluate the link width of PCIe bridges interconnecting all the PCIe devices. reply wiseowise 20 hours agoprevAt this point Valve should open source all of their games if they’re not planning to develop new ones. reply talldayo 20 hours agoparentThey basically have. Half Life has been Open Source for a while, Portal and Half Life 2 were both more-or-less open along with the Source SDK. Team Fortress 2 and CS:GO aren't technically open, but their source does exist in the wild and can even be built on modern machines. That still leaves DOTA, Artifact and Half Life: Alyx without any proper source code, but the stuff people actually care about has mostly been freed. reply redox99 20 hours agorootparentThey are in no way open source. In fact Valve has killed some projects. reply stryan 19 hours agoparentprevThey've released five games in the past 4 years[0] and have another one in active closed testing[1], along with keeping DOTA2 running with major game changes and mini-games[2]. [0] https://en.wikipedia.org/wiki/List_of_Valve_games [1] Random article about it: https://gamerant.com/deadlock-gameplay-footage-valve-leak-6v... . [2] DOTA's had massive patches recently, including adding a new attribute, expanding the map, and redoing every hero to have innate abilities and a choice of passive effects. They've also been adding some pretty big PvE/external content events between Aghanim's Labyrinth and the current Crownfall battle pass (which just added a surprisingly fleshed out fighting game mini-game). reply wiseowise 12 hours agorootparentDota 2 is a prime example of enshifitication of modern games where content is added for the sake of content and regards for original style are completely ignored. Ironically, Dota 2 would benefit as a game if they STOPPED developing it and focused just on stability of the game, but alas. reply metadat 21 hours agoprevWhere does all the money go if it isn't being reinvested into the business? Are the massive profits immediately syphoned out by the execs and owners? It's interesting there isn't a well-known real competitor to Steam (the Blizzard and Ubisoft stores, etc, don't count, because so few external parties publish there). My gut says the lack of reinvestment could be someone else's opportunity, especially considering many of the longstanding VAC and other quality issues which have persisted on Steam's platform for more than 15 years. However, because of all the necessary catch-up, such an enterprise would be tricky to bootstrap and get to critical mass. Even then, where is the moat? reply lkramer 21 hours agoparentThere are plenty of competing stores, all of which pales in comparison. I don't know what their secret sauce is, but for me personally what makes a difference and why I keep preferring them is their dedication to Linux as a platform and a general feeling that they are not just driving me towards rent seeking. I know that last bit is slightly absurd, and I can't quite explain, but I think the fact that they are private means their priorities don't just exists as quarterly goals. reply doix 20 hours agorootparent> but for me personally what makes a difference and why I keep preferring them is their dedication to Linux as a platform and a general feeling that they are not just driving me towards rent seeking For what it's worth, I have the exact same feelings. They provide so much value on-top of just being a store, and it does feel like they take the money and re-invest it into their ecosystem to make it better. They made gaming on Linux as easy as gaming on windows. They improved the controller situation in games. They do the whole remote-play stuff so you can play local-coop games online. They do the family sharing stuff. They do the steam link stuff. And they don't lock any of it down to their first party offerings. e.g. * They could have easily made steamlink only work with their set-top box, but they just made it an android app. * They could have limited the controller bindings and tech to only their controller, but they made it work for all controllers. * They could have forced you to use their VR headset with steam, but they let you use any headset. I can't think of any other company off the top of my head that has done something similar. reply GuB-42 20 hours agorootparentprevSteam is actually made with their users in mind. So much that they managed to beat piracy. It is no easy feat, you can't beat piracy on price, and it is hard to beat in convenience when all you have to do is download a torrent and run an exe file. Piracy is like the final boss of software distribution platforms, but they beat it. To play a game, select it on the store, pay (with all sorts of payment options), download (from damn fast servers), and press play, that's all. The game sucks or crash, click a button and you are refunded, no need to be always online just for DRM check (eventually you'll need internet, but you have enough leeway so that it is rarely a problem), ads are very unobtrusive and maybe even desirable (suggesting games you may actually like or announcing sales), they don't usually force an update just before you want to play a game (a much too common occurrence), dark patterns kept to a minimum (or at least subtle enough), generally good software quality (in terms of bugs, broken UI/UX, etc...). You bought a game, it stays in your account, Steam has 20 years of history of not screwing you, and to play it, just install Steam on any PC (it it doesn't already have it), log in to your account, download and play, it is that easy. You can even share games (with some reasonable limitations). Others just have some annoyance here and there. Some always-on DRM, bugs, dubious UI choices, annoying ads, etc... Only Valve seems to understand the importance of not getting in the way between gamers and their games and do the appropriate amount of effort (which is a lot). Not even in the name of profit, because they understand that it is not worth it, if gamers are satisfied, profits will come. Remember, they have piracy as a competitor, and it is hard to compromise against such a powerful competitor. reply tgsovlerkhgsel 20 hours agorootparentprevMy guess is that the secret sauce is network effect. People have all their games in their steam libraries, so they want the next game in their steam library too. They perceive the Steam launcher as something positive that provides them convenience. The network effect also makes it hard for game companies not not be on Steam, allowing Valve to enforce some customer-friendly policies like refunds, further making the platform nice to use. Epic is trying to break through this by giving away endless free games. All other stores are generally forced upon the player through exclusives (you buy the game on Steam, but you still have to install their shitty launcher-store because the parent company owning the game wants some of the store-platform cake), breeding resentment and further cementing Steam's position. (GOG being an exception, but they don't have enough major/interesting games to get players to use them as their primary platform.) reply dangrossman 21 hours agoparentprevAt $1.3 million per employee in payroll expenses, it seems like much of the profits are being distributed to the employees that work there. reply metadat 21 hours agorootparentAgreed, that's generous. TFA also states Valve still makes $15m/employee p.s. thanks so much for hnreplies.com, Dan! It dramatically increased the utility of HN for me. Your creation is fantastic. reply awwaiid 17 hours agorootparentprevIiuc the payroll comes out before profit. reply upon_drumhead 21 hours agoparentprevGOG is a compelling option for a lot of content. DRM free and a huge library available reply djhaskkka 20 hours agorootparentI never even considered buying games again until gog. pay. download. run in dosbox or something. done. steam is just a crap I refuse to put up with. and that's the store. imagine the drm. reply tokai 20 hours agorootparentAlmost all games can be run without steam. reply BlueTemplar 19 hours agorootparentThis varies wildly depending on their level of DRM : https://www.gog.com/forum/general/how_to_run_steam_games_off... reply blackhawkC17 19 hours agoparentprevGabe Newell has a nice superyacht. Part of the profits surely went there :) https://www.superyachtfan.com/yacht/rocinante/ reply metadat 19 hours agorootparentThink he named The Rocinante after the one from The Expanse? Knowing that a not insignificant part of the thousands I've spent on Steam over the years went to fund that.. isn't a great feeling. reply bobthepanda 21 hours agoparentprevThere is Epic and Itch. The problem is that Steam for all its faults is DRM, anti-cheat, and a well functioning store rolled all in one. Most of the alternate stores are bad in one way or another; Epic launched without a cart, and it still doesn’t have user text reviews. Itch had to redo its whole UI after that one bundle they did that had thousands of games in it. Notably Valve doesn’t really use its position to make competing games. This is an unknown for most publishers, and Epic pretty blatantly pivoted Fortnite to eat PUBG’s lunch after working with them. If anything the leading platform for competition as an indie store is the Nintendo eShop. Which is probably why the Steam Deck exists. reply metadat 21 hours agorootparent> There is epic. How is Epic a serious competitor or alternative, then? reply bobthepanda 21 hours agorootparentIt is one of the most serious, in that it exists, and probably has one of the biggest catalogs next to Steam, which is not true of say whatever Ubisoft or EA is doing. at the very least there is substantial enough catalog overlap that one can price compare. reply Aerroon 20 hours agorootparentprevThere's also Nutaku that's supposedly as big as Steam (they are focused on NSFW games). Imo Epic is the only competitor to Steam that tries to offer a similar service to Steam. It's just popular to hate on it among pc gamers. Many people cite a lack of features on Epic's store as the reason, eg no forum feature, but then turn around and decry Steam forums as the worst place on the internet. I don't think the problem is a lack of features (for players, devs have a point), but rather the Epic Games Store isn't cool enough. reply pbhjpbhj 19 hours agorootparentEpic games seems to lack even basic functionality. I just added Win11 as triple boot on one computer, the kids have loads of games already installed from win10 in a tertiary drive ... on the website it says the solution to them not having a facility to tell their installer/launcher which folder to use is to reinstall all the games. Now, once you open Epic you can ignore it, use Explorer open the game folders and launch the games and they run. So it looks like they literally need a way to change a config file to avoid re-downloading 100s of GB of games. I find it literally crazy. They even scatter .egstore files around so actually scanning for and adding games to the library automatically should be easy. Steam makes the same task easy, and why wouldn't it it's pretty rudimentary for an installer/launcher to have a easy way for you to tell it where your downloaded games are. Oh, and the instructions for reinstalling Epic _launcher_ say to just re- download all your games. They're insane. YMMV. reply Aerroon 17 hours agorootparentI agree, but this is a small thing. It doesn't make or break your use of the software. reply bobthepanda 20 hours agorootparentprevI think that a lot of features on Steam don’t need to be copied, but I think Steam does have a giant moat in terms of actually useful features. Steam for example, allows user submitted ratings with explainer text. This is huge for online shopping decisions. Steam Workshop is easy to integrate mod distribution for games. Etc. reply red-iron-pine 3 hours agorootparent> Steam for example, allows user submitted ratings with explainer text. This is huge for online shopping decisions. Arguably, it requires an account, and has things like \"number of hours played\" included in the review. Far, far more useful than the easily-gamed spam reviews on places like Amazon. Doesn't mean spammy things can't happen, and the aggregate reviews on Steam usually mean what the 14-24 year old demo think reply Aerroon 17 hours agorootparentprev>Steam for example, allows user submitted ratings with explainer text. This is huge for online shopping decisions. I don't know anyone who actually relies on Steam ratings/reviews to make purchasing decisions. I've always assumed that rating systems on the platform that is selling you the product are not trustworthy. But then again, I find reviews for media to generally be useless. >Steam Workshop is easy to integrate mod distribution for games. That's a developer feature rather than a player one though. reply bobthepanda 16 hours agorootparentSteam reviews can be useful depending on their content and their general direction, particularly if they mention specific things they do or don’t like, and there are consistent things popping up. Game devs are not flush with money and I’ve never seen a flood of suspiciously positive reviews. Steam also has the features to do social proof of reviews. The reviews contain metadata about if the person got the game for free, and if you click the profile associated with it, you can see all the games a user has played and their achievement progress, which can help signal if they are a real gamer, how far they've gotten in the game they reviewed, but also how far they get in other games they've played to check that they're not a sock puppet. This has enough value that Epic displays a star rating that is allegedly user generated, but because it doesn't indicate who is doing the reviews, and there is no text, their star rating is a lot more suspect. --- Modding support is also a player feature. Official, well supported modding can make or break a game, and Steam facilitates quite a lot in that regard; Nexus, as an alternative, is ad supported and caps download speeds for mods unless you shell out for a premium membership. reply JSteph22 21 hours agoparentprevSteam is like any network effect based business. It's extremely difficult to dethrone when well entrenched. reply infecto 20 hours agorootparentI believe this is largely the case and I would add that there general pro-gamer attitude would make it quite difficult to dethrone. reply lkdfjlkdfjlg 20 hours agoparentprev> Are the massive profits immediately syphoned out by the execs and owners? Have you actually looked at the numbers or are you just parroting memes? In the numbers I'm looking at some divisions are $1M per employee. reply LordShredda 21 hours agoprev [–] Steam might have a very disproportionate market share, but no one complains a lot because the competition is downright laughable. Internet companies are very low maintenance It has a very low barrier to entry for devs and a permissive DRM compared to other stores. A powerful review system and a return policy better than anyone else I've seen so far. Very community friendly and has an entire feature dedicated to hosting user mods and creations. Right now they're wasting money on VR just because they can Is it a monopoly when no one else is able to compete even on the technical side? reply mrkramer 21 hours agoparent>Is it a monopoly when no one else is able to compete even on the technical side? Steam is 20 years in the making, so it is very hard to catch up with them. If you want to compete, you need to offer something different. reply 0cf8612b2e1e 20 hours agorootparentWhile Steam did have a rocky launch, the competitor stores since launched are laughably bad. Seemingly no attention to detail in making a product people would want to use. Epic has a huge war chest from its successes, but the application is still not at feature parity with Steam. reply devit 20 hours agorootparentprevIs there a reason why game developers can't just sell a key on their website using Shopify/WooCommerce/etc. and distribute the game files publicly via a CDN? Assuming they can get their game discovered via some sort of non-store channel (e.g. Reddit, social media, YouTube, Twitch, ads, etc.), the user can simply search the web for the game, find the website, click \"buy\" and proceed. Probably a bit more work than publishing on Steam but not that much. Seems worth it for large budget games or games that are very popular in a specific niche discoverable outside Steam. reply mrkramer 20 hours agorootparent>Is there a reason why game developers can't just sell a key on the web using Shopify/WooCommerce/etc. and distribute the game files publicly via a CDN? It's not that easy; Steam brings down dramatically complexity of self distributing and it offers a plenty of Web 2.0 features e.g. reviews, discussions forum, connecting with friends, playing with friends, chatting with friends etc. It's like asking why Amazon exists when sellers can have their own store page and distribution channel.... Some games that I know that are popular and not on Steam are for example Starsector and Escape from Tarkov. Game devs choose not to publish their game on Steam either because their game is not finished and they do not want to get negative reviewed to the ground and/or they think their game's brand is so strong that they do not want to pay 30% tax to Steam. For example Minecraft most likely wasn't on Steam because of aforementioned reasons. reply redox99 20 hours agorootparentprevSome games do that (Escape From Tarkov), but it's rare because it's just not worth it. reply popcalc 20 hours agorootparentprevTwo words: chargebacks and fraud. Keeping this to a manageable level requires scale. reply knowaveragejoe 20 hours agorootparentprevSome devs do essentially do that in addition to Steam. You're missing a million and one features on top of simply running the game itself if you restrict yourself to that, though. reply BlueTemplar 20 hours agorootparentprevStardock's (then Gamestop's) Impulse, Paradox' GamersGate, IGN's Direct2Drive are all competitors that are about as old as Steam. (Or at least were, these days I don't think they are much more than Steam keys resellers ?) reply redox99 20 hours agorootparentprevEpic Games Store is just as atrocious as it was years ago. It's not about time. reply poikroequ 20 hours agoparentprevMicrosoft could certainly compete, if they invested more into improving the experience on Windows, and released their own handheld device. reply BlueTemplar 20 hours agorootparentMicrosoft \"competed\" with Games For Windows Live (Dead). Though I guess that's now probably what Battle.net is ? reply poikroequ 15 hours agorootparentMicrosoft now has several games available to download for Windows, a far cry from the size of Steam's library, but it exists. If you have a Game Pass subscription, many of those games are available to install on Windows as well. Microsoft also supports streaming, both cloud streaming and locally from an Xbox. Yeah, I agree that Games for Windows Live was a joke. But Microsoft today is certainly capable of bringing competition, with the right leadership. reply BlueTemplar 20 hours agoparentprev [–] Well, since you mentioned DRM and return policy, let's not forget that GoG has no DRM, and unlike GoG's 30 day return policy Steam didn't even have one until Australia threatened to ban them. And the Workshop and Steam MP might be very convenient for some, but are walled garden features that ought to be boycotted. P.S.: More of a monopsony than a monopoly : https://medium.com/@KingFrostFive/steam-monopoly-monopsony-4... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Valve employs a surprisingly small workforce of just 336 staffers as of 2021, according to leaked payroll data from Wolfire’s antitrust lawsuit.",
      "The data reveals Valve's employee distribution across \"Admin,\" \"Games,\" \"Steam,\" and \"Hardware\" categories, with the \"Games\" payroll peaking at $221 million in 2017.",
      "Valve's small staff size and high profitability per employee, surpassing Google, Amazon, and Microsoft, explain its limited product list and reliance on external help for hardware and software."
    ],
    "commentSummary": [
      "Leaked payroll data shows Valve employs just 79 people to manage Steam, a major gaming storefront.",
      "Valve's small team is seen as efficient, focusing on stability over constant feature changes, unlike larger companies that often expand unnecessarily.",
      "This lean approach highlights Valve's emphasis on agility and efficiency, contributing to its success."
    ],
    "points": 93,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1720991979
  },
  {
    "id": 40968245,
    "title": "Creature that washed up on New Zealand beach may be rarest whale",
    "originLink": "https://www.cbsnews.com/news/creature-that-washed-up-on-new-zealand-beach-may-be-worlds-rarest-whale-a-spade-toothed-whale/",
    "originBody": "World Creature that washed up on New Zealand beach may be world's rarest whale — a spade-toothed whale Updated on: July 15, 2024 / 9:59 AM EDT / AFP Endangered whale species finds home in waters off New York, New Jersey Endangered whale species finds home in waters off New York, New Jersey 02:20 Wellington, New Zealand — Spade-toothed whales are the world's rarest, with no live sightings ever recorded. No one knows how many there are, what they eat, or even where they live in the vast expanse of the southern Pacific Ocean. However, scientists in New Zealand may have finally caught a break. The country's conservation agency said Monday a creature that washed up on a South Island beach this month is believed to be a spade-toothed whale. The five-meter-long creature, a type of beaked whale, was identified after it washed ashore on Otago beach from its color patterns and the shape of its skull, beak and teeth \"We know very little, practically nothing\" about the creatures, Hannah Hendriks, Marine Technical Advisor for the Department of Conservation, told The Associated Press. \"This is going to lead to some amazing science and world-first information.\" In this photo provided by the Department of Conservation, rangers Jim Fyfe and Tūmai Cassidy walk alongside what's believed to be a rare spade-toothed whale, on July 5, 2024, after its was found washed ashore on a beach near Otago, New Zealand. Department of Conservation / AP If the cetacean is confirmed to be the elusive spade-toothed whale, it would be the first specimen found in a state that would permit scientists to dissect it, allowing them to map the relationship of the whale to the few others of the species found and learn what it eats and perhaps lead to clues about where they live. Only six other spade-toothed whales have ever been pinpointed, and those found intact on New Zealand's North Island beaches had been buried before DNA testing could verify their identification, Hendriks said, thwarting any chance to study them. This time, the beached whale was quickly transported to cold storage and researchers will work with local Māori iwi (tribes) to plan how it will be examined, the conservation agency said. New Zealand's Indigenous people consider whales a taonga - a sacred treasure - of cultural significance. In April, Pacific Indigenous leaders signed a treaty recognizing whales as \"legal persons,\" although such a declaration is not reflected in the laws of participating nations. Nothing is currently known about the whales' habitat. The creatures deep-dive for food and likely surface so rarely that it has been impossible to narrow their location further than the southern Pacific Ocean, home to some of the world's deepest ocean trenches, Hendriks said. In this photo provided by the Department of Conservation, rangers inspect what's believed to be a rare spade-toothed whale on July 5, 2024, after it was found washed ashore on a beach near Otago, New Zealand. Department of Conservation / AP \"It's very hard to do research on marine mammals if you don't see them at sea,\" she said. \"It's a bit of a needle in a haystack. You don't know where to look.\" The conservation agency said the genetic testing to confirm the whale's identification could take months. It took \"many years and a mammoth amount of effort by researchers and local people\" to identify the \"incredibly cryptic\" mammals, Kirsten Young, a senior lecturer at the University of Exeter who has studied spade-toothed whales, said in emailed remarks. The fresh discovery \"makes me wonder - how many are out in the deep ocean and how do they live?\" Young said. The first spade-toothed whale bones were found in 1872 on New Zealand's Pitt Island. Another discovery was made at an offshore island in the 1950s, and the bones of a third were found on Chile's Robinson Crusoe Island in 1986. DNA sequencing in 2002 proved that all three specimens were of the same species - and that it was one distinct from other beaked whales. Researchers studying the mammal couldn't confirm if the species went extinct. Then in 2010, two whole spade-toothed whales, both dead, washed up on a New Zealand beach. Firstly mistaken for one of New Zealand's 13 other more common types of beaked whale, tissue samples - taken after they were buried - revealed them as the enigmatic species. New Zealand is a whale-stranding hotspot, with more than 5,000 episodes recorded since 1840, according to the Department of Conservation. More from CBS News 77 pilot whales die in mass stranding on Scottish beach New search for 1969 murder victim mistaken for Rupert Murdoch's wife Gallery's Picasso exhibit that sparked gender war wasn't painter's work 1st Tulsa Race Massacre victim from mass graves ID'd as WWI vet In: Whales © 2024 AFP. All Rights Reserved. This material may not be published, broadcast, rewritten, or redistributed.",
    "commentLink": "https://news.ycombinator.com/item?id=40968245",
    "commentBody": "Creature that washed up on New Zealand beach may be rarest whale (cbsnews.com)90 points by Brajeshwar 4 hours agohidepastfavorite4 comments quuxplusone 1 hour agoStop clickbait: Spade-toothed whale, https://en.wikipedia.org/wiki/Spade-toothed_whale reply MattGaiser 54 minutes agoparentThis seems to confirm the title. reply MPSimmons 28 minutes agorootparentI clicked JUST to see the name of the whale. It should be in the title. reply pvaldes 1 hour agoprev [–] A male of a rare species of Mesoplodon, a relatively big (5m), pelagic toothed whale. Very interesting discovery that will allow to look at their DNA and find its position in the family. This group of animals are the least studied big mammals alive and the family has increased a lot in the last years. https://www.stuff.co.nz/nz-news/350343353/rare-whale-washes-... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A rare spade-toothed whale, the world's rarest whale species, washed up on a New Zealand beach, offering scientists a unique research opportunity.",
      "This five-meter-long beaked whale is only the seventh specimen ever found, with no live sightings recorded, making it a significant discovery for marine biology.",
      "The whale has been transported to cold storage for examination, involving local Māori iwi, and genetic testing to confirm its identification could take months."
    ],
    "commentSummary": [
      "A rare spade-toothed whale was found on a New Zealand beach, offering a unique chance for DNA analysis.",
      "This species is among the least studied large mammals, sparking increased scientific interest."
    ],
    "points": 90,
    "commentCount": 4,
    "retryCount": 0,
    "time": 1721054204
  },
  {
    "id": 40964852,
    "title": "ZeroMQ: High-Performance Concurrency Framework",
    "originLink": "https://zeromq.org/",
    "originBody": "Documentation Community The Guide Wiki GitHub Twitter ZeroMQ An open-source universal messaging library Get Started Why ZeroMQ? ZeroMQ (also known as ØMQ, 0MQ, or zmq) looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fan-out, pub-sub, task distribution, and request-reply. It's fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems. Universal Connect your code in any language, on any platform. Smart Smart patterns like pub-sub, push-pull, and client-server. High-speed Asynchronous I/O engines, in a tiny library. Multi-Transport Carries messages across inproc, IPC, TCP, UDP, TIPC, multicast and WebSocket Community Backed by a large and active open source community. The Guide Explains how to use ØMQ with 60+ diagrams and 750 examples in 28 languages Used by Microsoft Samsung AT&T Spotify Facebook Digital Ocean Auth0 Bitcoin Jupyter Mongrel2 Jina Get Started © 2024 The ZeroMQ authors Code of ConductThis site is powered by Netlify",
    "commentLink": "https://news.ycombinator.com/item?id=40964852",
    "commentBody": "ZeroMQ: High-Performance Concurrency Framework (zeromq.org)88 points by klaussilveira 16 hours agohidepastfavorite49 comments thesuperbigfrog 15 hours agoZeroMQ was followed by nanomsg (https://nanomsg.org/) and nng (https://nng.nanomsg.org/) Some of Martin's rationale here: https://250bpm.com/blog:23/index.html ZeroMQ is still widely used and popular, but I am not sure if it is still actively developed. reply jvanderbot 15 hours agoparentI've used zeromq, nanomsg, and nng. The differences are subtle and focused on native library support, background threading model, and other systems level things. All of them are based on specs that are widely published. I've had zero problems implementing real robotic systems in nng, zmq, etc. And it is so damn easy to use it's amazing to me the whole world doesn't use it. reply taspeotis 8 hours agorootparent> it's amazing to me the whole world doesn't use it. I Googled nng and apparently it's this? [1] It's written in C so if you click the issues tab the second and third issues are \"IPC - Use After Free\" and \"Setting TLS config option via nng_socket_set_ptr causes access violation if you free config.\" Why would you want the world to build other software on this? [1] https://github.com/nanomsg/nng reply jvanderbot 3 hours agorootparentWell good to know, but I've never used TLS, since the apps were P2P over a secure overlay, so encrypting payloads was sufficient. reply rcxdude 10 hours agorootparentprevI've had some bad experiences with zeromq in robotic systems contexts: it's very assert-happy, and therefore tends to bring down the whole process in corner cases, and it's quite difficult to debug. It caused me quite a lot of headache and I'm no longer particularly enamored of the approach (the internal architecture is one which makes error propagation very difficult, so even if the individual bugs were fixed there's not a good overall handling strategy). reply AnonHP 13 hours agorootparentprev> And it is so damn easy to use it's amazing to me the whole world doesn't use it. Perhaps many of them are locked into “the cloud” and “serverless”, by default choosing the proprietary solutions offered by these providers on their platforms? reply throwaway984393 14 hours agorootparentprevI'm pretty sure the world doesn't use it more because it doesn't have a flashy marketing campaign and trendy developer tools/libraries don't have a plugin for it. If a whole bunch of developers aren't writing blog posts about it, does it even exist? Plus, it's old, so it's bad. reply rcxdude 10 hours agorootparentzmq was pretty dang trendy for a while. It had a well styled website and very wide language support. I think it just failed to deliver on its promise for most users. reply nlnn 11 hours agoparentprevI've been looking at these recently for a project. nng looks promising, but the guide from zmq seemed like a killer feature. It describes all sorts of high level patterns, gotchas, etc. For nng I mostly found API documentation, which made me a bit more cautious (though to be fair, I've not tried it yet). reply masfoobar 11 hours agorootparentI have used ZeroMQ with C, Dlang, and Python -- mostly for learning. However, I have used NetMQ.. a C# implementation of ZeroMQ in live software and the results are very positive! I used a Pub-Sub pattern for one program to keep users informed on progress of a task, which could have taken hours to complete. They had a GUI program which spits out updates. It worked really well. I was also tasked updating a Till software, which the integration of orders to the central system was extremely slow. I used NetMQ which was looking extremely successful but was put on hold due to IT manager not understanding Software Developers -- if something starts taking more than a week to do (which I stated I needed a month) they get itchy and move onto to something else. Sadly, that never got completed. Now, I have played with NNG and there are some interested articles (or hidden pages from memory) about companring NNG to ZeroMQ - it seems the \"patterns\" are simplified. I am currently in the progress of creating bindings for NNG. Seems to be pretty good, so far. I plan to move away from NetMQ (C#) in favour of this language moving forward. Whether you use ZeroMQ or NNG - I dont think you can go wrong. It is all about the process more than anything, ensuring you do not lose data. reply CharlieDigital 5 hours agorootparentIn 2014, I was tasked with rebuilding an event processing engine to increase throughput and performance. Used ZeroMQ with C# and also had a very positive experience. It was very easy to build a multi-node, distributed event processing engine (think Apache Flink) that could scale by simply adding more nodes or threads. ZMQ makes coordination and management of messages easy and low-fanfare. In our use case, it was stable and it was the least problematic part of a relatively complex platform. reply mindslight 14 hours agoparentprevI looked at 0mq et al, and what I couldn't understand is why sockets have a single exclusive type. Like say I want a process that generally sends out streaming broadcast updates, but is also controlled through request-reply. It would need two separate sockets. Then I'd have to reinvent some sort of ordering protocol across the two (as well as program logic to handle the partially-connected state), which would defeat much of the point of using 0mq? reply nurettin 11 hours agorootparentWithout knowing any details, it sounds like a hard problem whether you use 0mq or not. reply mindslight 5 hours agorootparentThe problem generally needs to be solved by the transport protocol. Having two sockets communicating in parallel means the problem needs to be solved again. reply exe34 10 hours agorootparentprevnews = -setup broadcast socket- orders = -setup req rep socket- while 1: ----order = orders.read (with timeout) ----orders.send(ack) ----if order: --------do stuff differently ----news.send(status) reply mindslight 5 hours agorootparentAnd then how does a client know whether an `order` was processed before or after a specific `status`? If you start talking about adding sequence numbers or duplicating application data between `status` and `order` (and its reply), that's the creation of an ad-hoc ordering protocol I'm talking about. reply exe34 1 hour agorootparentthat doesn't sound like a socket level problem, it's more like an application level problem? how long does it take to obey-orders()? do you want to keep spitting out logs while doing it? maybe give orders a number, and the logs can include the order number and the progress? reply mindslight 21 minutes agorootparentIt's a problem that the transport level would normally solve and then provide a single coherent stream to the higher layer, but seemingly cannot with 0mq - the existence of two parallel streams means that events from each of those streams aren't necessarily received in relative order. And yes depending how long obey-orders() takes, there are actually two instances of this ambiguity per command - when in the log stream did the call start happening, and when in the log stream did it finish. Assigning serial numbers to commands on the request-reply channel and then having the log channel publish those serials is exactly the type of having to invent an ad-hoc ordering protocol that I was talking about. Of course this can be done, but at the expense of adding accidental complexity and unnecessary resource usage. There's a similar problem with MQTT where the spec explicitly disclaims the ordering of messages across topics. Lots of people just ignore this and write software that works perfectly fine in the real world where messages generally get transported in order anyway. But it's still technically incorrect and seems like a breeding ground for Heisenbugs. reply sigmonsays 14 hours agoprevI would rather use sockets. Not getting errors when a client times out is a bad api design to me. I've used zeromq an only kept it around for IPC. I may have been doing it wrong, but i personally want to know when clients disconnect/reconnect/etc. the API seems to hide all that from you and your send or recv just block. reply haneul 13 hours agoparentImo, ZMQ is more of an abstraction with which to design protocols, rather than a message queue ready to use, Kafka-style. I found unexpectedly that I needed to really read the entire manual and work through the worked examples and some of my own to start getting it, rather than the usual incremental read. So, you can set up a protocol using ZMQ such that you become aware when a client times out, and you can set behavior regarding the high water mark, and other things, but you have to actually do it explicitly - it's not required that you do it, because you can choose to maximize throughput or minimize latency instead. But, whatever protocol behavior / performance you want, you can pretty much build it with ZMQ. In Python, ZMQ was the only \"feasible for my not-a-network-engineer-self\" to get a system with 100μs latency with sufficient throughput and guarantees (when used for IPC, although not using the IPC transport type). gRPC was a lot less performant for me, granted it would've been more convenient, but the low latency was a hard need. Although, networks are one of my noobier areas, so I might be blind in many ways here. reply otabdeveloper4 4 hours agorootparentI don't see what ZMQ abstracts that you don't get in TCP already. Is this all just about having a common cross-language API for TCP? Wasn't \"BSD sockets\" supposed to be that? reply ohnoesjmr 10 hours agoparentprevPretty much this. I always found it crazy how zmq gained any traction at all. \"Oh, I have a req/resp workload\" - one of the sides restarts, goes out of rhythm with the state of the connection (whether its req or resp), unrecoverable errors. Every system I've seen use zmq usually use it without these fancy patterns (use yeet messages in any order), and usually have some sort of \"Is anybody there on the other side?\" message to combat the fact there is no way to introspect connection state (otherwise your writes just block at the high water mark), at which point it would have been easier to just use tcp. The whole thing to me reeks of mongo. It's great if you are completely incapable/incompetent of solving the problem properly. reply bheadmaster 9 hours agorootparent> at which point it would have been easier to just use tcp Funny, I always used ZeroMQ as a message-oriented TCP-like protocol that reconnects automatically. > It's great if you are completely incapable/incompetent of solving the problem properly. Oof. I think it's great if you don't want to mess with low-level socket details and just want to write a actor-like messaging protocol. reply packetlost 5 hours agoparentprevWe've used ZMQ rather successfully, but step 1 of ZMQ is don't use REQ/REP at all, ever. It has advantages primarily around throughput and abstracting details across different mediums (which is sometimes actually a detriment). reply 1vuio0pswjnm7 8 hours agoprevAppreciate the rosettacode-like (chrestomathy) approach to documentation: https://zguide.zeromq.org/ reply aitchnyu 13 hours agoprevAs a new dev in 2011, I was unsuccessfully making a http worker thing in Python with Zeromq and I was distracted by their docs were encouraging replacing callback architecture with message passing with inproc queues. reply notachatbot1234 10 hours agoparentIs this criticism still relevant? 2011 is 13 years (as in: a teenager's life) ago. reply aitchnyu 10 hours agorootparentFWIW most discussion about inproc is around 2010, per $SEARCHENGINE. Guess that architecture didnt catch on. reply mafuy 8 hours agoprevI've never used ZeroMQ. How does it compare to MPI? I assume it is easier to use? reply sidcool 15 hours agoprevAnything new with ZeroMQ? It's been around for a long time. reply joezydeco 14 hours agoparentThe licensing was changed about nine months ago. LGPL-3.0+ is out and MPL 2.0 is in. Very thankful for that. reply josephcsible 14 hours agorootparentWhy are you thankful for the MPL instead of the LGPL? Is there any advantage to the MPL other than being easier to incorporate MPL code into proprietary software? reply jillesvangurp 13 hours agorootparentMaking the software easier to use from a legal point of view was indeed the reason. They explained why they did this here: https://github.com/zeromq/libzmq/issues/2376 Bottom line is that their licensing with a static linking exception was kind of weird and creating a lot of issues combining zeromq code even with other open source licenses (like Apache 2.0). Interesting to see how they gathered permission to do this from the developer community. License changes like this are usually hard to realize unless you insist on copyright transfers. But in this case they managed to do it without that. So it was a collective decision. Hard to argue with that. reply CJefferson 10 hours agorootparentprevYes, GPL compatibility -- in particular GPL v2 isn't compatible with LGPL v3, but it is compatible with MPL. Several projects, including some I work on, only found out how much a mess (L)GPL v2 vs v3 is once important developers had passed away, meaning it's very hard to get out of the resulting mess. reply joezydeco 4 hours agorootparentprevMPL doesn't have an anti-TiVoization clause. The company I'm working for has a complete ban on (L)GPL3.0 source code. reply josephcsible 12 minutes agorootparentIs the ban because your company does TiVoization? If so, then that sounds like the (L)GPL3.0 is working as intended. reply joezydeco 0 minutes agorootparentYes, and yes. My project is an IoT node that requires secured and auditable software up and down the chain. We can't allow user replacement. We acknowledge that the spirit of LGPL3 is for a reason and it works for a lot of parties, just not us. masfoobar 11 hours agorootparentprevI always get confused when I see MPL... part of me panics thinking ZeroMQ went with the \"Microsoft Public License\" :-) reply tracker1 3 hours agorootparentSimilar here... The \"look but don't touch\" license they used for some things was kind of irksome in practice. reply sidcool 14 hours agorootparentprevAh ok. Was wondering why this was on HN front page today. :) reply joezydeco 3 hours agorootparentThis is probably not the reason. reply jobulanus 14 hours agoprev [–] NATS is the ZeroMQ of today. reply veggieroll 4 hours agoparentPeople are replying that NATS isn't the same as ZeroMQ. But, I think the missing piece here is that most people who are looking at ZeroMQ really just want something like NATS. There is a decent amount of effort required to make ZeroMQ work. Whereas, NATS just does the thing you wanted in the first place without all that effort. reply packetlost 5 hours agoparentprevThey're not even remotely the same thing. You could build NATS on ZeroMQ but not the other way around. ZeroMQ is more like a network/message-passing abstraction library while NATS is a service. reply veggieroll 4 hours agorootparentI'd 90% agree with this, except that NATS does have an embedded mode.[0] That plus the clustering, gateway, and leaf-node features gets it very close to what ZeroMQ is doing, though at a much higher complexity level that's maybe unnecessary depending on what you're doing. Embedding is exclusive to Go programs, so that's very limited compared to ZeroMQ. [0]: https://www.youtube.com/watch?v=cdTrl8UfcBo reply packetlost 3 hours agorootparentEven in embedded mode NATS is doing way more than ZeroMQ is, it's a full-blown message queue system while ZeroMQ at best gives you the tools to build a message queue. reply lgas 14 hours agoparentprevThey aren't really the same thing. NATS is a message queue/MOM, ZeroMQ is a smarter abstraction over sockets. reply BiteCode_dev 9 hours agoparentprevNATS looks a lot like crossbar.io, what's the differences? reply kromnomo 9 hours agoparentprev [–] I think https://zenoh.io is more of a successor or replacement. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ZeroMQ is an open-source, universal messaging library that functions as a concurrency framework, providing sockets for atomic messages across various transports like in-process, inter-process, TCP, and multicast.",
      "It supports multiple language APIs and operating systems, offering patterns such as pub-sub, push-pull, and client-server, making it fast and scalable.",
      "ZeroMQ is backed by a large and active open-source community and is used by major companies like Microsoft, Samsung, and Facebook."
    ],
    "commentSummary": [
      "ZeroMQ is a high-performance concurrency framework, but its active development status is currently unclear.",
      "Users have noted various issues and differences with ZeroMQ and its successors, nanomsg and nng, including threading models, error propagation, and specific bugs.",
      "ZeroMQ's licensing has changed to MPL 2.0, making it easier to use legally, and there are discussions about alternatives like NATS and zenoh.io."
    ],
    "points": 88,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1721010959
  },
  {
    "id": 40962829,
    "title": "A.I. Needs Copper. It Just Helped to Find Millions of Tons of It in Zambia.",
    "originLink": "https://www.nytimes.com/2024/07/11/climate/kobold-zambia-copper-ai-mining.html",
    "originBody": "An exploration site run by KoBold Metals in Chililabombwe, Zambia, in June. Credit... Zinyange Auntony for The New York Times A.I. Needs Copper. It Just Helped to Find Millions of Tons of It. The deposit, in Zambia, could make billions for Silicon Valley, provide minerals for the energy transition and help the United States in its rivalry with China. An exploration site run by KoBold Metals in Chililabombwe, Zambia, in June. Credit... Zinyange Auntony for The New York Times Listen to this article · 13:52 min Learn more Share full article 111 By Max Bearak Max Bearak reported from Zambia and Silicon Valley. He spoke with coders and geologists, diplomats and local people. Published July 11, 2024 Updated July 15, 2024, 11:00 a.m. ET Peering into their computer screens in California last year, the data crunchers watched a subterranean fortune come into focus. What they saw transported them 10,000 miles across the world, to Zambia, and then one more mile straight down into the Earth. A rich lode of copper, deep in the bedrock, appeared before them, its contours revealed by a complex A.I.-driven technology they’d been painstakingly building for years. On Thursday, their company, KoBold Metals, informed its business partners that their find is likely the largest copper discovery in more than a decade. According to their estimates, reviewed by The New York Times, the mine would produce at least 300,000 tons of copper a year once fully operational. That corresponds to a value of billions of dollars a year, for decades. The New York Times also reviewed an independent, third-party assessment of KoBold’s claims, which, while slightly more conservative than KoBold’s own, largely corroborated the size of the deposit. In a statement, KoBold said it expected the value of the mine to grow because it had yet to map the full extent of its highest-grade ore. It’s the first confirmed success for a company that hopes to radically transform the way we find metals critical not only to the tech industry but to the fight against climate change. The geopolitical significance is vast. KoBold’s find comes as the United States and China are increasingly clashing over global access to the minerals needed to manufacture clean-energy technologies. KoBold originated half a decade ago in the belated realization among Silicon Valley’s barons of what lay ahead. Their products had become the backbone of the United States economy. But their businesses couldn’t grow much further without a gargantuan increase in the mining of a handful of raw materials that make batteries, without which everything from cellphones to electric trucks simply can’t function. They needed far more copper, cobalt, lithium and nickel. Image KoBold workers drilling a core sample in Chililabombwe. Credit... Zinyange Auntony for The New York Times Image Evaluating core samples and entering the results into a database. Credit... Zinyange Auntony for The New York Times Image A brickmaking operation in Kawama, Zambia. There is little employment in the area. Credit... Zinyange Auntony for The New York Times Hundreds of new mines would be necessary, analysts calculated. And not just for consumer products, but for the house-sized lithium-ion batteries needed for backup on the nation’s power grids as solar and wind power ebbs and flows. A.I. data centers demand huge amounts of copper. Advanced weaponry requires nickel and cobalt. Over two decades of production, KoBold’s find in Zambia would yield enough copper for 100 million of today’s average-size electric vehicle batteries. “The more you realize how dependent we are on these technologies, the more you ask: How the hell were we so slow to the fact that we needed vast amounts of raw material to make it all possible?” said Connie Chan, a partner at Andreessen Horowitz, the biggest venture capital firm in the United States and an early investor in KoBold. The traditional mining industry lacked a convincing solution. Using exploration techniques largely unchanged in a century, the cost of new discoveries was rising while the pace of the finds slowed. Around the same time, the U.S. government had a similar lightbulb moment. America had become far too reliant on China for these essential resources. China had been heavily investing in global mining and metal processing, and controlled the production of 20 to 80 percent of its supply chains. The United States, on the other hand, has relatively few processing plants or mines, domestic or foreign, for most battery metals. The International Energy Forum, a research organization, recently estimated that the world would need between 35 and 194 large new mines for copper alone through 2050. That translates to between one and six new copper mines, every year, the size of the one KoBold plans to dig in Zambia. Image Kurt House, the KoBold chief executive. Credit... Mike Kai Chen for The New York Times Image KoBold employees on the Stanford University campus pointed out geological peculiarities on a stone bench. Credit... Mike Kai Chen for The New York Times Image Connie Chan, a partner at the venture capital firm Andreessen Horowitz, discussed KoBold’s technology at Stanford. Credit... Mike Kai Chen for The New York Times Investors from American and European private equity funds that collectively manage trillions of dollars in assets, including ones started by Silicon Valley giants like Bill Gates and Sam Altman of OpenAI, were joined by more traditional industrial companies in putting hundreds of millions of dollars into KoBold. It already has 60-odd exploration projects in various countries. In some cases, like Zambia, where production is expected to begin in the early 2030s, it plans to own stakes in the mines themselves. Its chief executive and co-founder, Kurt House, embraces the moneymaking potential of KoBold’s technology, which is the company’s alone to profit from. He is fond of saying “I don’t need to be reminded again that I’m a capitalist.” The work is about to get a lot less theoretical. KoBold is pumping $2.3 billion into its first mine and is negotiating tricky partnerships with contractors and governments alike. It is relying on the U.S. government to finance a new railway to export the copper. And, like the mining barons of yore, its leaders will soon be exposed to the social and environmental trade-offs that almost all mining poses. Mining and muon detectors Image Daniel Snowden-Ifft, left, prepared to lower the muon detector into the ground for a test in Los Angeles. Credit... Mike Kai Chen for The New York Times On a quiet residential street in Oakland, Calif., Tom Hunt, who leads a team of data scientists at KoBold, gathered colleagues on Zoom. His setup was quintessentially white-collar. He and his wife, Lauren, share a work-from-home space. When he got animated explaining something, she peeked in from the porch. The few clues to his life and work were on his desk, where a miniature model of the copper find in Zambia sat below a note from Lauren saying how much she loved him. Beaming in from Southern California was Daniel Snowden-Ifft, head of the physics department at Occidental College in Los Angeles. He had dedicated much of his career to looking for dark matter. “I spent 20 years and never found any,” he said. Now, he’s developing a device that KoBold might use to find especially valuable members of the periodic table. His gadget would be lowered into a drill hole, from where it would identify muons, infinitesimal subatomic particles, and send back density readings of the hidden underground world. It’s a technique that would be new to mining, but it has a proven, if unusual, record. Previously it’s been used to suss out the location of burial chambers in Egyptian pyramids. Researchers have studied its potential for pinpointing illegal cross-border tunnels. Image Tom Hunt and a colleague showed a digital model of drilling locations in Zambia. Credit... Mike Kai Chen for The New York Times Image The muon detector on the Occidental College campus in Los Angeles. Credit... Mike Kai Chen for The New York Times Image Mr. House, at Stanford University, explained how KoBold is looking for lithium in Canada. Credit... Mike Kai Chen for The New York Times The muon detector is at the futuristic end of KoBold’s growing database, called TerraShed. At the other are yellowing paper maps and typewritten reports gathering cobwebs in Zambia’s mining archives, many a century old or more, which KoBold is digitizing. Elsewhere, KoBold collects its own radar and magnetic readings by flying modified Cessnas over promising territory. TerraShed includes tens of millions of documents that can be overlaid to yield three-dimensional models of what might lie below. “We think we’re mostly done with the easy era of mining,” said Mr. Hunt, who joined KoBold after working at Google and other Silicon Valley companies. KoBold’s discovery is a case in point. The area of Zambia where it was found is known for copper. In fact, its name is Copperbelt Province. Still, nobody had been able to home in on this mile-deep vein until KoBold did, right under everyone’s noses. Image An abandoned open-pit mine in Chingola, Zambia. Credit... Zinyange Auntony for The New York Times Image An exploration map of the KoBold site in Chililabombwe. Credit... Zinyange Auntony for The New York Times Image A mining archive in Kalulushi, Zambia. Credit... Zinyange Auntony for The New York Times The economies of Zambia and its colonial-era predecessor, Northern Rhodesia, have long been defined by copper, a material that humans have sought for millenniums. Well before wire heralded the modern age of electricity, copper was essential to Bronze Age toolmaking. Many of Zambia’s best mines were originally spotted simply because colonial officials noticed that local people had already been mining them, sometimes for centuries. Finding other battery metals will present bigger challenges. Lithium, for instance, wasn’t widely sought until a few decades ago. Mr. House, the KoBold chief executive, said this was exactly why the exploration industry needed to get creative. “We don’t drill for metals, we drill for information,” he said. “It puts the science into eureka.” Mr. House estimated that TerraShed contained about 3 percent of the world’s available geological data. Back on Mr. Hunt’s Zoom screen, Audrey Lawrence, who manages TerraShed, raised her hands above her head and spread them out in trying to explain how big 3 percent already is. In doing so, she set off Zoom’s automatic fireworks reaction. Will Zambians benefit? Image A core-drilling rig at the KoBold site in Chililabombwe. Credit... Zinyange Auntony for The New York Times KoBold’s find looks like a proof of concept with potentially huge payout for investors. And the company’s partners include Zambia itself: The state mining company owns 20 percent. That Zambians will benefit, though, is far from a foregone conclusion. Mining has left waste piled across Copperbelt Province, spawning lawsuits. One case alleges that local rivers at one point ran bright blue with copper tailings. And despite a century of mining, Zambia remains one of the world’s least-developed and most indebted countries. “The value of copper that has left Zambia is in the hundreds of billions of dollars. Hold that figure in your mind, and then look around yourself in Zambia,” said Grieve Chelwa, a Zambian economist. “The link between resource and benefit is severed.” KoBold’s biggest investors are the heirs of that legacy of inequity. Copper from Zambia helped build the economies on which Silicon Valley fortunes are based. KoBold says it aims to uplift local communities, and has attracted some of Zambia’s top geologists back to the country from overseas. But given the specific type of mine KoBold plans to dig, it’s unclear if many locals will get hired. Underground mines, like this one will be, typically employ far fewer people than open-pit mines. Kennedy Bondola, 40, worked in a nearby open-pit mine for 15 years before it was nearly exhausted. He lives in Kawama, a village directly above KoBold’s find. Kawama is poor but resourceful. Its main road was busy with welders repairing household goods on a recent afternoon. Bar owners made their own liquor. Others smuggled maize flour into Congo, a mile or two away, where it can fetch double the price. Image Kasumbalesa, near the border with the Democratic Republic of Congo. Credit... Zinyange Auntony for The New York Times Image Kennedy Bondola at his hardware store in Kawama. Credit... Zinyange Auntony for The New York Times Image A farmer near Kawama. Agriculture is one of the few ways to make a living in the area. Credit... Zinyange Auntony for The New York Times With a new mine, Mr. Bondola said, there was only one way Kawama could go: up. “Maybe it will become a real town,” he said. KoBold’s team is still determining exactly where to dig the mine’s shaft. There is a lake directly above the ore, and a major highway, along with the village of Kawama. “For a find this valuable, there’s nothing on the surface we can’t move,” said George Gilchrist, a South African geologist who is leading KoBold’s exploration in Zambia. In an interview in the capital, Lusaka, the Zambian president, Hakainde Hichilema, said one way to ensure greater benefits was for his country to own more of the mine. He said he was pressing KoBold to increase the share owned by the state mining company to above one-third. The extra money “will allow us to invest in sectors that ordinarily are difficult for us,” he said. Mr. Hichilema is desperate for revenue. More than a third of his budget goes toward repaying international debts, leaving little for health and education. Most of the capital gets only a few hours of electricity per day. It will take more than a slightly bigger stake in one mine, however large the mine may be, to solve that. “The benefits of this mine for Zambia,” Dr. Chelwa said, “are purely theoretical at this point.” Image President Hakainde Hichilema of Zambia. Credit... Zinyange Auntony for The New York Times Image KoBold investors met with Mr. Hichilema last month in Lusaka. Credit... Zinyange Auntony for The New York Times Image Mfikeyi Makayi, center, head of KoBold Africa, and Mr. House, right, at the president’s official residence in Lusaka in June. Credit... Zinyange Auntony for The New York Times For the United States government, the benefits are far more clear. “We are the American beachhead in Africa,” said Jennifer Fendrick, KoBold’s director for government affairs. “And that’s how the government sees it.” Before joining KoBold, she was a senior State Department strategist on critical minerals. While the U.S. government hasn’t directly invested in KoBold, it is partially underwriting a $2.3 billion railway to the Angolan coast from Copperbelt Province, enabling copper to be more easily shipped to the United States. It’s Washington’s single-biggest investment to catch up to Beijing in Africa’s battery-metal sweepstakes. Recently, Mr. House prepared to meet face-to-face with Mr. Hichilema. He was accompanied by his main investors along with Linnisa Wahid, the acting U.S. ambassador. Mr. House, wearing a lapel pin with the American and Zambian flags, asked for Ms. Wahid’s help when he brought up the railway. “Your affirmation at that moment would be great,” he said. Image George Gilchrist examined a core sample. Credit... Zinyange Auntony for The New York Times Image Workers photographed core samples. Credit... Zinyange Auntony for The New York Times Image The photos will be used in the A.I.-powered database. Credit... Zinyange Auntony for The New York Times While U.S. mining interests in Zambia are progressing, accessing neighboring Congo’s cobalt reserves is trickier. China-based companies own or have major stakes in most cobalt-producing sites in Congo, which produced 76 percent of the world’s supply last year. Biden administration officials are debating whether to lift sanctions on a billionaire Israeli mining executive accused of creating a web of corrupt practices in Congo’s mines. The sanctions have dissuaded American companies from investing there. There are other ways the United States could procure battery metals, but they face big obstacles. Parts of the seabed are rich in critical minerals, but a dispute has raged for years over how, or whether, oceans should be mined. And more mines could be opened within the United States itself, but that prospect has drawn objections, particularly from Indigenous communities. Whether the United States sticks with its climate goals also hinges on this year’s presidential election. A win by former President Donald J. Trump would very likely severely curtail energy transition incentives. For now, the Biden administration is pushing ahead. “We need 25 times as much cobalt as we currently mine,” said Jose W. Fernandez, the State Department’s top energy official, in an interview in New York recently, not to mention all the other metals and minerals reshaping the global economy. “We’ve got to get into the ring. We’ll be skewered if we fail.” Image Credit... Mike Kai Chen for The New York Times A correction was made on July 11, 2024: An earlier version of this article stated incorrectly the projected production of the KoBold mine in Zambia. It is expected to yield 300,000 tons of copper per year, not per month. An earlier version also stated incorrectly the number of countries where KoBold operates. While the company has around 60 projects in various parts of the world, it does not operate in 60 countries. And, an earlier version described incorrectly the muon detector. The device identifies muons, it does not emit them. How we handle corrections Max Bearak is a Times reporter who writes about global energy and climate policies and new approaches to reducing greenhouse gas emissions. More about Max Bearak 111 Share full article 111 Our Coverage of Climate and the Environment News and Analysis Climate change is causing more wildfires to burn overnight, growing bigger, lasting longer and challenging the fire teams trying to control them. Amid soaring temperatures, hundreds of climate activists in New York are staging boisterous blockades and solemn marches at banks and insurers that support fossil fuel projects. Climate change is driving home insurance rates higher, but not always in areas with the greatest risk. Read More Ideas to Beat the Heat: As temperatures soar around the world, an array of practical innovations are emerging to protect people most vulnerable to the hazards of heat. A Red State Weatherman: Chris Gloninger said he was hired by a Des Moines television station to talk about global warming in his forecasts. That’s when things heated up. A Surprise After a Disaster: A study found that monkeys in Puerto Rico, reeling from a hurricane, learned by necessity to get along. It’s one of the first studies to suggest that animals can adapt to environmental upheaval with social changes. F.A.Q.: Have questions about climate change? We’ve got answers. ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=40962829",
    "commentBody": "A.I. Needs Copper. It Just Helped to Find Millions of Tons of It in Zambia. (nytimes.com)87 points by bookofjoe 22 hours agohidepastfavorite52 comments hn_throwaway_99 22 hours agoSo I guess \"A.I.\" is the new buzzword for how we describe all digital technology going forward. FWIW I thought this was a better article that described the actual tech used by KoBold, https://spectrum.ieee.org/ai-mining. Now, that article was written by the KoBold CEO, so there are certainly parts of it I'd take with a giant chunk of salt, but I think it's easier to read that article (and read past some of the AI buzzwords) to see how they're probably using (a) better surveying tech and (b) standard machine learning techniques to generate maps of potential deposits. reply Wowfunhappy 22 hours agoparentI generally dislike the term AI because it could reasonably describe most computer programs. However, in this case machine learning is involved, so even by a narrower definition calling it \"AI\" seems more than fair. reply chefandy 21 hours agoparentprev> So I guess \"A.I.\" is the new buzzword for how we describe all digital technology going forward I wonder if any companies are getting deals on compute for making a big splashy deal out of the part ML played in these processes. Kind of a B2B meets Twikstogrube Influencer marketing strategy, but instead of companies having cachet because a bunch of social media followers find them appealing, they actually manifest things in the physical world. That is a big hole in the Generative AI company sales pitch for a) non-early-adopter potential customers, and b) many others looking uneasily at the kind of resources they're tearing through when the only tangible things they've seen from it are a pitches for features they never asked for and don't care about, and very concerning faked images and videos for extortion, bullying, porn, and political shit. I'm not saying those things are all its good for, but the communication about the real-world value of this stuff has been pretty lacking, and the drawbacks have been understandably shouted from the rooftops, so they're probably preeeetttyyy thirsty for stories like this. reply pgorczak 21 hours agoparentprev“That means the conventional predictions are largely inference—and worse, they result in unquantified uncertainty.” Wild claim given the fact that Gaussian process regression / Kriging was invented in the 1960s in geoscience to do exactly what the article claims only their models do: “quantify uncertainty, which in turn guides our data collection, as the most uncertain rocks often represent the most valuable ones to sample” reply Mathnerd314 22 hours agoparentprevIMO AI means neural net. I get that people use it for other things, but that's what I use it to mean - there is just no other term that's easy to say. And at this point the idea of breaking problems down into \"neurons\" and activation patterns is inherent to most AI models. Here though the keywords are \"ensemble machine learning\" and \"Bayesian\" - they could have used a neural net for the machine learning but most likely it is just XGBoost or similar. https://ia.acs.org.au/article/2021/the-ml-technology-looking... mentions they are also doing full-physics joint inversions and computer vision, perhaps the vision is a neural net. reply varjag 21 hours agorootparentHistorically however AI didn't mean a neural net specifically. reply simonw 18 hours agorootparentYeah, historically AI incorporates most of modern computer science, dating back to the 1950s. reply varjag 4 hours agorootparentNo, not really. If you compare proceedings of even the earliest AI and CS conferences it is clearly not the case. reply simonw 3 hours agorootparentCan you expand on that with some examples or links? reply mistrial9 19 hours agorootparentprevmy friend, XGBoost is not a neural net-based method, it represents a best-of-breed for an alternative family of methods. \"While the XGBoost model often achieves higher accuracy than a single decision tree, it sacrifices the intrinsic interpretability of decision trees. For example, following the path that a decision tree takes to make its decision is trivial and self-explained, but following the paths of hundreds or thousands of trees is much harder.\" https://en.wikipedia.org/wiki/XGBoost reply hn_throwaway_99 18 hours agorootparentUnless the parent edited their comment, you misread it, because they are specifically saying XGBoost is not a neural network: > Here though the keywords are \"ensemble machine learning\" and \"Bayesian\" - they could have used a neural net for the machine learning but most likely it is just XGBoost or similar. I.e. they could have used a neural network but they probably used something else, that something else being XGBoost. reply ViktorRay 21 hours agoparentprevThe article you linked was written by the CEO of the company. So it’s not going to go into detail about possible negative impacts of the mining in the same way that the original NYtimes article does. reply shhsdydywhwhb 21 hours agoparentprevMl became ai because it's the base for ai. It's just what it is. Nonetheless LLM Made it a lot easier for people to understand that investment in ml is really really helpful reply saulpw 22 hours agoprevTurns out it won't be a paperclip maximizer but a battery optimizer. reply tonetegeatinst 15 hours agoparentSurprised it wouldn't be the rare silicon needed to make the AI chips...or the data storage it uses reply lifeisstillgood 22 hours agoprevCopper mining is insane - vast vast machines (like apartment buildings on wheels) because the ore extraction rate is so low. But this is a boon to a democratic light in the Southern Africa, (https://georgetownsecuritystudiesreview.org/2023/05/05/zambi...) here’s hoping they negotiate hard with the miners and stuff as much revenue into solid projects (roads hospitals schools etc) reply jm_l 21 hours agoparentHaving valuable natural resources historically seems to be a big detriment to stable democratic governance. reply robocat 21 hours agorootparenthttps://en.wikipedia.org/wiki/Resource_curse The resource curse, also known as the paradox of plenty or the poverty paradox, is the phenomenon of countries with an abundance of natural resources (such as fossil fuels and certain minerals) having less economic growth, less democracy, or worse development outcomes than countries with fewer natural resources. There are many theories and much academic debate about the reasons for and exceptions to the adverse outcomes. Most experts believe the resource curse is not universal or inevitable but affects certain types of countries or regions under certain conditions. reply throwaway211 18 hours agorootparentI take your resource curse and raise you a Prebisch-Singer hypothesis > Prebisch–Singer hypothesis argues that the price of primary commodities declines relative to the price of manufactured goods over the long term, which causes the terms of trade of primary-product-based economies to deteriorate. As of 2013, recent statistical studies have given support for the idea. https://en.wikipedia.org/wiki/Prebisch%E2%80%93Singer_hypoth... Derived demand (copper) is likely to be more price sensitive than demand for the end good (AI services) over the long run due to substitution that can occur in factor inputs. Meaning Prebisch-Singer's true again. reply kristianp 15 hours agorootparentI imagine that assumes we can find large new deposits to meet increasing demand. That's not happening with copper. The deposit in this article is an exception. reply throwaway211 12 hours agorootparentIt doesn't assume. Hence 'substitute'. long run refers to the amount of time for prices to adapt, not a short run dependence on one thing. Hence change in factor inputs. An example of a change in factor inputs was, for example, whale oil which was a big deal for lighting systems in the 19th century. But as cheap whales were running out, so whale oil, thought to be so important for many and which there were technical improvements in sourcing and usage, was shifted away from. As will be the case for copper, as it's a derived demand. The derived demand will be more price sensitive than the demand for the end service (lighting in the case of whales, or Open AI for copper). Not happening with copper in the short run will only make the long run change happen even (as time passes) faster. reply awinter-py 17 hours agorootparentprevacemoglu development econ paper on this worth a read follows postcolonial trajectories of coastal countries used for ports + trade vs inland countries used for resource extraction labor reply lifeisstillgood 11 hours agorootparentOoohhh - intriguing. And presumably lines up with the Prebish-Singer hypothesis- that ports can switch to substitute goods - also interesting that almost every major city globally is a port city / river city reply fuzzfactor 2 hours agorootparentThe math on that never looked very promising. All wealth arises from natural resources. A truly dense population can not be sustained in a location without local resources unless there is a way to import at least the necessities for survival from somewhere else where they are more abundant. Some of the best places for that, naturally are port cities, and for millennia merchant marine moves more goods more sensibly than most. Inland cities can get big more easily when there are abundant local resources, well developed, and if there is some huge excess of something like timber, coal, gold, or whatever that is widely desirable. A local market will develop first in the land of abundance. Then if it's possible to arbitrage in a world market, the cost will be paid to shuttle commodities to an international port. Where the trading merchants will be able to buy low and sell high in a way that is further out of reach for the local producers. Eventually the traders make more money on the same tonnage of natural resource flow than the extractors do, but the extractors got there first. Depending on what the resource owners do with that early advantage, and whether there is a regime in place which values the local resource more than human life, and stuff like that has a much bigger effect on the imbalance between producer and trader, as well as local versus international prosperity, and that's with raw commodities not yet subject to value-added manufacturing. Seems to me port cities mostly arose due to upstream export needs before they were utilized as major import hubs and mercantile centers. reply testrun 19 hours agorootparentprevNot always. Australia, Canada, Norway and USA as counter examples. reply energy123 17 hours agorootparentI would argue Australia suffers from a lite version of the resource curse. There's undue control over politicians and resulting political resistance to invest in things that would diversify economic complexity or go against mining interests. Norway however is a strong counter example. reply defrost 16 hours agorootparentThe key is whether or not a country, it's people and their representatives, are in control of the deal making wrt the resources within their boundaries. Australia and Canada are, their indigenous people less so, and we can argue about the quality of the many resource deals within Australian and Canadian borders - overall they do less well than Norway. This is in strong contrast to many African countries, Papua, and elswhere about the globe where often the key parts of government are wholly in the pocket of outside transnational corps who frequently have small divisions of PMC's (private militay contractors) for 'security' and land deals are forced through with near zero compensation to former land holders and NSR (Net Smelter Returns)leasing returns to the country and people are near non existant. The reality of what another peer commenter in this thread decsribed as > But this is a boon to a democratic light in ... Africa is anything but. eg: US PMC's in Africa .. acting for multiple clients, including China .. but not for Africans. https://inkstickmedia.com/an-american-mercenary-resurfaces-i... https://www.africaintelligence.com/central-africa/2020/12/01... Good ol' Erik Prince, doing for world peace what his sister did for US education. reply energy123 13 hours agorootparent> Australia and Canada are [in control of the dealmaking wrt resources] It's not a binary. It's a spectrum. The capability of Australians to control the deal making is diminished by the control that the mining industry has over elected representatives. reply defrost 12 hours agorootparentPlease don't incorrectly paraphrasestrawman my comments. Your point is implicit within: > and we can argue about the quality of the many resource deals within Australian and Canadian borders - overall they do less well than Norway. \"The mining industry\" should include energy extractors who, IMHO, do more harm to Austrlia than mining - many of the mining operations (not all by any means) are majority Australian owned|controlled with that money staying within Australia (even if with individuals rather than spread out across the entire community). Even significant energy extractors such as Santos are Australian companies .. it's literally an acronym of South Australia Northern Territory Oil Search, but they're no angels although perhaps arguably better than the non-Australian gas operators. reply fritzo 21 hours agorootparentprevWitness Bougainville reply oliwarner 9 hours agoparentprevHistorically speaking, the people and the environment come a very, very distant second and third place to mine owners. Even moreso these days because China and North Korea have sponsored so much infrastructure in Africa over 20 years as European colonisation has pulled out, and bought so many mines and in-field processing plants that the local people are just an ablative grease for mechanised and chemical mining. reply kristianp 21 hours agoprev> We need 25 times as much cobalt as we currently mine Is that really true? I've lost significant money investing in explorers that hoped the Cobalt price would stay high. It hasn't, but also its hard to compete with Congo's mines. reply mschuster91 21 hours agoparentWell... most current lithium battery chemistries absolutely require cobalt, and given the demand for EVs and grid-scale PV storage there's bound to be a lot of interest for it. Unfortunately for cobalt miners, there's significant r&d investment into chemistries free of materials which are dominantly sourced from questionable countries/conditions - some driven by preparation for trade conflicts, and some driven by law (EU supply chain / anti slavery acts). And that is yielding its first results, e.g. [1]. [1] https://www.global.toshiba/ww/technology/corporate/rdc/rd/to... reply ClassyJacket 18 hours agorootparentBut notably, LFP batteries do not use cobalt, which includes the Rear Wheel Drive Tesla Model 3. reply energy123 17 hours agorootparentIsn't LFP now the dominant chemistry in grid scale storage? reply danans 13 hours agorootparentNot yet, NMC Li-ion is still the largest represented in grid scale battery storage [1]. Pumped hydro is dominant overall in storage. LFP represents an increasing share of new grid scale battery storage, though [2] 1. https://www.iea.org/energy-system/electricity/grid-scale-sto... 2. https://about.bnef.com/blog/2h-2023-energy-storage-market-ou... reply bookofjoe 22 hours agoprevhttps://archive.ph/82UT7 reply kristianp 10 hours agoprevIve heard that incresed demand for copper is through electrification of vehicles and increased demand on the grid to charge them, not so much AI. reply mproud 22 hours agoprevOf course they don’t explain how it found the copper, but I suppose the article is less about A.I. and more about the need for copper. reply technothrasher 17 hours agoparentWhen I was last in Zambia, I saw truck after truck full of huge copper sheets lined up at the border waiting to pay duty. I don't think it is any big news to anybody that there's copper there. It sounds like the AI just help find the extents of this particular deposit, not that it surprised anybody with secret copper. reply Am4TIfIsER0ppos 22 hours agoparentprevIsn't that half the point of new AI, or \"AI\" stuff? That it can't tell you how it knows X? reply HenryBemis 22 hours agoparentprevMy imagination tells me that they 'fed' it with all geographical/geological data available, and cross referenced them with various locations of mines (different ores)(gold, cobalt, copper, etc.) and the \"machine\" \"figured out\" that (silly example) river + mountain + over500m + earthquakes (or lack of) + various other parameters = so-and-so ore. There may be some 'coincidences'/similarities that require many parameters that the eye misses, while the \"AI\" can combine far more parameters. reply wyldfire 19 hours agoprevDon't make the mistake of bringing about Roko's basilisk too slowly. reply Loughla 19 hours agoparentThe problem with that is that someone like myself offers nothing for the AI. So why would it waste resources keeping me alive to torture me? reply entangledqubit 18 hours agorootparentTo set an example for others... :-P reply energy123 17 hours agorootparentThe thought experiment makes no sense. Once the basilisk is created it has no reason to create an incentive structure that applies to the past because the past is fixed. If the basilisk is actually superintelligence it's not going to fall for the sunk cost fallacy. reply iftheshoefitss 12 hours agoparentprevTurns out Roko boy is a few years off until the giant computer learns how to manipulate matter other stuff we can adapt to reply ta988 18 hours agoprevHumans need copper for building more machines that help them in everyday task. Humans used the machine to find more copper. reply awinter-py 17 hours agoprevtelegraph needs copper. it just helped find some reply big-green-man 14 hours agoprevPaperclip maximizer confirmed. reply floppiplopp 12 hours agoprevFEED ME, MEATBAGS! reply ein0p 19 hours agoprev [–] Exclusive footage from Zambia: https://m.youtube.com/watch?v=C7BCZCWlvEc reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "KoBold Metals has discovered a massive copper deposit in Chililabombwe, Zambia, potentially the largest in over a decade, with an annual production estimate of 300,000 tons.",
      "The discovery, driven by KoBold's A.I. technology, has significant geopolitical implications, aiding the U.S. in its rivalry with China over essential minerals for clean-energy technologies.",
      "Backed by investors like Bill Gates and Sam Altman, KoBold plans to invest $2.3 billion in the mine, with production expected to start in the early 2030s, while the U.S. government supports a $2.3 billion railway for copper exports."
    ],
    "commentSummary": [
      "AI technology has been used to discover millions of tons of copper in Zambia, highlighting its potential in resource exploration.",
      "The increased demand for copper is driven by vehicle electrification and grid demand, not directly by AI.",
      "The discovery is significant due to the rarity of large new copper deposits, providing a substantial economic boost for Zambia."
    ],
    "points": 87,
    "commentCount": 52,
    "retryCount": 0,
    "time": 1720986938
  }
]
