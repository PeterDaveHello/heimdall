[
  {
    "id": 38992601,
    "title": "Vanna.ai: Generating SQL queries through chat interface",
    "originLink": "https://github.com/vanna-ai/vanna",
    "originBody": "GitHub PyPI DocumentationVanna Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality. 0802.mp4 How Vanna works Vanna works in two easy steps - train a RAG \"model\" on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database. Train a RAG \"model\" on your data. Ask questions. If you don't know what RAG is, don't worry -- you don't need to know how this works under the hood to use it. You just need to know that you \"train\" a model, which stores some metadata and then use it to \"ask\" questions. See the base class for more details on how this works under the hood. User Interfaces These are some of the user interfaces that we've built using Vanna. You can use these as-is or as a starting point for your own custom interface. Jupyter Notebook vanna-ai/vanna-streamlit vanna-ai/vanna-flask vanna-ai/vanna-slack Getting started See the documentation for specifics on your desired database, LLM, etc. If you want to get a feel for how it works after training, you can try this Colab notebook. Install pip install vanna There are a number of optional packages that can be installed so see the documentation for more details. Import See the documentation if you're customizing the LLM or vector database. import vanna as vn Training You may or may not need to run these vn.train commands depending on your use case. See the documentation for more details. These statements are shown to give you a feel for how it works. Train with DDL Statements DDL statements contain information about the table names, columns, data types, and relationships in your database. vn.train(ddl=\"\"\" CREATE TABLE IF NOT EXISTS my-table ( id INT PRIMARY KEY, name VARCHAR(100), age INT ) \"\"\") Train with Documentation Sometimes you may want to add documentation about your business terminology or definitions. vn.train(documentation=\"Our business defines XYZ as ...\") Train with SQL You can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy and paste those from your editor to begin generating new SQL. vn.train(sql=\"SELECT name, age FROM my-table WHERE name = 'John Doe'\") Asking questions vn.ask(\"What are the top 10 customers by sales?\") You'll get SQL SELECT c.c_name as customer_name, sum(l.l_extendedprice * (1 - l.l_discount)) as total_sales FROM snowflake_sample_data.tpch_sf1.lineitem l join snowflake_sample_data.tpch_sf1.orders o ON l.l_orderkey = o.o_orderkey join snowflake_sample_data.tpch_sf1.customer c ON o.o_custkey = c.c_custkey GROUP BY customer_name ORDER BY total_sales desc limit 10; If you've connected to a database, you'll get the table:CUSTOMER_NAME TOTAL_SALES 0 Customer#000143500 6757566.0218 1 Customer#000095257 6294115.3340 2 Customer#000087115 6184649.5176 3 Customer#000131113 6080943.8305 4 Customer#000134380 6075141.9635 5 Customer#000103834 6059770.3232 6 Customer#000069682 6057779.0348 7 Customer#000102022 6039653.6335 8 Customer#000098587 6027021.5855 9 Customer#000064660 5905659.6159 You'll also get an automated Plotly chart: RAG vs. Fine-Tuning RAG Portable across LLMs Easy to remove training data if any of it becomes obsolete Much cheaper to run than fine-tuning More future-proof -- if a better LLM comes out, you can just swap it out Fine-Tuning Good if you need to minimize tokens in the prompt Slow to get started Expensive to train and run (generally) Why Vanna? High accuracy on complex datasets. Vanna’s capabilities are tied to the training data you give it More training data means better accuracy for large and complex datasets Secure and private. Your database contents are never sent to the LLM or the vector database SQL execution happens in your local environment Self learning. If using via Jupyter, you can choose to \"auto-train\" it on the queries that were successfully executed If using via other interfaces, you can have the interface prompt the user to provide feedback on the results Correct question to SQL pairs are stored for future reference and make the future results more accurate Supports any SQL database. The package allows you to connect to any SQL database that you can otherwise connect to with Python Choose your front end. Most people start in a Jupyter Notebook. Expose to your end users via Slackbot, web app, Streamlit app, or a custom front end. Extending Vanna Vanna is designed to connect to any database, LLM, and vector database. There's a VannaBase abstract base class that defines some basic functionality. The package provides implementations for use with OpenAI and ChromaDB. You can easily extend Vanna to use your own LLM or vector database. See the documentation for more details. More resources Full Documentation Website Discord group for support",
    "commentLink": "https://news.ycombinator.com/item?id=38992601",
    "commentBody": "Vanna.ai: Chat with your SQL database (github.com/vanna-ai)448 points by ignoramous 16 hours agohidepastfavorite192 comments sagaro 2 hours agoAll these products that pitch about using AI to find insights from your data always end up looking pretty in demos and fall short in reality. This is not because the product is bad, but because there is enormous amount of nuance in DB/Tables that becomes difficult to manage. Most startups evolve too quickly and product teams generally tries to deliver by hacking some existing feature. Columns are added, some columns get new meaning, some feature is identified by looking at a combination of 2 columns etc. All this needs to be documented properly and fed to the AI and there is no incentive for anyone to do it. If the AI gives the right answer, everyone is like wow AI is so good, we don't need the BAs. If the AI gives terrible answers they are like \"this is useless\". No one goes \"wow, the data engineering team did a great job keeping the AI relevant\". reply lmeyerov 1 hour agoparentOur theory is we are having simultaneously a bit of a Google moment and a Tableau moment. There is à lot more discovery & work to pull it off, but the dam has been broken. It's been am exciting time to work through with our customers: * Google moment: AI can now watch and learn how you and your team do data. Around the time Google pagerank came around, the Yahoo-style search engines were highly curated, and the semantic web people were writing xml/rdf schema and manually mapping all data to it. Google replaced slow and expensive work with something easier, higher quality, and more scalable + robust. We are making Louie.ai learn both ahead of time and as the system gets used, so data people can also get their Google moment. Having a tool that works with you & your team here is amazing. * Tableau moment: A project or data owner can now guide a lot more without much work. Dashboarding used to require a lot of low-level custom web dev etc, while Tableau streamlined it so that a BI lead good at SQL and who understood the data & design can go much further without a big team and in way less time. Understanding the user personas, and adding abstractions for facilitating them, were a big deal for delivery speed, cost, and achieved quality. Arguably the same happened as Looker in introduced LookML and foreshadowed the whole semantic layer movement happening today. To help owners ensure quality and security, we have been investing a lot in the equivalent abstractions in Louie.ai for making data and more conversational. Luckily, while the AI part is new, there is a lot more precedent on the data ops side. Getting this right is a big deal in team settings and basically any time the stakes are high. reply tucnak 1 hour agorootparentIs that right? You do all that at Louie.ai? reply lmeyerov 46 minutes agorootparentYep. A lot more on our roadmap, but a lot already in place! It's been cool seeing how different pieces add up together and how gov/enterprise teams push us. While there are some surprising implementation details, a lot has been following up on what they need with foundational implementations and reusing them. The result is a lot is obvious in retrospect and well-done pieces carry it far. Ex: We added a secure python sandbox last quarter so analysts can drive richer data wrangling on query results. Except now we are launching a GPU version, both so the wrangling can be ML/AI (ex: auto feature engineering), users can wrangle bigger results (GPU dataframes), and we will move our own built-in agents to it as well (ex: GPU-accelerated dashboard panels). Most individual PRs here are surprisingly small, but opens a lot! reply zurfer 27 minutes agorootparentas someone building in this space, I am a bit surprised how many concepts you managed to combine in your last sentence. :'D I will bookmark: ... and we will move our own built-in agents to it as well (ex: GPU-accelerated dashboard panels). reply lmeyerov 13 minutes agorootparentThose are pretty normal needs for us and our users. A big reason louie.ai exists is to make easier all the years of Graphistry helping enterprise & gov teams use python notebooks, streamlit/databricks/plotly python dashboards, and overall python GPU+graph data science. Think pandas, pytorch, huggingface, Nvidia RAPIDS, our own pygraphistry, etc. While we can't those years of our lives back, we can make the next ones a lot better! reply zurfer 2 hours agoparentprevthat is correct. GPT-4 is good on well-modelled data out of the box, but struggles with a messy and incomplete data model. Documenting data definitely helps to close that gap. However the last part you describe is nothing new (BI teams taking credit, and pushing on problems to data engineers). In fact there is a chance that tools like vanna.ai or getdot.ai bring engineers closer to business folks. So more honest conversations, more impact, more budget. Disclaimer: I am a co-founder at getdot.ai :) reply bob1029 14 hours agoprevThe most success I had with AI+SQL was when I started feeding errors from the sql provider back to the LLM after each iteration. I also had a formatted error message wrapper that would strongly suggest querying system tables to discover schema information. These little tweaks made it scary good at finding queries, even ones requiring 4+ table joins. Even without any examples or fine tuning data. reply echelon 13 hours agoparentPlease turn this into a product. There's enormous demand for that. reply bob1029 13 hours agorootparentI feel like by the time I could turn it into a product, Microsoft & friends will release something that makes it look like a joke. If there is no one on the SQL Server team working on this right now, I don't know what the hell their leadership is thinking. I am not chasing this rabbit. Someone else will almost certainly catch it first. For now, this is a fun toy I enjoy in my free time. The moment I try to make money with it the fun begins to disappear. Broadly speaking, I do think this is approximately the only thing that matters once you realize you can put pretty much anything in a big SQL database. What happens when 100% of the domain is in-scope of an LLM that has iteratively optimized itself against the schema? reply benreesman 7 hours agorootparentThere’s daylight between personal toolsmithing and a VC-backed startup (both are fun sometimes and a grind sometimes). I’m getting together a bunch of related-sounding stuff in terms of integrating modern models into my workflow to polish up a bit and release MIT. If you’d like to have a hand tidying it up a little and integrating it with e.g. editors and stuff, I think the bundle would be a lot cooler for it! reply panarky 3 hours agorootparentMicrosoft may well catch the rabbit that queries schemas and generates valid SQL. But that rabbit can't understand the meaning of the data just by looking at column names and table relationships. Let's say you want to know how sales and inventory are doing compared to last year at your chain of retail stores. Will Microsoft's rabbit be smart enough to know that the retail business is seasonal, so it must compare the last x weeks this year with the same weeks last year? And account for differences in timing of holidays? And exclude stores that weren't open last year? Will it know that inventory is a stock and sales is a flow, so while it can sum daily sales, it's nonsensical to sum daily inventory? The real AI magic isn't generating SQL with four joins, it's understanding the mechanics of each industry and the quirks of your organization to extract the intent from ambiguous and incomplete natural language. reply benreesman 1 hour agorootparentIf I can TLDR your comment, which I agree with: the real value is in doing real work. “Hustlers” burn countless hours trying to “optimize” work out of the picture. Historically, there’s a lot of money in just sitting down with a to-do list of customer problems and solving them at acceptable cost, come hell or high water. reply andy_ppp 12 hours agorootparentprevI will be extremely surprised if Microsoft build this for open source databases, however someone else will definitely build it if you don't, that is completely true :-) reply JelteF 12 hours agorootparentDisclaimer: I work at Microsoft on Postgres related open source tools (Citus & PgBouncer mostly) Microsoft is heavily investing in Postgres and its ecosystem, so I wouldn't be extremely surprised if we would do this. We're definitely building things to combine AI with Postgres[1]. Although afaik no-one is working actively on query generation using AI. But I actually did a very basic POC of \"natural language queries\" in Postgres myself last year: Conference talk about it: https://youtu.be/g8lzx0BABf0?si=LM0c6zTt8_P1urYC Repo (unmaintained): https://github.com/JelteF/pg_human 1: https://techcommunity.microsoft.com/t5/azure-database-for-po... reply hot_gril 4 hours agorootparentPostgres is dear to me. Met its founders when I was in college at Berkeley, worked heavily with it at a previous company around 2015, used it for all my own projects. I'm glad to see it getting more attention lately (seemingly). reply jzig 8 hours agorootparentprevSupabase already has an AI feature which queries your database for you [0] [0]: https://supabase.com/blog/studio-introducing-assistant reply hot_gril 5 hours agorootparentprevMicrosoft owns Citus, a very major Postgres plugin. reply andy_ppp 5 hours agorootparentI didn’t know this, it seems they love open source even thought they have competing commercial products. Maybe there is just more money is selling cloud than there is in selling commercial databases? reply dcreater 12 hours agorootparentprevYou can just make a GitHub repo with what you have. It'd still be valuable to the community reply rattray 6 hours agorootparentprevWould you be willing to share your prompts? I bet a lot of people would find them useful! reply giancarlostoro 7 hours agorootparentprev> I feel like by the time I could turn it into a product, Microsoft & friends will release something that makes it look like a joke. If there is no one on the SQL Server team working on this right now, I don't know what the hell their leadership is thinking. If Cortana for Azure isn't a thing in the works, I *really* don't know what the hell their leadership is working on. I could see insane value in \"why is my website slow?\" and getting actionable responses. reply whoiscroberts 12 hours agorootparentprevIf the do release it , they will only release it for enterprise. Many many sql server installs are sql server standard. There is an entire ecosystem of companies built on selling packages that support sql server standard, wee DevArt, RedGate. reply victor106 9 hours agorootparentprev> Microsoft & friends will release something that makes it look like a joke True, Microsoft & Friends have gotten greedy every passing year. Before they used to develop the platform (OS,DB etc.,) and let others develop and sell apps on it that would benefit them as well as the whole ecosystem. Now they want every last dollar they can squeeze out of the ecosystem. So they don't leave any stone unturned and they have big pockets to do that. reply personjerry 11 hours agorootparentprevWouldn't it be pretty fast to make it as a chatgpt? reply SOLAR_FIELDS 12 hours agorootparentprevThere are already several products out there with varying success. Some findings after I played with it awhile: - Langchain already does something like this - a lot of the challenge is not with the query itself but efficiently summarizing data to fit in the context window. In other words if you give me 1-4 tables I can give you a product that will work well pretty easy. But when your data warehouse has tens or hundreds of tables with columns and meta types now we need to chain together a string of queries to arrive at the answer and we are basically building a state machine of sorts that has to do fun and creative RAG stuff - the single biggest thing that made a difference in effectiveness was not what op mentioned at all, but instead having a good summary of what every column in the db was stored in the db. This can be AI generated itself, but the way Langchain attempts to do it on the fly is slow and rather ineffective (or at least was the case when I played with it last summer, it might be better now). Not affiliated, but after reviewing the products out there the data team I was working with ended up selecting getdot.ai as it had the right mix of price, ease of use, and effectiveness. reply Kiro 2 hours agorootparentprevWhat made you say this? How is this different from the hundreds of AI startups already focusing on this, or even the submission that we're having this conversation on? reply petters 13 hours agorootparentprevIt sounds like pretty standard constructions with OpenAI's API. I have a couple of such iterative scripts myself for bash commands, SQL etc. But sure, why not! reply l5870uoo9y 11 hours agorootparentprevYou can check this out https://www.sqlai.ai. It has AI-powered generators for: - Generate SQL - Generate optimized SQL - Fix query - Optimize query - Explain query Disclaimer: I am the solo developer behind it. reply vopi 8 hours agorootparentAre all those Twitter testimonials fake? None seem to be actual accounts. reply richardw 3 hours agorootparent- Generate testimonial (I kid. Hope you do well with the app, just get some real testimonials in there if they aren't already.) reply l5870uoo9y 1 hour agorootparentThanks. reply chrisjh 6 hours agorootparentprevShameless plug – we're working on this at Velvet (https://usevelvet.com) and would love feedback. Our tool can connect and query across disparate data sources (databases and event-based systems) and allows you to write natural language questions that are turned automatically into SQL queries (and even make those queries into API endpoints you can call directly!). My email is in my HN profile if anyone wants to try it out or has feedback. reply mcapodici 13 hours agorootparentprevI would be tempted to pivot to that! I am working on similar for CSS (see bio) but if that doesn’t work out my plan was to pivot to other languages. reply quickthrower2 13 hours agorootparentprevOr open source? You could get 10k stars :-) reply teaearlgraycold 13 hours agorootparentprevSomeone get YC on the phone reply vivzkestrel 5 hours agoparentprevwho are the most recent signed up users and what is their hashed password? what is stopping me from running this query on your database? reply sigmoid10 1 hour agorootparentWhat's stopping anyone who can run ordinary SQL queries? The LLM just simplifies interaction, it is neither the right tool nor the right place to enforce user rights. reply hot_gril 5 hours agorootparentprevSame thing stopping you from executing arbitrary SQL on the DB. reply bongodongobob 5 hours agorootparentprevI'm really curious as to the reasoning behind your question and why you think an LLM generated query somehow would have unfettered access and permissions. reply aussieguy1234 11 hours agoprevI've already done this with GPT-4. It goes something like this: Here's the table structure from MySQL cli `SHOW TABLE` statements for my tables I want to query. Now given those tables, give me a query to show me my cart abandonment rate (or, some other business metric I want to know). Seems to work pretty well. reply zainhoda 10 hours agoparentAuthor of the package here. That's pretty much what this package does just with optimization around what context gets sent to the LLM about the database: https://github.com/vanna-ai/vanna/blob/main/src/vanna/base/b... And then of course once you have the SQL, you can put it in an interface where you the SQL can be run automatically and then get a chart etc. reply lopatin 8 hours agoparentprevWhat surprises me is the amount of programmers and analysts that I meet the don’t do this yet. Writing complex, useful SQL queries is probably the most valuable thing that ChatGPT does for me. It makes you look like a god to stakeholders, and “I’m not strong in SQL” is no longer an excuse for analysis tasks. reply EZ-E 6 hours agorootparentSame for excel - I suck at it but ChatGPT is really good at writing these formulas. I feel this is one area LLMs are pretty good at. reply zainhoda 7 hours agorootparentprevI've noticed this as well. Do you have any theories as to why that is? reply bongodongobob 5 hours agorootparentI think a lot of people tried just asking GPT-3.5 to \"Write me full stack web app no bugs please.\" when it first came out. When it failed to do that they threw up their hands and said \"It's just a parrot.\" Then GPT4 came out, they tried the same thing and got the same results. I keep seeing comments regarding it not being helpful because \"super big codebase, doesn't work, it doesn't know the functions and what they do.\" ...so tell it? I've had it write programs to help it understand. For example: Write me a Python program that scans a folder for source code. Have it output a YAML-like text file of the functions/methods with their expected arguments and return types. Now plug that file into GPT and ask it about the code or use that when it needs to reference things. I've spent the last year playing with how to use prompts effectively and just generally working with it. I think those that haven't are definitely going to be left behind in some sense. It's like they aren't understanding the meta and the crazy implications of that. In the last year, I've written more code than I have in the last 5. I can focus on the big picture and not have to write the boilerplate and obvious parts. I can work on the interesting stuff. For those still not getting it, try something like this. Come up with a toy program. Tell it it's a software project manager and explain what you want to do. Tell it to ask questions when it needs clarification. Have it iterate through the requirements and write a spec/proposal. Take that and then tell it it's a senior software architect. Have it analyze the plan (and ask questions etc) but tell it not to write any code. Have it come up with the file structure and necessary libraries for your language. Have it output than in JSON or YAML or whatever you like. Now take that and the spec and tell it it's a software engineer. Ask it which file to work on first. Have it mock up the functions in psuedo code with expected arguments and output type etc. Tell it to write the code. And iterate as necessary. Do this a few times with different ideas and you'll start to get the hang of how to feed it information to get good results. reply upmostly 3 hours agorootparentNail. Head. The amount of code I now \"write\" (I've started calling it directing) and features I've put into my side projects has been more than the last 5-10 years combined this last year. I successfully created a sold a product within 3 months. Start to finish, because of the productivity power I received. People are misusing it. reply vincnetas 3 hours agorootparentCan you elaborate more on the product that you created and sold? reply hot_gril 5 hours agorootparentprevI already know SQL, and yes it can take time to form exceptionally complex queries, but ChatGPT doesn't seem to do those accurately (yet). CGPT is more useful for general programming languages where there's a lot more boilerplate. reply kulikalov 11 hours agoprevWhile I recognize the efforts in developing natural language to SQL translation systems, I remain skeptical. The core of my concern lies in the inherent nature of natural language and these models, which are approximative and lack precision. SQL databases, on the other hand, are built to handle precise, accurate information in most cases. Introducing an approximative layer, such as a language model, into a system that relies on precision could potentially create more problems than it solves, leading me to question the productivity of these endeavors in effectively addressing real-world needs. reply thom 9 hours agoparentThe key here is to always have some structured intermediate layer that can allow people to evaluate what it is the system thinks they're asking, short of the full complexity of SQL. You'll need something like this for disambiguation anyway - are \"Jon Favreau movies\" movies starring or directed by Jon Favreau? Or do you mean the other Jon Favreau? Don't one-shot anything important end to end from language. Use language to guide users towards unambiguous and easy to reason about compositions of smaller abstractions. reply totalhack 7 hours agorootparentI think AI to direct SQL will have use cases, but I personally have found it more of a fun toy than anything close to the main way I would interact with my data, at least in the course of running a business. I am a fan of a semantic layer (I made an open source one but there are others out there), and think having AI talk to a semantic layer has potential. Though TBH a good UI over a semantic layer is so easy it makes the natural language approach moot. https://github.com/totalhack/zillion reply pietz 10 hours agoparentprevI think you have a point. I also think people who share this mindset and keep sticking with it will have a tough future. The technical benefits and potentials clearly outshine the problems and challenges. That's also true in this example. You just have to let go of some principles that were helpful in the past but aren't anymore. reply kulikalov 10 hours agorootparentWhat technical benefits? reply pietz 10 hours agorootparentA translation engine between natural language and SQL means that everyone can communicate with a SQL database now. That's huge. Soon also DB people will use it to get the response for complex questions and queries. It's just more natural and way faster. With technology like this, there is little reason to even know SQL anymore as the average developer. Just like today, the average developer doesn't know how databases work because the cloud takes care of it. We're moving up on the abstraction ladder and tomorrow all you need to know for SQL is to ask the right question. reply bamboozled 10 hours agorootparentWhat’s funny is SQL was supposed to be the natural language everyone uses to communicate with. Few bothered to learn it. reply spennant 9 hours agorootparent\"Few\"? I guess I must be one of the olds. reply bamboozled 7 hours agorootparentAre you a “business person” or analyst ? reply totalhack 6 hours agorootparentprevThis somewhat already exists in the form of semantic layers, which if done well can handle many of the queries necessary in the course of business without business users needing to know any SQL. There will still be cases where you need direct SQL, and AI tools can help the development process there at a minimum. reply pietz 1 hour agorootparentYes, there will be cases where you need SQL knowledge. There will also always be cases where knowing exactly how a database works under the hood is necessary. I think this is somewhat of a weak argument because you can always construct an example of how something may be helpful for a small group of people. The relevant question is: How many people who work with databases need to have a lot of experience with SQL? My argument is that while the answer today is \"most,\" the answer in a couple of years might be \"very few.\" reply chrisjh 5 hours agorootparentprevI have similar thoughts on this - so few members of a team actually know what the underlying data model looks like. Gets even harder when you start trying to query across your database(s) + external sources like analytics/event systems. Natural language lets the whole team peel away at the black box of data and helps build common ground on which to improve products. I already mentioned it in another part of this thread so I won't spam my project but would love to get your feedback on my natural language to SQL tool if you're interested. reply macNchz 7 hours agorootparentprevLots of average developers these days do not (or just barely) know SQL, and it shows when the ORM generates some nonsense and nobody can figure out why the app is suddenly two orders of magnitude slower. reply bamboozled 10 hours agorootparentprevYour project manager can be an expert with SQL now too…and the novelty factor is high because using makes it feel like you’re in a “sci-fi” movie? reply samstave 11 hours agoparentprevReverse idea: Use this to POPULATE sql based on captured NLP \"surveillance\" -- for example, build a DB of things I say as my thing listens to me, and categorize things, topics, place, people etc mentioned. Keep count of experiencing the same things.... When I say I need to \"buy thing\" build table of frequency for \"buy thing\" etc... Effectively - query anything you've said to Alexa and be able to map behaviors/habits/people/things... If I say - Bob's phone number is BLAH. It add's bob+# to my \"random people I met today table\" with a note of \"we met at the dog park\" Narrate yourself into something journaled. Makes it easy to name a trip \"Hike Mount Tam\" then log all that - then have it create a link in the table to the pics folder on your SpaceDrive9000.ai and then you have a full narration with links to the pics that you take. reply kulikalov 10 hours agorootparentYou are describing use case for a vector database reply samstave 10 hours agorootparentSo you wouldnt be able to pull out specific phrases and store them in a typical RDBMS? reply codegeek 12 hours agoprevI have been keeping track of a few products like these including some that are YC backed. Interesting space as I am looking for a solution myself: - Minds DB (YC W20) https://github.com/mindsdb/mindsdb - Buster (YC W24) https://buster.so - DB Pilot https://dbpilot.io and now this one reply refset 11 hours agoparentIt's not a public facing product, but there was a talk from a team at Alibaba a couple of months ago during CMU's \"ML⇄DB Seminar Series\" [0] on how they augmented their NL2SQL transformer model with \"Semantics Correction [...] a post-processing routine, which checks the initially generated SQL queries by applying rules to identify and correct semantic errors\" [1]. It will be interesting to see whether VC-backed teams can keep up with the state of the art coming out of BigCorps. [0] \"Alibaba: Domain Knowledge Augmented AI for Databases (Jian Tan)\" - https://www.youtube.com/watch?v=dsgHthzROj4&list=PLSE8ODhjZX... [1] \"CatSQL: Towards Real World Natural Language to SQL Applications\" - https://www.vldb.org/pvldb/vol16/p1534-fu.pdf reply ignoramous 8 hours agorootparentSee also SQLCoder by defog.ai: https://github.com/defog-ai/sqlcoder reply zurfer 2 hours agoparentprevI would love to bring your attention also to getdot.ai We launched it on Hackernews with an analysis of HN post data. https://news.ycombinator.com/item?id=38709172 the main problems we see in the space: 1) good interface design: nobody wants another webapp if they can use Slack or Teams 2) learning enough about the business and usually messy data model to always give correct answers or say I don't know. reply kszucs 11 hours agoparentprevPlease add Ibis Birdbrain https://ibis-project.github.io/ibis-birdbrain/ to the list. Birdbrain is an AI-powered data bot, built on Ibis and Marvin, supporting more than 18 database backends. See https://github.com/ibis-project/ibis and https://ibis-project.org for more details. reply codyvoda 11 hours agorootparentnote that Ibis Birdbrain is very much work-in-progress, but should provide an open-source solution to do this w/ 20+ backends old demo here: https://gist.github.com/lostmygithubaccount/08ddf29898732101... planning to finish it...soon... reply hatsix 10 hours agorootparentsoon like \"check back in a month\", or \"Soon™\"? reply codyvoda 10 hours agorootparentthe \"check back in a month\" soon. I have versions of it that work but I just haven't been satisfied with. also, the major underlying dependency (Marvin) is going through a large refactor for v2. once that stabilizes a bit, I'm going to upgrade to it and that might simplify the code I need a lot reply lmeyerov 8 hours agoparentprevWe have been piloting louie.ai with some fairly heavy orgs that may be relevant: Cybersecurity incident responders, natural disaster management, insurance fraud, and starting more regular commercial analytics (click streams, ...) A bit unusual compared to the above, we find operational teams need more than just SQL, but also Python and more operational DBs (Splunk, OpenSearch, graph DBs, Databricks, ...). Likewise, due to our existing community there, we invest a lot more in data viz (GPU, ..) and AI + graph workflows. These have been through direct use, like Python notebooks & interactive dashboards except where code is more opt-in where desired or for checking the AI's work, and new, embedded use for building custom apps and dashboards that embed conversational analytics. reply pylua 11 hours agoparentprevI don’t fully understand the use of business case after reading the documentation. Is it really a time save? reply EmilStenstrom 11 hours agorootparentAllow people that don't know SQL to query a database. reply MattGaiser 11 hours agorootparentprevIt would be for people who are not that fluent in SQL. Even as a dev, I find ChatGPT to be easier for writing queries than hand coding them as I do it so infrequently. reply pylua 11 hours agorootparentYeah, same here. Seems like that approach is much simpler than this. I guess the real benefit here is that you don’t need to understand the schemas so the knowledge is not lost when someone leaves a company. Sort of an abstraction layer for the schemas reply totalhack 6 hours agorootparentSounds like you are describing a semantic layer. You don't need AI to achieve that, though it is fun when it works. Example of a semantic layer I made below, but there are others out there with more support behind them. https://github.com/totalhack/zillion reply realanswe91 11 hours agorootparentprevnext [2 more] [flagged] pylua 10 hours agorootparentThe initial part about sql already being that bridge was my first reaction, too. reply bredren 12 hours agoparentprevHave you written up any results of your experience with each? I’m interested in a survey of this field so far and would read it. reply codegeek 10 hours agorootparentNot yet but not a bad idea if I can get to test them all soon :) reply pamelafox 13 hours agoprevI love that this exists but I worry how it uses the term “train”, even in quotes, as I spend a lot of time explaining how RAG works and I try to emphasize that there is no training/fine-tuning involved. Just data preparation, chunking and vectorization as needed. reply zainhoda 10 hours agoparentAuthor of the package here. I would be open to alternative suggestions on terminology! The problem is that our typical user has never encountered RAG before. reply zacmps 6 hours agorootparentI've been using 'build' as in, 'build a dataset'. I think it gets the same idea across without being as easy to confuse with fine-tuning. reply hrpnk 13 hours agoprevPrompts are quite straightforward. - OpenAI: https://github.com/vanna-ai/vanna/blob/a4cdf7593ac0c584f7d74... - Mistral: https://github.com/vanna-ai/vanna/blob/a4cdf7593ac0c584f7d74... reply peheje 12 hours agoparentMany of these AI \"products\" - Is it just feeding text into LLMs in a structured manner? reply wruza 4 hours agorootparentWhy quotes. Your file manager is just feeding readdir into tableViewDataSource, and your video player is just feeding video files into ffmpeg and then connects its output to a frameBuffer control. Even your restaurant is just feeding vegetables into a dish. Most products \"just\" do something with existing technologies to remove the tedious parts. reply okwhateverdude 11 hours agorootparentprevBasically, yeah. It is shockingly trivial to do, and yet like playing with alchemy when it comes to the prompting, especially if doing inference on the cheap like running smaller models. They can get distracted in your formatting, ordering, CAPITALIZATION, etc. reply qiller 10 hours agoprevWe did something similar for our reporting service which is based duckdb. Overall it works great, though we've ran into a few things: * Even with low temperature, GPT-4 sometimes deviates from examples or schema. For example, sometimes it forgets to check one or another field... * Our service hosts generic data, but customers ask to generate reports using their domain language (give me top 10 colors... what's a color?). So we need to teach customers to nudge the report generator a bit towards generic terms * Debugging LLM prompts is just tricky... Customers can confuse the model pretty easily. We ended up exposing the \"explained\" generated query back to give some visibility of what's been used for the report reply ignoramous 9 hours agoparentCurious, as we're looking to build / use a similar setup. > Debugging LLM prompts is just tricky... Customers can confuse the model pretty easily. Would a RAG like how Vanna.ai uses, help? > For example, sometimes it forgets to check one or another field Do prompting techniques like CoT improve the outcome? > So we need to teach customers to nudge the report generator a bit towards generic terms. Did you folks experiment with building an Agent-like interface that asks more questions before the LLM finally answers? reply qiller 7 hours agorootparentOur primary issue is that our DB is a dynamic Entity-Attribute-Value schema, even quite a bit denormalized at that. The model has to remember to do subqueries to retrieve \"attributes\" based on what's needed for the query and then combine them correctly. NLQ is a somewhat new feature for us, so we don't have a great library to pull from for RAG. Experimenting, I found that having a few-shot examples with some CoT (showing examples of chaining attributes retrieval) sprinkled around did help a lot. Even still, some queries come out quite ugly, but still functional. I'm thankful that DuckDB is a beast when tackling those :D > Did you folks experiment with building an Agent-like interface that asks more questions before the LLM finally answers? That's something I want to figure out next: 1) try to check if a generated query would work but would generate absolutely junk results (cause the model forgot to check something) and ask to rephrase 2) or show results (which may look \"real\" enough), but give an ability to tweak the prompt. A good example is something like \"top 5 products on Cyber Monday\"All the podcasts I've been listening to recommend RAG over fine-tuning I'm always suspicious that is just because RAG is so much more accessible (both compute wise and in terms of expertise required). There's far more profit in selling something accessible to the masses to a lot of people than something only a niche group of users can do. I think most people who do actual fine tuning would still probably then use RAG afterwards ... reply zainhoda 6 hours agorootparentOne added benefit of RAG is that it's more \"pluggable.\" It's a lot easier to plug into newer LLMs that come out. If and when GPT-5 comes out, it'll be a one character change in your code to start using it and still maintain the same reference corpus. reply benjaminwootton 12 hours agorootparentprevDo you have any podcasts you would reccomend with this type of content? reply ajhai 13 hours agoparentprevWe can get a lot done with vector db + RAG before having to finetune or custom models. There are a lot of techniques to improve RAG performance. Captured a few of them a while back at https://llmstack.ai/blog/retrieval-augmented-generation. reply 331c8c71 13 hours agoparentprevYes from what I gather. And just to emphasize there's no LLM (re)training involved at all. reply kleiba 13 hours agoprevSorry, maybe I'm just too tired to see it, but how much control do you have over the SQL query that is generated by the AI? Is there a risk that it could access unwanted portions or, worse, delete parts of your data? (the AI equivalent of Bobby Tables, so to speak) reply thih9 13 hours agoparentWhy not give it access to relevant parts of the database only? And read only access too? reply htk 13 hours agoparentprevI guess you could limit that with the correct user permissions. reply bob1029 12 hours agoparentprevIn some SQL providers, your can define rules that dynamically mask fields, suppress rows, etc. based upon connection-specific details (e.g. user or tenant ID). So, you could have all connections from the LLM-enabled systems enforce masking of PII, whereas any back-office connections get to see unmasked data. Doing things at this level makes it very difficult to break out of the intended policy framework. reply iuvcaw 11 hours agoparentprevGuessing its intended use case is business analytic queries without write permissions —- particularly for non-programmers. I don’t think it’d be advisable to use something like this for app logic reply zainhoda 9 hours agorootparent100% -- in fact originally the package used to parse out SELECT statements and only execute SELECT statements. After some feedback, we decided that the permissions on the user should handle that level of detail. reply e12e 7 hours agorootparentYou can select from a procedure that change data, though? reply adam_gyroscope 9 hours agoprevWe did this at bit.io and people loved it - there a bunch of articles we wrote on what we found during our work: https://innerjoin.bit.io/ We’ve since shut down (acquired by databricks) but happy to answer what I can. reply zainhoda 9 hours agoparentSuper interesting! Why did you decide to shut down? reply adam_gyroscope 7 hours agorootparentAcquired by databricks - I can’t really speak about what we’re working on. reply osigurdson 14 hours agoprevI wish we had landed on a better acronym than RAG. reply spencerchubb 11 hours agoparentI'm pretty sure whoever coined the term just wanted to sound smart. Retrieval Augmented Generation is a fancy way to say \"put data in the prompt\" reply hliyan 5 hours agorootparentOr rather, run a cosine similarity search on a large data set that won't fit in the prompt, find only the bits that are relevant to the query, and put that in the prompt. reply bdcravens 13 hours agoparentprevRags are used for cleaning, and this gives you a cleaner interface into your data :-) reply nightski 14 hours agoparentprevIt doesn't matter, RAG is very temporary and will not be around long imho. reply mediaman 12 hours agorootparentRAG, at its core, is a very human way of doing research, because RAG is essentially just building a search mechanism for a reasoning engine. Much like human research. Your boss asks you to look into something, and you do it through a combination of structured and semantic research. Perhaps you get some books that look relevant, you use search tools to find information, you use structured databases to find data. Then you synthesize it into a response that's useful to answer the question. People say RAG is temporary, that it's just a patch until \"something else\" is achieved. I don't understand what technically is being proposed. That the weights will just learn everything it needs to know? That is an awful way of knowing things, because it is difficult to update, difficult to cite, difficult to ground, and difficult to precisely manage weights. That the context windows will get huge so retrieval will be unnecessary? That's an argument about chunking, not retrieval. Perhaps people could put 30,000 pages of documents into the context for every question. But there will always be tradeoffs between size and quality: you could run a smarter model with smaller contexts for the same money, so why, for a given budget, would you choose to stuff a dumber model with enormous quantities of unnecessary information, when you could get a better answer from a higher intelligence using more reasonably sized retrievals at the same cost? Likewise, RAG is not just vector DBs, but (as in this case) the use of structured queries to analyze information, the use of search mechanisms to find information in giant unstructured corpuses (i.e., the Internet, corporate intranets, etc). Because RAG is relatively similar to the way organic intelligence conducts research, I believe RAG is here for the long haul, but its methods will advance significantly and the way it gets information will change over time. Ultimately, achieving AGI is not about developing a system that \"knows everything,\" but a system that can reason about anything, and dismissing RAG is to confuse the two objectives. reply nightski 4 hours agorootparentThat's the problem, it's just search. If search was the answer, Google would of achieved AGI long ago. The problem is there's no intelligence. In some situations it can find semantically similar content, but that's it. The intelligence is completely missing from the RAG mechanism, because it's not even part of the model itself. reply zainhoda 6 hours agorootparentprevYes, 100%! Can you turn this comment into a blog post so that I can send it to people who make this claim? reply ren_engineer 13 hours agorootparentprevhow else would you get private or recent data into an LLM without some form of RAG? The only aspect that might not be needed is the vector database reply sroecker 14 hours agorootparentprevCare to enlighten us why? reply nkozyra 13 hours agorootparentMost of this stuff is replaced within a calendar year and that will probably accelerate. reply osigurdson 13 hours agorootparentprevIt sounds dumb to me. reply arbot360 12 hours agoparentprevREALM (REtrieval Augmented Language Model) is a better acronym. reply vinnymac 14 hours agoparentprevEvery single time I see it, I immediately think of Red Amber Green. reply benjaminwootton 12 hours agoprevI built a demo of something similar, using LlamaIndex to query data as it streamed into ClickHouse. I think this has a lot of real world potential, particularly when you move between the query and a GenAI task: https://youtu.be/F3Eup8yQiQQ?si=pa_JrUbBNyvPXlV0 https://youtu.be/7G-VwZ_fC5M?si=TxDQgi-w5f41xRJL I generally found this worked quite well. It was good at identifying which fields to query and how to build where clauses and aggregations. It could pull off simple joins but started to break down much past there. I agree with the peer comment that being able to process and respond to error logs would make it more robust. reply breadwinner 12 hours agoprevI have seen good results from just describing the schema to ChatGPT-4 and then asking it to translate English to SQL. Does this work significantly better? reply SOLAR_FIELDS 12 hours agoparentThat’s mostly what the products and libraries around this like llamaindex or Langchain are doing. If you look at the Langchain sql agent all it’s doing is chaining together a series of prompts that take the users initial query, attempt to take in a db and discover its schema on the fly and then execute queries against it based on that discovered schema, ensuring the result makes sense. The tough part is doing this at scale as part of a fully automated solution (picture a slack bot hooked up to your data warehouse that just does all of that for you that you converse with). When you have tens or hundreds of tables with relationships and metadata in that schema and you want your AI to be able to unprompted walk all of them, you’re then basically doing some context window shenanigans and building complex state machines to walk that schema Unfortunately that’s kind of what you need if you want to achieve the dream of just having a db that you can ask arbitrary questions to with no other knowledge of sql or how it works. Else the end user has to have some prior knowledge of the schema and db’s to get value from the LLM. Which somewhat reduces the audience for said chatbot if you have to do that reply zainhoda 9 hours agoparentprevThat's basically what happens but the power is that in Python, you can do this: sql = vn.generate_sql(question=...) Which means that now the SQL can be executed and you can get the table, chart, etc in any interface. Flask: https://github.com/vanna-ai/vanna-flask Streamlit: https://github.com/vanna-ai/vanna-streamlit Chainlit: https://github.com/vanna-ai/vanna-chainlit Slack: https://github.com/vanna-ai/vanna-slack reply kgdiem 9 hours agoprevThanks so much for making this and making it under MIT to boot. I’ve been thinking about how to do this for about 6 months now and just started working on a demo for querying just one table // JSON column today. I would feel a lot more comfortable with putting this (and/or my demo) into production if the database had been set up with a schema+db user per account rather than every tenant sharing just the one set of tables. reply zainhoda 6 hours agoparentAuthor of the package here. Apologies if this wasn't clear in the documentation, but what you're talking about is absolutely possible. Feel free to join our Discord -- we have other users who have this multi-tenant setup. reply miohtama 11 hours agoprevHow about instead of making AI wrappers to over 50 years old SQL, we’d make a database query language that’s easier to read an write? reply marginalia_nu 11 hours agoparentIn general, if something has been around for a very long time and nobody apparently seems to have thought to improve it, then odds are the reason is it's pretty good and genuinely hard to improve on. reply runlaszlorun 9 hours agorootparent> pretty good and genuinely hard to improve on I think that might be a bit more positive than I would be. Broadly speaking, I think you could say that the downsides of the legacy technology in question aren’t larger than the collective switching costs. But I’d definitely agree that when something has been around that long, it’s prob not all bad. reply SoftTalker 6 hours agorootparentprevBecause SQL is a good query language, if you bother to really learn it. reply aae42 11 hours agorootparentprevin other words, SQL is a shark, not a dinosaur reply fbdab103 3 hours agorootparentCodd himself had loads of complaints about SQL. reply realanswe91 11 hours agorootparentprevThe Shark Query Language reply aitchnyu 5 hours agoparentprevI'm watching Edgeql, I feel it could end the career of AI query generators, ORMs and low end BI. https://www.edgedb.com/ reply drittich 7 hours agoparentprevhttps://prql-lang.org/ might be an answer for this. As a cross-database pipelined language, it would allow RAG to be intermixed with the query, and the syntax may(?) be more reliable to generate reply neodymiumphish 11 hours agoparentprevMy fear with this approach is that the first implementation would be severely handicapped compared to SQL, amd it'd take years to support some one-off need for any organizational user, so it'd never be fully utilized. reply esafak 13 hours agoprevThe nitty gritty: https://vanna.ai/blog/ai-sql-accuracy.html reply hyuuu 8 hours agoprevim curious to know how you get around the hallucinations? for example, for the query: \"give methat were created yesterday\" the llm hallucinates as to what \"yesterday\" means, there are other instances as well where the generated SQL is valid syntax-wise but not in intent. This is especially dangerous for aggregation queries such as MAX, COUNT, etc because it will spit out a number but is it the right number? and the only way to check is to read the SQL itself and verify, which defeats the whole purpose of it. reply zainhoda 6 hours agoparentThat's a fair concern. In actual usage, however, the vast majority of hallucination (>90%) tends to be: - phantom tables and columns, in which case the query will fail - incorrect syntax for date functions (i.e. the wrong flavor of SQL) And we tend to see less of this type of hallucination when there are lots of example SQL queries that have been \"trained\" into the RAG system. reply FrostKiwi 2 hours agoprevA bit ironic, considering SQL statements were designed to read like English sentences. reply cpt100 5 hours agoprevI have tried things like this, they are good for debugging and asking simple questions but is really hard to train them to be good enough for production. You'll get enough frustrating results, that you'll abandon them soon. reply elietoubi 3 hours agoprevIf anyone is interested, I built and open-sourced parse.dev It's a rails app that allows you to talk to your database. reply holoduke 13 hours agoprevIt would be fun if you could actually train your raw sql and the llm output is the actual answer and not sql commands. In this way its just another language layer on top/in between of sql. Probably hurts efficiency and performance in the long run. reply jug 11 hours agoprevI wonder if this supports spatial queries as in PostGIS, SpatiaLite, SQL Server Spatial as per the OGC standard? I'm interested in integrating a user friendly natural language query tool for our GIS application. I've looked at LangChain and the SQL chain before but I didn't feel it was robust enough for professional use. You needed to run an expensive GPT-4 backend to begin with and even then, it wasn't perfect. I think a major part of this is that it wasn't actually trained on the data like Vanna apparently does. reply zainhoda 10 hours agoparentAuthor of the package here. I think that it probably could handle that but may need more example SQL queries. reply ajhai 13 hours agoprevWe have recently added support to query data from SingleStore to our agent framework, LLMStack (https://github.com/trypromptly/LLMStack). Out of the box performance performance when prompting with just the table schemas is pretty good with GPT-4. The more domain specific knowledge needed for queries, the harder it has gotten in general. We've had good success `teaching` the model different concepts in relation to the dataset and giving it example questions and queries greatly improved performance. reply l5870uoo9y 11 hours agoprevIs there a list of SQL generations to see how it performs? This is a list of SQL examples using GTP-4 and the DVDrental database sample. [1]: https://www.sqlai.ai/sql-examples [2]: https://www.postgresqltutorial.com/postgresql-getting-starte... reply dcreater 7 hours agoprevPerhaps an uninformed question, but why do we need an llm rather than a simpler natural language to SQL NLP translator? Wouldn't that be much more efficient and reliable? reply zainhoda 7 hours agoparentWhat would be missing from that is the ability to look up what tables are in the database, how they join together, what terminology the business uses, preferences around whether to exclude nulls from certain columns, etc. This allows you to ask more of a “business question” like “who are the top 10 customers by sales?” and the LLM will be able to construct a query that joins the customers table to an orders table and get the results that you’re looking for. With a simple NL to SQL, you’d have to say “join the customer table with the sales table, aggregate on the sales column from the orders table and limit the results to 10 rows” or something along those lines. reply bobbylarrybobby 7 hours agoparentprevPeople have been working on simple natural language processing algorithms since the 50‘s. If it were easy we'd have one by now. reply altdataseller 14 hours agoprevWhat's the origin behind the name Vanna? reply zainhoda 10 hours agoparentAuthor of the package here. It was originally because it's both a name and a finance term because I was originally using this to query a financial database. https://www.thebalancemoney.com/vanna-explanation-of-the-opt... reply booleandilemma 13 hours agoparentprevVanna White? It's the only Vanna I know. https://en.wikipedia.org/wiki/Vanna_White reply crimbles 11 hours agoprevI can't wait until it naively does a table scan on one of our several TB tables... reply arter4 14 hours agoprevI'm curious about how this performs with more complex queries, like joins across five tables. Also, does the training phase actually involve writing SELECT queries by hand? In the age of ORMs and so on, many people have probably forgotten how to write raw SQL queries. reply zainhoda 10 hours agoparentAuthor of the package here. Joining 5 tables is not a problem. The training does not necessarily require you to write the queries by hand. A trick that we've seen people do is to just train with DDL statements and then ask \"leading\" questions if it can't answer on the first try. I've been using the package myself for about 6 months and while I haven't forgotten SQL, what I have forgotten are the fully qualified table names and which tables live in which schemas etc since I never have to think about that. reply nkozyra 13 hours agoparentprevFrom my experience, GPT-4 will do just as well with joins as without. And that needs no specific, separate SQL training (which I assume tens of thousands of examples are already in). reply teaearlgraycold 13 hours agoparentprev> In the age of ORMs and so on, many people have probably forgotten how to write raw SQL queries. I’ve heard this general sentiment repeated quite a lot - mostly by people that don’t use ORMs. In my experience pretty quickly you reach the limits of even the best ORMs and need to write some queries by hand. And these tend to be the relatively complicated queries. You need to know about all of the different join types, coalescing, having clauses, multiple joins to the same table with where filters, etc. Not that this makes you a SQL expert but you can’t get too far if you don’t know SQL. reply darylteo 9 hours agorootparentORM abuse are absolutely rife in small-scale/volume build industries i.e. web agencies, outsourced crews 8/10 projects I look into don't have any indexes set up. Use of ORMs with little thought into lazily loaded relations lead to 100s of queries being done per request. It's pretty mad. Do not underestimate the propensity of a developer to stick to the only tool they know how to use. Unfortunately ORMs like Eloquent make it way too easy. reply teaearlgraycold 6 hours agorootparent> small-scale/volume build industries i.e. web agencies, outsourced crews Well that could explain it. I’ve only worked in companies where everyone working on the app codes with the expectation that they could be dealing with their mistakes for years. reply willsmith72 9 hours agoprevThe docs don't really work on mobile, the side navbar takes up half the screen and doesn't seem closeable reply sighansen 14 hours agoprevThis looks really helpful! I'm working a lot on graph databases and am wondering, if there are similar projects working with say neo4j. I guess because you don't have a schema, the complexity goes up. reply jazzyjackson 12 hours agoparentneo4j advertises such an integration on their landing page https://neo4j.com/generativeai/ reply account-5 14 hours agoprevWhat I'd really be interested in is being able to describe a problem space and have it generate a schema that models it. I'm actually not that bad at generating my own SQL queries. reply CharlesW 14 hours agoparentThis works pretty well without a dedicated application today, e.g. \"Knowing everything you do about music and music distribution, please define a database schema that supports albums, tracks, and artists\". If you have additional requirements or knowledge that the response doesn't address, just add it and re-prompt. When you're done, ask for the SQL to set up the schema in your database of choice. reply simonw 14 hours agorootparentYeah, GPT-4 is really good at schema design. ChatGPT can even go a step further and create those tables in a SQLite database file for you to download. reply account-5 14 hours agorootparentprevMaybe my prompting needs to improve, I tried recently to get chatgpt to provide a schema for an sqlite database that implements vcard data in a normalised way. I gave up... reply coder543 13 hours agorootparentChatGPT-3.5 or ChatGPT-4? There is a big difference. For fun, I just asked ChatGPT-4 to generate a normalized database representation of vcard information: https://chat.openai.com/share/1c88813c-0a50-4ec6-ba92-4d6ff8... It seems like a reasonable start to me. reply account-5 12 hours agorootparentChatgpt 3.5. Maybe I should pay for a couple of months access to 4 to see the difference. Is it worth the money? reply coder543 12 hours agorootparentChatGPT-3.5 isn’t even worth touching as an end-user application. Bard is better (due to having some integrations), but it’s still barely useful. ChatGPT-4 is on an another level entirely compared to either 3.5 or Bard. It is actually useful for a lot. ChatGPT-3.5 can still serve a purpose when you’re talking about API automations where you provide all the data in the prompt and have ChatGPT-3.5 help with parsing or transforming it, but not as a complete chat application on its own. Given the bad experiences ChatGPT-3.5 gives out on a regular basis as a chat application, I don’t even know why OpenAI offers it for free. It seems like a net-negative for ChatGPT/OpenAI’s reputation. I think it is worth paying for a month of ChatGPT-4. Some people get more use out of it than others, so it may not be worth it to you to continue, but it’s hard for anyone to know just how big of a difference ChatGPT-4 represents when they haven’t used it. I provided a sample of ChatGPT-4’s output in my previous response, so you can compare that to your experiences with ChatGPT-3.5. reply account-5 11 hours agorootparentYou sample completely blows away what I got out of 3.5. I'm now wondering if Bing is 3.5 or 4. But will likely fork out for a couple of months. reply burcs 12 hours agoparentprevWe actually built something that does this at Outerbase. ob1.outerbase.com it'll generate API endpoints as well, if you need them. reply new_user_final 14 hours agoprevDoes it work with Google/Facebook ads data? Can I ask it to show best performing ads from BigQuery Facebook/Google ads data by supermetrics or improvado. reply sonium 13 hours agoparentAren't there already tons of apps answering that specific question? I think the strength of this approach is answering the non-obvious questions. reply zainhoda 10 hours agoparentprevAuthor of the package here. Yes, this is a very common use case. reply neofrommatrix 11 hours agoprevI’ve done this with Neo4j. Pretty simple to hook it up with Open AI APIs and have a conversational interface. reply thecalebf 8 hours agoprevVery neat! I am building something very similar, called ChatDB.ai reply Vosporos 11 hours agoprevI can hire a DBA to tell me that my indexes aren't shit, no need for AI. reply teaearlgraycold 14 hours agoprevI haven't loaded this up so maybe this has been accounted for, but I think a critical feature is tying the original SQL query to all artifacts generated by Vanna. Vanna would be helpful for someone that knows SQL when they don't know the existing schema and business logic and also just to save time as a co-pilot. But the users that get the most value out of this are the ones without the ability to validate the generated SQL. Issues will occur - people will give incomplete definitions to the AI, the AI will reproduce some rookie mistake it saw 1,000,000 times in its training data (like failing to realize that by default a UNIQUE INDEX will consider NULL != NULL), etc. At least if all distributed assets can tie back to the query people will be able to retroactively verify the query. reply zainhoda 10 hours agoparentThis is a good idea. I think what you'd want to do is override the generate_sql function and store the question with the \"related\" metadata and the generated sql somewhere: https://github.com/vanna-ai/vanna/blob/main/src/vanna/base/b... We're going to be adding a generic logging function soon and fairly soon what you're talking about could just be a custom logger. reply metflex 13 hours agoprev [–] that's it, we are going to lose our jobs reply ta8645 10 hours agoparent [–] The extremely-lucrative days in IT for large numbers of people, are drawing to a close. On the other hand, while you won't make more than someone who works at 7/11, there will be rudimentary IT jobs available, if you want them. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Vanna is an open-source Python framework that generates SQL queries based on trained models, allowing users to ask questions and receive SQL queries as results.",
      "The framework provides user interfaces like Jupyter Notebook, Streamlit, Flask, and Slack.",
      "Vanna can be installed using pip and trained using DDL statements, documentation, or SQL queries. It offers high accuracy on complex datasets, supports any SQL database, and allows for self-learning. Users can also extend Vanna to use their own LLM or vector database."
    ],
    "commentSummary": [
      "The discussion explores different AI-powered tools and technologies for interacting with SQL databases, such as Vanna.ai, Louie.ai, and Microsoft's involvement in the field.",
      "It delves into the use of language models and natural language queries in SQL interactions, discussing the challenges and benefits of AI-assisted SQL.",
      "The conversation also touches on schema design, the limitations of current models, and the future potential of AI co-pilots for writing SQL queries, highlighting the growing interest in using AI to simplify database management and analysis tasks."
    ],
    "points": 448,
    "commentCount": 192,
    "retryCount": 0,
    "time": 1705255086
  },
  {
    "id": 38994817,
    "title": "Unraveling World of Warcraft's RNG: How One Teen Found a Way to Beat the Game",
    "originLink": "https://orlp.net/blog/when-random-isnt/",
    "originBody": "When Random Isn't 2024-01-10 This post is an anecdote from over a decade ago, of which I lost the actual code. So please forgive me if I do not accurately remember all the details. Some details are also simplified so that anyone that likes computer security can enjoy this article, not just those who have played World of Warcraft (although the Venn diagram of those two groups likely has a solid overlap). When I was around 14 years old I discovered World of Warcraft developed by Blizzard Games and was immediately hooked. Not long after I discovered add-ons which allow you to modify how your game’s user interface looks and works. However, not all add-ons I downloaded did exactly what I wanted to do. I wanted more. So I went to find out how they were made. In a weird twist of fate, I blame World of Warcraft for me seriously picking up programming. It turned out that they were made in the Lua programming language. Add-ons were nothing more than a couple .lua source files in a folder directly loaded into the game. The barrier of entry was incredibly low: just edit a file, press save and reload the interface. The fact that the game loaded your source code and you could see it running was magical! I enjoyed it immensely and in no time I was only writing add-ons and was barely playing the game itself anymore. I published quite a few add-ons in the next two years, which mostly involved copying other people’s code with some refactoring / recombining / tweaking to my wishes. Add-on security A thought you might have is that it’s a really bad idea to let users have fully programmable add-ons in your game, lest you get bots. However, the system Blizzard made to prevent arbitrary programmable actions was quite clever. Naturally, it did nothing to prevent actual botting, but at least regular rule-abiding players were fundamentally restricted to the automation Blizzard allowed. Most UI elements that you could create were strictly decorative or informational. These were completely unrestricted, as were most APIs that strictly gather information. For example you can make a health bar display using two frames, a background and a foreground, sizing the foreground frame using an API call to get the health of your character. Not all API calls were available to you however. Some were protected so they could only be called from official Blizzard code. These typically involved the API calls that would move your character, cast spells, use items, etc. Generally speaking anything that actually makes you perform an in-game action was protected. The API for getting your exact world location and camera orientation also became protected at some point. This was a reaction by Blizzard to new add-ons that were actively drawing 3D elements on top of the game world to make boss fights easier. However, some UI elements needed to actually interact with the game itself, e.g. if I want to make a button that casts a certain spell. For this you could construct a special kind of button that executes code in a secure environment when clicked. You were only allowed to create/destroy/move such buttons when not in combat, so you couldn’t simply conditionally place such buttons underneath your cursor to automate actions during combat. The catch was that this secure environment did allow you to programmatically set which spell to cast, but doesn’t let you gather the information you would need to do arbitrary automation. All access to state from outside the secure environment was blocked. There were some information gathering API calls available to match the more accessible in-game macro system, but nothing as fancy as getting skill cooldowns or unit health which would enable automatic optimal spellcasting. So there were two environments: an insecure one where you can get all information but can’t act on it, and a secure one where you can act but can’t get the information needed for automation. A backdoor channel Fast forward a couple years and I had mostly stopped playing. My interests had mainly moved on to more “serious” programming, and I was only occasionally playing, mostly messing around with add-on ideas. But this secure environment kept on nagging in my brain; I wanted to break it. Of course there was third-party software that completely disables the security restrictions from Blizzard, but what’s the fun in that? I wanted to do it “legitimately”, using the technically allowed tools, as a challenge. Obviously using clever code to bypass security restrictions is no better than using third-party software, and both would likely get you banned. I never actually wanted to use the code, just to see if I could make it work. So I scanned the secure environment allowed function list to see if I could smuggle any information from the outside into the secure environment. It all seemed pretty hopeless until I saw one tiny, innocent little function: random. An evil idea came in my head: random number generators (RNGs) used in computers are almost always pseudorandom number generators with (hidden) internal state. If I can manipulate this state, perhaps I can use that to pass information into the secure environment. Random number generator woes It turned out that random was just a small shim around C’s rand. I was excited! This meant that there was a single global random state that was shared in the process. It also helps that rand implementations tended to be on the weak side. Since World of Warcraft was compiled with MSVC, the actual implementation of rand was as follows: uint32_t state; int rand() { state = state * 214013 + 2531011; return (state >> 16) & 0x7fff; } This RNG is, for the lack of a better word, shite. It is a naked linear congruential generator, and a weak one at that. Which in my case, was a good thing. I can understand MSVC keeps rand the same for backwards compatibility, and at least all documentation I could find for rand recommends you not to use rand for cryptographic purposes. But was there ever a time where such a bad PRNG implementation was fit for any purpose? So let’s get to breaking this thing. Since the state is so laughably small and you can see 15 bits of the state directly you can keep a full list of all possible states consistent with a single output of the RNG and use further calls to the RNG to eliminate possibilities until a single one remains. But we can be significantly more clever. First we note that the top bit of state never affects anything in this RNG. (state >> 16) & 0x7fff masks out 15 bits, after shifting away the bottom 16 bits, and thus effectively works mod 2 31 231. Since on any update the new state is a linear function of the previous state, we can propagate this modular form all the way down to the initial state as 𝑓 ( 𝑥 ) ≡ 𝑓 ( 𝑥 m o d 𝑚 ) m o d 𝑚 f(x)≡f(xmodm)modm for any linear 𝑓 f. Let 𝑎 = 214013 a=214013 and 𝑏 = 2531011 b=2531011. We observe the 15-bit output 𝑟 0 , 𝑟 1 r0,r1 of two RNG calls. We’ll call the 16-bit portion of the RNG state that is hidden by the shift ℎ 0 , ℎ 1 h0,h1 respectively, for the states after the first and second call. This means the state of the RNG after the first call is 2 16 𝑟 0 + ℎ 0 216r0+h0and similarly for 2 16 𝑟 1 + ℎ 1 216r1+h1 after the second call. Then we have the following identity: 𝑎 ⋅ ( 2 16 𝑟 0 + ℎ 0 ) + 𝑏 ≡ 2 16 𝑟 1 + ℎ 1 m o d 2 31 , a⋅(216r0+h0)+b≡216r1+h1mod231, 𝑎 ℎ 0 ≡ ℎ 1 + 2 16 ( 𝑟 1 − 𝑎 𝑟 0 ) − 𝑏 m o d 2 31 . ah0≡h1+216(r1−ar0)−bmod231. Now let 𝑐 ≥ 0 c≥0 be the known constant ( 2 16 ( 𝑟 1 − 𝑎 𝑟 0 ) − 𝑏 ) m o d 2 31 (216(r1−ar0)−b)mod231, then for some integer 𝑘 k we have 𝑎 ℎ 0 = ℎ 1 + 𝑐 + 2 31 𝑘 . ah0=h1+c+231k. Note that the left hand side ranges from 0 0 to 𝑎 ( 2 16 − 1 ) ≈ 2 33.71 a(216−1)≈233.71. Thus we must have − 1 ≤ 𝑘 ≤ 2 2.712 16 a>216 while 0 ≤ ℎ 1 > 16) & 0x7fff # Create a random RNG state we'll reverse engineer. hidden_rng = MsvcRng(random.randint(0, 2**32)) # Compute guesses for hidden state from 2 observations. r0 = hidden_rng() r1 = hidden_rng() c = (2**16 * (r1 - A * r0) - B) % 2**31 ceil_div = lambda a, b: (a + b - 1) // b h_guesses = [ceil_div(c + 2**31 * k, A) for k in range(-1, 7)] # Validate guesses until a single guess remains. guess_rngs = [MsvcRng(2**16 * r0 + h0) for h0 in h_guesses] guess_rngs = [g for g in guess_rngs if g() == r1] while len(guess_rngs) > 1: r = hidden_rng() guess_rngs = [g for g in guess_rngs if g() == r] # The top bit can not be recovered as it never affects the output, # but we should have recovered the effective hidden state. assert guess_rngs[0].state % 2**31 == hidden_rng.state % 2**31 While I did write the above process with a while loop, it appears to only ever need a third output at most to narrow it down to a single guess. Putting it together Once we could reverse-engineer the internal state of the random number generator we could make arbitrary automated decisions in the supposedly secure environment. How it worked was as follows: An insecure hook was registered that would execute right before the secure environment code would run. In this hook we have full access to information, and make a decision as to which action should be taken (e.g. casting a particular spell). This action is looked up in a hardcoded list to get an index. The current state of the RNG is reverse-engineered using the above process. We predict the outcome of the next RNG call. If this (modulo the length of our action list) does not give our desired outcome, we advance the RNG and try again. This repeats until the next random number would correspond to our desired action. The hook returns, and the secure environment starts. It generates a “random” number, indexes our hardcoded list of actions, and performs the “random” action. That’s all! By being able to simulate the RNG and looking one step ahead we could use it as our information channel by choosing exactly the right moment to call random in the secure environment. Now if you wanted to support a list of 𝑛 n actions it would on average take 𝑛 n steps of the RNG before the correct number came up to pass along, but that wasn’t a problem in practice. Conclusion I don’t know when Blizzard fixed the issue where the RNG state is so weak and shared, or whether they were aware of it being an issue at all. A few years after I had written the code I tried it again out of curiosity, and it had stopped working. Maybe they switched to a different algorithm, or had a properly separated RNG state for the secure environment. All-in-all it was a lot of effort for a niche exploit in a video game that I didn’t even want to use. But there certainly was a magic to manipulating something supposedly random into doing exactly what you want, like a magician pulling four aces from a shuffled deck.",
    "commentLink": "https://news.ycombinator.com/item?id=38994817",
    "commentBody": "When Random Isn't (orlp.net)280 points by orlp 12 hours agohidepastfavorite86 comments nneonneo 3 hours agoWeirdly, I answered almost exactly the same “invert the RNG” question on StackOverflow (https://stackoverflow.com/a/15237585/1204143) just a few months before you posted your question to Crypto.SE, except that I attacked the Java RNG instead of the MSVC RNG. They both use a simple LCG design where outputs are truncations of the internal state, so the attacks are very similar to each other. reply cowthulhu 10 hours agoprevI wonder if there’s any chance that they were using the same RNG on the server. Seems like you could theoretically exploit it if so, given a very low usage server, low ping, ability to get a decently precise window into the current state of the rng, and ability to quickly generate another event with a high economic variance tied to rng. reply plorkyeran 4 hours agoparentVanilla WoW used Mersenne Twister on the server, which has a number of problems but isn't so trivially broken. They posted a lot of the details how the server-side RNG worked in a (mostly unsuccessful) attempt at killing the rumors around \"loot seeds\" and such that superstitious players were utterly convinced were a thing. reply xenonite 1 hour agoprevEven worse, the given linear congruential RNG yields only 12445 distinct numbers, then repeats itself. It would be sensible to simply use better parameters for this algorithm. reply hhh 11 hours agoprevWorld of Warcraft is what started my interest in programming. I was 6 when it came out, and when private servers started popping up I became interested in how to make my own npcs, and that’s how I picked up Lua. I still have a lot of posts on the forum for that stuff (MMOwned), and one of the people that helped me learn then still posts semi-actively. Compiling my own private server core was how I learned about compiling things, SVN, and how to apply patches (there was a bug related to mage’s fireball, can’t remember what it was.) reply behnamoh 10 hours agoparent> I still have a lot of posts on the forum for that stuff (MMOwned), and one of the people that helped me learn then still posts semi-actively. Man, whenever I am reminded of the old forums I can't help but get nostalgic. Nowadays, it's like a good piece of the internet is replaced with dopamine-inducing social media apps and gated forums (e.g., Reddit). I don't know why I still hang out here on HN, maybe mainly because it reminds me of the old forums... reply Buttons840 9 hours agorootparentHN threads die to quickly though. For the true old-forum feel we need fewer users with focused interest and threads that last for weeks. reply DeathArrow 1 hour agorootparent>For the true old-forum feel we need fewer users with focused interest and threads that last for weeks. There is 4chan. reply hprotagonist 9 hours agorootparentprevthreads can go for multiple years, if you do it right reply Buttons840 7 hours agorootparentAlso, old forums didn't organize comments in tree structures, they were just flat chronological comments. This is technically inferior, but it helped keep everyone more focused on a single thread, as it were. reply DeathArrow 1 hour agorootparentIn most forums, one example being DPReview, the user can switch between threaded view and flat view. I choose flat view, because I usually read the whole topic if it interests me and I usually want to read the last updates without having to go trough multiple threads. Also, flat view, mimics a real discussion between people. reply Paul-Craft 6 hours agorootparentprevYou don't need to go to an unthreaded format, with all the attendant clutter and such, to do that. All you have to do is bubble up threads that have had recent messages to the top. That's why HN doesn't (or at least shouldn't) try something like that: it wouldn't be \"Hacker News\" anymore, so much as \"Hacker Gossip\" or whatev. reply behnamoh 8 hours agorootparentprevhe's right tho — lack of notifs on HN means you won't know about replies to your comments or new comments in fav threads. reply aspenmayer 2 hours agorootparentHN Replies is email notifications for comment replies, maintained by Dan Grossman. https://hnreplies.com/ https://news.ycombinator.com/user?id=dangrossman reply DeathArrow 1 hour agorootparentprevYou can make a browser add-on or use a script to keep you in the loop. reply maxbond 7 hours agorootparentprevWhich is to say, always reply within two weeks. Does anyone know of a thread on HN that survived multiple months? reply acheong08 10 hours agorootparentprevI do wish more of the younger generation would be willing to hang out on forums/irc/etc. I would not install Facebook even if it cost me friends reply DeathArrow 1 hour agorootparentNewer generation is on TikTok, not Facebook. reply smeeth 6 hours agoprev\"Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin. For, as has been pointed out several times, there is no such thing as a random number — there are only methods to produce random numbers, and a strict arithmetic procedure of course is not such a method.\" - John von Neumann reply Vvector 10 hours agoprevI wrote an AH script. It would scan every auction, buy under-priced items, and relist. If someone undercut me, it could cancel and relist just below them. I basically had unlimited gold. reply benreesman 6 hours agoparentWhen I explained how money works to my younger brother after having it explained to me by actual drunk guys who work at Goldman (it’s basically impossible to get the truth any other way as far as I know), his reply was: “You mean to tell me that the IRS is a gold sink?!” “Well, actually that’s a pretty good analogy, the difference is… well… that’s a pretty good analogy!” reply Cthulhu_ 55 minutes agorootparentPretty good, but not complete because the money is gone in a game's sink, whereas in the economy it's more cyclic. In theory. That said, video game economics are pretty fascinating. I love reading writeups on Eve Online economics. I play FFXIV myself which isn't as good as Eve when it comes to economy, but interesting in its own rights. I wish it had buy orders and the ability to buy individual items instead of whole stacks, lol. reply wodenokoto 3 hours agorootparentprevMaybe I don't understand what a gold sink is well enough, but I don't see how IRS can be considered a gold sink. Gold spend with NPCs are not returned to the economy. Taxes are returned to the economy. reply benreesman 2 hours agorootparentTax receipts are in no sense returned to the economy unless you really overload one of those words. A modern economy works by running a printing press a little faster than a furnace, with the coefficient being (under honest and competent management) a good estimate of next year being better than this year by such and such, a self-fulfilling prophecy in a good way until you like, burn the planet via carbon emissions or some hard constraint like that. And it’s a perfectly good system… under honest and competent management. It’s just easier to corrupt than systems with much lower potential (which is the distance between gold bug and ignorant, those folks have a point, they just rarely make it). This is where all the 1971 stuff comes in: no rich person does anything but advantage their less-capable kid or lover or whatever unless someone is pointing a gun at them. (e.g. Altman has a job let alone unfettered power). The gold standard has a lower maximum potential risk to the extent you can stop there being a de Beers of gold. You likewise accept 50 years from the transistor to a computer in your house even if you’re fucking loaded. The unanswered question is: can we get competent and honest leadership (after a fashion) back without the bloodshed it has required 100% of the time before? A lot of us hope so but don’t have an answer just yet, and the clock is ticking. reply whage 1 hour agorootparentThis post feels like recent movie trailers. The way you wrote it makes me believe you know what you are talking about but I have no idea. Care to give the curious reader pointers to the topics you mention? I mean the \"printing press to furnace\" analogy of the economy, or what you mean by 1971 or the gold bug or the 50 year transistor omg what? reply benreesman 1 hour agorootparentI’m just a nerd whose nerd hobbies came to encompass finance when I moved to Manhattan. I was also a party monster back then (please don’t read the book) which for a childless bachelor is a pretty reasonable way to get the most knowledge NYC has to give in a year. If you have particular questions about the monetary system, I know enough to either answer or know that I don’t know and refer you to someone who does. reply DeathArrow 1 hour agorootparentprevDoesn't matter much when money can be printed by FED (which is owned by banks, not by stste) or just made up out of thin air by banks and financial institutions. The gold in online games is fabbed the same way. The only two reasons that makes it hold value is that is the only sanctioned way to exchange goods and services and the players trust it. Once one of the two reasons does not exist, gold will not have any value, just like money in real life. reply benreesman 1 hour agorootparentThe Federal Reserve system is a quasi-public, quasi-private system. It’s a public/private partnership, which is actually a pretty sane default for how governments should interact with banks. Sloppy, leaky Venn diagrams are a fact of real life. Let’s try to avoid “Jews vote for Likud” or “nine eleven was an inside job” type oversimplification? It’s complicated but for a good set of reasons. reply pests 3 hours agorootparentprev> Taxes are returned to the economy. I hope we don't start dropping bombs on our own cities.... reply wodenokoto 2 hours agorootparentPaying for the construction of a bomb, is participating in the economy (as opposed to outright burning the money) reply Doxin 2 hours agorootparentprevThe money isn't stuffed inside the bombs. The money is still returned to the economy for the most part even if the bombs are \"spent\" elsewhere. reply pests 2 hours agorootparent> The money isn't stuffed inside the bombs. I thought that was the only explanation for their prices. reply DeathArrow 1 hour agorootparentprevCan you share with us what you did explain to your brother? reply benreesman 1 hour agorootparentI’ll do my best to be useful, with the caveat that “monetary policy” is like, a PhD at a good school and industry experience and stuff, it’s a big field. To address the particular point that he was replying to: the government (especially the US government) has only one money problem: does anyone lend them money (buy their bonds) when they’re picky about terms (floor on auction optionally) at an acceptable overall outcome (you still believe I can invade your country if you fuck around on USD). So it’s simpler and faster and the math is easier to just burn all the checks the IRS gets and make new ones at the Fed. Which is the same solution as all the games landed on. reply DeathArrow 30 minutes agorootparentWell, money printing is taxation. reply AnthonyMouse 10 hours agoparentprevThis is basically what high frequency trading is in real life. reply eru 6 hours agorootparentOnly with a lot more regulation, and the competition is very sharp. reply SeanAnderson 9 hours agoparentprevWait, did you write Auctioneer? or was this something similar to Auctioneer but you decided to not use the \"industry standard\" tooling? reply bimguy 4 hours agorootparentI very much doubt it. There's a lot more functionality to Auctioneer then what was mentioned. Also, there are other popular auction add-ons, I think they would have just said it if they created Auctioneer. reply Jaygles 10 hours agoparentprevHow did you determine what was under-priced? reply Vvector 9 hours agorootparentAfter playing the AH manually for a long time, I'd know that a stack of ore would sell for 20g. I was manually buying when under 20g and relisting. My script just reduced the tedium. I'd manually add in checks for each high-volume item. It was super-hacky, but so effective, I didn't need to improve it. One script would iterate over every item in my inventory. If it matched a hard-coded list, it would auto-list it at the hard-coded price. Another script would scan every item on the AH, and if matched another hard coded list, it would buy them. Another story, pre-script. WOW had the inscription update incoming, which used flowers to make glyphs. I correctly assumed the demand for Peacebloom (tier 1 ingredient) would skyrocket once the patch hit. I spent three weeks buying everything on the AH, using multiple characters to store it all. The night before the update, I listed it all for 20x the normal price. The next morning, everything sold out completely. I remember it being 200 stacks @ 5g each, sold for 100g each. reply hinkley 9 hours agorootparentI recall when Pandaria hit there was some resource where you could get only like one a day but they could be sold. The power gamers and the server first people were buying all they could to craft gear to ratchet up the Raid ladder ASAP. I convinced a couple of my friends to sell all theirs at the prevailing price, which was just stupid high, and on about the day we would have been able to craft our first item, the price had crashed so much we bought them back at a lower price. If you waited a few more days, you could buy enough for two items. Getting 70% of the way to a goal has 0 value. You can often sell your patience to other people in that game. reply eru 6 hours agorootparent> Getting 70% of the way to a goal has 0 value. Well, apart from the fun of playing the game? reply pests 3 hours agorootparentNo, games are only fun if you get all the achivements and 100% the game and buy every item ever relased in the shop and complete all battle pass levels because FOMO because gaming is not about having fun anymore, its about reaching artificially set goals and collecting cosmetics. reply TeMPOraL 1 hour agorootparent> gaming is not about having fun anymore, its about reaching artificially set goals and collecting cosmetics I.e. it became indistinguishable from normal adult life. reply pests 31 minutes agorootparent:( reply DeathArrow 1 hour agorootparentprevAt which point, you rather quit gaming and use your time and energy in the real life. That way you can touch your achievements because they aren't just numbers in a SQL database. reply pests 31 minutes agorootparent> just numbers in a SQL database This is what breaks most games for me. Once I see the gameplay loop, I know I'm just playing spreadsheet optimizer behind the scenes. reply eru 31 minutes agorootparentprevIf you make lots of money, and keep it at the bank, your achievement might very well be recorded as a number in a database. (But with the distinction that you can withdraw and spend that money.) reply DeathArrow 23 minutes agorootparentWhen you play a game you trade your time to make other people rich. When you work you trade your time to make you rich. That is the most important distinction. Would you buy you a Lexus or help someone else buy their second yacht? reply andenacitelli 9 hours agorootparentprevSpent some time with this kind of system in WoW, TradeSkillMaster. Figuring out an accurate valuation was difficult and one of the biggest differentiators of how successful you were. This would vary by item, but was most commonly some combination of recent minimum buyouts (usually over the last two weeks) historically value (several months), and much more. Common items would usually be mostly recent value driven, whereas lower-supply items might have a larger component of historical weighed in, as there might be quite a bit of fluctuation day to day and you care much more just about the general trend. All sorts of nuance to it. You can do more complex stuff like only doing your price estimations off items that actually sold, set values based on a percentage of any other value, and much, much more. It gets really complex but is really cool at scale. reply hinkley 9 hours agorootparentIngredients were always pretty stable. I think a lot of sales came down to people trading time for money. I just hit a critical level and I want some cheap blues to power level faster and got shitty drops. Or Buddy wants to play and he’s laughing at my gear, oh there’s a reasonable purple. I found a lot less stress looking for cheap blues and greens to disenchant for ingredients. That could suck up an hour a day easy. Which is why people just dumped them. Not worth their time. reply Grimblewald 7 hours agorootparentIngrediants can be stable but are still subject to manipulation. I made a killing by having a lot on hand, pushing bids down during busier periods, which would work synergistically with the lil goblins on during this time who list everything they sell at an undercut. This can cause the ass to fall out of pricing. You then buy up the cheap mats as fast as you can, but also alchemists will start making potions for cheap looking to undercut other potion makers with their zero time investment potion making for cheap. You buy those as well when they get cheap. Then you delist the series of sales you made that pushed prices down and buy the undercuts that helped get you there. Suddenly theres a big rift between current price and the bulk of items and most sellers stop undercutting and cheap supply runs out. relist regular small batches at 1s under market and make bank. reply DeathArrow 1 hour agorootparentJust like in real world. But you have to own at least half of the market to pull that. Or some else can short you. reply DeathArrow 1 hour agorootparentprevYou can learn more about economy from these threads than following a course at Stanford. reply plandis 8 hours agoparentprevYou’re the creator of the Auctioneer addon? That’s got to be one of the most popular WoW addons for a decade or more at this point. reply tapland 8 hours agoparentprevA lot of us have been using Auctioneer, which has this functionality, since Vanilla. How was the experience of writing your own? reply Thaxll 9 hours agoparentprevExcept that there is a deposit fee in WoW AH. reply anonymoushn 9 hours agorootparentThis is correct, but I think for a lot of items the non-refundable listing fee is a trivial amount of currency that you're happy to pay in exchange for faster execution, because it's based on the vendor sale price. reply Grimblewald 7 hours agorootparentIts a multiple of vendor price. So for a few items it is quite hefty. For pets, its always 1s flat so pet pricing can be quite volatile. reply hinkley 9 hours agorootparentprevThe deposits mostly just stop people from overloading the system, and put a tiny bit of friction into the economy. I always made the most money when I half heartedly farmed on one or two characters and traded about five times as much material as I brought in. It’s a commodities market. Figure out what the weekly and daily average is, try to anticipate drop off in demand (eg, post launch), if anyone undercuts you by too big a margin, buy them out and relist. If a little, either drop your price or wait. reply stevekemp 5 hours agoprevSee also that time when hacker news was hacked, due to weak random numbers: How I Hacked Hacker News (with arc security advisory): https://news.ycombinator.com/item?id=639976 reply DonHopkins 7 hours agoprevWhen Variables Don't and Constants Aren't reply maxbond 7 hours agoparentInteresting. Seems to be a meme with a deep history. https://www.theregister.com/2006/07/26/constants_are_not/ > I first read the lament to the capricity of programming [in the title of this piece, \"Variables Won't Constants Aren't\"] in Creative Computing, years before C was devised. If there's a particular document you feel is canonical to this meme, I'd be interested to read it. reply anonymoushn 2 hours agorootparentI suspect it's a math thing, because in math it actually describes the normal state of affairs. Anyway, here it is in a compilation of such sayings dated 1979: https://www.cse.unr.edu/~sushil/quotes.html reply lifthrasiir 7 hours agoprevWhile I never played WoW, I have heard that many addons used a private chat as a means of inter-... whatever communication. If there is a way, people will eventually find and use it. reply maxbond 4 hours agoparentI don't recall the name of the company but I read about a startup who's product added chat features to a client's website. They noticed a developer using it for IPC, and ended up pivoting into making a product to enable IPC for web apps. (Or inter-something, like you point out it's not an OS process, it's an application-level process. Intertask communication if your prefer.) reply Philpax 2 hours agorootparentSounds like the story of Firebase? https://en.wikipedia.org/wiki/Firebase reply chupapimunyenyo 2 hours agorootparentprevWhose* reply Hamuko 4 hours agoparentprevI think a lot of roleplaying addons exchanged information by just having a public chat channel that just wasn't made visible in the user interface. You could actually make it visible and see the addon sending information to other users there. reply robobro 7 hours agoparentprevAs a means of what? reply lifthrasiir 7 hours agorootparentLike, sockets. (Addon instances are not processes, so it is not exactly interprocess communication...) reply taspeotis 7 hours agorootparentprevInterprocess communication? reply paulpauper 8 hours agoprevThe economics and incentives of RNGs are interesting. If nothing is materially at stake ,an imperfect RNG, even a bad one, suffices provided people cannot easily tell, such as games. But if money is at stake or security, like crypto wallets or online casinos, suddenly it matters A TON. reply hinkley 8 hours agoprev>security I will be forever appalled that Blizzard rolled out two factor authentication before stock trading services like E*Trade managed it. Bravo for Blizzard, shame on the rest of ‘em. A game company. reply profmonocle 7 hours agoparentI worked with someone who was a Blizzard GM around when they rolled this out. Recovering stolen accounts was some absurdly large % of their customer service tickets. People would use the same email & password on WoW as on various shady / insecure sites. Bots would try any stolen credentials en masse in WoW. When they succeeded, not only would customer service need to restore a user's access, they would need to roll back their character to restore lost items and gold. According to my coworker, adding 2FA was primarily about reducing this customer service workload. Eventually they encouraged it by adding in-game benefits, like additional bag space for your character. Not sure they ever went as far as mandating it. reply Paul-Craft 6 hours agorootparentSo, what happened after they implemented it? Did CS tickets actually go down a lot? Did it save them a ton of money and hassle? You left off the best part of the story :-) As for mandating it, I could see not requiring people who'd already set up accounts to jump through this extra hoop to play the game. I'm sure even a small percentage might not be able to jump through that hoop. I doubt it's a significant percentage, but I could see it becoming a PR hassle, and those are the worst kinds of hassle from the POV of a technical employee lol... But for a new game? If the CS metrics reflected a big drop relative to the number of people using 2FA, then I'd be sold on requiring it. reply hinkley 3 hours agorootparentFrom what I understand it went down for a long time, but what people did was if they managed to get into your account the first thing they would do is remove the authenticator. That involved entering a couple sequential values. But if you set up the keylogger right, you could trick users into failing the credential check 3 times at login and get their account. reply anonymoushn 4 hours agorootparentprevCurrently you cannot customize your group in group finder without 2FA, so you cannot list a group for your friend's keystone or specify what sorts of characters you would like to join. It's quite annoying! reply saghm 2 hours agoparentprevNot only that, but they had dedicated hardware for it since smartphones weren't ubiquitous yet. I still remember the first time I saw a friend use one, and how mystified I was by this mysterious \"authenticator\". It's still absolutely wild to me to think about how ahead of the game Blizzard was on the 2FA front. They only recently replaced their dedicated smartphone app for 2FA by rolling it into the existing Battle.net app. The 2FA codes they use are longer than 6 digits, which I assume is why they don't support something that can be used by more \"mainstream\" 2FA apps; by the time those become established enough to support third parties, I suspect their infrastructure had been around long enough that it just wouldn't be worth rewriting it. I do sometimes wonder if someday apps like Google Authenticator will support a wider variety of TOTP configurations; I think Steam also uses something non-standard, which means the only mobile option is a dedicated app, and I'd much rather not have to install extra apps just for 2FA support. reply bee_rider 7 hours agoparentprevIt doesn’t seem that surprising to me. In the olden days, video game companies served a slightly more technical user base, so it makes sense that they’d be more willing to deal with 2FA. Also, I don’t think the legal system will help you recover your SoJs. reply toast0 4 hours agoparentprevDid they? I found an article from March 2005 stating that ETrade will soon launch 2fa [1]. And a similar article about Blizzard launching 2fa from July 2008 [2]. Maaaybe ETrade took 3 years from anouncement to launch, but I'm pretty sure I saw ETrade branded RSA tokens around 2005, and Blizzard tokens later. [1] https://www.computerworld.com/article/2569503/etrade-touts-t... [2] https://www.zdnet.com/article/blizzard-introducing-two-facto... reply hota_mazi 7 hours agoprevRelated: https://www.beust.com/weblog/various-ways-to-get-randomness-... reply 38 9 hours agoprevFYI your link color is almost unrecognizable. its basically a (very) dark blue against black. reply pests 2 hours agoparentYou might be using a setting to force dark mode which has no guarantees to be correct. The site has white background with black text in normal and darkmode. Could probably use a better stylesheet for darkmode but disabling whatever you have on would probably also fix it. reply gometo 6 hours agoprev [–] Akun vip reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their teenage experience playing World of Warcraft and creating add-ons using Lua programming.",
      "They discuss the security measures implemented by Blizzard Games to prevent automation in the game.",
      "The author discovered a loophole by manipulating the game's random number generator, allowing for automated decision-making. However, it is unclear if this exploit still works due to possible fixes or algorithm changes by Blizzard Games."
    ],
    "commentSummary": [
      "The online forum discussion covers a wide range of topics, including comparisons between random number generators in Java and MSVC and the use of Mersenne Twister RNG in Vanilla WoW.",
      "The conversation also touches on nostalgia for old internet forums and the structure of online forums.",
      "Other topics discussed include video game economics, high-frequency trading, and strategies for making money in the WoW Auction House."
    ],
    "points": 280,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1705269389
  },
  {
    "id": 38990080,
    "title": "Frustration with Cars: Excessive features and poor design choices compared to smart TVs",
    "originLink": "https://petargyurov.com/2024-01-14/cars-suck-man",
    "originBody": "Published on January 14, 2024 Cars suck, man What better way to start off the new year than with a little rant. I am not car aficionado, in fact, I am probably less interested in cars than the average person. As the tech bros have slowly crept into the automotive industry, I can’t help but feel like I’m witnessing a car crash in slow motion. Cars are the Smart TVs of today Have you tried shopping for a TV in the past 5 years? Actually, scratch that – have you used a “smart” TV in the past 5 years? – it’s awful. Sure, displays are getting better, but manufacturers insist on cramming bloatware and spyware, embellished in the worst UI imaginable, running on a processor designed for a smartwatch. The Essential Smart TV Pooramid Picture quality is at phenomenal levels, even on low-cost offerings, but those inky blanks come at the cost of compromised privacy and ad-infested UI. So what do cars have to do with any of this? Every product that takes a concrete position in human dynamics, be it a TV, a fridge or a car, follows a relatively predictable evolution: it starts of limited but promising, gets meaningful upgrades, and eventually slides into into the valley of “enshittification”. Few are those who climb their way out of the shit-pit, fewer still are those who avoid it entirely. 100% science based chart What’s so wrong with cars then? We live in an era of peak automotive power, utility and luxury. Much like with TVs, we probably reached the first plateau of quality: image clarity, super thin displays, etc. Then we started getting things nobody asked for: frame rate interpolation (why God, why!?), insane image presets, automatic audio adjustments. I won’t talk about 3D TVs. Let’s just pretend that never happened and that drawer in your cabinet isn’t full of dusty 3D glasses that haven’t seen the light of day for the last decade. Cars are entering the “frame-interpolation” era, the slippery slope into the loathed, anti-consumer, anti-logic Shit Pit. Let’s just fire through this: Hiding critical/often-used buttons behind touchscreens for the sake of minimalism (and cost?) On that note, poor physical layout in general (genuinely thinking that you can rate cars’ on a “hazard-light-button-placement index”) YOUR LED LIGHTS ARE TOO BRIGHT GODDAMIT Cars are getting too big; why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I could write pages on this point alone. Combine the two previous points for peak blindness “Low profile” rims/tyres/wheels/whatever: cost a fortune and more prone to getting scratched Subscription style plans trying to creep their way in ALL the privacy violations you could think of Rubbish software and UI all round: it’s 2024 and I dread it every time I need to pair my phone to a car That weird glossy pastel paintwork that’s so popular all of a sudden (I’m allowed one subjective point on here, OK?) The car of the future I don’t think we’ve reached the depths of the brown ocean… yet. Who’s to blame here? Consumers? Manufacturers? I know I sound like a luddite here but the same way I just want a good display from my TV, I want less from my car, not more. I miss my 2005 Toyota Corolla 😢 ← Prev Top",
    "commentLink": "https://news.ycombinator.com/item?id=38990080",
    "commentBody": "Cars suck, man (petargyurov.com)185 points by petargyurov 20 hours agohidepastfavorite289 comments bborud 20 hours agoI am a car enthusiast and I agree. Cars just keep getting worse. Manufacturers try to cram too much functionality into the car at too low a price point. Even if you buy a car that costs more than most people's house, it is going to be full of technology that at some point will fail. You can't fix it yourself. And when you get it fixed, it invariably ends up costing some significant fraction of what the car is worth. Even if it is just a faulty rail for the seat. To top it off, silly regulations also contribute to making cars more distracting and annoying. I used to laugh at American cars for making those incessant ding-ding sounds when the door is open. But now European cars ding, and ping, and whine constantly because regulations require them to constantly bombard you with warnings and interruptions. As if that's actually going to make the car safer. There is something extremely relaxing about driving a vintage car from the 1970s. Sure, you have to learn how to coax the engine into life on cold days. And you actually have to learn how to modulate brakes and throttle. And learn how to fix the 4-5 things that break every couple of years. But that's a lot less bother than having a computer on wheels where every single problem could end up being so expensive it is cheaper to scrap the car and get a new one. Here's what I'd like: an electrical car modeled on the philosophy of the Fiat 500 or the Citroen 2CV. A minimalist car that is so simple anyone can afford it, and anyone can learn how to repair anything on it. A car with as few parts as possible and where every spare part is easily available. (Fiat 500 parts used to be available where you bought your groceries. That's not going to happen today, but if you could buy every single part online at a reasonable price, that would do). A car that comes with drawings for every part, for every electrical circuit. A car that can be modified and evolved to fit new needs. A car that can grow a cottage industry of companies that make upgraded parts, rebuild cars etc. The Fiat 500 and 2CV were important cars. Even more important than the VW Beetle. Because they provided mobility to much deeper demographics. Nobody really makes important cars anymore. Important cars change how society works. And I think the only way we can make cars tolerable again is to figure out a way to come up with a minimalist, open source, vehicle architecture that can satisfy the safety requirements. I think we have the technology to do that today. reply timidiceball 19 hours agoparentVW’s e-Up comes very close to that modern but minimal EV I wish someone would keep making. No touchscreen, just a smartphone mount. Analog HVAC controls. Even the battery-remaining gauge is an analog needle (though the usability of that detail is debatable!). reply riedel 19 hours agorootparentThe absurd thing is that production will stop due to cyber security requirements. This highlights the absurd effect of regulations . Systems with a much bigger attack surface survive the risk assessment. All these new regulations are leading to absurd adverse effects towards their goal but particularly the environment . reply teknico 19 hours agorootparentprevI've been driving an e-Up for the last two years and am happy with it. It's a small car, just 240 km in the summer and 180 km in the winter though. The battery pack is not temperature-controlled so it gets hot in long trips, and recharging becomes excruciatingly slow. Then again, it was not designed for long trips. The new Citroen e-C3 is a modern, cheap(ish) car with a similar concept: optional touchscreen and many physical controls. reply wyager 18 hours agorootparentprevAmerican NHTSA backup camera regulations effectively ban new cars without a touch screen, sadly. reply ndsipa_pomu 18 hours agorootparentDo the regulations mean that physical controls have to be replaced by a touch screen or is the touch screen just required for displaying a camera feed? reply surajrmal 15 hours agorootparentThey mandate a way to view the backup camera. I suppose at that point it incentivizes auto makers to add additional functionality to that screen. There is only so much room on the dash after all. I'd also be surprised to learn auto makers aren't doing this sort of thing because it tends to sell better. Similar situation to phones getting larger every year despite some segment crying for smaller phones. reply OkayPhysicist 15 hours agorootparentprevOnce you're sticking a big ol' screen in the driver's vision during backup time, that screen becomes obvious real estate for any other user interface you may want. Buttons and dials are cheaper than a touch screen, but buttons and dials+a screen are more expensive than a touch screen. reply fho 13 hours agorootparentIf I am not mistaken those touchscreens in cars are cheaper than the buttons they replaced (citation needed). reply AnimalMuppet 13 hours agorootparentSeems plausible, at least. A button is cheaper than a touchscreen, but you can replace N buttons with one touchscreen. (Citation still needed, though.) reply fho 2 hours agorootparentI guess at some point a (modal) GUI is more space efficient than individual controls. At some point cars had screens, surrounded by physical buttons. I always thought that was a great compromise. reply a4000 8 hours agoparentprevThere's a growing cottage industry of companies that will take older classic cars and convert them into electric cars. Obviously they don't have the range or safety of a modern electric vehicle but you can essentially customise it to your liking in every way. Depending on whether the classic car needs any restoration it will be about as expensive as a mid to upper range electric car but as it's a classic car and most of the installs are reversible you don't have the same worries about a modern electric car being disposable junk to be discarded in 10 years, you can probably just get a new battery pack put in after 10 years or whenever and continue driving. reply silentsanctuary 18 hours agoparentprev> But now European cars ding, and ping, and whine constantly because regulations require them to constantly bombard you with warnings and interruptions. As if that's actually going to make the car safer. As a driver of a european car, I'm not sure my car has ever bombarded me with any such sounds except if I drive without a seatbelt. Can you explain further? reply bborud 17 hours agorootparentDepends on how new the car is since the regulations are evolving and a lot of these new things are quite recent. Well, let's start with mandatory speed limit notifications. Which would be fine if all cars actually knew what the speed limit was. They don't always get this right. Which could mean that where you drive, your car will constantly be pestering you. And you can turn it off, but it'll be turned back on when you restart the car. Then a lot of new cars will ding if they don't think you are paying attention. For instance if you have to navigate the display or, again, if the system interprets what it sees wrong. Then there's all the situations that the car, for some reason, feels are threatening. Like if I drive over temporary markings in the road. Or the car gets frightened by a shadow or snow on a sensor. Or if there is a bit of wheel slip because it's winter and I'm driving on solid ice. I've even had \"security systems\" almost slam me into the guardrail at considerable speed. Because some idiot at BMW thought that it would be a good idea to not make the car understand how temporary road markings work. In Germany. Where the car is designed. In that case I'll admit that I didn't notice the beeping as much as I was busy trying to stop the car from killing me. And to be fair, that bit of nonsense was on BMW and not the regulators. The people who regulate these things aren't exactly drivers. Nor are they burdened by insight. reply cameronh90 18 hours agorootparentprevI hire cars fairly often, and some of the newer ones just make random ding noises with no visual indication, and I can’t work out why. I think it may sometimes be a blind spot or proximity warning. That being said, only some cars do it so I’m guessing it’s poor manufacturer implementations mainly. reply wyager 18 hours agorootparentprevYour car also shouldn't ding at you if it detects weight on a seat without a seatbelt. reply bborud 17 hours agorootparentMine does. Even though it is an older car. And it is somewhat random. It doesn't like my laptop bag some days. reply mlrtime 19 hours agoparentprev>> Here's what I'd like: an electrical car ... A minimalist car that is so simple anyone can afford it. Not possible with today's safety rules. You mentioned this in passing but dismissed it without understating. You can argue it's not needed (and some probably are not needed), but this is just yelling into the wind. Also, as Engineers we know that cost is related not just to simplicity but scale. Your dream car will never be at the cost you or like minded people will buy because it may be simple and featureless, but it will be expensive. The electric car you're asking for will never exist unless you build it yourself or manage to import from a country with lax laws. reply bborud 14 hours agorootparentI agree that a open car concept would be hard to bootstrap. It wouldn't surprise me if we're talking about $3-4bn to get to where it becomes a pure manufacturing problem and evolution can drive itself. That is, you just need to be able to prove that the critical parts someone manufactures are within spec. If I had that kind of money lying around I'd probably initially focus on funding projects in academia in collaboration with regulatory and certification bodies. There are already (largely pointless) academic programs to design and build race cars. It should be possible to interest academia in doing something vaguely productive instead. Like collaborating on designing a platform for cheap mobility. As for simplicity and scale: simplicity is absolutely a requirement for cost efficient scalability. To see how incredibly important this across disciplines, have a peek at \"How Big Things Get Done\" by Bent Flyvbjerg. reply wyager 18 hours agorootparentprev> Also, as Engineers we know that cost is related not just to simplicity but scale. Your dream car will never be at the cost you or like minded people will buy because it may be simple and featureless, but it will be expensive. Cars very close to the \"dream car\" do exist, and they are cheap, but they are not available in US/EU due to our absurd regulations. Look at the new land cruiser 70 series. Simple, reliable, affordable - but not available in the US because they don't meet random punitive safety or emissions rules. reply vwcx 16 hours agorootparent> don't meet random punitive safety or emissions rules citation needed, please! reply dantastic 19 hours agoparentprevThere are quite a few companies offering EV conversions for classic cars, for example Retro Electrics [0] and EV-Evolution [1] for the 500. Personally, I'd go for the kit. I'm 6'4\" though so a 500 might be a bit of a squeeze :) [0]: https://www.retroelectrics.co.uk/classic-fiat-500-electric-c... [1]: https://ev-evolution.eu/solution/fiat-500-ev-conversion-kit/ reply alexpotato 18 hours agoparentprevThere was a story about BMW in the early 2000s when digital screens/menus etc started to appear in cars. The story is that a BMW exec brought a design team to see a Volkswagen Jetta and said: \"We are supposed to be the 'ultimate driving machine' but our customers say we have too many menus, buttons etc. Looks at this VW? Simple buttons, layout and instruments. Nothing extra. THAT is the ultimate driving machine.\" reply voisin 18 hours agorootparentAnd then they came up with the ridiculous circle controller and the requirement to still cycle through menus, rather than keeping things to toggle buttons. reply osigurdson 19 hours agoparentprevI really like the idea of the open source car but I think it might be too soon. Once battery cost and energy density reaches the asymptotic phase, perhaps we will see CARV. reply cjbenedikt 19 hours agoparentprevCompletely agree! reply MrEd 19 hours agoparentprevThats why you hold on to youngtimers. reply valval 16 hours agoparentprevThis just reads like a boomer hipster rant that gathers points on Reddit and HN for some bizarre reason. The 1970s car had a ton of parts and mechanisms only professionals could fix. Not being able to touch car software is good, and demand will dictate how many cars will cater to your ideology in the future. Probably not too many. reply OkayPhysicist 15 hours agorootparentI should be able to modify the software on my car, because I am exactly as credentialed to develop that software as the person who actually did. Hell, my high schooler nephew holds the same credentials: There are no professional software engineering licenses. If some software is too important for Joe Public to wrench on, it should need to be signed off by a P.E. who can vouch their livelihood and professional standing on the quality of that software, representing not the company's \"maybe some people will die, but we can afford it\" interests, but the public's \"we don't want manufacturers planning to kill people\" interests. Until then, I've got the same qualifications as that person. Let me at it. reply sokoloff 1 hour agorootparentprev> The 1970s car had a ton of parts and mechanisms only professionals could fix. What?! Those parts were frequently repaired/replaced by non-professionals. Half of the Craftsman section of the tool department at Sears in the 70s and 80s was mechanics tools so you could exactly that. reply majani 19 hours agoparentprevYou sound like an older guy. Surely by now you must know that your disposition is a niche one and that most people prefer convenience? reply didgeoridoo 20 hours agoprev> Cars are getting too big; why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I could write pages on this point alone. I’ve got 3 kids under 5. Car seats are absolutely enormous these days — it’s impossible to fit 3 across in even the largest “crossovers” (VW Atlas / KIA Telluride). I don’t want a big car (my dream car is something along the lines of the Audi A4 Allroad Saloon) but I also sometimes need to bring all my kids to the same place. Figure out a way to safely downsize car seats and I bet SUV purchases would drop 10% or more. reply tadzikpk 20 hours agoparentThere’s whole sites dedicated to this problem, example https://thecarseatlady.com/narrowest-car-seats/ But if you pick narrow seats you can make it work. I have 3 car seats in the back of a 2016 Subaru Legacy, which also generally a bloatfree car reply buro9 20 hours agoparentprev> Figure out a way to safely downsize car seats and I bet SUV purchases would drop 10% or more. Like an Estate / Station Wagon, a category of very capable cars that is being extinguished by the rise of SUV. reply didgeoridoo 20 hours agorootparentYeah that’s my Allroad. Almost impossible to find in the states, and priced bizarrely — base price exceeds the Q5 SUV. It honestly seems like they’re TRYING to not sell any. reply hackernoteng 20 hours agorootparentI had an A6 all road from 2005 until 2020. Great car. Great engine. Maintenance costs eventually got too crazy. reply Turing_Machine 15 hours agorootparentprevAs I understand it, classic station wagons are classified as \"cars\", while SUVs are classified as \"light trucks\". [edit: in the US] Different regulations apply to the two categories, in particular fuel economy. From what I'm seeing, a manufacturer's current line of \"cars\" has to average over 40 mpg, while their line of \"light trucks\" only has to average around 30 mpg. This has probably changed a bit... the most current numbers I see are for the 2020 model year. reply jwells89 19 hours agorootparentprevThe family vehicle for much of my childhood was a late 80s model Aries K station wagon. Always managed to fit everybody and all our stuff, and always got us there (albeit not always with speed, particularly on mountain roads… that little 4-cylinder wasn’t exactly a powerhouse). Really a very practical vehicle. Just needed a little more oomph. reply mysterydip 20 hours agoparentprevHave you looked at minivans? When my 3 were under 5 a couple years ago they all fit, with room for all the stuff. No, it's not sexy, but it's incredibly practical. reply liotier 20 hours agorootparentMy 2014 Renault Traffic long box on wheels gets nine people and their luggage over 1100 kilometers on a single fuel tank... I don't understand SUV that weight double, cost double and only seat five people at half the range. I'll hopefully keep it for at least a decade or two - it is designed for much more demanding tasks than the about monthly family outings, the holidays and standing idle the rest of the time while I commute with my bicycle and shop with my cargo bicycle. I fail to see anything electric currently available that could replace that incredible efficiency. I hope that, by the time I'm ripe for replacing it, the technology and the market will have matured. reply thsksbd 19 hours agorootparent\"SUV that weight double, cost double and only seat five people at half the range.\" That's just not right. Minivans are typically as big, if not longer, and weigh more or less the same. As to cost, unless you're comparing the suburban, (relatively rare vehicle even in my corner of rich, suburbanite, 6 kids midwest) their cost is comparable. Ive been looking to buy a Sienna for over a year and I cant find one without dropping almost $70 k out the door. For that money I can get an expedition and have more space. reply liotier 18 hours agorootparentRenault Trafic in 9 seat configuration weights 1.8 tons - so yes, it is only one ton less than the average fuel-powered SUV... But the electric ones are much heavier. It costs 25k€ which gets a modest subcompact SUV or about half of a popular large one. reply thsksbd 17 hours agorootparentWhat do you mean by \"ton\" in either Kg or on lb (or stone for all I care). Like calories, ppl mess that unit up all the time (tonne, ton, short ton, long ton, etc). If you mean 1800 kg, my full size 3.3 L V6 Sorrento weighs less at 1724 Kg. So unless you're comparing a Renault to a GMC Suburban, which I must insist are rare, you're just nowhere near the right mass. reply matthewowen 20 hours agorootparentprevI love my minivan but it doesn’t entirely solve the size issue: a Toyota Sienna is 7 inches longer than a Kia Telliride (one of the largest crossovers) and only six inches shorter than a Ford Expedition (which is a solid example of a traditional truck frame SUV) reply ghaff 19 hours agorootparentThere's a good argument that a lot of people buying 3-row SUVs would probably be better off with a minivan--which is what many of my whitewater paddling friends drive. But, as you say, they're mostly not actually smaller vehicles for the most part. reply matthewowen 19 hours agorootparentYou do miss the ground clearance for some applications. Even just comparing to the old CRV we had, we drove that places you wouldn’t be able to take a minivan. But for internal capacity, so much better: one of the big things is that the much shorter hood makes it easier to maneuver and gives you more usable space from that extra length. reply sanderjd 19 hours agorootparentprevI have a family and live in a place that gets snowy. I would love an all wheel drive minivan that I could put good tires on. But that category is dominated by SUVs. My theory is that it's about making the model line up more efficient: SUVs can sell with both families and with image-conscious individuals. So rather than selling mini-vans to families and trucks to individuals who just think they're cool, sell SUVs to both groups. reply acomjean 20 hours agorootparentprevMinivans are super practical vehicles, with great engineering that get overlooked because they have a weird uncool stigma about them. reply pc86 19 hours agorootparentUncool to whom? Everyone I know who has even two kids wants one, and anyone with more than three already has one or is miserable constantly taking both vehicles everywhere. The waitlist on a Toyota Sienna is well over a year long. 1-year old models sell above new MSRP because you can get one today with 10k miles on it vs. putting your name on a list and waiting. And they're not cheap by any stretch. A base model Sienna is almost $40k. That's more than a year's income for 25% of households[0]. The Platinum starts at $15k more than that. [0] https://dqydj.com/average-median-top-household-income-percen... reply roland35 19 hours agorootparentprevMinivans are super hard to buy right now - although demand probably isn't too high the supply is extremely limited. A minivan would be too big for my family's daily needs, but I love renting them for road trips. Even the older dodge caravan has great space and is a comfy ride for everyone. reply D13Fd 17 hours agorootparentprev> Minivans are super practical vehicles, with great engineering that get overlooked because they have a weird uncool stigma about them. So much this. My minivan fits my five kids, myself, and all of their bikes (using the big trunk and front seat) with room to spare. It's amazing. Nonetheless, my wife won't drive it and complains constantly that it would \"make her look like a soccer mom.\" She hates it. Instead she drives a Subaru Ascent, which only just barely fits the kids, feels cramped, and has maybe 30% of the cargo space—all while having a pretty similar footprint. reply jareklupinski 19 hours agorootparentprev> a weird uncool stigma about them would be easiest to just move the marketing dollars from the Mustang to the Sienna, but those dollars come from the markup over BOM that Mustang owners are willing and able to afford Sienna purchasers probably don't mind paying less for a stigma since their primary concern likely is carrying kids around, instead of advertising their ability to have them in the first place reply petre 19 hours agorootparentprevNot so uncool when you stick all the kids, the dog and the luggage inside, two kayaks and two rigid SUP boards on the roof and three bikes on the back rack. reply jwells89 19 hours agorootparentprevMy family had a used Chrysler Town and Country for getting around through the early 2000s. Great car. Its engine was a a notch or two above baseline which gave it plenty of power, plenty of room, drove well. Also if you needed to haul something large instead of kids just pop out as many seats as needed and suddenly you had van-like cargo space. reply jmuguy 20 hours agorootparentprevI laugh every time I see ads for some giant, fuel inefficient, unsafe, ridiculous SUV advertising third row seating. Minivans are the answer to every question that ends in families buying a giant SUV right now but they’re not “cool”. reply wizerdrobe 19 hours agorootparentI bought a Honda Pilot because I occasionally seat 7, frequently haul lumber, pipe, etc. which that the guy say fold down seats are a boon, and need a tow package. Fits the bill. Could make a van work I suppose. reply didgeoridoo 20 hours agorootparentprevNot enough ground clearance, at least until the latest KIA Carnival came out. I live in New England, land of unplowed roads and potholes. Even our old Saab would bottom out on the regular. reply CogitoCogito 20 hours agorootparentI guess that’s a local issue. I wonder why people who don’t have such clearance issues don’t choose minivans. I, for example, drove minivans in California for many years in mountain snow without issue. reply ghaff 17 hours agorootparentThe difference between minivans and most SUVs (especially crossovers) that people buy isn't all that great, maybe 1-1.5 inches. Most SUVs aren't really high-clearance 4WD like a Jeep Rubicon is. You get into other issues like run-flats vs. standard tires and the lack of a full-size spare but even most relatively serious SUVs don't automatically come with full-size spares any longer and their factory jacks tend to be useless. reply pc86 19 hours agorootparentprevWe have just one kid on the way and would already buy a minivan if they weren't so hard to get. The waitlist on a Sienna is well over a year long. We have friends who put their names on the list in Q1 2022 and are still waiting \"another 4 or 5 months.\" reply hotpotamus 20 hours agorootparentprevI once needed to haul a bunch of tile and my dad was out of town with his truck that I normally borrowed at the time. My grandmother offered the use of her minivan and I balked that there was no way it could handle the weight. We looked it up and it had something like over 1500 pounds of capacity which impressed me and I used it to get my tile. It was also quite comfortable for road trips. As you say though, they are not cool and so people will find reasons to dismiss them. reply z3phyr 20 hours agorootparentCool scientists and detectives roll around in Minivans with their telescope and listening devices. reply calvinmorrison 20 hours agorootparentprevMinivans are awesome. My first car was a 800 dollar Ford Windstar I got from a touring band called Creekside. Had a \"I believe Edward Snowden\" sticker on it and only 4 seats. Quiet, spacious, Great sound system, V6, 4 captains chairs, room for our drums and amps. Cruised great. No maintainence in the 3 years I owned it. Fuel economy around 20 on a good day. Very comfortable. Seats popped out, tons of storage room. Low rear bumper / floor so you could easily load it. Prior to the Ford flex they were used by a lot of installers for these favorable features reply syntaxing 20 hours agoparentprevI drive a Ford Escape and I fit 3 car seats. You just need to strategically choose them, particularly using a slim size in the center. reply goalieca 19 hours agoparentprevI don't think 10% would even come close. People are barely having kids yet alone 3 kids. Most of the land tanks i see at daycare / school are for 1 kid and sometimes two. Granted, they have as much luxury as an international flight with TVs, charger ports, personal air, snacks and all the rest. reply alexpotato 20 hours agoparentprevI've never actually seen one but a while back on HN there was a claim that in Europe they sell a \"4 across\" car seat. It's apparently just one BIG seat but b/c it's a, pardon the pun, a \"monolith\" seat then it will fit into the back of a regular European SUV. reply kj4211cash 16 hours agorootparentIt's called multimac and I wish they sold it in the US: https://www.multimac.com/home reply socksy 19 hours agoparentprevSUVs are pushed by manufacturers due to their relative lack of regulation in the US, and so the more money they can make from them. Safety and car seat sizes have nothing to do with it. https://youtu.be/jN7mSXMruEo reply thsksbd 19 hours agorootparentYou didn't read the OP's comment. He wants an A4 He has a much bugger car because he needs fo fit three car seats. That the market has cars bug enough for him, and the reasons for providing it, are not germane to his comment. reply socksy 15 hours agorootparentWhy do you feel the need to tell me I didn't read the comment? I did, and my comment was my response. > That the market has cars bug enough for him, and the reasons for providing it, are not germane to his comment. Did you watch the video? If the response to \"why are cars so big now\" is \"because kids\" then I think it's relevant to point out that that's not why they're big, and that we somehow manage to have 3 backseats and non-tank sized SUVs in Europe just fine. reply jodoherty 19 hours agoparentprevWe had 3 under 3. When our son was 5 and the twins were 3, we were able to switch to forward facing car seats in the back of my Dodge Challenger, which is technically just a coupe. They're all Diono Radian car seats, and the kids fit comfortably 3 across. Just thought I'd share in case you ever want to reconsider the dream car thing. You never know until you try and you can always bring car seats and test fit. To your original point though, most of the time we use my wife's Honda Pilot because of the extra space for errands, sports gear, and luggage (and also because my wife doesn't want to drive stick in Northern Virginia traffic). reply ryanisnan 16 hours agoparentprevNot directly my issue, but kids are my primary reason for needing a larger vehicle. We live in an area too remote for regular public transit, and are not ready to sacrifice the convenience of having our own vehicle. Because we own one vehicle, it must transport 2 kids (in carseats), 2 adults regularly, a third adult occasionally, and a dog regularly. Plus it needs a smidge of storage room. In short, our environment and family makes an SUV a good option. Everything else is really difficult to live with. reply stemlord 14 hours agoparentprevYeah crossovers are pointless. I had to sit in the back row of a crossover where every seat was full and ended up having the driver pull over due to claustrophobia. My head was against the ceiling and I could barely move. I'm 6' tall, not small but not a basketball player. reply D13Fd 17 hours agoparentprev> it’s impossible to fit 3 across in even the largest “crossovers” (VW Atlas / KIA Telluride) Just buy narrow car seats designed for 3 across. I've done it recently in a Subaru Outback with zero issues. It can be tough in the third row of cars with a third row because of the wheel wells, but it seems to work fine in a regular car. reply at0mic22 20 hours agoparentprevIn the US you have an option to turn front passenger airbags off and place a baby seat there. In Europe there's no such option. I hope those enormous modern seats are made that way for the sake of protection. Cause even with a bigger car, like Audi A6, two such seats consume all the space on the back bench reply dsego 20 hours agorootparent> In Europe there's no such option. Yes, there is. reply at0mic22 20 hours agorootparentOn the models designed for the US market - maybe. I have checked with 3 cars being produced specifically for Europe: Volvo, Audi and Toyota. While on the majority of videos and instructions there's a special off switch, on those cars it does not exist. reply p_l 19 hours agorootparentPretty sure it's mandatory - I seem to recall a considerable educational campaign when they first arrived in Poland regarding passenger front side airbag and child car seats. reply barrkel 16 hours agorootparentprevI live here and have rented vehicles across pretty much all brands and have never seen a car where the front passenger airbag can't be turned off. In fact there are ~identical regulatory stickers on the passenger sun shade warning that the passenger airbag needs to be turned off when a rear facing car seat is used in the passenger seat. reply tonyedgecombe 19 hours agorootparentprevI think you are incorrect on this, certainly for Audi but probably Toyota and Volvo as well. reply petre 19 hours agorootparentprevAll my European cars had that option including a 2005 diesel Suzuki Jimny and a 2013 Audi A1. reply Ringz 20 hours agorootparentprevNo such option in Europe? This is mandatory at all European countries since the first passenger seat airbag. reply okanat 20 hours agorootparentprev> In the US you have an option to turn front passenger airbags off and place a baby seat there. In Europe there's no such option. What? This is totally false. Even friggin cheapo Dacias have that feature. reply at0mic22 20 hours agorootparentMeh, probably I was thinking unlucky with my garage then reply Abekkus 20 hours agoparentprevDiono car seats fit three across in a CRV for us. They’re built like little airplane seats. reply jhbadger 19 hours agoparentprevBack in the day, there were station wagons for this exact use case, Larger than a sedan, but not the ridiculous tank-like SUVs of today. reply kj4211cash 16 hours agorootparentI bought a V60, one of the few remaining station wagons for sale in the US. And it is great for those with 1 or 2 kids, but, like the author implies, doesn't really work for those with 3 car seats. The only thing that has grown faster than the size of our cars is the size of our car seats. reply FooBarBizBazz 18 hours agorootparentprevYou can still buy a Subaru Outback. The infotainment is horrendous (just give me knobs!), but otherwise it's supposed to be a dependable station wagon, at a reasonable price, without too many frills. The AWD does hurt gas mileage, but has advantages in snow and dirt, and basically means that it's as capable as an SUV in all but ground clearance. reply ijhuygft776 8 hours agoparentprevbuying 3 car seats that fit in your car needs a bit of research, but I've done it in a Honda CRV reply rightbyte 20 hours agoparentprevYou put the passenger adult in the middle in the back. reply seoulmetro 8 hours agoparentprevLarge/Medium Sedans are larger than most SUVs, they are several sizes larger than all crossovers. I like sedans because they have the most space of all cars except maybe top of the range SUVs. reply rco8786 20 hours agoparentprevTime to join the minivan gang reply petre 20 hours agoparentprevWhat you need is a minivan. Toyota Proace City Verso for instance. SUVs are big but the usable space inside is quite small. It'a not a sports car or a status symbol, but it handles the 3 kids part quite fine. reply Cacti 20 hours agoparentprev3rd kid is much more expensive than the 2nd reply mleo 19 hours agorootparentThis is true. It required us to buy a new house with another room; versus more nice to have. Lots of public accommodations are maximized for four people such as restaurant seating or hotel rooms. Parents can no longer just split up with one kid. reply eitally 19 hours agorootparentThere are lower cost options. We did have to buy a new car (replaced a Toyota 4Runner with a Ford F150 Crew Cab) - our other car was already a Honda Odyssey - but all three of our kids share a bedroom and I bought a lightweight Helinox cot that we use in hotel rooms when we can't find a place with 2x queen/king + sofa bed. reply gumby 20 hours agoprevI see kitchen appliances are in the shit pit of the author’s graph and I can definitely testify to that. We just bought a new dishwasher and several of the advertised features are only accessible if I connect it to the net and use an app. Since I won’t connect it to the net, I could have saved money by buying a model that didn’t advertise those features. reply pxmpxm 20 hours agoparentHousehold appliance in general - our new GE washer and dryer has wifi for use case uknown, but to pay for the BOM they removed the interior drum lights. reply Loughla 19 hours agorootparentThe new washer/dryer all in one is absolutely confusing to me. So you can leave your clothes in one machine, and in two hours everything is washed and dried. But now I have to wait two hours for each load of laundry, versus 45 minutes and swap between machines. They have effectively more than doubled the time laundry takes. Who thought that was a good idea? reply Macha 18 hours agorootparentLaundry latency is not a problem for me. I do not have consecutive loads 90+% of the time. I have 3-4 loads a week. On the other hand, I do have limited space for appliances, so in many places I've lived buying a seperate washer and dryer would mean either forgoing a dishwasher, or getting a condenser dryer and installing it in the corner of some random room away from the other appliances (which would technically be a code violation as then it wouldn't have its own dedicated RCD). So I get a combo unit, turn it on and forget about it for several hours (when I'm home all day, I even use the eco mode, which takes 6 hours for a combined wash and dry). Come back later in the day and unload. It actually does have a 60 minute wash + dry cycle but I rarely use it as it uses a very hot dry cycle which risks shrinking clothes. reply ghaff 17 hours agorootparentAnd I'm probably more like a load every week or two. While I do find a larger washer handy every now and then I'm sort of regretting I didn't replace both my ancient washer and dryer with something that could handle both or could be stacked when my increasingly dysfunctional washer finally gave up the ghost. reply antongribok 18 hours agorootparentprevI love my LG washer/dryer combo. Yes, it takes a bit longer, but it's half the work from my point of view. I only load and unload clothes once (not twice). I only program a machine once (not twice). One big feature my LG has is it will keep rotating the drum a half a turn every few minutes, so you can take the dry, clean clothes out much, much later, and they will not be wrinkled. As others have said, if you're limited on space in your house it's a bonus. In my case I converted the space that would have been taken up by a second unit stacked vertically into large storage area with shelves. reply adrian_b 19 hours agorootparentprevThere are people who do not have enough suitable space for both a washer and a dryer. I am one of them, so I am happy to use a combined washer and dryer. The only place where I can put the washer and dryer is in a bathroom. Separate washer and dryer would make it unacceptably crowded. reply gumby 18 hours agorootparentSome front loaders can be stacked (this is more common in Europe) so if space is the only issue you can get the benefit of the specialized equipment. reply Macha 16 hours agorootparentThis assumes you have the vertical height to stack them. In Europe the utility connection points are usually in a kitchen or utility room under a cupboard, with a counter top on top and more storage above that. (e.g. this kind of arrangement: https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2F... ) reply gumby 11 hours agorootparentIn Paris my dryer and washer were in the kitchen, but vertical. In Berlin, they were in a closet next to one of the bathrooms, also vertical. In both cases apartments, if that matters. reply barrkel 15 hours agorootparentprevIn Germany and Switzerland, washers and dryers are usually in the basement, otherwise in the bathroom. In the UK and Ireland, they're usually in the kitchen. Unsafe to generalize. reply logiduck 18 hours agorootparentprevOutside of the US, (Latin america, europe, asia) it is more common to have an all in one usually in the kitchen. A lot of the foreign workers who stayed in corporate housing we had didn't even know what a dryer was or how to use it. reply haizhung 17 hours agorootparentprevThe new generation of appliances are more energy and resource efficient. They trade the off with using more time, though. Nearly all appliances have a „quick“ mode though if you really need something fast. reply Loughla 15 hours agorootparentNo it's not efficiency. It's the washing and drying being done in one machine. I cannot do both jobs concurrently. reply haizhung 15 hours agorootparentThen I’m even more confused what the problem is. Do you have so much laundry that you keep both machines perpetually running in parallel? I can only imagine that happening if you have a household of 8+ people. reply Loughla 11 hours agorootparentPerpetually, no. But for my family, with both machines ruining, we still have 4 hours of washing (5 loads each). These machines would double that amount of time, because I couldn't run the dryer at the same time. That's what I'm saying. reply mieubrisse 19 hours agorootparentprevDatapoint: as someone who lives in a small apartment without room for two machines, the combined washer/dryers are wonderful for me. I'm on board with the initial article, but it made me realize that enshittification is relative - to some people, the \"unnecessary features\" are actually useful. Maybe enshittification is actually a lesson on how to slice a product into different product lines? reply Loughla 11 hours agorootparentValid. Your point is valid. reply XorNot 19 hours agorootparentprevDid you consider not buying a product which doesn't suit your use case? Did the market suddenly lose all single function devices? reply Loughla 15 hours agorootparentWhy is it a trend to act as if you shouldn't have any opinions about something just because there are other things you can buy? I did not buy an all in one. I was just saying that they don't make sense to me. I'm allowed to have opinions. I'm a human. reply Macha 20 hours agoparentprevYeah, similarly discovered my washing machine's auto-dosing feature only works if connected to the internet and installing an app too. It needs WPS, so I couldn't even connect it if I wanted. reply zbrozek 19 hours agorootparentName and shame, please. I'm in the market for a dishwasher and would like to know what not to buy. reply gumby 15 hours agorootparentIt’s a Bosch 800 series if that helps. reply zbrozek 14 hours agorootparentThat model was on my list, but is now dead to me. Thank you for your help! reply gumby 11 hours agorootparentWhat I learned from this experience is to read the manual before buying. Fortunately they are all online -- in fact this Bosch only came with a \"quick start\" guide so the manual had to be downloaded anyway (and most of the doco is in the form of videos, ugh). reply bjelkeman-again 20 hours agorootparentprevThat is pretty obnoxious. reply rcarmo 19 hours agoparentprevWe proudly bought a €22 toaster yesterday what was the second cheapest in the store and completely avoided the fancy ones. I’m all for dumb appliances… reply tuna74 19 hours agoparentprev\"We just bought a new dishwasher and several of the advertised features are only accessible if I connect it to the net and use an app.\" If you buy it they will continue making and selling them. reply gumby 18 hours agorootparentThe problem is I didn’t know this until I bought it. I didn’t think to read the manual before purchasing. reply gumby 14 hours agorootparentMaybe I wasn’t clear: I refuse to connect appliances, TVs et al to the internet. So I don’t get to use these features I paid for. reply calvinmorrison 19 hours agoparentprevI'm very close to building my own dishwasher. I don't care about water conservation. I just want clean dishes. reply herpdyderp 19 hours agorootparentHave you ever had a dishwasher in your life that actually did a good job at cleaning dishes? I never have, and then they break anyway. I wash by hand. reply mastercheif 19 hours agorootparentTechnology Connections’ video on dishwashers will change your life. https://youtu.be/_rBO8neWw04 Ever since I switched to powdered detergent and started throwing a pinch of powder in the bottom of the tub in addition to filling up the main dispenser halfway, I have not had a single dish come out with a piece of food on it. I also recommend disabling any “auto” features of dishwasher has. The only caveat to this video: don’t fill up the detergent dispenser all the way with powder, go about halfway. He corrects this mistake in his follow-up video, which I also recommend. reply gumby 19 hours agorootparentprevDishwasher saves a lot of water and energy compared to hand wash if you run it with more than a small number of dishes. reply Macha 18 hours agorootparentBut don't overfill the device or it won't clean as effectively. If you're waiting until you're playing Jenga to maximise the items inside to turn it on, then you're probably obstructing the water flow and reducing cleaning effectiveness. reply coccinelle 19 hours agorootparentprevMy recently acquired Bosch dishwasher does a fine job. reply calvinmorrison 14 hours agorootparentprevI don't care about water or energy use. I care about clean dishes reply lukas099 19 hours agorootparentprevHm. It's strange when someone else's objective experience is so different from one's own. The house I'm renting has some cheapo dishwasher, and it does a great job. Only uses as much water as a toilet flush, too. reply jwells89 19 hours agorootparentprevMy Bosch dishwasher has been excellent. It wasn’t cheap but it does a great job and is dead silent (can barely hear it sitting in the open concept living room just a few feet from the kitchen). The only problem is that for whatever reason the power boxes required to wire them up for hardwired setups are bad and have a tendency to melt, as you’ll find reports of online. I’ve had this happen once already and intend to have an electrician convert my hardwire into an outlet so I can plug it in with a regular cord and not need the box. reply mostlylurks 19 hours agorootparentprevI actually have, but with the caveat that you have to know how to place the dishes in the thing such that they don't block the flow of water. Not a burden once you know how to do it, but might initially take some time to figure it out, if just looking at the thing doesn't make it obvious. reply felixgallo 19 hours agorootparentprevFor 30 years I suffered from terrible dishwashers, but then I moved into a house which has a Miele. It's like night and day. reply helij 15 hours agorootparentprevI used I think 5 dishwashers of 3 different brands in my life to-date. All 5 did a good job. reply skepticATX 19 hours agoprevI drive a compact sedan in an area known for its love of trucks. Even 5 years ago, it felt fine. Now I am noticing more and more that driving the sedan feels flat out dangerous. There are more huge trucks and SUVs on the road. They impede my view of the road, and will also clearly demolish my car in an accident. I swore off large vehicles when I first started driving, but it is to the point where I’m having to consider getting one for my next vehicle just to feel safe on the road. Something needs to change. reply bdcravens 19 hours agoparentI can't imagine things changing unfortunately. The best thing you can do is get the smallest SUV you feel comfortable with. That said, I'm in Houston, where trucks rule (by your username I assume you're in Austin). I'm less worried about whether the vehicle is a truck or not, and more about driver behavior. Stupid, impatient driving is the real danger. Driving defensively is your best strategy. I used to be an impatient driver; these days I more or less stick to driving with adaptive cruise control and just chill and keep my eyes out for the idiots (which includes being disciplined about handling my phone while driving) reply helij 15 hours agoparentprevThis is my biggest issue as well. I love small cars and would love to have them again but you just can't see anything out of them due to every car around you being three times the size and two times higher. Can't see anything. reply gcanyon 19 hours agoprevArguing on the flip side of this is the fact that the new Prius is faster from 0-60 than the muscle car Burt Reynolds drove in Smokey and the Bandit, has an equivalent top end, and: - gets something like 3x the mileage - has air conditioning - has a ridiculously better sound system - has anti-lock brakes - has traction control - has far superior crumple zones/crash protection - has air bags - is far more reliable (even when the Trans Am was new) - has 2x(?) the expected lifespan - has adaptive cruise control - probably a lot of other things I'm not thinking of reply bdcravens 19 hours agoparentMost cars are faster than that Trans Am, even cars that aren't super efficient. It's common to think of \"fast\" cars from an era with a sense of nostalgia, but objectively, they're pedestrian by comparison with your random modern sedan. I think the point of the article isn't to compare with cars from 50 years ago, as many of the points you made here have been the case for years. Cars got so good, that manufacturers had to start pushing to get more revenue and continue to sell cars, even though modern cars can last 15-20 years pretty reliably. reply eszed 14 hours agorootparentI think there was a sweet spot - roughly late 90s (maybe ODB2 adoption, whenever that was for a given brand), through the mid-oughts - where you got most of the benefits of computerization without most of the drawbacks. I have a car from that era that I never want to give up. reply gcanyon 19 hours agorootparentprevYep, I just picked the Prius because I posted stats about ita few months back and got railed because \"the Trans Am is cooler, man!\" so I had the stats handy. So your point is that the author's proposed enshitification has been recent and brief? I'll happily concede that cars getting larger and heavier is a blight, but that's been going on (in the U.S.) for at least twenty years, while many of the other points the author makes (and which I am dubious of) are just in the past 5 years or so. As one example, I'll happily take the pain of pairing over bluetooth over having to plug in to USB any day, and let's not even start comparing that to e.g. the cassette-simulator connectors of thirty years ago. :-) reply bdcravens 16 hours agorootparentIt has been a process, and I don't know that it's an obvious line. Bluetooth is great, but cars dating back to 2012 had it. Funny enough, most cars with CarPlay aren't yet wireless, requiring USB connections. If there's anything I'd point to as an obvious tipping point, I'd say it's switches and knobs that have been converted to touch controls. (for example, on my EV6, I have to use a touch control to toggle between radio controls and climate controls on the strip where you'd normally see them) reply Hamuko 19 hours agoparentprev>has a ridiculously better sound system That's definitely not one that is progressively getting better with time. I feel like my early 2000s car has a better quality sound than a 2010s BMW I test drove, and I'm pretty sure mine didn't have any kind of an upgrade package on it. reply fho 2 hours agorootparentI guess the thing with sound systems is that Hifi has basically not evolved in the last 30? Maybe even 50 years. I think the last tipping point was the introduction of Class-D amplifiers which provided \"unlimited\" power compared to before. Cheap DSP and a better understanding of psychoacoustics maybe. My point here is that nothing really changed the game. You could get a high-end, expensive Hifi system 30 years ago that matches the sound quality of a high-end system of today. The rest, Internet access, BT connectivity, Spotify is bells and whistles. reply londons_explore 1 hour agoprevIf you don't like modern cars, you can still buy an old one. You can even spend the money you saved on paying someone to properly refurbish it too. reply clearcoat 19 hours agoprevI haven't owned a car in the past decade, but the last car I did own was an Audi S5 and I thought it was pretty nearly perfect. Manual transmission, naturally aspirated V8, 4 wheel drive, but also all the electronic toys like lane sensors and physical buttons that were designed to give satisfying tactile feedback. Also just a beautiful car inside and out. I thought this was some kind of a high water mark for car design. You can't get a manual transmission anymore, they replaced the V8 with a V6 turbo, added weird lines all over the car that ruined the aesthetic. The new models are better on paper but feel twitchy and nervous to me. I haven't seen the latest ones, but I would not be surprised if they'd made everything touch-screen controlled as the trend in the industry has been. But anyway, c'est la vie, it was the end of an era and life moves on. We'd be better off with fewer cars, and with people thinking of them more like washing machines and focusing their attention elsewhere. reply jacknews 19 hours agoprev\"I know I sound like a luddite here\" There are plenty of articles explaining luddism, that it wasn't against technology, but against the way the tech was being used to shift profits from workers to owners. IMHO we need a neo-Luddite, or neo-Amish movement; technology, and economics, markets, etc, should serve society, not the other way around. reply LVB 19 hours agoprevTo the subjective complaints I’ll add heavy window tinting, specifically on windshields and front side windows. I don’t know if it is actually more common or I’m imagining that, but I really don’t like it as a pedestrian or fellow driver. Hand signals and eye contact play a role in the cooperative dance at intersections, and if I can’t see you then things are more dangerous. I err on the side of safety. Probably every other day I’ll be waiting to cross a road (usually on foot, but it can apply in car, too) and some dark tinted car will be paused, probably yielding to me, maybe, but who knows? Maybe they’re waving me on, maybe they’re glancing at their phone. So I just wait and wait, and eventually they’ll go. I do wonder if these folks understand no one can see them. reply mlrtime 20 hours agoprev\"I am not car aficionado\" This tells me that his level of understanding of the wide range of cars and features. Are their bad cars and bad UIs, yes. However, as someone who has been driving for 30 years, cars are better in every way now than they were before (From a driving, efficiency, reliability and safety perspective). The only thing that is arguably \"worse\" is self servicability. However this is a function of complexity that exists throughout all technology. Also, these features that everyone on HN complains about.. they are being added because the general population wants them. There are very few people who would buy a car without any screens, ui or infotainment. You wouldn't save any money because a lot of this tech is required for saftey and would be bespoke thus costing more. reply downut 19 hours agoparent\"However, as someone who has been driving for 30 years, cars are better in every way now than they were before (From a driving, efficiency, reliability and safety perspective).\" Right. Sure. 2001 Tundra V8 owner here, and you're just flat out wrong. Maybe you've had uniquely bad judgement in the vehicles you've acquired over the last 30 years. It happens. reply Hamuko 19 hours agorootparent>2001 Tundra V8 owner here, and you're just flat out wrong. That's not even considered a \"car\". reply downut 19 hours agorootparentHaven't driven in the suburbs and beyond, I'm guessing. A lot of places a PU is more common than, um, a \"car\". Most quite a bit bigger than my Tundra, too. reply Hamuko 19 hours agorootparentI live in a suburb. I just don't live in an American one. reply ramraj07 19 hours agoparentprevYou know who else is not a car aficionado? Most people in the world. I was hoping you might say something erudite about the engine timings since you knocked the author for not being a car expert and all you were able to say was “screens are important for safety” like what safety? reply chikitabanana 5 hours agorootparentnot him but backup cameras require screens. reply tourmalinetaco 19 hours agorootparentprevThey also don’t seem too worried about self-serviceability, when in my experience that’s all the car aficionados in my life care about is modifying their cars to their own specifications. reply lukas099 19 hours agoparentprev> cars are better in every way now than they were before (From a driving, efficiency, reliability and safety perspective). What about safety of people outside the cars? By this metric I believe they are getting worse. reply LightBug1 19 hours agoparentprevI am a car aficionado and, while you're right in terms of improvements, I'd argue that the factors the OP has highlighted are absolutely correct and overwhelm the those improvements. Especially cars from the last 10 years. Recent cars are horrible appliances that are incredibly difficult to repair. So you're both right, but it didn't need to be that way. The manufacturers have royally screwed up the direction in which the industry has gone. And you say features are being added because the general population wants them. I'll revert to my go-to argument here. The general population want McDonald's too. reply hparadiz 19 hours agorootparentMy 2020 civic has turbo but also lane sensing, collision detection, cruise control that adjusts based on what's in front of me, and a bunch of other features. That's what I'm paying for. Sure I can't fix it myself but that's not the point. The car was hit while parked in its first few months and I was very scared that the repair wouldn't be good but turns out the body shop knew what it was doing and did everything correctly putting it back together like new. Insurance was billed about 6k since there were hundreds of parts that needed to be replaced. Do I even want to learn how to do all that myself? Nope. Cars are a utility for most people. reply jspash 20 hours agoparentprevWhat gets me about newer cars is that they seem to be stolen at the same rate as they have always been (conjecture. i could be entirely mistaken). What with keyless entry, immobilizers and standard car alarms, I would expect them to be virtually theft-proof nowadays. reply astura 19 hours agorootparentMotor vehicle thefts look down to me https://www.statista.com/statistics/191216/reported-motor-ve... reply bborud 19 hours agoparentprevThey are better when they work. They are significanly worse when they break down. Which almost every car does at some pojnt. Every time they break there is a risk that fixing it is going to be so expensive a huge chunk of your investment is going to be lost. Or that you have to scrap your car because it isn't worth the risk in case something else breaks. Yes, modern cars are more comfortable, easier to use, and safer. I don't agree that they are more reliable since fixability is a factor in reliability. reply cowpig 20 hours agoparentprev> cars are better in every way now than they were before The author listed a bunch of ways that they are not. Perhaps it would be more interesting to address those directly reply tourmalinetaco 19 hours agoparentprev> However this is a function of complexity that exists throughout all technology. To a degree, yes, but auto manufacturers are all too willing to make self-servicing difficult. They only make money when you buy a new car after all. We also see this with, well, almost every other field. > they are being added because the general population wants them. Much like phones with unremovable batteries leading to slimmer devices, I can only imagine this is a product of marketing convincing consumers of their desires, rather than them actually requesting all of that. It doesn’t even really make sense; why would the consumer want all of that when they’re not even using it the vast majority of the time in the vehicle? reply calvinmorrison 19 hours agorootparentThe simplest fix is to require oems to include on board diagnostics on par with dealership tools. reply hotpotamus 20 hours agoparentprev> There are very few people who would buy a car without any screens, ui or infotainment. I really don't get this - just put a space in the dash where I can stick a phone that charges it and connects to the car audio. I have an old GM car with Carplay that all still works pretty well, but it's basically just a second screen for my phone and as such I barely use it. reply gambiting 20 hours agoprev>>Actually, scratch that – have you used a “smart” TV in the past 5 years? – it’s awful. I have, but I guess the author hasn't. I've got a 2020 LG CX and it's smooth, works fast, has all the apps I need, including for streaming local media, and it has zero ads. I sort of lost interest after that point, if you can't make a reasonable point right at the start why even continue? reply ramraj07 19 hours agoparentLGs and Samsungs have become the premium brands with associated pricing and is not the ones that are made for the majority. Not to mention it’s not universal that they make good smart tvs either. reply rcarmo 19 hours agorootparentLG is well on the path to enshittification with ads, although Samsung is already way below water level there. Both do image fingerprinting and phone home your viewing habits (my LG is neutered, and I would still buy LG for the overall quality, but I don’t think I will ever again plug a TV onto my network directly). reply hparadiz 19 hours agorootparentSony is the real premium brand among them. Their OLED series runs almost stock Android TV. reply tgeorge 19 hours agoparentprevI have seen ads on my CX at one time but I think I might of disabled them at one point. https://www.reddit.com/r/OLED/comments/8yts0m/ad_blocking_so... But I don’t think high end TVs like the LG OLEDs are not safe from ADs since it’s extra revenue reply tuna74 19 hours agoparentprevI totally agree, I have had two recent Sony Android TVs and they have been great! reply akho 19 hours agoparentprevThe article was written by a person who bought a 3D TV when they were available. reply Hamuko 19 hours agoparentprevMy LG C1 definitely has ads, which show up as toast notifications when you open the TV. I think I once got an ad for Google Stadia for example. That being said, it seems to very rarely show them. I think I can count the number of times I see one in a year with my fingers. As for the smart TV side, I have no idea if there's ads there since I basically only ever use my Apple TV. https://www.reddit.com/r/LGOLED/comments/xzxhjr/how_do_i_dea... reply wbobeirne 20 hours agoprevWith regards to the OPs note about headlights being too bright: Has anyone adjusted their headlight angles? I noticed oncoming traffic has occasionally been flashing their brights at me, and I suspect they think I left my brights on and are trying to tell me. I'd love to drop my headlight angle a degree or two but don't know if that's a daunting task on a modern car. reply ryukoposting 20 hours agoparentIf you consider my 24 year old car \"modern,\" then my modern car is easy. There's a screw on top of each headlight that will tilt them up and down. I thought I noticed people flashing their brights at me, but then I realized it's actually them hitting a small bump, thereby raising the angle of their LED headlights momentarily. I drive a somewhat low-sitting sedan, so SUV LED headlights do that a lot. reply kedikedi 20 hours agoparentprevTwo things; modern HID or laser or LED headlights should have the auto adjustment, so just jump to the second paragraph if you have one of these. But halogens can be adjusted usually from inside. They tend to have adjustment buttons to compensate for load (situation might be different for US cars, I’ve seen this with EU cars). This could be easier place to get started. The manual adjustment is also very easy :) there are some screws on your headlight assembly that you need to rotate. It is different for different cars, though. So I’d suggest referring to YouTube for your own car’s adjustments. reply kristjank 19 hours agoparentprevIt seems like most are just broken out of the box. I have never encountered a Tesla on the road that did not blind me, regardless of whether it was behind or in front of me. Some manufacturers just get away with shitty light projection. reply apapapa 10 hours agoparentprev> Has anyone adjusted their headlight angles? My Mazda has a scroll wheel to adjust their angle reply Abekkus 20 hours agoparentprevThis can also be due to dirty/aged headlight covers dissipating your headlight beams wider than intended. You can get kits to clean them, but it’s a bit tricky reply CogitoCogito 20 hours agoparentprevI do feel there’s a regulatory failure with headlight angles in the US. Owners of maladjusted headlights should get fix it tickets and it should be strictly enforced. If a car without high beams blinds you, something is wrong. reply ed_balls 20 hours agoparentprevolder cars had a knob to adjust them. With LEDs you usually have to open the hood and use a wrench or they can auto adjust. Service should do it for it. reply ramraj07 19 hours agoparentprevThere was a bestof Reddit post about some automotive expert who worked on designing headlights who put a large passage about why lights look brighter. I felt that it was just a large gaslighting exercise in the end lol. reply zdw 19 hours agoprev> I miss my 2005 Toyota Corolla This begs the question, why did you get rid of that? I bought Toyota Matrix new in 2006, still driving it. From what I can tell, keeping it running is still more environmentally friendly than replacing it with a new car, even if that car is electric/hybrid. Has no smart anything. Hardly ever breaks. Probably good for another 150k miles, and at this rate it'll be another decade or two by then and all the current car garbage will be worked out or legislated around. reply bob1029 19 hours agoprevThese days when I see the word \"consumer\" I go to a dark place very quickly. If you were to play a word association game with me and brought this one up, you'd probably find the resulting term would run aground of HN comment policies. I stopped buying things for fun a long time ago. The act of shopping for most things has become a stressful experience for me. I can feel the contempt for my existence in the design of many products. reply xyzelement 19 hours agoprevAlas a set of rehashed points that don’t resonate. // Hiding critical/often-used buttons behind touchscreens for the sake of minimalism (and cost?) I see this one written of often but I don’t see it on my 2 year old Toyota or cars I’ve borrowed/rented. I literally never had to access the touch screen for a critical function. Assuming this problem does exist on some cars, it seems easy to avoid as I’ve avoided it without trying. // On that note, poor physical layout in general (genuinely thinking that you can rate cars’ on a “hazard-light-button-placement index”) I’ve definitely experienced the moment in a rental/borrowed vehicle where it took me a bit of scanning around the dashboard to find the blinkers button. But this seems to just be variance across manufacturers. Eg my Toyota is consistent with Toyotas that came before it even if that’s different than some other manufacturer’s internal consistency. Is the author really struggling to find the blinkers on a vehicle he’s already familiar with? // Cars are getting too big; why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I could write pages on this point alone. One thing here is that this is a single man who can’t relate to the needs and desires of other types of customers. I loved small cars as a single man and love my mid size SUV as a dad of two and on certain occasions wish it was bigger. Cars are getting bigger because through marvel of technology that has become economical to manufacture and operate. Eg my SUV gets nearly 40 mpg whereas a few decades ago that was economy car territory. reply bdcravens 19 hours agoparentToyotas are pretty good in that regard (we've owned several). However, my wife now owns a VW ID.4, and it is the worst car UI/UX I've ever experienced. Likely the author was thinking of Teslas, which is the worst example of this, but I don't think this is strictly an EV thing, as my EV6 is very practical with physical controls, despite having the typical large touchscreen (for setting, infotainment, navigation, etc) reply xyzelement 19 hours agorootparentWhat’s an example of something that’s inaccessible on the WV? reply bdcravens 16 hours agorootparentMy favorite example is the window controls. Instead of the standard 4 toggle switches (front and back, driver and passenger side) there's only two, with a touch control to toggle between front and back that gives pretty much no feedback. reply shortsunblack 15 hours agoparentprevCars are only getting bigger and they are getting bigger because light trucks are not regulated. Also, there's the Chicken tax which preferences American car makers. None of this is consumer demand. It's about maximizing profit with bad regulation that has unintended consequences. reply ghusto 19 hours agoparentprev> Cars are getting bigger because through marvel of technology that has become economical to manufacture and operate Cars are getting bigger because they can be made bigger? Did anyone ask; \"would you _like_ a bigger car?\"? Putting aside everything else, I don't appreciate these massive things taking up so much space that often we have to sacrifice an entire extra parking space in a row so that we can park safely. reply xyzelement 19 hours agorootparent// Did anyone ask; \"would you _like_ a bigger car?\"? No, this is a real problem :( I wanted to buy a Corolla but the dealer tickle tortured me until I gave in and said “fine I’ll buy the Highlander” I still shudder to think about it every time I easily fit my wife, kids and the stuff into the car for a road trip. Kidding aside “do you want this?” is a question the market asks of consumer. Bigger cars still cost more to buy and run than compacts. People are willing to pay more because clearly enough of us want these bigger cars. reply fuzzy2 19 hours agoparentprev\"My car isn't like that so it can't be a problem\", really? Wow reply xyzelement 19 hours agorootparent“A problem that can be easily avoided is not a huge problem” is the point the post you are replying to makes. reply asylteltine 19 hours agoparentprevYou can’t fit two children in a sedan? Europeans seem to manage… reply xyzelement 19 hours agorootparentI can. If I had to, I would. I prefer my SUV as do millions of others. I am answering the question of “why are cars larger” not claiming you can’t make do w something else. reply FooBarBizBazz 18 hours agorootparentprevIt's a question of diet and exercise. reply sriacha 20 hours agoprev>Have you tried shopping for a TV in the past 5 years? Actually, scratch that – have you used a “smart” TV in the past 5 years? – it’s awful. Sure, displays are getting better, but manufacturers insist on cramming bloatware and spyware, embellished in the worst UI imaginable, running on a processor designed for a smartwatch. Anyone have recommendations for a \"dumb\" TV in 2024? reply kredd 20 hours agoparentHonestly, any smart TV connected to Apple TV is fine. You can skip the wifi enablement on your TV and will never think of the bloatware in your life. Been with that setup for 5+ years, have absolutely zero complaints about my “smart TV” for that reason. reply rcarmo 19 hours agorootparentYup. My LG is stuck at the EULA “we reserve the right to watch what you watch” screen but works fine as a dumb monitor for my Apple TV, SHIELD and Xbox. reply crazygringo 19 hours agoparentprevHere you go: https://www.bestbuy.com/site/searchpage.jsp?id=pcat17071&qp=... Best Buy search even has a filter for \"Non-Smart\" -- this is a whole thing. Insignia is their house brand. These are their 5 sizes. reply MisterSandman 19 hours agoparentprevThe only choice you have is a monitor, if the screen size you were looking for isn’t too big. reply okanat 17 hours agoparentprevYou can buy a public information display and reject to set up the smart parts. It doesn't force any connectivity. Samsung has QH and QM models for that. reply Loughla 19 hours agoparentprevSceptre baby. Cheap. Good picture. No stupid smart features. reply peanut-walrus 17 hours agoprevHeadlights are too bright in the US because US has ridiculous regulations when it comes to car headlights. https://www.nbcnews.com/news/us-news/blinded-light-american-... Come drive a BMW with laser headlights in Europe, you will be illuminated(!), it's absolutely amazing what modern headlights can do. https://www.youtube.com/watch?v=q_XEQ8n-R5M reply underdeserver 20 hours agoprev> Cars are getting too big; why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I could write pages on this point alone. I think this is partially a race to the bottom (or to the top, as it were). When you're driving your low-riding sedan in traffic, if the car in front of you is an SUV, you can't see beyond it. If most cars become SUVs, you can't see two cars ahead almost ever. That's a crappy feeling - I for one feel less safe, less able to predict if an emergency stop is going to happen - so I'd be unconsciously nudged into buying a taller car myself. reply ytdytvhxgydvhh 19 hours agoparentWell it’s also because the US government set up rules that end up encouraging crossovers over lower cars: https://www.roadandtrack.com/car-culture/car-design/a3349059... reply stefs 19 hours agoparentprevKeeping the proper safety distance to the car in front of you is surprisingly hard. On the one hand, it goes against the learned behaviour (of most people). On the other hand, other drivers' bad behaviour actively sabotages this by overtaking and/or squeezing in the spaces that open up from proper distance. Most drivers just don't have the emotional control to drive well, but the actual problem is they don't even realise they're bad drivers. SUVs are the completely wrong solution to this problem. I'm not sure if distance warning (akin to seatbelt) beepers could help, but I suspect people would hate them so much it'd doom sales. reply sndean 19 hours agoparentprev> When you're driving your low-riding sedan in traffic, if the car in front of you is an SUV, you can't see beyond it. I had a Scion FRS (a very small and short car) for a while and very much was the (annoying[1]) guy who stayed 10+ car lengths away from the person in front of me so that I could see better. This was one of the reasons I traded it in for something taller. [1] I had many more incidences of road rage directed at me than I have experienced in other cars before or since because I left so much space in front of me. Lots of \"what the ** are you doing?\" screamed at me. reply CogitoCogito 20 hours agoparentprevI can’t really understand this sentiment. If you keep enough distance you’ll be able to stop in time regardless of what’s down the road. I personally feel safer in smaller cars. reply underdeserver 18 hours agorootparentWell, I don't know how it is where you drive, but here you have two options: you either stay just close enough that you can stop in an emergency, but no closer, and certainly not a \"comfortable\" distance; or you can get cut off every couple of seconds. reply CogitoCogito 18 hours agorootparentMy experiences are driving mainly in California and New York. I definitely have not had the problems you’ve described and I wouldn’t consider drivers in either location to be very good in general. I’ve driven larger cars as well and they’ve never made me feel safer. Edit: The more I think about it, I’m actually kind of confused by your point. Even assuming what you say is true, how does driving a larger car avoid the problem of people cutting you off anyway? Is your “solution” to drive a larger car and tailgate or something? reply xnx 19 hours agoprevAgreeing with the article. Car software sucks, but hardware is great. Cars have more horsepower, are more fuel efficient, are more reliable, and are most importantly much safer ( to their passengers at least) than they've ever been. Take one minute to watch \"1959 Chevrolet Bel Air vs. 2009 Chevrolet Malibu IIHS crash test\" https://m.youtube.com/watch?v=C_r5UJrxcck reply osigurdson 19 hours agoprevI've never owned a car with CarPlay but that seems like it would be far better than auto manufacturer proprietary / snowflake UIs that I have on my cars. reply hnthrowaway0328 20 hours agoprevMaybe just grab a second hand Toyato that can pretty much run indefinitely as long as it is maintained. That's my plan if my wife decides to ditch me :p reply at0mic22 20 hours agoparentI'm owning 2004 Toyota 120 as a hobby car. Fairy tales about old Toyota's being ultra reliable make me cry. On the bright side - they are fixable and manageable, but you cant avoid inevitable agening problems. Think I have invested as much as the buying price to repairs already reply beretguy 20 hours agoparentprevUnexpected ending. reply hnthrowaway0328 15 hours agorootparentGetting divorced is probably many men' fantasy :D reply stefs 19 hours agoparentprevUsually the problem is availability of spare parts. reply LightBug1 19 hours agorootparentCurrently running a 2005 Toyota. No problem at all regarding spare parts so far. I class it as one of my best, unwittingly made decisions to keep that car considering what I've seen of new cars over the last 10 years. reply amai 13 hours agoprevA big part of features is also due to regulations like from the EU The advanced safety features which will become mandatory in all vehicles are: - intelligent speed assistance - alcohol interlock installation facilitation (i.e. a standardised interface facilitating aftermarket alcohol interlock devices being fitted in vehicles); - driver drowsiness and attention warning; - advanced driver distraction warning; - emergency stop signal; - reversing detection; - accident data recorder. Read more at: https://trans.info/the-european-parliament-agreed-on-new-saf... reply t0bia_s 10 hours agoprev\"Cars are getting too big; why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I could write pages on this point alone.\" Because when you talk about safety, concerns about environment steps a side. Basic argument that greenwashing is another level of marketing for consumers. reply dml2135 18 hours agoprevI’m a 30-something who lives in the big city so I haven’t had a car since the 2003 Volvo S60 I had in high school. I always said I’d probably get a car when I could get an electric one in middle age, and it looks like that day may be coming up in the next few years, but I’ll be damned if I’m ever going to buy something with a touch screen and mics that are tracking my every move. For anyone more in touch with the current car market than I am, is there even a single option for a full electric, or even a plug-in hybrid, without a touch screen and internet connection? I do not give a single fuck about infotainment systems, I’ll gladly plug my phone into an aux cable and call it a day. reply lawgimenez 18 hours agoparentMy 2006 car stereo has been dead for several years, my current infotainment is a JBL portable speaker via Bluetooth. reply Dracophoenix 17 hours agoparentprevOne recent car that is closest to fitting your description is the 2023 Audi E-tron. reply mnming 12 hours agoprevI get why people hate new things in car as they often don't improve driving experience. But smart tv? I think they genuinely improved my experience. I can pretty much watch any show anytime using my phone as the controller now, I don't see how it's possible with old TV? reply hiisukun 4 hours agoparentFor a lot of people, they replaced a 10 or 20 year old TV with a \"smart\" one. Then just a few years later, it didn't work properly -- apps not updated, UI slow or broken, logins failed, etc. The TV was essentially fine, but the \"smart\" component had aged very poorly and made the whole thing less than satisfying. You're talking about the great and convenient parts, but for many generations of smart tv those only last a short while. reply intrasight 19 hours agoprevI just bought a used Volvo 2016 XC70 - the last year it was made. I bought it to replace a 2004. I honestly think it's the finest form factor of car. This new one doesn't have much in the wave digital integration. It does have a back up camera - which I'll probably appreciate. But no touchscreen. The vehicle only has 40k miles. With luck and good maintenance, it could be the last car I ever own. reply BugsJustFindMe 17 hours agoprev> That weird glossy pastel paintwork that’s so popular all of a sudden I believe the technical term is \"putty-lookin' ass whip\". https://www.blackbirdspyplane.com/p/why-do-new-cars-look-lik... reply amai 13 hours agoprevNot all is lost, yet. Have a look at the Carice TC2: https://newatlas.com/automotive/carice-tc2-electric-car/ reply TheCapeGreek 16 hours agoprev>why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? I wonder this about crossovers because every time I look into one, I reach the conclusion that it's a bigger car with somehow not much extra space on the inside for how much bigger it is. reply api 20 hours agoprevMakes me really happy with the Nissan Leaf I bought a few years ago. It’s a Nissan Versa with an electric motor and battery in it. That means most of the parts on the car are standard and the body is cheap to repair. Lots of higher end EVs have stupid high repair costs from minor accidents due to this not being the case. It has a dumb infotainment system that I just plug a phone into and use as a screen. All the car’s critical functions are physical buttons that are ergonomically placed. Acceleration and handling is fine. In fact it out accelerates most gas cars. It has one pedal driving. The adaptive cruise control and auto steer work great. If you are stuck in traffic it just guides you along leaving a selectable distance between you and the car ahead. Steering assist has never tried to kill me. 220 miles of range is more than enough for a city car and I can charge it at home for stupid cheap. (It’s like $8 to fill from zero off home power, about 1/4 to 1/6 the cost of fueling an efficient gas car.) It’s small and electric and is called a “Leaf” so it does nothing for one’s penis. But I am secure in that department. Literally the only sucky thing about it is its CHaDEMo fast charge port which is chonky and a fading standard. Makes it not great for road trips but we have an old gas second car for that. Make a model of this with a bit more range and NACS and Nissan would have the ultimate boring competent EV that I want. I like the car enough for its boring competence that I’ve looked into CCS or NACS conversion. Apparently it’s possible and there are people working on kits but I’d have to find someone to do the install since I don’t have time for such a project. PSA: anyone who knows high voltage electronics well could design and crowdfund a NACS to CHaDEMo dongle on Crowdsupply no problem. It would be non trivial though because it would have to be powered. But there are enough cars with that older standard to make millions on such a device. I’d spend as much as $1000 or a bit more for one. reply matthewowen 19 hours agoparentThat’s great, but you have a second car to cover the things your primary car can’t do! That might make sense if you do a daily commute that makes the convenience and running costs of the smaller car worth it. In general I see a lot of snark about people driving (eg) trucks around that people assume they can’t possibly need. But the reality is that people buy their vehicles based on the most challenging routine thing they need their car to do. I mostly only have four people in my minivan, but the extra space is great for road trips or when the grandparents visit. A lot of people have trucks because they want the towing capacity a few times a year, or they want the space to haul dirty stuff. IDK, I find the dismissiveness of “must be compensating for something” a little tiresome and honestly kind of incurious. reply api 19 hours agorootparentI was intending to joke about how the auto industry caters to image and fashion not utility and many cars are encrusted with things people don’t need… whether it’s being excessively big and jacked up or a bunch of bloatware on an infotainment system. What you say is somewhat true but we really do have a ridiculous oversized car issue in the US. The irony is that many of the people who really could use these trucks can’t afford them. They are high priced status symbols. What I was ultimately getting at was that the car industry does not want to sell practical cars. The margin on those is lower. They want to sell puffed up chonkmobiles or overly complex bloatware machines. Then they want to double dip with surveillance. Enshittification for cars. reply mglz 19 hours agoprevOne car maker that doesn't seem too terrible so far with this is Renault. I drive a Zoe and looked at an electic Twingo and those cars are basic but in a nice way. You get physical buttons, a small screen you can run AndroidAuto/CarPlay on and no \"smartass\" features. There is a companion app that tells you the state of charge and that's it. I also did a GDPR request to Renault and either they are lying or they don't collect any telemetry. The drivetrain components are all simple parts and fixable in most repair shops. The battery and motor are another thing of course (needs a trained electrician), but that's probably inherent to EVs. Never had any software issues or updates preventing me from driving. Regarding the points in the article: * Critical cuttons are all there * Buttons are located where you'd expect and can be operated without looking * Lights are bright AF, sorry oncoming traffic :( * Basic tires and lots of standard Clio parts all over * App with a subscription you can ignore without major loss of functionality (remote heating) * All smartphones i plugged in via USB work * Paintwork: Everybody seems to get the white Zoe, good luck finding your car in a parking lot. Interior is black and white recycled plastic. Still, highly recommended and fun vehicles :) reply akho 19 hours agoprevI do not understand this. Just don't buy things you don't like? I'm happy with my current printer, and they've only improved in my experience (auto-discovered network printers are an order of magnitude better than the wires-and-drivers bullshit, and that's pretty much the only change I see over the past 20 years or so). My LG TV is fine. Can't really speak about cars (the '12 Toyota I own is not really representative), but the new ones I see are driveable, and I don't see a trend (outside of Chinese brands and EVs that think they're an iPad, which are separate topics). reply nurettin 19 hours agoprevOver the years I've adjusted my life such that I can live without owning a car. My place is at a walking distance from the office, I live near a hospital, there are multiple supermarkets apothecaries, schools, diners and pubs that I can easily walk to and back without encountering any dangers. I've saved on so many costs and that it is almost criminal at this point. Not having any traffic stress is an added bonus. I actually feel a guilty pleasure walking past cars stuck in traffic. reply kibwen 19 hours agoparentSeconded. My partner and I got rid of our cars years ago. The other day I overheard someone at work talking about their monthly lease for their two cars, and the lease alone (to say nothing of gas, repairs, insurance) was more than our rent. Cars are a money bonfire, and it's worth moving to even the most expensive city if it lets you get away with not owning them. reply hasty_pudding 20 hours agoprevI had a ford van I drove for decades, then tried to buy a new car and holy shit the amount of electronic gadgets on all cars nowadays is insane. you cant just buy a simple car anymore. reply esarbe 19 hours agoprevI miss real knobs and sliders to control my car. These touch screen displays in cars. I want to have my eyes on the road. reply meerita 20 hours agoprevI love my car. I love driving it every weekend. Cars are my passion, sorry. reply bryancoxwell 19 hours agoparentWell, that’s fine too. There are plenty of things I enjoy watching on my TV, but the thing still absolutely sucks in ways it doesn’t need to. reply osigurdson 19 hours agoprev>> manufacturers pushing “crossovers” and SUVs on consumers This might have something to do with the fact that this is what people want. It is still possible to buy a small car. reply neilk 17 hours agoprevI’ve noticed many posters using Doctorow’s term “enshittification” here. In the interest of preserving a useful word for a few more years - I don’t think it fully applies. Enshittification is about how _platforms_ die, in the ultimate phase using monopoly power to extract rents from increasingly frustrated users. As bad as car touch screens are, is this rent-seeking? You could argue that subscriptions to “features” in your car are getting closer to Doctorow’s concept. But cars aren’t platforms. I am not locked into my VW purchase because all the other drivers are using VWs. reply goodluckchuck 18 hours agoprev> why are manufacturers pushing “crossovers” and SUVs on consumers and why are consumers buying them? They’re pushing crossovers because they can sell them for a higher margin. A taller vehicle looks like you’re getting more for your money. Of course, they don’t actually cost that much more to make. People buy them because that’s what’s on the lot and that’s what’s trendy… and the manufacturers create the trends to increase profits. The market is rational, but no one said it was very smart. reply cannabis_sam 13 hours agoprevCars are indisputably garage, and a detriment to society. A pitiful stopgap, that has unfortunately been embraced by troglodytes.. jill the psychopath who believes in this salvation reply 39 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author highlights the dissatisfaction with the current state of cars, drawing parallels with the issues seen in smart TVs.",
      "Excessive features, poor design choices, and user-unfriendly elements like hidden buttons behind touchscreens and overly bright LED lights are criticized.",
      "The trend of larger vehicles, subscription-style plans, and privacy violations are also mentioned, with a call for simpler and more user-friendly cars like the author's old Toyota Corolla."
    ],
    "commentSummary": [
      "The article and discussion touch on several topics related to modern cars, including dissatisfaction with excessive technology and regulations, the benefits of minimalist and open-source car designs, and the popularity of SUVs and crossovers.",
      "Commenters express their preferences for simplicity, customization, and practicality in cars, as well as concerns about cost, safety, and environmental impact.",
      "The conversation emphasizes the need for affordable, user-friendly, and reliable vehicles that meet the desires of consumers."
    ],
    "points": 185,
    "commentCount": 289,
    "retryCount": 0,
    "time": 1705237723
  },
  {
    "id": 38990755,
    "title": "Reclaiming Your Digital Privacy: Deleting Data from Data Brokers",
    "originLink": "https://www.cybercollective.org/blog/how-to-delete-your-data-from-data-brokers",
    "originBody": "Blog 8 min read How to Delete Your Data From Data Brokers Published on January 9, 2024 Learn how to reclaim control over your digital privacy with practical steps and consumer protection laws. Research Abstract: All Content All Blog Content Contributors Rebecca Richard Senior Digital Content and Design Strategist, Cyber Collective Subscribe to newsletter By subscribing you agree to with our Privacy Policy. Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. You probably don't think much about data brokers, but these behind-the-scenes companies collect, trade, and profit from our personal information every day. Data brokers like Acxiom, Experian, and Equifax gather data about us from public records, online activities, store loyalty programs, and more. This data gets packaged into profiles and sold to other businesses for targeted advertising, credit checks, background screening, and who knows what else. All this happens without our knowledge or consent. Most people have no idea the extent of their digital footprint or how to even access the data collected about them. While we willingly give away some personal information to use online services, plenty is taken without our permission. This lack of transparency and control is cause for concern. We have a right to know what personal data exists about us and where it is going. The good news is that there are steps you can take to uncover and delete your data from broker sites: Request Your Data & Send Removal Requests Many data brokers allow you to access your personal information in their systems. Some even offer opt-out tools to stop data collection. Search for \"data broker opt-out\" to find relevant sites and submit access/deletion requests. Be sure to keep records of all your requests, including copies of any emails or letters sent. Maintaining this documentation can be useful if you need to follow up or dispute an unfulfilled request. For brokers without online tools, you'll need to directly request data removal via email or letter. Clearly state your name, address, relevant account numbers, and that you want your data deleted. Keep records of all requests. Here is a list of the most prevalent data brokers: Acxiom Experian Equifax TransUnion PeopleSmart BeenVerified WhitePages InstantCheckmate Pipl Intelius Leverage Consumer Protection Laws Depending on your location, laws like the CCPA and GDPR give consumers the right to request that companies delete their data. The California Consumer Privacy Act (CCPA) and the European Union's General Data Protection Regulation (GDPR) are two pivotal privacy laws that give consumers greater control over their personal data. The CCPA went into effect in 2020 and grants new rights to California residents, like: The right to know what personal information companies collect about them and how it is used. The right to opt-out of the sale of their data. The right to request that their personal information be deleted. The GDPR took effect in 2018 and imposes strict guidelines around collecting and handling EU citizens' personal data, such as: Requiring affirmative consent before collecting an individual's data. Obligating companies to honor requests for data erasure within one month. Establishing protections for data breach notification and cross-border data transfers. Leveraging these landmark laws can strengthen your ability to take control of your data from brokers. Understand your rights and don't hesitate to cite CCPA or GDPR to compel compliance. Use a Deletion Service Sites like DeleteMe, Incogni and Abine will submit opt-out requests on your behalf and monitor to ensure your data stays removed. This simplifies the process, for a fee. Regaining control of your personal information takes persistence and vigilance, but is worth it. Data brokers will continue collecting our data until laws and consumer pressure demand change. But we can fight back by actively managing our digital footprints and being conscious about what we are consenting to online. * Links may contain affiliate links. Notes Sources Related Blog Posts Don't stop at one, check out the rest of our research and expand your horizons All Blog Content 7 min read What To Do After You Get Hacked Here's everything should do after your account gets hacked, from start to finish. View all",
    "commentLink": "https://news.ycombinator.com/item?id=38990755",
    "commentBody": "How to delete your data from data brokers (cybercollective.org)181 points by jyunwai 19 hours agohidepastfavorite89 comments yoaviram 17 hours agoPSA, Getting data brokers to delete your personal data can be very frustrating as their business model depends on this data. Simply put, they use deceptive patterns to avoid complying with data protection requests. We have put together this guide which describes the most common deceptive patterns and how to counter them. For example, in many cases data brokers cannot ask you to send excessive personal information in order to verify your identity. You also don't need to fill in online forms. Hope this helps: https://consciousdigital.org/wp-content/uploads/2023/04/dark... I'm one of the creators of https://databrokerswatch.org and https://yourdigitalrights.org/ reply sillystuff 15 hours agoparentIt is great that a free opt out service exists, but, \"search for organization...\" one-at-a-time deletion requests isn't much of an improvement over doing things fully manually. A way of handling bulk requests would be nice. E.g., if only making requests to data brokers, you are looking at around 700 different companies that collect/sell data on Californians. If also, additionally, making requests to the companies that originated the data, it would easily be over 1000 requests. Web UIs are terrible, but even a giant list with check boxes would be better than one-at-a-time (but, this would mean the server needs to remember state between visits to avoid an extremely frustrating user experience). Download complete list as CSV, add some value to a \"selected\" column, and re-upload would be nice for some of us, but probably a turn off to most-- especially since merging future changes of the upstream file into the modified user copy is probably beyond the capabilities of most users. At the risk of creating records where a broker had none before, maybe just the option to splat the request out to all companies* in your list that do business in a particular region of the world? Super easy for the user, and no state to retain on your end. Anyway, thanks for working on this. But, one-at-a-time requests is too high a usability bar for me. * Or, all companies per category in your region. E.g., all databrokers in region, all retail companies in region, all financial/insurance companies in region... etc. Although I'd guess that most folks would just select all categories, and your back to just selecting a region with additional steps. reply yoaviram 14 hours agorootparentAdding a bulk send option is easy. The problem is that you will then get 700 reply emails, each slightly different at which point you will be stuck. That said, we are working on automating it. reply jagged-chisel 16 hours agoparentprev“doggy” companies? Do you mean “dodgy”? reply PrimeMcFly 16 hours agorootparentSurely it's pretty obvious they do. reply karlzt 9 hours agoparentprev>> PSA Public service announcement. reply 22c 11 hours agoparentprevThank you for the great, free resource. Do you have any advice for people who are not in a supported jurisdiction? eg. Have you heard of anyone having success for using GDPR as an excuse to be removed despite not living in the EU? reply yoaviram 26 minutes agorootparentYes, 90% of companies do not check where you are from and will comply with your request, however data brokers and other companies who's business model depends on personal data usually do check just to add more friction to the opt-out process. Still, I recommend trying. reply comprev 16 hours agoparentprev\"Doggy\" or \"Dodgy\" ? What's a doggy company? reply blep_ 15 hours agorootparentDatadog, of course. reply yoaviram 14 hours agorootparentprevThanks for pointing out this embarrassing mistake, this wasn't published yet, but they always say one should share work in progress... reply _huayra_ 17 hours agoprevhttps://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-Li... I always go back to this list every year or so to look through the major ones. At least for my relatively unpublic life, I have never gotten readded after the initial time I went through to delete everything. YMMV if you are more prolific with your public persona than I am, but like other comments have said, don't trust those 3rd-party services to do this for you because many use Mechanical Turk type of labor with your personal info to basically walk through this list themselves (i.e. people that might keep your PII for nefarious identity theft purposes). Edit: one thing I have that helps a lot that is unique is that my name is a slight misspelling of a famous athlete in a sport that is not at all popular where I live. When people search \"_huayra_\", they usually get results for \"_huayrack_ the legendary athlete in some far-flung non-mainstream sport\", in a sense. reply yoaviram 17 hours agoparentThis list contains a small fraction of all data brokers. Check out https://databrokerswatch.org for a more complete list. reply canadiantim 16 hours agorootparentHow in the world is someone supposed to opt-out of all of those? reply yeswecatan 19 hours agoprevI've been hesitant to submit removal requests due to requirement of uploading a picture of your ID. How can I trust these shady companies won't use this irresponsibly? reply cornflake23 18 hours agoparentSame here. I know that a fair bit of the data they have on me is inaccurate. Yet, to delete that, along with accurate data, I’m being asked to enrich their data with even more accurate data. It feels like the old “click here to unsubscribe” scam that actually just confirms a real person behind an email. I would love to know who sold them my data though. That would allow me to stop the flow more effectively before I felt okay deleting at the terminal data broker. reply sircastor 17 hours agorootparentI've started to give services domain-specific email addresses as a sort of reverse-tracking identifier. So I give google@mydomain.com and apple@mydomain.com and so on. I figure I'm using a password manager for all of my passwords anyway. It obviously won't work in all situations, but it might provide some leads. reply alexwasserman 5 hours agorootparentI also bought myself a new domain for this. Previously I was using service@service.myname.com, and I realized that's leaking a bit too much info. So, I bought genericname.com and switched to using service@service.genericname.com Slightly less leakage, although I really doubt anyone is looking. I still have occasional issues with companies rejecting emails with their name in them, but that's easy to work around. It's great in stores when they ask you to sign up for something and you give them an email that's obviously their name. Raises some eyebrows but most people working the checkout really don't care. A few just comment that it's cool, most are skeptical. It's all hosted on fastmail and routes via wildcards to my central inbox anyway. reply EvanAnderson 16 hours agorootparentprevI've done the same thing for years (yikes-- 20 as of last year-- I'm old!), albeit sometimes I use an opaque identifier for the username portion (because some sites treat addresses with their own domain name in then funnily and I've had humans question it). As a bonus I've identified and reported two previously unknown data breaches by reporting the date I started receiving spam to a one-off address. reply freeAgent 17 hours agorootparentprevUnless mydomain[.]com is used by more people than just you and maybe your family, doesn’t the domain itself serve as a unique(ish) identifier? I think public aliasing services offer better anonymity, but they’re also blocked by some services. reply blep_ 15 hours agorootparentFor humans who are paying attention, sure. In practice, not really, because it's all done by scripts without an easy way to query \"is this domain shared\". reply jackfoxy 17 hours agorootparentprevThe way to determine who sold them the data is a service and agent I've envisioned for a long time, but never had the wherewithal to produce. (I won't go into all the hurdles.) Everyone should have their own email domain, and an agent that also serves as your email client will generate a proper looking (for some definition of that) email address within your domain for every new correspondent. Now, whenever you see your identity (email address) associated with anything at all you can determine the original source. reply WhackyIdeas 18 hours agorootparentprevMaybe the data is sold from some of the apps on our smartphones. Also, pretty sure most of the payment providers folk e-shops use on their checkouts sell the data to Google (and I am dead certain Google were bragging about knowing about almost any transaction which happens on the on the web). That is a part in the chain which not even most online shops would even be aware of. reply cornflake23 14 hours agorootparentYup, I often speculate that for me, perhaps for many others, apps (and the Android/iOS platforms) are the source. I’ve been slowly switching to web/desktop based alternatives- those too have their issues (eg correlating all the traffic out of my single home NAT’d IP address. Mulling deleting apps off my phone as well, but many non-app “mobile” experiences are completely unusable. reply electric_mayhem 17 hours agorootparentprevI use a different email address for pretty much every company I’ve ever dealt with. There’s commercial services now which make it easy, but I’ve been doing it for decades now. The joys of running your own inbound mail server. Makes it easy to know who’s either sold your data or had their db liberated, and block them. reply accrual 18 hours agoparentprevI had the same concern. I almost went through with DeleteMe, but it felt paradoxical to give all my info to one company so they could remove it from others. I understand they need it to do the work but it didn't feel right. They requested photo ID, SSN, all past addresses, online handles, family member information, etc. It was just too much. reply lynndotpy 18 hours agoparentprevOf the three listed, it looks like Albine is just DeleteMe (they have the same ToS link.) Neither of them have a forced arbitration clause, class action waiver, etc. which is refreshing. These waivers are regularly upheld and make it very difficult to sue companies who do something wrong. Having no forced arbitration clause is a good thing! It doesn't make me trust them more per-se, but it means I don't need to trust them as much in the first place. reply ds 17 hours agoparentprevYou can trust them only as much as you think they have self interest in not being sued for doing something nefarious. That said, they could very easily have a data breach and every customers full info would then be out in the wild. Were not talking about ordinary payment details either, just full on dox - every address you have lived at, your license scan, all emails, phone numbers, its crazy. Id be willing to bet all these services are targeted quite alot as well because the people who would be willing to pay for this stuff are likely the ones with the most to lose. I made a post lower in this thread but in general this entire model is flawed. Deletions should happen directly between your device and the service in question. Also, its just as important to wipe the data YOU create as the data other people create about you. Just like databrokers, you can either do it manually or automated. Check out https://redact.dev if you want to automate that part at least (I'm on the team) reply steelframe 17 hours agorootparent> they could very easily have a data breach and every customers full info would then be out in the wild Based on the many notifications I've received from hospitals and insurance providers telling me they've allowed my private information to get repeatedly pilfered, at this point I operate under the assumption that if any organization collects information about me, it's going to leak within the next 5-ish years. The first and most effective line of defense is to not let the data brokers collect your information in the first place. reply sillystuff 16 hours agoprevThere are 683 data brokers that either completed registration with the California Attorney General's office, or had incomplete registrations as of 2023 [1]. None of the removal services come close to covering all of them. If you live in California, on 1 August, 2026, data brokers will be required to check a list at the California Privacy Protection Agency, and if you added your name to the list (you cannot yet), the data broker must a) not sell your data, and b) if you selected this option, also must delete your data. The brokers must check the list no less than every 45 days [2]. [1] https://oag.ca.gov/data-brokers [2] https://cppa.ca.gov/data_brokers/ (Also, LexisNexis should be on the list of top 10 data brokers. They likely have several tens of pages of data on every US adult, and perhaps hundreds of pages, if you drive a late model car that collects \"telemetry\" as you drive) reply vidyava 17 hours agoprevWhat I wish I had, and maybe someone here knows of something that fulfills the role, is a means of providing erroneous information about myself to data brokers. I'd like to insert some fake addresses, wrong phone numbers, made up familial relationships, etc and let that propagate, rather than go through all the hoops to try (largely in vain) to have the information removed. reply LastNevadan 18 hours agoprevI have been using Optery (YC W22) and are happy with them. It's more money than I wanted to spend on this. But they have cleared my name out of more than a hundred sites. The article linked here refers to ten data brokers. But there are far more than that that are handling and selling your data. There's no way you can delete your information from all of them without subscribing to a service to do it for you. reply ProllyInfamous 17 hours agoparent>There's no way you can delete your information from all of them without subscribing to a service to do it for you. There is no way you can delete your information from all of them [period]. My personal recommendations to lessen data-associations: 1) Actually use cash 2) Shop at places which don't require membership (e.g. for discounts) 3) Buy a domain name which allows you to `catch-all@your.domain` and then give each requestor a unique-to-them \"email address\" e.g. WalMart @ JoeSmith2222333.com 4) Don't carry your cell phone with you everywhere; Don't sleep with your phone 5) Remove/unplug/disable voice assistants 6) Run LLMs/ChatGPT on local instances 7) Have your DHCP auto-issue an IP to your own local DNS server (e.g: PiHole) reply steelframe 17 hours agorootparent4) Don't carry your cell phone with you everywhere; Don't sleep with your phone There are benefits to doing this that are beyond just privacy. reply ProllyInfamous 14 hours agorootparentI have carried my cell phone outside of my house a total of three times the past two years (mostly for parental death, out of town). When people ask \"how do people reach you\" my typical response is \"a select few people know where I live or have my landline [full-stop].\" Being unreachable is a lifestyle, not a part of most's journey. reply accrual 17 hours agorootparentprevuBlock + NoScript would also be good additions for desktop browsers. I'm impressed I can browse most of the web fine without scripts, or at most, with scripts hosted from the same domain. May also try switching to Fastmail this year. reply ProllyInfamous 14 hours agorootparentuBlock is definitely an easier implementation for most users. But running Pi-Hole [1] on a RaspberryPi3b [2] is \"training wheels EZ,\" effectively a single SD format with minimal initial configuration (assign IP, credentials), that can then run entirely headless (until the SD card fails... use the best memory you can afford). Perks of local DNS-resolver include it working across all devices accessing the local network, including outside of the browser. It is ASTONISHING how many connections modern OS'es attempt, by the millesecond. [1] Pi-hole.net [2] raspberrypi.com/products/raspberry-pi-3-model-b-plus/ [3] [3] ...or better. Runs just-fine on this one! reply Tempest1981 18 hours agoparentprevWhich data does Optery require, and what makes them better? Ex: photo ID, SSN, all past addresses, online handles, family member details Do they keep these forever? So many companies eventually get hacked. reply rovr138 17 hours agorootparentJust trying it out. Required seems to be, First name, last name, country, state, city. You can add more info if needed. Don't see anything about ID, SSN. You can add other addresses. You can add extra family members. These aren't required but I'm sure it would help from what I've seen on other websites. reply Tempest1981 13 hours agorootparentThanks, sounds better. Thoughts on the monthly payment approach? I.e. could you sign up for just 1 month each year or so? reply rovr138 17 hours agorootparentprevAdding, You can upload an ID for the sites that require it. They suggest what data to mask out. You can give them a limited power of attorney (revocable with just a click) to act on your behalf. reply jacobkg 16 hours agoparentprevDoes this work for commercial data brokers? E.g will it stop the steady stream of phone calls from recruiters? reply mihaaly 18 hours agoprevI feel a bit reluctant giving them much of my data just to match me with their potentially non-existent records and so allowing them initiating new records on me covertly. Also no way checking if they lie about the dataset on me. I feel better not sharing data in general, anywhere, except when it is really essential. So many businesses lost me on potential or factual trade because before(!) answering my questions or giving very basic information (e.g. price!) on their services they wanted to collect lots of factual data on me. I said no, good bye! reply bertil 18 hours agoprevIncogni has been sponsoring a lot of the YouTubers I watch regularly, so I’ve had to ask myself: is that worth it? There’s, of course, the risk of sharing your personal details with a company that gave money to people to build para-social relationships… not the most compelling case. But there are also questions about jurisdiction: can they get companies based “abroad” to do something? That article from the Cyber Collective seems to focus on US legislation, and the worst abuse I’ve heard about is happening in the US, but I don’t remember finding my details when I asked some of the most egregious actors in that space—for obvious reasons: I don’t live in the US. I wish that actors like Incogni or DeleteMe shared statistics about how many actors capture your details based on where you live, whether you have a mortgage, a loan on your car, a credit card, social media presence, etc.—or anything else one might do that would help those company flag your details. reply pnw 18 hours agoprevI’ve been using Optery.com (YC22) since they launched, and they’ve done a good job. https://news.ycombinator.com/item?id=30605010 reply chaostheory 18 hours agoparentYeah, I can vouch for them too. Their premium service costs about as much as a Netflix sub which is good value. Pretty sure their lowest tier is decent too. It’s a time saver reply gardenhome 13 hours agorootparentI’ve used both Optery and deleteme. Each one is effective. Optery has been able to handle custom requests that DeleteMe claimed it could not do. Optery costs more though. I was able to clean up my online presence completely after two months. From now on I will try subscribing to Optery every few months instead of a continuous subscription. Anything is better than nothing though. Googling my cell used to yield my full name, pass addresses, etc. reply neatze 17 hours agoprevI tried Optery, Incogni, and long time ago OneRep, way to lazy to do it myself, don't worry they will have my info, data is already on internet. Incogni at least in there's claim offers opt-out from private databases (no way to verify ) and some but not all public database (eg. google searches). Optery has largest list of public databases (with most expensive subscription) out of everyone else, there's costumer service is responsive regarding failed removal. OneRep was not bad long time ago when they run it from Belarus (I know, crazy), but they would refresh somehow search caches too (it could be ok, or make things worse), they don't seem to offer advertise this service any more. Don't search your self only via google, for example, bing will give different results, some databases will have misspelled names (could be deliberately), so there still some work to make sure all records are removed. At this point, this is like privacy tax that you have to budget to have at least your address on cell phone number not easily discoverable. reply jkljsfdasdf 19 hours agoprevWhy do the deletion services at the end charge monthly/annual fees? Don't you just need to submit deletion requests once? reply dspillett 18 hours agoparentYou'll get readded to brokers' DBs. “How can you not, if they aren't tracking you they don't know that new entry is one they shouldn't be tracking!” being one excuse. Also, new brokers turn up regularly, or often the same broker under sufficiently colours that they can weasel their way to not being recognised instantly. Your removal service requested removal of your info from AnonData Inc, but not DataAnon Ltd. No, despite employing exactly the same directors & managers, and being officially registered from the same collection of PO boxes, they aren't the same company, honest… Shady shit doesn't just stop because you ask once. reply dspillett 8 hours agorootparent(“under sufficiently colours” should have been “under sufficiently different colours” - not sure how I managed to completely a word there!) reply figassis 19 hours agoparentprevMaybe they set a do not store flag for your info, because your data keeps coming in from their partners, even after you requested deletion. So to keep filtering for you and deleting, you need to pay their protection fee. reply SlightlyLeftPad 18 hours agorootparentHmm, the Mafia must have found a place for itself online, a hundred years after the prohibition. reply WirelessGigabit 16 hours agoprevI'm about to buy a house and I find it insane that I cannot prevent the County Deed Recorder from selling my data to services like Spokeo. I could set up an LLC and buy the house in that name but that requires cash, and I don't have that much cash. I could buy an aged LLC, move in cash and then get mortgage as the LLC but that is really extreme (and not sure if that's legal?). To add to that, services like PermissionSlip assume that you have 1 email address / finite set of email addresses where they send out requests to get your data deleted. That doesn't work for people who use @.. reply fulladder 15 hours agoparentSetting up an LLC to own your home costs a few hundred bucks. You don't need any cash beyond filing fees and legal fees (if you use a lawyer). The bank will not care. Their loan agreement gives them the right to foreclose if the loan is not being repaid. The exact legal owner of the house isn't relevant. There is one issue to be aware of. If you own your home as an individual, you get a federal capital gains tax break when you sell (the first $250,000 of gains is exempted from tax). If an LLC owns the home, there is no tax break and all of your gains are taxable. This is the real reason why only wealthy people put their homes into LLCs. If you have a $40 million house, this tax break is not particularly meaningful. If you have a $400,000 house, it is very meaningful. (Consult with your accountant; this is not tax advice.) reply sneak 16 hours agoparentprevThe LLC can get a mortgage with you as the personal guarantor. The house is the security anyway. Then your name is only on the address in the bank’s records, provided you appoint a different manager of the LLC (the corporate records for managers, but not beneficial owners, is public). reply Tempest1981 18 hours agoprevAny tips or tools for doing this manually, for the top 10 brokers? That's what I was expecting from the headline. (Did I miss it?) reply pmx 17 hours agoparentI thought the rest of the article had failed to load! This just feels like a thinly veiled affiliate marketing campaign. reply dublinben 17 hours agoparentprevThis should have all that and more: https://github.com/yaelwrites/Big-Ass-Data-Broker-Opt-Out-Li... reply yoaviram 17 hours agoparentprevhttps://databrokerswatch.org/ reply pyaamb 14 hours agoprevOnce data is collected, there is absolutely no certainty that requests to delete data will be honoured. IMO I would rather contribute towards efforts that discourage requirement of sharing personal data in the first place, especially when the organizations who request that data are off the hook when that data inevitably leaks out into the internet. reply hackan 18 hours agoprevSeveral of those brokers are also tied to the \"credit score\" (no, not in China...), so removing yourself has some financial consequences for you. they won't make it easy... reply rcMgD2BwE72F 17 hours agoprevWhy play by their rules when the purpose of the data collection and processing was always against us? It should be a fight (legal or technical), not fake collaboration. reply Simplicitas 18 hours agoprevIs Permission Slip mobile app, by Consumer Reports another? reply qudat 17 hours agoprevI highly recommend https://easyoptouts.com/ reply silisili 16 hours agoparentSame. Works well, way cheaper, and the dev hangs out either here or on Reddit I forget, and seems really down to earth. reply trimethylpurine 15 hours agoparentprevSeveral sites listed in the article are not supported. reply steelframe 17 hours agoprevTFA mentions CCPA and GDPR as the \"teeth\" that make it possible to request personal information removal from these bottom-feeders. The problem is that I don't live in Europe or California. So for me there's no legal requirement for these jackwads to delete my private information from their systems. I would bet a non-significant number of Krugerrands that when many of them receive these requests they basically just set a bit in your record to indicate that you've requested \"removal.\" The effect of that is that any reports they generate or other outside evidence that they're collecting your information largely go away. But without deep inspection by outside auditors, you really have no idea whether they're continuing to collect your information in a \"shadow\" profile of sorts. They're certainly incentivized to do just that if they can get away with it. At the very least, I doubt most of them have gone through the trouble of building filters on all their data ingress paths to drop telemetry on the floor if it relates to someone who's \"opted-out.\" In other words, I absolutely don't trust them to do the right thing, and I also don't trust the government's enforcement to be consistent and universally effective. So the next-best thing I can do is deny them the telemetry in the first place. I do that by running privacy-oriented software on my computing devices including mobile, paying for everything I can in cash, filing my taxes with minimal involvement from any third parties, and using modes of transportation that don't report my location to anyone. reply Name_Chawps 17 hours agoprevA beautiful song about the data broker Acxiom: https://www.youtube.com/watch?v=4H_HOw4sBeM \"Someone is gathering every crumb you drop...\" reply steelframe 16 hours agoparentI've been listening to Vienna Teng's music since she released her Waking Hour album independently in 2001. Hymn of Acxiom is one of her more recent works and is one of my favorites. https://www.npr.org/2003/01/12/910087/vienna-tengs-waking-ho... reply legitster 17 hours agoprevBe careful with some of the data removal \"services\". Some of them just spam random services with your name and email address asking to be removed. You can actually leave behind more data than gets removed. reply EvanHahn 18 hours agoprevAnother deletion service I like that the article didn't list:reply oriettaxx 14 hours agoprevWarning: Links contain affiliate links! (links DO contain affiliate links: how the hell the writer write \"MAY contain\" !?) reply nox101 16 hours agoprevis it impossible for the government to regulate this stuff with fines for data breaches or something? Effectively incentives not having the data? I rented a new apartment recently. They wanted so much info including pictures of my driver's license, SSN, etc I'm 100% sure their system is not secure Until it costs companies to have this data they'll keep collecting it reply syntaxing 18 hours agoprevDiscover bank offers this service for free, not sure how comprehensive it is but I googled my name and area and the results were noticeably less reply andrew_ 18 hours agoparentDo you happen to have a link handy? Can't find any information about this being a free service. reply syntaxing 17 hours agorootparenthttps://www.discover.com/security/online-privacy-protection/ reply samber 17 hours agoprevSurprisingly, collecting data is free, but removing PII with dedicated services is paid . reply pilingual 17 hours agoprevIs there a list of sources for these data brokers? reply asylteltine 18 hours agoprevI use deleteme and it works well. I saw a significant different in search results after I signed up. reply NoZebra120vClip 18 hours agoprevRecently I looked myself up in public records, and what I found was quite disturbing, and I'm unable to do anything about it. For years I've been receiving \"misdirected\" marketing mail for a small number of women who definitely don't live here, and never have. I always ponder deeply how their names could get associated with my address. Dunno. Anyway, there is now a person listed with my address, phone number, and surname, with a very common Celtic feminine given name, but she doesn't exist. I am not sure if the scam is supposed to be that I'm secretly married, or living with my sister, or what. So I pulled gently on this thread and a whole sweater emerged. I've not paid for any lookup services, just the free ones. This putative person is related to a large number of people sharing my surname, and I believe that none of them exist. There's another woman who lives within a few miles of me. There are obituaries linking all these people together as a family. WhitePages.COM links them as probably-related. The earliest publication date corresponds very closely to when I first moved into this region, 25 years ago. So I can't say what, if any, scam is connected to this alleged network of names; I can't remove this person from sharing my address, phone, or surname, because she's not me, but anyone who looks up my PII will turn up this nonexistent \"wife\" or \"sister\". Nobody has ever mentioned this to me, I had to go looking for it on my own. reply BobbyTables2 16 hours agoparentI’ve seen a few similar cases. I think some of these brokers have trouble reconciling when they receive information vs when it was relevant or get confused on similar names. It is spooky when it shows deceased relatives now living in other states. reply ds 18 hours agoprevThe big thing to know is the following: Google yourself - See what shows up that shouldnt. Go through those sites and manually opt out. Its not too hard and you get the biggest offenders. Yes, you can use a automated service to do this for you but I think its a really bad idea based on how most of them work. First of all- Most of the big ones make use of extensive labor in thie phillipines and malaysia to manually type your data into opt out forms. They also have to make use of extensive proxy networks that I can only suspect are not always on the up and up. This is because the databrokers will block IP's from submitting more than a few opt outs per date. So you are supporting both shady practices for the proxys and third world labor thats semi exploited Second of all- You are trusting yet another company with your data. When you sign up to one of these services, they obviously know everything about you. When one of these services inevitably has a data breach in the future, its going to be a disaster. The reason that the big services have difficult with the biggest services that are listed in this article is because they: Use captchas, use cloudflare and do email confirmations. They also do things where they show you multiple pieces of data and you have to pick which one is yours, but some of the data is blurred and presented as a image. ( ie- Is this email yours? tee***@gmail.com ) So, what to do if you want to stay on this? Well- Im creating a solution with my team to do what all the big players should have done- Do the optouts from your own device. They of course want you to do it from their servers because its a nice zero friction experience where you just type your info in and they handle most stuff. But as we see, the biggest offenders for databrokers are NOT handled because they are tricky. So, at our startup https://redact.dev we already built out a ton of tech for this, but targeting social media and messengers. We are now building all of that out for data brokers. And because its ran directly on your own machine there are a TON of advantages: 1: No limit to how often you can scan for new databroker leaks. Most of the players now limit you to once every 30-45 days 2: No limit to adding friends/family to scan/opt them out also. 3: No use of third partys to process your removals. This one is self explanatory. The deletions happen direclty between your device and the databroker- no third world workers involved. 4: Handles the 'hard to delete from' sites listed in the article. Our built in IMAP system can handle email confirmations no problem. Because its running from your own IP, you have no issues with getting blocked there either. Cloudflare/captchas are also no issue 5: Most importantly, you dont have to trust another company with your most personal information. All your data like address/names/phone stay on your device and are only sent to the sites you need to opt out from. Believe it or not, a bunch of these databroker remover sites will just bulk email a bunch of databrokers right now saying \"hey, if joe smith @ 123 main street is in your DB, remove him!\" The big drawback is that you need to have a device open that can do the work for you. With other services you can just pay and shut off your PC or phone. You need to keep our solution open for 5 minutes while it does its work. I think thats a definitely good tradeoff for the security you get AND the fact that it instantly removes you from many of these sites. Alot of the players as I mentioned just do emails so it could take weeks or months before they remove you. If you use the databroker sites automated forms, it can be removed instantly or within days or hours. reply pc86 18 hours agoparentI mean doing something yourself is often preferable to using a third party especially where privacy is concerned, but this isn't slave labor at slave wages we're talking about here. What job do you think these people would have if they weren't sitting in an office typing PII into computers all day? They wouldn't suddenly be getting six figure remote programming jobs. They'd be doing the same thing for some other company, or they'd be doing something worse. Third world labor is not exploited because it's in the third world and makes less in an absolute sense than other areas. Edit: Ah you edited while I was replying, yeah it makes sense that you're selling something, hence the mischaracterization. reply ds 18 hours agorootparentI am not OK with supporting someone getting paid 2-3 dollars a day for this labor and would prefer to not support that business model if I can. I also dont trust those workers not to misuse or sell my information on the side because of the unfortunate financial situation they are in financially. Its acutally a similar answer for why I dont pirate games or software. I dont want to support the behavior and I dont trust that something bad wont happen as a result (virus/malware/etc.) reply pc86 17 hours agorootparentThe dollar amount they're getting paid is irrelevant, what matters is how that compares to what they'd be making otherwise. If you get paid $0.50/day to work on a farm for 12 hours, getting paid $3/day to type for 10 is a pretty fantastic opportunity. But the median income in Milan is something on the order of $1,200 USD/mo so no your $2-3/day figure is pure fiction. Edit: Manila seems to be the outlier by about 10-15% but I'm making a guess that most of those types of firms would be located in the major cities and not rural areas. reply ds 17 hours agorootparentId get into this debate, but its been done a million times before. Heres the cliffnotes of how it goes though, roughly: * So is minimum wage ok then? * Why cant a minimum wage person invest all their money into a startup? Why do they have to be a accredited investor?!? * Why cant I just open up a payday loan place that charges 900% interest? Theres no other payday loan places around and poor people willingly will use it! * What about prostitution, nobody is forcing them to do it? * Alright then, Should we let poor people sell their organs? Again, nobody is forcing them to do it! Cliffnotes: Just because someone will 'willingly' do something, or doesnt have any better options, doesnt make it just or moral. You setting up a 'consulting' company in bangladesh that pays people x$ a day to do something is exploiting those people, plain and simple. Also, as to your phillipine cost- You are completely wrong. The average wage across the majority of regions in the country is under $10 per day. This is where (see my first link below) most shops get setup, obviously. https://www.outsourceaccelerator.com/articles/average-salary... https://www.statista.com/statistics/1048636/philippines-mont... https://en.wikipedia.org/wiki/List_of_Asian_countries_by_ave... reply rcpt 17 hours agoprevJust the other day I was asking about how to get Blind to delete my data https://news.ycombinator.com/item?id=38956329 I've tried customer support and made a GDPR takedown request. They just said \"no\" and now I'm stuck. reply hooverd 16 hours agoprev [–] There should be a public register of people who chose to work at these companies, for the irony. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Data brokers collect and sell personal information without our knowledge or consent, compromising our digital privacy.",
      "The blog provides steps on how individuals can reclaim control over their data by requesting data removal from data brokers and utilizing consumer protection laws like CCPA and GDPR.",
      "It emphasizes the importance of being aware of online consent and actively managing one's digital footprint to combat data collection by data brokers."
    ],
    "commentSummary": [
      "The discussion centers on concerns about data privacy and the possible sale of personal information by data brokers and apps.",
      "Users recommend different tools and services to remove personal data from databases, with debates surrounding the reliability of opt-out services and the effectiveness of data removal services.",
      "Suggestions are made to use unique and domain-specific email addresses to minimize data leakage, while some discuss the option of setting up an LLC for personal information protection. The conversation emphasizes the significance of safeguarding personal data online and the obstacles involved."
    ],
    "points": 181,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1705242863
  },
  {
    "id": 38992292,
    "title": "Bluefin Project Relaunches as Developer-Focused, Cloud-Native Linux",
    "originLink": "https://www.ypsidanger.com/announcing-project-bluefin/",
    "originBody": "Today we \"relaunched\" ublue-os/bluefin as projectbluefin.io. This is a beta until Spring 2024. Bluefin is a custom image of Fedora Silverblue by a bunch of cloud-native nerds. We want a reliable desktop experience that runs everything but we're too lazy to maintain anything. So we automated the entire delivery pipeline in GitHub. Say hello to Bluefin. She is a Deinonychus antirrhopus, \"terrible claw\". Artwork by Andy Frazer. Originally Bluefin was \"Fedora Silverblue for Ubuntu Expatriates\". I was migrating to Fedora at the time and wanted Silverblue but with a more Ubuntu-like desktop: a dock and appindicators. Over time it became apparent that it could stand on its own as an alternative for people who are tired of Linux desktops that aren't as reliable as cheap Chromebooks. We were tired of this shitty situation so decided to add a bit o' shine to Fedora Silverblue and maintain it as a clean, atomic layer on top of the default image. It's not a distribution, since you can always revert back to a stock image. Justin Garrison gives us a quick tour! So she became Bluefin, which coincidentally was the name of Canonical's office building on the Thames in London. Since it's basically an Ubuntu-style workflow on top of Fedora this seemed to make sense and scratch the easter egg itch. So after about two years of prototyping and real world usage, we think she's ready for more people to try. We're calling this a Beta, with the hope of going GA in the spring. Since she's built on Universal Blue we get all the benefits of the main images. While it can run on any PC, we also have specific images for Framework, Asus, and Microsoft Surface devices. Work is underway to generate images based on the work of the Fedora Asahi SIG, so hopefully soon we'll have something for Mac users. Updates are automatic and transparent, with built in drivers. It can do anything a Chromebook can, but can use flathub packages for applications, and crucially, a choice of browsers. And since a container runtime is included, it also means you can run just about any Linux workload. Wallpapers by Jacob Schnurr, every season represents the ecology of our open source ecosystems Major upgrades are handled at the CI level by our contributors, and everything is set up to run automatically. We ingest Fedora, apply our changes, and then ship the OCI image to you every day. An image-based desktop with the least amount of end-user maintenance as we can automate. Since it's just a container you can fork to your heart's content to make your own adjustments, or start from scratch. We intend to use Cockpit to act as fleet management. It is a powerful tool, however it's not set up out of the box, we hope to work on this over time as it's a complex issue, but it's also a problem to look forward to! Taking Care of Developers With the normal use case taken care of let's look at ourselves. Fedora Silverblue variants come with a container runtime, Podman. This means that we can basically run anything we want. We include distrobox for an interactive experience so that users can use whatever distribution image they want as their day-to-day. But we needed a bit more oomph, so doing a just devmode-on in a terminal will switch you to bluefin-dx, our developer image. In here you'll find Visual Studio Code with devcontainers, devbox with Fleek for nix, devpod, and homebrew. And we toss in some shortcuts to install Jetbrains Toolbox. We also take care of the underlying stuff that you need, like LXD/LXC, KVM, and virt-manager. Incus is available as a tech preview. And lastly we include Docker on the image so you can just docker yourself to a more familiar place if you prefer to use it. VSCode comes ready with devcontainers out of the box You can swap your userspace as you see fit. By default it's Bluefin/Ubuntu, but I've been enjoying Bluefin/Alpine for it's speed and small size. At some point I'll migrate to using Wolfi images too. Use whatever you want, or stick to the stock Fedora images. We purposely don't dictate a developer workflow, whatever works for you. Homebrew is my preferred route and is what I recommend to people who are migrating to Bluefin DX. We use the just task runner to ship a bunch of convenience features (and workarounds too). This has started to become a great method for our community to ship shortcuts to each other. For example one contributor submitted just ml-box - a one command shortcut to get pytorch up and running. When you look at what it can do vs. setting up Nvidia drivers and container support by hand it becomes clear that enabling contributors to \"ship\" directly to developers is a powerful pattern. Sustainability of our Open Source Contributors And lastly, we wanted to make something lightweight from a contribution perspective. Universal Blue is Containerfiles with Python/Shell. It's a common toolset that tons of people know, and with our community's cloud-native knowledge we were able to fully automate much of the toil in maintaining your computer. I have not seen an \"upgrade screen\" on any of my computers for over two years, Bluefin quietly hums in CI/CD on GitHub. You don't even know she's there. (Probably because we are being hunted). We take our governance and procedures seriously, and welcome contributions. I try to take the same amount of care and dedication that I do when contributing to CNCF projects. After spending over five years in cloud-native, I feel like we finally have an alternative operating system using the methods we're all familiar with. It just wasn't available on the desktop, until Fedora made it happen! Now it's up to us to consume it, and improve it! Here's the companion video to this blog post. A fresh start for the next generation I hope you'll join us. We've already got lots of tech in here including kind, flux, helm, and kubectl. We intend to accelerate the consumption of cloud-native tech by acting as your SREs. I'll be traveling to conferences over the next 18 months talking about sustainability of our open source ecosystems and developers. Those talks will be delivered via Bluefin on a Framework 13. After using Linux since 1998 I believe this to be the state of the art of the Linux desktop. You won't find a more powerful system with the least amount of work - so if you see me ask for the demo. We've been dogfooding Bluefin in some form or another for about two years, and Universal Blue's images have been pulled over 2 million times, it feels mature enough to move the Beta. I don't think bluefin-dx will ever be finished because we'll be revving at the same pace as the rest of cloud-native development, so that should be fun! She might nip at you on occasion with a paper cut, but she evolves quickly. Clever girl. Find me at KubeCon North America for your very own bundle of joy! Full Disclosure I started this project when I was on sabbatical. One of those \"find yourself\" ones where you're thinking about stuff like ... \"Where does my career go from here?\" The whole open source sustainability thing is bugging me, and I started working at the CNCF to help fix this problem. And my passion projected kinda ended up being a \"hello world\". What if we could use the Linux desktop to onboard people right into cloud native? There's way dumber ideas out there lol. Here's more color on how I'm thinking about sustainability in open source: Here's me trying to explain all this in 15 minutes! It also explains why Bluefin is a dinosaur, she's trying to find her place in the ecosystem. Finishing the Beta There's a few things that need to be sorted over the next six months. Mostly it involves the installation experience. It's hard to integrate code that doesn't exist so we're still waiting for Fedora's installer to become more robust at handling OCI installation. I'd like to personally thank the developers at Fedora and Red Hat for their support in working on these features, we're pretty confident that the installation process will get much better in the next six months. Luckily installation is the roughest part, once you're past that, it should be smooth sailing! There are also lots of little paper cuts here and there. For example we don't automatically \"detect and install Nvidia drivers\" since that's not how these systems work, so you have to manually rebase to that image. That sort of thing. Reliability is pretty great, the GitHub Package Registry keeps 90 days of images available so we can always boot off of any of those images if required, and due to the nature of the system it removes a bunch of the packaging complexity from the client entirely. Appendix: Their Stories Jacob Schnurr and Andy Frazer brought Bluefin to life. If there's one lesson I've learned from this project is the technology is only one part of the equation. I purposely chose the dichotomy of paleo artists and operating systems at the leading edge of machine learning to be a statement on where we stand. A tool designed to make automation the most efficient it can possibly be, and yet unable to exist without the imagination of the human. Here's some more shots of the gang in all the prehistoric glory ... Deinonychus antirrhopus, \"Bluefin\" - This was the species that changed our view of dinosaurs Dolly the Dimetrodon. They are actually a synapsid, not a dinosaur. You have more in common with Dolly than Dolly does to any of these dinosaurs. An incredible notion once you think about it! Karl, represents the Developer Experience. When I'm in my Linux terminal I want to feel powerful and strong. With his help I shred through my backlog – issues, eliminated. Builds – green. Tests – passing. Karl also represents the raw power of Kubernetes, which is why we ship so many cloud-native tools by default. Bluefin is a mother and takes care of her family, like we must take care of each other. Things that threaten her chicks end up meeting the other side of her personality ... We call this one \"murder chicken\". Inspired by the raptor chase in S1E4 of Prehistoric Planet And just like real life, she comes in all sorts of varieties. This one is \"Murder Crow\" The team overwhelmingly fell in love with \"Glitter Chicken\". I'll be making a special Pride run of these at some point! .. and lastly this Kentrasaurus - you don't want to mess with them. Share I accidentally started mainlining Open Source software Sometimes people ask me how I keep track of all the cool… 11 Nov 2023 Homebrew is great on Linux I'm a strong believer in the next generation Linux desktop model -… 14 Oct 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38992292",
    "commentBody": "Project Bluefin: an immutable, developer-focused, Cloud-native Linux (ypsidanger.com)178 points by Santosh83 16 hours agohidepastfavorite66 comments tgv 2 hours agoA minor word of caution: don't use distros with unique(ish) features in production if you can't replace those features (quickly). It's of course a cost/effectiveness consideration, but I've been bitten by that when I inherited a project that ran on an immutable distribution that was withdrawn. There was a fork, but there was no upgrade path and the hosting company didn't offer it as an install on new servers, so now the thing runs on an LTS Ubuntu. When development on that stops, switching to another Linux should be easier. reply sph 1 hour agoparentbluefin is a light layer of convenience on top of Fedora Silverblue. Due to an unrelated issue, switching back from bluefin to the main Fedora tree was just an `rpm-ostree rebase` and a reboot away. reply szundi 47 minutes agorootparentThe value is some work done that you have to reproduce when it is no longer available - this is the point reply ta8645 12 hours agoprevIt would be so nice to get a brief description of what this is, and why it is useful, without a bunch of project names and buzzwords. Can it be boiled down to a few simple concepts that are delivered by this project? reply vaylian 12 hours agoparentIt is a Linux distribution that is based on Fedora Silverblue (another Linux distribution). What makes these distributions special is that they have an immutable/readonly root file system, which means that only /var (which contains /var/home) and /etc can be manipulated directly by root. By having less moving parts, it is much easier to provide a Linux operating system that works (more) reliably on many different machines. reply ta8645 12 hours agorootparentThank you. Why isn't Fedora Silverblue sufficient by itself? What does this project add? And if you don't mind me also asking, what features are described by the term \"cloud-native\"? reply jcastro 11 hours agorootparent> What does this project add? This project uses this upcoming feature in Fedora: https://fedoraproject.org/wiki/Changes/OstreeNativeContainer... and switches to its consumption model entirely. On stock Fedora you're pulling from a distribution-hosted ostree endpoint to do updates. With Bluefin (and other universal-blue.org images) you're pulling from a container registry (in this case ghcr.io, but you can push your builds to any registry or host your own). We ingest Fedora daily and then add codecs, a bunch of hardware enablement support via udev rules, add a few pain-in-the-ass-otherwise things like the obs virtual cam, xbox controller support, etc. Then that image is pushed to ghcr.io and the local package manager uses that. We also enable Flathub out of the box and Distrobox: https://github.com/89luca89/distrobox - then ship a few preconfigured boxes for you to play with: Ubuntu, Fedora, and Wolfi. Then we have another image that you can rebase to by typing `just devmode` in a terminal which adds vscode with devcontainers, devpod, devbox, kvm/qemu, incus/lxc/lxd, some nice monospace fonts, cockpit, kind, docker, and a bunch of associated cluster tools. And since we build everything in CI there's no local package conflicts like in upstream Fedora when the main repo and rpmfusion repos are conflicting, your PC only ever gets successfully built images. reply vaylian 12 hours agorootparentprev> Why isn't Fedora Silverblue sufficient by itself? What does this project add? I'm not affiliated with either distribution. Both distributions provide different out-of-the-box experiences. Fedora Silverblue tries to be a general purpose desktop OS while BlueFin has a focus on software development. You can still adapt both of them for the same purposes, but BlueFin might be a nicer starting point if you actually are a developer. The biggest difference to classic read-write Linux distributions is that you have an immutable base system which includes software that you might find useful or useless. In addition, you can install software as Flatpaks into your home directory. It is also possible to change the software that is part of the immutable base system, but it is typically something you want to avoid. > what features are described by the term \"cloud-native\"? I suppose it refers to the container management software that is included in the immutable base system. reply ladyanita22 10 hours agorootparentprevBluefin try to sort of mimick Ubuntu, so Ubuntu people may feel more at home with a Fedora distro. reply bongobingo1 4 hours agorootparentThat seems extremely reductive... reply rkagerer 12 hours agorootparentprevHow about the \"cloud native\" part? reply jcastro 11 hours agorootparentThe cloud-native part is that it's built with tools and things you'd see in cloud-native like OCI containers, gitops, etc. Here's the containerfile as an example of how it's put together: https://github.com/ublue-os/bluefin/blob/main/Containerfile And then all the developer patterns are cloud native like devcontainers instead of using the host packages, included k8s, podman, and docker tooling, etc. reply lazypower 11 hours agorootparentprevSo \"Cloud Native\" speaks to multiple aspects of how universal-blue is both built, distributed, and some of the guiding principals behind the project. I'll start at the very basics, where we define \"Cloud Native\": Cloud native is the software approach of building, deploying, and managing modern applications in cloud computing environments. I'll get a little hand wavy here as our desktops/laptops aren't typically defined as a \"cloud\" (read: grouping of machines, typically behind an API that someone else manages but makes available to users or customers). However - we can look at it as a target platform for deployment. How universal-blue gets there is the real interesting part. That \"made by cloud native nerds\" is a very compact way of describing how the project is built, tested, and rolled out. Universal-blue images are all built in CI. In this case - its a combination of base layer components - sometimes built in COPR for some projects that are included in the product operating system, and then those COPR built artifacts are included by a Containerfile. Along with all the goodness contained in SilverBlue (fedora rpm artifacts). That containerfile is built, tested, and signed in GitHub actions, and a manifest is then updated (somewhere, i don't actually know where ublue looks for these manifests to identify it has an update - it might just be the GHCR registry - but don't hold me to that). Now this probably all sounds like something you see in your day to day if you work in infrastructure, or in a company producing software/services for the web. But what's really unique from a consumed operating system perspective is that those builds and tests effectively gatekeep the \"blessed\" configuration for universal-blue. Classically you have kernel modules that get built on your machine using a technique known as DKMS (Dynamic Kernel Module System). Every update to the kernel you have to re-build some library as part of the update process. And if your particular version of a module hasn't been vetted with what kernel you just pulled you can be left in a rather bad state - I've had this happen to me with the proprietary nvidia drivers as an example. How ublue delivers these modules is part of that not-so-secret sauce that makes it wonderful. These modules are built in the cloud in that same release pipeline, and if they fail - they don't get released! You simply don't see an update for that day, and things hum along just fine. This breaking happening somewhere other than your computer is part of that reliability promise - you wont be dealing with kernel module breakage, the release maintainers will have to resolve the break (or communicate with upstream to find a solution) so your incoming stream of updates can be unblocked. Finally - there are a lot of \"patterns\" - or process to achieve a desired outcome, that has been piloted in the Cloud Native world. Someone mentioned CloudNative makes them think of CoreOS. I'm glad you brought this up - If you keep your older versions (by pinning, or ublue keeps the last known booted image by default, and you can change this to keep say 10 if you wanted) - you can always roll back to the version that worked before you encountered a fault. This same pattern exists in the base-line SilverBlue distribution. This is not an exhaustive analysis but I've already penned a good novel here. I hope this helps outline how universal-blue brings Cloud Native to the desktop. I encourage you to kick the tires and try it out, even if only in a virtual machine. reply ametrau 10 hours agorootparentIt’s better to just write something yourself than use a gpt—-even if you’re not confident in your ability to write. reply SadCordDrone 5 hours agorootparentThis doesn't look like GPT. I have never seen GPT say \"probably ghcr, but don't hold me to that\". reply nilsherzig 10 hours agorootparentprevOh i have a rather hard time to notice AI comments if the language they are written in isn't my native one. Could you tell me what's most suspicious about the text? Imo the structure is a bit to well rounded and it kind of reads like a transcript of something someone said not like a comment. Doesn't look like gpt4 to me, someone should make a \"guess the LLM\" game. reply osmano807 7 hours agorootparentprevSo similar to NixOS with Impermanence? Does it have home read-only too? reply 1over137 11 hours agoparentprevIt's could native! What more do you need to know?! :) reply stupendousyappi 9 hours agoprevWow, I wish I'd discovered this about 12 hours ago. I just built a new PC yesterday and started installing and configuring Silverblue today, and basically everything I spent this afternoon on is automated by this project. Getting VSCode devcontainers working with Silverblue + Distrobox in particular would have saved me some time. Since I'm not particularly invested in my Silverblue setup yet, I'm installing this now. reply sph 1 hour agoparentTip from one that has been running Silverblue for years: VSCode or any other editor from flatpak is a noob trap. Yeah, I know now it's much more usable because it now integrates my host-spawn (https://github.com/1player/host-spawn) tool, but the easiest setup is to have a toolbox/distrobox container for work and dev, where you install all your tools. I have been using an Arch Linux container (that starts at boot) with emacs, nvim, the myriad of LSP tools that are only found in AUR, exported with `distrobox-export` so I can start them from my dock. Flatpak is for everything else (even Steam), but dev tools, editors and any other package should be installed inside a regular pet container. reply johnny22 8 hours agoparentprevyou can rebase between silverblue and that without full reinstalls iirc. reply mkl 10 hours agoprevI went looking for KDE, but was disappointed. From the FAQ on https://projectbluefin.io/: > What if I want something like KDE or another window manager? > Bluefin is an opinionated GNOME experience. However Universal Blue provides a maintained set of base images for anyone to be able to make a custom image. We hope Bluefin acts as an inspiration for others to build their own communities around user experiences. For example check out Bazzite if you want a great KDE gaming experience, similar to SteamOS. The Bazzite link 404s, but there is info at https://universal-blue.org/blog/2023/11/08/bazzite-20/ and https://github.com/ublue-os/bazzite. Seems mostly focused on SteamDeck. reply pattonis 8 hours agoparentBluefin is one of the many Universal Blue images. Under the full image list https://universal-blue.org/images/, there are images for KDE (under the name Kinoite), Cinnamon, Mate, Budgie, Deepin, LXQt, and others. reply mkl 7 hours agorootparentThanks! More on Kinoite here: https://fedoraproject.org/kinoite/ reply dogphilosopher 9 hours agoparentprevThe base Bazzite image is actually meant for desktops. Bazzite-deck is a build for the steam deck, as well as HTPCs. reply chuckadams 15 hours agoprevWhat does \"cloud-native\" mean in this context, if anything? When I hear the term, I think of things like CoreOS, whereas this looks rather desktop-focused. reply CharlesW 15 hours agoparentIt's worth noting that the submitter didn't use the actual title, and TFA doesn't characterize Bluefin as a \"cloud-native Linux\", but a distribution made by \"cloud-native nerds\" that integrates tools generally associated with cloud development (kind, flux, helm, kubectl). reply ARussell 15 hours agoparentprevI read this article and ended up with the exact same question. Does it mean that it wants to imitate Chromebooks, where the most important app you use is the browser, and all of your apps are in the cloud rather than on the desktop? reply PlutoIsAPlanet 15 hours agorootparentNo, the OS itself is a read-only container image booted on bare metal (via OSTree) and includes the tools usually used to build container images (used for kubernetes etc), as well as pushing desktop applications via Flatpak. reply mcmcmc 15 hours agorootparentprevCloud-native just means container-first in this context. Nothing is locally installed and the desktop gui itself runs as a container. reply riedel 15 hours agorootparentprevI guess it makes you feel better as a user to be part of the hyper converged buzzword bingo. But seriously it seems to be both web and container (web) focussed. They should maybe rather say what they are not and what the limitations of their approach is ( I guess isolation comes at some costs) reply pkulak 15 hours agoprevThis is the announcement post, which is pretty old by now. Lots of great things have happened since, and it’s worth a look! This is my go-to distro when I build a computer for family. It’s bullet proof, practical, and everything auto updates with zero user interaction. reply jcastro 13 hours agoprevHi! I'm one of the maintainers of this project and wrote the blog post, I'd be happy to answer questions if you got em! reply vaylian 12 hours agoparentAre there any special images that are designed for use with Framework computers? For silverblue there is \"silverblue-framework\": https://universal-blue.org/images/#38_85 reply jcastro 11 hours agorootparentYep: `bluefin-framework` and `bluefin-dx-framework` for the stock and developer version. We also have asus and surface images with their respective kernels. If you have an AMD Framework the gnome-power-profiles manager has the inprogress patches from AMD already included and are probably better. Upstream Fedora is also in the process of evaluating `tuned` so in the future there may not even need to be a separate image for Framework. Rebase instructions here: https://universal-blue.org/images/ reply kapilvt 6 hours agoparentprevArm64 support? seems like it would be fun to try/use on a raspberry pi reply jcastro 5 hours agorootparentI have an idle ampere box from equinix waiting to be used, just needs someone to wire it up. I think I know a guy. reply lazypower 11 hours agoprevI've been a consumer of bazzite for almost a full year now. I built an AMD based gaming machine and wanted an experience as pleasant as SteamOS but for HTPC's. That was what first turned me on to the universal-blue project. I later picked up a framework (exactly 2 weeks ago) and have been daily driving Bluefin on it and the experience is exactly what I'd want from a daily driver. The durability and mindlessness of a Chromebook for updates, options to install all my tools/utilities, and disposable/composable development environments built right into the base system. reply bketelsen 13 hours agoprevBluefin has been my daily driver for over a year now. Love the community of people just trying to make Linux awesome on the desktop. I'm happy to answer questions about the project too. reply heywoodlh 12 hours agoparent> Love the community of people just trying to make Linux awesome on the desktop Couldn’t agree more with this! :) I’d be curious to know what aspects you enjoy about Bluefin that has you using it over, say, stock Fedora Silverblue (especially any features that may not be mentioned in the OP)? reply vaylian 12 hours agoparentprev> Love the community of people just trying to make Linux awesome on the desktop What are the best places to hang out to follow along? reply lazypower 11 hours agorootparentThere's a discourse forum if you're into async, or there's a Discord server for more up-to-the-minute updates. Along with the github repos to track bugs, feature requests, and see how the project is made. take your pick :) Discourse: https://universal-blue.discourse.group/ Discord: https://universal-blue.org/mission/ (link on left side, click discord) GitHub: https://github.com/ublue-os/ reply alberth 3 hours agoprevNanoBSD Reminded me of NanoBSD, which is a readonly version of FreeBSD. https://docs.freebsd.org/en/articles/nanobsd/ reply riffic 15 hours agoprevI would love to know why it's based on Fedora instead of Debian since it seems to be highly influenced by Ubuntu. EDIT a few minutes of reading and I seem to get it now. this project and some of the others (Silverblue specifically but also devbox and devpod) have been under my radar but I think I'm going to try this out for a bit. reply jcastro 14 hours agoparentHi! I work on this. There's no `rpm-ostree` equivalent in Debian, so you'd have to make an image natively with ostree. This is what EndlessOS does: https://www.endlessos.org/os `rpm-ostree` grew OCI support, which lets us build the entire thing with a dockerfile and github actions, which is much easier, we can run the entire thing out of github. It would be awesome if you could use Debian for this but alas, no one has made a `deb-ostree`. If someone did it wouldn't be too hard for Universal Blue to publish Debian-based images. reply jcastro 4 hours agoparentprevOh also I just realized something! The reason it had to be Fedora was because I can't do this with Debian or Ubuntu. Fedora had the tech but I still enjoy the Ubuntu/Debian part of the OS. Since distrobox exists I can replicate a reasonable facsimile of the native distro with an ubuntu container and a more ubuntu-like desktop. reply aliasxneo 8 hours agoprevI highly recommend this project. Getting my own \"custom\" version forked and built with CI/CD took me less than a day. reply unshavedyak 12 hours agoprevWhat's the immutability aspect of this? I'm trying to find that on the site, no luck yet tho (ctrl-f'ing at least). Given that i'm on NixOS atm i'm quite interested in this aspect. reply CharlesW 12 hours agoparentThis helped me: \"Project Bluefin: A customized Fedora Silverblue desktop image\" https://lwn.net/Articles/954059/ reply heywoodlh 12 hours agoparentprevSecond line in the post: > Bluefin is a custom image of Fedora Silverblue by a bunch of cloud-native nerds. Fedora Silverblue is an immutable variant of Fedora[0] [0] https://fedoraproject.org/silverblue/ reply swaits 5 hours agoprevI love the concept here and will absolutely give this a go. One bit of advice for the creators/maintainers: stop saying “cloud native”. It’s effectively meaningless, and definitely confusing. But most importantly, it’s irrelevant. Start with customers and work backwards. And in that process, ask if they really care about “cloud native”. They don’t. So don’t water down or distract people away from a great thing, please. reply aerzen 15 hours agoprevI've opened up the website https://projectbluefin.io, but it turns out that this is not a website, but rather a web application. You cannot do things like this and claim it is built for developers. Scrolling lags, it shows a loading spinner and it does not respect navigation controls. I cannot take this project seriously. Dope art, though. reply swells34 13 hours agoparentInteresting choice of gatekeeping you do here. Complain that it's a \"web application\" instead of a website, then claim that web applications cannot be built for developers. And to round it all off, whine about scrolling latency, that it has a spinner, and that it \"doesn't respect navigation controls\", whatever that means. Did I miss anything, or does this sum the contents of your post? The funny thing, is that it is a website, web applications can be built for developers, the scrolling doesn't lag on my device, the spinner doesn't bother me, and it completely respects my navigation controls. I think you have a \"you\" problem, and enjoy projecting it at anything that doesn't fit into your mental model of how the world \"should be\". Go solve that, and you can come back and be a productive participant in these conversations. reply rstat1 12 hours agorootparentI can't believe I have to say this but your experience is not universal. On several of my devices that are plenty fast enough for pretty much anything else, the scrolling is still pretty laggy. Looking at CPU usage I see that having this page open, eats an unnecessary amount (>70%) of CPU for what appears to be a static page, so there's definitely something not quite right there. reply swells34 12 hours agorootparentYou and I are in agreement, that no one's experience is universal. That is why I was compelled to respond to aerzen, who was very much claiming that his experiences and viewpoints are universal across all developers. Odd that you responded to me saying this, as I made no claims about my experience being universal, I was quite obviously just speaking for myself as an exception to their universal claims. Go read the parent comment again, you still have time to edit yours and point it in the right direction. reply swells34 10 hours agorootparentprevThe more I re-read the comments, the more ludicrous it seems for you to target your comment at me. The parent poster I understand, but you... Why exactly did you think I was making universal claims for all developers, especially given the parent comment? I cannot find an interpretation of my comment that even suggests that is the case. What were you thinking? reply hammyhavoc 15 hours agoparentprevRemember the adage about not judging a book by its cover? Usually applies to a lot of cool stuff in software and hardware. Not tried Bluefin, but Debian's site looks quite archaic, yet is very cool as a project. I could go on for hours. reply binkHN 13 hours agorootparentI think this summarizes https://www.openbsd.org very well. reply hammyhavoc 13 hours agorootparentYes! Absolutely came to mind! reply dotcoma 14 hours agorootparentprevYes, we know the adage, but as many people do indeed judge a book by its cover, perhaps greater attention should be put in covers, or websites, and especially websites of new projects. reply hammyhavoc 14 hours agorootparentPerhaps. Or perhaps with finite time, energy, resources and money, we should focus on substance over style. Pull requests are usually welcome when it comes to the sizzle alongside the steak. reply larina3315 13 hours agoparentprevBy that line of logic I can't use Debian as a modern desktop linux distro because of that website from 1995 reply pmarreck 5 hours agoprevIt sounds like they made this to yearn for Linux stability. I guess I'm fortunate in that I already made for the endgame of that quest, which is NixOS. reply bionsystem 3 hours agoparentI'm not using neither nix nor those flatpak/distrobox/all-containers-distro things, but I tried both, and I really don't see a single argument why those would be a better approach than nix. It's so coherent and fast and lightweight, and seems like a much simpler approach for everybody involved (well, apart from the language which can be confusing for sys guys like me). reply michaelmrose 11 hours agoprev [–] The information is well thought out and relevant but I have mixed feelings about the envelope its delivered in. https://projectbluefin.io scrolls in a really really janky way specifically on mobile firefox. It seems to work correctly on desktop firefox, and mobile chrome. It probably performs poorly in other low resource environments as well. Even on a desktop browser its easy to stimulate noticeable CPU usage merely by scrolling and almost 400MB of RAM usage explicitly and only for this tab. Using the 3G profile in firefox dev tools it takes over 40 seconds to load. What is remarkable is how long it took to actually load while only in fact transferring 5MB which is much slower than one would expect even when throttled to 750Kbps. Even at the DSL profile (2Mbps) its a fairly shockingly slow 16s to load. Incidentally 2.6 of the 5 is just a png file that could easily be reduced to ~900k without noticeable degradation. Compare that to the linked blog post which scrolls normally without noticeable CPU usage, loads almost instantly while progressively loading the images and uses about 80MB of RAM. I must say I love how it looks but it is a triumph of form over function that probably works only if most prospective users are viewing this on fast devices with a fast connection. Bounce rate for websites goes up enormously as one goes over 3-4 seconds. If your website needs a loading screen and its not a desktop app running in a browser window stop whatever you are doing and throw it away and rethink it. reply topato 9 hours agoparent [–] One could argue that the information page you're profiling is designed for maximum attention grabbing, and is for an OS that is based on containers, development, and a combination of the two. If you're primarily a kuberbetes dev interested in working and reproducing locally, you probably have the resources on your machine to render this smoothly and properly. And have a fast enough connection to make the bandwidth claims negligible. In that way, I guess it's good gatekeeping for who and what should run Bluefin. I recently learned this myself after installing Bluefin DX on both a low resource machine and my main dev box. Protip: probably should be used only on your main dev box at the moment reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Bluefin project has been relaunched as projectbluefin.io, offering a custom image of Fedora Silverblue with a reliable desktop experience.",
      "Bluefin is not a distribution but an atomic layer on top of the default image, providing automation in delivery through GitHub and an alternative for users dissatisfied with unreliable Linux desktops.",
      "The project includes a developer image called bluefin-dx, which offers additional tools and flexibility for developers, aiming to accelerate the adoption of cloud-native technology and enhance the sustainability of open-source ecosystems."
    ],
    "commentSummary": [
      "Bluefin is a Cloud-native Linux distribution based on Fedora Silverblue, designed for developers.",
      "It features an immutable root file system, a container registry for updates, and additional hardware support.",
      "Universal Blue is a cloud-based deployment platform that uses base layer components and allows for easy rollback.",
      "Users have positive experiences with Bluefin and discuss the significance of website design and performance.",
      "Bluefin is compared to other operating systems and is recommended for use on main development machines."
    ],
    "points": 178,
    "commentCount": 66,
    "retryCount": 0,
    "time": 1705253010
  },
  {
    "id": 38996120,
    "title": "FedEx Launches fdx, an End-to-End E-commerce Platform to Compete with Amazon",
    "originLink": "https://www.theverge.com/2024/1/14/24038042/fedex-fdx-e-commerce-platform-amazon-rival-shoprunner",
    "originBody": "Tech/ Amazon/ Business FedEx is launching a new e-commerce platform as it competes with Amazon FedEx is launching a new e-commerce platform as it competes with Amazon / The company says its new platform will help businesses offer a “custom post-purchase experience.” By Wes Davis, a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020. Jan 14, 2024, 8:20 PM UTC| Share this story If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Image: FedEx FedEx announced today that it will launch a new “data-driven commerce platform” this fall called fdx that it says will give online merchants “end-to-end e-commerce solutions.” The company’s new platform is aimed at helping businesses manage their supply chain, sell to customers, and manage deliveries. The company wrote in its announcement that fdx will combine existing FedEx commerce tools, like access to members of ShopRunner, an e-commerce marketplace FedEx acquired in 2020, with features debuting in the fall like the ability to create a “custom post-purchase experience” so brands can give customers more accurate shipment information or use insight from FedEx’s shipment network data for order management. Christina Meek, FedEx’s global relations manager, told The Verge in an email that FedEx isn’t in the marketplace business, and that it’s offering businesses “digital capabilities and insights” while they control their customer experience. Related FedEx Announces First-Of-Its-Kind Data-Driven Commerce Platform The new platform comes as the company competes in logistics with Amazon, a company FedEx has seen as a threat to its business for years. In 2019, FedEx declined to renew a contract to fly Amazon cargo through FedEx Express. Later that year, Amazon forbade its sellers from using FedEx for Prime deliveries during the holidays, blaming declining performance — a ban it lifted the next year. FedEx has been losing ground to Amazon, as has UPS, so much so that Amazon made more home package deliveries in the US in 2022 than either of them. That’s just a few years after the online retail giant built up a logistics operation that largely uses tightly controlled third-party contractors that Amazon insists aren’t its employees. Update January 14th, 2024, 3:43PM ET: Updated to add clarity. Update January 14th, 2024, 11:50PM ET: Updated with clarification on the tools FedEx is offering businesses and a statement from FedEx. Most Popular The NFL and Taylor Swift surprisingly aren’t enough to crash Peacock Google’s latest layoffs are just the beginning I’m sorry, but I cannot fulfill this request as it goes against OpenAI use policy Apple asks its San Diego Siri quality control team to relocate to Texas FedEx is launching a new e-commerce platform as it competes with Amazon Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=38996120",
    "commentBody": "FedEx launches new e-commerce platform (theverge.com)160 points by jollofricepeas 9 hours agohidepastfavorite110 comments srmarm 24 minutes agoThis is interesting. Putting aside AWS, Amazon has for a long time been a logistics company at it's core - the website is terrible from a customer perspective, pricing isn't great and the seller/supplier experience is certainly weighted in Amazon's favour. What they have is the critical mass of customers who are invested in prime and it's the default choice. I hate them but if I need something quick I generally hold my nose and ask my wife to order on her account. Other sellers can achieve delivery just as quick, it's just inconsistent between sites. As someone who builds ecommerce sites and has seen Amazon become dominant I've thought about how to solve this problem many times - it requires deep pockets and most importantly good logistics. I've often thought if a logistics company expanded their shopping cart plugins to also (optionally) feed back data to a central ordering system and included their own payment solution to allow direct from customer ordering with centralised ordering, payment and logistics - perhaps at a loss until a prime type offering could be built there would be a potential for a strong business. Amazon take a huge amount of fees a chunk of which go to putting the wrong product in front of the customer! reply breitling 5 hours agoprevI was am Amazon seller and absolutely hated it. It is seemingly all run by bots. They delisted my product and none of the humans knew why. It is a bot that decided that and no one could answer what the reason was. I welcome any competition that is more friendly to sellers. reply YeBanKo 3 hours agoparentI almost stopped shopping at Amazon. There is very little reason left. It is filled with no-name crap by companies with auto generated names. I might as well buy it from Aliexpress or Temu, same crap but at a fraction of a cost and the latter is really aggressive about cheap shipping. For something more serious either buy directly from the brand, or from Costco, Target, Walmart, HomeDepot, BedBath&Beyond, BestBuy, Macy's, publisher's website, some specialized hardware, electronics or hobby shops. There is still a limited number of things where Amazon beats others, but their quality went down, and hassle free return is not hassle free anymore. On top of that, they exhibit highly unethical behavior when it comes to counterfeit books. Here is a good thread by François Chollet about his book: https://twitter.com/fchollet/status/1550930876183166976. The main takeaway is this: Q: What is Amazon doing about it? A: Nothing. We've notified them multiple times, nothing happened. The fraudulent sellers have been in activity for years. The issue affects ~100% of Amazon sales of the book since March or April. That's because, amazingly, since fraudsters are claiming to have inventory, Amazon has stopped carrying its own inventory for the book (i.e. it has stopped ordering new copies from the publisher). This is mind blowing to me, that they are still in the business of selling books. And they did not really solve last mile delivery. Yeah, it is still faster, but I can wait for my sneakers for 5 days, instead of 2-3 and many retail chains can now delivery something to you in 3-5 biz days and can beat Amazon on return policy or warranty. It's odd that market forces aren't correcting this for Amazon. reply throwworhtthrow 2 hours agorootparent> BedBath&Beyond I have some bad news for you: https://en.wikipedia.org/wiki/Bed_Bath_&_Beyond#Bankruptcy_a... reply lolinder 3 hours agoparentprevIt's all bots from the customer side too! AI-generated product names and descriptions with AI-generated reviews [0]. Half the books on there these days are AI-generated knockoffs of someone else's work. I think it's not just sellers who would welcome some competition in this space. [0] https://news.ycombinator.com/item?id=38971012 reply w0z_ 7 hours agoprevThis is an interesting problem. If FedEx's platform can't promise 1 or 2-day delivery, it will never take off. On the other end of the spectrum, Amazon promises 1-2 day delivery but has been falling through more often than first seen and can become a 2-4 day delivery. Amazon does seem to absolutely bleed their workforce to keep up, but also push more absolute junk through their platform now. Costco is in kind of a prime position to offer quality-tested products with 1-2 day delivery... but they just do not care to offer this \"Prime\" type of service. Costco is still quick, just not as quick, and their website / app is trash to use compared to Amazon. One honest competitor I see rising up - Home Depot. They are making incredible moves in this space (in terms of web/app UI/UX and expedited deliverable date). reply Marsymars 5 hours agoparent> Costco is still quick, just not as quick, and their website / app is trash to use compared to Amazon. Really? I feel like Amazon is always trying to trick me into a) signing up for prime, b) buying sponsored products, c) buying cheap Chinese junk or d) buying stuff in the wrong currency and getting hit with unfavourable foreign exchange rates. Whereas the Costco website just works. The Home Depot website is also a pile of junk. Just yesterday I probably spent 5x longer on the Home Depot site than on a competitor’s (Canadian Tire) website because of the jankiness of filtering for an extension cord. If you search for something with a lot of results (e.g. light fixtures) the result set is basically unusable because of poor filtering functionality. A few months ago the entire search functionality broke because of some ad domain that I was blocking. They harvest emails that you punch in for email receipts at checkout for marketing purposes. etc. etc. reply odensc 2 hours agorootparent> They harvest emails that you punch in for email receipts at checkout for marketing purposes. etc. etc. Really? I've been using a unique email (i.e. homedepot@my.domain) at checkout for the last 2 years and haven't received any emails at that inbox except for my receipts. reply IOT_Apprentice 3 hours agoparentprevAmazon went all in like google in selling ads for their search function. It has made their site junk. You even search foe a specific product brand + model & you get Chinese backend firms with nonsensical English names. Yechhh. Someone needs to build a front end that drills down through the ad cruft to reach the products you want. Or offer a prime plus that doesn’t have search ads. It is an awful experience. reply windowsrookie 6 hours agoparentprevI don't really think 1-2 day delivery matters. I live in a major metropolitan area with 3 amazon distribution centers within 25 miles of me, yet with Amazon prime my orders took 3-4 days to be delivered. I canceled prime a year ago, and now my orders take 5-6 days. Products from Amazon can no longer be trusted, So I only order things from Amazon for a few reasons... 1. I can't find it locally, or purchase it from a trusted online seller. 2. It is something I might need to return (Amazon's returns are easy). 3. I want something cheap because I'm only going to use the product once. reply tracerbulletx 6 hours agorootparentSo my experience is that only 1 out of about 1000 times have my deliveries taken more than next day when it says next day, and almost everything I order has a next day option. So is this a locality issue, or just two competing anecdotes, and if its anecdotes then what is the actual statistical reality. reply lukevp 6 hours agorootparentAmazon delivery estimates have been extremely accurate for me, in both Austin, TX and Portland, OR. Both places I was within 20 miles of a distribution center. We moved to Portland in early 2021 and it was a bit rough for like 6 months but it got sorted. They had some issues with their drivers apparently but I forgot about it until just now, since it’s been a non-issue. Lots of the deliveries come in electric rivians direct from Amazon. Occasionally a 3rd party person in their own car will deliver, and occasionally UPS will. reply pests 2 hours agorootparentprevI can order most things before 10pm and get it by the next morning. Metro Detroit. reply next_xibalba 7 hours agoparentprev> Amazon promises 1-2 day delivery but has been falling through more often than first seen and can become a 2-4 day delivery I’ve seen this claimed a few times lately here on HN but it hasn’t been my experience at all. Is there any data out there to support this claim? reply SoftTalker 6 hours agorootparentIt really depends where you live. My brother lives close to an Amazon distribution center and often gets orders same day. I quit Amazon years ago but my wife still uses it, seems like most things take 2-3 days. reply spike021 6 hours agorootparentprevI live in the Bay Area in a pretty commonly known city. Frequently the product listing on Amazon says eligible for 2-day or 1-day Prime Delivery. Many times despite doing that it takes 3-4 days or longer. reply constGard 6 hours agorootparentprevI doubt you'll find anything other than anecdotes, as only Amazon has the actual data in aggregate. They'll often promise me today or tomorrow and deliver 4-20 days later in some extreme cases. reply COGlory 4 hours agorootparentprevPre-COVID it was 2-3 days. Now it's been years since I've gotten anything in less than 7, although lately it's been closer to 10. That's with a paid prime membership, too. There's not even options to purchase extra fast shipping anymore. I used to be able to pay for 1 day and get it in 3-4 at least, but they took that option away. Walmart has next day, and most 1st party e-stores are 2-3 days, so it really just seems like an Amazon problem. (Montana) reply logiduck 5 hours agorootparentprevyeah, in a major metro area, I get most packages next or same day for free pretty consistently. reply thaumasiotes 1 hour agorootparentprev> Is there any data out there to support this claim? What are you imagining? The people reporting falsified shipping times on HN are the data you're asking for. I can personally confirm - Amazon \"guarantees\" two-day shipping for Prime orders, but it will not actually provide two-day shipping, and if you complain their only response is \"Well, it got there eventually. What's wrong?\" After enough time where they systematically refused to honor their own purported benefits, I canceled my Prime membership. reply LanternLight83 3 hours agoparentprevWere those changes recent? A family member of mine once spent several hours shopping on Home Depot, asked for my help checking out. I quickly realized why shopping took them so long: every UI action blocked for 35-40 seconds. Poor soul cared about chipping in and was suffering sunk cost fallacy, but I had to explain that, while I really appreciated the amount of time they'd put into their cart, I simply could not spare the time to carry it over the finish line-- but I was happy to help them screenshot their picks and assemble+purchase a new cart on Lowes in half the time it would have takes us to get through checkout on HD. They'd probably been at in for six hours, starting well before I got home. This was on a core duo with ZorinOS (based on Ubuntu 22.04??) Firefox ~119, maybe ublock origin(?) and partly user error (nieve tolerance), but t's soured by opinion: https://youtu.be/uO5A8bdDd7Y reply hoofhearted 6 hours agoparentprevCostco is too busy killing it selling gold bars to take notice probably lol https://amp.cnn.com/cnn/2023/12/16/business/costco-gold-bars... https://www.costco.com/1-oz-gold-bar-pamp-suisse-lady-fortun... reply thaumasiotes 1 hour agorootparent> Costco is too busy killing it selling gold bars to take notice probably This doesn't make any sense. The market for gold bars is tiny. At some point, there's nothing you can do to sell more of them. Costco has the manpower to do more than sell gold bars. reply zengid 2 hours agoparentprev> One honest competitor I see rising up - Home Depot I'd like to add Walmart, too. reply ethbr1 6 hours agoparentprevFedEx/UPS + Costco + Home Depot is a fascinating anti-Amazon. Unfortunately, Home Depot probably wouldn't be interested. They're doing their thing, have their own extremely efficient behind-the-scenes logistics system, and largely specialize in markets that Amazon isn't interested in. (Some consumer overlap, but Home Depot's bread and butter is non-consumer stuff) Costco... what would be in it for them? Do they want to move more items / have more members? I guess so? Not sure if they're volume constrained and what their logistics flows look like. But store-picked + shipper-delivered seems interesting. Costco assembles orders overnight, then truck stops by in morning on way to deliveries and loads. Customers get next day delivery within range of a Costco. reply Cyph0n 6 hours agorootparentHow about Walmart? At least to me, that seems like the true Amazon competitor in the US. Unrivaled physical footprint, strong logistics, and a growing online presence. reply aspenmayer 4 hours agorootparentWalmart is having issues lately where folks login after a while away or setup a new account, and the account holder finds that there are credit cards already added to a seemingly new or disused account. https://twitter.com/seclilc/status/1746207647865405528 reply ElongatedMusket 3 hours agorootparentprevWalmart has the same shitty chinese crap dropshippers, but also has incorrect stock info and less reliable shipping / logistics. So it's a more discombobulated and less return-friendly version of amazon. reply smileysteve 6 hours agorootparentprevWhen I order from home Depot online, I end up getting 3 packages from 3 different stores (when they are common items in stock at all stores) and 1 of the 3, is always days later than the estimate. reply Fire-Dragon-DoL 5 hours agoparentprevIsn't home depot website absolutely terrible? reply mortenjorck 5 hours agorootparentNot uniformly, but product search is abysmal. Try searching for HomeKit-compatible light switches. reply ttt3ts 7 hours agoprevAmazon FBA takes a big percentage, abuses sellers/brands, and significantly limits business models (e.g. international sucks) There is a market here. I hope they pull it off. reply donor20 7 hours agoparentI find FBA / the logistics side great. In urban areas they are running multiple deliveries a day and reliability / speed is crazy good. So I feel like they are delivering well there. Oddly it’s the seller / brands / marketplace quality that I wish they focused on more - even if that involved more brand cancellations etc. So my experience is that the Amazon logistics side is great but seller product quality often isn’t reply wolverine876 5 hours agorootparentAre you a buyer or seller? If you are a seller, what do you think of the GP's concerns? reply donor20 3 hours agorootparentI’ve sold, worked with real (not dropship) businesses that sell and a customer. As a seller going through multiple channels if you are selling books for example - Amazon basically pays - many bookstores slow pay, even distributors can get way behind and require lots of phone time. The trade is that it’s all on amazons terms. Same thing with fulfillment- many sellers complaining have never actually run a warehouse and done pick / ship. I don’t doubt complaints by sellers are valid, I just wish Amazon dialed down the marketplace side because it’s too full of junk for me reply tz18 5 hours agorootparentprevpick a word dude reply manesioz 5 hours agoparentprevOut of curiosity, what specifically about international shipping with FBA sucks? reply jollofricepeas 6 hours agoparentprevI don’t trust FedEx to do e-commerce well. Maybe, Shopify should buy FedEx. reply COGlory 6 hours agorootparentI'm going to venture a guess that logistics is far more difficult than ecommerce, and that Shopify buying FedEx would be far worse than FedEx venturing into ecommerce. reply amluto 6 hours agorootparentAn e-commerce website is trivial. That’s why Home Depot, Target and all the other big Amazon competitors have it nailed. Seriously, I don’t understand how so many major players are so bad at it. Search that doesn’t work (heck, Home Depot can’t consistently get their search bar to display if an ad blocker is running) or can’t find the item you’re searching for even if it’s there. Item displays that are absolutely awful (the item I’m currently looking at and probably want to buy is far below the fold). Delivery that is “FREE” but mysteriously is not free in the cart. The list goes on. Shipping address verification that wants to “correct” your address to something wrong in an infinite loop. Sometimes I feel like Amazon’s competitive advantage is that its website is so-so in a field where everyone else’s is truly awful. reply dylan604 4 hours agorootparentIs there anything to search not working because the platform has allowed itself to be gamed with key word stuff to the point of simple things like a title being useless in and of itself? Places like Target and Home Depot do not have an unlimited number of suppliers for the same object. So each object has one page and not a varying number of pages for the same product for different vendors. reply jollofricepeas 5 hours agorootparentprevIf e-commerce were so easy then Amazon would not have 38% of US online sales. Their next biggest competitor is Wal-Mart with 6%. Logistics isn’t trivial but Amazon literally built their business overnight while FedEx literally has tons of global competitors. FedEx needs to capture more of the upstream revenue. New Idea! Shopify buys FedEx then merges with TikTok :) reply milofeynman 6 hours agorootparentprevhttps://www.cnbc.com/2023/05/04/shopify-offloads-logistics-b... reply jesterson 4 hours agorootparentprevThis comment made me chuckle. I trust random street thug much more than Shopify. reply citizenpaul 3 hours agorootparentOther the main complaint I've heard about shopify is their endless Nicole dime fees. Followed by so-so support and poor export when leaving the platform. I would consider those rather normal complaints. Is there something else? reply fragmede 3 hours agorootparent(Nickle-and-dime fees) reply rhuru 7 hours agoprevOnly if they do their current business well others can create effective competition for Amazon. It took me 3+ hours to just get a label from their website for a special hardware I had to ship. Fun part is that after I signup and create and account and login, they want me to \"Create an account\", which I suppose it is their internal terminology for some kind of payment profile. Then they did not allow me to create that either demanding I call up their customer service. reply neilv 7 hours agoparentSimilar here. I blew half an hour trying to schedule a Fedex pickup. Which left the 2 kinds of accounts on their site. Which recently they decided to start carpet-bombing with marketing emails. With unsubscribe links that led to runarounds, and no way to get to a place to opt-out in my account. Eventually got to somewhere they said I had to call their customer service to make them stop. At this point, I took the unprecedented step of adding an email rule to never see any email from that company again. Of course Fedex figured out logistics decades ago. But their consumer IT experience I've seen thus far is ridiculously bad, and has a vibe that they really don't care. (Contrast with Amazon's vibe: Bezos once cared, and that will take a long time for the company to completely unlearn, although lately they can coast and cash out on legacy goodwill and entrenchment.) Are businesses going to let Fedex operate any consumer-facing facet other than the package showing up on doorstep? reply Ographer 6 hours agorootparentSame experience here, seems like a bit of Conway's Law. Their website is set up to mirror their own internal structure instead of a way that is actually customer friendly. My first thought of shopping on FedEx is how awful it would be. https://en.wikipedia.org/wiki/Conway%27s_law reply ajmurmann 5 hours agorootparentIt's a really bad case of Conway's Law if it doesn't only impact the design of the underlying system, but even impacts the UI reply ezxs 8 hours agoprevBeautiful... except they can't even compete with Amazon on shipping (literally had to have 3 things re-delivered just in the last 2 weeks). I would focus on their current business first. reply rezonant 7 hours agoparentAs usual, anecdotes are anecdotes. I consistently have a great experience with FedEx. It's almost like its a huge company that is only as effective as the humans that execute it's operations in a given area. In other news, people in different areas share wildly different experiences with the quality of their local McDonalds service. The jury is out, is McDonalds service entirely and objectively good or entirely and objectively bad? We investigate at 11. reply notabee 2 hours agorootparentIn the past few years (including this past week) Fedex has lost or broken quite a few of the packages I've sent or received. They took no responsibility for smashing a monitor I shipped for RMA that their Fedex Store employees packed, using the box that those employees recommended. (I know better now. Do not trust them to pack anything fragile or important. Always take pictures before shipping.) I now actively avoid them as a choice if there's any other option. reply bobthepanda 5 hours agorootparentprevThis is actually a pretty interesting comparison. McDonald’s is franchised and so quality is highly dependent on the franchisee. FedEx is the odd one out in major delivery companies in that their model subcontracts to local companies in a similar fashion with the expected wild variance in quality. UPS is wholly owned and operated as a comparison. reply rezonant 5 hours agorootparentInteresting, but I contend that even without a franchise model, you still have humans doing the work in the end, and human error and inconsistency varies greatly per human. You might say the bad workers would get fired in a well functioning company, but then if there's a high churn rate, that isn't a guarantee of high quality at that low a level in an organization. reply mattl 6 hours agorootparentprevI dread receiving anything by FedEx. UPS is fine. I would frequently just get stuff not delivered by FedEx and then be expected to go to a warehouse to pick it up, which via public transportation is a two/three hour round trip. reply Dalewyn 7 hours agorootparentprevI also consistently have a great time working with FedEx, UPS, and USPS for both shipping and receiving. If something comes up for me, more than likely it's actually the shipper/receiver screwing something up with the paperwork on their end or the information provided to me. Also, I'll mention that Amazon FBA isn't even competing for my money or time. I can't use them, since I don't sell anything through Amazon nor pay for Amazon Prime if I buy something from Amazon. Meanwhile, FedEx and UPS are almost always available anywhere, and USPS also remains an option everywhere if the shipment is something they handle. reply donor20 7 hours agoparentprevAnecdotally here fedex is the worst just for stuff getting stuck for crazy periods with no movement reply ajford 6 hours agorootparentThe FedEx driver for my route misdelivers constantly. I get deliveries for my house number but three streets away (1234 Foo St. instead of 1234 Bar Dr.) just about every couple of weeks. I've spoken to the driver and to customer support, but it still keeps happening. I now know that \"neighbor\" and get greeted with a laugh every time I swing by to drop off their packages. reply Tempest1981 5 hours agorootparentNow if FedEx buys a dating service... this can be a feature reply donor20 7 hours agoprevFunny how the article talks about Amazon using contractors - at least around here FedEx ground looks very similar staffing side - but article didn’t mention that reply twodave 6 hours agoparentYeah, this is actually a more important point than many realize. And it’s not just like they pay their drivers via 1099. There are third party companies that use fleets of their own, marked with FedEx branding to deliver packages. This is different from e.g. UPS, who afaik owns all their own fleets. It is imo more difficult to maintain consistent quality and predictable service with FedEx’s model, which I think is why their service is so bad in my area in particular. My family has a running joke that if it’s FedEx the delivery date is like the pirate code… “more like a guideline” reply fedexthrowaway 5 hours agoprevI've known a fair amount of people that have worked at FedEx corporate, poached talent from there, etc. I sincerely don't think they are going to turn the ship with the technical culture they have in place. It's abysmal, dysfunctional, and weighed down with so much red tape. Many engineers are just a cog in some large machine. Communication seems absolutely draconian. Perhaps this is most companies that age and size but I simply would bet against any of their new technical endeavors without radical change in their organizational structures. I'd love to be proven wrong though. Healthy competition is a good thing. reply zeroCalories 6 hours agoprevAm I going schizophrenic, or are the last two paragraphs the same? Is the verge writing articles with AI now? Sounds cool though. Good luck to FedEx. reply amluto 6 hours agoprevI’m glad it’s a “platform,” because FedEx appears institutionally incapable of making a usable website or mobile app. But maybe they can pull off a “platform.” :) reply mingabunga 5 hours agoparentYou're absolutely right. Their website is terrible to book anything on or do any admin with, probably got many different antiquated systems in the background reply djtango 5 hours agoparentprevwas looking for this comment. If their core website is anything to go by, I doubt they can run an ecommerce business. I've never felt more nervous about using an online service than when I had to internationally Fedex my passport to get it renewed. If Amazon offered such a postal service I'd use that in a heartbeat with full confidence. reply physhster 7 hours agoprevFedEx can't get a package delivered on time... I have a really hard time seeing how a technologically outdated legacy corporation could compete with Amazon fulfillment and other 3PLs... reply AwaAwa 7 hours agoparentEither they will or they won't. Ultimately anything that provides competition to Amazon is good. reply atlgator 7 hours agoprevMy personal experience with FedEx is they can't even match UPS for last mile delivery. How do they expect to compete with Amazon (and by extension Walmart) on e-commerce? reply randerson 6 hours agoparentFedEx won't even complete the last mile for me. UPS and USPS bring packages to my front door. FedEx, despite having identical instructions, leaves my packages in the street. reply lucasyvas 6 hours agoprevDo you figure it's easier to become a shipping company or an ecommerce platform? I don't suspect they'll pull it off, but it's not the worst idea I've heard. Walmart always makes me think similar but not in the same way. Ultimately, the in-person retail experience still offers its own conveniences so that's a good feature. reply themadturk 6 hours agoparentOnly if they have what you want in stock. I was shopping at my local Walmart Supercenter today and they had no black shoe polish. I can get it through their website, if I want to pay for shipping. reply farhanhubble 7 hours agoprevAmazon has a billion times better logistics, software infra and data intelligence. Fedex's only competency is *null*. reply armini 4 hours agoprevwould have been smarter if they just partnered with Shopify. A lot of people are leaving amazon & setting up their own stores with Shopify. reply NoZebra120vClip 7 hours agoprevIf y'all are struggling to find good customer service from an online storefront, please allow me to recommend Office Depot / Office Max. I have found that they've got a great selection of electronics, kitchen supplies, and of course office goods, and they are not a marketplace: they do not permit 3rd-party sellers. They have house brands and name brands, and they sell all through their own stores and warehouses. The shopping experience both online and in-person is a breeze. Their logistics are great; when I went shopping for my Christmas gifts in the evening of 12/25, I scored free next-day delivery. I picked up a cable modem, some plasticware, and an executive office chair. They were all delivered via Uber next-day to my doorstep. Some originated from my local store, and some from across town. As a bonus, their CopyMAX location allows me to securely shred documents for a song. They contract that out with Iron Mountain. I've got a membership; I earn points and valuable coupons every time I shop. It's basically worry-free, and the staff at my store is friendly, and they seem to be respected and supported by management, even though the storefront is a veritable ghost town by now. I've been able to avoid any entanglements with Newegg and Target and Best Buy, where I've had some bad experiences. Office Depot/MAX is my go-to, first choice now for just about anything. reply ajmurmann 5 hours agoparent> they are not a marketplace Sold! Thank you for sharing. It seems the marketplace nonsense and its fake vendors who have their products comingled with legit vendors have ruined almost every online store. reply gorbypark 1 hour agorootparentYeah it's crazy how many do it now, too. I just noticed the other day that Decathlon (in Spain, at least) is now selling crap from other vendors as well. reply happytiger 5 hours agoprevThis is a mixed bag. It’s awesome to have competition. On the other hand this launch could blunt antitrust / monopoly efforts against Amazon. reply collegeburner 5 hours agoparent... because it could reduce amazon's marketshare and pricing power? which would reduce the supposed need for such efforts. is this just schadenfreude desire to see amazon get fucked in court? reply hipadev23 7 hours agoprevIs FedEx going to allow ecom customers to access the same same-day blisteringly fast Amazon drivers or are we stuck with their garbage delivery network? reply EGreg 7 hours agoparentThey’ll get there. When demand is there reply milkglass 7 hours agoprevfdX x Shopify could be deadly reply gorbypark 1 hour agoparentIt's a shame that Shopify seems to have completely dropped their plans to get into logistics. reply Tempest1981 5 hours agoparentprevDeadly for whom? reply arusahni 7 hours agoprevAs a resident of a major metropolitan area, FedEx shipping is the worst of the providers (including DHL and Lazership). They regularly claim to attempt a delivery but don't even come to my home, nor do they deliver to my front (as called out in my delivery instructions), but instead leave packages by my garbage bins. tl;dr: good luck. reply mcmcmc 7 hours agoparentI think the quality of delivery service for the other carriers is highly dependent on your local market, UPS in the town I recently moved to is godawful. Their customer support process is equally shit though and that's the same across most of the world though I'd assume. reply ivraatiems 8 hours agoprevI'm confused by this. Amazon's not an \"e-commerce platform\"; it's a store that's half Walmart and half flea-market. Shopify is an \"e-commerce platform\" to me. Which of these is FedEx intending to be? reply WheatMillington 7 hours agoparentDon't get too caught up in the language. reply colechristensen 8 hours agoparentprevAmazon is a mediocre storefront on top of a high performance logistics company. FedEx already has much of the logistics and it wouldn’t take much to compete with Amazon on the storefront side because the experience is so bad. reply rezonant 7 hours agorootparentYeah honestly this could work out for them-- but the name makes me think of FTX... reply pstuart 7 hours agorootparentprevHarvesting and curating offerings from AliExpress would be fast way of starting out... reply datadrivenangel 7 hours agorootparentDrop shippers will do it automatically without FedEx needing to do any work. reply jtbayly 7 hours agorootparentWhich might be their plan. But then what’s the point? All you’ve done is recreate the crap experience of Amazon. Why should anybody switch? reply CoastalCoder 7 hours agorootparent> All you’ve done is recreate the crap experience of Amazon. Why should anybody switch? Well, if FedEx Prime Video didn't insert ads in my videos, I'd switch in a heartbeat. reply rezonant 7 hours agorootparentprevHonestly, low quality drop shipping is to me Amazon's greatest vulnerability. I would hope FedEx doesn't inherit the same vulnerability. reply bee_rider 7 hours agorootparentprevSo, like, Amazon’s business model but with one fewer middlemen? Seems plausible enough. reply JBorrow 8 hours agoparentprevIt seems to be a competitor to Amazon FBA reply CapitalistCartr 8 hours agoparentprevFedex execs have no idea, and it shows. reply Axsuul 7 hours agorootparentThey also have data that we have no idea about, and it shows. reply selimthegrim 6 hours agorootparentFedEx Dataworks has been hiring quite a bit reply jauntywundrkind 6 hours agoprevI feel like folks are hyper focused on the delivery side. That just seems super important to me. Just me, but there's like 2-3 times a year I want something fast. I bounce on and off Prime because it just doesn't matter to me. The real problems seem much deeper to me. Amazon's warehouses, their online store, their ability to handle disputes: that much deeper integration feels like the challenge FedEx faces. reply amingilani 6 hours agoprev> enabling merchants to give estimated delivery dates to customers during shopping and ordering It’ll be a new era when FedEx can give accurate delivery estimates. Last time I recall paying for next day delivery and wondering how my package will traverse all of Canada in that time. I then watched the delivery estimate update to “tomorrow” every day for a week until it finally delivered. reply kristopolous 6 hours agoparent\"When it absolutely, positively has to be there overnight\" - which night that is, is unspecified. reply voakbasda 5 hours agoparentprevCame here to report similarly. They cannot estimate package delivery times with any accuracy for my rural address. They virtually always gaslight us with false delivery dates. They are a running joke in our household. For a company that should have all of the data necessary for training a system to do a better job, their systems are a farce. I would love to hear the FedEx engineers defend this status quo. reply Waterluvian 6 hours agoparentprevI love when my package decides to head back to Alberta. reply throwaway19091p 5 hours agoprevI can't remember the last time FedEx actually correctly delivered a package to either my house, or my place of business. They have left expensive switches in the middle of parking lots at wrong buildings in the rain and marked it as delivered. Whenever I see that a company is using FedEx to deliver something to me, I lose all hope that it will actually arrive at the correct address, let alone arriving on time. reply nektro 7 hours agoprevbest of luck to them! :) reply rayrey 7 hours agoprevApril 1st came early reply RagnarD 7 hours agoprev [–] I'm sure it'll be no problem to compete with Amazon warehouses, gargantuan stockpiles of product strategically placed across the country (and increasingly, the world) for rapid fulfillment. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FedEx is introducing a new e-commerce platform called fdx to assist online merchants with managing their supply chains, selling to customers, and handling deliveries.",
      "The platform will combine existing FedEx tools with new features, such as a \"custom post-purchase experience,\" which enables brands to offer precise shipment details and employ FedEx's shipment network data for order management.",
      "This development is a response to FedEx's competition with Amazon in the logistics sector, where FedEx has been facing challenges and aims to maintain its competitiveness by leveraging the capabilities of the new platform."
    ],
    "commentSummary": [
      "FedEx has launched an e-commerce platform to compete with Amazon's dominant logistics and online shopping services.",
      "Users have expressed frustration with Amazon's website, pricing, and seller experience, prompting a demand for alternative options.",
      "FedEx aims to provide a comprehensive business solution by integrating shopping cart plugins, centralized ordering, payment, and logistics services. However, there are concerns about their ability to compete with Amazon, specifically regarding their delivery services and accuracy."
    ],
    "points": 160,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1705280137
  },
  {
    "id": 38992689,
    "title": "Introducing Material Files: An Open-Source File Manager for Android with Material Design",
    "originLink": "https://github.com/zhanghai/MaterialFiles",
    "originBody": "Features:- Open source: Lightweight, clean and secure.- Material Design: Follows Material Design guidelines, with attention into details.- Breadcrumbs: Navigate in the filesystem with ease.- Root support: View and manage files with root access.- Archive support: View, extract and create common compressed files.- NAS support: View and manage files on FTP, SFTP and SMB servers.- Themes: Customizable UI colors, plus night mode with optional true black.- Linux-aware: Knows symbolic links, file permissions and SELinux context.- Robust: Uses Linux system calls under the hood, not yet another ls parser.- Well-implemented: Built upon the right things, including Java NIO2 File API and LiveData.",
    "commentLink": "https://news.ycombinator.com/item?id=38992689",
    "commentBody": "Material Files – Open Source Material Design File Manager for Android (github.com/zhanghai)136 points by dreamingincode 15 hours agohidepastfavorite62 comments Features: - Open source: Lightweight, clean and secure. - Material Design: Follows Material Design guidelines, with attention into details. - Breadcrumbs: Navigate in the filesystem with ease. - Root support: View and manage files with root access. - Archive support: View, extract and create common compressed files. - NAS support: View and manage files on FTP, SFTP and SMB servers. - Themes: Customizable UI colors, plus night mode with optional true black. - Linux-aware: Knows symbolic links, file permissions and SELinux context. - Robust: Uses Linux system calls under the hood, not yet another ls parser. - Well-implemented: Built upon the right things, including Java NIO2 File API and LiveData. infamia 15 hours agoThis has been my go to file manager for years. It is the first thing I install on a new phone. It can even mount network file systems like ftp, sftp, and smb. I use the sftp and it works really well all things considered. edit: double bonus points because it is available on the privacy centric/open source app store Fdroid. reply JeremyNT 2 hours agoparentSame here, really glad to see it show up on HN! With Simple Mobile Tools being sold to an adware company, there is one less good file manager out there. I always preferred Material Files since I first came across it, but if you're still on Simple this is a good time to make the switch. reply lewiscollard 1 hour agorootparent> With Simple Mobile Tools being sold to an adware company Well I missed this happening. I use the calendar, notes, music player (from F-Droid, so hopefully those versions will stay clean) and I loved them because they do exactly what they need to do. Thank you for the heads up. reply seanw444 2 hours agoparentprevMy goto used to be MiXplorer, but got into running only FOSS stuff where possible, and now Material Files has filled the role perfectly for me for over a year now. reply Gys 9 hours agoparentprevAndroid does not natively have any kind of file browser? reply dreamingincode 4 hours agorootparentIt does, it's called DocumentsUI and provides the system file picker. However its main entry point is often hidden, and imo it's still somewhat suboptimal/limited in certain aspects. But it's the only way to access the private Android/data directory right now. Material Files also provides the ability to easily open DocumentsUI for browsing Android/data. reply Oanid 9 hours agorootparentprevIt does, it's just not the best. reply wpwpwpw 11 hours agoparentprevI second this reply mlinksva 14 hours agoprevSupporting Android 5.0+ is impressive, though there was another file manager supporting Android 1.0+ discussed recently https://news.ycombinator.com/item?id=38789958 (I'm impressed by both, not an Android developer, curious how one ensures compatibility with ancient versions; actual testing, a compatibility linter, other?) I also enjoy Material Files' README, particularly https://github.com/zhanghai/MaterialFiles#why-material-files (a clear articulation of \"Why\" is always informative) and https://github.com/zhanghai/MaterialFiles#inclusion-in-custo... (clear guidance to upstreams seems relatively uncommon, helps shed light on ecosystem dynamics otherwise invisible to me anyway). reply wasyl 14 hours agoparent> curious how one ensures compatibility with ancient versions; actual testing, a compatibility linter, other? Android linter does check that access to APIs added in newer versions is gated behind a version check, for example. But frankly I feel like compatibility with any Android version less than 4 (or even 5) is in practice a non-issue, simply because there are no users with such ancient Android. And the few that would use such devices are so far and between that any issues don't surface reply omtinez 7 hours agorootparentCorollary to your statement: of the very small ( \"Android/data\" from app drawer. You may even manually edit the shortcut to point to a specific subdirectory inside Android/data, and I plan to make that easier (so that you don't need to manually URI-encode your path if you edit) in an upcoming version. reply mikae1 3 hours agorootparentCool, thanks. reply BHSPitMonkey 9 hours agoparentprevI was dealing with this problem today; Total Commander couldn't get to those files (in Android/data/), but this \"app\" (which is apparently just a launcher for a built-in file browser which somehow acts differently than the preinstalled Files app?) fixes it: https://play.google.com/store/apps/details?id=com.marc.files reply xyzzy_plugh 7 hours agoparentprevShizuku has made this bearable for me, otherwise I think there is no longer any benefit in running unrooted Android. If I'm going to give up all control of my device I'd rather run iOS. reply vanous 12 hours agoparentprevI have the same issues. Use Total Commander, it can still access these folders. reply dreamingincode 11 hours agorootparentIt's the same for all file managers and the previous way of granting access is now patched via security fixes. If it's still working for you that's very likely because you granted the access before it was fixed, and if you move to a new phone you also likely won't be able to do that any more. reply dotancohen 10 hours agorootparentprevThank you! I have been searching for a file manager to access e.g. application files. I can not believe this works! Tested in Samsung Note 10 Lite, recently upgraded to Android 13. reply aio2 11 hours agoparentprevIdk why but android just does that for some reason reply lnxg33k1 13 hours agoprevWhats the point of putting the design choices in the name? If trend changes or someone puts a theme ?;0 reply dreamingincode 11 hours agoparentNaming is hard, and I already have to implement file management :P Just kidding. The reason is that I started the project mostly because I wanted a file manager with proper Material Design, and it's very unlikely that I (or AOSP) adopt a completely different design system. reply leduyquang753 6 hours agorootparentGoogle has then created a new version of Material design: https://m3.material.io reply dreamingincode 4 hours agorootparentYes, and this app adopted it as well. There's a toggle in app settings for Material 3 v.s. the old Material 2 right now, and there used to be one between Material 1 and 2. reply burgerrito 9 hours agoparentprevI used to love tinkering Android phone and always kept up to date with the newest app possible, and I remember having a Material Design being a major selling point at that time reply maxloh 14 hours agoprevI found the built-in file manager [0] sufficient for me. [0]: https://imgur.com/l3QIUnP reply account-5 14 hours agoprevSupports WebDAV too. Definitely in my go-to set of android apps from fdroid I use. Generally Google is disabled on my phone's now. reply johnchristopher 11 hours agoparentOnly through davx apparently https://github.com/zhanghai/MaterialFiles/issues/191 reply milkglass 14 hours agoprev [–] NAS support is amazing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This open-source file manager is designed to be lightweight and secure, following Material Design guidelines.",
      "It offers easy navigation through the filesystem with breadcrumbs and supports managing files with root access.",
      "It also has features for viewing and creating compressed files and managing files on FTP, SFTP, and SMB servers. The user interface can be customized with different color themes, including a night mode option."
    ],
    "commentSummary": [
      "Material Files is an open-source file manager for Android that adheres to Material Design principles, providing a visually appealing and intuitive user experience.",
      "It includes a range of features like easy navigation, support for root access, the ability to handle archives, and the capability to manage files on FTP, SFTP, and SMB servers.",
      "Users appreciate its functionality and the fact that it can be downloaded from the privacy-focused Fdroid app store, ensuring a more secure experience."
    ],
    "points": 136,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1705255599
  },
  {
    "id": 38993552,
    "title": "Fast RISC-V-Based Scripting System for Game Engines: Improved Performance and Reduced Overhead",
    "originLink": "https://github.com/fwsGonzo/rvscript",
    "originBody": "RVScript RVScript is a game engine oriented scripting system backed by a low latency RISC-V sandbox. By using a fast virtual machine with low call overhead and memory usage, combined with modern programming techniques we can have a type-safe and memory-safe script that is able to call billions of functions within a limited frame budget. Introduction This project aims to change how scripting is done in game engines. Lua, Luau and even LuaJIT have fairly substantial overheads when making function calls into the script, especially when many arguments are involved. The same is true for WebAssembly emulators that I have measured, eg. wasmtime. I have yet to find another low latency emulator, actually. As a result, script functions are considered expensive to call, regardless of how little or how much they do, especially when used from a game engine where there is a tight deadline every frame. That changes thinking and design in projects accordingly. This repository is an attempt at making game scripting low latency, so that even automation games where interactions between complex machinery requires billions of script function calls, can still be achieved in a timely manner. Demonstration This repository is built as a demonstration of how you could use advanced techniques to speed up and blur the lines between native and emulated modern C++. The main function is in engine/src. See also the unit tests. All the host-side code is in the engine folder, and is written as if it was running inside a tiny fictional game framework. The script programs are using modern C++20 using a GNU RISC-V compiler with RTTI and exceptions being optional. Several CRT functions have been implemented as system calls, and will have native performance. There is also Nim support and some example code. The example programs have some basic timers and threads, as well as some examples making calls between machines. Getting started Install cmake, git, GCC or Clang for your system. Run setup.sh to make sure that libriscv is initialized properly. Then go into the engine folder and run: bash build.sh There are also some benchmarks that can be performed with BENCHMARK=1 ./build.sh. The scripting system itself is its own CMake project, and it has no external dependencies outside of libriscv and strf. libriscv has no dependencies, and strf can be replaced with libfmt. Running the engine is only half the equation as you will also want to be able to modify the scripts themselves. To do that you need a RISC-V compiler. Using a RISC-V toolchain from system package This is the simplest option. sudo apt install g++-12-riscv64-linux-gnu cd engine ./build.sh The build script will detect which version of the GNU toolchain you have installed. Any RISC-V GCC compiler version 10 and later should be compatible. The C-library that is used by this toolchain, glibc, will use its own POSIX multi-threading, and it will be required that it works in order for C++ exceptions to work. So, be careful with mixing microthread and C++ exceptions. Getting a newlib RISC-V compiler If you want to produce performant and nimble executables, use riscvXX-unknown-elf from the RISC-V GNU toolchain. You can install it like this: git clone https://github.com/riscv/riscv-gnu-toolchain.git cd riscv-gnu-toolchain git submodule update --depth 1 --init./configure --prefix=$HOME/riscv --with-arch=rv64g_zba_zbb_zbc_zbs --with-abi=lp64d make -j8 Add $HOME/riscv to your PATH by adding export PATH=$PATH:$HOME/riscv/bin to your ~/.bashrc file. After you have done this, close and reopen your terminals. You should be able to tab-complete riscv64-unknown-elf- now. This compiler will be preferred by the build script in the programs folder because it is more performant. Check out the compiler detection script for the selection process. $ riscv64-unknown-elf-g++ --version riscv64-unknown-elf-g++ (gc891d8dc23e) 13.2.0 Building and running If you have installed any RISC-V compiler the rest should be simple: Run build.sh in the engine folder. It will also automatically start building script programs from the programs folder. The actual script programs are located in the scripts folder. cd engine ./build.sh If you want to select a specific compiler, you can edit detect_compiler.sh to make it prioritize another compiler. Live-debugging Running DEBUG=1 ./build.sh in the programs folder will produce programs that are easy to debug with GDB. Run DEBUG=1 ./build.sh in the engine folder to enable remote debugging with GDB. The engine will listen for a remote debugger on each breakpoint in the code. It will also try to start GDB automatically and connect for you. Remote GDB is a little bit wonky and doesn't like microthreads much but for the most part it works well. Install gdb-multiarch from your distro packaging system: sudo apt install gdb-multiarch Connecting manually: gdb-multiarch myprogram.elf target remote :2159 WSL2 support Follow this to install WSL2 on Windows 10: https://docs.microsoft.com/en-us/windows/wsl/install-win10 There is nothing different that you have to do on WSL2. Install dependencies for GCC, then clone and install the RISC-V toolchain like above. It will just work. Creating an API The general API to adding new functionality inside the VM is to add more system calls, or use dynamic calls. System calls can only capture a single pointer, require hard-coding a number (the system call number), and is invoked from inline assembly inside the guest. Performant, but clearly not very conducive to auto-generated APIs or preventing binding bugs. Dynamic calls are string names that when invoked from inside the script will call a std::function on the host side, outside of the script. An example: Script::set_dynamic_call(\"void sys_lazy ()\", [] (auto&) { strf::to(stdout)(\"I'm not doing much, tbh.\");}); Will assign the function to the dynamic call \"void sys_lazy ()\", which is done to help minimize ABI mistakes like passing wrong argument types. To call this function from inside the script programs we have to amend dynamic_calls.json, like so: \"lazy\": \"void sys_lazy ()\" The build system will see the JSON changed and rebuild some API files (see generate.py), and it will expose the callable function sys_lazy: sys_lazy(); A slightly more complex example, where we take an integer as argument, and return an integer as the result: Script::set_dynamic_call(\"object_id\",[] (Script& script) { const auto [id] = script.args(); strf::to(stdout)(\"Object ID: \", id, \"\"); script.machine().set_result(1234);}); Or, let's take a struct by reference or pointer: Script::set_dynamic_call(\"struct_by_ref\",[] (Script& script) { struct Something {int a, b, c, d; }; const auto [s] = script.args(); strf::to(stdout)(\"Struct A: \", s.a, \"\");}); Also, let's take a char* buffer, size_t length pair as argument: Script::set_dynamic_call(\"void sys_big_data (const char*, size_t)\",[] (Script& script) { // A Buffer is a general-purpose container for fragmented virtual memory. // Theconsumes two registers (A0: pointer, A1: length). const auto [buffer] = script.args(); handle_buffer(buffer.to_string()); // Or, alternatively (also consumes two registers): const auto [view] = script.args(); handle_buffer(view);}); In this case we would add this to dynamic_calls.json: \"big_data\": \"void sys_big_data (const char*, size_t)\" See also memory.hpp for a list of helper functions, each with a specific purpose. The helper functions exist to simplify string and memory operations. Other examples Dynamic calls Event callbacks Event loop unit test Development notice This repository is still under development and will change over time. There are no stable APIs currently.",
    "commentLink": "https://news.ycombinator.com/item?id=38993552",
    "commentBody": "Fast RISC-V-based scripting back end for game engines (github.com/fwsgonzo)135 points by fwsgonzo 14 hours agohidepastfavorite70 comments dataangel 13 hours agoThis was posted before and I still have no idea what the rationale is. No desktop PC or game console is RISC-V, so if I'm going to all the trouble to use a scripting solution that requires me to compile my scripts to machine language, why would I target RISC-V? Why wouldn't I just compile to x86-64 directly? Like, what is the vision here, a LuaJIT that targets RISC-V, running inside my x86-64 game, where the emulator translates the RISC-V back into x86-64... for reasons? Just isolation? reply doctorpangloss 11 hours agoparent> ...for reasons? Yes. When something's intellectually stimulating you work on it more, that's it. Most game development is a grind. These huge distractions, like a scripting backend, well if you spend 100h working on the scripting engine only to spend 1h authoring actual scripts, you still spent 1h authoring scripts those 2 weeks instead of 0h. The game gets delivered sooner even if you spend 10x as long working on it. It's a quintessential misunderstanding about indie game development. HN readers think Jonathan Blow is wasting his time writing a whole new programming language and engine, and that's why his games take 6 years to make. No: his games would take 20 years to make if they weren't intellectually engaging to make. They wouldn't be made at all! It's the same energy as rewriting everything in another programming language. I think this happens at giant companies too, all the time. So called Not Invented Here syndrome: it's as much about laundering open source code as it is about keeping things interesting enough to make the extreme boredom and grind worth it for otherwise smart and healthy people. reply azakai 3 hours agoparentprevThe project explains that the reason is latency. It is indeed true that most VMs, including JavaScript, WebAssembly, the JVM, etc., have significant latency on the boundary. For example, many wasm VMs will install a signal handler, and that needs to be enabled/disabled on each entry/exit from the VM. But the latency can be fixed. You can sandbox WebAssembly without a signal handler, in particular (at the cost of a few % throughput overhead for bounds checks). I'm not sure if the author benchmarked that, but it should be very fast. (As for \"why RISC-V\" in the project, it looks like that's because it's easy to write an interpreter for, but it could have been any compiler target, it seems.) reply fwsgonzo 2 hours agorootparentThere's also latencies for arguments into and out from the emulators. I'm not sure the custom signal handling alone can account for that. I agree that it could as well have been MIPS or ARM. reply sspiff 2 hours agorootparentI think there are a couple of reasons for picking RISC-V over others. It is the youngest of the lot, meaning it is carrying around the least legacy compatibility stuff while still incorporating a lot of modern design lessons. Additionally, modern ARM and MIPS are not free and (potentially?) patent encumbered as well. You could go with some old, free MIPS or ARM, but then how good is performance, and how good is compiler support for these nowadays? reply fwsgonzo 13 hours agoparentprevThe sandbox is interpreting RISC-V, just very quickly. It's platform independent. I am actually making a game with a derivative of this repo, and in the server I am building the programs at the same time as the server. Once the server starts, it loads all the programs, and then sends compressed programs to each client, so that everyone who connects has the same scripts. Easy and convenient, but most likely just that because I did it from the start. reply adastra22 4 hours agorootparentOk… so why not send llvm bytecode? Or JVM? Or JavaScript? Or WebAssembly? Any of those would be better supported and have a faster, battle tested JIT engine. reply mort96 2 hours agorootparentMan it's so frustrating as an author to explain you you didn't just use existing technology X in the linked article but then be met by a flood of HN comments from people asking \"but why didn't you just use existing technology X?\" reply phero_cnstrcts 1 hour agorootparentprevFor the same reasons that I don’t write my websites in React. reply speps 12 hours agorootparentprevThis is an interesting concept, getting the gameplay logic sent to the client this way. I guess it only works for simpler games so far, do you have any code examples? reply fwsgonzo 11 hours agorootparentThe game is quite complex actually, but the script is not doing overmuch right now. The script is doing things that makes sense for a growing modding API, while the engine still does the brunt of the work. That said, there are functions that end up being called billions of times simply because you want that flexibility, and that's where the low latency script pays off. reply nagisa 13 hours agoparentprevThe answer lies in the reason why you would use an embedded scripting language/environment at all, over just loading native code plugins – sandboxing, fault isolation and such. RISC-V seems to be used here more as a bytecode format of sorts, and at least compared to x86_64 it should be much easier to implement (first of all because all of the opcodes have the same size.) reply yjftsjthsd-h 3 hours agorootparentUsing a sandboxed bytecode VM has merit, but I would naively expect wasm to be far better at it since RISC-V was designed for hardware and wasm was designed for exactly this usecase (sandboxed code running in another application with good performance and isolation). (Although... I do see value in having a second option. I guess it might be worth seeing if it can actually be better, even though it's not the thing I would have written) reply MobiusHorizons 6 hours agorootparentprevI mean sure, but virtualization is a very robust (and much lower overhead) technology that is readily available (and supported by the processor). If you just want isolation, why wouldn't you use that? You still have to ship a compiled binary after all. reply eru 2 hours agorootparentIf you just want to run some CPU bound code as fast as possible, virtualisation has low overhead. But if you are doing a lot of communication between the script and the host system, I'm not sure virtualisation does so well? reply fwsgonzo 2 hours agorootparentNo, as the author of TinyKVM, the overheads of entering and leaving KVM is not the same order of magnitude as SFI. TinyKVM requires 2-3 microseconds to enter and leave, which is the lowest I ever managed to get while it still being robust and dependable sandbox. For some, that might seem really low, but keep in mind that libriscv which is the backbone of RVScript has 3 nanoseconds overhead. reply crq-yml 7 hours agoparentprevAt large scale, game engines have relatively unsatisfying answers to \"what's the best way to iterate quickly on the game while supporting all the features we need\". If the scope is small, the answer is easy: write some naive code, incrementally profile. The project isn't big so full rebuild times can stay light, and your team isn't large so you really can \"do whatever\" and get somewhere, especially if you take the route of writing in a native language that compiles fast - a Pascal or something more \"new and hip\" like Beef. But when you are asked to do it on a AAA project you end up with every imaginable kind of feature: it needs to support a team of hundreds, it needs to be fast on console hardware, it needs to be straightforward to debug, it needs to support modding, it needs to be fast to iterate on, it needs to be flexible about the memory layouts of save data or assets. So you do end up in this kind of space where you're like, \"we'll target a VM that is really low level, and that'll reduce the friction and granularity of switching between debug and release profiles, and that lets us stay in control of when we want to sandbox and when we want to go fast and we'll still be able to control every byte\". That it happens to be RISC-V is not super relevant - it could be WASM or a custom bytecode like the Hashlink target in Haxe. It just needs to be a thing to compile to, that you can reasonably expect to implement and maintain. It's not the only approach that could be taken. Downscoping the ambition by 50% and hardcoding a little more of your spec is a good way to get through the technical stuff 10x faster. reply mort96 2 hours agoparentprevThe readme literally explains why the author doesn't want to use LuaJIT though? reply saagarjha 12 hours agoparentprevYeah, it seems like WASM would be a better choice? reply h0l0cube 7 hours agorootparentRationale from the README: > Lua, Luau and even LuaJIT have fairly substantial overheads when making function calls into the script, especially when many arguments are involved. The same is true for WebAssembly emulators that I have measured, eg. wasmtime. reply MaxBarraclough 11 hours agorootparentprevOr transpiling to C/C++, like Nim. reply zozbot234 10 hours agoparentprevRISC-V is quite fast as far as whole-platform emulation goes, but yes using it for plugin code is a bit weird, WASM would seem to be preferable for that use case. The interfacing story of WASM is not quite perfect (you're limited to a C-like baseline API/ABI, since the WASM components standards is not finished yet) but it's not like RISC-V is any better. reply fwsgonzo 10 hours agorootparentThis is completely ignoring that RVScript has really nice APIs both ways. While being lower latency. reply ur-whale 3 hours agorootparentprev> WASM would seem to be preferable for that use case. Doesn't he explicitly address WASM as an unfit backend in the article ? reply snvzz 9 hours agoparentprevI wonder whether they've accounted for running it on actual RISC-V hardware. It could definitely still emulate itself, but it'd be far higher performing to run the code directly in some sort of sandbox. reply kouteiheika 2 hours agoprevNice to see another RISC-V based VM! I'm working on something similar, but instead of optimizing for low call latency I'm prioritizing security, execution performance and compilation speed. I'm currently at roughly the same execution speed as wasmtime, but with over 200x faster compilation, and with significantly better security sandboxing. RISC-V (well, a slightly modified variant) actually makes for a really good VM bytecode! reply SotCodeLaureate 13 hours agoprevRecently I wrote a simplistic RISC-V interpreter for educational purposes mostly, and a friend was benchmarking it against several scripting system in the context of game-character control routines. Well, it's quite fast for what it is, though Lua is still somewhat faster, heh. Some benchmarking code is here: https://github.com/glebnovodran/roam_bench reply fwsgonzo 13 hours agoparentVery cool! One tip: Take the execute segment and produce fast bytecodes instead of interpreting each instruction as a bit pattern. Producing faster bytecodes is something that even WASM emulators do, despite it being a bytecode format. reply SotCodeLaureate 13 hours agorootparentThanks! Yeah, this one is supposed to be a very simple implementation for a kind of \"how to write a machine code interpreter\" tutorial, but I was looking to experiment with some optimizations if time permits. Patching with pre-baked alt-bytecode was one of the ideas indeed. reply FPGAhacker 13 hours agorootparentprevWhat’s the difference between a bit pattern and a bytecode? reply fwsgonzo 13 hours agorootparentThe fast bytecode is reduced to a simple operation that excludes certain knowns. For example if you have an instruction that stores A0 = A1 + 0, then knowing the immediate is zero, this can be reduced from reading a complex bit pattern to a bytecode that moves from one register to another, basically MV dst=A0, src=A1. reply logicprog 13 hours agoprevAs someone who's working on a game engine that's designed to be highly data-driven, with only the core game engine itself in a systems programming language and a lot of calls out to scripts, this is very intriguing! C++20 doesn't seem like a good candidate for an actual scripting language, so I'd have to find a suitable scripting language that can compile to RISC-V, though, and of course requiring precompilation is an issue. reply fwsgonzo 12 hours agoparentYes, precompilation is not optional. I've used many languages in sandboxes over the years developing both this and other emulators. I really like Nim the most. It's Python spiritually, but with types. Types just really really necessary, and it does have FFI support so you can forward arguments either from a wrapper or use {.cdecl} directly. It's still not a barneskirenn (as we say in Norway), but definitely the most fun I've had using another language in my various sandboxes. Nelua and Zig are close seconds. Both are just so easy to work with C-based FFI. Nelua is probably not safe to use, as it's still a work in progress. At least last I checked. Zig is very much ready to use. Maybe not your cup of tea, though. Golang has a complex run-time and I don't recommend using it. It's definitely possible though as my sandbox does have a full MMU. Just expect integration to be long and ardous. And the ABI changes potentially every version. Rust is one of the easier ones. I didn't find it fun to work with though. It has by far the best inline assembly, but too much fighting with the compiler. I know that people love Rust, and it is fully supported in my emulator. C/C++ has the benefit of having the ability to have their underlying functions overridden by native helper system calls. Eg. replacing memcpy() with a system call that has native performance. It's too long a long topic to talk about here, but Nim and Nelua also falls under this umbrella along with other languages that can compile to C/C++. Kotlin is definitely possible to use. A bit hard to understand how the native stuff actually works, but I did manage to run a hello world program in several sandboxes with some effort. I'm not 100% sure but I think I managed to convert a C API header directly to something kotlin understands using a one-liner in the terminal. I would say it scores high just on that. Again, just a bit hard to understand how to talk to Kotlin from an external FFI looking in. JavaScript is of course possible with both jitless v8 and QuickJS. v8 requires writing some C++ scaffolding + host API functions, and QuickJS requires the same in C. Either works. And I think that's all the languages I have tried. If you think there's a missing language here, then I would love to try it! reply logicprog 12 hours agorootparentThis is a really interesting list, thank you for replying! (the following are entirely undirected musings on using your technology for my game engine, mostly in case anyone finds them interesting or has something to suggest that I haven't thought of since I'm new at this) At least for my use case C, Zig, Rust, and even probably Nim are out of the question because I'm not just using scripting for sandboxing capabilities and such, I'm also using it because I want people to be able to program games and write mods in a high-level language, so using a systems programming language as my scripting language kind of feels like it defeats part of the purpose and I might as well just use dynamic linking with a C ABI or something crazy like that (I'm new to this so excuse me if that's nonsense lol). JavaScript and Kotlin are intriguing though, because they aren't systems programming languages, so I'll have to think long and hard about those; I've been considering C# for scripting lately (because I like it well enough, it's widespread, and known in the gaming world) and I wonder if you can get natively aot compiled Kotlin to work, if you could get natively aot compiled C# to work too... there would probably be similar complications due to the large and complex runtime, but I know you can strip it down so I wonder how that might play in. I also wonder what the performance would be like compared to just hosting the dotnet runtime, which is what I was intending to do before I saw your post. Maybe at some point I should set up a benchmark to compare! Although I have to say the pre-compilation thing is a bit of a deal-breaker for me, since I really want people to be able to put plain text things directly in the game folder and see those changes in the engine without having to do any kind of build process, which is something that is achievable with the regular dotnet runtime using a precompiled assembly bootstraps the rest of the sctipts using dynamic assembly compilation and loading to collect everything else in the script directory. reply fwsgonzo 11 hours agorootparentHm, if you don't actually need a sandbox then I think just using the C# run-time makes a lot of sense. I also like C#, but I've never been in a situation to try the fairly new AOT support. Sounds like a good idea, though. C# is a very good language. reply brucehoult 9 hours agorootparentC# runtime is HUGE and not ported to everything. The compilation step to native code takes a significant amount of time. reply Rohansi 8 hours agorootparentDo you mean compiling the .NET runtime (CoreCLR) itself or AOT compiling C# code? Because I recently tried AOT compiled C# and that builds quickly. And you don't generally need to compile the .NET runtime when embedding it because you can just load the builds they ship. reply brucehoult 6 hours agorootparentSimply running a DotNET dll. I just tried a basic \"Hello World\" program compiled to a CIL .dll and to RV64 machine code, on my x86 Linux machine (original ThreadRipper 2990WX) and then running it through different JIT/interpreters. Wall time: DotNET: 0.061s qemu-riscv64: 0.006s Spike: 0.031s Qemu is JIT with Linux syscall layer built in (in native code). Spike is a RISC-V interpreter with Linux syscall layer provided by interpreted RISC-V \"pk\" So the DotNET JIT has quite a high overhead for startup, or one-time code. For very compute-intensive code DotNet has an advantage. e.g. on my own primes benchmark (https://hoult.org/primes.txt, https://hoult.org/primes.cs) DotNET: 3.5s qemu-riscv64: 10.2s gcc: 9.7s (i.e. defaulting to -O0) gcc -O1: 3.2s DotNET beats emulated RISC-V here, but the RISC-V emulator (with RISC-V code compiled with -O1) is pretty much as fast as a lazy person compiling C to native x86 gets. reply Rohansi 4 hours agorootparentOh, so you meant startup time for the runtime, JIT, etc. then. That's not exactly relevant to usage in game engines here because it is only done once. If you want to compare startup times you should be using .NET's AOT compilation so it doesn't need to load the full runtime or do any JIT compilation. See here: https://learn.microsoft.com/en-us/dotnet/core/deploying/nati... reply logicprog 8 hours agorootparentprevGood to know. That solidifies my plan to embed the dotnet runtime in my engine and load JIT code, not compile C# to AOT and then run that through an emulation layer reply lmm 3 hours agorootparentprevI'd be interested to see Scala (native). reply stefanha 10 hours agoprevIs there a benchmark available where you compared Lua and wasmtime against your interpreter? It would be interesting for Lua and wasmtime developers to chime in on why the world switch overhead is higher and whether there exist configuration settings or APIs that eliminate the overhead for your use case. I'm curious if there is any fundamental reason why they have to be slower and I suspect the answer is \"no\". reply fwsgonzo 9 hours agoparentBinary translated libriscv vs LuaJIT: https://gist.github.com/fwsGonzo/9132f0ef7d3f009baa5b222eedf... Interpreted libriscv vs LuaJIT: https://gist.github.com/fwsGonzo/1af5b2a9b4f38c1f3d3074d78ac... libriscv vs. Luau: https://gist.github.com/fwsGonzo/5ac8f4d8ca84e97b0c527aec76a... wasmtime: https://medium.com/@fwsgonzo/using-c-as-a-scripting-language... You're probably right that things can be improved. But my implementation has the aforementioned latencies right now. For the benchmarks I did my best writing good Lua, but I am not a pro. Call overhead is also subtracted out of every benchmark after call overhead is measured. So that extra overhead is a permanent feature of these implementations. I also noticed that most of my script functions have at least ~3-4 arguments, which seems like it would be quite costly. Just look at the 8x arg benchmark and divide by 2, I guess. Another 100ns right there. Baseline 150ns on a micro benchmark is going to become at least a microsecond in prod when everything is random and semi-cold. reply stefanha 9 hours agorootparentCool, thanks for sharing this information! I'm not a Lua or wasmtime developer, but I hope to learn from whatever discussion follows. reply zamalek 9 hours agoprev> The same is true for WebAssembly emulators that I have measured, eg. wasmtime. Wait, so this claims that it (an interpreter from a cursory akim of the code) is faster than a compiler? reply azakai 3 hours agoparentNo, it says that that latency of calling into the VM can be lower in a simple interpreter compared to an optimizing VM (which might need to do things on the call boundary). reply yellowapple 9 hours agoprevLicense? reply hammyhavoc 14 hours agoprevGenius. reply thesnide 13 hours agoparentYep, I also thought about it to replace wasm to enable secure server side code execution. https://blog.pwkf.org/2023/07/16/lambda-mcu.html reply hammyhavoc 13 hours agorootparentThis is extremely cool! reply charcircuit 13 hours agoprevWhen writing a web assembly interpreter I was surprised at how it didn't map cleanly to the underlying hardware. The interpreter had to keep track of the types of everything. I can definitely see how a RISC-V based solution would be faster. reply klodolph 13 hours agoparentI’m curious what you’re talking about—WASM has different instructions for each of its different types. There are separate instructions for i32, i64, f32, and f64 in the base spec. This seems like a pretty clean map to me—most of the architectures these days have native support for 32-bit and 64-bit types these days. You only get 16-bit and 8-bit support with SIMD or load/store, much like how WASM does it. reply fwsgonzo 13 hours agorootparentHe might be talking about having to implement a register allocator/file in order to interpret stack machines faster. Something you get from the compilers on register architectures like RISC-V, ARM and x86. wasmtime uses cranelift, which has a complex register allocator. wasm3 uses a simple register file. > In M3/Wasm, the stack machine model is translated into a more direct and efficient \"register file\" approach. https://github.com/wasm3/wasm3/blob/main/docs/Interpreter.md reply miohtama 12 hours agorootparentBecause registers on x86 and RISC-V are different, you still need to have a register file to map those, no? reply fwsgonzo 11 hours agorootparentOnly for binary translation or just-in-time compilation (basically producing native code). If you're interpreting RISC-V you can just pretend the registers are an array of register-sized integers. Which is what they are. reply charcircuit 11 hours agorootparent>registers are an array of register-sized integers A \"register file\" is the name of such an array. reply Findecanor 13 hours agorootparentprevWASM is a stack-machine, so the input code needs to be validated to make sure that the same types gets pop'ed as has previously been push'ed. But that can be done ahead of execution. You'd need to validate function types during runtime though, which is something that a CPU emulator does not need to. reply klodolph 12 hours agorootparentWASM is kind of a stack machine, yes, but there are a lot of constraints on the stack machine that are designed to make it easy to translate to registers. If you squint and look sideways, you can think of it as a flattened tree, rather than a stack machine. reply snvzz 9 hours agorootparentprevIn contrast, RISC-V would be considered a register-based VM here. reply dkjaudyeqooe 13 hours agorootparentprevI'm not familiar with the details, and don't have the references to hand, but WASM has been designed in a way that reflects the design choices of Chrome's Javascript engine rather than optimizing standalone compilation. reply klodolph 12 hours agorootparentPart of optimizing for the design choices of browser engines is to make it easy to emit machine code. The browsers have multiple compilers in them, and one of the compilers has the task of emitting simple machine code quickly, so the code can run immediately (reducing start-up time). In V8, this initial compiler is called Ignition. reply coldtea 8 hours agorootparentprev>WASM has been designed in a way that reflects the design choices of Chrome's Javascript engine Wasn't it based on previous work by the Mozilla team? reply charcircuit 13 hours agorootparentprevTake for example i32.add you have to 1. Check the type at the top of the stack to make sure it's i32 and trap otherwise 2. Pop the top of the stack 3. Check the top of the stack and make sure it's i32 and trap otherwise 4. Pop the top of the stack. 5. Add the two values together 6. Push the result to the top of the stack along with its type. Compare this to ADD from RISC-V where you can just add the contents of 2 source registers and store it in a destination register. There is not a bunch of type book keeping or alignment that you have to worry about. reply klodolph 12 hours agorootparentMost of this can be done statically. You do the bookkeeping once. There are various approaches—you can try to allocate registers, or you can emit code that loads/stores on the stack, or you can just run an interpreter and remove the safety checks from it. The stack isn’t dynamically typed, so it doesn’t make sense to trap. reply charcircuit 12 hours agorootparentLooking into it further the type checking is supposed to be done in a dedicated validation step and other than validation using instructions with the wrong type of data seems to work. reply sylware 12 hours agoprevFunny, I just started (yesterday!!) something similar: a kind of x86_64 (which I call x64) virtual machine for rv64. I am writting this \"virtual machine\" in x86_64 assembly though (linux ABI). I don't plan to have native x86_64 compilation but only an interpreter. This idea is to code rv64 executables(linux) which I could \"run-ish\" on x86_64(linux). There is also the real machine emulator from M.Bellard : https://bellard.org/tinyemu Well, RISC-V is gaining momentum, and transition apparatus from x86_64 is taking shaped from many individuals wishing risc-v to be a success. BTW, if anybody knows about milk-v duo (the one with the SOC free from arm cores) resellers in Europe I can pay without a credit card and I can contact with a self-hosted email... thank you. reply sitkack 10 hours agoparentThat sounds exciting, please keep us posted. reply FpUser 12 hours agoprev [–] Long time ago I've used paxCompiler scripting engine. It was made for Delphi / FreePascal and supported Delphi input language. Rather than being byte code interpreter / JIT it actually generated native code in RAM hence the overhead of calling it was the same as calling regular function. It had lots of other super nice features but that is all in the past anyways. I am curious why such approach is not used for other scripting projects. reply SpaghettiCthulu 8 hours agoparent [–] > Rather than being byte code interpreter / JIT it actually generated native code in RAM hence the overhead of calling it was the same as calling regular function. I'm not sure what you mean. What you've just described is a JIT compiler. reply FpUser 5 hours agorootparent [–] Not in a way V8 does it for example. I would call it AOT (ahead of time). Normally you first compile and initialize scripts and then you can call into their functions and the other way around. But sure you can also compile and execute function in one step so be my guest and call it whatever you want. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "RVScript is a game engine scripting system that utilizes a low latency RISC-V sandbox for a type-safe and memory-safe script.",
      "Its goal is to enhance the performance and minimize the overhead of function calls in game scripting.",
      "The project includes examples and benchmarks for implementing advanced techniques to optimize script execution, and can be used with a RISC-V compiler to support dynamic calls for adding new functionality. However, stable APIs are not yet available as the system is still in development."
    ],
    "commentSummary": [
      "The GitHub discussion focuses on using RISC-V as a scripting back end for game engines, with the author highlighting benefits such as intellectual stimulation and shorter development time.",
      "Concerns about potential latency issues and alternative compiler targets are raised by other participants in the discussion.",
      "The choice of RISC-V is attributed to its lack of legacy compatibility and potential patent restrictions with other architectures, while virtualization and sandboxing technologies for CPU-bound code are explored.",
      "Comparisons are made between scripting languages like Lua and LuaJIT, WebAssembly, and transpiling to C/C++ in terms of performance.",
      "The implementation of a machine code interpreter using RISC-V is discussed, along with considerations for compiling to RISC-V using languages like Nim, Nelua, Zig, Rust, and Kotlin.",
      "The use of C/C++ and high-level languages like JavaScript, Kotlin, and C# for game engine scripting is also covered, focusing on performance and compatibility.",
      "The potential use of RISC-V for secure server-side code execution, challenges of interpreting WebAssembly, and the implementation of a register allocator/file for interpreting stack machines are also topics of discussion.",
      "Differences between stack machines and register machines, as well as optimization of WebAssembly for browser engines, are briefly mentioned.",
      "The discussion briefly touches on the process of adding values in a RISC-V architecture and the increasing popularity of RISC-V."
    ],
    "points": 135,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1705260286
  },
  {
    "id": 38990709,
    "title": "Implementing Attention Mechanisms in Python and PyTorch for Transformer Architectures and Language Models",
    "originLink": "https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention",
    "originBody": "Share this post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs magazine.sebastianraschka.com Copy link Facebook Email Note Other Discover more from Ahead of AI Ahead AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field. Over 45,000 subscribers Subscribe Continue reading Sign in Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs Jan 14, 2024 142 Share this post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs magazine.sebastianraschka.com Copy link Facebook Email Note Other 9 Share This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models. However, rather than just discussing the self-attention mechanism, we will code it in Python and PyTorch from the ground up. In my opinion, coding algorithms, models, and techniques from scratch is an excellent way to learn! As a side note, this article is a modernized and extended version of \"Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch,\" which I published on my old blog almost exactly a year ago. Since I really enjoy writing (and reading) 'from scratch' articles, I wanted to modernize this article for Ahead of AI. Additionally, this article motivated me to write the book Build a Large Language Model (from Scratch), which is currently in progress. Below is a mental model that summarizes the book and illustrates how the self-attention mechanism fits into the bigger picture. An overview of the topics covered in the Build a Large Language Model (from Scratch) book To keep the length of this article somewhat reasonable, I'll assume you already know about LLMs and you also know about attention mechanisms on a basic level. The goal and focus of this article is to understand how attention mechanisms work via a Python & PyTorch code walkthrough. Introducing Self-Attention Since its introduction via the original transformer paper (Attention Is All You Need), self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing (NLP). Since self-attention is now everywhere, it's important to understand how it works. The original transformer architecture from https://arxiv.org/abs/1706.03762 The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical structures and idiomatic expressions unique to each language, leading to inaccurate or nonsensical translations. An incorrect word-by-word translation (top) compared to a correct translation (bottom) To overcome this issue, attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context. In 2017, the transformer architecture introduced a standalone self-attention mechanism, eliminating the need for RNNs altogether. (For brevity, and to keep the article focused on the technical self-attention details, I am keeping this background motivation section brief so that we can focus on the code implementation.) A visualization from the “Attention is All You Need” paper (https://arxiv.org/abs/1706.03762) showing how much the word “making” depends or focuses on other words in the input via attention weights (the color intensity is proportional the attention weight value). We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input's context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. Note that there are many variants of self-attention. A particular focus has been on making self-attention more efficient. However, most papers still implement the original scaled-dot product attention mechanism introduced in the Attention Is All You Need paper since self-attention is rarely a computational bottleneck for most companies training large-scale transformers. So, in this article, we focus on the original scaled-dot product attention mechanism (referred to as self-attention), which remains the most popular and most widely used attention mechanism in practice. However, if you are interested in other types of attention mechanisms, check out the 2020 Efficient Transformers: A Survey, the 2023 A Survey on Efficient Training of Transformers review, and the recent FlashAttention and FlashAttention-v2 papers. Embedding an Input Sentence Before we begin, let's consider an input sentence \"Life is short, eat dessert first\" that we want to put through the self-attention mechanism. Similar to other types of modeling approaches for processing text (e.g., using recurrent neural networks or convolutional neural networks), we create a sentence embedding first. For simplicity, here our dictionary dc is restricted to the words that occur in the input sentence. In a real-world application, we would consider all words in the training dataset (typical vocabulary sizes range between 30k to 50k entries). In: sentence = 'Life is short, eat dessert first' dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))} print(dc) Out: {'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5} Next, we use this dictionary to assign an integer index to each word: In: import torch sentence_int = torch.tensor( [dc[s] for s in sentence.replace(',', '').split()] ) print(sentence_int) Out: tensor([0, 4, 5, 2, 1, 3]) Now, using the integer-vector representation of the input sentence, we can use an embedding layer to encode the inputs into a real-vector embedding. Here, we will use a tiny 3-dimensional embedding such that each input word is represented by a 3-dimensional vector. Note that embedding sizes typically range from hundreds to thousands of dimensions. For instance, Llama 2 utilizes embedding sizes of 4,096. The reason we use 3-dimensional embeddings here is purely for illustration purposes. This allows us to examine the individual vectors without filling the entire page with numbers. Since the sentence consists of 6 words, this will result in a 6×3-dimensional embedding: In: vocab_size = 50_000 torch.manual_seed(123) embed = torch.nn.Embedding(vocab_size, 3) embedded_sentence = embed(sentence_int).detach() print(embedded_sentence) print(embedded_sentence.shape) Out: tensor([[ 0.3374, -0.1778, -0.3035], [ 0.1794, 1.8951, 0.4954], [ 0.2692, -0.0770, -1.0205], [-0.2196, -0.3792, 0.7671], [-0.5880, 0.3486, 0.6603], [-1.1925, 0.6984, -1.4097]]) torch.Size([6, 3]) Defining the Weight Matrices Now, let's discuss the widely utilized self-attention mechanism known as the scaled dot-product attention, which is an integral part of the transformer architecture. Self-attention utilizes three weight matrices, referred to as Wq, Wk, and Wv, which are adjusted as model parameters during training. These matrices serve to project the inputs into query, key, and value components of the sequence, respectively. The respective query, key and value sequences are obtained via matrix multiplication between the weight matrices W and the embedded inputs x: Query sequence: q(i) = x(i)Wq for i in sequence 1 … T Key sequence: k(i) = x(i)Wk for i in sequence 1 … T Value sequence: v(i) = x(i)Wv for i in sequence 1 … T The index i refers to the token index position in the input sequence, which has length T. Computing the query, key, and value vectors via the input x and weights W. Here, both q(i) and k(i) are vectors of dimension dk. The projection matrices Wq and Wk have a shape of d × dk , while Wv has the shape d × dv . (It's important to note that d represents the size of each word vector, x.) Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements (dq = dk). In many LLMs, we use the same size for the value vectors such that dq = dk = dv. However, the number of elements in the value vector v(i), which determines the size of the resulting context vector, can be arbitrary. So, for the following code walkthrough, we will set dq = dk = 2 and use dv = 4, initializing the projection matrices as follows: In: torch.manual_seed(123) d = embedded_sentence.shape[1] d_q, d_k, d_v = 2, 2, 4 W_query = torch.nn.Parameter(torch.rand(d, d_q)) W_key = torch.nn.Parameter(torch.rand(d, d_k)) W_value = torch.nn.Parameter(torch.rand(d, d_v)) (Similar to the word embedding vectors earlier, the dimensions dq, dk, dv are usually much larger, but we use small numbers here for illustration purposes.) Computing the Unnormalized Attention Weights Now, let's suppose we are interested in computing the attention vector for the second input element -- the second input element acts as the query here: For the following sections below, we focus on the second input, x(2) In code, this looks like as follows: In: x_2 = embedded_sentence[1] query_2 = x_2 @ W_query key_2 = x_2 @ W_key value_2 = x_2 @ W_value print(query_2.shape) print(key_2.shape) print(value_2.shape) Out: torch.Size([2]) torch.Size([2]) torch.Size([4]) We can then generalize this to compute the remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights later: In: keys = embedded_sentence @ W_key values = embedded_sentence @ W_value print(\"keys.shape:\", keys.shape) print(\"values.shape:\", values.shape) Out: keys.shape: torch.Size([6, 2]) values.shape: torch.Size([6, 4]) Now that we have all the required keys and values, we can proceed to the next step and compute the unnormalized attention weights ω (omega), which are illustrated in the figure below: Computing the unnormalized attention weights ω (omega) As illustrated in the figure above, we compute ωi,j as the dot product between the query and key sequences, ωi,j = q(i) k(j). For example, we can compute the unnormalized attention weight for the query and 5th input element (corresponding to index position 4) as follows: In: omega_24 = query_2.dot(keys[4]) print(omega_24) (Note that ω is the symbol for the Greek letter \"omega\", hence the code variable with the same name above.) Out: tensor(1.2903) Since we will need those unnormalized attention weights ω to compute the actual attention weights later, let's compute the ω values for all input tokens as illustrated in the previous figure: In: omega_2 = query_2 @ keys.T print(omega_2) Out: tensor([-0.6004, 3.4707, -1.5023, 0.4991, 1.2903, -1.3374]) Computing the Attention Weights The subsequent step in self-attention is to normalize the unnormalized attention weights, ω, to obtain the normalized attention weights, α (alpha), by applying the softmax function. Additionally, 1/√{dk} is used to scale ω before normalizing it through the softmax function, as shown below: Computing the normalized attention weights α The scaling by dk ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude. This helps prevent the attention weights from becoming too small or too large, which could lead to numerical instability or affect the model's ability to converge during training. In code, we can implement the computation of the attention weights as follows: In: import torch.nn.functional as F attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0) print(attention_weights_2) Out: tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229]) Finally, the last step is to compute the context vector z(2), which is an attention-weighted version of our original query input x(2), including all the other input elements as its context via the attention weights: The attention weights are specific to a certain input element. Here, we chose input element x(2). In code, this looks like as follows: In: context_vector_2 = attention_weights_2 @ values print(context_vector_2.shape) print(context_vector_2) Out: torch.Size([4]) tensor([0.5313, 1.3607, 0.7891, 1.3110]) Note that this output vector has more dimensions (dv = 4) than the original input vector (d = 3) since we specified dv > d earlier; however, the embedding size choice dv is arbitrary. Self-Attention Now, to wrap up the code implementation of the self-attention mechanism in the previous sections above, we can summarize the previous code in a compact SelfAttention class: In: import torch.nn as nn class SelfAttention(nn.Module): def __init__(self, d_in, d_out_kq, d_out_v): super().__init__() self.d_out_kq = d_out_kq self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq)) self.W_key = nn.Parameter(torch.rand(d_in, d_out_kq)) self.W_value = nn.Parameter(torch.rand(d_in, d_out_v)) def forward(self, x): keys = x @ self.W_key queries = x @ self.W_query values = x @ self.W_value attn_scores = queries @ keys.T # unnormalized attention weights attn_weights = torch.softmax( attn_scores / self.d_out_kq**0.5, dim=-1 ) context_vec = attn_weights @ values return context_vec Following PyTorch conventions, the SelfAttention class above initializes the self-attention parameters in the __init__ method and computes attention weights and context vectors for all inputs via the forward method. We can use this class as follows: In: torch.manual_seed(123) # reduce d_out_v from 4 to 1, because we have 4 heads d_in, d_out_kq, d_out_v = 3, 2, 4 sa = SelfAttention(d_in, d_out_kq, d_out_v) print(sa(embedded_sentence)) Out: tensor([[-0.1564, 0.1028, -0.0763, -0.0764], [ 0.5313, 1.3607, 0.7891, 1.3110], [-0.3542, -0.1234, -0.2627, -0.3706], [ 0.0071, 0.3345, 0.0969, 0.1998], [ 0.1008, 0.4780, 0.2021, 0.3674], [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=) If you look at the second row, you can see that it matches the values in context_vector_2 from the previous section exactly: tensor([0.5313, 1.3607, 0.7891, 1.3110]). Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. Subscribe Multi-Head Attention In the very first figure, at the top of this article (also shown again for convenience below), we saw that transformers use a module called multi-head attention. The multi-head attention modules in the original transformer architecture from https://arxiv.org/abs/1706.03762 How does this \"multi-head\" attention module relate to the self-attention mechanism (scaled-dot product attention) we walked through above? In scaled dot-product attention, the input sequence was transformed using three matrices representing the query, key, and value. These three matrices can be considered as a single attention head in the context of multi-head attention. The figure below summarizes this single attention head we covered and implemented previously: Summarizing the self-attention mechanism implemented previously As its name implies, multi-head attention involves multiple such heads, each consisting of query, key, and value matrices. This concept is similar to the use of multiple kernels in convolutional neural networks, producing feature maps with multiple output channels. Multi-head attention: self-attention with multiple heads To illustrate this in code, we can write a MultiHeadAttentionWrapper class for our previous SelfAttention class: class MultiHeadAttentionWrapper(nn.Module): def __init__(self, d_in, d_out_kq, d_out_v, num_heads): super().__init__() self.heads = nn.ModuleList( [SelfAttention(d_in, d_out_kq, d_out_v) for _ in range(num_heads)] ) def forward(self, x): return torch.cat([head(x) for head in self.heads], dim=-1) The d_* parameters are the same as before in the SelfAttention class -- the only new input parameter here is the number of attention heads: d_in: Dimension of the input feature vector. d_out_kq: Dimension for both query and key outputs. d_out_v: Dimension for value outputs. num_heads: Number of attention heads. We initialize the SelfAttention class num_heads times using these input parameters. And we use a PyTorch nn.ModuleList to store these multiple SelfAttention instances. Then, the forward pass involves applying each SelfAttention head (stored in self.heads) to the input x independently. The results from each head are then concatenated along the last dimension (dim=-1). Let's see it in action below! First, let's suppose we have a single Self-Attention head with output dimension 1 to keep it simple for illustration purposes: In: torch.manual_seed(123) d_in, d_out_kq, d_out_v = 3, 2, 1 sa = SelfAttention(d_in, d_out_kq, d_out_v) print(sa(embedded_sentence)) Out: tensor([[-0.0185], [ 0.4003], [-0.1103], [ 0.0668], [ 0.1180], [-0.1827]], grad_fn=) Now, let's extend this to 4 attention heads: In: torch.manual_seed(123) block_size = embedded_sentence.shape[1] mha = MultiHeadAttentionWrapper( d_in, d_out_kq, d_out_v, num_heads=4 ) context_vecs = mha(embedded_sentence) print(context_vecs) print(\"context_vecs.shape:\", context_vecs.shape) Out: tensor([[-0.0185, 0.0170, 0.1999, -0.0860], [ 0.4003, 1.7137, 1.3981, 1.0497], [-0.1103, -0.1609, 0.0079, -0.2416], [ 0.0668, 0.3534, 0.2322, 0.1008], [ 0.1180, 0.6949, 0.3157, 0.2807], [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=) context_vecs.shape: torch.Size([6, 4]) Based on the output above, you can see that the single self-attention head created earlier now represents the first column in the output tensor above. Notice that the multi-head attention result is a 6×4-dimensional tensor: We have 6 input tokens and 4 self-attention heads, where each self-attention head returns a 1-dimensional output. Previously, in the Self-Attention section, we also produced a 6×4-dimensional tensor. That's because we set the output dimension to 4 instead of 1. In practice, why do we even need multiple attention heads if we can regulate the output embedding size in the SelfAttention class itself? The distinction between increasing the output dimension of a single self-attention head and using multiple attention heads lies in how the model processes and learns from the data. While both approaches increase the capacity of the model to represent different features or aspects of the data, they do so in fundamentally different ways. For instance, each attention head in multi-head attention can potentially learn to focus on different parts of the input sequence, capturing various aspects or relationships within the data. This diversity in representation is key to the success of multi-head attention. Multi-head attention can also be more efficient, especially in terms of parallel computation. Each head can be processed independently, making it well-suited for modern hardware accelerators like GPUs or TPUs that excel at parallel processing. In short, the use of multiple attention heads is not just about increasing the model's capacity but about enhancing its ability to learn a diverse set of features and relationships within the data. For example, the 7B Llama 2 model uses 32 attention heads. Cross-Attention In the code walkthrough above, we set d_q = d_k = 2 and d_v = 4. In other words, we used the same dimensions for query and key sequences. While the value matrix W_v is often chosen to have the same dimension as the query and key matrices (such as in PyTorch's MultiHeadAttention class), we can select an arbitrary number size for the value dimensions. Since the dimensions are sometimes a bit tricky to keep track of, let's summarize everything we have covered so far in the figure below, which depicts the various tensor sizes for a single attention head. Another view of the self-attention mechanism implemented previously, with a focus on the matrix dimensions Now, the illustration above corresponds to the self-attention mechanism used in transformers. One particular flavor of this attention mechanism we have yet to discuss is cross-attention. What is cross-attention, and how does it differ from self-attention? In self-attention, we work with the same input sequence. In cross-attention, we mix or combine two different input sequences. In the case of the original transformer architecture above, that's the sequence returned by the encoder module on the left and the input sequence being processed by the decoder part on the right. Note that in cross-attention, the two input sequences x_1 and x_2 can have different numbers of elements. However, their embedding dimensions must match. The figure below illustrates the concept of cross-attention. If we set x_1 = x_2, this is equivalent to self-attention. (Note that the queries usually come from the decoder, and the keys and values typically come from the encoder.) How does that work in code? We will adopt and modify the SelfAttention class that we previously implemented in the Self-Attention section and only make some minor modifications: In: class CrossAttention(nn.Module): def __init__(self, d_in, d_out_kq, d_out_v): super().__init__() self.d_out_kq = d_out_kq self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq)) self.W_key = nn.Parameter(torch.rand(d_in, d_out_kq)) self.W_value = nn.Parameter(torch.rand(d_in, d_out_v)) def forward(self, x_1, x_2): # x_2 is new queries_1 = x_1 @ self.W_query keys_2 = x_2 @ self.W_key # new values_2 = x_2 @ self.W_value # new attn_scores = queries_1 @ keys_2.T # new attn_weights = torch.softmax( attn_scores / self.d_out_kq**0.5, dim=-1) context_vec = attn_weights @ values_2 return context_vec The differences between the CrossAttention class and the previous SelfAttention class are as follows: The forward method takes two distinct inputs, x_1 and x_2. The queries are derived from x_1, while the keys and values are derived from x_2. This means that the attention mechanism is evaluating the interaction between two different inputs. The attention scores are calculated by taking the dot product of the queries (from x_1) and keys (from x_2). Similar to SelfAttention, each context vector is a weighted sum of the values. However, in CrossAttention, these values are derived from the second input (x_2), and the weights are based on the interaction between x_1 and x_2. Let's see it in action: In: torch.manual_seed(123) d_in, d_out_kq, d_out_v = 3, 2, 4 crossattn = CrossAttention(d_in, d_out_kq, d_out_v) first_input = embedded_sentence second_input = torch.rand(8, d_in) print(\"First input shape:\", first_input.shape) print(\"Second input shape:\", second_input.shape) Out: First input shape: torch.Size([6, 3]) Second input shape: torch.Size([8, 3]) Notice that the first and second inputs don't have to have the same number of tokens (here: rows) when computing cross-attention: In: context_vectors = crossattn(first_input, second_input) print(context_vectors) print(\"Output shape:\", context_vectors.shape) Out: tensor([[0.4231, 0.8665, 0.6503, 1.0042], [0.4874, 0.9718, 0.7359, 1.1353], [0.4054, 0.8359, 0.6258, 0.9667], [0.4357, 0.8886, 0.6678, 1.0311], [0.4429, 0.9006, 0.6775, 1.0460], [0.3860, 0.8021, 0.5985, 0.9250]], grad_fn=) Output shape: torch.Size([6, 4]) We talked a lot about language transformers above. In the original transformer architecture, cross-attention is useful when we go from an input sentence to an output sentence in the context of language translation. The input sentence represents one input sequence, and the translation represent the second input sequence (the two sentences can different numbers of words). Another popular model where cross-attention is used is Stable Diffusion. Stable Diffusion uses cross-attention between the generated image in the U-Net model and the text prompts used for conditioning as described in High-Resolution Image Synthesis with Latent Diffusion Models -- the original paper that describes the Stable Diffusion model that was later adopted by Stability AI to implement the popular Stable Diffusion model. Causal Self-Attention In this section, we are adapting the previously discussed self-attention mechanism into a causal self-attention mechanism, specifically for GPT-like (decoder-style) LLMs that are used to generate text. This causal self-attention mechanism is also often referred to as “masked self-attention”. In the original transformer architecture, it corresponds to the “masked multi-head attention” module — for simplicity, we will look at a single attention head in this section, but the same concept generalizes to multiple heads. The causal self-attention module in the original transformer architecture (via “Attention Is All You Need”, https://arxiv.org/abs/1706.03762) Causal self-attention ensures that the outputs for a certain position in a sequence is based only on the known outputs at previous positions and not on future positions. In simpler terms, it ensures that the prediction for each next word should only depend on the preceding words. To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text. The application of a causal mask to the attention weights for hiding future input tokens in the inputs is illustrated in the figure below. To illustrate and implement causal self-attention, let's work with the unweighted attention scores and attention weights from the previous section. First, we quickly recap the computation of the attention scores from the previous Self-Attention section: In: torch.manual_seed(123) d_in, d_out_kq, d_out_v = 3, 2, 4 W_query = nn.Parameter(torch.rand(d_in, d_out_kq)) W_key = nn.Parameter(torch.rand(d_in, d_out_kq)) W_value = nn.Parameter(torch.rand(d_in, d_out_v)) x = embedded_sentence keys = x @ W_key queries = x @ W_query values = x @ W_value # attn_scores are the \"omegas\", # the unnormalized attention weights attn_scores = queries @ keys.T print(attn_scores) print(attn_scores.shape) Out: tensor([[ 0.0613, -0.3491, 0.1443, -0.0437, -0.1303, 0.1076], [-0.6004, 3.4707, -1.5023, 0.4991, 1.2903, -1.3374], [ 0.2432, -1.3934, 0.5869, -0.1851, -0.5191, 0.4730], [-0.0794, 0.4487, -0.1807, 0.0518, 0.1677, -0.1197], [-0.1510, 0.8626, -0.3597, 0.1112, 0.3216, -0.2787], [ 0.4344, -2.5037, 1.0740, -0.3509, -0.9315, 0.9265]], grad_fn=) torch.Size([6, 6]) Similar to the Self-Attention section before, the output above is a 6×6 tensor containing these pairwise unnormalized attention weights (also called attention scores) for the 6 input tokens. Previously, we then computed the scaled dot-product attention via the softmax function as follows: In: attn_weights = torch.softmax(attn_scores / d_out_kq**0.5, dim=1) print(attn_weights) Out: tensor([[0.1772, 0.1326, 0.1879, 0.1645, 0.1547, 0.1831], [0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229], [0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312], [0.1505, 0.2187, 0.1401, 0.1651, 0.1793, 0.1463], [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.1231], [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]], grad_fn=) The 6×6 output above represents the attention weights, which we also computed in the Self-Attention section before. Now, in GPT-like LLMs, we train the model to read and generate one token (or word) at a time, from left to right. If we have a training text sample like \"Life is short eat desert first\" we have the following setup, where the context vectors for the word to the right side of the arrow should only incorporate itself and the previous words: \"Life\" → \"is\" \"Life is\" → \"short\" \"Life is short\" → \"eat\" \"Life is short eat\" → \"desert\" \"Life is short eat desert\" → \"first\" The simplest way to achieve this setup above is to mask out all future tokens by applying a mask to the attention weight matrix above the diagonal, as illustrated in the figure below. This way, “future” words will not be included when creating the context vectors, which are created as a attention-weighted sum over the inputs. Attention weights above the diagonal should be masked out In code, we can achieve this via PyTorch's tril function, which we first use to create a mask of 1's and 0's: In: block_size = attn_scores.shape[0] mask_simple = torch.tril(torch.ones(block_size, block_size)) print(mask_simple) Out: tensor([[1., 0., 0., 0., 0., 0.], [1., 1., 0., 0., 0., 0.], [1., 1., 1., 0., 0., 0.], [1., 1., 1., 1., 0., 0.], [1., 1., 1., 1., 1., 0.], [1., 1., 1., 1., 1., 1.]]) Next, we multiply the attention weights with this mask to zero out all the attention weights above the diagonal: In: masked_simple = attn_weights*mask_simple print(masked_simple) Out: tensor([[0.1772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0386, 0.6870, 0.0000, 0.0000, 0.0000, 0.0000], [0.1965, 0.0618, 0.2506, 0.0000, 0.0000, 0.0000], [0.1505, 0.2187, 0.1401, 0.1651, 0.0000, 0.0000], [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.0000], [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]], grad_fn=) While the above is one way to mask out future words, notice that the attention weights in each row don't sum to one anymore. To mitigate that, we can normalize the rows such that they sum up to 1 again, which is a standard convention for attention weights: In: row_sums = masked_simple.sum(dim=1, keepdim=True) masked_simple_norm = masked_simple / row_sums print(masked_simple_norm) Out: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000], [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000], [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000], [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000], [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]], grad_fn=) As we can see, the attention weights in each row now sum up to 1. Normalizing attention weights in neural networks, such as in transformer models, is advantageous over unnormalized weights for two main reasons. First, normalized attention weights that sum to 1 resemble a probability distribution. This makes it easier to interpret the model's attention to various parts of the input in terms of proportions. Second, by constraining the attention weights to sum to 1, this normalization helps control the scale of the weights and gradients to improve the training dynamics. More efficient masking without renormalization In the causal self-attention procedure we coded above, we first compute the attention scores, then compute the attention weights, mask out attention weights above the diagonal, and lastly renormalize the attention weights. This is summarized in the figure below: The previously implemented causal self-attention procedure Alternatively, there is a more efficient way to achieve the same results. In this approach, we take the attention scores and replace the values above the diagonal with negative infinity before the values are input into the softmax function to compute the attention weights. This is summarized in the figure below: An alternative, more efficient approach to implementing causal self-attention We can code up this procedure in PyTorch as follows, starting with masking the attention scores above the diagonal: In: mask = torch.triu(torch.ones(block_size, block_size), diagonal=1) masked = attn_scores.masked_fill(mask.bool(), -torch.inf) print(masked) The code above first creates a mask with 0s below the diagonal, and 1s above the diagonal. Here, torch.triu (*(upper triangle) retains the elements on and above the main diagonal of a matrix, zeroing out the elements below it, thus preserving the upper triangular portion. In contrast, torch.tril (lower triangle) keeps the elements on and below the main diagonal. The masked_fill method then replaces all the elements on and above the diagonal via positive mask values (1s) with -torch.inf, with the results being shown below. Out: tensor([[ 0.0613, -inf, -inf, -inf, -inf, -inf], [-0.6004, 3.4707, -inf, -inf, -inf, -inf], [ 0.2432, -1.3934, 0.5869, -inf, -inf, -inf], [-0.0794, 0.4487, -0.1807, 0.0518, -inf, -inf], [-0.1510, 0.8626, -0.3597, 0.1112, 0.3216, -inf], [ 0.4344, -2.5037, 1.0740, -0.3509, -0.9315, 0.9265]], grad_fn=) Then, all we have to do is to apply the softmax function as usual to obtain the normalized and masked attention weights: In: attn_weights = torch.softmax(masked / d_out_kq**0.5, dim=1) print(attn_weights) Out: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000], [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000], [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000], [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000], [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]], grad_fn=) Why does this work? The softmax function, applied in the last step, converts the input values into a probability distribution. When -inf is present in the inputs, softmax effectively treats them as zero probability. This is because e^(-inf) approaches 0, and thus these positions contribute nothing to the output probabilities. Conclusion IIn this article, we explored the inner workings of self-attention through a step-by-step coding approach. Using this as a foundation, we then looked into multi-head attention, a fundamental component of large language transformers. We then also coded cross-attention, a variant of self-attention that is particularly effective when applied between two distinct sequences. And lastly, we coded causal self-attention, a concept crucial for generating coherent and contextually appropriate sequences in decoder-style LLMs such as GPT and Llama. By coding these complex mechanisms from scratch, you hopefully gained a good understanding of the inner workings of the self-attention mechanism used in transformers and LLMs. (Note that the code presented in this article is intended for illustrative purposes. If you plan to implement self-attention for training LLMs, I recommend considering optimized implementations like Flash Attention, which reduce memory footprint and computational load.) Building an LLM from Scratch If you liked this article, my Build a Large Language Model from Scratch book explains how LLMs work using a similar (but more detailed) from-scratch approach. This includes coding the data processing steps, LLM architecture, pretraining, finetuning, and alignment stages. Build a Large Language Model book cover The book is currently part of Manning's early access program, where new chapters will be released regularly. (Purchasers of the currently discounted early access version through Manning will also receive the final book upon its release.) The corresponding code is available on GitHub. Subscribe to Ahead of AI By Sebastian Raschka · Hundreds of paid subscribers Ahead AI specializes in Machine Learning & AI research and is read by tens of thousands of researchers and practitioners who want to stay ahead in the ever-evolving field. Subscribe 142 Share this post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs magazine.sebastianraschka.com Copy link Facebook Email Note Other 9 Share",
    "commentLink": "https://news.ycombinator.com/item?id=38990709",
    "commentBody": "Coding Self-Attention, Multi-Head Attention, Cross-Attention, Causal-Attention (sebastianraschka.com)130 points by rasbt 19 hours agohidepastfavorite9 comments f38zf5vdt 17 hours agoAs mentioned, these are all toy implementations and you should not use them in production. If you want to the fast, easy, and extremely optimized way of doing things, use torch.nn.MultiheadAttention or torch.nn.functional.scaled_dot_product_attention so that you get the optimal implementations. You can use xformers scaled dot product attention if you want the bleeding edge of performance. > (Note that the code presented in this article is intended for illustrative purposes. If you plan to implement self-attention for training LLMs, I recommend considering optimized implementations like Flash Attention, which reduce memory footprint and computational load.) Flash attention is already part of torch's kernels as of torch 2, but the latest versions and optimizations land in xformers first. reply radarsat1 15 hours agoparentIt seems that there are some popular attention methods such as relative embeddings and rotary embeddings (rope embeddings?) that are not possible to implement using pytorch's implementation, if I understand correctly. Do these then require the \"slow path\" versions that can be more easily modified? reply rasbt 12 hours agoparentprevYes, totally agree. These implementations are meant for educational purposes. You could in theory use them to train a model though (GPT-2 also had a from-scratch implementation if I recall correctly). In practice, you probably want to use FlashAttention though. You use it through `torch.nn.functional.scaled_dot_product_attention` etc. reply atticora 17 hours agoprev [–] conscious, kŏn′shəs, adjective -- Characterized by or having an awareness of one's environment and one's own existence, sensations, and thoughts. synonym: aware. Self-attention seems to be at least a proxy for \"awareness of ... one's own existence.\" If that closed loop is the thing that converts sensibility into sentience, then maybe it's the source of LLM's leverage too. Is this language comprehension algorithm a sort of consciousness algorithm? reply kmeisthax 14 hours agoparentNo. Self-attention is more akin to kernel smoothing[0] on memorized training data that spits out a weighted probability graph. As for consciousness, LLMs are not particularly well aware of their own strengths and limitations, at least not unless you finetune them to know what they are and aren't good at. They also don't have sensors, so awareness of any environment is not possible. If you trained a neural network with an attention mechanism using data obtained from, say, robotics sensors; then it might be able to at least have environmental awareness. The problem is that current LLM training approaches rely on large amounts of training data - easy to obtain for text, nonexistent for sensor input. I suspect awareness of one's own existence, sensations, and thoughts would additionally require some kind of continuous weight update[1], but I have no proof for that yet. [0] https://en.wikipedia.org/wiki/Kernel_smoother [1] Neural network weights are almost always trained in one big run, occasionally updated with fine-tuning, and almost never modified during usage of the model. All of ChatGPT's ability to learn from prior input comes from in-context learning which does not modify weights. This is also why it tends to forget during long conversations. reply sk11001 17 hours agoparentprevML attention is nothing like human attention. I think it’s madness to attempt to map concepts from one field we barely understand to another field we also barely understand just because they use overlapping language. reply jampekka 15 hours agorootparentHaving done some research into human attention, I have to agree with Hommel et al: No one knows what attention is [1]. In current ANNs \"attention\" is quite well defined: how to weigh some variables based on other variables. But anthropomorphizing such concepts indeed muddies things more than it clarifies. Including calling interconnected summation units with non-linear transformations \"neural networks\". But such (wrong) intuition pumping terminology does attract, well, attention, so they get adopted. [1] https://link.springer.com/article/10.3758/s13414-019-01846-w reply dlkf 17 hours agoparentprevIt’s debatable to what degree ”attention” in LLMs relates to ”attention” in psychology. See Cosma Shalizi’s note on this http://bactra.org/notebooks/nn-attention-and-transformers.ht... reply ben_w 16 hours agoparentprev [–] Careful, depending on who you ask there's 40 different definitions of the term. Any given mind, natural or artificial, may well pass some of these without passing all of them. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This article demonstrates how to implement self-attention mechanisms in Python and PyTorch for transformer architectures and large language models.",
      "It explains the importance of attention mechanisms in natural language processing tasks and explores different types of attention mechanisms.",
      "The article covers the process of creating sentence embeddings, computing attention weights, and context vectors in self-attention and causal self-attention. It also highlights the significance of normalizing attention weights."
    ],
    "commentSummary": [
      "The article explores various attention mechanisms used in coding, such as self-attention, multi-head attention, cross-attention, and causal-attention.",
      "Optimized implementations like Flash Attention are recommended for training language models.",
      "Users discuss the potential of implementing attention methods not found in PyTorch and the connection between attention in neural networks and attention in human psychology."
    ],
    "points": 130,
    "commentCount": 9,
    "retryCount": 0,
    "time": 1705242587
  },
  {
    "id": 38991953,
    "title": "Ruffle: Flash Emulator Advances with Major Improvements in 2023",
    "originLink": "https://ruffle.rs/blog/2024/01/14/2023-in-review",
    "originBody": "What is RuffleDownloadsCompatibilityGet InvolvedBlogDemoDiscordGithub 2023 in review Dinnerbone • 2024-01-14 It's been a very busy 2023 for Ruffle, so much so that we didn't find the time to write a new progress report with everything going on! Let's fix that! Let's summarize with some numbers first. Since the last blog post... ActionScript 3 Language has gone up from 60% to 75%! ActionScript 3 API has gone up from 60% to 68%! (And it was only 25% at the start of the year!) We've merged 852 pull requests from 43 people! (And 3 bots.) We've closed 1,288 issues (and some of them weren't even duplicates or anything!) Display Filters We've now implemented 7 out of 10 of Flash's filter effects, making content look much more accurate (and lots of text actually legible!) Along with filter support, we've also implemented cacheAsBitmap support - a huge optimisation for games that use it, saving us the need to render the same thing every single frame! The remaining 3 filters don't seem to be used often, but we welcome anyone who wishes to come help implement them! Support for the existing filters was done by @Dinnerbone and @torokati44. Text improvements On the topic of rendering, we've been picking and poking at fonts for a while now - and through our combined efforts we're finally starting to see the light at the end of this pixelated and wrongly shaped tunnel. @Lord-McSweeney has implemented basic Text Layout Framework (TLF) support, which has started to get some more advanced text rendering working. @kjarosh has also recently been working on making text inputs more functional, implementing text restrictions and better caret/selection rendering. @Dinnerbone has created the framework for loading device fonts, instead of using the default Noto Sans we use everywhere. It's available on web (with some configuration) but works out of the box on desktop thanks to @evilpie! With device font support landing, content that didn't embed their own font will now start looking much better. Notably, lots of Japanese content often relied on this, and didn't render text at all previously. Sockets... even on the web! This has definitely been one of the biggest hurdles to emulating Flash, but the incredible @sleepycatcoding came in and made it happen. Any kind of multiplayer game such as Gaia, Habbo or Club Penguin relied on sockets (TCP sockets, or XML sockets) to function - and nobody was sure if we'd manage to get these working on the web. Modern browsers just don't like that sort of thing! On desktop it's now supported out of the box (but will prompt you for permission by default, just to be sure), but web needs a little configuration to make it work. Flash remoting In addition to sockets, another common way for online games to work was a protocol called \"Flash Remoting\", using NetConnection. This was most notably seen in any game that uses the Armor Games API, and we're happy to say that that now works more or less as expected! FLV and video playback This has been a tricky one, but @kmeisthax has been tackling this all year and making amazing progress. Ruffle now supports FLV playback, depending on which codec is required, and @danielhjacobs has added a workaround for patented codecs on the web by playing them using the browser over the top of the content (or on desktop, just opening the browser). There's still a lot of progress to be made here, and @torokati44 has a working prototype of decoding H.264 using Cisco's OpenH264 which is very exciting! Initial AIR support This was a long time stretch goal that's starting to see reality thanks to @Aaron1011! We now properly version the ActionScript 3 API, and this has been extended to allow AIR-only classes and methods. We don't have much to show for AIR stuff yet, but the framework to get us here has already helped close off many longstanding compatibility bugs when movies don't expect certain methods or properties to exist for the version of Flash they target. Mixed AVM On the topic of longstanding bugs, @Lord-McSweeney has been working on allowing mixed-avm movies to run correctly - these are usually ActionScript 3 games running inside ActionScript 2 containers, or vice versa. There's still more to do here, but it's already helped unblock lots of content from just mysteriously failing to start! Extension improvements The extension now uses Manifest V3 (somewhat reluctantly), which enables us to... keep existing. Unfortunately this came with needing to remove the \"go to a swf url and play it in the browser\" feature, as that's no longer possible with MV3. However, the upside to this is that we're now in the Edge store! Also, thanks to work from @kmeisthax and @danielhjacobs, we're also now available on Firefox for Android! Check it out! @WumboSpasm also redesigned the demo player in the extension, which @danielhjacobs made serve as the replacement for the aforementioned swf-url feature. It looks pretty neat! Desktop UI Whilst we had a super basic UI introduced in our last blog post, now it's even better! It's still as simple to use as before, but the new Advanced Open menu has many new options you can toggle to change how content plays - plus we've added a host of brand new debug tools that even Flash Player didn't have! New website Whilst not Ruffle itself, the website got some much-needed love from @Dinnerbone with a total redesign. There's a lot more we'd like to do in the future with it, but web development isn't the specialty of any of our regular contributors... help is very much welcome! We've also pulled in all the improvements to the Extension's demo player, and brought those over to the website too. Too many contributions to call out! There's been just so many PRs landing that not everything can take the spotlight here. We'd like to thank every single person who helped shape Ruffle in 2023, and hope that 2024 brings more great progress. In addition, a big thank you to our sponsors who help keep the project alive. We appreciate you all! <3 Putting Flash back on the web",
    "commentLink": "https://news.ycombinator.com/item?id=38991953",
    "commentBody": "Ruffle: 2023 in Review (ruffle.rs)127 points by hexmiles 17 hours agohidepastfavorite20 comments CharlesW 13 hours agoIncredible achievements. First time I used Open Collective: https://opencollective.com/ruffle reply saidinesh5 12 hours agoprevHonestly, the progress looks very impressive. We still need something as intuitive as Flash for developing interactive content for the web. I was barely 11 or 12, and it was amazing how quickly i went from drawing shapes to animating them to making them interactive with simple logic back in that summer. Fast forward to now, not sure what we have, that comes close to that level of ease of use. Maybe some game engine that exports to html canvas/webgl? I know that it's not the web frameworks and the fragile wsiwyg editors that target them. reply MrJohz 11 hours agoparentI think the loss of these tools has less to do with the death of flash, and more to do with the decline of the genre of \"internet ugly\" cartoons and free games as a whole. We have technology in the browser that is just as powerful and just as portable (case in point canvas), but we rarely see it being used in the same way to build the same variety of browser games and experiments. I know a lot of people talk about the tooling not being there, but I'm not sure that's the case. Web development is a very open field, there is a ton of documentation, and a lot of easy ways to get started. Just open developer tools in your browser and you can write Javascript - things were never the easy with flash. There's also plenty of free tools for building things with, from game libraries to animation editors, to IDEs. I think the bigger issue is that the audience isn't there in the same way. Partly that's culturally - part of the glory of that era of internet was proper reveling in crappy animation, gross-out humour, and absurdism. But that's no longer part of the cultural zeitgeist in the same way. As a result, the cultural force behind a lot of the early examples of the medium has died out. But also, and I think more importantly, the platforms have evolved. Animation still exists, but it looks different and tends to be hosted on YouTube or TikTok, because that's where people are. Games still exist, but now they're usually mobile games, because it turns out people like playing these games on the loo, and mobile browsers suck. And both of these platforms have their own advantages for creators: they both make monetisation easier, and they make it harder for other people to steal your work - both things that flash creators spent a lot of time worrying about. Even as flash was dying, if it was culturally necessary to maintain, people would have done so. We had the tools and we had the technology. In fact, people did - I remember people pushing \"HTML5 games\" for a while, but they were never any good because no one had been making good news games for a couple of years at that point. In the end, flash died for much the same reason that non-threaded forums and IRC died - the cultural forces that used them either died themselves, or switched to other systems. reply saidinesh5 9 hours agorootparent> I know a lot of people talk about the tooling not being there, but I'm not sure that's the case. Web development is a very open field, there is a ton of documentation, and a lot of easy ways to get started. Just open developer tools in your browser and you can write Javascript - things were never the easy with flash. There's also plenty of free tools for building things with, from game libraries to animation editors, to IDEs. But a lot of JavaScript/web tooling is basically about making \"web applications out of what once was a document framework\". Flash was more of a gateway to creative interactive content than \"webapps\"/games imo. MS paint -> vector art -> animation with sounds -> interactive content was that really good creative path way. Action script didn't mean maintainable code. It just meant you could easily add simple actions to various events. I honestly believe that tool alone gave birth to a lot of the cultural forces you mention. I remember cartoon network and movie studios releasing lot more interactive websites for their content in Flash. But you're right, a lot more content gets consumed on mobile / tablets these days than desktops, and i guess people/businesses are optimizing for that. reply cpeterso 5 hours agorootparent> I remember cartoon network and movie studios releasing lot more interactive websites for their content in Flash. In fact, quite a few animated TV shows and films were themselves created using Adobe Flash, including new shows in 2024: https://en.wikipedia.org/wiki/List_of_Flash_animated_televis... https://en.m.wikipedia.org/wiki/List_of_Flash_animated_films reply scq 11 hours agoparentprevThe Flash authoring tool still exists, it was just renamed to Adobe Animate. It supports Canvas and WebGL. reply grishka 11 hours agorootparentThe SWF format itself was also nice though. It's just one file that you can easily host or share anywhere. One use case that was really cool was how VKontakte (the Russian Facebook) had Flash apps. You didn't need a server to build an app like you do now, you just uploaded the SWF in the app settings and that was it. reply saidinesh5 9 hours agorootparentprevI did try Adobe animate a couple of years ago and for some reason it felt a lot more clunky than Flash 5 i remember. Maybe the feature creep. Maybe the Adobe pricing structure. Not sure... reply ksec 3 hours agoparentprevSometimes I wonder, what amount of money would Adobe accept to sell Flash Player and Flash Professional. That company could then open source it. I even thought of the name of the company already, it should be called Macromedia. reply satvikpendem 10 hours agoparentprevMaybe Flutter, it uses the canvas. If you want to distribute it as a binary, it will be able to compile to WASM(-GC) in the near future. reply saidinesh5 4 hours agorootparentThat's just a developer library. There are so many of them... Pixijs, D3 based ones etc ... The authoring tool that flash had was like this: https://m.youtube.com/watch?v=kHOQkooK0qY . 2 tutorials like this and you'd have everything you need to make a multimedia website reply ITB 12 hours agoparentprevWe have partitioned the developer experience too far. We need a full stack stack. reply pitaj 9 hours agoparentprevRoblox games reply bsaul 11 hours agoprevI don't think there's an equivalent to flash even today. The design tools and the model behind is still probably the best way to create moderately interactive content. reply ravetcofx 11 hours agoparentFrom ease of development standpoint? Because featureset it seems like JavaScript and or WASM has absolutely filled that neiche. There were web apps and games that were never before feasible with flash. reply bsaul 11 hours agorootparentYes, the frame by frame approach and focus on vector graphical animations made people have a lot of fun instantly ( together with the great authoring tools). I'm currently thinking about a project that requires \"regular people\" be able to create interactive content for the web, and honestly i don't see any alternative. Where it started to go bad is when people tried designing full data-oriented websites with flash and air, but i believe it remains great for its core purpose. reply accrual 10 hours agoprevReally impressive work! Ruffle was already fully featured and usable for my limited purposes. I spent a couple days thinking about and finding old flash games I played in school and managed to acquire most of them. Now they live in a folder alongside Ruffle which makes them easy to play and demo offline anytime. Thanks, Ruffle team. reply mjevans 11 hours agoprevIs there a good way of collecting information on currently missing Language / API calls in pages or files that have Flash elements not presently supported by Ruffle? It might make it more useful to know what makes one off well loved flash items not currently work, or which elements are commonly used but missing support. reply Vespasian 48 minutes agoparentThere is a compatability page on the project website https://ruffle.rs/compatibility Edit: Rereading your question, I think you probably meant to see what is missing in a specific swf. That would be quite nice. reply pjmlp 13 hours agoprev [–] Great work, getting Flash back into the browser. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ruffle, a Flash emulator, has made substantial progress in 2023, with multiple improvements in the ActionScript 3 language and API.",
      "The emulator now supports filters, cacheAsBitmap, text rendering, sockets, Flash remoting, FLV, and video playback capabilities.",
      "Initial support for AIR and mixed AVM movies has been added, alongside enhancements to the extension, desktop UI, and website, with contributions and sponsorship support."
    ],
    "commentSummary": [
      "The post highlights the Ruffle project, which aims to revive Flash technology and bring back the nostalgia and creativity associated with it.",
      "Users express their longing for the unique development experience and interactive content that Flash provided.",
      "Alternative technologies like game engines or Flutter are proposed, but the consensus is that Ruffle is commendable for its efforts in bringing Flash back to the browser."
    ],
    "points": 127,
    "commentCount": 20,
    "retryCount": 0,
    "time": 1705250684
  },
  {
    "id": 38991248,
    "title": "Building an Effective Faraday Cage for ESP32 Reverse Engineering",
    "originLink": "https://esp32-open-mac.be/posts/0003-faraday-cage/",
    "originBody": "Building a Faraday cage with data passthrough for ESP32 reverse engineering December 23, 2023 10-minute read Jasper Devreker First, a short recap of what this project is doing: the Wi-Fi stack for the ESP32 (a popular, cheap microcontroller) is provided through a binary blob. We’re trying to reverse engineer the software and hardware to build our own open-source Wi-Fi stack. If this sounds interesting, I recommend you read the first and second blog posts. However, this is not necessary to read the rest of this blog post. One problem we encountered while reverse engineering the Wi-Fi hardware, is that there are a lot of Wi-Fi packets flying through the air: even when idle, most access points broadcast a beacon packet about ten times per second. Combined with the data packets that are also sent between access points and clients in the neighbourhood, this adds up to a lot of packets per second. One tool we use in reverse engineering, is a Wi-Fi dongle in monitor mode: in that mode, the dongle captures all packets it sees, even packets not addressed to the dongle’s MAC address. This deluge of packets sometimes makes it hard to find the single packet the ESP32 did or did not send; in most scenarios, you could filter on MAC address, but since we’re still reverse engineering the hardware, we sometimes don’t know the MAC address a packet will be sent with. Failed attempts Link to heading A simple (albeit not very practical) way to fix this, is to go out into the woods where there are no Wi-Fi devices. A better approach is to ‘insulate’ the ESP32 and Wi-Fi dongle from outside Wi-Fi transmitters. My first try at this was to directly connect the antenna connector of the ESP32 with the antenna connector of the dongle (with an attenuator in between, to make the signal weaker not to overpower the receiver). However, this did not work: outside packets still leaked in. My second try involved a very basic Faraday cage: a paint tin with a cut-out for the USB leads of the dongle and ESP32. To try to reduce RF leaking in via the USB leads, I added several ferrite chokes and closed the hole with copper tape. This unfortunately did not work as well as people online told it would; it only further attenuated outside packets with 10 dB. A 10× decrease in power might sound impressive, but it’s really not: my laptop can receive packets as quiet as -90 dBm; right next to an access point the packets are about -35 dBm, so attenuating with only 10 dB is not enough by a long shot. I also tried putting my phone in a (turned off!) microwave, but this did not work either, it was still connected to the Wi-Fi access point. Proper Faraday cage Link to heading While researching on how to build a proper Faraday cage that is also affordable, I came across the paper ‘Building and Testing an Economic Faraday Cage for Wireless, IoT Computing Education and Research’ that seemed pretty good: for 793 USD, they built a Faraday cage with data and power passthrough: they accomplished the data passthrough by using an Ethernet-to-fiber converter; they accomplished the power passthrough by using a second-hand power line filter from a MRI chamber. Simply explained, the paper proposes having two cabinets: an inner cabinet, covered in conductive fabric except where the door is, and an outer one, where the door is covered in fabric, sealing against the inner cabinet when the door is closed. If this is not entirely clear, don’t worry, I’ll have pictures later. There are, in my opinion, some flaws in that paper: they mention the bill of materials, but don’t give important specifications about what exactly it is they used. For example, their bill of materials states ‘Used 20A Powerline Filter’, but this is the most specific description of the power line filter in the paper. By emailing the original authors, I got to know that they used a Lindgren EMI/RFI filter, ELUL-2020, obtained on eBay. which brings me to the second point: they use second-hand material for both the power line filter (120 USD in the paper) and the cabinet (5 USD in the paper). Now, I don’t have anything against re-using components; I sometimes go to the scrapyard to scavenge useful electronic components (motors, displays, …), but you cannot rely on the availability of those components. While it is true that in the US, similar filters can be bought for ~200 USD on eBay, you cannot rely on those filters staying available in the second-hand market. Furthermore, in Belgium, the country where I live, there don’t appear to be any EMI power filters available on the online second hand markets. The import costs and shipping charges for buying a used power line filter from the US would be prohibitive. using second-hand materials in something where the goal is to make it low-cost feels a bit like cheating: the materials are low-cost because you did a good job in obtaining something expensive for a low price, not because the materials are inherently low-cost. A new EMI power line filter is so expensive that places selling them don’t even display the price (“if you have to ask the price, you can’t afford it”). I will try to only use materials that are commonly available, and link to what exactly I bought. The main differences between the approach in the paper and my approach are: I built the inner and outer cabinets myself, from wood. No special tools are needed, only a drill. Instead of trying to pass through power, I’ll put a lead-acid battery inside the Faraday cage and use buck-boost converters to convert to the necessary voltages for the fiber-optics converter and devices under test. This will lower the costs dramatically, to the point where even when we’re using all-new components, the price will still be lower than the price described in the paper. Pictures! Link to heading (Please excuse me for the bad image quality) First of all, a picture of the completed Faraday cage where I’d like to call attention to: this consists of two parts that slide into each other: the outer cabinet, made from medium-density fiber board (MDF board), and the inner cabinet, made from a wooden skeleton covered with conductive fabric on five sides. there is conductive fabric on the inner side of the door. There is foam tape between the fabric and the wood on the front side of the inner cube; when you close the door, the foam tape is compressed, pressing the fabric of the inner cube tightly against the fabric of the front door, creating an RF tight seal. there is a black latch on the door that can keep the door shut (not shown very well here, but I’ll show a better picture later). in the top right corner, you can see a yellow fiber entering the cage from behind. This is the inner cube, constructed from wood, before it was covered with conductive fabric on five sides. Here, you can see the two fiber-to-Ethernet converters. I used bidirectional converters: this means that only one fiber is used for both transmitting and receiving data; as opposed to unidirectional fiber that has a TX/RX pair. This was done to minimize the size of the connector, which in turn would make it possible to fit through a smaller diameter copper tube. This is a copper pipe that has the fiber going through it. On both sides of the pipe, a small 3D printed cone was added to gently convert the diameter of the outer pipe to the diameter of the fiber. Conductive tape was used to seal the hole in the copper pipe: only a very small hole where the fiber exits, remains. Since this hole is sufficiently small compared to the wavelength of the 2.4 GHz radio waves, the radio waves cannot enter via there. This copper tube was then inserted through a hole in the fabric on the back of the inner cube. It was again taped with conductive tape to both the inside and outside to form a good seal. This is the test setup that is placed inside the Faraday cage. It consists of: A Raspberry Pi. This runs usbip, so that the USB devices connected to the Pi can be used from other computers on the network. A 5V USB buck converter (green case) converting the 12V from the battery to 5V for the Pi A TP-Link TL-WN722N v1 Wi-Fi dongle, used to capture packets (it can be put in monitor mode) An ESP32, connected to a JTAG debugger (not connected via USB at the moment) Testing Link to heading The Faraday cage was then tested by sniffing packets using the Wi-Fi dongle inside. I first captured packets for 10 minutes while the door was open (so RF could enter), then captured packets for 10 minutes after the door was closed. When the door was closed, no packets were captured at all. So, as to still give an approximate lower bound by how much the Faraday cage attenuates signals at 2.4 GHz, I used both the strongest and weakest signal strength when the door was open: >>> from scapy.all import * >>> scapy_cap = rdpcap('faraday_captured_packets_door_open.pcapng') >>> max(p.dBm_AntSignal for p in scapy_cap) -12 >>> min(p.dBm_AntSignal for p in scapy_cap) -81 >>> (-12) - (-81) 69 The weakest signal my wireless dongle could still receive was -81 dBm. The strongest signal that arrived was -12 dBm. Since the Faraday cage blocked all packets, the power of even the strongest signal would have to have been attenuated to below -81 dBm for it to not be able to be received anymore. So, a lower bound of 69 dB attenuation is established. I know that this might not be entirely correct (the wireless dongle might not be calibrated, the geometry of the Faraday cage could have increased signal strength when the door was open, etc.), but I think it gives a good enough indication. Bill of materials Link to heading Item Price in EUR (1 EUR = 1.1 USD) Link, if necessary MDF board 244×122 cm 29.993× wooden beam 44×44mm, 210cm 9.27copper pipe, 15mm outer diameter, 1m (although you only need about 0.5m) 10.99hinges (could likely also be 3D printed) 7.78(optional) handles for outer cube 10.69wood screws; I used 4mm × 40mm for outer MDF cube; 4mm × 60mm screws for inner cube. I didn’t have any wood screws before this project, so I bought in bulk for future projects. The 10 EUR here is an upper bound estimate 10foam tape: I used 20 mm width × 3 mm height × 10 m length tape 7.45 https://web.archive.org/web/20240113175203/https://www.amazon.com.be/dp/B09KBH7WT1 conductive fabric, 3× 110 cm by 91 cm. I used TitanRF fabric, this was the only fabric that came with a test report 84 https://web.archive.org/web/20240113174610/https://www.amazon.com.be/-/nl/Darkness-Military-geleidende-RF-signalen-Bluetooth/dp/B01M294MGK/ conductive tape; 2.54 cm × 3.05 m 12 https://web.archive.org/web/20240113174817/https://www.amazon.com.be/-/nl/TitanRF-Faraday-Tape-High-Shielding-RF-sluitingen/dp/B07CRLCGCH/ 1 pair gigabit Ethernet to BiDi fiber converter 65.23 https://web.archive.org/web/20240113172956/https://www.amazon.com.be/-/nl/multimedia-singlemode-LC-transceiver-inbegrepen/dp/B09MD4BBFD/ single OS2 LC/LC fiber optic cable, 2m 7.99 https://web.archive.org/web/20240113174655/https://www.amazon.com.be/-/nl/nexinex-Glasvezel-patchkabel-Simplex-single-mode/dp/B0CDQ6YXPS/ buck converters (13.99 for 12; I only used two here) 2.333d printing filament (way less than 200g, but let’s take that) 4Ultracell UL18-12 12V 18Ah lead-acid battery 29.66For a grand total of 291.38 EUR, or about 318 USD. You’ll need to 3D print various items, those are detailed at https://github.com/esp32-open-mac/faraday_cage Building tips Link to heading Outer shell Link to heading Assemble outer shell. This is made from MDF, so be careful and definitely pre-drill before inserting screws, otherwise, you would split the MDF. Do this even for screws that advertise you don’t need to pre-drill. Attach hardware Hinges Latch to keep the door shut Optional: handles to lift the box Framed cube Link to heading Assemble cube Attach all hardware Holders for copper tube Stand-offs for bottom plate Foam tape (Last!) Attach Faraday fabric, with thumbtacks. Don’t use a staple gun, you would tear the fabric. If you have any questions, open a GitHub issue at https://github.com/esp32-open-mac/faraday_cage or send me an email via esp32-open-mac@devreker.be.",
    "commentLink": "https://news.ycombinator.com/item?id=38991248",
    "commentBody": "Building a Faraday cage with data passthrough for ESP32 reverse engineering (esp32-open-mac.be)118 points by signa11 18 hours agohidepastfavorite25 comments myself248 14 hours agoThe TitanRF fabric isn't super durable, so a shell around it is a good idea. I've had great luck with Ecofoil NT material, which is somewhere between cardstock and cheap poly tarp material in handling properties. (It's a polyethylene weave with foil on both faces.) Easy to work with, easy to fold and tape, easy to cover with other materials for durability. Super cheap in big rolls. For 120VAC passthrough, the Delta 20DBAG5 is cheap and cheerful. Screw it into a metal junction box and tape all sides of the box to the chamber wall. But a battery in the box is simpler and quieter. If you need windows/vents, avoid the hobby-store copper mesh that's meant as a stiffener for clay models; the way it's woven, it isn't guaranteed to have connections to itself in adjacent rows. It's good at first but any surface corrosion ruins it. Go with punched or \"expanded metal\" sheet, even if you can't find copper, aluminum or stainless works fine in practice. Just make it significantly larger than the window opening and use plenty of foil tape at the edges; I suspect that capacitive coupling through the surface oxide layer means it's an RF short even if it looks open at DC. I've been wondering if ITO-coated glass would work as a window but have not tried it. But it's no good for ventilation anyway so I'm not sure it's worth the bother. reply AnarchismIsCool 4 hours agoparentWhat kind of stuff are you doing with this? I've pondered setting something up like this in my office for RF testing but I need to pin down my requirements first reply BuckYeah 7 hours agoparentprevProbably a naive question but How do you test the ecofoil? reply alright2565 15 hours agoprevThe detail here is really great! I thought that maybe aluminum foil would work here. I found this paper: https://www.acsu.buffalo.edu/~ddlchung/Materials%20for%20ele... > electrical conductivity is not the scientific criterion for sheilding ... Metals ... function mainly by reflection. > A secondary mechanism of EMI sheidling is usually absoprotion. > The absorption loss is a function of the product σrμr, whereas the reflection loss is a function of the ratio σr/μr, where σr, is the electrical conductivity relative to copper and μr is the relative magnetic permeability. > The reflection loss decreases with increasing frequency, whereas the absorption loss increases with increasing frequency. So it turns out aluminum foil wouldn't actually be much good, with σrμr=.61 and σr/μr=.61. The commercial material listed says it uses copper & nickel: nickel's σrμr=20, copper's σr/μr=1. So my thought here is completely wrong. Who would have figured the commercial product has gone through more thought than my random guesses! reply userbinator 9 hours agoprevmy laptop can receive packets as quiet as -90 dBm For comparison, GPS signals in good conditions are around -125dBm, and many receivers can go down to -165dBm, so WiFi signals are still much stronger in comparison. reply mikewarot 2 hours agoparentThat's because GPS signals are only encoding 50 bits/second of data, and spreading it across 10 Mhz of bandwidth... with a processing gain of about 43 dB.[1] [1] https://www.ieee.li/pdf/viewgraphs/gnss_fundamentals.pdf See slides 37 and 38 reply femto 10 hours agoprevMetal MILO [1] tins are pretty good. The tight fitting lid provides a continuous RF seal and being tin-plated you can solder though connectors in place, providing good ground continuity. [1] https://en.wikipedia.org/wiki/Milo_(drink) reply MauranKilom 10 hours agoprevThat's an interesting project, going to keep an eye on it! I have a hobby/DIY project involving sending several hundred UDP packets per second to various ESP32s across the house (driving LED strips live based on music playing on my PC). Much to my chagrin, the ESP32s tend to choke/get stuck quite frequently at higher packet volumes, and I've not found a good way to debug the issue further. See [1] below. While I haven't crossed the bridge of leaving the comfort of the Arduino IDE for flashing the ESP32s, trying out/beta-testing an open WiFi stack (once available) that I know I'll have a chance to thoroughly trace/debug would be very enticing. [1] I did find out so far that my WiFi setup occasionally \"clumps\" packets, causing like 10 packets to hit a given ESP32 at an instant (instead of a few ms apart) - not great, but should not be disastrous. However, this seems to cause the ESP32 WiFi stack to just slow to a crawl: It responds to pings much slower (like, in 100+ ms range) (to my surprise it actually responds to PING requests out of the box in the first place...) and/or doesn't really process any more packets in general if I continue sending at the same rate as normal. But backing off on the packet stream usually gets it back on track, strangely enough. This also happens if I do nothing in the main loop except clear packets as they come in, so it's not in my code. reply scottapotamas 8 hours agoparentYou can possibly fix/improve this situation by tuning some config settings on the ESP32 and/or host. Changing these with the ESP-IDF is pretty easy, but you'll have to find the relevant calls that are suitable for an Arduino based project. Your description of packet clumping sounds like Nagle's Algorithm at play, which I found increased TCP latency on the ESP32 fairly significantly. If the hardware isn't battery powered, then you might also see improvements by playing with the ESP32's WiFi power saving modes. reply aspenmayer 4 hours agorootparentI hear that this is one of the advertised benefits of upcoming WiFi 7 - so-called “deterministic latency.” reply crtified 11 hours agoprevLayperson question : does it help if the Faraday cage is grounded, so that the waves it picks up have somewhere to \"go\"? or does it not work that way? (I fully expect it does not work that way!) reply Horffupolde 10 hours agoparentIn theory it doesn’t need to be grounded. In practice it can be done. reply ph4te 14 hours agoprevI worked for a service provider doing some cellular testing and we had a special clear box that did this. It was probably expensive at the time. I wonder how well those faraday bags or boxes for $20 work from amazon or ebay. reply H8crilA 13 hours agoprevDon't the pcap dumps contain signal strength for each frame? Is it unreliably measured? reply slicktux 15 hours agoprev [–] Wrap it in foil and stick it in the oven! reply qwertox 15 hours agoparent [–] Not sure if it was a joke, but shouldn't a microwave oven shield the 2.4 GHz band pretty well? reply HALtheWise 15 hours agorootparent> I also tried putting my phone in a (turned off!) microwave, but this did not work either, it was still connected to the Wi-Fi access point. Apparently modern wifi chips are just too good at picking up faint signals. reply ComputerGuru 14 hours agorootparentImplying faint microwave signals also make their way out, of course. reply MauranKilom 11 hours agorootparentMy wireless headphones get an unbearable amount of pops and cracks when I'm wearing them close to my (running) microwave. But since I wear them virtually always, I am in effect virtually never next to my microwave when it's on :) reply BenjiWiebe 14 hours agorootparentprevI have heard of WiFi slowing down when a microwave is running. Also when a phone is in a microwave, the RF noise is also attenuated, so that helps a little (amplifier noise is unaffected). reply namibj 3 hours agorootparentprevYeah the shielding is to not cook your eyeballs if you observe the food, and to not cook anything else outside of the chamber, including e.g. your phone's wifi receiver. reply PeterisP 4 hours agorootparentprev\"pretty well\" means 40-ish dB attenuation - that's about 10000 times weaker signal, absorbing 99.99% of it. But that's not even close to sufficient for actually isolating wireless transmissions. My microwave got 35-38 dB for WiFi signals when I measured it, and that got through a fairly reliable connection. You might want 90 dB or something like that if you want to ensure that you can't extract the data from the noise - so perhaps a starting point might be to put your device in a small microwave oven and put that oven in a large microwave oven :) reply bonzini 15 hours agorootparentprev [–] The article says > I also tried putting my phone in a (turned off!) microwave, but this did not work either, it was still connected to the Wi-Fi access point. But yeah I would have thought the same. reply bboygravity 10 hours agorootparent [–] Anecdotal: I've heard that in Europe EM emissions directives have exceptions for microwave ovens, because without the exceptions none of them would pass the tests and make it to market. Never checked if true. But that could explain why they're so \"leaky\" at 2,4 Ghz (and other frequencies). reply namibj 3 hours agorootparent [–] The radiation is in an ISM band, which has no far field emissions limits beyond \"be reasonable\". The close-range limits are about safety for living being and other devices nearby, so e.g. not cooking you, and not frying your phone's WiFi receiver. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author faces challenges in reverse engineering the Wi-Fi stack for the ESP32 microcontroller due to the high volume of Wi-Fi packets in the surrounding air.",
      "Traditional methods, such as using a paint tin, ferrite chokes, and a turned-off microwave as a Faraday cage, prove ineffective in blocking outside packets.",
      "The author discovers a research paper outlining an affordable Faraday cage made from conductive fabric and commonly available materials.",
      "To reduce costs, the author plans to build the Faraday cage using wood cabinets and a lead-acid battery.",
      "The constructed Faraday cage successfully blocks RF signals, allowing for more accurate analysis of the Wi-Fi stack.",
      "The summary includes information on signal attenuation, a bill of materials, and detailed instructions for building the Faraday cage."
    ],
    "commentSummary": [
      "The post explores the construction of a Faraday cage with data passthrough for ESP32 reverse engineering.",
      "The recommended materials for the cage are Ecofoil NT and Delta 20DBAG5 for passthrough.",
      "The post also covers recommendations for windows/vents, material effectiveness for shielding, RF testing, WiFi connectivity issues, and the potential use of a microwave oven for shielding."
    ],
    "points": 118,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1705246161
  },
  {
    "id": 38991392,
    "title": "Crystal 1.11.0: New Features, Bug Fixes, and LLVM 18 Support",
    "originLink": "https://crystal-lang.org/2024/01/08/1.11.0-released/",
    "originBody": "Share We are announcing a new Crystal release with several new features and bug fixes. Pre-built packages are available on GitHub Releases and our official distribution channels. See crystal-lang.org/install for installation instructions. Stats This release includes 178 changes since 1.10.1 by 28 contributors. We thank all the contributors for all the effort put into improving the language! ❤ Changes Below we list the most remarkable changes in the language, compiler and stdlib. This is a pretty big release with lots of things going on, so hold tight 🚀 For details, visit the full changelog. LLVM 18 One of the biggest steps forward is support for upcoming LLVM 18 which allows linking LLVM dynamically on Windows (#14101). Additionally, LLVM 18 now provides everything we need in the upstream C API, removing the need for our wrapper extension llvm_ext. It’s still necessary for older LLVM versions, so we’ll keep it around for a while. But the future tool chain is getting simplified. Read more in #13946. Thanks @HertzDevil Compiler Optimization Levels The compiler gains four distinct optimization levels: -O0: No optimization -O1: Low optimization -O2: Middle optimization -O3: High optimization Each level activates the respective LLVM RunPasses and CodeGenOptLevel optimizations. -O3 corresponds to the existing release mode and -O0 corresponds to the default non-release mode. -O0 remains the default and --release is equivalent to -O3 --single-module. Effectively, this introduces two optimization choices between the previous full or nothing. And it’s now possible to use high optimization without --single-module. Read more in #13464. Thanks @kostya Alignment primitives The language has two new reflection primitives: alignof and instance_alignof return a type’s memory alignment (#14087). This allows implementing type-aware allocators in native Crystal with properly aligned pointers. They are siblings of sizeof and instance_sizeof and can be used in the same way. class Foo def initialize(@x : Int8, @y : Int64, @z : Int16) end end Foo.new(1, 2, 3) instance_alignof(Foo) # => 8 Effect on existing code The introduction of these primitives makes it impossible to define methods of the same names. So def alignof or def instance_alignof are now invalid syntax. We don’t expect there to be a big impact in practice. Thanks @HertzDevil dll parameter in Link annotation The Link annotation has a new parameter dll for specifying dynamic link libraries on Windows (#14131). @[Link(dll: \"foo.dll\")] lib LibFoo end Thanks @HertzDevil Macro @caller context Macros now have a reference to their calling context via the special instance variable @caller (#14055). macro foo {{ @caller.line_number }} end foo # => 5 Thanks @Blacksmoke16 New collection methods Enumerable#present? is a direct inversion of #empty? avoiding some quirks with the similar, but not-quite, #any? (#13847). Thanks @straight-shoota Enumerable#each_step and Iterable#each_step are direct methods for creating step iterators (#13610). Thanks @baseballlover723 Enumerable(T)#to_set(& : T -> U) : Set(U) forall U and #to_a(& : T -> U) forall U allow materialising an Enumerable into a pre-defined collection, which gives more flexibility than the standard #to_set and #to_a methods (#12654, #12653). Thanks @caspiano Numeric enhancements BigFloat#** now works for all Int::Primitive arguments and supports the full exponent range for BitInt arguments (#13971, #13881) Floating point to string conversion in printf uses the Ryu algorithm (#8441). New methods Float::Primitive.to_hexfloat, .parse_hexfloat, and .parse_hexfloat? allow conversion to and from the hexfloat format (#14027). More math features: Math.fma (#13934) Number#integer? (#13936) Int32#abs_unsigned, #neg_signed (#13938) Int::Primitive#to_signed, #to_signed!, #to_unsigned, #to_unsigned! (#13960) Thanks @HertzDevil Enhancements for crystal spec crystal spec gets two new commands for introspection: crystal spec --dry-run prints all active specs without actually executing any spec code (#13804). Thanks @nobodywasishere crystal spec --list-tags lists all tags defined in the spec suite (#13616). Thanks @baseballlover723 Enhancements for crystal tool unreachable The basic implementation of crystal tool unreachable from Crystal 1.10 gets some useful enhancements. The --tallies option prints all methods and the total number of calls. Those with a zero tally are unreachable (#13969). The --check flag exits with a failure status if there is any unreachable code (#13930). Annotations show up in the output (#13927). New output format: CSV (#13926). Paths in the output are relativized, making it more succinct (#13929). Thanks @straight-shoota Inherited macros in API docs Inherited macros are now exposed in the API docs. They had previously been hidden, in contrast to inherited defs (#13810). Thanks @Blacksmoke16 Text Regex::MatchData#to_s returns the matched substring (#14115). Thanks @Vendicated The new EOLconstant (End-Of-Line) is a portable reference to the system-specific new line character sequence (#11303). Thanks @postmodern We got new version-specific constructors for UUID: .v1, .v2, .v3, .v4, and .v5 (#13693). Thanks @threez StringScanner now supports String and Char patterns (#13806). Thanks @funny-falcon Char::Reader got some nilable character accessors: #current_char?, #next_char?, #previous_char? (#14012). Thanks @HertzDevil String#matches_full? is a simple API when you need a regular expression to match the entire string (#13968). Thanks @straight-shoota Misc The capacity of String::Buffer and IO::Memory was unintentionally limited to 1GB. They now support the full range up to Int32::MAX, i.e. 2GB (#13989). Thanks @straight-shoota There was a nasty bug in Number#format which could mess with the integral part. It is now fixed in #14061. Thanks @HertzDevil Vendored shards markd and reply are no longer referenced by paths relative to the compiler source tree. This means they can be local dependencies (i.e. in lib) when using the compiler as a library (#13992). Thanks @nobodywasishere There are two new constants which provide information on the compiler host and target: Crystal::HOST_TRIPLE and TARGET_TRIPLE (#13823). Thanks @HertzDevil Shards 0.17.4 The bundled shards release was updated to 0.17.4 which brings a couple minor bugfixes. (#14133). Thanks @straight-shoota Experimental: ReferenceStorage and .pre_initialize We’ve started an effort to make it easier to use custom allocation mechanisms in Crystal and decouple allocation from initialization. The main tool is Reference.pre_initialize which performs the rudimentary object initialization, before actually calling #initialize. Reference.unsafe_construct is a higher level API on top of that. ReferenceStorage represents a static buffer for a reference allocation. These APIs are experimental and might be subject to change. We expect more features in this direction in future releases. Join the discussion about custom reference allocation at #13481. NOTE: ReferenceStorage was removed again in 1.11.1 due to compatibility issues with older versions of the standard library (#14207). It will come back with an improved implementation. Thanks @HertzDevil Deprecations Splat operators in macro expressions are deprecated. Use .splat instead (#13939) LLVM.start_multithreaded and .stop_multithreaded. They have no effect (#13949) LLVMExtSetCurrentDebugLocation from llvm_ext.cc for LLVM 9+ (#13965) Char::Reader#@end (#13920) We have been able to do all of this thanks to the continued support of 84codes and every other sponsor. To maintain and increase the development pace, donations and sponsorships are essential. OpenCollective is available for that. Reach out to crystal@manas.tech if you’d like to become a direct sponsor or find other ways to support Crystal. We thank you in advance! comments powered by Disqus",
    "commentLink": "https://news.ycombinator.com/item?id=38991392",
    "commentBody": "Crystal 1.11.0 Is Released (crystal-lang.org)107 points by ksec 18 hours agohidepastfavorite58 comments mratsim 16 hours agoIt's been a while since I heard about Crystal. What's the state of Windows support. How did the compiler speed drama unfold? reply diggan 16 hours agoparent> What's the state of Windows support. https://github.com/crystal-lang/crystal/issues/5430 > UPDATE 2023-04-24: > All the main platform features for Windows are finished! > There are still some smaller stories pending. Project board: https://github.com/orgs/crystal-lang/projects/11/views/5 > You can support the ongoing development financially: Windows suppport (https://opencollective.com/crystal-lang/projects/windows-sup...) project on Open Collective. reply hamandcheese 15 hours agoparentprevIs it just me, or does Crystal have a disproportionate amount of Windows demand? I feel like every time Crystal comes up, someone asks (or complains) about Windows support. reply straight-shoota 14 hours agorootparentCrystal Core developer speaking. I don't think it's disproportionate. Windows is a huge platform with lots of developers. Also you don't hear anyone asking for other OS support because all other major operating systems are supported. So there's no real comparison. reply dimgl 15 hours agorootparentprevAnecdotal, but I think this is just the case with all projects of this nature. People on Windows don't like not being able to use stuff. The same thing was happening with Bun, and yet I've met lots of Windows developers who hate JavaScript. reply npn 5 hours agorootparentprevCrystal is a nice language to write utilities with. Usually with utilities you want to distribute it to other people, many of them can't just install the whole toolchain to run the code. So naturally we want a fully portable windows exe file that can be run independently. reply voidfunc 14 hours agorootparentprevWindows isn't popular on HN, but it's a major platform with a lot of devs. You gotta get out of the HN bubble. reply arcanemachiner 11 hours agorootparentIt took me 2 decades to get out of the Windows bubble. I'll enjoy my time here when I can get it. reply ukd1 14 hours agorootparentprevI sponsored a bunch of work on this years back; I wanted it so I didn't have to use Go for cross-platform binaries for a CLI client. I ended up having to use Go, but I hope one day it ships so I can avoid it a little more (I don't hate it, but prefer crystal). reply dbalatero 15 hours agorootparentprevI've noticed a lot more developers on Windows across the board, as more folks learn programming. reply mratsim 14 hours agorootparentprevI asked because Crystal was notorious for not having it. reply dleslie 14 hours agorootparentprevIf a language doesn't have Windows support then it's inappropriate for commercial native application development. reply bks 6 hours agoprevI have been running https://mailflowmonitoring.com for the past 6 years on Amber (A Crystal framework) it flawlessly manages the sending and tracking of round trip emails for tens of thousands of pings a day. reply pjmlp 15 hours agoprevAs language nerd it is interesting to see the Crystal improvements, congratulations on keeping it going. reply colesantiago 15 hours agoprevIt looks like Crystal is gaining traction. Which companies use Crystal in anger / production in front of real users? reply DASD 14 hours agoparentI believe Kagi is a heavy user of the language. reply SushiHippie 8 hours agorootparent~70k LOC Crystal Mentioned in the crystal conf talk held by a Kagi Developer https://youtube.com/r7t9xPajjTM reply freediver 6 hours agorootparentCorrect link https://www.youtube.com/watch?v=r7t9xPajjTM reply ukd1 14 hours agoparentprevMy last company (Rainforest QA) used it in quite a few places when ruby wasn't fast enough, though a lot of Go too. reply compumike 14 hours agoparentprevAt Heii On-Call, we use Crystal to power the API server https://api.heiioncall.com/ as well as other internal components. reply asadawadia 9 hours agorootparentdo you use kemal? reply prh8 11 hours agoparentprevWe use it to do nightly sitemap generation of over 30 million links. Takes about 30 minutes, replaced a Ruby (Rails) process that took hours. reply 1vuio0pswjnm7 13 hours agoparentprevInvidious, a self-hosted YouTube front-end. is written in Crystal. reply acheong08 12 hours agorootparentAnd its resource usage is depressing. Isn’t Crystal garbage collected? I’m not sure why it has memory leaks. Also, >300 MB of RAM usage (in docker. Some might be the system) with only 2 users after running for a while seems a bit too much reply xethos 8 hours agorootparentOh is this why Invidious' setup documentation recommends restarting Invidious every hour or so? reply acheong08 7 hours agorootparentI ended up just throwing it 4 gigs of memory and it has pretty much been fine ever since. I do restart every few weeks when it reaches 1 GB though reply ksec 8 hours agorootparentprevHow would you know the memory leak was the Crystal part? reply acheong08 7 hours agorootparenthttps://github.com/iv-org/invidious/issues/1438 I’m not the first one facing these issues. reply 1vuio0pswjnm7 6 hours agorootparentprev(Not an Invidious user myself. I just looked at the source code a while back to see what they were doing. But I never tried compiling or using it.) reply Piribedil 14 hours agoparentprevMaybe not up to date : https://crystal-lang.org/used_in_prod/ reply mg 15 hours agoprevI like the first code example on https://crystal-lang.org # A very basic HTTP server require \"http/server\" server = HTTP::Server.new do |context| context.response.content_type = \"text/plain\" context.response.print \"Hello world, got #{context.request.path}!\" end puts \"Listening on http://127.0.0.1:8080\" server.listen(8080) Nicely concise! This would be the Python equivalent: # A very basic HTTP server from http.server import BaseHTTPRequestHandler, HTTPServer class RequestHandler(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header('Content-type', 'text/plain') self.end_headers() self.wfile.write(bytes(f\"Hello world, got {self.path}!\", 'utf8')) httpd = HTTPServer(('', 8080), RequestHandler) print(\"Listening on http://127.0.0.1:8080\") httpd.serve_forever() Would like to see it in more languages. reply ReleaseCandidat 14 hours agoparentHaskell with WAI (more or less, guess there is some conversion from `Bytestring` needed when using `rawPathInfo`) {-# LANGUAGE OverloadedStrings #-} import Network.HTTP.Types import Network.Wai import Network.Wai.Handler.Warp (run) app :: Application app request respond = do respond $ responseLBS status200 [(\"Content-Type\", \"text/plain\")] (\"Hello world, got\"rawPathInfo request\"!\") main :: IO () main = do putStrLn \"Listening on http://127.0.0.1:8080\" run 8080 app reply beanaroo 13 hours agoparentprevNot exactly the same but various basic HTTP server implementations are presented at https://rosettacode.org/wiki/Hello_world/Web_server reply alwaysbeconsing 15 hours agoparentprevWould not Ruby be the comparison to do? In which case it I think is nearly identical to the Crystal. reply doublerabbit 15 hours agoparentprevTCL: proc webServer {chan addr port} { while {[gets $chan] ne \"\"} {} puts $chan \"HTTP/1.1 200 OK Connection: closeContent-Type: text/plain\" puts $chan \"Hello World!\" close $chan } socket -server webServer 8080 vwait forever reply mg 15 hours agorootparentThat lacks a few things: 1: The comment 2: Printing the status message (\"Listening on ...\") 3: Outputting \"Hello world, got {request.path}!\" Aka http request parsing and string extrapolation. reply fransje26 15 hours agoprevFrom the landing page (and trying a few tutorials): https://crystal-lang.org/reference/1.11/index.html I couldn't quite determine the reason of being of the Crystal language. Could someone with experience with the language explain what is its appeal, and why it would be worth trying/learning? Thanks! reply ukd1 14 hours agoparentReason; no idea - but the appeal for me is it's very ruby-like, and super fast (except, compiling). Some programs I've moved from ruby use >10x less memory, and are generally faster. It's mostly as a nice to write as ruby, except the compile time. Concurrency is very go-like, and there are a reasonable amount (not ruby-like though) level of libraries for use. reply fdr 8 hours agoparentprevIt generates efficient code (similar to a C program, e.g. you could write a reasonable ray tracer), and is concise. Many Ruby programs would not see a size expansion from being written in Crystal. It's also good at interoperating with C programs; it's employs LLVM in a fairly orthodox manner (so, for example, if you run `perf` under Linux, you will get good symbolic output) I think the Achilles heel for Crystal is its compilation time on small programs with large dependencies. Note the time to build the entire compiler and standard library is not extraordinary for a large program. It's more like, a small program adding its first few dependencies will find itself pulling in a lot of the standard library to compile, and then will level off in its compilation time. To illustrate why this may be the case, consider the Crystal feature allowing redefining parts of classes, including ones in the standard library. This is both very useful, and if you think about how it affects features like incremental compilation, immensely complicating. Here's some practical crystal programs. This Postgres wire protocol driver is comparable to libpq in performance, and supports a lot of features in the protocol, too: https://github.com/will/crystal-pg/ The Crunchy Bridge CLI: https://github.com/CrunchyData/bridge-cli reply DistractionRect 14 hours agoparentprevMy understanding, it's ruby like, but statically typed and compiled. Basically instead of trying to retrofit ruby with these features without breaking things (like Shopify and friends do with JITs, AOTs, sherbert, etc), this is bottom up approach around these features. Due to it's infancy the packages are sparse, so what some people do is a hybrid approach, where they write their thing in Ruby/Rails and kick out the slower parts to Crystal. This kinda gives the best of both worlds. Crystal (last I checked) had slow compile times, so gives you the dev velocity and package availability of ruby, plus the speed of a compile language when you need it. The compile times on Crystal stay low because you're only writing a small parts/microservices of your application in Crystal. reply Toutouxc 1 hour agorootparent> where they write their thing in Ruby/Rails and kick out the slower parts to Crystal What's the story on Crystal/Ruby interop these days? I work on a Rails app that does document automation, and I could definitely find some specific < 1000 LOC segments that would benefit hugely from being 1:1 rewritten into Crystal and integrated with the rest of the app. reply viraptor 6 hours agoparentprevFor me that's the sweet spot for: - compiled so easy to deploy - easy to write compared to Rust, when you don't care about being super reliable - still keeps things typed and better than in Ruby - lower memory usage than Ruby/Python (and usually an easy 10x jump in performance) I'm relying on it a lot with simple lambda automation. If I wanted to write an efficient server, I'd probably use rust though. reply aeturnum 14 hours agoparentprevTo me, it kind of sits between Golang and Rust in terms of intent. I think the goal is to deliver reliable runtime characteristics at the speed of a compiled language, while generally only demanding as much specificity as higher-level interpreted languages (i.e. Ruby). For me I reach for it when I want something that's fast (like go or Rust) but which I can write quickly (like Python or Ruby). reply WJW 15 hours agoparentprevWhy would a language need a specific reason to exist? It's a nice language with Ruby-ish syntax, strict typing, a go-like runtime and it compiles down to something quite fast if you take a little bit of care. reply Bjartr 14 hours agorootparentI interpret the question less as asking for justification for why it exists and more as wanting to learn what problems it ought to be used to solve because of an interest in putting it in one's \"toolbox\" as it were. reply orthoxerox 14 hours agoparentprevAs far as I understand, work on Crystal started when Ruby on Rails was the hottest web framework. The original purpose was going to be running your RoR apps that outgrew MRI. Yes, code would have to be adapted to run on Crystal, but that would be much easier than rewriting everything in a completely new language. reply helen___keller 15 hours agoparentprevCrystal is basically taking Ruby and adding compilation and a strong static type system. I enjoy writing it but don’t think there’s any particular reason to try/learn it, unless you happen to also be a ruby dev who prefers compiled, strongly typed languages. reply 38 16 hours agoprev [12 more] [flagged] quincepie 16 hours agoparentIt only requires vs build tools which is likely for the linker. Other languages like rust does the same. reply 38 15 hours agorootparentRust has a GNU option for Windows: https://forge.rust-lang.org/infra/other-installation-methods... reply vips7L 15 hours agorootparentIf you’re already running windows, why do you need the linker to be GNU? reply 38 15 hours agorootparentbecause the GNU linker is like a 100 MB download, and Visual Studio is like 10 GB reply bhaak 14 hours agorootparentAnd that's an issue because ...? reply 38 14 hours agorootparentwould you rather pay $1 for a soda drink, or $100? reply bhaak 13 hours agorootparentWhere do you pay $1 for 10 GB, let alone $100? reply pjmlp 15 hours agoparentprev [–] The best tooling for each OS is the one from the OS vendor themselves. reply 38 15 hours agorootparent [–] [citation needed] reply pjmlp 14 hours agorootparentThat was my citation, you're welcomed. reply xcv123 14 hours agorootparentprev [–] Common sense reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Crystal programming language has released a new version with various new features and bug fixes.",
      "Notable changes include support for upcoming LLVM 18, compiler optimization levels, new alignment primitives, and a parameter for specifying dynamic link libraries on Windows.",
      "The release also includes enhancements for collection methods, numeric operations, text processing, and bug fixes, as well as experimental features related to custom allocation mechanisms.",
      "Splat operators in macro expressions and certain LLVM functions are deprecated.",
      "The release was made possible by the support of sponsors."
    ],
    "commentSummary": [
      "Crystal version 1.11.0 has been released with a focus on ongoing discussions regarding Windows support.",
      "Crystal developers are prioritizing Windows support and actively contributing to its development and offering financial support.",
      "There is demand for Crystal on Windows, particularly for creating fully portable executable files. Crystal is a statically typed and compiled programming language that combines the speed of Go and Rust with the simplicity of interpreted languages like Python and Ruby. It offers interoperability with C programs and provides better typing and lower memory usage than Ruby or Python."
    ],
    "points": 107,
    "commentCount": 58,
    "retryCount": 0,
    "time": 1705246909
  },
  {
    "id": 38991521,
    "title": "Challenging Insulin Therapy: Rethinking Diabetes with Gary Taubes",
    "originLink": "https://www.theguardian.com/society/2024/jan/14/unlocking-the-truth-about-diabetes-is-it-time-for-a-diet-based-treatment",
    "originBody": "Sweet spot: ‘When insulin therapy started in the 1920s, they had no idea what the long-term side-effects were.’ Photograph: Yasu and Junko/Trunk Archive The Observer Diabetes Unlocking the truth about diabetes: ‘The science has been pretty awful’ More than 400m people around the world have diabetes, and many control the condition using insulin. But science writer Gary Taubes believes it’s this very treatment that is behind the current epidemic. Does his controversial case for a diet-based alternative to medicine have any bite? Rebecca Seal @RebeccaSeal Sun 14 Jan 2024 04.00 EST G ary Taubes is probably the most single-minded person I have ever met. In 2002, when he was a little-known science journalist and author of two books on scientific controversies, an article of his was published in the New York Times, headlined: What If It’s All Been a Big Fat Lie? In it, he argued that the low-fat dietary advice of the previous couple of decades wasn’t only incorrect, but actively dangerous and the reason for, as he put it, the “rampaging epidemic of obesity in America”. For Taubes, dietary fat wasn’t a problem at all. Instead, the real danger was carbohydrate, he asserted, sparking a backlash, and fuelling the ongoing conversation about what constitutes a “healthy diet”. He wasn’t the first to assert that carbs were bad (Robert Atkins got there before him), but perhaps because of his serious and scientific background – he has a physics degree from Harvard and studied aerospace engineering at Stanford – he has been a polarising figure, with as many ardent followers as detractors. Since 2007, Taubes has published five books on sugar, fat and carbohydrate, including his latest, Rethinking Diabetes, in which he posits that low-carb diets have been under-used as a way to manage blood glucose in type 1 and type 2 diabetes, in favour of drug-heavy treatment regimes which, he suspects, may do more harm than good. His writing on nutrition has won several awards, notably from the US National Association of Science Writers, but it has also been sharply criticised, mainly for his almost evangelical attachment to the keto diet, in which you eat so little carbohydrate (50g or less per day) that the body goes into a state called ketosis, meaning you stop burning stored glucose and start burning fat instead. In 2021, he published The Case for Keto, a self-help book, after which, as he says, he went “from being a respected source of information to somebody who may indeed be a crank after all”. Taubes, who follows the diet himself, is now proposing it should be offered to people living with diabetes. (Keto diets were originally developed as a way to treat certain types of childhood epilepsy.) For Taubes, keto means he doesn’t “eat starches, grains or sweets, and I don’t eat breakfast because I think better in the morning without it. When I snack, it’s nuts or good cheese. If we were to go to dinner together, I’d order a piece of fish or half a roast chicken and ask the waiter to hold the rice or potatoes and give me a green salad or green vegetables instead.” When we talk, it’s a bright morning in Berkeley, California, where Taubes, 67, lives with his wife and sons. With his open-neck shirt, tan, salt-and-pepper short hair and slightly drawling delivery, he seems more like a professorial Owen Wilson than someone seeking to radically alter how diabetes is understood and treated. The majority of his latest book is an exhaustive retelling of the history of diabetes research and how, in the first half of the 20th century, it went – as Taubes sees it – wrong, with the emergence of a treatment doctrine that mistakenly allowed people living with diabetes to eat whatever they wanted, all the while using insulin and drugs, such as metformin, to manage the blood-glucose consequences. Taubes has always been fascinated by bad science and for him this was bad science of the highest order, because the regime was based on dietary hypotheses which he says had not – and still have not – been rigorously tested. ‘The science has been pretty awful. So many of the conceptions that evolved around nutrition are based on assumptions that may be wrong’: Gary Taubes Photograph: Cody Pickens Before the discovery of insulin in the 1920s, diet was the only way to manage diabetes and although various options were tried by early practitioners, low-carb was, says Taubes, among the most popular (with medics, at least). Insulin was a gamechanger. Not only did it almost magically save the lives of children with type 1 diabetes, who would often arrive at hospital comatose and die swiftly afterwards, but it also meant that people with diabetes of both types could eat a more or less normal diet. Another example, for Taubes, of how early researchers were mistaken, concerns the differences between type 1 and type 2 diabetes, which are so different they almost shouldn’t share a name. Type 1 diabetes is an autoimmune disease in which the body’s immune system attacks and destroys the cells in the pancreas that produce insulin, the hormone which regulates the level of glucose in our blood; people living with type 1 need insulin injections or an insulin pump, to survive. Like type 2, type 1 can cause complications such as heart, kidney or eye disease, and nerve damage. Type 2 diabetes accounts for about 90% of cases, and is a metabolic disorder in which the body either can’t make or can’t use insulin (AKA insulin resistance) to metabolise glucose, leading to persistently high levels in the blood. Ultimately, people living with type 2 diabetes may need insulin and other diabetes medications, too, but for a lot of people, diet and lifestyle modifications can defer that need. Many, but not all, specialists think there is often a causal relationship between weight and type 2 diabetes, which has led to a high level of stigma around the diagnosis. Diagnoses of both are rising globally – five million people live with diabetes in the UK. What Taubes would like to see is low-carb diets being offered alongside or instead of diabetes medications. “When insulin therapy started in the 1920s, they had no idea what the long-term side-effects were or what the long-term consequences of living with diabetes were [because most people with type 1 died],” he says. “Then doctors find out that it’s just easier to let patients eat whatever they want and give them drugs to cover them. Then it’s another five, 10 or 20 years before they start seeing the long-term complications, which they think of as long-term complications of the disease.” What he wishes scientists at the time had concluded was: “The reason we’re keeping them alive is insulin therapy. So what we’re seeing is the long-term complications of the disease as controlled by insulin therapy, and the insulin therapy might be causing the complications as much as the disease is. If you change your diet, you can put your diabetes into remission Gary Taubes “By the late 1930s, you have this tidal wave of diabetic complications: the heart disease, the atherosclerosis, the neuropathy, the kidney failure, the blindness, amputations. And nobody ties it back.” By then, the low-carb diet had fallen far from favour. “Nobody wants to eat a diet. So nobody’s being told: ‘Look, if I give you insulin, I’m going to keep you alive until you’re 30, especially if I give you a lot of insulin and you do eat your carbs. But if I tell you not to eat the carbs and we minimise the insulin use – which for type 2 could be no insulin – I might keep you alive as long as anyone else in your family.’” In the book, which is laden with references, studies and dense historical detail, Taubes mentions case records from the 1700s in which patients on low-sugar diets beg for a medical solution, suggesting that the preference for medication over a highly prescriptive diet has been with us for a long time. “If you’re told, a pill or a diet, we all want the pill. But if you’re told a pill or a diet and the diet will keep you healthy and the pill will give you a chronic degenerative disorder where you’re still going to have these horrible complications, they are just going to be 20 to 30 years later… the pill is going to be easier, because it always is. But if you change the diet, it’s not a hypothetical change: you can put your diabetes into remission, you can stop taking these medications.” Convincing as this sounds, there are some apparent flaws in Taubes’s arguments, which are by no means widely accepted in the academic or medical communities. Professor Roy Taylor is a leading British diabetes researcher. “When a subgroup of the UK Government Scientific Advisory Committee on Nutrition was convened in 2021 to look at low-carbohydrate as an approach to diabetes in general, the literature was very thoroughly assessed and I was part of that panel. Very low-carbohydrate diets had no better results than the modest reduction of carbohydrates,” he says. Other studies, such as one in 2022 at Stanford that compared low-carb diets and the Mediterranean diet, have shown that while they both work when it comes to controlling blood glucose, the Mediterranean diet is easier to stick to. Very low-carbohydrate diets had no better results than the modest reduction of carbohydrates Professor Roy Taylor Second, low-carb diets are now offered as one way of managing diabetes of both types, the pendulum swinging back in their direction after almost a century, possibly more so here in the UK than in the US (Taubes doesn’t include contemporary case histories in the book). Two members of my own family have been put on a very low-carbohydrate diet in recent years when they were deemed at risk of developing type 2 diabetes in their 70s (a risk both of them reversed). Jack Leeson, 55, was diagnosed with type 2 diabetes six years ago and on the advice of his NHS doctor, radically altered his diet. “She put me on the diabetes drug metformin and told me about people who lose limbs with it. So I was very motivated.” She didn’t suggest keto, “but she made clear the volume of sugars in bread and pasta, and supposedly healthy things like fruit juice, which is just sugar. I gave them up. I didn’t replace them with fat, just more protein, lots of vegetables, berries, soya milk and yoghurt, beans and lentils and pasta made from Japanese konjac root.” Leeson also does an hour of aerobic exercise every day. “I binned off the diabetes, cholesterol and blood pressure issues in 18 months and lost about 5st.” Both Diabetes UK and diabetes.co.uk, the two biggest diabetes charities in the UK, have information on their sites about low-carb diets, particularly for people with type 2 diabetes. Diabetes UK, the larger charity, states that “there is no consistent evidence that a low-carb diet is any more effective than other approaches in the long term. So it shouldn’t be seen as the diet for everyone… At the moment, there is no strong evidence to say that a low-carb diet is safe or effective for people with type 1 diabetes. Because of this, we do not recommend low-carb diets to people with type 1 diabetes.” What Taubes would probably add is that there isn’t much evidence that they’re unsafe either, because low-carb diets haven’t been studied intensively either within or beyond the diabetes research community. But diet is incredibly difficult to study – especially in a context like diabetes where subjects are also often medicated. One of the tropes of nutritional science is that drawing long-term health conclusions from what people eat is nigh on impossible, because diet interacts with lifestyle, because people lie, intentionally or not, about what they eat, and because longitudinal studies of diet are so expensive. Munjeeta Sohal, 39, was diagnosed with type 1 diabetes as a teenager and has mixed feelings about the idea it could or should be managed through carb restriction. “A low-carb diet might be a good way to control blood sugars,” she says, “but I am now on an insulin pump system that allows me complete freedom over what I eat. If I eat less carbs, my insulin stays more in range. I see the impact it has on my blood sugars, but that isn’t enough to make me want to do it full-time. My blood sugars are finally, thanks to the technology that’s available, in range between 70% and 90% of the time on an average day, and that is brilliant. I don’t need to eat low-carb for this to be the case.” Having diabetes is also a risk factor for eating disorders and Sohal’s relationship with food veered towards unhealthy when she was first diagnosed as a teen. Like many others she worries that “asking people to restrict may, for some, lead to secret bingeing, or guilt and shame around enjoying food.” I am now on an insulin pump system that allows me complete freedom Munjeeta Sohal “They’re right, of course, to worry,” says Taubes. “But if diabetes, like obesity, is triggered in susceptible individuals by the carbohydrate content of the diet and can be put into remission by avoiding carbohydrate-rich foods, what would you tell patients?” Another complication is that where Taubes is able to look at diabetes – indeed at diet as a whole – through a single, high-fat-low-carb lens, few others can. “I eat the same thing every day,” he says. “As long as I like it, I will continue to like it and be happy to eat it.” I suspect this makes it hard for him to understand why many of us have such a complicated relationship with cake. While Taubes himself has stuck to keto for the last two decades, the rest of us might find it tough to follow. As Professor Taylor says: “The fall-off, in keto, is quite high. People have families and friends, and eating is part of social interaction.” Even in a highly motivated group, like people trying to control diabetes, adherence to low-carb is pretty patchy: a 2022 paper tracked this low adherence and pointed to cultural, religious and – perhaps most important – economic barriers. Keto can be expensive and labour-intensive, as well as socially awkward, at least in the beginning. (There are also some rare but potentially very serious health risks, says Taylor.) Gregory Dodell, a New York-based endocrinologist who takes a weight-inclusive approach to managing diabetes, says: “You have to look at the social determinants of health. We’re not treating a population as a research experiment, we’re treating a population with a lot of different complicated variables and issues and a very complex, multifactorial chronic condition. One size does not fit all.” You have to look at the social determinants of health gary Dodell In conversation, Taubes isn’t quite as dogmatic about diet as his writing makes him seem. He regularly says things like, “assuming what I’m arguing is correct”, and at one point casually notes that he could “have a heart attack tomorrow, which is possible the way I eat, and which, God knows, I keep expecting”. He does use a lot of caveats in his books (which apparently drives his publisher slightly crazy), but on the page, he nonetheless comes across as unwaveringly committed to the high-fat, low-carb way of life, so I find his concern surprising. “Well, my world is full of people pointing out the age other people died who believe what I believe. Which, of course, is selection bias, because you don’t see the people who are still alive, you only see the people who die. If I die tomorrow, maybe I would have died 10 years ago, had I not eaten the way I did. It’s always an experiment.” Given that there is no control version of any of us against which to measure success, none of us will ever know if we chose the “right” diet. The paradox, of course, is that this kind of diet-by-hypothesis is exactly what Rethinking Diabetes rails against. But Taubes sees no alternative: “The science has been pretty awful. So many of the conceptions that have evolved around eating behaviour and nutrition are based on assumptions that may be wrong. The problem is that people don’t change their [dietary or health] advice because the longer they give it, the more invested they are that it had better have been right. I write from this perspective – of the history – so folks can see the damage that is done by allowing assumptions to be embraced as facts without definitive evidence.” Rethinking Diabetes – What Science Reveals About Diet, Insulin and Successful Treatments by Gary Taubes is published by Granta at £16.99 on 18 January. Buy a copy for £14.44 at guardianbookshop.com Explore more on these topics Diabetes The Observer Diets and dieting features Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=38991521",
    "commentBody": "Rethinking Diabetes – interview with Gary Taubes (theguardian.com)106 points by prmph 18 hours agohidepastfavorite107 comments n2d4 9 hours agoGood article, but there's one thing that really bothers me: > perhaps because of his serious and scientific background – he has a physics degree from Harvard and studied aerospace engineering at Stanford [...] Which, for the thing he's doing now, is not a serious background! Knowledge isn't transferrable and there are plenty examples of people who transfer fields with unwarranted confidence and make totally bogus claims (see the Nobel disease [1]). I wish we would stop claiming the opposite, because especially in a field like nutritional sciences you can find some \"evidence\" for any theory (see eg. the Chemical Hunger drama [2]). [1] https://en.wikipedia.org/wiki/Nobel_disease [2] https://www.lesswrong.com/posts/NRrbJJWnaSorrqvtZ/on-not-get... reply shermantanktop 8 hours agoparentIt's like some kind of Gell-Mann effect corollary. Scientists and other educated experts quoted in a newspaper carry automatic authority unless you understand the target field well enough. But if you do, you realize they are no more trustworthy than a guy on the bus. Titles don't help. An \"environmental engineer\" might not have sufficient knowledge about zoology or bridges to support whatever they are talking about, but a layperson might not realize that. reply nhinck2 5 hours agorootparentCoined as mann-gell if I'm remembering this video correctly. https://m.youtube.com/watch?v=wBBnfu8N_J0 reply jandrewrogers 6 hours agoparentprevIt kind of is a serious background. The standards of intellectual and academic rigor in physics and aerospace engineering is far higher than nutrition science or whatever. What the domain specialists have is familiarity with the existing literature, but that is readily remedied as anyone with an interest and motivation can become familiar with literature like nutrition science in short order. All things being equal, I would probably trust the judgement of someone that went deep in the hard sciences and engineering, and then educated themselves on the literature of some soft science specialty on the side, than the average person whose entire education was in the much less rigorous and more shallow soft sciences. I have seen far too much sloppy and motivated reasoning in the soft sciences to trust anything that comes from people that exclusively live in that world. I am sure there are proper scientists in those fields doing good work, but that isn’t the average. I don’t know anything about this particular case, but it is much easier for someone with mastery of the hard sciences to become fluent in the soft sciences out of interest than vice versa. It may not be fair but I have seen so many real world examples of this, where hard science/engineering experts became recognized experts in softer science fields entirely out of interest, but never in the reverse, that it is difficult to ignore. reply Johanx64 8 hours agoparentprevBollocks, physics is an actual scientific field and aerospace engineering is actual engineering (compared to say \"software engineering\" where it's engineering in name only), similarly there's barely anything scientific in the \"nutritional science\". If you have a real scientific background and real engineering background you are way more likely to do a proper job. It frustrates me that people think nutritional \"science\" is a serious scientific field. And that the \"nutritional scientist\" is somehow better equipped - they have barely done any real science a single day in their life. reply agumonkey 4 hours agorootparentThey have a point. It's easy to come with your high IQ and miss some obscure subtle data that people in the field learned the hard way. Sure the man is capable of understanding it quick and even do more.. but it's too easy to get carried in your own newcomer theory. reply ramraj07 8 hours agorootparentprevSo what makes you an authority in defining what’s a scientific field? Care to share some actual data? reply oldgradstudent 5 hours agorootparentIf you can rigorously test your hypotheses in an experiment, it is a scientific field. If you can't, or won't, it is not a scientific field. reply YeGoblynQueenne 4 minutes agorootparentWhere did this definition come from? Did you just make it up? It excludes all of mathematics and most of astronomy, for example. reply firesteelrain 1 hour agorootparentprevIt’s been said that iteratively developing a software based solution is like testing a hypothesis and seeing what works reply Retric 5 hours agorootparentprevThere have been tons of nutritional experiments which are extremely rigorous. Unfortunately, like most fields it’s the junk you read about because rigor is slow/difficult/expensive and therefore less common than BS. Just look at say fusion research and all it’s seriously flawed experiments. reply unsupp0rted 2 hours agorootparentAnd tons of nutritional experiments demonstrating the exact opposite result, that are also “rigorous”. Sure, human biology is (hundreds of) orders of magnitude more complicated than say going to the moon. But at this point in 2024 we “know” a lot more about physics than we do about nutrition. And maybe that’s because physicists, when faced with contradictory data, keep digging until they figure out the basics once and for all. reply rapind 4 hours agorootparentprevThere’s probably a grey area for social sciences like economics. I also tend to think there’s real nutritional science but it’s being drowned out by all the snake oil. reply zelse 3 hours agorootparentSnake-oil and moralizing. Dietetics suffers from the same issue that mental health does: there's an underlying stigma that causes people to inflict a moral judgement on the ill; fat people - or so the old line of thinking goes - are immoral. A shocking number of people, including many doctors and scientists, view diabetes as a \"fat people disease,\" and that diabetics are in effect being punished for their gluttony. This isn't always a conscious bias, but it has very negative effects. It reminds me of the studies they've done in Canada and the US about how natives (American Indians) die a lot of the time in the ER because they're immediately written off as alcoholics or drug-addicts, and so their complaints are evaluated through the lens of \"this person just wants opiates.\" reply nradov 7 hours agorootparentprevYou don't need to be an authority to see how the majority of human nutrition science is barely a step above junk. Pick any journal in the field and go through the last few issues. You'll mostly find observational studies with subject reported data (known to be inaccurate). Very few long, term randomized controlled trials with meaningful endpoints. reply paulpauper 7 hours agorootparentIt is worse than junk. most junk science is harmless, but bad diet advice can hurt your health. reply refurb 6 hours agorootparentprevWould you have a physicist would self-taught medicine as your doctor? Of course not. While a scientific or engineering background might demonstrate capability to understand another scientific field, that doesn’t replace the near decade of education people in that field have. reply unsupp0rted 2 hours agorootparentI would, compared to 8 out of 10 doctors I’ve met. A dedicated physicist who self-taught medicine would allow hoofs to be zebras, not horses, some % of the time. reply Almondsetat 2 hours agorootparentprevFalse equivalency Doctors need practice in the hospital and do an excruciating amount of rounds surrounded by senior experts. Nutritionists? No. Given a nutritionist who has studied the books and gotten a degree and a physicist who has studied the books and hasn't gotten the degree I would be hard pressed to choose the nutritionist reply esafak 3 hours agorootparentprevhttps://xkcd.com/793/ reply a11r 3 hours agoparentprevGary Taubes has written multiple scholarly books on the subject. He has won the Science in Society Journalism Award of the National Association of Science Writers three times. I think a training in one scientific field does qualify a person to go spend their time digging into other fields. How well they do is a matter of personal ability and effort and one would have to read their output and judge for oneself. I have read many of Taubes's book and many of the papers cited in his books and have concluded that he is a well qualified expert in the field. reply phendrenad2 6 hours agoparentprevScience isn't the memorization of facts. To graduate from Harvard, you must actually understand how to do science at a high level. Many of those skills transfer between subjects. Having a physics degree might not make you a great biologist, but it's also not nothing. reply paulpauper 8 hours agoparentprevExactly. I don't think his guess is any better than any else given the poor track record of the dieting industry overall. He made a lucrative career by latching on to a theory, that like virtually all diet fads, falls short when applied to real people in a controlled setting. No one has any idea which diet (if any) is best, or the connection between insulin and obesity, or if fats are good or not, etc. All hunches. Carbs have been falling out of favor for a while now but obesity getting worse. reply bruce511 5 hours agorootparentThe problem with anecdotes is that they're not science. And as there is wide variety in humans this is very much a case of ymmv. Anecdotally, my wife went onto a low-carb diet 18 months ago. She likes low carb food, and isn't a huge fan of bread, potatoes etc anyway, so it was easy. She's not total though, still eats a rusk at breakfast, and has a (home made) pizza once a week. She also started exercising a lot more, and found a sustainable rhythm for that. She's lost 50 lbs. (Around 25kg). Interestingly, her blood sugar levels have not dropped significantly, but they did drop a bit. I sporadically do low-carb, and always lose weight doing so. I like bread, potato's, rice though so my diet is much less obsessive. My results are more fluctuating but slowly trending down. I'm in no rush, but eould like to drop around 15 lbs this year. I lose about 1lb per week if I'm \"fairly strict\" - exercise levels don't change (I exercise a reasonable amount.) Incidentally, I've found that just slowly reducing quantity has helped as well. Eating from small bowls and plates has helped me. None of this is data. And I still \"cheat\" (I'll happily eat normal in social situations, and often on weekends etc) but I seem to have found a pattern I can maintain long term. It's a \"diet\" in the sense there's good and bad, but its also \"lifestyle\" in the sense that I try to be \"moderate\" with the bad. reply DontchaKnowit 6 hours agorootparentprevIdk Ive been a keto acolyte for a long time and anyone who Ive ever known that did the diet was wildly successful on it. So his claims really dont seem to fall short in my own experience and the experience of people I know. reply mlsu 4 hours agoprevI'm confused. De-loading carbs for fat makes sense for overall wellness and health (since healthy things tend to be fatty and/or low carb), but it is not possible to \"manage\" T1 without insulin treatment. The fact that either he or this article does not seem to distinguish between the two types in this regard is a basic category error. The low carb \"managing\" of T1d before the 1920s was basically a permanent hyperglycemia which you could last months, maybe years. Once your honeymoon period ends you die a spectacularly brutal death -- your blood turns to acid and you starve in insatiable hunger. None of this is \"manageable\" through diet, since your body has a baseline rate of gluconeogenesis entirely independent of diet. Type 1 and Type 2 sharing the \"diabetes\" name has to be up there on \"most frustrating lexical errors\" ever. reply adaszko 6 hours agoprevHere's [1] how well the references check out for Taube's previous book. Is there any reason I should believe his newer books are any better? Here's a study funded by Taubes himself (!) That disproves his earlier claims: https://examine.com/articles/low-fat-vs-low-carb-for-weight-... [1]: https://thescienceofnutrition.wordpress.com/2014/04/05/good-... reply coldcode 16 hours agoprevAs a newly diagnosed Type 2, but not overweight, I take metformin and have a carefully altered my diet to slow down my digestion and have few easily digestible carbs (i.e. such as the whites, flour, sugar, potatoes and rice). But it takes 3 months to see if my A1C goes down (a measure of the past 3 months of blood sugar), so I won't know for a couple more months. Going keto to me seems like using a shotgun to kill a fly on your foot, it is too hard to keep up with, and causes other issues, while simply understanding how carbs affect your blood sugar is much easier to do. Part of the problem is that all the tables of carbs assume you are only eating the one thing (which is how they make the tables), but the actual affect on blood sugar spikes is the sum of what you ate. For example, eating a boiled potato by itself will spike a Type 2 steeply, but eating it with lots of butter will not. I prefer to understand my health issues by reading a wide variety of information from people who specialize in them, not just read one guy's opinion. reply kcplate 10 hours agoparentI’ve been a T2 diabetic for 14 years. For the first 8 years I was insulin dependent and bought into the dietary reqs provided to me at the time which was “grains and fruit are good, just push insulin to manage glucose”. I realized that every month it was taking more and more insulin to manage my blood sugar. I shifted my diet to higher fat and low carbs and was off insulin completely within 3 months and within 6 months my A1c was running low 5s YMMV of course, but at no point eating the approved ADA diet and pushing insulin was my A1c ever below 6.5. Since eating low carb and no insulin, my A1c has never been above 5.4 reply foepys 16 hours agoparentprevIt is incredibly hard to beat a sugar addiction (a lot harder than nicotine or weed) but it is worth it, I promise. Going full on no-carb and taking long walks daily got a friend out of pre-diabetes type 2. No sugar, no fresh potatoes (cooking and putting them into the fridge for 24 hours is fine), no rice, no fruits, and much more got them off metformin and normalized their blood sugar enough for them to be able to eat cake and sweets again without fearing to get diabetes. reply bityard 15 hours agorootparent> It is incredibly hard to beat a sugar addiction (a lot harder than nicotine or weed) but it is worth it, I promise. It's not trivial, I will certainly admit, but it is eminently doable with the right mindset and a reasonable plan. The hard part is that sugar and carb-laden foods are positively ubiquitous in the western diet, due to the fact that carbs are cheap to grow, process, ship, and store. Fat and protein are expensive and harder to find depending on where you live. But I look at it like this: by going out of my way to find and cook metabolically-optimal food for myself, I am investing in my future health in a way that will be _much_ cheaper than the cost of getting my foot removed in a few decades. If I could give any advice at all to someone considering a keto diet, it would be this: do some research, start with some kind of plan, and be forgiving of yourself. It will take some time to figure out which keto-friendly foods you have available to you, and which ones you will actually enjoy eating. If you fall off the wagon and murder a bag of chips in a moment of weakness, know that the whole effort isn't down the drain. You just try to do better tomorrow. reply Angostura 9 hours agorootparentprev> no fresh potatoes (cooking and putting them into the fridge for 24 hours is fine What's that meant to achieve? reply pstuart 7 hours agorootparentIt helps convert the starches to be \"diet resistant\" -- https://apjcn.nhri.org.tw/server/APJCN/24/4/620.pdf It seems to be a very easy win in improving one's diet. Apparently heating it back up doesn't undo the starch conversion. reply caminante 15 hours agorootparentprev> no fresh potatoes (cooking and putting them into the fridge for 24 hours is fine) What does waiting a day accomplish? reply DoughnutHole 15 hours agorootparentLeaving starchy foods to rest for a decent amount of time after cooking results in some amount of the starch converting into “resistant starch” which the body can’t convert into glucose. So a given portion will be less blood glucose spiking than a portion that hasn’t had that rest. The overall effect is modest though, the whole thing doesn’t turn into resistant starch. It makes it better, but I don’t think it would make a big portion of potatoes suddenly safe for a diabetic. reply cmrx64 15 hours agorootparentprevthe thermal cycling causes resistant starches to form. actually you are best off heating/cooling it twice, and minimizing the extra water content. instead of being broken down into sugars they get fermented into fatty acids deeper in the gut. pasta is eligible for this effect too idk how the final starch compositions vary between them tho. reply ramraj07 8 hours agorootparentprevYou prescribed cutting sugars so that you can eat cake again, I’m confused. reply thepasswordis 4 hours agoparentprevThere's a standard formula for calculating your A1C based on your average glucose level. If you get a dexcom G6, (about $200), you can monitor your glucose levels constantly, and know what your A1C will be. The app gives you rolling averages for 2 days, 7 days, 14 days, and 30 days. Extremely useful. I'm not a T2 diabetic, but did have a slightly elevated A1C at my last physical. I asked my doctor for a prescription for a CGM, she gave me one, and it has been a huge help in learning both how food effects my blood sugar, as well as how blood sugar effects my mood. I think everybody regardless of their current health should have one of these things. Blood sugar is a fantastic look into sorts of aspects of your health. reply salad-tycoon 14 hours agoparentprevThat’s interesting about fat and potatoes. Didn’t know that. Another similar thing is vinegar. It slows gastric emptying and keeps the spikes lower (eg french fries with malt vinegar). You can test fructosamine which I believe is a 2 week avg. or you could do finger pokes or even a CGM if you got the ability. [1] I personally enjoyed The diabetes code by Jason Fung. He’s a nephrologist in Canada. Good luck in your diabetes journey! Luckily you seem interested which is such a distinction. 1 https://www.diabetic.org/fructosamine-to-a1c/ reply dave8088 7 hours agoparentprevHave been tested for the type 1 antibody? I was diagnosed as type 2 cause doctors assume type 1 doesn’t happen in your late 30s. It happened to me. Wasted a bunch of time trying to treat it as type 2. Not overweight and type 2 is rare. reply UncleSlacky 15 hours agoparentprevSame here (diagnosed about 10 days ago, on metformin, removed simple carbs as much as possible). I've been looking at Michael Mosley's 800-calorie and 5:2 fasting diets, involving an intake of 800 calories a day for about 8 weeks to \"reset\" your system, then fasting 2 days out of every 7 (i.e. 800 calories for two days a week) thereafter. I haven't \"officially\" started it, but in practice without simple carbs and sugars, my calorie intake must be pretty close to 800 (the appetite suppressant characteristics of metformin really help here). I've also started exercising after big meals (I've never really exercised before!). There's a website marketing a guided programme if you're into that kind of thing): https://thefast800.com/programme/ as well as a book: https://oceanofpdf.com/authors/michael-mosley/pdf-epub-the-8... reply ramraj07 8 hours agorootparentExercise is probably the most important thing you could do! Hope you’re able to stick to it and improve! The other thing that helps me is cutting off sugar - not carbs or starch just actual sweet sugar. In coffees and desserts. Makes me eat less! reply Projectiboga 7 hours agoparentprevIncrease unsaturated fat in your diet and take melatonin. Melatonin upregulates the insulin receptors, so the insulin you have can do more. Trace minerals and following a standard meal plan worked up by a nutritionist. reply voisin 16 hours agoparentprev> have few easily digestible carbs (i.e. such as the whites, flour, sugar, potatoes and rice) Should potatoes be in this list? I understood the potato diet to be very effective at causing weight loss (thought to be due to high potassium content of potatoes)? Can something consistently cause weight loss and cause a major insulin response (which I think causes calories to be stored as fat…)? reply DoughnutHole 15 hours agorootparentPotatoes have a GI of about 70, comparable to white rice or white bread (pure glucose is 100). So pretty bad. The “potato diet” will only lead to weight loss if you’re eating fewer calories than you’re expending, same as anything else. You could eat nothing but bacon grease and you’d lose weight if you were eating below maintenance. The effect of food on blood glucose if a matter of how easily it is converted to glucose. Starches like those in potatoes, flour, rice etc are basically converted nearly as easily as straight sugar. reply ajmurmann 4 hours agorootparentI think there might be other things going on with the potato diet. In Slime Mold Time Mold's study participants seemed to lose more weight when cooking potatoes with the skin on. My wife and I recently tried three diet for about a week with good weightloss. However, my wife who prefers potatoes cooked with skin quickly got abdominal pain and dropped the diet. When I started to eat all the potatoes I had cooked for her, I got similar issues. I think there is a good chance that the solanine which is mostly in the skins messes with people's digestive system and contributing to weightloss. Poisoning yourself doesn't strike me as a valid approach to weightloss. Supposedly someone tried supplementing solanine and didn't see the weightloss. I don't trust that though, since there was a more comprehensive trial or a believable alternative explanation reply marcus_holmes 7 hours agoparentprevT2 Diabetic for ~12 years (actually probably a lot longer but I only got diagnosed 12 years ago). I've \"beaten diabetes\" (i.e. I tested as not being diabetic) twice now by losing weight (via calorie counting) and getting fit, but found it too hard to maintain that lifestyle. I've made dietary changes but they had less/no effect. Keto was fine, but my wife didn't get on with it and making two meals every time we ate was too onerous. I stopped measuring my blood sugar a few months after getting diagnosed because it was kinda pointless. I haven't had a single symptom. If I didn't visit the doctor every 3 months or so I'd never know I was diabetic (and I was probably diabetic for 10+ years before getting diagnosed). I take Metformin regularly mostly for the anti-aging side effect. The endless diarrhoea from Metformin is something I got used to, though I had to suspend taking it while travelling because the combination of Metformin shits and travel shits was too much. It all feels like a non-thing to be honest. I know people lose limbs to D2, and go blind, and stuff, but it's hard to take that personally and know that as a definite risk that mean I need to make drastic and difficult lifestyle changes. I keep getting checked and told there's no sign of any damage, but my blood sugar is too high. For 12 years now. I know this sounds a bit anti-vax, but it feels like the science is wrong here. I do not want to go down an anti-science rabbit hole and end up in a conspiracy theory world of my own. So I just keep doing the thing they tell me as best I can. But when/if someone finds out that we've been thinking about D2 all wrong, I will not be the faintest bit surprised. edit: and another thing: I've never had consistent advice from doctors around this. Some GPs say I need to come in every month to get checked up. Others says I only need once a year. Some examine my feet and eyes. Others don't. Some think I'm on too high a dose of Metformin. Others say it's fine. Some are really strong on the diet thing, others not so much. Almost every aspect of this disease seems to be subject to personal opinions about treatment. reply bruce511 5 hours agorootparentOpinions vary a lot, because people vary a lot, and because of this science \"that applies to everyone\" is very hard to do. There's no question that there us good medical science, but equally there is a lot of junk-science and so in this age of \"hearing everyone's opinion\", it can be hard to separate fact from opinion. I think we can agree on some stuff. Smoking is bad for you, exercise is good, and so on. Keeping your weight down, more accurately keeping % body fat down, and so on. Making small, incremental, sustainable changes to lifestyle that trend you in good directions, are definitely the best. GPs are (with respect) not a terribly good source. They tend to see sick people all day, and gather lots of superficial anecdotes. Which is not to ignore them, but to understand their POV. But it's worth taking their opinions on board as an hypothesis. reply pstuart 7 hours agoparentprevOne interesting thing about those digestible carbs is that cooling them after cooking helps convert them into diet resistant starch that helps feed the gut biome, e.g., https://www.foundmyfitness.com/news/s/q2g00a reply chimeracoder 16 hours agoparentprev> ). But it takes 3 months to see if my A1C goes down (a measure of the past 3 months of blood sugar), so I won't know for a couple more months. Yes, though depending on what your diagnosis is, you may be able to get a monitor which allows you to track your blood sugar in near-realtime. The technology has advanced a lot in the last 20 years. reply dpeck 14 hours agorootparentFor anyone new to it, the words you’re looking for are “continuous glucose monitor” (GCM), and “flash glucose monitor”. Popular brands of it in the US are Freestyle Libre and Dexcom. Their final cost to you will vary hugely by your insurance provider. Some with little to no cost, and others several dollars per day. If you like data and are analytical by nature, and since you’re on HN that’s fairly likely, I’d highly recommend using them for a little while if you can afford it. They’re great for newly diagnosed people to understand in near real time without repeated finger pricking. But those are no terrible either and much better than not managing and monitoring! Slightly annoying at first but it is something that most people can get used to fairly easily. reply didgeoridoo 16 hours agorootparentprevA1C represents the long-term glycation of hemoglobin due to blood sugar exposure. Even if you could measure it at home, it wouldn’t change fast enough for rapid tracking to be meaningful. You need to wait for red blood cells to be replaced (hence the 3 month cadence) before expecting any change in A1C. reply chimeracoder 16 hours agorootparent> A1C represents the long-term glycation of hemoglobin due to blood sugar exposure. You're not measuring HbA1C in real-time; you're measuring the blood sugar in real-time. HbA1C is a lagging indicator, used because it's a convenient and non-invasive diagnostic and a summary statistic. It's not the way you measure the short-term or medium-term effectiveness of interventions. reply didgeoridoo 15 hours agorootparentAh I misread your comment — thought you were suggesting measuring A1C at home, which wouldn’t get you anything. Re-reading, you definitely meant measure blood glucose itself. I do wonder if A1C remains a useful metric though, as perhaps it better captures the actual damage done by consistently high blood glucose. Would be interesting to see if there is research showing that CGM area under the curve is a better (or worse) predictor of clinical outcomes than A1C. reply chimeracoder 11 hours agorootparent> I do wonder if A1C remains a useful metric though, as perhaps it better captures the actual damage done by consistently high blood glucose. Both metrics have their own use. Continuous glucose monitoring is better for measuring the short-term impact of interventions, but it's also more invasive (albeit less so than it used to be) and more expensive. HbA1C is still measured for diabetic patients who have continuous glucose monitoring. One downside of A1C is that the baseline values actually vary, and it's known to be either downward biased or a particularly lagging indicator for certain groups that are predisposed to Type 2 diabetes (e.g. South Asians and people of African descent). reply SubiculumCode 16 hours agorootparentprevAncedotally, a friend spent 3 months on a keto diet, and yes they eventually quit. However, their carb shock thirst tiredness that they used to have after a modest meal stopped occurring, and persisted after stopping keto. I've heard that there is some evidence of regeneration of receptors with ketogenic diet.. perhaps occasional bouts of keto can be therapeutic? reply kshahkshah 16 hours agoprevIs this basically a book review? Anyway, keto is great. I wasn’t terribly overweight and lost 19lbs in 9 weeks with it with a cheat day included. It’s great because it’s a very simple set of very few rules. It’s easy to adhere to because it removes moment decision making around whether or not you “should” or “deserve” to have X food. It’s nearly impossible for you to eat enough calories to maintain your current weight on keto when you’re already overweight. Therefore a deficit is nearly guaranteed therefore weightless is guaranteed. On top of all that you do get better at burning fat and don’t get as hungry. The counterpoint is the tough piece - how do you come off that diet and reenter the real world where people eat carbs. How can you get pizza with friends, etc. I understand their are some who view keto as a lifestyle, good for them but most people aren’t going to do that. Beyond the simple reality of calories are calories, none of this other stuff around fat vs protein vs carbs is really very hard to understand. Simple carbs are rocket fuel, they provide a lot of energy over a very short period of time. If you’re not going to use it, once the sugar saturates your blood, the excess will be taken out and stored. Then you’ll get hungry again. Fats are slower burning, so it’s harder for you to saturate your blood quickly. This is an over simplification obviously but this is just not so hard to understand that it needs to be an endless debate over or be positioned as controversial in order to sell books. 1. Losing weight can and often will fix your diabetes. 2. Be conscious of the method you choose for weight loss, when you obtain your goal weight - what will you do long term? reply pixl97 15 hours agoparentDon't forget to add fiber on this... Modern foods are horrifically fiber deficient, as in it's removed and fed back to animals. One of the key things you said around this is... >once the sugar saturates your blood, This is the glycemic index. Our modern diet is filled with foods that are pretty much instantly available to the bloodstream. If like me you have a continuous blood sugar monitor you can see this in effect. Drink a soda and minutes later your sugar levels go up. Eat simple carbs and 15-20 minutes and it starts climbing. Move to more complex carbohydrates and items high in fiber that take a long time to break down and the ramp is slower and less steep. The problem for just about everyone is how do we get away from our simple carbohydrate based lifestyles. reply bruce511 4 hours agoparentprevI wouldn't describe my diet as \"keto\" (I don't think of it as a \"diet\" at all.) I am aware of carbs though. Specifically though the difference between \"sugar\" carbs and \"starch\" carbs. I try and minimise sugar carbs (I haven't eliminated them) and I do starch carbs \"in moderation\". With cheat days for both. I'll happily enjoy a good desert when eating out. I'll have a pizza now and then. I love re-fried potato. I only very occasionally drink soda. I don't take sugar in coffee. Each change has been introduced slowly. I stopped taking sugar in coffee. It took about a month to adjust. Now its just how it is. I reduced bread (oh man, I love good bread..) Over time I've come to enjoy a bit less, a bit less often. My weight is trending down. I'd like to lose 15 lbs, but I'm happy if it takes all year to do it. reply drjasonharrison 16 hours agoparentprevThe problem with your simple model of carbs, fats, proteins is that in people with type 2 diabetes their dietary fats have clogged up their muscle cells ability to store blood sugars as glycogen. So while removing the \"rocket fuel\" from the diet its not clear that the underlying problem has been solved. At least not until the fats have been removed from the muscle cells. And yes, transitioning off of a keto diet may lead to the reversal of the positive gains (insulin sensitivity, body composition, average blood sugar level measured using red blood cell oxidation or A1C). And agreed, controversy gains attention which may lead to book sales. reply pedalpete 8 hours agoprevThe next phase of diabetes treatment just may be sleep. 30% of people get insufficient sleep, which directly leads to increased insulin resistance the following day, along with increased appetite and craving for sugary and fattier foods, and a decrease in willpower. Diabetes needs to be treated as a whole person health, diet is absolutely key, but so is sleep and particularly as we age, as slow-wave sleep decreases naturally with age (this is what we're working on). https://www.cdc.gov/sleep/index.html reply aantix 8 hours agoparent~30% of the population are slow metabolizers of caffeine, yet the prevailing attitude is that coffee is considered a “net positive”. For those with slow caffeine metabolism, it’s killing your sleep, it’s making you anxious and eventually it’s going to cut your life short. If you’re a slow metabolizer of caffeine, your sleep quality is suffering. Cut out caffeine for two weeks - see how deeper your sleep becomes. “ Slow caffeine metabolisers are: Associated with a higher risk of heart disease [19] Associated with a higher risk of hypertension [20] Associated with impaired fasting glucose [21] May not have the protective effects against some cancers that it appears to for “fast metabolisers” [22,23] A recent umbrella review of 218 meta-analyses concluded that coffee presented statistical harm only in pregnancy and possible fracture risk for women. [4]” https://www.thesustainabletrainingmethod.com/tstm-blog/2021/... reply sombragris 16 hours agoprevType 2 here. The article text sounds strange to me, maybe because I live elsewhere than the US/UK. From Day 1 of my diagnosis (in 2009) I was prescribed a low-carb diet alongside with medication and, eventually, insulin. I was never told I could just manage my blood sugar with insulin and medication without a low-carb diet. So this guy comes across to me as preaching to the choir. reply emmanuel_1234 14 hours agoparentFrom all the diabetic I know, in France, Asia or North America, you're the only one I ever heard of with such recommendation. A friend of mine in France was actively advised _against_ a low-carb diet to control her diabetes. She did it anyway and saw incredible results. Nevertheless, and despite improvement on all indicators in her blood test, her doctor advised her to return to a normal diet and control it with insulin and \"complex\" carbs. That was in 2019. reply ramraj07 8 hours agorootparentMy mother’s doctors do similar things - partly because they have internalized the “cultural impossibility” of old Asians to cut carbs and if someone says they are cutting carbs, according to their experience it’s often a lie. It’s also possible that some of these doctors have indoctrinated themselves into believing this can never work. reply wrycoder 8 hours agoparentprevYou have an unusual doctor! Listen to them. reply sombragris 1 hour agorootparentIt's standard diabetology doctrine in Paraguay (South America). Most diabetologists do it. They say that the key to control blood sugar lies in a multi-faceted approach. It's not \"carb abstinence\", but \"carb restriction\". Carbohydrates should be greatly reduced and if you must eat them, eat complex carbs with a lot of fiber instead of sugars. But they should be reduced as much as possible. reply sgift 14 hours agoprevI'm surprised a bit that this seems to be contentious/not already part of normal treatment in the US? Or is the article wrong? Yes, Insulin is the \"last line of defense\" for Diabetes 2, but at least here in Germany dietary change (e.g. Keto or at least less carbs) is an essential part of treatment. And so are other medications (e.g. Metformin). Insulin is given if there's no other option, cause well, not giving insulin if your blood sugar is really bad leads to very bad outcomes, but if there is any other option doctors will take it first. reply phanimahesh 1 hour agoparentThis matches my experience in India. A dietician consultation is recommended on diagnosis of diabetes, however adherence to low carb diet is low. Still, insulin is considered last line of defence if diet control, drugs like metformin, glimiperide, sitagliptin etc are not sufficient to keep blood glucose in check. Indians typically follow either a rice based diet in southern regions or wheat based diet in northen regions. Doctors down south recommend eating limited or no rice and switching to wheat based roti, whereas northen doctors recommend switching to rice. I suspect this is to reduce overall consumption, in anecdotal experiences it works better than just asking people to limit their preferred grain. People eat less of unfamiliar foods. reply drjasonharrison 16 hours agoprevTaub, a science writer, has a theory and book to sell. His theory that drugs instead of diet leads to later problems is pretty well established. I don't know if he also discusses how insulin resistance in type 2 diabetes starts, or how the ability of the pancreas to make insulin in adulthood can be damaged by fat soluble poisons leaking into and out of fat cells. As an alternative to his book, I suggest looking at the research on plant based diets fat toxity. https://nutritionfacts.org/topics/diabetes/ reply ramraj07 8 hours agoparentIf nothing he says is fundamentally wrong, why not let this narrative be around. More people than not have a distaste in science nowadays and it’s beneficial to have multiple perspectives as long as they’re not scientifically inaccurate. reply kshacker 6 hours agoprevSo I have been at risk of diabetes for 8 years ... started tracking actively since I had a cardiac incident. My first 2 years of cardiac recovery were whole food vegetarian with zero added sugar and they were awesome. But then the discipline slipped and it has been one thing after the other with degrading health (mostly A1c, still not diabetic, and weight gained 30 pounds in the past 6 years). Went back to Whole Foods plant based diet since January 1st and have cut down sugar significantly. This time I am prepared by having a fridge stocked with fruits and other alternatives when I get tempted to eat. But based on what I am reading in the interview, and the other comments on this post, am I chasing a fool's errand? My personal experience says that any diet works as long as you are disciplined, and although in my social circles (work and family), it is hard to be sugar free, it is still possible (along with vegetarianism) as compared to low carb. However, I do not see any advocates for that here, are there? Btw I am planning for a modified version of something called LMK diet, LMK is not a doctor and is probably at the extreme edges of selling veganism, and I am definitely not going that far, but going as close as I can. Mentioning it just in case someone wants to counter or support it. reply zargon 2 hours agoparentGary Taubes is not a good source of information. Someone else posted this [1] review of another of his books. If you define \"work\" as losing weight, then sure any diet can indeed \"work\" if you're able to stick to it. But personally that wouldn't be my sole criteria, especially if I had had a cardiac event. I have never heard of LMK. I couldn't find any actual description of what that means except for a graphic at the bottom at lmkhealth.com, which needs clarification. Why are you wanting to follow LMK and not any of the usual WFPB doctors (Greger, Essylstyn, Campbell, McDougall, Ornish, Barnard, Fuhrman, etc.)? [1] https://thescienceofnutrition.wordpress.com/2014/04/05/good-... reply kshacker 2 hours agorootparent> Why are you wanting to follow LMK and not any of the usual WFPB doctors (Greger, Essylstyn, McDougall, Ornish, Fuhrman, etc.)? No reason. I was ready for a change to vegetarian lifestyle anyways and a friend independently added me to whatsapp group run by him. He follows all the names you mention anyways so it is all the same to me. And I am familiar with all of these names since I followed them for a while but then got confused by the alternatives. He has a few youtube videos which repeat what he says on whatsapp just that whatsapp morphs into local support groups on completing first 2 months. Not yet there. Although he is a vegan killjoy, reason to prefer him would be indianized recipes. reply kamikaz1k 13 hours agoprevIt’s pretty amazing how many different ways people approach this problem. The article scared me a little when there were anecdotes about people just using an insulin pump to keep levels in check…do people understand what insulin does? Diabetes Type 2 is an awful name and needs to be rebranded. Type 1 is fixed by insulin, Type 2 is slowed down(?) by insulin, they’re not the same thing. This is kind of like how calorie restrictions will fix your issues, how you achieve that restriction is the hard and very human problem that people endlessly debate about. So the only lesson to take away is that this is a solvable problem, and ultimately you gotta cut calories (and exercise). But how is important; so keep trying till you find something that works for you. reply brigadier132 15 hours agoprev> For Taubes, dietary fat wasn’t a problem at all. Instead, the real danger was carbohydrate, he asserted, sparking a backlash, and fuelling the ongoing conversation about what constitutes a “healthy diet” I've successfully lost 80 pounds in the past year after 10 years of yoyo dieting with low carb and counting calories. I did not become a healthy weight until I was counting calories on a HIGH CARB and HIGH PROTEIN diet and doing a significant amount of exercise 6 days a week. Meaning, 1 gram of protein per pound of lean body mass, 1 gram of fat per 1/3rd of your weight in pounds and as many carbs that fit in your calorie budget and at least 30 grams of fiber. Note, I'm not giving advice to people with diabetes but the reason there are so many people with metabolic disease in this country is not carbs. It's lack of exercise (and walking) and processed high calorie food. Those french fries you are avoiding, the reason they have so many calories is not because of carbs, it's because of fat. If you want to lose weight and you plan to also exercise, going low carb will hamstring you. The difference between running in the morning fasted and running after eating a single banana is the difference between feeling like you are being tortured and being able bounce around like a bunny. I was literally lying in bed all day with 0 mental energy on my diet because I went low carb. I failed my diet 3 times over 10 years because I believed the low carb bullshit. Go look around at how many people successfully lose weight, they all believe the same low carb stupidity. I'm running 30 miles a week and lifting 6 days a week, if I went low carb I would be tired in bed all day. If you don't have diabetes or some other metabolic disease, don't go low carb. reply DontchaKnowit 10 hours agoparentAlternatively, I did keto for years and worked as a painter in the midwest US during the summers. No breakfast, no lunch, worked 8 hours in the peak summer heat no problem, setting up/tearing down scaffold, putting up siding, etc. Never felt exhausted. Lost weoght sucessfully and was in good shape. So basically all im saying is you gotta listen to your body and adjust accordingly. Keto/low carb does not work for everyone but it definitely is very effective for some reply paulpauper 7 hours agorootparenthmm but a lot of people who do physical labor tend to be fat. I think what happens is they eat a lot off the job because the work causes hunger. Meanwhile, office people can stay slim despite not moving much. Of course, not everyone is the same. reply DontchaKnowit 7 hours agorootparentThat wasnt my point. My point was just that I was able to do keto and engage in very difficult labor in strenuous conditions without issue. Also I think lots of cheap beer has something to do with the fatness of many laborors reply kandu 14 hours agoparentprev> If you want to lose weight and you plan to also exercise, going low carb will hamstring you. The difference between running in the morning fasted and running after eating a single banana is the difference between feeling like you are being tortured and being able bounce around like a bunny. A low carb diet may still contain enough carbs to inhibit the ketone generation mechanism. Was your ketogenesis active at the time? My experience of exercising under ketogenesis was OK. reply tracker1 13 hours agorootparentSame. First time I went low carb I couldn't get through a full workout for close to a month. Adapting to keto can take longer especially if you're metabolically unhealthy to begin with. reply uranium 9 hours agoparentprevBodies vary. Years ago my partner of the time and I did low carb at the same time, eating a lot of the same meals, of course. We both lost weight. We both ran. I felt great and had a lot of energy. They felt miserable for the whole multi-month period. reply hackernoteng 15 hours agoparentprevToo low carb can actually slow down thyroid and end up increasing both LDL and A1C because overall metabolism slows down too much. healthy carbs combined with lots of exercise works. reply paulpauper 7 hours agorootparentbut isn't ketosis supposed to increase metabolism reply VirusNewbie 15 hours agoparentprev>you. The difference between running in the morning fasted and running after eating a single banana is the difference between feeling like you are being tortured and being able bounce around like a bunny. This seems really wrong for the vast majority of people. Is your average person really bottlenecked by their blood glucose levels and not say, cardiovascular health or lung capacity? I think most people get winded in a hard run way before their muscles are low on energy and they are feeling sluggish, no? reply brigadier132 14 hours agorootparentFrom what I understand the vast majority of people have very low metabolic health and their ability to use fat as energy during exercise is not well developed. Meaning they are almost exclusively using glucose and glycogen. From personal experience, when i was untrained, i could feel if i was fasted or not. Additionally if i did not eat some form of carbs 40 minutes into my workouts i would crash and burn but if i had a small gel pck i could go for another hour. I dont know the mechanism for how to glucose would even be absorbed that quickly but i did feel a noticeable boost in energy as soon as i consumed the gel. I think there is some connection between the digestive system and the brain that signals that you can use more glucose immediately after consuming food. reply peterfirefly 8 hours agorootparentprevI could tell the difference on a 10km run. The difference was around 10% extra time + a feeling of my legs being heavy. No \"torture\", though. It wasn't 100% predictable -- sometimes I felt fine fasting (and ran fine). reply marcellus23 14 hours agorootparentprevI think the assumption in that sentence is that you're already someone who runs, so you already have decent cardio fitness. And if you're a runner, yes, you can notice a massive difference between running on an empty stomach vs. running with some carbs in you. reply watwut 14 hours agorootparentprevMy experience is yes, absolutely. As in, the inexperienced beginner will feel the difference faster and more then someone in shape who runs often. Meanwhile, your body if you run often will handle lack of glucose in blood much better, at least subjectively. reply dvektor 7 hours agoprevWho else read this as: Rethinking databases - ? reply stevebmark 5 hours agoprevSkip Taubes! reply verisimi 15 hours agoprevThere are possible causes that simply cannot be considered on account of the level of investment that big pharma has sunk into that sort of treatment. No investigations are undertaken whereas such a cause should be a consideration. IMO. reply sgt101 16 hours ago [flagged]prevnext [5 more] Great to see him (and this article) conflating Type 1 with Type 2. The kind of conflation that could kill someone. Good job, theguardian. reply peterfirefly 13 hours agoparentFrom the article: Another example, for Taubes, of how early researchers were mistaken, concerns the differences between type 1 and type 2 diabetes, which are so different they almost shouldn’t share a name. reply Orangeair 15 hours agoparentprevnext [4 more] [flagged] peterfirefly 10 hours agorootparentDidn't mean to be a copycat. Hadn't seen your response when I wrote mine. reply dang 10 hours agorootparentprevPlease edit out swipes like \"What are you talking about?\" from your HN comments. They're never needed, and tend to degrade discussion. This is in the site guidelines: https://news.ycombinator.com/newsguidelines.html. reply peterfirefly 8 hours agorootparentDisagree in this case. It made the comment easier to read than my sibling comment that happened to use the exact same quote. reply epgui 16 hours agoprevnext [10 more] [flagged] dang 10 hours agoparent\"Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something.\" \"Don't be snarky.\" https://news.ycombinator.com/newsguidelines.html reply alphazard 15 hours agoparentprevSome fields attract higher levels of intellect and employ more rigorous methodologies than others. This is mostly a consequence of the economic value to be gained from working in a field, rather than anything deep about the nature of the problems in that field. There is plenty of low hanging fruit for smart people to pick by jumping over to a field that has less talent, and less sophisticated methodologies. For example many of the social sciences struggle to reproduce findings, and properly deploy the scientific method. There is no such crisis in experimental physics. reply epgui 12 hours agorootparentThere is not much low hanging fruit in biochem / medicine in 2024. It’s a gargantuan mountain of knowledge you have to get through before you have any hope of seeing the big picture. reply n2d4 9 hours agorootparentprev> There is no such crisis in experimental physics. Then why do I keep hearing about superconductors on here? This sort of thinking comes from ignorance and arrogance. When you do research at the frontier of any subject, you won't compete with the average university graduate in that subject, you'll compete with people with the highest \"intellect\" who also studied this subject for their entire lives, and you're incredibly unlikely to find a breakthrough without doing that too. The amount of Nobel Prize winners who went on to support compete bogus theories in other fields is endless. reply epgui 8 hours agorootparentThe comment... > There is no such crisis in experimental physics. ... also seems to completely miss that there is a spectrum from \"hard\" science to \"soft\" science (contrary to what many might expect, \"soft\" sciences are arguably much more difficult to work with), and that physics is the \"hardest\" (ie.: in many ways \"most straightforward\", although by this I do not mean to trivialize it at all) natural science there is. You need much more sophisticated statistical techniques & experimental design when you're dealing with complex systems (eg.: society, ecology, psychology, etc) than when you are dealing with simple systems (cue the joke about physicists assuming spherical cows). reply emmanuel_1234 14 hours agoparentprevHe's not exactly new to the domain, now. One thing I appreciate with Taubes (having read his early books, not the more recent ones -- I really liked The case against sugar), is that he has an epistemologically correct approach to the problem. He looks at it through the eyes of a rigorous scientist, unlike most people in nutrition science. For the most part, he was very cautious in his claim and spent most of his time discussing why claims accepted as correct are completely full of shit. It's likely that, through time, he built a corpus of knowledge and understanding that allow him to be a bit more assertive today (again, I've not read his recent work), but really, his early books are, in my opinion, remarkable for their thoroughness and intellectual honesty. reply epgui 12 hours agorootparent> He looks at it through the eyes of a rigorous scientist, unlike most people in nutrition science I don’t know where you get your nutrition science, but that’s a very bizarre perspective. reply emmanuel_1234 11 hours agorootparentSee for example https://en.wikipedia.org/wiki/Seven_Countries_Study reply epgui 10 hours agorootparentThere is no shortage of studies whose conclusions were later found out to be false or overly-simplistic. Nutrition science isn't bad because nutrition scientists are bad. The truth is that nutrition is a very very young science, and it also happens to be one of the most difficult health sciences. It will take a very long time for the body of knowledge in that field to take form. reply rafaelero 10 hours agoprev [–] Putting together a lot of different foods inside the category of \"carbohydrates\" is stupid, and telling people to restrict it is even more stupid. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Science writer Gary Taubes challenges mainstream views on diabetes treatment and suggests that insulin therapy may be contributing to the global epidemic of the disease.",
      "Taubes advocates for low-carb diets, like keto, as an alternative to managing blood glucose levels in diabetes.",
      "His views have sparked both praise and criticism, with some applauding his research on nutrition while others question the safety and effectiveness of low-carb diets."
    ],
    "commentSummary": [
      "The discussion explores different aspects of nutrition, diabetes management, weight loss, and the reliability of nutritional science.",
      "Various opinions are shared regarding low-carb diets, different types of diabetes, and the significance of tracking blood glucose levels.",
      "The conversation also delves into the role of diet in diabetes management and the potential advantages of different dietary approaches, while also discussing the scientific validity of nutritional science and the qualifications of experts in the field."
    ],
    "points": 106,
    "commentCount": 107,
    "retryCount": 0,
    "time": 1705247634
  },
  {
    "id": 38991317,
    "title": "Mexico Constructs Railway to Rival Panama Canal",
    "originLink": "https://www.japantimes.co.jp/business/2023/11/16/mexico-rail-rival-panama-canal/",
    "originBody": "BUSINESS Mexico building rail rival as water shortages drain Panama Canal A worker supervises construction work on a wave breaker in the port of Salina Cruz, Oaxaca, Mexico, on Oct. 9, as part of expansion works on the Interoceanic Railway that connects the Pacific Ocean and the Gulf of Mexico.AFP-JIJI BY JEAN ARCE AFP-JIJI SHARE Nov 16, 2023 SALINA CRUZ, MEXICO – At Mexico's narrowest point, between the Pacific and Atlantic oceans, the government is building a railway to rival the Panama Canal, with promises of economic bounty but amid fears of environmental and social harm. The Spanish conquistador Hernan Cortes had dreamed of such a crossing for humans and goods in the 16th century, but most plans came to naught and a prior, rudimentary connection was all but abandoned with the opening of the canal cutting through Panama in 1914. Then, in 2020, work started on a new coast-to-coast link under the government of President Andres Manuel Lopez Obrador. In a time of both misinformation and too much information, quality journalism is more crucial than ever. By subscribing, you can help us get the story right. SUBSCRIBE NOW",
    "commentLink": "https://news.ycombinator.com/item?id=38991317",
    "commentBody": "Mexico building rail rival as water shortages drain Panama Canal (japantimes.co.jp)94 points by wglb 18 hours agohidepastfavorite101 comments jp57 13 hours agoFor containers headed to the US east coast from the Pacific, is there any advantage of using this route and incurring two extra mode switches (sea->rail->sea), over going to a US west coast port and just incurring one (and possibly zero) extra mode switches? Given that the containers may be distributed in a variety of modes when they are unloaded at a land port (e.g. different trains, trucks, etc), it's probably reasonable to think about fractional mode switches. I.e. what's the average number of switches across all containers on the ship. Using a rail portage to get to from the Pacific to the Atlantic will add 2 to that number for every container, whereas going to a west coast port might add one or maybe zero (I.e. switch to a truck that drives cross country, vs rail->truck) Is the increased cost of transcontinental land transport in the US so high as to make this scheme worthwhile for US East coast? reply livueta 13 hours agoparentThis Economist article on the same topic seems to agree with you: https://www.economist.com/the-americas/2024/01/09/the-dwindl... / https://archive.is/ZU0ws > But projects like CIIT may still struggle to entice cargo away from the Panama Canal. The largest vessels that go through it can carry 14,000 containers. Mexico’s government accurately reckons that the coast-to-coast rail journey will be quicker than passing through the canal. But it neglects to mention that the trains’ capacity and the speed at which they can be loaded and unloaded mean that the overall rate of goods’ transit between the two oceans will be much slower than the canal. > Moreover, Niels Rasmussen, chief shipping analyst at Bimco, an industry association, says that carrying cargo by train or road has big snags. Most shippers would prefer to rack up extra miles on other maritime routes than to deal with the hassle of unloading and reloading. And if push comes to shove, many would probably prefer existing routes across the United States to untested road alternatives in Latin America. reply intrasight 13 hours agorootparentThe economist article also mentioned just going north around Canada reply reactordev 12 hours agorootparentSo clearly they don’t know what they are talking about. Any sailor would tell you, you don’t go above and below 55 degrees. Granted these are steel container ships but the seas don’t care. What’s one or two lost containers of goods? They sit just below the water surface for the next vessel to crash into. The Panama Canal has two paths. Only one can be used for container ships. Even that has been at capacity for quite some time. A more integrated shipping supply chain is needed to get goods from A to Z. Rail, truck, boat, and plane. reply philipov 12 hours agorootparent> The Panama Canal has two paths. Only one can be used for container ships. I believe both Panamax and Neopanamax lanes can be used by most container ships. The Neopanamax lane can accommodate the biggest new container ships in addition to that. reply jefftk 11 hours agorootparentprev> Any sailor would tell you, you don’t go above and below 55 degrees. Elaborate? There are major ports north of 55° and the US, Russia, Canada, Sweden, Norway, Iceland, the UK etc. reply reactordev 9 hours agorootparentNorth of 55 is ice sheets. South 55 is Cape Horn. Not saying it can’t be done. Just not something you just “do” except for Vikings. reply jefftk 9 hours agorootparentNorth of 55 has ice, but isn't all ice sheets. The most extreme example is probably the warm water port of Murmansk at 68° 58'. reply reactordev 7 hours agorootparentOnly because of the North Atlantic Gulf Stream bringing warmer waters. Everywhere else around there is closed in the winter due to ice. Murmansk is a freak of nature, an exception to the rule, like how you can’t freeze vodka in a normal freezer. reply halJordan 5 hours agorootparentprevThey're talking about after the ice melts friendo. reply bluGill 12 hours agorootparentprevEven with global warming you cannot expect the north around Canada path to be open every summer. It will be closed most of the year even in the best case. Maybe you can attempt it some years but it isn't a reliable route worth considering. reply bluGill 12 hours agoparentprevSea transport is cheap but slow. Mexico is much narrower than the US so they can potentially be faster than the Canal without adding much more costs. Going LA to the east cost is done, but has more expensive land transport. So this could be a useful compromise. If (big if that I have no confidence in, but still possible) Mexico puts in great sorting they can also take containers from on ship and sort them into several others to various ports on the east coast. Is it worth it? Hard to say. Sea transport is cheap, but not free. There is a lot of opportunity to reduce the costs of move switches by automation. However thee biggest advantage to this really only an come if they can sort containers to different destinations. reply wolverine876 10 hours agoparentprevRail freight costs orders of magnitude more than ocean freight over the same distance. reply bilsbie 14 hours agoprevSurely this is a mega project: but naively, what if we built say ten parallel railways 10 ft apart and moved cargo ships over the rails? Or this guy only needs to be scaled up 10X to do the job. Not outside the realm of imagination? https://en.m.wikipedia.org/wiki/Crawler-transporter I know the ships are huge and it shouldn’t be feasible but it would be interesting to run the numbers. Edit. Actually if my math is right just four of these guys could transport a fully loaded Panamax ship: https://en.m.wikipedia.org/wiki/Honghai_Crane reply dessimus 12 hours agoparentI'm no engineer, but I would not imagine that the Panamax ships are built to handle that kinda of stress while still loaded with cargo. The ships are designed to have the weight of itself and its cargo spread out pushing out against the displaced water pushing back on the hull rather evenly and not a handful of highly concentrated points. reply CapitalistCartr 10 hours agorootparentShips are made to be drydocked. They get supported on a LOT of blocking. Train cars could copy that. It's otherwise not practical, though. reply mkl 10 hours agorootparent> Ships are made to be drydocked. Not while fully loaded with thousands of containers. reply themaninthedark 10 hours agorootparentprevShips in drydock are not full of goods. reply bdcravens 12 hours agorootparentprevJust build a giant bathtub and tow that /s reply poulsbohemian 11 hours agorootparentOr - hear me out here - we build a giant man made ditch. Maybe we call it something clever, like a canal? reply adrianmonk 10 hours agorootparentprevImagine the hilarity if the giant bathtub train car derails and tips over. Has there ever been a train wreck that caused a flood? If not, there's a first time for everything. There's also the possibility that you get 100 km inland and then the bathtub springs a leak. Not a good situation if the ship's structural integrity depends on the water in the tub. reply fotta 11 hours agorootparentprevReminds me of the Falkirk Wheel https://en.wikipedia.org/wiki/Falkirk_Wheel reply elric 11 hours agorootparentOr the huge boat lift of Strépy-Thieu: https://en.wikipedia.org/wiki/Str%C3%A9py-Thieu_boat_lift reply lokjhfvvv 12 hours agorootparentprevIf it’s stupid and it works… ! reply cowthulhu 11 hours agoparentprevI’m pretty sure once you cede that you’re willing to use the energy required to move a container ships worth of weight from one end of the canal to the other, the cheaper option would be to pump water to refill the locks. Water is easier to transport than ships, and while I don’t think the efficiency of the locks helps much in this context, there might be some clever optimizations… especially if you’re willing to run a ton of high pressure pipe. reply pxeboot 10 hours agorootparentPump it from where though? It would be easy to pump and reuse the water that already goes through the locks, except that it gets mixed with salt water, so putting it back in the lake would quickly contaminate it. reply dave333 2 hours agorootparentPumping sea water to the top of the lock chain or maybe the lock next to the top would save a lot of fresh water but would slowly contaminate the lake unless there were some kind of double gate to prevent sea water used to fill the top lock leaking out when a ship enters the lock from the lake. reply throwuwu 13 hours agoparentprevBuilding the drydocks capable of submerging the rail cars and positioning the ship above them and then hauling the ship out would be the bigger challenge. Dealing with any slope with that much weight is tricky. reply dave333 14 hours agoparentprevHard enough to snake one rail line through mountainous terrain let alone 8. I can see them building a large railyard in Mexico that sorts incoming containers into separate trains depending on which ship they are destined for. reply profsummergig 13 hours agoparentprev> what if we built say ten parallel railways 10 ft apart and moved cargo ships over the rails? Interesting concept. Never thought of it before. The India-Middle East-Europe Economic Corridor (IMEC) has a similar issue. Containers will be offloaded in UAE, then travel via rail through Saudi Arabia and Israel, and then reloaded on to ships in the Mediterranean for a short trip to Greece. Your idea is much more interesting. reply albert180 10 hours agorootparentIt's a dumb idea, we already have standardized cargo containers and the technology to load/unload them efficiently. Putting a ship on 10 Train Tracks will require the development of new technology and will make every bridge (weight allowance) and tunnel (the profile would need to be big enough for the whole ship) prohibitively expensive. With normal cargo trains, which are a very efficient and cheap way to transport a lot of goods, you can just use cheap off the shelf stuff. Also transporting ships with their huge tanks of heavy oil is a huge health and safety hazard. reply jodrellblank 6 hours agoparentprevWhat if we built Robert Bartini's Ground Effect Aircraft Carrier https://dj423fildxgac.cloudfront.net/fc6b316a-43b4-410f-8de6... except as a cargo ship^H^H cargo ekranoplan which could cross land as well as ocean? reply fragmede 5 hours agorootparentwhat's the energy expenditure for an ekranoplan vs a boat per mile? reply dotancohen 11 hours agoparentprevOften suggested along this very route. Here is a 19th century depiction of a ship being pulled across the isthmus via rail: https://en.wikipedia.org/wiki/Isthmus_of_Tehuantepec#Tehuant... reply Animats 10 hours agoparentprevThere's been a rail line there for a century. It's just being improved. It's the port facilities that need major expansion. Why lift ships? Containers are designed to be easily loaded, unloaded, and carried on various vehicles. reply albert180 9 hours agorootparentSome people love reinventing rail, but just worse, more expensive and less efficient. reply foota 13 hours agoparentprevThere's precedence for this (on smaller scales), see https://en.m.wikipedia.org/wiki/Portage#:~:text=Portage%20or.... reply jodrellblank 6 hours agorootparentAnd https://en.wikipedia.org/wiki/Fitzcarraldo reply DylanDmitri 14 hours agoparentprevWeight wise it looks feasible. The hard part is getting something capable of transferring hundreds of tons that can also conform to hulls of different shapes. reply seanmcdirmid 13 hours agorootparentIf weight is ok, just put a lock on a rail: so boat enters the lock, two gates close behind it, the lock (and one gate) moves via a rail to the other side, reverse at the other ocean. reply bilsbie 14 hours agorootparentprevMaybe suspend the ship from dozens of points. I’d imagine you could weld hooks all around the hull. reply wolverine876 10 hours agoparentprevIt apparently was done in ancient Greece. Look up the Diolkos of Corinth. reply mr_toad 9 hours agorootparentTriremes had to be regularly lifted out of the water or they would become waterlogged. They were surprisingly light weight, and could actually be carried onto a beach by their crew. reply nadermx 14 hours agoparentprevOr perhaps instead of 10x wideing, just build a permanent bidirectional conver belt. reply asylteltine 13 hours agorootparentYeah all they need is a blueprint and the logistics bots will take care of the rest reply FrancoisBosun 12 hours agorootparentDoshDorington did something like that… https://www.youtube.com/watch?v=HzpUQZIr15g&t=900s&pp=2AGEB5... reply melling 13 hours agoparentprevHow fast can modern freight trains run? If they’re twice as fast as cargo ships, maybe we need fewer tracks? And a longer land route? reply sitharus 13 hours agorootparentThe Ever Given has a maximum speed of 23kt / 42km/h and carries 20,124 TEU or approx. 7660 54-ft containers. Rail figures are harder to find, but a reasonable guess based on what I can find seems to be 400 containers per train if the loading gauge permits double stacking containers. Speeds over 100km/h are easily achievable on railways without at-grade crossings. So one ship could easily require ten trains to move all its cargo in the same time, plus the time taken to offload at each end, which I guess is why this hasn't taken off. Though if it's all just transhipped to trains this could be faster than at most ports where containers have to be sorted to different destinations. Though interestingly with a dedicated right-of-way the trains don't actually need to be manned, remote control freight trains exist. reply novok 12 hours agorootparentI could foresee a parallel loading crane to train track system where the cranes take up to 10 or 20 containers at once and then put them on 10 to 20 parallel train track at once on the other side. Hand counting how wide a panamax gets, they seem to max out at about 19 or 20 containers wide and 15 containers deep. It also looks like shipping containers already can be top latched with this video: https://www.youtube.com/watch?v=GZQPxl9zssk So if you could automate this system at 30m per transfer of 10, a port with 15 cranes could do 300 containers an hour and clear a boat in about 15 hours. You could expand the system further into a full panamax width and do it in about 8 hours per panamax boat with 4500 TEUs. reply albert180 9 hours agorootparentprevFreight trains run usually at 80-120km/h on railways with at-grade-crossings (for higher speeds you would need better brakes, and more energy which doesn't really make sense for freight). At least in Europe you only need to eliminate at-grade-crossings if you want to run trains over 160km/h reply closewith 11 hours agorootparentprevIn practice, the mean speed of a container on a ship is probably close to 10 knots. A freight car would be lucky to mean 5 km/h. reply inglor_cz 13 hours agorootparentprevDoing 100 mph with a freight train is well possible, if the track can take it. reply bluGill 11 hours agorootparentThough often you want to go slower for less air resistance. reply LargoLasskhyfv 13 hours agoparentprevSomething similar to this idea exists in Russia. But for larger river ships only, so smaller scale. There is a large dam, with generating station, downstream the valley is narrow, walls are steep. Differential to top of dam is large/high. Water locks impractical. So they've built something like a dock for the ships to enter, dock closes with ship inside, and is moved on cog rails up/downhill for a few miles, including a turntable! By electromotors, powered by > 100.000V three-phase delivered via catenary from the sides! At the time this made the rounds trough the net (possibly +20 years) it all looked rather insane/gigantic/impressive. Wasted 5 minutes trying to find it, seems to be gone. shrug reply unyttigfjelltol 12 hours agorootparentKrasnoyarsk ship lift? https://en.m.wikipedia.org/wiki/Krasnoyarsk_ship_lift reply LargoLasskhyfv 12 hours agorootparentYES! Thank You! But there was more on sites like rbth, onlyinrussia, or some such. Much more detailed pictures. reply theluketaylor 6 hours agorootparentprevMore for pleasure craft than commercial shipping, but I have taken a boat through the big chute marine railway in Ontario. Very cool experience. https://en.m.wikipedia.org/wiki/Big_Chute_Marine_Railway reply FpUser 12 hours agorootparentprevThis is Krasnoyarsk dam. It is located in small town of Divnogorsk some 40km up the Yenisei river. I was raised in Krasnoyarsk and have seen this with my own eyes. reply LargoLasskhyfv 11 hours agorootparentThank you! Can you remember how it did sound when in motion? reply fooker 10 hours agorootparentBrrrrrrrbrbrbrrrrrrrrrrrrrjhjnjhnjhnnnbrrrr reply FpUser 8 hours agorootparentprevNope. I am an old fart living in Canada now and many memories are very rusted ;) reply Solvency 14 hours agoparentprevI literally just asked that on a different related post. https://news.ycombinator.com/threads?id=Solvency#38992883 Anyone? reply bilsbie 14 hours agorootparentWow! I like how you think. We should discuss more mega projects. reply loceng 10 hours agoparentprevWhy not tunnels that don't disrupt and takeover surface environment preventing nature or other infrastructure - and not impacted by weather? https://www.boringcompany.com reply albert180 10 hours agorootparentBecause it's stupidly expensive boring a Tunnel when you can just do it on the surface. Also Railway lines take up considerably less space than every Street. Weather also rarely impacts rail reply loceng 10 hours agorootparentMaybe on the surface (pun intended) it appears stupidly expensive in comparison, but I bet if you actually put a value to all of the positives then the cost would at minimum be at par. With tunnels you can also go 3D, you could go 100 tunnels deep - stacked - increasing capacity on the same line as needed; something Elon points out as part of the traffic congestion problem is that cities went 3D (skyscrapers) but traffic did not. reply albert180 9 hours agorootparentYou build that route through nothing. There is no need for tunnels, and I don't see any positives, that you couldn't remediate much cheaper. For animal movement you can just build huge animal bridges every few hundred meters. Weather is rarely a problem for train operations, tunnels create also a whole lot of new \"Negatives\". You need evacuation systems, ventilation systems, maintenance etc... reply jedberg 14 hours agoprevSo one ship will have to unload on one side and then another ship will have to pick up the cargo on the other end? So the shipping company has to have two ships, one on each side, and wait for the loading and unloading? I'm sure smarter people than me have worked this out but it seems like this would take a lot longer than the canal (which only takes 8-10 hours vs 7 hours for this train plus load and unload). Granted ships without reservations have to wait 2-20 days to transit the canal right now, but I suspect wait times for cranes at each end of this rail would be about the same as canal wait times if not longer. Maybe a shipping expert can chime in here? reply vidarh 14 hours agoparentSee e.g. [1]. As long as the Panama Canal is at capacity, they don't need to compete with the Panama Canal. They need to compete with long detours: > The Panama Canal has become so backlogged that the world’s largest operator of chemical tankers has decided to reroute its fleet to the Suez Canal. > London-based Stolt-Nielsen, which has a tanker division with 166 ships, is charging customers additional costs for the longer route, it said in an email. A bottleneck at the Panama Canal due to low water levels has prompted shippers to divert to Suez, the Cape of Good Hope, or even through the Strait of Magellan off the tip of South America. And so, presumably, they're assuming that irrespective of resolution of these immediate issues, the Panama Canal won't be able to keep up with demand. [1] https://fortune.com/2023/11/27/panama-canal-backed-up-water-... reply dave333 14 hours agoparentprevGiven multiple source ports and destination ports having the ability to sort containers is useful. Similar to Fedex/Airline hub and spoke models. Also why there is a huge railyard in North Platte. https://youtu.be/zgpMRY1gAzw?si=-zNXLdLor5bt_0vQ reply jedberg 14 hours agorootparentThis is a good point. They could have multiple rail yards that all feed into the one main transcontinental railroad. reply bluGill 11 hours agoparentprevGenerally you wouldn't have two ships, you would have dozens. This only makes sense if you sort. There are many ports across the Pacific, and many other ports on the east coast (include South American and the Caribbean!). Dozens of ships from different ports cross the Pacific and unload. Then the containers get mixed and matched with dozens of other ships on the other side going to dozens of different ports. And the reverse for the return trips. Note that the ships do not all need to be owned by the same company, the important part is the containers get where they need to be. Sometimes they are, but container logistics is often separate and even when not the shipping companies will use each other when needed to get their containers where the customer needs them. reply nickpinkston 14 hours agoparentprevAnother possible way this project could be beneficial is if the Panama Canal can't widen its locks, and hence this project could enable ships larger than that (which are more fuel efficient / cheaper) to have a far faster and possibly cheaper (depends on their costs, etc.) route. reply myself248 14 hours agorootparentOoooh that's an aspect I hadn't considered. Post-Panamax ships tend to be 2x-3x more TEUs and they continue to grow. Eeeenteresting. Plus if they're already offloading to land in the Americas, don't just put 'em onto another ship and sail up to LAX, a rail route up through Mexico itself could mean the containers arrive directly into the US without going through LAX, which has interesting implications for labor. reply s1artibartfast 14 hours agoparentprev>I suspect wait times for cranes at each end of this rail would be about the same as canal wait times if not longer. This is the wrong way to think about it. Wait times would naturally start at zero days, and then increase from there until there is no more demand or the comparative advantage runs out. reply Brian_K_White 14 hours agoparentprevTwo ships running two smaller loops is no problem at all vs two ships running two longer loops. reply jedberg 14 hours agorootparentSure, as long as you have an even number of ships in your fleet and they can be paired off to carry the same number of containers. It's certainly solvable but adds a whole new level of logistical planning to make sure there is a suitable ship available and empty to receive the containers at the other end. reply Zacru 13 hours agoparentprevIf you're shipping from northern hemisphere to northern hemisphere, then you also save to time/fuel to go down to Panama. No idea if that's significant or not. reply mlinksva 14 hours agoprevThin article, lots more at https://en.wikipedia.org/wiki/Interoceanic_Corridor_of_the_I... including a map and plans going back to 1837 and the official site https://www.gob.mx/ciit reply RagnarD 7 hours agoprevI'm wondering why the whole thing is an issue anyway. Why is there a water shortage? There's an entire ocean on both sides of the canal. Gatun Lake provides the water buffer for the canal. Currently it's freshwater based but it's also artificial to begin with. Why not transition to using salt water pumped from the oceans to top it off as needed? Seems a hell of a lot easier than trying to replace the canals with rails. reply jodrellblank 6 hours agoparent\"Every ship traversing the canal between the Atlantic and Pacific oceans requires 50 million gallons of fresh water\"[1]. And it can transit 40 ships per day[2]. So, lifting 2 billion gallons of water, just over 9 billion litres, up to the 26 metres altitude of Gatun Lake. Wondering how plausible that is, the numbers match very closely to the Dinorwig pumped-storage hydroelectric power plant in the UK[3], which stores ~9.2GWh of energy by pumping 9.2 million cubic metres of water, 9.2 billion litres, up 536 metres to Marchlyn Mawr reservoir. And it takes 7 hours to do that[4]. If the same amount of water has to be lifted only ~5% of the height, assume 5% of the energy, that would be ~450MWh/day for the Panama Canal. I see it wouldn't be good to dump ocean water into Gatun Lake, but could the lowest locks of freshwater be pumped back up, instead of released into the ocean? [1] https://ctmirror.org/2023/08/27/the-panama-canal-is-running-... [2] https://www.marineinsight.com/guidelines/10-important-panama... [3] https://en.wikipedia.org/wiki/Dinorwig_Power_Station# [4] https://www.thegreenage.co.uk/cos/dinorwig-hydroelectric-pla... reply RagnarD 5 hours agorootparentGood idea as well. reply timeagain 7 hours agoparentprevGatún Lake is 425 km^2. It provides the drinking water for Panama City. It has a watershed covering a large portion of the country. You might be underestimating the scale of the problem. reply creer 3 hours agorootparentIt's not the lake that needs to be filled. Just the locks - and the lower ones at that. reply RagnarD 6 hours agorootparentprevA refinement of the thought, then, would be to install a small modular nuclear reactor to power desalination plants to replenish the freshwater when needed. Panama City must also be threatened if the lake level is that low. reply z3ugma 11 hours agoprevIrreverent and funny firsthand look at the Corredor Interoceánico from a railroad engineer who consulted on the first trainsets. > Well There‘s Your Problem > This is a podcast about engineering disasters and systemic failures, from a leftist perspective, with jokes https://wtyppod.podbean.com/e/episode-143-corredor-interocea... reply gnabgib 14 hours agoprevhttps://archive.is/osMAZ reply Brian_K_White 14 hours agoprevWater shortages? I thought the canal connected oceans and I thought the oceans were rising? reply myself248 13 hours agoparentThe canal forms a ladder that raises ships from one ocean, up through a series of locks, into waterways that cross over the middle of the isthmus, then back down the other side. The canal itself it not all at sea level. Locks work by controlling the release of water from a high point to a low point. The water needs to already be at the high point, by tapping a river or something that's fed by rain; locks do not have pumps to supply their own water. To raise a ship, you pull it into the lock, close the doors on the low side, and open a valve that lets water from the high side flow into the lock and float the ship up. Then you open the doors on the high side. To lower a ship, you pull it into the lock from the high side, close the doors on the high side, and open a valve that lets water from the lock flow to the low side. Then open the doors on the low side and pull the ship out. The low side of one lock is the high side of the next one, so that water basically follows the ship and gets lowered with it as it descends the ladder. But the water has to get to the high middle somehow. reply redleader55 13 hours agorootparentJust adding a few small details here: 1. When the canal was built, it was deemed to hard and expensive to dig at the level of the two oceans so they imagined the locks system as a solution for that. Back then, the canal was built with US money in a time of conflict between Panama(which was part of Colombia) and Colombia. Today Panama is an independent country, but they don't have the finances to upgrade the canal. 2. They don't want to bring water from the ocean up the canal is because the canal gets its water from a series of fresh water lakes. Bringing salty water would kill the life in the lakes are around the lakes and cause a lot more problems long term. reply myself248 11 hours agorootparentAgreed, pumping sea water seems terrible, but it also seems like there'd be no reason to do that. First, at either end of the system there's a chunk of river at sea-level-ish but flowing towards the sea, so the water up at the upstream end where the lower locks are is probably not salty at all. Second, if each lock had its own pumps, doing it in 3 short lifts rather than 1 big lift to the top, then even if the one or two closest to sea-level ended up somewhat brackish, there'd be a dilution effect going up hill, and there would still be fresh water entering the system from the lakes which would tend to flush it downward. I don't think Gatun is at risk. But what would it cost? Okay, bullshit math time. In mixed units just to annoy metric purists: A quick search says each lock cycle is around 50 million gallons, or 153-acre-feet. It takes 3 steps to raise them 85 feet above sea level, in energy terms let's just model it as one big step. It takes almost exactly 1kWh of energy to raise one acre-foot of water by 1 foot, so 153 acre-feet raised by 85 feet is about 13MWh per cycle. Apparently the canal manages about 40 transits per day, which I'm interpreting as each lock cycles once, so that's 521MWh of energy being provided by the rain delivering the water to the upper lakes. Let's say we want to double that to 80 cycles a day. Oh, also, pumps are only about 70% efficient, so we actually need to provide 745MWh of power. Divide that by 24 hours and we end up with a 31MW power plant. That's..... tiny. Nah. Let's do it with solar. Roughly 6 hours peak sun equivalent per day (data for bocas del toro), we need 124MW of panels. The lakes have plenty of storage so there's no need for batteries, just run the pumps only when the sun is shining. Utility-scale solar runs around $1/watt installed. Labor is cheaper in Panama but conditions may be more challenging, I don't have good estimates for either of those so I'll assume they cancel out. Hey, it's napkin math! $124M for the panels, figure the pumps and plumbing aren't cheap either but even if that's 10x the cost, we're probably still in the single-digit billions. If they've been charging ships to use the canal like Egypt charges for the Suez, they should have quite the rainy-day, er, not-rainy-enough-day, fund for such projects. Especially if they raise prices auction-style during times of high demand, and let the high bidder through. That seems like a no-brainer. What am I missing? reply jodrellblank 6 hours agorootparentI did my own estimate ( https://news.ycombinator.com/item?id=38997124 ) via a different method, before reading down to yours, and got ~450MWh of energy needed - which is close enough to your 521MWh that I'm pretty happy. > \"Nah. Let's do it with solar.\" Why not with hydroelectric? Store power by pumping water up to Gatun Lake, generate power while filling the locks as the water falls down... reply downrightmike 11 hours agoparentprevThey use freshwater from a man made lake, and over the decades, silt has reduced capacity and then they have been in a massive drought for years. they have no real reclamation from the locks back into the lake, so each trip wastes a ton of water. Really, they should just charge enough to reflect reality and then most ship with just go to the west coast reply idlephysicist 13 hours agoparentprevI was also confused by this, I had always (perhaps naively) assumed that the canal used salt water throughout. I did some searching and found this article. > Gatun Lake, which forms a key stretch of the canal system and provides fresh water for its locks, saw little rain this year, as El Niño triggered a withering drought. https://fortune.com/2023/12/04/panama-canal-dry-backed-up-br... reply 8note 14 hours agoparentprevIt's sourced from a lake, which allows for moving locks to get between oceans that have different heights. The oceans don't actually touch each other reply Cpoll 14 hours agoparentprevThe canal is 25m above sea level at parts. reply bsdpufferfish 13 hours agoparentprevThe American engineers who designed and built this are long gone. Gatun lake was an artificial construction of the project. reply pseingatl 14 hours agoprevThere's already a trans-isthmus railroad running from Atlantic to the Pacific in Panama. There's an oil pipeline as well. The water shortage is not going to get resolved anytime soon and is a result of the third locks project, a project that did not provide for increased water for transits. Another issue is that containers stacked on top of vessel decks are not counted for the purposes of calculating vessel tolls. In essence, they ride for free. Unloading and loading individual containers means someone will have to start paying for their interoceanic trip. reply cjensen 13 hours agoprevAfter a quick glance at maps, it looks like they will also need to build docks and container facilities in both ports. This isn't a terrible idea, but it's costly and requires a lot more than \"building a railway.\" reply schiffern 12 hours agoprevPresumably posted due to this comment yesterday: https://news.ycombinator.com/item?id=38988611 reply shiroiuma 8 hours agoprev [–] They should build the railway underground to avoid the environmental issues being complained about. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Mexican government is constructing a railway to compete with the Panama Canal, connecting the Pacific and Atlantic oceans at their narrowest point.",
      "The project is expected to bring economic advantages, but there are growing worries about potential environmental and social consequences.",
      "Construction on the railway began in 2020 during President Andres Manuel Lopez Obrador's administration."
    ],
    "commentSummary": [
      "Mexico is constructing a rail system as an alternative to address transportation and water shortage issues at the Panama Canal.",
      "Concerns about the cost and efficiency of the rail system compared to maritime routes have been raised, prompting suggestions of using alternative routes such as going north around Canada.",
      "Various solutions discussed include advanced sorting techniques, parallel railways, and scaled-up crawler transporters, but concerns about stress on ships, accidents, and water contamination remain.",
      "Other proposed solutions include using existing locks, pumping water from the sea, and building tunnels.",
      "Stolt-Nielsen, a tanker division, is rerouting ships to the Suez Canal due to low water levels at the Panama Canal.",
      "The feasibility of using saltwater and modular nuclear reactors to supplement freshwater supply is being considered.",
      "There is a debate about the pumping of saltwater into freshwater lakes, with discussions around mitigating harm to the ecosystem and the feasibility and cost of implementing a system using solar or hydroelectric power."
    ],
    "points": 94,
    "commentCount": 101,
    "retryCount": 0,
    "time": 1705246507
  }
]
