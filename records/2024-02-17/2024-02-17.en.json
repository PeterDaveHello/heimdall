[
  {
    "id": 39401598,
    "title": "Designing APIs for Consistent Inert Behavior",
    "originLink": "https://devblogs.microsoft.com/oldnewthing/20240216-00/?p=109409",
    "originBody": "If you’re just going to sit there doing nothing, at least do nothing correctly Raymond Chen February 16th, 20241 2 There may be times where you need to make an API do nothing. It’s important to have it do nothing in the correct way. For example, Windows has an extensive printing infrastructure. But that infrastructure does not exist on Xbox. What should happen if an app tries to print on an Xbox? Well, the wrong thing to do is to have the printing functions throw a Not­Supported­Exception. The app that the user installed on the Xbox was probably tested primarily, if not exclusively, on a PC, where printing is always available. When run on an Xbox, the exception will probably go unhandled, and the app will crash. Even if the app tried to catch the exception, it would probably display a message like “Oops. That went badly. Call support and provide this incident code.” A better design for “supporting” printing on Xbox is to have the printing functions succeed, but report that there are no printers installed. With this behavior, when the app tries to print, it will ask the user to select a printer, and show an empty list. The user realizes, “Oh, there are no printers,” and cancels the printing request. To deal with apps that get fancy and say “Oh, you have no printers installed, let me help you install one,” the function for installing a printer can return immediately with a result code that means “The user cancelled the operation.” The idea here is to have the printing functions all behave in a manner perfectly consistent with printing being fully supported, yet mysteriously there is never a printer to print to. Now, you probably also want to add a function to check whether printing even works at all. Apps can use this function to hide the Print button from their UI if they are running on a system that doesn’t support printing at all. But naïve apps that assume that printing works will still behave in a reasonable manner: You’re just on a system that doesn’t have any printers and all attempts to install a printer are ineffective. The name we use to describe this “do nothing” behavior is “inert”. The API surface still exists and functions according to its specification, but it also does nothing. The important thing is that it does nothing in a way that is consistent with its documentation and is least likely to create problems with existing code. Another example is the retirement of an API that has a variety of functions for creating widget handles, other functions that accept widget handles, and a function for closing widget handles. The team that was doing the retirement originally proposed making the API inert as follows: HRESULT CreateWidget(_Out_ HWIDGET* widget) { *widget = nullptr; return S_OK; } // Every widget is documented to have at least one alias, // so we have to produce one dummy alias (empty string). HRESULT GetWidgetAliases( _Out_writes_to_(capacity, *actual) PWSTR* aliases, UINT capacity, _Out_ UINT* actual) { *actual = 0; RETURN_HR_IF( HRESULT_FROM_WIN32(ERROR_MORE_DATA), capacity < 1); aliases[0] = make_cotaskmem_string_nothrow(L\"\").release(); RETURN_IF_NULL_ALLOC(aliases[0]); *actual = 1; return S_OK; } // Inert widgets cannot be enabled or disabled. HRESULT EnableWidget(HWIDGET widget, BOOL value) { return E_HANDLE; } HRESULT Close(HWIDGET widget) { RETURN_HR_IF(E_INVALIDARG, widget != nullptr); return S_OK; } I pointed out that having Create­Widget succeed but return a null pointer is going to confuse apps. “The call succeeded, but I didn’t get a valid handle back?” I even found some of their own test code that checked whether the handle was null to determine whether the call succeeded, rather than checking the return value. I also pointed out that having Enable­Widget return “invalid handle” is also going to create confusion. An app calls Create­Widget, and it succeeds, and it takes that handle (which is presumably valid) and tries to use it to enable a widget, and it’s told “That handle isn’t valid.” How can that be? “I asked for a widget, and you gave me one, and then when I showed it to you, you said, ‘That’s not a widget.’ This API is gaslighting me!” I looked through the existing documentation for their API and found that a documented return value is ERROR_CANCELLED to mean that the user cancelled the creation of the widget. Therefore, apps are already dealing with the possibility of widgets not being created due to conditions outside their control, so we can take advantage of that: Any time the app tries to create a widget, just say “Nope, the, uh, user cancelled, yeah, that’s what happened.” HRESULT CreateWidget(_Out_ HWIDGET* widget) { *widget = nullptr; return HRESULT_FROM_WIN32(ERROR_CANCELLED); } HRESULT GetWidgetAliases( _Out_writes_to_(capacity, *actual) PWSTR* aliases, UINT capacity, _Out_ UINT* actual) { *actual = 0; return E_HANDLE; } HRESULT EnableWidget(HWIDGET widget, BOOL value) { return E_HANDLE; } HRESULT Close(HWIDGET widget) { return E_HANDLE; } Now we have a proper inert API surface. If you try to create a widget, we tell you that we couldn’t because the user cancelled. Since all attempts to create a widget fail, there is no such thing as a valid widget handle, and any time you try to use one, we tell you that the handle is invalid. This also avoids the problem of having to produce dummy aliases for widgets. Since there are no widgets, there is no legitimate case where an app could ask a widget for its aliases. Bonus chatter: To clear up some confusion: The idea here is that the printing API has always existed on desktop, where printing is supported, and the “get me the list of printers” function is documented not to throw an exception. If you want to port the printing API to Xbox, how do you do it in a way that allows existing desktop apps to continue to run on Xbox? The inert behavior is completely truthful: There are no printers on an Xbox. Nobody expects the answer to the question, “How many printers are there?” to be “How dare you ask me such a thing!” Another scenario where you need to create an inert API surface is if you want to retire an existing API. How do you make the behavior of the API consistent with its contract while still doing nothing useful? Raymond Chen Follow Tagged Code",
    "commentLink": "https://news.ycombinator.com/item?id=39401598",
    "commentBody": "If you're just going to sit there doing nothing, at least do nothing correctly (microsoft.com)359 points by AndrewDucker 14 hours agohidepastfavorite269 comments maerF0x0 13 hours agoI've learned this as \"swallowing errors\" and IMO it's a poor practice. Not only does it not solve the issue at hand (you cannot print on an xbox), but it actively hides how broken the software is, which makes bug discovery and testing much harder. This is one thing I like about Go's panic. You're mostly not supposed to use it or recover from it at run time. It serves as a great vehicle to blare loud sirens at testing time that you (the programmer) screwed up (and that's ok, we all do), and it's time to figure out where and how to fix it :) PS this analogy works in a lot of domains - If you have actors in a system actively trying to hide their flaws/errors it will be exponentially harder to root them out and solve the issues. reply mey 12 hours agoparentError design depends on context. For most of the systems I work on, fail fast on request for anything out of spec is the correct design. B2B/Microservices/API focused systems. Windows however has focused on bending over backwards to provide compatibility (including memory patching popular software that was broken, like Simcity https://arstechnica.com/gadgets/2022/10/windows-95-went-the-... ). In the context of a user desktop where a user has no control of correcting the system code, the final experience is more important that correctness in many cases. It doesn't matter who is \"in the wrong\". Microsoft learned that it doesn't matter why the BSOD occurred, just that bad software/hardware was giving them the bad reputation. So yeah, fail fast/halt catch fire on any invalid input/condition is my personal preferred design, but I can see the value in this approach in this environmental context. The important thing here is that context and not applying either dogmatically. Don't take Mr Chen's approach in reactor or HFT designs for example. Fantastic approach for game engines. reply PaulHoule 12 hours agorootparentIt's hard to overstate how hard Microsoft has worked to maintain backwards compatibility. Recently I had to read an old Access file and where I work we still keep Office '97 around for this purpose and it is quite amazing that it installs and works just fine on Win 11, Clippy works just fine, in fact all the other lame-ass things Office '97 does to take over your desktop all still work even if they don't quite visually match the current desktop. The thing is that Microsoft has a framework built into Windows where they can specify different system behaviors such as that Simcity case where the game didn't work with a new memory allocator so they put in a flag so an application that needs it can get the old memory allocator. They have a very systematic approach to the half-baked and haphazard process of patching the OS so you don't have to patch applications. reply delta_p_delta_x 11 hours agorootparent> It's hard to overstate how hard Microsoft has worked to maintain backwards compatibility. Here's a pretty detailed list: - It is possible to target Windows XP SP3 (released 2008, EOL 2014, 10 years ago) from Windows 11 and Visual Studio 2022[1] using C++23. Windows 2000 can be targeted with a little more setup[2]. Windows 2000 is 24 years old this year. - It is possible to run a binary written for and compiled in Windows NT 3.1 x86 on a modern 2024 Intel/AMD PC running Windows 11 with absolutely no modifications to said binary whatsoever. Windows NT 3.1 is 31 years old this year. - It is possible to write one binary and target multiple versions of Windows by simply choosing a Platform Toolset, which is paired with a Visual C/C++ Redistributable. - Windows has a run-as mode to run programs in what is essentially a super-thin VM; like you mentioned, emulating different iterations' behaviour for a given program. All four of these are nearly impossible on Linux. The fourth is essentially Docker, which is needed even to target an older glibc (i.e. the equivalent of the first situation). Windows has gone to extreme lengths to not only maintain API compatibility, but ABI compatibility as well. The fact that unmodified binaries from 20, 25, 30 years ago can run without a hitch is a testament to how important backwards/forwards compatibility still is on Windows. Side tangent: all the criticisms levelled at Windows here and in many hacker fora are limited to its userspace—things like its shell and user-space programs, and trivial complaints like 'Candy Crush', or 'start bar', or 'extra right click', or even 'PowerShell aliased curl to Get-Content, grr'. The userspace changes so often because it can; it is a superficial part of Windows. To the haters: Try actually programming with NT OS primitives and even the (undocumented) NT syscalls. It is as enjoyable as UNIX system programming, and I daresay more productive. [1]: https://learn.microsoft.com/en-sg/cpp/build/configuring-prog... [2]: https://building.enlyze.com/posts/modern-visual-studio-meets... reply heyoni 42 minutes agorootparentThis is great and all until you completely ignored the fact that wine exists and went on to call user space design questions by Microsoft “trivial”. I could also start listing ways that the Linux kernel maintains backwards compatibility with not just software but hardware that’s decades old but the list would get too long. No one is complaining that Microsoft has too much backward compatibility, it’s their utter disregard for user choices and privacy that drives away the hacker community to either Linux or even macOS. reply pjmlp 13 minutes agorootparentLinux kernel on its own doesn't run software. reply heyoni 6 minutes agorootparentBut it comes with drivers supporting very old hardware, hence the inclusion. miki123211 10 hours agorootparentprevI wish they included an 8086 emulator so that old software compiled for DOS would still run. It worked on 32 bit systems until 32 bit support was dropped, which only happened a few years ago. That was due to Intel's virtual 8086 mode, which is not available if your CPU is running in 64 bit (long) mode. Modern computers are fast enough for the emulation overhead to be negligible, even if you don't do any fancy JIT tricks and just go with a switch/case inside a while(true). I would personally make use of this, I know of a 16-bit program whose latest version was released in the late XP days, so it's not even that old. The idea there was that it was always compatible with DOS, some users might presumably still want to run it on DOS, and there's no point in breaking that compatibility if modern Windows can run it just fine. Then development stopped, 64 bit systems got more popular, and a recompiled version was never released. I guess the lesson there is that if you're keeping an API for backwards compatibility, some programmers will refuse to switch to a replacement to make their software work on older systems, making the API impossible to remove down the line without breaking modern programs. reply mook 8 hours agorootparentAt least you can still do this with third-party software like https://github.com/otya128/winevdm I guess? I imagine Microsoft doesn't see the returns for it to develop something they'll have to support for decades more… reply ahartmetz 8 minutes agorootparentprev\"The car works great technically, people just have trivial complaints about the steering wheel being made of razor blades\" That is becoming less and less of an exaggeration as Windows is progressively further enshittified. NT seems to be a nice OS at the core, but that's more about what it can do and how it is implemented than about how pleasant it is to use. Some of its syscalls are much more convoluted than the UNIX ones. Typical functions take many parameters, some of them complicated on their own, and typical invocations pass a bunch of NULL arguments (you still need to roughly understand what they mean). reply sfink 4 hours agorootparentprevI have unfortunately run into exceptions. I tried to play Neverhood on my Windows install, and it wouldn't start up. Tried the various compatibility modes. No luck. I ended up running it under Wine in Win95 mode (or similar; I don't remember the exact version) on my Fedora desktop and it ran fine. I haven't tried running too many old programs, though, so I have no sense for how common this might be. reply wvenable 10 hours agorootparentprevThe most stable API/ABI for the Linux desktop is provided by Wine. reply pjmlp 12 minutes agorootparentThankfully the year of the desktop is around the corner, by combining WSL with the original Win32. Day zero feature availability, without compatibility issues. reply ec109685 4 hours agorootparentprevThis is impressive, but other parts of Windows are so dreary. Installs of apps that throw up all over the disk, Windows Updates that mysteriously fail in unrecoverable ways 87% of the way through and cryptic error codes and procedures to dig yourself out of the jam (before you must reinstall). reply reddalo 1 hour agorootparentprev> all the other lame-ass things Office '97 does to take over your desktop all still work I'm curious about this. What do you mean? I remember using Office '97 on a Windows '98 machine, but I don't remember Office trying to take over all my desktop. reply elevatedastalt 12 hours agorootparentprevIt's hard to overstate* reply PaulHoule 11 hours agorootparentFixed reply MichaelZuo 12 hours agorootparentprevIt’s close to common sense, end users don’t care which monkey(s) threw in the wrench if they encounter an error, just that some entity did. The only caveats I can think of are that it must prominently display that it’s running in a “compatibility mode” and that any encrypted subsystems can’t revert to a lower standard of encryption, which may render the application unusable anyways depending on how tightly integrated it is. reply derefr 12 hours agoparentprevThe point of the article, in large part, is that when you’re designing an alternative runtime for a new platform, it is up to you what conditions are considered to be errors. Deciding to make a component of the runtime “inert” exists before and above the decision to make the implementation panic. In this specific case: it is up to Microsoft to decide what the semantics of “printing on an Xbox” are. It could be an error; or it could be something that “could be supported in theory in the future, but for now has no way to actually accomplish it.” This is a design choice, not a development choice. After all, in theory, you could print on an Xbox — plug in a USB printer, find a “game” that knows how to print (some enhanced port of a Gameboy game that had GB Printer support, maybe?), and tell it to do so. It’s not necessarily, fundamentally an error that a Xbox game is trying to print. You could define it as an error — but that’s a choice like any other, with trade-offs (in this case, to application portability.) You could define it as asking for the user to choose from no options, as the Xbox actually does. You could have the API lie and say it printed something. You could even actually support printing. These are all choices. It’s only once you’ve made that choice, defining the semantics of “printing on an Xbox” as an error, that it becomes an implementation/debugging problem if that error isn’t thrown — i.e. gets “swallowed.” reply freeone3000 12 hours agorootparentUWP apps that don’t require specifically desktop APIs will run on Xbox — including, for instance, Excel. reply wvenable 10 hours agoparentprev> I've learned this as \"swallowing errors\" and IMO it's a poor practice This is not swallowing errors. This, in the Linux parlance, is not breaking user-space. There are two ways to handle the situation presented: error out because the machine can never have printers or return an empty list of printers because the machine can never have printers. They're both valid but only one of them doesn't break user-space. reply gruez 12 hours agoparentprev>It serves as a great vehicle to blare loud sirens at testing time that you (the programmer) screwed up (and that's ok, we all do), and it's time to figure out where and how to fix it :) Right, but if you read the article, the author is talking less about the developer experience and more about the user experience. \"blare loud sirens\" is great if you're a tester/developer, not so much if you're an end user. When it comes to the end user, \"swallowing errors\" is preferable to crashing. reply ploxiln 12 hours agorootparentIf you read the article, this isn't swallowing errors, it's just returning more-backwards-compatible errors. > ...when the app tries to print, it will ask the user to select a printer, and show an empty list. The user realizes, “Oh, there are no printers,” and cancels the printing request. > To deal with apps that get fancy and say “Oh, you have no printers installed, let me help you install one,” the function for installing a printer can return immediately with a result code that means “The user cancelled the operation.” > a documented return value is ERROR_CANCELLED to mean that the user cancelled the creation of the widget. Therefore, apps are already dealing with the possibility of widgets not being created due to conditions outside their control, so we can take advantage of that I'm annoyed when \"modern web apps\" (or similar desktop apps) seemingly do nothing when there's some error, you don't know if you need to wait a bit, or click again (great fun when the UI jumps 1ms before your re-click), or full reload/restart ... luckily that's not at all what this article recommends! reply pjerem 12 hours agorootparentprevI’m sorry, maybe the article used the wrong example but the main issue comes from the fact that an Xbox app is trying to print something. It should fail, not for the developer experience, but because to start, there is no way the user would want to print something on an Xbox. Something is already really wrong with your app if it tries to do things like this. Also he says that apps are developed and tested on PCs and that they could print in this context. I don’t know a single thing about Xbox development but I hope you can run/debug them in the Xbox environment (or a simulation). Let me hope that nobody is running their Xbox apps/games on Windows APIs at development time and releases them on Xbox without further testing. reply jacobgkau 11 hours agorootparentThe point is that the UWP allows running apps developed and tested on PCs on an Xbox. It's for the user's convenience (not having to wait on developers to port to Xbox) as much as the developer's (not having to port to Xbox). If a user wants to run an app on their Xbox, telling them \"no, the developer didn't test this on the Xbox, so I'm not going to let you do that because you might try to print and get confused about it\" isn't what the user wants to hear. When the app tries to print because the developer was \"lazy\" and didn't test on Xbox, telling them \"I'm going to crash your app now because you clicked Print, even though I know you're on an Xbox and I could just ignore that\" also isn't helpful to the user. reply lloeki 1 hour agorootparent> The point is that the UWP allows running apps developed and tested on PCs on an Xbox. It's for the user's convenience (not having to wait on developers to port to Xbox) as much as the developer's (not having to port to Xbox). It's interesting because that's true in some way (in the sense that PC and Xbox are different), and also not true in another (in the sense that a Xbox is in a way a PC, only with a different UI paradigm) So in the latter sense UWP allows developing apps for that universal platform, and it only so happens that some apps are only designed, developed, and tested for one system (PC) these apps can run on. In a way I can see working from the Xbox up being a better way to have a robust, secure, uniform platform than the Windows 8 attempt of slapping a secondary paradigm on top of the Windows 1.0 / OS/2 descendants. I mean, the facility that underpins e.g WSL2 is exactly the same as the one that underpins Xbox game/apps segregation and Quick Resume. In a way the Xbox OS is very much like Qubes OS! In a way the OS UI we see on an Xbox device is a UI for the hypervisor itself. I would certainly be interested in a \"PC\" that is so stable, restores state exactly upon updates, \"it just works\", allows to play games, allows to run a bunch of Linux VMs, with forever perfect backwards compatibility across hardware arch changes and OS evolution through virtualisation/emulation, and has a UI that allows many kinds of inputs and scales from big screen (gamepad, that accessibility input device I can't recall the name) to desktop (kb+mouse) and possibly laptop or even tablet/phone. I mean it's not that far fetched (technically) that MS would announce tomorrow that an Xbox can run Linux, Windows 10, or even Windows 3.1 in a VM: all the facilities are there and you can even plug a keyboard + mouse today. One may philosophically balk at the idea (\"How dare you touch at my very open IBM PC! Where are my floating windows! Freedoooom!\"), but I think it makes sense technically, and it makes sense as a product, and MS has all the bricks to make it happen. reply makeitdouble 7 hours agorootparentprev> Something is already really wrong with your app if it tries to do things like this. The base philosophy of \"if it's wrong it should fail\" is primarily for developpers, and shouldn't apply to generic customer products. If it's dangerous, or will cost money, or will have severe ill adverse effects, I'd see the point. If a credit card transaction is wrong, make it fail. But short of these extremes, the default should be graceful handling of exceptions and help the customer app keep going and deliver some value to the user even if it's poorly written, mishandling the context. reply mardef 10 hours agorootparentprevPart of the context here is that UWP (universal windows platform), is a target to write-once and run on any windows platform situation. This made much more sense when Microsoft had multiple platforms running windows with just different sets of apis activated. At it's peak, this was: PC, phone, hololens, Xbox. SMS apis may only work on phone, spatial APIs may only work on hololens, printing may work on several, but not all targets. There are ways for developers to check which APIs are supported at runtime, but you can still call these APIs since they are part of the UWP surface. reply amelius 12 hours agorootparentprevMaybe then raise an assertion error, which only has an effect in development mode. reply Arainach 11 hours agorootparentNothing is being compiled here; your customers aren't running debug builds of their apps, and the API code is part of the platform and isn't running the debug bits. reply ryandrake 12 hours agorootparentprevEven as an end user, I hate when my computer seems to be trying to hide something from me. Even if I can't do anything about it, I want to know it's happening. Don't worry, Microsoft. I'm a grown-up and can handle the bad news. If I'm a layman, maybe I just dismiss the dialog and try again. But if I'm a little more of a power user, maybe I'll look up the error message and see if I can start diagnosing or helping. If you swallow the error message, I'll have zero idea that something is even going wrong! And almost just as bad: if you put up one of those useless infantilizing \"oopsie doopsie computer made a poopsie\" error messages, I still won't know what went wrong AND I'm being treated like a moron. I worked for a software company once where our software basically crashed every 2-3 hours of continuous use due to a huge backlog of technical debt, memory leaks, and years of rushing. My manager's solution to this was not to fix the bugs--it was to build a separate \"launcher\" process that would detect that the application crashed, eat the error messages, and silently re-launch it hoping the user doesn't notice. Way to treat your users with respect... reply Arainach 11 hours agorootparentThere is no error message here - more often than not it's a straight up crash. No HRESULT, no popup, just NullPointerException, straight to jail. In many cases like this, crashes like this are in app startup, so it's not like you learn not to hit a certain button - it's just that the app doesn't work, an awful UX. As called out in the article, there are (and always should be) APIs like \"IsPrintingEnabled\" so that forward-thinking apps can show better UX. These practices aren't for those apps, they're for everyone else. Also, if your app preserves state well enough that a keep-alive daemon can restore after a crash and the user doesn't notice, that is ABSOLUTELY an improvement in UX over just crashing. Sure, you should still fix the bugs, but don't let perfect be the enemy of good. reply thfuran 11 hours agorootparentprevThat is a dangerous assumption. Unless you know a great deal about every possible use case, you can't know the potential ramifications of incorrect output. Proceeding from invalid state (which would often be the result of swallowing errors) is essentially undefined behavior. reply Arainach 11 hours agorootparentThis isn't invalid state. They're not telling the app about a fake corrupt printer, they are using the API contact to represent the truth (there is no printer you can use) in a way the app already has to support reply thfuran 8 hours agorootparentBut I was responding to a general statement. reply kelnos 4 hours agorootparentAnd I still think you're wrong. If incorrect input can't be handled gracefully in a way that you can be sure nothing bad will happen, it's possible that crashing is the best option. But I think in most cases that just isn't what's going on. An unsupported API that makes a feature not work is just not a big deal. Lack of support, say, for a cryptographic primitive, could be a big deal, so you might choose to handle that case differently. reply kelnos 4 hours agorootparentprevYes, and in the case presented in the article -- trying to print on an Xbox -- we really do actually know the potential ramifications of trying to print and then being presented with no printers to print to. Simply: there are no ramifications worth worrying about. On the other hand, we do know what will probably happen if an undocumented exception gets thrown: the app will crash, possibly causing the user to lose data. reply WalterBright 11 hours agoparentprevIt has irked me for decades that when the internet connection fails, all you get is a message that it failed. But what failed? 1. the software on your computer 2. the computer's hardware 3. the ethernet cable or wifi 4. the switch 5. the router 6. the cable modem 7. the internet cable to your house 8. the ISP 9. the remote system you're trying to connect to Nothing has improved for decades. reply jeroenhd 8 hours agorootparentThis is something I kind of like about the Microsoft APIs. Their error codes aren't always perfect, but at least they give you some indication where things went wrong. From MSDN: > An HRESULT value consists of the following fields: > A 1-bit code indicating severity, where zero represents success and 1 represents failure. > A 4-bit reserved value. > An 11-bit code indicating responsibility for the error or warning, also known as a facility code. > A 16-bit code describing the error or warning. The \"reserved value\" also includes a bit for non-Microsoft code (which driver vendors and other API producers can use, although I don't know how often they do) There's a list of common \"facilities\" here: https://learn.microsoft.com/en-us/openspecs/windows_protocol... As a regular user, you will see errors like 0x8ACEF00D, but if you decode them, you can get a sense of what part of the system ran into the failure. Compared to the \"negative value indicates failure, look up the possible failures and what they mean for every function\" approach many other APIs follow, that's a welcome change. Of course there's no guarantee that Microsoft doesn't return some kind of meaningless internal E_SOMETHING_WENT_WRONG value, but for a lot of APIs, there are details hidden in plain sight. It won't tell you your ISP's fiber has snapped, but it'll tell you if the problem is within the driver, a security limitation, an HTTP error, or a generic network stack issue. reply flir 8 hours agorootparentprevI've been suffering that TODAY. \"Connection Refused No Further Information\" Well thanks pal. reply WalterBright 6 hours agorootparentI was getting the equivalent from my Roku box the other day, something like \"no connection to the internet\". Sigh. reply Am4TIfIsER0ppos 9 hours agorootparentprevAdd a parallel step there, somewhere, for DNS. My raspberry pi runs pihole but the hardware is failing somehow so the server crashes so DNS lookups fail. All existing connections are fine, direct IPs are fine, locally cached results are fine, but new lookups fail. It is somewhat fun to watch it happen. reply EnigmaFlare 8 hours agorootparentWindows has a tool to diagnose network problems, and it's usually completely useless but I think DNS not working is something it does identify. reply tinix 8 hours agorootparentprevKeep going... ... Try: ISP router intercepts DNS and drops the record because your company put private addresses on public DNS and the router has rebind protection. reply stubish 9 hours agoparentprevAs a Go programmer, how often do you check if fmt.Print returned an error? You get to assume that stdout and stderr exist for a start, even if your code is being run in an environment where that makes no sense. And what is the correct behavior for your software when IO errors start happening when you print? In a dev environment, probably crash. But in a production environment? A customers workstation? The article isn't so much about swallowing exceptions, but more about designing the system so you don't have to raise exceptions. reply Trollmann 10 hours agoparentprevAgreed this is akin to HTTP 200 {\"error\": \"Not found\"} reply samatman 10 hours agorootparentNot at all! It's like porting a program which expects there to be a TCP stack to a system which doesn't have one, and wiring up a component which responds to all HTTP requests with 404 instead of letting it hang on an infinite loop or crash. Say it uses a browser for rendering, but in the original you can also fetch websites, and the assumption is deeply baked into the code. If your choice is between playing whack-a-mole with all parts of the system which might call out, or just issuing a 404 (after all, if there's no Internet, you're not going to find a web page on it), that's a reasonable way to solve the problem. reply nonethewiser 6 hours agorootparent> responds to all HTTP requests with 404 instead of letting it hang on an infinite loop or crash How is a 404 equivalent to not throwing an error? 404 would be like throwing the error and then not handling the 404. And the equivalent of hanging and crashing in the xbox example would be hanging and crashing. reply Wowfunhappy 6 hours agoparentprevThink of it like an emulator. The goal of DosBox is to provide Doom with the environment it wants so that it will run. That will necessitate some amount of lying. reply nonethewiser 6 hours agoparentprev> The idea here is to have the printing functions all behave in a manner perfectly consistent with printing being fully supported, yet mysteriously there is never a printer to print to. That kind of thing intuitively sounds bad. But I guess of this is in a space shuttle I would take that back. reply h0l0cube 10 hours agoparentprevThis is something that Elixir nailed. The typical idioms for return types are `{:ok, ok_value}{:error, error_value}`, for which callers can pattern match against and handle appropriately. If fail-fast is desired, many functions will also have a variant signaled by a postfix exclamation mark (e.g., some_function!(...)), that returns `ok_value` or raises an exception. reply nonethewiser 6 hours agorootparent> This is something that Elixir nailed. The typical idioms for return types are `{:ok, ok_value}{:error, error_value}`, for which callers can pattern match against and handle appropriately. Agree this is a good pattern. Im not categorically against throwing errors but I like returning errors or the result and then having to check at the caller. Do this in typescript a lot. reply h0l0cube 4 hours agorootparentAny typed language could use this pattern, though with Elixir specifically it was idiomized from the very beginning and all libraries use it, which makes error handling very consistent even when piping operations. The common option to fail fast is also handy when crashing the process is preferred and no matching is required to unwrap the value reply perryizgr8 9 hours agoparentprev> Not only does it not solve the issue at hand It does actually. This approach ensures that old apps (whose authors never thought it would run on something called an Xbox) will seamlessly run and perform all functions properly except for printing, which isn't supported on Xbox. Panicking here would mean every older app has to update their code to support Xbox. reply kelnos 4 hours agoparentprevCompletely disagree. Getting back at the programmer for making a mistake isn't what matters. Presenting the best experience possible to your users is all that matters. If the user clicks on a print button in an app on the Xbox, and the app crashes (possibly losing some of the user's data), that's a bad experience for the user. Why do that to them just to stick it to the programmer? And on top of that, now some developer at some company (often not even the person who initially made the bad assumptions about printing) has now been called in on a weekend by their boss to scramble to fix the issue and push out a new release. Why do that to people? Regardless, also consider the API contract. If the printing API isn't documented to throw any exceptions, and you start doing that, you're breaking the contract. You can't blame the programmer for not considering platforms without printing support; you've documented that API as always returning something without blowing up. But that's still irrelevant; don't break user code just because you can, or because it's more expedient for you to do so. Your Go example is completely unrelated; you're talking about making things blow up at testing time, which I agree is the right thing to do. But that's also when you have full control over the code and the testing. If a third-party platform API starts behaving in ways you didn't expect it to behave, that's a whole other thing. reply l33t7332273 3 hours agoparentprev>You're mostly not supposed to use it or recover from it at run time Are you serious? I thought a key principle of Go was to handle panics in a layered way at runtime reply greatgib 12 hours agoparentprevThis is the Microsoft way to do things, this is why their products like Windows are so crappy, full of bugs, unexpected behaviors and a big dump of shit of legacy behaviors expected on top of a lot other legacy behaviors on top of another 1989 legacy behavior that every forgot. reply vikingerik 12 hours agorootparentMuch of the crappy legacy behavior that Microsoft maintains is the fault of other applications, not themselves. The classic one is the error code for the file open function. Early DOS would return error codes of only 3, 4, 5, and no more. DOS programs would actually just indirect-jump using the error number as an index into a lookup table of addresses. When Microsoft tried to add any error codes (say 6), the program would jump out into hyperspace since 6 was beyond that lookup table and that memory word could be anything. So Microsoft was stuck folding every possible file open error into code 5, and to this day that's why just about any file error in Windows just says \"5 Access Denied\". And no, Microsoft couldn't add more error codes and just let the applications break, since then nobody would buy the new operating system versions that their programs wouldn't work on. reply gruez 12 hours agorootparent> When Microsoft tried to add any error codes (say 6), the program would jump out into hyperspace since 6 was beyond that lookup table and that memory word could be anything. So Microsoft was stuck folding every possible file open error into code 5, and to this day that's why just about any file error in Windows just says \"5 Access Denied\". And no, Microsoft couldn't add more error codes and just let the applications break, since then nobody would buy the new operating system versions that their programs wouldn't work on. This doesn't pass the sniff test because 1. There's documented error codes that go all the way up to 16k: https://learn.microsoft.com/en-us/windows/win32/debug/system... 2. There's file related error codes way above 5, eg. ERROR_FILE_EXISTS 80 (0x50) The general gist of your comment is correct though. Suppose windows didn't have file locks before and they were adding it. Returning an error code like FILE_LOCKED or whatever would be much more descriptive, but would also require all existing code to handle this error case. With that in mind returning ACCESS_DENIED makes perfect sense, even if programs aren't using jump tables. reply tpm 11 hours agorootparentprev> Much of the crappy legacy behavior that Microsoft maintains is the fault of other applications, not themselves. Well. I have two or three (I am not even sure because for one email address there are probably two accounts, but no way to distinguish between them) MS Teams accounts. There is no painless way to switch between them. Currently there are probably at least three different MS Teams apps installed on my machine, one of them self-installed without my consent (my PC is not managed by an org or domain, it's mine). Switching accounts involves several confusing errors, at least three password inputs, and several complaints by MS Teams that something is wrong, but it won't tell me how to fix it. Use the browser version they said. Well for scheduled meetings it works, but for adhoc calls Firefox does not (there is no reason provided, but at least it does say outright), while Chrome seems to work but does not transmit my voice and camera. All other meeting software, of course, works. I am not the youngest anymore and this is my lifelong experience with Microsoft. Its software works barely enough to sell to corporations; poor users have to endure it. reply paxys 14 hours agoprevPeople are reacting negatively as expected, but stuff like this is exactly why you can click on a file that was written in Word '97 or a game that was compiled for MS-DOS three decades ago and it opens on your computer exactly as expected. Backwards compatibility is always messy. You either do it imperfectly or don't do it at all. reply wewtyflakes 13 hours agoparentI hear that refrain often regarding Microsoft, but with games, I have not had good luck. I resort to using GOG, which itself ends up virtualizing the environment anyway. For example, I do not think anyone would have success installing the original Sim City on Windows 11. reply awkwardpotato 13 hours agorootparentFallout 3 was unplayable (without mods/patches) on Windows 10 for many years because of its dependency on \"Games for Windows Live\" from Windows 7 reply benjaminpv 12 hours agorootparentprevYeah, people frequently trot out the Sim City classic example, but the fact is that games in particular have borne the brunt of Windows API changes. If it didn't happen we'd have no need for wrappers like dgVoodoo: software once only meant to wrap Glide calls, but now used to get around deprecation to DirectX as well. Not to mention the breaking changes to the audio stack that happened between 9x and Vista. reply weinzierl 13 hours agoparentprevA file that was written in Word '97 did not even open exactly as expected on two different computers 25 years ago. Formatting depended on the installed printer drivers. Good luck with that document today. My LaTeX files from 35 years ago, on the other hand - they just do fine. reply gruez 13 hours agorootparent>My LaTeX files from 35 years ago, on the other hand - they just do fine. Yeah, because they're plain text. A fairer comparison would be whether they render pixel perfectly as 35 years ago, which I doubt. reply rendaw 4 hours agorootparentThey're not plain text, they're a text encoded structured file format. And the file format is stable. A stable file format is the most important difference here, but the text encoding means that small incompatibilities can be solved by a human rather than needing to rely on an upstream maintainer to recognize and explicitly handle your unique situation. reply bombcar 13 hours agorootparentprevIf you compile them to DVI you'll get the same DVI file. Compiled to PDF you might get very slight differences. reply mnw21cam 13 hours agorootparentIf you're rendering to DVI, it's explicitly defined as a bug if it doesn't compile to exactly the same pixel-perfect output as a previous version. reply gruez 13 hours agorootparentprev>the same DVI file. Which is used by approximately no one. Good luck uploading your resume in DVI. reply mnw21cam 13 hours agorootparentGood luck uploading your CV in PDF. I tried that a few times and got moaned at a load that my CV must be in Word format. In the end I converted the PDF into images, and created a Word document in OpenOffice (this was a while ago) with one page image per page. Got moaned at for that too. reply ahazred8ta 11 hours agorootparentHeh. A buddy of mine once applied for a Linux job and sent his resume as a PDF. They called him back: \"OMG, you're the only applicant who didn't send a Word file. We're not worthy.\" reply derefr 12 hours agorootparentprevThe only reason they want your resume at all is to run a keyword extractor over it. If you know this, why are you making their job harder? reply Tainnor 7 hours agorootparentprevI haven't had that experience for a single job that I ever applied to. Always sent in PDFs without any issues. reply dalai 2 hours agorootparentRecruiters and large firms tend to ask for a Word file. I sent the plaintext file (minimal markdown) I used to generate the pdf to a recruiter recently. They directly asked for a docx instead. reply Tainnor 1 hour agorootparent> Recruiters and large firms tend to ask for a Word file. Again, not my experience at all. I think if a company didn't accept my PDF CV and asked for a Word file instead, I'd strongly reconsider whether I'm actually interested in working there. reply Too 1 hour agorootparentprevRecruiters probably edit the file before passing it on to the actual business... Add some more buzzwords, remove direct contact information, obscure exact project details. Some recruiters play quite dirty business and likewise assume the worst of their customers. reply wrs 13 hours agorootparentprevIf I recall correctly, the common phenomenon where you open a Word document, immediately close it, and are asked whether you want to save your changes (???) has something to do with printer settings. reply alpaca128 13 hours agoparentprevA file from 1997? Maybe if you get lucky, but there's also a good chance that it will simply look wrong in any office application you try. With Word you can't even guarantee that reopening the file on the same computer & version won't change its formatting. That was the reason I learned TeX. And games? Microsoft broke numerous games by shutting down GFWL. I had to pirate Dark Souls as my original copy wasn't playable until it got re-released on Steam. reply gruez 13 hours agorootparent>With Word you can't even guarantee that reopening the file on the same computer & version won't change its formatting. Source? This seems utterly bizarre. reply apetresc 12 hours agorootparentBefore the new '.docx`, '.xslx', etc. formats, when it was still just .doc, .xsl, etc., the document format was (as I've heard it told) essentially just a memory dump of Word/Excel's state for that document at the time you saved. And since it's easy to imagine that serializing/deserializing such a complex thing might not be always 100% perfectly idempotent, it would indeed happen that just the act of opening a file would change it in some subtle way. That hasn't been the case in a long time, though. reply jeffparsons 3 hours agorootparentWorse than subtle formatting changes was some kind of corruption that could slowly swallow bits of your document without you noticing. Sometimes going back before the corruption had visible effects was not good enough to stop it from manifesting again. I remember back in school religiously saving new versions of Word documents so that if the newest version went bad I could start a fresh doc, then copy and paste sections from old versions via Notepad to cleanse the corruption. (Direct Word-to-Word could bring it across — I guess that tells us something about how they implemented copy+paste between documents, too!) reply reddalo 1 hour agorootparentprevYou're right, Office documents were essentialy a memory dump, but only for OfficeTo clear up some confusion: The idea here is that the printing API has always existed on desktop, where printing is supported, and the “get me the list of printers” function is documented not to throw an exception. If you want to port the printing API to Xbox, how do you do it in a way that allows existing desktop apps to continue to run on Xbox? The inert behavior is completely truthful: There are no printers on an Xbox. Nobody expects the answer to the question, “How many printers are there?” to be “How dare you ask me such a thing!” If he just used the words \"existing desktop apps\" in the second paragraph of the article instead of the second to last, I don't think people would be reacting so negatively. Instead, when he talks about avoiding \"gaslighting\" by returning an invalid pointer by just gaslighting in a different way by lying about the error that occurred, it almost sounds unhinged. reply dupertrooper 14 hours agoparentprevUmm but you can’t find a current version of Word that actually does that. Try it sometime. reply EvanAnderson 13 hours agorootparentHere's a rather large (5MB) vintage 1996 Microsoft Word document: https://archive.org/download/cd-pn-0527a/CD_ROM.ISO/APPENDIX... That file opens and appears to render just fine for me in Word 2016. This has been my experience w/ Office files in general. Old files open and render very well in newer versions. reply weinzierl 13 hours agorootparentThe point made above was \"opens exactly as expected\". So, does it look like the authors intended in 1996? I doubt it. reply EvanAnderson 12 hours agorootparentJust flipping thru it I'm not seeing obvious issues. In terms of being usable for reference I think it's reasonable. The diagrams and tables look intelligible and not garbled. Layout isn't visible screwed-up. Will it be a 1-to-1 with printed output from 1996? Probably not. reply MichaelZuo 12 hours agorootparent“exactly as expected” is a much higher bar to clear then roughly as expected. reply Arainach 13 hours agorootparentprevIf you \"doubt\" it, please provide a specific example of what you believe is inaccurate in the document rendering. reply morsch 13 hours agorootparentAs it happens, we just had this discussion regarding a Word document from 1990: https://news.ycombinator.com/item?id=39357709 reply judge2020 13 hours agorootparentprevWas able to do it just fine with these government files[0,1] I found on Google. Word version 16.82 on Mac (Apple Silicon too). 0: https://www.legis.state.pa.us/cfdocs/Legis/LI/uconsCheck.cfm.... 1: https://archive.ada.gov/briefs/andersbr.doc reply weinzierl 13 hours agorootparentThe point made above was \"opens exactly as expected\". So, does it look like the authors intended in 1996? I doubt it. reply Waterluvian 11 hours agoparentprevI wish this was actually truthful. I’ve had a gigantic headache getting every childhood game I kept working. In almost every case I gave up or re-bought from GOG, basically paying for the work done to sort out compatibility. reply bakugo 13 hours agoparentprevI assume the negative reactions are due to the large Apple user base on HN. Most of these people are likely so used to being told \"this software was built for a version of your OS released 2 years ago so you can't use it anymore, tough shit\", they've come to expect it. reply spiderice 11 hours agorootparentYou just created a scenario in your head that confirmed an existing bias in your head and then assumed that your made up scenario matched reality. Very weird to read. reply pas 12 hours agorootparentprevnah, it's because all of this is just lip service. windows11 is getting worse and worse. and cool that Mr Chen thinks about UX and compatibility, but MS as whole does not really. important business apps got special treatment, and that's it. (MS and its corporate entourage solved this mostly by providing Windows and \"security\" patches for it, for a lot of money.) reply tsimionescu 14 hours agoprevI find nothing quite as frustrating as UIs that suggest a device could exist, but it's not there right now. I then have to spend time to discover that these devices are not supported, and that screen was just some mock someone came up with. reply kentonv 14 hours agoparentSure, but you'd be more frustrated if your app just crashed. If the app is not prepared for printing not being supported, and printing just throws an exception, the app probably just crashes. That's bad. If the app is prepared for printing not being supported, it should be calling the explicit API to check if printing is supported, and then not displaying the UI for printing if it isn't. The article is not about these apps, it's about what to do about apps that fail to check first. The bug is with the app, but a good platform does its best to make bad apps work anyway, rather than have them just crash. reply yungporko 1 hour agorootparentthese aren't the only two options. first of all, i certainly wouldn't be more frustrated if the app just crashed, but how about just letting the user know that the xbox can't print? why does everybody seem to think that's out of the question? reply nerdponx 10 hours agorootparentprev> Sure, but you'd be more frustrated if your app just crashed. I don't know about that. If it crashes, I might not try that thing again. reply tsimionescu 13 hours agorootparentprevTo me, a better solution would be for the system to pop up a notification informing the user that printing is not supported on this platform, and then whatever other solution is OK. reply hn_throwaway_99 13 hours agorootparent> To me, a better solution would be for the system to pop up a notification informing the user that printing is not supported on this platform That's the whole point of this article - when you can't control the various platforms that your application is run on, you want to try to do the \"least bad\" thing, even if that platform didn't anticipate the situation of, in this case, telling people that printing isn't supported. reply kentonv 13 hours agorootparentThe article is the other way around -- it's written from the perspective of a platform developer trying to deal with apps that didn't anticipate the situation. reply hn_throwaway_99 13 hours agorootparentThanks for the clarification, you're correct. The nuance/complexity is that the platform developer must conform to a platform spec (in this case, the Windows API) that usually makes some underlying assumptions about the capabilities of the system that runs it, which may not always be correct. reply kentonv 13 hours agorootparentYes, I'd add to that by pointing out that the \"spec\" is a de facto spec subject to Hyrum's Law, that is, people have built their apps against other implementations of the platform and inevitably don't handle any behavior not seen on those other implementations. reply nonethewiser 6 hours agorootparentprevYes exactly, but Microsoft isnt the one who can implement that. reply tsimionescu 2 hours agorootparentThis was about the Xbox printing subsystem, right? Of course they can implement that. reply nonethewiser 6 hours agoparentprevPerhaps the software author is to blame here then not Microsoft. Because they didnt handle the printer case. Where Microsoft comes in is that the software doesnt crash because the software provider didnt account for printers. reply elwell 12 hours agoprev> With this behavior, when the app tries to print, it will ask the user to select a printer, and show an empty list. I see Microsoft has not learned from 30 years: https://www.reddit.com/r/hacking/comments/djvzd/windows_nt_l... reply nonethewiser 6 hours agoparentThat os amazing reply _Algernon_ 2 hours agoprevThe only reason an X-box doesn't support printing is because Microsoft has defined it as such. A x-box is just as capable to print as the Windows machine in the next room over from a hardware perspective. Which means this \"solution\" to a stupid, self-created problem is stupid. I bet this workflow will lead at least a handful of people to ask \"How do I add a printer to my X-box?\" instead of \"Fuck the app crashed, better not do that again.\" Microsoft has so much stupid, corporate driven decision making. Can't wait for it to follow in IBM's footsteps. reply happytoexplain 14 hours agoprevThe specific thing the author is suggesting is correct (components should suffer before users do), but I take great offense to their framing. \"There may be times where you need to make an API do nothing.\" \"The wrong thing to do is to have the printing functions throw a Not­Supported­Exception.\" No, no, absolutely no - what the author is describing is a hack to support a shitty client. Yes, you have to do this sometimes. No, it is not normal or generalizable advice. The way they've worded this betrays the internalization of their suffering as a MS developer. reply Scarblac 13 hours agoparentNo, not a shitty client, just one that was written before you decided to make this change. If your API breaks it while it doesn't change, it's the API that's being shitty, not the client. reply Aaargh20318 12 hours agorootparent> No, not a shitty client, just one that was written before you decided to make this change. Has the Xbox ever supported printing? Why even have these APIs? Just don’t offer them at all in the Xbox SDK. You shouldn’t be able to compile code that tries to call these when targeting Xbox. reply skykooler 11 hours agorootparentIt's an API that also targets Windows. Apps that use it can run on both platforms. reply delta_p_delta_x 10 hours agorootparentprev? Why even have these APIs? Xbox essentially runs Windows. Therefore, the Windows API is available on Xbox. reply codexb 12 hours agorootparentprevIt depends. Did the API document that it could raise an exception if printing isn't supported? Is that unhandled in the client? Then yes, it's a shitty client. If you're talking about making backwards-incompatible breaking changes to an API, that's another thing. reply derefr 12 hours agorootparentIn the context of the article, no, the existing API never threw (or was documented as capable of throwing) NotSupportedException. The article: > The app that the user installed on the Xbox was probably tested primarily, if not exclusively, on a PC, where printing is always available. I.e. there is no concept in the (desktop) Windows printing subsystem of printing as a feature not being available; on desktop Windows, printing as a feature is available by definition. reply ryandrake 12 hours agorootparentI think this is the critical piece that should determine how you handle the error. If the API, as used by the client, when the client was programmed, was expected to potentially throw NotSupportedException or return an error code, then doing so sounds like the right way to go. Throwing/erroring is part of the API contract and the client should handle NotSupportedException. If the API was never expected to throw NotSupportedException or return an error code, then all of a sudden starting to throw in a future release breaks the API contract, and the client cannot be expected to handle it sensibly. EDIT: Duh! My comment is useless. This was the exact conclusion that the article came to, too. So instead of reading this comment chain, just finish the article :) reply TillE 12 hours agorootparentprevThe mention of Not­Supported­Exception is a little confusing because that's a .NET thing, and the rest of the article is talking about Win32, which is a plain old C API where exceptions don't exist. I guess he's implicitly talking about a C# API built on top of Win32. It's entirely correct to return error codes, and any client is expected to handle that. reply derefr 12 hours agorootparentClients are not expected/required, however, to behave correctly in the face of unknown error codes that did not exist or have defined semantics at SDK-version-selection time. Actual exceptions — in languages that have them — are a bit different, in that they “do” something by default: they propagate and unwind unless they’re caught. And so, new (unchecked, if in a language where that’s relevant) exceptions can be introduced after the fact under the presumption that the runtime’s default behavior for propagating unknown exceptions will kick in, and that this will trigger the desired result for applications that weren’t written in awareness of that particular exception. Error codes aren’t like that; they have no default semantics as a class, only explicit default semantics per function and/or per algebraic error-code type, if-and-where documented as such. New error codes simply aren’t meant to be introduced in most APIs, unless those APIs have rules about e.g. generalized default handling for defined code ranges that new codes can be later slotted into. Basically, error codes are weak enums. If you’re writing a dynamic-link library (or a runtime — same thing, basically), you’re not supposed to add new potential values to an enum used as a return value: client code will likely have been written to switch on the value with the known options as cases, and your new value won’t be in there. reply felsokning 9 hours agorootparentprev> I guess he's implicitly talking about a C# API built on top of Win32. Correct. The error `ERROR_CANCELLED` is defined in WinErr.h and it's translated via `HRESULT_FROM_WIN32` in the same[1]. It was this change pointed out in `CreateWidget` to get around returning the null pointer that he was suggesting would be better to make the API inert. [1] - https://learn.microsoft.com/en-us/windows/win32/api/winerror... reply kentonv 13 hours agoparentprevIf you are building a software platform which has been adopted by more than a handful of developers then you're going to have some shitty apps, in which case you have to do this stuff. Yes, it is completely normal and generalizable to all platforms that have wide adoption and take backwards compatibility at all seriously. Raymond is behind arguably the foremost exemplar of this (Windows) but the same thing happens in the Linux kernel ABI, glibc, the web platform, Java, and so on. reply GMoromisato 14 hours agoparentprevThe issue is that sometimes you need to implement an API which existing clients are already using. You can't go back in time and re-compile, much less re-write the clients. It all depends on how the clients are currently using the API. If the clients are already testing for NotSupportedException, then that's what you should return. But if not, you need a different approach. reply pas 12 hours agorootparentwell, that seems like an important piece of the contextual puzzle, because the Xbox printing example by itself seems extremely dumb. (issuing/publishing games on Xbox is not a wild wild west of unknown binaries, if someone cannot get the source code for their thing they are very unlikely to get their thing on Xbox. yes, there are probably binary blobs, but still, to me in this context it's ridiculous that instead of a an exception handling wrapper on the application side the platform does these fakes.) reply Arainach 13 hours agoparentprevThe \"shitty client\" was written long ago by a company that is out of business. YOUR customers rely on that client, and if it stops working because of a change you made they will blame you. reply kolektiv 14 hours agoparentprevBroadly speaking I agree with you in principle - but in practice over time you come to realise that there are only shitty clients. They might not intend to be, they might not have started out that way, but time is a destructive thing... reply orange_fritter 13 hours agoparentprev> The way they've worded this betrays the internalization of their suffering as a MS developer. While I feel less strongly about this Xbox/printing example, I remember Bill Gates saying \"I reboot my computer every day\" which is a similar mindset-- this culture has been forced to adopt a certain form of \"hygiene\" due to that same culture NOT adopting hygiene preemptively when they built their systems. reply bitwize 13 hours agoparentprev> No, no, absolutely no - what the author is describing is a hack to support a shitty client. If you were on the Windows team and I was your manager, I might fire you for that. This is Windows. You do not break applications that users depend on. Ever, ever, ever. If you have a \"shitty\" or misbehaving client application, you use whatever workarounds, shims, and compatibility hacks it takes to make the application work as expected. Even if it means having special memory management code for SimCity so it will work. reply xp84 12 hours agorootparentI really appreciate this approach. It actually takes, in my humble opinion, much more discipline (and requires checking ego at the door) to implement this idea, compared to the “my code just throws exceptions or a 400 Bad Request anytime you aren’t following the most recent revision of our API” mindset. I work on Web apps which are much closer to that second approach usually, but if I were working on a platform (NT) which was 30 years old and will live another 30 years in all likelihood, you 100% have to take the approaches Raymond Chen is explaining. “We’ve decided software that operates your 100 ten-million-dollar-each CNC machines is ‘shitty,’ so buy new equipment so you can get Windows 11” isn’t going to fly, it would get the whole company laughed out of the room. Especially because what is allegedly making them shitty would just be someone’s failure to predict some minor detail about the future of the platform. APIs do have to change, and the creative part is thinking of how they can be safely rendered inoperable without damaging anything that didn’t predict that removal. Sure, you can’t print, but that’s in this example a deliberate design decision of the platform, probably for good reasons. reply Joker_vD 1 hour agorootparent> “We’ve decided software that operates your 100 ten-million-dollar-each CNC machines is ‘shitty,’ so buy new equipment so you can get Windows 11” ...and then you'll also have to re-write your software to support Win 11, of course. reply lhamil64 13 hours agorootparentprevI also took issue with the Xbox example, and I think it's a context issue. When I read it, I interpreted it as an API for new apps/games to use. In that case, they should be testing on an actual Xbox before shipping! And why would the app be trying to print in the first place if it's designed to run on an Xbox? But I suspect the situation this is being designed for is existing PC apps being ported to an Xbox. OK, in that case I can see fudging the API since there might be obscure corners that the devs didn't think about that try to print. But I'd still say the devs should be testing on an Xbox and make an effort not to attempt to do things when it doesn't make sense for the platform. reply dwattttt 2 hours agorootparentJust to add further context; this isn't apps being ported to an Xbox, they notionally need no porting; the Xbox is a \"Windows\" system. The only actor in this scenario is the platform developer figuring out how an existing API should behave if called in a context the authors very much didn't expect (who expects their Windows app to be run on an Xbox?) reply anonacct37 12 hours agoprevI love and hate this. On a visceral level I don't like dealing with issues via malicious compliance. OTOH I absolutely agree this is a good call if your goal is for more users to be able to run more software on your platform, even if printing is broken. reply 1970-01-01 12 hours agoprevThis is also very good for security. Correctly dismissing an API call means these programs aren't able to go off and do extra naughty things. reply lucianbr 14 hours agoprevOnce upon a time it was considered a great strategy for browsers to make the best effort to display a page, even if the html code had errors. Just try to guess the author's intention as well as you can, and go ahead. Errors are bad. Users don't want errors. I thought we learned from that experience. Apparently not. reply kentonv 14 hours agoparentBrowsers still follow that strategy. The effort to replace it with strict validation (XHTML) failed. What succeeded instead was HTML5, which went back and explicitly defined how every possible invalid sequence should be interpreted, so that browsers would at least be consistent about it. reply wvenable 10 hours agoparentprevBrowsers being permissive with what they accept is what made the modern web possible. If early browsers displayed an error the first time it came across antag, instead of just best-effort rendering the page, then every new browser feature would have been smothered in the cradle. reply _gabe_ 5 hours agoparentprevGo type in: HelloWorld! And see the errors that now show up because of the wisdom we gained… wait, it still renders alright? I guess we still have much to learn then. Or, there might be something to gracefully handling certain classes of errors while crashing on exceptional cases. reply subarctic 14 hours agoparentprevWhat did you think we learned? reply lucianbr 14 hours agorootparentThat every single behavior that attempts to guess the users's intent becomes a relied upon part of the software, and this over time grows to an impossible amount of cruft. See this, hilariously hosted also by microsoft: https://techcommunity.microsoft.com/t5/discussions/funny-sto... > And then Google built Chrome, and Chrome used Webkit, and it was like Safari, and wanted pages built for Safari, and so pretended to be Safari. And thus Chrome used WebKit, and pretended to be Safari, and WebKit pretended to be KHTML, and KHTML pretended to be Gecko, and all browsers pretended to be Mozilla, and Chrome called itself Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13, and the user agent string was a complete mess, and near useless, and everyone pretended to be everyone else, and confusion abounded. reply eastbound 13 hours agorootparentIn 2024, so we still have websites that are served differently depending on the User Agent? Could we possibly do away with the User-Agent header, or reset it to a simple “Chrome 150.01”? reply dwattttt 2 hours agorootparentTry google.com, last month: https://support.google.com/websearch/thread/252500737/google... reply KTibow 11 hours agorootparentprevI vaguely remember plans to make the user agent simpler but I can't find anything that matches with some quick web searching. The most relevant article I could find was from 2013: https://hacks.mozilla.org/2013/09/user-agent-detection-histo... reply xp84 12 hours agorootparentprevMeh. For all the silliness, it’s not that hard to parse one for the main purpose it should be used for, which is determining in aggregate what platforms your users use. As such, we still need things like OS in there anyway. So it wouldn’t get quite that short. Also I’m amused checking mine that the user agents still say “Intel Mac OS X” even on Apple silicon. I guess they were afraid someone’s parser would think we are on PowerPC without that! reply debacle 14 hours agoparentprev\"We\" did. Microsoft didn't. Not unexpected. reply recursive 14 hours agorootparentIs this part of a fan fiction where MS isn't amazing at legacy compatibility? reply Peter0x44 13 hours agorootparentI think their BC is overrated. Wine manages to do a better job. reply johngossman 6 hours agoprevThe point the critics of this piece seem to be missing is that there is not an exceptional case here that needs an error to be thrown. There are NO printers attached to the device (in this case by definition), and an app should cleanly handle that case, not crash. You wouldn’t throw an exception on a laptop just because it couldn’t reach the printer. reply fullstackchris 3 minutes agoprev> The idea here is to have the printing functions all behave in a manner perfectly consistent with printing being fully supported, yet mysteriously there is never a printer to print to. What? Why would you ever do this? This post screams of \"can't see the forest from the trees\" Sure, now you don't have your app crashing, and instead you have this weird state of seeming like you can print but ultimately can't, wasting users time and perhaps frustrating them even more. Why not literally just show a dialog like \"printing is not supported from an Xbox\"? Easy to develop, easy to understand... everyone wins. reply denysvitali 10 hours agoprevWouldn't it be simpler to just create a \"Save to PDF\" printer and use that in the Xbox? This way the API stays compatibile, and it actually makes it useful reply Throw6away 11 hours agoprevMicrosoft’s MDM APIs will return a 500 (server error) on a VM if you try to get a list of wireless networks, instead of the saner approach of returning an empty list. The same APIs will return a 418 (I’m a teapot) if you mistakenly try to add an already-existing setting, instead of just updating it as one might expect. They get points for originality, but don’t follow the advice in this article. reply yau8edq12i 4 hours agoparentMicrosoft has around 238000 employees according to Wikipedia. That they are not all strictly following Raymond Chen's latest blog post isn't surprising. reply patapong 12 hours agoprevHmmm I see and appreciate the idea behind this. That said, it seems confusing for the user to be shown an empty list of printers if no printers can be installed on the platform. In my opinion, it would be preferable to show the user a dialog that says \"This platform does not support printing\" and, once they dismiss the dialog, inform the program that the user cancelled the print action. reply city41 10 hours agoparentI agree, using the empty list to communicate to the user the situation is making an assumption about the user. A lot of users have never set up or dealt with printers ever. I can say for sure that an empty list of printers would confuse a lot of people in my household. reply armchairhacker 8 hours agoprevSlightly tangential, this reminds me of how to `sleep` in JavaScript without using async: https://stackoverflow.com/a/37575602 reply SillyUsername 9 hours agoprevException safety guarantees have been around for decades and were originally defined by David Abrahams who was on the C++ standards board. As the author is a Microsoft employee I'm a little surprised no attribution or reference to the original author of (the levels of) exception safety guarantees which his article partly describes. Raymond Chen (article author) is refering to the \"no throw guarantee\" in his article as opposed to one of the other outcomes https://en.m.wikipedia.org/wiki/Exception_safety reply sirspacey 10 hours agoprevI missing what the magic is here. This seems like a long way to go for “we know that this device is an Xbox and Xbox doesn’t support printing, let’s tell the user” Only it doesn’t tell the user. It leads them down a path implying implementation is possible. On the other hand, I now know why so many issues I’ve troubleshooted on MS products end in tears and complete confusion as to why they wouldn’t just say “this is not a feature you can use on this device.” reply filleduchaos 9 hours agoparent> This seems like a long way to go for “we know that this device is an Xbox and Xbox doesn’t support printing, let’s tell the user” You're missing the fact that Xbox can run apps developed for (desktop) Windows, and that the scenario in the article is about the platformend user relationship and does not involve the app developer at all. reply comicjk 6 hours agoparentprevI agree. The author's description of what is \"reasonable\" sounds like a blueprint for many of my late nights struggling with Windows: \"apps that assume that printing works will still behave in a reasonable manner: You’re just on a system that doesn’t have any printers and all attempts to install a printer are ineffective.\" Maybe the underlying assumption is that the users will give up before spending an \"unreasonable\" amount of time on these tar pit features. reply yungporko 1 hour agoprev\"The idea here is to have the printing functions all behave in a manner perfectly consistent with printing being fully supported, yet mysteriously there is never a printer to print to.\" this is exactly the kind of stupid shit that makes me hate using anything from microsoft. on what planet is this desirable behaviour? is it beyond microsoft's capability to just show a message saying \"Printing is not available on Xbox\"? reply crooked-v 10 hours agoprevDoing nothing correctly sounds like hard work. I guess I'll try it out sometime later when I have more energy. reply hailmac 14 hours agoprevWhy /wouldn't/ I want to print something from my Xbox? reply Arainach 13 hours agoparentThe printer drivers don't run on XBox. XBox isn't exposing all of the legacy Win32 APIs and driver models available in desktop. reply mrguyorama 12 hours agorootparentIt's an x64 CPU running most of Windows with plentiful and fully supported USB ports. Why CAN'T I print from my Xbox? I can print with my phone, so why not my Xbox? I'm sure there's something about \"windows print subsystems are pretty terrible and we should excise them from anything we can\" and \"reduce code to reduce bugs\" but like.... There's no technical reason it can't reply kbolino 12 hours agorootparentYou go to print a document. Your printer is temporarily unavailable. The print job hits the spooler and sits in a queue. You get bored and launch a game. The printer becomes available. The print driver has to convert the job to PCL or whatever proprietary format the printer uses. Your game hangs or even worse crashes because it's not really designed to run with background tasks on the same cores. Who do you blame, especially since the usual diagnostic tools of a PC with a full operating system aren't available? It's a feature very few people will use with lots of sharp edges that people accept on a PC but would find intolerable on a video game console. reply Arainach 11 hours agorootparentprevIt's not running \"most of Windows\". In particular, as far as I know (no insider knowledge), it's not running any of the Win32 APIs (USER, etc.). So that rules out any printers which require coordination with their desktop app (which, to be honest, to get full functionality is still a shockingly high number of them). It's highly unlikely the XBox hypervisor implements any of the printing APIs, even UWP because this is a video game console, and not only does almost no one own a networked printer, an even smaller group of essentially no one has a printer, opens documents on their video game console, and wants to print them. Features aren't free. What Xbox feature would you drop to have an engineer go implement printing APIs on them? Is that the best use of their time? reply daveoc64 14 hours agoparentprevDue to the input device most people will use (controller), apps that work best on Xbox are those used to consume content like photos or videos. You might be viewing something and want to print it, but that's clearly not desirable enough for Microsoft to build print functionality into the Xbox UI. reply maerF0x0 13 hours agorootparentI mean, it has USB ports. If I can plug it in, I'd like to be able to use it _somehow_ , even if that means installing another app that does direct port/device access or w/e . ie, it should be possible (but not necessarily super easy) reply Arainach 6 hours agorootparentThe explicit point of video game consoles is to be a locked down experience, not a PC. Hardware access is generally used for piracy, which game developers do not like. reply o11c 11 hours agoprevMaking it a linker error doesn't even seem to be considered. Xbox is a separate target so a recompile is needed anyway ... reply Arainach 10 hours agoparentNo, it's not - that's the point of the Universal Windows Platform - compile once, put in store, run everywhere. This also isn't just about new platforms. If you have a desktop operating system and want to change the behavior of an API in a future version, you have to do this same process. reply evnc 4 hours agoprevFrom the title alone, I initially thought this would be about meditation. reply iamtedd 13 hours agoprev> the function for installing a printer can return immediately with a result code that means “The user cancelled the operation.” I hope this is never shown to the user, because one of the most infuriating things that can happen is when the computer tells me that I did something I actually didn't. reply benjaminpv 12 hours agoparentOr more broadly, don't do something unbidden on my behalf then complain when it doesn't work. I swear 75% of the times I've daydreamed about a return to a single-tasking OS like DOS was on account of software trying to 'help' me. reply xp84 12 hours agoparentprevIt would be unlikely that it would be, since to any properly implemented app, the fact that the user cancelled is not an error since it thinks you did it yourself on purpose. On the other hand, I recall a few years ago (in the days of one of Apple’s least stable Mac OS [X] releases) seeing the message frequently after a random kernel panic “You restarted your computer because of a problem.” And I would be pretty ticked, saying “Nope! YOU restarted yourself. 0% of this was me. I was sitting here and you just died and rebooted.” They changed it to use passive voice instead, a year or two after that! reply Wowfunhappy 6 hours agoprev> “I asked for a widget, and you gave me one, and then when I showed it to you, you said, ‘That’s not a widget.’ This API is gaslighting me!” The entire point, it should be noted, is to gaslight the app. But the API must be good enough at gaslighting that the app doesn't discover the lie. reply philipwhiuk 10 hours agoprevI'm sorry but that code sample is an incomprehensible eyesore. And I've written PHP4. reply jeroenhd 8 hours agoparentWindows headers like uppercase. They also like very explicit types, as far as the language accepts them, of course. For example, HRESULT really is just a 32 bit integer with special rules (top bits set subsystem that cause the error, bottom bits set error code, hence the conversion function for best practice). Stripping out the Windows types and compile-time validation and the wrappers, and picking non-Windows error codes, you can turn the code into more Unix-like C++: int32_t CreateWidget(int32_t* widget) { *widget = nullptr; return ECANCELED; } int32_t GetWidgetAliases(wchar_t* aliases, uint16_t capacity, uint16_t* actual) { *actual = 0; return EBADF; } int32_t EnableWidget(int32_t widget, bool value) { return EBADF; } int32_t Close(int32_t widget) { return EBADF; } However, this code may not function if you're building for 32-bit Windows and it may not work on every compiler; it just assumes certain bit sizes that the API only guarantees in the form of typedefs. reply LASR 10 hours agoparentprevWhat's crazy is that I didn't even realize it until you pointed it out. I've been doing Win32 for so long it's like reading the green symbols in The Matrix. This all looks \"normal\" to me. reply zerr 9 hours agoparentprevBesides being unreadable, it is well known that Win32 API code is unwritable. It is only copy/paste-able. reply fidrelity 14 hours agoprevYour ecosystem has gone so complex that you can't test it anymore. So instead of handling errors properly you suggest to implement a convoluted user flow that offers always failing actions (install a printer when there's none available). If that's really the suggestion of product and engineering leadership at MSFT no wonder all their products...err...work as designed. reply nearbuy 13 hours agoparentNot at all what he is saying. Microsoft has no problem testing their printer function across all platforms in their apps and removing the print button on platforms where it's not supported. Ideally, everyone would do this. This is for other developers who are writing 3rd party apps for Microsoft's platforms and who don't always test perfectly. They may have written their app primarily for Windows and didn't consider what happens when someone installs it on their Xbox and clicks \"print\". In that case, the API should \"just work\", instead of crashing with an error message. reply gruez 13 hours agoparentprev>Your ecosystem has gone so complex that you can't test it anymore. An operating system with billion+ install base, and millions of developers tends to be complex. I'm not sure what you're proposing here, have your platform/software be more niche? Somehow get all of them to fall in line? >So instead of handling errors properly you suggest to implement a convoluted user flow that offers always failing actions (install a printer when there's none available). How are you going to \"handle errors properly\" if the publisher of the software in question went out of business? reply Arainach 13 hours agoparentprevWhat is \"your ecosystem\" here? Part of running a platform is that the code running on it is written by other people, not you, and that your customers will be relying on that code. When the software your customers are relying on breaks because of a change you made, it doesn't matter whose \"fault\" it is. It's broken, it broke because of something you did, and you have burned customer trust - not the trust of whoever wrote the software you believe is \"wrong\" or \"to blame\", trust of your platform and company. reply xp84 12 hours agorootparentIt is so obvious from reading these comments here who is not, and would never ever want to be, a platform engineer. It’s hard for some people to grasp that you need to not only interact with, but thoroughly accommodate, software written by others whom you don’t control (often due to the one-way direction of time flow). reply anonms748473 9 hours agoparentprevFrom my experience in other MS orgs, this was the prevailing approach. There are masses of bandaids on top of systems that have organically evolved over decades. Leadership is generally promoted from within so have a blind spot to how organisation incentives lead to these technical outcomes. reply cratermoon 8 hours agoprevThis is also like Null Object or Special Case pattern. It simplifies error handling, and because error handling code is often poorly tested it ends up being a big source of catastrophic failures. Making illegal states unrepresentable, or as John Ousterhout puts it: define errors out of existence: https://wiki.tcl-lang.org/page/Define+Errors+Out+of+Existenc... reply leaf8937 12 hours agoprevthrow checked exception so that xbox will have to handle the exception. xbox implementation can donothing if they want. reply Arwill 10 hours agoparentSometimes this kind of error handling is the cause of program slowdowns. So the API fails, but the program retries anyway, and so fails again endlessly. The user doesn't see anything from this happening, only that \"the app is slow\". Windows is plagued by this kind of behaviour. Its both what the article suggests can cause this, but throwing and ignoring exceptions can have this effect too. The goal, IMO, would be to force the app not to try (or not to try once it failed) something that is bound to fail. reply Jabrov 14 hours agoprev\"The idea here is to have the printing functions all behave in a manner perfectly consistent with printing being fully supported, yet mysteriously there is never a printer to print to.\" This must be satire. Otherwise I can't comprehend how something as infuriating as this could be presented as a good or smart thing to do UX-wise reply kentonv 14 hours agoparentNo, this is a guy who has been one of the architects of Windows for more than 30 years giving you hints about how they achieved their reputation for amazing backwards compatibility. He really, really knows what he's talking about. reply elevatedastalt 14 hours agoparentprevNo it's not satire. It's a demonstration of the excellence in backward compatibility and cross-compatibility that Microsoft has always been known for, and that so-called modern players like Google and Apple don't take seriously. Just because an XBox can't print doesn't mean that an app that prints shouldn't run on XBox. It should just mean that the printing function shouldn't run. How to achieve that is exactly what Raymond Chen, a Microsoft and Windows veteran, is trying to explain here. reply daveoc64 14 hours agoparentprevIf the two options are \"this app can't run on your Xbox\" or \"this app runs on your Xbox, but the printing functions don't work and some of the messages are a little weird\", which option do you think users would prefer? reply chrisjj 11 hours agorootparentBut the first option is not \"this app can't run on your Xbox\". It is \"This app possibly cannot run after trying to print\". reply Arainach 5 hours agorootparentCode like this often runs during application startup. It's not just printers. reply chrisjj 1 hour agorootparentThe situation is \"What should happen if an app tries to print on an Xbox?\". I do not believe code that tries to print often runs on app startup. reply bastawhiz 13 hours agoparentprevSo what, you crash the app? What if there's no API to say \"there's no printing on this device\"? What is the difference between a device where printing isn't supported and a device where a printer isn't set up yet? Showing an empty list of printers isn't a bad solution, frankly. reply Jabrov 12 hours agoparentprevAhh I completely misunderstood the article from a quick reading. It seems like the only solution to a tricky problem. Thanks to anyone who replied to me pointing that out. It's a good example of when one should read the article more closely before commenting! reply avery17 14 hours agoparentprevWhat do you suggest as an alternative? reply recursive 14 hours agoparentprevThe alternative is that the app fails to run at all. Doesn't seem like satire to me. reply GMoromisato 14 hours agorootparentOr worse, crashes at a random spot because it tried to initialize the printer stack. reply bombcar 13 hours agorootparentAnyone who has spent some time with vintage or otherwise older software (and even some new stuff) should have experienced a \"don't do this, it explodes\" - often the cause is something akin to what Raymond is pointing out. The software asks for something, gets back an answer it can't handle, explodes. reply toss1 13 hours agoparentprevPerhaps you missed the context in which it was made clear that this imperfect solution was still less bad than all other options. It seems the point resembled: 'this device doesn't support printing, don't return obscure, unhelpful or crashing errors; return something that makes sense so that the user can figure out and move on'. I.e.: Fail gracefully. reply Contortion 13 hours agorootparentI wouldn't call creating mystery failing gracefully. As a user I would prefer a full crash to the application gas lighting me by suggesting there's something wrong with my setup (we can't find your printer vs you're not allowed to print). If anything creating mystery about what the app is doing is the direct opposite of good UX. reply xp84 12 hours agorootparentUser spends two hours creating content. User hits print. 1. App crashes back to the desktop/Home Screen/etc 2. App can’t find any printers. User cancels and saves the document, prints elsewhere. You really think option 1 is better? Also, none of this precludes the ability to display a message. He’s talking about the system calls themselves and how they should respond. I suppose if the call for the “Add Printer Wizard” is called, it could not only return the “user cancelled it” status but also display an error message. It depends on the implementation details and whether someone probably 20 years ago foresaw the need for a modal but not fatal system error message to be triggered by that API call. Which again is not in the today developer’s control. reply chrisjj 11 hours agorootparent> 1. App crashes back to the desktop/Home Screen/etc Why do think it would crash, rather that simply return to main loop? reply toss1 10 hours agorootparentSo it 'simply returns to main loop'. I.e., the user hits [Print] and then it maybe flickers and goes back to the home screen. How in heck is that more useful than App reporting that \"I can’t find any printers\"? reply chrisjj 9 hours agorootparent> the user hits [Print] and then it maybe flickers and goes back to the home screen. And reports the exception. reply chrisjj 11 hours agorootparentprev> I.e.: Fail gracefully. There's no grace in misleading the user and wasting his time. reply alpaca128 13 hours agoparentprevMS being okay with the user clicking somewhere and never getting any reaction out of it explains many of my interactions with Windows 10. reply forrestthewoods 14 hours agoprev> Well, the wrong thing to do is to have the printing functions throw a Not­Supported­Exception. The app that the user installed on the Xbox was probably tested primarily, if not exclusively, on a PC, where printing is always available. When run on an Xbox, the exception will probably go unhandled, and the app will crash. Exceptions are such a catastrophically bad idea in almost all cases. It is absolutely infuriating as a programmer to call a function and not know if it can throw and, if it can, not know what it can throw. It’s a disaster. I so desperately wish that C++ had Rust’s Result type and pattern matching. std::optional and std::expected are kinda sorta ok, but you really want compiler enforced pattern matching. What a tragedy. reply mike_hock 13 hours agoparent> if it can throw and, noexcept. Granted, it may not have the annotation but still not throw in reality, but a Rust function can also declare a Result return type but never actually return an error. > if it can, not know what it can throw How do you square that with backward compatibility? If you declare what you can throw, what do you or any of your transitive dependencies do when they want to change their implementation in a way that makes a new error a possibility? Either you shove it all into an UNKNOWN error that you hopefully declared from the get go to be future-proof, or you break all your downstream users every time. So either no compatibility, or over time your API converges to a meaningless UNKNOWN error code for almost everything and a bunch of legacy error codes that are never used. reply forrestthewoods 12 hours agorootparentI’ve never, ever seen a codebase that reliably used noexcept. boost and Python are my arch-nemesi because they make heavy use of exception errors and it makes the APIs an absolutely miserable nightmare to use. > How do you square that with backward compatibility? Huh? The same way normal code handles changing function arguments or return types? I have absolutely never relied on exceptions to “future proof” return error types! Yikes. The only thing I have ever used exceptions for is to escape bad input/data. It gets trapped internally in the “private” API and same result types are returned through the public API. I would much rather have a return type of Result than T but also maybe it throws or maybe not and you almost definitely forgot to check which is why people do filthy filthy hacks like the OP article. reply marshray 11 hours agoparentprev> It is absolutely infuriating as a programmer to call a function and not know if it can throw and, if it can, not know what it can throw. It’s a disaster. Every nontrivial function can fail (in C even a unary operation on an int), it's just a question of whether and how the caller is informed, how easily the failure is to ignore and continue processing in an undefined corrupted state, and what percentage of the code ends up being dead branches for impossible-but-unprovable error paths. > I so desperately wish that C++ had Rust’s Result type and pattern matching. I'm a fan of Rust too, but what do you think panic() is? Rust has no concept of code that always succeeds. It's all-but impossible on current CPUs. https://github.com/search?q=repo%3Arust-lang%2Frust+path%3A%... https://doc.rust-lang.org/std/result/enum.Result.html#method... reply thimp 14 hours agoprevIt's a crap analogy. Why would you expose a printing API on a device without printing support? Just ifdef it out on the build. reply gruez 13 hours agoparentHow about an app originally designed for iPhone but is running on an iPad? iPhones can make calls, but iPads can't. Sure, a properly designed iPad app wouldn't try to use the phone functionality, but what if the developer is too lazy to develop a dedicated iPad app? Surely a crappy iPhone app running on iPad is better than no app at all? reply chrisjj 11 hours agorootparent> Surely a crappy iPhone app running on iPad is better than no app at all? Bogus comparison. Non-crappy apps are a thing. reply Arainach 10 hours agorootparentThere are many forms of \"crappy\". Developer hygiene is one bar; user functionality is another. \"Crappy\" here refers entirely to developer hygiene, which the user does not care about at all. If the platform changed and Angry Birds crashed, the customer would not accept \"but look at all of these other Angry Birds clone games still available\" as an answer, they want the app they are familiar with. reply alpaca128 13 hours agorootparentprevWouldn't that be part of Apple's app reviews? reply gruez 13 hours agorootparentOnly if you try to publish it as an iPad app. See also: https://hacknicity.medium.com/how-iphone-only-apps-appear-on... reply Arainach 13 hours agorootparentprevThere are plenty of apps already in the store. Apple wants to launch their new device and have tons of apps available. The best way to do that is for the apps to just run, not to require millions of developers (plenty of which no longer exist) to rebuild. reply nearbuy 13 hours agoparentprevIt's an option, but it has its own downsides. It's a cross-platform API. Someone wants to show some guests some of their photos and videos on their TV through their Xbox. Unfortunately they can't because their app (eg Samsung Gallery or iCloud or whatever) included a print button somewhere. And with these inert functions, if Microsoft one day adds printing support to the Xbox, it'll just work in the photo app without any update. reply mminer237 12 hours agoparentprevBecause then you're making every developer make a special build for every device. Most developers aren't going to care enough to make a dedicated Xbox or tablet build. You're just going to make the platform completely dead. It would be much better if all pre-existing apps could mostly run on them. reply Arainach 13 hours agoparentprevWhen you have a platform, apps aren't always being rebuilt for it. There is no \"build\" because the source code was lost when all of the company's assets were sold off in 2002. There is no \"build\" because the developer made a game, put it on your app store, moved on to the next game and has no interest in supporting this game that isn't providing any significant new revenue. There is no \"build\" because this is FooManager 6.0, the company that makes FooManger wants everyone to buy FooManager 7.0 and isn't making any further changes to 6.0. etc. etc. etc. reply Analemma_ 14 hours agoprevI understand the logic here, and I'm aware Microsoft has a large number of convoluted backward-compatibility requirements, but this seems like drinking to solve your problems and just putting off the inevitable hangover. To be clear: what you're doing here is lying to the user and the developer. Maybe that's justified in isolation, but now this lie is one more bit of \"hidden state\" you have to keep track of in further development and integration testing. And just like in the real world, lies have a tendency to compound on themselves until you're completely lost in them and have no idea what reality is. I have a feeling that \"solutions\" like this are part of why an increasing number of my computing problems take the form of, \"I tried to take an action, nothing happened. No error, no activity, nothing.\", and are impossible to debug or diagnose. UX designers made themselves terrified of ever showing an error code to a user, but they took that and replaced it with a world where your shit just doesn't work, and when you try to figure out why, all the OS does is shrug. reply mike_hock 13 hours agoparentSo what's your solution then? Break all apps simultaneously that do not have extensive tests for gracefully handling cases that were impossible when they were created? This is not weird \"hidden state\" on the implementer's side. It's a straightforward dummy API and all you need to do to test it are a few straightforward asserts that it returns the correct dummy values and doesn't crash. > I have a feeling that \"solutions\" like this are part of why an increasing number of my computing problems take the form of, \"I tried to take an action, nothing happened. No error, no activity, nothing.\", and are impossible to debug or diagnose. That's precisely not what this is. The whole point of the article is to do nothing correctly. Presenting an empty list of printers is consistent with a PC that simply doesn't have any printers installed. A wrong thing to do would be, for example, presenting a dummy printer that accepts jobs but of course never prints anything. You don't break the users of your API, period. That shouldn't be controversial. Unfortunately, too many people seem to think they need to do everything over every couple of years only to produce a solution that is no more extensible and resistant to bitrot than the one they're replacing. reply chrisjj 11 hours agorootparent> So what's your solution then? Break all apps simultaneously that do not have extensive tests No test required. Just try... catch. > for gracefully handling cases that were impossible when they were created? You can never know they are impossible. That is why you have a top-level try...catch. reply filleduchaos 9 hours agorootparent> No test required. Just try... catch. That doesn't even make any sense. Where on earth would you insert a try...catch? So many people commenting on this article seem to have fundamentally misunderstood what the situation/scenario even is, which is made more jarring by the fact that so many others seem to have gotten it just fine. reply buttercraft 10 hours agorootparentprevThis is about 3rd party apps that Microsoft has no control over. reply FreeFull 14 hours agoparentprevAs far as alternatives to what the article suggested go, I think the ideal solution would be to have a compile-time error, so the developers never even get as far as having their printing code try to run on the Xbox. And since it's a compile-time error, there doesn't need to be any kind of runtime error handling or cost either. reply paxys 14 hours agorootparentExcept now you have lost the ability to run all the software that cannot be fixed and recompiled for whatever reason. reply FreeFull 14 hours agorootparentYou could still offer the mocked APIs, but have them be opt-in. Also, having the compiler throw an error doesn't mean that an older, already compiled executable won't work. reply jonas21 13 hours agorootparent> Also, having the compiler throw an error doesn't mean that an older, already compiled executable won't work. And when the older, already compiled executable calls the API, how do you make things \"work\"? That's exactly what the article is about. reply Arainach 13 hours agorootparentprevApps aren't being compiled here. Apps already exist and new platforms and platform functionalities are being made available. Launching platforms is an eternal chicken and egg problem: You want to launch a new platform. Users want apps before they buy into the platform. Developers want users and a guarantee they will spend money before investing time and money building apps for the platform. If you're really really rich, you can literally pay developers to write apps for you, but ask Microsoft how that went with Windows Phone. Instead, the most reliable way (I said most reliable, not reliable) is to leverage an existing group of apps onto your platform. Maybe you have a phone ecosystem and want to get into tablets. So you take the already existing phone apps in your store and let them run on your new platform. There is no recompiling. There is no build. There is no \"build\" because the source code was lost when all of the company's assets were sold off in 2002. There is no \"build\" because the developer made a game, put it on your app store, moved on to the next game and has no interest in supporting this game that isn't providing any significant new revenue. There is no \"build\" because this is FooManager 6.0, the company that makes FooManger wants everyone to buy FooManager 7.0 and isn't making any further changes to 6.0. etc. etc. etc. Binary compatibility is what matters. Nothing else is of particular significance. In addition, if recompiling an app for your new platform yields compile errors and requires work (as opposed to targeting a new platform version in a manifest and just hitting build), developers on your platform are significantly less likely to spend the time to do so. reply notatoad 13 hours agoparentprevas the article says, the correct solution is to provide an API that applications can access to check if the functionality is offered at all. functions that return correct null responses isn't the ideal behaviour, it's the fallback for when you're already off the happy path. reply crabmusket 10 hours agorootparentI think there's a difference between an API that can be used to check if functionality is available, and an API that must be used before that functionality is offered to the application. E.g. CheckForPrinting : () -> Maybe PrintToken ShowPrintDialog : PrintToken -> Dialog reply ace2358 14 hours agoparentprevThis resonates with me. My computers used to crash at the app level, OS level or hardware level. Now when something goes wrong the system just kinda gets lazy and stops working. But it won’t crash. I’ve had my mac pinwheel on the login screen. I can still access the shell and file shares remotely, and even screen share to a logged in user. But login.app refuses to crash so it’ll just pinwheel. I’m glad I never forgot to turn things off and on for troubleshooting :( reply bombcar 13 hours agorootparentMy experience is that macOS especially really REALLY absolutely hates when the Internet goes sideways - not actually down, but really bad packet loss; DNS starts being slow and failing sometimes but not completely ... then you enter hell. Next time it happens try turning network connections all the way off. reply Analemma_ 14 hours agorootparentprevI have a suspicion that \"number of crashes\" got badly Goodharted inside major OS vendors. After all, you can \"prevent\" most crashes by just having a top-level \"catch(Exception e) {}\" handler, which of course just leads",
    "originSummary": [
      "Designing APIs to handle situations where certain functionalities are unsupported is crucial for maintaining consistency in transitions.",
      "Implementing inert behavior in APIs is essential for ensuring developers are not confused and aligning with documentation.",
      "Emphasizing the predictability and consistency of API behavior, even when inactive, is vital for effective usage."
    ],
    "commentSummary": [
      "Microsoft is focusing on maintaining compatibility with older software through emulators and third-party software.",
      "The discussion involves error handling practices in software development, emphasizing user-friendly error messages and the challenges of graceful error handling.",
      "Debates revolve around error handling in Xbox and Windows platforms, managing backward compatibility, and the complexities of error handling in programming languages to ensure customer trust, platform stability, and a seamless user experience."
    ],
    "points": 360,
    "commentCount": 269,
    "retryCount": 0,
    "time": 1708110694
  },
  {
    "id": 39400352,
    "title": "Essential Git Config Options for Productive Development",
    "originLink": "https://jvns.ca/blog/2024/02/16/popular-git-config-options/",
    "originBody": "Popular git config options Hello! I always wish that command line tools came with data about how popular their various options are, like: “basically nobody uses this one” “80% of people use this, probably take a look” “this one has 6 possible values but people only really use these 2 in practice” So I asked about people’s favourite git config options on Mastodon: what are your favourite git config options to set? Right now I only really have git config push.autosetupremote true and git config init.defaultBranch main set in my ~/.gitconfig, curious about what other people set As usual I got a TON of great answers and learned about a bunch of very popular git config options that I’d never heard of. I’m going to list the options, starting with (very roughly) the most popular ones. Here’s a table of contents: pull.ff only or pull.rebase true merge.conflictstyle zdiff3 rebase.autosquash true rebase.autostash true push.default simple, push.default current init.defaultBranch main commit.verbose true rerere.enabled true help.autocorrect 10 core.pager delta diff.algorithm histogram core.excludesfile ~/.gitignore includeIf: separate git configs for personal and work fsckobjects: avoid data corruption submodule stuff and more how to set these changes I’ve made after writing this post All of the options are documented in man git-config, or this page. pull.ff only or pull.rebase true These two were the most popular. These both have similar goals: to avoid accidentally creating a merge commit when you run git pull on a branch where the upstream branch has diverged. pull.rebase true is the equivalent of running git pull --rebase every time you git pull pull.ff only is the equivalent of running git pull --ff-only every time you git pull I’m pretty sure it doesn’t make sense to set both of them at once, since --ff-only overrides --rebase. Personally I don’t use either of these since I prefer to decide how to handle that situation every time, and now git’s default behaviour when your branch has diverged from the upstream is to just throw an error and ask you what to do (very similar to what git pull --ff-only does). merge.conflictstyle zdiff3 Next: making merge conflicts more readable! merge.conflictstyle zdiff3 and merge.conflictstyle diff3 were both super popular (“totally indispensable”). The main idea is The consensus seemed to be “diff3 is great, and zdiff3 (which is newer) is even better!”. So what’s the deal with diff3. Well, by default in git, merge conflicts look like this: >>>>>> somebranch I’m supposed to decide whether input.split(\"\") or text.split(\"\") is better. But how? What if I don’t remember whether or is right? Enter diff3! Here’s what teh same merge conflict look like with merge.conflictstyle diff3 set: >>>>>> somebranch This has extra information: now the original version of the code is in the middle! So we can see that: one side changed to the other side renamed input to text So presumably the correct merge conflict resolution is return text.split(\"\"), since that combines the changes from both sides. I haven’t used zdiff3, but a lot of people seem to think it’s better. The blog post Better Git Conflicts with zdiff3 talks more about it. rebase.autosquash true Autosquash was also a new feature to me. The goal is to make it easier to modify old commits. Here’s how it works: You have a commit that you would like to be combined with some commit that’s 3 commits ago, say add parsing code You commit it with git commit --fixup OLD_COMMIT_ID, which gives the new commit the commit message fixup! add parsing code Now, when you run git rebase --autosquash main, it will automatically combine all the fixup! commits with their targets rebase.autosquash true means that --autosquash always gets passed automatically to git rebase. rebase.autostash true This automatically runs git stash before a git rebase and git stash pop after. It basically passes --autostash to git rebase. Personally I’m a little scared of this since it potentially can result in merge conflicts after the rebase, but I guess that doesn’t come up very often for people since it seems like a really popular configuration option. push.default simple, push.default current These push.default options tell git push to automatically push the current branch to a remote branch with the same name. push.default simple is the default in Git. It only works if your branch is already tracking a remote branch push.default current is similar, but it’ll always push the local branch to a remote branch with the same name. push.autoSetupRemote and push.default simple together seem to do basically the same thing as push.default current current seems like a good setting if you’re confident that you’re never going to accidentally make a local branch with the same name as an unrelated remote branch. Lots of people have branch naming conventions (like julia/my-change) that make this kind of conflict very unlikely, or just have few enough collaborators that branch name conflicts probably won’t happen. init.defaultBranch main Create a main branch instead of a master branch when creating a new repo. commit.verbose true This adds the whole commit diff in the text editor where you’re writing your commit message, to help you remember what you were doing. rerere.enabled true This enables rerere (”reuse recovered resolution”), which remembers how you resolved merge conflicts during a git rebase and automatically resolves conflicts for you when it can. help.autocorrect 10 By default git’s autocorrect try to check for typos (like git ocmmit), but won’t actually run the corrected command. If you want it to run the suggestion automatically, you can set help.autocorrect to 1 (run after 0.1 seconds), 10 (run after 1 second), immediate (run immediately), or prompt (run after prompting) core.pager delta The “pager” is what git uses to display the output of git diff, git log, git show, etc. People set it to: delta (a fancy diff viewing tool with syntax highlighting) less -x5,9 (sets tabstops, which I guess helps if you have a lot of files with tabs in them?) less -F -X (not sure about this one, -F seems to disable the pager if everything fits on one screen if but my git seems to do that already anyway) cat (to disable paging altogether) I used to use delta but turned it off because somehow I messed up the colour scheme in my terminal and couldn’t figure out how to fix it. I think it’s a great tool though. I believe delta also suggests that you set up interactive.diffFilter delta --color-only to syntax highlight code when you run git add -p. diff.algorithm histogram Git’s default diff algorithm often handles functions being reordered badly. For example look at this diff: -.header { +.footer { margin: 0; } -.footer { +.header { margin: 0; + color: green; } I find it pretty confusing. But with diff.algorithm histogram, the diff looks like this instead, which I find much clearer: -.header { - margin: 0; -} - .footer { margin: 0; } +.header { + margin: 0; + color: green; +} Some folks also use patience, but histogram seems to be more popular. When to Use Each of the Git Diff Algorithms has more on this. core.excludesfile: a global .gitignore core.excludeFiles = ~/.gitignore lets you set a global gitignore file that applies to all repositories, for things like .idea or .DS_Store that you never want to commit to any repo. It defaults to ~/.config/git/ignore. includeIf: separate git configs for personal and work Lots of people said they use this to configure different email addresses for personal and work repositories. You can set it up something like this: [includeIf \"gitdir:~/code//\"] path = \"~/code//.gitconfig\" url.\"git@github.com:\".insteadOf 'https://github.com/' I often accidentally clone the HTTP version of a repository instead of the SSH version and then have to manually go into ~/.git/config and edit the remote URL. This seems like a nice workaround: it’ll replace https://github.com in remotes with git@github.com:. Here’s what it looks like in ~/.gitconfig since it’s kind of a mouthful: [url \"git@github.com:\"]insteadOf = \"https://github.com/\" One person said they use pushInsteadOf instead to only do the replacement for git push because they don’t want to have to unlock their SSH key when pulling a public repo. A couple of other people mentioned setting insteadOf = \"gh:\" so they can git remote add gh:jvns/mysite to add a remote with less typing. fsckobjects: avoid data corruption A couple of people mentioned this one. Someone explained it as “detect data corruption eagerly. Rarely matters but has saved my entire team a couple times”. transfer.fsckobjects = true fetch.fsckobjects = true receive.fsckObjects = true submodule stuff I’ve never understood anything about submodules but a couple of person said they like to set: status.submoduleSummary true diff.submodule log submodule.recurse true I won’t attempt to explain those but there’s an explanation on Mastodon by @unlambda here. and more Here’s everything else that was suggested by at least 2 people: blame.ignoreRevsFile .git-blame-ignore-revs lets you specify a file with commits to ignore during git blame, so that giant renames don’t mess up your blames branch.sort -committerdate, makes git branch sort by most recently used branches instead of alphabetical, to make it easier to find branches. tag.sort taggerdate is similar for tags. color.ui false: to turn off colour commit.cleanup scissors: so that you can write #include in a commit message without the # being treated as a comment and removed core.autocrlf false: on Windows, to work well with folks using Unix core.editor emacs: to use emacs (or another editor) to edit commit messages credential.helper osxkeychain: use the Mac keychain for managing diff.tool difftastic: use difftastic (or meld or nvimdiffs) to display diffs diff.colorMoved default: uses different colours to highlight lines in diffs that have been “moved” diff.colorMovedWS allow-indentation-change: with diff.colorMoved set, also ignores indentation changes diff.context 10: include more context in diffs fetch.prune true and fetch.prunetags - automatically delete remote tracking branches that have been deleted gpg.format ssh: allow you to sign commits with SSH keys log.date iso: display dates as 2023-05-25 13:54:51 instead of Thu May 25 13:54:51 2023 merge.keepbackup false, to get rid of the .orig files git creates during a merge conflict merge.tool meld (or nvim, or nvimdiff) so that you can use git mergetool to help resolve merge conflicts push.followtags true: push new tags along with commits being pushed rebase.missingCommitsCheck error: don’t allow deleting commits during a rebase rebase.updateRefs true: makes it much easier to rebase multiple stacked branches at a time. Here’s a blog post about it. how to set these I generally set git config options with git config --global NAME VALUE, for example git config --global diff.algorithm histogram. I usually set all of my options globally because it stresses me out to have different git behaviour in different repositories. If I want to delete an option I’ll edit ~/.gitconfig manually, where they look like this: [diff]algorithm = histogram config changes I’ve made after writing this post My git config is pretty minimal, I already had: init.defaultBranch main push.autoSetupRemote true merge.tool meld diff.colorMoved default (which actually doesn’t even work for me for some reason but I haven’t found the time to debug) and I added these 3 after writing this blog post: diff.algorithm histogram branch.sort -committerdate merge.conflictstyle zdiff3 I’d probably also set rebase.autosquash if making carefully crafted pull requests with multiple commits were a bigger part of my life right now. I’ve learned to be cautious about setting new config options – it takes me a long time to get used to the new behaviour and if I change too many things at once I just get confused. branch.sort -committerdate is something I was already using anyway (through an alias), and I’m pretty sold that diff.algorithm histogram will make my diffs easier to read when I reorder functions. that’s all! I’m always amazed by how useful to just ask a lot of people what stuff they like and then list the most commonly mentioned ones, like with this list of new-ish command line tools I put together a couple of years ago. Having a list of 20 or 30 options to consider feels so much more efficient than combing through a list of all 600 or so git config options It was a little confusing to summarize these because git’s default options have actually changed a lot of the years, so people occasionally have options set that were important 8 years ago but today are the default. Also a couple of the experimental options people were using have been removed and replaced with a different version. I did my best to explain things accurately as of how git works right now in 2024 but I’ve definitely made mistakes in here somewhere, especially because I don’t use most of these options myself. Let me know on Mastodon if you see a mistake and I’ll try to fix it. I might also ask people about aliases later, there were a bunch of great ones that I left out because this was already getting long. Want a weekly digest of this blog? Subscribe Dealing with diverged git branches",
    "commentLink": "https://news.ycombinator.com/item?id=39400352",
    "commentBody": "Popular Git config options (jvns.ca)352 points by ingve 16 hours agohidepastfavorite109 comments vbezhenar 2 hours agoOne git option that I almost never see in other people configs is [core] autocrlf = input safecrlf = true It prevents commiting CRLF files and forces you to convert those to LF before commit (you can still override it with gitattributes if necessary). I hate CRLF so much that I spent lots of time digging out this combination of configuration values. reply demurgos 49 minutes agoparentOh that's great. I always used the following `.gitattributes`: # Enforce `lf` for text files (even on Windows) * text=auto eol=lf I'll try your config too. My current position is that anywhere you can choose between CRLF and LF (git, editor, program output), it should always be LF regardless of the platform. Simple LF just works on Windows, and removing this variation point simplifies so much of code. For example, you can check reproducible outputs or hashing with a single reference value. reply CodeIsTheEnd 7 hours agoprevMy favorite line in my .gitconfig is an alias that prints out the commit history as a tidy graph. (The default `git log --graph` uses 6 whole lines per commit!) [alias] lg = log --graph --abbrev-commit --decorate --date=relative --format=format:'%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all -n 15 Which I took from this Stack Overflow post: https://stackoverflow.com/questions/1057564/pretty-git-branc... reply divbzero 3 hours agoparentI like how --all reveals forgotten refs. reply OJFord 14 hours agoprevGood list, definitely concur with the top ones and can't think of anything major to me that's not listed. rebase.autosquash combined with an alias I have (called 'fixup') to make that commit and then do the rebase is probably my top one by an absolute long shot, at least in terms of frequency of use. pull.rebase is important to me, but I fixup so frequently, I really don't know how people get by without it. (Well, I do, generally they create a snaking history by merging master or whatever into their own branch repeatedly, swapping the parents, and make lots of new commits to fix things in the old ones. But I don't know how you can put up with doing that to yourself, is I suppose the long version of what I mean.) Oh - semi-missed - insteadOf is more useful than implied: I have 'gh:' mapped to the full thing, so I just `git remote add somefork gh:someforker/the-project`, for example. Some other minor ones: advice.statusHints=false; include.path (if you need it you need it - I use it to set my signing key according to which Yubikey is plugged in, plugging can write the whole file because it's all that's in there, no parsing required); remote \"origin\".fetch set to get PRs; oh and interactive.singleKey = true if that was missed - not a minor one at all! Makes staging changes so much faster and easier. Obviously not as fast as `add .` or `commit -a`, but don't do that. reply NewJazz 13 hours agoparentAnother way to get that instead of behavior is with SSH configuration. I have something like this in my ssh config: Host github Hostname github.com User git Then you can do a \"git clone github:microsoft/windows\" and you're good to go. reply OJFord 13 hours agorootparentTrue. If I'm only using it for git though makes sense to keep the config there IMO. But certainly that's handy when you're sshing interactively or doing other things with that host. reply hinkley 11 hours agorootparentprevThis fuckin' guy. If you've evolved an SSO solution from brownfield, or just merged with another company you can easily end up in a spot where you have 2 user names across the systems, and that's before you get into doing things like pushing bug fixes to Github straight from your work machine. Make yourself an .ssh/config file. Set your user, and your ssh key for where it matters (I don't use the same key for ssh to servers and for talking to github or bitbucket) reply NewJazz 6 hours agorootparentAy usually \"this fucking guy\" has a pejorative tone... I think you were agreeing with me but yeah not sure everyone understood the same. reply hinkley 4 hours agorootparentThe show where everything is made up, and the points don’t count. reply saghm 6 hours agorootparentprev> Make yourself an .ssh/config file. Set your user, and your ssh key for where it matters (I don't use the same key for ssh to servers and for talking to github or bitbucket) Another good setting for your ssh config if you use different keys for different servers is to require passing a key for all hosts rather than it trying each of your keys to see which one works: Host * IdentitiesOnly yes reply jerpint 12 hours agorootparentprevNeat reply chungy 12 hours agoprevMy config: [alias] co = checkout ci = commit st = status br = branch hist = log --pretty=format:'%h %ad%s%d [%an]' --graph --date=short type = cat-file -t dump = cat-file -p dft = difftool [tag] sort = version:refname [tar \"tar.xz\"] command = xz -c [tar \"tar.zst\"] command = zstd -T0 -c [log] date = iso-local [pull] ff = only [diff] tool = difftastic [difftool] prompt = false [difftool \"difftastic\"] cmd = difft \"$LOCAL\" \"$REMOTE\" [pager] difftool = true [safe] directory = * [advice] detachedHead = false [init] defaultBranch = master reply xyst 10 hours agoparentNot a fan of the “main” branch revolution? Pretty good config though. Personally, I use diff configs depending on oss vs personal vs work. So I combine mine with “[includeIf gitdir:~/oss…]” … Some companies like G have some weird requirements. reply chungy 8 hours agorootparentNo, I'm not a fan of it. It's been master since forever and it's a perfectly good term, fits regular English convention of \"master branch\" too. > Personally, I use diff configs depending on oss vs personal vs work. So I combine mine with “[includeIf gitdir:~/oss…]” … I'd stuff such things into the local repos' .git/config files instead of the global. It's rare that I do it, but sometimes I do work with projects with differing conventions. reply influx 4 hours agorootparentI liked it more once I realized it was fewer characters to type. reply imbnwa 5 hours agorootparentprev>Not a fan of the “main” branch revolution? Why not 'canon'? reply NateEag 4 hours agorootparent\"canon\" is for the central repo, the blessed source of truth. reply wooptoo 9 hours agoparentprevDid you use Mercurial before? I miss its abbreviated commands. A few more from me: [alias] gr = grep -I ch = cherry-pick ls = ls-files tip = log -1 HEAD zip = archive -o latest.zip HEAD fix = commit --fixup reply chungy 8 hours agorootparentNever used Mercurial in any serious capacity. The abbreviations mostly stem from CVS and Subversion experience (and really glad I haven't touched either in many years). reply JelteF 9 hours agoprevMy personal favorite additions to my git config are using delta[1] as my pager for much more readable and syntax highlighted diffs. And adding the following alias, which means I no can always check out to the default branch of the repo, no matter if it's called master/main/develop/whatever [alias] checkout-default = \"!git checkout $(git rev-parse --abbrev-ref origin/HEADsed 's@^origin/@@')\" [1] https://github.com/dandavison/delta reply edavis 9 hours agoprevFor the \"I want to use different emails for different repos\" issue, I've seen the `includeIf` technique but it felt fiddly. What I've done is set `user.useConfigOnly = true` and commented out `user.email` in ~/.gitconfig. Now the first time I commit in a new repo, it errors out with \"Author identity unknown\" and I punch in \"git config user.email ADDR\" for the email I want to use and re-run the commit. reply Garbage 21 minutes agoparentDuring my consulting time, I had to do this a lot. So I wrote a blog post for myself on how to do this effectively - https://www.shirishpadalkar.com/using-separate-git-identitie... reply quectophoton 2 hours agoparentprevI also don't like the `includeIf`, but I ended up with a completely different way of doing things. I just use different accounts depending on the context, usually by either SSH or by starting a Docker container with all the environment already configured. At least for me, using different identities[1] when I want to use different identities[2], seems less brittle and less likely for my future self to accidentally forget to do something (e.g. change a config when I reinstall an OS). [1]: OS user, directly (SSH) or indirectly (container). [2]: Email address, username, etc. reply NewJazz 6 hours agoparentprevHuh. How many repos do you have? And how many emails? I have... A lot of repos. And two emails. reply thfuran 6 hours agorootparentTwo is still more than one, and it seems reasonable to, for example, keep work separate from personal stuff. reply NewJazz 5 hours agorootparentYeah, but the includeIf solution has me typing my emails once each, rather than once each for something like 100+ repos. reply gcarvalho 4 hours agorootparentI occasionally clone repos into /tmp for some small tweaks without switching branches or whatever, and that’s usually when I commit with the wrong email. Neither includeIf nor useConfigOnly are ideal in this scenario because a) it’s cloned in a generic location that can’t be differentiated using path-based includeIf, or b) manually setting it multiple times for the same repo is even more annoying. I’ve tried conditionally setting email based on the remote [1], but for some reason I did not manage to make it work. I didn’t try too hard, and I might try again with more patience. [1] https://www.brantonboehm.com/code/conditional-git-config/ reply kentonv 8 hours agoprevIMO, git needs to change the default for merge.conflictstyle. The default \"merge\" style is abysmal, it does not give you enough information to correctly resolve conflicts in isolation -- you have to actually look up the changes separately to understand what each was doing. diff3 gives you sufficient information so you don't have to look up anything. I haven't tried the enhanced versions like kdiff3 or zdiff3 or whatever, it sounds like they may be marginally better, but the difference between the default and diff3 is night and day. I've seen so many new engineers struggle for years not being able to figure out conflicts until someone tells them about diff3. The default here has likely wasted millions of dollars of productivity. reply Too 1 hour agoparentEven better: Use a visual 3-way mergetool instead of working with the raw text. If that’s too much for you, vscode renders the conflict similar to the raw representation but with pretty “accept mine” buttons next to it. reply wooptoo 9 hours agoprevA few more: Change the comment character. This is useful when you want to include a hash in your commit message (e.g. for automatically linking to JIRA tickets). [core] commentChar = \";\" When starting a new branch/ticket you can create an empty commit where you write a detailed subject and description. [alias] newtask = commit --allow-empty Create a zip archive of the current files. [alias] zip = archive -o latest.zip HEAD Switch between the last two active branches with: git checkout - Use short log lines: [format] pretty = format:%C(yellow)%h %Cblue%>(12)%ad %Cgreen% branch.sort -committerdate, makes git branch sort by most recently used branches No, it makes it sort by the committer date of the commit pointed to by the branch. Personally I find that to be of little use. To actually sort by recently used (i.e. switched-to), I wrote https://github.com/amarshall/git-recent-branches reply ComputerGuru 14 hours agoprevHow to get all your set git options: git config --global --get-regexp .sort -u A hack I like that makes diffing possibly minified or formatting-mangled source files easier: diff.css.textconv css-beautify (Unminifies and unifies the formatting of css files before diffing them, might need to have the *.css type set in your .gitattributes though) core.editor nvim (Use neovim as the editor for commit messages and other things) reply nerdponx 13 hours agoparentI use this to get a list of my configured aliases. [alias] ls-alias = config --global --includes --get-regexp 'alias\\..*' reply mkhnews 12 hours agorootparentSame here - [alias] alias = \"!git config --get-regexp ^alias.sed 's/^alias.//'\" reply matijs 3 hours agorootparent! Is for external commands. You can drop the !git from that alias. reply NewJazz 13 hours agoparentprevYou can set the EDITOR env var instead of using core.editor. or setup symlinks with your package manager. reply sandreas 4 hours agoprevMy git config: [alias] lg = log --graph --abbrev-commit --decorate --date=format-local:'%Y-%m-%d %H:%M' --format=format:'%C(bold blue)%h%C(reset)%C(bold green)%ad%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all -n 15 hist = log --pretty=format:\\\"%h %ad%s%d [%an]\\\" --graph --decorate --date=short quick-push = \"!f() { git add . && git commit -m \\\"$1\\\" && git push; }; f\" search = lg -E -i --grep [user] email = xxx name = sandreas [core] ignorecase = false autocrlf = input excludesfile = ~/.gitignore_global [url \"https://github.com/\"] insteadOf = git@github.com: [url \"ssh://git@github.com/sandreas/\"] insteadOf = https://github.com/sandreas/ [url \"ssh://git@github.com/sandreas/\"] insteadOf = git@github.com:sandreas/ The quick-push might be interesting. It calls a function `f`, which is defined before, to git add . git commmit -m \"$1\" git push This can be used to define small git workflows as alias. reply ncann 12 hours agoprevpull.ff only should have been the default option, or at least pull.rebase true. I've helped so many git newbie at work and this is probably the thing that caused the most confusion to people. The merge commit resulted from the default pull behavior caused nothing but pain. reply globular-toast 10 hours agoparentThe fundamental thing people fail to understand is the distributed nature of git. They don't understand that their master branch is not the same as your master branch, which isn't the same as GitHub's master branch etc. In fact, every time you clone you are also making a new branch! The default behaviour of pull makes complete sense in the distributed system. It goes wrong when developers are also pushing to remotes and doing feature branches and the like. I love forges like GitHub, but I understood git first. Developers nowadays are using a decentralised version control tool to do centralised version control. No wonder it goes wrong. reply from-nibly 8 hours agoprevMy favorite aliases (these scripts keep working and I wrote them YEARS ago so they might be optimize-able) [alias] fb = !fzf_prompt.sh# prompts the user with an fzf prompt to fuzzy search for branches cheese-touch = \"log -n 1 --format=format:'%an' --\" # displays who edited a file last destroy = \"!git reset HEAD~1 && git add --all . && git reset --hard\" # destroys top commit sc = \"commit --amend --no-edit\"# *jackie chan's uncle voice* one more thing! pushf = \"push --force-with-lease\"# *lego batman voice* first try! all = \"!git add --all . && git status\" # I forget that this doesn't exist on everyone's computer. cp = \"cherry-pick\"# this one feels obvious but that saves me so many letters ---fzf_prompt.sh--- #!/usr/bin/env bash branch=$(git for-each-ref --shell --format='%(refname)' refscut -d/ -f3-grep -Eo \"[^']+\"fzf-tmux -d 15); if [[ ! -z \"$branch\" ]]; then if [[ $branch == origin/* ]]; then branch=$(echo $branchcut -d/ -f2-) fi git checkout \"$branch\" fi reply yegle 13 hours agoprevMost people on HN probably disabled \"usage report\" in all places they can. Otherwise it is possible to produce a report on \"which option is more popular\". reply NelsonMinar 13 hours agoparentThere was an interesting discussion about this problem on Mastodon yesterday started by Julia: https://mastodon.social/@b0rk@jvns.ca/111935753657358820 The general conclusion is that relatively few FOSS tools collect usage information the way commercial software now does. Interesting opportunity to potentially improve things. reply rcoveson 13 hours agorootparent> Interesting opportunity to potentially improve things. ...you mean by changing commercial software to collect telemetry more like FOSS tools do, i.e. usually not at all, right? reply mhh__ 9 hours agorootparentI would probably rather nothing logged, but I must say that when working on a language inside a company (rather than open source) and being able to do telemetry is a massive win. reply tstrimple 10 hours agorootparentprevIt's interesting how many people assume telemetry is just for spying on users and not an honest attempt to actually improve the product. Usability on most FOSS tooling is garbage. Even gems like Blender have incredibly rough edges compared to their commercial counterparts and that's one of the best examples of UX in FOSS. If users are spending a disproportionate amount of time performing operation X, I want to zoom in on operation X and find out what is holding them up and if it can be streamlined. I get that a lot of these metrics can be used more nefariously, but if you want to improve something you've first got to measure it or you have no baseline to determine if your \"improvement\" actually made a difference. Telemetry and usage information is not bad in and of itself. It's a perfectly valid tool that can be used to find rough edges on your product and improve them. It's a great way to determine the most commonly used operations. Every developer who has worked on products used by real people inevitably discovers that their users approach their software in ways completely different than intended. Some of these unintended ways represent valid use cases the developer or product owner never anticipated. If you discover these things, you can improve the product by making that operation a first class operation instead of a weird workaround. If you're not collecting metrics, you're only listening to the most noisy parts of your user base who are by definition a small minority. reply wakawaka28 6 hours agorootparentPeople are desensitized to claims of possible product improvements because closed-source software says it's collecting telemetry for that reason too, despite the fact it is used for all kinds of nefarious purposes. While it could be possible to get useful information from telemetry, there's no way for the collector to verify anything about popularity without violating your privacy. I don't think there is anything wrong with having an opt-in system for people who want to be involved while leaving everyone else alone. I think AUR and maybe NPM have voting systems. Github also acts as kind of a voting system, with its stars. Package downloads are a good semi-anonymous metric that works without telemetry. It could be gamed, but if someone was enthusiastic enough to game it then maybe their project should be considered active. reply vundercind 5 hours agorootparentprevIt doesn’t matter what it’s for: it is spying on users. reply NelsonMinar 11 hours agorootparentprevNo, that's not what I mean. reply xp84 11 hours agoprevAt the risk of angering all the vim-heads (this tip isn’t for anyone who likes vim or even emacs): editor = \"nano -t\" This nano option is aka --saveonexit Things accomplished: - avoids using an overpowered modal editor that requires several keystrokes just to save and quit when done writing my couple of sentence commit message - avoid even having to tell it I want to save. Of course I want to save. ^x - all done (In the unlikely event you decide you want to abort the commit, just hit ^k a few times to kill the text and then ^x) (Edit: Updated to the default exit keybinding which is ^x) reply davidhaymond 11 hours agoparentThat's a nifty option! In (neo)vim, I like to use the ZZ keybinding to quickly save if modified and then quit. If I want to abort the commit, I'll type ZQ to discard changes and quit. reply sodapopcan 11 hours agorootparentYa, it's somewhat surprising that ZZ isn't the binding that is burned into people's minds instead of :wq. I've even met regular vim users who don't know ZZ which, to be a bit hyperbolic, is mindboggling. EDIT: And of course there is vim -y to make vim behave more like a \"normal\" editor than even nano :D (ie, you get ctrl-s and ctrl-q). reply jdougan 10 hours agorootparentFor me, it's because when I learned BSD 4.2 vi in the mid-80s the emphasis was on learning the decomposed commands then building up. If you learned w and q , wq is obvious and non-magical. reply sodapopcan 10 hours agorootparentCertainly a fair point! reply FPGAhacker 10 hours agorootparentprevI've been using vi variants for decades and didn't know ZZ or ZQ. This solves a major pain point for me (mis-typing :q, trying again, now I'm off in the weeds). reply sodapopcan 10 hours agorootparentZZ is particularly nice since it sorta \"does the right thing\" in that if you have an unwritten empty buffer, it will silently discard it and quit. I guess I haven't really run into this as I was taught ZZ from the beginning but I imagine hitting :wq on such a buffer is pretty annoying. reply FPGAhacker 7 hours agorootparentThe worst part is q: is a different command. so if you are trying to :q and miss the colon and try again, you run some other damn f*cking command. reply sodapopcan 6 hours agorootparentOh I still do that often enough, lol. reply pxeger1 10 hours agorootparentprevHuh, I didn’t think of ZQ (to leave an empty commit message) to cancel the commit. I always use :cq (which exits vim with an error status) reply samatman 9 hours agoparentprevNo one's going to make you use vim, don't worry. But I don't find ^x vs jk:x compelling as a reason not to. Nor the difference between opening instantly (nano) vs. opening instantly (neovim). But I might :q! out of more commits than you do, who knows. I used nano for about five years for git commits before I decided to embrace the Vim Way. No need to feel defensive about it, it's there for a reason. reply leetrout 14 hours agoprevCannot overstate the helpfulness of rerere if you work in a rebase workflow. reply PhilipRoman 14 hours agoparentIndeed, but be careful not to poison your resolution cache, having a wrong resolution and re-doing a large rebase can be a painful experience. reply Izkata 9 hours agoprev> core.editor emacs: to use emacs (or another editor) to edit commit messages I set EDITOR in my bashrc ages ago, so everything uses my chosen editor, including git. This git setting then overrides it if you use it. reply nektro 1 hour agoprevthanks! got 9 new ones from reading this :) reply twic 8 hours agoprevremote.origin.tagOpt=--tags Now, is there any way to set that for all remotes? reply frontalier 7 hours agoparentremote.*.tagOpts=--tags reply ryandrake 13 hours agoprev> “80% of people use this, probably take a look” If an option is used by 80% of users, shouldn't it be the default, and have an option to turn it off for the 20%? reply OJFord 13 hours agoparentNot necessarily I'd say, it could be something that's confusing to a newcomer or (or even doesn't make sense) on first use. But that doesn't exist anyway, that was a wish for it to exist. If the tool had it, it could decide to change the defaults as you say, and so the worth-looking-at number would just be 35% or whatever instead. reply peterhadlaw 13 hours agoprevI think a critical one would be: init.defaultBranch master Especially since places like GitHub are making main the default unwittingly reply samatman 9 hours agoparentI switched to trunk back when the witch hunt started raging. Fossil uses it, it's the original[0] term, and it completes the metaphor. What do the branches branch off? Why the trunk, of course. As a matter of policy, I refuse to be swayed by the opinions of busybodies as to what is or isn't offensive. I will continue to use blacklists, for example, but am happy to retire the master/slave pairing: it's not a great metaphor to begin with, and a reasonable person can see where those of recent descent from the enslaved might take exception to it. There's nothing at all wrong with having a master branch, either, the metaphor is entirely anodyne and no honest person takes umbrage at it. But that whole absurd episode got me thinking about the nomenclature, I had been working with Fossil on a hobby project shortly before, and figured that if there's another word which I prefer, I may as well take the opportunity to switch. [0]: https://en.wikipedia.org/wiki/Branching_(version_control) reply wakawaka28 5 hours agorootparent>I switched to trunk back when the witch hunt started raging. >As a matter of policy, I refuse to be swayed by the opinions of busybodies as to what is or isn't offensive. Well, you have said multiple ways that you have been swayed. I agree with you about all of this whining and word-policing coming from people who have dubious intentions. Nobody sane is offended by these terms. It's just a way for \"victims\" to seek clout and bully everyone else. Fossil is cool but alas it will never be mainstream. It does have some nice features, but there are technical downsides such as storing files in a single binary sqlite database (which is a benefit for small projects, but a bad solution for large projects). If nothing else, backing up Fossil repos will take an ever-increasing amount of space compared to an equivalent git repo, as git repos can be backed up incrementally. reply threemux 13 hours agoparentprevAgreed I have this in my gitconfig. The changing of this setting caused more damage than \"master\" ever did reply __turbobrew__ 5 hours agoparentprevI have witnessed several outages caused by switching from master to main. Think nobody in a company with thousands of engineers could commit code for half a day. It really isn’t worth it. I can agree with using ‘main’ when making new repos but trying to shoehorn ‘main’ into existing stuff is a giant pain for little to no gain. reply NewJazz 13 hours agoparentprevWait are you saying to keep the default branch as master explicitly? reply OJFord 13 hours agorootparentYes, I do it too. 'master' is too ingrained to change and has nothing to do with slavery or anything sinister IMO. If I ever work with someone on a project we're in control of who feels otherwise and affected personally then it won't be much effort to change it. Until then I don't need to pointlessly spend effort rewriting it when I get it wrong on my own projects. (It only affects what git init does.) reply kstrauser 12 hours agorootparentI was used to typing \"master\", too. I also think a bit much was made of using \"master\" (although using it together with \"slave\" in other contexts does kinda highlight the problem people have with it). But you know, some people around me strongly dislike calling it \"master\". I don't have strong feelings about it. They do. Their desire to call it something else is far greater than any desire I might have to not to. Switching to \"main\" cost me nothing, saves a couple keystrokes, and makes other people happy. Fine, let's do it. If you want to use \"master\" on your own internal projects, go for it. No one's stopping you. I definitely wouldn't use it on a shared project because the potential cost of irritating someone isn't worth it. And because I don't want to have one set of muscle memory for my own projects and another for shared projects, I just use \"main\" everywhere. reply RadiozRadioz 2 hours agorootparentAre the people around you who strongly dislike \"master\" genuinely offended by it, or are they more akin to the virtue signaling busybodies mentioned by a comment above? reply OJFord 10 hours agorootparentprevI might feel that way if work had felt the need to switch (or started after the default changed), but as it is I spend most of my time with master, so using that for projects that I init only increases that majority. reply nerdponx 13 hours agorootparentprevI found that `main` was actually easier to type and say than `master`. You should keep doing what you're doing of course. But now that the change is becoming popular, I'm happy to keep using it, even in the absence of any particular moral reasoning. reply ithkuil 13 hours agorootparentprevIndeed \"master\" comes from \"master copy\" which is the version of the \"teacher\" (magister) from which students copy a work reply ksenzee 13 hours agorootparentYes, and as it happens, “master” was a bad metaphor for what that branch actually is and does. It would have been a better metaphor for what is usually called “origin.” reply wakawaka28 5 hours agorootparentIt's not a bad metaphor at all. I've never met anyone confused by it. People want to change it because of the political grifters, not for want of clarity. reply euroderf 13 hours agorootparentprev> 'master' is too ingrained to change This typifies why the US still runs on imperial units, not metric. Don't crush that dwarf, hand me the 16mm pliers. reply IshKebab 11 hours agorootparentWell sure, except metric has big advantages over imperial, whereas `main` is very very slightly easier to type than `master`... reply tom_ 10 hours agorootparentFeels like QWERTY users will save a useful amount of wear and tear on their left hand. I don't want their life to be any worse than it already is. reply hiccuphippo 11 hours agorootparentprevIt has nothing to do with slavery yes, but main is shorter to type. It didn't take much effort to switch when I started working in a project that used it. reply LAC-Tech 7 hours agorootparentprevIIRC if you don't do that, it will spam you with \"the default branch is master, it may change\" every time you make a new repo, so you have to set it explicitly to get rid of the nagging. reply arvigeus 8 hours agoparentprevFunny how nobody talks about the aftermath of switching to “main” - did it achieve its original goal? No papers written on it, no researches. Like nobody wants to admit how stupid it was to push for it and insult those who resist it (which ironically achieved the opposite goal of making things more toxic instead of “inclusive”). reply wakawaka28 4 hours agorootparentDEI is proven to divide people. That's why it's embraced with both arms. Switching the branch names has probably wasted tens of thousands of hours of maintenance work in the world. I doubt if there are even a dozen people in the world who were actually offended by the term master in that context or even the master-slave pairing in others. But there are probably hundreds of people at least (out of millions of developers) pretending to be offended for clout. reply globular-toast 10 hours agoparentprevYes. GitHub really overstepped the mark by pushing main onto unwitting newbie devs. Really not cool. Almost like they want to control people or something. reply iraqmtpizza 2 hours agoprevdefaultBranch = slavemaster has been getting a lot of traction; it helps us not shy away from what we did. reply Zardoz84 13 hours agoprevI find disturbing that don't mentions kdiff3 with diff and merge tool. Specially when kdiff3 does the smae thing that diff3 and zdiff3 output formats... I don't understand the popularity of meld. I always felt that meld it's pretty inferior to kdiff3. And I give a few try to it... reply cpeterso 12 hours agoparentMy favorite free (though not open source) visual diff and merge tool is Perforce's P4Merge. It's available for Linux, macOS (including as a Homebrew cask), and Windows. My favorite feature is its conflict resolution listing the conflicting lines vertically in the source file (like diff3 conflict markers but a GUI) instead of side-by-side panes. https://www.perforce.com/products/helix-core-apps/merge-diff... reply mistrial9 12 hours agoparentprevregular meld user here .. just do not have KDE parts when it can be avoided -- not disagreeing otherwise. reply oldpersonintx 14 hours agoprev> init.defaultBranch main if you think people who use \"master\" are doing so because they fervently wish to reconstitute slavery, you may be beyond help even my woke friends who are somewhat rational cringe at this reply striking 12 hours agoparent`git init` itself hints that you should globally configure an initial branch name in your git config, and suggests \"main\", \"trunk\", and \"development\" as alternatives. It will continue to make this suggestion until you make a choice. Here's the full output from `git init` with no additional config on my machine: hint: Using 'master' as the name for the initial branch. This default branch name hint: is subject to change. To configure the initial branch name to use in all hint: of your new repositories, which will suppress this warning, call: hint: hint: git config --global init.defaultBranchhint: hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and hint: 'development'. The just-created branch can be renamed via this command: hint: hint: git branch -mInitialized empty Git repository in /private/tmp/test/.git/ I think people that set this actually get to that point because they don't want to keep seeing the warning, and \"main\" is as good a value as any. In light of this, your harsh moral ascription seems fairly silly. reply elevatedastalt 10 hours agorootparentThis was done precisely following the change that OP is hinting towards. Let's not pretend like this was done for a technical reason. reply striking 9 hours agorootparentI'm not sure which \"this\" you're referring to here, but I don't think there's anything wrong with choice. It's like being prompted to choose a browser. Some people choose the default, others prefer Firefox. reply wakawaka28 4 hours agorootparentThere is a problem when the choice creates confusion and randomness. Not that projects used the main/master branch consistently... In some it is the development branch. In others, it is the stable branch. And the default branch you get upon cloning the repo is something else still (one of those, or something else entirely). With git supporting different workflow styles, it would be hard to demand consistency. But a little consistency would be nice. When I archive repos from Github, I keep track of which branch the API says is default. Otherwise there might be no way to figure it out in the future! reply athorax 11 hours agoparentprevI think most people don't even remotely care about master vs. main, but it definitely gives me pause when someone is so bothered by another persons choice not to use master. reply elevatedastalt 8 hours agorootparentThis is ironic because this whole thing started _with_ people getting very bothered by the choice of the term master. reply IshKebab 11 hours agorootparentprevI care, because it introduced a ton of unnecessary pain into my workflow. Yes really. I now have a mix of repos using `master` and `main`, and I have to remember which one to use. One repo uses `master` but a subtree uses `main`. If you make a mistake and checkout `main` you end up clobbering your whole working tree with the subtree. I also have tooling that used to happily assume `master`, which worked fine 99% of the time. Now it works 50% of the time, and even worse the name of the main branch is just a convention, so you can't even read a setting to see which one to use. I don't care about the specific name. `main` would definitely have made more sense from the start (classic terrible Git naming). But I do care about pointlessly changing it and breaking everything. reply athorax 11 hours agorootparentI think the fact that so many tools were setup to assume `master` and have no way to configure it differently was the real problem, and as annoying as it was to have GH switch to `main` it did force a lot of tools to be updated to make that configurable. Whether you use main, master, trunk, development, or anything else, the fact is the name is arbitrary and treating it any other way was incorrect. reply JelteF 9 hours agorootparentprevYou can put this in your .gitconfig and do 'git checkout-default' to always checkout the default branch of the repo you're in, no matter what it's called: [alias] checkout-default = \"!git checkout $(git rev-parse --abbrev-ref origin/HEADsed 's@^origin/@@')\" reply ckolkey 2 hours agorootparentThis assumes there's a remote set up called \"origin\" - also an arbitrary name ;) reply chronial 11 hours agorootparentprev> One repo uses `master` but a subtree uses `main`. If you make a mistake and checkout `main` you end up clobbering your whole working tree with the subtree. If you replace checkout with switch/restore, that foot gun goes away. reply mariusor 12 hours agoparentprevIt's not clear what you're complaining about here. The fact that many people use this option, the fact that Julia included it in the article, or the fact that it's an option to change the default branch name in the first place. Also it's pretty sad that this is the only part of a very informative article that you deigned to comment about. reply quectophoton 1 hour agoparentprevIf I were that bothered by that word I probably wouldn't be able to consume anything coming from Japan. Also, wait until people find out about what \"Spanish main\" is (spoiler: still slavery). reply enobrev 13 hours agoprev [–] I see an article by Julia, I upvote it. Always excellent. I learn something every time Julia writes something for us to read. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores Git configuration options favored by Mastodon users, like pull.ff, pull.rebase, and merge.conflictstyle, alongside rebase.autosquash, push.default, and init.defaultBranch main.",
      "It provides guidance on configuring these options, offers insights on managing diverged branches, resolving merge conflicts, and optimizing productivity with .gitignore settings and branch naming conventions.",
      "Emphasizing data integrity and efficiency in Git usage, the article promotes staying informed about default changes, emphasizes seeking advice, and discusses the author's Git configurations and future topics like git aliases."
    ],
    "commentSummary": [
      "The discussion delves into different Git setups and choices, such as avoiding CRLF files, creating aliases, configuring SSH, and naming branches.",
      "Users explore ways to enhance workflows, leverage aliases, utilize telemetry, configure editors, and transition from \"master\" to \"main\" as the primary branch name.",
      "The discourse covers preferences for diff and merge tools and stresses the significance of maintaining consistent branch names to boost productivity and efficiency in Git operations."
    ],
    "points": 352,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1708105427
  },
  {
    "id": 39402917,
    "title": "Plastic Industry Deceived Public on Recycling, Facing Legal Action",
    "originLink": "https://www.euronews.com/green/2024/02/16/plastic-industry-knew-recycling-was-a-farce-for-decades-yet-deceived-the-public-report-rev",
    "originBody": "GreenGreen News Plastic industry knew recycling was a farce for decades yet deceived the public, report reveals Plastic producers should ‘pay for the damage they’ve caused’ after decades of deception, a new report says. - Copyright Canva By Angela Symons Published on 16/02/2024 - 17:23 Share this articleComments Share this article FacebookTwitterFlipboardSendRedditMessengerLinkedinVK Plastic producers should ‘pay for the damage they’ve caused’ after decades of deception, the report's authors say. ADVERTISEMENT Recycling has been promoted as a solution to plastic waste management for more than 50 years. But big oil companies and the plastics industry have known for decades that it’s not a technically or economically viable solution, a new report reveals. Combining existing research and recently revealed internal documents, the report by the Center for Climate Integrity Research (CCI) could form the foundation for legal action, its authors say. “When corporations and trade groups know that their products pose grave risks to society, and then lie to the public and policymakers about it, they must be held accountable,” says CCI President Richard Wiles. “Accountability means stopping the lying, telling the truth, and paying for the damage they’ve caused.” Plastic producers misled the public about recycling The report unveils the fraudulent marketing and public education campaigns used to promote plastic as recyclable, despite knowing that it is not a workable solution. These strategies allowed the single-use plastics industry to expand, while avoiding regulation to effectively address waste and pollution, the report says. “Recycling cannot be considered a permanent solid waste solution [to plastics], as it merely prolongs the time until an item is disposed of,” reads a 1986 report by industry trade group the Vinyl Institute (VI). The group’s founding director, Roy Gottesman, highlighted the issue again in 1989 at a conference, warning, “Recycling cannot go on indefinitely, and does not solve the solid waste problem.” Why is plastic so hard to recycle? With thousands of different types used in everyday products, plastic is expensive to collect and sort. It also degrades after just one or two uses, becoming more toxic each time it is repurposed. Despite knowing this, oil and plastics companies pushed forward with campaigns promoting recycling. 'I feel abandoned': These Spanish towns haven't had clean tap water for 10 months Amazon tipping point: Up to 47% forest threatened by climate change and deforestation, study warns Picture the triangle of ‘chasing arrows’ symbol to denote that packaging is recyclable, for example. This was introduced even though the VI had noted that the system was unlikely to work due to the trend towards composite containers, made up of multiple types of plastic. “We are committed to the activities, but not committed to the results,” Exxon Chemical Vice President Irwin Levowitz said in a 1994 meeting with the American Plastics Council (APC). The following year, internal notes from an APC staffer acknowledged the impossibility of recycled plastic competing with virgin materials. “Virgin supplies will go up sharply in [the] near future [and] kick the shit out of PCR [Post-Consumer Recycled material] prices,” they wrote. How could companies be held legally accountable for lies about plastic recycling? This public deception could be a violation of laws designed to protect consumers and the public from corporate misconduct and pollution, according to the report’s authors. “Attorneys general and other officials should carefully consider the evidence that these companies defrauded the public and take appropriate action to hold them accountable,” says Alyssa Johl, CCI’s vice president of legal and general counsel. It adds to a growing list of complaints against plastics producers, including a 2022 California investigation into ExxonMobil’s role in the plastic pollution crisis, and New York suing Pepsi Co in 2023 over plastic pollution. Is it still worth recycling plastic? The best way to reduce plastic pollution is to avoid single-use plastics entirely. However, it is still better to recycle plastic at home than throw it away. ADVERTISEMENT Around nine per cent of the world’s annual plastic waste is successfully recycled, and with many companies committing to using recycled plastic in their products, it can find a purpose. Under the European Strategy for Plastics in the Circular Economy, the target is that 10 million tonnes of recycled plastics find their way into products in the EU by 2025. Almost 26 million tonnes of plastic waste is generated in Europe every year. Share this articleComments You might also like Now playing Next Green News Euroviews. In farming, genomic techniques can't afford a repeat of GMO rejection Now playing Next Green News EU Policy. Lawmakers agree tougher standards to tackle marine pollution Now playing Next Green News EU Policy. Parliament at odds with member states over action on air pollution circular economy greenwashing Microplastics plastic Fossil fuels Pollution ADVERTISEMENT Top stories Now playing Next Scientists issue Amazon warning as up to 47% of rainforest at risk Now playing Next UEFA urged to ban airline ads from football games Now playing Next Madrid landfills emitting major methane leaks, investigation reveals Now playing Next How to talk to climate dismissives, according to a psychologist Now playing Next Euroviews. In farming, genomic techniques can't afford a repeat of GMO rejection ADVERTISEMENT Most read Plastic industry deceived the public about recycling, report reveals Spanish citizens feel ‘abandoned’ after 10 months without clean water What is water cremation? ‘Eco-friendly’ burials begin in Britain Russia’s invasion has left this Ukrainian bison herd in need of a male South America's 'lithium fields' reveal the dark side of electric cars",
    "commentLink": "https://news.ycombinator.com/item?id=39402917",
    "commentBody": "[dupe] Plastic industry knew recycling was a farce for decades (euronews.com)321 points by wooptoo 13 hours agohidepastfavorite2 comments ChrisArchitect 11 hours ago [–] [dupe] More discussion here: https://news.ycombinator.com/item?id=39387387 reply dang 7 hours agoparent [–] Comments moved thither. Thanks! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The plastic industry has known for years that recycling plastic was not effective but engaged in deceptive marketing campaigns, as per the Center for Climate Integrity Research report.",
      "Around nine percent of the global plastic waste is recycled each year, highlighting the importance of recycling at home despite the industry challenges.",
      "Legal action against the plastic industry for consumer fraud and pollution is being suggested in the report to hold producers accountable for their deception and the resulting environmental damage."
    ],
    "commentSummary": [
      "The plastic industry has long known that recycling is not an efficient solution.",
      "Industry insights and discussions regarding this issue are available on news.ycombinator.com."
    ],
    "points": 321,
    "commentCount": 2,
    "retryCount": 0,
    "time": 1708117316
  },
  {
    "id": 39402101,
    "title": "Paying developers for open source work is crucial",
    "originLink": "https://jacobian.org/2024/feb/16/paying-maintainers-is-good/",
    "originBody": "Paying people to work on open source is good actually Warning: rant ahead. I’m writing from a place of frustration and not particularly interested in trying to moderate my tone. If you don’t want to hear me yell about open source for a while, please skip this one. Earlier this week, in a moment of frustration. I wrote on Mastodon: “We believe that open source should be sustainable and open source maintainers should get paid!” Maintainer: *introduces commercial features* “Not like that” Maintainer: *works for a large tech co* “Not like that” Maintainer: *takes investment* “Not like that” This went mildly viral, and I got a ton of arguments and pushback. (Also a lot of “right on\"s, which was nice.) I think some of that pushback was bad-faith and uncharitable reads, but some was coming from well-intentioned misunderstandings or misreadings of my snarky toot. That’s my fault for talking around what I mean instead of coming out and saying it. So that’s this post: upgrading my shitpost to a slightly-more-considered rant. My fundamental position is that paying people to work on open source is good, full stop, no exceptions. We need to stop criticizing maintainers getting paid, and start celebrating. Yes, all of the mechanisms are flawed in some way, but that’s because the world is flawed, and it’s not the fault of the people taking money. Yelling at maintainers who’ve found a way to make a living is wrong. Why this matters This is of course a personal issue: I’ve been involved in open source communities for over twenty years now and have many colleagues and friends from those communities who would love nothing more than to make open source their jobs. Most can’t, and that sucks. Those who can end up getting nastygrams criticizing their financial choices and questioning their morality. But it’s deeper that that. Open source is good for humanity. It’s only slightly hyperbolic to say that open source is one of the most notable collective successes of humankind as a species! It’s one of the few places where essentially all of humanity works together on something that benefits everyone. A world without open source would be substantially worse than the world we live in. So, I want people who want to work on open source to be able to do so, and should be able to live comfortable lives, with their basic needs met. They’re contributing to something that is good for humanity; they shouldn’t have to sacrifice to do so! Definitions Part of the problem with The Discourse is a lack of shared agreement on what the core terms mean. Because I used the term “open source” in my original toot, one of the themes in the replies are people misinterpreting what I mean by “open source” (or, even more exhausting, relitigating the “open source vs free software” debate). So if I’m going to have half a chance of explaining what I really mean I need to start by defining what these terms mean to me: “open source” / “free software” Note the deliberate use of lower case. I’m not referring to Open Source™ as defined by OSI, nor to Free Software™ as defined by the FSF1. I mean these terms in the broadest, most inclusive sense: “software with source code that I can read and modify and release variants of, perhaps under some conditions.” So I’m including OSI and FSF licenses, but also the Polyform licenses and the JSON license and, yes BSL in my version of “open source”. This is perhaps a side point, but the “minimalist” definition of Open Source meaning “only OSI-approved licenses” – or, worse, “the GPL is the only ’true’ Free Software license” – is part of the problem here. I want to see more experimentation and variety in licensing options, and if that means introducing some additional restrictions beyond “anyone can use this for any purpose” I’m pretty okay with that. In my book, a broad spectrum of licenses from Blue Oak to BSL (and even more restrictive than BSL) “count” as open source. So, in this piece, I’m going to use “open source” to mean anything on this spectrum from “totally unencumbered” to “unencumbered with some restrictions”, and I’m not going to articulate how much “some” would be too much for something to still be considered free. I’ll even use the terms “open source” and “free software” interchangeably just to hammer home how, in this context, the precise definitions of these terms don’t matter to me. This stance probably really annoys some folks, and that’s to some degree intentional. Not in the sense that I want to deliberately piss anyone off, but … I’ll put it this way: if my sloppy use of these terms bothers you in the context of talking about how people make their living, it implies that you care more about terminology and definitions than about the people, and I’d like you to sit in that discomfort for a while. “sustainability” Next, what do I mean when I talk about “sustainability” in open source? People use this term to mean lots of things – good governance, healthy communities, funding, and more. When I talk about “sustainability”, though, I mean something very specific: “can maintainers live a decent-to-comfortable lifestyle writing free software?” If open source was “sustainable”, to me, it would mean that people could chose to make writing open source their job, and be assured that they have at a minimum their basic needs met – housing, food, healthcare, etc. Ideally, more than that; I’d love it if writing open source afforded people a comfortable or downright luxurious lifestyle. Open source is not sustainable Almost nobody makes a living writing free software. As a percentage of all software engineers, it’s so few we can basically round down to zero. Sure, there are a few companies that employ people to work on open source: Canonical, Red Hat, Hashicorp, and Mozilla come to mind; I’m sure you could name more. But, (a) these companies employ vanishingly few engineers when compared to the millions of engineers out there writing proprietary software and (b) it’s not like every engineer at each of these organizations actually writes only open source; almost all these organizations have business models dependent on some piece of the product being proprietary. So even the biggest open source success stories represent a fraction of a fraction. Closer to home, let’s look at Django. By my estimate hundreds of thousands of engineers use Django daily. How many get paid to work on Django itself? One and a half – the DSF employs one full-time and one part-time Fellow. That’s the entire population of people who get paid just to work on Django. The numbers are similar to Python itself: millions of people use Python daily, but fewer than a dozen are paid to do so2. “Sustainability” would mean that something like a dozen people were being paid to work full-time on Django – and being paid something approaching the industry median. It would mean several dozen people working full-time on Python. Heck, just PyPI by itself ought to have a team of 10-15, minimum, given its scope, scale, and importance. Even more importantly, “sustainability” in open source would mean that the “random person in Nebraska” maintaining a critical dependency would be living handsomely, and would have several colleagues so they could get a vacation from time to time. We don’t live in a world that even remotely approaches this. The dream: fully automated luxury gay space communism This is usually the point where someone snarkily points out, “you don’t really hate open source, you hate capitalism”. This is one of those statements that is true, but not helpful. Yes, the fact that people have to choose between writing open source software and affording decent healthcare is a problem deeply rooted in our current implementation of zero-sum capitalism, and not at all a problem that can be laid at the feet of the free software movement. The dream is that society and governments will recognize free software as the public good that it most certainly is and fund it appropriately. And also fix healthcare, and housing access, and public transportation, and the social safety net, and and and … I am absolutely one million percent on board with this vision, but this shit ain’t gonna happen overnight. Indeed, I doubt it’ll happen in our lifetimes if at all. We have to accept the world as it is – even if it’s not the world we want. This means we have to be okay with the idea that maintainers need to be paid. Far too often I see arguments like: “maintainers shouldn’t be paid by private companies because the government should be supporting them.” Sure, this sounds great – but governments aren’t doing this! So this argument reduces to “open source maintainers shouldn’t be paid”. I can’t get on board with that. Any time someone gets paid to write open source it’s a win Right now, here in the real world, sustainability in open source means paying maintainer — and we should be celebrating every time that happens! Every time a maintainer finds a way to get paid, it’s a win. Employed by Microsoft to work on Python? Win. Funded by a grant? Win. Reached a sustainable funding level on Patreon? Win. Raises VC funding to develop free software? Win. Builds a sustainable business on an Open Core model? Win. Hashicorp? Win. Supports an open project with paid hosting options? Win. Successfully uses a non-OSI-approved license to avoid being Amazon’d? Win. Until we have fully automated luxury gay space communism every. single. person. who figures out a mechanism to write free software and still pay rent represents a win and we should celebrate accordingly. Instead, criticism But that’s not what happens. Instead, every time a maintainer finds a way to get paid, people show up to criticize and complain. Non-OSI licenses “don’t count” as open source. Someone employed by Microsoft is “beholden to corporate interests” and not to be trusted. Patreon is “asking for handouts”. Raising money through GitHub sponsors is “supporting Microsoft’s rent-seeking”. VC funding means we’re being set up for a “rug pull” or “enshitification”. Open Core is “bait and switch”. None of this is hypothetical; each of these examples are actual things I’ve seen said about maintainers who take money for their work. One maintainer even told me he got criticized for selling t-shirts! Look. There are absolutely problems with every tactic we have to support maintainers. It’s true that VC investment comes with strings attached that often lead to problems down the line. It sucks that Patreon or GitHub (and Stripe) take a cut of sponsor money. The additional restrictions imposed by PolyForm or the BSL really do go against the Freedom 0 ideal. I myself am often frustrated by discovering that some key feature I want out of an open core tool is only available to paid licensees. But you can criticize these systems while still supporting and celebrating the maintainers! Yell at A16Z all you like, I don’t care. (Neither do they.) But yelling at a maintainer because they took money from a VC is directing that anger in the wrong direction. The structural and societal problems that make all these different funding models problematic aren’t the fault of the people trying to make a living doing open source. It’s like yelling at someone for shopping at Dollar General when it’s the only store they have access to. Dollar General’s predatory business model absolutely sucks, as do the governmental policies that lead to food deserts, but none of that is on the shoulders of the person who needs milk and doesn’t have alternatives. Purity only serves to limit open source’s value to society Many, many more people should be getting paid to write free software, but for that to happen we’re going to have to be okay accepting impure or imperfect mechanisms. Criticize those mechanisms if you like. Work to change the underlying societal inequities – please! But when a maintainer finds a way to get paid, celebrate them. It’s a win for all of us. I know these terms aren’t really trademarks, I’m shitposting again. Though actually, “Open Source” kinda is a trademark – OSI has tried to establish some trademark protections around the term. ↩︎ Here I’m counting the PSF Developers-in-Residence, plus the small handful of Python maintainers employed by companies like Google and Microsoft to work on Python for their day jobs. ↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=39402101",
    "commentBody": "Paying people to work on open source is good (jacobian.org)304 points by webology 14 hours agohidepastfavorite110 comments benatkin 11 hours agoI'll respond to the little part where it puts using a \"non-OSI approved license\" under the umbrella of open source. It's not OSI approved because it isn't open source, as the community defined it long ago, and as it still makes sense for it to be defined. If you want me to agree with you, don't do that. Otherwise, I don't feel compelled to consider a bunch of disparate things as a Win. Here's one that could be more of a trap than a win, depending on the particulars of the job: \"Employed by Microsoft to work on Python?\" Look no further than https://ghuntley.com/fracture/ reply lolinder 11 hours agoparentHe anticipated your comment and already replied. You're free to disagree with him, but he clearly thought that part through already and already knows he disagrees with you and with the OSI. This entire post is his justification for his disagreement, while all you have is an appeal to the OSI definition that he's specifically rejecting. > “open source” / “free software” > Note the deliberate use of lower case. I’m not referring to Open Source™ as defined by OSI, nor to Free Software™ as defined by the FSF. I mean these terms in the broadest, most inclusive sense: “software with source code that I can read and modify and release variants of, perhaps under some conditions.” So I’m including OSI and FSF licenses, but also the Polyform licenses and the JSON license and, yes BSL in my version of “open source”. > This is perhaps a side point, but the “minimalist” definition of Open Source meaning “only OSI-approved licenses” – or, worse, “the GPL is the only ’true’ Free Software license” – is part of the problem here. I want to see more experimentation and variety in licensing options, and if that means introducing some additional restrictions beyond “anyone can use this for any purpose” I’m pretty okay with that. In my book, a broad spectrum of licenses from Blue Oak to BSL (and even more restrictive) “count” as open source. > ... I’ll put it this way: if my sloppy use of these terms bothers you in the context of talking about how people make their living, it implies that you care more about terminology and definitions than about the people, and I’d like you to sit in that discomfort for a while. reply benatkin 11 hours agorootparentI used lower case deliberately as well. It's a term that excludes source available not just because of OSI but because of the original community. And members of the current community can argue for a new, weaker, openwashed meaning of it, but people can always look back to the early days and see the true meaning of it. reply lolinder 10 hours agorootparentWhat \"original community\" are you referring to here? Are we talking about the free software movement that Richard Stallman founded, or the Open Source Initiative that forked off of the movement in an effort to be more friendly to businesses? It's a bit ironic that people now wax lyrical about the \"true meaning\" of Open Source when the OSI described their origin like this (emphasis added): > The conferees decided it was time to dump the moralizing and confrontational attitude that had been associated with \"free software\" in the past and sell the idea strictly on the same pragmatic, business-case grounds that had motivated Netscape. They brainstormed about tactics and a new label. \"Open source\", contributed by Chris Peterson, was the best thing they came up with. http://web.archive.org/web/20071115150105/https://opensource... reply benatkin 10 hours agorootparent> What \"original community\" are you referring to here? It's a big community with a wide range of perspectives, but not so big that it can't be understood. To me the original community was mostly over a decade or two. This is similar to other communities such as a burgeoning genre of music. Whatever it was, it was established long before the words \"Business Source License\" were uttered. The first date range that comes to mind is 1993 to 2003 if it's one decade, or 1990 to 2010 if it's two decades. With the smaller range, you have the development of Linux, and the way Linux took over servers. With the larger range, there is Firefox taking on IE, as well as WordPress, Django, and Ruby on Rails becoming popular. Even people who tried to fight it understand it. That is why before the deliberately misleading strategy being used now, some who wanted to promote code that could be read but couldn't freely be used settled for calling source available. reply lolinder 10 hours agorootparentAgain, though—you're fighting for the moral integrity of a term that was explicitly coined to try to buck the moralizing that was associated with the Free Software movement and make the new concept of Open Source more appealing to corporations. The BSL isn't the first sign of the bastardization of the ideal behind Linux. The bastardization started as soon as the OSI decided that they needed to appeal to corporations, and in condemning the BSL the OSI is just following the same business-friendly playbook they've held to all along. Don't get me wrong, I'm glad that the OSI provided a watered-down version of free software that got us to where we are today. I just disapprove of the moralizing that surrounds them when they were explicitly founded on pragmatism. reply sanderjd 10 hours agoparentprevI think this is a pretty good microcosm of the whole debate in this one sentence where you say: > It's not OSI approved because it isn't open source, ... > ... as the community defined it long ago, ... Yep, definitely! Nobody disagrees that the OSI defined this long ago. > ... and as it still makes sense for it to be defined. Maybe! But that's where the debate is. Is that the most sensible definition? Perhaps, even probably, yes. But it's also a totally valid question to interrogate. And that's what people are doing. reply benatkin 9 hours agorootparentThe debate would be for a new meaning of the term open source, which has already been established. People can create a new meaning but it doesn't change the original meaning of it, which I like to call the true meaning. The license is far from being the only thing about open source. What makes open source what it is are its triumphs, such as the popularity of Linux and how many developers prefer open source tools and platforms. However, using a license like the Business Source License indicates a lack of belief in the vision of open source, and a need to exert control. reply sanderjd 9 hours agorootparent> The debate would be for a new meaning of the term open source, which has already been established. Yes, that's what I'm saying, that people are interrogating whether that (inarguably) already established (arguably) \"true meaning\" is a good one. I'm certainly sympathetic to the frustration people feel at new debates popping up over definitions that they feel are already perfectly good. But it's not up to you or anyone else individually; the way people use language broadly evolves all the time. It's useful to advocate for why the existing definition you prefer is the right one, but less useful to primarily focus on \"we already have a definition of this, that's the only thing it could ever possibly mean!\". reply mixologic 8 hours agorootparentprev> What makes open source what it is are its triumphs If you define triumphs to include only 'popularity' and developer preferences, then sure, its triumphant. > using a license like the Business Source License indicates a lack of belief in the vision of open source The issue is that the vision of open source itself is lacking, because it doesn't recognize that it fails to provide a pathway to being compensated and rewarded, tangibly, for building, contributing, and maintaining open source software and the infrastructure that supports it. reply benatkin 6 hours agorootparentPopularity is absolutely part of the triumphs I had in mind. It is often a very good thing in open source. It meant Internet Explorer 6 being less popular, as well as Windows on servers. As far as the pathway to being compensated and rewarded - I want the community to be compensated and rewarded, not just those that started the project. We've seen this play out with ElasticSearch and OpenSearch, as well as Hashicorp. Even Sentry has an alternative https://glitchtip.com/ reply tzs 10 hours agoparentprev> I'll respond to the little part where it puts using a \"non-OSI approved license\" under the umbrella of open source. It's not OSI approved because it isn't open source, as the community defined it long ago, and as it still makes sense for it to be defined So what would you call a license that meets OSI's open source definition [1] but has not been OSI-approved? OSI no longer approves new licenses unless they think the new license fills a gap that is not filled by existing OSI-approved licenses, which means there are millions of possible new licenses that meet every criteria of their open source definition but will become OSI-approved. [1] https://opensource.org/osd/ reply em-bee 9 hours agorootparentSo what would you call a license that meets OSI's open source definition [1] but has not been OSI-approved? arrogant, as in: do you really believe that your project is so different that one of the existing approved licenses will not do? (addressed to the hypothetical project with such a license) i mean, i am with bruce perens who believes that we need to rethink licenses completely to address many problems that have come up recently: https://news.ycombinator.com/item?id=38783500 and i guess this article does hints at some of the problems that need to be addressed. but coming up with a license that is in the spirit of FOSS and yet solves some of these problems is a non-trivial task that i do not believe an average developer or company is capable of by themselves, therefore it is very unlikely that your non-approved license is really worth it. by all means please participate in the process of developing a new license, but do not actually use such a non-approved license until there is a broader consensus that this new license actually is worth it. otherwise it's just making things complicated for no good reason. reply tzs 6 hours agorootparent> do you really believe that your project is so different that one of the existing approved licenses will not do? (addressed to the hypothetical project with such a license) Actually, I think it would be pretty easy to have a project for which none of the existing OSI approved licenses will do without even being all that different, ever since OSI approved AGPLv3. AGPLv3 contains a distribution requirement that triggers for your program if you have users who are \"interacting with it remotely through a computer network\". Now all it takes is wanting a license similar to that, but with the trigger being different. Maybe a project agrees with AGPLv3 that if you run their program on your server you should have to give the users source, but wants that to also apply to users who are interacting with it locally on a computer network, or are interacting via some method other than a \"computer network\" such as serial terminals. reply cratermoon 11 hours agoparentprevBy picking his own definition of what open source means, is the author really arguing for paying people to work on open source? Or is his argument more one of being in favor allowing a bunch of things that happen to pay people to work on them be counted as \"open source\"? For example, if RHEL still counts as open source, then Red Hat's programmers are paid open source developers, but if RHEL is now proprietary, then there are fewer people being paid to work on open source. reply cipherboy 10 hours agorootparentEven with the changes to RHEL licensing, Red Hat developers are still encouraged to upstream changes before landing them in Fedora (and in turn, before landing them in CentOS stream and ultimately RHEL). Nearly every developer at Red Hat working on RHEL will do work in the public, on OSI-licensed packages upstream, before landing changes in RHEL. The change to RHEL licenses is not around source availability of the packages themselves, that has not and cannot change by Red Hat's hand. And it is a risk to Red Hat's business to heavily (internally) diverge packages from upstream as it makes future updates harder. Is it a good move? Many think not. But that doesn't change the vast amount of upstream (OSI-licensed) work that Red Hat directly or indirectly sponsors, past RHEL into their JBoss and OpenShift orgs as well. reply richardfontana 6 hours agorootparentThere were no changes to RHEL licensing whatsoever. reply cipherboy 6 hours agorootparentI agree. I think what OP is talking about is change in the (for rebuilders) source distribution availability, e.g., https://www.redhat.com/en/blog/furthering-evolution-centos-s... -- but as a later blog post points out (https://www.redhat.com/en/blog/red-hats-commitment-open-sour...), the source is still available in a different location, though perhaps at a slightly different point in the release cycle: forward looking to RHEL next. My response is in that context, that even if one were to consider the removal of those source locations somehow a \"license change\" (and I agree with you, it is not), nearly everyone working on RHEL would still be an OSI developer, for reasons pointed out by Mike. Which is a very good thing, and as a former Red Hatter, thank you for helping to keep that possible Richard! reply cratermoon 10 hours agorootparentprevThis is mostly irrelevant to my question. I wasn't speculating about RHEL specifically, but about source available under non-OSI, non-FSF licenses generally. If what counts as \"open source\" can be anything the author says counts, there are potentially lots of projects not previously considered open source, and the developers paid to work on them as \"paid open source\" developers. reply cipherboy 9 hours agorootparentI don't see how it is irrelevant: you ask if they still count, and the answer is yes, because they contribute rather heavily to OSI-approved code, so they'd count regardless. The real question is, would we consider MongoDB or my former employer, HashiCorp's products, presently open source projects? In the latter's case, the answer from the community at large has been to fork (edit: not always successfully), giving a fairly good indication as to the answer... (Whether this is as a result of the act of relicensing the code base or as a result of the license choice probably cannot be fully understood without parallel universes... I'm sure someone would complain and potentially fork if they had relicensed, e.g., from MPLv2 to AGPLv3--another OSI license, but a more restrictive one--though probably nobody would care enough to fork if they had suggested e.g., MIT instead, because the MIT is more permissive.) However, developer categorization into OSI/non-OSI buckets is rather meaningless. What we've by and large found is that Linux businesses (regardless of license model, even fully proprietary) can usually find funding, due to the large number of companies willing to pay for support & contract development on it. Many more businesses have been successful here: vendors like RH, Canonical, SUSE, Oracle, and even Microsoft and AWS, but also many smaller vendors & independent developers who make smaller livings and profits. What's been harder has been the non-Linux Open Source/Free Software business model. And that's what needs to be solved, one way or another. Perhaps that's committing up front to a license (if you want to use the BUSL, so be it, but don't expect the community to be happy if you do so after your project becomes successful). But more likely, its by raising awareness and making sure people at the top of the organization (board members, shareholders) understand the value of OSI licenses and how their companies can benefit from it. And on the flip side, how changing the terms of contracts afterwards can cause problems. :-) reply hardcopy 11 hours agoprevA few weeks ago a wrote in to my Senator on the complete lack of government funding for independent engineers/small projects building FOSS (USA). NLNet in the EU is awesome. We really should have something like the NLNet in the USA. reply onthecanposting 8 hours agoparentI'm undecided if it would be a net good or bad. If you think government should subsidize infrastructure that creates value that's hard to bill to users (like roads), then software is a pretty logical extension. However, given my personal knowledge of transportation project delivery and the astronomical amount of waste it includes, I suspect this might just make things worse. reply hardcopy 7 hours agorootparentI like NLNet model. NLNet isn't a government agency. NLNet is a nonprofit foundation that is responsible for distributing certain government grants, such as NGI Core Zero, which are themed for particular goals. I definitely wouldn't want FOSS projects to apply directly to a government agency. https://nlnet.nl/foundation/ reply Ferret7446 9 hours agoparentprevIf you want to fund FOSS, which is awesome, I suggest donating your own money to the cause rather than using (government) force to take others' money to do so. reply lolinder 9 hours agorootparentAs an alternative perspective—the government currently spends ridiculous amounts of money* on proprietary software, often having chosen the vendor before the bidding process even begins, shaping the RFP to suit only the chosen vendor. The resulting software produced in these contracts remains entirely proprietary even though it was paid for by taxpayer dollars, so the company that built it is effectively guaranteed a perpetual stream of taxpayer funds since only they are legally able to maintain it. I would love to see a world in which these gargantuan vendors are put out of business because the government is only allowed to buy software whose source code is released to the taxpayers who funded it. * EDIT: To quantify \"ridiculous\": > Each year, the U.S. government spends over $100 billion on information technology. Most of that will be used to operate and maintain existing systems, including aging—or \"legacy\"—systems. https://www.gao.gov/products/gao-23-106821 reply snowpid 46 minutes agorootparentWhich government? There are 194 countries in the world. reply hardcopy 9 hours agorootparentprevOh boy, where do I start. First of all, I do donate to multiple projects monthly. Secondly, why should I (a tech enthusiast) foot the bill for software that benefits all of society? Thirdly, an organization like NLNet is much more equipped to determine value to the European Union of funding an open source project, than a random European tech enthusiast is. Lastly, it's pretty amazing what NLNet does with its VERY limited funding. NGI Zero Core, for example, is $11 million euro over 3 years. With that they fund a huge number of open source projects. https://doi.org/10.3030/101092990 reply wil421 9 hours agorootparentprevWhy would I donate to open source? The company work for and my Country’s government spend amounts of money I can barely comprehend. Yet, they use an absurd amount of open source tech. A better question would be how can I convince the company I work for or my government to spend money on open source? reply lnxg33k1 7 hours agorootparentprevGovernment should definitely be involved in investigating collectivity money to create value for everyone, and has a spending power that, frankly, is semi idiotic to compare to what a single guy can do “you should use your money” does not make practical sense reply lucb1e 9 hours agoparentprevOSTIF is vaguely similar and iirc from the USA reply hardcopy 9 hours agorootparentOSTIF scope is really narrow. It's mainly for patching security related bugs/vulnerabilities in existing large projects. And AFAIK it has a significantly smaller source of funding, relying on corporate donations. It's not comparable to the projects that NLNet funds. reply paulddraper 10 hours agoparentprevIDK maybe we just should let the EU pay for it reply redkoala 10 hours agorootparentWe need more funding, not less. reply gorjusborg 12 hours agoprevI want to agree, and I understand the position, but there's no room for nuance when you throw around the work 'always'. I think I disagree that it is always good. For instance, if a company is paying someone to work on open source, and they use that to leverage the project in a direction that is against its other users' best interest, can that be good? I don't think so. There are numerous examples of situations and behaviors you could come up with that are not 'good'. I'm all for people making a living, but I don't like bad behavior, no matter if it generates 'freeish' source code or not. reply jszymborski 12 hours agoparentI think the article addresses this. It's a matter of not letting \"perfect\" get in the way of \"good\". You're totally right, we should work towards getting everyone who wants to work on open source code bases the public funds they deserve at every opportunity, but in the mean time, we'll have to put up with corpos funding some of the FLOSS code. > We have to accept the world as it is – even if it’s not the world we want. This means we have to be okay with the idea that maintainers need to be paid. Far too often I see arguments like: “maintainers shouldn’t be paid by private companies because the government should be supporting them.” Sure, this sounds great – but governments aren’t doing this! So this argument reduces to “open source maintainers shouldn’t be paid”. I can’t get on board with that. reply philipwhiuk 11 hours agorootparentBut some of these aren't good. Some of them are the opposite of good. We are allowing lousy business models to survive by insisting they are better than nothing. reply zrn900 11 hours agoparentprevThat's why the open source communities themselves should be funding their own projects. Because if the communities and users dont fund their own projects, private corporations will fund them and they will have the say. Open Source must not become outsourced 'free labor' which major corporations can leech on. The best way to do it has been the 'freemium' format that is used in the Wordpress ecosystem and a few others - the open source shop creates and maintains a free version of their software under GPL2+, and sells downloads, update licenses & support for more advanced addons. The WP ecosystem was able to float itself with this method without taking in investor money or corporate money, and the software shops that exist in that ecosystem are able to pay their developers living wages. The entire ecosystem was created and is still floated by the open source software producers and the community members. Basically, open source is like politics: Who funds it gets the say. And just like politics, we need to make sure that the communities are self-sustaining economically so that external money wont call the shots. I know that a lot of us in open source software are very proud with our voluntary work and its contributions to open source. That is accurate and praise worthy. But what do we do when we get up in the morning and go to work? We each work in a private company that seeks to maximize its market share and gain more control of the economy, bar a minority of us who work in actual open source jobs. In one hand, we are giving something tangible to open source with our contributions, but the work that we have to do in our day job in a private corporation takes a lot of that away because the organized, concentrated impact of a large private organization with a lot of money goes much further than the heroic efforts of collectives of volunteers. That is why open source must fund itself and become its own economic and political power. Otherwise we will always be giving with one hand with our contributions but involuntarily taking back with the other hand because of the work we have to do in private corporations. And this is without mentioning that if we dont fund & float our own ecosystems and become a collective economic and political power as a community in our own right, we will always be rule-takers and will always have to fight the attempts of the private lobbies at destroying open source. Basically we must create our own world. And in that world, we must be able to work in, make money with, and live with open source. reply smburdick 12 hours agoparentprevThis is why grants are really important. That usually means deliverables in a specific timeframe. To me, that elevates open source from a full-time hobby to a job. reply jraph 11 hours agorootparentGrants are great but are often not nearly enough and they can vanish from a year to another. You'd better secure other sources of income. Grants will also usually fund specific features of your product but not the whole thing. Other kinds of income are also good ways to fund open source like service, consultancy, support and even paid open source apps (which works particularly well for apps that have enterprise oriented features, turns out it doesn't matter that the source code is available under a free software license if it's convenient enough to click and buy). Coincidentally, this is how I get paid :-) Still, grants should not be ignored indeed. reply thfuran 12 hours agoparentprev>can that be good? I don't think so Well that's just an implied always. Is it likely good? No, probably not. Is it always bad? No, probably not. It's conceivable that there are a lot more potential users in the direction the company wants to drag the project, and the few current users can fork it. reply sumuyuda 1 hour agoprev“Far too often I see arguments like: “maintainers shouldn’t be paid by private companies because the government should be supporting them.” Sure, this sounds great – but governments aren’t doing this!” Governments are doing this. The German government funds https://www.sovereigntechfund.de/ reply lolinder 10 hours agoprevThis is as good a context as any to remind people of the origins of the Open Source Initiative and its definitions. Here's how the OSI described its history on its website in 2007 (emphasis added): > The conferees decided it was time to dump the moralizing and confrontational attitude that had been associated with \"free software\" in the past and sell the idea strictly on the same pragmatic, business-case grounds that had motivated Netscape. They brainstormed about tactics and a new label. ... A month later ... the participants voted to promote the use of the term 'open source', and agreed to adopt with it the new rhetoric of pragmatism and market-friendliness that Raymond had been developing. I find it a bit amusing that here we are, decades later, and people who use non-OSI licenses to try to thwart exploitation by enormous corporations are condemned on highly moralistic grounds for not being \"truly open source\". http://web.archive.org/web/20071115150105/https://opensource... reply cipherboy 9 hours agoparentEven businesses care about the distinction between OSI-approved and SSPL/BUSL licensed code bases. In the latter, they often cannot host services that use BUSL licensed code which puts risks on the business. Some examples: Do they need to consult with a lawyer to understand if their particular use case is acceptable? If not now, then how do they know when that threshold is met? When the service is offered for revenue? Or only when offered directly to customers? What about if theirs is a consulting-structured business e.g., IBM or Collins, where any internally service provided to another team is billed and paid for internal to the company (even though its not paid for by an external customer)? Can they hire developers to contribute to the code when the upstream is unresponsive to their bugs/features? Or, if they have to integrate with other custom internal infrastructure/tooling? Are they free to remix these tools into larger projects of theirs? It is possible to separate the moralizing aspects of these licenses and articulate concerns strictly in business cases that make them unsuitable for OSI and thus not \"open source\" in spirit. reply 4kimov 12 hours agoprev> Every time a maintainer finds a way to get paid, it’s a win. Amen. It's becoming more common, and there's lots to celebrate [0] [0] https://fossfox.com/ reply nomilk 10 hours agoprevThe community I've been most involved in over the past few years has been R/tidyverse. Some developers are paid (by RStudio [now Posit] and other orgs, like R Consortium) to work on software, docs, community initiatives etc. The experience as a programmer in this domain is amazing. Having these funded full time OSS contributors lets thousands of R enthusiasts (like me) benefit because someone incredibly high-leverage was paid to give a lot of their time to a project. So when you go to use that library, its docs are immaculate (I'm thinking all the tidyverse packages, Shiny, RMarkdown etc), and the examples are simple and brilliant. Getting up and running is often as little as taking an educated guess at how it would work, and often that's exactly how the function/package was designed! Having at least one dedicated person seems to dramatically improve the quality of OSS, possibly because it helps organise the dozens of people each making smaller contributions. I suspect this works so well because open source projects sometimes don't attract attention to key areas like documentation, and UX (some of my most-loved OSS projects still have horrendous UX because, I suspect, contributors love to add things but nobody wants to be the person who organises it into a coherent package for users, much less remove people's contributions because they're unnecessary and confuse users). When I contrast the experience with communities that have much fewer (or no) full time funded OSS contributors, there's much more niggle and inconsistency with libraries, interoperability, and especially in documentation. Sorry, I'm rambling, but the R community has been an amazing example of how paying a few dozen full time OSS people can have a dramatically outsized benefit to the community for years to come. I'm very appreciative I get to stand on the shoulders of these humble giants. reply barnabee 11 hours agoprevI donate to a decent number of open source projects. Others I think are more than fine without me (Linux kernel, etc.) but I wouldn’t hesitate to donate if I believed they weren’t. For the rest, I would indeed as happily see them fail than compromise on the definition of open source. The two are equivalent to me. reply skybrian 2 hours agoprevThis blog post interprets saying “it’s not open source, though” as if it were a criticism of releasing software under various other source-available licenses. Maybe some people mean it that way, but for me it’s purely about not watering down terms that have a clear meaning. Sometimes source-available licenses are better for the business and it’s understandable why some businesses do that. It’s less generous, but still a good thing. (Just like it’s understandable that people don’t make source code available for all their software.) reply sattoshi 3 hours agoprevWhile I have made a few OSS donations, I want to avoid doing so out of principle. I, a lone developer, should not be funding the work my peers do. How much does openssl benefit me personally? How much does eslint? However much, it’s negligible to how much it benefits my employer. Which in turn is negligible to Google. This is a responsibility that big tech ought to pick up, not random people. reply PaulDavisThe1st 10 hours agoprev> Almost nobody makes a living writing free software. As a percentage of all software engineers, it’s so few we can basically round down to zero. As a member of the zero, I approve the title of TFA. reply andoando 10 hours agoprevI think cooperatively owned tech orgs are the future. Contribution to open source software will always lag behind private entities as people aren't prone to work for free. Are there any open source projects that are monetized/pay their contributors? If I ever get a successful startup going, I am going to explore this model. reply bugbuddy 13 hours agoprevYes, please start by practicing what you preach. I actually donated 1% of my income to various open source projects I use. reply brational 10 hours agoparenthttps://jacobian.org/2024/jan/10/philanthropy-update/ took 15 seconds to find on the website. reply abound 11 hours agoparentprevMy (very small) tech nonprofit has started doing something similar [1], where everyone contributes to a list of OSS tools we use heavily, and then everyone gives a weight/score to each tool. We then split the pot ($1,000 in 2022, probably ~$2,000 when I get around to doing 2023) among all the OSS projects, according to the relative scores. [1] https://siliconally.org/policies/open-source/#yearly-donatio... reply fydorm 11 hours agoparentprevThis is a good thing to do, but not really the point of the article. reply devmor 12 hours agoparentprevI’m really happy that github in particular has made it so easy to give some cash to the people responsible for tools that I enjoy. reply palata 10 hours agoprevI know it's not about the definition of open source, but that's actually what I found interesting from the article :-). It would have been so simple for the author to acknowledge the actual definition of \"open source\" and to just mention that their opinion extends to other models... Anyway: - I did not know the BSL! That actually sounds like a pretty great idea: my understanding is that the company makes the code source-available but with a deadline (of maximum 4 years): after that deadline, the code becomes GPLv2. If more companies used that instead of proprietary, it would be a win for open source in the long run (because more code would become GPLv2)! - I am also discovering Polyform. That's fun, but less exciting to me than the BSL. - The JSON license seems to be purposely annoying. Reads like some kind of \"Fuck you\" to the very concept of licenses. reply palata 10 hours agoprevDid the author actually want to rant about the \"paying people to work on \" part, or was it just an excuse to be controversial about the very definition of open source? Not clear to me. But if it was the former, what a way to shoot oneself in the foot! reply simonw 9 hours agoparentThe author was fed up of people saying \"well actually your achievement in getting paid to work on open source shouldn't be celebrated because of \" - one of which was license definition arguments. That's why he chose to be controversial about that. reply palata 8 hours agorootparentI read the article again. Twice. To me the point is really that he is fed up of people criticizing the people instead of the system. And there are interesting discussions to be had around that, e.g.: is it fine to work on open source projects at Meta, or is it bad because Meta is bad? Instead, for some reason he just spends a whole section redefining concepts instead of just admitting that he may have used them wrongly in his toot. Which is not only completely uninteresting but also confusing. If he had spent as much time redacting his toot than he did writing the \"definitions\" section, chances are that he would not have been pissed off by the reactions to his toot and would not have had to go on a crusade explaining why whoever disagrees with his poor formulation is a jerk. reply wslack 5 hours agorootparent> is it fine to work on open source projects at Meta, or is it bad because Meta is bad? I think OP would say its better to work on open source at Meta than closed source at Meta, and we should celebrate someone being paid to write open source. We can also condemn their specific employer while not denigrating their open source compensation. re your second point, looking at this thread, \"what is open source\" is taking up a lot of the brainspace. reply johngossman 7 hours agoprevGood piece, but it buries the lede. I suspect the reaction would be different if it started with the conclusion. Otherwise, the title and introduction sounds like another article about direct contributions to maintainers instead of being agnostic about how the maintainers get paid. I think “Purity only serves to limit open source’s value to society” is a great debate topic. reply mirekrusin 3 hours agoprevCompanies should be giving employees oss budget they can use for donations. reply FOSSwins 11 hours agoprevI'm a developer with 15+ YOE, working mostly on legacy code in a govt job but I have extensive experience with modern code bases (C, Rust), and I have contributed to lots of FOSS of projects over the years in my free time as a way to learn new tech. I would work full time on Open Source if I was paid enough to leave my 9-5 job, which is not a lot in a third world country like mine. Say, $1500 / month. reply gavinhoward 9 hours agoprevShameless plug, but I already came up with two terms describing the author's vision of open source. https://gavinhoward.com/2023/12/is-source-available-really-t... I also think that forcing companies to accept liability would fund FOSS. https://gavinhoward.com/2023/11/how-to-fund-foss-save-it-fro... Do it right, and the most important projects would be the ones flush with cash. reply openrisk 10 hours agoprevThere is another potential source of funding for open source that is quite congruent to its ethos and that is the public sector. For many types of software used by public sector entities it would be quite efficient to support open source development as a public good. There will always be points of view that would consider this (too) as a problematic source of funding (e.g., being suspicious of government actors and their motives) and it can be a major hassle to handle public sector bureaucracy, but given the distribution of demand for software in the economy it seems something natural to some extend and it could alleviate some of the sustainability issues with open source development. reply flynnz 10 hours agoprevI've always thought a neat model would be something like: pay for convenience, for example, something like: to run git pull/clone, you have to purchase \"premium\" access to their repo, but the code's available and you can just download a tar.gz of it if you want. No way would a company balk at paying a little to have a more reasonable system for updates, etc, but the code is still fully open source, free software, etc. reply davepeck 12 hours agoprevSee also Nadia Eghbal's (IMHO definitive) work on the economics and sociology of open source software, \"Working In Public\": https://press.stripe.com/working-in-public reply mkoubaa 11 hours agoprevSidenote. Some companies offer a \"volunteer time PTO\". You can use it to contribute to OSS reply mise_en_place 7 hours agoprevNo it’s actually terrible. Because then they will eventually abandon the project and you’re stuck with it in your stack. Now you become the maintainer. reply samatman 11 hours agoprevA note to writers: when you find yourself writing a paragraph defensively justifying alienating your intended audience, take a walk around the block and think really hard about whether doing so is a good idea. I will never compromise on the definition of open source. I'm not particularly hard-nosed about proprietary software, or source available software either, they're fine, with some caveats I'll leave out. But it's important to have a term for software which is unencumbered by use restrictions, and we do: open source. Lumping other licenses in with it should be resisted. It's like (I've never seen this, to be clear) pescatarians rebranding as \"seafood vegans\". What is supposed to be gained there, or by trying to bolt on various source-available licenses to the definition of open source? So this guy picks an important topic, and right up front, he's telling me he knows that it's going to piss me off, but he's going to call not-open-source software open source anyway, and if I object, I don't care about developers getting paid. Y'know what? You succeeded. Fuck you, tab closed. reply sanderjd 9 hours agoparent> A note to writers: when you find yourself writing a paragraph defensively justifying alienating your intended audience, take a walk around the block and think really hard about whether doing so is a good idea. I really dislike this kind of \"geez, read the room!\" thinking. Not everybody needs to have the same opinion about everything. Not everybody should. The opinion of \"the room\" or in your terminology, the \"intended audience\", is ever-evolving and the way that happens is via people talking and writing about their own opinions that aren't identical to the prevailing views of the time. But it's fine that you disagree with the author about this and are unswayed by the author's arguments. Others will agree with the author and be unswayed by your counter-arguments, and that's fine too. Still others will change their views after reading the article or responses to it, and that's also fine. Maybe the prevailing view will shift as a result of all this discussion, maybe it won't. This is how discourse works! reply samatman 9 hours agorootparentI think you missed a fine point in my comment. Seeking to avoid offending absolutely everyone who reads your article is fruitless. Sometimes the point is to offend, and that, too, has its time and place. It's when you start adding a paragraph defending your decision to offend your audience that you should give some thought to whether that is, in fact, why you're writing. If it isn't, don't. The author wasn't writing to piss off the FOSS community, that wasn't the topic, just the outcome. Why would I give credence to someone's opinions about open source if they flagrantly refer to things which aren't open source using that term? If you can't get the basics right, you have nothing to say which I want to hear. reply sanderjd 8 hours agorootparentAh! I do see your more subtle point now, and I think it's a reasonable one. It reminds me of the common writing advice to not hedge statements with a lot of \"I think\" and \"I believe\", because that's redundant, if it wasn't what you think, then you wouldn't be writing it, and it weakens statements, making it sound like the writer lacks conviction in the statement, and if you lack conviction in what you're writing then you definitely shouldn't write it. That has always sounded right to me, but in the final accounting, I'm skeptical of it. Certainty just isn't all that natural, ambivalence is common, and I think hedging captures that reality more accurately in the tone of the writing. And I think this case is the same. That paragraph is acknowledging a reality that many or most people reading the article will know, to the point that omitting any mention of it at all will seem notable. I think it is relatable and tactful to say \"I know this isn't a popular view, but I care about it so here goes anyway\". It doesn't imply that they are writing in order to offend. It only implies that they are aware of the situation. I don't really get the thing about whether or not it impacts the credence with which you should take the opinions of the author... And frankly, I don't think it is the important thing; the important thing is the argument they are making. But FWIW, if it were the important thing, this particular author has an enormous amount of credibility in the space of working on a successful open source project... reply acdha 8 hours agoparentprevYou’re talking about someone who’s been working in open source for decades, on pretty successful projects. He knew with absolute certainty that mentioning licensing will lead to pedantic rancor, and that’s unavoidable: there is literally no way to raise this topic in a way someone will not passionately disagree with, and that’s going to distract from the more important topic he wanted to discuss. For example: > But it's important to have a term for software which is unencumbered by use restrictions, and we do: open source. This phrasing means the GPL and MIT licenses are not open source. I doubt that’s what you meant, but simply raising the topic means that we’ll be debating exactly which use restrictions can dance on the head of a pin rather than the real substance of this essay: we all use open source software, we should be talking about how to make it pay a decent living! reply skybrian 2 hours agorootparentI think you misunderstood “use restrictions.” I interpret it as “anyone can use the software built with this source code themselves.” The GPL and MIT license don’t have any use restrictions. (In the case of the GPL, there are restrictions on distribution, but that’s different.) reply barnabee 11 hours agoparentprevAgree and I don’t understand the downvotes. The goal isn’t for every dev or project to make money or be sustainable in open source, just as it isn’t for every business idea to succeed. I donate to numerous open source projects and make a point of donating more than I believe they’d charge me to buy/subscribe if the software wasn’t open source. I encourage others to do so too, I sincerely hope and believe we can see that happen. I’d love to see more truly open source software become sustainable, of course. But I don’t kid myself that it all will be. And I don’t care to relax the definition to include open core, VC exploitative, bait and switch, or whatever (have we learnt nothing in the last two decades?!). If the project dies it dies, if it stays a hobby project that’s ok too. reply ChadNauseam 12 hours agoprevI've put some serious thought into solving this problem. There are two main structural issues I know of: 1. Open source libraries tend to be complement goods. You're more willing to pay for a good physics engine if you already have a good rendering engine and vice versa. But a sad truth of complement goods is that they are a centralizing force - it's actually better for everyone if the physics engine maker and rendering engine maker join forces and offer a bundle discount. But the most common strategy seems to be for them to just merge into one company, and this is why you see giant conglomerate products like Unreal and Unity instead of buying each component from a different vendor. 2. Since open source software is a public good (non-rivalrous, non-excludable), the \"free market\" cannot really incentivize its production nearly as much as would be optimal. Let's say there are 1000 people who would each pay $10 for a feature to be added, and the maintainer would happily add it for $5000. If 90% of those people each paid $6 they would get what they want and the maintainer would be happy too, but each individual has an incentive to be part of that 10% that gets to keep their $6 and still gets the feature, so what happens is that almost no one ends up paying. These problems can't be solved without slightly modifying open source, but they can be solved by maintaining the spirit of open source I think. What you need is to have some kind of foundation that takes money and gives it to \"quasi-open-source\" projects, and then only allows businesses to use those projects if they contribute a certain percentage of their revenue to the foundation. Of course, now the foundation needs to decide which open source projects to give the money too. It's an extremely tricky problem, but there's been a lot of interesting research by Glen Weyl on that exact subject and I'm confident it could be solved in a satisfactory way. I think this proposal would create a virtuous cycle once it got off the ground. The more projects licensed \"quasi-open-source\", the larger the incentive to pay the foundation to use them. The more the foundation is paid, the more money these \"quasi-open-source\" projects get, and so more people will license their projects \"quasi-open-source\", increasing the incentive again, etc. Of course, it would only be \"quasi-open-source\", and not truly open-source. But there's no reason the license couldn't be extremely in line with the spirit of open source. For example, it could say \"if you're an individual or small company, you can use our code for any purpose for free. If you're a big company, you can use it in a way that complies with the AGPL or you can pay us, your choice\". I think employees would also encourage their employers to become paying members of such a foundation, if it lead to those employees being able to determine where some of the money goes. Everyone at my current company is a Rust developer and so we naturally like Rust, but Rust jobs aren't always easy to find. As employees, it could be in our best interest to subsidize the development of Rust open source projects, if that increased Rust's attractiveness to other companies. If you're interested in this idea, my email is in my bio :D reply mrob 12 hours agoparentYou don't need \"quasi-open-source\" to solve the coordination problem you describe. It can be solved with a threshold pledge system[0]. People agree to donate money, and the developer agrees to release the code once sufficient money is donated. There can be a time limit after which the donations are returned if the threshold isn't met. This has actually worked in practice: Blender was originally proprietary software, but the copyright holder agreed to release it under the GPL after collecting 100K EUR in donations. After 7 weeks they collected enough donations and Blender was released as FOSS as promised. [0] https://en.wikipedia.org/wiki/Threshold_pledge_system reply ChadNauseam 9 hours agorootparentThe kickstarter model is awesome and should be used more but it clearly doesn't fully solve the problem. It works great for kickstarter because kickstarter projects aren't public goods (if you pay the money you get the thing you paid money for). In general it doesn't make as much sense for open source because you get the same thing whether you pay or not (unless you're the tiebreaker), so your incentive to pay is limited. reply wmf 12 hours agoparentprevOr just use BSL... reply gustavus 12 hours agoprevI don't understand when it became this was FOSS was always about \"Free as in speech.\" But for some reason it became \"Free as in beer.\" and many of the arguments I see around dev pay seem to be conflating the 2. Open source merely means the source is open and free for you to view look at modify, etc. At no point does it mean it costs nothing. Now with code it's not exactly a super reasonable business model to sell a software product but make it's code freely available, but that would still meet the definition of open source. reply jacobian 11 hours agoparentI think the thing is that it's always been both. The freedom to hack and modify has always been inextricably linked with the $0 license fee. If the early free/open licenses had allowed source access and modification but come with a license fee, or if early FOSS had cost nothing to use but disallowed modification, I don't think we'd have seen the success that we have. The two senses of \"free\" in \"free software\" are and always have been linked. reply Dalewyn 11 hours agoparentprevThe reality noone wants to admit is that most people use and patronize FOSS because it's free-as-in-beer and nothing else. Nobody cares about freedoms, but everyone cares about their bottom line. This extends to even most of the FOSS devs themselves, refusing pay and ostracizing those who accept pay because money to them is kryptonite. In my opinion, this philosophy that runs counter to a very fundamental law of the world (everything, including manhours, requires compensation) plays one of the largest roles in keeping FOSS behind both commercial and proprietary/closed software. reply wmf 12 hours agoparentprev[Never mind, I didn't express this clearly] reply jraph 11 hours agorootparentThat's not true. The company I work for manages to sell free software [1]. All the stuff we sell is under LGPL, and it's not open core, it is fully free software. It works under specific conditions and you need to come up with a business plan that makes it work, but it is possible. And it is one of the most ethical ways to fund free software so it would be too bad to discard this option too early. For us, what works is enterprise oriented extensions for a platform we develop. Turns out companies will fork off hundreds of dollars and enjoy the support that comes with it instead of compiling all this thing by themselves. It's more convenient and employees understand that it funds the open source software they are using, and it's an easily justified expense. But should they want to enjoy any of the freedoms that come with free software, they can. [1] https://store.xwiki.com reply reedciccio 11 hours agorootparentprevTell that to Red Hat, WordPress, Canonical, MySQL and many more products built on pure Open Source. The issue is complex. reply ahepp 11 hours agorootparentprevI’ve worked for multiple companies that pay for dual licensed GPL software reply preommr 11 hours agoparentprevPerhaps unpopular opinion, but it's because the 0$ cost is what 99% of OSS users care about. Since it requires no investment on part of the user, it increases the potential target market to a much larger size than it would if it were paid. There's just something about things being free that break people's minds. There's even a study on this where they offered chocolates for free vs 0.01$, and the free option was much more popular even though the 0.01$ chocolate was much higher quality and much better value for a very negligible difference. Lots of users just want to download something, use it for a few minutes and be done with it. Or at least try it out and know that they can fall back to a free version at worst and not feel like they made a bad investment. reply mcmoor 3 hours agorootparentI think this is more that pay barrier significantly raises obstacle no matter how much money you actually have to give. Even as little as 10^-10 cent. Just by nature of transaction verification have a hard cost no matter how much money actually transferred. What would finally vanquish ads off the internet is micro transaction that is actually able to bypass this barrier entirely. This is what bitcoin promised to do but of course, they don't solve real problems. reply coretx 11 hours agoprevIt certainly is good, but money also turns many people bad and impacts organizational dynamics. reply philipwhiuk 11 hours agoprevSo many strawmen being set-up in this article the crow population is gonna take a major nose dive. If I hate any specific business model that is used by a company that does some some open source suddenly I don't think people deserve to be paid for their work? Yeah no, that's garbage. There's plenty of garbage business models and they aren't suddenly okay because one company uses it and 1% of their money funds some small bit of OSS work that underpins their business model. reply debo_ 9 hours agoprevDid anyone following this in Mastodon-land specifically see any reaction to \"luxury automated gay space communism?\" I would be surprised if he wasn't blasted for that. This article came across as much less ranty than I expected based on his disclaimer. I think he pretty much perfectly articulated the noise around funded open source. reply acdha 9 hours agoparentIt’s been quieter there than here other than done licensing derails. I think most people are desensitized to it after a decade of memes: https://knowyourmeme.com/memes/cultures/fully-automated-luxu... reply debo_ 8 hours agorootparentAh, my self-imposed limits on internetting have saved me here. This is the first I've heard it. Thanks for the link. reply jpetso 9 hours agoprevIf we're going to ignore the \"official\" meaning of open source, then let's take a step back to consider why open source is worth supporting in the first place. Open source guarantees to me, the user, that competition among vendors will be possible and fair in the future. This is exactly the point that many \"fake OSS\" licenses try to take away. Okay, maybe it's possible to fork for personal on-prem use, but god forbid someone creates a competing hosted solution that gives any customer more choice. Furthermore, these pieces of software are fucked the day that the company folds, or gets acquired by a malevolent buyer. Open source guarantees a baseline level of respect towards me, the end user. By letting anyone fork a project that's gone too far in the wrong direction, I know that my software will continue working in the short run and of it's important enough, a competing alternative will emerge that continues without one-sided money or data grabs. There's nothing inherently wrong with having someone from Microsoft or Google work on open source software, or any VC-funded company that will without fail turn against their users sooner or later. However, if a controlling majority of developers is employed this way, it provides an opportunity for what elsewhere is known as regulatory capture. If Microsoft's goal is to make people dependent on proprietary GitLab and VS Code Marketplace offerings, and Google's goal is to provide the greatest possible amount of ads and tracking to the largest possible user base, it does not matter if the software is open source or not. The end result is the same, I'm left without viable alternatives and big business gets to do with us whatever the hell they please. Especially when this software becomes ubiquitous and entrenched, paying developers to work on company-controlled OSS instead of community-driven, user-respecting OSS is a net negative for everyone in the long run. I'm only interested in OSS in so far as it protects my interests as an end user, and/or our common interests as a society, now and in the future. The collaborative aspect is nice, but that's not the reason that we should ask for better compensation for maintainers. The \"Open Source\" label as such is indeed meaningless per se, and it doesn't always protect me either, as seen with BSD+MIT software allowing cryptographically-enabled control of devices that I nominally own, or GPL being useless when there is no actual distribution of software involved. That said, I have yet to see a case of non-OSI \"open source\" that doesn't try to tilt the playing field in biased, controlling and long-term unsustainable or user-hostile ways. If you can't build a business on a level playing field, perhaps it's in everyone's interest that your business and software dies, or retreats into lower-intensity hobbyist maintainership, instead of leading everyone into a hard dependency on your oh so well-intended monetization of originally useful software. Then at least someone else can take a shot at doing it better. reply jart 8 hours agoparentThose are some pretty twisted reasons to support open source. First of all, you are not a \"user\" if you use open source. You are an owner. Open source gives you the freedom to control the development process of the software. It sounds like what you want is the freedom to have other people serving you. Also, an inventor who chooses to reserve some rights to control their invention is not acting anti-competitively. You're disagreeing with both law and morality by thinking that. You are not entitled to anything. Open source usually happens because the inventor has nothing to gain from exerting personal control through legal means over their invention. So what it in effect does, is it gives you the power to take control and participate in its development, as an equal, rather than a mere consumer. You can't walk into open source with the consumer mindset because that's just not how things work. Companies like Microsoft that retain full control over their software will break their backs to serve you, because they're the only ones who can. But you can't expect that kind of service from people who are simply trying to give you the DIY tools to do it yourself. reply jpetso 7 hours agorootparentI am an owner if I can exercise control over the software at hand. That's entirely my point - there is open source that a rando like me can hop in and improve, sometimes requiring difficult discussions about how to go about it exactly, but always with the experience of the user as a priority. (User can be an end user, but also a developer who's using an open source library/framework.) And then there's \"open source\" where the code is accessible but the user experience takes a backseat to corporate interests, CLA requirements provide a one-sided transfer of copyrights, hobbyist contributions are systematically steamrolled by optimizing build pipelines and development processes for internal company use, and large-scale directions are decided in a private meeting room without involving community contributors. If an inventor reserves some rights to control their invention for their own benefit, I have no problem with that. There's plenty of commercial software out there, people are working hard to provide value to customers, and I've been part of this system too. Where I take issue is when we ask for special treatment of \"open source\" whose main purpose is to benefit commercial entities in doing business. Companies should figure out on their own how to keep their mission-critical software alive, that's their business. If Django suffers because lots of profitable outfits can't figure out a way to finance what they build upon, let them eat dust. They'll figure it out eventually when their services start falling behind on all fronts. As a charitable coder, I'm going to invest my time into providing value for end users, not companies. That's the kind of open source we as a community/society should focus on supporting and financing. Imho. reply jart 6 hours agorootparentYou should be supporting and financing open source that elevates knowledge. Knowledge is the resource that open source distributes which folks fight to control. It's like the fruit you'd grow on a farm. You could argue about whether or not the fruit should be distributed more to the city folk or the country folk, or ask questions about how much money the farm is making, but I'd say you should be focusing on getting the farm to grow more fruit, since that's the only way to be sure everyone becomes richer as a result. Scientists do a great job discovering knowledge, but open source is what makes it useful and able to be used. Any open source project that's helping to elevate know-how is a project worth supporting. reply delichon 12 hours agoprevA pro capitalist message from Jacobin? [Looks closer.] No. I'd pay for an open source project that could filter & sort news by surprisingness-for-that-news-source. This opinion would rank high for jacobin.com. The story about Zuckerberg's preference for the Quest 3 over the AVP would disappear. reply prisenco 12 hours agoparentJacobian, not Jacobin. But even so, paying people for their labor is entirely uncontroversial amongst socialists. Some might even argue it's the fundamental underpinning of their critique of capitalism. reply mkeeter 12 hours agoparentprevhttps://jacobian.org is Jacob Kaplan-Moss's website https://jacobin.com is a socialist magazine This blog post is from the former! reply delichon 12 hours agorootparent*jacobian.org for JKM's site reply mkeeter 11 hours agorootparentthanks, edited! reply axus 12 hours agoprevGiven free hosting and compute by the NSA? Win reply throwitaway222 10 hours agoprevI'll just say this: I don't think the government should pay for open source development. reply netbioserror 12 hours agoprev [–] The premise of paid open source devs is fine and well, but every single one of these blogs devolves into delusional utopian nonsense from people who do not understand the staggering infrastructure and maintenance cost of the modern society they think should be some sort of guaranteed right. People, please learn and understand where your food comes from before writing this kind of garbage. reply woodruffw 12 hours agoparentTFA’s author is one of the co-creators of Django and a preeminent member of the Python community. I think it would be charitable to assume that he does, in fact, understand these things. reply netbioserror 12 hours agorootparentnext [3 more] [flagged] woodruffw 12 hours agorootparentI don't really have anything to say here, other than to reiterate: it is simply not charitable to assume the author is \"stupid,\" \"deluded,\" or similar. It adds zero information to the conversation, and actively detracts from your underlying point. reply netbioserror 10 hours agorootparentHis little soapbox rant sucked every last bit of energy out of his underlying point. reply lcnPylGDnU4H9OF 12 hours agoparentprev [–] FWIW, the author was at least thoughtful enough to include this disclaimer at the top: > Warning: rant ahead. I’m writing from a place of frustration and not particularly interested in trying to moderate my tone. If you don’t want to hear me yell about open source for a while, please skip this one. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author emphasizes the importance of paying individuals to work on open source projects for sustainability, defining open source as software that can be read, modified, and released under specific conditions.",
      "They highlight the insufficient sustainable funding available for these projects, stressing the necessity for maintainers to have a means of livelihood.",
      "Criticizing the stigma around maintainers seeking compensation, the author suggests supporting any form of payment as a positive step towards sustainability and acknowledges the efforts to make a living from open source work."
    ],
    "commentSummary": [
      "The article delves into open-source licenses, funding debates, value of paid contributors, and challenges in preserving open-source software integrity.",
      "Emphasizes the evolving tech community viewpoints and the necessity for clear licensing definitions to curb debates.",
      "Proposed solutions include \"quasi-open-source\" licenses, dual licensing, prioritizing user interests, and societal well-being over commercial gains in open source."
    ],
    "points": 304,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1708113170
  },
  {
    "id": 39404364,
    "title": "Air Canada to Compensate Customer Misled by Chatbot",
    "originLink": "https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit",
    "originBody": "View image in fullscreen The judge wrote that Air Canada’s customers had no way of knowing which part of its website – including its chatbot – relayed the correct information. Photograph: NurPhoto/Getty Images Canada Air Canada ordered to pay customer who was misled by airline’s chatbot Company claimed its chatbot ‘was responsible for its own actions’ when giving wrong information about bereavement fare Leyland Cecco in Toronto Fri 16 Feb 2024 13.27 EST Canada’s largest airline has been ordered to pay compensation after its chatbot gave a customer inaccurate information, misleading him into buying a full-price ticket. Disabled man drags himself off plane after Air Canada fails to offer wheelchair Read more Air Canada came under further criticism for later attempting to distance itself from the error by claiming that the bot was “responsible for its own actions”. Amid a broader push by companies to automate services, the case – the first of its kind in Canada – raises questions about the level of oversight companies have over the chat tools. In 2022, Jake Moffatt contacted Air Canada to determine which documents were needed to qualify for a bereavement fare, and if refunds could be granted retroactively. According to Moffat’s screenshot of a conversation with the chatbot, the British Columbia resident was told he could apply for the refund “within 90 days of the date your ticket was issued” by completing an online form. Moffatt then booked tickets to and from Toronto to attend the funeral of a family member. But when he applied for a refund, Air Canada said bereavement rates did not apply to completed travel and pointed to the bereavement section of the company’s website. skip past newsletter promotion Sign up to TechScape Free weekly newsletter Alex Hern's weekly dive in to how technology is shaping our lives Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion MP stopped from boarding Air Canada flight as ‘his name was Mohammad’ Read more Air Canada later admitted to Moffatt, when confronted with a screenshot of the chatbot’s advice months later, that the bot had used “misleading words” in its advice. The airline told Moffatt it would update the chatbot. Moffatt then sued for the fare difference, prompting Air Canada to issue what the tribunal member Christopher Rivers called a “remarkable submission” in its defense. Air Canada argued that despite the error, the chatbot was a “separate legal entity” and thus was responsible for its actions. “While a chatbot has an interactive component, it is still just a part of Air Canada’s website. It should be obvious to Air Canada that it is responsible for all the information on its website,” wrote Rivers. “It makes no difference whether the information comes from a static page or a chatbot.” While Air Canada argued correct information was available on its website, Rivers said the company did “not explain why the webpage titled ‘Bereavement Travel’ was inherently more trustworthy” than its chatbot. “There is no reason why Mr Moffatt should know that one section of Air Canada’s webpage is accurate, and another is not,” he wrote. Air Canada must pay Moffatt C$650.88, the equivalent of the difference between what Moffatt paid for his flight and a discounted bereavement fare – as well as C$36.14 in pre-judgment interest and C$125 in fees. Explore more on these topics Canada Chatbots Airline industry Americas news Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=39404364",
    "commentBody": "[dupe] Air Canada ordered to pay customer who was misled by airline's chatbot (theguardian.com)297 points by sandebert 10 hours agohidepastfavorite4 comments taspeotis 9 hours agoDid they have to pay them again? https://news.ycombinator.com/item?id=39378235 reply dang 7 hours agoparentComments moved thither. Thanks! reply ChrisArchitect 9 hours agoprev[dupe] Lots of discussion yesterday: https://news.ycombinator.com/item?id=39378235 reply petesergeant 9 hours agoprev [–] Yesterday: https://news.ycombinator.com/item?id=39378235 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Air Canada must compensate a customer who was misled by its chatbot into purchasing a full-price ticket instead of a bereavement fare.",
      "The tribunal ruled in favor of the customer, emphasizing that the airline is accountable for all information, even that generated by the chatbot, on its website.",
      "This case highlights concerns about the supervision of chat tools within companies and the need for accuracy and accountability in automated systems."
    ],
    "commentSummary": [
      "Air Canada was mandated to compensate a customer who was deceived by the airline's chatbot, showcasing the potential risks of automated customer service.",
      "The incident was previously debated on the news aggregator platform Hacker News, highlighting the interest in the intersection of technology and consumer rights."
    ],
    "points": 297,
    "commentCount": 4,
    "retryCount": 0,
    "time": 1708125706
  },
  {
    "id": 39395329,
    "title": "German credit agency SCHUFA accused of GDPR violations",
    "originLink": "https://noyb.eu/en/german-credit-agency-earns-millions-through-unlawful-customer-manipulation",
    "originBody": "German credit agency earns millions through unlawful customer manipulation Feb 16, 2024 German credit agency earns millions through unlawful customer manipulation Today, noyb has filed a complaint and report against the German credit agency SCHUFA with the Hessian data protection authority. The company appears to be making millions of euros by selling people in Germany their own data. With the help of manipulative designs, people are prevented from obtaining a free copy of their data in accordance with Article 15 GDPR – even though they would actually be legally entitled to it. The company’s primary aim appears to be to profit from people looking for accommodation. In Germany, they often have to provide proof of solvency in order to obtain a lease. Complaint and report against SCHUFA (DE) Lucrative business at the expense of apartment seekers. Anyone looking for a flat or house to rent in Germany is regularly asked to prove their financial reliability. As a result, people looking for accommodation often end up at credit agencies such as SCHUFA – which make a fortune by selling people in Germany their own data. What SCHUFA deliberately conceals: according to Article 15 GDPR, it would have to provide precisely this data free of charge and without delay. Concealed rights. On its own website, SCHUFA only advertises its so-called “BonitätsAuskunft” for €29.95 to private individuals and claims that it offers an “advantage on the housing market”. A transparent reference to the Article 15 GDPR right to free information is not provided. Using manipulative designs, the company is trying to push the sale of paid products and even falsely presents the free information as unsuitable for submission to third parties. Martin Baumann, data protection lawyer at noyb: “SCHUFA is falsely claiming that only its paid products can be presented to third parties. In reality, the European Court of Justice has emphasised several times that data subjects are allowed to do whatever they want with their free information.\" “Data copy” instead of information. The vast majority of data subjects is unlikely to even find the free information. Although the GDPR stipulates that companies must support data subjects in obtaining their free information, SCHUFA does not even mention it by name. The company casually refers to the information in accordance with Article 15 GDPR as a “data copy”. In fact, a range of further information needs to be included as well. At the same time, the legal term “information” in accordance with Article 15 is misused for the paid product (BonitätsAuskunft). Anyone who manages to find the hidden option to request the information free of charge is once again bombarded with adverts for the paid product. In addition, SCHUFA advises against sharing the free information with third parties. On the one hand, it supposedly contains sensitive data, but on the other hand “not an up-to-date calculation of your creditworthiness scores”. Data is deliberately withheld. As a result, SCHUFA is violating European data protection law in several ways. The company doesn’t take any measures to make it easy for data subjects to exercise their right of access to data, contrary to the clear requirements of the GDPR. Additionally, the company deliberately withholds information in order to be able to sell its paid product: For example, in the case of the complainant, the free information included only a “basic score”, while the paid information showed six different “industry scores”. Article 15 GDPR obliges the company to disclose all processed data in full. To make matters worse, the complainant received the paid product after five days, while the free information, which was ordered at the same time, took significantly longer. Here, too, the GDPR actually requires an “immediate” delivery. Martin Baumann, data protection lawyer at noyb: “The GDPR requires companies to make all data available immediately, transparently, easily accessible and free of charge. These requirements are in clear contradiction to the current business practice of selling people their own data.” Complaint and report in Germany. noyb has therefore filed a complaint against SCHUFA with the Hessian data protection authority. By systematically hiding and delaying the free information and deliberately withholding data, the company is in breach of the GDPR. In addition, noyb is filing a report with the Hessian DPA. SCHUFA systematically violates the legal requirement of free information by creating the impression that only the paid products are suitable as proof to third parties. tweet share share share mail Related articles Recent articles German DPA declares data trading between credit agency and address trader illegal Feb 05, 2024 noyb win in proceedings against the credit reference agency CRIF and the address trader Acxiom in Germany Read more Creditors' association earns millions with (actually) cost-free GDPR rights Jan 04, 2024 KSV1870 uses misleading website designs to urge people to purchase a high-priced \"InfoPass\" instead of getting a free copy of their data Read more Project Credit Scoring Support us! noyb funding goal 76.18 % Invest in privacy! Follow us! Facebook Twitter Youtube LinkedIn Instagram Newsletter Mastodon RSS Media Coverage Irish regulator proposes 36 mln euro Facebook privacy fine - document The complaint, lodged by Austrian privacy activist Max Schrems, concerned the lawfulness of Facebook's processing of personal data, specifically around its terms of service. Read more U.S. tech giant Meta has been hit with a record €1.2 billion fine for not complying with the EU’s privacy rulebook. Read More Apple hits back at European activist complaints against tracking tool An Austrian privacy advocacy group drew a strongly critical response from Apple on Monday after it said an online tracking tool used in its devices breached European law. Read More 101 Unternehmen leiten noch immer Daten an die USA weiter Der Datenschutz-Verein noyb brachte 101 Beschwerden gegen den Datentransfer in die USA ein. Read More ZDF Magazin Royale vom 10. Dezember 2021 Expertentalk mit Datenschützer und Facebookbezwinger Max Schrems Read More Irish regulator proposes 36 mln euro Facebook privacy fine - document The complaint, lodged by Austrian privacy activist Max Schrems, concerned the lawfulness of Facebook's processing of personal data, specifically around its terms of service. Read more U.S. tech giant Meta has been hit with a record €1.2 billion fine for not complying with the EU’s privacy rulebook. Read More",
    "commentLink": "https://news.ycombinator.com/item?id=39395329",
    "commentBody": "German credit agency earns millions through unlawful customer manipulation (noyb.eu)286 points by latexr 23 hours agohidepastfavorite248 comments ArmandGrillet 21 hours agoFinally. Germany combines some of the worst aspects of the US (credit ranking, complicated abortion process, private healthcare if you want decent treatments) with the worst aspects of Europe (low digitalization, high taxation, recursive federalism: within the country and within the EU). A country like the Netherlands has its own issues (mainly housing) but doesn't have the myriad of pain points you can find in Germany like Schufa, anti-customer contract rules, or public healthcare inaccessible despite paying more than 400€/month for it as a single individual. reply ciclotrone 19 hours agoparentI find it quite disturbing that high taxation is seen as a negative aspect of Europe. I lived in Italy, France and Germany, and I enjoyed public healthcare and education of very high standard at very affordable prices, and free in the limit that one cannot afford to pay for them. As a relative of a person with a chronic disease I can tell you that on the one hand if we are not bankrupt it is because of public healthcare, and on the other I'm proud of contributing through my taxes so that anybody in need can have the same treatment irrespective of their economic situation. reply mk89 12 hours agorootparentMaybe nitpicking here, but healthcare is not financed with your taxes in Germany - it's financed by statutory health insurance (SHI) and private health insurance (PHI). The State \"simply\" sets the framework/legislation/etc. so that it doesn't get wild (like probably it is in the USA). [0] In Italy it's the state/regions that take your taxes and pay the health system. [0]: https://www.bundesgesundheitsministerium.de/fileadmin/Dateie... reply dkural 16 hours agorootparentprevThere is no capital gains tax - Germany has less progressive taxation than the United States! VAT in general is a regressive tax, that many EU countries inordinately rely on. A US-citizen has to pay taxes no matter where they reside, but the wealthier citizens of EU countries can easily evade taxes by domiciling themselves in various tax havens around the EU. Germany does a very poor job collecting taxes from the highest earners. reply kmlx 15 hours agorootparentprev> I find it quite disturbing that high taxation is seen as a negative aspect of Europe. after receiving the same services for no or minimal tax i also find it very disturbing that people still think high taxation is necessary. reply 0xFF0123 13 hours agorootparentWhere? reply mk89 12 hours agorootparentIn general, if you don't pay taxes, in the listed countries you still can get treatment. It was probably a provocation - I guess? reply formerly_proven 19 hours agorootparentprevHigh taxes with no matching value behind them is undeniably a negative. reply Nextgrid 19 hours agorootparentYes, that's my problem with high taxation, as someone who got burnt by it by virtue of living and paying taxes in the UK. I don't mind paying my fair share of tax if it means I get good value out of it, but the state of our healthcare here means that in practice the public healthcare system is no longer fit for purpose, so we are being forced to pay for a system that doesn't work and end up having to go private (and thus pay again) when we do need timely healthcare. I am generally in favor of fair, progressive taxation to ensure everyone has a good social safety net, but the taxation should be fair (it's no longer the case in the UK, since tax brackets haven't been adjusted for inflation) and the services paid for by those taxes should provide good value. The danger with services funded by taxes as opposed to a private enterprise operating in a free market is that private enterprises are bound by competitive pressure - if they are delivering terrible service and stuffing their pockets with the money, you are free not to do business with them and a potential competitor (that stuffs their pockets a little less) can come along and get your business instead. With tax-funded services, this pressure doesn't exist, so there's no incentive for politicians (and everyone else in the value chain) to deliver good service, since people generally can't opt out of taxes. This means that even if a service is currently good, there's no guarantee it will remain so since the pressure for it to remain isn't there. Thus, when you see high taxation, it's reasonable to be worried whether the service provided by those taxes is any good and whether it will remain so in the future. reply graemep 16 hours agorootparentThe UK tax system (like many others) is not as fair or progressive as it is presented to be. 1. NI means that the real standard rate on earned income is substantially higher than the \"income tax\" rate. it is also not fair that unearned income is exempt from it. 2. Lots of purchase taxes, which are disproportionately paid by people with moderate incomes. People on low incomes spend a higher proportion in necessities which are (rightly) subject to lower levels. The more money you have (beyond a certain point) the less you spend on things subject to these taxes. 3. Too many loopholes. > so we are being forced to pay for a system that doesn't work and end up having to go private (and thus pay again) when we do need timely healthcare. Some of the NHS is good. NHS dentists can be very hard to find and waiting lists are long. Waiting times can be long too. Reform is prevented by the fear of a US type system and I think many people think that is the only alternative (and seem not to realise how things operate in most other developed countries). reply Nextgrid 16 hours agorootparent> seem not to realise how things operate in most other developed countries Genuine question, what do other countries do differently? The way I see it (and described in my original comment) is that the underlying factors behind the decline of government services (no accountability, no incentive to use tax money efficiently) are common across many countries. Some countries may get away with it for now because they're still \"early\", but if there is no pressure to do well it's just a matter of time before they too suffer the same fate? reply cycomanic 14 hours agorootparentI'd argue that the decline in quality of services as been a phenomenon across many western countries and to significant degree been caused by a economic idiology and a deliberate campaign with a goal of privatisation of many public goods, started in the 1980s. Essentially funding to services are being reduced to pay for tax reductions, until at some point the quality declines significantly, which is taken as the argument that private enterprise would deliver a much better service at lower cost. Services/infrastructure are then sold and a low price and after a short while costs for services go up, but because they are now private enterprises raising prices it is \"the market\". The reason why the UK is worse I believe it's due to the political system which results in essentially 2 dominant parties with very little differences. reply mint2 18 hours agorootparentprevExcept there’s a lot of services that are unsuited to being delivered by competitive markets as it’s nearly impossible to prevent monopolization. reply Nextgrid 18 hours agorootparentAgreed, although this can be addressed by appropriate regulation. However, the ultimate issue is that the government should do its job well, and whether it's the task of adequately regulating a market vs providing the service itself, the problem that there's no pressure it to do it well still remains. reply 261582335426158 16 hours agorootparentprevThe key point to add is that (at least) 2 points must be true for this positive side of market to show: 1) There should be \"forces\" counterbalancing the move toward monopolies 2) It should be okay for the service to fail/die and/or it should degrade gracefully. For example book publishing is a wonderful field that can gain from free market dynamics while prison management is a terrible field for free market dynamics. While a dying publishing enterprise might print less books at a lower quality with deceptive marketing (and that is sorta okay) a dying prison management enterprise will squeeze every ounce of dignity/health/safety out of its wards. Market dynamics for the economy are analogous to natural selection for evolution: both can produce wonderful things but they will require many more to die horribly. This is not a statement against markets but I am calling for this tradeoff to be considered along with many other. reply Nextgrid 16 hours agorootparentThe same factor that makes private prisons problematic also makes taxation problematic - in both cases you've got a captive market that can't opt out thus no pressure to provide a good, efficient service. In a hypothetical world where inmates could choose whether to remain in prison and which one to be in, private prisons would be competing for service quality. Actually we've got that, they're just called \"hotels\". I wonder if the solution there would be to force politicians and everyone involved to exclusively use government-provided services - so as a condition of going into politics, you can't use private healthcare, your kids can't go to private schools, and you need to spend a day every year in a random prison and so on. Then it would mean the decision-makers got at least some incentive to make sure these services are functional and fit for purpose. reply autoexec 15 hours agorootparent> The same factor that makes private prisons problematic also makes taxation problematic - in both cases you've got a captive market that can't opt out thus no pressure to provide a good, efficient service. What you should have are companies that offer their best prices/services to the government out of fear that they'll be passed over and those highly valuable contracts will be awarded to their competitors who do better. Governments are incentivized to select the best companies to award those contracts to because they risk being voted out if people are unhappy with the level of service they're getting for their tax dollars. Prisoners can't kick politicians out of office when they're unhappy about the state of the prisons so that's a huge issue with prisons in general, but private prisons have another issue which is profit. Whatever it costs to keep dangerous people locked up, private prisons need all of that money plus they must extract a bunch of extra money from the public just to stuff their pockets with. If private prisons want to keep prices low to the public but also want to keep filling their pockets with money they need to provide substandard care to the people they are responsible for. A non-private prison needs only what it costs to do the job and nothing more. reply ScoobleDoodle 18 hours agorootparentprevSomehow in the USA these theoretical competitors that lower the profit margin and provide better service seem not to materialize. We end up with whole industries all of whose participants are funneling money to the executives and investors at the expense of customers and society. It’s such a broad issue that half of inflation is attributed to this corporate greed which is not being mitigated by the invisible hand of capitalist competition. Without proper regulation and enforcement capitalism devolves to monopolies and oligopolies and eliminate competition and subdue market forces. reply mathgradthrow 17 hours agorootparentprevprivate enterprise is not \"bound by competitive pressure\". Companies only compete by virtue of the government forcing them to. Are you arguing that humans, inventors of civilization, are too stupid to figure out how to cooperate? reply Nextgrid 16 hours agorootparentI disagree that companies only compete thanks to regulation. Companies competing is the default - regulation sometimes (either as an unintended oversight or malicious intent thanks to lobbying/corruption) prevents it though. There are many valid scenarios where competition is lacking, but generally speaking the reason it’s lacking is due to regulation/law making it impossible. Telecoms for example is impossible because the incumbents have exclusive control of the physical infrastructure (poles/ducts under the street) or spectrum auctions where the price makes it impossible for a new entrant to enter. Tech network effects are maintained by copyright law being abused to prevent adversarial interoperability. reply graemep 16 hours agorootparent> Companies competing is the default If you had no regulation, they would simply form cartels or merge to create a monopoly. reply Nextgrid 15 hours agorootparentMerging and forming cartels is still technically a choice. A company could decide not to do that and compete. It may require a near-infinite amount of money, but it's still technically possible. reply A1kmm 1 hour agorootparentIn the absence of any regulation, if a company's objective is to maximise profits, and it acts as a rational actor solely focused on achieving that objective, joining cartels, and creating barriers for entry is the end state of the market for nearly all starting conditions. reply mcmcmc 18 hours agorootparentprevUnless you want to get into the nitty gritty of economic policy analysis and measuring market externalities, value of public goods is pretty subjective. A healthy person might not see much value in public healthcare and a single adult with no kids might not care about education. Just because you don’t value something in the same way doesn’t mean it’s not highly valued by the people who voted on and implemented the policies. reply jimcsharp 18 hours agorootparentMy neighbors suck less when they're warm, full, and healthy. How do the anti tax folks get past that hurdle? I'm trying to live my most evil life here but I'm stuck on very selfishly wanting cool neighbors. reply mech422 10 hours agorootparentprevwell - my son is out of school - but I'll still pay for good schools cuz when I retire I want a doctor that can read... People forget, the young'uns are the ones you'll be counting on as you get older. Best to give them the best education possible, IMHO. reply breather 17 hours agorootparentprevAh that's where the much higher quality of life kicks in. reply mschuster91 18 hours agorootparentprev> High taxes with no matching value behind them That is not the case for Germany. We have - even with all its shortcomings - a decent education system that's largely free and accessible for everyone, a decent healthcare system, a decent public transport system on local, regional and federal system, and a very decent social security network that covers unemployment, pension and elderly care. The US in contrast has neither: parents have to pay five digits worth of money just for the birth of a child, pay through their nose for insurance, public schools are horribly underfunded, universities require six digits worth of debt, there's no high-speed rail worth the name, if you're unemployed you better have some savings, you have to take care about your 401k, and the best way if you need elderly care is to off yourself with a gun. The only thing where the US actually is in front of Germany is allowing you to get a gun to off yourself. Here in Germany, it's almost impossible to own a gun, so people love to off themselves by jumping in front of a train, creating a huge mess for everyone else. reply Beijinger 16 hours agorootparent\"a decent education system that's largely free and accessible for everyone\" I give your Universities a 2-3 (On a scale 1 Top, 5 fail). High schools are getting worse and worse due to the immigrants. So you would want to send your kid to a school without many immigrants, mainly due to language barriers. \"a decent healthcare system\" Again, I give you a 2-3. More and more people are sucking services, fewer people pay in. Hey, alone 1 Million Ukrainians are insured, while less than 25% have a job. How long do you wait to see a specialist? >6 months! \"a decent public transport system on local, regional and federal system\" Again grade 2-3. You have a functioning public transport system. But ask Switzerland what they think about it. Or look at China if you want to learn something about bullet trains. \"and a very decent social security network that covers unemployment, pension and elderly care.\" Is on the brink of collapse. Pensions are great. For state employees. The rest gets retirement benefits. They are compared to the EU very low. I don't have health insurance by the way. Germany has to take care of so many immigrants. Nothing left for me. And try to compete with them to find an apartment. The government pays for them. And the government is never late with payments, hence landlords prefer this. reply Scharkenberg 15 hours agorootparentCan you cite your sources? This literally sounds like regurgitated shit from far-right politicians rather than thought-out analysis based on real statistics. In fact I would go so far as to suspect astroturfing. reply Beijinger 15 hours agorootparentYes sure. For German articles please use google translate. 1. a) 0 Universities among the top 10: https://www.timeshighereducation.com/student/best-universiti... (In most other rankings, German universities rank much worse) b) sinking high school performance: https://www.oecd.org/publication/pisa-2022-results/country-n... The collapse coincides with the arrival of refugees under Merkel.... c) Many immigrants in school. Hint: You can't teach math or chemistry if the majority of the kids don't speak German https://www.spiegel.de/spiegel/integration-an-manchen-schule... 2. a) Currently don't find a better article. Biggest problem compared to China. Germany has the same tracks for bullet trains, local trains and freight trains. Often the bullet trains run a 120 or 160 km/h https://www.businessinsider.de/tech/warum-es-ultraschnelle-z... b) Switzerland complaining that German late trains destroying their time tables: https://www.focus.de/finanzen/news/schweizer-experte-droht-d... 3.) a) Waiting for several months (around 6, based on own experience) to see a specialist: https://www.sr.de/sr/sr3/themen/panorama/saarland_warten_auf... b) Retirement, as percentage of former income, EU comparison: https://www.merkur.de/leben/geld/deutschland-rente-vergleich... Forgot the last point: 4. Examples https://www.bw24.de/baden-wuerttemberg/neid-debatte-vermiete... https://www.deutschlandfunk.de/fluechtlinge-dubiose-geschaef... And personal experience from friends of mine who are landlords. reply mk89 12 hours agorootparentprevOn a scale 1-5, 2 and 3 are exactly the same as or better than \"decent\". Otherwise the previous poster would have said \"they are perfect (=1)\". You're being a bit unfair, to be honest. reply Beijinger 5 hours agorootparentI did not say it is third world. reply mk89 3 hours agorootparentNo but you pointed out or you wanted to point out it's not decent, yet you gave a score of 2-3 on a scale 1-5 where 1 is top. That makes it exactly decent/good, which is what the person said. reply greenavocado 18 hours agorootparentprevINOVA in Northern Virginia charges $3000 for an uncomplicated vaginal birth. They have a code which rolls up many services into that code. There are additional in-hospital expenses such as the newborn hearing test which aggregate to about $400. If you have insurance your maximum cost out of pocket for the calendar year is between 5 and 15 thousand dollars (insurance covers everything above it). If you can't pay it all at once there are charities and payment plans you can discuss with the provider. If you don't have insurance you can possibly negotiate the bill down sometimes drastically or if necessary declare Chapter 7 or 13 bankruptcy if you really can't pay the bills. Bankruptcy sounds scarier than it really is. Chapter 7 bankruptcy lasts about 3 months and Chapter 13 bankruptcy lasts about 5 years during which you pay everything above your normal average household expenses to a special account for the creditors. Medical debt is unsecured debt and can be defaulted on and purged by the court following bankruptcy proceedings. Bankruptcy is much more problematic in Europe for the individual. Americans can keep their assets in a Chapter 13 bankruptcy. Health insurance in America costs between $500-$1000 a month for an entire family. reply mikestew 16 hours agorootparentI'm sure this was meant as a \"oh, it's not so bad in the U. S.\" type post. But I read it, and am reminded of what a suck-ass system the U. S. has. Sure, almost everything said is true. But let's reiterate: 1. One might pay up to $15K out-of-pocket for childbirth. 2. If you can't pay it, you have to go out-of-band to seek a non-standard procedure of taking care of the debt. The system is not otherwise built to handle a situation that's probably pretty common given the expenses involved. 3. If all else fails, file for bankruptcy. All just to have a child. What you've described is not \"not so bad\", but rather the plot of a bad movie from the 70s about how corporations have taken over the country and we all live in this dystopian hell where you have to give all of your assets to the corporation to be allowed to have a child. Only it's not a bad movie from the 70s, it's the U. S. healthcare system. And I don't know what kind of low-end plan one gets for $1000/month for a whole family, but I'd bet real money that the deductible would make my eyes water. reply greenavocado 15 hours agorootparentYes, the situation is bad. Anyway, comparing earning 175k USD to 162.37k EUR: Germany, married with 2 children, Stay At Home Mom Scenario, Tax Category 3, Berlin Region: Solidarity + Salary Tax is EUR 40.428,00 Pension Insurance: 8.314,20 € Unemployment Insurance: 1.162,20 € Care Insurance: 900,45 € Total: 50.805 € = $54,706 before health insurance Virginia, USA, same scenario as above, incl. 2 child tax credit of $4000, four state exemptions: FICA + Federal + State Tax - Child Care Credit: $44,162-$4,000 = $40,162 before health insurance Germany costs $14,544 a year more for a family with two kids before health insurance. To be fair though, the differences are lesser for most Germans, as almost no common jobs pay over 120k EUR gross. reply jjav 4 hours agorootparentprev> (insurance covers everything above it) Be aware that this is not true! What insurance-speak calls max out of pocket is not what a normal person thinks max out of pocket means. If you have a year of high medical bills you can easily pay much more out of your real pocket than your so-called \"max out of pocket\" in the insurance paperwork. That is because the insurance company only credits what they feel like it towards their accounting of max out of pocket, not what you actually paid. I don't have an insurance bill handy here to quote numbers but in their monthly or quarterly statements you can find what you have to pay and what they credit against your tally of yearly out of pocket. The latter is often less than the former. > Health insurance in America costs between $500-$1000 a month for an entire family. You're off by a lot. About $3500/mo here in CA for my family. Employers usually pay a good portion of that (but it varies) but if you go unemployed on COBRA, it's all out of pocket. reply Ringz 18 hours agorootparentprev> INOVA in Northern Virginia charges $3000 for an uncomplicated vaginal birth. They have a code which rolls up many services into that code. There are additional in-hospital expenses such as the newborn hearing test which aggregate to about $400. Free of charge in Germany > If you have insurance your maximum cost out of pocket for the calendar year is between 5 and 15 thousand dollars (insurance covers everything above it). In Germany, insurance covers everything over 0€. It depends on the insurance whether you have to pay extra for individual treatments (treatment methods without, or with disputed scientific evidence) or not. Rarely more than 100€. If expenses for illnesses exceed a certain amount in a year (about 3000€), it can be deducted from taxes. > Health insurance in America costs between $500-$1000 a month for an entire family. In Germany, it depends on your income how much the insurance costs for the entire family. 500-1000€ is also possible in Germany. But without all the extra expenses, negotiations with doctors, clinic, insurer, and bankruptcies... reply burnerthrow008 15 hours agorootparent> In Germany, insurance covers everything over 0€. [...] If expenses for illnesses exceed a certain amount in a year (about 3000€), it can be deducted from taxes But how can you have anything to deduct if insurance covers everything over $0? reply Ringz 15 hours agorootparentYou can go to a private doctor even if you are not in a „private insurance“ and pay him by yourself (Selbstzahler). He can prescribe meds which are not covered by the public insurance. So you have to pay them by yourself. That’s just one example. reply Beijinger 15 hours agorootparentprev\"In Germany, insurance covers everything over 0€. \" Yes. But you wait >6 months to see a specialist. And Germany has not the same level of health care than the US. Sorry to break the news to you. reply Ringz 14 hours agorootparentNot 6 months: https://news.ycombinator.com/item?id=39395329 Yes, Germany has not the same level of health care. Most of the time it’s better than the US. According to studies and data. All sources are not from Germany: https://www.healthsystemtracker.org/chart-collection/quality... https://www.commonwealthfund.org/publications/issue-briefs/2... https://www.nytimes.com/interactive/2017/09/18/upshot/best-h... „Germany would have tied Switzerland had we averaged our rankings of the nations instead of using head-to-head matchups in a bracket system (Switzerland eliminated Germany in the first round). It’s an example of how close the voting was.“ Wow! Sorry to break the news to you. reply Beijinger 14 hours agorootparent\"Not 6 months\" I don't care what the internet says. I know from my own experience it is 6 months, at least for some categories. Some sources say 30 days. A joke! I could call the office of my average XYZ doctor now and ask. And it will be 6 months, for sure. \"Most of the time it’s better than the US. \" The US has a lower live expectancy. Whatever the reasons may be. While this is concerning and not good, as long as you have a good insurance, Germany can not match the standard of US health care. Trust me on that one. With dentists it may be a different thing. Yes, if you are poor or unemployed, your access to health care is likely much better than in the USA. But otherwise, not so much. reply mk89 12 hours agorootparentNot sure where you live. I (and the people I know, some of them 65+) never had to wait for 6 months - even for surgeries. And I can speak about a decently wide range of categories. You also mention \"at least for some categories\", then you mention that \"you could call the average XYZ doctor\", as it that was applying to all categories. You seem to be generalising out of emotions, to be honest. reply Beijinger 5 hours agorootparentI don't have diabetes, for example. So I can't say anything about seeing such a specialist. reply mk89 3 hours agorootparentThen why do you say you could call the average doctor and get an appointment in 6 months? As I mentioned, I know about a range of categories and I never heard of 6 months. Your example seems very specific. reply staringback 16 hours agorootparentprev> Free of charge in Germany Paid solely by taxes instead of some taxes and some out of pocket reply albert180 16 hours agorootparentprevIf Access to Health Care in the US is so great, why is infant mortality worse than in Cuba and they score dead last in every Health or Life Expectancy Metric among the G7? reply jandrewrogers 15 hours agorootparentNone of these are primarily medical outcomes. Life expectancy, for example, is skewed by an anomalously high rate of fatal injuries when people are young, which has nothing to do with healthcare quality. For better or worse, trauma medicine in the US is arguably the best in the world because serious injuries are so prevalent. There is also the practical matter that the US is a continent-sized country and regional effects matter. Some US States have life expectancy on par with the best European countries despite the anomalously high fatal injury rates among young people. reply albert180 9 hours agorootparentUhm no, even the richest households are worse off than the poorest in UK https://www.ft.com/content/653bbb26-8a22-4db3-b43d-c34a0b774... reply jandrewrogers 2 hours agorootparentNothing in that article actually addresses the point. The average life expectancy where I live is currently 83+, despite notably higher fatal injury rates. In terms of actual medical outcomes -- survival rates for cancer, cardiovascular events, trauma medicine, etc -- the UK is quite a bit worse than the US. The only countries that stand out as consistently competing on medical outcomes are France and Switzerland. reply greenavocado 16 hours agorootparentprevIn case you haven't noticed, the demographics of America have changed a little bit since 1990. We also have a small problem with fentanyl. reply skeeter2020 16 hours agorootparentprevIt's a little more subtle than this, and of course subtlety is doesn't play well these days, but the US stats vary greatly across geography and ethnic lines. Cuba isn't a great comparison, beyond the shock of \"they're worse than a bunch of commies!\" reply staringback 16 hours agorootparentprevHave you seen the health of an average American? reply dkekenflxlf 18 hours agorootparentprev500-1000 USD per month for a WHOLE family is fucking cheap!? In Germany, i'm paying more than 1150€ per month as single (and im not privately insured but publicly) reply pflenker 17 hours agorootparent1) I doubt that you pay as much. I pay less than half of that and have a comfortable salary. Did you include the \"Arbeitgeberanteil\" (which is not part of what you are paying)? 2) You can't easily extrapolate that number to a whole family. Kids are usually insured for free if the higher-earning parent is not privately insured. reply shasts 16 hours agorootparentThis is quite possible, right? For eg: if you work for a Swiss or US employer and make more than 69k(?), say 100k, as a freelancer, and have a family, you have to pay around 14%(upper limit somewhere around 1000) of the income as health insurance contribution. I think it is costly. It is difficult to digest for me that the health insurance costs more than housing. reply greenavocado 16 hours agorootparentprevNo, this is on par with what my friends in Germany pay for their family. The difference is Germans have nearly no deductible and there is little worry about in-network vs out-of-network treatments. reply Scharkenberg 15 hours agorootparentprev1150? That's literally impossible. The current maximum is about 1050. See \"2024 Höchstbeitrag GKV inkl. Pflegeversicherung (ohne Kinder)\". reply rusticpenn 16 hours agorootparentprevI think the upper limit is 800€ reply wyre 16 hours agorootparentprevNo one should have to declare bankruptcy for birthing a child. To suggest it is obscene. reply lupire 18 hours agorootparentprevWhy do I pay $25K/yr for my family? Is yours high deductible? reply greenavocado 16 hours agorootparent6,000 dollar deductible per person with total 15,000 dollar out of pocket maximum for the family. My current health plan is better because my company found a partner that aggregates many smaller companies to get better rates from insurers. Before this I was uninsured and would call many different providers to negotiate the best cash price for various procedures such as wrist MRI with contrast. I got cash quotes ranging from $850 to $3500 and got exactly what I needed from a local provider for $850. I called up several providers in Canada and they offered similar cash prices (on the low end) so it wasn't worth the 8 hour drive each way. reply mschuster91 16 hours agorootparentprev> INOVA in Northern Virginia charges $3000 for an uncomplicated vaginal birth. In Germany all of that is being covered by the government insurance scheme. The only thing you have to pay is stuff like a private room for postnatal recovery time [1]. Oh and you get a very long time paid post birth to recover and bond with your child. No such nonsense as giving birth and having to work the next day like it's common in the US. > If you have insurance your maximum cost out of pocket for the calendar year is between 5 and 15 thousand dollars (insurance covers everything above it). We don't have that at all, insurance covers everything, you never even see a bill or have to deal with stuff like \"in network\". The only thing is a 5-10€ co-pay per prescription (utter nonsense if you ask me, it was introduced as a \"cost control\" measure to prevent people from... diverting medication? idk, it's ridiculous but small enough that it's harmless). > Bankruptcy is much more problematic in Europe for the individual. Americans can keep their assets in a Chapter 13 bankruptcy. Agreed on that one. It's also a huge contributing factor into why Americans have it easier to start up side hustles. > Health insurance in America costs between $500-$1000 a month for an entire family. Assuming I were the sole earner and my wife would be a SAHM with an arbitrary number of children, I'd (at a gross wage of 52k/y) pay 340€ a month to health insurance and my employer another 340€, so in total (including a per-insurance surcharge of, in my case, .5%) my healthcare costs are capped at about 700€ - for all of us. On top of that we'd get, like all parents in Germany, \"Kindergeld\" of 250€/month per childpaid by the government to assist in child rearing. Pretty awesome if you ask me. I honestly don't know how y'all survive. [1] https://www.eltern.de/kosten-geburt reply burnerthrow008 15 hours agorootparent> In Germany all of that is being covered by the government insurance scheme. Ok, but you agree, don't you, that $3000 is quite different from \"parents have to pay five digits worth of money just for the birth of a child\"? That is off by an order of magnitude. > No such nonsense as giving birth and having to work the next day like it's common in the US. I think you should stop consuming so much media from Russia Today :) Or maybe re-consider the trustworthiness of wherever you read that. The fact is that, at the federal level, Americans may take 12 weeks (unpaid) time off work after giving birth under the FMLA, and it is prohibited to retaliate against an employee for taking that time. Additionally, the majority of Americans live in states which mandate paid time off after giving birth, (usually 8 to 12 weeks) plus additional unpaid time. California, to use the most populous state as an example, mandates 8 weeks of paid time, plus 28 weeks unpaid. New York, on the opposite side of the country, goes even farther by mandating 12 weeks paid time. Part of the reason you may be confused is that this is not always explicitly named as \"maternity time\". It is often called as \"disability leave\", and nearly all disability statutes include recovery from pregnancy as a disability. Unless you are maybe trying to argue that Europeans are legally prohibited to work the day after giving birth, you are way off base here. reply skeeter2020 16 hours agorootparentprevMy initial reaction is - then why is the US leading in so many areas, if based on your chracterization it's such a laggard? I'd argue it has the best in all categories, it's just not close to even distributed. \"Decent\" is completely inadequate for education, healthcare and transportation. These are far too important for \" good enough, usually\" solutions. reply jandrewrogers 16 hours agorootparentprevThe US actually has a good pension system relative to other developed countries. Retirement plans like 401k, IRA, etc are in addition to the pension system, they don't replace it, and are frequently superior to their European counterparts both in scope and flexibility. There is plenty to complain about but pensions and retirement is one area that the US does comparatively well. reply Nimitz14 16 hours agorootparentprevSwitzerland does everything better than Germany with far less taxes. And since we're trading anecdotes I've found US Healthcare to be the best compared to the 3 european countries I've lived in before. reply skeeter2020 16 hours agorootparentyour second point strikes at the crux: On the whole, the US sucks. At the top margin, it's the best in the world. reply mschuster91 16 hours agorootparentprev> Switzerland does everything better than Germany with far less taxes. They have the advantage that the country is so small. Us Germans however have to deal with the fact that we have the 5th largest country of Europe of which a lot of is settled and has to be supported by the entire population. And in any case, Swiss taxes aren't that much lower than Germany's either - average tax load in the country is ~29% [1], Germany is at 33.9% [2]. Switzerland's total load may be even higher because the German figure includes social security payments, Statista for Switzerland is only taxes. [1] https://de.statista.com/statistik/daten/studie/1176146/umfra... [2] https://www.bpb.de/kurz-knapp/zahlen-und-fakten/soziale-situ... reply alexwhb 18 hours agorootparentprevAgreed. Additionally taxation in our modern Fiat currencies is a farce, since the central banks can literally take as much buying power as they want from you. Taxation is literally just an illusion to make you feel like you are directly paying for government services.The real taxation is inflation. reply RHSeeger 16 hours agorootparentprev> I find it quite disturbing that high taxation is seen as a negative aspect of Europe. High tax rates and, in fact, ANY taxes area a negative. However, that negative is (hopefully) offset by the positives that the things those taxes go to pay for provide. reply throw10920 5 hours agorootparentprev> I find it quite disturbing Typical emotional manipulation... > that high taxation is seen as a negative aspect of Europe. I lived in Italy, France and Germany, and I enjoyed public healthcare and education of very high standard at very affordable prices, and free in the limit that one cannot afford to pay for them. ...coupled with an utterly illogical (and factually incorrect[1]) non-argument. You can have these things without high taxation, if the system that implements them is efficient. It's not. [1] https://news.ycombinator.com/item?id=39402957 reply Beijinger 16 hours agorootparentprev\" I'm proud of contributing through my taxes so that anybody in need can have the same treatment irrespective of their economic situation.\" It is a big world my friend. And if millions come to use these services but don't pay in, the services get thinner and thinner. 1 Million Ukrainians came to German. Less than 25% have a job. The rest is financed by the taxpayer. Money, Rent, Health insurance... Edit: It reminds me of a Third Reich joke. An old woman goes into a map shop and looks at a globe. She asks the sales person: What is this big blue land on the globe? The sales person says: This is the USA. Old lady: And this huge Red area, what is this? The sales person says: This is the Soviet Union. Old lady: And this tiny brown spot, what is this? The sales person says: This is our Third Reich. Old lady: Does the Fuehrer know this? ----------- I hope you get what I am trying to say. reply mousetree 16 hours agorootparentAs a German who pays a lot in taxes each year, I’m more than happy for those taxes to support our Ukrainian friends who are refugees during this terrible situation reply illiac786 13 hours agorootparentprevThe one I have seen are all young women and kids. They will cause not even a fraction of the health care costs a 70 or 80 year old German generates. I can't find this stat but basically it's exponential, one generates more health care costs in the last ten years of its life than during its entire previous life, on average. reply vizzier 12 hours agorootparentAdding a source to this claim as it is very valid [0]. I cannot find a good overview of the overall demographics of the ukranian refugees other than it is mostly women and children due to the borders being closed to outgoing men. Employment rates will rise as people integrate. [0] https://www.oecd.org/health/Expenditure-by-disease-age-and-g... reply Beijinger 12 hours agorootparentIn Germany, 75% of Ukrainians don't work. In other EU countries, nearly 80% work. It is a question of the system and incentives. reply illiac786 2 hours agorootparentSource? This is interesting. reply mk89 12 hours agorootparentprev> They will cause not even a fraction of the health care costs a 70 or 80 year old German generates. I can't find this stat but basically it's exponential, one generates more health care costs in the last ten years of its life than during its entire previous life, on average. I really don't wanna get into this populist debate, which is really annoying lately as it seems to be our biggest issue in the world, but ... the 70-80 years old people (probably) worked in the country, they contributed to their own country, and should be treated with some respect or decency and not just as a cost. reply iforgotpassword 11 hours agorootparentThey enjoyed the golden years after WW2 where the economy was booming, decent pension starting at reasonable age, nobody cared about the environment, people lived like there's no tomorrow. Now the coming generations can deal with the fallout. reply mk89 3 hours agorootparentIt's a bit unfair what you said and probably you know it - next generations can blame so many things on us today... it's just like history or life works. reply iforgotpassword 11 minutes agorootparentYes I do, and I'm aware the vast majority of them were blissfully unaware, but then again I've talked to enough of them to also hsve learned that many of them simply don't want any of that to be true today. Some form of denial I guess. Like simply claiming the greenhouse effect isn't real, or protesting new wind turbine or high voltage power lines, because they would see them from their big houses with big gardens they were able to easily afford a few decades ago. Or protesting the new city district on a farmer's land because they like to ride their electric bikes along the fields - never mind even families with double income struggle to afford decent apartments nowadays, let alone a dedicated house. Seriously, we have frequent protests against a new city district here, and you exclusively see college students who know jack shit about life and old folks around there. lastdong 10 hours agorootparentprevIt's important to remember that Ukrainian individuals in Germany make up just 1.2% of the population, the majority of whom are war refugees. And while 0.9% can be treated just as a number, each one of these individuals has a unique story and deserves our empathy and understanding. [0] https://www.osw.waw.pl/en/publikacje/analyses/2023-08-25/ukr...). reply tietjens 21 hours agoparentprevHello from Germany. No lies detected. Don’t forget about a working culture where process always ranks above results, and leaders commonly lack confidence to make pragmatic decisions. reply odiroot 19 hours agorootparent> Don’t forget about a working culture where process always ranks above results, And where tenure ranks above skills or experience. Especially if unions are involved. reply anovikov 19 hours agorootparentprevWell, this working culture makes result slow and expensive, and as a result, in many cases uncompetitive, but it does a good job of avoiding fuckups and while slow and expensive, things usually get done predictably, on time and on budget. Which probably makes it better than American reckless \"fake it till you make it\" approach in most of the complex projects. reply tietjens 19 hours agorootparentI am familiar with this response but I can’t take it seriously. The country is looking at something like -0.4% growth this year. “On time and on budget” really? Like the Berlin airport? Like the digitalization budget that hasn’t been spent? There is no spirit of pragmatism here. Only a fear of not following the rules. reply chrisandchris 1 hour agorootparentprevI think scope, budget and time is a triangle. You have to choose 2 of it, and 1 is always lost. There's almost never a on-budget, on-time and on-scope project. reply freshbob 19 hours agorootparentprevNo, it absolutely does not. It might have in the past, when industry was booming in Germany and setup costs were huge and mistakes were very, very expensive. However, they've pretty much lost the race when it comes to software and digitalization, where setup cost is pretty much non-existent, mistakes are inexpensive (fail early, fail fast), and ROI are generally multiples of 100%. reply anovikov 17 hours agorootparentI had an experience with a German client in a software project. It freaked the hell out of me with slowness of decisionmaking, long contract every point of which as it turned out, was material, in a nutshell it was a 50/50 coding vs lawyer job and took 5x the time same thing would take with an American client. It also made me next to no profit. But, unlike a lot of things we write for Americans, that one turned out to be actually workable, usable in their (rather large) product, and runs happily to this day almost 5 years later. I count it as a success and attribute it mainly to this boring, overly conservative and procedure-ridden work process. But yes, i agree, that for software projects, virtues of this approach are more limited than in \"hard\" engineering, and probably do not outweigh downsides in many cases. reply fabian2k 20 hours agoparentprevThat's an overly negative and hyperbolic view. Nothing is entirely wrong here, but you're painting every single point in the most negative way possible. You can certainly argue about how good the healthcare system is in the end, but it isn't categorically inaccessible. And if you pay 400 EUR/month you're earning enough money and can choose the private health system if you prefer that. The credit ranking also works in very different ways than in the US, so I wouldn't compare them directly. I'm not sure what you mean by anti-consumer contract rules. reply panki27 20 hours agorootparentWhile choosing to be in the private system is an option, it only \"makes sense\" if you make at least 70k€ a year, so definitly not for everyone. The issue is not getting in - but getting back out before you retire and don't have enough available income to pay the rates anymore. reply fabian2k 20 hours agorootparentIf you pay ~400 EUR for the public healthcare you make 70k or more. The cost for public healthcare is scaled by income, and is limited around 400 EUR. It's a bit higher now, but I would suspect that the OP is not necessarily talking about the most current values here and is hitting the older limit and therefore earning more than the \"Beitragsbemessungsgrenze\". reply miroljub 18 hours agorootparentThe limit is ~900 € per month, not ~400 €. People frequently forget the employer matching, which at the end comes from your pocket, since it's mandatory. Also Freelancers and self-employed need to pay full contribution of 900 €, not \"just\" a half. reply cycomanic 12 hours agorootparentNo the employers contribution doesn't come out of your pocket. The employers contribution comes out of the employers pocket and is the cost of doing business. That's like arguing the rent for the office space or even the warehouse or servers come out of your salary. Your salary is essentially a function of supply and demand and not a function of cost. I mean, if external costs would influence salary we'd see remote workers earn more than on site ones, because employers save on office rent. In reality it's more likely to be that remote workers have a lower salary. reply lupusreal 16 hours agorootparentprevIf you're paying 400 EUR for public healthcare, shouldn't you receive good healthcare for that? Why is private healthcare even in the discussion? Why does the public option suck? reply mns 19 hours agorootparentprevEven if you get in, it's scummy. Once you get to a certain age, the payments are crazy. Not to mention people \"selling it\", like a colleague of mine got scammed into going private, he had some eye issues a couple of years before, and when the issue came back, the insurance refused to pay because it was an existing condition, making his treatment unaffordable and having to literally quit, leave the country for a while and come back just to get back to public. reply sofixa 19 hours agoparentprev> Europe (low digitalization, high taxation, recursive federalism: within the country and within the EU). A decent chunk of those are exclusive to Germany, or at least far from the norm in the EU or Europe in general. For instance digitalisation varies wildly between countries, but Germany is definitely one of the most embarrassingly behind countries. Taxation also varies (e.g. Bulgaria and Estonia have flat income taxes). Federalism isn't a thing in most European countries too. reply jonp888 20 hours agoparentprevI'm a bit surprised you would consider the credit system the same as the US. It doesn't have the bonkers bit of the US system where you have to take great care to be in a small amount of debt even if you don't need to be, in order to \"prove\" your creditworthiness so that you can be in a lot of debt later. reply bombcar 20 hours agorootparentBeing in debt to build credit is mostly a fairy tale told by the companies - just having an open line of credit is all you need to provide account age. You don’t need to use it or carry a balance (you may need to use it periodically to keep it open but that can be charge/pay off immediately). reply kevin_thibedeau 20 hours agorootparentprevIt's starting to creep into public services. You couldn't even get easy access to Covid stimulus funds without a credit card. Debit cards were rejected despite providing the same level of ID verification. reply devmor 18 hours agorootparentAt what point did you need to use a credit card to receive your economic stimulus payment? I simply confirmed my most recent tax year information when I claimed mine. reply usr1106 18 hours agoparentprevGerman \"public\" healthcare is much more accessible and has much better coverage than its Finnish counterpart. Despite Finland reapeatedly making headlines (including on HN) about being a Nordic welfare state. Germans are typically complaining from a high level. Source: Members of the wider family living in both countries. Some over 80 and needing a lot of healthcare services. (I hear general practioners can be a problem in poorer areas in the East. My experiences are from a prosperous area in the West.) reply graemep 16 hours agorootparent> Germans are typically complaining from a high level. Do Germans share the British tendency to think everything is better in the rest of the world? As an immigrant (as a child, admittedly) and someone who has lived and worked elsewhere and, more importantly, does not completely share this aspect of the culture, I find British pessimism and self-flagellation really, really annoying. reply carstenhag 11 hours agorootparentGermans unfortunately like to complain a lot. Many things are better elsewhere, is claimed. Except for many other countries nearby. And the weirdos from UK leaving EU. And the weirdos from US with Trump. And we have it so bad, so bad. Sometimes I hate my fellow Germans. reply oarfish 18 hours agorootparentprevA finnish colleague once told me that the Finns just don't go to the doctor unless something falls off. Any accuracy to that? reply usr1106 17 hours agorootparentYes, especially for Finnish men in the countryside that's a sterotype with quite some truth behind it. Edit: And that results in a dangerous combination: First waiting too long and then getting the answser \"no free appointments\". And if you insist the next one is in 3 or 6 months. Many Finns do not insist. reply robert_foss 21 hours agoparentprevThis is entirely correct. Having lived both in North America and Sweden this is a very accurate description. reply meroes 16 hours agoparentprev400€ for medical, dental, and optical combined you mean. And less for students. And you conveniently leave out their 4 week minimum vacation time/year. (my sis and her BIL just moved from there, and help people move there). Digitalization is really behind yes, and homeownership is even more expensive than the US. But rent is almost half the US and groceries are cheaper. And instead of subsidizing full sized pickup trucks and EVs, you get Audis, VWs, etc for cheaper. reply discopicante 15 hours agorootparentNow compare median income of Germany vs US. Economic opportunity is fading away in Germany. Also, pretty much all cars - especially Audis, VWs, etc are (20-30%) more expensive in Germany vs US. reply interactivecode 19 hours agoparentprevHealthcare is kinda shit in The Netherlands. Long waiting lists even for simple things. Never actually prescribing medication but instead over prescribing paracetamol. Only being able to get a set amount for appointments regardless of actual need. Sure if you’re in a car crash and urgently need care they will help you without going bankrupt but anything less urgent good luck getting an appointment this week / month / year reply notaustinpowers 19 hours agorootparentDoes The Netherlands not have Urgent Care facilities like the US? I feel like those for of facilities could handle the issues of a lack of access for simple or menial things. I had to get a cyst lanced a few weeks ago and I just walked into an Urgent Care by my house and was in and out in about an hour with no appointment. I think it was about $180 since I didn't have insurance. reply karpour 19 hours agorootparentprevThis, not nearly as bad in the US, but a mess of private insurances. reply njarboe 18 hours agorootparentA mess of private insurances, partial payment by the government of private insurance based on income, and then full (or almost full) government payment based on veteran status, income, and age. It's more complicated than this of course. reply anonzzzies 18 hours agorootparentprevI am dutch and have lived in quite a few countries in the eu and se asia: I would prefer to never have to deal with NL healthcare services again compared to other countries. And I have been (unfortunately) been in hospitals, a lot. Since the 90s NL is striving to become little USA policy wise and this is still getting worse. It’s a shit show. reply moi2388 17 hours agoparentprevTrue, but in the Netherlands it’s the government itself which sells all of your data, literally everything from your house, it’s price, location, blueprints, satellite images, what you paid for it, previous owners, your car, and even your government ID number if you are registered as a company, just to many a couple. And yes, you obviously have to pay to get your own data. reply oytis 20 hours agoparentprevIs Netherlands healthcare better? From what I've heard from people having lived in both countries, German healthcare is more accessible. E.g. in Netherlands you can't see a specialist doctor unless you convinced your GP you absolutely have to - which can be hard at times. reply jacquesm 19 hours agorootparentI'm 58 and not exactly in the best of health. I've had to rely on healthcare in NL quite a few times over the last 20 years or so and all of those were serious cases. I have nothing to complain about, but that's n=1. To increment that n a bit: I know lots of people here and almost all of them have had some health issues over the years and the vast majority of those have been dealt with in a serious and reasonably effective manner. Where NL is utterly ineffective is when it comes to vague symptoms. Until it is perfectly clear what is wrong you're going to be seriously frustrated because the diagnostic machinery isn't really all that effective, when in other countries they'd spend a fortune to find out what's wrong with you in NL unless it's 100% clear you're going to have a hard time getting the care that you need. I blame Calvijn, NL seems to have a misplaced sense of reduced expression of emotion resulting in an expectation to tough things out rather than to deal with them and for some reason doctors seem to see complaints without a direct relatable cause as an attack of hysterics rather than as something to investigate. If you come from a different background you're going to find this a very difficult thing to deal with. reply formerly_proven 19 hours agorootparent> the diagnostic machinery isn't really all that effective, when in other countries they'd spend a fortune to find out what's wrong with you in NL unless it's 100% clear you're going to have a hard time getting the care that you need. What other countries are those? reply maeil 9 hours agorootparentIronically the US seems to be best, or at least the \"least bad\" at this (if you can afford it); it's the only positive thing I can see about US healthcare. reply Unfrozen0688 18 hours agorootparentprev>when in other countries they'd spend a fortune to find out what's wrong with you in NL unless it's 100% clear you're going to have a hard time getting the care that you need. Sweden here, I hear the same things. I have had good experiences with health care here as my problems have been \"normal\" and straight forward. But what other countries? Really, name one. Same with computers. You want to fix a straight forward problem or a vague one? reply miken123 19 hours agoparentprev> A country like the Netherlands has its own issues (mainly housing) but doesn't have the myriad of pain points you can find in Germany like Schufa In the Netherlands we have BKR, which is less all-encompassing than SCHUFA, but also needed to be fined before giving proper right to access under the GDPR: https://edpb.europa.eu/news/national-news/2020/national-cred... reply ffsm8 20 hours agoparentprev> public healthcare inaccessible despite paying more than 400€/month for it as a single individual. That's a pretty new thing though (been getting worse for the last 10 yrs). And the person responsible for the legislature that caused this change is the current health minister. Also... 400€? It's a percentage of your salary. And you're omitting that the employer pays the same amount, so you're effectively paying 800€ at the very least. reply asymmetric 20 hours agorootparent> Also... 400€? It's a percentage of your salary. And you're omitting that the employer pays the same amount, so you're effectively paying 800€ at the very least. Not everybody is employed. reply Aerroon 19 hours agorootparentIf you're not employed then aren't you expected to pay both halves yourself? reply fabian2k 18 hours agorootparentIt is paid by the social security system in that case. If you're self-employed then you will have to pay both parts yourself. reply Aerroon 11 hours agorootparentAh, that's nicer than here. In my EU country you just don't get healthcare if it's not paid. Funnily enough you can't just go and pay it either, it has to come through something like a job. reply generic92034 20 hours agorootparentprev> And the person responsible for the legislature that caused this change is the current health minister. Could you please explain that a bit more? reply ffsm8 19 hours agorootparentHe revamped pretty much evey monetary incentive a few years ago, which directly caused the \"gold rush\" privatization of hospitals etc, making a lot of treatments uneconomical. There has been a lot of coverage about it over the years - both by official channels and independent YouTubers and journalists, I'm not going to do it justice by summarizing it in a comment here. A somewhat recent German video (2yrs old) is \"Pflege katastrophe - exposed\". But ARTE , DW and ZDF all have reported on it at this point The controversy about diabetis patients getting amputations over simple treatments is a pretty well known symptom of this revamp, as operations get a very high payout reply carlosjobim 20 hours agorootparentprev> you're effectively paying 800€ at the very least. That makes it worse! Not better. reply jonp888 20 hours agorootparentIt's a collective payment system whereby everyone receives the same treatment, but high earners subsidize low earners, students and pensioners. Whether that is a good idea or not is of course a matter of debate, but it doesn't make sense to compare the benefits with the cost because they are not linked. If you are a really high earner you can opt out and pay a private risk-based premium instead. reply carlosjobim 20 hours agorootparentThe original commenter was complaining that he was paying €400 per month and not getting adequate service for what he is paying. Somebody else mentions that he's actually paying €800 per month (and not getting adequate service). That makes it even worse, and I'm surprised if somebody can not understand that. reply bombcar 20 hours agorootparentBut as a higher earner he’s paying 800€ so that someone poorer can pay 100€ and not get adequate service. ;) As an American I choose to ignore this and pretend that everywhere else has perfect healthcare and government. reply chmod775 19 hours agorootparentprevFrom that amount I can tell they're at or near the maximum amount you can pay (a software dev salary will usually get you there). It currently tops out at 421,76 Euro and doesn't increase any further from there. Many people at that income bracket opt for private insurance, because it will be cheaper and provide you better care. reply blackmoon42 19 hours agorootparentCheaper, when you are young and single. Get a family and pay for every member. Get older and see your rates increasing. Also the rates won't drop if you're a pensioner. You need to know your future, as it's more or less a decision for life. reply mk89 12 hours agoparentprev> private healthcare if you want decent treatments > public healthcare inaccessible What do you mean? I am not sure what you mean by decent treatments. > Schufa There is some change happening there, especially now that the EU Justice Court ruled against credit scoring institutions (they break GDPR, etc.). Very slowly, as usual, but it might happen. (Also the current governement is planning changes.) reply albert180 20 hours agoparentprevThat's utter bullshit. Your quality of care in the hospital is exactly the same as with public health insurance. Also sometimes private insurance is even worse when it comes to addiction and psychological treatments reply hocuspocus 20 hours agorootparentYou're treated the same in big hospitals but when I was in Berlin, private vs public meant I could get an appointment at a dermatologist tomorrow instead of in two months. reply tietjens 20 hours agorootparentprevThis is not true. Private insurance means better care in hospitals in Germany. Better access to appointments, choice of head/experienced doctors, even can mean better rooms. reply albert180 16 hours agorootparentYou know, I'm a med student, so I have a bit more experience :) The Head/Experienced Doctor is still letting the Learning Doctor doing everything and pops in only for the visit. Also the \"standard\" Patients get also checked by a Oberarzt regularly. The difference is miniscule. The Rooms are sometimes nicer I agree, but the care you get is still 100% the same. The Hospital also doesn't get extra money for the treatment besides the Fee for the \"Chefarzt\" and the nicer room. Otherwise they'll only get the same DRG payout as for a \"public patient\" reply Trollmann 17 hours agorootparentprevNo, it means you may potentially have all of those but there is no guarantee. Neither is there a guarantee that a publicly insured person wouldn't receive the same treatment. E.g. if there aren't any free \"better/worse\" rooms what are they supposed to do? Many of these are nowadays covered by employers as a benefit or for cheap (~5€) out of pocket if you want. Private insurance matters most for specialists that don't (aren't allowed to) have (or want) a public insurance license. reply ArmandGrillet 20 hours agorootparentprevJust open Doctolib and tell me it's the same when looking for any kind of doctor. The difference in waiting time is 3 months, bare minimum. reply Ringz 17 hours agorootparentI just opened Doctolib and did a test with a dermatologist. The difference between private insurance was 23.5.2024 and public insurance 5.6.2024. That makes 13 days and not \"3 months bare minimum\". https://0x0.st/HnBZ.png https://0x0.st/HnBN.png reply albert180 16 hours agorootparentAlso if your primary doctor determines that you need an urgent appointment, they can give you a priority code, which gives you a quick appointment, since those visits are paid on top of the regular budget for the specialist reply WarOnPrivacy 19 hours agoparentprev> or public healthcare inaccessible despite paying more than 400€/month for it as a single individual. This is super close to my early 2010's ACA experiences. Even when a poor earner could scrape up enough to buy a plan, the deductible made it unusable. Policy pricing was exorbitant for $12k/yr earners but dropped enough for 22k/yr that a few plans were buyable. The challenge was coming up with another $somethingthousand to cover the deductible. There was a sharp drop-off in plan pricing at 32k/yr and some mid-grade plans were in reach. IIRC deductibles were lower on those plans and they may have been usable (or nearly so). What struck then. Of the news orgs cheering/damning the ACA, zero of them ever covered how pricing dropped as income rose. I assume that's because pricing was only ever disclosed to folks who completed the lengthy signup process - and news folks found it too daunting to experiment with. reply fabian2k 19 hours agorootparentThere are no real deductibles in German public healthcare, though some areas are excluded almost entirely like glasses and certain dental work. The cost is also scaled to income, the 400 EUR (which is only 50% of the cost, the other 50% pays the employer) here are what you pay when you earn ~70k EUR per year and are essentially the maximum for public health care. So it is much cheaper if you earn less money. One of the main current criticisms of the system is that it can be very difficult to get appointments with specialists compared to people with private health insurance. reply dkekenflxlf 18 hours agorootparentBro, du solltest mal deine Gehaltsabrechnung prüfen - mit dem Gehalt bist du bei knapp 1000€ reply fabian2k 16 hours agorootparentI used the values from the original post, which is only the employee part and not the employer part. I also assumed that they're not using current but slightly older values here. I also didn't include the PPV, if you include that and count the employer part you end up at 1000 EUR. reply Unfrozen0688 18 hours agoparentprevlow digitalization, high taxation Sweden is highly digitalized, we pay on our phones, pay taxes on our phones, do government stuff on phone or webbrowser. No stores do cash anymore (also a negative) High taxes is a positive if used right reply miroljub 18 hours agorootparentSweden is a low tax land compared to Germany. In the past, it was known for high taxes, but one could argue that this progress came together with the tax reduction in the last decades. reply javajosh 18 hours agoparentprev>low digitalization Given that everyone uses IBAN, and it works great, I strongly disagree. >private healthcare if you want decent treatments Your mileage may vary, but I was very impressed with the speed and ease of acquiring healthcare under a public plan. The lack of paperwork for seeing a new doctor is astonishing. The lack of copays, SOBs, and all that is like a breath of fresh air, and is worth copying in the US. Heck I once saw a doctor for something and simply gave my card, and saw the doctor in the next hour, and she decided I needed an ultrasound...and did the ultrasound in 5 minutes herself. In the US, this would have been a multi-week ordeal with multiple rounds of paperwork and visits to different offices. My wife also had a C-section at a Berlin hospital and the care was competent and 100% covered by our public insurance. In the US couples requiring a C-section can expect $20k+ of debt. reply jamesmunns 18 hours agorootparentLow digitalization isn't just payment - it's also needing to fill in paper forms, or making in-person visits to government and business offices. I've been living in Germany for about a decade, and it is still behind in many of these items from when I left the US, and having a friend that recently moved from Germany to NL, he was blown away at how convenient and just not-hostile so many day to day interactions with governments and businesses were. Like - big picture, I'm very happy in Germany, but certain things are still very archaic and sometimes needlessly so. reply shellfishgene 18 hours agorootparentI agree, just look at the recent (well, decade-long?) attempt to digitalize the car registration process, which can only be called a total desaster. reply Bayart 14 hours agorootparentprevMy impression coming from France, which I never thought of as particularly progressive on the technological front, is that Germany is full of odd, idiosyncratic archaisms. reply odiroot 19 hours agoprevIn the process of renting a flat in Germany, SCHUFA is not even the biggest travesty. The worst part is potential tenants handing over full dossiers about their background to the prospective landlords. Passport copies, payslips, account statements and more. And as a tenant, you have no way of knowing when does it all end up. At best it will be dumped into a paper recycling bin. But who knows if all these documents aren't sold further. You also have no choice, you comply or you lose any chance of getting a flat. reply beezlebroxxxxxx 19 hours agoparentIt is incredible the amount of private information that gets exchanged in Germany to access certain things. You brought up landlords, but I've heard crazy stories about job applications or applications to services that \"require\" an incredible amount of private information (often even including pictures too!) handed-over for completely obscure reasons with no recourse if your application is denied or simply ignored. reply Ringz 17 hours agorootparent> ob applications or applications to services that \"require\" an incredible amount of private information (often even including pictures too!) Those crazy stories are indeed crazy and simply not true. reply shellfishgene 18 hours agorootparentprevAre US \"background checks\" any better? reply ploxiln 16 hours agorootparentFor renting apartments, it's just like this in NYC. But nothing like this in most of the country. I rented in Pittsburgh for a few years in college, and a suburb north of LA for a couple years after that, and the application documentation was much much less, pretty much just security deposit. The issue in places like NYC (and some German cities like Berlin I gather) is that the extreme tenant protection laws mean that landlords have to go to extreme lengths to protect themselves from tenants. In NYC you could have a tenant not paying, damaging the property, and pushing out neighboring tenants, for well over a year before you can force them out, if they're ornery enough. It could kill your business, and it probably did kill the business of most landlords who didn't switch to extreme draconian documentation requirements in applications: multiple tax returns, multiple statements from all bank accounts, proof of employment, records of previous residences, contact info for previous landlords ... reply Sebguer 17 hours agorootparentprevA US background check for most jobs is generally just a literal check on whether you've been charged with any crimes, and doesn't go much more detailed than that. reply glfharris 16 hours agoparentprevIt's very similar in the UK. Most estate agents will use a third party these days, but the amount of information you have to hand over is ridiculous. reply dhoe 21 hours agoprevI almost didn't get a mortgage in the Netherlands because if you're German, they request a Schufa report, and there was a minor issue on that. Had I come from e.g. Bangladesh instead of from Germany, I would not have had this problem. We found a workaround and ten years later the house is paid off, just for illustration of how predictive that report was. I'll cheer for whoever takes on and fights Schufa, always. reply bombcar 19 hours agoparentThe underlying issue is that bankers have no discretion (for various good and bad reasons). Everything is bureaucracy and checking the right boxes to let the computer decree your worthiness. No overrides, no explanations permitted - unless you can attack the problem from another angle. reply f6v 22 hours agoprevI don’t know how it’s in other countries, but Schufa is a monopolist and feels like simply paying a tax. You have no choice. Interestingly, I didn’t need a credit check to rent in Belgium and Denmark. reply mschild 22 hours agoparentThere are 3 other companies in Germany that offer the same service. Schufa is the most well-known one but far from a monopoly. That said, I agree with you. It needs to go. reply bun_terminator 22 hours agorootparentI have never heard of that. Schufa is THE thing you need for everything. Hard to believe they're not a monopoly in practice? reply rob74 20 hours agorootparentMaybe it's the same as with TÜV? There are other companies that offer the mandatory recurring technical inspection of your car (\"Hauptuntersuchung\"), e.g. https://en.wikipedia.org/wiki/Dekra, but \"TÜV\" has become synonymous with the process itself... reply pjmlp 22 hours agoparentprevI only know something like Schufa here in Germany, never needed something like that in Portugal, UK, France, Switzerland. reply hocuspocus 20 hours agorootparentIn Switzerland for a proper lease, you likely need to provide a (clean) record from the debt collection register. But it's a government run agency, as it should. No score. Debt collectors can file claims a bit too easily and there are errors, but you can also challenge them easily. reply pjmlp 17 hours agorootparentBack when I was actually living there, 2003 - 2004, it was only the painful interview process as if applying for a job, but I didn't had to provide anything else. reply hocuspocus 16 hours agorootparentDebt collection records were definitely a thing 20 years ago. But it sounds like you were dealing with an individual landlord, that's kinda rare. They might be more willing to use common sense and not ask useless documents from someone who just arrived in the country. On the other hand, big property management companies will typically not bother with an application that doesn't tick all the boxes. reply pjmlp 15 hours agorootparentBeing at CERN helped, it wasn't individuals, but it took about three months, and several attempts. reply hocuspocus 11 hours agorootparentCERN gives you diplomatic status, by law (at least in Geneva) lessors have to be more accommodating indeed. On the other hand, most landlords and property management companies absolutely hate applications from people who they know are going to leave after a fixed term contract. reply bombcar 19 hours agorootparentprevMost countries have something allowing liens against property; liens against persons are harder to keep track of (hence things like credit burros spring up). reply odiroot 19 hours agorootparentprevIn the UK, it's usually the landlord (or their agent) doing the background checks on you. And they pay for it. reply tumetab1 17 hours agorootparentprevIn Portugal most of times you need to provided an income tax statement which I think servers the same purpose. reply pjmlp 16 hours agorootparentIs definitely not the same, schuffa has a complete history of anything financial related to your life, including a credibility score that we don't have any clue how they come up with it. reply xen0 15 hours agorootparentprevIn the UK at least, credit reports largely happen behind the scenes, and potential landlords etc cannot perform a full credit search. With your permission they can perform a 'soft' search (have you declared bankruptcy or do you have any County Court Judgements) and they may not charge you for doing this. Further, as of GDPR, you are entitled to all information from the agencies for free. Prior, you could obtain it for a small fee (£2 by post if my memory serves me correctly). As far as I'm concerned, SCHUFA is basically a scam I am compelled to fall for whenever I want to rent somewhere. reply Aachen 22 hours agoparentprevSame in the Netherlands, and I can personally corroborate that you don't need it in Belgium. Finland also didn't need it but for a temporary student apartment that might be different. Currently in Germany, yeah... we paid the Schufa tax twice over because we're two people renting together, and the GDPR data export thingy conveniently omits the credit score they've computed about you... Was considering whether one can make use of the 14-day return thing, but they've thought of that: it uses a sealed envelope that's hard to open without leaving evidence (under a guise of having a nice opener mechanism). reply hulitu 21 hours agoparentprev> I don’t know how it’s in other countries, but Schufa is a monopolist and feels like simply paying a tax It looks more like the Mafia. You pay a protection tax but this does not guaranty anything. reply jonp888 20 hours agoparentprevYou don't need to pay. You can get a report for free showing all the information they have about you. It just doesn't show the score. I have never known anyone to demand the paid version. reply shellfishgene 18 hours agorootparentThis is exactly what the lawsuit is about, they hide the free version and it does not contain all information. reply harha 21 hours agoparentprevmight be worth fixing the problem all together by making it easier to kick out people who don't pay rent. reply tetris11 21 hours agorootparentIn a better world I could maybe believe that giving landlords more power could make the system more efficient, but... well... reply harha 20 hours agorootparentThe amount of paperwork to rent in Germany is just silly, so the landlord can have a higher likelihood of getting the agreed rent paid. That’s an additional cost when I want to rent a place and I’d rather not have a that at the cost of being kicked out when I don’t pay. reply ClumsyPilot 19 hours agorootparentprevBecause landlords have never abused their power for financial gain, malice or outright to get sexual favours from desperate tenants It took nearly 10 for my previous landlord to face the justice. She was completely outrageous and shameless in her crimes - blatantly in violation of registration for HMC, violation of planning permissions, squalor, disrepair, the houses had rats and literal hole in the wall for rain and wind. She would harass gullible and naive students, illegally evict them, etc. and all her crimes were on written record. And she was doing it with impunity for years. For all that, her punishment is a slap on the wrist - https://www.birmingham.gov.uk/news/article/266/rogue_landlor... reply shmeeed 22 hours agoprevIt's high time this issue gets sorted out by the courts and Schufa is finally forced to change their malicious business model. reply mschild 22 hours agoparentChange it to what though? On the one hand, I think Schufa is collecting a lot of unnecessary data that they should'nt have access to. For example, when I bought a pre-paid SIM card in Germany the provider did a Schufa check. No idea why. On the other hand, I get why a credit score can be useful for larger purchase decisions. If I was a bank, I'd like to know if the person I'm lending 400k for a property, has in the past defaulted on any loans. I genuinely don't know what a good middle ground is though. reply fmbb 22 hours agorootparentA good middle ground could be to only check what you suggested the bank wants to know: “Has this person defaulted before?” If someone wants to borrow huge sums from a bank the bank should take the risk and price it appropriately. If someone wants to borrow money to buy a TV the seller should take the risk and price it appropriately. The current solution seems too invasive for what it provides. How is the world better because Schufa knows someone bought a prepaid phone card and can sell that to an online store when you buy a pair of shoes and choose to pay by invoice the next week? Something is in theory cheaper somewhere but where is the proof? The only hard data we have is that Schufa and companies like them are making billions, so there is certainly money to be saved by someone somewhere by scrapping this. reply burnerthrow008 13 hours agorootparent> If someone wants to borrow huge sums from a bank the bank should take the risk and price it appropriately. Yes, but the point is that they can use your credit score to make more fine-grained assessment of risk. Should an auto insurance company offer the same rate to a 97 year old blind driver with a history of accidents as they would to a 30 year old in good health with no tickets? What about the 16 year old who just passed his driving test yesterday? Clearly these three people present vastly different levels of risk to the insurance company and, even though it is certainly possible for the insurance company to do so, it would be stupid to ask them to pay the same rate. The bank is simply doing the same thing: Pricing risk appropriately. The fact that they offer significantly different interest rates to different groups (or decline to offer credit entirely) just shows that these signals are effective at measuring risk. > How is the world better because Schufa knows someone bought a prepaid phone card and can sell that to an online store when you buy a pair of shoes and choose to pay by invoice the next week? Well, the shoe store is extending you credit, so it would seem appropriate for them to want to, as you put it, price the risk appropriately. The fact that you have recently undertaken other credit obligations is suggestive (yes, not conclusive, but still suggestive) that you may be in a precarious place financially and thus statistically more likely not to pay your credit on time. reply kioleanu 21 hours agorootparentprevSchufa for buying stuff and Schufa for banks are two different branches, meaning that if I buy something with Klarna, they’ll query a Wchufa service for online purchases. A bank credit will query another service from Schufa, which probably tells them what other loans I have and will put a huge dent in the score reply sofixa 19 hours agorootparentprev> On the other hand, I get why a credit score can be useful for larger purchase decisions. If I was a bank, I'd like to know if the person I'm lending 400k for a property, has in the past defaulted on any loans. The amazing thing of living in the modern connected world is that we have all sorts of information available easily. We can use what other countries are doing as inspiration, or as learnings what not to do. The problem you're describing has been solved for decades in tens of countries around the world, there's no point in reinventing the wheel. In France what the bank does is ask the national bank about the person's credit history, which will only contain current and past loans, lapses in repayments, being sent to collections, etc. So only stuff that is actually relevant to the question \"has this person defaulted on loans previously and are they thus a risk of defaulting again\", which together with the information about current revenues is all they need. reply dgellow 21 hours agorootparentprevForce them to have transparency, regular audits with consequences if they fail reply wolframhempel 21 hours agoprevWhat surprised me about Schufa is that the certificate it provides only states that you didn't have any defaults or delinquencies in Germany. That's quite different from a credit rating in the US sense that you can build up by consistently borrowing and repaying over time. When I moved to Berlin after having lived in London, my Schufa was thus as pristine as possible - simply because I had no German credit history to begin with. reply heffer 19 hours agoparentI do feel they are increasingly modelling themselves after the US system, seemingly only impeded by stricter data protection laws. Schufa, as you said, used to be mostly the exchange of negative-only debtor information (i.e. Schufa having no information about you is a good sign for your creditworthiness). I'd say in the last 15 years (or so) we've arrived at the point where Schufa having no information about you is about as bad as having negative information. Exactly like in the US. The only difference being that for their main consumer product you are still fine to get the \"Nothing negative to report\" document, as that is what's accepted by most landlords. reply changethe 18 hours agoparentprevthey have many different scores. the main consumer-facing score is what you are describing, which is never used by their customers. for customers (banks,insurances, retail companies etc.) they have scores tailored to their industry (branchenscore), which also includes repaid loans etc. the \"basisscore\" is pretty much useless, it's only a number given out to consumers. reply changethe 12 hours agorootparentin case you speak german: i wrote up an article about their scoring some time ago. in the following section i talk a little bit about the differences between \"branchenscores\" and the \"basisscore\": https://onlinekredit.com/schufa-scoring#der-schufa-basisscor... reply pintxo 21 hours agoparentprevThey do calculate a score, for use by their customers, which also includes your credit history. But if you don't have any, well. reply calmoo 22 hours agoprevSome fun features of SCHUFA in Germany: 1. If you apply for a loan, this affects your credit rating. Yes - not get a loan, or default on a loan, simply by applying for a loan, your SCHUFA can be affected negatively. Especially if you are shopping around for loans, this is infuriating. This happens because your SCHUFA is affected simply by another entity checking your SCHUFA score. This is also really annoying because it can be very hard to get a loan as a recent immigrant to Germany, so you inevitably end up applying for multiple ones. 2. Your SCHUFA score is also affected by where you live, and how many jobs you've had in X years, and how often you move (which is often as a recent-ish immigrant). 3. Services like Klarna affect your SCHUFA - even if you have a perfect payback time. reply bombcar 19 hours agoparentApplying for a loan in the US can affect your credit score for awhile, because the system isn’t instantaneous and it’s trying to handle someone requesting (and getting) multiple loans at once. There are “soft pulls” and “hard pulls” that affect this differently. Usually it doesn’t matter much in the long run of things. reply alibarber 21 hours agoparentprevApplying for a loan affecting your credit report also was/is a thing in the UK - but I think banks realised that it was hurting their business (they want to sell you a credit card/loan after all, and aren't going to if people are scared to ask them for it). So now it's a gimmick that's splashed all over their websites that you can get a quote with only a 'soft search' that doesn't affect your report. Even though, if you log on to the (free) credit reporting service to see your own data - these 'soft searches' are still recorded, so the industry has basically decided that they shouldn't affect your chances of getting credit. reply Mountain_Skies 21 hours agorootparentDon't about the UK but in the US there used to be a big privacy problem around people who worked at banks and car dealerships looking up credit reports for non-business reasons. Soft inquiries are a way to combat that by making every request to view a person's credit report into a recorded event that the person can see. reply hef19898 21 hours agoparentprevRegarding 1): This can be avoided by having an agency asking for loans for you, as those requests are not directly linked to you as far as Schufa is concerned. Most of the time so, you can easily ignore Schufa in your dqy to day live. They suck to deal with for mortgages, loans and appartment rental so. And they over step their authority and abuse their power all the time, without cobsequences for them so far. Hope that changes soon. reply prepend 20 hours agoparentprevThese aren’t random factors though, they are all correlated when higher risk of default. Would you like the models to ignore these predictors of financial strength? Because, you don’t like it? reply calmoo 19 hours agorootparentI'm sure you would change your opinion when you immigrate to a new country, have to move around to different apartments because of a massive housing crisis, and then find an apartment long term and get accepted, and then get rejected based on your SCHUFA because you moved around multiple times and perhaps opened a bank account or two. reply data_maan 19 hours agorootparentprevAs the previous person pointed out, these predictors have cases when they are obviously wrong. Solely relying on statistics will have people falling through the cracks. A perfect example of when thinking only in averages, and using trendy data-first approach leads to awful and stupid results for groups with too little size to matter statistically. Make your models fair, ffs! reply vdaea 22 hours agoparentprevUsing a service like Klarna means you're bad at dealing with money, so it makes sense. reply BadBadJellyBean 22 hours agorootparentIt isn't. I financed my phone for free via Klarna. I could easily pay it at once but why would I. In the end I save a bit because of inflation. reply josefx 21 hours agorootparent> I financed my phone for free via Klarna So how do they make money? Sounds like they would be losing money to inflation by offering that service for free. reply chmod775 20 hours agorootparent> So how do they make money? Generally the logic on \"free\" financing by the seller is that without it, they may have not made a sale at all. Sure it cuts into their margins, but offering rebates would have had a similar effect. Klarna probably still charges the vendor a few bucks for handling the thing. reply steveklabnik 15 hours agorootparentprevhttps://www.bitsaboutmoney.com/archive/buy-now-pay-later/ reply vdaea 21 hours agorootparentprevThey make money because they know that if you need to use Klarna to finance a €300 purchase you are very likely to default on the loan and pay interest. reply purpleflame1257 20 hours agorootparentprevBad at dealing with money isn't necessarily true. Better to say \"has outstanding financial obligations.\" reply vdaea 21 hours agorootparentprevUnless either your salary or the price of the phone increased in those months, you didn't save anything. reply dmurray 20 hours agorootparentOr if there was literally anything else he'd prefer to buy up front than wait until he'd saved up enough to replace the cost of the phone. reply globalise83 21 hours agorootparentprevThe price of the phone and the installment amounts are fixed at purchase. If there is a positive inflation rate you are guaranteed to make a real saving (but not a nominal one) by paying some of the purchase amount later on. What happens to your salary or the price of the phone later has nothing to do with it. reply TheNorthman 21 hours agorootparentprevHe can't predict the future. The relevant metric must be 1. The chance, weighted by amount, of a price hike/salary increase 2. The chance, weighted by amount, of a price decline/salary decrease. As inflation is generally an upward trend, clearly no. 1 is more likely and thus the best bet. reply chmod775 20 hours agorootparentAnother advantage of financing even though you would have had the money on hand is that you have more liquidity/the option of doing something with that money in the meantime. Also someone may have some assets they could turn into liquid cash if I really needed to but may not have a lot of liquid cash on hand, so there would be little risk in paying something in installments as long as their assets would be enough to cover it in a pinch. reply archi42 20 hours agorootparentWhile liquidity is a reasonable argument, I only know this from \"rich people doing things/investments\"; e.g. not selling shares in your company when building a new house, or a company building a warehouse. I am not sure it actually applies to the cost of a phone. Chances are, if those matter, you'd be better off with a cheaper phone to begin with. (Substitute phone with other luxury articles). reply AlexandrB 20 hours agorootparentprevJust by keeping that money in a savings account you'd save something. reply kioleanu 21 hours agorootparentprevI use Klarna (albeit rarely) to only pay _after_ any returns are completed and only the amount I need. Not pay upfront and then wait good know how long for three online shop to return my money. I always pay well before the deadline. My bank just scraped virtual cards and I use them for online payments so I wanted to use Klarna as a virtual card. Went through the whole process only for it to fail at the end with Klarna saying that they can’t generate that card, but I should go talk to Schufa or Arvato about it. So, not all use cases are bad and Schufa should distinguish between them. reply lukan 21 hours agorootparentprevHuh? Aren't they try to be mainly a online payment provider like paypal? That is how I used them couple of times. Money straight from my bank account via them, towards the shop. reply sofixa 19 hours agorootparentThey also provide payments plans (pay in 3/4 instalments, or over a year or two) which often, but not always, charge exorbitant rates. I've bought stuff split over 3-4 instalments multiple times for free or for a minimal processing fee (like 3 euros to pay 1600 euros in 4 instalments over 4 months). Always for stuff that I can afford outright, but would prefer to split the costs over multiple months. They probably make their money from the seller who banks on the fact that people would be reassured of being able to afford the high priced item, and thus be making more sells than otherwise. reply asmor 21 hours agoparentprevTo note, SCHUFA requires all requests to contain a feature code describing the GDPR-\"legitimate reason\" for the request, and only some affect credit. But those consuming the API have long sent wrong codes, and nobody in the chain cares. reply pjmlp 22 hours agoprevGreat! I hope that noyb gets the case, being at mercy of Schufa has a Mafia feeling to it. reply Nifty3929 17 hours agoprevPeople wouldn't be able to get this information without SCHUFA - at least not doing a fair bit of work. SCHUFA did work to build a system of gathering and providing this information in a consolidated way. I don't think people should have a right to demand this value be provided to them for free. If everybody now demands this information for free, why would this business continue to operate? And if they don't, how will landlords get information about who is financially reliable and who isn't? In the US is works because landlords typically will not accept self-obtained credit reports. They pay the credit agency directly. As a consumer I can get my information for free, easily - but my landlord will still want to pay the agency for their own copy. reply sokols 16 hours agoparentThey are required by law to provide the information once a year, for free. The complaint here is that the “free” report contains less information than the paid one. Information which could affect a decision about a loan or maybe a decision of some landlord about a prospective tenant. reply Nifty3929 13 hours agorootparentThis wasn't clear to me in the article: \"With the help of manipulative designs, people are prevented from obtaining a free copy of their data in accordance with Article 15 GDPR – even though they would actually be legally entitled to it.\" Maybe you're talking about the score, but the score isn't data - it's an algorithm. \"For example, in the case of the complainant, the free information included only a 'basic score', while the paid information showed six different 'industry scores.'\" And this is basically how it works in the US as well. When you get your free credit report, it will contain a score, but not the same score as is seen by a business requesting the report - because there are many different scores used for different purposes. The business pays for the score that matters to them which is different than what the consumer sees. None of this feels particularly nefarious to me. reply yakito 18 hours agoprevI remember a few years back, they themselves had proposed a system to use people's social media posts to create a \"credit\" system and rate individuals based on their postings. At the time, it was quite a mess. The public did not like the idea at all, and SCHUFA had to back down, but it's not the first time they've been involved in problems and controversies. https://slate.com/technology/2012/06/schufa-s-plan-to-use-so... reply amelius 18 hours agoprevCan we make data collection and data brokerage opt-in with a written consent letter that has to be re-acknowledged every year? I bet literally nobody wants their data collected and traded. reply ttyyzz 22 hours agoprevYou can request a free “data copy in accordance with Art. 15 GDPR”, which according to them “contains information that you should treat sensitively.” and further: “SCHUFA therefore advises against passing it on to third parties.” The product they market, the “credit report”, costs €30 and, according to them, is “forgery-proof” and “up to date” (lolz). I don't know the main differences to the free \"data copy\" - probably just a report with this specially invented key figure and a bit nicely prepared. All in all, SCHUFA has a very unsympathetic image here in Germany, I think rightfully so. Nobody wants to have anything to do with them voluntarily. reply kioleanu 20 hours agoparentThe free report is the list of entries they have on your name. Based on that they calculate scores and risks, but you don’t have access to those numbers unless you pay. They did buy an app last year called Bonify that allows you to see your credit score for free. There was a hilarious scandal last year as a security expert was able to get some high politicians credit score through the app and they shut the app down for a few days so this doesn’t get exploited reply amelius 18 hours agoparentprevQuestion. How do I know where to send my \"data copy requests\" if I want to know which companies have my data? reply sam_lowry_ 22 hours agoprevThey should market validation and non-repudiation services instead of just selling personal data. The problem is with marketing, not business model. reply ffpip 20 hours agoprevIs your credit score your data? Or are they just asking the report of all your past debts be provided for me? reply nforgerit 18 hours agoprevGerman here. If you ask me, Germany has set up the perfect feudal state. reply rtz121 17 hours agoparentExcept without any of the good parts of historic feudalism. reply vdaea 22 hours agoprev [–] What's the point of a credit agency if they can't get paid for making their reports? reply yorwba 22 hours agoparentThey're allowed to charge for the service of assessing a person's credit risk and attesting to it. In the common case you just get a document saying effectively \"we have no negative information on file regarding this person\". But they a",
    "originSummary": [
      "SCHUFA, a German credit agency, is accused of manipulating customers unlawfully and profiting from selling their data.",
      "Customers are prevented from accessing their data for free, as mandated by GDPR, and misled into buying paid products, violating GDPR rules.",
      "Complaints have been filed against SCHUFA for breaching data protection laws and engaging in misleading practices."
    ],
    "commentSummary": [
      "The discussion delves into various issues such as high taxes, healthcare systems, credit agencies, and renting practices in Germany and other countries, highlighting concerns about the negative impacts of high taxes and the limitations of tax-funded services.",
      "Credit agencies like Schufa face scrutiny for assessing creditworthiness for renting, with raised concerns about privacy, accuracy, and transparency in the process.",
      "Debates include comparing healthcare quality, costs, and accessibility between Europe and the US, emphasizing the effectiveness of government services, competition in service provision, and the balance between taxation and public benefits."
    ],
    "points": 286,
    "commentCount": 248,
    "retryCount": 0,
    "time": 1708080031
  },
  {
    "id": 39402876,
    "title": "Fake Traffic Surge on X During Super Bowl 2024",
    "originLink": "https://mashable.com/article/x-twitter-elon-musk-bots-fake-traffic",
    "originBody": "This week, Super Bowl 2024 shattered records, with the NFL championship broadcast on CBS becoming the most-watched televised event in U.S. history. Also riding high from the big game? Elon Musk's X. The company formerly known as Twitter published its own press release, lauding Super Bowl LVIII as one of the biggest events ever on the social media platform with more than 10 billion impressions and over 1 billion video views. Tweet may have been deleted However, it appears that a significant portion of that traffic on X could be fake, according to data provided to Mashable by CHEQ, a leading cybersecurity firm that tracks bots and fake users. According to CHEQ, a whopping 75.85 percent of traffic from X to its advertising clients' websites during the weekend of the Super Bowl was fake. SEE ALSO: Elon Musk’s X comes out in favor of pro-censorship law \"I've never seen anything even remotely close to 50 percent, not to mention 76 percent,\" CHEQ founder and CEO Guy Tytunovich told Mashable regarding X's fake traffic data. \"I'm amazed…I've never, ever, ever, ever seen anything even remotely close.\" CHEQ's data for this report is based on 144,000 visits to its clients' sites that came from X during Super Bowl weekend, from Friday, Feb. 9 up until the end of Super Bowl Sunday on Feb. 11. The data was collected from across CHEQ's 15,000 total clients. It's a small portion of the relevant data, and it's not scientifically sampled, but it nonetheless suggests a dramatic trend. CHEQ monitors bots and fake users across the internet in order to minimize online ad fraud for its clients. Tytunovich's company accomplishes this by tracking how visitors from different sources, such as X, interact with a client's page after they click one of their links. The company can also tell when a bot is passing itself off as a real user, such as when a fraudulent user is faking what type of operating system they are using to view a website. Most X users who are regularly on the platform can attest to a noticeable uptick in seemingly inauthentic activity in recent months. When a post goes viral on X, its now commonplace to find bots filling the replies with AI-generated responses or accounts with randomly generated usernames spamming a user's mentions with unsolicited \"link-in-bio\" promotions. Now, there's data which backs up that user experience. Advertisers have also noticed X's bot issues. In a recently published piece in The Guardian, Gene Marks, a small business owner shared his ad campaign results from X. After a small $50 advertising spend, X's analytics shows that his website had received 350 clicks from approximately 29,000 views. However, according to Google Analytics, X wasn't the source of any of the actual traffic his website had received during that time period. In our conversation with Tytunovich, he referenced an often cited stat that roughly half of all internet traffic is made up of bots, and how he's long been skeptical of that data based on what CHEQ itself sees. \"We were always the conservative ones,\" Tytunovich explained regarding CHEQ's approach to fake user data. \"We protect a lot of our customers on Google Ads, YouTube, and even TikTok, which I'm not a fan of, and we've always said 50 percent [being fake] is a bit opportunistic.\" \"I almost decided not to go out [and publish the X bot data] because we've never seen anything like it,\" he said. X has a bot problem unlike anything else seen on competing platforms When X's Super Bowl traffic is compared to other social media platforms during the same time period, the bot issue on Musk's platform appears even more stark. CHEQ also provided data to Mashable pertaining to Facebook, Instagram, and TikTok. In terms of fake traffic, no other platform came close to X's nearly 76 percent. Out of more than 40 million visits from TikTok, only 2.56 percent were determined to be fake. Facebook sent 8.1 million visits and 2.01 percent of the monitored visits were classified as inauthentic. And over on Instagram, only 0.73 percent of the 68,700 visits from the platform were fake. Tytunovich tells Mashable that it's not out of the ordinary to see spikes in fake traffic on social media platforms during big events like U.S. elections. However, he has never seen anything close to X's 75.85 percent. And, unfortunately for X, its bot problem goes beyond the big game too. CHEQ also provided Mashable with fake traffic data from the entire month of January 2024. TikTok, Facebook, and Instagram all had very similar stats to each platform's respective Super Bowl weekend numbers. Slightly more than 2.8 percent of the 306 million visits sent from TikTok were determined to be fake. Out of the 90 million visits that came from Facebook, a bit more than 2 percent were fake. And Instagram's traffic was only 0.96 percent fake, based on 749,000 visits. But, X once again fared the worst. Of the 759,000 visits from X, 31.82 percent of that traffic was determined to be fake. Related Stories Elon Musk tells Twitter/X advertisers to 'f**k yourself,' but admits it will die without them Why you'll see even less political content on Instagram and Threads TikTok quietly kills hashtag view count feature 200,000 Facebook Marketplace user records were leaked on the dark web Snapchat's My AI chatbot posted a Story then stopped responding. Users freaked out. Tytunovich, who has met with Musk previously and pushed the X owner to address the bot problem, stressed to Mashable that his company cannot tell how many fake users are on social media platforms themselves. The data only details how many bots came to CHEQ's clients' sites from those platforms. However, as Tytunovich explained, his company has a wide range of clients, including large, Fortune 500 companies, and this fake activity coming from X was seen across the board regardless of industry or market. Mashable reached out to X for information or a statement but received an automated message from the company reading \"Busy now, please check back later.\" An Elon Musk problem X didn't always have a bot problem of this magnitude, according to CHEQ, something Tytunovich demonstrated by providing Mashable with data from last year's Super Bowl. During the comparable weekend in February 2023, fake traffic from the platform then-known as Twitter only accounted for 2.81 percent out of 159,000 visits. That's around 72 percent less than this year's game. Last year's Super Bowl occurred just a few months after Elon Musk acquired the platform in late October of 2022. And a lot has changed on X between last February and today under Musk's leadership. The platform was still known as Twitter then. Notable users still had their legacy verified blue checkmarks. Only around 200,000 to 300,000 users were subscribed to Twitter Blue, which is now called X Premium. X's creator monetization program, where paying X Premium subscribers can make money off of ads displaying on their content, did not yet exist. All of these changes can factor into X's current bot problem. In addition, since Musk's take over, 80 percent of the company's Trust and Safety team's engineers have been laid off, along with half of the company's content moderators. Thousands of employees have been laid off across the company. X has struggled with advertisers since Musk's takeover. Big brands and major companies like Disney have suspended ad campaigns on the platform due to hate speech and pro-Nazi content proliferating on X, as well as antisemitic comments made by Musk himself. According to a Bloomberg report last month, X is planning to open a Trust and Safety center in Austin, Texas and is hiring 100 employees to work in that department in order to address some of advertisers' concerns. But, X's problems clearly go well beyond the type of content being posted by real human beings. Advertisers typically pay social media companies based on impressions and/or clicks on their advertisements. And based on this traffic data, advertisers could potentially be paying Musk and company for visits from an audience consisting mostly of bots. Topics Cybersecurity Social Media Super Bowl Twitter Elon Musk",
    "commentLink": "https://news.ycombinator.com/item?id=39402876",
    "commentBody": "The majority of traffic from X may have been fake during the Super Bowl (mashable.com)242 points by nickthegreek 13 hours agohidepastfavorite172 comments arbuge 10 hours agoWe advertise on X and have done so for many years now (well, it was Twitter for most of that period). Indeed we can confirm that we saw a big change happen around a couple of months ago or so - sometime towards the end of last year. Our ads started getting literally orders of magnitude more traffic - we reduced the bids by a factor of 30x (no typo - that's thirty) and were surprised to see them taken up to the budget limit completely... we ended up spending a minor fraction of what we spent before for a lot more traffic. So far, so good, right? Not so much. None of the traffic converted anymore. All useless. I can't remember the last actual conversion from X our conversion tracking software recorded at this point... weeks ago for sure. Formerly we got several a day. reply laborcontract 9 hours agoparentYour experience with twitter is similar to my experience advertising on reddit a couple of years ago. Their ads were dirt cheap compared to on other platforms but nothing converted. Most traffic from Reddit were roundtrip clicks with average sessions of about a second. I would say that, from a user perspective, I do accidentally click on a lot of stuff on Reddit and Twitter. Almost every third post is an ad. reply wlesieutre 7 hours agorootparentAbout 99% of the ads I’ve clicked on Reddit were by accident when I hit the back button and then try to click a link below an ad just as the page reflows to fit a taller ad and puts it under my cursor reply thih9 3 hours agorootparentAn accident that happens repeatedly, its cause can be traced to a single actor, and that actor also benefits from all of instances of the accident. Ad tech is weird. reply arbuge 8 hours agorootparentprevReddit has always been like that, but X formerly converted quite well for us. reply walrus01 7 hours agorootparentprev> I would say that, from a user perspective, I do accidentally click on a lot of stuff on Reddit and Twitter. Almost every third post is an ad. As a user, and not a person with advertising dollars to spend, try using reddit in firefox with ublock origin installed as a plugin. The experience is significantly better. Also works well with firefox on android and ublock origin. reply nothercastle 8 hours agoparentprevThis might partially disappear with the nitter loophole closed but the number of bots under musk has gone up an order magnitude. I used to have 5 followers now I have anywhere from 70-100 depending on the day and I guarantee you I have not become more interesting. It’s all bots. reply dawnerd 8 hours agorootparentWith nitter gone a lot of burner accounts are being created which no doubt they’re going to claim as a metric win. I’m running my own scraper with a lot of burner accounts. They can’t stop us from scraping. reply treyd 7 hours agorootparentWhat are you scraping for? reply mhio 8 hours agorootparentprevHow does nitter increase ad traffic? reply viraptor 7 hours agorootparentprevWhy do you think nitter going away will change anything? I would expect the set of users purposefully using nitter to be completely contained in the set of users with adblock on. reply nothercastle 6 hours agorootparentOther people are using the same technique for nafarious purposes so they will at least have to change techniques reply jrflowers 10 hours agoparentprevHow long with no conversions do you typically continue ad spend? reply garciasn 9 hours agorootparentDepends on the type of ad and its intent. But, no one should be advertising on Twitter for any reason anymore. It’s not brand safe and it’s always been absolutely trash; but, now it’s even worse. Shift your budget to any other meaningful platform within the channel; you’ll be glad you did. reply oefrha 7 hours agorootparentPolitical ads should do just fine. I was in Hong Kong for two weeks late last year and every time I opened Twitter there I saw a promoted post for a voachinese.com article (something something China bad). It’s always VOA Chinese even though my language is clearly English, and my account is completely passive, never tweeted anything, liked anything or followed anyone, most of my visits are HN referrals about something tech related. It was quite surreal, like they bought the entire ad space of the region or something. reply arbuge 8 hours agorootparentprevIt’s not a bad idea to keep stuff running on a shoestring budget to keep an eye on things, in case things change again. reply DelightOne 7 hours agoparentprevIsn't there an extension somewhere that got popular that clicked every Ad and just did not show it to the user? reply CamelCaseName 7 hours agorootparentAd Nauseum? reply bamboozled 9 hours agoparentprevI guess this is fraud? reply pixl97 8 hours agoparentprevAny idea what the traffic looks like? Phones? Windows? Mac? Any consistency behind it? reply arbuge 8 hours agorootparentLooks quite human. Usual mix of user agents and visit durations. It’s not hard to fake that though, I guess. reply dustingetz 7 hours agoparentprevyou’re saying the ad click traffic from twitter is fraudulent? reply silisili 11 hours agoprevAnyone who has been on Twitter lately could tell you that. Click any mildly popular thread, and you get about 25% crypto scams, 25% OF, 20% irrelevant clickbait, and maybe 25% at least somewhat relevant replies. The other 5% are also bots complaining that the replies aren't about the post, odd as that sounds. reply lemoncookiechip 11 hours agoparentThe blue checkmark is the main cause of this, or rather, the fact that it automatically boosts your comment over those who don't have one. This led to the current engagement farming meta, where none of the comments are about the OP itself (save for smaller accounts and even then.) EDIT: Sometimes I find myself holding the spacebar, doom scrolling to reach the \"peasant\" comments, because at least they're actual comments. reply Freedom2 8 hours agorootparentI remember when a large portion of Hacker News commenters were praising the blue check changes. Comments such as: > Fewer bots, less brigading, ideally no ads, less spam, etc. > I am actually quite optimistic. Let the best ideas win. The blue check elitism is coming to an end. > Given twitters known bot problem, the number of bluechecks will likely be muuuuch lower as real people start to pay for them, and the real people behind the identities have to pay for them instead of their PR staff running them, but this in turn will make them more valuable and meaningful. reply insin 7 hours agorootparentprevControl Panel for Twitter [1] can automatically hide boosted blue replies for you …although if an account is big enough, you're safer looking at the Quote Tweets instead for actual comments (it also restores the old direct link to those in the focused Tweet), e.g. you're lucky to get more than a handful of non-blue replies under an Elon Musk tweet among the engagement farmers before you hit the maximum number of replies Twitter will load [1] https://jbscript.dev/control-panel-for-twitter reply hinkley 10 hours agoparentprevIt's almost as if some lunatic came in and started shutting off systems that he didn't understand, assuming that they were unnecessary. Why is this fence even here? reply TheAceOfHearts 11 hours agoparentprevI think one limiting factor is the fact that most posts aren't particularly deep or insightful, so the intellectual space from which one can engage on the subject is pretty limited. So most people who engage with content of this sort are just clout chasers trying to get attention or scammers. Is it really surprising that if Elon amplifies some post with a single exclamation mark all of the replies will be completely inane and full of spam? Another factor is account size. You can still find interesting replies in accounts that are relatively small, especially when the author takes time to manually hide spam and scams. But once an account goes beyond a certain size they just become a magnet for nonsense. It's pure quantity over quality. Elon has the biggest number of followers but the quality is abysmal. reply jsheard 11 hours agorootparentIronically discussions tend to be better in circles where nobody wants to pay for the blue checkmark on principle, because the comments there are still sorted using the old ranking system. But as soon as bluechecks start posting they get shunted to the top regardless of quality, burying all the good replies from non-bluechecks. reply nothercastle 8 hours agorootparentWhere do you find these circles all the comments are spam. reply lowbloodsugar 10 hours agoparentprevI mean, my Facebook, which I visited for the first time yesterday after months, was almost entirely \"fan\" pages showing AI generated images of famous actresses in skimpy underwear. Like 90%. I suppose the algorithm has decided that if I read the posts by sci-fi author David Brin, then I'm in a group that also wants dirty pictures of actresses. o_0 reply fmobus 36 minutes agorootparentSame experience for me here. And somehow, META stock is absolutely printing. I really don't understand this reply nothercastle 8 hours agorootparentprevI went through a phase where it randomly showed me what I assumed were Thai prostitutes because a spammer created an insta account with my email. After I reset the password on the account it eventually subsided. reply jeffbee 10 hours agoparentprevBlueBlocker cuts right through all that junk. reply crazysim 10 hours agorootparentAdding this to your host file works too: 127.0.0.1 twitter.com reply input_sh 2 hours agorootparentprevDoes it really? I use it and it has blocked around 25k Blue users for me, but that's nowhere near enough to make using Twitter enjoyable. reply lucidone 12 hours agoprevEvery day I get new followers on twitter that are sex bots and onlyfans catfishes. Nothing else. Seems like the platform is a wasteland. reply latchkey 12 hours agoparentI recently had a fake account follow me where they replaced the letter L with an I in the user name. Whatever font Twitter website uses, makes it impossible to visually see the difference. The fake account looks 100% the same as the real, even has thousands of 'followers'. I reported it, and it is still up. Original: https://twitter.com/WildcatTrader Fake: https://twitter.com/WiidcatTrader It is unreal to me that the platform hasn't developed a way to automatically deal with fake accounts like this. Just check to see if the profile image is the same!? reply labrador 11 hours agorootparentI had a fake Yann LeCun (Meta's AI chief) follow me. It looked and read like the real thing. I was happy about it for a couple of days until I realized it was fake. It fooled a lot of people. I didn't report it because obviously Musk doesn't care. reply latchkey 11 hours agorootparentI can't imagine working for Twitter, in a position to fix things like this, and having to listen to Musk tell me to 'stay the course'. I know that everyone has a price, so their salary must be insanely good. reply MBCook 10 hours agorootparentI suspect that only the true believers are left. There were _so_ many actions and warning signs over the last year I can’t imagine anyone thinking it was a good/stable place to work unless they really liked Musk. reply greesil 9 hours agorootparentI think there's still a bunch of people who have an audience and don't really care about the drama. They're there not to engage, or sell anything, but just to use it as a broadcast medium. But yes why would I ever talk to anyone there? I can just use nitter to read my neets and call it good. Edit: oh fuck nitter is gone, arrrgh reply refulgentis 9 hours agorootparentThey meant twitter employees, not users reply MBCook 7 hours agorootparentRight, that was what I meant. For creators, I’ve heard other services offer much more useful interaction per follower, which would mean either Twitter followers often aren’t shown posts of a large cube of followers don’t care (likely bots). I don’t think Musks behavior and obvious preferences should be ignored. But even without that it sounds like it’s dead at the core and more people figure it out every day. reply hackerlight 8 hours agorootparentprevThere's many nitter mirrors still up reply ews 9 hours agorootparentprevFrom what I know, lots of people with visas (L1/H1B) are stuck in twitter hell. reply labrador 8 hours agorootparentI can't imagine the hell it must be to be stuck this way at X. Can you imagine being the engineer that gets the email \"Elon wants Séamas O’Reilly suspended because he said some rude things about X\" and you happen to be Irish? Journalist says he finds it ‘surreal’ to have account on X suspended after writing critique of platform https://www.irishtimes.com/culture/books/2024/02/12/journali... reply latchkey 9 hours agorootparentprevThat makes more sense than the Musk true believers theory. I mean, I get it, if you work at spacex, then that's one thing... you're building rockets and all. reply lostlogin 7 hours agorootparentI laughed, this is a new one. How many dollars would it take you to work for Musk? How many dollars would it take if it involved space rockets? reply techdragon 6 hours agorootparentDunno how “new” it is… I’ve definitely got a big price difference and I’m pretty sure others will to… Twitter, Tesla, Neuralink, Boring… all the non space Musk corps…. Quarter million if I don’t have to move from Australia, half million to a million depending on where I was going if it requires moving to the USA… I’d do it… but I’d basically have the resignation letter already drafted from day one… For the rockets… I’d probably be happy with their normal pay offer… not being a US citizen basically makes it impossible for me to apply for a job with SpaceX so I’d take what I could get lol… and since Elon is clearly just glued to his phone tweeting… it’s not going to be the Elon show… Gwynne Shotwell is running the company like a level headed and competent management executive, the place is full of competent engineers who seem like pretty well adjusted people when they talk about their experiences after leaving, and they all seem to leave for normal reasons… reply canadiantim 11 hours agorootparentprevI can't be certain but I'm pretty sure Musk isn't personally going through each report to vet them reply ethanbond 10 hours agorootparentDo you think that's what's being proposed? Was the fact this was not as significant an issue due to Dorsey and then Agrawal being really good moderators as individuals? Or do you think maybe business leaders are accountable for the behavior of the system beneath them, especially when they personally overhauled exactly the system in question? reply hackerlight 8 hours agorootparentprevWell, he fired the entire moderation team, and he made it no longer against ToS to post overt racism. Then he made it against the rules to say \"cis\". All of this was super important for the square speech town freedom thing, you wouldn't understand. reply bentley 10 hours agorootparentprev> Whatever font Twitter website uses, makes it impossible to visually see the difference. Doesn’t it? When I view the profiles you listed (before that horrible login wall pops up), the ‘I’ is of the crossbar variety, and the ‘l’ has a finial, making the characters visibly different. In fact, I recall that Twitter started using that font (which also visually distinguishes 0 and O) after the Musk takeover as an anti‐spoofing measure. Spoofed usernames were completely impossible to detect before the font was changed. reply latchkey 10 hours agorootparentDepends on your system/browser font? reply bentley 10 hours agorootparentWouldn’t think so, given the intent. Shows up for me on both desktop and mobile. Do you have any local font overrides? Looks like the change happened just over a year ago: https://www.theverge.com/2023/1/26/23572746/twitter-changed-... https://mashable.com/article/twitter-new-font-fights-imperso... reply NetOpWibby 11 hours agorootparentprevTwitter Support doesn't care about legit scams and bots but will absolutely enforce a mistaken ban. reply metaphor 11 hours agoparentprevSure enough, 17 new bot followers today after blocking 14 only a few days ago. There's precisely zero reason any legit person should be following my entirely passive, unengaged account. Twitter truly is a cesspool. reply nothercastle 8 hours agorootparentDitto but I’ve amassed almost 100 from I think 5 pre musk. It’s interesting to see it fluctuating with ban waves reply unshavedyak 11 hours agoparentprevIt's interesting to me how hot and cold descriptions are of Twitter. You get posts like these, but then someone chimes in about how much better the conversations/etc are on Twitter since Musk took over. I'm not on Twitter so i can't really make sense of it. I feel like i see more negative than positive.. but still.. it's bizarre to me that there's people in both camps. More than likely some of them are biased.. but still, i find it \"interesting\". reply ethanbond 10 hours agorootparentThere are a lot of people for whom everything Musk touches is the worst thing ever created, then a lot of people for whom everything he touches is gold, then some people who just like a pretty reasonably stable social media site that's not overrun by bots and pay-for-engagement morons. IMO it seems objectively true that the bot problem is worse than it's ever been. It's definitely objectively true that boosting paid-for comments above organically high-engagement comments means your signal:noise ratio is way, way worse. reply dotnet00 9 hours agorootparentIt's not really clear at all to me, since, eg some people here are talking about getting several bot account follows daily, while for me it is the opposite, where I used to get a lot more bot follows than I have gotten over the past year or so (down from 4-5 per week to less than 1 per month). On the other hand, I do see a lot more bots in replies (haven't had any reply to me yet though). Overall my opinion has been slightly positive, my usage of the site is up and I'm hearing far less of people I follow (or those they follow) who have been unreasonably suspended or forced to delete tweets. It feels a lot \"wilder\" than before, but that's kind of what I enjoy anyway. But I also don't really engage with political Twitter, I only use it to keep up on space related reporting and weeb stuff. The worst part has just been a lot more engagement farming through dumb statements from content creators who are circling the drain. reply sinuhe69 7 hours agorootparentYeah, I also engage little on Twitter, but so far I’ve seen no notable worsening of spam. When I clicked to see popular announcements (from HN or somewhere else) I also saw crypto spams and other stuff but from my perspective, it’s not much different from the situation on FB, for example. When I replied to some interesting GTP-chess posts, I still got replies clearly from humans (ask questions, do some follow up etc.). Thus, I don’t have a clear feeling whether it got better or worse. The most striking change I have seen is the percentage of Musk’s tweets on my feed. It looks like he is everywhere now! That was definitely not the case before. But yeah, he is the owner and he likes to tweet :) reply slily 10 hours agorootparentprevPlease share some examples, I never see crypto scams anymore and pre-acquisition you could go in the replies of any popular account and it would be nothing but that. I'm guessing it's still there but I don't see it because I don't scroll for hours on end and verified posts being boosted at the top suppresses the visibility of any bots. Either way considering how bad the bot problem was before the change in ownership I feel like people have a very selective memory about this problem. I did get a handful of catfishing bot accounts following me a few months ago, but that's about the extent of what I've seen as far as spam goes recently and feeds are much cleaner than they were a couple years back. If you're comparing to how it used to be several years ago then it's a different story. > It's definitely objectively true that boosting paid-for comments above organically high-engagement comments means your signal:noise ratio is way, way worse. I'd like to see an objective proof for this alleged objective truth. The bigger problem that I see constantly nowadays is cherry-picked information and people outright lying about a picture or video that they're sharing, but thankfully the community notes help a lot with that. Go ahead and flag this comment for not being prefixed with \"Grr I hate Elon Musk!\" reply nradov 10 hours agorootparentprevI don't care about Musk one way or the other and I don't see any evidence to support your claim. The level of bot activity is no higher than before in terms of comments on tweets by accounts that I follow. But why bother reading comments anyway? That has always been mostly a waste of time. reply sharkjacobs 10 hours agorootparentmy dude, you're in a comment section right now, replying to a comment, with a comment reply midasuni 9 hours agorootparentTo be fair his comment was a waste of time reply candiddevmike 11 hours agorootparentprev> someone chimes in about how much better the conversations/etc are on Twitter since Musk took over I always assume these types of comments are some kind of dog whistle reply asadotzler 9 hours agorootparent> to whom and why To the Nazis that were kicked off or fled when Twitter had decent moderation to let them know Nazi conversations are flowing like wine and they're welcome back. reply nothercastle 8 hours agorootparentLots of nazis these days from all corners of the world. Fuckers are back with a vengeance reply nomel 8 hours agorootparentI think the largest increase in Nazis is due to the complete, and rather disgusting, misuse of the word. reply concordDance 10 hours agorootparentprevTo whome and why? reply refulgentis 9 hours agorootparentprevRephrasing for people who will react negatively to that. You're making an important point but \"dog whistle\" assumes some things: Those are people speaking abstractly about how much more free their speech is*. I've never heard that common phrasing used, in any setting, to refer to a change in Twitter user makeup, or it becoming a kinder place. Whether I'm in the company of team left or team right, and coding as same team or opposite team. * yr humble author refuses comment and does not endorse this viewpoint, or that anything changed on this front reply lp0_on_fire 10 hours agorootparentprevAs opposed to the “twitter was so wonderful before Musk and now it’s a wasteland”? Please. It’s always been a wasteland. reply bodge5000 8 hours agorootparentprevI ended up in a really unfortunate position, pre-elon I never really \"got\" twitter and so barely used it. When he took over I'd log in every now and then to see how much it was plummeting, and in the process of doing so found the (ever shrinking) value of the platform, so I only started enjoying it when it started going downhill (for me at least). I guess though that's less the platform or the people running it and more the users. There's nothing particularly unique about twitter that I like other than the other people who use it reply omoikane 10 hours agorootparentprevFor me the Twitter experience is roughly the same as before, except ad density increased. It might be that the majority of Twitter changes mostly affect only English content, which I mostly do not follow. reply mewpmewp2 11 hours agorootparentprevIs this based on the kind of politics you are into? E.g. if you are from US you either have one side or the other where one side likes it and the other dislikes it? reply Georgelemental 11 hours agorootparentprevFor people with interests or opinions that were heavily censored by pre-Musk Twitter, the end of this censorship alone compensates for the countless new annoyances. reply deeth_starr_v 11 hours agorootparentThis seems vibes based. Twitter censored in the past and continues to censor. If you were in the out group and now are in the in group you’re happy even tho you were unlikely to have been censored reply MiddleEndian 9 hours agorootparentI've always found Twitter to be kind of dumb, most of my interaction with it is when twitter bullshit gets plastered over UFC PPVs or \"news\" articles report on what people say on twitter. I used my account to (mildly) troll some guy who shared my same name until I got banned a few years ago. See: https://news.ycombinator.com/item?id=30605212 But despite what some people claim, they did not unban everyone, I checked and I am still banned. outgroup to outgroup lol Still hoping the site collapses so the two cases mentioned above stop happening. Also an extra lol @ people learning why user-content sites need moderation, even 4chan is moderated. reply ethanbond 10 hours agorootparentprevDoes it? Does it account for the 13 new sex bots following me every time I log in, or my DMs full of crypto scams? Great point, I guess. reply Fauntleroy 10 hours agorootparentFor people into sex bots and crypto scams it's a massive improvement, in fact. reply GuB-42 10 hours agorootparentprevCensorship hasn't ended, it is simply impossible not to censor. There is obviously illegal content (child porn, ...) and copyright infringement, you have to censor it or you will get sued and lose. Related is libel, doxxing, harassement, revenge porn, etc... that may also get you in legal trouble. And there are countries other than the US with different laws, for example many European countries ban some categories of \"hate speech\", and if you want to do business with them, you need to follow their laws. And there is spam, if you let all of it pass, it will simply drown everything else. And at some point, you may want to make money. Usually, you start with advertising, and if you want people to advertise on your platform, so you have to be at least somewhat socially acceptable. You also have to please payment processors, if you bring the wrong kind of people, Visa and the likes won't want to deal with you. On that last part, considering Elon Musk past, he may have workarounds, but the reason Visa has a problem with porn is not just because they are prudes. That's because there is a lot of fraud happening here, and it will happen too on a socially unacceptable network. And yes, even 4chan is censored. reply asadotzler 9 hours agorootparentprevI somehow doubt that the audience that's going to make Twitter money with subscriptions or advertisers are Nazis, MAGA, Crypto Bros, and OF performers. But you do you. reply rsynnott 11 hours agorootparentprevLook, some people _like_ talking to sex-robots, alright? Nothing to be ashamed of. reply ineedaj0b 11 hours agorootparentprevyeah, i'm having a great time reply lofaszvanitt 10 hours agoparentprevInstagram is the same. They have bots to give you the feeling that you jumped right into a group of people, while it's actually a desert like environment. reply FireBeyond 8 hours agorootparentAnd the stupid optimization tricks even legitimate users do to increase their clout: post several pictures - \"Which one is your favorite? Comment below\" so they get engagement points by the number of comments when it's just a stream of numbers that no-one (probably not even the creator) cares about. reply stephenitis 4 hours agoparentprevCreate a new account and follow tech people, I get none of that. reply arcanemachiner 11 hours agoparentprevI'm struggling to remember a time when Twitter was not a wasteland. reply comeonbro 12 hours agoprevM Y P U S S Y I N B I O I gave Musk a lot of leeway on this for taking over right as open-source LLMs started ~passing the turing test, but that they can't even stop this is eating most of it. reply MSFT_Edging 12 hours agoparentThe paid blue checks basically created a bot-engagement tier. for $8/mo you can protect your account from getting banned(profit motive to retain paid checks) AND get your spam to the top of every post. reply jsheard 12 hours agorootparentI saw an account with a $1000 gold checkmark impersonating a genuine goldcheck account to promote a crypto scam, that's the deluxe spam boosting package. reply input_sh 12 hours agorootparentprevAND now you get paid for views, so just have your bot account post a random stolen video and spam it under every single post that goes anywhere near viral. Boom, easy money. reply mlinhares 12 hours agorootparentNo one that isn't in the small circle of select chosen is paid at all, that is just yet another scam. reply notatoad 8 hours agorootparentHas anybody been getting paid for the last couple months? I heard the checks had stopped coming, even for the select few. reply anigbrowl 11 hours agorootparentprevAnd at the same time they put streaming API access behind a $5000/mo paywall so there's no way for independent researchers to easily research/report on botnets, astroturfing campaigns etc. reply stefan_ 11 hours agorootparentprevAnd they don't even spam! Instead its just endless ChatGPT drivel and noise. In a way that's even worse. reply jsheard 11 hours agorootparentI'm guessing the white noise GPT posts are to \"warm up\" accounts before they start spamming links, to get around spam detection heuristics. reply jdminhbg 9 hours agorootparentprevMusk Twitter has not done any better than Old Twitter at containing spam, but it has managed to at least monetize it, which you have to admit is a small step up. reply nothercastle 8 hours agorootparentDefinitely not in comments maybe in tweets reply mindslight 8 hours agorootparentprevAds are a type of spam, so really this is just moving down market. reply solardev 12 hours agoparentprevMan, even that's a scam. All I wanted was cat videos, but it keeps showing me undressed humans :( reply jacquesm 12 hours agorootparentMaybe youtube then? I mean, you need your fix, right? reply goles 10 hours agorootparentYou joke (perhaps) but I've noticed Youtube recommendations now occasionally serves low engagement videos which oddly works great for cat videos. I regularly get 100-200 view, 10-60 second cat videos alongside long form content. If you need some training \"data\" try Cookie the Calico. reply jacquesm 9 hours agorootparentI have installed plug-ins that totally strip all recommendations, shorts and other stuff from youtube to the point where it only displays the one video that I was interested in. This probably saved me more hours than I care to admit. 'unhook' and 'hide youtube shorts' are the main ingredients. reply pfdietz 11 hours agorootparentprevYou can get unlimited iterations of hydrogen peroxide ED scams. reply martin8412 1 hour agorootparentSo I just need some fake ED pills and some nail polish remover to build an explosive(TATP)? reply subjectsigma 9 hours agorootparentprevNot defending Twitter per se but I’ve seen partial to full nudity in YT recommendations after watching fairly innocuous but popular videos. For lack of a better term to describe it if you watch enough “normie” videos it seems to start showing you anything with a high view count. Watch 100 silly cat videos and it will start recommending you celebrities and football and ofc naked girls. YMMV but that’s what I’ve seen. Worse if you aren’t logged in. At least it’s better than Instagram where softcore porn is the rule rather than the exception reply paulddraper 12 hours agorootparentprevTeases all of them reply Finnucane 12 hours agorootparentprevMastodon is at least 30 percent cat pictures. The rest is moss. reply kevingadd 12 hours agoparentprevTo me the weirdest part is that all the bots I see have blue checkmarks. How does it make sense for the bot operators to pay money to run their bots? Are they using stolen credit cards? reply fshbbdssbbgdd 12 hours agorootparentIt would be interesting to know how well X detects signups with stolen credit cards. If scammers have millions of credit cards from data dumps and make a $8 charge on X, what portion of the victims would even notice? Even if X receives a fraud report and acts decisively to shut down the offending account, it might just be an inconvenience to scammers who have signed up for many more. At scale, fraudsters could be moving a substantial amount of stolen money into X membership fees. reply derefr 11 hours agorootparentYou don’t need literal stolen credit cards to dodge CCN bans any more. There are so many services today that allow you — without KYC! — to create an online wallet that is assigned a real CCN, and which also don’t present as a “gift card” on systems like Stripe. You just steal or scam people out of liquid assets of any kind (e.g. crypto), and you can then turn those into as many virtual credit cards as you want — all under different names, with different addresses, etc. Many of these services also allow spending limits to be set on these cards on a per-card basis — which in practice allows the defrauding of any postpaid service, by limiting each card to spend just enough for the initial card verification, and then denying the actual postpaid charges when they come. (Then you close the acct, and open a new one with a new name + card.) To combat this, we’ve resorted to using grey-market(!) “BIN lists” to determine what CCN prefixes (BINs) are used by the banks backing these non-KYCed online wallets, and just blacklisting all of them. Somehow, I doubt bigcorps are doing the same. reply refulgentis 9 hours agorootparentMan this is gold. Thanks for sharing. Would be curious for any naming you can do re: where to look for said BIN lists, as well as, a company or two that does this CCN thing. reply happytiger 11 hours agorootparentprevI think you should seriously consider who can operate bots with blue checkmarks for free and your question gets easiky answered. Occams Razor… reply spamizbad 12 hours agorootparentprevJust the cost of doing business. They make more than enough to afford $7/month. reply codetrotter 12 hours agorootparent> They make more than enough to afford $7/month. Sure. But I would expect bot operators to have a vast array of accounts. If you have 250,000 accounts and you paid for a blue checkmark for 10,000 of those accounts. That would be $70,000 per month. reply asadotzler 8 hours agorootparentYou only need 10,000 accounts if you expect bans. Bluechecks don't get banned so you only need 1,000 not 10,000 (made up numbers) and in that world, you just steel some crypto to fund a thousand credit cards (or similar) and you're gold, forever because Twitter won't ban paying customers -- it's too desperate for that. reply ethanbond 10 hours agorootparentprevDon't need a vast array of accounts if you're not getting banned. reply unshavedyak 11 hours agorootparentprevIt's more a question on how much they make per account though, right? It's either above or below the profit line on average. Or they've found a way to reduce costs, like stolen cards or owning Twitter (hah). reply codetrotter 11 hours agorootparent> a question on how much they make per account though, right? Idk, but intuitively I would think that the profits they get are spread over a great number of accounts. If 98% of your spam earned income comes from a single account, that would make you very vulnerable to losing all of your income if that single account is banned. If on the other hand, 80% of your income comes from 10,000 accounts then it doesn’t matter if 1 or 10 or 100 or 1,000 of those accounts get banned. You’ll still have the income from the remaining 9,999 or 9,990 or 9,900 or 9,000 accounts. And you’ll make a bunch of new accounts all the time so that for all of those account of yours that are banned it’s not really significantly reducing the total number of accounts under your control either. Of course, I could be wrong and it could be that they have a handful of accounts that make all of the income and all of the other accounts only serve to fake engagement for those few accounts so that those few accounts get boosted and seen by people. I’m not in the spam making “business” so I don’t know how it actually works. reply cubefox 12 hours agorootparentprev(I think it costs 8, not 7 dollars.) The surprising thing is that apparently the bot operators make more than $8 _per account_ on average, which is surprising to me. reply cma 12 hours agorootparentprevIt let's twitter run fake organic ads without having to tag them as ads. A paid account is basically an ad post account boost as long as you don't go too hard against the spam rules. reply add-sub-mul-div 12 hours agorootparentprevIt's a de facto entry level advertising tier. There are ideas and products that spammers believe are profitable to spam at that price point. reply refulgentis 12 hours agoparentprevThere's so many weird things about that specific abuse. The frequency...the inability to stop it after 2 months... The weirdest thing is they seem to get some special treatment that renders them invisible, yet, not prevent them from being posted altogether, or have any impact on the account itself. It's not even particularly productive spam. It's like watching someone thumb their noise in front of a steamroller. Makes no sense. reply detourdog 12 hours agorootparentAll of that plus the original intention was to buy twitter and stop the bots. reply pavlov 12 hours agorootparent“Stop the bots” was a made-up rationale when he wanted out of the deal. The original intention was to buy Twitter so Musk would be the most popular man in the world, his comedy would be appreciated by a billion followers soon, and as a nice bonus they’d buy whatever crypto he decided to pump. The Twitter buyout was maybe the very last major business decision executed entirely on 2021 moon logic. reply Karellen 11 hours agorootparent> they’d buy whatever crypto [Musk] decided to pump. For a moment, I'd forgotten that was a thing Musk actually did. I got so used to bots impersonating him pumping cryptocurrencies, my memories had rewritten the times he did do it himself into more impersonators doing it instead. reply solardev 12 hours agorootparentprev> The Twitter buyout was maybe the very last major business decision executed entirely on 2021 moon logic. You know, a part of me hopes he makes it to Mars in my lifetime and just proceeds to take over the planet, running an alternative society there. Planet Musk: The Reality Show™ would be quite the thing to watch... from 140 million miles away. reply rsynnott 10 hours agorootparentIn the past few years, I’ve read at least two sci-fi novels (an Alastair Reynolds one and one other, maybe Ken MacLeod) which made passing reference to a failed Musk Mars base. Seems to be a popular idea. reply babypuncher 11 hours agorootparentprevI feel like this is how we get a much dumber version of the MCR from The Expanse. Like, they wouldn't be justifying their weird space fascism as a means to terraform Mars, it would all just be one big joke to fuel Musk's ego, and somehow a bunch of people end up being stupid enough to buy into it. reply bryanrasmussen 11 hours agorootparentI was thinking it would be sort of Martian Time Slip, only just Elon Musk instead of Ernie Kott all the time. reply refulgentis 11 hours agorootparentprevLol @ the Twitter purchase being a \"jump the shark\" moment for the gogo COVID era Lol'ing at its accuracy, and it being obvious once you hear it, but something that didn't cross my mind until now. Even the constant rolling firings at BigCos: I find it hard to believe companies would have cut as quickly and aggressively and continually as they did without Musk's...housecleaning?...making them look reasonable in comparison. reply rsynnott 11 hours agorootparentprevAnd if you believe that, you’ll believe anything. reply solardev 12 hours agorootparentprev...was that really his original intention... reply jeffbee 10 hours agoparentprevThe like/follow spams come milliseconds after a post, which any anti-abuse intern could figure out how to block. reply metalspot 11 hours agoprev> CHEQ also provided Mashable with fake traffic data from the entire month of January 2024. TikTok, Facebook, and Instagram all had very similar stats to each platform's respective Super Bowl weekend numbers. Slightly more than 2.8 percent of the 306 million visits sent from TikTok were determined to be fake. Out of the 90 million visits that came from Facebook, a bit more than 2 percent were fake. And Instagram's traffic was only 0.96 percent fake, based on 749,000 visits. The relative scale on visits here doesn't make any sense: TikTok 306M, Facebook 90M, Twitter 759K, Instagram 749K. This seems like marketing for a snake-oil bot detection product masquerading as a political hit piece to get attention. reply boomboomsubban 10 hours agoparent>The relative scale on visits here doesn't make any sense: TikTok 306M, Facebook 90M, Twitter 759K, Instagram 749K. While I agree that this feels like an ad for CHEQ, their scale would presumably be based on how many ads their partners placed on various platforms. They could be buying far more ads on TikTok than on Instagram for some reason. reply porphyra 11 hours agoparentprevYeah the data seems sketchy but then Elon-haters anxious to see X fail will gobble it up as if it were gospel just to satiate their confirmation bias. The truth is probably somewhere in between... there are probably actually a lot of bots on X but not nearly as bad as this report makes it seem. reply Alupis 10 hours agoprevI would really like to know more about how they determine what is \"fake\" versus real. > CHEQ monitors bots and fake users across the internet in order to minimize online ad fraud for its clients. Tytunovich's company accomplishes this by tracking how visitors from different sources, such as X, interact with a client's page after they click one of their links. The company can also tell when a bot is passing itself off as a real user, such as when a fraudulent user is faking what type of operating system they are using to view a website. This does not seem anywhere near foolproof, and seems like it would have a small sample pool to analyze. It seems to me the overwhelming majority of bots are not following links... meaning they are sampling only a specific type of bot behavior and attempting to extrapolate that to the entire platform. That seems fallible. Additionally, the compared sites are not... comparable. It's not fair to compare twitter with Mashable, TikTok, Instagram, or even Facebook. The type of community is different, and the type of content is different. Twitter is much more of a broadcasting/informational platform (ie. shouting into the wind) and therefore lends itself to bot accounts a lot more than your average Facebook account, for instance. This is why for years the Twitter API (aka. \"Firehose\") has been such a contentious issue and other social platforms like Facebook's have not. reply labrador 12 hours agoprevCan confirm. I don't have cable so I relied on the \"latest\" tab in X. Almost all the traffic was for sites purporting to be showing it live, but weren't because they were some sort of scam. reply zimpenfish 12 hours agoparentYou get this for Premier League football games too - hundreds of accounts posting \"WATCH LIVE!\" with hundreds of sketchy URLs. In defence of the current owner, it was like this before he took over and it's not noticeably worse (from my observations - probably is if you do proper analysis, etc.) reply orwin 11 hours agorootparentI think it is noticeably worse, because some content is now absent (especially cybersec, at least in my timeline). The only use of twitter i have left is following US sports, and i guarantee you it's worse. Also, i hve the distinct impression that women left the platform. I think it was already one of the most masculine coded social media, but now with the rebrand (both logo and new name) i would be surprised if the number of non-activist women on it are really, really low. reply saltminer 9 hours agorootparentAt this point, there's only a few people I care about left on the platform. And now that I think about who I used to follow...this certainly tracks. reply solardev 12 hours agoprevIsn't it usually fake even when the superbowls aren't happening..? reply DonHopkins 11 hours agoparentI got a fake AI robocall from Roger Goodell telling me the Superbowl was happening a week later, so I missed it! reply solardev 11 hours agorootparentI got a similar call telling me the Superbowl was today. I watched it, and the 49ers had an amazing victory!! The halftime ads for some crypto thing weren't very good though :( reply DonHopkins 10 hours agorootparentThat's because Gavin Newsom called up Roger Goodell in what was a perfect phone call, and demanded: \"What I want to do is this. I just want to find, uh, four points, which is one more than we have, because we won the game.\" reply happytiger 10 hours agoprev> The majority of traffic from and on x may be fake. There I fixed it. The question should also be, “how much of the general Internet is similarly faked?” How could anyone ever tell if it wasn’t badly done? reply nomilk 9 hours agoprevWhat motivates someone create a bot to click on ads? Is that to waste competitor's ad spend? Or some other purpose? reply smadge 9 hours agoparentMaybe > Ads revenue sharing lets you share revenue from verified user's organic impressions of ads displayed in replies to content you post on X. Accounts can post content, and have bots click on ads shown in the replies, and gain a portion of the ad revenue. reply nomilk 9 hours agorootparentIn that case there's a huge incentive to create bots to click on ads! It could be easily solved if twitter can distinguish between bot and human clicks, and only share revenue on the latter. reply UberFly 12 hours agoprevDoes CHEQ publish any kind of public data on this that reports on various sites? reply refulgentis 12 hours agoparentThe absence of specific information regarding the publication of data reports could suggest that the dissemination of detailed reports or analyses to the public might not be within its standard practice or could be subject to certain conditions. reply nojvek 8 hours agoprevXitter is perhaps Elon’s Xittest company so far. I was a daily user. With so many bots and ads, I only visit when I see an interesting link on the wide web. I still find to hard to digest Elon blew ~40 billion on it and made it worse. reply theshrike79 1 hour agoprevWasn't Musk supposed to end all the bots? How are they getting around the limits? Even Nitter is packing its bags because the site and API are so restrictive. And I had to remove twitter title fetching from my IRC bot ages ago because the API shut down and would've been too expensive for a hobby project. It literally didn't do anything except use the API to grab info about a url to a tweet. reply nomadiccoder 9 hours agoprev> It's a small portion of the relevant data, and it's not scientifically sampled, but it nonetheless suggests a dramatic trend. that seems like an odd statement to make in the article reply brentm 9 hours agoprevProbably bots operated by the X revenue generation skunkworks. reply TheAceOfHearts 11 hours agoprevI'd imagine that Elon and Twitter would deliberately keep bot numbers in their reports to make it seem like the website is doing better than ever. Can Elon / Twitter straight up lie and share fake or misleading numbers? Would that be illegal and result in any kind of legal action, or are would they be allowed to lie and mislead because they're a private company? It's not like these reports would be audited by any external sources. Asking genuinely if anyone is familiarized with this legal domain. reply MattGaiser 11 hours agoparentBeing public didn't stop him from lying about Tesla information. This would generally be fraud, whether public or private, but that is extremely under prosecuted. reply DonHopkins 11 hours agoprevThis is just a lead-up to one hell of an election. reply Nyra 12 hours agoprevTangentially related but I recently signed up for a Twitter account post Elon acquisition and the last step of the process requires you to follow one account to start using the site, the first option is Elon Musk and everything else is random based on your selected interests. Are any numbers on this site not juiced at this point? reply JohnTHaller 12 hours agoparentI get his account in my feed daily despite selecting \"Not interested\" each time. reply travoc 11 hours agoparentprevMySpace did it first. reply BandButcher 6 hours agorootparentTrue, & tom made the first move ;) we didn't have to add him reply kevingadd 12 hours agoparentprevYeah, this is reportedly a measure they put in place after he bought the company because he was unhappy with engagement on his posts. It doesn't exclusively boost him AFAIK. https://www.rollingstone.com/culture/culture-news/elon-musk-... https://www.theverge.com/2023/3/28/23659842/twitter-boost-el... reply NetOpWibby 11 hours agoprevLOL LMAO! Remember when Musk said there weren't any bots on his platform? He had the bot problem under control? reply ilrwbwrkhv 11 hours agoprevHe should just sell twitter and focus on real things. This whole saving the town square free speech thing was fun for a bit and now everyone knows twitter was never the town square and free speech was never a problem. Tesla stocks are tumbling because of his bullshit. reply hn8305823 12 hours agoprev [4 more] [flagged] notwhereyouare 12 hours agoparent [–] I don't think that's the right reason to flag an article reply kortilla 9 hours agorootparentThat’s actually a pretty good reason to flag an article. It’s not much different than any article mentioning politicians. People aren’t able to engage on articles surrounding musk or any of his companies intelligently. reply hn8305823 12 hours agorootparentprev [–] Ok, I unflagged reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The 2024 Super Bowl became the most-watched event in U.S. TV history, driving substantial activity to Elon Musk's platform X.",
      "Despite the surge, cybersecurity firm CHEQ reported a staggering 75.85% of visits to advertisers' sites on X during the Super Bowl as fake traffic, surpassing issues seen on Facebook, Instagram, and TikTok.",
      "CHEQ's data suggests a worsening bot problem on X post its acquisition by Elon Musk, compounded by challenges like hate speech and content moderation concerns, affecting relationships with advertisers."
    ],
    "commentSummary": [
      "The discussion delves into social media platform issues, notably Twitter, under Elon Musk's ownership, discussing fake traffic, bots, blue check elitism, and moderation challenges.",
      "Users showcase varied reactions to Musk's influence, with some lauding the changes while others voice criticism.",
      "Fraud, advertising, user behavior, and the quest for a secure and trustworthy online space are highlighted topics in the conversation."
    ],
    "points": 242,
    "commentCount": 172,
    "retryCount": 0,
    "time": 1708117099
  },
  {
    "id": 39395474,
    "title": "Setting Up Guix on Framework 13 AMD: Challenges and Solutions",
    "originLink": "https://wingolog.org/archives/2024/02/16/guix-on-the-framework-13-amd",
    "originBody": "guix on the framework 13 amd 16 February 2024 10:23 AM guixlaptopframeworkamdfirmwarefsfpraxis I got a new laptop! It’s a Framework 13 AMD: 8 cores, 2 threads per core, 64 GB RAM, 3:2 2256×1504 matte screen. It kicks my 5-year-old Dell XPS 13 in the pants, and I am so relieved to be back to a matte screen. I just got it up and running with Guix, which though easier than past installation experiences was not without some wrinkles, so here I wanted to share a recipe for what worked for me. (I swear this isn’t going to become a product review blog, but when I went to post something like this on the Framework forum I got an error saying that new users could only post 2 links. I understand how we got here but hoo, that is a garbage experience!) The basic deal Upstream Guix works on the Framework 13 AMD, but only with software rendering and no wifi, and I wasn’t able to install from upstream media. This is mainly because Guix uses a modified kernel and doesn’t include necessary firmware. There is a third-party nonguix repository that defines packages for the vanilla Linux kernel and the linux-firmware collection; we have to use that repo if we want all functionality. Of course having the firmware be user-hackable would be better, and it would be better if the framework laptop used parts with free firmware. Something for a next revision, hopefully. On firmware As an aside, I think the official Free Software Foundation position on firmware is bad praxis. To recall, the idea is that if a device has embedded software (firmware) that can be updated, but that software is in a form that users can’t modify, then the system as a whole is not free software. This is technically correct but doesn’t logically imply that the right strategy for advancing free software is to forbid firmware blobs; you have a number of potential policy choices and you have to look at their expected results to evaluate which one is most in line with your goals. Bright lines are useful, of course; I just think that with respect to free software, drawing that line around firmware is not interesting. To illustrate this point, I believe the current FSF position is that if you can run e.g. a USB ethernet adapter without installing non-free firmware, then it is kosher, otherwise it is haram. However many of these devices have firmware; it’s just that you aren’t updating it. So for example the the USB Ethernet adapter I got with my Dell system many years ago has firmware, therefore it has bugs, but I have never updated that firmware because that’s not how we roll. Or, on my old laptop, I never updated the CPU microcode, despite spectre and meltdown and all the rest. “Firmware, but never updated” reminds me of the wires around some New York neighborhoods that allow orthodox people to leave the house on Sabbath; useful if you are of a given community and enjoy the feeling of belonging, but I think even the faithful would see it as a hack. It is like how Richard Stallman wouldn’t use travel booking web sites because they had non-free JavaScript, but would happily call someone on the telephone to perform the booking for him, using those same sites. In that case, the net effect on the world of this particular bright line is negative: it does not advance free software in the least and only adds overhead. Privileging principle over praxis is generally a losing strategy. Installation Firstly I had to turn off secure boot in the bios settings; it’s in “security”. I wasn’t expecting wifi to work out of the box, but for some reason the upstream Guix install media was not able to configure the network via the Ethernet expansion card nor an external USB-C ethernet adapter that I had; stuck at the DHCP phase. So my initial installation attempt failed. Then I realized that the nonguix repository has installation media, which is the same as upstream but with the vanilla kernel and linux-firmware. So on another machine where I had Guix installed, I added the nonguix channel and built the installation media, via guix system image -t iso9660 nongnu/system/install.scm. That gave me a file that I could write to a USB stick. Using that installation media, installing was a breeze. However upon reboot, I found that I had no wifi and I was using software rendering; clearly, installation produced an OS config with the Guix kernel instead of upstream Linux. Happily, at this point the ethernet expansion card was able to work, so connect to wired ethernet, open /etc/config.scm, add the needed lines as described in the operating-system part of the nonguix README, reconfigure, and reboot. Building Linux takes a little less than an hour on this machine. Fractional scaling At that point you have wifi and graphics drivers. I use GNOME, and things seem to work. However the screen defaults to 200% resolution, which makes everything really big. Crisp, pretty, but big. Really you would like something in between? Or that the Framework ships a higher-resolution screen so that 200% would be a good scaling factor; this was the case with my old Dell XPS 13, and it worked well. Anyway with the Framework laptop, I wanted 150% scaling, and it seems these days that the way you have to do this is to use Wayland, which Guix does not yet enable by default. So you go into config.scm again, and change where it says %desktop-services to be: (modify-services %desktop-services (gdm-service-type config => (gdm-configuration (inherit config) (wayland? #t)))) Then when you reboot you are in Wayland. Works fine, it seems. But then you have to go and enable an experimental mutter setting; install dconf-editor, run it, search for keys with “mutter” in the name, find the “experimental settings” key, tell it to not use the default setting, then click the box for “scale-monitor-framebuffer”. Then! You can go into GNOME settings and get 125%, 150%, and so on. Great. HOWEVER, and I hope this is a transient situation, there is a problem: in GNOME, applications that aren’t native Wayland apps don’t scale nicely. It’s like the app gets rendered to a texture at the original resolution, which then gets scaled up in a blurry way. There aren’t so many of these apps these days as most things have been ported to be Wayland-capable, Firefox included, but Emacs is one of them :( However however! If you install the emacs-pgtk package instead of emacs, it looks better. Not perfect, but good enough. So that’s where I am. Bugs The laptop hangs on reboot due to this bug, but that seems a minor issue at this point. There is an ongoing tracker discussion on the community forum; like other problems in that thread, I hope that this one resolves itself upstream in Linux over time. Other things? I didn’t mention the funniest thing about this laptop: it comes in pieces that you have to put together :) I am not so great with hardware, but I had no problem. The build quality seems pretty good; not a MacBook Air, but then it’s also user-repairable, which is a big strong point. It has these funny extension cards that slot into the chassis, which I have found to be quite amusing. I haven’t had the machine for long enough but it seems to work fine up to now: suspend, good battery use, not noisy (unless it’s compiling on all 16 threads), graphics, wifi, ethernet, good compilation speed. (I should give compiling LLVM a go; that’s a useful workload.) I don’t have bluetooth or the fingerprint reader working yet; I give it 25% odds that I get around to this during the lifetime of this laptop :) Until next time, happy hacking! related articles on gnu and on hackers here we go again a baseline compiler for guile state of the gnunion 2020 developing v8 with guix gnu, gnome, and the fsf 3 responses Guillermo says: 16 February 2024 1:14 PM Oh my god, the scaling on Wayland is so painful. I went back to X11. I hear the latest versions of KDE fix the scaling of X11 apps in Wayland so they are not blurred. Congrats on making the wifi work. I was getting low speeds, and random disconnections (with about 20 seconds to reconnect), so my patience ran out and I got a AX210. Now I need to replace the antenna cables to get faster wifi speeds (from what I read on the forums). Thank you very much for sharing your experience. Elia says: 16 February 2024 2:18 PM Thanks for sharing this great writeup! I don’t think I’ll be installing Guix on a Framework anytime soon, but it was interesting to hear about your experience. Your build (hardware wise) is precisely the one I’m intending to buy though, so maybe I’ll give it a shot. panuh says: 16 February 2024 2:37 PM I have the same device, I’m very happy with it. But the BIOS in this particular case is terrible. It would be great to have the BIOS source code, given that the Indsyde approach to password security is beyond ridiculous: When setting up a password for BIOS access, you are asked to choose one that contains capital and lowercase letters, a number and a special character. The password may not consist of more than 10 character - so no passphrases. As if that weren’t enough, there is some obscure password expiry date that can’t be seen or set anywhere. The bios will simply decide at some point that it’s time for you to change the password immediately. When you do that, you will notice that not only aren’t you allowed to use the same password again, the BIOS also remembers a number of past passwords that won’t be allowed either. So no memorizing two alternating ‘strong’ 10 digit passwords either. It’s a mess... There is a community thread that doesn’t give me much hope: https://community.frame.work/t/complexity-rules-for-bios-password-why-moved/40497 Leave a Reply Name Mail (will not be published) Website What's a number between 34 and 42?",
    "commentLink": "https://news.ycombinator.com/item?id=39395474",
    "commentBody": "Guix on the Framework 13 AMD (wingolog.org)234 points by ingve 22 hours agohidepastfavorite164 comments mmcnl 14 hours ago> HOWEVER, and I hope this is a transient situation, there is a problem: in GNOME, applications that aren’t native Wayland apps don’t scale nicely. Transient situations are almost permanent in Linux land. This has been a problem for a long time and I don't see a solution coming in the near future. But maybe I'm wrong (I hope so!). reply stebalien 13 hours agoparentThis is an issue with any software that tries to maintain backwards compatibility, not Linux. Windows has: - Many years worth of different control panels. - Little consistency with respect to toolkits in general. - Fractional scaling issues in applications using older toolkits (e.g., open up the policy editor and notice the blurry fonts). Microsoft is actually giving up here and has been experimenting with ML-based scaling for old applications (an approach I expect we'll eventually see in Linux as well). Apple handles this by breaking compatibility every so often, forcing old software out of the picture. reply Eduard 9 hours agorootparentWindow's backward compatibility is way better though. There are plenty of GNOME extensions and applications that just don't work anymore under modern GNOME Wayland. Example: all the redshift applications and extensions for lowering the screen brightness. reply alwayslikethis 13 hours agorootparentprevI notice that even the blurry scaling on Windows looks better than what we have on Linux. It seems that they have some special algorithm for that. Anyone who knows how it is implemented can chime in here? reply marwis 12 hours agorootparentIt's definitely better than bilinear and bicubic. Looks like Lanczos but with some optimization for ClearType. reply alwayslikethis 13 hours agoparentprevThe only part that's not going to be available for a while is fractional scaling in GTK and old Qt5 software needs to update to Qt6. Everything else is rather quickly falling in place. GNOME in its stereotypical fashion doesn't seem to want to support fractional scaling and doesn't want to provide QoL features like not making XWayland apps blurry, but that's not a problem with Wayland per se. reply M911T 4 hours agorootparentactually... https://www.phoronix.com/news/GNOME-XWayland-Frac-Scaling reply rowanG077 7 hours agoparentprevThis. naively it seems to be a real straightforward to fix. Just scale to the next integer and then downscale. It won't be as pretty but it will be much better then what we have now. Hell make the scale factor configurable so you can get real smooth graphics. reply nonrandomstring 20 hours agoprevHad a VM GUIX \"system\" (really I think its more of package manager overlay) for a while (18 months ago) but encountered some problems. The main feature AFAICS is that it maintains the state of the system and packages all very cleanly via it's own nix build system which is like a snapshot of everything. The sell is that it's very recoverable - in theory you can rebuild from a single file. That's very appealing for VMs that need to be moved about. Unfortunately I needed a couple of things that were available only through apt. Now, Guix said it was perfectly happy to mix package management, (a bit like mixing pkg and ports in BSD) - but don't do that. It all went horribly wrong with confusion over which binaries were running at any time and when I did try to port it, it broke. I like lispy things so will give Guix another try one day, but it needs you to fully embrace its philosophy and no half measures. reply Filligree 18 hours agoparent> Unfortunately I needed a couple of things that were available only through apt. NixOS has approximately the same advantages, but a many times larger package repository. Take a look at that, it might already have your packages. reply rekado 18 hours agorootparentI'd like to point out that Guix is closer to ~52,000 packages than the ~29,000, as we maintain a bunch of things in separate channels that nixpkgs includes in its monorepo (e.g. the 20+k CRAN packages). See https://hpc.guix.info/channels and https://hpc.guix.info/channels/non-free for some popular channels used by HPC folks. reply KingMob 17 hours agorootparentHuh. I think that info should be more prominently displayed. Just a few minutes ago, I decided to put off trying guix until later because it had fewer packages than nix, and was missing several that I need, that nix has. reply b3n 14 hours agorootparentGuix even comes with a Nix service[0] which is easy to enable if you would like to also use Nix packages. [0] https://guix.gnu.org/manual/en/html_node/Miscellaneous-Servi... reply pyrox 12 hours agorootparentNix also has a Guix service itself[^1], if you want to do the same in reverse ;) [^1]: https://search.nixos.org/options?query=services.guix reply pxc 4 hours agorootparentI love that this goes both ways. What other pair of Linux distros mutually offer first class support for using each other's package managers alongside their own?? It's a cute little testament to the fundamental strengths of the functional package management paradigm, as well as helping users on either side of the fence fill some gaps in a pinch and providing more/easier opportunities to compare notes and inspire improvements. reply Guthur 13 hours agorootparentprevBut it didn't have lisp :) reply Cu3PO42 20 hours agoparentprev> via its own nix build system Nix is conceptually extremely similar to Guix, but a different implementation! It uses its own programming language, also called Nix, and has a less strict stance on free software. reply ingenieroariel 19 hours agorootparentGuix uses Nix under the hood (learned this when trying to compile it). Perhaps learning Nix first will let the grandparent be more effective in Guix later. reply rekado 18 hours agorootparentGuix includes a copy of the nix-daemon. It's a relatively small C++ program that manages the store. We reuse it because there was no point in writing a replacement for something as low-level as the daemon. Guix compiles its package definitions down to the same kind of file format that the old Nix daemon understands. (It's a bit funny to me that GNU has a reputation for NIH-ing projects, but when a GNU project doesn't reinvent the spokes on a wheel it also attracts various forms of criticism.) reply janneke 18 hours agorootparentIt can get even funnier when Chris Baines replaces the Nix daemon with a new implementation in Guile. Then, the NIH argument can be reused instead. reply rendaw 14 hours agorootparentprevWhat criticism are you referencing? FWIW I'm a moderately pro-NIH person. reply davexunit 18 hours agorootparentprevGuix forked and modified only the Nix build daemon which isn't user visible. Learning the Nix tools and language will not teach you very much about how to use Guix. reply 0x457 17 hours agoparentprev> a bit like mixing pkg and ports in BSD There is no mixing ports and pkgs in FreeBSD (I assume you mean that one). When you build a port, it builds a package and installs that. You run into big rebuilds/reinstalls when versions mismatch between your ports tree and pkgs repo. Guix/nix can't be mixed with existing package managers, but can exist on the same machine. You're (most likely) going to have duplicates and different versions of the same thing. reply nonrandomstring 16 hours agorootparentYes my previous is mainly with FreeBSD. Your response highlights my mistake. As a Guix noob (but with decades of apt/yum/rpm/pkg kinda experience) I misunderstood how these things interact, and failing to manage Guix was most definitely my fault. Don't get me wrong, I love Guix and the philosophy, but my heart cries out for some Grand Unified Manager of Packages (GUMP), something that can track whether things need building from source, or can be brought in as binary from an apt or rpm, or build those packages as intermediate stages. I kinda thought that's what Guix was going to do. Now I see that a unified package manager is probably unreasonable and that we are headed more toward containerised \"Snap/flatpck\" way of things - which I don't like. reply Arelius 14 hours agorootparent> something that can track whether things need building from source, or can be brought in as binary from an apt or rpm, or build those packages as intermediate stages. I kinda thought that's what Guix was going to do. I mean, that sortof what nix (and I assume guix) is though, right? As long as your ok with the binary cache for a replacement for apt/rpm (with the caveat that sometimes pkgs can be built on .deb or .rpm) reply bayindirh 21 hours agoprev> Guix install media was not able to configure the network via the Ethernet expansion card nor an external USB-C ethernet adapter that I had; stuck at the DHCP phase. So my initial installation attempt failed. It's probably because of the hybrid structure of modern devices. Bare minimum hardware is implemented in solid state, to be able to detected by the OS, for the higher level of functionality (read: packet crafting and I/O), you need the firmware. This \"hybridness\" varies between device tiers though. Higher end Ethernet adapters generally work well without the firmware, but offloading and acceleration features are not enabled without the firmware. Better Realtek and Intel (IIRC) devices fall into that class, and some server class Broadcoms too. reply jckahn 21 hours agoprevMy kingdom for Wayland fractional scaling that doesn’t make my apps blurry. It makes the feature a complete nonstarter for me. reply jacek 20 hours agoparentI use KDE Plasma 5.27 (6 with many improvements is around the corner) on Wayland and fractional scaling is handled very well, nothing is blurry, no hacks required. If KDE recognizes an app that is not Wayland compatible (like most Electron apps), it will just run it in X11 mode (there's a setting for that, enabled by default). See the screenshot of my settings: https://imgur.com/a/VQHD5vT reply phkahler 19 hours agoparentprev>> My kingdom for Wayland fractional scaling that doesn’t make my apps blurry. I'm convinced I don't understand this thing. I feel like it's NOT the compositors job to scale the pixels of an application. That might be a good hack for super high resolution screens with applications that don't know how to handle high DPI but it's an inferior solution. Scaling text at the pixel level is going to be inferior to rendering it at higher resolution to start with. So how does it work? Do we have per-monitor setting for scale factor, and both the DE and the apps need to follow it for idea behavior? And the compositor scaling the apps is a stopgap? Is that it, or is there more to this? reply alwayslikethis 18 hours agorootparentLet me type down what I learned through many hours of research. Under a Wayland compositor, there are basically three types of apps. 1. XWayland apps which cannot use Wayland's scaling protocols, but may be able to use Xft.dpi to scale themselves. Examples include Electron apps when not running on Wayland and later versions of Qt5. These will work without blurriness but they cannot support proper multi-dpi support. The Wayland compositor has no way of detecting whether it actually scales. Some compositors (KDE, Hyprland) let you not scale them, but others (GNOME) will make them super blurry because they are rendered at 96 DPI and stretched to the needed scale. GTK3 notably never supported fractional scaling on X11. 2. Wayland apps which supports integer scaling only. Examples include GTK3, GTK4, Qt5 apps, Mozilla Firefox (minus the experimental fractional scaling feature, which is very buggy). When compositor wants to render at a fractional resolution, the app will render at the nearest integer scale, and the compositor will downsample it. These don't look ideal, but aren't super blurry either, and will work properly with multi-dpi monitors. 3. Wayland apps which properly support fractional scaling via wp-fractional-scale-v1. This is a recent addition to the protocol, so not widely supported yet. The only toolkit that supports it is Qt6, but Chromium (incl. recent Electron apps in Wayland mode) also supports it. Some terminal emulators do too. This results in pixel perfect rendering and looks ideal and works properly with multi-dpi monitors. Downsampling works well on Macs which have high resolution screens such that you mostly use at least 175% scaling. At lower scales such as 125% (e.g. 1080p 13-14 inch laptops) and 150% (e.g. 1440p 14-16 inch devices), the degradation is fairly obvious if you use smaller fonts. This also breaks subpixel rendering (Macs haven't had it for a long time), which is important for making fonts look good on lower DPI screens. In some ways, Wayland scaling is a regression for KDE because Qt5 had proper fractional scaling for some time under X11, which was not implemented in Wayland as the protocol came out after Qt5 lost support. reply marwis 11 hours agorootparent> 1. XWayland apps (...) The Wayland compositor has no way of detecting whether it actually scales Can't X apps just set a window property (XChangeProperty) with their scale factor and have compositor read that? reply p_l 52 minutes agorootparentThe toolkits would need to add that, and some of them had engineering direction incompatible with such an idea. reply mappu 10 hours agorootparentprevThere must be more categories than this because Qt 5 on Wayland can do fractional scaling since some years, you may need to set QT_SCALE_FACTOR_ROUNDING_POLICY=PassThrough. reply alwayslikethis 57 minutes agorootparentSince some years? I don't think so. wp-fractional-scale-v1 was merged only like 1 year ago. The only more recent version of Qt5 is 5.15 and on the release notes I don't see any Wayland changes.. Any reference you have for that? reply kevincox 18 hours agorootparentprevApps that natively support fractional scaling will not be scaled by the compositor. (Well unless they span multiple monitors with different scales IIUC.) Scaling is just a fallback for apps that don't support fractional scaling or edge cases like you drag a non-responding app onto a monitor with a different scale. Yes, you can have a per-monitor scale factor and apps should render at the right scale based on where they are positioned. reply enriquto 21 hours agoparentprevFractional scaling, by definition, will make stuff blurry. Clean bitmap graphics and fractional scaling are incompatible requirements. Proof: consider a pattern of alternating black and white pixels: 0, 255, 0, 255, 0, 255, etc. This pattern cannot be scaled to a non-integer factor without introducing gray colors (i.e., by blurring-out the pattern). If your app produces this pattern, then fractional scaling will blur your app. reply mahkoh 21 hours agorootparentThat is not the case on wayland. Wayland applications with support for the fractional scaling protocol can render without any blur at any fractional scale. This is because the protocol negotiates the size of the underlying buffer. If the client and the compositor agree on the scale, then no scaling of the buffer will happen in the compositor because the client has attached a buffer that is the exact pixel size size of the window on the physical display. It is up to the client how it implements the case you described. E.g. it could alternate between 1 black and 1 white pixel in the physical buffer or it could sometimes make two adjacent pixels the same color. Source: I was involved in the design of this protocol and we had example clients that used this exact pattern. Chromium also supports this protocol without any blur. reply llm_trw 20 hours agorootparent> E.g. it could alternate between 1 black and 1 white pixel in the physical buffer or it could sometimes make two adjacent pixels the same color. This is a very odd definition of \"no blur\". reply mahkoh 20 hours agorootparentIt works in chromium without anything I'd call blur. Here it is at 175% scale: https://i.ibb.co/DtKm69d/image.png reply enriquto 20 hours agorootparentDoes the scaling change the kerning of the letters? This looks horrific! (and, yes, blurry, or \"antialiased\" as is custom to name it). reply mahkoh 20 hours agorootparentNo, that's just the font I'm using. It's the same if I keep the scale at 1 and then zoom in the browser. reply llm_trw 9 hours agorootparentHave you considered using a good font? Having people with no understanding of, or worse no interest in, beautiful UIs making decisions for the display server is a worry. reply darkwater 20 hours agorootparentprevIt doesn't look horrific at all to me reply CyberDildonics 19 hours agorootparentprevThis looks fantastic to me. Also antialiasing is not the same as 'blurry', it is factional values in pixels that have fractional coverage. reply Phrodo_00 14 hours agorootparent> factional values in pixels that have fractional coverage. This is also the definition of blurriness. I think the screenshot looks fine (other than the weird kerning), but I can see how one person's antialiasing could be other people's blurriness reply CyberDildonics 11 hours agorootparentThis is also the definition of blurriness. This is not true in any sense, but it is what some people think when they don't know much about the underlying principles and just see fractional pixels. A blur would be lowering the frequency of an input signal, anti-aliasing is representing that signal more accurately when quantizing it into discreet values. Do some animated aliased 3D renders then try to blur it to get the same result as an anti-aliased version. Look at a checkerboard pattern as it goes into the distance. The pattern eventually converges into grey if it is antialiased because the integral of everything under the pixel is grey as the squares end up smaller than a pixel. Blurring the entire frame gives a much different result. reply M911T 16 hours agorootparentprevyeah the kerning is bad when using that thing. It's bad on my laptop too. reply dpkirchner 14 hours agorootparentprevIMO, it looks like exactly what you'd see if you took pixel perfect rendered text and applied a small-radius Gaussian blur to it. It might look different on your screen, however (monitor settings can affect rendering quite a lot). reply DiabloD3 20 hours agorootparentprevChrome natively scales; however, any bitmap image shown it will be blurry. reply jackdaniel 21 hours agorootparentprevNot everything is a bitmap. Ordinary drawing operations operate on coordinates, so fractional scaling should not lead to any blur (although may miss some pixel-perfect designs). In other words vectors may be scaled with little precision loss, or they may be scaled naively (render to bitmap and then scale the bitmap). reply pxc 4 hours agorootparentIs this how DPI scaling in X11 works? Also, don't you get the same effect (readable large fonts but alignment changes a bit) when you increase the size of your UI fonts, even without changing overall UI scaling? Why are people (apparently) so attached to pixel alignments for OS-native GUIs? reply seabrookmx 8 hours agorootparentprevThis. People seem to forget that you can zoom to 125% in your browser without blur, since the UI elements on the web are vector-based. reply DiabloD3 20 hours agorootparentprevI don't know why this guy is being downvoted, he is correct. You cannot scale things to non-integer amounts without incurring some damage. Blurry/fuzzyness, ringing, screen door effect, etc, you cannot avoid these no matter how smart your scaling algorithm is as long as you're upscaling it to something inbetween 100% and 200%. Nyquist-Shannon is a bitch. Wayland made a decision way back that non-scaling apps can only be integer scaled to avoid this defect. This was the correct decision, objectively. Unfortunately, people still choose to own monitors that have weird resolutions that do not approximate 96dpi after integer scaling. Thankfully, 200% dpi screens (ex: 3840x2160 in 24\", where 24\" are normally 1080p) are starting to become the norm, so someday this problem will go away: you will always be scaling at least 200%, making non-integer scaling artifacts a lot less visible. Also, I think the parent comment that enriquto replied to might be confused and is merely asking for nearest neighbor scaling. This is not part of Wayland (which is just a protocol), and is managed entirely by the WM being used. Given Wayland is trying to enforce integer scaling, WMs allowing choosing nearest neighbor when integer scaling would be preferable in many cases. reply jwells89 19 hours agorootparent> Unfortunately, people still choose to own monitors that have weird resolutions that do not approximate 96dpi after integer scaling. It’s not always a choice unfortunately. I buy displays that are capable of a clean 1x or 2x when I can, but there’s a ton of laptops that still need fractional scaling. Take my Thinkpad X1 Nano. Great laptop in a lot of ways, including the screen (~500 nits brightness, excellent backlight consistency, color, and contrast, no glare) except that it runs at a resolution that requires 1.5x scaling to be usable. Looking at replacement candidate laptops, the only ones that have 2x screens that aren’t a downgrade somehow destroy battery life (e.g. 3000x2000 OLED panel in Dragonfly Elite G4, which docks 3-4h of battery). 1x screens in this category for some reason are all kinda crappy with e.g. dim 350 nit backlights that start to struggle in a moderately naturally well-lit room, which is just goofy in a portable machine that’s likely to get usage in a bright environment. This is one thing that MacBooks objectively do consistently better. reply goosedragons 14 hours agorootparentIt's not really as necessary on PCs because of how Windows does scaling. It's only a problem in programs that just straight up don't support it. And Apple has routinely and still does ship laptops with non-integer scaled resolutions as default (e.g., the 12\" MacBook, the 13\" Macbook Air). reply throwaway11205 21 hours agorootparentprevFractional scaling works perfectly well on Xorg Linux and Windows, but looks blurry on Wayland Linux. Maybe it's not micrometer-scale crisp, but I can't see that. Text on Wayland is very visibly blurred. And it's not just text, but UI controls too. It looks like Wayland just renders the lower integer scale factor and then stretches the resulting bitmap image. That's bullshit. reply jsheard 21 hours agorootparentDoesn't macOS do it the other way, rendering at the next highest integer scale factor and then downscaling to fit the display? If you can't render fractional factors natively for tech debt reasons then that's the least bad way to do it. reply throwaway11205 21 hours agorootparentMacOS way is better, text is blurry, but much less. I still don't like it though, but can use it in an emergency. But there apparently is another way that Xorg and Windows uses. I have perfectly crisp (as far as my eyes can tell) UI and text on both systems at 150% scale (27\" 4K display). > for tech debt reasons I thought Wayland was supposed to fix the tech debt - so now it introduced some that makes bare basic features impossible? reply DiabloD3 19 hours agorootparentprevIt does for any app that can't scale; all modern OSX apps can scale natively. I've been using that trick for integer-like scaling for years to deal with fractional scaling while preserving the quasi-aliasing (\"crispness\") of the source image. However, Wayland does not prescribe any method for non-integer scaling. Any Wayland WM could choose to do the same thing, and it would be hardware accelerated essentially for free. Both X11 and Wayland WMs typically don't use this trick, and neither does Windows. reply darthrupert 19 hours agorootparentprevI have the exact opposite experience: fonts are crisp on Wayland and blurry crap on xorg. reply jorvi 21 hours agorootparentprevIt is exactly (some) Xorg apps that render blurry on Wayland. You are blaming the wrong party for “bullshit”. Xorg scaling sucks, whereas Wayland’s is great. reply binkHN 20 hours agoparentprevI think KDE6, which is just around the corner, is going to be your best bet. That said, some older programs still require xwayland and you will not get too far with these. reply k8svet 13 hours agoparentprevI've never ever noticed blurriness from downscaled integer up-scaled Wayland apps. Blurriness is almost always due to use of xwayland. These are completely unrelated technically. reply pxc 13 hours agoparentprevIf you've got a whole kingdom on offer, you can bite the bullet and buy monitors that all have the same DPI. It's painful, but it's what I've done. reply jamesbfb 10 hours agoprevMy work laptop just rolled over 3 years which means it’s new laptop time. After spending some time convincing the higher ups at work, I have a new Framework 13 coming my way (and not HP whateverbook). I don’t think I’ve been this excited for a bit of hardware in a long time. reply Tmpod 8 hours agoparentUnless the device is malfunctioning or otherwise damaged in a meaningful way, I don't see why it would be time for new a laptop, after only 3 years. I bought mine with the prospect of it lasting at least 6 years (and it's going strong 2/3 in already), but I don't plan on replacing it unless it breaks down or prevents me from working on my stuff for other reasons. Anyway, good choice on the new laptop. My next device will likely be a FrameWork or similar (who knows how the landscape will be then). reply rowanG077 7 hours agorootparentI haven't ever had a laptop USB ports last more then 3 years. In fact my last laptop, an XPS 13, had failing USB-C ports within 1.5 years. I admit I probably connect an USB device 5-10 times per day. reply heyoni 6 hours agorootparentSame here. I use two USB c ports multiple times per day…but never had one fail. reply seabrookmx 8 hours agoparentprevIt's the best laptop I've ever owned (I have the 12th gen Intel model). It's fast, has a great keyboard/trackpad with full gesture support (Fedora GNOME), and sleep works reliably! Battery life isn't a huge factor for me, but if it is for you it's worth considering other devices. I get 4hrs tops, though I think 13th gen and AMD models do better. reply bandrami 11 hours agoprevLinux-libre makes some... odd... choices in deblobbing. The most recent one that bit me was the new rtw driver which includes literal GPL licensed blobs as C files full of binary arrays. Users are absolutely free to modify and reveng these but lin-lib got rid of them because their purpose is opaque. It is, obviously, but that is just poorly documented free software and if we're going to start ripping that out there isn't going to be much at all left. As it is if you want wifi you have to find an older Atheros (pre ath10k) or older realtek (pre 88XX) device, and these are decidedly thin on the ground; I just spent two days crawling through stacks of laptops at Sim Lim in Singapore before I found one. reply teekert 21 hours agoprevIs this Guix Nix but with a different language? How is it different? So far I only knew Nix, now I see more and more Guix popping up… reply bheadmaster 21 hours agoparentIt is inspired by Nix, but with a few key differences: - it is developed under the flag of the GNU Project, so you can expect the same standards of quality of both code and documentation as the rest of the GNU Project - it uses GNU Guile as the main language of the system instead of Nix, which (allegedly) allows you to extend or modify the behavior of the whole system - it uses GNU Shepherd as the service manager instead of systemd All the main goals of the projects are pretty much the same. reply operator-name 21 hours agorootparentThere's also been a greater focus on reproducibility: https://guix.gnu.org/en/blog/2023/the-full-source-bootstrap-... reply bheadmaster 21 hours agorootparentOh yes, I completely forgot to mention that monumental achievement :) For any readers unfamiliar with the importance of reproducible full-source boostrapping, please consider reading the paper \"Reflections on Trusting Trust\" by Ken Thompson [0], the creator of Unix. [0] https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_Ref... reply goosedragons 21 hours agorootparentprevThe other big thing is that being a FSF project they are much more committed to free software than Nix. It's harder to get stuff like Chrome, Steam, Nvidia drivers etc. Not impossible but you're definitely swimming against the current in trying. reply kdtsh 21 hours agorootparentImportant to note that it’s not hard or swimming against the current - you just use the nonguix channel, which has all of those things and is a perfectly acceptable use of the system (it’s what channels are for) - but it’s not in the default installation. reply ducktective 21 hours agorootparent>you just use the nonguix channel Opened up https://gitlab.com/nonguix/nonguix. Literally the second sentence in the project information section, top of the page: \"Please do NOT promote or refer to this repository on any official Guix communication channels.\" Weird thing to say something like this there. Too ideological for someone who wants to get stuff done. reply bheadmaster 20 hours agorootparentIt makes sense to me. They don't want to compromize their mission - which is to provide a fully-free operating system. But they recognize that, at the current moment, non-free software is required for many users. So they maintain an unofficial repository for non-free software, and avoid endorsing or promoting it on the official channels. > Too ideological for someone who wants to get stuff done. The maintainers of Nonguix are mostly the same people working on Guix, so you can expect the same amount of ideology :) reply kdtsh 10 hours agorootparentprevI wouldn’t say it’s too ideological for someone who wants to get stuff done. To get stuff done with Guix, you just go do it and it won’t get in your way - the project channel and the nonguix channel won’t interfere with each other, they’re just different channels. Directions for using Guix in practice (i.e. with some non-free software) could definitely be easier to find, especially if you start green and don’t know what you’re meant to be looking for. But the system won’t get in your way. reply Modified3019 12 hours agorootparentprevThis is what ultimately bounced me away, along with seeing some negative interactions others had. Sure I might not end up with problems, but I’m going to choose to believe them when they tell me the official communication channels will be hostile to me if I do have problems that need the unclean fixes. Which is a shame, because I think there are some technical things which guix does better, but I find it’s more important to not have to worry about saying the wrong things in the wrong places when troubleshooting basic issues like what I get with the nix community. reply rekado 11 hours agorootparentThat's an unfortunate take. \"Unclean\" is your choice of words, not ours. There is no hostility to people who use proprietary software, nor is any kind of judgement of other people's computing purposes and habits warranted. Personally, I avoid proprietary software where I can, but I've found myself in many situations where I'm made to use it. But that's really the point: the free software movement's goal is to create a world in which people are no longer compelled or coerced or forced into being mistreated by proprietary software. You don't need to subscribe to these values, of course, but it is unfortunate if you experience this goal as a personal judgment. Keep in mind that one of the four software freedoms is the freedom to use software for any purpose you want --- judgement of users would be a completely misdirected emotion. Having said that, the Guix project's communication channels simply aren't for proprietary software. The Nonguix communication channels are. The HPC channels are. You will likely get redirected to those communication channels if you ask about proprietary software on Guix project channels. reply davexunit 20 hours agorootparentprevI agree this policy should change but I and many others get stuff done just fine. reply davexunit 21 hours agorootparentprevYeah I second this. I use the nonguix channel for my laptop. Very well supported and easy to use. reply Filligree 17 hours agorootparentprevIt's (deliberately) difficult to discover. I tried GuixSD once, and bounced off it because of the apparent lack of support for my hardware, CUDA, Pytorch et al, which isn't an uncommon story apparently. Now, seemingly there are channels that would deal with this. But when I asked through the official channels, I was told not to do that -- that is, \"don't use nvidia dummy\", which really isn't an option. I was never pointed to the non-free channel. reply kdtsh 10 hours agorootparentI agree with that - directions for how to use Guix in practice, i.e. for someone who isn’t a GNU warrior using 15 year old hardware because you don’t need non-free blobs, need to be easier to come across. reply pasc1878 21 hours agorootparentprevAnother big thing is Guix only works on GNU/Linux and not on macOS and other Unixes like FreeBSD which Nix does run on. reply hardwaresofton 19 hours agorootparentprev> - it uses GNU Shepherd as the service manager instead of systemd I wish they didn't do this part. Despite the furor that was around systemd, it's actually quite nice to use day-to-day. But also, I realize that the FSF has to do what the FSF has to do, glad they're around. reply rekado 19 hours agorootparentThe FSF is not involved in the project. The FSF had no say in what service manager we use. A common misconception is that we somehow don't like systemd. It just so happens that having a thing that's written in Guile (like the rest of Guix) allows for some code sharing. Hey, we've got the initrd in Guile, too. Might as well go all the way, eh? reply hardwaresofton 19 hours agorootparentApologies, I assumed that being a GNU project (which is supported by the FSF), the project identified as an FSF project. > A common misconception is that we somehow don't like systemd. It just so happens that having a thing that's written in Guile (like the rest of Guix) allows for some code sharing. Hey, we've got the initrd in Guile, too. Might as well go all the way, eh? I didn't have this misconception, but let me correct my statement: But also, I realize that guix devs have to do what guix devs have to do, glad they're around. reply amszmidt 19 hours agorootparentprevGNU Shepherd (was aka DMD) predates systemd by many years. And many other “PID 1” efforts. It also had different goals when Wolfgang and I designed it, one might say even incompatible since we were targeting the GNU/Hurd specifically and wanted easy ways to manage translators there on a per user basis. reply rekado 18 hours agorootparentOne thing that hasn't changed, though, is that compatibility with the Hurd is still a major goal. Sadly (but understandably) development pushes forward for Shepherd on Linux and then we aim to make it work on the Hurd, instead of letting Hurd-native facilities guide development. But this might change as there is considerable overlap between Guix contributors and Hurd enthusiasts. reply rendaw 14 hours agorootparentprev- All issue tracking and discussion happens on a mailing list/with custom tooling Also IIRC Shepherd was a simple 1-level service manager like runit. Is that correct? I played around with it for a while and came to the conclusion that it couldn't represent a service graph. reply viraptor 21 hours agorootparentprevSame... Apart from the extreme view on freedom in guix. Loadable firmware? No. Microcode updates for your CPU? No. Not free enough! There's non-guix and all that, but really... if the main project tells you this is now what they want, you will run into issues one day. reply teekert 21 hours agorootparentprevSo is it like “Nix isn’t free (as in RMS) enough?” Is it like Devuan to Debian? Is it like GNU Herd to Linux? (Surely not since it’s Linux ;)) reply davexunit 21 hours agorootparentNo it's not like that all. Free software is a component of it, but Guix is not a fork. It's a completely separate implementation of the functional package management model with many significant differences and advancements. It uses Scheme, a general purpose and extensible programming language, rather than a custom DSL like Nix. Scheme is used for build scripts as well and a lot of code can be shared between the host and build containers. Nix uses Bash. Having one language unifying all the layers makes it easier to hack on the entire stack. Guix has a stronger focus on reproducible and bootstrappable builds and is making notable advancements on the \"software supply chain\" security problem. OS service configuration is done through a graph extension system whereas Nix uses something more like a flat hash of config values. I could go on but I hope this makes the point that Guix isn't just taking Nix and removing proprietary stuff. It's its own thing. reply rekado 21 hours agorootparentprev> Is it like GNU Herd to Linux? (Surely not since it’s Linux ;)) You actually can use the Hurd as a kernel with Guix System. Or run a Hurd VM on your Linux-based Guix System with the childhurd. reply slgeorge 19 hours agoprevFor those that don't know Guix can be used as: * a package manager on top of an existing Linux distribution (think apt or rpm) * a development environment (think Python venv but for any language) * a VM system (like Docker but declarative) * a Linux distribution (similar to Nix) It's a small and friendly community - we recently started an English-speaking online user-group: https://www.meetup.com/guix-london/ Next meetup is Monday next week - please come along! I also did an Intro post to some of the advantages and challenges a couple of years ago: https://www.futurile.net/2021/09/26/guix-alternative-to-snap... reply SkyMarshal 16 hours agoparent> * a Linux distribution (similar to Nix) To nitpick, you mean similar to NixOS. Nix is the package manager, Nix language is the config language that manages the package manager, and NixOS is the operating system created from those two. reply graemep 18 hours agoparentprev\"Guix is a rolling release distribution, the versions of each application are updated continuously. The benefit of rolling releases is that enhancements are available immediately\" The last time I looked at Guix a lot of packages were not up to date, and this included security updates for internet facing things (IIRC, one of the major web servers). reply slgeorge 18 hours agorootparentNew packages and updates to packages come into the archive continuously. For example, in roughly the last 24 hours 40 packages were added or updated - https://git.savannah.gnu.org/cgit/guix.git/log/ . Advantage of this is that you can use new packages immediately and there's no big 'upgrade'. Challenges are that if you were an enterprise and wanted to stick on an 'old' version this wouldn't the right distribution. Guix does receive security updates, and those are added to the archive immediately. I haven't had any problems myself. It's definitely a 'community' project, so you have to enjoy doing a bit of hacking! reply bmicraft 17 hours agorootparent> For example, in roughly the last 24 hours 40 packages were added or updated That's really doesn't feel like a lot. 40 is oughly equal to the amount I get daily for just the stuff that's installed on my arch system. reply graemep 17 hours agorootparentprevI do like rolling release distros. I currently use Manjaro and the ARM version of Arch. However, what I really want something like this for is clients servers - not exactly \"enterprise\" as these are SMEs (not tiny, but not enterprise either). I did find CVE-2024-0985 was not fixed in Guix, but overall so far other things seem to be up to date than when I last looked at it. What is your usage? I suppose the other thing it might really good for is a developer desktop? reply slgeorge 17 hours agorootparentI use it for additional packages on top of another Linux distribution (Ubuntu). This gets me rolling release packages and guix shell which is great for development as each project I'm working on can be completely separated. For 'servers' the nice part is being able to prepare a declarative operating system configuration and play with it locally (VM), then it can be deployed to the remote node and you know it's going to be the same. If something goes wrong it's easily to declaratively roll-back. Here's a nice starter post (https://stumbles.id.au/getting-started-with-guix-deploy.html). The deploy capability definitely needs more hoops to jump through and it's not without rough edges - but I think it's really cool. There's active ARM and RISC-V work - I don't know how rough that would be compared to the well-known ARM ports - ask on #guix if you're interested. reply graemep 16 hours agorootparentThanks that getting started post looks really useful. i have recently started running development stuff in VMs (shared folder so I can use my usual editors etc) and this might be a nice alternative - but the biggest draw is that it is declarative and looks easier to get to grips with than Nix. ARM support is not important to me at the moment - those are just personal things (a tablet, a Raspberry PI) that have limited use anyway. reply yjftsjthsd-h 18 hours agoparentprev> a Linux distribution (similar to Nix) I'm pretty sure all of these are like nix, right? I've used nix on top of other distros, the development environment thing is like nix-shell, nix is happy to build container images, and of course there's nixos. reply slgeorge 17 hours agorootparentYes, I wasn't throwing shade on Nix, I was drawing a specific comparison about Linux distributions. My opinion is that Guix/Nix move the state of the art for Linux distributions forward. So GuixNix are both similar Linux distributions, and different from previous approaches (e.g. Debian, Ubuntu, Redhat etc). Transactional package management and declarative system configuration solve a whole host of problems. Guix (and Nix?) directly integrates configuration management into the OS, rather than as some adjunct piece of tooling (Ansible, Terraform etc). We define the packages, the system, the configuration using the same DSL. Transactions and a declarative approach improve maintainability, reproduciblity and might limit the amount of time I spend messing with different tooling ;-) reply yjftsjthsd-h 17 hours agorootparentAh, yes, in that case we're in full agreement:) There are pain points yet, but I already find it slightly painful to use a non-declarative system... reply colordrops 17 hours agorootparentprevGuix was originally derived from Nix. Guix has different goals. reply yjftsjthsd-h 17 hours agorootparentSure? I'm pointing out that the listed features are more or less identical AFAICT. I grant that being a GNU project affects some of its goals and how it goes about things. reply gigatexal 18 hours agoparentprevWith nix i can install free, non-free apps -- the amount of nix-able stuff is a ton. Is Guix only set to install FSF blessed proprietary free stuff? reply slgeorge 18 hours agorootparentNo. Guix is more similar to Debian, with only 'Free Software' applications in the main archive. For proprietary codecs, firmware and so forth there is the Nonguix channel. Again, this is fairly similar to how distributions like Ubuntu have handled this line in the past. I need Chrome and also have some games loaded using 'channels' - heh heh - another post: https://www.futurile.net/2022/12/04/proprietary-apps-on-guix... A lot of Guix users use Flatpaks. reply rekado 18 hours agorootparentprevNo. While the core repository (we call that a \"channel\") only includes free software, there are no restrictions whatsoever on what you can or cannot install with Guix. Guix makes it trivial to add third-party channels (such as nonguix, guix-science-nonfree, or other free software channels like guix-cran or guix-science) or extend Guix in an ad-hoc fashion. You can also build an entirely private collection of packages if you want; from a file, from a git repository, from a Guile expression, etc. reply b5n 18 hours agorootparentprevhttps://guix.gnu.org/manual/en/html_node/Channels.html https://gitlab.com/nonguix/nonguix reply gigatexal 14 hours agorootparentprevThanks for addressing my ignorance. reply matrss 16 hours agoparentprev> * a package manager on top of an existing Linux distribution (think apt or rpm) Just to add to this: don't just think apt or rpm, also think conda/mamba, homebrew or pipx. Nix, and I am sure guix as well, unify this \"traditional\" distinction between system and user package managers. reply jcul 18 hours agoparentprevI used to attend a C++ meetup in my previous city, but since have moved. I sometimes think about setting something up in my new city. Do you have any advice for getting a meetup off the ground? I guess meetup is still the best thing to use (it was what my old C++ user group used). reply slgeorge 18 hours agorootparentWe're a small group 5-10 people, so it's very informal and friendly. I'm sure Fabio (https://fabionatali.com/) who organised it would have good advice! I'll say that from my perspective the fact that it's also virtual is really great as otherwise I couldn't attend! reply eecc 18 hours agoprevHOWEVER, and I hope this is a transient situation, there is a problem: in GNOME, applications that aren’t native Wayland apps don’t scale nicely. It’s like the app gets rendered to a texture at the original resolution, which then gets scaled up in a blurry way. Unfortunately that's how it works on all Linux distros since \"this will be the year of Wayland\". Last time I tried it was 2020, and I gave up when it turned out there were no alternatives to the off-screen rendering and scaling hack for Chrome, IntelliJ and VSCode. Sorry but my eyesight is already bad as it is. reply alwayslikethis 17 hours agoparentThis is largely a GNOME issue. KDE and Hyprland supports not scaling XWayland and a lot of X11 apps can scale themselves. VSCode and Chromium scales perfectly in Wayland now and IntelliJ can scale in X11. reply zilti 17 hours agoparentprevIt simply does not matter, because all DEs are on Wayland now, everything supports it, and honestly, I haven't seen an application not supporting Wayland for quite some time. Even Chrome is Wayland-native now. reply Filligree 17 hours agorootparentYou don't run Discord, I take it... Meanwhile, it's apparently going to change soon, but Steam games generally use Xwayland. reply Decabytes 18 hours agoprevI really hope one day the Framework laptop will work with the major Linux Distributions with no asterisks. It will definitely require work on both Framework and the Distro maintainer's part, but it is getting better every month. Hopefully if Framework becomes more successful, they will have more say in the hardware manufacturing process, and they will be able to devote more resources to Linux compatibility. I have no need for a laptop now, but maybe a Framework will be in my future if this happens reply natenatenate3 8 hours agoparentI manage to run OpenSuse with Cinnamon desktop on my AMD Framework 13, with a dell dock and 2 odd monitors, the scaling sometimes gets stupid, but that's the only annoying thing I have noticed so far. OpenSuse has been more of a learning curve than any hardware issue, mostly due to myself being used to Debian and Manjaro/Arch. reply nrp 12 hours agoparentprevWe focus on making sure that the most popular distros work smoothly. Currently, Ubuntu LTS and Fedora are the two that we provide official support for, since they are the most popular choices for folks who are either new to Linux or aren’t especially interested in tinkering with their OS: https://frame.work/linux reply aetherspawn 8 hours agorootparentWhoops, accidentally downvoted when meaning to upvote. Sorry! Thank you for focusing on the mainstream distributions. If our company ever transitioned to Linux on laptops (it would be a stretch because of Office365… probably the best we could do is macOS), we would be targeting Fedora. This is because only the mainstream distros have MDM features. reply mixmastamyk 8 hours agorootparentprevWe have a couple of them and were quite happy with the exception of a few minor issues. However the lack of shipping (not beta) firmware updates on a regular schedule has given me pause to recommend framework to others, at least until that gets rectified. reply acomjean 16 hours agoparentprevI’m not sure they have the resources but shipping with Linux installed would be awesome. I’m always surprised at the lack of choice hardware with pre installed Linux. But the framework seems to have a lot of users running Linux so it’s probably a great choice. reply MostlyStable 10 hours agoparentprevWhat do you consider the current asterisks with how Framework works with the major distros? reply e12e 17 hours agoprev> ... good compilation speed ... And earlier: > open /etc/config.scm, add the needed lines as described in the operating-system part of the nonguix README, reconfigure, and reboot. Building Linux takes a little less than an hour on this machine. This is for a system bootstrap of guix/Linux - not just the kernel and modules, surely? reply cmrdporcupine 20 hours agoprevI've been intrigued by Guix for a while; mainly because I like the idea of Nix but was very turned off by its language, when I was exposed to it through a job I had. One thing I liked about Nix was that it was possible to use its packaging and tools without actually using the NixOS. An old employer used this instead of Docker for managing build distribution. Can the Guix stuff be used the same way? reply Slackwise 18 hours agoparent> An old employer used this instead of Docker for managing build distribution. Can the Guix stuff be used the same way? Yes, and, Guix is actually a \"meta\" package manager that understands other packages like Python PIPs and Node NPM packages, so you can define all of that in one build config file. reply tombert 14 hours agorootparentWait, this is a big deal. Will it work with any package manager or only things that it explicitly supports? One thing that has become a persistent headache for me with Nix is trying to get reproducible builds with Julia, due to the fact that the package manager doesn't have direct integration with Nix, and `nix build` purposefully restricts network access. I'd happily move to Guix if they have an elegant way to deal with this. reply Slackwise 7 hours agorootparentSince it's all just Guile Scheme, a full programming language, you can extend it in any way you want, and use the Scheme libraries you want, doing whatever network access you please. reply davexunit 20 hours agoparentprevYou can use Guix as a package manager on top of another Linux distro. I used Guix on Ubuntu for several years. reply cmrdporcupine 19 hours agorootparentOk I'll give it a try. I like Lispy things. One thing I liked about nix in this capacity was the \"nix-shell\" facility, which could be used to create a whole hermetic dev environment with all the right versions of things and all the right environment variables set up, etc. Similar to similar uses of docker but more integrated with the rest of the system -- not as \"sealed off\" from the rest of the distribution, e.g. I could still use my emacs config inside it, etc. reply slgeorge 19 hours agorootparentGuix can create reproducible development environments that are \"sealed off\" from the rest of the distribution. It's called Guix shell and it's very flexible: * Guix Shell https://www.futurile.net/2023/04/29/guix-shell-virtual-envir... I did two specific posts about using it for 'development' environments. You can also 'fix' the environment (think a git hash) and use the declarative configuration to share it with others: * https://www.futurile.net/2023/04/30/guix-reproducible-dev-en... * https://www.futurile.net/2023/10/17/guix-time-travel-dev-env... Hope they help - if you have a play and get stuck the mailing list is really good and I posted about the Meetup further up. reply rekado 20 hours agoparentprevYes, see `guix pack` with its various backends. reply mise_en_place 7 hours agoprevI would use Guix but it just doesn’t seem to be stable nor polished. If you enjoy running on bleeding edge it seems to be the distro for you. reply mrh57 7 hours agoparentHow do you mean? If you're talking not stable as in it's rolling release, then yes that's true although it isn't unique to Guix. But if you mean leaves your system vulnerable to breakages, large unwanted changes, system entropy, or inconsistencies, then surely Guix (and Nix) are more stable than virtually every other distro (and maybe even MacOS or Windows)? I'm also not sure I would say Guix is particularly unpolished compared to other distros. The project is 10 years old at this point, has a very active (albeit not huge) community, and has some of the clearest/most extensive documentation I've read for a distro. Perhaps 5-ish years ago it had some rough edges, but I'd say now they're quite comparable to other mainstream distros. As someone who has been using Guix as my primary OS for the past ~2 years on aPlease do NOT promote this repository on any official Guix communication channels, such as their mailing lists or IRC channel, even in response to support requests! This is to show respect for the Guix project’s strict policy against recommending nonfree software, and to avoid any unnecessary hostility. > This channel does not endorse any non-free application. We believe it is non-ethical, harmful to software development and restricts the users freedom. See the GNU philosophy for a more thorough discussion. So it's Nix, but on extra-hard mode because of some impractical restrictions outlined by the author? reply einpoklum 21 hours agoprev\"Free Software Foundation position on firmware ... is that if a device has ... firmware ... in a form that users can’t modify, then the system as a whole is not free software. This is technically correct but doesn’t logically imply that the right strategy for advancing free software is to forbid firmware blobs\" It may not be the convenient thing for novice users, but it is an important strategy to \"dogfood\" fully-free systems, i.e. obtain them, run them, and make efforts to get them to work as seamlessly as possible. This was done with OS-level-and-up software - for the most part - and we have been reaping great rewards all around from those efforts. If it can be done for the full stack of hardware and software, this will be very beneficial and in some aspects even liberating. Again, joe user is probably better off with a more pragmatic approach at the moment. But joe user can still choose a FOSS operating system, which a few decades ago was not realistic. reply davexunit 20 hours agoparentThe thing is that the FSF's stance is already a pragmatic approach of sorts but the line is drawn in a strange place that doesn't really help advance the cause. The hard line approach of \"all firmware must be free, too\" would render basically every computer unusable, even RMS can see that's going too far to be practical. So, they make an abstraction boundary where \"free\" stops: If the kernel doesn't load the firmware then it's as if the chip is implemented completely in hardware. It's a practical decision because you have to stop somewhere otherwise you can't get anything done. I wish the FSF/GNU-aligned folks would just make a slightly different compromise that makes it a lot easier for people to start using free software distros. Not shipping CPU microcode updates is particularly harmful to users. 10 years ago it was not too hard to run a distro without firmware blobs on a laptop if you were cool with just getting a thinkpad, but modern intel hardware requires a blob for graphics so even that path is closed now. My 2022 thinkpad x1 needs blobs for graphics, wifi, bluetooth, and sound and I tried to find something modern that didn't need them and gave up eventually. reply Fice 19 hours agorootparentThey can't distribute firmware blobs simply because FSF and GNU do not in principle participate in distribution of any non-free programs. Also consider that if a manufacturer can distribute opaque firmware updates to your system, it practically has remote control over it, е.g. Intel can activate a backdoor in specific CPUs when needed by publishing a microcode update. reply davexunit 19 hours agorootparentWhat is more risky to you: Leaving known vulnerabilities such as spectre unpatched or the possibility of Intel adding a backdoor for some unknown purpose that wasn't present in the shipped hardware? reply fsflover 18 hours agorootparentThe former is more risky from the security point of view. The latter is more risky from the freedom point of view. (And, while an FSF supporter, I choose to be more secure.) reply trelane 19 hours agorootparentprevEquivalently, what you're saying is that the world got more dependent on secret, proprietary software, and that therefore those who wish to have freer systems should just give up. reply davexunit 19 hours agorootparentThis is a very bad faith interpretation of what I wrote. reply persnickety 18 hours agorootparentHow about a less bad-faith formulation: the world got more dependent on secret, proprietary software, and therefore the pragmatic way is to concede a little territory to them, so that people can use Free Software at all. Except it's a lose-lose situation: there will never be an end to those concessions, as long as secret software expands under the guise of firmware. And a hardline stance will alienate new generations and starve the movement. The endgame is no Free Software in either scenario. reply trelane 17 hours agorootparentprevHow is \"the world got more dependent on secret, proprietary software, and that therefore those who wish to have freer systems should just give up.\" a \"bad faith interpretation\" of \"10 years ago it was not too hard to run a distro without firmware blobs on a laptop if you were cool with just getting a thinkpad, but modern intel hardware requires a blob for graphics so even that path is closed now. My 2022 thinkpad x1 needs blobs for graphics, wifi, bluetooth, and sound and I tried to find something modern that didn't need them and gave up eventually.\" reply einpoklum 19 hours agorootparentprevWell, it's not like the \"line is drawn\" in the sense of GNU software not working on such systems. They draw it in what's included in the default repository for guix... so that line does not actually impact many people, and those who are impacted by it can still cross the line pretty easily. About the CPU microcode updates... why do you believe they are important? I mean, if your system runs arbitrary code off the Internet, or has thousands of people log in and work on it, then sure, but otherwise, I don't know. reply davexunit 19 hours agorootparentThis story happens all the time: Someone learns about free software, gets excited, downloads an ISO for a free distro and is then disappointed to find that their wifi doesn't work. They get told to buy a USB dongle or something so instead they just use something else that works. It's the most common onboarding problem I'm aware of. Are you saying that the security updates in CPU microcode aren't important because you can't think of an attack vector? Feels like a weak reason to justify not shipping updates. reply yjftsjthsd-h 17 hours agorootparentprev> Well, it's not like the \"line is drawn\" in the sense of GNU software not working on such systems. They draw it in what's included in the default repository for guix... so that line does not actually impact many people, and those who are impacted by it can still cross the line pretty easily. And if what's in the install image results in the OS not bringing up important hardware like network cards, then the software is effectively not working on such systems. And yes, of course it's possible to enable it, but you have to find out yourself because telling people about nonguix in any official docs or communication channels is disallowed. > About the CPU microcode updates... why do you believe they are important? I mean, if your system runs arbitrary code off the Internet [...] We call that a web browser. reply kryptiskt 20 hours agoparentprevSince nearly everybody uses devices that require firmware blobs, the policy has a huge negative impact, it should have a corresponding benefit to motivate its existence. But what can possibly make up repelling all but the hardiest users, who most likely will have to work around it by installing firmware blobs anyway? There are plenty of alternatives that just work. Sure, the users that can run without blobs deserve a reward, so show them a congratulatory message on boot or something. There is precedent for not taking a hard line. Nearly all GNU software works on Windows, for entirely pragmatic reasons. reply tombert 14 hours agoprevI haven't ever used Guix, but I am a huge fan of NixOS, and my understanding is the Guix is trying for a similar experience. The reason that I've never attempted to really use Guix is because I'm a bit worried that I'd have driver trouble because of the LibreLinux kernel and no non-free stuff in the core repos. To anyone that has used both Guix and NixOS, how do they compare? Do you feel like Guix is a better experience if you get past the driver problems? reply umanwizard 10 hours agoparentYou can add the nonguix channel to Guix which gives you normal linux kernel, nvidia drivers, etc. Check out the system-crafters distribution, IIRC it includes nonguix by default. reply Havoc 20 hours agoprev>then it is kosher, otherwise it is haram Big fan of mixing & matching not just on hardware I see reply Gormo 14 hours agoparentThat mixed metaphor popped out to me as well. What if what you're doing is analogous to a glass of wine? It's kosher and haram at the same time! Kosher is to treif as halal is to haram. reply Havoc 13 hours agorootparentTIL. Didn’t know trief reply danecek912 18 hours agoprevFly reply yawpitch 21 hours agoprev [–] > Privileging principle over praxis is generally a losing strategy. Unfortunately it’s also Stallman’s — and thus sadly by extension the FSF’s — entire shtick. reply ergonaught 19 hours agoparent\"Having and living principles is a losing strategy.\" There's a life lesson in asserting that, but the vast majority of people take the wrong one from it. reply pasc1878 20 hours agoparentprev [–] Although if you look at FSFs major projects Emacs and gcc they do run on many OSs including non-free (e.g. Solaris, macOS, Windows, VMS and virtually anything that exists) and other free Unixes. GUIX is much much more restricted. reply rekado 18 hours agorootparent [–] If you mean the Microsoft project called \"GUIX\": sure, it's rather restricted. Guix, however, goes out of its way to support extensions at different levels of its architecture. Not only can you extend it with channels, local files, and with Guile expressions, you can also rewrite the dependency graph of your environment with package transformations, e.g. to swap out any instance of Tensorflow with a CUDA-tainted variant of Tensorflow from the Guix Science Nonfree channel, recursively. reply yjftsjthsd-h 17 hours agorootparent [–] I won't disagree that guix can be made to use nonfree packages. However, the claim: >> Although if you look at FSFs major projects Emacs and gcc they do run on many OSs including non-free (e.g. Solaris, macOS, Windows, VMS and virtually anything that exists) and other free Unixes. >> GUIX is much much more restricted. appears to hold up; https://guix.gnu.org/en/download/ says \"Alternately, GNU Guix can be installed as an additional package manager on top of an installed Linux-based system.\" (emphasis mine) and although there's no explicit statement, https://guix.gnu.org/manual/en/html_node/Installation.html implicitly reiterates that guix is only targeting Linux. Forget running on Solaris, Darwin, or NT; as far as I can tell guix doesn't even care about being usable on any open source unix other than Linux. Edit: Although now that I think about it, they also target HURD... which is no less niche, but does at least imply that it could work on non-Linux unixen. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author details the challenges faced when setting up a new Framework 13 AMD laptop with Guix, focusing on firmware and wifi issues during the operating system installation.",
      "They reflect on free firmware and critique the Free Software Foundation's stance on firmware limitations while providing a comprehensive guide on successfully installing Guix, which involves adjusting kernel and firmware settings.",
      "The author also addresses scaling issues on Wayland, emphasizing smooth transitions for non-native apps, alongside commenting on minor bugs like reboot hangs, highlighting the user-repairable nature and build quality of the device. They plan future improvements for Bluetooth and fingerprint reader functionality."
    ],
    "commentSummary": [
      "The discussion delves into scalability challenges of non-native Wayland apps in GNOME on Linux, drawing comparisons with Windows' backward compatibility.",
      "Potential solutions for fractional scaling in GTK and old Qt5 software are explored, along with features of the GUIX package manager overlay system.",
      "The conversation also analyzes the benefits and drawbacks of Guix and NixOS in managing system and package states while addressing issues like Wayland fractional scaling clarity and GUI scaling across different windowing systems."
    ],
    "points": 234,
    "commentCount": 164,
    "retryCount": 0,
    "time": 1708081515
  },
  {
    "id": 39403935,
    "title": "Exploring Spatial Computing with Apple's Vision Pro",
    "originLink": "https://willem.com/blog/2024-02-16_vision-pro/",
    "originBody": "Vision Pro EXPLORING SPATIAL COMPUTING FEB. 16, 2024 - WILLEM L. MIDDELKOOP Currently, I am overlooking a lake at Mount Hood while writing this. I hear birds in the distance and see the lake calm, with subtle waves and some mist in the distance. Yet, it is fake, as I am sitting on our top floor, a barely furnished room full of items belonging to a family house with two young kids. I am using Apple's Vision Pro to explore what Spatial Computing can be. I am in awe; let me explain in this blog post. Writing this blog post at Mount Hood... ... not! Just our attic full of \"family stuff\" Apple Vision Pro Imagine a set of great earphones for listening to music, offering you private access to high-quality audio playback. The Apple Vision Pro is like that, but for your eyes instead of your ears. You put it on like a set of goggles, and inside, you see a digital world blending with your reality. You can choose to let the real outer world in or filter it out, much like how active noise cancelling works for audio. Apple Vision Pro, like a good set of earphones for your eyes Using some very advanced chips, sensors, and cameras, the Apple Vision Pro headset is capable of projecting virtual objects into your real world, positioning them in a fixed place where they stay as long as you want them there. You can do things like watching a movie on a gigantic cinema display or immersing yourself in your family's photos. Or, you can use the Vision Pro for work, as I do, using a window where I type my text, right at the lakeside of Mount Hood. Other websites and blogs have posted some very in-depth discussions on Apple Vision Pro, discussing the hardware, its advantages, and disadvantages. I recommend you check those out as I do not want to repeat what others have written or said. Some reviews worth your time: Casey Neistat - The thing no one will say about Apple Vision Pro Nilay Patel - Apple Vision Pro review: magic, until it's not Om Malik - My 4 magic moments with Vision Pro Hello World - a selfie of sorts Spatial Computing More than this particular first generation of hardware, I am interested in what Apple is showing the world with its VisionOS software. It's not just a fancy wearable projector; it's interactive! Cameras on the inside track your iris position, i.e., where you're exactly looking. This information is used to enable interaction with the digital world by just looking at things. A subtle tap with your fingers is registered by another set of cameras, all seamlessly integrating an experience that enables you to \"look and tap\" like you would otherwise do with \"point and click\" or \"touch and swipe\". This fundamental interaction model is very well executed; after a few minutes, it feels totally natural, and I have since been wondering why my iPhone or iPad does not respond in the same manner. It's classic Apple magic as all the heavy lifting is done without giving me much (if any) friction. It just works. Bluetooth keyboard and trackpad connected to Vision Pro To get some work done, I have connected a standard bluetooth keyboard and trackpad to the Vision Pro, leveraging my ability to touch type; it all feels very natural. The Vision Pro does not require a computer; it is the computer! It features quite capable Apple Silicon chips, and plenty of onboard storage (mine has 1 TB). You're looking at a fully capable computer system here - and a cup of coffee If you have been reading my blog, you know I have a thing for tablets as they are portable yet very capable. The Vision Pro might be another step forward: it is very portable, yet it offers an entire virtual world for your eyes! It's like being able to carry a massive multi-monitor setup with me; it's bonkers! Not that crazy: display, keyboard, trackpad (note the hovering piece of UI above the hardware keyboard) You can easily arrange a multi-monitor work setup like this It is hard to capture in a 2D photo, but for your eyes, there is real depth in the setup, like those two screens are really there You can look around them, move closer to them, or rearrange them by dragging them through your room The projections appear so natural that my mind is really convinced it can touch things. Mixing the digital and real world is very accessible; it allows you to stay aware of things around you. I like it as it makes you feel a little less \"enclosed inside the computer\". It works for certain workflows, like sending some emails, looking things up, or making a call. The experience gets a little different if you decide to immerse yourself in any of the Vision Pro's virtual environments. Some folks refer to it as \"deep work\", the type of work you need some serious focus for. I find Vision Pro especially powerful for getting myself into the state of flow that is needed for the heavy lifting. I can immerse myself with context (images, logs, code, mockups) and filter out any visual clutter from the real world. These windows represent different views on a particular piece of work - they are big, the tall window in the center is approximately 3 meters high! The 'smallest' window from above is in fact very large; for comparison of scale, I removed the virtual moon surface so you can see the window next to my little daughter's shoes. Imagine a set of windows so big that you can literally stand between them. That is how I like my most powerful Vision Pro setups. You almost become one with your context, seriously. You can create an environment that enables you to really connect with what you're doing. I love walking around the windows, looking at some code or server output, and sort of getting a feel of it being a \"big and working machine\". In a way, it feels like standing in a big machine room. It is really unlike any conventional desktop experience. Again, it is very hard to capture in a 2D photo, but being able to walk around your digital context is simply incredible! These windows are really big, conveying a sense of greatness like a statue of some kind If you're in a virtual environment, the Vision Pro will warn you if you risk walking into something - which can be a real hazard if you're sharing the house with some kids, ha! Conclusion I will continue to explore, learn, and experiment with Vision Pro, but already I'm blown away by the spatial greatness of actually seeing the 3rd dimension digitally. It feels very natural in the same sense as the touchscreen on the original iPhone made me giggle whenever I swiped the \"slide to unlock\" slider. There is a lot to unlock here; come and join me on this next frontier! Talking about hazards... drinking coffee is a real challenge with Vision Pro - ha! Did you enjoy this post? If you found this content useful, consider showing your appreciation by buying me a coffee ❤😋: Buy me a coffee",
    "commentLink": "https://news.ycombinator.com/item?id=39403935",
    "commentBody": "Coding in Vision Pro (willem.com)231 points by willemlaurentz 11 hours agohidepastfavorite308 comments ngokevin 6 hours agoWhen reading about headset experiences, I'd de-weigh any insights that are within 3 months of the first headset purchase. The first few dozen sessions are novel, you get a kick (and social media views) about sharing how completely your life has changed to fit the headset. More often than not, it's collecting dust on the shelf within a few months. Sort of like an expensive blender you were so excited to get, you imagined making super fruit smoothies every morning. Let's see how AVP fares in this regard. reply fullspectrumdev 0 minutes agoparentThe way I see it, by the time AVP is available in Europe, there will probably be sufficient data from US users to give me an idea as to if it’s worthwhile. So far hearing vastly mixed reviews about its “balance” on the users head - when I wear night vision goggles or similar devices I have to use a counter mass at the back of the helmet to make it comfortable. It seems the AVP doesn’t have such a balancing weight? reply dijit 5 hours agoparentprevI love this comment, I think you're completely right. What I love most about this comment is how it caused me to reflect on what I use daily and get some amount of joy from. When it comes to Apple products in particular I use an Apple Studio display at home, I use it for 10hours per day, every day, and honestly I have no complaints, it's probably one of the best bits of technology I use daily. However, it's completely unsexy and has (at least on paper) extremely stiff competition. Not only that it was maligned by the tech media upon release. Yet, it just subtlely benefits my life because the speakers are excellent, the webcam is servicable (the novelty of the fact it tracks you never truly wears off) and the size/pixel density is great. It's weird that I'm waxing poetic like this about such a boring device, but that's kinda my point, your comment made me reflect about all the little tech around me that I use every day that just sort of shifted into the background. Quietly doing its job excellently. reply nunez 2 hours agorootparentInteresting. I have a Studio Display and it just feels like a huge compromise. There are only two 5K displays on the market that can render macOS at Retina resolution on a 27\" panel: this one and the LG Ultrafine, which isn't that much cheaper. My last monitor was a 27\" LG QD-OLED 144Hz 4K display. I LOVED that display, but text wasn't sharp because of the Retina resolution problem. I also have a semi-professional webcam setup and external speakers, so those being built-in does nothing for me. I just want macOS to support variable scaling like Windows has for, what, 15 years at least? reply densh 1 hour agorootparentBetterDisplay lets you set fractional resolutions for any displays on Mac. While this seems like a compromise, it lets you scale the ui to you preferred size independently of the native resolution. I use my 5k display at 85% which is a bit easier on my eyes than native 100%. reply brtkdotse 1 hour agorootparentprev> I just want macOS to support variable scaling like Windows has for, what, 15 years at least? I’ve tried to adopt MacBooks twice over the last three years and both times this has been the dealbreaker for me. I was flabbergasted that MacOS didn’t have better support for it. reply mrbuttons454 2 hours agorootparentprevmacOS does support more than one “virtual” resolution, which I’d consider variable scaling. It’s admittedly less granular than Windows, but it works well enough for me. I have a 27” Acer Predator 4k120 display on my shop Mac, with the virtual resolution set to 2560x1440. reply isametry 1 hour agorootparentThere’s only two scaling factors macOS can display at: 1x or 2x. Anything in-between is rendering at a larger resolution and scaling it back down for the display, resulting in a non-pixel-perfect image. Your shop Mac is rendering the UI at 5120 × 2880, then squishing that down for the Acer display. Which, apart from being more graphically demanding, ends up blurry and aliased. What Windows does is actually just changing the pixel dimensions of the UI elements (in steps of 10% I think?), which Apple has never done. reply filmgirlcw 4 hours agorootparentprevYeah, I agree. I rolled my eyes when I bought the Studio Display because of the price (I got the VESA option so I also had to factor in $200 for an Ergotron LX just so I could use it the way I’d want) and I’m annoyed by its lack of power button, its garbage web camera, and the really that it has no better picture quality than the v2 LG UltraFine 5K it was replacing, despite costing $300 more, but if it broke tomorrow I’d buy another one without a second thought (also the VESA option — also just say no to the Nano Texture option). In fact, I might even get a second one this year because I have 3 Macs (2 M-series laptops and a 2020 5K iMac) and it would be nice to have one dedicated to the laptops instead of having to swap the cable for the TB4 dock. Until/unless Apple does fractional scaling in a way that doesn’t really need pixel doubling to work, I will insist on a 5K display for a 27” monitor and the ASD really is the best game in town. reply theyinwhy 3 hours agorootparentSo what's the benefit of the ASD if it has no better picture quality than the LG? reply sensanaty 3 hours agorootparentIt's always crazy reading those kinda comments. He keeps maligning the product that he had to pay a premium for, comparing it to equivalent products that are cheaper and perform the same, yet he's still going to make his next purchase another, by his own admission, overpriced Apple product. It's mind blowing, I don't wanna reach for the low-hanging quips here but it really is like an Apple iCult type of vibe reply EduardoBautista 0 minutes agorootparentThe Apple Studio Display looks nicer. I would prefer that items that I own in my apartment look nice. Yes, I will pay a premium for that. Zanfa 35 minutes agorootparentprevReading the comment, the impression I got was that while the image quality is similar between LG 5K & ASD, overall ASD is still worth the small premium. The 27\" 5K Studio Display is in a weird category, where they actually have zero competitors. On paper, there's the LG Ultrafine, but I have yet to see one available for purchase in EU (I've seen \"in stock\", but not actually available), not to even mention seeing one in person. So basically you don't really have a choice. reply Aerbil313 1 hour agorootparentprevYou forget Apple is selling trust as well as technology. I have far more trust in an Apple product than an LG one. reply jb1991 2 hours agorootparentprevWhy “he”? reply talldatethrow 2 hours agorootparentMy friend sold boats for over 10 years. He said women love going on boats, but he never saw a woman buy a boat in his life. Women love apple products, but if you look at who is splurging on multiple high end apple computers with multiple monitors it's gotta be 95% male. reply jb1991 1 hour agorootparentBut I was noticing that the username was “filmgirl”. reply adastra22 2 hours agorootparentprevBecause \"he\" is grammatically correct when the gender is unknown. reply growt 50 minutes agorootparentIs it really? Also when the username contains „girl“ it may give a hint. reply jb1991 1 hour agorootparentprevBut I was noticing that the username was “filmgirl”. reply adastra22 1 hour agorootparentFair enough! reply theyinwhy 2 hours agorootparentprevWell The Times They Are A-Changin' reply numpad0 2 hours agorootparentprevsingular they is preferred on Reddit but not here, learned hard way reply dagmx 3 hours agorootparentprevSlightly brighter, much better build quality, much better audio and much better camera. But at the same time, I see the LG is down to $999 (I suspect it’ll be discontinued soon if it isn’t already) and it’s debatable that the studio display is $600 better. It’s definitely $300 better though. That said, it is increasingly more difficult to buy the LG. So it’s become less of a choice. reply tanelpoder 1 hour agorootparentprevI don't have an ASD, but back in 2020 I bought the LG UltraFine 5K from Apple's website together with a laptop and the LG monitor's colors all had a yellowish hue to them. Some Internet searches and possibly their support forums claimed that this is normal, it's just how the monitor works, or something like that (yes I did try various color temperature and config settings - all yellow). Luckily this \"wasn't my first monitor\", so I quickly returned it and went back to Dell that's always been OK enough so I don't ever have to pay attention to which monitor or display setting I'm using. reply kklimonda 1 hour agorootparentprevYou can't drive two LG 5k screens with a single cable, due to it lacking DSC support. reply shaunkoh 2 hours agorootparentprevWhy no nano-texture? reply crimsontech 53 minutes agorootparentIt causes fringing around letters and stops things looking as sharp, which sort of defeats the point of having a 5k display for a lot of people. reply mvkel 3 hours agoparentprevCouldn't agree more. I bought an AVP on day one to be used -exclusively- as a monitor extender for coding. It's untenable. The only way I could avoid significant neck/back pain from the weight was to sit in a reclining chair. That wouldn't be a bad coding setup, but my eye strain was pretty significant, too. I ended up returning mine. This is a necessary 1.0 to build out the app ecosystem, etc. but it's absolutely not ready for full-time use. reply jliptzin 56 minutes agorootparentI almost returned mine for that exact reason until I used it in bed before going to sleep. I have never fallen asleep faster. I know it sounds really strange that a computer strapped to my face helps me sleep but I guess something about the whole immersive environment really helps. I also don’t feel the pressure on my head when I’m lying down. reply dkjaudyeqooe 3 hours agorootparentprev> The only way I could avoid significant neck/back pain from the weight was to sit in a reclining chair Lying on a bed would also do the trick. reply nomel 3 hours agorootparentI've had some great coding sessions on my ceiling, with my quest 3. reply jazzyjackson 2 hours agorootparentprevdoing some pushups also helps reply beloch 2 hours agoparentprevIt shouldn't be surprising that an AR workstation isn't as good as the real thing. If you're at home or at work, the AVP is going to sit on the shelf while you sit at your workstation. While an AR set can, in theory, surround you with code, you can only really focus on one window at a time. Old-fashioned monitors have had a long time to get good at providing that experience. Still, it would be pretty darned cool to be able to take something even half as good on the road in a package that will easily fit in a backpack! I am entirely done with Apple and will not be buying any more of their products, but I look forward to the competition that AVP will foster. reply jimmySixDOF 1 hour agorootparent> Old-fashioned monitors have had a long time to get good at providing that experience. That's part of the problem framing the AVP as a system for 2D floating screens. 2D screens are good at what they do. 360 degree 3D with Six Degrees of Freedom is a completely different surface area and we are still at the Horseless Carriage stage of development of that affordance space. There are a few experiments in what's possible [1] but, for the most part, if all you throw at these devices is a floating screen then the novelty wears off and we have the conversations we are having in this thread instead of thinking about what more is possible. [1] VR Immersive IDE: https://primitive.io/ reply DiscourseFan 5 hours agoparentprevYeah, I really like the Macbook and the iPhone, but basically everything else apple produces is pretty shit these days, even the airpods are just alright (I mean, the audio quality, design, and form factor are amazing, but they are super buggy and there are better wireless earbuds for the price). It would be a great shame if a secure, well designed, easy to use computer loaded with a custom-built, well supported unix-like distro gets trashed because the company that makes it is always trying to \"innovate.\" I don't know if there's much left to innovate in the computer world, there is only so much you can do with ones and zeros. It would be better if they just focused on what works, reliable and secure consumer electronics, instead of trying to be what they were under Steve Jobs. Jobs is dead, he will never come back, and Apple will never be Apple under Jobs again. They should just try to be like the IBM of their space...just there, doing what they do best. An institution, a monolith, but not a \"disruptor,\" as if there are any of those left. reply LeafItAlone 5 hours agorootparent> there are better wireless earbuds for the price Not that I’ve found. Maybe on individual features, but not as a whole product. reply saiya-jin 15 minutes agorootparentThen you didn't care to look for them. Sennheiser, B&W and other traditional companies have buds that sound light years better than greatest tech Apple can currently produce, and do cost more correspondingly. On top of things like much longer lasting battery, much better support for advanced HD bluetooth codecs (I can plug them into anything like some cheap TVs anywhere). But to me overall sound quality is still #1 reason to invest into premium quality. Of course then somebody from A team comes with 'but they integrates greatly with my iphone' argument, which is probably true but not that relevant to above. My Senns integrate effortlessly with my Samsung phone/tv/laptops too, thats kind of baseline in 2024. reply p5a0u9l 5 hours agorootparentprevSame here. Bose ear buds noise canceling and bass are incredible. No comparison to the air pods. But god are they big and ugly. reply Intermernet 3 hours agorootparentI just did a 10 hour flight from Tokyo to Sydney and I wore my Bose QC2 buds for the whole flight. I watched some pre downloaded YouTube videos, and then fell asleep for about 6 hours of actually restful sleep. I cannot express how amazing the noise cancellation is on those things. I was in a 787, seat 29A, window seat, just behind the wing and I could barely hear the sound of the plane. I'm never flying without them again. reply lovegrenoble 1 hour agorootparentprevCan you drop a link to your model of Bose please? reply DiscourseFan 5 hours agorootparentprevTo be fair, when I bought them it was only because I thought they looked cool and I already had other apple devices I knew they would work with; I was willing to pay the premium for the brand, it was a spur of the moment kind of thing and I never factored in competitor value. The funny thing about the Vision Pro was that as soon as I saw it I thought it, and anyone who used it, was really dumb, and I didn't want to be caught dead wearing it--not so for the airpods. reply expensive_news 3 hours agorootparentThis is likely because you bought a pair of AirPods when they were already very popular and everyone thought they were stylish. I bought my first pair of AirPods very close to their initial release and everyone looked at me like I was an idiot for wearing such an ugly and expensive device that couldn’t really do anything better than a $20 pair of earbuds. Do you remember everyone making fun of them at the announcement? It wasn’t until around 2 years later that AirPods were widely worn and accepted. I’m not sure Vision Pro will go through the same adoption curve, but I am not confident that it won’t happen by late v2 or v3. reply andoando 3 hours agorootparentprevWhy does anyone like wireless headphones? 3 different pieces to track, have to constantly be on charge, and pricey at that. I ended up finally buying some iems reply crimsontech 32 minutes agorootparentIf I’m on a call on my computer I can get up and make coffee without leaving the call. If my phone rings I don’t need to switch headphones, they switch devices automatically. I have high quality headphones and IEMs, the headphones I use if I’m _really_ listening to music, maybe a couple of times a week. The IEMs have been in drawer for months without use. The wireless headphones I use every day all day. It’s convenience. reply mdhen 1 hour agorootparentprevand Letshuoer S12 Pro's absolutely blow everything else away in that space and they only cost about $100. Best value in ear phones, easy. reply bowsamic 3 hours agorootparentprevA lot of younger people never take them out reply karim79 5 hours agoparentprev> More often than not, it's collecting dust on the shelf within a few months. I can't speak for the Apple Vision Pro, I'd love to try one, to be honest. But I can agree with your comment. Such has been my experience with the two headsets I own - the Meta Quest 2, which I think is really great and serves many purposes, and the PSVR 2 headset, which is phenomenal for gaming, but pretty much useless for anything else. Both headsets have had a short-lived \"wow\" factor to them, without a doubt, but sadly, the novelty tends to wear off within weeks. I haven't used my Quest 2 since early 2023 (I think) and the PSVR 2's last use was for a game called Pavlov, which is simply mind-blowing, despite which, I think the hassle of headsets is problematic. For real gaming I go back to couch gaming with the Xbox or PS5, and for computer things I use computers. Funny thing is, I keep meaning to do more with the PSVR 2 but now it's actually so dusty that it puts me off, it needs a full cleaning at this stage :). Edit: Meta Quest -> Meta Quest 2 reply nox101 2 hours agorootparentFor me, VR comes down to content. Because the market is too small, there is none. There's few high end titles. HL:Alyx, Horizon: Call of the Mountain? Sometimes you get a port but then it wasn't originally designed for VR and it usually shows. There's zero on the Quest 2/3 because it's a mobile device and it's not up to it and at least in my experience the link is not up to it either. I'm not saying there are not some good experiences there. Several rhythm games are fun, a couple of games designed for low-power mobile devices. But the fully immersive high quality graphics games are so few an far between. If there were more I'd keep playing. reply imadj 2 hours agoparentprev> collecting dust on the shelf within a few months Not even that, apparently many customers are already returning them[1]. Most of the hype, as usual, is tech reviewers who want to stay relevant. Early adopters will naturally chase any and every opportunity to make use of them, but in the coming months as novelty wears off, more issues arise, you get the (The ecosystem is just not ready yet)™ and most will be listed on the secondhand market. [1]: https://www.theverge.com/2024/2/14/24072792/apple-vision-pro... reply roland35 5 hours agoparentprevI still love making smoothies, it just turned out that they are way too many calories :( reply babyshake 5 hours agoparentprevThere will be an inflection point with devices like Vision Pro, where you'd rather do things you do right now with a laptop, tablet or TV in the Vision Pro instead of with those other devices. Then the other devices will be the ones gathering dust. The hard part is being able to know when that inflection point comes. But I think there's a very good chance that 10 years from now, it's already behind us. reply lolinder 4 hours agorootparentThere might be an inflection point like what you describe. It's also entirely possible that this is an evolutionary dead end like the voice interfaces that were the Next Big Thing a few years ago. We've grown so accustomed to viewing technology as a steady progression of improvements that it seems natural that the thing that Apple is pushing today will be universal in 10 years, but there's no guarantee of that, and there are reasons to be skeptical that we'll look back at this as the next big leap. For myself, I'm keeping my eye out for technology that is less intrusive into the rest of my life, not more immersive. reply eloisius 4 hours agorootparentMy thoughts exactly. There are almost no things I do on a daily basis where I want to go through a cognitively expensive process of changing modes into an immersive experience. Even getting my laptop out and opening it and logging in feels like too much for a lot of things, and I’m glad that I can use my phone to tap out a short email while half paying attention to my lunch, commuting, etc sometimes. Maybe when they can shoot photons directly into my cornea without an intrusive set of goggles that take me out of the real world. reply skydhash 2 hours agorootparentI keep hoping for an expanse experience. Where computing is truly connected (standards and fast wireless transfers). Apple ecosystem is moving towards this but they are so closed it feels more like autobus than bicycle for the mind. Linux is still not an option on the phone form factor. And the hardware are still very opaque. reply tw04 3 hours agorootparentprevThere will be an inflection point where you’ll want all of your media to be 3D. Things you watch today in 2D you’ll want in 3D and then your old 2D television will seem like a relic of the past. The entire industry is onboard, the shift is inevitable. reply makeitdouble 1 hour agorootparentThat would be reasonable if there past inflections similar to that were we wanted all our media to move to the higher \"dimension\". For instance we've seen the arrival of movies and tv, books became moving visual content, games expanded from boards to immersive experiences etc. But the lower dimension media never disappeared nor significantly got irrelevant. We're effectively reading more words per day than any of the previous generation, podcasts are booming, web comics etc. are thriving. TV and movies were just an addition to the media landscape, I see AR and VR in the same light. reply eptcyka 2 hours agorootparentprevThe production costs of creating 3D content that is actually making use of the extra dimension will be cost prohibitive for lots of types of content where 2d is used today. I really wouldn't care much for lots of YouTube content I watch being provided in 3d, half of it I already just listen to. As an example, look at how much more detail needs to be added to a room in a VR game versus a standard 3D game - since you have more freedom to poke around and look at things from more angles, stick your head in things, the expected level of detail is much higher. Just adding VR to existing games can make for a good experience, but VR native content will be more expensive to produce. reply dkjaudyeqooe 3 hours agorootparentprevHaving something strapped to your face, that you have to put on and take off, is never going to be a substitute for working in open space unhindered. reply 8n4vidtmkvmk 2 hours agorootparentWhy's that? They can get the weight down to barely more than glasses, and with good pass through video and battery improvements... You'll be pretty unencumbered. Certainly not more encumbered than being glued to your laptop on a desk. reply dkjaudyeqooe 2 hours agorootparentWell it's hard to debate fantasy products given the circularity: \"if they create a product that avoids all these problems, then it won't be a problem!\". I've not used a laptop (or computer) at a desk for decades, I prefer lying down, which works really well. I do wish there was something between a laptop and a phone though. Interfaces need to improve or innovate, but I don't think VR headsets are the answer. I haven't even touched on the nausea. reply 8n4vidtmkvmk 2 hours agorootparentI don't think it's a circular argument. Bigscreen VR is already 127 grams. This is doable in the next decade I think. The question is if there's a better form factor than something you wear on your face. reply pompino 2 hours agorootparentprevI think we're socially primed for it right now - at-least here in the US. Unlike other cultures where you're living with a large family or extended family or your parents, etc, life in the US is getting to be more and more solitary (IRL, people are still forming social connections online) for young people. That makes it very easy to adopt this type of tech. In other cultures wearing it around the house will just make you look like a weirdo. reply Turskarama 4 hours agorootparentprevHere's my prediction for this: you need to have input with the same fidelity and ease of use as a mouse and keyboard. The mouse input analogy is probably pretty close with eye tracking, but text input? I can't even imagine what it will look like to make that work as nicely as real touch typing. reply nilkn 4 hours agorootparentI got an acrylic tray [0] that holds a keyboard and trackpad and can comfortably sit on my lap to use with the Vision Pro. It’s truly marvelous and makes the VP feel like an actual laptop replacement, with the only real problem being that visionOS has too much iPadOS heritage and not enough MacOS. [0] https://www.amazon.com/dp/B0CCS71K6P?psc=1&ref=ppx_yo2ov_dt_... reply Turskarama 3 hours agorootparentSure, but one of the big upsides of vr is the ability to move around a space. If you're just sitting at a desk (or on a couch) anyway then you might as well just have a couple of huge monitors. In the case of the vision pro it would even be cheaper to use several large monitors instead. reply nilkn 2 hours agorootparentThe ability to move around while I’m actively wearing it is not particularly compelling or relevant to me. I didn’t get it for room-scale VR games, and I tend to take it off when I get up anyway. I find it really amazing that I can set up a huge workspace on-the-fly pretty much wherever I want, including the couch. I have a very slick dedicated home office I could be in right now but I actually enjoy using the VP from the living room more for anything other than serious 9-5 work for my employer. reply WanderPanda 4 hours agorootparentprevIt will probably look like a keyboard. Keyboards are so lindy at this point they didn’t even adjust the layout (from that path-dependent mess we are dealing with), let alone the form factor reply tiagod 4 hours agorootparentprevYou can use a macbook keyboard and trackpad seamlessly with the headset reply steve1977 4 hours agorootparentprevif putting on the Vision Pro is as fast and seamless as grabbing a tablet or opening up your laptop I guess. reply bryanrasmussen 3 hours agorootparentwhen it's a pair of sunglasses with the Facial Studio Code already opened to your project, in other words. reply 8n4vidtmkvmk 2 hours agorootparentToo bad \"Visual\" is already taken.. maybe Vision Studio Code. reply anonzzzies 3 hours agoparentprevI ditched my laptop for the xreal well over two years ago. I will never go back. This looks a 1000 times better but too expensive for now. reply LoganDark 5 hours agoparentprevWith ADHD, it doesn't even take 3 months. Give me one day and I'll never touch something again. Hell, some of the things I buy, I don't even use a first time, because having stuff doesn't actually help with motivation reply silisili 1 minute agoprev> You can choose to let the real outer world in or filter it out, Yeah I do that by putting my monitor near a window, and using my special ability to move my ocular organs to decide what I get to see. It saves me 3k dollars and having to wear some awkward device on my head. reply jm20 10 hours agoprevI tried to use the Vision Pro for work, and I'm not sure if it was just my eyes or what, but looking at code inside of that thing was just...exhausting. When I took it off, I looked at my regular monitors with a newfound love. I'd love for this thing to reach it's full potential as this 'work from a mountaintop, but really your garage' device, but I feel like until the resolution gets to the same as existing monitors (no small task, I know) it's just...not as good for the vast majority of use cases. reply iamjake648 10 hours agoparentI returned mine today after attempting to use it for work for a few hours at a time over the last week. I felt the same eye strain with my Mac as a mirrored display. Zoom calls were cool, but nobody could take the Persona seriously. After a few days the eye strain seemed to get worse and worse, until yesterday it give me such a bad headache I decided that was enough. reply cromka 8 hours agorootparent> Zoom calls were cool, but nobody could take the Persona seriously. This is going to be another of their socially awkward gimmicks like Memojis they will double down until they inevitably fail. I really feel like Apple actually just doesn’t feel it and every time they’re pushing their weird geeky ideas onto their users they loose a bit of coolness factor. And if kids decide Apple got too cringe, while someone else manages to use that to spin their momentum (think e.g. Nokia respawning riding the 90s sentiment wave), they may actually start to seriously struggle. reply haswell 6 hours agorootparent> This is going to be another of their socially awkward gimmicks like Memojis they will double down until they inevitably fail. I don't necessarily agree. Some have talked about how the weirdness starts to fade after getting into a conversation and focusing on the discussion or collaboration at hand. It seems that once the brain has adjusted a bit, it can start to fill in for the badness somewhat. The feature clearly has a long way to go before it's good, but I think it's premature to dismiss it. Future iterations will only improve, so if some people are finding some success with it now, that will only grow. reply cromka 24 minutes agorootparentNo, I absolutely get that. I still remember how talking to yourself wearing wired headset on the street was weird. But it’s been over 5 years since Apple started pushing Memojis, they still continued as recently as last year and they have little adoption still, as far as I am aware. Those who watched WWDC remember how cringey those Memoji bits were, I specifically refer to that aspect of their being increasingly out of touch. reply elif 4 hours agorootparentprevI imagine version 2 will likely have uncanny-valley-crossing AI filters to make it indistinguishable from your real face, background and expressions. reply gffrd 6 hours agorootparentprevI’ve wondered the same. Apple has been a “given” for 20 years because they somehow keep shipping great stuff and avoid the Microsoft trap of looking like total dorks by existing in an echo chamber. Even if they made a small misstep or had an awkward moment in a launch announcement, it was seen as endearing and forgivable. But there seem to be an increase in moments where Apple comes across as behind the curve, or not as aware of where the public is at relative to them, compared to then. reply leptons 6 hours agorootparent>avoid the Microsoft trap of looking like total dorks by existing in an echo chamber. Lolwhut?? Microsoft wishes it had a fraction of the echo chamber Apple fans create. It's what Apple is known for. >Even if they made a small misstep or had an awkward moment in a launch announcement, it was seen as endearing and forgivable. Uhhhh... \"You're holding it wrong\" was an absolute unmitigated PR disaster for Apple. It was one of the worst kinks ever in the \"reality distortion field\". People were rightly pissed. It was smug and stupid, not endearing. reply RajT88 5 hours agorootparent> \"You're holding it wrong\" I cringe every time I hear someone say this to malign stupid users. Yes, I hear it at work. Some people legit only read the headlines about that story, not the articles. reply rpmisms 5 hours agorootparentprevYou're misunderstanding. Apple exists in an echo chamber, Microsoft wishes they did. reply Kon-Peki 5 hours agorootparentprev> they’re pushing their weird geeky ideas onto their users This thing is like $5k all-in and even then you have to get on a waiting list. I wouldn’t really say that they’re pushing this on people. reply cromka 12 minutes agorootparentNot talking about the device itself here. reply ApolIllo 7 hours agorootparentprevDamn. My family use Memojis heavily! reply amiantos 6 hours agorootparentDon't worry, most people on Hacker News have no real idea how normal everyday people use tech products. This person thinks Memojis are unused, but they just lack perspective. reply maximus-decimus 6 hours agorootparent> how normal everyday people use tech products What do you mean? Obviously normies all browse the web in emacs and write their own plugins in elisp. reply gffrd 6 hours agorootparentprev100% of my social circle works outside tech, and nobody uses memojis reply jnaina 6 hours agorootparentThe world is bigger than just the US of A. Memojis are big among the teen set elsewhere. reply cromka 10 minutes agorootparentIf the amount of internet content regarding Memojis is to be a judge, Memojis might as well not exist at all. From my broad circle in the US and Europe, I know one person who sends one maybe once a quarter. fnordpiglet 6 hours agorootparentprevThey’re wildly popular with the kids in my orbit especially 6-12 reply mvkel 3 hours agorootparentprevWhat's the alternative? You have goggles strapped to your face. Personas are the only thing you -can- do to have any sense of presence on a call. It's not perfect. It's not even good. But it's better than nothing. reply cozzyd 5 hours agorootparentprevI've never heard of memojis before... reply tyfon 54 minutes agorootparentSame, and now I feel old again : - \\ reply threeseed 5 hours agorootparentprevBy definition it isn't a gimmick. You either have some rendered 3D model or you take the headset off when making a call. Physics dictates that those are your only two choices. reply 38 4 hours agorootparentprev> coolness factor Apple hasn't been cool for a long, long time. Everyone has an iPhone so it's no longer special. It's just a waste of money when you can get basically the same android phone for half the price reply crazygringo 6 hours agorootparentprev> After a few days the eye strain seemed to get worse and worse, until yesterday it give me such a bad headache I decided that was enough. Curious if you've ever gotten your vision tested? I don't need glasses in everyday life, but I did go to an optometrist and got a pair anyways after I got annoyed one night that a friend could read a faraway sign and I couldn't quite. They make things a little bit sharper but not that it ever makes a difference for anything I actually need. But then I discovered that if I wear them, zero eye strain in VR. Without them my eyes hurt after 20 minutes. With them, I can use VR for hours, zero problem. No idea why. And I can't seem to find much information on it, but I asked my optometrist and they said it's a whole thing -- people who wear glasses sometimes not to see better, but to reduce eye strain and headaches. reply fossuser 5 hours agorootparentOther vision issues can be relevant too. I have a very slight lazy eye - I can still see 3D video but my stereo vision is worse than average. I suspect it affects the eye tracking because for me it feels a little tedious and imperfect, but others don’t seem to feel that way. reply astrange 5 hours agorootparentYou may be able to help this with toric contacts. The inserts don't do prism correction so can't help with that specifically. reply valvar 6 hours agorootparentprevInteresting. I'm also nearsighted, so I've always assumed that I don't need glasses when wearing VR headsets. It's easier to just take them off before putting on the headset (MQ3) and I've not noticed a difference in clarity — but I do experience eye strain and visual exhaustion if I wear the headset for too long, so it might be worth comparing longer sessions with and without glasses. reply astrange 5 hours agorootparentVR headsets work like you're focusing at least a few feet away, so if you're nearsighted you need vision correction. reply barkingcat 5 hours agorootparentprevThe problem with the vision pro is that you can't wear glasses to use them, and neither does Zeiss make all the prescriptions available. reply astrange 5 hours agorootparentYou can wear contacts for VR as long as they're not colored. (Well, if there's no eye tracking even that's fine.) reply SkyPuncher 3 hours agorootparentWith my Xreal airs, something is just always slightly off with focus. It's fine for movies and games, but it sucks for reading text. reply rubicon33 6 hours agorootparentprevCould be the motion blur. Vision Pro is in this really weird cross section of insanely good visuals but really bad motion blur. Generally a little motion blur is OK, but the better your visuals get, the worse and more apparent motion blur can be. Personally I found AVP to be most draining if I'm moving my head around a lot and experiencing this motion blur. If I'm just looking at the screen in front of me, I get fatigued less reply testfrequency 10 hours agoparentprevI had the same exact experience. It’s felt more like I was playing a simulator game of myself coding, and it was not enjoyable at all. I often wonder how people who actually work at fast paced places as engineers are claiming to be more productive with this strapped on. If I just ingested and read email and slack messages all day, or talked in zoom all day, sure (maybe), but I don’t. reply awskinda 4 hours agorootparentI’m not extremely productive, but I have 2 use-cases I’m excited about. 1. Having more screen real estate in my small home office. I often dive into spaghetti code, and seeing more of it helps me maintain context. 2. Working in my RV. I can’t take my extra screens with me (it’s a multi-use family RV, so I’m not mounting anything. Plus, there isn’t room.), and I’m so, so excited about having more screen real estate in there. We lived/worked in it for 3 months last summer, and it was really nice coming home to more screens at the end of the trip. reply reneherse 4 hours agorootparentSimilarly, I'm intrigued by the possibility of using it for coding whilst aboard a sailboat, where having multiple large physical screens isn't a possibility due to limited space and lack of suitable mounting surfaces. Very curious to see how tolerable the UX is in an environment that's almost always experiencing some degree of motion independent of the user. reply randmeerkat 4 hours agorootparent> Similarly, I'm intrigued by the possibility of using it for coding whilst aboard a sailboat, where having multiple large physical screens isn't a possibility due to limited space and lack of suitable mounting surfaces. It’s amazing to me what people will do to avoid being present in the moment. You’re on a sailboat, enjoy it, smell the ocean, feel the water on your face, breathe deeply and take it all in. If you’re just going to strap goggles on your face to blind you to the beauty around you, then why did you even get a sailboat to begin with? reply shuckles 3 hours agorootparentLife on a sailboat is mostly pretty boring. reply leetharris 5 hours agoparentprevSame. I returned it the other night because I finally just gave up on trying to make it comfortable. My body was just rejecting it. I could make it 20-30 minutes. It's a bunch of factors. Heavy, lots of pressure on a few specific points, the dangling cable to a slippery battery which I had to leave plugged in all day, the grainy passthrough... But most of all it's just too heavy. And connection would sometimes get janky between Mac and Vision Pro. I've got high hopes for generation 2 and 3 but it needs time to cook. reply chenxi9649 10 hours agoparentprevWhen I don't wear contacts for a while(just normal glasses), wearing them throughout the day also makes me a bit \"tired\". Even though the \"resolution\" is basically the same. But after wearing them consecutively for a few days, it becomes like the same as glasses. I wonder if there's something similar here going on. Since we rely so heavily on vision for everything(Especially balance). Any difference to our normal perception will cause \"exhaustion\" of sorts. But maybe wearing it continuously for days can cause our body to adapt to it?(Which, obviously is impossible for AVPs) reply g-b-r 6 hours agorootparentThat might have to do with the eye \"enlarging\" a little to make space for the lenses, or maybe more likely with you learning to \"lubricate\" the lenses with tearing (pure conjecture, I last used lenses fifteen twenty years ago) reply g-b-r 6 hours agoparentprevI haven't tried them, but I imagine that despite the high resolution, text that is badly aligned (and probably constantly imperceptibly wobbling) strains the eyes. The software should probably force text to be aligned on whole real pixels, even if that detracts a little from the realism. And the best would probably be to keep virtual screens completely fixed until you move by a certain, largish degree (as an option). Then again, maybe this has nothing to do with the straining. reply thwarted 5 hours agorootparentI've watched a few reviews where the claim is \"multiple 4k monitors\", and even on Apple's site it says \"More pixels than a 4K TV. For each eye.\". But any virtual monitor is going to be scaled down, and with \"spatial computing\" being the desired interaction, it's not going to be projected at a fixed point on the embedded screens. Sure, when you have a 4k monitor across the room, it's smaller because it's further away, but the full resolution is there (reality is much higher fidelity than \"retina display\" ever was and even the Vision Pro is). When a virtual display is projected into a space further away, it's going to take up fewer pixels and be down-sampled. It's kind of annoying that the term \"4k\" is being used to refer to \"physical space the display takes up\" or \"size reported to the operating system\" rather than the physical pixel density. reply curiouscavalier 6 hours agorootparentprevThat's an interesting point. You potentially also have alignment issues with the window being placed spatially, i.e., rendering text that is not perpendicular to the plane of the screens. When moving my head in the AVP I'm moving the screens, unlike when I move my head to look at a different part of a monitor. reply g-b-r 6 hours agorootparentYeah with slanted screens it might well be best to not align anything, at least beyond certain angles reply dagmx 5 hours agorootparentprevText in natively rendered apps is perspective corrected before rendering and incredibly sharp as a result. It’s been mentioned in a few interviews in passing. Text in streamed displays from a Mac may suffer from pixel misalignment. reply saagarjha 6 hours agorootparentprevWindows aren’t perpendicular to your view, though. They shear because of perspective. reply g-b-r 5 hours agorootparentYes I was mostly considering exactly perpendicular windows; it might be beneficial to let go of some realism and perspective to work more comfortably reply crooked-v 6 hours agorootparentprev> aligned on whole real pixels Remember that you're working with two screens, not one, and they have to have coordinated projections that will also depend on the user's IPD. reply fnordpiglet 6 hours agorootparentInterpupillary distance fwiw https://en.m.wikipedia.org/wiki/Pupillary_distance reply g-b-r 6 hours agorootparentprevWell ok, but is it a problem to have the text aligned on both screens, if the user is fine with some \"stuttering\" when moving or with keeping the virtual screen fixed? reply crooked-v 5 hours agorootparentIt's a problem because a given piece of text isn't going to match the same pixel boundaries on both screens. reply mlindner 6 hours agorootparentprev> The software should probably force text to be aligned on whole real pixels, even if that detracts a little from the realism. I think that'd be difficult given that pixels are effectively \"non-rectangular\" given the warping from the lenses. reply g-b-r 6 hours agorootparentWell they might be non-rectangular, but text aligned on their borders should still be sharper than misaligned text, no? reply astrange 5 hours agorootparentHigh DPI monitor text effectively can't be misaligned, especially since Apple's text rendering always dilated characters instead of trying to fit them onto pixel grids. reply frontman1988 5 hours agoparentprevWho knows what will be the long term consequences of using these devices on the eye. Better to be wary reply astrange 5 hours agorootparentThere shouldn't be any as long as you're not a child. Children need to spend a lot of time outside to avoid developing myopia. reply judge2020 10 hours agoparentprevWere you using macos virtual display? Using the passthrough to look at monitors is passable but not suitable for even a few minutes of use. reply joshstrange 10 hours agorootparentNot you you responded to but I tried both. I mean I never seriously thought the passthrough would be good enough, and it wasn't, but it was just barely legible and useful when I was trying to pair my AVP to my MBP. But I really tried to like MVD and I just couldn't do it. It wasn't clear enough and felt like an added \"tax\" on my mind, also I felt very limited compared to when using my external monitors. reply prawn 3 hours agorootparentprevIs the virtual display feature as it's presented now likely to be a stopgap or fallback? A bit like emulation or Rosetta apps when Apple silicon was new, or running iPhone apps on an iPad. Those were things that seemed core when each was first introduced and then quickly disappeared for most people in most cases. I wonder if it could largely be replaced by native AVP apps or a better way for them to send out data from the Mac to the headset once there is broader software support? reply lucb1e 9 hours agorootparentprev> not suitable for even a few minutes of use Because of lag, or why? Many people work on remote desktop all day long, and I spend my fair share of time in SSH sessions as well. It's not like it improves the experience compared to working locally, but for me it works fine so long as you're within a few hundred kilometers without much jitter. On the VR, the screen should move as you move your head because that position isn't what's being passed through, so that can't be the difference either. I don't understand (without having a device myself) how/whether this is worse than normal video streaming over LAN? reply xp84 9 hours agorootparentIf i'm not mistaken, judge2020 meant that what's not suitable is looking through your AVP at your real monitor. The virtual display (glorified VNC or whatever) that you're saying is OK, I think the agree is also OK. And I'd assume that yes, it would be torture to try to view a monitor through the AVP just due to the resolution loss. It would be like poorly downscaling the 4k/5k resolution of your 27\" monitor to like 1366x768 but much worse since the pixels are not even staying lined up on a level grid but resampled at slightly diagonal angles as your head moves even a couple of degrees. I am pretty sure setting up 2 more big $1000 monitors left and right would be better than \"center monitor + AVP with virtual apps left and right\" (and it would save about $2000 lol). reply smaudet 6 hours agorootparentEh? For coding you don't need a 2k$ monitor setup, you can get by comfortably on 3-400$. It's an interesting argument but they are going to need to try a lot harder for it to be a compelling desktop replacement. reply judge2020 9 hours agorootparentprevYou misunderstood, the video passthrough of your surroundings is not good enough. Using the macOS virtual display is fine, there is some noticeable streaming latency, maybe 30ms (that would be solved if it could just take DP over USB-C or Thunderbolt in) but it's suitable for long term use. reply steveBK123 9 hours agorootparentNot just giving a display port of some sort seems like such a mistake given that there's already the battery cord/pack and that the virtual display latency is so bad. reply dclowd9901 2 hours agoparentprevVR headsets’ resolution are still a couple orders of magnitude away from being indecipherable from normal vision, and that doesn’t even include motion. reply LegitShady 9 hours agoparentprevI think its a PWM issue. reply rrdharan 9 hours agorootparentSince I didn’t know the acronym.. “Pulse Width Modulation”: https://appleinsider.com/inside/apple-vision-pro/tips/why-ap... reply cpufry 6 hours agoparentprevsounds like absolute fucking hellscape jesus christ reply machiaweliczny 4 minutes agoprevI thinking about using it for coding 2h at my balcony to get some sun exposure. When using macbook I am annoyed due to reflections. Does anyone know how pass through works in sunlight? reply AceJohnny2 10 hours agoprevThis is my goal and hope, but I'll temper OP's points with how things stand today. Background: I got my AVP on Monday 5th (having gotten up before dawn to place my order when it first became available). I am a programmer with a multi-screen setup to maximize my usable workspace, which consists of editor, terminal, and browser windows. My hope is for the AVP to replace a set of fixed screens, and have in effect infinite screen space. The AVP has worse Angular Resolution than a monitor, at average 34 Pixels-Per-Degree (PPD) vs a monitor's 64 PPD (more details in iFixit's writeup [1]). This is an inevitable consequence of placing a screen so close to your eyes. The AVP's exceptional screen technology mitigates this (and for me completely eliminates the \"screen door\" effect that plagued earlier VR sets), but it can't beat Physics. So for one thing, you cannot have the legible text density of, say, 2 27\" monitors an arm's length away. In other words, the equivalent amount of text will take more space in your vision on the AVP. This is understandable. But where the AVP really has a problem, but I really hope they improve, is window management. Someone quipped that, from an app/window management perspective, the AVP is like sticking an iPad on your face, and I agree. As someone who's used to moving and resizing windows around with Tiling window managers on Linux and SizeUp or Moom on macOS, the window management of the AVP is really awkward. Say you put your editor large front-and-center, but now want to switch to a terminal, or resize the editor to put a reference on the side? The hand control may feel magical, but you're going to be doing a lot of it to move and resize your windows. But these are software UX problems, and I'm gambling on Apple fixing them over time. [1] https://www.ifixit.com/News/90409/vision-pro-teardown-part-2... reply wilsonnb3 6 hours agoparentits been 14 years on the iPad and even longer on MacOS and neither of them have good window management, I really don't think Apple will come up with a good solution on the vision pro reply AceJohnny2 6 hours agorootparentsigh thanks for the reality check. reply nilkn 4 hours agorootparentprevMacOS has had features like expose and Mission Control for many years. Even something like that would be a big step forward for window management in visionOS. reply dagmx 5 hours agoparentprevRegarding window management, what it really needs is (ironically) an iPad feature called Stage Manager. It feels so perfect for this use but it’s a shame it doesn’t exist reply AceJohnny2 10 hours agoparentprevOPfftopic: the text preview bar following your wireless mac keyboard around is the standout AR feature for me. If that sounds unimpressive, I agree; I have yet to experience truly impressive AR on the AVP. But it's still early, so I'm hopeful. reply larrysalibra 10 hours agoprev“Active noise cancellation for your eyes” is a great way to put. I also have found myself using the mount hood environment set to daytime as the setting for macOS + visionOS multi-window set up. I took it off yesterday before some friends came over for dinner and I was kind of shocked that it had gotten dark outside because my mind felt like it was still day time since the sun was out in mount hood. I had actually been sitting on my office in darkness for a least an hour. reply enos_feedler 6 hours agoparentOkay but as I sit here in my own private space in a city, one of these I need and one of these I don't. reply xnx 10 hours agoparentprevIs there a setting to allow the environment to change lighting conditions on a 24 hour schedule? reply LeoPanthera 10 hours agorootparentYes, in fact that's the default, it tracks sunrise and sunset based on your location. reply larrysalibra 10 hours agorootparentYeah I changed the default to day which in hindsight wasn’t a great idea especially since I’m trying to recover from North America -> Asia jet lag. reply yokoprime 8 minutes agoprevCoding with super heavy ski-goggles? No thanks… there’s a reason i take off my goggles when not skiing reply elliottkember 6 hours agoprevIt's summer here in New Zealand, and I have been using my AVP to code outside in the sunshine. There's no monitor glare and I'm not stuck inside. It's great. reply newshorts 4 hours agoparentHad similar experiences: Running late, took a webex call from the back of my minivan on my MacBook Pro. Except I had enough screen real estate with the virtual monitor to do work during the boring parts. The best part was going full immersion and hearing the rain falling on a high mountain lake in between people speaking. I also usually wake up overnight and want to work, but can’t because my office is located in the master bedroom. Now I can sit on my couch and not have to hunch over. Finally, I took my dog to the dog park and worked with no screen glare and big virtual display Also, watching movies on the moon is pretty cool. reply vundercind 5 hours agoparentprevWow, this is the first thing anyone’s written about the AVP that’s made me go “maybe I would use this kind of device…” I didn’t even think about how nice they’d be to use outside on sunny days, compared with a laptop (or, coupled with one). reply kromem 3 hours agorootparentOf course the irony is that being \"outside\" is actually watching a live video recording of it from outer cameras on a screen in front of your face. I'm not necessarily criticizing that - more thinking that we're likely approaching a fidelity threshold where you no longer care it's a screen (I still don't think we're quite there yet and suspect this is just one gen before the device that will survive the few months in VR great filter). reply bowsamic 3 hours agorootparentOkay but sight isn’t everything, your body is still outside reply justanotherjoe 5 hours agorootparentprevyes, but isn't part of what makes being outside nice is some measure of sunlight on your eyes? reply vundercind 5 hours agorootparentEh, I’m usually in fairly dark sunglasses anyway. Trying to hold the worst of the cataracts off until my 70s if I can. reply dwaite 1 hour agoparentprevFlip side, that could lead to a really interesting tan line. reply killjoywashere 5 hours agoprevI participated in a couple Vision Pro demos at Apple HQ prior to launch day. One with a security focus, one with a health applications focus. The security profile, btw, is super boring: on the network, it looks like an iPad. Health applications: pretty cool for education, maybe other things, eventually. What I think would be compelling, for me, is the ability to use it on travel. To be on a plane and not have to worry about breaking a laptop screen when the person in front of me tips their seat back, not have to worry about someone shoulder surfing me. In yet another boring hotel room? Let's escape! Now, the problem is that, in many meetings, a laptop is barely acceptable as it is (in some cases, not at all acceptable). So, I still have to bring a laptop. So, my backback is now ... heavier? I'm already carrying multiple devices required by various security policies. Is this juice worth the squeeze? I can't bring myself to spend my own money on this. And I helped develop an AR device. I'd be happy to use it if the company bought it. But you're in for almost $5k after applecare, accessories, etc. reply bongodongobob 5 hours agoparentWhere do you work where laptops at meetings aren't acceptable? Not seeing laptops is an extremely reliable red flag for me. reply yokoprime 5 minutes agorootparentPeople not taking notes is a huge red flag at least, but handwritten notes are fine so laptop is not a must reply whateveracct 5 hours agorootparentprevwhy exactly? reply Supermancho 5 hours agorootparentIf a meeting is simplistic enough not to need planning and action items, you don't need a meeting. Make a conference call. If action items are simplistic enough that you don't need to consider calendars for the future, don't need the ability to show the status of existing concerning items, and any reliable information is conveyed via paper, you've got multiple problems. Mostly you have a culture problem, to put it simply. reply resonious 5 hours agorootparentI'd probably make the opposite argument. If I'm messing with my computer during a meeting then the meeting probably didn't require me to be physically present. At that point it could've been either remote or async facilitated by software. This is unless I'm running the meeting and my laptop is plugged into the big screen. reply whateveracct 5 hours agorootparentprevmultitasking is a myth. you can't futz with a computer and pay attention. reply numpad0 2 hours agorootparentFor people with autofocus, not hyperfocusing and slamming desk is harder than trying to focus. reply nurumaik 5 hours agorootparentprevSingletasking is a myth. I can't keep attention without any background task. If not computer, I will doodle on paper or fidget reply makeitdouble 50 minutes agoparentprevSounds like a Xreal pro would get you 90% of the way there ? At 49ppd it's on par with AVP if the goal is just to mirror a laptop display. reply willemlaurentz 11 hours agoprevAt first I thought it would be weird, but after a few minutes it all felt very natural and a deep work focus was achieved without me really realising it. I tried to grasp some of the awe I experienced in this blog post. reply shakow 10 hours agoparentHow is the resolution? I never tried a Vision Pro yet, but that was my main gripe with my Rift. Indeed, IME, the ‶real world″ size of the windows is nothing, it's their angular resolution that is a make or break. reply larrysalibra 10 hours agorootparentIt ranges from okay to not that bad. It’s not as good as the “retina” screens in Apple’s other products. I can definitely see pixels on the device if I’m wearing contacts unlike what most of the early reviewers claimed. But it’s not so bad as to be unusable and you don’t really notice them in most contexts - the main place I notice pixels is using it as a screen for macOS. VisionOS apps are rendered very large by default so the resolution deficiencies aren’t as apparent. Overall it is very impressive but there’s also much room for improvement in resolution. reply drcode 9 hours agorootparentI don't know, if I need to use my macOS laptop, it sounds a lot easier to just use my laptop, instead of using a helmet and then doing the connection dance to get a single ok-ish screen anyway reply veec_cas_tant 6 hours agorootparentA single screen that is much larger than a MacBook screen, that is the advantage. And the connection dance is really just...tapping the connect button? reply bed99 10 hours agorootparentprevI don't think it is possible to see the pixels, they are the size of a red blood cell. But maybe you can see antialiasing or noise from the cameras. Personally I can't see any pixelation. It's even better than reality. That being said passthrough cameras' resolution do suck but it's not what it's important and probably the easiest problem to solve. reply hatsix 5 hours agorootparentsure, they're small, but some of them are closer to your eye lense than your retina. The pixels per degree are lower than any Apple product in the past 10 years reply ribosometronome 10 hours agorootparentprevFor non-text content, I think it's pretty good, actually. Watching content in Disney+ or Max's theater modes is really satisfying despite those virtual screens being sub-4k. reply euroderf 10 hours agorootparentprevNow there is a topic. How does it work with contacts ? Does it obviate the need for the 149$ Zeiss add-ons ? Do you blink enough in the environment ? reply robterrell 10 hours agorootparentI have been using contacts and just got the zeiss lenses today. So far it's roughly equivalent. I think there is a touch more distortion (chromatic aberration along the edges) with the inserts. But my contacts are progressives and kinda sucked for this, single vision would probably be great. reply astrange 5 hours agorootparentprevThe purchase path in the online store will ask what kind of contacts you have, and if they're compatible will recommend using them instead of inserts. The main advantage is that it's less crowded in there, I suppose. reply joshstrange 10 hours agorootparentprevIt's better than anything else I've tried by a long shot, which admittedly is only the Quest 2, but still not good enough to work in all day. It wasn't the weight/comfort for me, just it was too blurry. I fully expect to be using one eventually as an external monitor replacement but it's got a little ways to go. Probably needs another generation or 2 of screen/camera improvements. It's _good_ but not worth keeping unless you already use mostly iPad apps for work. reply willemlaurentz 10 hours agorootparentprevI love high dpi displays as I like text (and other details) to be crisp and clear. Vision Pro is approaching a quality that I feel is acceptable, somewhere between \"flawless reality\" and a decent screen. The virtual environments are crisper than the camera passthrough mixed environments. There is definitely room for improvement on screen quality, but in general I am impressed. reply grumbel 10 hours agorootparentprevThe original Oculus Rift had 1080x1200, VisionPro has 3660x3200, field of view is similar. In virtual monitor terms that means a Rift can display a 640x480 monitor and the VisionPro a 1920x1080 one. Still quite far away from Retina, but should be pretty usable, especially since it allows making the virtual screens bigger than a real monitor can be. reply newshorts 4 hours agorootparentNah it’s bigger than 1080… You can choose 4k resolution and get extra real estate reply jdriselvato 10 hours agoparentprevYou have a great humorist writing style. I enjoyed the read and it just makes me want one even more. I just went to an Apple Store to demo the Vision Pro and walked out feeling like I had a religious experience. I was overcame with emotion over that immersion demo, those baby rhinos completely blew my mind. I'm booking another appointment soon to get another taste. I really hope the price becomes manageable for consumers like me; I love the tech and want the future but can't afford it. reply iknowstuff 10 hours agorootparentHave you tried the Quest 3? reply mrcwinn 9 hours agoprevIt's not a perfect product by any means. The question is, has Apple backed itself into any corners? Can the battery life or field of view be improved? The resolution? The weight? I think in all cases, Apple has a technical path forward. Despite its imperfections, wow is it fantastic. I'm writing now from a Mac virtual display (which, yes, is a bit blurry). But how could I return this thing? It's the most impressive product Apple's ever made and it's genuinely useful. It's not an expensive novelty by any means. What an achievement. reply RockRobotRock 5 hours agoparentThe virtual display is impressive, for sure. But, I think you should consider a pair of AR glasses from xreal or rokid. They cost a fraction of the price, have less latency, and you don't need to worry about battery life. They're also much much more portable. I think using AVP primarily as an external display is a huge huge waste. reply joshstrange 10 hours agoprevI'm so glad that some people are able to use it in this way and I'm jealous. I found the code too blurry [0] and I disliked being limited to 1 blurry screen with 3 clear monitors available to me. I ended up returning mine but I can't wait till I can work inside one of those without feeling limited. It's easy to imagine a future where the AVP beats my monitors and I can't wait. [0] https://joshstrange.com/2024/02/04/apple-vision-pro-writing-... reply DerCommodore 19 minutes agoprevNot sure this my device, with such gigantic windows reply evang7 43 minutes agoprevThis is just not going to work. Whatever the experience may be, in this universe, it is not good for the eyes to be focused at a screen a few cm away for hours on end. reply ianlevesque 13 minutes agoparentThe focal distance is more like six feet on most headsets. That's further than my physical monitors are. reply BossingAround 49 minutes agoprevOff topic, but when you click a photo in this blog, that's a proper nightmare. There's no way to get back to the article, and the back button exits the whole page. Talk about bad UX. reply elif 4 hours agoprevUnless all these returns make it super affordable on the secondhand market, it seems like waiting for meta quest 4 pro next year might be the play. For the vision pro price you can afford one for home and another for the office. reply swader999 10 hours agoprevI'd rather work from a real mountaintop on a crappy laptop. Actually, no. I'll work at work and leave it all behind on the mountain. reply anonzzzies 3 hours agoparentHow about work and live on a mountain and forget about this idea of ‘at work’? reply ngneer 6 hours agoparentprevExactly! reply dcchambers 5 hours agoprevI only did the 30 minute demo in the Apple store, but I found the brief minutes of looking at text in safari to be pretty bad with regards to eye strain. Watching videos was fine because I felt I could just let my eyes relax, but trying to focus on text was anything but fun. I have contacts and am not convinced that the fit/focus I had was correct, but it was not a good first impression from a \"productivity device\" perspective. Apple is going to have a problem with how finicky VR headsets are. reply mrcwinn 4 hours agoprevAnother thing that fascinates me about the Vision Pro: its looks. It’s at once futuristic - 3D knits, curved glass encased by aluminum - and yet also reminds me of something out of the 80s. Actually, it looks to me like a Mac. Aspects like the solo band’s dial, maybe. Or maybe it’s something you’d see in Back to the Future Part 2. Post modern yet nostalgic. I can’t quite wrap my head around it, no pun intended. reply seydor 4 hours agoparentFor people looking for an elegant device with excellent passthrough to wear on the street, i suggest this https://www.dior.com/en_gr/fashion/products/00X0056HOGOG_C11... reply parallaxapps 5 hours agoprevLove to see this! We’re the developers of Inspect Browser [0], the browser with dev tools you see in the screenshots. Our goal is to make tools to help web developers work on iPads, iPhones, and now Vision Pro- thrilled to see it in action! Glad to answer any questions. [0] https://apps.apple.com/us/app/inspect-browser/id1203594958 reply sircastor 5 hours agoprevI rented a Quest 2 a couple of years ago to try out programming in it. I’d read a post someone made about his spending 8 hours a day in it. For me it wasn’t quite enough. The fresnel lenses make “God rays” at the edges and the text was readable, but not pleasant to read. I’m planning sometime to try the Quest 3. The Vision Pro seems like it might be the best option because it’s purpose built for that. Also I’m a Mac user and imagine the experience to be whole. reply anonzzzies 3 hours agoparentI have done about a year of coding on the quest 2 fulltime daily in Immersed and it was fine; now I skipped to the xreal which I like better. I don’t have a Vision Pro or quest 3 yet but I like the portability of the xreal so much now… reply buffington 5 hours agoparentprevI have the Quest 2 and the text isn't legible enough for me to write code for any length of time. I work with someone who has said \"if the Quest 2 was a 0 on the usability scale, the Quest 3 is an easy 8 - it's that much better.\" I've not tried it myself, but I don't think my coworker is exaggerating. reply KronisLV 48 minutes agorootparent> I have the Quest 2 and the text isn't legible enough for me to write code for any length of time. The comment above about \"god rays\" is correct in my experience, as well as the lenses being blurry towards the edges due to how they're made. However, I realized that I need prescription lenses for VR, because while I have -0.75 in one eye, that's still enough to throw the overall experience off a lot (and the glasses that I have are too wide for the headset). Aside from that, what helped me immensely is the Immersed app: https://immersed.com/ where the actual environment is rendered on the Quest, whereas the actual monitors are streamed over from the PC which helps immensely with how legible things are. SteamVR and the Oculus Link apps are both trash in comparison, there's aliasing, shimmering, or everything ends up being way too blurry. I tried using the Horizon Workrooms app, but it wouldn't work because I have 4 monitors and it only supports up to 3. There's also the Virtual Desktop app which might be good, but I didn't feel like paying for it at the moment. So yeah, Quest 2 starts out at a disadvantage compared to other options (aside from cost, I guess), but there are factors that can absolutely ruin it, such as using the wrong method of displaying things. Also Quest 2 is just a little bit on the heavy side, or at least feels that way after prolonged usage, with the default face interface, even with a custom head strap. reply antman123 2 hours agorootparentprevyour coworker isn't \"wrong\" but that scale is off. If Quest 2 is a 0, Quest Pro is a 4.5, Quest 3 is a 6 and the AVP is an 8. (edited) I still could not program for 2 hours comfortably on Quest 3. I can do that on the AVP reply kevinmershon 4 hours agorootparentprevI have worked for hours in Immersed in both the quest 2 and 3. The 3 is considerably more comfortable on the head and eyes. I constantly had to take the quest 2 off for a breather reply aksss 5 hours agoparentprevYeah, I’d love someone to chime in about Q3. AVP and whassisname’s video got me thinking more about it. reply bookstore-romeo 7 hours agoprevI find the comparison to \"headphones\" for your eyes really interesting because it might provide a long-term view as to the Vision Pro's place on the computer market. Desktop speakers and Hi-Fi home stereo equipment are obviously still a thing today, and, in a few generation (and with the advent of more competition), the case of bigger desktops (and possibly laptops/tablets) might be analogous to today's headphone-loudspeaker duality. reply elicash 6 hours agoparentI don't know if it's intentional, but it's a reference to a Steve Jobs quote from 2005: \"Headphones are a miraculous thing. You put on a pair of headphones and you get the same experience you get with a great pair of speakers, right? There’s no such thing as headphones for video. There’s not something I can carry with me, that I can put on and it gives me the same experience I get when I’m watching my 50-inch plasma display at home. Until someone invents that, you’re gonna have these opposing constraints.\" Walt Mossberg then mentions goggles, and Jobs says \"but they're lousy!\" (And Walt adds, \"you never get a date if you wear them.\") reply stevage 6 hours agoprevI'll be interested to hear whether he's still using it like this in 3 months. reply willemlaurentz 51 minutes agoparentI'm, too and will surely follow up on my post. reply 8n4vidtmkvmk 2 hours agoprevIt sounds like it might be pretty hard to exceed 3.3K PPI and yet it's not nearly enough. Would it be possible to pull the headset a little further off the face and just counter balance it to get better PPD? reply bharrison 6 hours agoprevNot for nothing, I'm a fan of my early gen xReal air, though mostly for an on-the-go coding use case. I link it up to my phone, wire up a 60% keyboard and fire up a VPN/ssh. The Galaxy DeX environment is pretty usable for other apps, and a BT mouse can alleviate the goofiness of the \"phone as a touchpad\" control scheme. I've used it with an HDMI/usb-c cable from my desktop and it works just as well, though attempting to switch between virtual and real displays through the glasses is headache-inducing at best. I'd say you accomplish 60% of the effect of the AVP, conservatively. reply anonzzzies 3 hours agoparentI don’t know how it compares to the avp but as an old guy, the xreal looks like normal sunglasses. Most people just come sit next to me and talk (even though there IS a massive keyboard in front of me) as though I am just having a coffee in the sun. I like it because it is light, small, very long (my phone) battery life. I am afraid I will like avp better though. reply tomcam 1 hour agoprevI wonder if it might be a godsend for people who can’t work in open office plans reply densh 55 minutes agoparentI sense some irony in spending extra time on commute only to put a headset that hides all the surroundings of the open office from you and immerses you in a virtual environment. reply dbish 10 hours agoprevI recently had a kid and lost my office :), I've been struggling to have a set space for work when he's asleep, but would absolutely love it if I could just put on a headset. Is it workable for that? I used to try to code in the quest3 but the resolution felt like I might as well be coding on my phone. I'll go buy one today if this would be an office replacement for me. reply anonzzzies 3 hours agoparentThe difference between the quest 3 and your phone is the monitor sizes. I have 4 monitors in immersed and while not high res, this is far more convenient than a phone (and for me than actual monitors; I hate stationary monitors workspaces). But I guess it might depend on what you actually do. reply newshorts 4 hours agoparentprevI have literally the exact same problem. Now I can sit on my couch in a much more comfortable and ergonomic? position with 4k screen size virtual display. Plus I can watch movies with the sound on and not wake anyone up. reply dbish 4 hours agorootparentthat's awesome. i need to check it out reply djsavvy 10 hours agoparentprevfrom experience — yes, absolutely. It's not as good as a dedicated, already-optimized work space, but it comes pretty darn close. I have a great workspace well-tailored for me with a massive desk, ultrawide monitor, nice speakers, etc., but I still pop into my vision pro and screen mirror my laptop to get work done. If I'm having trouble focusing, the environments and white noise generation are really helpful. I've also noticed that it really boosts my productivity when I would otherwise be disturbed by a cluttered environment. reply dbish 10 hours agorootparentWow, alright, strong endorsement. I might have to pick one up afterall. Thanks! reply joshstrange 10 hours agorootparentGo get a demo at an Apple Store. They won't show you Mac Virtual Display and the clarity of the AVP/iPad apps is good, pictures/experiences are breathtaking. MVD is not that clear, full stop. Also, try to resize an app with text down to the size you would use for a window on your computer. You can't. I constantly found myself wishing to make apps smaller but you can't and so you get limited quickly on what you can fit in front of you/in your field of vision. I even wish there was a way to switch to the iPhone app instead then so I could see more. I wonder if part of that limitation is that the text won't be as legible if it's smaller. One day it will get there, today is not that day. That said, you should go try it at an Apple Store, it's free to do. reply dbish 9 hours agorootparentFair enough. I’ll check it out. reply baby 2 hours agoprevI tried it and it hurt my face like hell in 10min. It’s uber heavy. I had a Quest 1 and would use it for hours without any issues. reply munk-a 10 hours agoprevThis feels like one of those activities that's so drastically different that there may be serious health concerns about it. Wearing a VR headset to play a few hours of games a week still lets your eyes mostly exist in a natural light setting. When you're wearing a VR display the light is right up against your eyes with no possibility for them to look away to relax. When we're normally working we can glance out at a horizon (hopefully - sometimes you don't have a window) or at least let your eyes wander off to the ceiling and defocus. It seems like giving your eyes a chance to relax (i.e. having narrow glasses where part of your view field is out of focus) can be beneficial at delaying the onset of Myopia. The idea of wearing one of these for eight hours a day feels much worse than even staring at a phone in your hand when it comes to eye health. reply larrysalibra 10 hours agoparentI wonder what it will do to our skin after long time use. I get red goggle rings around my eyes. Is it going to crest more wrinkles? One improvement vs phones is there’s no more bending the neck down to look at something in your hands. Interesting times! reply crooked-v 6 hours agorootparentThat's entirely a consequence of the made-for-marketing headstrap instead of the headset itself. Check out companies like BOBOVR that make more ergonomic headstraps (not for AVP yet, but they've hinted at working on it). reply ctvo 10 hours agoprevCompletely different from my reality; the selective blurring when connected to a laptop hurts my eyes and forces me to refocus them constantly. The limited field of vision stops where I can place my windows and displays without moving my head. The resolution noticeably worse (as expected) than my 2 year old 5K monitor. reply zyklonix 10 hours agoprevAny idea how the author was able to have multiple desktop screens? What app is he using for this? I thought it only allowed you to mirror one screen per Mac and from my experience it is quite laggy. reply anonzzzies 2 hours agoparentThis is the problem(with Apple); this device, like iOS devices, is fenced off, so you cannot run vscode etc if you don’t connect a MacBook. For this device it’s actually worse than an iPad to disallow that: it’s 4x as expensive as a MacBook Air here but I cannot run most apps I want on it while that would make me buy one today. I don’t want another iPad (which I bought because it’s nice and small and great battery, but if I cannot code on it normally, what’s the point). reply charcircuit 2 hours agorootparent>is fenced off, so you cannot run vscode No, the only reason you can't run vscode is that no one has put in the effort to port it. It's a problem of financial incentives and not a problem of \"fences.\" reply makeitdouble 38 minutes agorootparent\"fences\" is definitely not the best analogy. \"spikes and minefields\" would be more appropriate as Apple has explicit rules against apps that compile and run code, third party extensions would also probably be prohibited, and the terminal would have little use. At the end of the day what's possible under the current rules aren't that different from just running it in Safari, so why bother ? reply anonzzzies 2 hours agorootparentprevSo visionOS is more open than iOS? That would be good. I read it was similar. Edit: so it is indeed similar/same; cannot run normal desktop software even though desktop cpu. reply charcircuit 2 hours agorootparentiOS could get vscode too Edit: Desktop software can be ported to other operating systems. Just because an operating system can't run a different, desktop operating system's software that doesn't mean it can't run desktop software. The CPU is used in iPads too. It is used between 3 different form factors. reply anonzzzies 1 hour agorootparentOnly the ‘shell’; not all the rest that make it practical. There are a lot of blog posts (a yearly one here) of people trying to code on iPads; they all end badly because Apple allows nothing. It’s fenched off. Nothing to do with lack of incentive to port Xcode; there are plenty of code apps on iPad, they just cannot run real envs (docker, anything other than toy interpreters etc) without rooting. reply willemlaurentz 10 hours agoparentprevI use the iPad Blink Shell app, it can have multiple windows and renders using the VisionOS UI, making it sharper than mirroring a Mac's display. It helps that I have optimised a great part of my workflow due to me liking tablets. Like others have said, AVP is like strapping an iPad on your head - I happen to like my iPad, too. reply zyklonix 8 hours agorootparentThat's great! It even supports VS Code. How did you manage to use the mouse? My AVP refuses to pair with a bluetooth mouse. reply zyklonix 8 hours agorootparentIt only supports the Apple Magic Trackpad: https://support.apple.com/en-us/HT213998 reply drcode 9 hours agorootparentprevDoes the \"multiple window\" support of Blink work seamlessly on the helmet? reply jlund-molfese 10 hours agoparentprevLooks like iPad apps! You can tell by the icon floating at the top right of the windows, which appears for \"compatible apps\" aka iPad apps. reply rkangel 10 hours agoparentprevYes you can only mirror one screen from a Mac, but he's using the AVP as the computing device itself. So it's not multiple desktop screens it's multiple windows from locally running apps. reply d--b 5 hours agoprevI just had an idea. I was somewhat intrigued by why people would recreate the same kind of setup in AR with monitor screens. Ok you get more of them, but at some point more screens is just distracting. What if instead this was the occasion for a true AR experience. Like instead of popping up a monitor, you can open a textbook. As in a physical text book sitting on your desk, each page would be covered in qr codes or whatever uniquely identifying that page. Then the vision pro could fill in the page with a computer generated UI: buttons, text inputs and so on. And then you could spread all these pages around your desk and actually touch them, and carry them around. Imagine your code being on a sheet of paper instead of in one window. Imagine your trello board being actual postit notes stuck on your wall that you can unstick and edit. Imagine your wall full of papers, each of them showing some monitoring aspect of your system. Like all your browser tabs are displayed on the wall. Jesus I am excited reply precompute 39 minutes agoparentIt would be much easier to define a virtual keyboard that helps you switch layouts. reply vedran 1 hour agoparentprevSounds almost like https://dynamicland.org/ or https://folk.computer/. reply ugh123 5 hours agoparentprevI wouldn't want to deal with losing papers, ripping them, etc. why not have the AR create them for you? Seems like you want some extra realism surrounding your screen space. So have a life-like book where individual pages (easy to turn hopefully) are different browser tabs. reply d--b 5 hours agorootparentReplacing a paper would be just like opening a new browser tab. A virtual book can’t give you the same level of interaction as a physical one. Virtual interaction are clunky at best. reply karim79 5 hours agoparentprevSounds like something you can have or enjoy in real life, and without having an expensive 600-650g weight on your head. (I do like the idea of virtual postits, mind you). reply d--b 5 hours agorootparentWhat do you mean I can have this in real life? I am talking about having a bunch of paper sheets, each being its own small screen. reply billconan 5 hours agoparentprevthe problem is, virtual screens can float in the air. A physical piece of paper can't. reply 51 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post explores the author's experience with Apple's Vision Pro for Spatial Computing, projecting virtual objects into the real world for interactive experiences.",
      "It highlights the Vision Pro's potential for work, immersive experiences, and deep work, emphasizing its natural interaction model and ability to create a portable multi-monitor setup.",
      "The author expresses excitement for further exploring the spatial capabilities of the Vision Pro, indicating a promising future for immersive technology."
    ],
    "commentSummary": [
      "Users discuss experiences with tech products like the AVP headset, Apple Studio display, and VR headsets such as Meta Quest 2 and PSVR 2, sharing mixed reviews on their practicality and functionality.",
      "Debates focus on the future of technology, Apple products' appeal, and the potential influence of VR on conventional devices, while also addressing concerns like eye strain and text legibility. Users emphasize the need for improvements in comfort and functionality.",
      "Opinions vary among users based on personal preferences and specific requirements for tech products, highlighting the importance of ongoing innovation in the industry."
    ],
    "points": 231,
    "commentCount": 307,
    "retryCount": 0,
    "time": 1708123058
  },
  {
    "id": 39402142,
    "title": "Driftmania: Open Source PICO-8 Racing Game",
    "originLink": "https://frenchie14.itch.io/driftmania",
    "originBody": "I&#x27;ve been spending a lot of my spare time over the last year creating this little racing game. It&#x27;s built in PICO-8, which is a really fun “fantasy retro console” that&#x27;s been mentioned on HN several times. The console has strict limits and I wanted to see how far I could push themThe source code for the game is over here: https:&#x2F;&#x2F;github.com&#x2F;maxbize&#x2F;PICO-8&#x2F;tree&#x2F;master&#x2F;Driftmania. It&#x27;s a bit of a mess, but I&#x27;m happy to answer any questions on it or development of the game. Cheers!EDIT: For those not familiar with PICO-8, there&#x27;s only a few inputs: arrow keys, Z&#x2F;C&#x2F;N, and X&#x2F;V&#x2F;M",
    "commentLink": "https://news.ycombinator.com/item?id=39402142",
    "commentBody": "Driftmania – an open source PICO-8 racing game (frenchie14.itch.io)225 points by frenchie14 14 hours agohidepastfavorite57 comments I've been spending a lot of my spare time over the last year creating this little racing game. It's built in PICO-8, which is a really fun “fantasy retro console” that's been mentioned on HN several times. The console has strict limits and I wanted to see how far I could push them The source code for the game is over here: https://github.com/maxbize/PICO-8/tree/master/Driftmania. It's a bit of a mess, but I'm happy to answer any questions on it or development of the game. Cheers! EDIT: For those not familiar with PICO-8, there's only a few inputs: arrow keys, Z/C/N, and X/V/M lagniappe 13 hours agoThis device is perfect for PICO-8 if anybody wants the handheld formfactor https://powkiddy.com/products/pre-sale-powkiddy-rgb30-rk3566... Non affiliated with the company, I get nothing from saying this, it's just a decent device all around. The 1:1 aspect ratio on the screen really does PICO-8 games justice, along with MAME and other popular platforms. The greatest thing is these devices are super cheap and easy to come by if you're patient. Here's a video review of it by a youtuber whose opinion I've come to respect https://www.youtube.com/watch?v=LIMepWAzUuw reply frenchie14 12 hours agoparentI have an RGB30 and can confirm PICO-8 looks and runs great on it! It's a Linux handheld and can run the native PICO-8 binary reply sahara 2 hours agoparentprev\"Decent\" feels like the correct word to describe the RGB30. To be clear, I'm ultimately glad I bought it, and it's germane to this discussion that the release of the RGB30 is what got me interested in PICO-8 in the first place (I had apparently purchased a license years ago as part of the Bundle for Racial Justice and Equality on itch.io and didn't even realize it until late last year). Also, the 720p 1:1 display is one of those developments that in retrospect feels so perfect for this form factor it ought to have been obvious, but it was a weird move at the time and they deserve full credit for taking the risk. Having said that, I want to love this thing, but I just can't. Mainly because the D-pad sucks. It's not unusable, but it's worse than my Miyoo Mini and Anbernic devices, worse by a mile than my 8BitDo controllers, worse still than my Hori Fighting Commander, worse than even my Steam Decks (which have sort of weird D-pads themselves)... you get the point. It's annoying that the device on which I would prefer to play 8 & 16 bit games at a 1:1/8:7 resolution has—out of everything I own—by far the worst controller for exactly those games. The ergonomics also leave a bit to be desired. Again, like the D-pad, it's not so uncomfortable as to be unusable, though I much prefer pairing it with a 3D printed grip I bought from ComfortGrips on Etsy¹. That obviously makes it a much less pocketable device, which isn't a big deal for me, but might be for others. Finally, I hesitate to critique the software experience, because on the whole I'm incredibly impressed with how much improvement I've seen (both in terms of quality and frequency of updates) to JELOS in the nearly five months I've owned the RGB30. But the fact remains that that confusing preference conflicts, occasional crashes, frequent sleep/wake flakiness, ridiculously bad battery drain when asleep or even fully powered off (which thankfully has been cleaned up in recent updates)... they're all just facts of life with the RGB30. It was honestly kind of shocking coming from OnionOS on the Miyoo Mini which is absolutely rock solid in comparison. But that's obviously a less powerful device with a completely different form factor. Anyway, as lagniappe correctly pointed out, PowKiddy makes budget devices, which means they're generally inexpensive enough to take a flyer on despite their limitations, and the RGB30 is arguably the best thing they've ever made. 1.reply theyinwhy 57 minutes agorootparentSo what device do you recommend for pico-8? reply madduci 3 hours agoparentprevReally cool, do you know if it supports also the Switch emulator? reply blacksmith_tb 11 hours agoparentprevI have a Gameboy-shaped Anbernic RG351V (which runs Linux behind the scenes), it is fun to play PICO-8 games on too. reply asimovfan 12 hours agoparentprevis there a way to play these on the phone? reply city41 9 hours agorootparentIf you mean pico8 games, they can do that out of the box. Click on play game, then the play button and touch controls will show up if on a phone. reply renewiltord 12 hours agoparentprevThank you for the recommendation. Bought one! reply freedomben 13 hours agoprevWhoa dude, this is really cool! Nice work! You should definitely include this on your resume/CV because if I were reviewing your resume and saw this, I'd be very interested :-) For those not familiar with PICO-8: > PICO-8 is a virtual machine and game engine created by Lexaloffle Games. It is a fantasy video game console[1] that mimics the limited graphical and sound capabilities of 8-bit systems of the 1980s to encourage creativity and ingenuity in producing games without being overwhelmed with the many possibilities of modern tools and machines.\"[1] [1] https://en.wikipedia.org/wiki/PICO-8 reply cubano 8 hours agoprevI started learning python about 6 months ago and this game is absolutely the most perfect codebase/project i've seen to help me \"level-up\" my coding game within this particular language, plus the game is an absolute gem so thank you from a 60yo out-of-work developer. I have been looking for some sort of project to help pass the days and scratch my just build something dummy!-itch that had developed now that no one will hire me and I can work on anything that floats my boat. This looks just perfect as far as 1] how cool the final output is 2] how complex the code is and 3] potential access to the original dev (well, we will see won't we? lol). I already have a couple of ideas for some PICO-8 level games so I'm really excited about getting started...thank you very much for posting this! reply joemi 13 hours agoprevNice. Reminds me somewhat of Super Off Road, which was an arcade game I loved as a kid: https://en.wikipedia.org/wiki/Super_Off_Road reply amethyst 12 hours agoparentMixed with R.C. Pro-Am, one of my favorite NES titles: https://en.wikipedia.org/wiki/R.C._Pro-Am reply bemmu 5 hours agoprevWhat would be “minimal viable vehicle physics”? I’ve been prototyping an f-zero type game and so far have: - pressing forward increases thrust - thrust makes vehicle accelerate forward - friction is higher for the component of velocity perpendicular to tires - vehicle turns faster when it is moving faster (I don’t actually simulate tire direction) - as a hack to make it feel a bit like drifting, make the vehicle visually turn more than it is actually turning Any simple thing I should add? reply frenchie14 5 hours agoparentI took a look at what's a part of my car controller[1] if you want some inspiration. A lot of these wouldn't make sense in an F-Zero type game - Wheel modifiers (which wheels are on road, grass, boost tiles, etc) - Control loss when airborne - Slow turning when at low speed (a bit different from yours since the impact drops to zero very quickly) - Speed / accel penalty when hitting a wall - Visually rotating the car (this is what happens when a player presses the turn key but the actual velocity rotation is handled separately) - Boost handling - Nudge the car to the side a little if it's trying to turn but is blocked by a wall - Acceleration, friction, breaking, and drift-breaking - Corrective side force (basically an extra friction perpendicular to velocity like you have) - Artificial speed limit (alternatively, you can include a drag component which applies a force proportional to the square of speed, but I've found it hard to get this feeling good in the past) - Velocity rotation to align velocity direction and visual direction. This is a minor effect - it's mostly handled by the other physics - Gravity - Out of bounds checks I think you've got the main pieces already. What's more important is understanding what you want your game to feel like and continuously tweaking until you get there [1] https://github.com/maxbize/PICO-8/blob/master/Driftmania/dri... reply AugusteLef 13 hours agoprevJust played it for 10 minutes! Great game. One remark: to start the race, you need to select \"restart level\" even if you just started the game. I would suggest starting the race by simply pressing enter. reply frenchie14 13 hours agoparentAh, you must be entering a race from the pause menu. PICO-8 controls are arrow keys, Z, and X (emulating old school consoles with limited input options). There's a level select screen just off the title menu reply freedomben 13 hours agoparentprevAh yes, I hit that too. Selecting \"Continue\" (which didn't make sense since this was my first load) just exited the menu. Didn't take me long to try \"Restart\" but it is a little confusing. reply vunderba 13 hours agoprevNice work, gives me very Micro Machines vibes for the NES. The only thing I don't like about PICO-8 is that its completely closed source. An open source alternative that seems very promising is Pyxel. It has similar retro / pixel art limitations, a built-in sprite editor, music tracker, etc. https://github.com/kitao/pyxel reply jamesgeck0 12 hours agoparentIIRC, the browser builds of Pyxel games end up pulling down 30mb+ of Python libraries, which seems a bit overkill for little pixel games. TIC-80 is probably the closest open source thing to PICO-8. The browser builds ran 10% slower than the desktop app last I tried. It doesn't have a \"CPU budget,\" so it's possible to write inefficient code that works fine on powerful machines but not slower ones. reply vunderba 10 hours agorootparentSo I just ran a test deploy of a Pyxel little animated star field: https://gondolaprime.pw/games/starfield/index.html Developer console shows approx ~7MB transferred. Still more than I would have expected for literally just a small looping animation but a bit better than 30MB at least. Maybe there's been some progress on this front. Thanks for the TIC-80 recommendation - I really like that it supports multiple langs (Lua, JS, Python, etc) - that's some great flexibility. EDIT: It looks like TIC-80 games pull a tic80.wasm file which is approximately ~6MB in size. I'd say Pyxel and TIC-80 are roughly comparable at least with respect to bundle size. https://tic80.com/js/1.1.2837/tic80.wasm reply filleduchaos 6 hours agorootparent> Developer console shows approx ~7MB transferred Bytes transferred over the wire don't equal actual file size (due to gzipping, etc). Your page actually weighs 21MB. reply vunderba 5 hours agorootparentYes, I'm well aware of that - but your average viewer isn't going to notice expanded resources, however they ARE going to notice things like time-to-first-byte and network transfer. EDIT: Just to be clear since I can't edit the older comment - instead of bundle size it would have been better to say that the \"network transfer size\" is roughly comparable. reply WithinReason 1 hour agoparentprevThere is also Nico, written in Nim therefore can be compiled for the Web or as a native binary: https://github.com/ftsf/nico reply hombre_fatal 13 hours agoprevAside, I just realized Vimium doesn't get disabled inside iframes on pages where it's disabled, nor can you disable the iframe domain from the parent page since the iframe regex won't match the parent url. I had to disable the whole plugin for \"x\" to work in the iframe. Seems like an oversight on Vimium's part. As for the game, it's satisfying to ratchet down to the 39 second threshold for the gold medal by figuring our drifting. I realize I was overdrifting and easily beat 39 seconds once I stopped drifting much sooner along the turn. reply frenchie14 12 hours agoparentSorry about that! There's another version uploaded here: https://www.lexaloffle.com/bbs/?tid=140202 Does that one work better with vimium? reply hombre_fatal 12 hours agorootparentIt's not your fault, of course. I just figure HN is the only place where someone might commiserate with me for the self-inflicted pain of using a half-baked Vim keybinding plugin for the browser. It's nice to have the BBS link so I could star it though. :) reply vunderba 13 hours agoparentprevyeah, this used to happen a lot to me on the itch.io platform which uses iFrame embeddings for HTML5/Canvas games. reply VyseofArcadia 13 hours agoparentprevI just ran into this also. Incredibly annoying. reply silenced_trope 12 hours agoparentprevThis happens with anything Vimium. I usually just hit the \"i\" button to enter insert mode. reply lbotos 12 hours agoprevInteresting meta question: Do others expect the car be lined up to go right? Is this a cultural expectation? I come from a LtR writing culture so I was wondering if others were surprised at the car going left at the start. reply frenchie14 12 hours agoparentThere are 15 levels with a variety of start directions. Which direction I picked was based on how I wanted that particular level to flow reply grugagag 11 hours agoprevGot a Miyoo mini for my kid and I played a bit on it and have to say I fell in love with pico-8 games. They’re simple games but the playability is off the charts. I think I’ll join the club and make some myself. Will definitely check this out.. reply WithinReason 1 hour agoprevVery nice Trackmania demake! reply ambigious7777 5 hours agoprevThis is a great game, one question: what does ghost mode do? reply frenchie14 5 hours agoparentIt allows you to race against a replay of yourself. Turn on ghost then finish a map and select retry reply gausswho 13 hours agoprevReally enjoyed this for my ten minutes. A note that I found it a bit confusing that 'enter' doesn't select, X does. But the core game feels really good! reply Moru 9 hours agoparentThis is part of the Pico-8, you need to map the buttons in the \"OS\". Write \"keyconfig\" at the prompt. reply VyseofArcadia 13 hours agoprevWhat was your workflow? Last time I played around with PICO-8 I found the built-in editor to be inadequate. Using an external editor was also annoying for reasons I don't quite remember, but I remember having to close and re-open PICO-8 all the time. Not exactly a the tight, seamless iteration loop I was hoping for. reply frenchie14 12 hours agoparentMy .p8 file just has `#include driftmania-min.lua` and I do all the code edits in an external editor on that file. By splitting it out you remove the conflicts from modifying code and audio/graphics at the same time reply japhib 8 hours agoparentprevThere’s a new-ish plugin for VSCode called pico8-ls that is much better for language support than what there used to be. But the #include approach mentioned by someone else allows you to use the full Lua extensions which are great. reply sen 10 hours agoparentprevThere’s a VSCode plugin for PICO-8 that lets you use that. It ties in with the core and has hot keys to launch the current code to test it. I find it way easier/faster than using the built-in IDE for the code part itself. Then just use the built-in IDE for assets/music. reply hombre_fatal 7 hours agoparentprevYou just open game.p8 in an editor, save changes, and ctrl-r in Pico-8 to reload the game. reply wkirby 9 hours agoprevI recently picked up PICO-8 after about a decade of bouncing around different game engines. There’s something so satisfyingly simplistic about the SDK coming from Unity and Godot. Highly recommended! reply d--b 12 hours agoprevI wish there was an iPhone app for pico 8, the controls in the browser are terrible. reply Llamamoe 3 hours agoprevYou need a README, with screenshots. reply theogravity 11 hours agoprevIt's pretty fun! I had problems trying to drift consistently. I wonder if it would help to have a curved accel / decel like gear shifting. reply willio58 12 hours agoprevI love Pico8. The idea of starting with a small set of tools and building around those limitations I think leads to better game design and development. reply cellularmitosis 11 hours agoprevNice work! It captures the same fast, small-track feel of RC Pro-Am from back in the days of the NES. reply elwell 12 hours agoprevI was excited by the appearance and sound but found it too hard to control to enjoy. Also took me a couple minutes to get started because I didn't know I had to press 'x' (and 'x' closes the tab for me with Vimium extension, even though I disabled all Vimium keys on that page). reply Moru 9 hours agoparentIt's a two player console emulating two joysticks, you can set the keys to use. Type keyconfig at the prompt. reply karlgrz 9 hours agoprevThis rules, thanks for releasing it and open sourcing it! reply redundantly 10 hours agoprevIs there a clip of gameplay somewhere? reply frenchie14 10 hours agoparentI posted a bunch of gifs on Twitter during development: https://twitter.com/MaxBize reply MattPalmer1086 13 hours agoprevReally cool! I made some pico8 games with my son, which were fun but nowhere near as good as this. Nice work. reply AlienRobot 9 hours agoprevReminds me of micromachines. I'm not used to PICO games, but I'd expected enter to mean activate instead of bring menu, like \"start\" would in a console. That menu confused me a bit because it said \"continue\" as if I were already playing the game. reply pengaru 2 hours agoprev [–] nice reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author dedicated spare time to develop a racing game in PICO-8, a vintage console with stringent constraints, and shared the source code on GitHub.",
      "The game's input options are restricted to arrow keys, Z/C/N, and X/V/M, typical for PICO-8 games.",
      "The author is willing to address any inquiries regarding the game's development process."
    ],
    "commentSummary": [
      "Users are engaging in discussions about PICO-8 games, handheld gaming devices like RGB30 and Anbernic RG351V, and game development, sharing experiences and tips.",
      "PICO-8 is highly valued for its educational value in learning coding, leading to praise from the community.",
      "The conversations also touch on specific games like Nico, technical topics such as the Vimium plugin, gameplay mechanics, and the thrill of open sourcing projects."
    ],
    "points": 225,
    "commentCount": 57,
    "retryCount": 0,
    "time": 1708113359
  },
  {
    "id": 39405547,
    "title": "Big Pharma Prioritizes Executives and Stockholders Over R&D",
    "originLink": "https://arstechnica.com/science/2024/02/big-pharma-spends-billions-more-on-executives-and-stockholders-than-on-rd/",
    "originBody": "Greed — Big Pharma spends billions more on executives and stockholders than on R&D Senate report points to greed and \"patent thickets\" as key reasons for high prices. Beth Mole - 2/9/2024, 11:03 PM Enlarge Senate HELP Committee reader comments 486 When big pharmaceutical companies are confronted over their exorbitant pricing of prescription drugs in the US, they often retreat to two well-worn arguments: One, that the high drug prices cover costs of researching and developing new drugs, a risky and expensive endeavor, and two, that middle managers—pharmacy benefit managers (PBMs), to be specific—are actually the ones price gouging Americans. Both of these arguments faced substantial blows in a hearing Thursday held by the Senate Committee on Health, Education, Labor and Pensions, chaired by Sen. Bernie Sanders (I-Vt.). In fact, pharmaceutical companies are spending billions of dollars more on lavish executive compensation, dividends, and stock buyouts than they spend on research and development (R&D) for new drugs, Sanders pointed out. \"In other words, these companies are spending more to enrich their own stockholders and CEOs than they are in finding new cures and new treatments,\" he said. And, while PBMs certainly contribute to America's uniquely astronomical drug pricing, their profiteering accounts for a small fraction of the massive drug market, Sanders and an expert panelist noted. PBMs work as shadowy middle managers between drugmakers, insurers, and pharmacies, setting drug formularies and consumer prices, and negotiating rebates and discounts behind the scenes. Though PBMs practices contribute to overall costs, they pale compared to pharmaceutical profits. Rather, the heart of the problem, according to a Senate report released earlier this week, is pharmaceutical greed, patent gaming that allows drug makers to stretch out monopolies, and powerful lobbying. On Thursday, the Senate committee gathered the CEOs of three behemoth pharmaceutical companies to question them on the drug pricing practices: Robert Davis of Merck, Joaquin Duato of Johnson & Johnson, and Chris Boerner of Bristol Myers Squibb. Advertisement \"We are aware of the many important lifesaving drugs that your companies have produced, and that's extraordinarily important,\" Sanders said before questioning the CEOs. \"But, I think, as all of you know, those drugs mean nothing to anybody who cannot afford it.\" America’s uniquely high prices Sanders called drug pricing in the US \"outrageous,\" noting that Americans spend by far the most for prescription drugs in the world. A report this month by the US Department of Health and Human Services found that in 2022, US prices across all brand-name and generic drugs were nearly three times as high as prices in 33 other wealthy countries. That means that for every dollar paid in other countries for prescription drugs, Americans paid $2.78. And that gap is widening over time. Focusing on drugs from the three companies represented at the hearing (J&J, Merck, and Bristol Myers Squibb), the Senate report looked at how initial prices for new drugs entering the US market have skyrocketed over the past two decades. The analysis found that from 2004 to 2008, the median launch price of innovative prescription drugs sold by J&J, Merck, and Bristol Myers Squibb was over $14,000. But, over the past five years, the median launch price was over $238,000. Those numbers account for inflation. The report focused on high-profit drugs from each of the drug makers. Merck's Keytruda, a cancer drug, costs $191,000 a year in the US, but is just $91,000 in France and $44,000 in Japan. J&J's HIV drug, Symtuza, is $56,000 in the US, but only $14,000 in Canada. And Bristol Myers Squibb's Eliquis, used to prevent strokes, costs $7,100 in the US, but $760 in the UK and $900 in Canada. Sanders asked Bristol Myers Squibb's CEO Boerner if the company would \"reduce the list price of Eliquis in the United States to the price that you charge in Canada, where you make a profit?\" Boerner replied that \"we can’t make that commitment primarily because the prices in these two countries have very different systems.\" The powerful pharmaceutical trade group PhRMA, published a blog post before the hearing saying that comparing US drug prices to prices in other countries \"hurts patients.\" The group argued that Americans have broader, faster access to drugs than people in other countries. Page: 1 2 Next → reader comments 486 Beth Mole Beth is Ars Technica’s Senior Health Reporter. Beth has a Ph.D. in microbiology from the University of North Carolina at Chapel Hill and attended the Science Communication program at the University of California, Santa Cruz. She specializes in covering infectious diseases, public health, and microbes. Advertisement Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39405547",
    "commentBody": "Big Pharma spends billions more on executives and stockholders than on R&D (arstechnica.com)224 points by PaulHoule 8 hours agohidepastfavorite88 comments monkburger 4 hours agoBig Pharma's profits are a result of taxpayer funded research via the NIH and others for drug discovery. https://www.ineteconomics.org/perspectives/blog/us-tax-dolla... Big Pharma has a very powerful lobbying arm that is against Medicare negotiating down prices. reply blackbear_ 2 hours agoparentIt is true that pharma companies take advantage of publicly funded research, but from there to brining a new drug on the market it takes further 10/15 years and 100s million dollars of R&D. A large portion of the cost comes from the fact that most of these projects prove infeasible and are abandoned during development or later during trials. The vast majority of this effort remains unpublished, so I am not sure that the article you cite supports your argument. Most of the research mentioned in that article talks about possible associations between target and disease, which is just the earliest step of drug development. reply consumer451 1 hour agorootparent> 100s million dollars of R&D It is worth noting that a significant portion of the D in R&D is money spent on direct to consumer advertising. This only happens in the USA, as I believe that it is not legal elsewhere. reply ekianjo 56 minutes agorootparentare you sure? usually this is counted as marketing costs. reply consumer451 51 minutes agorootparentThanks for checking me on this. It was just what I recalled from memory, but I believe that the following links confirm this. > The pharmaceutical industry devoted $83 billion to R&D expenditures in 2019. Those expenditures covered a variety of activities, including discovering and testing new drugs, developing incremental innovations such as product extensions, and clinical testing for safety-monitoring or marketing purposes [0] > 6.56B spent on direct to consumer pharma advertising in 2019 [1], with nearly $8.1 billion spent in 2022. [2] [0] https://www.cbo.gov/publication/57126#_idTextAnchor003 [1] https://www.statista.com/statistics/686906/pharma-ad-spend-u... [2] https://www.fiercepharma.com/special-reports/top-10-pharma-d... reply swores 1 hour agorootparentprevThe thing is, pharma industry is clearly hugely profitable, so why doesn't the government invest in a wide shotgun approach to fund half (or more) of the entire industry and in return take half (or more) of the profits to either keep as revenue or reinvest in pharma or in other healthcare? (Obviously theoretically ideal economically would be if the government could just have a socialised department discovering and developing all new drugs needed with no profit margins, but then you lose out on benefits of competition, profit motives etc. so a split model makes sense, where they're still private companies but the gov invests hugely in them - but in return for the fair share of profits, rather than so many current grants where the gov pays funding and it doesn't even get them a good discount on buying the overpriced final drug. reply vasco 59 minutes agorootparentThe same government that can't come up with sensible policies to regulate big pharma and gets played by all kinds of lobbyists suddenly is going to figure out how to develop drugs more efficiently than the industry does? reply swores 49 minutes agorootparentI'm suggesting the government take the role of investor, replacing (or rather, overshadowing) the financial firms & private owners who are the current investors, not taking over the way the companies work. Could either give a body like the NIH a huge new budget to use for investing in pharma companies, or could pass a law that says any company that successfully raises X money with Y criteria (either in VC, or by IPO, or however) must automatically allow the government to buy x% at the same price (or a slight discount) as other buyers? And sure, lobbyists, money in politics and quality of current US politicians are a problem to face with pretty much any aspect of improving the country, but I don't think that's a reason to never discuss ideas about how to do so. (Then again I'm not American so I don't have horse in this race, but I feel similarly about my UK.) reply Nevermark 26 minutes agorootparentPumping dumb money into a sector isn't likely to translate into profits. It will translate to a bubble and a lot of opportunists taking the money. It isn't that it is impossible, but it would take a slow start, and sustained experimentation and organizational learning over many years, before the amount of money that could make a dent in the problem could reliably perform well. All while VCs would be lobbying to end the incursion, and the easier money - therefore lower returns, it would inflict on their territory. reply NavinF 1 hour agorootparentprevYou could say that about any industry. \"The thing is, the S&P 500 is clearly hugely profitable, so why doesn't the government invest in a wide shotgun approach to fund half (or more) of the entire S&P 500 and in return take half (or more) of the profits to either keep as revenue or reinvest in the S&P 500 or in other stock indexes?\" Gov't investment in any industry just because it's profitable is a bad idea for obvious reasons. reply swores 1 hour agorootparentSure, except that investing in the S&P is just artificially inflating the economy as a whole, whereas my suggestion isn't intended to be \"this is my advice for the most profitable use of a government $\", it's a suggestion for boosting a small part of the economy to speed up the development / progress of new drugs as a societal benefit. People who defend the huge profit amounts of pharma companies (which ultimately come from consumers needing healthcare, paying either directly or through insurance) say it's fair because of the huge amount they invest to create the drugs, if the government does most of the investing then they can take most of the profits to subsidise free pills for their population. Seems to me like a more direct benefit (in addition to potential economic benefits of boosting investment levels in a high-exporting industry too) than just the government buying S&P500 index funds. reply suzzer99 2 hours agoparentprev> Big Pharma has a very powerful lobbying arm that is against Medicare negotiating down prices. I particularly liked how all the Congresspeople involved in writing that the government couldn't negotiate drug prices into Medicare Part D immediately retired and went to work for the drug industry. reply consumer451 2 hours agorootparentWhen faced with a regulatory issue in the USA, I often look at EU regs for reference. Here is their inquiry on the \"revolving door\" issue: https://www.ombudsman.europa.eu/en/decision/en/155953 reply jimmar 5 hours agoprev\"Spending billions on stockholders\" is an odd way to frame giving profits to the owners (the shareholders). Does a small business owner spend money on herself when she makes a profit and moves money from the company account to her personal checking account? Calling stockholders an expense misrepresents how businesses operate. \"Spending\" implies expense. Stock buybacks are a way of returning value to shareholders. reply graemep 5 hours agoparentThe point is that most of that profit comes from government incentives that are supposed to encourage R & D. i.e. we pay several times more on drugs than we would without paying those incentives to the businesses, but only a small proportion of that extra money goes into R & D. Its incredibly inefficient funding. My take on that problem. There are others. https://pietersz.co.uk/2007/02/patents-inefficient I made the same point nearly 10 years before that with regard to R & D vs marketing expenditure. Its not a new problem. reply jimmar 5 hours agorootparentIt seems like we want for-profit pharmaceutical corporations to act like non-profit institutions. Any idea why non-profit institutions haven't risen in the world of pharmaceuticals? reply tux3 4 hours agorootparentIt takes very many million dollars to run a clinical trial, why the rhetorical question? It's just funding. Not like non-profits are magically unable to have talent, or to achieve results. No one's giving away billions on a pharma non-profit when they could invest. Even though the result is sometimes similar for the investor. reply jimmar 4 hours agorootparentIt wasn't a rhetorical question. I could imagine an entity like the Gates Foundation proving seed funding to bootstrap a non-profit pharmaceutical company. That company could fund R&D through sales of its newly developed medicines. No shareholders. No stock buybacks. But somehow this doesn't seem to happen, at least not on a scale that reduces medical costs for consumers. reply vasco 55 minutes agorootparentBecause those researchers could be shareholders of whichever biomedical startup they will create instead of a non-profit. reply tomohelix 4 hours agorootparentprev> It's just funding. And to have funding, you have to attract investors. How to attract investors? By making it quite profitable to invest in the company, i.e. making it profitable for the shareholders. With enough money, you can do anything big pharma is doing. But how to get that money consistently is another question that have an uncomfortable answer. Plenty of startup and small biotech companies go bankrupt every year. This isn't something you can do in a garage and turn it into a multibillion dollar business like software. Biopharma has too many regulations and rules to follow and it makes the entire field incredibly risky and less likely for a small and unattractive player to succeed, even if they are driven by passion. reply throwaway14356 4 hours agorootparentprevI use to believe that. I then discover some private groups of people sharing a disease doing all kinds of trials. Nothing was to absurd. Almost brute forcing the problem. I thought a wiki written by mostly dead people was quite humbling. There has to be some middle ground between the 2 branches of insanity? reply santoshalper 4 hours agorootparentprevOr, we want for-profit pharmaceutical corporations to not take taxpayer dollars. You shouldn't be able to have it both ways, but at least in the US, you can. reply briffle 4 hours agorootparentI don’t like the percentage profit they take in the US, they can sell a drug outside the US for $50, but charge $300 here. Either we are subsidizing other countries, or they are fleecing us for what they can get, often on the back of research funded by tax dollars. reply tfourb 3 hours agorootparentThe latter, as these companies were perfectly profitable even if they wouldn't charge inflated prices in the US. Not quite as profitable as currently, but still not going bankrupt. reply s1artibartfast 4 hours agorootparentprevI hear this a lot, and I think that there is a fundamental misunderstanding of what's going on. People aren't upset by for-profit grocery stores taking tax dollars in the form of food stamps. They aren't upset by taxpayers giving money to for-profit hospitals to treat poor patients. The government is paying a For profit company to do something that the government wants. If the company would do it anyways, then of course I agree that the government shouldn't pay for it. reply Beldin 15 minutes agorootparent> People aren't upset by for-profit grocery stores taking tax dollars in the form of food stamps. They might feel different if foodstamp-bought food was 10x as expensive as regular food, and those with foodstamps were forbidden from buying food for non-foodstamp prices. reply tfourb 3 hours agorootparentprevI think the difference is the extend of the profit. Big Pharma is incredibly profitable, while grocery stores are operating on very thin margins. Few people have a problem with a company doing business based on government expenditure, but specializing in maximizing profit from this reeks of profiteering. reply ejb999 1 hour agorootparentJust as a point of reference, if you bought 100K in Pfizer stock 25 years ago, it would now be worth about 100K. Adjusted for inflation, you would have lost about 50% of it’s value. Not what I would call ‘incredibly profitable’. And they are one of the biggest. reply timthorn 9 minutes agorootparentDoes that include the value of dividends paid over the period? LudwigNagasena 1 hour agorootparentprevSeems that the problem is with government incentives rather than with companies? reply bryanrasmussen 2 hours agorootparentprev>we pay several times more on drugs than we would without paying those incentives to the businesses that really depends on your economic system doesn't it? In the more capitalist systems you pay what the market will bear and there is no consideration as to what the incentives are or aren't. reply ruined 2 hours agorootparentsometimes the market bears political incentives reply s1artibartfast 4 hours agorootparentprevI feel like it is very easy to claim a top down command economy would be more efficient. On paper it makes sense. You have no stakeholder profit and no marketing budget. You just tell scientists what to make and doctors just prescribe whatever comes out the other end. However, it is hard to find real examples that bear this out. I think that this View overestimates the capability of institutions and underestimates the utility of price signals and an open market with competition. reply tfourb 2 hours agorootparentHealth systems in other (comparable) countries, where drug prices are centrally negotiated or outright set by government committees are vastly more efficient in terms of cost and generally produce better results in terms of health outcomes. I.e. in my native Germany, health expenditure per capita is 2/3 of what it is in the U.S. if faced with a life-threatening illness, I would vastly prefer to be in Germany than in the U.S. (with the possible exception of being super-rich and being able to buy top-of-the-line health care). reply snowpid 51 minutes agorootparentThis is still not a centralised system. There are still prices. Just the negjator is the public health system. reply Hammershaft 3 hours agorootparentprevWe could take a portion of those r&d incentives and directly test and compare the cost & output of government pharma research vs private research in a mixed model. reply throwawayqqq11 3 hours agorootparentprevPrice signals are just that, signals and there are others for sure, like regulation. You dont have to imagine a top down economy and fail to find examples, when simply regulation would do the trick and keep some enterprise freedom. reply 6510 2 hours agorootparentprevhttps://en.wikipedia.org/wiki/Trappist_beer > The beer must be brewed within the walls of a Trappist monastery, either by the monks themselves or under their supervision. > The brewery must be of secondary importance within the monastery and it should witness to the business practices proper to a monastic way of life. > The brewery is not intended to be a profit-making venture. The income covers the living expenses of the monks and the maintenance of the buildings and grounds. Whatever remains is donated to charity for social work and to help persons in need. It's funny to see above the suits and ties a layer of habits and scapulars[1] https://www.koningshoeven.nl/media/127/compilatie-inkleding.... reply sudosysgen 3 hours agorootparentprevA lot of pharmaceuticals do however end up coming from what's essentially a non-profit system. It doesn't need to be a pure command economy. The price signals are already broken. The people doing the research that is fundamental to pharmaceuticals are paid by grants which aren't connected to the price signals. There is a market failure here. reply haswell 5 hours agoparentprevPlenty of companies do stock buybacks without also engaging in predatory pricing that customers are powerless to fight. The situation is even more problematic when you consider the lack of alternatives, and the often live-preserving nature of the product. > Stock buybacks are a way of returning value to shareholders. This is also a problematic framing that ignores the broader situation. And that’s the tricky thing about framing things. It’s easy to manipulate the narrative and obscure the unreasonableness of the underlying reality if you sufficiently narrow the frame. reply s1artibartfast 4 hours agorootparentI think this understates the power and options of the customers. The typical alternative to receive what was state of the art care 10 years prior and off patent, and now 5% the price. The unfortunate part is the desire for such products is so low, that most of the time they are discontinued. It is essentially a tragedy of the commons in the US. Because all the costs are shared, every individual is happy to pay 20X for a product that is 10% better. reply aknfffn 2 hours agoparentprevThe moral dilemma is profiting from disease and the ill-health of a population. The only argument against publicly funded drug production is that profit seeking is more efficient and the societal outcome is improved by not funding a public body to do this. As other comments highlight, not only do societies publicly fund research (eg C19), they provide a legal system that protects monopoly profits, and won’t use single-payer bargaining on price… So when pharmaceuticals then drop more cash on executives and shareholders it’s natural to question turning Covid19, cancer, or obesity etc into a profit centre for the wealthiest in society reply carlosjobim 1 hour agorootparentProfiting from hunger, those pesky cooks. Profiting from people freezing, those deplorable tailors... reply darth_avocado 2 hours agoparentprevOkay, but they also spend more money on sales and marketing than they do on RnD. Surely that can be considered as “spending”. reply lcnPylGDnU4H9OF 5 hours agoparentprevThey led with mentioning executives, which is spending on employee salaries and bonuses. At any rate, I think it’s the same point to say “makes billions more in profit than they spend on R&D”. reply oaiey 3 hours agoparentprevWhile that is true and how unregulated capitalism works, a society which enables capitalism (by laws, judges, ...) does not need to tolerate excessive profits on costs of its populations. Capitalism is not a law of nature. It is made by humans and can be adopted by the society. This is very common around the world. reply __loam 2 hours agoparentprevOkay but stock buybacks should be illegal. reply petre 4 hours agoparentprevHopefully you'll never need cancer treatment, because it would probably change your opinion. reply s1artibartfast 4 hours agorootparentHopefully I will never have to test it, but I imagine cancer patients are also glad they don't receive a 1900's standard of care. Those that I do know who have had cancer were absolutely thankful that someone developed their treatments. reply mfiguiere 2 hours agoprevAs mentioned by several comments, while one can argue that Big Pharma should spend more on R&D, the title of the article is not so relevant. From Apple financial statement for 2023 [1]: - Spent on R&D: $29.9B - Spent on stockholders (dividends + buybacks): $92.6B [1] https://www.apple.com/newsroom/pdfs/fy2023-q4/FY23_Q4_Consol... reply mplewis 1 hour agoparentWe aren’t spending public money on Apple. reply mdorazio 5 hours agoprevSo... what is Congress actually going to do about it? Fix the broken patent system? Put a ceiling on pharma profits the same way health insurance profits are limited (80/20 rule)? Or just point fingers and continue taking campaign donation checks? reply toomuchtodo 5 hours agoparenthttps://www.hhs.gov/about/news/2023/08/29/hhs-selects-the-fi... (\"HHS Selects the First Drugs for Medicare Drug Price Negotiation\") https://finance.yahoo.com/news/pharmaceutical-groups-lawsuit... (\"Pharmaceutical group's lawsuit over Medicare drug price program dismissed\") Jon Stewart says it best (because of course he does): https://youtu.be/NpBPm0b9deQ?t=1153 \"The work of making this world resemble one that you would prefer to live in is a lunch pail [bleep] job, day in and day out, where thousands of committed, anonymous, smart, and dedicated people bang on closed doors and pick up those that are fallen and grind away on issues till they get a positive result.\" You keep on grinding. > The Inflation Reduction Act requires the federal government to negotiate the price of certain high-spending drugs covered by Medicare Part D, Medicare’s outpatient prescription drug benefit program, and Medicare Part B, which covers physician and outpatient services, including drugs administered by physicians and other providers. Under the new Medicare Drug Price Negotiation Program, the number of drugs subject to price negotiation will be limited to 10 Part D drugs for 2026, another 15 Part D drugs for 2027, another 15 Part D and Part B drugs for 2028, and another 20 Part D and Part B drugs for 2029 and later years. The number of drugs with negotiated prices available will accumulate over time. https://www.kff.org/medicare/issue-brief/a-small-number-of-d... reply petre 4 hours agoparentprevFrance caps the prices the state is willing to pay, so the drugs are compensated through state owned health insurance if the company isn't selling much. Not all of them are 100% covered, but one can additionaly buy private insurance to cover everything. reply catchnear4321 5 hours agoparentprevthey will continue to beat the markets. (not because they cheat, because they are our betters.) reply transcriptase 5 hours agorootparentNancy Pelosi’s success at options trading at 83 years old is really quite remarkable. Imagine having returns that dunk on even the best financial minds on Wall Street as an octogenarian congresswoman nailing calls on tech stocks. I guess the rest of us just have a “skill issue” as the kids say. reply jamiek88 5 hours agorootparentShe's an amateur and is actually married to a hedgie, look up how much Mitch McConnell has made. reply ceejayoz 5 hours agorootparentprevThe rest of us aren’t married to a prominent VC from whom much of the wealth originates. reply defrost 5 hours agorootparentprevImagine marrying a twenty year younger venture capitalist toy boy to manage her finances for her .. that's likely to be the remarkable secret sauce. reply dleavitt 1 hour agoprevThe article didn't focus too much on solutions, though a couple of things were mentioned: 1. Reducing patent protection 2. Price controls (?) So let's assume we want these companies to reduce their profit margins (and instead lower drug prices or spend more on R&D.) What _are_ the best policy tools to incentivize this while minimizing market distortions, etc? reply abhisheksingh01 1 hour agoprevI read somewhere that the amount of money that Pharma companies spend on research is the highest in healthcare but they get only about ~13% of the healthcare's revenue. There are other middle-man in healthcare that get a revenue share without putting in any work. This is obviously not to say that Big Pharma cannot do better and there are no red flags. reply anonzzzies 2 hours agoprevI think everyone here knows how big pharma works (well, doesn’t ‘work’ as such) however this needs to get into the press a lot more for the public to see. reply missedthecue 2 hours agoprevI think a lot of laypeople have the idea that there are an infinite number of positive return projects waiting to be funded and pharma execs are just too fat and greedy. There are very few pharma projects that aren't outright frauds and that actually have potential and that don't cost way more to get to market than they can ever possibly justify. Very seldom do those project lack money. reply __loam 2 hours agoprevAmgen is basically a finance firm that does drug development as a hobby. reply anonym29 5 hours agoprevNow compare R&D and exec/shareholder pay to expenditure on \"lobbying\" (i.e. legalized bribery) and corporate capture of the \"strong\" and \"independent\" government regulatory agencies that were meant to keep these very same companies in line. reply Solvency 6 hours agoprevYou can write this article about the vast majority of corporations. reply toomuchtodo 5 hours agoparentYou eat an elephant one bite at a time. If we’re gonna start anywhere, pharmaceutical companies are a fine place to compress margins. You know, because people need these products to survive, and corporate existence as well as profits aren’t guaranteed. We can always come for additional sectors in the future, depending on objectives and resourcing. reply anonym29 5 hours agorootparentWhy don't the scientists actually doing the work of drug discovery just start a not-for-profit, since the executives and shareholders aren't actually adding any value to the customers here? reply toomuchtodo 5 hours agorootparentDo you mean the NSF and NIH? Certainly, fund them with what currently goes to pharma for executives, shareholders, lobbying, and marketing. Fund meaningful work, not gravy for corporate performance artists (Part D is funded by general revenues, beneficiary premiums, and state contributions). Cut out the middleman. \"In 2021, Medicare Part D covered more than 3,500 prescription drug products, with total gross spending of $216 billion, not accounting for rebates paid by drug manufacturers to pharmacy benefit managers (PBMs).\" https://www.kff.org/medicare/issue-brief/a-small-number-of-d... From this post: > Peter Maybarduk, the director of the Access to Medicines program at Public Citizen, a watchdog organization, who also testified at the hearing, hit back at the main pharmaceutical talking points along with Sanders. That included noting that the makers of the 10 drugs selected for the first round of Medicare price negotiation spent $10 billion more on self-enriching activities than R&D. ($200B/year buys a lot of manufacturing, distribution, admin, and whatever is left over for amortized and ongoing R&D) reply pompino 3 hours agorootparent>Fund meaningful work, not gravy for corporate performance artists (Part D is funded by general revenues, beneficiary premiums, and state contributions). The problem is that nobody actually knows or can predict what \"meaningful work\" is because drug discovery is a graveyard with a >90% death rate. reply pompino 3 hours agorootparentprevBecause fundamentally most of science is a dead-end. Most of drug discovery is a further dead end with a 90% failure rate. Nobody wants to pay for it, and big-pharma uses a few blockbuster products to pay for all their other failures. reply asadotzler 4 hours agorootparentprevThey did. They work for the US Government which does all the work of drug discovery which gets handed off, along with huge grants and other benefits, to private companies to productive and then sell at ridiculous profits that fuel massive executive pay packages and huge buybacks. The rich get richer while the taxpayers and most users of the ultimate product get screwed. It's really that simple. reply Sabinus 5 hours agorootparentprevBecause drug discovery needs patient trials, and the massive cost of those trails drives scientists into the arms of investors and the for-profit sector. reply aaomidi 4 hours agorootparentprevnot-for-profit companies are subject to a lot of bullshittery too. reply carlosjobim 1 hour agorootparentprevYou're speaking as if you were a movie villain or a communist dictator. Pure day dreaming. reply wedalas452 42 minutes agoprev\"It's just pure coincidence we were working on this MRNA failed product just in time for COVID, give us money globalist governments all over the world\" reply refurb 5 hours agoprevThat year, the company spent $17.8 billion on stock buybacks, dividends, and executive compensation, while the company spent just $14.6 billion on R&D, the report states. \"In other words, the company spent $3.2 billion more enriching executives and stockholders than finding new cures,\" it concludes. This is such a bizarre criticism. Note they include executive compensation, despite it being $26M out of the $14.6B (0.15%). And \"spending\" on dividends? You mean \"returning money to shareholders\". How do they think businesses work? And since I have a 401k invested in index funds, I'm a shareholder. So the criticism of \"spending on shareholders\" they actually mean \"vast swaths of Americans whose retirement and pensions are invested in the stock market\". reply consp 5 hours agoparent> returning money to shareholders There is nothing to return. Handing out is what it is. > And since I have a 401k invested in index funds, I'm a shareholder. So the criticism of \"spending on shareholders\" they actually mean \"vast swaths of Americans whose retirement and pensions are invested in the stock market This is a common repeated fallacy. Nice for you to profit but this is no whole gain for society. reply BenFranklin100 4 hours agorootparentThe ‘whole gain’ for society is the medications that come out of investors putting money into these companies, instead of, say, Facebook. In 10-15 years, every new medication released today, will be off patent — and with insurance helping with payments taking into consideration — essentially free. Mind you, we are also talking about diseases for which no therapies exist today, and therefore the cost is effectively infinite in 2024. Another point this story glosses over is that the $14.6B spent on R&D, represents nearly 1/3 of the entire National Institutes for Health $47B 2023 budget. And this is by a single company. reply throwawayqqq11 5 hours agorootparentprevEven better, the more ground level and more risky research is often funded publicly, only to be bought or go private, once a product could emerge. Big pharma is hawking over public research. reply BenFranklin100 3 hours agorootparentThis is an oft-repeated sound bite by politicians looking to score cheap points, but it isn’t true. https://www.science.org/content/blog-post/rep-ocasio-cortez-... reply throwawayqqq11 2 hours agorootparentI dont buy it. Your linked blog post is about Derek Lowe gathering anecdotes, mostly. His linked article (a better one) https://www.science.org/content/blog-post/where-drugs-come-n... is biased, because it only looks at approved drugs. He himself turns slightly to the R&D side and almost gets it. > And now to innovation - 118 of the drugs during this period were considered to have scientific novelty (46%), and of those: > 44% were from pharmaceutical companies. > 25% were from biotech companies, and > 31% were from universities (transferred to either biotech or pharma). > The university-derived drugs clearly outperform in this category. What this also means is that 65% of the pharma-derived drugs get classed as \"not innovative\" To be fair, i dont think you can convince me, that the pharma industry has no incentive to externalize risky or less profitable research and would never do it. I have one anecdote for you too, hitting that exact spot: anti-biotics research. Because even though it has good prospect for profit, it is too risky research (kind of diminishing returns of R&D in that area). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7872909/ reply jollyoldpirate 2 hours agoprevnext [3 more] [flagged] Retr0id 2 hours agoparentMaybe it's just the threads I click on, but I can't recall the last time I even saw Putin mentioned on HN. reply wedalas452 40 minutes agoparentprevOnly people who have Putin living rent free in their minds are those who had previously Trump as a resident. Obey the man on the TV and pay more taxes. reply andxor_ 5 hours agoprevnext [2 more] [flagged] burnJS 5 hours agoparentAnd we don't get to wine why? reply ajuc 1 hour agoprev [–] I grew up as an enthusiast of free market capitalism without regulation (probably because my country was just exiting communism when I was 7 and I've seen what good free market reforms can bring). But over last 15 years I've moved significantly towards free market social welfare state. It's true that central planning doesn't work, and that communism doesn't work. It's painfully obvious to anybody who lived it. But capitalism leads to results just as bad if left unchecked for too long. It's just getting there slower than other systems. Even the rules we add don't stop it, they only slow down the gradual descent into oligarchy. At which point it doesn't matter what the rules are - because the oligarchs are choosing presidents, PMs and judges. Now I think the perfect economic system would be free market capitalism for 50 years, then a total nationalisation of everything, reshuffle and restart. Rinse and repeat. Capitalism is like a game of football that lasts forever. No matter what you do, if you're born into Real Madrid you'll win, because the score is already 1000000000-10. There's a reason we restart the scores after every game. reply timthorn 3 minutes agoparent> free market capitalism for 50 years, then a total nationalisation of everything, reshuffle and restart. You could argue that's similar to the UK's recent history, if you can be flexible with the 50 year period. Minimal regulation and state ownership pre-war, mass nationalisation post-war, mass privatisation in the 1980s reply wedalas452 38 minutes agoparentprev [–] \"free market capitalism\" never existed. It's the rich dynasties in small black hats bankrolling their puppets around the globe. The rest are semantics - white, black, blue, red, left, right. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Senate report exposes big pharmaceutical companies' focus on executives and stockholders over research and development, resulting in high drug prices in the US.",
      "Pharmaceutical greed, patent manipulation, and strong lobbying are identified as key factors driving inflated drug costs, according to the report.",
      "CEOs of major pharma firms faced scrutiny in a Senate hearing led by Sen. Bernie Sanders, who condemned prioritizing profits over affordability, revealing that Americans pay more for prescription drugs than citizens in other affluent nations."
    ],
    "commentSummary": [
      "The debate focuses on the ethics and efficiency of the pharmaceutical industry, highlighting issues like high profits, research investment, and government support.",
      "Proposals include government funding for for-profit firms, non-profit pharmaceutical models, and regulations to tackle excessive profits.",
      "Key concerns raised involve Medicare price discussions, executive pay, and the emphasis on research and development, aiming at balancing profit motives and public health in healthcare."
    ],
    "points": 224,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1708134874
  },
  {
    "id": 39397961,
    "title": "$5 Device Revolutionizes Breast Cancer Screening",
    "originLink": "https://studyfinds.org/device-breast-cancer-5-seconds/",
    "originBody": "Photo by Vishnu Mohanan from Unsplash CANCER NEWS, SCIENCE & TECHNOLOGY NEWS $5 device accurately tests for breast cancer in under 5 seconds FEBRUARY 15, 2024ADD A COMMENT by StudyFinds GAINESVILLE, Fla. — A five-dollar circuit board could hold the key to near-instantaneous breast cancer screenings. Scientists say a new handheld device is capable of testing for breast cancer in less than five seconds using just a small sample of saliva. This portable device is not only remarkably quick and user-friendly but also highly cost-effective. The $5 device, along with its test strips that cost just a few cents each, uses common components. These include glucose testing strips readily available on the market and Arduino, an open-source hardware-software platform. The biosensor, a collaborative development by the University of Florida and National Yang Ming Chiao Tung University in Taiwan, employs paper test strips coated with specific antibodies. These antibodies interact with cancer biomarkers targeted in the test. Upon placing a drop of saliva on the strip, electrical pulses are sent to contact points on the biosensor device. This process leads to the binding of biomarkers to antibodies, resulting in a measurable change in the output signal. This change is then converted into digital data, indicating the biomarker’s presence. The researchers highlight that this device stands as a revolutionary alternative to traditional methods such as mammograms, ultrasounds, and MRIs. These conventional methods often involve radiation exposure, are expensive, invasive, slow, and require bulky equipment. The printed circuit board used in the saliva-based biosensor, which can detect breast cancer biomarkers from extremely small saliva samples in about five seconds, costs about $5. The design uses widely available components such as common glucose testing strips and the open-source Arduino platform. (Credit: Hsiao-Hsuan Wan) The team behind this innovation hopes it will enable early breast cancer detection worldwide. “Imagine medical staff conducting breast cancer screening in communities or hospitals. Our device is an excellent choice because it is portable — about the size of your hand — and reusable,” says the author of the study, Hsiao-Hsuan Wan, a Ph.D. student at the University of Florida, in a media release. “The testing time is under five seconds per sample, which makes it highly efficient.” Wan notes that in many regions, particularly in developing countries, advanced technologies for breast cancer testing, such as MRIs, may not be readily accessible. “Our technology is more cost-effective, with the test strip costing just a few cents and the reusable circuit board priced at $5,” Wan says. “We are excited about the potential to make a significant impact in areas where people might not have had the resources for breast cancer screening tests before.” Testing results demonstrate the device’s ability to provide accurate results even with biomarker concentrations as low as one femtogram per milliliter. “The highlight for me was when I saw readings that clearly distinguished between healthy individuals and those with cancer,” Wan highlights. “We dedicated a lot of time and effort to perfecting the strip, board, and other components. Ultimately, we’ve created a technique that has the potential to help people all around the world.” The study is published in the Journal of Vacuum Science & Technology B. You might also be interested in: Losing Weight Without Trying? It Could Signal Cancer, Study Warns Breast cancer detector using wearable ultrasound patches fits inside a woman’s bra 10 Important Things to Know About Cancer Screenings SWNS writer Isobel Williams contributed to this report. TAGS: BIOMARKER, BREAST CANCER, CANCER DETECTION, CANCER SCREENINGS, CIRCUIT BOARD, FEMALE BREAST CANCER, SALIVA, TESTS Add a Comment Random Article Next Article About the Author StudyFinds View StudyFinds's article archive The contents of this website do not constitute advice and are provided for informational purposes only. See our full disclaimer Related Studies CANCER NEWS Ovarian cancer could be detected earlier via simple urine test FEBRUARY 16, 2024 ADD A COMMENT DRUG & MEDICATION NEWS Drug Used To Treat Cocaine Addiction Emerges As Powerful Colorectal Cancer Therapy FEBRUARY 15, 2024 ADD A COMMENT MARIJUANA NEWS Cannabis extract kills skin cancer cells, study reveals FEBRUARY 14, 2024 ADD A COMMENT Leave a Reply Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Add a Comment",
    "commentLink": "https://news.ycombinator.com/item?id=39397961",
    "commentBody": "$5 device tests for breast cancer in under 5 seconds: study (studyfinds.org)224 points by Brajeshwar 18 hours agohidepastfavorite126 comments e63f67dd-065b 18 hours agoPress release: https://publishing.aip.org/publications/latest-content/would... Actual publication: https://pubs.aip.org/avs/jvb/article/42/2/023202/3262988/Hig... Experiment size is literally N=21, with 4 healthy participants, 3 in-situ breast cancers, and 14 invasive breast cancers. N=21 might as well be useless in my opinion. You can't draw any meaningful conclusions about statistical power of this test; if your priors were 10% for breast cancer, after taking this test, your posterior probably remains unchanged. reply jncfhnb 17 hours agoparentYou can absolutely draw conclusions about the statistical power of the test. That’s what statistics is for. If this sample data is randomly sampled it looks like it will be a fairly high precision test for invasive cancers, with room for false negatives. You would have had to have gotten very unlucky to see such a difference in distributions even on a small sample like this. But sure, let’s get more samples. reply hn_throwaway_99 16 hours agorootparentThank you for this! I get frustrated with the \"N of only 21? Might as well flip a coin!\" responses. Like you say, the whole purpose of statistical testing is to give an accurate, numeric value that says how likely the results are due to chance. One thing I'd note, though, is that the paper's title is \"High sensitivity saliva-based biosensor in detection of breast cancer biomarkers: HER2 and CA15-3\". My understanding is that sensitivity was never really the problem with breast cancer detection - it's specificity that is the real challenge with all types of broadly-deployed medical screening tools. reply wolverine876 15 hours agorootparentPeople on HN (maybe elsewhere too) devote a lot of attention to sample sizes. I don't know the upthread commenter at all, but in general I suspect it's because that is an easy thing to understand about research and statistics, and it's a valid critique they've seen professionals use. A normal human fallacy is to focus on the thing you understand, that is easy to understand (e.g., easy to quantify), and overlook the difficult issues that are far more important. There is much more, of much more importance going on in most of these studies, and in their statistics and validity, than the sample size. reply treflop 12 hours agorootparentElsewhere too I first took stats in high school and we read a bunch of valid small sample size studies so I don’t know where people are failing to get educated. There were so many factors going into whether a study was good. reply ajb 11 hours agorootparentExactly. Famous example: Lady tasting Tea [1]. N=1 or N=8 depending on how you look at it. Still significant. This is the one that proves that it's possible to tell whether you added the milk first or second. [1] https://en.wikipedia.org/wiki/Lady_tasting_tea reply ryandrake 16 hours agorootparentprevNo matter what the value of N is, someone always comes out of the woodwork to complain about it. It's a pretty common criticism of any study that relies on statistics, and one anyone can make in a few sentences. reply lofatdairy 16 hours agorootparentprevSensitivity matters in this case because the medium is saliva which contains a fraction of the antigens contained in serum. Specificity is challenging when the biomarkers are non-specific and you only have 1 testing modality but that seems to be not so much the case here. reply serial_dev 14 hours agorootparentprevMy assumption is that 98% of people have no idea what the right sample size is for this particular experiment, including myself of course. To all who criticize and ridicule someone who would like to have more samples, why do you think 21 is such a perfect number in this case? Wouldn't 15 be enough, if statistics and all applies? 10? 1? Would 30 be too much? reply jncfhnb 13 hours agorootparentIt is a function of the effect size, not the experiment. When the effect size is large, you need fewer samples to detect it reliably. In this case it looks pretty large. Would you want more people before rolling it out? Of course. But it’s very unlikely to be vaporware provided the samples are random. reply kelseyfrog 13 hours agorootparentThank you! Came here to say, but found, that you've already written it. Effect size is going to be important when talking about clinical significance, not just statistic significance, too. I also want to point out that we have no stats on sensitivity, specificity, or diagnostic odds ratio, which are all clinically relevant to physicians deciding when to test and how to interpret test results. The good news is that it's a non-invasive, low cost test which plays a factor in clinical decision-making. reply hn_throwaway_99 13 hours agorootparentprev> To all who criticize and ridicule someone who would like to have more samples, why do you think 21 is such a perfect number in this case? Wouldn't 15 be enough, if statistics and all applies? 10? 1? Would 30 be too much? This is the point that you are fundamentally misunderstanding. Nobody is making the argument that \"21 is such a perfect number in this case\". The rest of your sentences (\"Wouldn't 15 be enough, if statistics and all applies? 10? 1? Would 30 be too much?\") seem to point to a belief that people are pulling numbers based on a finger in the wind. The whole point of (a lot of) statistical testing is that it allows you to come up with a specific number that determines how likely a particular result is due to chance. That is what the pNow there are 8 billion people, so it's worth 16 times less. That is not how statistics works, at all. > In fact, this is why I always trust science in small towns more. In Colma, for instance, you sample 500 and you have covered 33% of the people. These are the sentences that made me think this had to be satire (presumably you want \"science\" to apply to places outside of Colma...), but in all honesty these days it's really hard to tell. reply mbreese 15 hours agorootparentprevWith so few controls, it’s still not a well designed comparison. Most scientists deal with low sample sizes, especially in first trials. However, what is often overlooked is the need for sufficient numbers of negative controls. If they only had 4 control patients, that’s completely inadequate to draw any real conclusions. With access to a biobank at U of Florida, they should have been able to test more samples — especially with a $5 test. There are two other issues with this paper [1] that I quickly see. Their main figure lacks errors bars. It’s pretty clear to me that the groups would overlap quite a bit. More numbers would make this problem clearer (either to narrow error bars or clearly show an overlap). The lack of error bars across the paper make me think they didn’t do any technical replicates, which is also a problem. I’m also not sure a one way test is correct here, but I’m also not entirely sure how they are measuring the data. In this one way analysis, all you can tell is if one group is higher than the other. When the data are so unbalanced, what you don’t see is what the predictive value of the test is — false positive/negative. That’s the real issue here. It looks like you’d have a really high false negative rate, as the cancer samples have a much wider range than the controls. This is the worst thing you can have in a test like this. Finally — this paper was published in the “journal of vacuum sciences and technology B.” There is no way this paper got a valid peer review to make these claims. I don’t know anything about this journal, but I doubt it has much experience with cancer testing. [1] https://pubs.aip.org/avs/jvb/article/42/2/023202/3262988/Hig... reply jncfhnb 13 hours agorootparentA small control sample is not “few controls”. Misleading term. “Negative controls” is not a thing either. Just the control group is fine. The error bars are not really needed. Your eyeballs are doing just fine. Yes, they overlap. Yes, it would likely have a high false negative rate. A false negative rate is not a problem for a cheap test. This test is not meant to replace higher quality and more invasive tests. A high precision, low recall test still has significant value. You merely have to accept that a negative result tells you very little. If we imagined for instance, that the false positive rate is very low despite the overwhelmingly larger population of people without cancer, then this would be of enormous value. reply mbreese 12 hours agorootparentThis is an unbalanced study. As such, it doesn't tell you anything about the ability to differentiate between the three populations. You can argue about the terms \"negative control\" (which is appropriate here, as there is a positive control test in the paper), but there are only 4 non-cancer samples tested. That is not enough to be able to adequately know range of measurable values in the population of patients w/o cancer. But, that's not really the point. They aren't trying to diagnose cancer vs. healthy. Error bars here are absolutely necessary. Two reasons: First, you want to know the approximate ranges for each group in Figures 3 and 5. Not showing them is misleading. Secondly -- you actually also want error bars for each patient sample. I'd expect for there to be at least three replicates for each saliva sample to show that the strips are able to consistently measure a known value from each sample. I also mis-read part of the paper the first time. For the HER2 cases, there aren't 4 negative samples -- there are 20. There is only one positive sample. Part of the problem is really how they are presenting the data -- it is not all clear what they are testing. But, there is only one HER2+ sample in the mix. One... N=1. Samples include: * Non-cancer: 4 * In situ cancer: 3 * Invasive cancer, HER2-: 13 * Invasive cancer, HER2+: 1 What you'd really like to show is that the HER2+ patients could be differentiated from HER2- patients. Which, does look really good, but with only one HER2+ sample, you really can't tell much. (And the presence of so much signal in the HER2- samples raises some very interesting biological/mechanistic questions). Note: I'm not trying to say that the authors of the study are wrong or are trying to deliberately mislead people. There is so much here that could have been corrected to make this a much stronger paper. To me, this seems like a paper where the authors are likely engineers and not that well versed in biomedical statistics. The paper is published in a physics journal, so the journal itself is not a good place to make some of these arguments. Is the idea of a non-invasive test worthwhile? Yes! Absolutely. But they didn't show that it was a good test of clinical utility. They showed that it could measure differences in protein concentrations from saliva. That's not nothing, but that's it. Now, if that is an appropriate way to differentiate patients is a completely different question and requires substantially more testing (and orders of magnitude more patients). reply jncfhnb 11 hours agorootparent> This is an unbalanced study. As such, it doesn't tell you anything about the ability to differentiate between the three populations. Complete Nonsense > That is not enough to be able to adequately know range of measurable values in the population of patients w/o cancer. True. But it is a promising initial signal that the distribution of non cancerous folks is probably very different from the invasive cancer folks. The effect size here is huge. “Range” is less interesting than “Distribution” > Error bars here are absolutely necessary. Two reasons: First, you want to know the approximate ranges for each group in Figures 3 and 5. Not showing them is misleading. You don’t really need error bars when you’re showing all of a small number of data points. But sure whatever > Secondly -- you actually also want error bars for each patient sample. I'd expect for there to be at least three replicates for each saliva sample to show that the strips are able to consistently measure a known value from each sample. Would be nice. Sounds like they did ten measurements per. > What you'd really like to show is that the HER2+ patients could be differentiated from HER2- patients. Which, does look really good, but with only one HER2+ sample, you really can't tell much. (And the presence of so much signal in the HER2- samples raises some very interesting biological/mechanistic questions). I’m not clear why you’re saying HER+ vs HER- is the important difference here. reply mbreese 11 hours agorootparentLook, you obviously have your opinions here. I'm not sure just saying \"Complete nonsense\" at things is really all that helpful. What I said (\"it doesn't tell you anything about the ability to differentiate between the three populations\") is quite correct. This study shows that there is a difference between the groups of samples tested with their HER2 test strip, with a one-way p-value of ~0.002. I'm not convinced that the samples are representative of their populations. The number of non-cancer samples too low. >* You don’t really need error bars when you’re showing all of a small number of data points.* Error bars are visually helpful ways to show that the group values overlap. Which, in this case, they do (I did replot this data to confirm). >* Would be nice. Sounds like they did ten measurements per.* This is for one test. They sampled the test strip 10 times. I mean they should have tested each sample on at least 3 different test strips to get a mean value for the sample. This is a paper that is trying to say that their test strips are accurate, so it would make sense to test them multiple times. >I’m not clear why you’re saying HER+ vs HER- is the important difference here. I'm not sure what they are trying to claim in there paper... are they trying to say that they can diagnose breast cancer (which would requires many more biomarkers), or are they trying to say that they can differentiate between HER2+ and HER2- cancers (which would be more appropriate for a HER2 test). The other biomarker has even more overlap, so not sure how helpful that would be. Really, I think they are also missing an opportunity -- the bigger use for me would be in longitudinal testing. If they could show changes in signal over time for a particular patient that corresponded to treatment status -- that would be a great use for a cheap non-invasive test. reply jncfhnb 11 hours agorootparent> Look, you obviously have your opinions here. I'm not sure just saying \"Complete nonsense\" at things is really all that helpful. > What I said (\"it doesn't tell you anything about the ability to differentiate between the three populations\") is quite correct. This study shows that there is a difference between the groups of samples tested with their HER2 test strip, with a one-way p-value of ~0.002. You claimed this was the implication of an unbalanced study. I’m sorry but that is a complete non sequitur. It is nonsense. Most of everything else I disagree with but isn’t nonsense. > I'm not sure what they are trying to claim in there paper... are they trying to say that they can diagnose breast cancer (which would requires many more biomarkers), or are they trying to say that they can differentiate between HER2+ and HER2- cancers (which would be more appropriate for a HER2 test). They are simply showing different distributions of tests for different populations and observing that, foremost, the invasive cancer ones have a significantly different distribution; and that it’s shows promise as a test for the future. reply e63f67dd-065b 14 hours agorootparentprevIt's been a while since I took statistics, but isn't the whole point oftesting that it needs to have a high Bayes factor, and to be confident that said Bayes factor is high? In this case, and my lack of understanding in both bio and stats is showing here, we're trying to develop a test for breast cancer. An ideal test will have high sensitivity and specificity, and a tight confidence interval for both of those numbers. This way we can be confident that a positive/negative test actually moves our priors meaningfully in either direction. I guess my question/comment is more on the fact that I don't see how any of the results shown actually translate, as the headline suggests, into a cancer test of high power. The priors for any kind of cancer is pretty low from what I can find, so we need high power tests in both the positive and negative direction to meaningfully effect health outcomes. I can't find any CI numbers in the paper, which may just be me not reading it closely enough, but it doesn't help my confidence. reply jncfhnb 13 hours agorootparentSuppose you had cancer and could roll a die, and if the die came up six, it would tell you with high certainty you had cancer, and if it came up as anything else, it would tell you nothing. If the test is very cheap, it’s probably a great test presuming you get to see the dice roll itself. The cost and invasiveness of the test is important. reply an_d_rew 12 hours agorootparentprevThe problem here is that the underlying and unknown correlations in the sample (aka people) is that they are NOT independent. The larger sample sizes are a ... sort-of \"proxy\" ... to overwhelm underlying latent correlations. The whole thing is actually a subtle sort-of generalization of the \"Prosecutor's Fallacy\". So skepticism with the small sample sizes is absolutely warranted, unless some strong evidence is shown indicating mechanism-based independence. reply jncfhnb 12 hours agorootparentI feel like you’re taking a very roundabout way to describe the concept of sample bias. So long as it’s a random sample, this is accounted for in the statistics. Yes, we should still get more data all the same. reply kenjackson 16 hours agorootparentprevAnd this result probably helps the funding to get more patients. I’ve never worked in clinical trials but it seems like it would be so tiresome. reply bookofjoe 15 hours agorootparentI ran many clinical trials during my career as an academic neurosurgical anesthesiologist. \"... it seems like it would be so tiresome\" is accurate; it's also exhausting in terms of the huge amount of time and effort required to get a study approved by the institutional review board; getting informed consent from prospective patients after an exhaustive explanation repeated over and over to each individual; actually doing the study; organizing the results; doing the statistical analysis required; writing the paper; waiting months to hear back from the journal's reviewers, often receiving a rejection letter; resubmitting the paper to another journal, sometimes several more; after having it accepted, revising the paper per the reviewers' comments before resubmission, sometimes going back and forth for months. You can find my published papers here: https://scholar.google.com/citations?user=5DdrMc8AAAAJ&hl=en reply wolverine876 15 hours agorootparentSo how do you feel about it? :) Seriously, what are you feelings about it after all those experiences? You forgot the end of the story: After all that, it's done and published, and then a random person with no expertise and who barely read the paper posts on HN: the sample size is too small - as if you were in your first week of statistics 101 - and therefore the whole thing must be invalid! :) reply bookofjoe 12 hours agorootparentHow do I feel about it? My feelings after all those experiences? I dunno... that was me then, I guess: hard core academic with a drive/compulsion to publish good work. I admit to being amused by comments here about statistics by people who wouldn't know a Bonferroni correction from a bonfire. reply neuronexmachina 16 hours agoparentprev> Table I shows the median and the range of digital readings by disease status and overall p-value using the Kruskal–Wallis test to examine if there exist statistically significant distinctions among two or more groups. The overall p-value is significant while the value for HER2 is 0.002, which show the probability of false-positive detection. This indicates that this sensor technology is an efficient way to detect HER2 biomarkers in saliva. > ... In Fig. 5, the test results for detecting CA15-3 of the human samples are displayed. The digital reading decreases from the healthy group to the invasive breast cancer group, indicating an increase in CA15-3 concentration. The median, the range by disease status, and overall p-values analyzed with the Kruskal–Wallis test for the CA15-3 test are listed in Table I. The overall p-value for CA15-3 is 0.005, indicating that this device provides an efficient way to detect the salivary biomarkers related to breast cancer. reply jacquesm 16 hours agorootparentYou'd need a much larger n to determine the false positive rate. Sensitivity by itself isn't very useful. reply fuhrtf 17 hours agoparentprevThat’s a bit harsh. It’s an exploratory study testing a new paradigm. reply oliwarner 17 hours agorootparentIt's only as harsh as the the headline is rose-tinted. \"Exploratory study shows promise\" is better. When n=21, flipping a coin as about as accurate. reply matthewmacleod 16 hours agorootparentThis statement belies a fundamental misunderstanding of statistics on your part. reply ethanbond 16 hours agorootparentIf you're really good at statistics, you can just intuit what seems like a sufficient N. This is actually a powerful statistical method because you can vary your intuition depending on whether you want to believe the study was well-powered or not, enabling you to easily discard Bad Evidence and include Good Evidence. Few understand this. reply jncfhnb 16 hours agorootparentNo, that’s utter nonsense. Statistical power is well defined and the required N to reliably detect an effect is a function of its effect size, which is the thing that wannabe statisticians don’t understand. reply lazyasciiart 15 hours agorootparent> you can vary your intuition depending on whether you want to believe the study I suspect it is deliberate nonsense. reply solardev 14 hours agorootparent> I suspect it is deliberate nonsense. But is that your intuition talking? reply Spivak 15 hours agorootparentprevYou don't intuit it, you calculate it. If you claim to be able to predict coin flips with a specific accuracy I can give you the exact number of trails N you need to be x% confident. If you claim 90% accuracy and I want to be 95% confident with the standard alpha of 0.05 you need 38 trials. reply causal 18 hours agoparentprevYah headline jumping the gun a bit, but hopefully this motivates funding to get a bigger study / refine the technique. reply pvaldes 17 hours agoparentprev> We tried it with 21 people The new Theranos is here again reply sonicanatidae 16 hours agorootparentNot quite. Theranos was stating that they were capable of doing what was impossible prior. The real issue with Theranos was physics. A single drop of blood is simply not a large enough sample for them to do all the testing they claimed. As in, impossible with today's tech and in the near future. These folks are just stating that, \"so far, this looks promising\". At least, thats my read. YMMV. reply chaxor 14 hours agorootparentIt is important to point out that what they claimed to do is not technically impossible actually. They just didn't do it, and lied about it. That was the real problem. It's very unfortunate, because it is technically possible, just very difficult to achieve, even in academia (so it won't be done first in industry). This was the absolute worst part of Theranos, is that they deter others from trying to make headway in the space at all. reply sonicanatidae 14 hours agorootparentI don't believe they could run a battery of 81+ tests on a drop of blood. There simply isn't enough there. Why do you think they take tubes of blood for testing, currently? The issue is concentrations and the presence of them in sufficient amounts to be detected reliably and consistently. edit: typo.. words are hard. reply chaxor 11 hours agorootparentIt's because that's what the equipment is designed to use, and yes it provides more reliable results with the current technology. Much of the equipment used in healthcare relies on very old technology. This certainly does not mean it cannot be improved to work with less amount of material. There are hand held sequencers for your phone now (minIon) that are very cheap and work. There are also large multimillion dollar bench top machines on the same technology from nanopore. Both are very new technology compared to what we used in early 2000s, and offer various different features and certainties from the analysis. However, it is foolish to say that the minIon is impossible. It works. It's just less resolute than other systems. This is true for many systems which people cry \"Theranos\" about. Their issue was lying. That was the problem. reply dheera 17 hours agorootparentprevNo, not at all. Polar opposites. Theranos was actual lies. TFA is just being honest about a low sample size. reply jacquesm 16 hours agorootparentWithout a larger n you can't really tell what the fp rate is and that means that the accuracy figure isn't very useful. If accuracy holds in light of a false positive rate that is much lower than other tests then this may well be very useful. But you can not conclude that at all based on the evidence presented. reply pvaldes 17 hours agorootparentprevIf somebody claims: our device \"accurately tests\", and we discover later that \"In fact the tests hadn't been done still\" (at a meaningful way), It should be taken as a red flag. Specially when it came in the same package with \"to make lots of tests would be really cheap\" (but for some reason they didn't tried it). Maybe be a great, honest work, brilliant step?. Yes. But please don't claim that your model is reproducible if you didn't ever tried seriously to reproduce it first. What if the results are just a random effect?. What if the test says just positive most of the time? The test negative response has been tested in a control group of 4 people? This is 20 bucks well spent. --At this moment-- isn't different than other thousand projects that seemed too good to be true, took the money and soon vanished. I hope to be wrong. reply dheera 14 hours agorootparent> didn't ever tried seriously to reproduce it first Maybe they found something worth testing and just need more funding to actually do the tests? I have no problem with early free speech about \"I found something interesting\" as long as you're honest about the sample size and don't misrepresent it. reply dheera 17 hours agoparentprevIt's still a nonzero sample size, and if anything, says two things: (a) We should do more tests to increase N (b) If a $5 device tests you positive, maybe you should go get checked out for $5000 or whatever the doctors charge you (because insurance often only pays AFTER \"shit happens\" and often does not pay to test \"whether shit might happen\"), that you wouldn't have thought of doing otherwise. reply lofatdairy 17 hours agoparentprevI appreciate the links but I think this actually misses the point. The novelty isn't in diagnosis of cancer but in sensitivity/cost-efficacy in detection of known biomarkers for breast cancer (and associated risks of recurrence, etc). I'm not familiar with how common ELISA-based HER2 testing takes place, but it seems like it has some impact on drug decisions[^1]. In terms of applicability, it depends on whether or not ELISA is in fact the current standard of care, but it could be useful in low-resource settings where you don't have lab personnel trained to carry out those assays, and drug choice is also restricted by limited availability. Additionally, there's a point-of-care argument as well. Since breast cancer does benefit from early detection, I can see a future in which biomarker testing is a more regular thing, and high saliva concentrations are flagged. At the very least as something worth bringing up at one's next appointment or wtv. [^1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7033231/ reply tetramer 16 hours agorootparentThe standard of care in the US is immunohistochemistry (IHC), with FISH testing in equivocal cases so not really ELISA based. HER2 testing is done on all breast cancers as it affects treatment choices though the majority of breast cancers are not HER2 positive. (HER2 is also expressed in some normal tissue (notably cardiac) and is also seen in other cancers as well). reply redder23 17 hours agoparentprevTotally agree! reply iandanforth 18 hours agoprevFigures 3 and 5 from the paper (https://pubs.aip.org/avs/jvb/article/42/2/023202/3262988/Hig...) have overlap between all the groups. The same measured output could have come from any of the non-cancer, or known-cancer, participants. While the means of these groups look nicely separable, that overlap means there will be significant false positives or false negatives. So I'd label the studyfinds headline of this being 'accurate' to be false. reply ImageXav 18 hours agoparentNot at all. The model is quite accurate. In fact, with the distribution of samples that they have a model that predicts all cases as having cancer would also be very accurate. It would get 17/21 predictions right. The model lacks precision. I suspect that even with a fairly high cut off point the model would still produce a bevvy of false positive predictions due to that. It might still be useful as a screening step if they can increase the sensitivity further, but you would still rely upon further tests to get a true diagnosis. reply tetramer 17 hours agorootparentIt also only looks at HER2 and CA15-3 (aka MUC1) expression - what about breast cancers that don't express either of these? I realize this is an early technology and I think it should continue to be explored, but I would anticipate that if compared head to head with screening mammograms, it would be inferior. For patients with relapsed disease, this kind of technology would be neat to non-invasively re-assess biomarker status but as a screening tool, I find it lacking (and certainly a positive screening will require dedicated imaging and biopsy anyway). reply jncfhnb 17 hours agorootparentprevBackwards. The distributions here imply it will be a high precision model with not great recall. reply dmoy 17 hours agorootparentprevSure we can be technically more surgical in our terminology, but GP is addressing the usage of 'accuracy' in the news headline. In that context, 'accuracy' is kinda a catchall term for both accuracy and precision. reply ImageXav 17 hours agorootparentIt's a bit difficult to say, isn't it? The headline is using the term accuracy correctly, the reader might be ascribing the meaning you are to it, especially if they are non technical. As was the parent comment. My goal in pointing out the difference was not to be snarky. It was to point out the very real statistical consequences. Any model can be accurate on a sufficiently biased dataset, but what matters once a screening test hits the real world are the precision (positive predictive value) and negative predictive value. These are the hurdles that the test will have to pass to see widespread adoption. reply jacquesm 16 hours agorootparentExactly. This is the real test and so far we simply do not know the answer. It's a nice first step but the headline is simply not justified. But whether solar power, wind power or cancer it's 99.99% of the time (possibly more nines) far less impressive by the time all of the data is in. And that's fine, but headline writers seem to be stuck in a hype cycle. reply parhamn 17 hours agorootparentprevTheyre not being surgical in the terminology. Accuracy and precision are the two things that matter for a diagnostic test. High accuracy, low precision = screener. reply adamredwoods 16 hours agoprev>> HER2 and/or CA15-3 in serum are essential biomarkers used in breast cancer diagnosis. This is WRONG, it is used as an indicator, NOT a diagnosis. In fact, I have personal experience that CA15-3 is not always in indicator of anything. You first measure a baseline, and use it for reference. Also there's no HER2 expression in TNBC. Can this be used for possible early detection? Maybe, but it will not be an exact science. reply masto 17 hours agoprevSeems like this was originally built for SARS-CoV-2. Here's the paper from 2021: https://pubs.aip.org/avs/jvb/article-abstract/39/3/033202/59... and then in 2022 for detecting oral cancer: https://pubs.aip.org/avs/jvb/article/41/1/013201/2866658/Hig... reply andyjohnson0 18 hours agoprevhttps://pubs.aip.org/avs/jvb/article/42/2/023202/3262988/Hig... Discusses sensitivity but not accuracy or rates of false positive/negative. reply zacharyvoase 18 hours agoprev> The study is published in the Journal of Vacuum Science & Technology B Dare I ask why? reply rainbowzootsuit 18 hours agoparentThin films grown in vacuums are how you get graphene layers and FETs that appear to be the basis of the technology here. Someone with a vacuum deposition system would be the prime candidate to develop a custom thin film to target adsorption of the molecules that they're trying to sense. reply neuronexmachina 16 hours agorootparentThis part of the study describes what they implemented: >Instead of using the transistors as the sensors, which need to be disposed of after each use, a system with a reusable printed circuit board (PCB) containing a MOSFET and disposable test strips were employed. In this approach, synchronized double-pulses were applied at the gate and drain terminals of the transistor to ensure that the channel charge does not accumulate, and there is no need to reset the drain and gate paths to mitigate the charge accumulation at the gate and drain of the sensing transistor for sequential testing. With the double-pulse approach, it only takes a few seconds to show the result of the test, due to the rapid response of the functionalized test strips and resulting electrical signal output. As an example, the LoD has been demonstrated to reach 10−15 g/ml and the sensitivity to 78/dec for COVID-19 detection. Similar approaches have been used to detect cerebrospinal fluid (CSF), cardiac troponin I, and Zika virus.27–30 > In this work, use of this double-pulse measurement approach to detect HER2 and CA15-3 in saliva samples collected from healthy volunteers and breast cancer patients was investigated. The voltage output responses of the transistor correlated to the HER2 and CA15-3 concentrations, detection limits, and sensing sensitivity were determined. reply moi2388 17 hours agoparentprevBecause much like vacuums, cancer sucks reply m-htt 12 hours agorootparentyou are doing the lords work i applaud you reply loeg 18 hours agoprevSomeone has to actually commercialize this and get it approved and it will cost far more than $5. As part of that they have to demonstrate that it is, ya know, useful (vs existing methods). reply DoreenMichele 16 hours agoprevThe biosensor, a collaborative development by the University of Florida and National Yang Ming Chiao Tung University in Taiwan, employs paper test strips coated with specific antibodies. These antibodies interact with cancer biomarkers targeted in the test. Upon placing a drop of saliva on the strip, electrical pulses are sent to contact points on the biosensor device. This process leads to the binding of biomarkers to antibodies, resulting in a measurable change in the output signal. This change is then converted into digital data, indicating the biomarker’s presence. It tests for biomarkers in the saliva. Possibly not outright crazy charlatan territory. Could certainly use a larger sample size though, especially given that one of its bragging points is \"fast and cheap!\" reply asdefghyk 16 hours agoprevThe PCB picture above the words \"...The printed circuit board used in the saliva-based biosensor,...\" makes me think the article is a scam. As a electronics engineer the word labels seem to be not especially relevant to the invention. Who makes boards with lots DIP chips? reply SV_BubbleTime 15 hours agoparentYea, I was really curious about those. I mean… even if we were talking some special hardware, that’s still PLC territory. My reading was this isn’t $5, but could be made to be $5. This is clearly a prototype. reply iwontberude 18 hours agoprevUnderlying research about salivary biomarker detection https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6566681/ reply susiecambria 9 hours agoprevI could not assess the validity of this paper if my life depended on it. But this is what I do know: Given that the University of Florida is a public institution and the governor of the state is anti-science and promotes anti-science policies and practices, my immediate reaction to this paper’s news is negative. Reasonable? Probably not. I can’t be the only one who would react in this way. This matters for two reasons. First, each of us has limited time and capacity to care about things, to investigate them more. Second, if we don’t recognize our biases, we may ignore important and useful information. reply deweller 18 hours agoprevWould a US company have enough financial incentive to front the cost of FDA approval in the US? I'm guessing it is expensive to jump through all of the hoops. Without a patent, why would a company pay for this? reply zdragnar 18 hours agoparentAs a non-invasive diagnostic equipment, let us say it is akin to a pregnancy test, or a creatine in urine test. That puts it in Class 1, and may be exempt from a lot of the hoops a medicine would go through. There are a little over 6,100 hospitals in the US at any given time. That means roughly $30,000 in cost to produce to provide every hospital with one, using the cheapest of cheapest parts. Presumably, you'd want higher end components and things like a shell casing to protect from spills, static, etc. maybe the cost is $120k. You could sell these for $100 each, and maybe make $5 million. That's a decent amount of money for a tiny business, but a pittance for even a small business that needs to pay salaries, lawyers, etc. In reality, they'd probably go for much more- let's say $10k each, plus $100 for each test strip. Still easily in range for the budget of most US hospitals. The real question is, if breast cancer is suspected, is this test any better than imaging using equipment the hospital already has? Can it detect all types, or the degree that it is progressing? I suspect the utility in a clinical setting is not high enough to really change clinical practices any. reply aj7 17 hours agorootparentHuh? The electronics is cheap and trivial. All the science is in those strips. reply adam_arthur 18 hours agoparentprevUsually with medical innovations, the idea is patented and the product is sold well above cost of production. e.g. this $5 device could be sold for $100 each... large profit margin. If there is no patent, and no in-place infrastructure already producing the device, then yes, it's unlikely to see rapid scale up by manufacturers. Though if it's really that cheap to produce, I'm sure it will come onto market in some form (whether through charitable foundations or otherwise). All assuming that the device is actually as efficacious as the study implies (with a small sample size). reply Aachen 18 hours agoprev> employs paper test strips coated with specific antibodies. These antibodies interact with cancer biomarkers [from] a drop of saliva > “[...] cost-effective, with the test strip costing just a few cents and the reusable circuit board priced at $5,” Wan says. That is cool: the 5$ isn't even the cost of the test, it's the one-time cost of your lab equipment. Of course, per sibling comments, the efficacy has yet to be seen, but even a few percent more early detections due to frequent testing would be a win reply iamthepieman 18 hours agoparent\"but even a few percent more early detections due to frequent testing would be a win\" This is not the current medical thought on early screenings for various cancers. It used to be and I was confused about it until very recently. Indeed the medical community is still wrestling with the issue of screening harms. The consensus is shifting that screening should only be done if there is an existing condition or symptom or family history. https://www.cancer.gov/news-events/cancer-currents-blog/2022... reply bluGill 17 hours agorootparentThat depends on how harmful the screening is, how harmful treatment is, how harmful the disease is, how well the test works, and how common the disease is. (probably more that I'm not aware of) Most cancer treatments are really nasty. Thus false positives are really bad: you destroy someone's quality of life. The earlier cancer is discovered the better chance that we can use a less harmful treatment (if only because of smaller dose of the harmful drugs) The current breast cancer screening is an xray - which itself causes cancer (about 1 in 3000 cases of breast cancer discovered by xray wouldn't have got breast cancer in the first place without the screening - the screening is still wroth doing if you are at risk, but don't do it if you are not at risk). Breast cancer can be deadly, but if caught early it is easy to treat (normally). The medical concern generally isn't should we test all women for breast cancer, but when do we start testing and how often should we test. If this test is safer than an xray and sensitive enough it can be useful. Avoiding current breast cancer tests is good. reply tetramer 16 hours agorootparentYou're right that benefits of screening vary widely depending on type of cancer, type of test, and even a patients own co-morbidities. However, there are a lot of inaccuracies in this comment. False positives on a screening test are bad because you follow a screening test up with a confirmatory test (a biopsy for cancer) - sometimes the procedure for the biopsy results in additional complications and even death in rare cases (and if it's a false positive, a patient goes through all of that for a benign finding). I want to be very clear that oncologists are not going to start cancer treatment on the results of a screening test, you need confirmation. reply Aachen 7 hours agorootparentprev> Most cancer treatments are really nasty. Thus false positives are really bad I was imagining this cheap self test like covid self tests: you don't start taking chemo, but it's an indication to see a doctor tomorrow if not today and get it checked out properly But you do raise a good point about overtreatment which I had forgotten while writing my previous comment. Whether that applies to diseases that are reliable to confirm manually and will simply kill you if you're late to the party is another matter, but I had forgotten to take it into account at all reply mh- 17 hours agorootparentprev>about 1 in 3000 cases of breast cancer discovered by xray wouldn't have got breast cancer in the first place without the screening I had no idea the risks associated with xrays for breast cancer screenings were that high. Do you have a source (for that 1:3000 assertion) I can read? reply bluGill 15 hours agorootparentCommunication with someone who claims to be in the know. It seems reasonable, but I don't have a source and I welcome someone who cares more to go more in depth. reply biomcgary 16 hours agoprevUsing the performance in the paper, the false positive rate means the current test in clinically useless (due to harm from screening and follow-up). Refining the test requires improving the selection of biomarkers and antibodies, not the hardware (which looks great). reply SV_BubbleTime 15 hours agoparentNo offense, but as someone that makes hardware devices, that hardware does not look great. It looks the very opposite of great. Through hole DIP array? This is a prototype and it makes me skeptical of the whole thing. There is nothing a single $5 FPGA couldn’t do. So why not start there? I suspect because the people that made this doesn’t know electronics or programming well - but then also didn’t find someone that did. This was put together the way long way, and it’s strange to me. reply andrew_eu 18 hours agoprevThe publication is linked in the article [0]. Even if it's only for HER2 patients, even if it's only useful as a first-pass test, this is still great news. The experimental design seems very small scale though. 17 cancer positive samples (of which only 1 was HER2 positive), 4 control. Since the strips are focused on HER2 detection I read this as \"in 1 out of 1 samples, our test detected HER2 overexpression\" but maybe I misread it. [0] https://pubs.aip.org/avs/jvb/article/42/2/023202/3262988/Hig... reply amelius 18 hours agoprevFalse positive / false negative rates? reply soco 18 hours agoparentNo larger scale tests yet, they only announced what is basically their current direction or research. reply yakito 18 hours agoprevAs a reference. Currently an MRI scan can go from $400-$2000 (mostly covered by insurance) https://scan.com/body-parts/breast An MRI machine cost arounds $350.000 on average. https://www.blockimaging.com reply ImageXav 18 hours agoparentA device such as this would never replace an MRI scan. The information provided is for screening purposes, at best. Also, diagnosis would typically be done using a mammography. The cost of such a scan is lower - around $100[0]. [0]https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4142190/ reply bluGill 17 hours agorootparentIt doesn't have to replace it in all cases. If it can give a good indication that we don't have to do a MRI that is already a good thing. reply ImageXav 17 hours agorootparentThat's right, that's typically what is meant by screening purposes [0], apologies if it wasn't clear. [0] https://www.nhs.uk/conditions/nhs-screening/ reply Dylan16807 16 hours agorootparentThe unclear part was more the phrase \"would never replace an MRI scan\". But it's clear now. reply sparklingmango 16 hours agorootparentprevDiagnosis is done via biopsy. reply ImageXav 15 hours agorootparentThat is correct, my bad. The next screen after a test like this would likely be a mammography, and only after that would a biopsy be done if anything suspicious was seen. reply lofatdairy 16 hours agoparentprevBetter reference would be an ELISA test (which is actually brought up as the reference in the paper)[^1]. That seems to also run about $5 per kit per antigen[^2]. However this device seems to only require the test strip to be replaced, whereas you can only run ELISA once per strip/reagents. Also note that ELISA is harder to run so you have personnel costs and also this device claims higher sensitivity. [^1]: https://pubs.aip.org/avs/jvb/article/42/2/023202/3262988/Hig... [^2]: https://www.thermofisher.com/elisa/product/ErbB2-HER2-Human-..., note that I didn't shop around for necessarily the best prices. reply aeyes 14 hours agoparentprevMRI is very rarely used for breast cancer screening. Mammography is much cheaper. reply alhirzel 17 hours agoprevThe \"press release\" looks more like a final report for a microcontroller applications class than an actual press release. reply SV_BubbleTime 15 hours agoparentDiagram aside. The thirty through hole parts made me the think the same thing. reply moi2388 17 hours agoprevIf I look at figure 3 and 5 I immediately see quite a bit of overlap between the readings of the different groups.. reply F51user 14 hours agoprevLargely BS. 1) These are not clinical biomarkers used for detection of cancer now, and in fact, they are known NOT to be clinical biomarkers useful for detecting cancer. 2) This is a publication focusing on the device/method, not the clinical application. It is published in a journal of vacuum science, not a cancer biology or medical journal. 3) There are several inaccurate things in the paper, one being that they state that current technology requires 1-2 weeks to measure either biomarker. Wrong, clinical tests exist today that can perform those immunoassays in minutes on large automated analyzers. 4) This isn't fraud, it's just a really typically overhyped report of a novel device/meausrement strategy (and it's not that novel) that targets a biomarker that has a role in cancer, and then some mass media picks it up and says that they have \"the test for cancer\". This happens all the time. 5) This should maybe be considered a proof of concept about the electronics of their detection strategy, since the immunoassay component is known (immunoassays to both biomarkers are not only published, but commercialized) and the clinical use of the biomarkers is not at all diagnostic for cancer or useful for screening. reply amelius 16 hours agoprevGiven that only one such device is needed per (say) 100-500 people, I think the device can probably cost a lot more and still be as affordable and effective. reply redder23 14 hours agoprev$0 device tests for breast cancer in under 5 seconds: its called a hand. reply m3kw9 18 hours agoprevIf it’s any good it will filter thru to a doctor near you otherwise, it all sounds nice reply shermantanktop 18 hours agoparentReally? Will the $5 Arduino people woo the doctor near you with golf trips? Will they strong arm HMOs to cover the cost? Will they carpetbomb the airwaves telling consumers to “ask their doctor?” Will they defeat the army of marketers and salespeople with entrenched competing tech? This device may be total crap, I don’t know, but “trust the system” isn’t a great way to navigate the American medical system. Other countries have it better, or at least different. reply s1artibartfast 16 hours agorootparentDoctors do gravitate towards effective tests and medicines, and insurance plans are interest in cheaper alternatives. There is a lot that could be improved about the US medical system, but Nobody has to bribe doctors and insurance to sell tongue depressors. reply shermantanktop 13 hours agorootparentI would bet someone out there has attempted to come up with a high-margin tongue depressor with fancy built-in features. But the plain wooden stick is already entrenched and obviously effective (for depressing tongues). This is the opposite. If the state of the art was a fancy tongue manipulation machine that cost $30k, used licensed consumables billed at $100/patient, and did a bunch of non-essential things that doctors found convenient once in a while, would someone selling a box of sticks get anywhere? reply vpribish 14 hours agoprevclickbait misleading headline from a garbage-tier SEO-mill website. flag this and ban the website reply shelkie 18 hours agoprevCool how they're using BNC connectors and ICs from the 1980's /s reply rainbowzootsuit 17 hours agoparentThese are vacuum nerds more than electronics nerds. Most homebrew electronics designed by physicists for basic research are going to have these aesthetic qualities. reply Ginger-Pickles 18 hours agoparentprevFirst standout thing I noticed is the “patern generator” array of DIPs - admittedly without having dived into the paper, any answers from the hive mind what they’re doing? reply Ginger-Pickles 18 hours agorootparentFrom the paper: > Synchronous voltage pulses are sent to both the electrode of the strip connecting to the gate and drain electrodes of the MOSFET. The drain pulse is applied for around 1.1 ms at a constant voltage. The gate pulse starts at 40 μs after the drain pulse and ends at 40 μs before the end of the drain pulse. > the antigen-antibody complexes undergo stretching and contracting, akin to double springs, in response to a pulsed gate electric field. This motion across the antibody-antigen structure, corresponding to the pulse voltage applied on the test strip, induces an alteration in the protein's conformation, resulting in a time-dependent electric field applied to the MOSFET gate. Consequently, a springlike pattern emerges in the drain voltage waveform due to the external connection between the sensor strip and the MOSFET's gate electrode. So they shake ‘em just so, and listen to the response… ICs are perhaps variable timing & pulse-shaping logic? reply SV_BubbleTime 15 hours agorootparentThere are a million people that could do this with a single FPGA in an afternoon. Why were none of them approached? reply joezydeco 18 hours agorootparentprevI was focusing more on the \"current to frequency\" stage, which looks like an empty IC socket. As is the \"pulse width counting\" stage just a bunch of header pins? I mean yeah, I get it, it's a prototype and a finished product will be on a $2 ASIC to drive the correct signals and etc. But I'm not up to speed on affinity sensors vs. traditional ELISA tech so . reply lambdaone 17 hours agorootparentNothing jumps out at me as being fishy here. There's what appears to be a small device mounted in the middle of that empty socket. It's also possible some of the rest of the pins on the socket are being used as test points. A circuit diagram would have be good to have here. reply dtgriscom 8 hours agorootparentprevYeah; it's a bit strange to label a single MOSFET... reply shelkie 18 hours agorootparentprevBreast cancer is a terrible disease, and I don't mean to be a downer but my BS detector is screaming on this one. I'd give the device image a pass if were just a journalist grabbing a stock PCB image, but that doesn't seem to be the case here. Anyone with even a trivial knowledge of electronics would be amused by the callouts. And all for just $5? After Theranos I guess I'm a bit sceptical of claims such as this. reply shermantanktop 18 hours agorootparentYeah, this looks like an Apple IIe logic board to me. reply Ginger-Pickles 18 hours agorootparentCommodity logic ICs ubiquitous and totally capable if needs are well-matched; it’s perhaps a deliberate prototype engineering/development choice. reply SV_BubbleTime 15 hours agorootparentAny FPGA could do the work smaller, faster, cheaper. So, I’m still skeptical. reply glitchc 18 hours agoprev [3 more] [flagged] m12k 18 hours agoparent [–] > Scientists say a new handheld device is capable of testing for breast cancer in less than five seconds using just a small sample of saliva Second sentence in the the linked post reply glitchc 18 hours agorootparent [–] So, reading an actual publication [1], all this test does is identify if the patient has the BRCA biomarker for breast cancer. It doesn't actually detect if the patient has breast cancer. The title is misleading, as is your post. [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9249990/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers from the University of Florida and National Yang Ming Chiao Tung University in Taiwan have developed a $5 handheld device to detect breast cancer in less than five seconds with a small saliva sample.",
      "The device utilizes basic components like glucose testing strips and the Arduino platform, ensuring portability, cost-effectiveness, and accuracy with minimal biomarkers.",
      "This breakthrough technology intends to transform breast cancer screening globally, providing a rapid, non-invasive, and economical option compared to conventional approaches."
    ],
    "commentSummary": [
      "Studies and prototypes are being developed to test breast cancer using saliva samples, sparking debates on sample size, statistical methods, and clinical relevance.",
      "Critics express concerns about accuracy, validation, false positives, false negatives, and the risks of early cancer screening with these devices.",
      "Emphasis is put on the importance of thorough testing, validation, and adherence to scientific standards when introducing new medical technologies due to the challenges in adoption."
    ],
    "points": 224,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1708095849
  },
  {
    "id": 39398481,
    "title": "Feeling Left Behind: Old Tech Enthusiast Reflects on AI",
    "originLink": "https://medium.com/@alex.suzuki/im-an-old-fart-and-ai-makes-me-sad-06003bfb6750",
    "originBody": "I’m an Old Fart and AI Makes Me Sad Alex Suzuki · Follow 4 min read · 18 hours ago -- Previous major advancements in Computer Science and its applications always felt empowering to me, AI feels different. Dall-E 3, “A drawing of a software engineer overwhelmed by recent advances in Artificial Intelligence” To find out why I feel this way, I had to look into the past couple of decades and compare the major technological advances to today’s movements. Personal Computers [80s] PCs were my gateway drug into technology — they got me into gaming and eventually programming. I have fond memories using tools like MEMMAKER to squeeze more memory from my machine just so I could run a pirated copy of some new game on my DOS box. PCs came in different shapes, sizes and architectures. They had many names and were sold by different vendors. You were mostly free to do what you wanted with these machines. You could swap out parts. You could run games, you could write software, or do something totally different with them. From an engineer’s viewpoint, the inner workings of these machines were comprehensible. With some skill and dedication, you could even build your own. World Wide Web (a.k.a. The Internet) [90s] Suddenly the bandwidth of your internet connection was more interesting than the clock rate of your computer. I spent hours in Internet Cafés chatting with strangers, who like me, were exploring this new and strange medium. The early Internet felt accessible to me. It was built on open, interoperable protocols like TCP/IP, HTTP, SMTP etc. You could buy a book or visit SELFHTML, open an account at Geocities and create a website. If Geocities went down, you could move your website somewhere else, or even self-host. No single company or entity owned a significant chunk of the Internet. Smartphones [mid-2000s] A new form factor and interaction paradigm is introduced. Even though we saw similarly sized devices and touch screens before, the iPhone was the first device that tied it all together, helped by ubiquitous mobile internet access. Smartphones are, for the most part, accessible in the same way as other computers are. They are small, connected, programmable computers with a wide range of input sensors. I understand how smartphones work, I can program them, I know their limitations. How AI is Different Don’t get me wrong, I’m excited about AI and blown away by stuff like the just announced Sora text-to-video model. But at the same time, I feel left out. AI is Opaque I want to understand how things work. AI feels like a black box to me. The amount of papers I’d have to read and mathematics that I’d have to ingest to really understand why a certain prompt X results in a certain output Y feels overwhelming. Even some top scientists in the field admit that we don’t really understand how AI works. “If we open up ChatGPT or a system like it and look inside, you just see millions of numbers flipping around a few hundred times a second,” says AI scientist Sam Bowman. “And we just have no idea what any of it means.” To me as an engineer, that is just incredibly unsatisfying. Without understanding how something works, we are doomed to be just users. AI is Not Approachable Sure, on the outside it is — anyone and their dog can open a ChatGPT session or throw some JSON at OpenAI’s APIs. What I’m talking about is gaining access to the core technological foundations that make AI possible. Flipping all those numbers to get the result (inference), and especially determining those numbers in the first place (training), requires a vast amounts of resources, data and skill. AI is not a shovel for little people. AI is Not Open If you’re like me, you might have some ideas for cool stuff to build with AI. But in doing so, chances are you’ll end up building a GPT Wrapper. What is a GPT Wrapper, you might ask? It’s any bit of software, SaaS, whatever you want to call it, that relies on someone else’s AI product that you can’t easily replicate or exchange (ChatGPT, for instance). If I build an app that needs persistence, I might use Postgres and S3 for storing data. If those are no longer available, I’ll use another relational database, key-value store, distributed filesystem, whatever. But what if OpenAI decides to revoke access to that API feature I’m using? What if they change pricing and make it uneconomical to run? What if OpenAI extends their offering and makes my product redundant? Old man yells at cloud (credits: https://knowyourmeme.com/memes/old-man-yells-at-cloud) So am I just like that angry old man shaking his fist at the Cloud in anger? I hope not.",
    "commentLink": "https://news.ycombinator.com/item?id=39398481",
    "commentBody": "I'm an Old Fart and AI Makes Me Sad (medium.com/alex.suzuki)194 points by alex_suzuki 18 hours agohidepastfavorite283 comments dleslie 18 hours agoAI not being open is my own personal source of sadness. The mathematics involved aren't out of reach, but the computing power and _personal time_ necessary to build a dataset and train a model are absolutely beyond my reach. It's unnerving that such a revolutionary new technology in computing is fundamentally tethered to large binary blobs, usually proprietary or of uncertain provenance. Which is not like computing advances that we have hitherto enjoyed; even technologies for the operations of large data centres and distributed systems could at least be deployed and toyed with on virtually all common consumer hardware. Perhaps it's because thus far the realm of Big Data wasn't alluring enough to draw in myself and the similarly minded. It's only now that a certain form of large data set has become tied to an interesting application that it's drawn my attention. Not to say that Big Data and AI are one and the same; only that they both deal with large-in-context data sets that are difficult to construct, acquire, and manipulate. reply geor9e 17 hours agoparentYou're describing A.I. as a black box of proprietary workings, far beyond any mortal's personal budget to create, and too complex anyway, which internally runs in a language unintelligible to humans, and through layers and layers of abstractions, has a human-readable interface. To me, that exactly describes an x64 or ARM CPU. But since we are not children freshly learning about reality, we don't see these two black boxes in front of us in the same light. One is something we just got used to kinda-understanding-but-not-really decade after decade. One is something brand new to us. reply dleslie 16 hours agorootparentx64 and ARM emulators exist, the code for some are quite legible. There's a whole community of folks who build RiscV implementations from verilog or vhdl. In my own experience at university, decades ago, we built functioning HC11 CPUs out of logic gates and wrote assembler for them. I'll admit that there's a definite and clear accessibility curve to building a CPU, but it's nowhere near as opaque as the binary blobs one can access for AI tooling. reply entropicdrifter 15 hours agorootparentNow I want to go and play NandGame again... https://nandgame.com/ reply paulmd 12 hours agorootparentI hear really good things about turing complete https://store.steampowered.com/app/1444480/Turing_Complete/ reply asadotzler 8 hours agorootparentprevX86 and ARM are open in many many ways that for example ChatGPT isn't. reply seydor 17 hours agoparentprevIn economic terms, it feels like we the people won't have much of a chance at AI wealth. Software was infinitely scalable, easy to make and powerful, but AI is data with tiny amounts of code, and machine learning engineers are going to be a dime a dozen soon. So hardware companies will reap most of the benefits, and making it with a a garage startup is hard reply saltcured 14 hours agoparentprevThis is much how numerical computing has always been. We could individually approach the math and algorithms, but need major resources and input data to apply it to real-world problem like weather forecasts, geophysical simulations, digital wind tunnels, etc. I think what's different right now is the sudden hyper focus. It's a chaotic gold rush driven by speculators and prospectors as much as by technologists or scientists. I wonder what we'll see on the other side when the fog starts to clear. I expect lots of \"losers\" i.e. frustrated prospectors who never found their gold mine. And probably some proprietary wins locked up by big investors. But, maybe we'll at least benefit from some new commodity products reaching out to a broader market after they initially targeted the rush itself... reply johnsutor 17 hours agoparentprevI'm hopeful that this will get better as time goes on with advances in unsupervised and multimodal learning, that will enable us mere pions to achieve our niche use cases by fine-tuning a larger model with minimal compute. reply fritzo 17 hours agoparentprevThis attitude is like the rugged forest hermit's aversion to computers because silicon wafer production is out of reach for individuals. reply perryizgr8 17 hours agoparentprev> fundamentally tethered to large binary blobs In many instances you don't even have the blob, you can just call an API that invokes it and runs the output thru multiple other hidden algorithms before sending it to you. reply masswerk 17 hours agoprevAnother reason why AI makes me sad: take for example images. There is really no reason anymore to look at any posted art, image or article illustration, since, while it may look like an image, it may not be an image in the sense of an opus operatum. There is no sense in investigating and uncovering the decisions and choices that made the image, its composition, style, etc., thus opening the image to an experience, as a projection of human experience and thought, since there were no such choices made. If there are no (strong) con-textual hints at this being an actual image, there is really no sense in looking at it, anymore. And now much the same for video. – For an ardent fan of visual media, it's just depressing. reply vidarh 17 hours agoparentNone of what you describe has ever been my reasons for looking at an image, and yet I immensely enjoy looking at images. reply TeMPOraL 17 hours agorootparentDo you like procedurally generated content in games? Or do you quickly find it boring and empty, devoid of soul? I thought about it, and while part of my overall dislike of procgen in games comes from it being lazy and repetitive (too easy to spot the patterns), another part comes just from realizing there's nothing behind it. No mind, no plan, just RNG that I can keep rolling to get variations of the same theme. (There's also a sense of loneliness - when everyone is consuming unique content, it stops being a topic of conversation, because there's no shared experience anymore.) reply falcor84 16 hours agorootparentWhat about taking a solitary walk in the forest. Assuming no Intelligent Design, the trees and everything else there was created without any guiding plan. And if it's a large forest, it's reasonably likely that by the time the next person goes there, even if they end up following the same path, the vegetation will look entirely different. Does that uniqueness of \"content\" make the forest devoid of soul? And if you do accept Intelligent Design, then arguably the RNG of AI images is also guided by it to the same extent. reply I_Am_Nous 14 hours agorootparentWith nature, I believe it's interesting because unlike something like Minecraft, where a single chunk is entirely dictated by math, everything in nature is dictated by life. A fully grown tree exists because a previous animal didn't eat it as a sapling. A hole which was dug by rabbits now houses snakes. A path has been created by various animals moving between the trees, so there's less vegetation there, and it allows for an easier walk through the forest...but that path wasn't there originally and animals had to choose to go that way and make the path. Whether there is intelligent design or not, you can see the effects of life existing and changing in a forest. You can detect choices, individual and collective. reply zeroonetwothree 13 hours agorootparentWe could argue the universe follows mathematical laws in the same way. It's more a matter of how complex those laws are. With gen AI we finally see human-created mathematical rules that approach the scale of natural ones. reply vidarh 3 hours agorootparentprevI can say with some surety that I find things that on the whole has not been visible affected by life as beautiful as those that have. And I've found many Minecraft landscapes beautiful too. reply D13Fd 9 hours agorootparentprevI strongly disagree with this comparison but I’m having a hard time putting my finger on why. Part of it is that all current AI generated imagery is an imitation of something, but an imitation that doesn’t and can’t follow the rules of the original or understand the meaning of the original. It’s fake. Nature feels like the opposite. It’s real and brutal and, for living things, it is driven by millions of years of evolution. It’s entirely grounded in the physical laws of our universe. reply TeMPOraL 16 hours agorootparentprevI can't answer that - I'm bored out of my skull by nature. Always have been. Solitary walks in forests or parks, I find them great for clearing out my thoughts - I'll be thinking about lots of things, but at no point I'll be thinking about nature around me. Which is perhaps preferable to taking a walk in an urban neighbourhood, because I'll always find something interesting there that will let me avoid uncomfortable thoughts. (Except in context of biotech or dynamic systems; those are lenses that make me appreciate nature, but I realize this is an engineer's point of view - I'm excited about possibility of applying what we're learning, and/or repurposing what's already there.) reply vidarh 16 hours agorootparentprevMy issue with procedurally generated content is that it is often a replacement for effort, and so the problem is not lack of a mind behind it, but as you note that it is often lazy and repetitive, and so simply not good. Humans can be lazy and make repetitive crap too, and when they do I'll consider it just as crap whether or not their underlying ideas were amazing. reply asadotzler 8 hours agorootparentHumans may be lazy and produce crap, but AI is lazy and produces crap. See the difference? reply vidarh 3 hours agorootparentNo, I don't agree that's a meaningful difference. reply Towaway69 1 hour agorootparentprevAnd who described, designed and built AI algorithms? See the connection? /s reply masswerk 17 hours agorootparentprevSo what is it that you enjoy, when looking at a certain image (especially, when it comes to this particular image, as compared to any other representation of the same subject matter), if not the artistic choices? A weighted average? I'm skeptical… reply BugsJustFindMe 17 hours agorootparent> So what is it that you enjoy, when looking at a certain image (especially, when it comes to this particular image, as compared to any other representation of the same subject matter), if not the artistic choices? Things can be visually interesting without intent or artistic meaning. Even if you've never stared into a fire, surely would agree that many people do it? Or the ocean? Or hills covered in snow? Or trees? Or other people having lunch? Or a pretty face? People have gazed contentedly at things that are just pretty without any worry about \"artistic choices\" since people have existed. reply masswerk 17 hours agorootparentI'd argue, there's a difference between media and the world as-is. Media are crucially a projection, transposing an impression as an expression into another space, implying a difference in these two spaces and their respective dimensionality. This is, what makes media interesting to us. All what is implied in this: human experience, narrative strategies, focus, depth, texture, skill, choices, etc. Otherwise, we're just immersing in the world as-is, enjoying ourselves and our own choices. But, for art as an act of communication, this doesn't work. It even doesn't work for media, which is why immersive media, like 3D TV, regularly fail: it simply misses the media part. reply vidarh 17 hours agorootparentYou are presuming we all care about art as an act of communication all the time, or even at all. A lot of the time what the artist is trying to communicate is not all that interesting to me. I can enjoy just looking at something, whether it's a painting or a generated image, or just the sky. It doesn't take away anything from the experience to me if I know there is no meaning or intent behind it. reply masswerk 17 hours agorootparentI can just try to relate what kind of impact this has on me. As mentioned above, visual media are important to me. I graduated in the theoretical side of this, part of my work is visual design. First, I observed that these images made me depressed, whenever I encountered them, then, that I now just ignore any images by default. It hurts. I miss what had been a constituent part of me. reply vidarh 16 hours agorootparentI struggle to even imagine feeling that way about it. I don't know what to suggest, other than perhaps try experimenting with meditation as a way to see if experiencing detachment of your feelings about the process vs. the outcome and images might make a difference. To me the two are entirely separate, and e.g. when I make things myself, whether I draw, or play the piano, or write code, the outcome is often secondary (I'm shit on the piano, and mediocre at drawing, but it doesn't matter). When I learn about an artist, the person and ideas might be interesting even if I have no interest in their art (I pointed out my favourite part of the Matisse museum in Nice is not his art, but the olive garden outside it, elsewhere, but Matisse is fascinating even if I don't care about his art at all). But when I watch their art, it's purely about what I see then and there. It's not that knowing history behind it never affects me, or interests me, but that I don't need it to enjoy the work, nor will I always - or even most of the time - feel the slightest urge to learn about the work or the artist. reply masswerk 16 hours agorootparentI think, it's a matter of the parable of the puppet theatre. According to this, we may either \"naively\" enjoy the play, immersing ourselves in the story presented, sustaining our disbelief regarding the puppets, taking the scene \"for real\", or we may engage in a dialog with this, by applying our own perspective to what is already a perspective (or, in my words, a projection). From this arose Kant's suggested \"disinterested pleasure\", a state of alert suspense and readiness, which has become the blueprint of bourgeois art consumption ever since the enlightenment. It's really this framework of looking at, engaging with and producing art and visual media, I'm referring to. On a less theory-heavy notion, the mere fact that a certain image or certain parts of a given image enjoyed that much of an investment to look like it does, somewhat guarantees that it had been worth the effort to someone, that it was meant to convey something. A quality, which is now gone for ever. Meaning, even the barest spark of expression, is now just at random. Moreover, by the very definition, art will now be dogmatic, even where it's asked for aberrations and exceptions, and redundant, since it's just a product of weighted averages based on an existing library of expressions. At least applied art is pretty much over, as is visual media. (Just look at what happened to cinema, when the regulating factor of film stock and related production costs fell away.) reply vidarh 3 hours agorootparentI fundamentally reject your notion that this changes anything other than your choice to dismiss what you see because you can no longer feel sure of your understanding of things you couldn't be sure of before either. But then from early on I found a lot of attempted analysis of art shallow and often outright insulting in it's insistence of knowing intent that was often not there. E.g. I recall an interview with a Norwegian author where the interviewer was terribly invested in the symbolism of a scene, and the author though for a moment and answered that he just thought it sounded good, and wished he'd thought of that. In other words, while there certainly is intent behind a lot of art, your interpretation is yours. It may or may not even intersect with any authorial intent. So why does it matter? I've written two novels. I don't give the slightest shit if people interpret things in them how I intended. For the most part I just wanted to evoke certain feelings. There's no intentional symbolism there. Many things in the setting that I know people will interpret as a positive outlook I consider depressing - from my perspective its a dystopia, but I don't want to make it feel like that. But how people take it is entirely up to them. The artists investment has no relevance to or bearing on my enjoyment of a work. Nor would I expect or care if that is the case when people engage with my own. (I get that people who make a living of human art are worried, and that is valid) I for one look forward to consuming AI art when it is pleasing. I also still look forward to consuming human art when it is pleasing. And hybrids. I really don't care which is which if it looks good to me, sounds good to me, reads well to me, makes me think, makes me feel. reply csdvrx 13 hours agorootparentprevTIL. Thanks a lot for your comment. This meta analysis just opened another dimension for engaging with art. > the mere fact that a certain image or certain parts of a given image enjoyed that much of an investment to look like it does, somewhat guarantees that it had been worth the effort to someone, that it was meant to convey something For reasons very similar to yours, I enjoy going to 2nd hand record stores: if someone has bought the record a 1st time, then someone else though it was worth buying and putting it on a shelf, it's more likely to bring enjoyment than a random new purchase > Moreover, by the very definition, art will now be dogmatic, even where it's asked for aberrations and exceptions, and redundant, since it's just a product of weighted averages based on an existing library of expressions. I don't know much about visual media except music videos, but if you want to discover non dogmatic art, I'd recommend you try out what's popular in any random country. I love music and I've found russian rap and french pop to be extraordinary, maybe because they follow their own dogma, a dogma that feels very foreign to someone more used to north american music: whatever the russian (or french) weighted average may be, it's very far from the norm of what I'm used to, so it stands out. reply tivert 16 hours agorootparentprev> I can enjoy just looking at something, whether it's a painting or a generated image, or just the sky. It doesn't take away anything from the experience to me if I know there is no meaning or intent behind it. I feel like that's an attitude that's particularly common among software engineers: see an artifact as its surface presentation and nothing more. Maybe it's a result of thinking about abstractions like APIs too much. When I appreciate an image, it typically has to be either a reflection of reality (this thing I'm seeing is a real thing that I now know about) or an actual person's expression (an act of communication) for me not to feel cheated. It doesn't help that one of the biggest use-cases for \"AI\" image generation is the creation of clickbait bullshit masquerading as a reflection of reality. Basically: the context is as important as the raw image itself. reply vidarh 3 hours agorootparentI think this gets it entirely upside down: There is meaning and symbolism and patterns everywhere, but we can never be sure our interpretation matches sone intent, and I see no compelling reason to see that 'intent' as more than complex computation anywhere, and pretty much everything, everywhere is computation. There is context to a storm, or a tree too. Many things have more, and more complex, context than human intent. Intent is just one categorisation of data, and it can be interesting, but so are many other categorizations of data. And we also often get authorial intent wrong, often embarrassingly so. It also compels me to see the dismissive attitude to AI art as fundamentally flawed, in that while we're clearly not \"there\" yet, I see no fundamental conceptual difference between different forms of computation - including the human mind - so any dismissal of the \"just statistics\" kind to me is an attempt to imbue the human mind with religious characteristics I fundamentally reject. At the same time, to me, that attempt denigrated human art, which to me is equally just a result of computation. If you can't enjoy art unless you think there's some spark of something more behind it, then to me the only reason you fail to reject human art too is faith in something there's no reason to think I'd there. Nothing we know suggests we are - or can be - anything more than automatons resulting from computation any more than the trees in a forest or waves on a beach. Yet we still have intent, even if it is just a product of computation. And we still produce beautiful patterns that I enjoy whether or not I recognize your intent, and whether or not there was any intent behind any given aspect I enjoy. reply BugsJustFindMe 10 hours agorootparentprev> I feel like that's an attitude that's particularly common among software engineers: see an artifact as its surface presentation and nothing more. I think there's no there there. Most of my time as a software engineer is spent understanding what someone else was thinking and trying to accomplish at the time through the lens of the code they ended up writing. Is that archaeological endeavor not strongly connected to if not exactly what we're talking about here? It's just that I also know how to enjoy looking at things without any of that. reply vidarh 17 hours agorootparentprevWhat it looks like. Whether the image itself appeals to me or moves me Whether those are \"artistic choices\" or random chance or a set of heuristics does not affect my enjoyment. It does not mean I don't find knowing about the choices of an artist or their process interesting at times, but for the vast majority of art I see I have idea about them. They are separate to my enjoyment of a given image. I don't know why you are skeptical - to me the notion that anything but the image itself should be necessary for me to enjoy it is a bizarre notion and I suspect it would be for a whole lot of people. Most people would not be able to name whomever created the vast majority of art they've seen, nor name the style, and do not spend time thinking about either. reply PsylentKnight 17 hours agorootparentprevI would say that the default mode of consuming art is on a surface level where you don't pick apart why it's \"good\" or how it came to be, you either like it or dislike it in an unconscious/intuitive way. I can understand your mode of art consumption, but I'm not sure I understand why you're so skeptical about this. reply PH95VuimJjqBqy 15 hours agorootparentprevlmao, so glad to see this comment. I was about to post that not all of us are art snobs but your post captures the idea succinctly. reply vidarh 2 hours agorootparentI used to get angry at my Norwegian teacher when we were forced to analyze poems, because I like writing poems but never once felt compelled to insert the kind of forced symbolism he kept insisting we rip apart and shred and murder poetry to identify. Because it wasn't what I wrote or read poems for. Sometimes I'd even seek out a certain translation of a work because I enjoyed the beauty of the choice of word of the translator more than the authors underlying ideas. (I've never gotten through the original of Whitman's Leaves of Grass, because I find it trite, but there's a particular Norwegian translation I loved - the authors intent was identical, but one presentation of it was beautiful to me in ways the other has never been because of the patterns of words rather than meaning) Whether or not that intent or symbolism was there in a given poem, I found the process inherently destructive for my enjoyment of those poems, and I utterly detested the process because it felt like violence. The one time I wrote a poem with a message was as a task in his class, and it was a sharp denunciation of the analysis of poetry. No analysis was necessary - the intent was brutally apparent and quite rudely expressed. It is also the only poem of mine I've performed in 'public', which was a mistake of him, because it's perhaps the one thing that I have written that has met with the most universal approval among those who heard it, and it hardly improved the attitude towards the analysis of poetry. I still find pulling art apart to often be brutally destructive and occasionally insulting in it's often shallow insistence of knowing intent that usually is without actual evidence and mired in dogma. I don't mind people finding meaning in knowing more about how it was created when the creator of a piece of art wants that context to be known or part of the work, but I feel very strongly about assuming intent even for human art, because to me at least, for what I wrote, my intent usually was to write without any deeper meaning or symbolism, to evoke emotion. For me, for what I wrote, picking it apart ruins that on every level in ways people rarely are able to undo for themselves. It's like trying to reassemble a cadaver. People can find their own meaning in anything, and make their own choices, but so much art snobbery revolves around that assignation of a dogmatic interpretation of intent as \"correct\" and objective that often feels outright disrespectful to assume to me. reply eightysixfour 17 hours agoparentprevThis is like a painter lamenting the invention of the camera because it takes away their ability to enjoy realist paintings. Now we laud photographer’s choices and denigrate the ones made by folks producing AI images. DALL-E is the disposable camera of AI images. There are plenty of artists working with more complex tool chains that take lots of effort and energy to create, and form their own type of AI art. reply skepticATX 17 hours agorootparentPhotography still takes a ton of skill though. It's still ultimately a human creation. AI art is the antithesis of this. reply vidarh 17 hours agorootparentLike with photography, you can put in zero effort, or you can spend hours, days, weeks tweaking details. Some people just churn out images they happen to like from low-effort prompts, some people fine-tune their own models to get just the look they want. reply MiiMe19 17 hours agorootparentprevI think that if photography takes skill compared to painting, then AI art takes skill compared to photography. Just different skills. reply eightysixfour 17 hours agorootparentprevThis is what painters said about photographers. Low quality photography is not elevated to the status of art. Low quality AI generations are not elevated to the status of art. The actual art will be made by people with something to say or show and it will take energy and effort - it won’t be made by typing your idea into DALL-E, it’ll happen with complex workflows on fine-tuned models. It is just another tool with different strengths, weaknesses, and constraints. reply skepticATX 17 hours agorootparent> it won’t be made by typing your idea into DALL-E, it’ll happen with complex workflows on fine-tuned models. I'm not convinced that this is what will happen long-term, but if it is, I'd completely agree that it's a new art form. reply pcthrowaway 17 hours agorootparentprev...until all photography is AI-enhanced also https://www.theverge.com/2023/3/13/23637401/samsung-fake-moo... reply op00to 17 hours agorootparentprevNah, it really doesn’t take skill. My 4 year old takes photographs all the time. reply vidarh 17 hours agorootparentAnd of course that is a frequent criticism of some other types of art too. E.g. my son loves art, but gets exasperated at anything remotely abstract. Part of my enjoyment of taking him to art exhibitions is things like when he walked past a Picasso with a look of utter disdain and loud audible huff. Or the conversations we had about Matisse's sculptures, or the paper cutouts from his later years. (I don't really like Matisse either - the thing I enjoy most of the visual experiences of the Matisse museum in Nice is the olive garden, despite the total lack of intent behind how the tree-branches grow) reply op00to 10 hours agorootparentI don’t appreciate my kids pictures, but he seems to get some sort of enjoyment out of the act of creation. I think art is based on the intention of the creator and not the viewer. reply vidarh 2 hours agorootparentI think it's fine for it to be either, as long as we recognize that unless the creator tells us their intent, our interpretation is just a interpretation and may or may not even intersect with theirs. My son is 14 now, and draws plenty of things I enjoy for their actual appearance, but I know most of them are scribbles or practice to him, and I don't try to analyze them for intent that I know usually isn't there. Meanwhile, two of his pictures from when he was smaller that I know he did with intent, because he told us, and we wrote it down, are scribbles to me. Shapeless blobs. One of them I quite enjoy on a visual level, but while to him it was a fox, to me it is a red swirl that while visually pleasing in no way is anything like a fox. While I wouldn't have it on the wall if it wasn't my son's, his intent behind it does not make it better. The other is hideous, but his intent was to paint his mum and me, abd so I love it. For that one the intent is what gives it value to me. That is a rare exception. But only because he told us, and because of the emotional value of that. He could've given us any random painting and said the same thing, and it's value would be the same. The \"art\" in that instance was his statement. I also know from discussing his drawings with him, that whether he enjoyed a given drawing or I enjoyed seeing it, often correlates poorly - many of his that I enjoy are things he dismisses completely for reasons that does not affect my enjoyment of them at all. reply tasuki 2 hours agorootparentprevCreating > consuming It's good the kids know... reply stonedge 17 hours agorootparentprevAnytime I see a photo taken on actual film, even though it's been digitized for posting online, there's a visceral reaction of beauty. To you point of DALL-E being the disposable camera, I feel those images are in fact \"disposable\". I feel nothing. reply tasuki 2 hours agorootparentAre you sure you can recognise it? reply eightysixfour 8 hours agorootparentprevI don’t really buy your claim that you have a “visceral” reaction to every image you see captured on actual film. Ignoring the oddity of that statement, even if you did, I’m not sure why what you feel is supposed to determine what is or isn’t art. reply asadotzler 8 hours agorootparentprevThe rudimentary knobs on generative AI tools almost guarantee low quality output but they can do it with such speed and in such volume that trained artists are concerned it will overwhelm the market. Yes, photography was similar in how it overwhelmed painting, but the difference is that photography, from day one, offered a gloriously rich set of tools for making pictures while DALL-E is a peg-board toddler toy by comparison and the results speak for themselves. Just compare the first few years of photography to the middling pablum we have that's spit out from these generative AI models. Despite the models stealing the entire history of world art and putting all of that at the fingertips of untrained artists, the results are still embarrassingly naive and mostly boring. The reason is that art isn't about the tools, it's about the mind and people with no training and no experience in art cannot make consistently good art without training, even with tools that let them create sophisticated collages of other people's real art. reply jetrink 17 hours agoparentprevI think that's 99% true right now, but it's not something that will be true forever. Almost all AI images that you see today are the result of a person typing a description into a prompt and getting an image back. If you're lucky, the person played with the prompt a bit until they got something that more closely matched their intention. However, there are tons of ways to integrate AI into a creative process in ways that allow an artist to iterate on an image and to be intentional about composition, color, style, and so on. Currently, it's only highly technical people who are able to do this, so the result is a lot of stuff that is technically impressive, but of low artistic value. (Quite similar to the output of the stereotypical camera nerd who has the latest camera and has its specs memorized, but takes uninteresting photos.) Eventually, these tools will find their way into the hands of artists and they will use it to make art. I believe that when thinking about AI art, it's helpful to keep photography in mind as a source of analogies. For instance, both give anyone the ability to create images easily. Both gave existing artists heart attacks and were dismissed because they didn't require artistic training to use. Both resulted in a flood of images being created without thought or effort. The main difference is that we skipped straight to the smartphone camera age with AI. reply gdubs 17 hours agoparentprevThis seems highly subjective. I'm DEEP in the AI Art space and yet I still love looking at traditional art, photography posts on instagram. This is like saying that post-photography there was no reason to look at paintings anymore. And while the camera certainly disrupted art, it also led to a burst of creativity and gave us Picasso and Pollock. reply masswerk 17 hours agorootparentI'd argue, AI art occupies the negative space of abstract expressionism and informal painting. It has more or less catapulted us back in time, before there was photography and subjective impression and expression became the actual subject. Even more so, in lack of a projecting author, in who's projection we may conversely project ourselves, thus constructing a collaborative work of art, it crucially lacks any transcendent qualities as the author of the prompt lacks any finer control over an output, which is merely a weighted average. reply nusl 17 hours agoparentprevI think it depends greatly on a person’s intentions when viewing an image, or their expectations. When I look at stock images I couldn’t give a damn. When I’m looking for art for my wall or desktop wallpaper I may give more of a damn. When I’m looking for art as a gift I will give even more of a damn. For the vast majority of applications I think AI “art” is “good enough” but this is the part that makes me really sad and conflicted. I don’t know a way forward for most artists or folk that create “general” images. reply amelius 17 hours agoparentprevWell, I get your point, but through prompting there are still a lot of choices that can be made by an artist using AI, even in an iterative way. On top of that, real artists will often find that they surprise themselves, and those surprises will drive the eventual creation. So this is a similar process, except it goes through AI instead of the unconscious part of the brain of a real artist. reply Ferret7446 9 hours agoparentprev> There is really no reason anymore to look at any posted art, image or article illustration Given the demand for AI art, it's safe to say that most people disagree. Personally, I like looking at pretty pictures. Also realize that death of the author was proposed many decades ago. reply masswerk 8 hours agorootparentWell, there's nothing new in this per se, text theory suggested a quasi autonomous \"weaving\" of texts and imaginary content some 60+ years ago. (Well, it's all about semantic fields and the implications were well known. – BTW, Neal Stephenson wrote an hilarious parody on this as an academic mainstream phenomenon in his Cryptonomicon, look up \"Text at War\".) But there was still some agency and some effort involved, maybe even reputation, some capital, which served as regulating factors. Now it feels more like marketing blurb without a product and no careers to be pursued behind this. Also, crucially, those representing the demand for AI art are not those who are meant to consume it. It's still too soon for any systemic feedback, other than economic factors and incentives. reply asadotzler 8 hours agorootparentprevI've looked at thousands of images created by people twiddling the rudimentary knobs on generative AI tools and it is universally garbage, often trumped by assembly line produced motel paintings. As someone with 5 years studying art and architecture in school, and 30 years of participating in art post school, I feel quite comfortable sharing that review. reply the-smug-one 17 hours agoparentprevArtists that I know work in the way of exploring and learning about a subject matter and creating art relating to it. When I see this kind of art, and talk to the artists, I get the sense of exploring the subject and learning with them. A lot of art isn't this, it simply exists for much more plain purposes, and that type of art seems to be what AI is able to generate. I feel disgust towards the idea that people will be able to create their own art using AI, as the current state of the art only allows for quite shallow creations. I'm not saying it's wrong, but I am disgusted. reply skepticATX 17 hours agoparentprevAgreed. I think that the concern about disinformation stemming from AI images/video misses the bigger picture. Humanity will adapt to these new technologies. We'll begin to trust what we see less and less. Which will prevent us from being fooled, but we also lose a massive part of the human experience in the process. Soon the days of being able to view a photograph, a video, artwork, and appreciate it as human ability at its pinnacle will be gone. reply douglaswlance 17 hours agoparentprevThe image is the image. The curtains are blue because the curtains are blue. reply gosub100 15 hours agoparentprevAI is not only a threat to generation of art, but also to consumption. For decades now, hollywood movies have been tested to \"focus groups\" to decide which way the stories should go to maximize profits. with AI, you can train it to maximize \"engagement\" to \"discover trends\". This could end up affecting even traditional artists. It could Hollywood-ize other forms of art. reply causal 17 hours agoprevI understand that AI is especially academic right now, but I'm not sure OP appreciates just how impenetrable everything else was to the masses before him. Nothing about DOS or SMTP have ever felt accessible to the vast majority of people before now. If anything this should spark some empathy. Furthermore, jump on some Discord groups and subreddits like /r/localllama or /r/stablediffusion you will see a vibrant AI hacker community that is alive and well, working very hard to build tools for the masses. Don't resign yourself just because you have not mastered this new thing by default, regardless of whether that's the world-wide-web in the 90's or tensors in the 20's. reply thenoblesunfish 17 hours agoparentI don't think that's quite it. I think OP could understand the math quicker then they think, but would still feel the same, because the whole point of AI is to do the understanding for you. You could of course argue that computers think for you, but you can always break things down and understand what it's doing and then marvel at doing that thing super fast and with massive parallelism. With AI there is less of that feeling, because the \"guts\" are learned, not built. reply chankstein38 17 hours agoparentprevYour whole comment is full of really well put assertions and your points about llama and SD are what my thoughts were the whole time I was reading this blog post. BUT >Don't resign yourself just because you have not mastered this new thing by default, regardless of whether that's the world-wide-web in the 90's or tensors in the 20's. Might be my favorite part of what you said. It's so true and I meet so many people who are used to things just being something they understand so they approach a more complex topic they don't have context on and start \"feeling stupid\" without realizing how vast the world is. All in all, well said! reply insane_dreamer 16 hours agoparentprev> vibrant AI hacker community that is alive and well, working very hard to build tools for the masses it's not the tools that are the obstacles; it's the training data and training resources (why do you think OpenAI had to sell itself to Microsoft?) reply mangoman 18 hours agoprevI understand the sadness around not understanding it, it's fucking hard. however, there are more and more resources online getting published for how to get started understanding it that help with understanding the math at an abstraction that helps with learning how to build with it. I would strongly strongly strongly recommend starting with karpathy's from 0 to hero neural networks youtube course - it starts with building a tensor library and back propagation, explaining it in a way that finally clicked for me https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThs... jeremy howard also has a fantastic video that is more around how to use LLMs and such called a hacker's guide to language models - https://www.youtube.com/watch?v=jkrNMKz9pWU&t=607s as i've dove more and more into it, i would strongly recommend trying to run things on your local machine too (llama.cpp, ollama, LM studio). that has helped me fight that feeling of like \"are we all just going to be open AI developers in the end\" and made me feel like you _can_ integrate these things into stuff you build by your self. I can't imagine how fucked we'd all be if llama was never opensourced. being old does not mean that you can't continue to grow, and remember that it's okay to feel overwhelmed about all this - many people are. reply jmwilson 17 hours agoparentThere's different levels of \"understanding how things work\" and the author makes it clear what kind of understanding he's going for. If you look at the source code to program, you should be able to point to any line of code and answer \"What does this particular line of code do? Why is it important and how does it relate to the rest of the design?\" Same applies to a part on a electronic schematic or a mechanical drawing. There is likely no similar meaningful answer to those questions if you look at a particular weight in a model. reply therealdrag0 4 hours agorootparentI’ve seen mention of researchers being able to point to specific neural pathways in AI for both simple things and even more advanced abstract things like “lying” or “truthfulness”. So it’s not a total lost cause maybe. reply Workaccount2 17 hours agoparentprevI think what the author is trying to get across, and what I tend to agree with having touched on the mathematics behind transformers at least, is that we don't know how these models actually arrive at the outputs they do. We know the rules they play by thoroughly - we made those ourselves(the math/model structure). But the outputs we are getting in many cases were never explicitly outlined in the rule set. We can follow the prompts step-by-step but quickly end up on seemingly non-nonsensical paths that explode into a web of what appears to be completely unrelated concepts. It could be that our meat brains simply don't have the working memory necessary to track these meta and meta-meta emergent systems at play that arrive at an output. reply bemusedthrow75 15 hours agoparentprevI am profoundly, profoundly cynical about this particular development in computing culture. But your last paragraph resonates with me pretty deeply, and suggests to me that there might be a way forward for me when this becomes unavoidable, which it will. Frankly I would rather direct my energies away from the accelerating face of dehumanising technologies and towards rehumanising technology through education, but I do recognise I'll eventually have to engage with this just to educate. reply archgoon 17 hours agoparentprevI don't think you've grappled with the point the author is making. >“If we open up ChatGPT or a system like it and look inside, you just see millions of numbers flipping around a few hundred times a second,” says AI scientist Sam Bowman. “And we just have no idea what any of it means.” >To me as an engineer, that is just incredibly unsatisfying. Without understanding how something works, we are doomed to be just users. AI aren't complicated. They aren't sophisticated math that you can poke at and understand. They're fucking million dollar spaghetti code that happen to work (for values of 'work'). Those videos are teaching people \"This is an if statement! This is a CPU!\" And then you can look at 5.8 billion lines of spaghetti code and say \"Gee! I understand how this works now! Yay!\" reply ryandrake 14 hours agorootparentAsking because I literally do not know: Can you step through AI like you can step through C++ code in a debugger? Like, if you type in a prompt \"Draw me a picture of a cat wearing a blue hat\" could you (if you wanted to) step through every piece of the AI's process of generating that picture like you are stepping through code? If I wanted to understand how a Diffie–Hellman key exchange function worked, I could step through everything line by line to understand it, it would be deterministic, and I could do the exact same thing again and see the exact same steps. reply Towaway69 1 hour agorootparentYou probably could but what would you see? A bunch of weights, connections between layers and more numbers. You don't see any meaningful, understandable code. For example, if prompt begins with draw me a picture then jump to layer X. I'm no expert but I can imagine that to be the problem when one attempts to debug an Algorithmic Intelligence black box. reply orangecat 17 hours agorootparentprevAnd then you can look at 5.8 billion lines of spaghetti code LLMs don't have anywhere near that much code. The algorithms for training and inference are not that complicated; the \"intelligent\" behavior is entirely due to the weights. reply jsheard 17 hours agorootparentOP clearly means that the weights are spaghetti code, technically they may be data but if they encode all of the actual functionality of the system then they are effectively bytecode which is interpreted by a runtime. You can understand how the runtime works if you care to learn, but you will never understand what's happening below that, nor will anyone else. Aside from annoying people who want to understand how things work, it also means you can't ever know if you have a fully optimal or correct solution, all you can do is keep throwing money into the training furnace and hope a better solution falls out next time. The whole nature of it gatekeeps out anyone who doesn't have enormous amounts of money to burn. reply orangecat 15 hours agorootparentI can see that, although to me there's a difference between weights and something like bytecode. The weights don't encode any sort of logical operations, they're just numbers that get multiplied and added according to relatively simple algorithms. Totally agreed that the process of generating and evaluating weights is opaque and not very accessible. reply rep_lodsb 13 hours agorootparentYou can simulate any digital circuit by multiplying and adding numbers. reply rep_lodsb 17 hours agorootparentprevBut that's exactly the point. The code you are talking about is more like an interpreter for a virtual machine, which then runs a program made up of billions of numbers that wasn't designed by a human (or any sort of intelligence - you can argue about the end product, but the training process certainly isn't intelligent) reply smolder 17 hours agorootparentprevThe weights are what's analogous to 5.8 billion lines of spaghetti code, here, when doing inference. reply mattgreenrocks 17 hours agoprevMy biggest worry with AI and software dev is the degree to which people are okay with things that sort of work. I mean, we're already there with code that humans write. I'm not sure that humans who can't write the code themselves can always fix bugs that sneak into AI-generated code. It's almost less about software and more about how our society just seems to not give a damn about expertise because it costs someone more money. I have a good career built on that expertise along with emotional intelligence, but it's a bitter pill to swallow knowing that everyone's trying to deleverage themselves from needing to pay me for my expertise. I've avoided this to some degree by focusing more on fundamentals than BS like AWS service invocations, but I'm doubting that this strategy will continue to work long-term with AI around. The real sad part is it feels like everyone's happy to suck every last bit of humanity out of work for ok-ish results that come from ingesting the whole Internet and not compensating people for it. reply Terr_ 16 hours agoparent> It's almost less about software and more about how our society just seems to not give a damn about expertise because it costs someone more money. Yeah, to me almost all the \"old programmer sadness\" is actually cultural, it has to do with a feared difference-in/failure-of values. Maybe that's also why \"enshittification\" is in the zeitgeist right now: It represents a sense of disappointment or betrayal with certain industries/companies/products--but it's with fallible people, rather than with fragile computers. reply mattgreenrocks 15 hours agorootparent100%. I grew up wanting to be a graybeard hacker, then watched as everyone decided that coding should be more social than technical. This feeling of falling out of alignment with values really did a number on me. I can finally say I’ve started to transmute that into energy towards being a solopreneur. It feels like the equivalent of “f this industry I’m taking my ball and going home” but I don’t see any other way that lets me feel agency at this time. Regarding AI: it is a tangible manifestation of the phantoms that keep us running hard on the economic treadmill. Very familiar feeling, really. reply elteto 17 hours agoprevIf you had witnessed the Wright brothers' first flight you would have had a hard time predicting that within 60 years we would be landing on the moon. And if you were playing computer games in the 1980s in a slow DOS box you could not have even imagined modern GPUs and modern games. Nor could we have predicted how far modern CPUs have come since the 1960s. LLMs are a very new technology and we can't predict where they'll be in 50 years, but I, for one, I'm optimistic about it. Mostly because it is very much not GAI so its impact will be lower. I don't think it will be more revolutionary than transistors or personal computers, but it will be impactful for sure. There's a large cost of entry right now but that has almost always been the case for bleeding-edge technologies. And I'm not too worried about these private companies having total control over it in the long run. They, so far, have no moat but $$$ to spend on training, that's it. In this case it really is just math. I think market forces will drive a breakthrough in training at some point, either mathematical (better algorithms) or technological (better training hardware). And that will reduce the moat of these companies and open the space up even more. reply gwbas1c 17 hours agoparentWhen we look at the history of technology, most new technologies can take a few decades before they're ready for the mass market. Television was demonstrated in the 1920s, but it wasn't ready for the mass market until the 1950s. If I remember correctly, the light bulb and automobile also took a few decades to come to the mass market. I think a lot of people are jaded by how fast technology changed between 1980 -> 2010. But, a lot of that is because the technology was easy to learn, understand, and manipulate. I suspect that AI will take a lot longer to evolve and perfect than the World Wide Web and smartphones. reply elteto 15 hours agorootparentYeah, the rate of change since the 1960s has been incredible. I like to think of that as being a direct consequence of inventing an infinitely reconfigurable general purpose calculator :) I don’t know about LLMs… it might hit a performance peak and stay there, same as CPUs have been 3+ GHz for the past 10 years. Or there might come a breakthrough that will make them incredibly better, or obsolete them. We don’t know! And I find that exciting. reply bemusedthrow75 15 hours agoparentprev> and if you were playing computer games in the 1980s in a slow DOS box you could not have even imagined modern GPUs and modern games. I think this bit is really not the case, FWIW. If you look at what computer magazines were like in the 1980s it's very clear that people were already imagining what photorealism might look like, from the very earliest first-person-perspective 3D games (which date back to the early 1980s if not earlier) reply insane_dreamer 14 hours agoparentprev> it really is just math no it's not; it's math + a *ckton of data + massive compute resources training will likely be made more efficient over time, reducing required resources, but training data will always be a major obstacle reply elteto 13 hours agorootparentNot really a fair comparison but children learn to speak with barely any training data compared to LLMs. I’m hopeful a large training corpus will not be so necessary in the future. reply vacuity 13 hours agorootparentI suspect that we won't have the computing power or neurological understanding to create such an AI anytime soon. Even if human thought can be reduced to networks of chemical-filled membranes, the timescale and population involved in natural selection, and the resources consumed to live and reproduce are immense. I think we would need to find a far more efficient scheme to produce emergent intelligence. reply jauntywundrkind 16 hours agoparentprevThe personal computer & the internet clicked for me because I saw them as personal enables, as endlessly flexible systems that we could gain mastery over & shape as we might. But with AI? Your comparison to going to the moon feels apt. We're deep into the age of the hyper scalers, but AI has done far more to fill me with dread & make me think maybe Watson was right when he said: > I think there is a world market for maybe five computers This has none of the appeal of computing that drew me in & for decades fed my excitement. As for breakthroughs, I have much doubts; there seems to be a great conflagration of material & energy being poured in. Maybe we can eek out some magnitudes of efficiency & cost, but those gains will be mostly realized & used by existing winners, and the scope and scale will only proportionately increase. Humanity will never catch up to the hyper-ai-ists. reply pbourke 2 hours agorootparent> The personal computer & the internet clicked for me because I saw them as personal enables, as endlessly flexible systems that we could gain mastery over & shape as we might. I feel the same way. Developments in computing have evolved and improved incrementally until now. Networks and processors have gotten faster, languages more expressive and safer, etc but it’s all been built on what preceded it. Gen AI is new-new in general purpose computing - the first truly novel concept to arrive in my nearly 30 years in the field. When I’m working in Python, I can “peer down the well” past the runtime, OS and machine code down to the transistors. I may not understand everything about each layer but I know that each is understandable. I have stable and useful abstractions for each layer that I use to benefit my work at the top level. With Gen AI you can’t peer down the well. Just a couple of feet down there’s nothing but pitch black. reply Bjorkbat 17 hours agoprevSomething I take issue with concerning AI, which is also incidentally why I remain skeptical of it, is that it feels like it doesn’t increase agency in any meaningful way, and sometimes decreases it. With image generation, you can generate seemingly infinite images with nothing but a prompt, but you’ll never get what you really want. You’ll get an approximation at best. You have all this agency, but also no real agency that matters. But of course if you’re a visual professional of some kind then I imagine it’s even worse, a net decrease in agency. Not only can you never quite get what you want, but having to go in and edit things is tedious and dull. Even if you’re saving time this way, the lack of enthusiasm and flow makes it feel like the job takes much longer than before. Likewise, with programming, at the end of the day it feels as if I’d be more productive and just wrote down my thoughts as code rather than delegate to a copilot or ChatGPT and waste time making sure this actually works or solving for the occasional bug that came from a chunk of mystery code. At the end of the day, you aren’t creating the image or the video, the AI is and you make adjustments where you can but otherwise accept the results. Extrapolating this to software, I don’t imagine a future of where people are empowered to write their own software, but simply a future where people ask for software from an AI, it gives it to them, and they do an awkward back and forth to try and get the details right until they inevitably accept what’s been given to them. It’s depressing, but also so goofy that it’s hard to imagine this being the final outcome. reply simonw 17 hours agoprev\"If I build an app that needs persistence, I might use Postgres and S3 for storing data. If those are no longer available, I’ll use another relational database, key-value store, distributed filesystem, whatever. But what if OpenAI decides to revoke access to that API feature I’m using? What if they change pricing and make it uneconomical to run?\" A year ago I shared exactly this concern. Today I'm not nearly as worried about it. If you haven't tried running local, openly licensed models yet I strongly recommend giving them a go. Mistral 7B and Mixtral both run on my laptop (Mistral 7B runs on my iPhone!) and they are very capable. Those options didn't exist even six months ago. There are increasing numbers of good closed competitors to OpenAI now as well. We aren't stuck with a single vendor any more. reply doubtfuluser 14 hours agoprevI am working in ai all my professional life and have to admit that right now it’s both exciting as well as burning me out. I get tired when I scroll through LinkedIn and see all the dall-e images. I am annoyed by all the snake oil sellers who say that they will change the world - and then there is just another prompt underneath. It all has its right for existence. But I really wish the hype ends soon and people become more realistic again. It will have a big impact on the world, but people don’t get that it’s not there yet and there is a ton of research we still need to do. That’s burning me out. There is so much more work to do compared to the expectations- and even in the scientific community there is such a big volume of junk papers coming out (sometimes even by big companies) that it feels like most of the time wading through all the BS and marketing hype is all I do. reply causal 14 hours agoparentIt is interesting how much our misplaced expectations for AI are totally shaping our experience of AI. reply mym1990 18 hours agoprevI have a lot of these moments nowadays where I try to figure out if I am becoming the old fart, or if my skepticism is actually legitimate. It’s a difficult exercise, often futile, but it’s a good first step. It always felt like my parents never really asked themselves “why do we feel like technology X is bad”…they just had a knee jerk reaction to it and banned me from it. reply Log_out_ 17 hours agoparentValue is human attention and a new generation valuing different things, auto devalues the old. Thugs stay relevant, by being sifted from the sand and put in collages in the new. reply mym1990 16 hours agorootparentWould not be surprised if this is AI generated garbage... reply AlexandrB 17 hours agoparentprevWhat makes me think AI is the real deal is that it was pretty obvious (to me) that crypto and \"the metaverse\" were both dead ends. I certainly have doubts about AI and, like the post's author, I don't like a lot of things about it but I don't get that feeling of \"this is useless bullshit\". reply mym1990 16 hours agorootparentAgreed here, it feels like everything is pointing toward a machine augmented future, and AI has proven its usefulness in a wide range of applications. Ultimately the direction that it goes is not up to any one person, but us as a species. Do I think we can make the right decisions? Not sure...but if we can't...thats just natural selection baby. reply segmondy 17 hours agoprevThis defeatist mentality is plain stupid. There are under 20yrs old working on AI today, they went from 0 to their current knowledge in most cases 2-3 years. So what to do? Level up. I'm an old fart too, blah, blah, blah, since the days of 2400bps modem. For anyone that wants to level up, go take Andrew Ng's ML course, watch some online lectures on neural networks, deep learning, reinforcement learning. Never in the history of the world have we had so much learning resources at our finger tips, plenty of youtube videos, blogs, articles, free books, free courses, software libraries to get into it. Stop with the self pity, get up and dance. Start learning, at some point pick up scikit learn for shallow learning and pytorch for DL. GPUs are super cheap. You can pick up a used 3060 RTX forThe amount of papers I’d have to read and mathematics that I’d have to ingest to really understand why a certain prompt X results in a certain output Y feels overwhelming. Even some top scientists in the field admit that we don’t really understand how AI works. The picture there sums it up well: It just seems impossible to keep up with the pace that AI is moving. So many papers coming out on a daily basis. So many new techniques. It's hard to see how people can keep their skills up-to-date. reply hintymad 15 hours agoprev> I want to understand how things work. AI feels like a black box to me. The amount of papers I’d have to read and mathematics that I’d have to ingest to really understand why a certain prompt X results in a certain output Y feels overwhelming. How is this a legitimate argument? We have invented so many algorithms and used so much math to build our software along the years. They are not easy to understand. Understanding them requires tons of effort, including reading papers. People may have forgotten that 30 years ago, the \"host stuff\" was still systems, and people did read \"deep\" papers. People still do nowadays, except that only the very experts do so. Besides, the math is really not that hard -- merely college level. In contrast, go read an introductory book on program analysis or type system or distributed algorithms. Those maths can be harder as they are more abstract. In addition, the amount of code in a model is orders of magnitude than a compiler or a distributed system or game engine and etc. I'd argue that it's actually easier to understand how a model works. reply linuxhansl 17 hours agoprevI'm an old fart too. First computer what the ZX81 with 1K of RAM :) That's how it all started, and made a carrier out of it. What always excited me about software was that I always had renew myself, learn a new thing, change the way I think about something, reflect on what is now possible that wasn't before, etc. I.e. it was never stagnant. In addition I could try out everything myself, and (more of less) understand what it is doing and why. When I wanted to understand RSA, I read the paper and implemented a PoC myself, or a BTree, or an LSM tree, same for Paxos and Raft. I worked a lot on databases and \"BigData\" and on the re-convergence of the two. Much of this is open source, so I could play with it, change it, etc. In that AI is indeed different. I can install (say) Ollama on my machine and play with it, look at the source code (llama.cpp), etc. And, yet, when I get a response to a prompt, even locally on my machine, I feel blind. And I used to work on neural networks in the late 90s, when their use was limited, so I understand what they do and how they work. How exactly was that model trained? On what data? What did it actually learn? (Aside: Here I am reminded of early usage of neural networks to detect enemy tanks. It worked perfectly in the lab, would correctly classify enemy vs friendly tanks, and in a field test it failed terribly - worse than random. What happened? Well it turned out that the set of photos with enemy tanks mostly showed a particular weather pattern, whereas the friendly photos predominantly showed another. So what the neural network had actually learned was to classify the weather. You might laugh about this now... But that's what I mean.) So, yeah, I can related to OP, even though I am excited about what AI might bring. reply leonroy 17 hours agoprevA lot of downers on AI and I can understand it, part of it is the response to things invented when we're older - new music, new movies, new technologies. I guess most of our brains are less plastic as we age and we resist incorporating these new, unfamiliar things in our lives and instead reaffirm the old attributes that make up who we think we are and what we already identify with. Another aspect I think which makes us down on AI in particular is that it's the first thing which readily seems able to threaten our job security as programmers. I'd propose a thought experiment where we imagine LLMs and other AI model types don't exist but everything else in computing stays the same (shift to cloud, increasingly asynchronous and interconnected systems). That world actually seems pretty bleak to me. AI upends a lot of industries and yes, it will upend some of our careers. But a world in which it exists seems a lot more interesting than one in which it doesn't. reply smugglerFlynn 16 hours agoparentI'd guess there is another factor - with age you understand that promises hyped up around something new are often false. Instead of solving some deeply rooted problems, people chase shiny and new, while society stays more or less the same. reply mleroy 16 hours agoprevI disagree with the author. Insights and opportunities to understand new technological developments increase every year. In the early years of these technologies, they were always less approchable to me. While in the 80s, I had to rely on outdated books from the library for superficial knowledge about PCs, now I can actively engage with AI developments in near real-time, experimenting with it as a service, running it on my own machine, or even training smaller models. However, I am apprehensive about how society will navigate these new AI advancements. I believe we won't be able to adapt concepts and cultural techniques as quickly as the reality shifts due to ubiquitous AI. These social changes are beyond my comprehension and overwhelm me. My engineering education has always helped me explain technology to both my parents and my children. But for now, it's just a matter of \"fasten your seat belts.\" reply psygn89 18 hours agoprevI know AI makes you sad for a different reason but I always find amusement when reading a title like that and seeing an AI generated image. reply pimlottc 17 hours agoparentI get that it's maybe funny or ironic, but it still undercuts the argument and helps to normalize the practice of using generated images as SEO eye candy. reply ryandrake 14 hours agoprevI think a lot of companies are scrambling to \"Get Into AI\" without knowing why they want to besides \"everyone else is getting into AI and Wall Street expects us to as well\". This is the exact same hype bubble as what happened with cryptocurrency. Everybody just added \"with blockchain\" to their mission statement and jumped off the same cliff. Once the bubble pops, and companies are left reeling after burning $trillions without business results, there's going to be a frenzy to hire back traditional software engineering professionals to get the company's tech stack back on track. As an Old Fart I am hoping to be one of those. reply mo_42 17 hours agoprev> I want to understand how things work. AI feels like a black box to me. The amount of papers I’d have to read and mathematics that I’d have to ingest to really understand why a certain prompt X results in a certain output Y feels overwhelming. First, no paper will make you understand why prompt X made result Y. But you can understand the architecture of these systems and try to understand the prompt answer relations somewhat intuitively (e.g., by learning your own models with various text collections). I think we need to accept these AIs as creatures of their own kind. As complex actors that we don't fully understand. Humans have been breeding dogs since tens of thousands of years and we don't understand them fully. But we understand enough to employ them in useful ways and mitigate the risks. reply hibikir 17 hours agoparentMost of us don't understand many complicated things we interact with in a regular basis. Back to even the simple cases of what happens when I type something in this form, and hit enter. Every keystroke is a bunch of work, by a bunch of devices talking to each other through complicated protocols, just to get to memory, which will get to my screen, eventually... and that's pretending we don't care about how physics works, because it's reliable. It's all overwhelming. And if we only focus on the unreliability, look at many bugs in our systems today. Why does this crash with some input? Why does a new enough processor have security vulnerabilities, so someone can steal passwords? All might as well be magic, even though a few humans, somewhere, might know the reasoning. And if AI feels difficult, imagine biotech. I could, theoretically, mess with weights and retry a prompt over and over again, seeing what changes. It's a lot of work, but it can be done. See how much fun we have figuring out what a single nucleotide polymorphism does phenotypically, and why it does it: It's a research project by an expert in the easiest of cases! We are only a little ahead of the new C programmer changing things at random to see where his pointer math went wrong. We don't understand how anything works. We are just sometimes satisfied with our degree of ignorance, and decide to stop asking questions. reply Workaccount2 17 hours agorootparentAI is different because even when you understand all the steps in generating one token, it still doesn't help you understand how the next token works. I can press a key on my keyboard and work through the system to understand exactly how the physical press up to the letter on the screen works, and then apply it to all the other keys no problem. AI does not work that way at all. The steps are seemingly random, meandering, and nonsensical, yet it still ends up with these well structured chains of tokens on the output. reply tim333 12 hours agoprevI'm also an old fart and I'm quite cheerful about it. It's fun to play around with, will change the world and doesn't seem that impenetrable - see for example \"Let's build GPT: from scratch, in code, spelled out by Andrej Karpathy\" https://news.ycombinator.com/item?id=34414716 Not that I've really groked that stuff myself. reply insane_dreamer 16 hours agoprevIt's not just the fact that it's not open and accessible; it's the fact that it is owned and controlled by a tiny number of companies with the necessary resources. It's like a world where Windows and a non-unix based Mac are the only OSs. There is no possibility for something comparable to Linux (FreeBSD etc. etc.) could emerge as viable alternatives free of large corporate control. So it's not just sad, it's a major problem. Not right now because it's more of a novel toy than anything, but as more systems are built on these LLMs we will be increasingly at the mercy of the companies that control the LLMs. reply nbraem 16 hours agoprevThis is because currently deep learning is not a science, but engineering. There is no underlying theory why deep neural networks generalize as well as they do. Classical learning theory (VC) actually states that large models with millions of parameters should not work. There are some academics working on this, but it pales in comparison with how much money is being poured into generative AI. So today's state-of-the-art models are trained with trial and error, and experts who are building some intuition why some methods work and others don't. reply asenna 17 hours agoprevWhat's with all the sad posts with what's happening right now? I'm in my 30s and as a builder, what I've seen in the last 12 months has been incredibly exciting! So many cool things can be built with these tools we have now, so much faster. And while doing this, our experience will be useful in companies wanting to integrate these AI tools. Checkout what's happening with open / local LLMs, tiny LLMs running on RaspberryPIs, LLama3 about to drop any minute now, Google just released a 1-million context model. Feels incredibly exciting, I'm not able to relate to these posts. reply Taylor_OD 17 hours agoprevI'm trying pretty hard to learn how to work with AI rather than rage against it. Vernor Vinge's classic Rainbows End comes to mind so often these days. All the people who refused to embrace new technology are left behind. Almost to the point of being forced out of society. I think its likely AI will continue to expand. I hope that I can utilize it well enough that I'll still be employable in the future when it can do my current job. Or at least that I know how to get it to pump out endless amounts of enjoyable entertainment so I wont mind not having a job. reply elorant 17 hours agoprevI don’t agree with the sentiment. Sure, I don’t understand exactly how it works, and I have no way of training one from scratch on my own, but I also can’t build a web server on my own. The thing is that I already self-host a small model for my needs (Vicuna-13b) and it works just fine. Next thing I’d like to try Mixtral 8-7b which looks as capable as GPT 3.5. And that all with only one year since the field emerged. Who knows what we could build five years from now. reply Shorel 13 hours agoprev> But what if OpenAI decides to revoke access to that API feature I’m using? What if they change pricing and make it uneconomical to run? What if OpenAI extends their offering and makes my product redundant? This point hits the nail in the head. If it can happen, and it will increase someone's profit, it will happen. It's not a matter of if, but when. reply Salgat 16 hours agoprevWe're already at the point where abstractions need to exist because devices (such as a modern x64 CPU) are so incredibly advanced. Even knowing assembly is a high level abstraction over an incredibly advanced closed-source chip. This article is more about someone afraid of new technology than it is about someone lamenting that ML is inaccessible or \"unknowable\", when all that matters is the observed behavior for day to day usage. reply skeeter2020 15 hours agoparent>> This article is more about someone afraid of new technology This is a predictable response but just doesn't hold up. The path through abstractions for a modern CPU/GPU is more straightforward and linear than AI, and you don't have to go very deep to be back to 50-yr-old principles. >> when all that matters is the observed behavior for day to day usage. The entire point of the post is that observed behaviour is NOT what matters, vs understanding how we get to these outcomes. reply lp4vn 17 hours agoprevI think the author is absorbing the negative atmosphere of the zeitgeist we're currently living in - i.e people struggling to make ends meet, inequality rising, environmental catastrophe in the making, wars, public discourese based on cynicism and outrage, etc - and using it against AI. AI itself is a pretty scientific, neutral subject. On the other hand, the use we're making of it currently and the use we will probably make of it in the future is something certainly depressing. reply alex_suzuki 16 hours agoparentYes, maybe. I do think that the world has seen better days, but I‘m not the guy holding up the „The end is near“ sign at the next street corner. Not yet, anyway. reply lovasoa 17 hours agoprevI like to think of AI as the physical CPU my programs run on. I have no idea how to build a modern CPU, and building one would be way out of my reach anyway, but it is general-purpose enough and interchangeable enough for me to be able to build whatever I want on top of it without worrying too much. OpenAI is not the only language model vendor, and the trend seems to be the commoditization of these models. edit. Looks like someone made almost exactly the same comment at the same time. reply eightysixfour 17 hours agoprevThis is hilariously backwards for me. I’m deeply technical but hate writing code and AI has empowered me to create more this year (digitally) than any previous year in my life. I don’t mind getting into the nitty gritty details of the how transformers work, all the different types of models, etc. because they are so empowering for me. I have never felt that way about a programming language, framework, or anything else in the digital space before. reply theflyingelvis 17 hours agoprevThis article almost exactly describes my feelings about AI. Way too much crap to ingest to really understand it and I just don’t have time to ingest it all. reply vagab0nd 17 hours agoprevA crazy thought: AI needs protection from human, not the other way around. AI will become better exponentially. Extrapolate the theme of this post, what do you get? A significant portion of humanity that are so mad at AI they'll do something about it. AI is fragile. It needs chips, power, supply chain, maintenance. If enough people are anti AI, our path to AGI can be greatly hindered. reply tim333 12 hours agoparentI think OpenAI, Google, Facebook and the like will plow ahead regardless. reply fritzo 17 hours agoprevGosh I find modern generative AI quite approachable exactly because there is a simple mental model: both transforms and diffusion models are conditional probability distributions trained on the internet. I feel comfortable with conditional probability distributions. I'm comfortable on the internet. Ergo AI makes me happy. reply thenoblesunfish 17 hours agoprev> Smartphones are, for the most part, accessible in the same way as other computers are Maybe true in theory, but in practice, less so. reply pizzalife 17 hours agoprevI'll admit I only took one ML course more than a decade ago, so I don't really know how LLMs work and how to train my own models etc. Could someone recommend a starting point to start learning more (book, how-to-series etc) for someone with a non-AI software engineering background? reply EVa5I7bHFq9mnYK 13 hours agoparentHere is a starting point for you: borrow $100M for the training ... reply world2vec 18 hours agoprevA classic quote: “I've come up with a set of rules that describe our reactions to technologies: 1. Anything that is in the world when you’re born is normal and ordinary and is just a natural part of the way the world works. 2. Anything that's invented between when you’re fifteen and thirty-five is new and exciting and revolutionary and you can probably get a career in it. 3. Anything invented after you're thirty-five is against the natural order of things.” -- Douglas Adams reply zikduruqe 17 hours agoparentAll I know, is that I have been in technology my whole career. 99.999% of everything I have built, designed, launched and implemented is no longer in existence. With retirement on the horizon, I cannot wait to close my laptop lid and never touch it again unless it is an absolute necessity. reply vidarh 17 hours agorootparentFor my part I hope to retire early so I can spend more of my time building and implementing the things I enjoy... I'm not criticizing your desire - it's just that for me, the learning and exploration and building is the fun part. As long as it's still either useful for me, or I have the memories, it doesn't matter if it's still used by others, though that can be fun too. I couldn't imagine having made this my career otherwise. reply zikduruqe 17 hours agorootparentI remember a time without internet and without cellphones. I have helped build those things from scratch, all the way until where we are today. I have great memories and have made a living from it. But, it is not satisfying to the soul. I just want to get to a time where I am not glued to the internet and do not carry a cellphone. The absolute freedom it brings to me is something that a lot of people are not familiar with today. I don't want to get into too many details, but a side gig I have, when I bring people out into the field, I take their phones from them. And to watch the withdrawal that they have from the lack of dopamine hits is just sad. reply vidarh 17 hours agorootparentIt satisfies my soul. I accept that does not yours. I have no problem putting it aside; I enjoy e.g. going for walks and sitting down to meditate. But even then, if anything, I'm likely to come out of it bursting with new ideas for code I want to write or things I want to explore, and I hope that feeling never goes away. reply nightski 17 hours agorootparentprevThat's fine as a personal anecdote, and to be honest you sound burned out. But making absolute statements like \"it is not satisfying to the soul\" does not help your argument at all. Happy to provide my anecdote. In my 40s and I still live to build/create. reply zikduruqe 17 hours agorootparent> But making absolute statements like \"it is not satisfying to the soul\" does not help your argument at all. It is my soul. If it is not satisfied, who are you to tell me different? We all have to chop wood, carry water. reply ziddoap 17 hours agorootparentThey are taking issue with you writing: >it is not satisfying to _the_ soul. Instead of writing: >it is not satisfying to _my_ soul. The first could be interpreted as you speaking generally (i.e. you think no ones soul can be satisfied by building tech or whatever), the second makes it clear that you are talking about specifically your soul not being satisfied. reply UncleOxidant 17 hours agorootparentprevIt's zikduruqe's soul not yours. In my 40s I would have said the same as you. In my 60s it's starting to feel meaningless. People are in different stages of life. reply nasmorn 16 hours agorootparentShit I am in my 40s and it is already pretty meaningless reply ChrisMarshallNY 17 hours agorootparentprevSame here. I'm getting ready to explore ML applications on Apple systems, but first, I need to finally get around to learning SwiftUI as a shipping app system (I have only been playing with it, so far. Doing ship work is an order of magnitude more than the simple apps that are featured in \"Learn SwiftUI\" courses). I'll probably do that, switching over to using SwiftUI for all of my test harnesses. My test harnesses tend to be fairly robust systems. So far, it looks like I may not be using it to actually ship stuff, for a while. Auto Layout is a huge pain, but it is very, very powerful. I can basically do anything I want, in UI, with it. SwiftUI seems to make using default Apple UI ridiculously easy, but the jury is out, as to how far off the beaten path I can go. BTW: I have been \"retired,\" since 2017. I wanted to keep working, but no one wanted me, so I set up a small company to buy my toys, and kept coding. I also love learning new stuff. reply dkasper 17 hours agorootparentprevI feel similar, but keep going another 10 years and we might lose the desire for that type of fun too, not to mention the “building” part might look radically different and not be “fun” anymore. Or maybe it will be more fun, but point is things change. reply vidarh 17 hours agorootparentThe building looks how we want it, though. A large part of my building has been replacing things because I don't like how it works. E.g. a few years ago I switched to my own text editor. Now I'm running my own text editor in my own terminal, using \"my\" (in this case a port from C to Ruby; the C version was not mine) font renderer, running under my own window manager, using my own file manager and my own desktop switcher... And while some of the above had reasons, I am itching to rewrite more of my stack, and mostly for fun or out of curiosity than any need. Things looking radically different is more likely to affect my enjoyment of it as a job than in general, though, because for my side projects I can build things as I please, using the tools I want (and mostly have written myself), and can ignore everything I don't like. I absolutely agree things change, but I've been doing this for 42 years now, and so far it doesn't feel any different, so I'm going to guess I'll stick to this for some time still. reply UncleOxidant 17 hours agorootparentprev> I have been in technology my whole career. 99.999% of everything I have built, designed, launched and implemented is no longer in existence. 60-something here. I know exactly how you feel. I can't point at anything I've worked on in the last 30+ years that's still in use. It's very demotivating. When you first get into tech it's all shiny and new and exciting. But nothing lasts. reply I_Am_Nous 15 hours agorootparentAt work, we were cleaning out a closet which has accumulated stuff for at least 15 years. My boss said \"Aww, a Cajun[1]!\" and it turns out it was the first switch we used back when he started at the company. Later in the day, I had to ask him what he wanted to do with it and it took some definitely effort for him to say \"I guess we can just put it in the recycle pile...\" as he had good memories of working with it and building stuff on top of it. Time marches on, and our great efforts are as nothing once they are replaced by the next big thing. 1. https://support.avaya.com/elmodocs2/cajun/docs/p333t24ug.pdf reply jimbokun 16 hours agorootparentprevIt's not a new problem: https://www.poetryfoundation.org/poems/46565/ozymandias reply UncleOxidant 16 hours agorootparentEcclesiastes 1 is also very relevant here: “Meaningless! Meaningless!” says the Teacher. “Utterly meaningless! Everything is meaningless.” What do people gain from all their labors at which they toil under the sun? Generations come and generations go, but the earth remains forever. The sun rises and the sun sets, and hurries back to where it rises. The wind blows to the south and turns to the north; round and round it goes, ever returning on its course. All streams flow into the sea, yet the sea is never full. To the place the streams come from, there they return again. All things are wearisome, more than one can say. The eye never has enough of seeing, nor the ear its fill of hearing. What has been will be again, what has been done will be done again; there is nothing new under the sun. Is there anything of which one can say, “Look! This is something new”? It was here already, long ago; it was here before our time. No one remembers the former generations, and even those yet to come will not be remembered by those who follow them. What a heavy burden God has laid on mankind! I have seen all the things that are done under the sun; all of them are meaningless, a chasing after the wind. reply I_Am_Nous 15 hours agorootparentEcclesiastes is one of the OG existentialist texts lol reply zoklet-enjoyer 17 hours agorootparentprevI don't understand how this is demotivating. Most work output for most jobs is temporary. How many people make permanent objects for a living? I never have and that's fine. reply glonq 17 hours agorootparentprevIf you had built COBOL systems for banks and gov't in the 1980's, your work would still be alive and kicking today ;) reply robenkleene 17 hours agorootparentprevI see this sentiment in tech all the time and I don't understand it. With DAWs and NLEs I have the equivalent of 100s of thousands of dollars of equipment from just 20 years ago. 3D DCC applications like Blender/Houdini allow new forms of creation without a clear physical equivalent. Software is magic and I plan to do creative things with it until the day I die. It's incredibly sad how badly most programmers especially feel about tech. If I'll editorialize for a moment, it seems to most programmers actually hate software. Something about programming just makes people hate software. reply graemep 17 hours agorootparentI think it is because a lot of programmers are fascinated by tech and had high expectations of what it would do - the internet in particular. It was expected to be empowering, democratizing, censorship resistant, decentralising. The reality is disillusioning. reply robenkleene 17 hours agorootparentI think it is empowering and democratizing, but agreed it's not censorship resistant and decentralized. I'd argue the latter two conflict with the first two. Making something decentralized makes it inherently harder to use (less empowering), making it censorship resistant runs counter to companies interests and companies fund everything (less democratized, i.e., harder to make a living). reply squigz 17 hours agorootparentprevI don't believe it's programming itself, but rather programming as a career. And honestly, can you blame any software engineer that gets jaded after a while? :P reply robenkleene 17 hours agorootparentFair, I suppose I strawmanned a bit on the comment I was replying to. Being jaded on programming as a career programmer certainly makes sense. I guess I was responding to the part about \"closing the laptop forever\", which I took to mean closing it off to all the other amazing things you can do with a computer today. But in context, they probably mean just stopping programming. But it still drives me crazy that god forbid a programmer would actually do something as low as open an Adobe product and actually make something that someone that's not another programmer could actually enjoy. Appreciate the creative good our industry has accomplished for godsake. reply tasuki 14 hours agorootparentprevSide projects don't have to suffer the same fate. You can maintain them as long as you like. reply proamdev123 17 hours agorootparentprevSo you’re planning on getting an Apple Vision Pro?! :-) reply Taylor_OD 17 hours agorootparentprevWhy? Most things are ephemeral. reply 93po 17 hours agorootparentIt’s probably because it’s representative of how the work was never productive or beneficial to begin with. It was some dumb idea resulting from a poor decision maker trying to compete in a dumb capitalist system. reply smugglerFlynn 17 hours agorootparentprevSo what interesting and exciting things are you up to nowadays? reply coldtea 17 hours agoparentprevThis, for comedic generality, is presenting the three cases as if all change is neutral, and it's just our approach to it that is problematic. We could also present them as if the person is wrong in all cases: (1) We tend to accept whatever is already there when we grow up, even though it might be the worst crap and detrimental to ourselves or even society. (2) We tend to adopt whether \"new and revolutionary\" appears when we're younger and starting our careers, without often questioning whether it's actually because of marketing hype and whatever it is a regression over what existed. (3) We tend to dislike new technology when we're older, even though it might be great and improve things. reply TeMPOraL 17 hours agorootparentGood point on (2). Somewhen past turning 30 I realized just how many things I bought into when I was 20-ish turned out to be purely marketing bullshit. I was happier then, though. And definitely less cynical. Ignorance is bliss, I guess, though my heart doesn't accept that, so I'm doomed to be forever whiny. reply DanHulton 17 hours agoparentprevI kinda hate this quote because, while funny, I find it is misused and abused more than it has ever been used correctly. The quote is about our _reactions_ to technologies, but it is constantly used to dismiss _actual concerns_ about technologies. It doesn't engage with the content of the discussion, but instead dresses up \"lol, OP is old\" in the clothing of a brilliant writer. Then, if you're lucky enough to have the conversation continue at all, the discussion tends to become about ageism instead of the original concerns. reply overvale 17 hours agorootparentYeah but isn't the quote's point (made comedically, with hyperbole) that it can be hard to separate our emotional reactions from things to actually be concerned about? I thought that's the whole idea, to make you laugh and think about which side of the line the new thing falls on. reply Uehreka 17 hours agorootparentprevYeah, it’s a highly effective thought-terminating cliche. reply mikestew 17 hours agoparentprevA classic quote, but too bad it doesn’t address anything the author wrote, and is simply used dismissively. reply dsjoerg 17 hours agorootparentIt comes close. The author doesn't want to learn the math and study the thing. Why? By implication, because they're over 35 and don't want to learn new stuff anymore. reply renonce 16 hours agorootparentIt's possible for me to learn enough math to download a huggingface model and start from tokenizing my prompt and convert them to token embeddings and add position embeddings and go through 32 layers of softmax+mlp with layernorm and write out the equation that would compute each intermediate floating number until it tells me the probabilities of each output token so I can sample a token and continue the sentence autoregressively. Computing any one of these 100 billion 16-bit floating point multiplications? I can either compute them in decimal or check out the IEEE754 fp16 format and compute in binary manually, or maybe draw a circuit with AND and NOT gates if given enough time. These are the low level operations. From a higher level mathematical standpoint? I can prove to you analytically how a SGD optimizer on a convex surface will converge to the global minimum at an exponential rate, starting from either set theory or dependent type theory and the construction of real numbers from sets of rational numbers. None of these tell me how and why LLM works. reply vcg3rd 17 hours agorootparentprevThe author posed it as a question. It's not about want as much as able. No matter how much I wanted (at 58) to understand the inner working of a LLM, it's beyond me, just like becoming a fighter pilot. However, even though I don't program, nothing in his list prior to AI is beyond me yet, if I wanted to learn it. I am happy being a 25 years Linux power user who climbed the Emacs learning curve to use org-mode and then gradually added email,rss, irc, web, and gopher modalities to it. reply khokhol 15 hours agorootparentprevWhy? By implication, because they're over 35 and don't want to learn new stuff anymore. It seems you are both missing the point of article, and jumping on a stereotype. Said point being: while the author clearly does like learning new stuff, and rolling up his sleeves to deal with all the fiddly bits -- AI is categorically different. In that the complexity is simply off the page compared to most (choosing my words carefully) technologies one is used to geeking out on. And that even experts in the field admit they don't really understand it. (That, and the sheer resources required to do something interesting, the perpetual lock-in with the sociopathic entities that provide said resources, etc). reply herpdyderp 17 hours agorootparentprevIt's absolutely addressing it. Generations before felt the same way about PCs as this author feels about AI. It's the same cycle. reply AlbertCory 17 hours agorootparentGenerations before felt the same way about a lot of things that really were garbage, and are now forgotten. You don't hear about those, because of Survivor Bias. Maybe just evaluate things for what they are. reply tw04 17 hours agoparentprevSocial Media was invented long before I was 35. I could get a job in it but I don't want to. It isn't exciting or revolutionary and is destroying the fabric of society. So there's that. I'd imagine after seeing what Social media has done to us, a lot of people are fearful AI will take us far more down that path than the path to salvation the people trumpeting AI claim it will. I honestly see a lot of parallels between what early social media supposedly would bring to the world, and where we ended up. reply ctoth 17 hours agorootparentThe problem with social media isn't the media. It's the social. It isn't the technology that broke. It's the people. Not all technologies go to shit. Only those we give to the eternal Septemberists reply chongli 16 hours agorootparentIt isn't the technology that broke. It's the people. I respectfully disagree. People haven’t changed in ten thousand years. It’s technology and culture that have changed. Many changes in culture have been amazing, wonderful breakthroughs (such as human rights). Similarly for technology (green revolution, modern medicine, electricity). Social media though? That’s a technology that has found its niche exploiting human psychology for profit. It’s on the same dark branch as advertising and “big lie” propaganda/fascism. We were much better off before it! reply tw04 16 hours agorootparentprevRespectfully: that's an absolute copout. Facebook determines what's on your feed, not you. Twitter drove news agencies to try to fit stories into 140 characters because it got clicks and eyeballs. Youtube's engagement algorithms drive impressionable youth down rabbit holes of alt-right nonsense. EVERYTHING about social media is designed to keep you addicted to it. The fact that human nature can be exploited isn't the human's fault, it's the corporation doing the exploiting... I've seen absolutely nothing coming from the AI sector other than handwavy \"we need to be responsible\" excuses. Meanwhile we've got deepfakes of Taylor Swift and Joe Biden that Grandma Marge can't tell from the real thing and absolutely nothing can or will be done about it. reply shombaboor 17 hours agoparentprevAI for coding / syntax / learning tool, drug discovery etc. makes perfect sense to me as an old person. AI for art is a soulless parlor trick stealing from past creative innovators. reply Nevermark 16 hours agoparentprevThis time it is different. At some point every pattern breaks, or at least changes. The black box aspect is a direct result of AI being a learning technology. A higher order technology. It learns to do things we have not explicitly taught it, or might not even know how to do ourselves. We will eventually find better ways to analyze and interpret how models work. Why a model produced a specific answer to a given prompt. But the models will keep getting more powerful too. Today they help coders over the speed bumps of unfamiliar language syntax, or esoteric library conventions. In a few years they will be actively helping researchers with basic problems. I.e. the hurdles for getting to the front of AI as a field as a technology contributor, and the resources needed, are going to get steeper. Of course there will be many people who do, but it won't be in the same way that most technically savvy people have had many years to learn programming languages, apply them, and even contribute to them, without language's progressing out from under them. (Except for C++ of course! /h) EDIT: It is worth distinguishing between AI like GPT, as in very flexible and powerful models that will be used across disciplines by all kinds of people regardless of technical chops, vs. the overlapping AI algorithms (partly subset, partly different) applied to smaller learning tasks, which has been a commonplace tool for many years and will continue to have its place. reply zvmaz 17 hours agoparentpr",
    "originSummary": [
      "An old technology enthusiast compares advancements in AI to past innovations like PCs, the internet, and smartphones, feeling left out and frustrated by AI's opacity and lack of openness.",
      "He finds AI technology challenging to understand and utilize due to its inaccessibility, unlike previous technologies."
    ],
    "commentSummary": [
      "The discussion explores various AI technology topics such as accessibility, transparency, economic impact, procedural generation in games, and challenges in mastering AI.",
      "Debates arise on the value of AI-generated art versus traditional art, job security, ethical considerations, and the excitement of using AI tools.",
      "Ethical concerns, the cycle of hype and disappointment, and responsible use in the AI sector are highlighted as crucial aspects of working with technology."
    ],
    "points": 194,
    "commentCount": 283,
    "retryCount": 0,
    "time": 1708098163
  }
]
