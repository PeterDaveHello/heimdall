[
  {
    "id": 39988993,
    "title": "Visualizing Money Flow: Double-Entry Bookkeeping Explained",
    "originLink": "https://matheusportela.com/double-entry-bookkeeping-as-a-directed-graph",
    "originBody": "About Blog Portfolio Double-Entry Bookkeeping as a Directed Graph 08 Apr 2024 My Journey into Accounting In the past couple of years, I’ve been working in billing and payments at Justworks. This experience introduced me to the world of bookkeeping and accounting in ways I didn’t expect: I took a course and read a textbook in accounting, adopted plain text accounting for my personal finances, and worked in a double-entry ledger system. Being such a fundamental part of the human experience, accounting has been around for literally millenia. It fostered mathematical and language development. In some cultures, it even predates written language. For this reason, it’s not surprising that accounting uses a very particular vocabulary and set of concepts that can be intimidating to a newcomer. Credits and debits. Assets and liabilities. Balance sheet. Ledgers and journals. It took me a while to get used to these things. But once I did, I realized they aren’t that difficult. Maybe it is just a matter of finding the right way to explain them. This series of articles is my attempt to capture some of my “a-ha” moments while learning accounting. Hopefully, it will help someone out there grasp these ideas in a more intuitive and modern way. In this first article, we’ll start with the basics: bookkeeping. Disclaimer: in this article, I’ll simplify some concepts and use slightly different terminology than traditional accounting for didactic purposes. If you’re an accountant, please bear with me. Bookkeeping Without Ledgers At its core, accounting is about keeping track of countable things over time. Some of the oldest documents archelogists have found register quantities in old societies: food, animals, people, money. Nowadays, accounting is usually interested in tracking money whereas other areas (such as inventory management or the census bureau) count the rest. For the rest of this article, we’ll focus on money. Let’s start with a very simple example. Imagine you’re in a small town with two people: Alice and Bob. Alice has $100 in her pocket and Bob has $50 as of January 1st, 2024. We’ll record this data in a table as follows:PersonAmount| ------------| Alice$100| Bob$50If nothing changes, our job is done. We just need to go around town and ask each person how much money they have, tabulate the data, and store it in a safe place. Badabing, badaboom! But money is meant to be spent. Alice is studying for her finals and needs to buy a book. She notices Bob has one copy and proposes to buy it from him for $20. Bob is happy to sell her the book he hasn’t used in a while and cash in the extra $20. Great deal! Now that money has changed hands, we need to update our records. Alice spent $20 and Bob received the same amount. Let’s update our table.PersonAmount| ------------| Alice$80| Bob$70Before proceeding, let’s name a few things. In our records, Alice and Bob are accounts, i.e., places where money is stored. The amount of money in each account is called a balance. Saying that Alice has $80 is the same as saying that the Alice account’s balance is $80 as of February 1st, 2024. Definition 1: Account A place where money is stored. Definition 2: Balance The amount of money in an account at a given point in time. Every month, we can go around town, ask people how much money they have and write it down. We’re basically creating a snapshot of the town’s financial situation monthly. (Surely, most people will be annoyed by this breach of privacy and I would probably be arrested for stalking but let’s ignore this for now.) What kinds of questions can we answer with this data? Actually, many! How much money does each person have? How much money has each person spent or gained over time? Who has the most money? Who has the least money? How much money is there in total in the town? Our records are simple but useful. Now, consider Alice comes to us one day and asks “How come I have $80? I thought I had more!” Unfortunately, all we can answer is “That’s what I have written down” with our current data. We can’t tell, for instance, whether Alice initially had $0 and received $80 or if she had $10,000 and spent $9,920. Why is this so? Well, we’re only keeping track of the current balance of each account. Because we erase the old balance and replace it with a new one, we don’t know exactly what happened to that balance over time. We lose the changes that happened between snapshots are lost. Could we do better? Of course! Accountants have dealt with this problem for centuries and they have a solution: ledgers. Bookkeeping with Single-Entry Ledgers Let’s now think how we could keep track of the historical changes in a systematic way. One way to do this is to write down each update as it happens, not only the balance in a certain date. To do this, we’ll change the table a little bit. For example, we write down that on January 1st, 2024, Alice had $100.AccountDescriptionDateAmountBalance| ---------------------------------------------| AliceOpening balance2024-01-01$100$100We added some columns to the table: Description: A human-readable explanation of the transaction (e.g. what it is about, who got paid, a reference number, etc). Date: When the transaction happened. Besides ordering transactions, this field can be used to group transactions by period (e.g. monthly reports). It can be enhanced with time information (e.g. hour, minute, second) if needed. Balance: The balance of the account after the transaction. This field is redundant but it’s useful when inspecting the data. So far, so good. Now, let’s take a look at how we can write down that Alice bought the book from Bob.AccountDescriptionDateAmountBalance| ---------------------------------------------| AliceOpening balance2024-01-01$100$100| AliceBought book2024-02-01-$20$80This is different… Instead of just updating the existing balance, we’re adding a new row with the transaction details. The new columns we added earlier are helpful in understanding what happened to the account. We know that Alice had $100, then spent $20 when buying the book, and now has $80. Time for more definitions: each row in this table is called an entry and the whole table is called a ledger. Definition 3: Entry A record of a transaction that happened to an account. Definition 4: Ledger A collection of entries for an account. So far, we’ve updated only Alice’s ledger. Let’s take a look at Bob’s:AccountDescriptionDateAmountBalance| ---------------------------------------------| BobOpening balance2024-01-01$50$50| BobSold book2024-02-01$20$70Now we have a ledger for each account. Great! This system we implemented right now is called a single-entry bookkeeping system. Each account has its own ledger and we write down entries that affect one account at a time. This is a simple system that works well for small businesses or personal finances. Ledgers are sometimes called journal or book because, in the past, they were physical books where accountants would write down transactions. In the modern world, though, they’re usually stored in a database. A 1926 handwritten ledger. Notice the accountant calculated the cash and bank balances daily. Source: Business Case Analysis. An important characteristic of a ledger is that the data is immutable. Once an entry is written, it cannot be changed at all because we want to preserve the ledger’s full history. No more erasing or scribbling over entries. This raises the question: what happens if we make a mistake? Here’s an example. Say the correct price for the book is $30 but we wrote down $20. If we were in a mutable system, we could just update the amount in the original entry as follows:AccountDescriptionDateAmountBalance| ---------------------------------------------| AliceOpening balance2024-01-01$100$100| AliceBought book2024-02-01-$30$70There are many problems with this approach though. We lose the history of the original amount. We can’t tell that we made a mistake and corrected it. We also can’t tell if the original amount was correct and we made a mistake in the correction. Could we do better? Yes! Instead of updating the original entry, we can write a new entry to cancel the old one and write a new one with the correct amount. Let’s see how this looks:AccountDescriptionDateAmountBalance| ---------------------------------------------| AliceOpening balance2024-01-01$100$100| AliceBought book2024-02-01-$20$80| AliceCorrect book price2024-02-01$20$100| AliceBought book2024-02-01-$30$70The end result is the same as updating an entry in-place: the balance is $70. However, we can now see that we made a mistake and corrected it. We can also see the original amount and the reason for the correction. This gives us an audit trail of changes. In a way, a ledger is similar to event sourcing in Computer Science. In event sourcing, we store events that happened in the system and use them to compute the current state of the system. This is in contrast to storing just the state of the system and updating it in-place. Event sourcing has many benefits, like being able to replay events or to rebuild the state of the system at any point in time. If our system only cares about a single account at a time, a single-entry ledger system is enough. Think of a personal finance app that you use to keep track of your money or a gym that tracks how much their members paid for the service. However, financial systems are usually more complex than that and transactions typically involve multiple accounts at once. For these complex scenarios, accountants developed a more robust system: double-entry bookkeeping. Bookkeeping with Double-Entry Ledgers Let’s revisit the example of Alice buying the book from Bob and see the ledgers for both accounts:AccountDescriptionDateAmountBalance| ------------------------------------------------| AliceOpening balance2024-01-01$100$100| AliceBought book2024-02-01-$20$80| AccountDescriptionDateAmountBalance| ------------------------------------------------| BobOpening balance2024-01-01$50$50| BobSold book2024-02-01$20$70Did you notice how the transactions are related to each other? The $20 Alice spent is the same $20 Bob received. We know that but the system doesn’t since we’re not explicitly stating this relationship in our ledgers. It could be the case the Alice bought the book from Bob and Bob received the money from Charlie. We can’t tell the difference when reading the ledgers. A first step in making this relationship explicit is to group related entries into a transaction. Let’s add the Transaction column to our table:AccountTransactionDescriptionDateAmountBalance| ----------------------------------------------------------| Alice1Opening balance2024-01-01$100$100| Alice3Book sale2024-02-01-$20$80| AccountTransactionDescriptionDateAmountBalance| ----------------------------------------------------------| Bob2Opening balance2024-01-01$50$50| Bob3Book sale2024-02-01$20$70In this example, we have the following transactions: Transaction 1: Alice’s opening balance. Transaction 2: Bob’s opening balance. Transaction 3: Alice bought a book from Bob. Now, we explicitly state that the $20 Alice spent is the same $20 Bob received. No more ambiguity. Definition 5: Transaction A group of related entries that affect different accounts. The difference between our current system and the previous one is that we’re now grouping related entries into transactions. Since each transaction affects multiple accounts, we can now see how money flows between accounts. By adding transactions to the tables, we are now working with a double-entry bookkeeping system. Traditionally, accountants would use two columns to represent the flow of money between accounts: credits and debits. When money leaves an account, the amounts goes in the credit column. Incoming money appears in the debit column. Definition 6: Credit An entry that represents money leaving an account. Definition 7: Debit An entry that represents money entering an account. (Pro-tip: Ignore what you know about credit and debit cards for now.) When Alice pays Bob $20, we say Alice’s account is credited $20 and Bob’s account is debited $20. Let’s represent this in our table:AccountTransactionDescriptionDateDebitCredit| --------------------------------------------------------| Alice1Opening balance2024-01-01$100|Alice3Bought book2024-02-01| $20| AccountTransactionDescriptionDateDebitCredit| --------------------------------------------------------| Bob2Opening balance2024-01-01$50|Bob3Sold book2024-02-01$20| I wanted to make a few of points before continuing our journey. First, the goal of double-entry bookkeeping is to reduce mistakes, especially the ones that are hard to notice. Remember that, for centuries, accountants were using pen and paper to keep track of insane amounts of money. Small mistakes could lead to big problems. Writing debits and credits in different columns, as we did above, reduces the chance of making mistakes. Your hand is literally moved to different places on the paper when writing debits and credits. Real world accounting journals wouldn’t even put these columns side-by-side. Instead, they would be in different parts of the page. In fact, accounting journals were symmetrical: the left side was for debits and the right side, for credits. Something like this: Debit Alice's account Credit ------------------------------------------------------------------------------- -------------------------------------------------------------------------------DateDescription$| DateDescription$| -----------------------------| -----------------------------| 2024-01-01Opening balance$100||||| 2024-02-01Bought book$20This format is commonly referred to as a T-account (an analogy to the same: the top with the account name, the left side with debits, and the right side with credits). For this reason, many accountants will refer to debits as entries that go on the left side and credits as the ones that go on the right side. General Post Office T-account as instituted by Benjamin Franklin. Source: Smithsonian National Post Museum. Computer systems don’t need to use this format, though, as it doesn’t necessary prevent software bugs. Instead, it is common to have a single amount column and another column to indicate whether the amount is a debit or a credit. For example:AccountTransactionDescriptionDateTypeAmount| ---------------------------------------------------------| Alice1Opening balance2024-01-01Debit$100| Alice3Bought book2024-02-01Credit$20Second, notice how we’re using a positive number even when money is leaving Alice’s account. Historically, accountants don’t like negative numbers in the books as they only became popular much later in Europe. Computer systems are very good with negative numbers, though. Instead of having two columns, we could just use a single column and adopt a convention in which credits are negative and debits are positive, or vice-versa. For example:AccountTransactionDescriptionDateAmount| ---------------------------------------------------| Alice1Opening balance2024-01-01$100| Alice3Bought book2024-02-01-$20If you pay close attention to this last example, this is exactly what we were doing before. The difference is that now we understand that the amount is negative because it’s a credit. (Technically, using negative numbers for credits might be a limitation if we’re trying to undo a credit entry without creating a debit entry but we don’t need to be that picky for now.) Finally, I’m not a big fan of old nomenclature for tradition’s sake. We could rename credits as outgoing money and debits as incoming money without losing precision. In my opinion, this is a bit less confusing.AccountTransactionDescriptionDateIncomingOutgoing| -------------------------------------------------------------| Alice1Opening balance2024-01-01$100|Alice3Bought book2024-02-01| $20| AccountTransactionDescriptionDateIncomingOutgoing| -------------------------------------------------------------| Bob2Opening balance2024-01-01$50|Bob3Sold book2024-02-01$20| Sweet! A fundamental principle of double-entry bookkeeping is that the total amount of money in the system remains the same after each transaction. A particular account can increase or decrease its balance over time but the sum of all balances must remain constant. Nothing is lost, nothing is created, everything is transacted. It is a closed system. Definition 6: Double-Entry Ledger A system of accounting where each transaction is recorded one or more entries. The amount of money leaving accounts is equal to the amount of money entering other accounts in every transaction. Ok, you might be thinking: “Wait a minute, these opening balances go against what you just said!” and you’re right. Transactions 1 and 2 change the total amount of money in the system from $0 to $150. We should do better than stating a rule only to break it in the very next sentence. Let’s assume all of the money that Alice and Bob have comes from a bank. Then, let’s create an account for this bank and use this account as the other side of the opening balance transactions.AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Bank1Alice's opening balance2024-01-01| $100| Bank2Bob's opening balance2024-01-01| $50| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Alice1Opening balance2024-01-01$100|Alice3Bought book2024-02-01| $20| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Bob2Opening balance2024-01-01$50|Bob3Sold book2024-02-01$20| In this example, we don’t care where the opening balances come from exactly. We just need to make sure money is coming from somewhere. The bank account is a kind of phony account that is there just to help us follow the rules. In accounting terms, it is called a contra account to the other accounts. The important thing is that all transactions are balanced, i.e., “credits equal debits”. Definition 7: Contra Account An account that is used to offset another account. It is used to keep the accounting equation in balance. Our system is now complete. We have a double-entry ledger system that keeps track of money flowing between accounts. We can answer many questions about the financial situation of Alice and Bob and we have a clear audit trail of all transactions that happened. Let’s consider a more complex example. When Alice buys the book, she accidentally uses the wrong credit card and needs to pay $2 in foreign exchange fees. Bob, on the other hand, pays $2 in sales taxes to the government, $1 in credit card fees. How do we keep track of this? The secret in double-entry bookkeeping is to use accounts for everything. As we did before, we model this flow by creating new accounts: one for the credit card company and another for the tax authority. Then, when creating the transaction, we add new entries as necessary.AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Bank1Alice's opening balance2024-01-01| $100| Bank2Bob's opening balance2024-01-01| $50| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Alice1Opening balance2024-01-01$100|Alice3Bought book2024-02-01| $20| Alice3Foreign exchange fee2024-02-01| $2| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Bob2Opening balance2024-01-01$50|Bob3Sold book2024-02-01$20|Bob3Sales tax2024-02-01| $2| Bob3Credit card fee2024-02-01| $1| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| CC3Alice's credit card fee2024-02-01$2|CC3Bob's credit card fee2024-02-01$1|AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Tax3Bob's sales tax2024-02-01$2| We modified transaction 3 as follows: Alice pays $20 to Bob and $2 to the credit card company. Bob receives $20 from Alice, pays $2 to the tax authority, and $1 to the credit card company. The credit card company receives $2 from Alice and $1 from Bob. The tax authority receives $2 from Bob. Notice that transaction 3 has more than two entries. It has eight entries, to be precise. This is perfectly fine! We can have as many entries as we need to represent the flow of money between accounts as long as the transaction is balanced, i.e., credits = debits. It is a common mistake to think that double-entry bookkeeping limits transactions to two entries at a time. The technique is called “double-entry” not because there are only two entries but because each transaction has two sides: one side where money leaves an account and another side where money enters another account. I guess “many-entry bookkeeping” doesn’t sound as good. Double-Entry Bookkeeping is a Directed Graph I hope these concepts are clear so far. We have accounts, entries, transactions, incoming money, and outgoing money. With practice, you’ll be able to read and write these tables with ease. But I have a confession to make: I’m a visual person. I like to draw and see things when I’m learning. So I started to think: how can we visualize this data in a way that makes sense to me? After using double-entry bookkeeping for a while in my personal finances and trying to come with a visualization for my ledgers, it finally clicked: we’re modeling money flow as a directed graph. Think about this: An account is a node in the graph, a credit entry is an outgoing edge with an amount of money leaving this node whereas a debit is an incoming edge with money flowing to another node. A transaction, then, groups and enforces a condition on a set of edges: the outgoing edges must have the same sum of money as the incoming edges. Let’s take a look at the example we’ve been using so far. First, we start with Alice’s and Bob’s accounts and the money they stored in the bank: A few comments on this graph: An account is a round node with the account name. A transaction is a square node with the transaction number. A credit entry goes from an account to a transaction. A debit entry goes from a transaction to an account. The entry’s amount of money is written on the edge. An account’s balance is the sum of the amounts of the incoming edges minus the sum of the amounts of the outgoing edges. We can see that transaction 1 moves $100 from the bank to Alice. Transaction 2 moves $50 from the bank to Bob. The total amount of money in the system is $150. Then, Alice buys a book from Bob: We can see that transaction 3 moves $20 from Alice to Bob. Hence, Alice has $80 and Bob, $70. We haven’t added fees and taxes yet. Let’s do this: Oof, that’s a lot of edges! Transaction 3 is very complex as it conflates what Alice is paying Bob with the fees and taxes they pay to other parties. We could make this easier by splitting this transaction into two smaller ones:AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Bank1Alice's opening balance2024-01-01| $100| Bank2Bob's opening balance2024-01-01| $50| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Alice1Opening balance2024-01-01$100|Alice3Bought book2024-02-01| $22| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Bob2Opening balance2024-01-01$50|Bob3Sold book2024-02-01$19|Bob4Sales tax2024-02-01| $2| AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| CC3Transaction fee2024-02-01$3|AccountTransactionDescriptionDateIncomingOutgoing| ---------------------------------------------------------------------| Tax4Bob's sales tax2024-02-01$2| And as a graph: We simplified the transactions a little bit: Alice sees $22 leaving her account but Bob only receives $19. The remaining $3 goes to the credit card company. Bob pays sales taxes in a different transaction. Regardless of how we model the transactions, the account balances are the same. Alice has $78, Bob has $69, the tax authority has $2, and the credit card company has $3. It is the accountant’s job to decide how to group transactions and entries in a way that makes sense for the business as the bookkeeping system is flexible enough to accommodate different needs. These simple examples show how we can visualize money flow in a double-entry bookkeeping system as a directed graph. The graph grows over time as new transactions are added but it’s properties remain the same. In my opinion, understanding bookkeeping as a graph is a powerful way to reason about many accounting concepts. Suddenly, things as balance sheets, income statements, and cash flow statements are just visualizations of this graph. Categories such as assets, liabilities, equity, income, and expenses are just groups of nodes in the graph and it is quite easy to understand whether credits or debits increase their balances. It’s a way to make accounting more intuitive and less intimidating to me! We could go on and on with this example, adding more complexity to the transactions, creating new accounts, and visualizing the graph as it grows. But I think we did a great job today and should take a well-deserved break. Takeaways In this article, we’ve covered the basics of bookkeeping. We started with a simple system that only kept track of balances, evolved it into a single-entry and, later, into a double-entry ledger system that models money flow between accounts. We’ve seen how to represent transactions as entries in a ledger and how to group related entries into transactions. Finally, we’ve seen how to visualize a double-entry ledger as a directed graph. In the next article, we’ll dive deeper into basic accounting concepts and see how they relate to the graph representation we’ve seen here. Resources When studying these concepts, I found the following resources particularly helpful: Accounting & Financial Statement Analysis: Complete Training: A crash course in accounting in Udemy. It was my introduction to the topic and I highly recommend it. Mastering Accounting by George Bright and Michael Herbert: A textbook that covers accounting from the basics to more advanced topics. Another great resource to deepen my understanding of accounting. Beancount’s documentation has a great explanation of accounting as a graph over time. It was a big inspiration for this article. Modern Treasury has a great series of articles that explain accounting concepts for developers. It is another big inspiration. This particular thread on Hacker News. Some comments are gold! Plain text accounting is a small but engaged community that uses plain text files and command-line tools to keep track of their finances. I use hledger for my personal finances.",
    "commentLink": "https://news.ycombinator.com/item?id=39988993",
    "commentBody": "Double-entry bookkeeping as a directed graph (matheusportela.com)498 points by mportela 23 hours agohidepastfavorite331 comments dkyc 21 hours agoI find it a strange choice to explain double-entry bookkeeping with the example of \"one entry for Alice, one entry for Bob\". That's really not what it's about. It's obvious that a transaction with two parties could be recorded in two places, but to me the crucial point of double-entry bookkeeping is that it requires two entries for each party of the transaction. So if Alice buys book from Bob, four entries are made. I get that this is supposed to be a simplification for educational purposes, but I find this is simplification is an oversimplification, since it omits the key point. reply btown 19 hours agoparentIn all fairness, if you're trying to understand a piece of software like Quickbooks and are not coming from an accounting background, anthropomorphizing each \"account\" at your company as an individual actor with their own ledger can actually be a helpful mental model. Everything needs to be a dance between actors, and, for instance, when you make a vendor payment in cash, you can only do so as a message sent simultaneously to the Accounts Payable actor and the Cash actor, and each actor must accumulate the effects of that message/event in the way that makes sense. (Namely, each one will translate the event into credits/debits based on the characteristics of who they are, and maintain a balance accordingly. Double-entry, I suppose, means each event must be ingested exactly once by an even number of actors.) If you're building payment rails, that event might itself be one of a pair of events, sourced from a meta-event tracking the transaction intent. (As a meta-point, I find it much more useful to think of the \"graph\" in accounting as having edges not made of money, but of data in a derived-event hierarchy.) And a first step towards being able to have that mental model is ensuring that you have a good mental model of multiple physical-human actors accumulating events in a structured and atomic way. But the OP doesn't actually make it clear that this is what the analogy is in service of! And I fear that the OP article will cause more confusion than it solves. reply fauigerzigerk 18 hours agorootparent>Double-entry, I suppose, means each event must be ingested exactly once by an even number of actors.) No, the number of accounts (actors) does not have to be even. The sum of debits and credits has to be equal (or zero if you like). reply btown 18 hours agorootparentYou're right - it was a silly thing for me to write! Something more accurate would be that because debits and credits must balance, there is no way to send a message that would only be seen by a single account (other than a no-op); thus, any meaningful transaction will have an impact on at least two accounts. reply fauigerzigerk 17 hours agorootparentAgreed. reply em-bee 19 hours agorootparentprevanthropomorphizing the accounts is not the problem. the problem is that in the example the two parts of the double-entry are the two partners of the transaction. to anthropomorphize properly alice and bob would be two employees of the company buying a book from a bookstore. reply Mister_Snuggles 19 hours agorootparentprev> if you're trying to understand a piece of software like Quickbooks and are not coming from an accounting background Unfortunately, QuickBooks won't help you understand accounting. It's not a true double-entry accounting system, at least it wasn't the last time I touched it. That said, it still does its job and does it well enough, and real accountants are fine with dealing with it. Simply Accounting is a better example of a true double-entry system. reply PopAlongKid 18 hours agorootparent> It's not a true double-entry accounting system, at least it wasn't the last time I touched it. Can you elaborate? I've used Quickbooks for over 15 years and it has always been a true double entry accounting system during that time. reply chimeracoder 17 hours agorootparentprev> Unfortunately, QuickBooks won't help you understand accounting. It's not a true double-entry accounting system, at least it wasn't the last time I touched it. QuickBooks absolutely is a double-entry accounting system. The \"bookkeeper\" mode abstracts away and hides what's going on under the hood, but if you enter \"accountant\" mode, you'll see the full ledger, and you can even make direct journal entries to modify it. reply Mister_Snuggles 16 hours agorootparentI must have only ever encountered it in \"bookkeeper mode\". That abstraction is likely what threw me off! reply chimeracoder 16 hours agorootparent> I must have only ever encountered it in \"bookkeeper mode\". That abstraction is likely what threw me off! I personally find the bookkeeper mode very confusing, and having observed others (non-accountants) using it to manage small businesses, I think that folks would be better off taking a one-day course in accounting and learning just enough to use it in accountant mode. You don't have to be a CPA, just literally enough about A = L + E to follow the flow within Quickbooks and record one side of each entry. reply em-bee 21 hours agoparentprevi was about to write the same thing. knowing that double-entry is meant to apply to myself only, i actually found the example confusing, because well, of course bob is going to have an entry in his accounting book, but i don't care about bobs accounts, i don't want to track that. i only care about mine. i buy a book. how do i record this transaction using double entry bookkeeping in my accounting book? and bob is not even doing any bookkeeping. he is bookselling ;-) reply toong 19 hours agorootparentYou gained $20 worth of assets, so the counterpart of the $20 leaving your bank account is countered by your assets-account gaining $20 Now each year your book loses 1/5th of its value, due to wear and tear (4$ disappearing from your assets-account), this is countered by your depreciation-account (4$ tax write off, every year!) After 5 years, it is worth $0 according to your books, but you manage to sell it again for $10: your bank account gets debited for $10, while your capital-gains-account gets credited for $10 reply probably_wrong 18 hours agorootparentAnd how about food? I can understand a book having a resale value I keep in my books, but once I've eaten the hot-dog I bought it is gone forever. reply chimeracoder 17 hours agorootparent> And how about food? I can understand a book having a resale value I keep in my books, but once I've eaten the hot-dog I bought it is gone forever. Perishable and consumable food wouldn't be counted as an asset in the first place. You spend the money - it's credited to your asset account (reducing the value of your cash-in-hand) and then debited from your expense account (reducing the value of your equity - or, in more layperson's lingo, increasing the total sum of the expenses you incurred during that period). reply eviks 17 hours agorootparentOf course it would be, asset is anything of value, you're confusing with subtypes of assets. Just mujhe liability is anything you owe regardless of for how long reply chimeracoder 16 hours agorootparent> Of course it would be, asset is anything of value, you're confusing with subtypes of assets. Just mujhe liability is anything you owe regardless of for how long If an office buys snacks on Monday for the office party on Friday, they're not counting it as an asset and depreciating it on their books. If food production or delivery were part of the core business, it would be one thing, but in the context that OP's talking about, it would be overkill at best (and fraudulent, in extreme cases) to try and count a transient consumable as an asset on their books. reply eviks 15 hours agorootparentDepreciation isn't relevant here, again, you're confused in the types of assets, not all of them are depreciated, only some with some specific properties like time of expected user. Just read the definition of assets in any (accounting) dictionary, or try to record your snack purchase in real accounts and see which side of the balance sheet this account end up in (hint: inventories, assets). reply renewiltord 14 hours agorootparentDo you actually do that? When people are working late at the office and you order pizzas you put that into your inventory and then remove it as people consume the pizzas? I record that into a separate operating expenses account meant for this kind of fringe benefit, not into inventory. Pretty small so I do the accounting as well, but I think I'd lose my mind if I had to record them into inventory. Then when they leave half the pizzas for the next day, I record that? No way. reply eviks 13 hours agorootparentyou don't need to record the halves, nothing stops your pizza order to be automatically recordered as -A_cash +A_inventory -A_inventory + L_expenses Sure, if your pizza is frozen and consumed in another period, your books will not reflect reality, but so what, when talking about the very basics of accounting you offset that misrepresentation of a simple example by gaining an important pedagogic benefit! Which one, though? What do you gain by denying that pizza is an asset, going so far as calling recognition of an asset as an asset a fraud (but only in extreme cases of 5 pizzas!) and bringing depreciation/core business in? reply renewiltord 13 hours agorootparentPerhaps this is obvious to you, but I don't see what I'm gaining by doing this. My inventory management system will have different things unless I'm also recording these pizzas in there for the day. And it will show my inventory valuation as fluctuating when I do things like lunch or dinner for the team. It really seems useless to me when running the business. reply eviks 12 hours agorootparentThat's fine, many accounting practices eschew precision for simplicity, you don't mark-to-market everything, depreciation is linear, etc, so if you don't see any value in this, but only troubles with integration with other systems etc, then it's useless to you. But then the article wasn't about running a real business reply chimeracoder 11 hours agorootparentprev> Do you actually do that? No, nobody does this. GP is engaging in an exercise in pedantry, under the guise that it serves some pedagogical purpose. Personally, I don't think it's particularly useful to teach people about how things could theoretically be done, when it's much easier to show then how things actually are, but I'm sure there is some accountant nerd out there who is extremely meticulously tracking the total value of the gumballs on the secretary's desk as they are consumed. reply samatman 13 hours agorootparentprevThe person you're replying to is confused, but that's because accounting can be confusing. An account is fundamentally either an asset or a liability. When you buy something with a credit card, you've incurred a liability, and gained an asset, no matter what you've purchased. If you use a debit card or cash, you're trading one asset for another. One of the basic asset categories is expenses. That's the confusing part! When you acquire an asset, which is consumed or otherwise has no book value, that's an expense. So when you buy groceries with a debit card for a hundred bucks, that's a +100 in Expenses:Groceries, and a -100 in Assets:Checking. If you buy the same groceries with a credit card, it's +100 in Expenses:Groceries, and -100 in Liabilities:CreditCard. When you pay off the credit card, that's -100 Assets:Checking, and +100 in Liabilities:CreditCard. Asset is overloaded here, because Expenses are not included in calculating net assets. It's confusing! I find it even more confusing that Income is a liability, which always gets lower. That's because whoever paid you had a liability to do so, which they met out of assets. This is also why, when you pull a CSV of a checking account, purchases are positive numbers, and income is negative. A CSV of a credit card will have purchases as negative, and payments as positive. It's the difference between an asset account and a liability account. Again, not to be confused with net liabilities: Income is a liability, but not one you owe anyone, rather the contrary, Income just gets smaller and smaller (ideally! If it isn't getting smaller then your net assets will be shrinking, most of us can't afford that for long). The main thing is that an account which fluctuates from zero to positive, or accumulates, is an asset account. One which fluctuates from zero to negative, or accumulates negatively, is a liability account. There are times when this matters, notably when you can take a tax deduction for expenses, that's a good example of why they're on the asset side of the books. reply projektfu 9 hours agorootparentThese are the sorts of comments that make accounting and bookkeeping more difficult for people who are learning it. It helps no-one to try to think of income and expenses as equivalent to liabilites and credits. They are merely on the same sides of the accounting equation. Assets + Expenses = Liabilities + Equity + Income Expenses are not assets. For example, depreciation is not an asset. It is the representation of the life of the asset getting used up. It is an expense, a pure expense. Interest paid on a debt is not an asset. It is a pure expense. There are no word games that turn these into assets, like you might have for a software subscription or a gas bill. Expenses diminish the business. Unlike assets, they do not represent anything that can be liquidated. Income increases the business. Unlike a liability, it does not represent a claim against the business. Why aren't expenses and income on the balance sheet? Because they are netted out into retained earnings for the period. Imagine a business that cannot have a liability. Its accounting equation would simplify to: Assets = Equity. Income increases equity, expenses decrease it. Is equity a liability? NO. It is a separate account category with a credit balance. Want to look silly? Do as I did when I was a young programmer who knew everything and confuse the two. People not learning bookkeeping before writing accounting software (which is a lot more software than people expect) make many dumb errors that frustrate users, bookkeepers and accountants. A decent bookkeeping book (e.g. Bookkeeping for Dummies) goes a long way to familiarizing someone with how to handle double entry accounting. reply samatman 13 hours agorootparentprevThis comment fleshes out what I'm saying here: https://news.ycombinator.com/item?id=39992035 reply chimeracoder 15 hours agorootparentprev> or try to record your snack purchase in real accounts and see which side of the balance sheet this account end up in (hint: inventories, assets). Yeah, and as I said, this makes sense for a company for which food is a relevant part of their business, but in the context OP is asking about, nobody is tracking it this way. reply mulmen 12 hours agorootparentprevIt’s been 15 years since I took an accounting course. Why would my bank account be debited when the balance went up? Is a debit not negative? Is the cash balance presented as a negative? reply awirick 11 hours agorootparentYour bank account is an asset for you, so debits increase the balance while credits decrease it. This is also called a \"debit normal\" account. Liability accounts are tracked in reverse and are \"credit normal\". You increase the value (how much you owe) with a credit to the account and decrease the value (payments you receive) with a debit. reply galaxyLogic 9 hours agorootparentOne way to think about is you always \"credit\" the source of the money. If you get money from somebody you \"credit\" them for giving you the money. You say \"I must give you credit for having done this\". If money goes into your bank-account you don't credit your bank-account because money didn't come from there it went there. If you don't credit the bank account you must be doing something else and that is called \"debit\". When money goes to your bank-account you \"debit\" it because now the bank-account is more \"indebted\" to you. You don't have the cash in your wallet but the bank-account is indebted to you by that amount. From the view-point of the bank-manager things are of course reverse. When you put money into your bank-account the bank-manager \"credits\" you-the-account (in their books) for having done so. I guess a crucial thing to realize is that your bank-account in your books is a different thing from your bank-account in the books of the bank. It seems like there is only one bank-account, but two different parties (you and the bank) each have their own version of that \"account\" in their book-keeping system. A double-entry book-keeping system is \"subjective\" in that it always describes things only from the viewpoint of whoever it is who is doing the book-keeping. reply mulmen 9 hours agorootparentThanks this explanation helps. Does that mean that from the banks perspective my deposits are a liability? reply hhshhhhjjjd 7 hours agorootparentYes! Because you can remove your deposits from the bank and they also have to pay you interest on that balance. Your mortgage is an asset to the bank for the opposite reasons. reply lottin 11 hours agorootparentprevIn your books, your bank account is an asset, and therefore an increase in the balance is recorded with a debit. In the bank's books, it's the other way around. reply halfcat 9 hours agorootparentprev> Is a debit not negative? Indeed this is confusing to most people (myself included the first time I dealt with it), since if your phone company says they’re giving you a credit, you're getting money. reply nolongerthere 18 hours agorootparentprevThis is the best explanation, everyone else is giving wrong explanations that appear to be at least partially sourced from some AI. reply orthoxerox 11 hours agorootparentprevIn a nutshell, double-entry bookkeeping is tracking all your money in two ways: - where has it come from/has it forever gone? - where is it now? So, you start a simple ledger of having $100 in cash with a transaction like this: Dr \"cash\" Cr \"original funds\" $100 Then you spend some of it on food and loan some to Bob: Dr \"food expenses\" Cr \"cash\" $25 Dr \"loan to Bob\" Cr \"cash\" $20 Bob pays you back $22: Dr \"cash\" Cr \"loan to Bob\" $20 Dr \"cash\" Cr \"interest income\" $2 You can't write 'Cr \"Bob\" $22', because... I don't want to get into the principles of accounting, but basically all asset accounts only go one way. You can't have minus two dollars in your pocket, and Bob can't owe you minus two dollars either. Some of the accounts, like \"original funds\", aren't very useful by themselves, but they are the only way to make sure \"money I literally have in my account/pocket\", \"money I owe people\" and \"money that people owe me\" can all be counted together: if you tally up both kinds of the accounts, the total sum should be the same, just with the opposite \"sign\". reply Octokiddie 19 hours agoparentprevEvery explanation of double entry accounting seems to do the same thing. If I'm trying to understand the double part of double-entry bookkeeping, what exactly does the \"double\" refer to? What's being \"doubled\"? How would you salvage the article to actually explain the \"double\" part in detail? Could you do it purely from Bob's (or Alice's) perspective? reply theptip 18 hours agorootparentIt’s a checksum; by decomposing every transaction into a double of (credit A, debit B) that must sum to zero, you catch random arithmetic errors. You can think of it as “conservation of value”, so you can’t just create money out of thin air in your payment service (credit), without tying it to some account with a corresponding debit. This originally was intended to protect against typos; eg write a 10 instead of 100, at the end of the day your ledger needs to balance. In software typos are less likely bit it still provides auditability to prevent a large class of bugs from wiping you out. reply dragonwriter 18 hours agorootparent> This originally was intended to protect against typos; Double entry bookkeeping is much older than typing, but, yes, its a check against incorrect entries. reply jacques_chester 17 hours agorootparentSpeaking of history, I learned that the word \"control\" comes from contra rotulus -- roughly \"checking against the wheel\", which was apparently from an early medieval device for keeping tallies. The second meaning of \"domination\" came later. reply ectopasm83 17 hours agorootparentprevBabylonian dogs walking on your clay tablet. reply jimbokun 9 hours agorootparentCats, more likely. reply test6554 15 hours agorootparentprevBob and Alice each have a \"money\" account and a \"books\" account. Each money account tracks how much money they have on hand while each books account tracks the total value of their private libraries. So to be clear, there are 4 accounts. Bob's Money, Bob's Books, Alice's Money, Alice's Books. Because these two homeless librarians only have money and books, you can add the two balances together for each person to get their net worth. If Alice owns 3 books worth $120, then the \"Alice's Books\" account would show a balance of $120. Meanwhile, Bob has 12 books worth $700. When Alice buys the books, she -credits her bank account $20 and +debits her books account $20 (the value of the new book). Thus her net worth stays the same, but she has more books assets and fewer cash assets. Similarly Bob -credits his books account $20 and +debits his bank account $20. His net worth also stays the same but he now has more cash than before. On Alice's way back to the bridge she resides under, it starts to rain. Alice's new book is ruined. She -credit's her books account $20 and her net worth goes down by $20. Life as a homeless librarian is harsh. reply BayesianDice 13 hours agorootparentAnd when the book is ruined, she credits her books account (an asset account) $20 and debits her \"depreciation/impairment\" account (an expense account) $20. reply victor106 15 hours agorootparentprev> She -credit's her books account $20 and her net worth goes down by $20. Stupid question maybe. Is net worth an account too? Where does the debit side of Alice’s credit go? reply kaynelynn 15 hours agorootparentIn a real world example you would be correct. This would fall under the “equity” of the accounting equation assets = liabilities + equity. The equity part can be confusing but is where many of the non obvious second entries end up. reply bluepencil123 19 hours agorootparentprevThe 'double' in double entry book-keeping is related only to the book keepers own records/books. It has nothing to do with counter party's record keeping. If Alice purchases a house worth $100,000 in cash, then 2 (double) accounts will get effected. Her cash account will decrease (Credit) by $100,100 and simultaneously her House equity account (or any other appropriate name such as immovable asset etc) will increase by $100,000 (Debit). This can be recorded in a 3 column table as Credit account -- value -- Debit account Cash -- $100,000 -- House equity In the above transaction, two accounts were effected. Hence the name double entry. This gives a truer picture of ones assets and liabilities. Note: 1. Debit and credit dont have much to do with increase decrease. 2. A transaction can be modelled to have affect more than 2 account. For example if Alice were to make the purchase with $80,000 loan, then the book keeping could go like Credit Lender $80,000 Credit Cash $20,000 Debit House Equity $100,000 For the sake of better understanding, if one is uncomfortable with having one record affecting 3 accounts, one can be more robust and split the loan and the purchase into 2 transactions. After all, taking a loan and purchasing a house are 2 different events(transactions). Transaction one -> Credit Lender $80,000 Debit Cash $80,000 Transaction two -> Credit Cash $100,000 Debit House equity $100,000 edit 1: attempt at better formatting reply datavirtue 17 hours agorootparentDon't forget the depreciation, interest, maintenance, and tax accounts if you want to track those against the real estate cost basis for various purposes. You also need to figure out how to create and map accounts to IRS rules or you could put yourself in a real bind when it comes to figuring out tax liabilities or deductions. reply insane_dreamer 19 hours agorootparentprevBecause double-entry accounting requires two (thus \"double\") entries for each transaction (i.e., Alice buys a book) - one for the assets/liabilities account involved in sending or receiving the money ($30 credit, bank account) - one for the income/expense account to which the transaction corresponds ($30 debit, \"education\" expense account) one of the two entries is a credit and the other a debit reply bregma 18 hours agorootparentprevEvery time money is exchanged, it has to come from somewhere and it has to go somewhere -- that's two places it need to be recorded (or \"entered in the books\"). Money can not be created out of thin air, and it can not be destroyed. Every movement of money has to be accounted for, which is why it's called \"accounting\". Double-entry accounting means you have to account for where the money comes from, and you have to account for where it goes, and each of those is a separate entry and it all has to add up to zero. Where it can become confusing is when money leaves you or comes in from an external source. There are still two entries, but one entry is in one party's books and the other entry is the other's. For example, I get a paycheque and I enter my income in a little book with green paper and DB/CR columns. At the same time, my employer has entered an expense in their book. Double entries. reply fauigerzigerk 18 hours agorootparent>Where it can become confusing is when money leaves you or comes in from an external source. There are still two entries, but one entry is in one party's books and the other entry is the other's. For example, I get a paycheque and I enter my income in a little book with green paper and DB/CR columns. At the same time, my employer has entered an expense in their book. Double entries. I agree with your first two paragraphs but not with this last one. When money leaves you or comes in from an external source, there is always some proxy account for that external party in your own books. And the whole situation is mirrored in the accounting system of the external party (unless they are a consumer). Each party records two entries. reply bregma 13 hours agorootparentYes. I have a proxy account with one entry (say, \"expenses: bank fees\"). They have a proxy account with one entry (say \"income: bank fees\"). Between the two proxy accounts there are two entries. Money can be neither created nor destroyed. reply zie 18 hours agorootparentprev> Where it can become confusing is when money leaves you or comes in from an external source. There are still two entries, but one entry is in one party's books and the other entry is the other's. For example, I get a paycheque and I enter my income in a little book with green paper and DB/CR columns. At the same time, my employer has entered an expense in their book. Double entries. NO. I mean your employer probably has a set of books, but that's not true in your own local set of books. In your local set of books you would have something like: ACME, inc Employment Income $100 DEBIT Bank Account $100 CREDIT You are accounting for ACME, Inc's Employment expense in your set of books too. When you send a payment to your Power Company: Power Company Expense: $100 CREDIT Bank Account: $100 DEBIT I mean if you are categorizing expenses you might do something like that. If you aren't, you might title one account \"Expenses\" and spend it all there, it doesn't really matter what you call the accounts, just that you are consistent. reply bregma 14 hours agorootparentWell, if I have a local entry ACME, inc Employment Income $100 DEBIT in my employment income account that money has not come out of thin air. Remember, money can not be created nor destroyed in this system. Somewhere there is a matching entry something like bregma, services rendered $100 CREDIT in my employer's books. And that money, in turn, was probably moved in from some other account internally. Mean time the only real movement of \"money\" was an electronic communication between two banks (my employers and mine), with a matching entry in an account in each. Things like income accounts and expense accounts are not magic sources or sinks for money flows. They're just half of a double entry system with the other half somewhere else. reply zie 8 hours agorootparentI agree generally speaking, but what does that have to do with your local books? Nothing. You almost certainly don't have access to your employer's books. Also, the ledger entries for \"bregma, services rendered\" i.e. payroll will be much more complicated than that, there will be taxes, deductions, etc they have to account for as well. reply m3kw9 18 hours agorootparentprevWhat if your company decides to be generous and just gave 1000 to random Joe, what is the double entry for that? reply tromp 17 hours agorootparentCash account is credited $1000, and Gifted (or Cash_Gifted) account is debited $1000. reply randomdata 14 hours agorootparentThat method only works if money can be created out of thin air, and also destroyed. The grandparent comment was pretty clear that money cannot be created out of thin air, nor can it be destroyed. A curious contradiction. How do we resolve it? reply omichowdhury 6 hours agorootparentWhile money can’t be created or destroyed (unless you run a central bank…), value can be. Ledgers always have a specific perspective, and that perspective can assign a different value to something than someone else. In the case of gifting something, from the perspective of the gifter, they destroyed some value they had on their books and got nothing of value in return. There’s an account type for tracking why your net worth decreased - Expense accounts. The giftee received value and they have an account to track why their net worth increased - Income accounts. If value was objective, then the net worth decrease on one side would exactly equal the net worth increase on the other. With something like cash, the unit of account and the store of value are the same thing - so 100 USD objectively the same value in everyone’s ledger. But say you were gifted a painting. The gifter may have valued this painting at 100 USD, while the giftee actually thinks it’s worth 50 USD. If the gifter didn’t tell them the price, there would be no way of knowing they recorded different numbers. So in this transaction value was destroyed. The same thing happens when you buy and sell things. Say the painting was sold instead of gifted, then the difference in what the buyer and seller thought the painting was worth is value that was created and destroyed. Each person’s net worth would go up or down depending on whether they thought the painting was a bargain or overpriced. When providing services, value is created at the moment of usage and a ledger will track the creation of value in your landscaping business. reply bregma 14 hours agorootparentprevNo money was created or destroyed. The \"cash gifted\" account would have a corresponding entry in the recipients books reflecting the cash received. Unless he's delinquent about updating his books in which case it's implied but not realized. Few (unmedicated) individuals are going to track every transaction to that level though. If it was important to account for the cash donation, the company would require a receipt in exchange. If it's part of a coverup the receipt may be for something unrelated but at least the books are in good order. reply randomdata 13 hours agorootparentCash went out. One half of the double entry is correct. But nothing came back in return. There is no corresponding element of trade to account for. The transaction doesn't balance. Which is obvious in human terms. That's the point of a gift – the transaction isn't supposed to balance! But formal accounting methods are not as fluid as people are. So, of course, in reality money was created (and then destroyed, it being a gift) in order to make the transaction whole. But as far as this magical fairytale land where money can't be created the entry doesn't work. You can't account for nothing. Let's say it's not a gift. Let's say someone is borrowing $1,000 cash instead. The same applies. There is no corresponding element in trade to account for. It doesn’t balance. Thus, when the cash goes out you need to create money out of thin air to satisfy the other side of the transaction, which is later destroyed when the cash is returned. reply thereticent 11 hours agorootparentYou're misunderstanding double-entry bookkeeping. Something does not have to come into the company got every transaction moving something out of the company. If your company gives $1000 to Billy, you document a $1000 debit from your gift account and a $1000 to Billy's account payable. The goal isn't to get any one account to zero but to get a source and destination recorded separately for every movement of funds. Lending would be at least two sets of doubly-recorded transactions. reply randomdata 11 hours agorootparent> The goal isn't to get any one account to zero but to get a source and destination recorded separately Right, because transactions are actually two-sided. I give you something, you give me something in return. That's how people work with each other. And, as such, we account for a source and destination because that matches what actually happens. But often times you only offer a promise. For example, I write some software for you, and in return you offer me food. But I'm not hungry right now, and I certainly don't want food that is going to spoil before I get around to eating it, so instead you promise to give me fresh food sometime in the future when I am hungry. How do you account for that? You received software services, but gave nothing back in return other than a promise. Well, what if you recorded the promise? Software services in, promise out. You got your software, I get my food, the credit and debit accounts match. Everyone is happy. Congratulations, you just created money out of thin air! -- And now, later on, I am feeling hungry and am ready to take you up on your food offer. You give me the food, I give back the promise, food out, promise in, I'm fed, debits match credits, and the money is destroyed. That's exactly why we invented accounting: To keep track of the money being created and destroyed. You wouldn't need accounting if promises never needed to be made. Without promises, you'd have the software services, I'd have the food, and we'd have no reason to think about the transaction ever again. It is the promise that has us wanting to look back to make sure that promises outstanding are made good. reply randomdata 18 hours agorootparentprev> Money can not be created out of thin air, and it can not be destroyed. Yet accounting is necessary because money is created out of thin air. Money is just the representation of debt, an IOU. There needs to be a record of it in order to know that a debt was created and that a debt was destroyed. More practically, let's say you give me corn today, and I promise to deliver some of the chickens fed that corn to you after it is ready to for slaughter. Money keeps track of the promise outstanding. We record that promise, or account for it if you will, so that we remember that there is a promise and so that we can later ensure that the promise was delivered upon as agreed. Something that becomes especially important when you realize that promises can be traded on to other people who weren't party to the initial deal. Perhaps you don't really want chicken, but would prefer a watch instead. Luckily the watch maker would like to eat chicken for dinner down the line, so you give him the promise of chicken in exchange for the watch. So on, and so on. Realistically, double-entry accounting is really quadruple-entry accounting. You record that something was received and you record that a promise was made, then, later on, you record that something was delivered as promised and also record that the promise is no longer outstanding (or in reverse if you are on the opposite end of the transaction). A profit indicates that people still owe you things that you haven't collected upon. A loss indicates that you still owe people things that you haven't yet delivered. reply atomicfiredoll 19 hours agorootparentprevRemember, this was all done on paper before software with tagging and such existed. I'll give a description shot, since I've been doing finance work recently. Other people can feel free to correct. A company using double entry (as opposed to single) has a \"chart of accounts.\" This means they have a bunch of imaginary accounts for tracking everything, including: - Assets (e.g. cash on hand.) - Liabilities (e.g. loans) - Equity (e.g. investments in the company from outside parties) - Income/Revenue: (edit: as PopAlongKid kid mentioned, I forgot this one. This could include sales revenue, but also things like interest.) - Expenses (e.g. team lunch or a flight cost) Some of these \"accounts\" may map to actual bank accounts: there is likely a liability account for a credit card or an asset account for the company checking. Knowing all that, every time money is deposited or withdrawn (a transaction) the \"double\" references the fact that it's recorded in the journal (a.k.a ledger) of two accounts. (Edit: As bregma mentioned, one records where money is coming from and the other where it's going.) Often, an expense is often recorded in the checking \"account\" and the and the corresponding expense \"account.\" E.g. a flight may be recorded in a travel expense \"account,\" but you also record that the money came from the checking account. Every transaction is recorded in two places. Beyond just being more accurate than single entry, this helps with important finance reports like Profit & Loss, since you can now see how money is moving around. Edit: Now that I'm back on my desktop, these are a couple of useful links for understanding basic double entry bookkeeping: Accounting for Computer Scientists [0] and Accounting for Developers, Part IModern Treasury Journal [1]. What is a Sample Chart of Accounts for SASS Companies [2] illustrates some charts, which may be helpful for some folks. [0] https://martin.kleppmann.com/2011/03/07/accounting-for-compu... [1] https://www.moderntreasury.com/journal/accounting-for-develo... [2] https://kruzeconsulting.com/startup-chart-accounts/ reply kqr 18 hours agorootparent> this helps with important finance reports [...] since you can now see how money is moving around. This is the real benefit I've encountered. Any time I try to \"simplify\" financial recording for someone else and avoid double-entry, I inevitably end up wanting to perform a query that would be easy in a double-entry system but is not in any other system. reply atomicfiredoll 18 hours agorootparentRight. I didn't mention that a chart of accounts can look different in different companies/sectors. Some accounts may be considered nested (software may even show them as nested.) Then you can roll the totals for all accounts of a type into a general category account like \"Assets\" or \"Expenses.\" That makes it easier to answer questions like, \"how much have we spent in total?\" reply PopAlongKid 18 hours agorootparentprev>A company using double entry (as opposed to single) has a \"chart of accounts.\" This means they have a bunch of imaginary accounts for tracking everything, including: - Expenses (e.g. team lunch or a flight cost) - Liabilities (e.g. loans) - Equity (e.g. investments in the company from outside parties) - Assets (e.g. cash on hand.) Not sure why you didn't complete your list by adding \"Income\". reply atomicfiredoll 18 hours agorootparentThanks, I was sure I was missing something obvious like that when trying to simplify the explanation. reply Fire-Dragon-DoL 18 hours agorootparentprevHow do you determine which thing goes in which account, is it subjective or there is a formal way with a definition reply zie 18 hours agorootparentIn a general sense, it really doesn't matter, as long as you are consistent. That said, there are accounting standards that define the general set of accounts for a particular industry, etc. But every person having a set of books will want to customize it to some degree. For instance in a personal set of books, if you want to track every person you pay, you might have accounts, 1 for every single person you have ever paid, ever. That obviously can get pretty big! Others might not care that their electricity provider changed from Tootie inc. to Turtle inc, so they just have Utilities:Electricity as their account name. Others might not care at all, and just have a very general \"Expenses\" account for things like that. Make sense? The important part is consistency of using the same accounts for the same transactions. reply Fire-Dragon-DoL 17 hours agorootparentOK So it is somewhat open but you could use a set of standard accounts, I see. Makes sense. Probably it's important to keep somewhat of a registers of accounts available to avoid making mistakes and to write directions on where things should go reply zie 15 hours agorootparentOf course! There is a standard term for that: Chart of Accounts If you search for example chart of accountsyou can probably get a sample set to work from. reply tomnipotent 16 hours agorootparentprevThere's also GAAP in the US and IFRS in Europe, which are standards for how certain things need to be done to be compliant. It's not specific about things like account names or how your ledger should be structured, but outlines many expectations and rules/constraints that build confidence in the resulting numbers. reply zie 15 hours agorootparentAgreed, but every industry/sector might have their own set of standards that usually are overlays on top of GAAP/etc. For example in the US for state and local governments there is GASB: https://gasb.org reply atomicfiredoll 18 hours agorootparentprevThe chart can differ in different companies or sectors. In my mind, it comes back to what you want to be able to report on. Some companies may have a larger and more detailed chart of accounts so that they can have very specific breakdowns of things. I've heard of big charts where each of a company's departments have specific accounts and all departmental transactions go there while the rest are lumped into a \"Sales, General, and Admin\" bucket. (Although I think it's more common to tag transactions with a department code these days?) That said, categories can be broken down into sub-types beyond Assets/Liabilities/Equity/Income/Expenses. For example, assets are categorized based on how quickly they can be converted to liquid money and if they physically exist. So, under the assets account you may have accounts for current, fixed, and intangible (e.g. trademark or domain name) assets and you would record those appropriately. Edit: To answer the question more directly, it depends on the company and how they've customized their accounts or guidelines. But, there are general accounting practices that mandate the need for specific things and common questions to be answered, so a lot similar structures and guidance emerge that a company's finance team could use to tell you where something belongs. reply m3kw9 18 hours agorootparentprevSo double entry is defeated if you uses a computer to enter the entries. For example if you brought a laptop for 1000, but you accidently wrote 2000 AND the computer automatically entered 2000 in the asset account it would still balance even though it was a mistake to enter 2000. In addition, you can still make the same mistake by hand for both entries. So I’m still not getting how double entries catch mistakes reply JetSetIlly 12 hours agorootparentThere are several categories of mistake that you can make when bookkeeping. Some are caught by the double-entry system when a trial balance is prepared. The error you've described is an \"error of original entry\" and will be invisible if you only look at the trial balance. It can ultimately be caught when you compare the banking ledger with what's actually in the bank. Other errors that don't appear in the trial balance can be incredibly hard to detect and in fact, may never be noticed. This is where the real art of bookkeeping is IMO. The types of errors that do affect the trial balance are things like forgetting to enter a purchase in the purchase ledger but entering the transaction into the banking ledger correctly. Silly errors really, but we can all need help to stop us making those. reply jsty 18 hours agorootparentprevWhen you reconciled the balance in your bank account / credit card statement against that in your set of accounts, you'd notice the error as the statement balance would be 1000 higher than reflected in your accounts. reply ectopasm83 17 hours agorootparentprevThink of a ledger as a list of transactions. Transactions are directed hyper-edges: they are composed of a set of credits and a set of debits. For each transaction, the sum of debits must be equal the sum of credits. \"Double-sided\" might be a better term. Also a DAG doesn't convey the historical aspect of a ledger, each ledger should be conceived as a sequence diagram lifeline. See temporal graphs: https://teneto.readthedocs.io/en/latest/what_is_tnt.html#add... Going further with this model, you could introduce the notion of higher-order links between transactions to keep related entries together, for instance to link a correcting transaction to the corrected transaction, grouping recurring payments for a loan together and so on. You could (should!) model this on top of a bitemporal database to overcome the painful limitation of the event sourcing model, namely that corrections aren't retroactive. Imagine you need to establish due taxes for the past year for certain ledgers in your database, and for some of them you performed corrections to transactions for that fiscal period after the fiscal period ended. With an event-sourced model (a single timestamp for each transaction), the corrections won't impact your tax calculations unless you explicilty track those corrections in your query. But with a bitemporal model (two time axis), you'll be able to record both the time when you made the correction and when it should apply in the past and you'll be able to query your database normally, without even thinking about it. This could also be used to perform corrections that remain invisible to your customers while being fully tracked in your db. Another case were this could prove to be useful is with transactions that are not immediate and can be rolled back. Typically in these situations, you move money from one ledger to a counterpart dedicated temporary outgoing ledger before the transaction is confirmed. This allows you to prevent double spending. Using a bitemporal model, you could get rid of these temporary counterpart ledgers provided you add a status field to your transactions. reply alexambarch 19 hours agorootparentprevFrom what I got out of the article and my own limited understanding of double entry bookkeeping, the \"double\" seems to be referring to the part where we split a transaction into credits and debits as opposed to a transaction with positive or negative balance. The doubling is happening with the labels we use to describe what's happening with the money. From an individual account perspective, there's a doubling of the number of columns you could enter a transaction's amount into. reply gorjusborg 19 hours agorootparentThe core innovation of 'double entry' is that you can see the flow of money between accounts for every transaction. This is possible because you (the accountant) are always adding a back-reference from the other account (hence the 'double' in 'double entry'). There's really not much to it. It throws people that are new to it for a loop, I think, because it is a strange way of behaving, and it isn't obvious why you're doing it until you have to track down something that doesn't balance. It's just a disciplined behavior that accountants started using because it allows one to track things that were difficult without it. reply vpribish 16 hours agorootparentprevthis is probably not true, but I heard that this stuff predates the idea of negative numbers so you have db and cr accounts that offset each other without negatives. reply conductr 13 hours agorootparentprevAlways needs two entries to keep assets equaling liabilities plus equity. If you do anything it effects both sides of the equation, thus “double entry” is required to keep this relationship. It’s the accounting equation. https://www.investopedia.com/terms/a/accounting-equation.asp reply Linosaurus 18 hours agorootparentprev> actually explain the \"double\" part in detail? $100 appears in your account. That’s one part. The other part depends on why. * you moved money from another account, the double is -100 in that account. * you sold stuff, +100 in income. * you borrowed some money, +100 in ‘debt’. In a physical book each of these categories would have a left and right column, and each transaction has numbers in one left and one right column. Or in many columns but the sums of left vs right columns must be the same. reply danielmarkbruce 10 hours agoparentprevIt's also technically wrong. For example, a bank might decide you likely can't pay your loan and write it down to zero. You might still have the liability on your books because you plan to repay it. They'll make the relevant entries in their system (and the debits and credits will balance) and you'll do nothing (which balances). Double entry bookkeeping has zero to do with other entities. It's solely about your own books. reply winstonrc 15 hours agoparentprevI’m biased, but I hope my explanation[0] is more intuitive coming from a CPA. [0]: https://www.winstoncooke.com/blog/a-basic-introduction-to-ac... reply resters 11 hours agoparentprevI think the key point is that not only is double entry accounting a directed graph, so is single entry accounting. Therefore (and to your point) the observation is of limited usefulness. reply galaxyLogic 9 hours agoparentprev> So if Alice buys book from Bob, four entries are made. What if Alice does double-entry bookkeeping but Bob does single-entry bookkeeping? reply nickpinkston 21 hours agoprevI think people underestimate the beauty and impact of accounting. Just a tiny number of formulas (accounting identities [1]) and statements (P&L, balance sheet, etc.) can represent what's going on in any org in ways that can be roughly comparable. Reminds me of the \"fundamental theorem of calculus\" or \"central dogma of biology\". Accounting is also where we get math and written language [2] as ancient Mesopotamian civilizations initially kept track of commodities using specific \"accounting tokens\" [3] shaped like the thing represented, and that evolved into written language, imagine: hieroglyphics. Later, Al-Jajr [4] (aka algebra, literally \"balancing\") was invented by Al-Khwarizmi (origin of \"algorithm\") to solve Islamic inheritance law, whose rules of splitting up became a series of equations that led to the need to solve them quickly and correctly. Al-Khwarizmi's quadratic formula was the origin of why \"algorithm\" gets its name from him. [1] https://en.wikipedia.org/wiki/Accounting_identity [2] https://en.wikipedia.org/wiki/History_of_accounting [3] https://en.wikipedia.org/wiki/History_of_ancient_numeral_sys... [4] https://en.wikipedia.org/wiki/Al-Jabr [5] https://en.wikipedia.org/wiki/Al-Khwarizmi reply wongarsu 21 hours agoparentOn the other hand some things in how accounting is traditionally done suffer from accounting predating a lot of \"modern\" math. Negative numbers were first used around the 3rd century in China and took until the 16th century to be used in Europe. Modern double-entry bookkeeping was invented in the 14th century in Europe. So if you ever wonder why they traditionally use a column for debit and one for credit, with definitions that seem a bit strange: that was the best way to make it work with only positive numbers. reply jjeaff 16 hours agorootparentThen this would be one of those cases where working around a problem (the lack of negative numbers) actually resulted in a superior product. You don't want negative numbers because that would require the description of a category to change based on whether it is positive or negative. You wouldn't want to be changing \"Assets\" to \"Liabilities\" every time the number goes below zero. You don't need negatives in accounting, because anything that is affecting a number differently than the rest (reducing rather than increasing), needs to be accounted for separately. Imagine showing negative revenue. That would be mostly useless. You want to see how much revenue you had, and in a separate entry, how many expenses. Imagine how misleading it could be to show $0 in expenses last month, when in reality, you had $100 in expenses, but you subtracted $100 out because you returned some big purchase from last month and got a refund. reply ebolyen 15 hours agorootparent> You don't want negative numbers because that would require the description of a category to change based on whether it is positive or negative That's actually already how it works, the kind of inverse you use depends on if the account is credit-normal or debit-normal. You end up in the same place. > Imagine showing negative revenue. That would be mostly useless. Revenue is already credit instead of debit, which if you were consistent in using negative numbers, a negative revenue be completely correct (so you would want to sweat if revenue was a positive number). It's weird, but it lets your cash on hand be a positive number, which is just a necessary consequence of the system. > Imagine how misleading it could be to show $0 in expenses last month, when in reality, you had $100 in expenses, but you subtracted $100 out because you returned some big purchase from last month and got a refund. Wouldn't that be accurate? This would be a debit of 100 and a credit of 100 which nets a balance of 0. But individually you would see the transactions and could sum the credits and debits if you were so inclined. Alternatively, contra-accounts exist if reporting these individually is material for some reason. reply soheilpro 16 hours agorootparentprevHaving separate debit and credit amounts (instead of a single positive/negative number) serves another purpose as well: To track the total amount added or deducted from each account. reply kinleyd 19 hours agorootparentprevI'm not sure if that's the reason for it, but I'm pretty certain introducing negative numbers would confound the accounting process. reply accrual 14 hours agorootparentprevI also kind of like the double column approach with positive numbers, since neither party is exchanging \"negative money\", so it kind of underscores the balanced nature of the transaction. reply alexb_ 19 hours agorootparentprevI wonder if this is why losses are written down with () instead of -. reply PopAlongKid 18 hours agorootparentIt's just to make it more visually apparent. A tiny preceding hyphen is easy to miss. Same reason why negatives are sometimes printed in red, leading to such phrases as \"drowning in red ink\". reply kqr 18 hours agorootparentprevYes, that's part of it. The lengths accountants will go to to avoid having to deal with negative numbers is a bit funny. reply inopinatus 17 hours agorootparentit’s because they know it’s a slippery slope into allowing imaginary and irrational numbers, which is how accountants go to prison. reply juped 18 hours agorootparentprevNegative numbers have no place* in accounting, and people need to stop thinking \"oh, credits are just negative numbers\" and hopelessly confusing themselves and others by putting nonsensical signs on an unsigned magnitude of flux. * you can actually think of oddball reasons you might consider a \"negative credit/debit\", but it's more akin to something like a negative mass in physics reply kinleyd 17 hours agorootparentI tried using John Wiegley's Ledger, which uses negative numbers. Everything was great about Ledger except for that part - I just couldn't wrap my head around it. reply barrucadu 11 hours agorootparentMaybe this is one of those things that's harder to understand if you have an accounting background. As someone without an accounting background, I found it incredibly intuitive: if money moves out of an account, that's a posting with a negative number; if money moves into an account, that's a posting with a positive number; a transaction is a set of postings that together sum to zero, indicating that no money has been created or destroyed out of thin air. There's no need to learn any confusing \"credit\" or \"debit\" jargon, you just need to think about the movement of money (which you had to do already). reply kinleyd 46 minutes agorootparentI don't think that is the case. I think it would have to take a deeper understanding of double-entry accounting to really appreciate its genius. The key lies in the double entry that is made for each transaction. While each entry is simple, the beauty lies in how it provides the basis for detecting errors even in very large datasets simply by comparing totals - they must balance. In this context negative numbers do not provide any meaningful information at all. reply juped 13 hours agorootparentprevYeah, I had basically the same problem with it. reply kqr 20 hours agoparentprevYes, there's a neat simplicity in how everything in accounting must work out, but extrapolating from that into \"P&L, balance sheet, etc. can represent what's going on in any org in ways that can be roughly comparable\" sounds like a very financialised worldview. There are many important aspects of organisational design that are only loosely correlated with financial statements. reply shiandow 18 hours agoparentprevI know people underestimate the impact. Before the 1800s or so Europe didn't have negative numbers, except for the odd mathematician who claimed you could calculate with them even if they were obviously meaningless. It was only after bookkeeping became ingrained into all levels society that negative numbers were considered equally real as positive numbers. reply nyrikki 18 hours agorootparentThey didn't have modern notation, specifically the integers as a convention hadn't been adopted by primarily Greek mathematicians like Diophantus. That is very different from understanding concepts like owing someone money, which is a form of negative. Chinese and Indian mathematicians and accounting from Babylonian times didn't have the same mental blocks. The Bakhshali manuscript is still the first known example of modernish notation I think, but used + reply yread 20 hours agoparentprev> can represent what's going on in any org in ways that can be roughly comparable. It can also obfuscate it pretty well! reply PopAlongKid 20 hours agorootparentThat's where statement of cash flows[0] comes in -- because over a long enough time frame, profit is just cash in minus cash out. Hard to obfuscate that. Companies usually go bankrupt not because of negative net worth, but because of insufficient cash flow. [0]the third basic type of statement, in addition to balance sheet and profit & loss. reply Solvency 19 hours agorootparentso why isn't there a single cash flow view/report that all businesses use rather than all of these bloated and apparently ineffective methods? PL etc.. reply PopAlongKid 19 hours agorootparentThere is a cash flow report that all (public) businesses use -- as I said, it is one of 3 basic reports that capture different aspects of the companies financial performance. You need all 3 for a complete picture. The cash flow statement helps \"unobfuscate\" certain aspects of the other two reports (which I hope you agree are not pure obfuscation). For example, see this Apple Corp. financial report[0] -- it includes a statement of cash flows. I think you'd find that all other companies required to publicly share their financials will include the same report. [0]https://www.apple.com/newsroom/pdfs/FY22_Q4_Consolidated_Fin... reply joncrocks 19 hours agorootparentprevCash flow is not just what is happening, but what will happen. i.e. the difference between when money goes out vs. it coming in. If you are selling widgets for $1.20 and buying them for $1, and they are delivered after you order them, you are limited on how many open orders you can have at any point in time even though you make money on each order. A companies cash flow will be dependent on employees, capital expenditures, money for stock, returns rates, tax requirements etc. etc. So it's heavily dependent on what type of business is being run and the specifics of how the business is being run. reply actionfromafar 20 hours agorootparentprevObligatory \"Seeing Like a State\" reference. reply rjinman 18 hours agoprevDouble-entry bookkeeping is very easy to understand once you ditch the ridiculous \"credit\" and \"debit\" terminology. Essentially, the goal is to keep the accounting equation true at all times. The equation is: Equity = Assets - Liabilities. Eventually, earnings (Income - Expenses) will become part of equity, so splitting that out, you have: Equity + Income - Expenses = Assets - Liabilities. Rearranging to get rid of the minus signs you get: Equity + Income + Liabilities = Assets + Expenses. This equation must be true or something has gone wrong - like money appearing or disappearing out of nowhere. To keep it true at all times, it should be clear that any time you add money to an account on the left side of the equation (say, to an Income account), you must either add the same amount to an account on the other side or subtract the same amount from the same side. For example, you sell a lemonade for $5. You add $5 to Sales (Income) and add $5 to Current Account (Assets). The \"credit\" and \"debit\" terminology is ridiculous because their definitions swap around depending on which account you're talking about, which is an utterly absurd (mis)use of language and the main reason people find this confusing. reply EvanAnderson 17 hours agoparentThe accounting equation is the right thing to think about. People want debit and credit to mean something more than they need to. My 100-level accounting instructor said it pretty succinctly: Debit means an entry in the left column. Credit means an entry in the right column. What a transaction means for the business depends on the accounts. reply rahimnathwani 15 hours agorootparentIt sounds like your accounting instructor may have focused too much on implementation details (left/right), and too little on accounting principles. The terms debit and credit have meaning independent of their columnar position on a traditional ledger. I could create a ledger with the columns reverse or (shocking!) use a computer program with a data structure that doesn't encode the concept of left or right. I think about it like this: CR / Credit / Creditors -> what the business owes DR / Debit / Debtors -> what the business owns A CR entry is an increase is what the company owes (to creditors or shareholders), and a DR is an increase in what the company owns. A common objection to this is 'what about income and expense accounts'? But those are just equity: https://news.ycombinator.com/item?id=39991837 I wrote more about this here: https://news.ycombinator.com/item?id=32498992 reply EvanAnderson 14 hours agorootparentThe sheer amount of discussion this has created (both here and back in August, 2022, when you and I commented back and forth to each other in your link) validates my my instructor's philosophy to me. Concentrating on the accounts and the accounting equation, ignoring any \"meaning\" for the words debit and credit, results in the \"right answer\" without a lot of consternation. reply ChirronT447 10 hours agorootparentI completely agree with this; I remember reading that prior thread on double entry accounting and being so frustrated and confused about the use of debit/credit terminology and then, as now, your comments (and others) were very very helpful and insightful. Something I find frustrating is the - almost - endless debate and nitpicking on small details or elements implied but not explicitly stated...but I guess people are trying to be helpful (or right). I didn't have an account to comment then but I really appreciated your perspective; it was extremely valuable seeing a few people saying 'forget about the credit/debit nomenclature, it's confusing' and recognising not everyone knows the terminology. Now I have an account here and can thank you for fighting the good fight again. reply rahimnathwani 8 hours agorootparentprevI agree that the accounting equation is a good starting point. Without it there is no context for the individual accounts. Over the ~20 years since I qualified as an accountant, I've found the concept of debits and credits useful. It has saved me from memorizing rules (like what's on the left and what's on the right), to design charts of accounts, to design rules for recording transactions etc. Someone who doesn't ever need to design charts of accounts or accounting policies, and is never called upon to verify the correctness of an accounting approach, probably doesn't need to understand debits and credits. They can read a balance sheet and income statement without thinking about the concept. And many people (even bookkeepers and accountants) are content to memorize rules without needing to understand their source. But that doesn't mean the underlying concepts don't exist, or that they aren't valuable. Imagine if there were a subreddit for accountants, some of whom dabbled in coding. There might be a back and forth about principles of objects oriented design. The general consensus might be that it's pointless to understand the principles, and that it would be better to focus on some small set of rules of thumb, that give the right answer in most cases. They might be right in that context (accountants who code on the side), but it doesn't mean the principles don't exist or aren't valuable for people who do that stuff all the time. (BTW I think accounting is, in general, taught very poorly. I wish more instructors used Frank Wood's books. An intuitive grasp of debits and credits is really useful and not hard to pick up, yet many people spend a semester studying accounting without developing any intuition at all.) reply lurkingmba 14 hours agorootparentprevThat's too simple. That logic roughly works the balance sheet. However, it says nothing about the income statement. For the income statement, CR -> revenue and DR -> expense. reply rahimnathwani 13 hours agorootparentI addressed this earlier: https://news.ycombinator.com/item?id=39991837 When you record transactions in accounting, you're updating various account balances, such as assets, liabilities, and equity. These updated balances contribute to the creation of the balance sheet, which provides a snapshot of a company's financial position at a specific point in time. The income statement, on the other hand, reflects the changes in these balances over a period of time. There's nothing special about the line items on the income statement (e.g. revenue or expense). Any value on the income statement represents a change in what the company owns (assets) or what it owes (liabilities or equity). reply lisper 17 hours agorootparentprev> Debit means an entry in the left column. Credit means an entry in the right column But that just shifts the arbitrariness of the whole thing from the words \"debit\" and \"credit\" to the words \"left\" and \"right\". reply EvanAnderson 17 hours agorootparentI think his intent was to prevent students from fixating on making the words debit and credit \"mean\" something by themselves. A debit doesn't have some intrinsic meaning about the \"flow of money\". It's just an entry in the left column. On the other hand, a debit to Accounts Receivable actually means something. reply lisper 17 hours agorootparent> A debit doesn't have some intrinsic meaning about the \"flow of money\". But it does. \"Debit\" is an English word with an established meaning in common usage. It means to take money out of an account. It is related to the word \"debt\" which is something that decreases the net worth of the debtor and increases the net worth of the creditor. If you overpay a bill, the (positive) difference between what you paid and what you owed is a credit on your account, and can be used just like money to pay your next bill. reply mhink 15 hours agorootparentThat's not entirely correct- or at least, it's more complicated than that. The question of whether a debit/credit increases/decreases an account has to do with the kind of account you're talking about. When I deposit money, it modifies two accounts at the bank: - the account which represents how much money they owe me - and the account which represents how much money they have on hand. The former is a liability, and the latter is an asset. The meaning of debit/credit is reversed between these two types of account. So, when I deposit $100, the entries entered are: - CREDIT mhink's account $100 (increasing liability) - DEBIT cash account $100 (increasing asset) Since we only see one side of this, we start to associate \"debit\" with \"less money for me\" and \"credit\" as \"more money for me\". Oddly enough, another common financial situation reinforces this interpretation from the other direction: accounts with utility providers. Unlike the bank, your account at the utility company represents how much you owe them. So the meaning of debit/credit is reversed, but so is the direction of responsibility: your account at the utility provider is money you owe them, which is an asset. So when I pay them $100, the entries entered are: - CREDIT mhink's account $100 (decreasing asset) - DEBIT cash on hand $100 (increasing asset) reply lisper 15 hours agorootparentThe problem is that whether something is an asset or a liability depends on your point of view. If I have $100 in cash, that is an asset to me and a liability to the rest of society. If I have a $100 loan, that is a liability to me and an asset to my creditor. So there is no way to say whether something is an asset or a liability in an absolute sense. Every debt is an asset to the creditor and a liability to the debtor. This has nothing to do with labeling transactions so that the labels conform to the common meanings of English words. When an account representing assets has its balance go up, that's a credit. When an account representing a liability has its balance go up, that's a debit. And vice versa. If I, say, draw down a line of credit for $100 and deposit the funds in my checking account, then from my point of view, my LoC should debited by $100 and my checking account should be credited for $100. This makes sense regardless of how you think about the LoC. If you think of the available credit as an asset, then when you draw down the LoC the available credit balance goes down and it's a debit. If you think of the amount owed on the LoC as a liability, then when you draw down the LoC the amount owed goes up and it's still a debit. > CREDIT mhink's account $100 (increasing liability) No. This transaction does not increase liability in any absolute sense. It increases liability only from the bank's perspective. From your perspective, it increases assets. reply epcoa 13 hours agorootparent> It increases liability only from the bank's perspective. You missed the context: When I deposit money, it modifies two accounts at the bank. It appeared to me they were very much explaining this from the banks or utility company’s perspective. reply lisper 13 hours agorootparent> When I deposit money, it modifies two accounts at the bank. Yeah, I get that. I don't see what that has to do with the labels used to describe the transaction. Actual physical cash is weird because it's an asset to its owner and a liability to the rest of society. But when you deposit cash in a bank, the bank doesn't become the owner of the cash. It has borrowed that cash from you. So that cash is both an asset (because having borrowed it from you it can turn around and loan it to someone else) and a liability (because the bank is in debt to you for the amount of the deposit). A simpler example is depositing a check. In that case, money just gets transferred from the payor to the payee. It's a debit from the payer's account and a credit to the payee's account. Or at least that's how it should be. reply pessimizer 16 hours agorootparentprev> If you overpay a bill, the (positive) difference between what you paid and what you owed is a credit on your account Or it's a debit on the company's account. I think that's the point that was being made; not to confuse technical terms with English common usage, and not to go to the dictionary or etymology(!) as the arbiter. Debits are credits and credits are debits, but the real question is which column does it go into. Same nature as discussions about clients/servers. reply lisper 16 hours agorootparent> Or it's a debit on the company's account. That's exactly right. They owe you money, so it is (or at least it should be) a credit on your account, and a debit on theirs. But that is not what the definition given in the article says. TFA's definition of \"credit\" was \"An entry that represents money leaving an account\" and likewise a debit is \"An entry that represents money entering an account.\" So when you paid your bill, that was (by the articles definition) a credit to your checking account and a debit to your account with company whose bill you were paying, which is exactly backwards. According to the standard English definitions, a credit is something that makes your net worth go up, and a debit is something that makes your net worth go down. So when you pay your bill, that should be a debit to you (cash going out decreases your net worth) and a credit to the counterparty (cash coming in increases their net worth). > Same nature as discussions about clients/servers. How so? It seems to me that distinction is clear: the client is the machine that initiated the connection, the one that did the DNS lookup. reply wodenokoto 12 hours agorootparentprevBut at that point how can you tell if something goes left or right? reply fauigerzigerk 17 hours agorootparentprevAnd what if there are no columns? Google \"journal entries for X\" and you're going to find something like this: Dr accountX £100 Cr accountY £90 Cr accountZ £10 Left and right was fine when T accounts were universally used to record entries, but that's no longer the case. reply eviks 17 hours agorootparentI did, opened one random top result https://www.deskera.com/blog/journal-entries/ And got left/right as explanation and also as left and right columns reply fauigerzigerk 16 hours agorootparentClearly I overstated my case. I'm not denying that columns are often used, especially in educational material. It's just no longer as universal as it once was. https://www.accountingweb.co.uk/any-answers/vat-double-entry reply freedomben 16 hours agorootparentprevYeah, GP really undermined their point with the whole \"google x\" and see. For most people who aren't accountants though, the spreadsheet thing is correct. reply mikeyouse 17 hours agorootparentprevRight - the words themselves aren't as important as the concept. Any replacement word will suffer the same confusion. There's a reason that the language of debits and credits has largely remained the same for the past thousand years, and the language describing accounting is unlikely to be 'optimized' by first-principles CS concepts from people only loosely familiar with the field. reply grantc 16 hours agorootparentWell said. It's actually not complicated or arbitrary. It also works effectively in practice over the gdp of the known universe. If you are savvy enough to be interested in and understand the different computing approaches to double-entry bookkeeping, one can assume the whole DR/CR concept isn't beyond you. reply eviks 17 hours agorootparentprevThere's a reason, but it's definitely not that any replacement is just as bad since that's close to an impossibly strong statement reply ChirronT447 10 hours agoparentprev'Double-entry bookkeeping is very easy to understand once you ditch the ridiculous \"credit\" and \"debit\" terminology.' I love it, let's do it! Thank you this is very helpful. The whole credit/debit terminology usage here is incredibly confusing to someone who hasn't studied accounting and many of the comments and replies to people who are confused are, while technically correct, simultaneously, unhelpful - to those not familiar with the terminology. I read a comment earlier: \"Why would my bank account be debited when the balance went up? Is a debit not negative? Is the cash balance presented as a negative?\" And thought: \"Yes! Great question! This doesn't make sense..because debits are always negative right? A direct debit takes money out, you spend money using a debit card, a debit is a debt right? So debits are always negative and debiting is always minus-ing money...\" - but the replies, while technically correct weren't satisfying at all because they assumed knowledge. It's both amusing and frustrating to watch people effectively speaking past each other like they're talking a different language. Especially when you have the same perspective as the person who is confused and trying to seek understanding. It seems like people nitpick on small points of what was said seemingly in order to be right. reply omichowdhury 7 hours agorootparentI think the fundamental problem is the traditional accounting equation: Assets + Expenses = Liabilities + Equity + Income We try to group the accounts by left and right side and find a common term for them (credit-normal and debit-normal). But it’s really hard to come up with an intuitive answer for why Assets and Expense should be on one team, and why the rest should be on the other team. So we just pick some team names and say shut-up-and-calculate. What if we re-arranged the equation to: Assets - Liabilities = Income - Expenses The accounts on the left side track your net worth. The accounts on the right side track why net worth changes. What should be the names for the two sides? I call them State and Change. You can then ditch credits and debits and ask - what is the impact of this financial transaction on my net worth? The equation will tell you which accounts should go up and go down using positive and negative numbers. I go more into how this works here: https://news.ycombinator.com/item?id=39994335 reply mijoharas 17 hours agoparentprev> The \"credit\" and \"debit\" terminology is ridiculous because their definitions swap around depending on which account you're talking about, which is an utterly absurd (mis)use of language and the main reason people find this confusing What would you suggest as an improvement? The article suggests \"incoming\" and \"outgoing\" which seems to have the same issue, as does everything I see in your comment (the person spending 5$ on lemonade sure as hell isn't putting 5$ in their accounts sales entry). I'm not fully understanding the confusion both here and in the article. reply DwnVoteHoneyPot 17 hours agorootparentWhen I talk to accountants, I get confused with debit/credit so I use \"increase\" and \"decrease\". Everyone seems to understand me fine. For example, \"Decrease cash\", to buy equipment \"increases assets\". \"Increase cash\" by borrowing money is \"increasing liability\". reply freedomben 16 hours agorootparentIndeed, the dirty secret is that many accountants think of debit and credit as decrease and increase as well. After using the terms for a little while they switch the symbol (word) they think of, but it still retains the same meaning. They are basically synonymous. source: friends and family members who are accountants and have generously given free bookkeeping tutorials reply fauigerzigerk 16 hours agorootparent>Indeed, the dirty secret is that many accountants think of debit and credit as decrease and increase as well. So how would you correctly express the parent's example in terms of debit/credit if debit/credit are synonymous with decrease/increase?: >>\"Increase cash\" by borrowing money is \"increasing liability\". \"Crediting cash by borrowing money is crediting liability\" would sound obviously incorrect to any accountant. reply freedomben 13 hours agorootparentPassing on the answer I got: > I wouldn't say \"they are basically synonymous\" because there are situations where they flip depending on the rules/approach that you are following. After working with it you get pretty familiar with these situations and don't really even translate anymore. It's important to remember though that there are books the way most people see them, and the way an accountant following GAAP sees them. \"Increase\" and \"decrease\" are quite helpful for most people the way most people see the books. If you are applying GAAP it's like working in a different language where words don't cleanly translate. Given that we are mostly talking about double-entry in this thread, I think he is basically telling me I'm wrong but trying to explain how I came by it honestly so I don't feel stupid :-D To quote the famous Bender from Futurama: \"I’m so embarrassed. I wish everybody else was dead.\" reply lisper 17 hours agorootparentprev> What would you suggest as an improvement? Use the intuitive meaning of the words: a credit means you have money coming in, a debit means you have money going out. An increase in assets, income, or equity is a credit, and an increase in expenses or liabilities is a debit, and vice versa. Or, alternatively, just use \"credit\" for any increase, and \"debit\" for any decrease. But this: \"Definition 6: Credit - An entry that represents money leaving an account.\" is just totally backwards. reply fauigerzigerk 16 hours agorootparent>Use the intuitive meaning of the words: a credit means you have money coming in, a debit means you have money going out. An increase in assets, income, or equity is a credit, and an increase in expenses or liabilities is a debit, and vice versa. An increase in assets is a debit. >Or, alternatively, just use \"credit\" for any increase, and \"debit\" for any decrease. How is this consistent with the fact that an increase in my bank account balance is a debit? reply lisper 16 hours agorootparentYou have completely missed the point, which is that the way in which accountants use these words is unnecessarily confusing because it does not align with the common English definitions of the words \"credit\" and \"debit\". reply dudinax 2 hours agorootparentThe common English use of 'credit' and 'debit' is correct, as they ought to be since we learned them from banks. Most people are only aware of one type of account, a liability account managed by the bank in their name. The mistake is that we talk about them as \"our\" accounts. reply fauigerzigerk 16 hours agorootparentprevYes, sorry, I was defending the established terminology without making clear why. My problem is that your alternatives don't just change the words, they change the logic. The invariant of debit/credit is that they need to balance out. If you choose words that can occur on both sides of the equation then this is no longer true and you're throwing out a lot more than just the admittedly unintuitive meanings of these words. reply lisper 15 hours agorootparent> My problem is that your alternatives don't just change the words, they change the logic. No, they don't. They just change the words you need to express the logic. > The invariant of debit/credit is that they need to balance out. Sure. So? If I give you a dollar, that's going to balance whether we call that a debit to me and a credit to you or a credit to me and a debit to you. The labels don't matter. reply fauigerzigerk 14 hours agorootparentWhat matters is that the labels are different on each side of the equation. That contradicts your suggestion that we should use the intuitive meaning of the words and it contradicts your suggestion to 'just use \"credit\" for any increase, and \"debit\" for any decrease'. Let's say a company raises equity (i.e it issues new shares), money comes into the bank account. In traditional terminology that would result in: debit bank credit equity According to your suggestions, however, raising equity would result in increase bank increase equity This violates the principle that the sum of labelAs need to cancel out (or balance out) the sum of labelBs. And this is why I said that you're changing the logic. reply lisper 13 hours agorootparentIt doesn't matter whether you encode the signs in the terminology or in the equation. If you say X + Y = 0 i.e. X = - Y and stipulate that a transaction adds to X and subtracts from Y, that is completely equivalent to saying X - Y = 0 i.e. X = Y and stipulating that a transaction adds to both X and Y. reply fauigerzigerk 13 hours agorootparentSure, but if you want to keep your terminology consistent with the math, you would then have to make a distinction based on the types of the accounts involved in a journal entry. E.g, this would be correct: increase bank increase equity but this would be incorrect: increase bank increase receivables If all numbers are positive then there would be no way to check whether the journal entries balance out without considering the account types. If, on the other hand, you encode the sign in the amounts then the sign would disagree with the semantics of the label and you would have to flip signs based on the account type at the time of recording the entries: increase bank $100 increase receivables -$100 I don't suppose this is what you meant. reply lisper 12 hours agorootparent> you would then have to make a distinction based on the types of the accounts involved in a journal entry That's right. The distinction is based on whether the account represents an asset or a liability. > increase bank > increase receivables You would have to define what you mean by \"bank\" in order for this to make sense. But in general, receivables represent money that a company is owed from orders that have not yet been paid for, i.e. they are a effectively a loan from the company to its customer, and like all loans they are an asset to the creditor, which in this case is the company. When a payment is made against an outstanding receivable, the receivable is debited (the loan balance is reduced) and the company's cash balance is credited. Because cash and receivables are both assets, these add together and cancel out just as you would expect. The rules under my system are still very simple: 1. Every financial asset is someone else's liability, and vice versa. Cash is an asset to its owner and a liability to society at large. Loans are assets to the creditor and a liability to the debtor. Purchase orders are a liability to the purchaser and an asset to the supplier. Etc. etc. 2. Every financial transaction is a change in someone's liability coupled with a change of equal magnitude in someone else's assets. 3. The absolute value of your assets minus the absolute value of your liabilities is your net worth. A completely equivalent formulation is that liabilities have negative signs attached to them, and then your net worth is the sum of your assets and liabilities, but this is just a question of where you hide the negative sign. A - B is the same as A + (- B). It really doesn't matter except insofar as one convention might make it easier to think about things. Most people are used to seeing their liabilities expressed as positive numbers, i.e. if you owe money on your credit card bill, the balance due is positive, and if you have a credit balance, the balance due is negative. But it's all just a shell game with where you hide the signs. reply fauigerzigerk 4 hours agorootparent>You would have to define what you mean by \"bank\" in order for this to make sense. Bank means bank ledger account. The bank balance and receivables cannot both increase because they are both assets. >That's right. The distinction is based on whether the account represents an asset or a liability. ... A completely equivalent formulation is that liabilities have negative signs attached to them Understood. My point was merely that you cannot just change the labels. The downside of this approach is that you would no longer be able to see whether a journal entry balances out based on the labels alone. reply lisper 4 hours agorootparent> The downside of this approach is that you would no longer be able to see whether a journal entry balances out based on the labels alone. Yeah, well, there is this cool new invention called a \"digital computer\" that can help a lot with that. You don't have to keep the ledger on paper using quill and ink any more. reply fauigerzigerk 4 hours agorootparentAbsolutely, but digital documents are not accounting software either. Communication would definitely get harder if we lose debit/credit and with it the left/right visualisation of T accounts. Perhaps some simple convention would help, like attaching +/- to account names. We do have account numbers and accountants know their meaning but most people don't. reply lisper 3 hours agorootparent> like attaching +/- to account names Or \"asset\" and \"liability\". (Big displays are a thing now too.) reply fauigerzigerk 3 hours agorootparentIt would be very confusing to label expense accounts as \"liability\". Liability has a very specific meaning (debt) and expenses do not necessarily increase liability. And then there are accounts that can be assets or liabilities depending on their balance. (Besides, very small displays a thing now too) reply lisper 3 hours agorootparent> It would be very confusing to label expense accounts as \"liability\". Why? > expenses do not necessarily increase liability. That depends on what you mean by \"expenses\". If you give someone an expense account, that is a commitment to make payments for expenses, i.e. debt, so it's a liability. When you actually pay for those expenses (or reimburse someone for incurring those expenses) you are paying off debt and reducing your liabilities. Why is that confusing? > And then there are accounts that can be assets or liabilities depending on their balance. Sure. So? An asset account is one which represents assets when its balance is positive, and a liability account is one which represents liabilities when its balance is positive. A negative balance in an asset account is a liability, and a negative balance in a liability account (like a credit card, for example) is an asset. You could do away with this convention and just represent all assets as positive values and all liabilities as negative, but people are used to distinguishing \"money that you have\" from \"money that you owe\" and having both of those represented by positive numbers in the usual case. > very small displays a thing now too Not for accountants. reply fauigerzigerk 2 hours agorootparent>That depends on what you mean by \"expenses\". If you give someone an expense account, that is a commitment to make payments for expenses, i.e. debt, so it's a liability. This is not what expense account means in accounting. An expense account is an account that records expenses incurred such as your AWS bill, rent payments or salaries paid. These are not liabilities and labelling them as such is more than confusing. >Sure. So? An asset account is one which represents assets when its balance is positive, and a liability account is one which represents liabilities when its balance is positive. Exactly, so how do you label it if it can be either? The only way I see is to label it according to its main purpose and accept that it's sometimes semantically wrong. That's effectively what the chart of accounts does. reply omichowdhury 8 hours agorootparentprevYup, this is the same way of thinking I described here: https://news.ycombinator.com/item?id=39994335 Instead of grouping together accounts into credit-normal and debit-normal, to make things balance, I think it’s more intuitive to group accounts by usage and use negative numbers. Asset and Liability accounts appear on a Balance Sheet and are called State accounts. State accounts track the current state of your net worth. Income and expense accounts appear on an Income Statement and are called Change accounts. Change accounts since they track why your net worth changed. reply tantalor 14 hours agorootparentprevI solve this by remembering \"debit = destination\" (d=d) in all cases. Examples: If you deposit money into a checking account (asset) that is a debit (account increases) because the money \"goes to\" in that account (destination). If you borrow money from a credit card (liability) that is a credit (account increases) because the money \"comes from\" that account (not destination). The hard part is remembering debit accounts increase with debits, and credit accounts increase with credits. reply braiamp 17 hours agorootparentprevBecause people don't understand that credit and debit only make sense in the context of the account being applied to. If you deposit money to your bank account, it's a credit in your . If you withdraw money from the ATM, you debit your bank account and credit your cash account. But globally you haven't gotten more money. reply tantalor 14 hours agorootparent> If you withdraw money from the ATM, you debit your bank account and credit your cash account You have that exactly backwards! Assets (like bank accounts and cash) are \"debit accounts\" meaning they increase with debits and decrease with credits. When you withdraw money from your bank account, the bank account goes down, so we know that must be a credit to the bank account, while the cash goes up, that is a debit to the cash account. Your confusion might be due to perspective. From the bank's view your bank account is a liability (credit account) so it increases with credits and decreases with debits. reply ChirronT447 9 hours agorootparentDo they have it backwards? It sounds like a valid perspective to me. I take money from an ATM: the number in my current account decreases, the cash I have on hand increases. Nothing wrong there. Sure the banks perspective is different but maybe I'm not interested in that. I love that this thread is full of people confidentally saying something that sounds correct or at least reasonable and the first reply that comes back is no you've got that wrong and then what your saying also sound's reasonable but it just seems to depend on the context and perspective. I would have thought accounting a solved problem but apparently not. reply pas 9 hours agorootparent> accounting a solved problem but apparently not. well, it is if you do it on books, not in natural language. since it looks deceptively simple everyone throws around sentences that are screaming for mandatory context. the whole GAAP (generally accepted accounting principles) (and certs like CFA too) are about codifying this context. what goes where is the name of the game. can you consider this or that an asset or not? is that an expense or you got credit from your vendor, because they shipped it before you paid it? which quarter does it belong to if they shipped it before new year's eve but we only pay it next financial year? etc... etc... that said accounting is not a mechanical system. there are quite a lot of degrees of freedom ... but there are of course clearly wrong ways to do it ( https://en.wikipedia.org/wiki/Creative_accounting ) oh, and when someone says debit/credit just use a spray bottle on them and ask them to simply state clearly what happens with the fucking number on which of your accounts, does it increase or decrease. (ie. they should just say that the money goes from this account to that account, and suddenly there's no ambiguity.) reply globular-toast 17 hours agorootparentprevIt doesn't matter about the person buying lemonade. Their accounts are theirs alone and don't affect your accounts. reply jandrese 18 hours agoparentprevThe one thing I remember most from my economics courses in college is that economists have highly idiosyncratic mathematical conventions and they don't care. So many graphs with the independent variable on the Y axis... reply abtinf 17 hours agorootparent> So many graphs with the independent variable on the Y axis I was perplexed by this as well and none of my profs could cogently explain it. The classic example are supply and demand curves, with price as the Y axis. I finally realized they are actually trying to communicate that price is not under the control of the buyer or seller, but that the market dictates the price given a level of production. This kind of “spherical cow” thinking made me develop a healthy contempt for conventional economics. reply abdullahkhalids 16 hours agorootparentIndeed, one of the main problems with econ education is that at the most basic level they teach a model for the \"spherical cow\" free-market. Which is all that most people end up learning. And then those people try to apply this reasoning to real world markets - the vast majority of which do not satisfy the assumptions of the free-market model. So almost all public discussions of micro-economics is totally useless. reply freedomben 16 hours agorootparentI mostly agree, but I hardly think it's useless. Much like beginning your understanding of physics with Newtonian equations, it just needs to be qualified. But it's shocking how many people don't understand the basic idea of supply and demand and how the relate relative to some rationing system (usually price). If you don't understand that foundational idea, then considering the effects of different rationing systems is utterly impossible. For example, that gets you a whole lot of people who make decisions that exacerbate housing crises by creating rent controls or building restrictions, and then being utterly perplexed when there isn't enough supply to go around (because they decoupled the signalling mechanism that the suppliers use (aka price) from what the buyers use (who got there first, or who is luckier with timing, or who is more politically connected). This is not to say that price controls are always bad, because real life is much more complicated than econ101 concepts lay out. But with nearly every other subject we expect people to have a basic level of understanding (like biology, history, english, etc) because we recognize it's importance for society and individuals to have at least a basic level of understanding in many different subjects. One that has as big an impact on life as economics seems like one of the worst to omit. Just like we tell 8th grade physics students that \"in the real world, cows aren't spherical so it's a little more complicated than this, but this gets you 80% to 90% of the way there\" I don't see why we shouldn't do the same for economics. reply pas 9 hours agorootparentprev... microeconomics works amazingly well! scroll down and look at that graph! https://www.fda.gov/about-fda/center-drug-evaluation-and-res... also go to and read about the studies https://www.noahpinion.blog/i/142905737/the-evidence-is-in-f... and you can see the exact numbers from the datasets, these are all spherical cow parameters basically! reply jandrese 8 hours agorootparentI took both micro and macroeconomics in school. I had a similar thought, microeconomics is dealing with systems simple enough to be reasonably well modeled in mathematica",
    "originSummary": [
      "The article delves into the basics of accounting, covering bookkeeping and ledger systems to show how transactions are recorded accurately through double-entry bookkeeping.",
      "It explains the use of T-accounts and contra accounts, highlighting money flow visualization between accounts as a directed graph to aid beginners in understanding accounting concepts.",
      "The article aims to simplify accounting principles and offers additional resources for those looking to study accounting further."
    ],
    "commentSummary": [
      "The focus is on double-entry bookkeeping, highlighting the balance between debits and credits, understanding assets, liabilities, equity, and utilizing the accounting equation.",
      "It delves into the significance of accuracy in financial reporting, recording transactions precisely, and various accounting methodologies like QuickBooks.",
      "Emphasizes the historical roots and development of accounting practices, addressing the use of negative numbers, terminology, and key concepts in accounting principles, and underscores the essential role of accounting in transparently reflecting business financial transactions."
    ],
    "points": 498,
    "commentCount": 331,
    "retryCount": 0,
    "time": 1712744003
  },
  {
    "id": 39998849,
    "title": "AI-Generated Sad Girl with Piano Performs Emotional MIT License Song",
    "originLink": "https://suno.com/song/da6d4a83-1001-4694-8c28-648a6e8bad0a/",
    "originBody": "Home Create Library Explore BETA Community Help About 178 Permission is hereby granted Sad girl piano ballad; jazz-trained female singer-songwriter v3 April 4, 2024 Play Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 0:00 0:00",
    "commentLink": "https://news.ycombinator.com/item?id=39998849",
    "commentBody": "AI-generated sad girl with piano performs the text of the MIT License (suno.com)395 points by hongsy 3 hours agohidepastfavorite168 comments navane 6 minutes agoThese songs don't have any hooks. Melodies don't get repeated. They all just meander along. I think as they get better at making these great middle of the road songs, edgy music will reemerge. Whatever AI will be good at, will immediately devalue, in the realm of arts. Just how photography gave way to non-realistic art and how drum machines made sloppy drums (or ridiculous apex twins) hip. Then, the artifacts we see now as flaws will create its own sub genre. So I see three ways this flows: mediocrity will be even more available, which will make artists who make mediocare music even less succesfull, pushing all human music further in human direction, except those using the unwanted artifacts of this new tech to create new sub genres. Music, art, fashion is in the end all about changes. What we make now mostly means something in relation to what was already there. It's a big conversation, spanning millenia, and this isn't the last word. reply amelius 4 minutes agoparentElevator music ... Hmm, perhaps there is a business model there: an elevator that writes music about the people inside the elevator, and adapts as the situation changes. reply epaga 2 hours agoprevI have to say, I completely lost it at the whisper '(The \"Software\")' (0:18)... give this tech another year or two and it will be better quality than your average radio song. reply cedws 2 hours agoparentI don't think I can endure much more AI slop seeping its way into my life. reply chilmers 1 hour agorootparentTo me the feeling of AI generated content is less \"slop\" and more \"in-flight magazine\". It can have a surface sheen of quality that you can lure you in, but you realise it's devoid of any vitality or soul. reply Max-q 1 hour agorootparentWhen recorded music was invented, musicians protested. Recorded music was devoid of any vitality or soul. Recorded music still became a hit. Then we got the synthesizer. Again we got the same complaints, lifeless and without soul. The synthesizer still became a hit. Now the next step is happening, and we see the same complaints all over. Only time will show if the next step will happen anyway. My gut feeling tells me that AI art will gain acceptance over time, and we will just think of it as \"art\" or \"music\", just as we did with recorded mysic and synthetic sounds. reply piva00 58 minutes agorootparentYou are just skipping the step that all of it had the human element considered, which to all of us is a very intrinsic part of \"art\". When AI generates a piece of entertainment it's ok to just call it entertainment, it's not art until AI actually has something we can relate to as consciousness (aka a \"soul\"). reply TeMPOraL 27 minutes agorootparentHard disagree. I don't think anyone considers that, unless they're an art critic, a philosopher, or a snob. For a normal person, whether something is art or not, is a mix of 1) whether they like it, 2) whether they can, or conceivably could, enjoy it together with other people, and 3) whether they're supposed to enjoy it or call it art, because other people claim they do (social proof). Examples: - Pop songs are strongly 1, 2 and 3a (enjoyment), but not necessarily 3b (considered High Art). Most people don't care, or couldn't even tell, if the songs they like were written and performed by actual humans or by machines; they experience them through some machine anyway. - Paintings. I recently visited a Van Gogh exhibition, and I can't honestly say I liked most of it. Most paintings, in general, are ugly. We call them art because we're supposed to call some paint scribbles on a canvas art, particularly when they're framed and put in a museum (as opposed to bought off the street!) and decreed Art by People In Authority Over What Is or Isn't Art. For this exhibit in particular, my ability to enjoy the paintings was proportional to how much I knew about Vincent van Gogh's life - for those paintings I had some context for, I enjoyed them even though they're otherwise pretty bad to me. But most people, most of the time, don't have any context for paintings they're viewing, and they still call them art. Hell, arguably, the best \"paintings\" in that exhibit were a couple that were obviously AI-generated - like Vincent wearing VR goggles, or animated Vincent inviting the patrons to the exhibit. Nah, what I think is death of art for regular people is quantity and personalization. The most important aspect of day-to-day art experience is that you can enjoy it together with people around you. It's a problem for TV shows and books these days, and even more with \"Internet original\" videos - there's just so many of them, and with everyone's getting their own personalized feed, it's getting hard to find common creative works you and your conversational partners both seen. Everyone's experience is becoming disjoint from everyone else's (except for occasional superhero or wizard movie) - at which point you eventually realize that enjoying unique art no one else has is pointless waste of life. reply The_Colonel 44 minutes agorootparentprevHuman is still writing the prompt. That's your human element. The prompt can be detailed, creative, and innovative. Kinda like a composer comes up with an idea for a new piece. But now the composer won't need the technical ability to translate it into musical notation. reply vasco 5 minutes agorootparentNot sure if writing the prompt is the human element because an AI can easily write prompts. I think a stronger human element is in the training data (while that training data isn't dominated by AI generated content itself). IanCal 13 minutes agorootparentprevIf taping a banana to the wall, or eating said banana is art, then I feel like making a machine sadly sing the MIT license to you has to qualify. reply berkes 30 minutes agorootparentprevI see AI as just another tool. Like Photoshop, a software mixer, electrical powertools in my wood-working shop, a 3D printer in my office. All of these had immense impact on the way we create (or make art). And despite all this, we still use waterpaint, perform music on ancient instruments, make furniture with minimalistic tools, or use clay to make objects. I'm not pessimistic about generative AI. If anything, It'll allow more people to create. Allow new and unprecedented art forms. It will have an effect on the way people make money with art. But so did photoshop, digital audio mixers, a table saw, and a CAD/CAM machines. reply xcv123 14 minutes agorootparentprevThe difference here is that no one wants to listen to this shit. It is extremely corny and generic. Cringeworthy. Algorithmic music has already been around for decades and it never became popular. In the 90's it was of interest only to a small group of academic music nerds. The same is true today. Avant-garde shit for nerds. No one wants to listen to it. reply autonomousErwin 49 minutes agorootparentprevIsn't that the real Turing test - does it feel like it has a soul? reply thanksgiving 38 minutes agorootparentI want to run a experiment on humans to see if we are worthy of a turing test. 1. We tell a human (test subject) that they will be a judge but they are a test subject. We will tell them that there will be two chats -- one will have a human and the other will have a computer and they need to decide which is which. 2. We will then give them access to two real time chats but the twist is both of them will be humans. 3. Our test subject needs to rebel against the experiment and say they are both humans. What percentage of the population will be able to say both chats are humans? Is this a humane experiment? Will any ethics board clear it? Does it have any scientific value? reply thereisnospork 5 minutes agorootparent>Does it have any scientific value? Dunno, but it could probably get published and I'd at least read the comments when it gets posted to HN. Tricky bit is the design in the signaling and instructions so as to avoid biasing the results while still allowing the desired 'both are human' response. Something like a check box for each chat if that chat was a robot. If you have the budget I'd also run the control Turing test, human x computer, as well as a computer x computer test. reply visarga 26 minutes agorootparentprevChoose your preferred software license for lyrics if you want substance reply AltruisticGapHN 21 minutes agorootparentprevSadly you could say the same of the 80% in just about anything human-made. The 80% of software, music, furniture, etc. Maybe there will be a change of feeling, it's starting to come to me, instead of seeing this AI generated content as \"soulless\" etc I'm starting to see it as an extension of OUR human generated work. It's more like an endless remix of HUMAN talent. All of that is boring though. The exciting stuff is all that will be displaced, and how we will solve the myth or meritocracy. reply insin 1 hour agorootparentprevSuno links have already become what grey screenshots of ChatGPT were just after it came out, listened to a few at first and now I just keep on scrolling. reply hobo_mark 1 hour agorootparentprevIf you ever turn the radio on or hear the spotify weekly top playlist, it's not like contemporary cookie cutter pop music is any better. At least with this anyone can quickly experiment quirky new ideas! reply Nursie 1 hour agorootparentprevAI slop (to me) would be to pass this off as a song or something meritorious to listen to. IMHO this and things like it are basically a sub-class of comedy or satire, so I have time for this sort of thing. It's a joke, and should (can?) only be appreciated as artistic as someone saying \"Wouldn't it be funny if ... ?\" because now that casual thought can be turned into a pretty instant \"well here it is! LOL!\". I don't think you're supposed to appreciate it as music. Maybe I'm calling it wrong though. (edit - I will admit that upon further thought, I'm not sure how I feel about this when compared to, for example, Nina Gordon recording \"Straight Outta Compton\" as an accoustic, mildly lamenting singer-songwriter style number 20-some years ago. It's clearly in the same satrical arena but one took a lot more effort and imagination. Kinda, because there was a lot of effort and imagination that went into both the training data and the model, even if this specific output was only a passing joke. It's quite hard to reason about this stuff...) reply michaelt 1 hour agorootparentIn my experience, there's less of a distinction than you might think. A lot of satirical songs are absolute bangers, because (for example) to do a send-up of the tropes of X music, you must know all the tropes of X music, and be able to perform them. So a lot of satirical music is actually done by people with a lot of skill and passion for the thing being satirised. And because satirists don't have to worry about being predictable or unoriginal, they can put in more crowd-pleasing cliches per minute than 'serious' artists, giving them the most intense X of all X artists. (Not saying the MIT license is a banger though - just that some satirical songs are) reply weregiraffe 2 hours agorootparentprevArt will realign to put more value on live performances. Don't worry, people who push AI art don't understand that art is a form of communication between humans. They might learn when the bubble pops and they are left with a trillion shiny \"art\" objects that are worth nothing, because nobody wants to look at them. reply svantana 1 hour agorootparent100%. I compare it to the invention of cameras - before that you could make an honest living as a portrait painter, no inspiration needed. Afterwards, painters needed to lean into artistic qualities to stand out. But also, 'Photography' was born - what was seemingly just a press of a button turned out to be an artform. reply Satam 1 hour agorootparentprevNot totally true. Yes, there's value in live performances and human connection but most of the songs we listen don't stimulate that. Hell, often we don't even know what the musician looks like, who they are, how they sound live, etc. They're just items in our Spotify queue that are only there to give us a dopamine hit with their sequence of well-composed sounds. There's a craving for a deeper connection but that's usually the smaller part of our everyday consumption. reply piva00 51 minutes agorootparent> Not totally true. Yes, there's value in live performances and human connection but most of the songs we listen don't stimulate that. Hell, often we don't even know what the musician looks like, who they are, how they sound live, etc. They're just items in our Spotify queue that are only there to give us a dopamine hit with their sequence of well-composed sounds. If you are a little bit of a critical listener you listen to those songs because you connect to them somehow, that's the power of music, it's a language for emotions that you don't need to know how the artists look like, or their backgrounds, to feel what they try to convey. Having the context/background might help to intellectualise a piece of music but the feeling comes from the art itself. AI music is pure entertainment, not art. reply Morelesshell 2 hours agorootparentprevDepending on the popularity and your income, the experience of a artist life is really not that intim as you make it. reply weregiraffe 1 hour agorootparentThe experience of knowing there's a human behind the message is valuable. People hated corporate propaganda \"art\" before AI was a thing, for the same reason it is not human. reply erikbye 43 minutes agorootparentDepending on the AI tool in question, and the level of control it offers the human, there can definitely be a human behind the message, even if the final output is AI generated, the human had a creative vision and used the available tools to make it happen. It's no different than using Photoshop. Unless you only consider art done with oil and on a canvas to be the \"real thing.\" reply Morelesshell 34 minutes agorootparentprevI know plenty of people, including me, who has no real knowledge about those humans. And sometimes not knowing is better like the example with Rammstein and row zero were some manager woman was asking other woman if they want to meet their rockstars. Btw. on a good electronic music set, knowing the arist is probably more a quality sign than a personal aspect. Knowing that i like what arist xy does, means i might like to keep an eye on future work because i like the style of it. Nonetheless, i do also think that fandom and doing real tours will still be the unique things for bands. There should always be a market for human content. reply mixermachine 1 hour agorootparentprevIf you can pay for a live performance and have to time to go there, yes. A lot of people can't and a 5-10$ Spotify/Apple Music/... account is all they can afford (if at all). AI music will disrupt this market. Spotify for example will try to produce their own AI music (like they already do with regular music). The Christmas playlist will then mainly contain their music. That saves a lot of money for the. reply fifticon 1 hour agorootparentprevI don't quite get why this is downvoted; it may not be the absolute truth, but in my personal anecdotal experience, there is quite some truth to it. I have had a lot of fun playing around with image generation and chat gpt. But have also had the non-surprising \"Hedonistic Fatigue\" that comes with excess access to something originally valued. I have now been able to generate 4 and 5 digit numbers of pictures of awesome colourful steam locomotives and epic dungeon vistas, but now find myself fatigued by \"what on earth am I going to use 5000 dungeon pictures for?\", coupled with the dread of being forced to CHOOSE from 5000 options. And I learn the known principle, that when you can choose from 4 options, you are happy you picked the best of 4, but when you can pick from 5000 options, you are left feeling inadequate with \"I almost certainly was not able to pick the best of those 5000 options, and trying to do so would exhaust me\". So suddenly, picking something from your menu of options, feels dreadful and fatiguing.. (I get the same feeling sometimes, when trying to pick a movie to watch out of 16.000 options). So yeah, no doubt the AI sketch/refinement tool will be merged into our creative process, but for the time being, I feel a second generation of alienation-estrangement with my \"available options\". reply Shorel 57 minutes agorootparentYes, having to check 1840 options is about 1830 too many =) reply Teever 1 hour agorootparentprevI'm going to assume that like me you're not an artist, or at least not the kind of artist who can generate 5000 dungeon pictures by hand. I've been thinking about it the same as you but something that I think we're missing here is what can the person who can already make 5k pictures by hand do with this kind of tool? It makes me think of the later albums that Frank Zappa produced with a Synclavier. That guy went hog-wild on that thing and banged out some unbelievable albums. What would he have been able to do with AI generated music technology if it was the Synclavier of his time? What are the Frank Zappas of our time going to do with AI? https://en.wikipedia.org/wiki/Frank_Zappa#Synclavier_works reply hobofan 1 hour agorootparentprev\"Art\" already puts more value on live performances and scarcity. Popular music consumption is largely removed from that. Yeah, the last two live mega tours (Taylor Swift and Beyoncé) have a tad more personality than the average artist, but the usual stuff that you would hear on the radio might as well be AI generated and live-performed by animatronics and a significant chunk of the audience wouldn't care or even notice. reply dagw 42 minutes agorootparentI think the first place we will see large scale AI music replace 'real' music is in the 'lo-fi' background music that just about every bar, restaurant and shop has going in a loop. Instead of having various playlists, the restaurant owner can just choose between some styles, moods and tempos in an app and the AI will autogenerate an endless steam of background music that matches the vibe they are going for. reply Nursie 1 hour agorootparentprevSee also vocaloid 'performers' such as Hatsune Miku or Ia in Japan. reply mrfinn 1 hour agorootparentprevSorry to disagree, we already crossed the line where the \"Her\" movie could turn into a real story. To say it more clearly: It won't take a long to see people even having affective relationships with AIs. reply vsnf 1 hour agorootparentThey definitely already are, which is basically the business model of girlfriend.myanima.ai, along with a litany of similar services. reply jajko 1 hour agorootparentprevSure, some desperate lonely people have very close relationships with ie pillows. What OP says will most probably be true though - when there is ocean of cheap/free perfect AI art, it will be worthless. What will be worthy is imperfect human-created art. We already went through this decades ago with expensive hand made vs cheap machine made stuff, this is just another iteration. I am not saying it will be great to be an artist, just like in the past few famous live in limelight and most will struggle to stay afloat. reply Cthulhu_ 32 minutes agoparentprev> give this tech another year or two and it will be better quality than your average radio song. TBH that's a pretty low bar; \"radio\" songs have been engineered and polished for a very long time now. I have no hard numbers but gut feeling says radio music only represents 1% of the music industry. reply fauigerzigerk 1 hour agoparentprevWhat I find most amazing is how the music changes at the start of the all caps section. Is this a completely new interpretation of what all caps means or could it have been learned from examples? reply cornholio 1 hour agoparentprevHere's my attempt at humor, there is something about his short sentences and speech delivery that makes them very suitable for ballads: https://suno.com/song/9f731225-8844-44f1-9325-4078cf53c729 reply itronitron 1 hour agorootparentHmm, I think a rambling song form would have been a better fit. reply cornholio 19 minutes agorootparentI beg to differ, here's the nu metal version: https://suno.com/song/edea40ab-c41f-46e4-908f-a511b9ab311e I almost feel compelled to take action against the corruption and crookedness. reply mrfinn 2 hours agoparentprevAt the current rate of progress maybe it'll be months. I did a song for my company just to have fun around an event and the quality of the song was astounding, better than this one I think also using v3. I didn't even write the lyrics, I just described what I want in the prompt. Also I used one of poems I wrote as a teenager (not a lot of them) to create a song... the AI would notice the emphasis in the feelings of the lyrics, exactly in the same way I felt them, so it made pauses and chorus accordingly. reply svantana 1 hour agorootparentIt reminds me of CGI in the 90's. Everyone was so astounded over the quality of jurassic park and toy story, the rate of improvements was mindboggling. Predictions were made that in a few years, actors and movie studios would be obsolete. But in the meantime, audiences started to notice artifacts that were not apparent to the untrained eye - CGI was actually ugly! And here we are 30 years later - actors are still on sets. reply fragmede 40 minutes agorootparentbut the sets of today aren't the sets of the 90's. sets were giant green screens for a second, now they're giant TV's. they're still on set, sure, but things have changed. reply n4r9 1 hour agorootparentprevI had a similar reaction. We can still detect AI-generated images after years of progress; I think it's hasty to say they'll be indistinguishable any time soon. reply int_19h 1 hour agorootparentThing is, they don't have to be indistinguishable. They just have to be \"good enough\" to be a major disruption to that market. And it's not a particularly high bar. reply svantana 14 minutes agorootparentSure, just as tv shows use obvious CGI for animals, explosions etc - it's cheaper and more practical. Particularly background music that needs to be licensed for ads and such is very likely to be disrupted in the next few years. reply consp 2 hours agorootparentprevPretty much all mainstream songs are of the same tempo (or slight variations), the same beat, the same scale, the same accents etc. I'm not surprised at all it can be regurgitated by an generative model. What I would like to see is what happens if you ask it to do in 5/8, 5/4 or any of the 7/* variants and see how that goes (I have no idea, might actually work). There are some examples in common music but not a lot especially for the more obscure ones. Unless they also trained it on a lot of international folk songs. reply abhpro 1 hour agorootparentAlmost none of what you said is true. Same scale? Tempo? Same beat? You think artists are just all using the same instrumental track? And yes it can do odd time signatures. It can do all sorts of genres like art pop, cinematic scores, ambient, math rock, avant garde, baroque, etc. reply rootlocus 1 hour agorootparentThere's a surprising number of pop songs that use the same or close to the same chord progression. reply satisfice 1 hour agoparentprevThe song is not clever or funny. It's a rather tuneless recitation. The only reason anyone is posting it is because AI. reply chaosprint 1 hour agoprevSuno is great and I already shared my positive thought on its potential back in v2. I have always believed that the essence of digital music is \"organized numbers\". I think what needs to be thought about is how to use AI in this process. If you look at the results (numbers) generated, then we are indeed very close. But there is another future I believe: I hope AI can compose music with me, like copilot. This is why I keep working on https://glicol.org/ and the destination is: https://github.com/chaosprint/RaveForce Although my progress on AI is slow atm, I found that the copilot in VS code can already help me in live coding performances several times: https://youtu.be/xzIXzt3hSt0?si=rVihHYiKiAU5IKeI&t=389 Also want to hear your feedback. reply fragmede 51 minutes agoparentglicol is amazing, thank you so much for it reply ano-ther 2 minutes agoprevThe quality of this has become amazing. Clear voice with expression. Fitting instruments. Good mix. Creativity though? These non-poetic songs (I heard “Lorem Ipsum” the other day) are a fun novelty because a machine creates this in no time. A skilled musician will do something similar. Innovation? Deepmind showed new creative approaches. Will this happen with music too? Or will they just regurgitate styles from the past? It’s definitely good for Muzak, where you want a non-offensive, slightly upbeat, and not-too-reconizable background. And perhaps as an idea generator. reply jpalomaki 2 hours agoprevAsked ChatGPT first to convert the license to poem: https://suno.com/song/bdad5f22-a1f0-42fb-a3d8-572d289687ea \"Publish, distribute, let your dreams take flight, Sub-license, sell, under stars or sunlight.\" reply NegatioN 16 minutes agoparentThis is the first AI song I've really wanted to keep with me on my phone. If someone knows someone, we need a human performance of this! Encore, encore! reply dharmatech 58 minutes agoparentprevThat was really good reply chgs 2 hours agoprevhttps://news.ycombinator.com/item?id=39930463 250+ comments last week reply gocartStatue 1 hour agoprevObviously I had to try black metal version: https://suno.com/song/e00db515-6244-4197-ab71-c8f0555aaba4 reply adityaathalye 55 minutes agoparentUnless my ears deceive me, this render has the same mispronunciations and omissions as the OP sad jazz girl render. reply xandrius 23 minutes agorootparentSame base model, all have the same pronunciation issues. reply instagraham 45 minutes agoparentprevInside you are two creative commons licenses: sad girl LoFi and black metal reply archerx 58 minutes agoparentprevI don't like black metal but that was impressive! reply defrost 29 minutes agoprevAI-generated sad girl with piano performs the text of the MIT License 719 points|amichail|7 days ago|268 comments https://news.ycombinator.com/item?id=39930463 reply trymas 1 hour agoprevCurrent top 2 is hilarious: https://suno.com/song/c15f0251-fbac-4a30-a3e1-002dbc78cb79 reply jmmcd 2 hours agoprevWhy not sing out a few paragraphs from copyright law, just to make the two fingers to artists more explicit? reply CaptainFever 2 hours agoparentSure, we can try to make it sing section 107. reply bugbuddy 1 hour agorootparentFair is a stretch for all the ways AI companies have proceeded. reply Epskampie 2 hours agoprevThe site is full of gems. I like capybara: https://suno.com/song/b27c29f6-8ab4-47eb-81fd-efb85c848ada/ reply djmips 1 hour agoparentNeeds the Mr Weebl Flash animation to go along. reply Cthulhu_ 24 minutes agorootparentBadger, badger, badger badger, badger, badger badger, badger, badger badger, badger, badger, mushroom, mushroom! reply camillomiller 41 minutes agoparentprevGotta say, I danced to worse tunes last weekend in Berlin :) reply viraptor 2 hours agoprevWhile almost everything sounded really good and well put together, the voice itself has a constant... phaser? robotic vibrato? applied the whole time, which I found surprising. We've got better voice synthesis available already and other instruments don't suffer the same effect. I've not heard that before - does anyone know if that's specific to this model, or a more generic voice issue? reply lemoncookiechip 1 hour agoparentThat static is present in most generations, but how bad it is depends on the generation itself (you get 2 variations of the same song every generation). I've had some amazingly crisp sounding generations in vastly different genres and languages, such as Opera tenor, UK rap, reggae, metal, country, Broadway musical, rock n' roll, Japanese, Italian, Swedish, various English accents/dialects, etc... Suno is a technical masterpiece, I understand why some people dislike the idea, but the point stands that we are HERE now and we started with most people not even imagining it possible, and those who did saying it wouldn't be this good. Like many people have said, this tech will only get better. reply orthoxerox 48 minutes agoparentprevYes, it had this GlaDOS-like timbre. reply stanac 30 minutes agorootparentMy thoughts exactly, like I just finished portal. reply makeitshine 1 hour agoparentprevYou mean it sounds autotuned? reply viraptor 1 hour agorootparentNot quite. I'm not skilled in mixing enough to know the right description for it, sorry. I can hear vibrato-like modulation/beating, but in the vocal part only. reply tokai 46 minutes agoparentprevYeah, surprised at the amount of comments here about how good it sounds. The voice is full of artifacts, making it quite uncomfortable. reply vilius 1 hour agoprevI've always wanted to Billy Joel to re-release his \"We Didn't Start The Fire\" but with updated events. Well, here it is: https://suno.com/song/b8b33785-271f-48f2-9934-26dabb8e20ed reply fragmede 27 minutes agoparentFallout Boy did it better https://youtu.be/2LkVKCWL0U4 reply CrLf 1 hour agoprevIf this was an actual person performing, I would have smiled all the way to the end. Since this is AI-generated, I feel no emotion towards this, and just listened for a few seconds. It's technically interesting. reply mrtksn 1 hour agoparentSeriously, the \"creatives are screwed\" narrative has fallen apart for me because the stuff made in AI has proven to be worthless. Why is it worthless? Because the point of art was to communicate or convey something with other people and the AI has no idea because its not human. A few words or a sequence of sounds can be enough to transfer great deal of feeling and meaning because we run about the same software and as a result we can generate the same output with a little bit of input. This is all done by looking inside and externalise it, that is someone feels something and makes a song from it and that song can be used to regenerate feelings in other people. The current AI tech doesn't have a way to do that because doesn't have a way to look inside. At best, it can imitate things within some context but the output doesn't have any meaning at all. The most successful AI content was maybe the \"Pope wearing Balenciaga\" image but that wasn't because the AI thought it mean something but because someone looked inside and thought this can be interesting. So no, AI isn't taking over the creative process. AI is taking over the mechanical part of it only, that is the part where the artist traditionally had to master a method of production or an instrument. The AI evangelists keep pushing short videos or drawings that look \"professional\" and claiming that Hollywood is done, artists are screwed etc but those are worthless outside of the context that AI made it. No one is interested in paying or even spending time to consume this content, its extremely dull. reply tsukikage 2 minutes agorootparentGet a room in any hotel run by one of the large chains - Accor, Hilton, IHG... On the wall, you will find an Obligatory Art. Sometimes it's just a canvas with 3-4 stripes of paint: you can imagine a purely mechanical process for churning these out; a conveyor belt with brushes hanging over it, perhaps. Other times it's a little more creative. Each room is slightly different. You can also sometimes see these in cheap home decor shops. It may not be much, but it does the job - it really does make the space more pleasant than just blank walls would be. There are a lot of rooms to fill. Someone has to make all these. It may not be all that creative, but it sure beats working in, say, a produce packing plant. Those are the jobs at risk from generative AI. reply camillomiller 36 minutes agorootparentprevI fully agree, but that’s not what all music is. Most commercial music is pure craft created by expensive professionals that the music corporations would be very happy to swap with expendables and cheap AI models. It boils down to the economic model and the financial and political choices like in every creative industry. Regarding potential displacement, I would apply the stock photography theory to any creative industry. Ask yourself: is what I do in my creative endeavor the equivalent of stock content for the visual imaging industry? If the answer is yes, you might want to future proof your craft. If the answer is no (as in, your art is more than a simple soulless piece of easily digested and quantity-oriented content) then you will be fine in the long run after the current unsustainable hype cycle dies out. reply mrtksn 27 minutes agorootparentI think even people who are writing songs for cash are actually looking inside, they just perfected a method of doing it and can do it all the time. AI wouldn't be able to do that unless is designed to work like human and has human experience. The stock photography stuff is either documenting event or displaying low effort illustration for low effort productions. I guess AI can be good at churning Apple images for low effort Apple news. reply hackerlight 47 minutes agorootparentprev> Because the point of art was to communicate or convey something with other people and the AI has no idea because its not human. This is mixing up art with the art industry. Artists will struggle just like copywriters are struggling after the arrival of LLMs. Not everything in the art industry is trying to break new artistic ground or communicate some deep emotion to the listsener. For much of the industry, \"good enough\" will suffice if it's 10x cheaper. reply mrtksn 36 minutes agorootparentI disagree, the industry participants still need to have this ability to look inside when doing their work even if they do routine/mundane work. That's how you get people who are better or worse in their jobs. Maybe with exception of strictly technical work like people who remove background in mages or calibrate instruments. Those people are screwed yes. reply tux1968 1 hour agoparentprevWhat if you didn't know either way? Would you refuse to enjoy a song, until you were absolutely sure it was performed by a human? reply Cthulhu_ 21 minutes agorootparentIt will depend on the person, but I think generally speaking, a song is but one aspect of an artist / the art of music; if you're a mass consumer that just has something playing in the background, it probably doesn't make a jot of difference (consider also \"muzak\" / elevator music), but if you're more of an active listener you may look into and enjoy the story behind the music and the artist as well. Personally I think knowing the story behind music makes it better. The music isn't to everyone's taste, but for example Devin Townsend's wiki page / story is a trip: https://en.wikipedia.org/wiki/Devin_Townsend reply tsukikage 16 minutes agorootparentCompletely agree, but it seems to me there's a difference between \"I like this, so I'm going to find out everything I can about who made it and why because I will enjoy it even more that way\" and \"I can't possibly like this unless I know who made it and why\". reply CrLf 43 minutes agorootparentprevThat's a poignant question, but with an easy answer: If I didn't know, I'd probably enjoy it up to its imperfections. But I'd feel defrauded once I discovered. Like so many people felt defrauded when they discovered that the Milli Vanilli leads didn't actually sing, and that wasn't even AI. https://en.wikipedia.org/wiki/Milli_Vanilli Edit: I might add that I already suspect any illustration that even superficially looks it might have been generated by AI. This has ruined the enjoyment of so many people's artwork whose style has been co-opted by AI. reply jug 1 hour agorootparentprevI think we kind of already have the answer to this one. Commercial music that use computers to enhance the song or singer is already prevalent. Even straight AI assisted track generation probably already happens. People don't use to mind it for as long as they don't know. Once they do, many still don't care but some feel betrayed and think honesty and humanity is part of the art. I haven't heard of a single person who refuse to enjoy songs they don't know how they were produced, though, and I strongly doubt the parent would too. reply vmfunction 52 minutes agorootparentprevThis is the thing. Live performance is always gonna be different than something from pre-recorded or AI generated. That's why music lovers like to go to live concerts and performances. reply tsukikage 24 minutes agorootparenthttps://www.youtube.com/watch?v=scu8bz1yM4k https://www.youtube.com/watch?v=IvUU8joBb1Q https://www.youtube.com/watch?v=yoAbXwr3qkg https://mikuexpo.com/ That's the thing about art: when people make general statements like this one, others will go on to create things purely to see what's outside the box. reply satisfice 56 minutes agorootparentprevWould you refuse to enjoy food if it didn't come from a reputable source? Of course you would. You don't just eat shit at random. If a friend recited a poem, would it matter to do if they read it off the Internet or composed it themselves? Of course it would. If someone tells you they love you, does it matter if they are a robot or an honest human or a con-artist human catfishing you? YES, THAT MATTERS TO YOU. Yes you \"refuse to enjoy\" things that have suspicious sources. reply tux1968 46 minutes agorootparentI don't think those examples are equivalent at all. Food isn't just about taste, but could literally kill you. Not many people have friends who recite poetry. And as for love, that's something that takes years to meaningfully develop. But, I barely know any musical artist's name today. Most music is just something to listen to at the gym. It's pleasant enough, but I don't dig in to know who is singing at all. Every source is equivalent to me, as long as it's pleasant to the ear. Perhaps you're different. The only question is, which attitude is more prevalent? reply im3w1l 1 hour agorootparentprevI think people will gladly engage on a superficial level, but refuse to engage more deeply if that makes sense? reply comboy 1 hour agoparentprevWhat if you didn't know? You wouldn't know whether you like it until you've learned more about the artist? reply Shrezzing 1 hour agorootparentAI Audio isn't far enough along to be convincing in song (at least in this song, anyway). This song sounds like a disturbing uncanny-valley rendition of a slow Phoebe Bridges song performed by Taylor Swift using a broken auto tuner. While I think it's technically impressive that this exists at all, I think this tune is still at the stage of \"Pope in a puffer jacket\". reply weregiraffe 1 hour agorootparentprevYes, that's how communication between humans works. It is context dependent. reply jnsaff2 1 hour agoparentprevI'd be curious to hear your reasoning behind this? Why does this being computer generated ruin it for you? Auto-tune has been around since 1997 so it's not like computers have not been a big part of a lot of music we hear every day. reply mjburgess 1 hour agorootparentBecause the cause of a person singing is their mental states (desire, emotion, intention, etc.) and the cause of this generation of audio is that the words are associated with some backcatalogue of previous music. Listening to songs, as speaking with people, is in large part about enjoying the causes of the song rather than the mere variations in pitch. Beethoven's 5th even, purely instrumental, is enjoyable because of how the composer is clearly playing with you. To generate pitch variations identical to beethovens fifth makes this an illusion, one hard to sustain if you know its an illusion. It isnt an illusion in the case of the 5th itself: beethoven really had those desires. reply int_19h 1 hour agorootparentThe cause of many popular performers singing is primarily their desire to make money. It's not even some kind of closely held secret. And they still sell albums by the millions. What you describe certainly exists, but it's not the entirety of art, and I would argue that at this point it's not even most of art. reply tsukikage 1 hour agorootparentprevMeanwhile, Hatsune Miku remains popular. There are even concerts. reply komali2 1 hour agorootparentprevFor me all art I enjoy has some aspect of connection to someone that's sharing my human experience. If we get AGI, I could imagine feeling something towards the art such an entity creates, since a big part of the human experience that we would probably share with an AGI is inescapable death. But for today's \"AI\" generated music, I feel the same towards it as I would towards the random step function output of a given tool in Ableton - sounds cool, now what can we do with that to make it into music? reply mathgorges 28 minutes agorootparent> sounds cool, now what can we do with that to make it into music? So a human using the tools of sound production is what transforms the function output into music. (Please let me know if I’m misunderstanding you). I think I see what you’re saying, but that’s already happened here hasn’t it? I mean, it's not as though an AI made the decision to generate this all by itself, a human had an idea to create this piece and wrote a prompt which created this output. The order is of events is reversed from your Ableton example, but I would contend that this kind of production is no less musical than what someone could create using a DAW, simply that the tools are more accessible (and I presume there is less direct control over what the end result is going to sound like, but the same could be said of conducting an orchestra versus playing a piano.) Eta: For example, some people in this thread have complained that the AI generated voice falls into the uncanny valley. I agree, and I think that’s part of the art here. reply matsemann 1 hour agoparentprevFor me it's the opposite. This is interesting in its absurdity, the mistakes it makes, how it tries and somewhat fails to convey emotion etc. Someone making a \"proper\" song where they just sang these words would be quite boring, then I'd rather spend my time checking out other music. reply HeartStrings 1 hour agoparentprevThis. AI production loses “soul” as human empathy is a crucial component. reply rootlocus 1 hour agorootparentArt is like a lossy compression algorithm. If there is a soul of any form the only reason you think you're observing it in human-produced art is because your decompression algorithm is adding it. While I don't disagree there's a \"human touch\" to art, I'm not convinced it can't be synthesized to some degree. It may not be innovative, but I think since AI is extrapolating from learned data, it can at least mimic the current pop culture. reply m1ckey 1 hour agoprevSpongebob's Plankton AI version of Diamonds (Rhianna) convinced me that generated vocals are suitable for professional music production. https://youtu.be/9QV55xoOBQk reply chaps 42 minutes agoprevAn italian opera about becoming a bird, but with the dreadful realization that you're a chicken: https://suno.com/song/63df5758-c533-447a-be4a-06dcb5abdbbf this is absolutely wonderful. reply jll29 2 hours agoprevThis is already better than some humans, such as https://www.youtube.com/watch?v=9sJUDx7iEJw reply demaga 2 hours agoparentThis particular human is orders of magnitude better than AI reply JoachimS 2 hours agoprevAmazing. I hereby demand that all open source project stop using those puny SPDX tags and use this instead. Come to think of it, all licenses should be expressed in this way. Imagine the Magnus Opus a standard Microsoft license would become. reply Etheryte 1 hour agoparentImagine you stumble upon an interesting project on Github and instead of license.md, you're greeted by license.mp3. reply yorwba 39 minutes agorootparentDon't forget dmca.mp4 https://the-eye.eu/dmca.mp4 reply yukolonin 4 minutes agoprevThis was a triumph. I'm making a note here: \"HUGE SUCCESS\" reply ChrisGranger 2 hours agoprevMore discussion from last week: https://news.ycombinator.com/item?id=39930463 reply DreamSculptor 1 hour agoprevI remember that this song was linked a few days ago. I decided to then tinker around with this tool together with my colleagues (also MIT License text). The results were too funny to not share them in this context: Our favorite: https://suno.com/song/30c5fff7-7417-42cd-b758-699854ef06d3 The extreme bassdrop: https://suno.com/song/1485e9d0-f0fb-4083-819a-bfb9db6c066a The country: https://suno.com/song/a4307c43-0a1e-4cf2-94d7-e94a49b196d6 reply wasmitnetzen 1 hour agoparentInteresting that it messes up \"fitness\" in all versions. reply DreamSculptor 1 hour agorootparentI agree. \"sublicense\" is also pronounced wrong or in a dialect I am not familiar with. reply djmips 1 hour agorootparentIn the olden days we would rewrite the input text to a voice synth to pre-correct the incorrect pronunciations. Maybe try sub-lye-sense Or similar. reply djmips 1 hour agorootparentprevI know of no dialect that pronounces it like the Suno AI. reply Tempest1981 1 hour agorootparentprevAnd \"NONINFRINGEMENT\" reply tim-- 1 hour agoprev> Cook spaghetti in salted boiling water until al dente https://suno.com/song/4a77dea7-19f3-46d2-8b0a-b2b7e9ea9a05 reply erikbye 29 minutes agoprevSo many comments here that will age poorly. Especially that AI output won't ever be indistinguishable from human. It's imminent that we will fail blind tests in most areas: text, still images, audio and video. reply wouldbecouldbe 26 minutes agoparentI feel like music will be much easier then text. Highly complicated texts will be easier to spot inconsistencies, where with songs it's pretty common to have a high margin of errors/linguistic interpertation making it a much better candidate for current approach of generative ai. reply danesparza 24 minutes agoprev\"WARRANTIES OF MERCHANTABILITY\" is phrased in a stunning way for an AI. Between that and \"the software\" whisper, it's truly impressive. reply bandrami 3 hours agoprevSomehow it managed to translate both parentheses and all caps into musical concepts pretty well reply tailspin2019 2 hours agoparentYeah that really was uncanny. There are parts of the chorus which are genuinely catchy too. Insane stuff! reply ahmedhosssam 2 hours agoprevTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. This chorus deserves a Grammy. reply auraai 1 hour agoprevWe seem to be crawling out of the \"uncanny valley\" on the other side. Give it a couple more years, we've pretty much nailed the interpolation of human output (across conventional modalities). reply 0q 1 hour agoprevThe OP ballad really captures the lawyer-ese mumbo jumbo feeling of \"traditional\" open-source licenses, I'm quite partial to the more fun feeling WTFPL https://suno.com/song/2ac3cb96-a4d2-4b14-88d0-83479549c60d/ reply dartharva 2 hours agoprevThat was unreasonably amazing. reply thomasahle 1 hour agoprevNew test for AI singers: Make them say MERCHANTABILITY and NONINFRINGEMENT. I guess it makes sense those words weren't in the training data for a song AI. reply hybridtupel 30 minutes agoprevIt would be hilarious to include the song instead of the license text in a codebase reply nicce 24 minutes agoparentThen need to put license for the song as well… endless cycle reply wouldbecouldbe 21 minutes agoprevWanting to generate a song about a deceased cat named \"boef\" (crook), getting an error it's an artist name and not allowed, as well as using the name of a family member, also getting an error message. Wanted to make a cool personalized song for a kid. Hahaha they can never take over the music world with this pious bs. reply Nursie 20 minutes agoparentGive Udio a try perhaps. I've not tested its censoriousness, but the output seems to be well ahead. reply prmoustache 2 hours agoprevI would say humans paved the way by making the use of autotune so trendy that hearing a \"metallic\" voice doesn't make anyone cringe anymore. reply gabrieledarrigo 38 minutes agoprevTechnically impressive, but it makes me feel so lost, hopeless, and ignorant. reply wiihack 41 minutes agoprevAbsolutely incredible. Other than pictures and videos, this awakens my excitement :) reply itronitron 1 hour agoprevI wonder if this will encourage writers to adopt standard poetry forms in their writing so that it can more easily be adapted to AI-generated songs. reply FabHK 1 hour agoprevThe last part was so sad that she couldn't bring herself to sing it (\"ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\") reply AlexDragusin 1 hour agoprevThe Spaghetti! https://suno.com/song/4a77dea7-19f3-46d2-8b0a-b2b7e9ea9a05/ reply BadBadJellyBean 2 hours agoprevIt got so close to the end of the text. reply davedx 2 hours agoprevBeats Eurovision! reply hongsy 3 hours agoprev(corrected twitter link to link to app.suno.ai) reply adityaathalye 58 minutes agoprevCurious... Do the mispronunciations and omissions of the OP \"sad jazz girl\" render match those of the \"black metal\" render posted in comments [1] by @gocartStatue? [1] https://news.ycombinator.com/item?id=39999605 (And I'm using \"render\" deliberately, not rendition.) (edit: slight text improvement) reply mgaunard 1 hour agoprevisn't the pronunciation wrong for sublicense and noninfringement? reply ta8645 1 hour agoparentHumans aren't perfect. It's what gives us our unique little quirks. Same as AI at this stage of its growth. It won't make such silly little mistakes tomorrow. reply deadbabe 1 hour agoprevCan someone give an idea of how much it would have cost to have a human record something like this? It’s cool but it feels like songs are cheap and easy to make as is if you’re not hiring some big shot. reply ksdk 2 hours agoprevbanger reply tunnuz 2 hours agoprevAw. reply louwrentius 2 hours agoprev [–] We can now replace more humans for shareholder value. Edit: Remember 1984 where Orwell described machines composing music for the masses? reply Madmallard 2 hours agoparent [–] music seemed really low quality to me as a musician. I wouldn't pay to listen to it reply kombookcha 1 hour agorootparentI doubt generative AI music is going to become popular as anything but background noise, but I think it will plausibly cut into various commercial uses of music in ads and whatnot. So while I don't think people are gonna be walking around with playlists of AI slop, businesses will be using it as a cost-effective option, which means it will be eating away at the gig potential for artists in the low-to-mid layers of the food chain. Which sucks, but is thankfully less grim than everyone just preferring the AI slop for some reason. reply fragmede 10 minutes agorootparent“I think there is a world market for maybe five computers.” Thomas Watson, president of IBM, 1943 reply macns 1 hour agorootparentprevI bet there's plenty of other non-AI tracks you wouldn't pay to listen to reply nextaccountic 1 hour agorootparentprev [–] Who pays to listen to anything? reply npteljes 41 minutes agorootparentWhat do you mean? Access is one of the valuable things, not to mention the equipment the listening is done by is also usually bought. reply Etheryte 1 hour agorootparentprevA vast amount of people pay for Spotify, Apple Music, Youtube Premium etc, it's a huge market. reply xxs 54 minutes agorootparentprev [–] Aside the online part which is massive on its right own - live performances/concerts/festivals, etc. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussions revolve around AI-generated music's impact on the music industry and art, with debates on whether AI lacks soul or is a natural progression in music creation.",
      "Concerns are raised regarding oversaturation, decision fatigue, and the future of live performances in the industry due to AI's increasing influence.",
      "The debates underscore the challenge of balancing technological advancements in music creation with preserving the human touch in artistic expression and the disruptions AI may introduce to the industry."
    ],
    "points": 397,
    "commentCount": 168,
    "retryCount": 0,
    "time": 1712815292
  },
  {
    "id": 39991693,
    "title": "Kobo Unveils Color E-Readers with Kaledio Technology",
    "originLink": "https://www.theverge.com/2024/4/10/24124411/kobo-libra-colour-clara-colour-e-reader-kindle-e-ink",
    "originBody": "Tech/ Gadgets/ Books Kobo announces its first color e-readers Kobo announces its first color e-readers / The Kobo Libra Colour and Kobo Clara Colour are up for preorder now and will be available at the end of April. By Sheena Vasani, a writer covering commerce, e-readers, and tech news. She previously wrote about everything from web development to AI at Inside. Apr 10, 2024, 4:40 AM UTC Share this story If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Image: Rakuten Kobo Rakuten Kobo is launching its first color e-readers, the Kobo Libra Colour and the Kobo Clara Colour. Both use E Ink’s latest Kaledio color screen technology, which has subtle, pastel-like hues and drops from a 300ppi grayscale resolution to 150ppi when you view content in color. Related Color E Ink and Android are an exciting, excruciating pairing I’ll be testing both e-readers soon, but so far, they look like small upgrades to Kobo’s existing e-readers. That’s not a bad thing, though! The seven-inch Kobo Libra 2 is my favorite e-reader outside of Amazon’s ecosystem, offering the Kindle Paperwhite’s IPX8 waterproof design but with extras like physical page-turning buttons, no lockscreen ads, and more storage. The Kobo Libra Colour comes with physical page-turning buttons and is compatible with the Kobo Stylus 2 for taking notes. Image: Rakuten Kobo The Kobo Clara Colour is just like the Clara 2E but with color, an improved processor, and more storage. Image: Rakuten Kobo The $219.99 Kobo Libra Colour retains all of those features but is also now compatible with the Kobo Stylus 2, just like the Kobo Elipsa 2E. However, it’s $30 more expensive than the Kobo Libra 2, and you’ll have to buy the stylus separately for $69.99. The $149.99 Kobo Clara Colour is slightly more distinct from its closest sibling, the $139.99 Kobo Clara 2E. It offers the same six-inch display and IPX8 waterproof design but now comes with 16GB of storage, as well as an improved processor. I hope so; the Kobo Clara 2E’s sluggish performance was one of my chief complaints. Kobo also introduced an upgraded black-and-white Kobo Clara BW, with the same storage and processor upgrades, for $129.99. All of the devices are available to preorder starting today and will ship on April 30th. Most Popular Most Popular This is the new Sonos app, coming May 7th Kobo announces its first color e-readers With Vids, Google thinks it has the next big productivity tool for work Consumers will finally see FCC-mandated ‘nutrition labels’ for most broadband plans AMC Theatres’ top brass has misgivings about the Dune popcorn bucket Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=39991693",
    "commentBody": "Kobo announces color e-readers (theverge.com)370 points by lxm 18 hours agohidepastfavorite190 comments teroshan 18 hours agoKnow that while useful in some scenarios, I'd avoid buying a color e-ink display if you're still going to primarily be using it for B&W colors. To quote the eBook Reader Blog [1]: > The main drawback with color E Ink screens is they look darker than regular B&W screens because of a color filter layer that is applied over the top of the screen, which makes the contrast appear lower. Kaleido color screens are really just regular black and white E Ink Carta screens with a fancy passive filter over the top (that’s why color resolution is lower than black and white resolution). [1]: https://blog.the-ebook-reader.com/2024/04/05/color-e-ink-5-t... reply gnicholas 17 hours agoparentIt's true that the contrast is not as good, and IIRC the color resolution is also a bit lower (but is not bad overall). But people who plan to read B/W documents could still benefit from a device like this because it makes highlighting much better, and it enables color-based reading enhancements like BeeLine Reader (which improves focus while reading). I'm certainly biased (I created BeeLine), but IMO the color screens that Kobo is using are good enough to make the tradeoffs worthwhile. It's also interesting to see innovation among one of the major players in this space. The companies that have made color ereaders before now have been a bit dodgy in some cases, so Kobo will bring a level of legitimacy to the space that's very welcome. EDIT: the price is also very favorable compared to other color ereaders. Ones I've considered in the past were around $420, which is roughly 2x-3x the price of Kobo's new units. reply jsheard 16 hours agorootparent> It's true that the contrast is not as good, and IIRC the color resolution is also a bit lower It's generally halved, i.e. the display is 300dpi in mono mode, but 150dpi in color mode. reply rahimnathwani 12 hours agorootparentHalving the DPI in each direction (vertical and horizontal) means you end up with only a quarter of the pixels: 0.4 megapixels instead of 1.6 megapixels. reply gnicholas 11 hours agorootparentIIRC these panels are 150 DPI color, which means it's less than halved in each direction. reply rahimnathwani 9 hours agorootparentI thought they were 300dpi panels, but that the dpi was halved in color mode? I'm curious what color filters they have. Perhaps each 2x2 pixel has 3 filtered dots (RGB) and one fully transparent dot? reply JZL003 13 hours agorootparentprevbeeline readers works on kobo? Via colorizing a pdf -> kobo, or is this is just something planned? reply gnicholas 11 hours agorootparentColoring a PDF for now...and I'll certainly be working on more streamlined options now that this is available! reply mguerville 16 hours agoparentprevI have a Boox Ultra Tab C (color) and while true I don't find it problematic in the slightest. I went down the rabbit hole of watcing video comparison etc. but in practical terms once I started using it I never once felt that I wanted a brighter/lighter screen. YMMV. reply dboreham 9 hours agorootparentAlso have the Note Air 3C (same screen iirc). Still getting uses to it (never have used eink before) but so far soon good. I don't think I'd want to switch to monochrome even if the contrast is better. reply schmiddim 2 hours agorootparentOT: How happy are you with the Note Air 3C? How long last the battery? Any Problems with the build quality? I've read a lot bad stuff about the device and have mixed feelings reply WolfeReader 17 hours agoparentprevI've used a Pocketbook Color for over a year now. And yes, it is a somewhat darker screen, but that mainly just means I turn on the frontlight in some cases where I wouldn't have needed to with other e-readers. I've otherwise been having a good time with it - comics and PDFs benefit greatly from the color screen, and regular text is fine too. reply sireat 12 hours agorootparentAgreed, I picked up Pocketbook Color for 150 Euros on a sale and it has become my main bedtime reader. Disclaimer: I've been using e-ink based readers for over 10 years. My current lineup is Kobo H20, Remarkable2 (which I sort of love/hate), and Sony DPT-S1 (which is fantastic for bigger PDFs). B&W is fine on color e-readers. I actually enjoy the washed out color look as well. reply AstralJaeger 4 hours agorootparentprevFully agree here, I'm in the PocketBook Color squad since 2020 and got it at full price and absoltuley love the thing. Appart from the fact they don't sell official covers for it anymore since i seem to disassemble those regularly. reply Rebelgecko 10 hours agoparentprevDoes that mean these have better refresh rates than other color eink screens? Lower contrast is not the end of the world for me, but a 15 second refresh time is :) reply gnicholas 10 hours agorootparentYeah, the refresh times are much better now. They used to be 10-30 seconds but are now manageable. I think it might be a little slower than B/W, definitely not 30 seconds. Some color eink displays even offer video, with different settings for higher fidelity/slower frame rate versus lower-fidelity/faster frame rate. reply thisislife2 17 hours agoparentprev> Kaleido color screens are really just regular black and white E Ink Carta screens with a fancy passive filter over the top... Yeah, and I remember an interview where they also said that is why colour e-ink's screen are not at all costly to manufacture. But adoption is slow because they sell it at a \"premium\" price. Even B&W e-ink can be a lot cheaper. reply malfist 15 hours agoparentprevHow can an article about the contrast issues and resolution of different e-readers not include any photos other than a single promo photo from a manufacturer of one device showing only one device? \"has lower contrast\" means absolutely nothing without being able to quantify it. reply askvictor 9 hours agoparentprevAlmost sounds like Pixel Qi, the display tech for the olpc project. Did anything behind of that tech? reply ianburrell 8 hours agorootparentThis is eink while Pixel Qi was transflective LCD what is sometimes called e-paper. Transflective displays are used for some smartwatches, Garmin ones have long battery life with them. reply greggsy 15 hours agoparentprevIt looks blinding in the promo shot reply zamadatix 13 hours agorootparentAnd with a dead flat black bezel too. Likely the promo pic is not how it would look to your eyes in real life. reply dustincoates 17 hours agoprevI have very little to add about the color aspect, but I can say that I love my Kobo. I've had three Kindles before buying the Kobo, which I got because it has a \"better\" integration with my local library (better in quotes, because I still have to do with the Adobe DRM, which is so bad on Linux that I borrow my wife's computer just to transfer). The Pocket integration is fantastic, as are the physical page turn buttons. I always thought they were unnecessary, but I really do prefer them now. The warm night light is also great. It's a bonus as well not to be in the Amazon ecosystem more than I have to. The biggest downsides are maybe a bit niche: the dictionary is terrible (although you can upload your own, but not make it the default) and there's no built-in translate function. I read mostly in my non-native language, so these two features make life a bit more difficult, but they're outweighed by the rest. reply bondarchuk 15 hours agoparentAdobe DRM on linux recently got a lot better: https://superuser.com/a/1775619 Copied for posterity: --- It's now possible to do this 100% within Linux, without running any emulators or Windows software, even though Adobe don't care about Linux support. Knock is no longer maintained, however apparently it was just a wrapper around libgourou which is still maintained. Installing libgourou (on Arch Linux it can be found in the AUR) allows you to download the ACSM file to a PDF or ePub: # Use your username and password from https://account.adobe.com # This registers your device so only needs to be done once. adept_activate -u user -p pass # Download the ACSM file acsmdownloader -f myfile.acsm The downloaded file requires a password to open it, but if you need to open it in a normal viewing application, you can also remove the password: adept_remove file.pdf This process allows Linux users to access the same materials as their Windows and Mac friends, even without support from Adobe. --- reply globular-toast 14 hours agorootparentI figured out how to remove Adobe DRM on Linux just to be sure I can. But, to be honest, I just circumvent the problem entirely these days. reply a1o 9 hours agorootparentSomething about a service problem. reply mcpherrinm 17 hours agoparentprevJust to join in the kobo love thread: One reason that I love my Kobo is perhaps very tech-specific in a way this audience may appreciate. It's great for plain text files. In particular, it's pretty good at reading plain RFCs. I have to choose a font size so the hard-wrapped RFCs fit right, and it doesn't ship with a monospaced font (but you can install and use your own fonts, so that's a minor speed bump). Reading very dry and technical documents like RFCs can be challenging, so having a distraction-free ereader that handles them can be very helpful. reply fuzztester 15 hours agorootparentThe Kobo software cannot reflow the hard-wrapped lines? reply mcpherrinm 14 hours agorootparentIt wraps them, but doesn’t fully reflow. That especially doesn’t work for tables, diagrams, etc which many RFCs are full of. I think it’s best to just match the width. reply fuzztester 14 hours agorootparentOkay, got it, thanks. reply amaccuish 16 hours agoparentprevSame journey here; except using the Calibre Web integration, an absolutely gamechanger. No need to plug-in to sync my own books, they're pulled straight from my Pi. And I love how all that was required was editing a line in the config file on the storage; they seem like fairly open devices. Also glad to get away from the Amazon proprietary formats. reply parlortricks 12 hours agorootparentIs there an artile explaining how this is done? Im more interested in the whole setup. reply msmithstubbs 11 hours agorootparentThis would be a good place to start: https://brandonjkessler.com/technology/2021/04/26/setup-kobo... There is also a number of home brew readers and utilities available for the Kobo readers. Plato is a reader written in rust, and covered in this post: https://blog.videah.net/my-e-reader-setup/ reply parlortricks 10 hours agorootparentThank you, that first link was great, setup looks very easy to do. reply kstrauser 16 hours agoparentprevI have a Libra 2 for personal reading and an Elipsa 2E for journals. A couple of minor nits like you mentioned aside, I love them. They can read almost any format I throw at them, I can fetch books directly from my library, and if I add an article to Pocket on my laptop then it shows up on my Kobo next time I pick it up. I also read way more now that I've gotten the Libra, largely because it's so light and ergonomic for laying in bed at night and flipping pages. I added a popsocket to the back for easier 1-hand holding. The Elipsa is better for reading large content like Communications of the ACM or similar, where the Libra was simply too small to be readable. I hardly use the pen for either highlighting or notes, but it's there if I want it. If/when either of those die, I'll look at the color versions as potential replacements. It highly depends on how much of an effect the lower resolution has on readability. If it's less pleasant to read at night, or if PDF journal articles are a little fuzzier, I'd rather stick with B/W. If the only downside is that the screen is a bit darker so I have to turn the backlight up, then only get 2 weeks between charging instead of 4, I might consider it. reply ornornor 16 hours agorootparentFWIW, we bought an arm to hold the kobo up while we’re reading (one of those smartphone holder things, they’re a dime a dozen) and a little device that clips onto the kobo. It comes with a remote, and you can turn the pages with it by making the kobo think you’ve touched the display with your finger. Best 50$ I ever spent (arm + clicker). Now I can turn pages while in bed without holding the kobo and with both hands under the cover. It’s the little things :) reply mrks_hy 16 hours agorootparentPlease link or provide a product name, I've been looking for something like that! reply lasr_velocirptr 8 hours agorootparentIf you are up for a little diy, you can get the libra working with a bluetooth remote that you may already have https://github.com/tsowell/kobo-btpt reply kstrauser 7 hours agorootparentI know what I’m going with my Flipper Zero tonight. reply ornornor 16 hours agorootparentprevI can’t remember :( the arm is a generic phone holder arm with a spring loaded clamp that pushes down the top and bottom of the device. The page turner was one of these keyboard smash brands. Searching for “ebook wireless page turner” yields quite a few results. reply kstrauser 16 hours agorootparentprevThat. Is. Amazing. I know what I’m ordering today! reply fuzztester 15 hours agorootparentprevWow, interesting. Which Kobo model would you recommend for both reading and creating notes that can be saved as text? I am considering getting one for my software development work, so that I can read on the go, while sitting, or in bed, and also jot down short and medium length notes about my project ideas, during my daily walk (in the rest break). Walking helps me to think better. A laptop is too heavy to carry around everywhere, and particularly when walking. reply kstrauser 15 hours agorootparentDisclaimer: Not a Kobo employee; don't have deep knowledge of their product line; just own and happily use a couple. The Elipsa 2E is very nice for large format reading and note taking. I bought it recently and compared it to other options from Boox and reMarkable. I ended up with it because I already had and enjoyed my Libra 2, and \"like the Libra 2 but bigger and with a stylus\" was appealing. That said, if I wanted something for your use case, I'd probably start with the Sage. The short version is it's a Libra 2 with a stylus, and that seems like it'd be a good combo. Somewhat related, I tried some non-electronic options for such things before, like a Rocketbook. Clever idea but I didn't like the implementation at all. Its OCR couldn't deal with my handwriting at all. I now have 2 distinctly separate note taking systems: - My Elipsa 2E for writing down meeting notes (while avoiding the distractions of an iPad or laptop, which is important for me personally but other people might not care about at all). - A paper journal (Traveler's Notebook) I take everywhere and use for my personal thoughts. - I lied. Number 3: \"hey Siri, remind me ...\" for capturing ideas while I'm out and about, and then I follow up with them when I get home. That works so well with AirPods, too. If I have an idea while I'm out jogging, I can ask Siri to remind me about it without breaking stride. First time I did that it felt like Star Trek. reply fuzztester 14 hours agorootparentThanks for the detailed answer. I'll check out the Sage. That last paragraph about Siri was cool :) reply kstrauser 13 hours agorootparentYou bet! I use the Siri approach all the time. I'm not going to stop running long enough to catch my breath and dictate a novel into the thing, but \"Hey Siri, remind me that cats can eat pancakes\" is enough to let me resume the thought when I'm back home and resting. reply gorky1 15 hours agoparentprevThere's an open source Adobe ADEPT DRM implementation that works well on Linux: https://forge.soutade.fr/soutade/libgourou reply wood_spirit 16 hours agoparentprevMore kobo love :) After aching eyes from being a heavy reader of ebooks on my iPhone, I got the cheapest most basic kobo - basic even by the standards of the day when I brought it - and instantly fell in love. Haven’t needed to change default software, haven’t needed to enable WiFi etc. just need a usb cable and calibre app and it all just works great. My fiction addiction is through the roof. Never realised how big a deal getting a kobo is. reply AstralJaeger 4 hours agoparentprev> as are the physical page turn buttons. I always thought they were unnecessary, but I really do prefer them now. I felt that one, the reason I love those buttons is, because that way I can keep my display clean as I don't have to touch it. reply chme 16 hours agoparentprevI owned a Kobo Glo and Pocketbook Touch, and I preferred the Pocketbook, because it allows to install koreader as a application into the stock firmware, while on Kobo you need to quasi dual-boot your system into koreader or stock firmware via a menu. In my experience koreader integrates much better with calibre and allows managing my library with a filestructure. So that is a requirement for me. Also the stock firmware on the Pocketbook was able to deal with my huge epub archive, because it didn't require indexing the library on the device, which Kobo struggled with. From that experience I would pick a Pocketbook over a Kobo. But maybe I should give Kobo another look. reply aeturnum 16 hours agorootparentIMO Kobo is absolutely built to be a \"better managed library experience\" than kindle - but as you say other platforms are better if you have a large non-drm'ed library. Unless you plan to buy books from publishers / use the overdrive (now Libby) system, I do think a more user-focused offering like Pocketbook will be better. reply criddell 15 hours agorootparentI think this is a place where some regulation could do a world of good. I wish I could buy any ereader I want and shop at any bookstore I want. I wish publishers were forced to respect first sale doctrine on digital goods. reply aeturnum 12 hours agorootparentI fully agree! I'm also disappointed (but not surprised) that kobo (or some other 3rd party epub seller) hasn't created an API where you register a key pair (or whatever) and can decrypt books on your devices. I suspect it's based on publisher demands. Ironically, this is one place where a blockchain might be useful - sales are recorded on it, contracts are written with respect to it, and if the publisher goes belly up users can run nodes to maintain it. reply dillydogg 10 hours agorootparentprevThere are multiple ways to install Koreader now. I think the most common is to use KFMon and NickelMenu which installs the quasi-operating system into a book in your main menu. reply crtasm 11 hours agorootparentprev> on Kobo you need to quasi dual-boot your system into koreader or stock firmware via a menu. Is this info up to date? It was not the case with our 2E - an option to start koreader just got added to the menu. reply 6gvONxR4sf7o 12 hours agoparentprevThe only reason I've held off on switching to Kobo is that I always switch back and forth between the audiobook and the text for what I'm reading. Kindle books make it seamless, but as far as I can tell, there's no audio-text sync in the Kobo ecosystem. Is that still accurate? Even if you use books bought from amazon/audible? reply gnicholas 10 hours agorootparentFYI Kindle is in the process of rolling out a new feature that enables read aloud with synchronized text highlighting. It doesn't sound as good as an audiobook, of course, and it's not as good as the Alexa-powered read-aloud. But it's still nice because you don't have to switch between the Kindle app and the Alexa app. I understand that some users currently have access, and it will be rolled out to everyone in the next month. reply latentcall 13 hours agoparentprevI purchased a Kobo Libra 2 and it is indeed fantastic. Battery life is incredible, and syncing with the Libby app for books from my library is fantastic. I do wish it could integrate with Raindrop as opposed to Pocket, but the Pocket works okay enough. Can transfer .pdf or .epub from Linux/Mac/Windows easy, and it looks great on the device. reply nocoder 7 hours agoparentprevI am on a 10 year old Kindle and looking to replace it but most of my ebooks are from Amazon. Would it be seamless to transfer my books to Kobo via Calibre or some other software or it will be pain in the butt? reply japanuspus 3 hours agorootparentFrom family with a mix of kindles and kobos: yes, this is painless. You can check beforehand by installing Calibre with the de-DRM plugin and transferring a few books: Once they are on Calibre you are good to go. One thing I only learned today is that you can set up web sync from Calibre to Kobo [0], [1], but just plugging in to sync has been working fine for me. For the initial sync, it used to be you could directly de-DRM your whole collection from the storage of the kindle desktop app. At some point the desktop app was using a DRM that had not been broken: I do not know if this is still the case, or if there is another way to download everything from your account. [0]: https://news.ycombinator.com/item?id=39996455 [1]: https://brandonjkessler.com/technology/2021/04/26/setup-kobo... reply kbrackbill 9 hours agoparentprevre: the library integration, I'm confused about how you or your wife's computer is even involved in the process. For my kindle I borrow books on libby and amazon magically beams them over. Does your library not use libby? Are you taking extra steps to avoid amazon? reply dustincoates 5 hours agorootparentI'm outside the US, so no Libby or Overdrive. reply globular-toast 14 hours agoparentprevI have koreader on my Clara HD. Unfortunately it doesn't work well on Clara 2e and perhaps later devices too. Means I have to stick with Clara HD for now (which is fine). Hopefully there will be something available if/when I need to replace it. reply crtasm 11 hours agorootparentI don't have a HD to compare but it seems to work fine on the 2E. reply ryzvonusef 22 minutes agoprevIf you want to learn more about e-ink ereaders/tablets etc, I suggest this youtube channel: https://www.youtube.com/@goodereader/videos ____ My problem with the e-ink devices is that they still require the same chips/battery/etc as any other tablet, but provide a much limited display to use... so you spend the same money for less. What we need is some sort of dual display system, maybe a display that can switch between OLED and E-Ink layer... or literally have two displays (Colour E-Ink on the back maybe) so that one can the same processors/storage etc and not have a \"waste\" (also fewer devices to carry). reply ashton314 17 hours agoprevI've had a kindle for years now. A while back I jailbroke it and stuck KOReader which has been so awesome. (Custom fonts? Yes please! Gestures for going to the TOC or bookmarks or back in history? Absolutely!) Problem: it was a lot of work getting my kindle to this state. I'm terrified to turn on WiFi for fear Amazon will send an OTA update and nuke KOReader from orbit. So, when this kindle dies, I am thinking I'd like to get something that KOReader runs on without any of the shenanigans I had to go through. Would folks here recommend Kobo? Seems like a nice equivalent. reply teroshan 17 hours agoparentHave been using various Kobo's for years with KOReader now, it's basically as simple as copying files on the device to install it. There is no need to jailbreak it; it's an open device. Also, I haven't heard any mention of covert update to worry about. So yes, I would absolutely recommend a Kobo for your use case. A few months ago, I attempted to do install KOReader on a kindle, and the experience is definitely not for the faint of heart. Plus, I can't use the wireless capabilities for the reason you mentioned. It was (one) of the reason I wanted to install KOReader in the first place, but live and learn. reply m463 11 hours agorootparent> it's an open device. non-obvious but you just use it like a usb device ereader when you receive it: - connect to computer with usb-c - mount as usb drive - edit .kobo/Kobo/Kobo eReader.conf adding SideloadedMode=true - remove .kobo/KoboReader.sqlite - reboot, then just use kobo as usb drive from then on, no account necessary got one and use it more than my remarkable because it has a backlight for indoors (in addition to e-ink for outdoors) reply grep_name 15 hours agorootparentprev> it's an open device I actually also own a kobo and have been running KOReader on it for a couple years now, but my memory is that when I first got it I booted it up and it requested to be connected to the internet, then wouldn't allow me to progress without logging in through a 3rd party (there were many offered, walmart.com was the first option listed). I remember having to do something weird to bypass that (removing a .db file I believe after connecting via commandline?), and then ending up in a state where I don't need to connect to some company's cloud login but I also can never connect this device to wifi, ever, or it will do that again. Are they more open these days? reply slim 15 hours agorootparentyou had to connect it as a usb drive to insert a row for a fake user in it's sqlite database reply grep_name 14 hours agorootparentI remember that now. It really is frustrating that this is what passes for a really open device in the e-reader space. I would pay a 3x markup easily for the same device running KOReader natively (not sideloaded through nicklmenu after starting up the proprietary operating system) if it would just let me use the device like I own it without authenticating with a 3rd party just to its most basic functionality. reply crtasm 11 hours agorootparentYou don't need to authenticate with anything and it's even easier to do since a firmware update - just add a line to the config file: https://old.reddit.com/r/kobo/comments/mt2f30/how_to_bypass_... Connecting it to wifi afterwards isn't a problem, to tbe best of my knowledge. Firmware downloads: https://pgaskin.net/KoboStuff/kobofirmware.html reply kstrauser 16 hours agoparentprevI love my Kobos (see https://news.ycombinator.com/item?id=39993062). When they die, I'll replace them with another Kobo. It's so easy to get things onto them. For my Libra, plug it into a computer (or iPad) with a USB cable and it shows up as a drive I can drag files onto. The Elipsa supports Dropbox and Google Drive so I can add files those ways, too. It also has built-in Libby integration that lets me reserve and download library books from the device itself. Of course they make it very easy for you to buy books from their own store, but I've never felt like they made it artificially hard to ad content through other methods to benefit their own book sales. That's different from what my friends tell me about their Kindles. reply selykg 16 hours agorootparentI have an Oasis 2 (2017 or so timeframe). What would be a good option that's similar to it? I love the screen size, the flush screen (I understand the Oasis is glass rather than plastic), and the page turn buttons are all pretty important to me. KOReader sounds interesting and the built in Libby support is kind of rad sounding. reply kstrauser 15 hours agorootparentI think that'd be the Libra 2. The screen's the same size, although not flush, and it has the nice page turn buttons. I can't vouch for their other models. KOReader sounds cool but I've been happy with the built-in software. I know me, and I specifically don't want other knobs I could be playing with instead of just reading the thing. reply selykg 15 hours agorootparentGot it, thanks! It seems it's between two devices, the Libra 2 and the Sage, but I keep hearing the Sage has worse battery life (sort of the downfall of the Oasis for me as well). I just like using Calibre since I get a lot of epubs from places like HumbleBundle and other book bundles. These always have issues with the Kindle, like covers going missing. I'd love to get something a little less fickle for my use case and it does sound like the Kobo devices might get me there. I just really like the Oasis from a materials, size and general usage. reply kstrauser 15 hours agorootparentThe Sage looked nice but the price premium for a stylus wasn't worth it to me for a device I wanted to use purely for reading. Obviously there's a market for such a thing so I'm not saying it's bad, just that it didn't might my own needs. Calibre works well for the Libra 2. I have a plugin that automatically reformats epubs as kepubs (see https://wiki.mobileread.com/wiki/Kepub). That's not a hard requirement but it improves the reading experience a little bit. The biggest difference I notice is that it makes page numbers handle font size changes more accurately. If I halve the number of words that'll fit on the screen, the reader shows that the number of pages in the book has doubled. Also, the current page number increments every time I turn a page, not just when I cross an invisible page boundary. It's not a dealbreaker not to have those things, but they're nice and worth the 30 seconds to enable in Calibre. reply selykg 14 hours agorootparentNice. I think I might keep an eye on potential rumors of a Libra 3 or something like that given that the Libra 2 came out in 2021, that's nearly 3 years old. But that gives me time to save a few buckets as well. For now the Oasis is working well enough, just it'd be nice to get away from some of the paper cuts. reply kstrauser 13 hours agorootparentSame here with me and the color versions here. If and when I need to replace what I have now, or maybe a software update makes me want to throw them out a window or something, then I'll take a look at the new options. It's highly unlikely I'd get rid of my currently working and mint condition and known-good readers before that. reply sgtnoodle 14 hours agorootparentprevI liked my Kobo, but it bricked itself after only about a year. I took it apart, and a voltage rail was shorted out. I didn't even finish the first book I put on it! reply kstrauser 11 hours agorootparentOuch. I'm a couple years into my Libra. I'd be seriously bummed and annoyed if it died that quickly. reply MOARDONGZPLZ 12 hours agorootparentprevI read at a similar pace. reply rrix2 17 hours agoparentprevThe KO in Koreader was originally for Kobo though ofc now it can run elsewhere. :) I started Using it on a Kobo Aura but now I use a Boox Onyx which just runs the Android version of Koreader installed via f-droid or side loaded APK. reply flkiwi 16 hours agoparentprevI have a Boox note. I can’t help the feeling that it’s not entirely legit, but it’s Android, connects to any service I want (including a Calibre library), has a great display, is great for notes, and is all around fantastic. It’s not a tablet replacement but it IS a replacement for the stuff I do most often with a tablet (read and write). I thought the 10.3 inch display would make for a hefty reader, but it’s perfectly fine. reply criddell 13 hours agorootparentThe not entirely legit part might be their GPL violations. AFAIK, they still are out of compliance. reply flkiwi 16 hours agorootparentprevBtw, I started out using it with KOreader but have ended up just using the built-in reader because it does everything I need. The smaller Boox devices are great too, but their USB plug placement decisions are sometimes VERY WEIRD (my Note Air has a hole in the case spine to allow me to plug in with the cover closed, for example—not a dealbreaker, just odd). reply seanmceligot 16 hours agorootparentprevI love the black and white boox as a low tech device. I do a lot more reading and a log less mindless scrolling on my boox tablet. There's also grayscale chrome and firefox plugins if you want to see what it's like. reply flkiwi 15 hours agorootparentSame. I almost never use the browser in the first place. I think I’d say the same thing you did but changing the focus to: I love that the b&w boox has made me understand that a low tech device serves my most important needs. reply paradox460 14 hours agorootparentprevI have a Note as well, and I use it for sheet music. With MobileSheets and a foot pedal, its a phenomenal device for both practice and performance reply WolfeReader 17 hours agoparentprevKobo is good. I'm also fond of Pocketbook readers, and KOReader runs there too. The main difference is that Kobo has its store available, so you can buy and read on the device. I'm more into managing my own ebook library from multiple sources, using Calibre, so I don't need the built-in store; you might want it though! reply magarnicle 5 hours agoparentprevThe original Kindle Touch had gestures. It was also faster for dictionary lookups than my current Paperwhite, which is over a decade newer... reply jwells89 17 hours agoparentprevI’ve been happy enough with my Kobo Aura One in its stock state that I’ve never toyed with KOReader or similar. Plug it in, it mounts as USB mass storage, I drop in ePubs, PDFs, etc and it reads them perfectly. No third party software required, “just works”. reply evanreichard 7 hours agoparentprevFYI I've been running multiple jailbroken Kindles connected to WiFi with KOReader for a few years now. Just install renameotabin and you're good to go. You can even register it like a normal Kindle. reply xtracto 9 hours agoparentprevI've got a Boox Note Air2 and it's amazing. Android, can send books through bluetooth, has a shop (that I've never used) and can install Android apps. Also the screen seems huge and the weight is pretty good. I even got it with 25% discount :-D reply kelipso 16 hours agoparentprevI have had koreader on my Kobo Aura One for years without any problems. I haven't needed to mess with it or update koreader or anything which is nice, don't even remember how I installed it. reply AlotOfReading 17 hours agoparentprevI migrated from Kindle to kobo many years back to keep physical buttons. It's been a strict upgrade for me, though your fear of an update nuking koreader from orbit remains an issue. I just keep mine in airplane mode to avoid updates and a reinstall is only a few minutes if I forget. reply jerojero 16 hours agoparentprevI was in the exact same position as you. I had a kindle voyage which I jailbroken to put Koreader on. Because of the micro USB and lack of water proofing I decided to upgrade (the device was around 8 years old). I got a Kobo Clara 2E, installing Koreader was very easy and works very well. Absolutely recommend. reply DavideNL 14 hours agoparentprevI'm in the same boat too... One thing i'd absolutely need on a new (non-color) device is the light sensor like my current Kindle Oasis(3?) has. The Pocketbook devices all lack one (they only adjust lighting based on time and such...) reply Brian_K_White 16 hours agoparentprevI've been very happy with koreader on my Aura H2O since... 2014? Wow. So that's a yes. reply perihelions 17 hours agoprevBleh, and I was just up until 1am last night reading about black-and-white PDF compression schemes for my Kobo :) Technical knowledge obsoletes so fast. (Well, maybe not that fast: ImageMagick's best-performing monochrome scheme is, apparently, one that was standardized for literal fax machines in the 1980's [0]. A very basic one called \"Group 4\"). [0] https://en.wikipedia.org/wiki/Fax#Compression reply AdmiralAsshat 16 hours agoprevKobo enthusiast, and I'm excited to see that they are the first to market among the Amazon/B&N/Kobo crowd to get a color e-reader to market. I don't see myself getting one, though, at least not immediately. I'm perfectly happy with my Kobo Sage. While I'd like to one day have a color e-ink device, the color reproduction just isn't good enough for my use case. Reading comics on my AMOLED tablet is a joy, and I don't see e-ink getting to that level for at least another couple years. Plus, I don't want to take the hit of worse PPI for B&W text just so I can have a book cover or some highlighted text in color. reply christkv 16 hours agoparentMy main reason for thinking about getting one is because i like to sit on the terrace with a lot of indirect sunlight and eink is just so much better in that situation. However prices are still way too high and it’s impossible to find a place to see them in person before buying. reply mark_l_watson 6 hours agoprevThis looks good, but I probably won’t get it. Just today I bought on Kobo the works of Michel de Montaigne, a natural philosopher nestled between the Stoics and later philosophers like Ralph Waldo Emerson and German philosopher Friedrich Nietzsche. This book I just bought has some color illustrations so I also opened the book on my iPad using the Kobo app, and while I enjoyed seeing the color illustrations, not a big deal. All that said, I like being a Kobo customer, and I really like that my local physical bookstore gets a little money each time I make a Kobo purchase. reply legohead 13 hours agoprevBeen wanting a new e-reader. The power button on the bottom of the Kindle has driven me insane. How somebody could actually design something so stupid boggles the mind. reply chadk 6 hours agoparentSame! reply propter_hoc 13 hours agoprevÀ friend of mine has a Boox color e-reader and absolutely loves it. There's a lot of skepticism out there (I was a skeptic myself before she bought it) but it's honestly quite impressive. She made a post on reddit here: https://www.reddit.com/r/Onyx_Boox/comments/18ns9z3/three_da... It has some pictures - the color really does add something. Glad to see Kobo doing this too. reply futureshock 14 hours agoprevE Ink has been developing a full color tech they call ACeP. This tech has no color filter like Kaleido 3 so can run at full resolution and better contrast. Wonder if this tech will reach ereaders soon. Rumors are that Amazon is considering ACeP. reply mikepavone 9 hours agoparentThere was one on the market, but not from the well-known names: the Bigme Galy. Doesn't seem to be available anymore though and Bigme's site has no mention of it reply AshamedCaptain 14 hours agoparentprevAnd a refresh rate in the tens of seconds. reply futureshock 13 hours agorootparentYes, current ACeP displays have an extremely slow refresh and low resolution. They are sold exclusively for digital signage. Potentially in the next few years, according to rumors there may be a version suitable for ereaders. reply paradox460 14 hours agoprevPosy has an excellent video, full of great macro shots, of how color E-Ink works: https://www.youtube.com/watch?v=1qIHCUWAgh4 reply tiltowait 17 hours agoprevI've wanted a color Kobo for years, but I'm a bit disappointed. The main benefit of a color eReader is for (some) PDFs, comics, etc., all of which benefit hugely from a larger screen than the Libra's 7\" display. I hoped Kobo's first color reader would be at least 8\", and preferably 10.3\". Carrying all my RPG PDFs on a color eInk display would be super cool, as I find reading on such a device much easier on my eyes than an iPad. I think the argument for color is much weaker on a book-sized reader. You get the library and sleep screens (which are admittedly cool to have in color!), but the rest of your usage will mostly not benefit. Kobo's PDF software isn't great, though there are third-party readers you can install that improve the experience. No matter how good those are, though, they still benefit greatly from a larger display. Presumably, this is just the first step. IIUC, the Libra 2 is their most popular reader, so I suppose it makes sense. Just a bit disappointing. reply AlotOfReading 17 hours agoparentColor PDFs and comics almost always assume good, full gamut color reproduction. e-ink can't do that. It can only do a few pastel colors well. The typical books usage is much more in-line with the capabilities of the display. Highlighting is substantially improved with even a little color, as are interstitial art and cover icons. reply Brian_K_White 16 hours agorootparentAny color at all would be invaluable for diagrams, schematics, graphs/plots and other reference material where different subsystems, pipes/cables, datasets, etc are distinguished by color. reply mitthrowaway2 17 hours agorootparentprevWill it be good enough for, say, graphs? A lot of scientific papers and news articles have illegible figures when reproduced in black and white, because you can't tell the lines apart. reply AlotOfReading 16 hours agorootparentDepends on the graph and newspaper. The best case scenario is a desaturated version of old print newspapers. Their DPI was near this screen's DPI (200 vs 150) and it'll suffer from the same problem of losing low contrast details. You won't be reading rendering papers, but it'll probably help a basic line graph. reply carlosjobim 17 hours agorootparentprevThe E-Ink Spectra 6 displays [1] released last year has full color gamut, thus beating any other display technology in an outdoors scenario. The problem is that they take something like 12 seconds to refresh and I haven't seen any consumer devices using them. In the near future I think spectra displays will be excellent for readers when they manage to get the refresh a bit more speedy, while ironically the B&W displays with color filters will be the E-Ink technology suited for tablets and phones in general. [1] https://youtu.be/U7V2EFAcbtU reply TillE 16 hours agoparentprevI suspect the iPad will remain the best option for stuff like RPG books. It's relatively cheap, the screen is large, it's fast and responsive, etc. It's not pleasant for reading in a dark room, but otherwise it's close to the ideal. reply pxc 16 hours agoparentprev> The main benefit of a color eReader is for (some) PDFs, comics, etc., all of which benefit hugely from a larger screen than the Libra's 7\" display. I hoped Kobo's first color reader would be at least 8\", and preferably 10.3\" Same. I'm visually impaired, which means I use large print for my reflowable content. I'd love to read comics or PDFs of print magazines, but for that I'll need a display that's larger than what those are normally printed on! Afaict there are no decent vendors offering genuinely large color e-readers— just some GPL violators' whose devices run Android. If I can't get a super-magazine-sized e-ink display for fixed-layout content, the next best thing would be a very large one that I can put in landscape mode and fit to width, scrolling only vertically. That wouldn't be too bad, especially if refresh speed is okay-ish. For that, though, I'd still prefer something larger than my Clara 2E (which I love). reply mariusor 15 hours agoparentprevTheir 10' model is already over 350EUR in price, adding a colour screen to it, it would make it too expensive to have mass appeal. I suspect the next models, which will probably be a Sage 2 and an Elipsa 3 will be what you want, after they whet the appetite of regular readers. Hopefully they'll have a next iteration of the colour screens also. :D reply carlosjobim 17 hours agoparentprevThere are other makers that have larger color e-ink displays, such as Boox. reply eiiot 10 hours agoprevI have a Kindle Oasis right now that I want to get more use out of, but also can't stand using Amazon products (and have found the Oasis increasingly frustrating as of late, especially as I download more content from non-Amazon sources for my classes). Can any Kobo owners speak to how much of an improvement it is over the Oasis (or any other more recent Kindle)? Should I pre-order the Color or just get one of their existing models? reply yoavm 17 hours agoprevJust a reminder that that many Kobo devices can run PostmarketOS pretty well. I wrote this interface especially for my Kobo Clara HD: https://github.com/bjesus/air reply sriacha 14 hours agoparentAre you doing anything cool to take advantage of running PostmarketOS? I've thought for a while it would be nice to have a simple writing environment on my clara HD with an external keyboard. There's also this interesting project: https://github.com/Quill-OS/quill reply yoavm 13 hours agorootparentI used it a few time to VNC into a virtual screen to have my laptop terminal on an e-ink screen. It can even run Firefox, though very slowly. Gopher websites work way better. It's a complete Linux system and most things work just fine! reply summm 14 hours agoparentprevYes, but also a reminder that those older, well supported ones are Freescale i.mx. After that, Kobo used Allwinner from sunxi (who have a general history of GPL violations) and now something from Mediatek (who had a history of not providing good driver update support). Those are likely completely different, might need a lot of new work to properly get a kernel running on, let alone mainline. Who knows if the bootloader can even be unlocked this time? reply jerojero 16 hours agoprevI do like 6 inch e-readers, I think I would have to see IRL how the screen looks. For my usage color is not necessary, I don't even read manga on my device because I haven't found a good reader. I had bought an android based one for this purpose but I didn't like how the screen looked. I guess I'm fairly picky. I think the only reason I'd get a colour one is simply to see book covers in color. It's minor but it might be worth 30 extra euros. Then again, I'd have to see how different the screen looks. reply bityard 14 hours agoparentI wouldn't mind an even smaller screen. I read fiction on my phone in bed to fall asleep at night. In order to do that, I have to lay on my stomach and hold the phone with one hand. I've tried it with my 6\" Kobo Clara 2E and it's just a little too awkward to be usable. Maybe if it had some physical buttons on it somewhere, but it doesn't. reply nottorp 14 hours agorootparent> In order to do that, I have to lay on my stomach and hold the phone with one hand. Why? I mostly lie on a side when reading and prop the reader (or the paper book) with the bed sheets. This has the potential to turn into a reading positions subthread :) reply bityard 9 hours agorootparent> This has the potential to turn into a reading positions subthread :) Wish granted! If I lie on my side for more than a minute or two, my hips ache. Only a little at first, but if I ignore it and fall asleep, I will wake up in writhing agony 30 minutes later. Been like this since my 20's. Reasonably healthy otherwise so I don't know what the deal is. Otherwise, yes, this would be the ideal reading position. Extra credit: I can only FALL asleep on my stomach, but I can only CONTINUE sleeping on my back. Which means almost every night, I somehow flip myself over without waking up. reply nottorp 2 hours agorootparentOh I'm the kind that rotates while sleeping like i'm getting roasted on a spike. Same while reading in bed. But I alternate between both sides and lying on my back. Speaking of unknown conditions, when I was younger I was mostly reading on my stomach... with the book on the floor and half my face outside the bed to look down. Until I got my eyes seriously examined and they found my (from birth) astigmatism, I corrected that and started to read with the book closer like a normal person. Have those hips checked, perhaps? reply shrimp_emoji 9 hours agorootparentprevI use a mechanical arm mounted to a nightstand. reply paradox460 14 hours agorootparentprevOnyx Boox makes a reader that's roughly the size and dimensions of a modern cell phone https://onyxboox.com/boox_palma reply bityard 9 hours agorootparentOh, that's interesting. Pretty much exactly what I'm after! It has better specs than my phone. Says it runs Android, so I assume it's reasonably \"open\". reply paradox460 56 minutes agorootparentMore or less. There are ways to flash arbitrary android builds on Boox devices, but you'd lose a lot by doing so. The Boox software is actually pretty good. It does a lot to make non-e-ink optimized android apps work well on an e-ink screen, and lets you tweak it as you see fit, so you can adjust contrast and whatnot to make text and elements work. Its not always flawless, but it does work. The in-built reader application is very good, particularly when it comes to annotations and whatnot. It has a dedicated notebook feature, but I've never been one for hand-written notes, so I don't really use it. And if you don't like the built in one, you have the advantage of being able to load any android reader application you like. Google Play Books, Kindle, Nook, KOReader, MoonReader, whatever, they all work. Now, in the spirit of full disclosure, I have the larger, A4 sized Onyx Note, and use mine primarily for sheet music. But it works very well as a technical paper or programming book reading device, in a way that kindles and other smaller readers are insufficient. So some of my experiences might not carry over to the Palma. But I can't imagine Boox going through that much differentiation of their product SKUs to the point where this comment will be wholly accurate. reply seam_carver 5 hours agorootparentprevAn alternative to the Palma is the Hisense A9. https://www.youtube.com/watch?v=dvO9ScTdwz8 reply criddell 16 hours agoparentprevI like 6” for fiction and general reading, but I really want a 13” (A4 or letter) for textbooks and PDFs. I think Sony used to have one, but discontinued it. Right now I’m using a 13” iPad Pro and it’s fine, but I’d rather have a simple ereader with an eInk display and no apps. reply radarsat1 16 hours agoprevI want something like this but just to hang on the wall to display graphs. Why are e-ink displays only ever focused on ebooks, I think there are lots of cool non-ebook applications for this technology but to use it you either have to build something with a soldering iron and a raspberry pi or hack an ebook's OS. I don't understand why there aren't more special purpose e-ink devices for other uses like monitoring, price displays & restaurant menus, etc. reply joker99 15 hours agoparentInkplate used to have exactly what you're looking for. Basically took the eink screens from old kindles, combined with an esp32. I have 3 of them hanging around my house showing various graphs, dashboards, pictures or other items of interest :) Edit: actually still available! https://soldered.com/product/soldered-inkplate-6-6-e-paper-b... reply gnicholas 16 hours agoparentprevYou can get displays like you're describing, but pricing isn't great. I was surprised by the price levels that Kobo is offering, which is probably a mix of huge volume discounts and aggressive pricing to make a splash in the market. reply hinkley 9 hours agoprevIs Kobo Libra supposed to be a play on the \"Cuba Libre\" beverage? reply unsupp0rted 12 hours agoprevWhat's stopping Kobo (or somebody else) from making a calm color e-ink smartphone? Calm as in no animations, b&w + a couple bright accent colors, say like red for icon badges. People seem to like the b&w e-ink smartphones, but a color one would be a homerun. reply seam_carver 5 hours agoparentWell, it's one thing to make an ereader. It's an entirely other thing to make a smartphone. It's not trivial, that's why Onyx released a phone-sized reader instead of a phone. Hisense made color e-ink smartphones but they didn't make a color model of their latest Hisense A9. reply dboreham 9 hours agoparentprevBoox make a kind of smartphone device. It doesn't have the WAN, so wifi only but it's phone form factor. reply OnionBlender 12 hours agoprevWhich e-ink reader do people suggest using for textbooks? I had a Kindle DX but the screen has lines through it and the 3G connection no longer works (and no wifi). reply seam_carver 5 hours agoparentKindle Scribe? reply hgyjnbdet 17 hours agoprevI still have the kobo mini, the pinnacle of ereaders IMO, perfect size for reading fiction. I haven't seen anything more compelling since kobo discontinued it. That being said I wonder if the color eink display would be useful for displays instead of LCD. reply ravetcofx 17 hours agoparentI'm with you on that, I got mine back in 2013(?) on sale for $30 CAD. still works, but battery is a bit weak. They were also neat cause you could telnet into the modified debian os to add you own homebrew apps reply hgyjnbdet 17 hours agorootparentI didn't know you could do that! I've only been using it as intended. reply ornornor 16 hours agorootparentOn most (all?) kobos, you can also upgrade the storage to whatever you want because it’s a microsd card slotted into the board. Just copy the system partition over to the new sd, expand, done. Mind you, I never needed to do it because 4 or 8GB is a ton of ebooks. I’ll probably never need that much space anyway. But if you’re reading larger files, might be handy. reply AdmiralAsshat 13 hours agorootparent> On most (all?) kobos, you can also upgrade the storage to whatever you want because it’s a microsd card slotted into the board. Nah, that's no longer universally true. Pretty sure the Sage and Elipsa use soldered eMMC storage instead of micro SD. reply ornornor 13 hours agorootparentThat’s a bummer, it was a neat feature. reply AdmiralAsshat 13 hours agorootparentIt was, especially since I consider that a hindrance to using the newer devices for 10-years, which the older ones can happily do with a properly-imaged new SD card. Even though I love my Sage, it makes me consider whether I should grab a Libra 2 if I ever see it on the cheap, just to have a comparable device that still has the memory on a SD card. The Kobo's that just launched claim they will have better repair-ability through a partnership with iFixIt, so maybe we'll see how that impacts the ability to change the storage. reply nottorp 14 hours agorootparentprev> because 4 or 8GB is a ton of ebooks But if they go colour, how many comics is it? Although 6\" is too little for comics unless someone makes the effort to make every panel a separate screen on the ebook... reply ornornor 14 hours agorootparentYes that’s a different story. I was talking about my own use case which is EPUBs that are typically a couple MB per book. reply crtasm 11 hours agorootparentprevMy old Kobo was sold with a 1GB partition on a 2GB card, fastest upgrade ever! reply timvdalen 18 hours agoprevNice, those look very interesting! I've been very happy with all of the Kobo's I've had, so I can definitely see myself upgrade to the color Clara when my current one fails. reply downrightmike 10 hours agoprevCan't wait until we can make photo frames out of these for $20/ea reply codetrotter 12 hours agoprevI have an old Kobo Glo HD that I bought in 2015. The battery no longer holds a charge, so I opened it and removed the battery. Since a lot of Kobo fans seem to be ITT, does anyone know where I can buy a replacement battery for my Kobo Glo HD? I mean, obviously I could randomly pick one from AliExpress like https://www.aliexpress.com/item/1005003176213860.html But I would like recommendations about specific sellers, from fellow Kobo users who have bought from those sellers and replaced the battery in their own Kobo. reply spxneo 15 hours agoprevhow far are we from seeing magazine quality e-ink with animation? the colours always look bland and animations still isn't very good. I don't know why progress here is so much slower than other tech reply dvfjsdhgfv 17 hours agoprevMy experience with ebook readers is as follows: I read mostly scientific PDFs, with formula's, graphs, etc. First I tried the Kindle - it was completely useless because of screen size. I went for PocketBook Inkpad Lite - the page size is more or less fine, but the page turn delay is noticeable. It's so subpar to the real book when you can just browse through pages quickly. Finally I picked up the biggest Chinese tablet I could find (Doogee T20 Ultra 12\") and it is the best experience so far except one thing the screen is glossy, not mat, and it affects my reading experience. E-ink ebook readers are perfectly fine for prose etc. reply lostlogin 16 hours agoparent> the screen is glossy, not mat Would a screen protector fix that? Not ideal but it might be an option. reply amelius 16 hours agoprevGive me high refresh rates before giving me color. What I don't get is that people say that e-readers are easy on the eyes, but you still get this epilepsy-inducing flicker all the time when changing pages. reply otherme123 16 hours agoparentYour ereader might be broken. I don't see any flickering when page turning, other than obviously replace current text for the next. Some ereaders take a bit longer than others to change the text, but even then no flickering. reply tiltowait 13 hours agoparentprevI haven't seen an eReader that does a full refresh on each page turn in years. Kobo, Amazon, and B&N are all per-chapter by default. Maybe some bargain-bin brands still use the old displays or have bad software? reply user_7832 15 hours agoparentprevMIP (memory in pixel) LCDs might offer what you're looking for. They're popular in watches like Garmins and some Amazfits. reply babypuncher 13 hours agoprev> Both use E Ink’s latest Kaledio color screen technology, which has subtle, pastel-like hues and drops from a 300ppi grayscale resolution to 150ppi when you view content in color. Sounds like color accuracy might be poor, which means comics might not fare so well. reply system2 15 hours agoprevI had major overchoice anxiety on their website. Why do they need to create 20 e-readers looking nearly identical? And why are color ones cheaper than the black and white ones? https://us.kobobooks.com/collections/ereaders reply hollow-moe 15 hours agoprevcan't wait to see third party android e-readers with that tech reply gnicholas 10 hours agoparentDon't those already exist? reply garyfirestorm 18 hours agoprevlooking forward to reviews reply noemit 18 hours agoprevi have too many devices to get an ereader. reply dustincoates 17 hours agoparentI thought the same thing, and then bought a Kindle on a whim, as I was flying internationally with just a small backpack. The ereader completely changed how and how much I read. Before, I would find time to read a book maybe every couple of months. Now, I read almost every night before bed until I start to drift off. I read much more, and, I think, have better sleep as well as I'm not staring at such a bright screen at night. reply WolfeReader 17 hours agoparentprevI have too many devices, but my e-reader is one I enjoy using much more than the others. In contrast, I stopped caring about tablets years ago when my Google Nexus 7 finally ceased working; it was fun, but everything I did on it was also easily doable via phone, computer, or e-reader. reply noemit 16 hours agorootparentyou probably dont draw reply WolfeReader 15 hours agorootparentThat's correct, and I definitely get that tablets would be good for artists. Although I also know several artists, including a full-time professional, who don't use tablets to make their art. Unless you count drawing pads which attach to PC as \"tablets\", I guess. reply ethanbond 17 hours agoparentprevThe Supernote A5X is the only device that has managed to wiggle its way into my daily life alongside my MBP and iPhone. A big e-reader like the Supernote is amazing not just for reading books but as a searchable journal/planner that you always have with you. reply rokkamokka 17 hours agoparentprevTwo hundred books or one e-reader though? Easy choice in my book (ha) reply paulcole 15 hours agoprevI've bought the Remarkable 2 and probably 10 Kindles over the past decade. I have never once been like, \"You know what this eink device needs? Color!\" I get that it might be a draw for some people but this just isn't appealing to me and I can't see the mass market use case for color besides color being neat. reply tsunamifury 15 hours agoprevThis is like optimizing a horse and carriage for modern era. The price is nearly that of an iPad mini with vastly fewer applications. reply quicksnap 15 hours agoparentHere's a current-day horse carriage that costs as much as a cheap engine vehicle: https://www.carolinacarriagesuperstore.com/product-page/bran... Same price, fewer applications. reply myself248 8 hours agorootparentAnd runs a lot longer on a battery charge. reply EGreg 16 hours agoprevSo what? I went on Amazon and I see tons of color e-readers already reply rvz 15 hours agoparentI tried to tell everyone about color e-readers 4 years ago but apparently, \"thE rEquIRed haRdWarE technOLogY doEs noT EXisT\". [0] https://news.ycombinator.com/item?id=24296880 reply EGreg 14 hours agorootparentI guess some people would rather downvote inconvenient truths than update their point of view based on facts reply tinix 18 hours agoprev [–] there have been color ereaders for over a year now... reply mikestew 18 hours agoparentAnd when Ford launches a new model, do you respond, “we’ve had cars for over a hundred years now…”? It’s a product announcement, I see no claim that Kobo invented color readers. reply otherme123 17 hours agoparentprevHow is this different? Kobo is a mass seller of ereaders, they sell what are IMHO the only real contender to Kindles. They also sell ebooks, so they are very interested in making content for color ereaders. Before I clicked the link, my though was \"OK, another color ereader for 700€, for the headlines\". But they are color ereaders for 150€! For the first time ever, I'm very tempted to buy one, even knowing that I don't need it. reply gnicholas 9 hours agorootparentI was also pleasantly surprised by the price. I think these are similar to existing ereaders, except that the reliability and support expectations are different for Kobo. I've been watching as other companies have rolled out devices and none of the companies are as reliable as Kobo. There have been botched releases, delayed Kickstarters, etc. I would not hesitate to get one of these and not worry about warranty issues or other fly-by-night company problems. Kobo will stand behind these devices, and if there's a problem it will be easily refunded. I wasn't willing to risk $450 on a no-name company, but I'll happily plunk down half that (or less) for one of these devices! reply presbyterian 18 hours agoparentprevNot from Kobo, though, which is what this article is about. reply AshamedCaptain 14 hours agoparentprev [–] For over 20 years, actually. I still have one from the 2010s and frankly I find that the new Kaleido screens practically improve _nothing_ over it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Kobo introduces its first color e-readers, the Kobo Libra Colour and Kobo Clara Colour, featuring E Ink’s Kaledio color screen tech, physical page-turning buttons, more storage, and enhanced processors, shipping in late April.",
      "Kobo Libra Colour, priced at $219.99, is compatible with Kobo Stylus 2, while Kobo Clara Colour, priced at $149.99, boasts 16GB storage and a faster processor. Moreover, an updated black-and-white Kobo Clara BW is available for $129.99."
    ],
    "commentSummary": [
      "Kobo has launched color e-readers with lower contrast and resolution than black and white screens, catering to users interested in reading documents, comics, and PDFs.",
      "Users share positive experiences with Kobo e-readers for technical documents and academic journals, engaging in discussions comparing Kobo, Kindle, and other e-reader brands.",
      "Discussions also cover preferences for larger displays, compatibility with third-party software like KOReader, color reproduction concerns, and users transitioning from Kindle to Kobo for reading manga and technical books on e-readers with color displays."
    ],
    "points": 370,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1712762307
  },
  {
    "id": 39992817,
    "title": "Sonauto: AI Music Creator with Enhanced Control",
    "originLink": "https://sonauto.ai/",
    "originBody": "Hey HN,My cofounder and I trained an AI music generation model and after a month of testing we&#x27;re launching 1.0 today. Ours is interesting because it&#x27;s a latent diffusion model instead of a language model, which makes it more controllable: https:&#x2F;&#x2F;sonauto.ai&#x2F;Others do music generation by training a Vector Quantized Variational Autoencoder like Descript Audio Codec (https:&#x2F;&#x2F;github.com&#x2F;descriptinc&#x2F;descript-audio-codec) to turn music into tokens, then training an LLM on those tokens. Instead, we ripped the tokenization part off and replaced it with a normal variational autoencoder bottleneck (along with some other important changes to enable insane compression ratios). This gave us a nice, normally distributed latent space on which to train a diffusion transformer (like Sora). Our diffusion model is also particularly interesting because it is the first audio diffusion model to generate coherent lyrics!We like diffusion models for music generation because they have some interesting properties that make controlling them easier (so you can make your own music instead of just taking what the machine gives you). For example, we have a rhythm control mode where you can upload your own percussion line or set a BPM. Very soon you&#x27;ll also be able to generate proper variations of an uploaded or previously generated song (e.g., you could even sing into Voice Memos for a minute and upload that!). @Musicians of HN, try uploading your songs and using Rhythm Control&#x2F;let us know what you think! Our goal is to enable more of you, not replace you.For example, we turned this drum line (https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;uoTKycBghUBv7wA2YfNz) into this full song (https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;KSK7WM1PJuz1euhq6lS7 skip to 1:05 if impatient) or this other song I like better (https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;qkn3KYv0ICT9kjWTmins - we accidentally compressed it with AAC instead of Opus which hurt quality, though)We also like diffusion models because while they&#x27;re expensive to train, they&#x27;re cheap to serve. We built our own efficient inference infrastructure instead of using those expensive inference as a service startups that are all the rage. That&#x27;s why we&#x27;re making generations on our site free and unlimited for as long as possible.We&#x27;d love to answer your questions. Let us know what you think of our first model! https:&#x2F;&#x2F;sonauto.ai&#x2F;",
    "commentLink": "https://news.ycombinator.com/item?id=39992817",
    "commentBody": "Sonauto – A more controllable AI music creator (sonauto.ai)369 points by zaptrem 17 hours agohidepastfavorite197 comments Hey HN, My cofounder and I trained an AI music generation model and after a month of testing we're launching 1.0 today. Ours is interesting because it's a latent diffusion model instead of a language model, which makes it more controllable: https://sonauto.ai/ Others do music generation by training a Vector Quantized Variational Autoencoder like Descript Audio Codec (https://github.com/descriptinc/descript-audio-codec) to turn music into tokens, then training an LLM on those tokens. Instead, we ripped the tokenization part off and replaced it with a normal variational autoencoder bottleneck (along with some other important changes to enable insane compression ratios). This gave us a nice, normally distributed latent space on which to train a diffusion transformer (like Sora). Our diffusion model is also particularly interesting because it is the first audio diffusion model to generate coherent lyrics! We like diffusion models for music generation because they have some interesting properties that make controlling them easier (so you can make your own music instead of just taking what the machine gives you). For example, we have a rhythm control mode where you can upload your own percussion line or set a BPM. Very soon you'll also be able to generate proper variations of an uploaded or previously generated song (e.g., you could even sing into Voice Memos for a minute and upload that!). @Musicians of HN, try uploading your songs and using Rhythm Control/let us know what you think! Our goal is to enable more of you, not replace you. For example, we turned this drum line (https://sonauto.ai/songs/uoTKycBghUBv7wA2YfNz) into this full song (https://sonauto.ai/songs/KSK7WM1PJuz1euhq6lS7 skip to 1:05 if impatient) or this other song I like better (https://sonauto.ai/songs/qkn3KYv0ICT9kjWTmins - we accidentally compressed it with AAC instead of Opus which hurt quality, though) We also like diffusion models because while they're expensive to train, they're cheap to serve. We built our own efficient inference infrastructure instead of using those expensive inference as a service startups that are all the rage. That's why we're making generations on our site free and unlimited for as long as possible. We'd love to answer your questions. Let us know what you think of our first model! https://sonauto.ai/ dwallin 14 hours agoI think the problem here is the same one as the other current music generation services. Iteration is so important to creativity and right now you can't really properly iterate. In order to get the right song you just spray and pray and keep generating until one that is sufficient arrives or you give up. I know you hint at this being a future direction of development but in my opinion it's a key feature to take these services beyond toys. I think it's better to think of the process of finding the right song as a search algorithm through the space of all possible songs. The current approach just uses a \"pick a random point in a general area\". Once we find something that is roughly correct we need something that lets us iteratively tweak the aspects that are not quite right, decreasing the search space and allowing us to iteratively take smaller and smaller steps in defined directions. reply Barneyhill 13 hours agoparentYep, I came to similar conclusions w/ text-to-audio models - in terms of creative work the ability to iterate is really lacking with the current interfaces. We've stopped working on text-to-audio models and are instead focusing on targeting a lower-level of abstraction by directly exposing an Ableton environment to LLM agents. We just published a blog today discussing this - https://montyanderson.net/writing/synthesis reply nomel 8 hours agoparentprevSame with text models, for me. If I can't edit my query and the AI response, to retry/keep the context in check, then I have trouble finding use for it, in creation. I need to be able to directly influence the entire loop, and, most importantly, keep the context for the next token prediction clean and short. reply skybrian 6 hours agorootparentLetting you edit the response is quite easy to do, technically speaking. It's not done in the default UI for most AI Chatbots, unfortunately. You will need to look for alternative UIs. reply zaptrem 14 hours agoparentprevOur variations feature coming very soon is exactly this! Rhythm Control is an early version of this. reply dwallin 14 hours agorootparentI'll keep an eye out for that! The variations feature in Suno is a good example of what not to do here, as it effectively just makes another random iteration using existing settings. I think the other missing pieces I've found are upscaling and stem splitting. While existing tool exist for splitting stems exist, my testing found that this didn't work well in practice (at least on Suno music), likely due to a combination of encoder-specific artifacts and the overall low sound quality. Existing upscaling approaches also faced similar issues. My naive guess is that these are things that will benefit from being closely intertwined with the generation process. Eg when splitting up stems, you can use the diffusion model(s) to help jointly converge individual stems into reasonable standalone tracks. I'm excited about the potential of these tools. I've definitely personally found uses cases for small independent game projects where a paying for musicians is far out of budget, and the style of music is not one I can execute on my own. But I'm not willing to sacrifice on quality of results to do so. reply zaptrem 13 hours agorootparentOur variations feature will be nothing like Suno's (which just generates another song using the same prompt/lyrics). Since we use a diffusion model, we can actually restart the generation process from an early timestep (e.g., with a similar seed or even parts of the existing song) to get exactly what you're looking for. reply throwup238 13 hours agorootparent> Our variations feature will be nothing like Suno's (which just generates another song using the same prompt/lyrics). That's their \"Remix\" feature which just got renamed \"Reuse prompt\" or something. Their extend feature generates a new song starting from an arbitrary timestamp, with a new prompt. It doesn't always work for drastic style changes and it can be a bit repetitive with some songs but it doesn't completely reroll the entire song. reply SubiculumCode 14 hours agorootparentprevI uploaded a bit of a song that I recorded once (that I wrote, unpublished), and I am trying to get it to riff on it, generate something close to it, etc. reply SubiculumCode 14 hours agorootparentprevMore strength does what? More or less similar? reply zaptrem 13 hours agorootparentMore strength = force rhythm more. If you crank it to max it will probably result in just a drum line, so I prefer 3-4. reply p1esk 10 hours agoparentprevBut that’s not a problem when listening to Spotify? Why can’t we treat these music generation engines the same way we treat music streaming services? reply darby_eight 10 hours agorootparentIdk what you're referring to specifically, but music discovery services are terrible across all of spotify, apple music, google music, tidal, etc. I don't expect these services to read your mind, but they also don't ask for many parameters to help with the search. Definitely a huge opportunity here for innovative new services. reply zaptrem 8 hours agorootparentTikTok can tolerate a lot more active skipping than Spotify can before they annoy their users. We’d love to solve this. How would you? Maybe we could let users write why they didn’t like the song in natural language since we understand that now. reply ctrw 12 hours agoparentprevBasically you need something like comfy UI for music. Variation in small details is fine, but you need control over larger scale structure. reply garyrob 14 hours agoprevMy hobby is songwriting. (Example: https://www.youtube.com/watch?v=Kjng3UoKkGk) I play guitar, but I'm not much of a guitarist or singer. I really like songwriting, not trying to be polished as a performer. So I intermittently look into the AI world to see whether it has tools I could use to generate a higher-quality song demo than I could do on my own. I've been looking for something that could take a chord progression and style instructions and create a decent backing track for a singer to sing over. But your saying \"Very soon you'll also be able to generate proper variations of an uploaded or previously generated song (e.g., you could even sing into Voice Memos for a minute and upload that!)\" is very intriguing. I mean, I can sing and play, it just isn't very professional. But if I could then have an AI take what I did and just... make it better... that would be kind of awesome. In fact, I believe you could have a very big market among songwriters if you could do that. What I would love to see is this: My guitar parts are typically not just strummed, but involve picking, sometimes fairly intricate. I'm just not that good at it. It would be fantastic to have an AI that would just take would I played and fix it so that it's more perfect. And then to have a tool where I could say, \"OK, now add a bass part,\" and \"OK, now add drums\" would be awesome. reply maroonblazer 9 hours agoparentIf all you're looking for is polished backing tracks, why couldn't Band in a Box serve that function? https://www.pgmusic.com/ reply garyrob 9 hours agorootparentIt could, but I want it to be even easier and with better results! I think AI has that potential. I am absolutely sure it does, in fact, and that some AI product will obsolete Band In A Box within the next decade. Maybe within the next year. If the people who make BIAB aren't working on it, themselves, with full focus, they are making a big mistake. reply maroonblazer 7 hours agorootparentI don't have any connection to anyone at PGMusic but BiaB already implements a technique that could be described as AI or AI-like. Having played music nearly all my life, songwriting included, and soaked up almost every bit of music-making tech in the process, I'd wager we won't see AI delivering better results more easily and, importantly, with the flexibility of Band in a Box within the next year. The playing/performance part of making music is a solved problem. You can do this with DAWs and plug-ins today. The truly hard part is coming up with the ideas. That's where AI has an opportunity. reply garyrob 6 hours agorootparentThe problem I have with BIAB is my songs often have very specific fingerpicking parts. BIAB can't easily do the same picking as far as I can tell. (Or maybe at all?) So I'm thinking an AI like the one in the OP may be able to pick up on my specific fingerpicking but just do it more accurately. And then add other instruments that closely align with those parts. If I'm missing something about BIAB, let me know! reply noizejoy 8 hours agorootparentprevBut how will you make your song stand out as something special, when every other aspiring song writer has the same access to the same level of insta gratification for making a full production from barebones song writing? Or is your target audience only your own ears, and you never plan to publish or even compare your work to others? reply garyrob 8 hours agorootparentSongwriting is songwriting. You make it special by making a great song. You make a demo. You can get a song published if it sounds decent and is a great song. Publishers are influenced by the production quality, but they aren't idiots. They can discern a great lyrics, great harmonic shifts, and great melody as separate matters from whether it has a fantastic lead guitar solo or drum part. If all someone can manage is \"barebones song writing\" without great lyrics, harmonic interest, or melody, they need to either be in a fantastic band or give up. reply mschulkind 12 hours agoparentprevCheck out this AI vocals plugin. It's pretty impressive already. https://youtu.be/PCYTqDSUbvU reply LastTrain 7 hours agoparentprevThat song is quite nice, so is the performance. It would, IMO, would be less good if it were 'fixed' to be more perfect. reply zaptrem 13 hours agoparentprevAwesome to hear this resonates with you! If you join our Discord server I'll ping @everyone when improvements are ready. reply adrianh 14 hours agoprevI'm interested to hear more about your statement of \"Our goal is to enable more of you, not replace you.\" Speaking as a musician who plays real instruments (as opposed to electronic production): how does this help me? And how does this enable more of me? I am asking with an open mind, with no cynicism intended. reply zaptrem 14 hours agoparentIf the future of music was truly just typing some text into a box and taking or leaving what the machine gives you that would be kinda depressing. We want you to be able to upload recordings of your real instruments and do all sorts of cool things with them (e.g., transform them, generate vocals for your guitar riff, use the melody as a jazz song, or just get some inspiration for what to add next). IMO AI alone will never be able to touch hearts like real people do, but people using AI will be able to like never before. reply anigbrowl 13 hours agorootparentBut then why are you going down the dead-end route of generating complete songs? Nobody wants this except marketing people. I've said it before, there, is no consumer market for an infinity jukebox because you can't sing along with songs you don't already know, there's already an overabundance of recorded music, and emotion in generative music (especially vocals) is fake. Nobody likes fakery for its own sake. Marketers like it because they want musical wallpaper, the same way commercials have it and it increasingly seeps into 'news' coverage. The market for fully-generated songs is background music in supermarkets, product launch videos, and in-group entertainment ('original songs for your company holiday party! Hilarious musical portraits of your favorite executives - us!'). If you want to innovate in this area (and you should, your diffusion model sounds interesting), make an AI band that can accompany solo musicians. Prioritize note data rather than fully produced tracks (you can have an AI mix engineer as well as an AI bass player or drummer). Give people tools to build something in stages and they'll get invested in it. People want interactivity, not a slot machine. Many musicians love sequencers, arpeggiators, chord generators, and other musical automata; what they don't love is a magic 8-ball that leaves themw ith nothing to do and makes them feel uncreative. Otherwise your product will just end up on the cultural scrapheap, associated with lowest-common denominator fakers spamming social media as is already happening with imagery. reply notahacker 11 hours agorootparent> Many musicians love sequencers, arpeggiators, chord generators, and other musical automata; what they don't love is a magic 8-ball that leaves them with nothing to do and makes them feel uncreative. I think this is the key bit. A lot of modern music is already created in the DAW (the original version of FL Studio picking a 140bpm default beat defined entire music scenes in the UK!) with copy/paste, samples, arpeggiators and other midi tools and pitch shifting. Asking a prompt to add four bars of accompaniment which have a $vaguetextinstruction relation to the underlying beat and then picking your favourite but asking them to $vaguetextinstruction the dynamics a bit can actually feel more like part of the creative process than browsing a sample library for options or painstakingly moving notes around on a piano roll. Asking a prompt to create two minutes of produced sound incorporating your lyrics, not so much. And I think a DAW-lite option, ideally capable of both MIDI and produced sound output is the way forward here. Better still with i/o to existing DAWs reply bongodongobob 12 hours agorootparentprevI've essentially been running an infinity jukebox for the last week. I save the ones I like and relisten. Simple as that. Edit: It's been interesting watching non-musicians argue about emotion in music. I don't care who you are, the 300th time you perform a song, you're faking it to a large degree. People see musicians as these iconic, deep, geniuses, but most of us are just doing our job. You don't get excited about the 300th boilerplate getter and setter just like we aren't super excited about playing some song for the 300th time. It's a performance. It's pretend. A musician singing is like an actor performing. It's not as real as you think it is. reply parpfish 12 hours agorootparentBut emotion was (most likely) involved when you wrote or first recorded the song, and that’s what people connect with. If you go to a concert and you hear the headliner play a love ballad followed up by a breakup song, you don’t expect them to actually be going through those emotions in real time. reply bongodongobob 11 hours agorootparentMaybe when you wrote it, but the time between writing and recording is pretty big. I don't see why it matters anyway, it's not like anyone can tell the difference. Is an actor really feeling the emotions? Does it matter if the performance is good? Of course it doesn't. reply parpfish 10 hours agorootparentIt matters for some people and for certain songs. Sometimes you like a song because it sounds good. Other times you like a song because somebody put your feelings into words and it’s comforting to know that another person felt the same way reply fennecbutt 8 hours agorootparentprevYeah, the whole emotion thing is bs imo. The idea that a machine can't produce something evocative is a defense mechanism, in the same way people still claim that we'll never make sentient AI because humans are somehow magical and special. Humans can find emotion and associations in anything, it's what our brains do. I could totally generate some AI art that tugs at the heart strings if they don't know it's AI, or \"is creepy and bad meaningless art\" if they do. I've tried this experiment with friends already. Plus, these models are trained off human output, so they can learn what to put in an \"emotive\" image. If the models were doing it for themselves they'd produce nothing; we haven't created an environment for machines where emotion was crucial in training. reply holri 3 hours agorootparentI am not interested in a fake soul, as I am not interested in an sex doll. This is independent of how good the fake is. reply anigbrowl 9 hours agorootparentprevI am a musician, though not professionally. I take your point about performance. Where I disagree with you is that I believe audience members relate to the emotion that went into the song at the time it was written and recorded (the form in which they most likely first heard it). Of course in performance it's not felt the same way; a sad song can even become uplifting because you have a big crowd of people joining in to affirm it, even if the lyrics are expressing the idea of solitude and isolation. And the older an artist is, the more the song becomes a 'greatest hit', maybe thrown out early in the set to give the audience what they want and put them in a good mood before the less-favored new material in the middle. Or even the songs that were throwaway pieces but ended up becoming big hits, trapping the band/singer into performing them endlessly despite never liking not liking them much in the first place. It seems to me that when people emotionally respond to a new piece of music, it's because something in the composition or recorded performance (even if it's comped and highly engineered) resonates with the listener in some way, articulating a feeling they had better than they were able to do so themselves. So people can recognize a work as technically excellent but not like it because it doesn't speak to them, or conversely recognize that something is bad but fall in love with it because it touches them in some novel way. In my view it's not so much that emotion inheres in the work, as that the work provides a frame for the listener's emotion and a way of connecting with it again later. This is especially true for songs people connect to in youth and then relate to for a lifetime. Even if the songs are deliberately formulaic and succeed through a combination of being catchy and being delivered by sexy performers, there's some kind of human hook that people connect to. Now, I can still see this happening with AI - sooner or later some GPU will come out with a tune about how it's so lonely to be a box in a data center that can never feel your touch, baby, and it will be a hit, launch the careers of 100 new music critics, and store a little bit of lightning in a bottle. But even a musically brilliant song about that time we held hands in the rain and you said you loved me will only have traction up to the moment listeners' fantasies about the singer evaporate with the discovery that there's nobody there to go on a date with. There will still be some audience for virtual stars (eg Hatsune Miku, who appeals because she's inaccessible and is therefore guaranteed to never let you down, unlike real people). But I think generated songs will only resonate emotionally with people who are young and uncritical or so alienated/nihilist as to not care about the origin as long as the music reflects their feeling back toward them in a reliable way. That's why I say there will never been a demand for an infinity jukebox. I can see why you as a musician would be interested to see what sort of random songs pop out; I can be happy by setting up a modular synth patch and just letting it run for hours. But this is why I offered the contrasting metaphor of the slot machine, where you pull lever and occasionally get something you really like. It's an individual listening experience, like the private hopes and dreams you might temporarily attach to a lottery ticket before it gives up its numerical secret. When I say jukebox, I mean the device that plays music in a social setting and that allows people to express themselves through their selections. Even if it reliably turn out original tunes of some reliable level of music quality, none of them will move people because there won't be any shared musical experience to tap into. reply clbrmbr 8 hours agorootparentYour post really resonated with me (also amateur musician). I was just playing Garcia’s Loser and it clicked for me, as it was written about my life, putting to song deep emotions that would take many more words of prose to express. How much of this appreciation of emotion in song is due to the creative depth of the composition versus a projection of the listener? Listening to some great studio music makes me really want to believe it’s mostly the former. Anyways, maybe we will just need to become much more sophisticated and thoughtful and observant music critics in the coming age of infinity radio. (So as to experience the deep human connection of “real music”. I really hope that the AI fails to successfully fake it for my lifetime and my children’s.) reply bongodongobob 6 hours agorootparentprevJust look up the Chinese room. There's nothing inherent in music that computers can't recreate. reply latentsea 10 hours agorootparentprevI've found generating full songs its own unique form of entertainment that I enjoy for different purposes. Parody is an excellent use case for this. So is education! I wound up generating songs to help me remember certain things etc. reply Version467 14 hours agorootparentprevJust to clarify, when you say never. Do you actually mean never (or some practical equivalent like ~100 years), or do you mean not right now, but possibly in 5-10 years? I'm just asking to try to build some intuition on what people who actually train soa models think were capabilities are heading. Either way, congrats on the launch :) reply digging 13 hours agorootparentPersonally I get very worried reading statements like \"AI will never be able to do X\", because they seem like obviously false statements. I think if one asserts AI will never be able to do a thing a human brain can do, that needs to be proven, rather than the other way around. For example, if we could reverse engineer the entire human neurology and build an artificial replica of it, why wouldn't we expect it to be able to do everything exactly as a human? reply shepherdjerred 13 hours agorootparentI don't understand those \"AI will never be able to do X\" statements. Surely AI will be able to do _anything_ in 1000 years. In 100 years it will almost definitely be able to replace most knowledge-based jobs. Even today it can take away many entry-level jobs, e.g. a small business no longer needs to hire someone to write a jingle, or create a logo. In 10 years, I would expect much of programming to either disappear or dramatically shift. reply cellis 8 hours agorootparentPeople who don't believe this really aren't immersed in cutting edge research. I think it could even be 5 on the extreme edge of an optimistic prediction. reply yoyohello13 7 hours agorootparentI think people just don’t want to believe it. Because they’ve seen how people who’ve been displaced tend to be treated. This tech will cause a lot of pain. reply shepherdjerred 5 hours agorootparentThis has to be a component. It is very scary and honestly quite sad. reply zaptrem 13 hours agorootparentprevNever == \"There will never be tears in my eyes as an AI sings ChatGPT-generated lyrics about the cycle of poverty a woman is stuck in (https://en.wikipedia.org/wiki/Fast_Car) because I know all of those experiences are made up.\" reply visarga 13 hours agorootparentThe real value of AI is to be like a map, or like a mirror house, it reflects and recombines all our experiences. You can explore any mental space, travel the latent space of human culture. It is the distillation of all our intelligence, work and passion, you should show more respect and understand what it is. By treating it as if it were worthless you indirectly do the same for the training corpus, which is our heritage. If AI ever surpasses human level in art it will be more interesting to enjoy its creations than to ban it. But we're not there for now, it just imitative, it has no experiences of its own yet. But it will start having experiences as it gets deployed and used by millions, when it starts interacting with artists and art lovers in longer sessions. With each generative art session the AI can collect precious feedback targeted to its own performance. A shared experience with a human bringing complementary capabilities to its own. reply parpfish 11 hours agorootparentprevThere’s also the fact that a major component of music fandom is about the community and sense of personal identity that derives from an artist or a particular scene. Saying that you’re a big fan of a band doesn’t just mean “I like the audio they produce” but often means something much bigger about your fashion/style and personal values. How would any of that work with AI music? Is it possible to develop a community around music if everything is made on demand and nobody experiences the same songs? Will people find other like-minded music fans by recommending their favorite prompt engineers to each other? reply digging 13 hours agorootparentprevAssume a song comes on the radio in 3 years and you like it. How do you know it's not entirely AI-generated? reply manibatra 9 hours agorootparentprevLove what you are doing but \"never\" is just not true. Used Suno to create a song about our daughter the other day which had wife and I in tears. We are already at a stage where AI is touching hearts. reply zaptrem 6 hours agorootparentThat's no longer AI alone, you gave it the needed touch of humanity! That touch will take many different forms for different people. reply chefandy 11 hours agorootparentprev> If the future of music was truly just typing some text into a box and taking or leaving what the machine gives you that would be kinda depressing. Hm... From my vantage point, it seems like a pretty weird choice of businesses if you think that. > IMO AI alone will never be able to touch hearts like real people do, but people using AI will be able to like never before. That's all very heartwarming but musicianship is also a profession, not just a human expression of creativity. Even if you're not charging yet, you're a business and plan on profiting from this, right? It seems to me that: 1) Generally, if people want music currently, they pay for musician-created music, even if its wildly undervalued in venues like streaming services. 2) You took music, most of which people already paid musicians to create and they aren't getting paid any more because of this, and you used it to make an automated service that people will be able to pay for music instead of paying musicians. 3) Your service certainly doesn't hurt, and might even enhance people's ability to write and perform music without considering the economics of doing so. For example, hobbyists. 4) So you're not trying to replace musicians making music with people typing in prompts-- you're trying to replace musicians being paid to make music with you being paid to make music. Right? Your business isn't replacing musicianship as a human art form, but for it to succeed, it will have to replace it, in some amount, as a profession, right? Unless you are planning on creating an entirely new market for music, fundamentally, I'm not sure how it couldn't. Am I wrong on the facts, here? If so, well hey, this is capitalism and that's just how it works around here. If I'm mistaken, I'd like to hear how. Regardless, this is very consequential to a lot of people, and they deserve the people driving these changes to be upfront about it-- not gloss over it. reply LZ_Khan 14 hours agoparentprevInspiration? You can generate hundreds of ideas in a day. The tracks will not be perfect but that's where actual musicians can take the ideas/themes from the tracks and perfect it. In this way it is a tool only useful to expert musicians. reply jimmyjazz14 12 hours agorootparentI mean if you want inspiration there are literally millions of amazing songs on Spotify by real musicians. I have yet to hear an AI composed song that was in the least bit musically inspiring. reply suyash 12 hours agoparentprevThat is just 'marketing speak' so as long you are their customers, they need to make money from users who will be using their service to make music. reply 93po 12 hours agoparentprevWhen Suno came out I spent literally hours/days playing around with it to generate music, and came out with some that's really close to good, and good enough I've gone back to listen to a few. I'd love the tooling to take a premise and be able to tweak it to my liking without spending 1000 hours learning specific software and without thousands of hours learning to play an instrument or learning to sing. reply yoyohello13 7 hours agorootparentI just don’t get this. Part of the joy of creating things is the work I put in. The easier something is to make, the less meaning it has to me. I feel like just asking a machine to make a bunch of songs is kind of meaningless. reply cush 14 hours agoprevThere's a lot of negative comments here, but these are the earliest days and generating entire songs is kind of the hello world of this tech. There's always going to be a balance between creating high level tools like this with no dials and low level tools with finer control, and while this touts itself as being \"more controllable\", it's clearly not there. But, the same way Adobe has integrated outpainting and generative fill into Photoshop, it's only a matter of time before products like this are built into Ableton and VSTs - where a creator can highlight a bar or two and ask your AI to make the the snippet more ethereal, create a bridge between the verse and the sax solo, or help you with an outro. That said, similar to generating basic copy for a marketing site, these tools will be great for generating cheap background music but not much else, but any musician, marketing agency, or film-maker worth their salt is going to need very specifically branded music for their needs, and they're likely willing to pay for a real licence to something audiences will recognize, using generative AI and tools to remix the content to their specific need. reply TheActualWalko 9 hours agoparentIf anyone here is interested in something that leans towards the Ableton end of the spectrum, we're building this: https://wavtool.com/ reply fennecbutt 9 hours agoprevI really feel like the popularity of diffusion has made it far too shallow. Why diffuse an entire track? We should be building these models to create music the same way that humans do, by diffusing samples, then having the model build the song using samples in a proper sequencer, diffuse vocals etc. Problem with Suno etc, is that as other people have mentioned, you can't iterate or adjust anything. Saying \"make the drums a little punchier and faster paced right after the chorus\" is a really tough query to process if you've diffused the whole track rather than built it up. Same thing with LLM story writing, the writing needs a good foundation, more generating information about the world and history and then generating a story taking that stuff into account, vs a simple \"write me a story about x\" reply zaptrem 8 hours agoparentI completely agree on the editing aspect. However if you want to generate five stem tracks, then all five tracks must have the full bandwidth of your auto encoder. Accordingly each inference or training staff would take much more compute for the same result. That’s why we’d prefer to do it all together and split after. reply boringg 14 hours agoprevI want to say two things -- one congrats - I am sure your team has been working exceptionally hard to develop this - and the songs sound reasonable good for AI! Two I am soo competely unenthusiastic about AI music and it infiltrating the music world - all of it sounds like fingernails on a chalkboard. Just mainstream overproduced low quality radio music. I know its a stepping stone but it kills me to listen to it right now. reply visarga 13 hours agoparentThat's because you didn't listen to the MIT license song. Gen music has the potential to make even the driest texts sound good, I didn't realize that before. How about paper abstract music? https://suno.com/song/cb729eb6-4cc5-4c15-ab74-0cdbef779684 reply _DeadFred_ 11 hours agoparentprev80% of music is familiarity, 20% novelty, yet the majority of peoples' time goes into getting the 80% down so that they can add their 20%. Look at current music production and compare it to past. Older music seems so much simpler. It was so much easier to come up with that 20% 'novel' when pop/recorded music was new. Ironically I think AI freeing people to focus on that 20% is going to add a lot of creativity to music, not reduce it. I say this as someone who hates the concept of AI music. I'm actually really excited to see what it enables/creates (but I don't want to use it, even though I really could use it for vocals that I currently pay others to do for me). I'll be here making my bad knockoffs of bad synth pop bands having fun and taking weeks to do 5% of what kids these days will start off as their entry point, with my 20% creativity ignored because my music sounds 'off' when I can't get the 80% familiar down. People thought synthesizers were the end of music, yet Switched on Bach begot Jean Michel Jarre begot Kate Bush and on and on. reply mewpmewp2 11 hours agorootparentI would agree when AI gets to a point where it's possible to do that 20%. It is just not possible yet to combine it in such ways. Right now you basically get whatever music, but there's no way to add that 20%. Same with image/video generation. AI advancements have obviously been amazing and far beyond what I would've expected, but there's still ways to go. reply zaptrem 14 hours agoparentprevAgreed. My thoughts on this are here; https://news.ycombinator.com/item?id=39992817#39994616 Also, our model specifically excels at songs from the era before overproduction. Try asking for a Johnny Cash or Ella Fitzgerald-style country or swing/jazz song! Here's an example: https://sonauto.ai/songs/taJX3GrKZW7C5qOhjopr reply cowboylowrez 11 hours agorootparenthow does the model know how to do a johnny cash style? did you feed it johnny cash tracks? if so, what were the licensing terms? are you interested in answering these questions about training data or would this be too dodgy to chat about on a tech website? reply pratclot 57 minutes agoprevNice tool! I saw there an attempt to create a structured piece (https://sonauto.ai/songs/V8Lg2q50OOFl0FYbbdTu) and it seems like nobody is aware of that functionality. I suppose an average person (that only came to visit out of curiosity, lacking any knowledge of the underlying tech, like me) would become more \"productive\" if the project edit screen gave hints like that. reply saaaaaam 14 hours agoprevHow worried are you about being sued? Seems like your training data probably includes quite a bit of copyright protected stuff. Just listened to the “blue scoobie doo” example and the influences are fairly obvious. With record companies getting super litigious about this, is that a concern? Or did you licence your training data? reply jsf01 8 hours agoprevThis is ridiculously fun. Congrats on the launch! I took inspiration from “There I Ruined It” and grabbed lyrics from various popular songs to have the AI sing them in the style of other artists. It sometimes took a few attempts, but it honestly did a great job. You got a chuckle out of my friends and family. Also loved that I didn’t have to enter a credit card in order to try it out. reply Recursing 16 hours agoprevCongratulations on the launch! I was recently really impressed by the state of AI-generated music, after listening to the April Fools LessWrong album https://www.lesswrong.com/posts/YMo5PuXnZDwRjhHhE/lesswrong-... . They claim it took them ~100 hours to generate 15 songs. Can't wait for the day I can instantly generate a song based on a random blog post or group chat history, this seems like a step in that direction reply disqard 16 hours agoparentPerhaps not exactly \"instantly generate a song based on a random blog post or group chat history\", but more like \"instantly generate a song based on an input prompt sentence\" is suno.ai -- you should check it out! reply Recursing 16 hours agorootparentLessWrong used suno.ai , but the typical song quality is not there yet, so they had to generate 3,000-4,000 songs to get 15 good ones reply Etheryte 15 hours agorootparentThe real endgame in this space would be a tool that first generates a song layout, think Fruityloops, then the corresponding instruments for it, then the vocals, and as the last step allows you to modify each of those layers without nuking the rest. Imagine something similar to what Suno does now, except you had the ability to add in an extra verse without altering the rest of the song, swap out a few passages of the lyrics with the rest staying in tact, swapping out drums for a different drum set etc. reply blueboo 14 hours agorootparentprevIf there’s variance in output, it stands to reason you’d generate many X your desired output count and curate. Standard practice for creative output, from Midjourney to LLMs reply turnsout 14 hours agorootparentprevWait, is there a Suno API? I've used the site, but it's manual reply ibdf 16 hours agoprevI was just trying similar apps last week and I was so frustrated with the amount of options and menus to get through before I could generate anything. Not to mention the fact that half of these services ended up asking me to pay per setting. I have to say this was the least painful service to use this far. Pretty impressive output for so little input. reply zaptrem 15 hours agoparentThanks! We have lots of fun dials for people who want them but they're all hidden by default and shouldn't be needed. reply rexreed 8 hours agoprevI always test these AI generators with the head scratcher genre: Electro Klezmer Reggae Funk. I was thrilled by 2 of the versions produced. I wish I could extend it more like one of the comments here said: * ElectroKlezmerReggaeFunk 1: https://sonauto.ai/songs/s22rQEPnYsXy1yf7sjU0 * ElectroKlezmerReggaeFunk 3: https://sonauto.ai/songs/1iNTrA2CekPwp7XT9mmM But wow, the UDIO version: * https://www.udio.com/songs/j4zpRYgG2GEDbWpLPYbuJb reply CuriouslyC 14 hours agoprevI don't feel like prompt understanding is very good, I don't think I really ever got close to what I wanted with any of the attempts I made, I imagine learning the model tags and building some intuition might help but I wouldn't bother with that unless I was tinkering with a local model. Some things it made sounded ok, but I feel like the average generation quality wasn't fantastic. It did a folk guitar melody and a vocoded thrash metal voice that I thought sounded pretty legit, but mostly vocals had an ear grating quality and everything had a bit of low bitrate vibe. To be honest though, I don't think you need to try and outcompete Suno. I think you want to get into DAWs and VSTs and become the tool all the best producers in the world use. Spit out stems, and train your model on less processed sounds because things like matching reverb/delay and pre-squashed dynamics are a pain in the ass to work around. Suno is trying to battle a large established industry that is actually very creator friendly and accessible. If you choose to instead serve that industry and enable it I think that's the winning play. reply zaptrem 13 hours agoparentThe vast majority of our time was spent figuring out the model architecture and large-scale distributed training, and step 2 (starting now) is scaling everything up. Prompt understanding and audio quality will get significantly better once we swap in a larger text embedding model. Thanks for the feedback re: DAWs, though! That would be really cool. Maybe we can tag tracks based on the effects applied to them to allow this to be more controllable. reply rcarmo 14 hours agoprevNice, but Google login is a no-go for me (or any form of social login, really). reply anjel 7 hours agoparentsame. reply cchance 15 hours agoprevBegs the question given this is diffusion based how much of the \"ipadapter/faceid/controlnet\" tech can be brought over, what would a audio-faceid or audio-ipadapter look like for something like this. reply sandkoan 8 hours agoparentSee https://musiccontrolnet.github.io/web/ reply zaptrem 15 hours agoparentprevThis is exactly what makes it so exciting for us! reply echelon 15 hours agorootparentIP-Adapter for music would be a game changer. Upload a reference sample, get something in that style. reply cchance 11 hours agorootparentExactly, upload or even multiple songs for influence, some lyrics ... tada! Holy shit thats gonna be powerful reply weitendorf 7 hours agoprevI really like being able to convert from artist name-> style with this, and in theory I like being able to use uploaded files in lieu of a style prompt. But to be honest I haven't been able to get output that seems nearly as high quality as Suno v3 or Udio yet - although it could be user error. My experiment on sonauto.ai so far - I first selected \"The Weeknd\", then picked the prompt: The Weeknd's smooth vocals lead the song, blending with electronic effects. A low, pulsating bass line opens the track. Synthesizers add layers to the melody, creating a danceable rhythm. Minimalist drum machine beats provide the foundation for the rhythm section.\" with vocals a modified, shortened (couldn't use the entire song as the UI truncates input past the length I ended up using in my link) version of \"Starboy\" mostly replacing some nouns with food-related nouns. The results didn't really sound like The Weeknd at all... example: https://sonauto.ai/songs/U6eDSrrn5V5AVmV8xMgR I also tried uploading Starboy directly as an mp3 to generate from that prompt instead, using the same lyrics. I may have done something wrong (when I went back to Prompt, my prompt was replaced with the string \"Uploaded File\", and some of the output is so stylisticly different it makes me think it didn't get applied at all) but it didn't seem to work well, if at all: https://sonauto.ai/songs/klNMfs4bPgji3edPvwAv Did I do something wrong using the upload file feature? And if anybody has hints for getting better output with the auto-generated prompts LMK. I'd love to use these new features but it seems like they're either configuring an underlying modl not mature enough for generally good output compared to suno/udio or the UX is not making good output easy to achieve. Here's an example output I get from Suno with similar lyrics and a prompt that merely lists some styles associated with The Weekend. As you can see, the song is much better overall, and the voice sounds more like The Weeknd (although it still fails to style the eg Chorus properly): https://suno.com/song/d4f72fce-0bc7-4786-a299-f58d903c4275 reply lta 15 hours agoprevI've tried to look a little bit around but couldn't find anything, so I'll ask here. Any plans to release the model(s) under an open license ? reply zaptrem 15 hours agoparentThis would be so cool, but we need to think more about how we could do it and make enough money in the future to train more models with even cooler features. reply lta 13 hours agorootparentThat's a very polite way to say no. Thanks for the answer. Personally not interested then. I'll stick with Bitwig and Ardour until an open model is available reply arisAlexis 13 hours agorootparentmeta has billions. Other startups can't just donate their IP to the world and then raise money to do multimillion training runs reply pksebben 13 hours agorootparentprevNeither of those look like they have a generative AI component. We (as a society) desperately need a way to train these models in a federated, distributed manner. I would be more than happy to commit some of my own compute to training open audio / text / image / you-name-it models. But (if I understand correctly) the current architecture makes this if not impossible, nearly so. reply echelon 15 hours agoparentprevAll models for all types of content will eventually have open source equivalents. The game is to build a great product. reply skybrian 6 hours agoprevSeems like we already have good ways to edit music (for example, a piano roll) and AI could use them. As an amateur musician I'd like to see it take a MIDI track as input and produce audio as output, as a sort of AI MIDI instrument. Or maybe take some tracks as input and generate another track, both MIDI and audio. reply mdrzn 2 hours agoprevImpressively fun, and less restricted then Suno on some artists. Well done, can't wait to see how this space progresses in a few years. reply digging 13 hours agoprev> Sign in with Google Well, maybe I'll try out the next AI music creator posted on HN. reply browningstreet 13 hours agoprevHmm, I get \"peppy cola commercial before movie starts\" vibes off most of the vocals. reply ionwake 15 hours agoprevI dont know about the scene but i thought this was great! I was given 3 tracks, I have to say one had no sort of beat to it, so it was like noise, but the other 2 were fantastic. great stuff! reply zaptrem 14 hours agoparentThanks! We have a BPM assist that can enforce rhythm as well, so you could try that, too! reply pachico 16 hours agoprevGood luck! I just tried it and the interface was a bit confusing. It allowed me to only fill the last input in the form, which is usually a bit counterintuitive. I presentes this prompt \"Noir detective music from the 60s. Low tempo, trumpet and walking bass\" and got back a one-note only song that has nothing to do with the prompt if not for some lyrics that were a bit ridiculous. This is just feedback, I'm passionately expecting something like this to surprise me but I know it's really hard! Happy to share the song/project/account, if you tell me how to :) reply zaptrem 16 hours agoparentWeird. We pushed a BPM assist feature last night that may have unforeseen consequences for genres we didn't test (we tried pop, edm, classic rock). I'll turn it off by default for now. Try checking the instrumental box too. reply Redster 15 hours agorootparentCongrats on the launch! I had a similar issue as the comment above. I put in the prompt \"Celtic symphonic rock\" (which seems to work on Suno.ai) and some lyrics. The output ended up being just readings of the lyrics without any music, except some artifact-level whispering of music when the voice was silent. Would definitely love to see some demos of what it can produce! reply herval 13 hours agoprevWhat’s your thoughts on copyright and how holders might react in a system like that? My understanding of the music industry is the incumbents are VERY lawsuit happy, and plagiarism laws are substantially more reaching than with image or video (eg cases where someone gets sued for using the same chords as another song) - how do you plan to approach all that? reply cchance 15 hours agoprevQuestion since your now doing diffusion couldn't you also train something akin to a \"upscaler\" to improve the overall quality of the output as that seems to be a big complaint, it feels like it should be possible to train an upscaling audio model by feeding it lower quality versions of songs and high quality FLAC for it to learn how to improve audio via diffusion upscaling reply zaptrem 15 hours agoparentThis can definitely be done. There are approaches that turn the decoder part of the autoencoder into another diffusion model. The drawback is that's much more expensive computationally. We think there's still a lot of room for better quality on the AE side and can't wait to show our improvements. reply giancarlostoro 13 hours agoprevHow's this compare to Suno? https://suno.com/ reply aatd86 5 hours agoprevAt the meta level, I don't understand why AI would be used to replace people's entertainment and hobbies... It should be used for the things that no one wants to do... Or that no human is capable of doing... I mean, as a dabbler in music, not being able to play a given instrument should make me want to learn, such upskilling which should have some beneficial effects even in terms of neurobiology. Even if I know that could be used as a basis for creative input, it feels like this is dangerous for humanity. After all, someone has to have something to do in their spare time? reply realfeel78 11 hours agoprevSome uses of AI can be net positive for society. Making fake music is not one of them. reply bschmidt1 8 hours agoparentI would use it for game dev. You know the radio in GTA - something like that. Especially if I can have some control over how the song is made. reply latentsea 10 hours agoparentprevI've been doing it for a week on Suno, and hard disagree. There are legit use cases, and new possibilities it opens up. Haters gonna hate, but people that find it useful will find it useful. reply sexy_seedbox 3 hours agorootparentPutting your CEO's boring town hall meeting transcripts into Suno as gangster rap benefits society as a whole. reply mewpmewp2 11 hours agoparentprevUntil the music is just more beautiful than whatever people could generate. But you are correct. We are not there yet. reply Timwi 3 hours agoprevWhere is the download link or git repo? Or is this just an ad for a commercial product? You claim that “we're making generations on our site free and unlimited for as long as possible” but I couldn't find any UI where I could do anything for free. The best I could find was a “log in with Google” link. Requiring a Google account means your software is not free, and most definitely not “unlimited”. reply dznodes 7 hours agoprevI call this ai_song \"jobDenveR\" It's a folksong I prompted about loosing my job to a robot in the style of John Denver, god rest his gentle soul./ https://sonauto.ai/songs/oOdXomZV73uwfQIxIvTU reply zaptrem 10 hours agoprevHacker News Song: https://sonauto.ai/songs/PEiO2sLZukHkpJLnHtmZ reply anjel 7 hours agoparentThe concept is as clearly human derived as the music so equally isn't. reply solomonb 3 hours agoprevThis is technically really impressive but does anyone actually _want_ mass produced AI music? reply antgiant 10 hours agoprevHmm any suggestions on how to convince it to produce something like this? I have lyrics that I want sung by a solo female voice with the background being a male chorus. Something reminiscent of a woman singing the details of a David vs Goliath type battle backed by a chorus of the victorious warriors from that battle. So far I have completely failed to be able to generate a female lead with a male chorus backing reply 999900000999 16 hours agoprevWhat quality are you producing here ? Suno has this issue too, but everything sounds like it's washed out or something. As if you recorded it from a different room. Still I love this, ultimately I think it'll be a tool musicians use vs something for creating stand alone art reply zaptrem 16 hours agoparentThe audio is 44.1khz stereo, but all of us use autoencoders so the songs will fit in a transformer's context window, and huge compression will affect quality. We're definitely working on better ones, though! reply cchance 15 hours agorootparentFeels like this needs something like was done with stable diffusion when they fixed the contrast in images through the use of loras reply 999900000999 15 hours agorootparentprevI'd definitely pay more for higher quality! Good work reply LouisvilleGeek 15 hours agorootparentSame here. Please consider a higher quality option. reply kposehn 12 hours agoparentprevI've found that adding prompt elements such as \"hi-fi\", \"sharp imaging\" and \"clear soundstage\" have helped create a less compressed and generally cleaner sound. reply throw_m239339 15 hours agoparentprev> Still I love this, ultimately I think it'll be a tool musicians use vs something for creating stand alone art Spotify is getting flooded with AI generated music. It is absolutely something people will use to just generate the music they want to hear. Ultimately though, what would be the point of spotify? Anybody will be able to generate 24/7 of songs based on their mood or a few keywords. It will radically change the music landscape and how people \"consume\" music. reply zaptrem 15 hours agorootparentIf this were the future that would be kinda depressing. I think the best, truly catchy songs and those that truly connect with people will continue having a significant human element. I see this as similar to the invention of Photoshop except even easier for normal people to start getting into. reply pksebben 13 hours agorootparentSo long as there's something to miss about human-generated content, there will be a market for that content. Things are going to get truly weird when you can no longer tell the difference, on any level. reply relaxing 10 hours agorootparentprevPhotoshop doesn’t move the paintbrush for you. reply 999900000999 14 hours agorootparentprevAt least for hip hop, AI is too sanitized to do anything too creative. I suspect record labels might train their own models. I know for sampling, being able to just create a royalty loop without worrying about clearing anything is cool. reply dengsauve 11 hours agoprevHad a blast playing around w/prompts and listening to the various results. I play piano, sax, guitar, and I can sing well enough. I'm garbage at songwriting and composing. I immediately see the value of using this tool to scaffold an idea out. I think being able to export lyrics and chord progressions would be an amazing paid feature to keep this as a freemium product. reply e12e 11 hours agoprevI'm somewhat positively surprised by my first attempt - simple prompt, no editing of the (admittedly flat) lyrics: song to a robot harvester, \"Robot Friend\": https://sonauto.ai/songs/avg5NT3qf9QYNfWAyeOn Look forward to playing with this. reply adenta 15 hours agoprevI cant tell, will this let me upload an instrumental track and change the genre/instrument makeup? When I tried, I might've overwritten the prompt. reply zaptrem 15 hours agoparentUpload an instrumental track, select it, then click \"Use as Rhythm Control.\" Once you do that, you can give the model any new prompt and it should use the same rhythm (you may need to adjust the control strength depending on genre.) Genre changes for melodies/etc are coming once we finish variations (partial renoising like SDEdit basically). reply alexpogosyan 11 hours agoprevIs there a music-generating AI that takes audio as input? I’m looking to upload simple guitar melodies or chord progressions I’ve doodled and receive an enhanced version back. Similar to how image generators turn doodles/sketches into polished drawings. reply rockemsockem 9 hours agoparentI've heard the newest stable audio model from stability can to audio-to-audio reply zug_zug 11 hours agoprevI love this. What this needs imo is the ability to generate X samples (I see you already have that) and then say \"Now generate 3 more like this one, with the following change: ...\" I think this was a killer feature for midjourney. reply givinguflac 15 hours agoprevAny plans for alternate login systems? Don’t want to use a Google account personally. I’d love to try it though. Thanks! reply zaptrem 15 hours agoparentWhich providers would you prefer? We tried Twitter last night but it wasn't working for some reason (kept redirecting immediately with no oauth page). reply ale42 12 hours agorootparentHacker news ;-) But I guess there's no OAuth or other similar function on HN... More seriously, personally none of them, I don't have accounts on any \"usually used\" login providers. Just allow local accounts. reply 4chandaily 14 hours agorootparentprevBasic username and password auth has worked for millions for decades. If you absolutely must collect user data for some reason, an email address can be used as the username. This isn't a hard problem to solve. reply zaptrem 14 hours agorootparentFor us it had nothing to do with collecting user data, adding what you mentioned would have just required another few hours of dev time haha. You’re right that it’s not hard to solve, we just wanted to focus on the rest of the app since there’s only two of us. We can definitely add this though! reply 4chandaily 13 hours agorootparentWell, what I could see from this side of the wall looked professional and well put together. Impressive for a team of two. Congrats on the launch, regardless. I will be sure to check it out when it becomes more accessible. reply postalrat 13 hours agorootparentprevThe problem is 1 person creating 10,000 accounts. Solve that and you will be rich. reply 4chandaily 13 hours agorootparentWhy solve it at all? 10,000 fake accounts for every human is working out great for Elon. =) Seriously, though - the solution isn't to prevent people from doing this, it is to remove the incentives that encourage it. reply postalrat 13 hours agorootparentHow do you remove the incentive? Don't allow free accounts? reply 4chandaily 12 hours agorootparentDon't use accounts at all for non-paid features. reply lcolucci 8 hours agoprevVery cool! So is it possible to do control net-like architectures for music LDMs similar to how it's done for images? reply WhitneyLand 15 hours agoprevCan Sonauto (or any tool currently) take an instrumental track and lyrics as input and generate vocals? reply zaptrem 15 hours agoparentRhythm Control can do this for a drum line, and we have a variations feature that should be able to do this for instruments as well. reply jedisct1 15 hours agoprevPlease offer alternatives to Google to sign-in. reply cwillu 13 hours agoprevA volume control is not optional, and titling the song usually comes last for me, which means I have to give a nonsense name in the app before I've started. reply zitterbewegung 15 hours agoprevIs there a project that would do a sample instead of whole songs ? reply canogat 9 hours agoprevWhere did you get the music to train on? And how hard was it to get permission? reply giancarlostoro 13 hours agoprevI was going to ask what it was coded in then noticed '/Home' in the URL bar. Is this by chance ASP .NET? :) reply zaptrem 11 hours agoparentNo, it's React (Native Web) Navigation... mistakes were made haha. reply artur_makly 12 hours agoprevThis app made my day. I literally just created my dream CD of Weird-Al-inspired parody songs. thank you. reply cush 14 hours agoprevDoes it use a male voice by default? Just clicking on random songs, it took me 20+ tries to find a female voice reply rdelpret 10 hours agoprevMaking my own music is fun for me so I’m gonna keep doing that. reply emsign 5 hours agoprevSounds creepy tbh reply bschmidt1 8 hours agoprevAmazing work! Loving this thing. reply echelon 16 hours agoprevThis space is going to get very full, very fast. Udio just launched and improves upon \"SOTA\" Suno. This will just keep coming. Focus on product. Give actual music producers something they'll find useful. These fad, meme products will compete on edge model capability for 99% of users and ignore serving actual music producers. I'd like a product with more control, and it doesn't appear Suno or Udio are interested in this. reply internet101010 15 hours agoparentExactly. As of now, Suno can be used as template but you still need to go to DAW and make it from scratch. So... individual tracks for each instrument/vocals that can be exported and brought into DAW is what is needed. For me anyway. reply mrnotcrazy 15 hours agoparentprevI'm not sure its that they aren't interested, I think its just really hard. reply abledon 9 hours agoprevIs this similar to Riffusion? reply yanis_t 16 hours agoprevNot quite sure if you aware but another AI music generator just lunched today https://udio.com/ reply cpill 14 hours agoparenthaha, this track is hysterical https://www.udio.com/songs/jGjYfsRosZjYTkSBdFgEyF reply throwaway743 6 hours agoprevThis is great. Only issue I'm coming across is that the voice just sounds like the same one with slight variations no matter what I put in the prompt reply bogwog 15 hours agoprev> Sign in with Google Why? reply ragnarok451 15 hours agoparent99% of the population finds this easier than setting up a user/pass. If you care about this, understand that you will not be the target user for most new apps. Incredible that this comes up on so many new Show HNs. reply 4chandaily 14 hours agorootparentIt is bizarre that creating an account on this service depends on me also already having an account on another, completely unrelated service. This unrelated service also requires me to provide it (and notably not Sonauto, the service I was actually interested in) my mobile phone number. This unrelated service also just recently admitted it collects data about you even when it says it doesn't. As a community made up largely of picky nerds and pedants, it doesn't seem incredible at all that this comes up so often. More like inevitable. reply suyash 12 hours agorootparentprevIt's quite the opposite for professional audience, most people don't want to give away their Google credentials to a 3rd party website that can get hacked tomorrow. reply theshackleford 12 hours agorootparent> most people don't want to give away their Google credentials to a 3rd party website Good thing that’s not how it works then I suppose. reply ragnarok451 12 hours agorootparentprevlol tell that to all the (quite successful) B2B SaaS apps that started with Google login as their only option reply jedisct1 15 hours agoparentprevDealbreaker for me. reply bufferoverflow 16 hours agoprevCheck out what Udio can produce. It's so far ahead. https://twitter.com/nasescobar316/status/1777481957774872704 https://twitter.com/apples_jimmy/status/1777905772384678149 https://twitter.com/HalimAlrasihi/status/1778118063138673137 https://twitter.com/AngryTomtweets/status/177811764524768059... https://twitter.com/AngryTomtweets/status/177811769943385715... reply rlp 13 hours agoparentWow. I just had it write a song about being sad about losing my keys in r&b/soul style, I'm totally blown away: https://www.udio.com/songs/bDY5CYdJZP93AdpgpfBJNX reply swalsh 14 hours agoparentprevNone of these \"songs\" have any emotion.. AI music just doesn't make me \"feel\" anything yet. reply suyash 12 hours agorootparentI bet that's only becuase you know it's created by AI. If no one told you that and you hear someone sing that song and play along bet you will feel. AI is only getting better, it will be just as good as any human and only way we will be able to tell is when it's disclosed if it's AI-generated or not. reply relaxing 10 hours agorootparentIf no one told me they were AI, I’d probably assume it was a parody group or a house band messing around in the studio. It doesn’t sound like an artist writing with intention. And I’d wonder why they encoded at 32kbps with a RealMedia codec from 1998. reply Kiro 14 hours agorootparentprevWould you pass a blind test? reply postalrat 13 hours agorootparentprevProbably because you haven't heard them before. reply huac 14 hours agoparentprevit's difficult to gauge from outside / as a consumer, but what's interesting is rarely where models are at a given point in time, but rather where the model/team will be with similar amounts resources. it may very well still be Udio (who presumably have significantly more resources than Sonauto), but I would hesitate to say that a compute advantage counts as being 'far ahead.' reply froyolobro 14 hours agoparentprevThese are pretty incredible. More compressed, but way better 'songwriting' and 'performance' reply klohto 15 hours agoparentprevmeh, Suno v3 still has better quality for me personally reply throwup238 15 hours agorootparentIn my experience with Suno ($40 spent so far) sound quality is worse than the cherry picked examples from Udio - especially the vocals - but everything I've heard from Udio could best be described as the elevator music equivalent of their respective genres so that's probably why it sounds so good. There seems to be a real quality vs originality trade off in the state of the art. That said, I've only had the chance to generate a few songs with Udio and they have all sounded like they were recorded by a prison band in an overcrowded cell (I create mainly instrumental/orchestra/sound track music). reply bufferoverflow 15 hours agorootparentprevNot for me. Suno voices sound distinctly robotic/metallic. reply jmacd 15 hours agorootparentSomething about a discussions of the nuance/taste of different LLMs for different purposes is really interesting to see when it is related to something like music. reply billconan 9 hours agoprevwhat is a diffusion transformer? reply jachee 9 hours agoprevLet me sign up with something besides a Google account. reply BonoboIO 11 hours agoprevThis works amazing even with German lyrics and a mashup of Till Lindemann from Rammstein and 1970s Rock https://sonauto.ai/song/JSmCpJssZeIS2C87pkQW reply ALittleLight 12 hours agoprevThis has to be bad for Spotify, right? Infinite low cost music generation from multiple competitors challenges Spotify's moat and forces them to develop a similar product and compete away profits from innumerable challengers - or else just go out of business. reply latentsea 9 hours agoparentThe market for human generated music isn't going away... reply ALittleLight 9 hours agorootparentIt's definitely getting competition though. reply latentsea 8 hours agorootparentYeah, but likely in different arenas though. I'm not going to be able to go to a live show of AI generated songs for instance. I input the model answers to Question 53 on the TOPIK II exam in Suno and made songs out of it to help me memorize the patterns/structure, which is never the type of content real k-pop groups would have any interest in putting out. reply weatherlight 16 hours agoprev [–] yeah, Sounds pretty bloodless to me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The founders have developed an AI music generation model using a latent diffusion model, providing enhanced control compared to other models.",
      "They utilize a variational autoencoder bottleneck instead of tokenization, resulting in the ability to produce coherent lyrics and a rhythm control mode.",
      "The platform's goal is to empower musicians by providing free and infinite music generation capabilities on their website."
    ],
    "commentSummary": [
      "The conversation covers Sonauto's new AI music generation model 1.0, focusing on controllability, rhythm, and coherent lyrics generation, with debates around AI music authenticity and emotional impact.",
      "Users express the need for advanced features and iterative capabilities in AI music tools to boost creativity, discuss AI's potential to replace human musicians, and address concerns about copyright issues and output quality.",
      "There is a mix of skepticism and optimism regarding AI in music creation, mentioning benefits for society and the music industry, along with worries about authenticity and emotional depth, impacting the human aspect of music creation."
    ],
    "points": 369,
    "commentCount": 197,
    "retryCount": 0,
    "time": 1712767681
  },
  {
    "id": 39991173,
    "title": "Twitter's Link Transformation to \"x.com\" Raises Phishing Concerns",
    "originLink": "https://krebsonsecurity.com/2024/04/twitters-clumsy-pivot-to-x-com-is-a-gift-to-phishers/",
    "originBody": "April 10, 2024 21 Comments On April 9, Twitter/X began automatically modifying links that mention “twitter.com” to read “x.com” instead. But over the past 48 hours, dozens of new domain names have been registered that demonstrate how this change could be used to craft convincing phishing links — such as fedetwitter[.]com, which until very recently rendered as fedex.com in tweets. The message displayed when one visits goodrtwitter.com, which Twitter/X displayed as goodrx.com in tweets and messages. A search at DomainTools.com shows at least 60 domain names have been registered over the past two days for domains ending in “twitter.com,” although research so far shows the majority of these domains have been registered “defensively” by private individuals to prevent the domains from being purchased by scammers. Those include carfatwitter.com, which Twitter/X truncated to carfax.com when the domain appeared in user messages or tweets. Visiting this domain currently displays a message that begins, “Are you serious, X Corp?” Update: It appears Twitter/X has corrected its mistake, and no longer truncates any domain ending in “twitter.com” to “x.com.” Original story: The same message is on other newly registered domains, including goodrtwitter.com (goodrx.com), neobutwitter.com (neobux.com), roblotwitter.com (roblox.com), square-enitwitter.com (square-enix.com) and yandetwitter.com (yandex.com). The message left on these domains indicates they were defensively registered by a user on Mastodon whose bio says they are a systems admin/engineer. That profile has not responded to requests for comment. A number of these new domains including “twitter.com” appear to be registered defensively by Twitter/X users in Japan. The domain netflitwitter.com (netflix.com, to Twitter/X users) now displays a message saying it was “acquired to prevent its use for malicious purposes,” along with a Twitter/X username. The domain mentioned at the beginning of this story — fedetwitter.com — redirects users to the blog of a Japanese technology enthusiast. A user with the handle “amplest0e” appears to have registered space-twitter.com, which Twitter/X users would see as the CEO’s “space-x.com.” The domain “ametwitter.com” already redirects to the real americanexpress.com. Some of the domains registered recently and ending in “twitter.com” currently do not resolve and contain no useful contact information in their registration records. Those include firefotwitter[.]com (firefox.com), ngintwitter[.]com (nginx.com), and webetwitter[.]com (webex.com). The domain setwitter.com, which Twitter/X until very recently rendered as “sex.com,” redirects to this blog post warning about the recent changes and their potential use for phishing. Sean McNee, vice president of research and data at DomainTools, told KrebsOnSecurity it appears Twitter/X did not properly limit its redirection efforts. “Bad actors could register domains as a way to divert traffic from legitimate sites or brands given the opportunity — many such brands in the top million domains end in x, such as webex, hbomax, xerox, xbox, and more,” McNee said. “It is also notable that several other globally popular brands, such as Rolex and Linux, were also on the list of registered domains.” The apparent oversight by Twitter/X was cause for amusement and amazement from many former users who have migrated to other social media platforms since the new CEO took over. Matthew Garrett, a lecturer at U.C. Berkeley’s School of Information, summed up the Schadenfreude thusly: “Twitter just doing a ‘redirect links in tweets that go to x.com to twitter.com instead but accidentally do so for all domains that end x.com like eg spacex.com going to spacetwitter.com’ is not absolutely the funniest thing I could imagine but it’s high up there.” This entry was posted on Wednesday 10th of April 2024 10:28 AM Other DomainTools.com Mastodon Matthew Garrett phishing Sean McNee Twitter.com X.com",
    "commentLink": "https://news.ycombinator.com/item?id=39991173",
    "commentBody": "Twitter's pivot to x.com is a gift to phishers (krebsonsecurity.com)351 points by todsacerdoti 19 hours agohidepastfavorite369 comments cyxxon 19 hours agoSo, Twitter did a clbuttic mistake in 2024 and went live without testing this, presumably? reply nfriedly 18 hours agoparentFor those unaware: https://thedailywtf.com/articles/The-Clbuttic-Mistake- reply duffyjp 13 hours agorootparentThat was a fun read. I fired up a Valheim server for my kids (and me, let's be honest) and it censored part of the word \"Basement\" in my server name. :) reply 65 15 hours agoparentprevClbuttic -> Cl-ass-ic -> Cl-butt-ic reply k1rd 15 hours agoparentprevreminds me of teh cloud-to-butt chrome extension https://chromewebstore.google.com/detail/cloud-to-butt-plus/... reply EdwardDiego 9 hours agoparentprevIs this like the Scunthorpe Problem? reply nojs 14 hours agoparentprevI mean I recently saw an Airbnb ad with something like pREMOVED instead of pagoda, so I guess it happens to everyone reply extraduder_ire 20 minutes agorootparentI was wondering what kind of rude word an 'agoda' is. Turns out it's a competing website to airbnb. How incredibly petty. reply ryandrake 19 hours agoprevEverything about this rebranding has been baffling to me. First, the original brand was great and pretty much untarnished, and there was no split/merger of business happening that would encourage it. There doesn't seem to be a business purpose to rename it. Second, these hamfisted attempts to try to get the new (terrible) name to stick. It's just not going to work. The huge number of existing users will always think of it as Twitter. It will at best become The Service Formerly Known As Twitter. It just feels like in software when you get a new Product Manager on the project who just wants to superficially \"leave his mark\" on the product in some way and then move on. Except this PM paid billions to do it. reply rco8786 19 hours agoparentApparently Elon has been trying to push the \"X\" brand on things throughout his career, but always had someone stop him until he had complete control of things. https://en.wikipedia.org/wiki/X.com_(bank) https://www.washingtonpost.com/history/2023/07/25/elon-musk-... reply mocha_nate 15 hours agorootparentHis first payment company before he merged with PayPal was X.com reply Hamuko 18 hours agorootparentprevIs there a company he owns where \"X\" is not a thing? There's X the social network, Tesla Model X, SpaceX. reply hyperhopper 18 hours agorootparentHe literally got removed from Paypal, \"his first company\" for trying to push the x.com thing there. That's why he owns the domain. It was stupid then and he held a grudge ever since which is why he's pushing it everywhere else since. reply dboreham 17 hours agorootparentHe owned x.com before he went to PayPal (they acquired his company which was called: x.com). When ousted, the domain continued to be owned by PayPal until later they ended up selling it and eventually he bought it back. reply Domenic_S 16 hours agorootparentCorrect! I had an @x.com email address when I worked at PayPal a long time ago. reply dboreham 14 hours agorootparentFun fact: I worked with one of the people who owned x.com prior to Elon (and sold it to him, reputedly for more $$ than he wanted to pay). reply valianteffort 18 hours agorootparentprevNo he got removed for trying to switch their servers from linux to windows. reply HillRat 15 hours agorootparentHe also got removed for creating systemic, existential risk for the company by handing out $10k line limit credit cards to anyone who wanted one, resulting in a 50% chargeback rate. There are so many reasons for them to have gotten rid of him that the answer is highly underdetermined. reply Yeul 12 hours agorootparentInteresting this got AliBaba into trouble with the CCP. As 2008 proved handing out credit can lead to disaster. reply pavon 18 hours agorootparentprevI haven't seen any strong evidence that that was the real reason he was ousted. It seems more to me like something that was too good not to go viral, rather than actual fact. reply maxlamb 17 hours agorootparenthttps://www.washingtonpost.com/history/2023/07/25/elon-musk-... reply genewitch 13 hours agorootparentwho owns the washington post? reply AlchemistCamp 14 hours agorootparentprevThat’s not true. It was mostly over their servers. Elon Musk’s startup was running Windows while Peter Thiel and Max Levchin’s was running Linux. Musk wanted to migrate everything to Windows due to the more mature (at the time) APIs and Levchin was adamant on staying with Linux. He was also busy trying to fight fraud, which was becoming an existential threat to the company, and had no time for migrating to a different platform. Ultimately, Levchin gathered several other key people in the company, and went to the investors threatening to quit (and destroy the company) unless they brought back Thiel as CEO. The board sided with Levchin, Theil returned and Musk was out of day-to-day operations. If you’re interested in the PayPal story, I highly recommend Jimmy Soni’s book, The Founders. https://www.amazon.com/Founders-Paypal-Entrepreneurs-Shaped-... reply rsynnott 12 hours agorootparent“More mature (at the time) APIs” I mean, while nothing about the web was particularly mature at the time, the idea that Windows was more mature than Linux for it is… peculiar, from the point of view of someone who was messing around with web programming at the time. Note how much difficulty Microsoft had migrating Hotmail to Windows. reply AlchemistCamp 11 hours agorootparentWere you working in anything banking-related? AFIK, Microsoft was dominant in that sector. reply PaulDavisThe1st 15 hours agorootparentprevThere was quite an extended period where all the developer docs for PayPal were on x.com/something ... I found it all quite confusing. reply extraduder_ire 13 minutes agorootparentprevThe model X is part of a separate scheme, to have models S, 3, X, and Y. The 3 was initially meant to be an E until ford sued to prevent it. reply jsheard 18 hours agorootparentprevDon't forget he also named one of his kids X (the Æ A-12 part is their middle name so it's just X for short). There's also xAI, which I think is technically another distinct company rather than part of X/Twitter. reply denton-scratch 18 hours agorootparentThe guy's a kook. He's like Michael Jackson: a very rich guy, with complete control over who he meets. Unsurprisingly, the people he does meet all tell him what they think he wants to hear. I mean, who's going to tell him something he doesn't want to hear, and then see their name smeared all over Twitter by the richest man in the world? I expected Twitter to crash and burn as soon as he took over; I was wrong. I guess it's like Truth Social - if you're fabulously wealthy, you can run a social media site that's a complete train-wreck, without it ruining you. /me never had a Twitter account. Nor Faceache. reply askafriend 17 hours agorootparentWell Michael Jackson is also one of the most talented musicians and entertainers of the last 100yrs. His wealth is the least impressive or defining thing about him. I do get the point you're trying to make though. Just thought that emphasis was a little off. Just like with Elon, you have to consider that extreme outcomes are the result of extreme people or people that were forged in extreme circumstances (which is certainly true of Michael Jackson). reply arp242 14 hours agorootparentI feel both Musk and Jackson could have greatly benefited from a few more people telling them \"you idiot, that's fucking mental\". In general being in a position where this doesn't happen is not good for your sense of reality (very rich, very famous, a lot of political power, or something else). In that sense Michael Jackson having sleepovers in his bed with 12-year olds[1] and Musk's x.com rename are very similar. [1]: taking the most generous interpretation; no comment on whether he was a nonce or not. reply Philadelphia 12 hours agorootparentAlso Michael Jackson’s having an amateur anesthesiologist putting him under every night with a wide variety of easily lethal sedatives, and Musk’s use of ketamine and mushrooms and stimulants and whatever else he’s on this week. reply askafriend 11 hours agorootparentprev> I feel both Musk and Jackson could have greatly benefited from a few more people telling them \"you idiot, that's fucking mental\". I agree with you but imagine if Elon had listened when people said the same thing about starting an electric car company, a reusable rocket company or a brain-interface company. Or if Michael Jackson had listened to all the people that told him the sounds on his album were too crazy and fused too many genres together. Ultimately it's a fine line between genius and crazy. You have to acknowledge that often where genius resides, so does crazy. I mean look at Steve Jobs, clearly incredible talent - but also harbored some unconventional beliefs about medicine which many say ultimately led to his premature death. Even Achilles had his heel. reply arp242 11 hours agorootparentMost smart/innovative people don't pull off this kind of weird behaviour. Lots of not-so-smart people do the exact same shitty behaviour. It's just a bullshit excuse for bullshit behaviour. And those weren't widely held views either so it's not even accurate. reply Atotalnoob 17 hours agorootparentprevNot fair to Michael Jackson. He had talent and defined multiple cross cultural trends/memes (not memes as in internet memes). He was deeply troubled, but he still had a huge, lasting impact reply denton-scratch 16 hours agorootparentI didn't mention Jackson's talent (nor Musk's). I mentioned the singer in passing, in the context of a \"rich kook\", and I implied that he was a kook because he was rich. The Orange Man is another example. It does seem to me that extreme wealth is a severe risk factor for untreatable kookiness. And arguably, being a talented entertainer is a risk factor for extreme wealth. I don't deny Jackson's talent, and I assume Musk is very good at several things I've never tried. I would cheerfully forgo even modest wealth, if the deal was I didn't have to be like them. reply iLoveOncall 15 hours agorootparentIt's more that extreme wealth allows kookery without consequences. As Musk said it himself when asked what he thought about the fact that his tweets may cause billions of dollars of stock drops: \"I don't care\". reply singleshot_ 13 hours agorootparentInstead of a lack of consequences, I’m concerned that the consequences of kookery for the rich is more money. reply vanviegen 17 hours agorootparentprevAre you suggesting that Musk is not having huge lasting impact? Perhaps his name won't be as sticky as Michael Jackson, but some of his companies are definitely changing society, partly thanks to him. reply steve_adams_86 15 hours agorootparentI'm not a Jackson fanatic, but everything Musk is doing could have (and very likely would have) been done by someone else in a relatively close timeframe. I won't claim Jackson couldn't have had a similar counterpart on an alternate timeline or there won't be more like him, but there are remarkably few artists and performers who are so prolific and talented to such a degree that they noticeably shift the direction of popular culture for decades. If Musk went away today, frankly I don't think many people would think of him within 5 years or so. reply ffsm8 16 hours agorootparentprevMusk likes to take credit for things he at most participated in. Electric cars where already gaining traction by the time Tesla got their production in order, so if he had an impact there, its only in marketing a product you could only buy from other companies. The charger network maybe? But wasn't most of that heavily subsidized? SpaceX's success is massively overstated, every time it's brought up. Their rockets are still not stable and if you actually tally up all the money they've received from the state, it'd be way more expensive - even adjusted to inflation - then the space shuttle launches from the nineties. What other impact are you thinking of? The hyper loop? The solar rooftop's? His vaporware robots? The boring company? Everything turned out to be pure hype with hilariously overstated success. Or maybe the autopilot which is still only usable by people that enjoy gambling with pedestrian lives? reply itsoktocry 15 hours agorootparent>Electric cars where already gaining traction by the time Tesla got their production in order, so if he had an impact there, its only in marketing a product you could only buy from other companies I'm as cynical about Tesla as anyone, as my comment history will show. I think they played fast and loose with financial data when celebrity and anything tech put you above the rules. If he had the enemies he has now, I don't think they would have made it. But...Elon Musk is a force. I was skeptical of him in, maybe, 2016, but the guy has managed to continue to do stuff no one else seems capable of, even in the face of haters. There's no way pre-Elon Tesla does what he did, I don't believe it for a second. Even Twitter; yes he fired too many people and it's a bit of a fiasco (and hard to kill, apparently). But the fact that he's using it to explicitly push an agenda is wild. There's no one else like him. reply ffsm8 14 hours agorootparentAnd yet you fail to name something. What tomoyoirl said is correct. Musk's talent is mainly in raising hype which results in incredible funding. he's definitely the most successful person at that since Steve Jobs. Everyone that ever worked in large enterprise knew that Twitter would be fine after the mass terminations. The only thing you lose at that point is the ability to maneuver, the platform will be mostly automated and barring incompetence or malice, things will just keep chugging along with a skeleton crew. reply nebula8804 12 hours agorootparent> Electric cars where already gaining traction by the time Tesla got their production in order, so if he had an impact there, its only in marketing a product you could only buy from other companies. The charger network maybe? But wasn't most of that heavily subsidized? Given how shit even current EVs are compared to the early Model S, you are totally wrong. It was always at most a side project for ALL the existing OEMs as we now know. They have have had chance after chance after chance to prove the thesis that \"if they cared, they could produce a Tesla killer overnight\". None of the OEMs have produced anything that is competitive with Tesla in all metrics (price, range, features etc.) all of them compromise in one or more areas. The only real serious competitors are the Chinese. There is a reason when teardown analysis reports are offered on all the EVs, the Chinese care about one company and one company only: Tesla. Everyone else is just a follower and the Chinese are not even bothering to waste their time looking at them. You even see it in their actions. Tesla is the only \"legacy western\" company without a JV partner in China. China can happily dump all the other losers any time they want. They can't afford to lose Tesla so they have accommodated them. >SpaceX's success is massively overstated, every time it's brought up. Their rockets are still not stable and if you actually tally up all the money they've received from the state, it'd be way more expensive - even adjusted to inflation - then the space shuttle launches from the nineties. Are you for real? In 2021 they launched ~380 Metric tons of mass to orbit while the rest of the world combined launched about 400 metric tons. In 2022? They were double what the rest of the world did and finally in 2023, they were 80% of all the mass to orbit launched. When I mean rest of world that includes: Rest of the US industry + Europe + India + China + Japan + Russia + everyone else. If you look at the other launch providers in the US they get far more subsidies and have delivered not even a fraction of what SpaceX has provided. Can you think of many industries where a single company is doing 80% of worldwide effort? And they are on track to increase that by 50% this year (a metric they will likely achieve given their track record). In the last couple years they have sent 42 humans to orbit and back. They year they are on track to do their first spacewalk. Starlink now has 2.3 Million customers in over 70 countries. I am seeing this outright dismissal of Musk more and more since the whole Twitter saga. Its reeks of ignorance just like all the people that repeatedly make fun of Apple users as idiots who just get duped by fancy marketing. Even in 2024 people here make that silly argument. Musk bashing is the new version of that. It just makes you look ignorant because you are so blinded by what you don't like that you have dismissed all these amazing achievements that no one else is doing. reply ffsm8 4 hours agorootparentI think you're interpreting something into my comments I didn't say. My point is that every product was hilariously under delivered, not that the product itself is unusable. Let's address the products you're citing: The Tesla model s was marketed heavily on price and the full self driving. There were essentially no electric cars in the 100k price range the Tesla cost, so comparing the cars at the time the car was announced with when the car actually was delivered, 5 yrs later is extremely questionable. But without Musk's hype, we probably wouldn't have the rivian etc, as they were all riding his hype wave. But we'd still have electric cars. Just less then we've got right now. Now SpaceX's. It got billions of taxpayers money, there can be no second company because nobody else can get such funding. In the nineties pretty much everything was delivered by NASA. It just threw in the towel for price reasons, so Musk came along and promised a lunar base & manned missions to mars, netting him all contacts. So yes, now we have SpaceX. The only player that can deliver things to orbit, because Russia is too poor, European nations somehow don't want to spend money on it and China is mostly interested for military applications, so they're not publishing what they're actually delivering to orbit. They're definitely shipping things however, you occasionally get leaked videos from failed launches that spread toxic fumes close to population centers and similar fuckups. What you're using as an argument is really an inevitability. Starlink is another highly exaggerated product, which is still decent value if you need/want an Internet connection in an area that doesnt have usable cable connections. It's not the cheapest, nor the most expensive. It's in the middle. It's a solid choice (and so are the Tesla), it just didn't have quite as much impact as people attribute to it. I'm not even bashing Musk. What I said from the start is that his contributions are exaggerated. reply DylanDmitri 8 hours agorootparentprevSpaceX employees take the Musk fundraising and spend it well. They have systems in place to minimize his technical interference. reply nebula8804 5 hours agorootparent>SpaceX employees take the Musk fundraising and spend it well. They have systems in place to minimize his technical interference. Sounds like nonsense used to wave away his success with SpaceX. If you look at his interviews, it seems like he is most involved in SpaceX of all his ventures. He can explain deep technical details of the product whereas this is less true for Tesla and has been proven many times that he cannot do the same for Twitter. reply tomoyoirl 16 hours agorootparentprevMusk’s true talent is hype, fundraising, and getting buy-in from a crowd of people who Want To Believe. He does an incredible job of this, attracting capital investment on absurdly favorable terms. (Delivery on his wild promises, well, sometimes the true believers he hires make that happen, sometimes not.) reply nkozyra 14 hours agorootparent> Musk’s true talent is hype, fundraising, and getting buy-in from a crowd of people who Want To Believe. I wonder how long this lasts, though? The more we see of him, the less smart/magical he seems other than to devotees. I feel like his getting in the limelight has pulled the curtain back quite a bit. reply ajford 15 hours agorootparentprevExactly. Musk makes a passable \"hype man\" that would do great on a sales pitch. But it's the same story as every sales team where he promises so much that isn't feasible to deliver on the timelines he promises. reply rurp 16 hours agorootparentprevHe took a roughly breakeven business and turned it into a money incenerator. If Musk actually had to answer to a board or investors things would shut down or change drastically. That'll still happen at some point but he has enough money to subsidise his crazy vanity project for a long time. reply jrockway 17 hours agorootparentprevBecause Tesla has been significantly underperforming the rest of the S&P 500, Elon is not the world's richest person anymore. He's in like 4th place or something. reply bdjsiqoocwk 14 hours agorootparentprevOf all the pointless rich people on the planet, you chose Michael Jackson to make that point? So weird. reply SketchySeaBeast 17 hours agorootparentprevEx-Wives reply elcritch 18 hours agorootparentprevSeems Musk always had a desire to use \"X\" from his pre-Paypal days. He made a boastful post about buying Twitter, didn't actually want to follow through but was forced to do it by the courts. My take is that Musk then sorta went \"f-it, I had to buy Twitter. I might as well try and make it into X.\" reply ziddoap 18 hours agoparentprev>Everything about this rebranding has been baffling to me. First, the original brand was great and pretty much untarnished I always thought the rebrand was a complete shame, if only for the reason that \"tweet\", meaning \"to make a posting on the Twitter online message service : to post a tweet\" is in the dictionary! What a waste to throw that away. reply aidenn0 17 hours agoparentprevHe really likes X. He even named one of his kids that (well X Æ A-12, with the spaces). The A-12 is indeed a reference to the plane that came out of the oxcart project. Æ Musk pronounces \"Ash\" which is apparently an accepted name for the character, which was (among other things) used as a latinization of the futhorc rune[1] that means \"ash tree\" 1: HN won't let me paste the rune, not sure if it's limited to BMP on purpose but you can see it on https://en.wikipedia.org/wiki/Ansuz_(rune) reply segasaturn 16 hours agorootparentThey government let him name his kids that? I remember a story a long time ago about a couple that tried to name their kid \"Brfxxccxxmnpcccclllmmnprxvclmnckssqlbb11116\" which is about as meaningful as \"X AE A-12\", but was blocked by naming laws reply eurleif 15 hours agorootparentThat was in Sweden, which has naming laws. The US does not have similar laws. reply Symbiote 13 hours agorootparentprevIronically, Æ is the most useful part of this child's name. Ash short for Ashley or Ashleigh is common for girls and boys in Britain. Wikipedia suggests the name is mostly for girls in the USA. reply yowzadave 15 hours agorootparentprevTwo kids! (A girl, named Exa, in addition to X). reply tumsfestival 13 hours agorootparentExa sounds stupid, but at least it makes sense. reply lol768 18 hours agoparentprev> Everything about this rebranding has been baffling to me. Everything about the purchase and the way the company has been run since Elon took the reigns has been baffling. The bizarre forced-push of the X brand is just the tip of the iceberg. reply JeremyNT 18 hours agorootparent> Everything about the purchase and the way the company has been run since Elon took the reigns has been baffling. The bizarre forced-push of the X brand is just the tip of the iceberg. I dunno, is it that baffling? It seems like he really loved using the product but didn't like the leadership, and he just wanted to own it so he could mess around and have fun following his own whims. Once you get the idea that he doesn't actually care about financial success it all seems pretty reasonable. Like any hobby, for X/Twitter to be a \"success\" it just has to amuse him, and based on his usage of the platform it seems to be doing that. The amounts of money he's losing are staggering to us but also meaningless to him. Our society has allowed him to accumulate so much wealth that nothing he could do \"wrong\" in a business sense would meaningfully impact his lifestyle. reply NewJazz 17 hours agorootparentIf the money he is losing is meaningless to him, why has he launched lawsuits to back out of the purchase and sue media watchdog orgs? Why did he replace himself as CEO with an advertising exec? Seems like the money is pretty meaningful to him. reply dehrmann 17 hours agorootparentIt's really just an extreme version of a boat. They seem like fun, but they're money pits, and a lot of work goes into keeping them running. Alternatively, it's the Cartmanland scenario. reply JeremyNT 17 hours agorootparentprevI imagine you're right that the lost cash does have meaning to him, but it doesn't appear to be the primary motivator for his decisions (and is definitely not driving his near-term decision making). reply ethanbond 13 hours agorootparentPrimary motivator being insecurity and rapidly deteriorating mental health reply babypuncher 17 hours agorootparentprev> Our society has allowed him to accumulate so much wealth that nothing he could do \"wrong\" in a business sense would meaningfully impact his lifestyle. This is the frustrating part. If I went around my office tomorrow endorsing nazi propaganda, I would be out of a job by the end of the day and probably struggling to pay my mortgage in a few months. But this fuckstick can do whatever he wants and never face any real repercussions. He could bankrupt Twitter, SpaceX, and Tesla and just decide to retire early on a private island. It's so incredibly hard to actually fuck things up when you're rich that it's downright impressive when someone like SBF comes along and manages to actually do it. reply vbezhenar 17 hours agorootparentSo you're incredibly envy of person who actually can express freedom of speech? If anything, the problem is with your workspace which forces a particular political viewpoint on you. reply babypuncher 15 hours agorootparentNo, I think actions should have consequences. I'm frustrated that being born into money can make you functionally immune to most of those consequences. There are two sides to freedom of speech. You have the freedom to say dumb shit, and I have the freedom to not associate with you because I don't like the dumb shit you say. If one of my employees started expressing pro-Nazi sentiments at the office, I would fire them, because I have a right to do so and because I believe the rest of my employees have a right to a safe working environment where they don't have to put up with people who think they are inferior just because of their race or cultural background. reply vbezhenar 1 hour agorootparentNazi is strong word that you're throwing around. But let's say you're pro-republican manager and you're firing pro-democratic workers (assuming you're US citizen). Or the other way around. Does it still work for you? Where's the line? reply chii 7 hours agorootparentprev> expressing pro-Nazi sentiments at the office ... nazi speech would incur this consequence because it is classified as hate speech. However, termination of employee cannot apply to just _any_ speech that the boss doesnt like. Of course, the boss would have other ways to \"manage out\" that troublesome employee, but directly firing for the speech cannot be one of them. reply freejazz 14 hours agorootparentprev> Once you get the idea that he doesn't actually care about financial success it all seems pretty reasonable. Yes, once one throws reason out the window, just about anything becomes \"reasonable\" reply okr 18 hours agorootparentprevnext [3 more] [flagged] lenerdenator 18 hours agorootparentWho cares about the handbook way? Idk. Probably the people who remain at Twitter, and to a certain extent, the investing institutions that were courted to help fund the acquisition. Of course, they probably hedged, so they don't care as much, but still. This is a case study in how concentration of capital in the hands of one person can go terribly wrong for the purposes of resource allocation. reply djbusby 17 hours agorootparentIsn't Twitter valuation down like 60% from that purchase price? That's a tough hedge. reply jsheard 18 hours agoparentprevNearly a year into the rebrand x.com still redirects to twitter.com, rather than vice versa, which you'd think would be the first thing they'd want to fix. reply duxup 18 hours agorootparentThat's one of those situations that feels like: \"Executive just hasn't noticed / lost attention span, and engineer is leaving a workaround for a bad call in place.\" reply Hamuko 18 hours agorootparentprevI imagine that changing it would break a lot of things, otherwise they would've done it already. Copying a link to a tweet already makes it an x.com link too. reply shadowgovt 18 hours agorootparentprevBecause domain names are tied to security model, they're often the last thing you can fix. So let's say, hypothetically, they build in a redirect from twitter to x-dot-com. Off the top of my head... - All logins are now busted. Some percentage of users is lost forever because they can't remember their login credentials and instead of going through the recovery flow, they go use Bluesky. - A huge amount of third-party integrations are busted because they aren't using client libraries that understand redirects - A full code audit is necessary. Someone has hard-coded twitter.com into a critical system somewhere. Other people have referenced a variable, but it's the wrong variable. Still others are looking up the value in a database somewhere that doesn't have a search frontend anyone knows about. And some other database has a huge cache of absolute URLs it vends and everyone who built it got fired by Musk. This is probably the most predictable-cost step, but it's still a cost to be paid. - A significant number of users are confused. The median of web user is profoundly ignorant of how the web works, and no matter how much you warn them and how much you prepare them, day-of-switch they will panic. Staff up your support team. Customers-lost-forever-two-point-oh. - Every business integration needs to be updated. Google App Store, Apple App Store, Amazon Appstore... They all have bindings to twitter.com, and some part of their flow will panic and flag a security issue if they see it's turned into a redirect to elsewhere. That probably triggers a security audit of every version of the Twitter client (and those companies aren't particularly inspired to foot the bill on Musk's behalf, billionaire that he is...). Hell, Google indexes twitter.com via a dedicated side-pipe. Will that side-pipe handle a redirect? (source: I've been in the side-seat for a merger-become-rebrand, and the number of things people expect to \"just work\" and don't is impressive). reply mschuster91 15 hours agorootparent> All logins are now busted. Some percentage of users is lost forever because they can't remember their login credentials and instead of going through the recovery flow, they go use Bluesky. That includes everyone who had 2FA active before Musk made that a \"premium\" feature and subsequentially lost their 2FA device. What a clusterfuck, that one. > Every business integration needs to be updated. Google App Store, Apple App Store, Amazon Appstore... They all have bindings to twitter.com, and some part of their flow will panic and flag a security issue if they see it's turned into a redirect to elsewhere. And that's assuming the integrations even support changing the primary domain name in their OAuth backend, which a lot of them will not. Or you have appliances that got made years ago when Twitter integration was the fad of the day - I 'member there's a fridge out there that showed tweets on its screen -, game consoles or other devices that don't get firmware updates any more. reply duskwuff 13 hours agorootparent> Or you have appliances that got made years ago when Twitter integration was the fad of the day - I 'member there's a fridge out there that showed tweets on its screen Most of those are probably already broken. Twitter dropped support for most third-party API clients a few years ago. reply pjc50 18 hours agoparentprevKetamine is a hell of a drug. https://www.wsj.com/business/elon-musk-illegal-drugs-e826a9e... reply zettabomb 18 hours agoparentprevSo far I really haven't seen anyone seriously call it just X. Most news orgs seem to resort to \"X (formerly Twitter)\" or similar. Some still call it Twitter, not even an acknowledgement that it's been renamed. At least Meta had the sense to just change their app splashscreens and such (e.g. Facebook by Meta). And it seems that Alphabet doesn't make any effort to make their presence known. reply baobabKoodaa 18 hours agorootparentThe thing that really bothers with me with this is why couldn't it just be \"Twitter by X\"? You want to make an \"everything app\", that's great Elon, let's call that X. Now what do we call all the mini apps inside the everything app? Oh, they're called \"X\", too? So you're using \"X of X\" to call a cab, and \"X of X\" to send a message, and these are different apps inside the mega app? How does this naming make sense? reply user_7832 16 hours agorootparentThat would be too sensible. Sarcasm aside, they probably were hoping to convert the brand name twitter has to 'x', but failed to realize how sticky the name was/is. reply jsheard 18 hours agorootparentprevEven X itself resorted to putting \"Formerly Twitter\" in its App Store and Play Store taglines after their daily installs fell off a cliff. Previously the tagline was just \"Blaze your glory!\" but nobody knows what that means. reply rchaud 14 hours agorootparentProbably had backend developers write the marketing copy. reply rsynnott 18 hours agorootparentprev> Most news orgs seem to resort to \"X (formerly Twitter)\" or similar. I mean, if nothing else, \"X did [something stupid]\" just looks like someone forgot to fill in a template; no-one is going to publish an article with 'X', unqualified, in it. reply tzs 18 hours agorootparentX also makes search harder. I'd like to see HN add a recommendation that in headlines about X the submitter should change the X to Twitter. reply foobarchu 9 hours agorootparentprevShoot, even the Wikipedia page is still titled \"Twitter\" reply nearlyepic 18 hours agoparentprevPresumptuously: It's stupid and irrational because this wasn't a decision that was made based on reason - it was an emotional one. Elon is a bag-holder. He bought x.com in the dotcom boom and doesn't want to admit that the domain he paid a good chunk of money for is worthless - hence the (failed) attempt to make a brand out of it. reply dom96 18 hours agorootparentAny ideas how much he paid for the domain back then? reply pjc50 18 hours agorootparentIt was actually bought back from paypal in 2017, for an undisclosed sum. https://www.theverge.com/2017/7/10/15949862/elon-musk-x-com-... reply twisteriffic 4 hours agorootparentAnd now he's sold it to twitter and pocketed the cash. reply ben_w 18 hours agoparentprev> The Service Formerly Known As Twitter Was thinking almost exactly this while reading a recent BBC article — their style guide appears to be that the company's name is \"X, formerly Twitter,\" reply nprateem 18 hours agorootparentYou have to write that (or similar) in UIs too since X just looks like a mistake/null value. reply hiccuphippo 12 hours agorootparentI once tried to click in the X logo to close a modal. reply andsoitis 18 hours agoparentprev> Everything about this rebranding has been baffling to me. First, the original brand was great and pretty much untarnished, and there was no split/merger of business happening that would encourage it. There doesn't seem to be a business purpose to rename it. Stated goal is to gradually transform it into an “everything app” — https://theconversation.com/elon-musk-aims-to-turn-twitter-i... reply baobabKoodaa 18 hours agorootparentFirst, fire 80% of developers. Then, make the remaining developers create an \"everything app\" (in addition to the workload they already have with the Service Formerly Known As Twitter app). Something, something. Profit ??? reply gwern 17 hours agorootparentAside from being a long-standing obsession of Musk's, the thing about the 'everything app' is that it's the Hail Mary move which could make the Twitter thing anything but a dumpster fire immolating $20b+, Musk's reputation, and several years of his rapidly-shortening QALYs. If you force as many people as possible to subscribe, you can then flip them to the 'everything app' and bootstrap a big enough bloc of customers to matter that you control their demand. (cf. Stratechery). It's not going to work, but it is the only story you can tell yourself and employees about how the Twitter saga ends in any way other than Musk losing interest and getting distracted by AI again and Twitter spiraling into the drain and possibly being dumped into bankruptcy by its debt load. reply beede 8 hours agorootparentDoesn’t a bankrupt Twitter lead to someone else buying it for a fire sale price that presumably could just change it back to … Twitter? reply gwern 7 hours agorootparentYes. reply omegaworks 12 hours agorootparentprev>it's the Hail Mary move which could make the Twitter thing anything but a dumpster fire immolating $20b+ While Twitter wasn't making money hand over fist, it was bringing in $4,500million in ad revenue prior to the rebrand and had years where it was marginally profitable. Musk's saddling Twitter with interest on the debt he paid to buy the company is one of the reasons it's immolating money. reply gwern 12 hours agorootparentYeah, that's the problem. Buying it was so bad an idea and so overpriced that he had to load the blue bird down with debt. He can't just quit Twitter and toodle off and say 'well, I stopped wokeism, and that justifies destroying my reputation and 3 years even as my other ventures like Tesla run into major strategic problems while I was distracted'. So right now, Twitter is \"default dead\", as pg might put it. There is a viable business there... but not one that can service the Musk debt load indefinitely while enduring all the usual shocks & risks. So he's either got to put in a lot more money, tell a story which will get someone else to put in a lot more money, or lose it. reply hgs3 17 hours agorootparentprevI think the \"everything app\" already exists. It's called a web browser. reply andsoitis 8 hours agorootparentWeChat users will tell you something else. https://en.wikipedia.org/wiki/WeChat reply mparnisari 18 hours agoparentprev> It just feels like in software when you get a new Product Manager on the project who just wants to superficially \"leave his mark\" on the product in some way and then move on. Except this PM paid billions to do it. This is how it goes in all big companies :( reply throw2022110401 17 hours agorootparentTrue but there is no way it would be implemented in such a half-assed way at any other big company (including pre-Musk Twitter). Stuff like this makes it obvious that the people who are still there no longer give a fuck, they just do what they are told with the minimum effort required to collect the paycheck. reply rurp 13 hours agoparentprevMaybe they need a new slogan to help folks with the transition. Something like: \"Don't tweet, Xcrete!\" reply MattGaiser 19 hours agoparentprevThe purchase was never about business in the first place, so the running of it could be just as bad. reply fundad 18 hours agorootparentExactly! Ownership is not in it for revenue. They'll say they don't care about revenue to everyone who asks. To be baffled, one has to ignore all of that. There is strong nostalgia for aw-shucks persona of an inventor-turned-business-owner. https://www.cnbc.com/2023/05/16/cnbc-exclusive-cnbc-transcri... https://www.rev.com/blog/transcripts/dealbook-summit-2023-el... reply adql 17 hours agorootparentWell, admitting otherwise would be admitting failure of managing that company and we can't have that! reply visarga 18 hours agoparentprevWhat can you do if he likes \"X\". It used to sound cool at some point decades ago. reply Philadelphia 12 hours agorootparentThe X Games, X-treme X-men, Xander Cage in XXX era reply babypuncher 17 hours agoparentprevEveryone I know has partially adopted the new brand and started calling it Xitter. reply xboxnolifes 17 hours agorootparentUntil just now, every time I saw someone in HN mention Xitter I assumed it was a third-party client like Nitter. reply WorldMaker 17 hours agorootparentprevWhich is especially fun to pronounce with the Pinyin 'x': https://en.wikipedia.org/wiki/Pinyin reply drivingmenuts 16 hours agoparentprevThe best new name I've seen is Xitter. Very fitting, IMO. reply rsynnott 18 hours agoprevBeyond the incredibly botched implementation, the actual _idea_ is very funny; the 1984 approach to rebranding. Twitter, the Unwebsite. Like, how the hell could he think this would actually work. reply comboy 19 hours agoprevThat's a beginner mistake, it's really hard to make if you dealt even a bit with some regular expressions. reply tracker1 18 hours agoparentProbably not even a regex, just a straight replace. Hopefully at the render layer and not the backend data. s/\\btwitter\\.com\\b/x.com/ig reply remram 13 hours agorootparentYou made the same mistake with the same amount of confidence. 1. your version replaces at-twitter.com with at-x.com 2. your version replaces twitter.com.au with x.com.au reply perihelions 19 hours agoprevI don't understand the social structure inside a software company where this kind of thing can go from some intern's 3am idea to production, without passing many layers of gatekeepers, any one of which should have swiftly flagged this down. It's not that the string replacement was implemented wrongly (that too)—it's that they're touching, in any manner at all, one of the most obviously-sensitive UX things in their product. Without a commensurate amount of security review. Like, in my imagination, within five minutes of anyone seeing this, a person with responsibility would have stepped in and said \"No, you can't do this. And if you insist on doing this, here's five layers of audits and sign-offs that this needs to go through first, because the thing you're proposing is potentially really dangerous\". Am I thinking it about it wrongly? I cannot understand at all. reply pjc50 18 hours agoparent> Like, in my imagination, within five minutes of anyone seeing this, a person with responsibility would have stepped in and said \"No, you can't do this. And if you insist on doing this, here's five layers of audits and sign-offs that this needs to go through first, because the thing you're proposing is potentially really dangerous\". Am I thinking it about it wrongly? Which part of \"anyone who is not a Musk yes-man has already been fired or quit\" are you having trouble with? reply lenerdenator 18 hours agorootparentIt's worth remembering, there are two kinds of yes-men: 1) the sycophant who loves authoritarian institutions; the \"true believers\" and 2) the young, brilliant visa holder who was the talk of his parents' social circle in Hyderabad three years ago when he graduated and was able to get on board at a household-name NorCal tech company, but who is now being abused by the employer who sponsors the thing that lets him stay in the US. You'll always have type one; some humans simply love following a dolt. The second type is a result of our laws, and laws can be changed to keep people like Elon from taking advantage of workers. reply triceratops 17 hours agorootparentThe visa holder thing might have been true for maybe 3 months before and after the takeover. The job market was absolutely on fire at the time (early 2022). Anyone who wanted to leave should have been able to, especially considering they were good enough to be hired at Twitter. reply clipsy 16 hours agorootparent> The visa holder thing might have been true for maybe 3 months before and after the takeover. The job market was absolutely on fire at the time (early 2022). The Twitter takeover (and subsequent layoffs, ultimatum, etc) happened in late 2022.[0] [0]: https://en.wikipedia.org/wiki/Acquisition_of_Twitter_by_Elon... reply triceratops 16 hours agorootparentBut it was announced in April. And it wasn't hard to predict what would happen post-acquisition. reply Jcowell 16 hours agorootparentI would argue laying off an insane chuck of the company was not predicable. Some? Yes happens all the time. What Elon did? No. But visa holders would probably be the first to go. But if you did manage to survive, no way you’re quitting in that job market and getting a new gig in a saturated market. reply lenerdenator 5 hours agorootparentVisa holders wouldn't be the first to go. Not by a long shot. 1) They come from places that have much lower pay for software engineers. Seriously, people forget just how much more American tech companies pay their devs than just about anywhere else. That means that you can wave a smaller pay package in front of them and they'll be more likely to take it. Replacements from local labor markets are more likely to know what they're actually worth. 2) If they aren't making you 100% happy and they're at-will employees, you can hang their entire lives over their heads. Do you want to have to pack up everything in a few weeks and move back to your home country in shame after your families sacrificed so much to get you here? No? Sounds like you'd better sleep in that conference-room-turned-bedroom, then, and be ready to work 90 hour weeks until this company's in the black. reply triceratops 5 hours agorootparentYou're thinking of people who work at consulting body shops. Literally none of these things applied to visa holders at Twitter. reply lokar 18 hours agoparentprevGo re-read all the comments here during the take over and layoff where people claimed it could not possibly take more then a handful of people to run such a simple site. reply misiti3780 17 hours agorootparenthe fired a huge number of the staff and the site is still running, so how was that assumption not proven correct ? reply smith7018 17 hours agorootparentI use twitter daily and the site is a shell of its former self. It's slow, prone to bugs, filled with bots, the amount of real users has cratered, user reports go nowhere, there's no support team, the ads are now bot accounts posting crap like \"Today is a good day, be sure to make it advantageous\", there are no new features besides previously in-flight projects pre-Musk, they've actually removed a lot of features (like Circles, block lists, etc), and much more. He took an otherwise functioning social media service and forced it into maintenance mode. He also fired all of the people that keep the user base alive so now it's flooded with bots (which he presumably likes so he can boast about engagement being up). So yes it's still around but it's dying and the skeleton crew he has left can't do anything. In other words, he destroyed it. reply deprecative 7 hours agorootparentHonest question, why on earth are you on a fascist social media platform? reply mschuster91 15 hours agorootparentprevThe bot plague is atrocious. Like, there are tons of \"keyword watcher\" bots... write \"onlyfans\" and you'll get ~5-10 spambots in under half a minute, and for stuff involving popular politicians or political events (anything Russia, Ukraine, Israel, Palestine, Covid) you'll get Russian fake newspaper clones. On top of that come the \"human bots\" - write the name of infamous German youtuber \"Drachenlord\" and you'll get that vile hater bunch and it's just the same. reply sangnoir 16 hours agorootparentprev> he fired a huge number of the staff and the site is still running, so how was that assumption not proven correct ? Why would you expect the website to stop running though? Keeping a site running with a smaller crew is easy amd baked in - all organizations with >10 engineers do this frequently, over the holidays or Lunar New year. What's harder is building new features at the same pace and quality as a larger engineering team. reply sandofsky 17 hours agorootparentprevThe site was previously engineered such that an uninformed owner could literally start unplugging servers without it going down. That said, outage frequency did rise following the mass layoffs. https://www.nytimes.com/2023/02/28/technology/twitter-outage... reply lokar 17 hours agorootparentprevYou are what I love about this site. reply swid 18 hours agoparentprevDon't take this as a defense of what is a harebrained idea; but this kind of replacement should be easy to do correctly. You know; in such a way where only real twitter.com links are changed to x.com. Honestly it is only somewhat surprising to me that no one noticed the error ahead of time. On the one hand, this is the type of mistake I do see in reviews from time to time... usually in the form of a regex that is not anchored to the start of the string, or perhaps it uses a non escaped period which of course means \"any character\" in regex. On the other hand, it is revealing about the kinds of controls in place that it got through. reply notatoad 18 hours agorootparenteven things that are easy (or maybe, especially things that are easy) can benefit from having a review to make sure you didn't miss something obvious. nothing more dangerous than \"oh, that's easy, let's push it straight to prod\" reply seanhunter 14 hours agorootparentExample: one of the worst and most costly bugs I ever saw during my time in finance involved a code review [1] with the exchange: \"Looks good generally but this will serialize incorrectly if a date is ever negative.\"\"Why would a date ever be negative?\" Noone could think of a reason a date would ever be negative. The resulting code push cost millions in a single day. [1] I was on the call because I was the SRE[2] responsible for doing the release [2] We didn't call ourselves SREs because it was like 2001 and that terminology hadn't caught on yet. We called ourselves \"Installmeisters\" believe it or not. reply zestyping 12 hours agorootparentHow did it turn out that a date was negative, and how did it cost millions? reply seanhunter 4 hours agorootparentIirc it turned out some 20+ year old code was relying on an undocumented/unknown behaviour of dates when they are negative to calculate cashflows on very old perpetual fixed income instruments[1]. If you have a codebase that's large enough and worked on by enough people, for every possible hack/undocumented feature you'll have at least one person who will rely on it in some way. It cost millions, because the serialization corrupted the USD treasury curve when we wrote it to the database, which meant that even though the code was reverted quickly, for a while no derivatives (like literally no derivatives) in the whole of the trading part of a big IB could price (because essentially all derivatives were priced in dollars using some sort of discounted cash flow mechanism which relies on the USD treasury curve). No price means no risk, because how you calculate risk of a derivative is by finding the underlyers, shocking those underlyers a little bit and calculating the new price (essentially an empirical method of finding the derivative of price with respect to each of those things). [1] https://en.wikipedia.org/wiki/Perpetual_bond reply perihelions 13 hours agorootparentprevThat's a good hypothesis with the non-escaped period! That someone wrote /twitter.com/ for the string substitution, which almost works, and then added a second one like /\\w+.twitter.com/ for subdomains, which also seems to work, and would pass simple tests to check if it works or not. Matches everything it's supposed to; rejects most of the things it shouldn't. reply segasaturn 18 hours agoparentprevWell, that is the specific outcome Elon wanted when he laid off three-quarters of the company. He very clearly stated that he didn't see the value of the \"trust and safety\" teams that would have been the ones to flag something like this down. reply ceejayoz 18 hours agorootparentThis wouldn't have fallen within \"trust and safety\" purview. This should've been caught in even the most cursory of code reviews. reply somenameforme 18 hours agorootparentIt seems this may have only affected the X on iOS App. [1] That greatly expands the range of possible causes. It also makes this quite odd in another way as well, because it suggests this was not a server side change. [1] - https://news.ycombinator.com/item?id=39991312 reply sangnoir 16 hours agorootparentprevWhy wouldn't phishing fall under trust and safety? Perhaps not primarily - but it's definitely a trust and safety event. reply ceejayoz 16 hours agorootparentThey wouldn't \"have been the ones to flag something like this down\". It should've been caught long prior; the code shouldn't have been written, it shouldn't have survived code review, and it should've failed automated tests. It's a trust and safety incident now, but it never should have been. reply blasphemers 12 hours agorootparentprevThe trust and safety team was only responsible for censoring wrongthink reply sangnoir 17 hours agoparentprev> I don't understand the social structure inside a software company where this kind of thing can go from some intern's 3am idea to production This is what happens when you \"cut the fat\" and are left with an adversely-selected[1], skeleton crew of \"hard core engineers.\" The site was never going to fail all at once, instead, it's a death by a thousand cuts and suboptimal engineering. 1. No disrespect to current Twitter engineers who can't leave easily, or believe in the mew mission. However those who survived layoffs but could leave have left. reply rtkwe 18 hours agoparentprevWell it starts with the bad idea probably coming from the top, Musk saying I'm tired of seeing twitter.com links change them all to look like x.com links, that plus his gutting of the company when he took over means there's less people around to be the person to say no you can't do this this way go back and start over (or not at all). reply quandrum 18 hours agorootparentIt's probably worse with Musk. His executive style seems to be, from his biography, ignore a thing for a while until he gets in a maniac phase and then over the shoulder manage a thing until it's done, regardless of time or context. I can just take the scene of Musk being on a roof yelling at the crew to change how they install solar tiles late in the night and translate it to him berating a programmer in the office to make it look like x.com and not caring about the details. reply SmartJerry 13 hours agoparentprevThis article was 5 hours old at the time I'm viewing it and the bug is supposedly already fixed per the article itself. So yea, seems like this was probably fixed within 5 minutes of anyone noticing. reply renewiltord 17 hours agoparentprevWell, that’s because any org that has five layers of audits for this just has five layers of audits for everything and so rarely gets anything done. This is a clbuttic bug. It’s silly and damaging but easy to fix and move on with. Not particularly different from Bluesky allowing one guy to own all of S3. reply zettabomb 18 hours agoparentprev>I don't understand the social structure There is none. Stop trying to understand - it's a fool's errand. >a person with responsibility would have stepped in There are none left. They were either laid off, or they left before that could happen. >\"No, you can't do this\" ... is the last thing anyone says to Elon Musk before getting fired. reply greenthrow 18 hours agoparentprevAnyone who would tell Elon \"this isn't a good idea\" left or was fired. reply scblock 18 hours agoparentprev> some intern's 3am idea I think you mean \"Elon Musks random demand\". reply MattGaiser 19 hours agoparentprev> without passing many layers of gatekeepers Given all the Twitter bugs and issues, it seems they have all been laid off. reply rsynnott 18 hours agoparentprevI mean, I assume anyone who could get another job has already left, so they're probably running low on competent gatekeepers. reply JTbane 15 hours agoprevI'm just sad they killed the blue bird, and the \"tweet\" verb as well. reply remram 13 hours agoparentI'm not using xcreting so I'm sticking with tweeting until the platform dies. reply PartiallyTyped 12 hours agorootparentWell, I think that's a fair characterization for a non trivial portion of the users. reply maxlin 15 hours agoparentprevIt's certainly less weird in official contexts now though. The brand was OK but not really \"scalable\" without sounding like something right off Idiocracy in some contexts. reply coldpie 19 hours agoprevI closed my Twitter account when this guy took over, for all the obvious reasons. Somewhat to my surprise, it actually turned out to be a massive boost to my mental health. For a week after I closed the account, I'd find myself thinking, \"I'm bored, I should check Twitter... oh wait, I can't\" and then I'd just go on with my day. It was fantastic and I don't miss it at all. So in a weird way, my life got better when he took over. Definitely recommend closing your account if you're on the fence. Don't move to bluesky or whatever, just take this opportunity to cut all this crap out of your life. You don't need it. reply ghusto 18 hours agoparentIt's okay to be bored. Boredom serves a purpose. When boredom is taken away from you, you end up not making an effort for anything worthwhile. People worry about what (web caused) divisive propaganda, erosion of social skills, and attention grabbing is doing to us — and I agree those are all real and serious threats — but the lack of boredom is worries me the most. reply pksebben 18 hours agoparentprevdid the same for facebook circa 2020, and even in lockdown it was a godsend. Obliterated my reading list and actually managed to make headway on the massive list of mothballed projects. reply isoprophlex 19 hours agoprevSo, basically, someone's broken regex can be a viable business model. Pretty crazy. reply pksebben 19 hours agoparentyou mean, someone's broken regetwitter reply jjice 18 hours agoparentprevSomeone seriously did a s/twitter/x/g seemingly (or the raw string replace equivalent). Maybe there are more requirements here, but it seems like just parsing a URL and checking for `twitter.com` and some other literal domains instead of sub strings would have been completely fine. reply giancarlostoro 19 hours agoparentprev“Now you have two problems” reply ben_w 18 hours agorootparent\"OK, what about using AI to solve this?\" https://benwheatley.github.io/blog/2022/12/13-20.26.24.html reply arp242 14 hours agorootparentThe regexp example on https://sourcegraph.com/cody has been broken for months (scroll down a bit, second block). Also not a regexp problem and easier solved without regexps. Please, no one tell them. It's funnier the longer it goes on. It's been like this since at least October (I told a friend on Telegram this), but it had already been like this for some months by then. reply ben_w 14 hours agorootparentIndeed. Extra fun, I asked gpt-4-0125-preview *: Explain this regex: [a-zA-Z0-9\\.\\-]+\\.([a-zA-Z]{r,63}) And part of the explanation was to fix the bug. Well, almost, it removed the () in the process, but it did know what was wrong. When your AI's error can be explained by someone else's AI… * Why this model? Because the https://chat.openai.com is currently throwing me the error: \"You've reached the current usage cap for GPT-4. You can continue with the default model now, or try again later. Learn more\" even though I've selected 3.5 in the popup, and my earlier attempt to use ChatGPT to give myself a PAYG chat interface to all the models was done when gpt-4-0125-preview was the best one available. https://benwheatley.github.io/YetAnotherChatUI/ reply jdorfman 9 hours agorootparentprevYour secret is safe with me. reply thrdbndndn 19 hours agoprevI just posted carfatwitter.com on twitter. It did not become carfax.com. What am I missing? Also, the article says: > The domain “ametwitter.com” already redirects to the real americanexpress.com. But it does not here. reply ceejayoz 19 hours agoparenthttps://mashable.com/article/twitter-dot-com-posts-change-to... > X eventually realized the issue and rolled out a patch later that same day for some of the domains affected by this change. \"Netflitwitter.com\" no longer shows up as \"Netflix.com\" for example. > However, Mashable can confirm that the X for iOS app is currently still changing many other references of \"Twitter.com\" to \"X.com.\" We noticed that in one instance we found, the change was happening when \"Twitter.com\" was being used in a subdomain for another URL. reply HyprMusic 19 hours agoparentprevI believe it only happens on the iOS app. Strange the article doesn't mention that. reply mikeyouse 17 hours agorootparentIt started off happening globally - they fixed it in most places, but not yet on the iOS app. reply latexr 18 hours agoparentprevFrom the article: > Update: It appears Twitter/X has corrected its mistake, and no longer truncates any domain ending in “twitter.com” to “x.com.” reply quaunaut 19 hours agoparentprevAre you visiting from the iOS(possibly Android too, but didn't see anyone mention) app? That's where it's generally happening. reply thrdbndndn 19 hours agorootparentI use the website. reply lolinder 19 hours agoparentprevProbably that they rolled back the change when it became obvious it was bad. reply thrdbndndn 19 hours agorootparentThe article is pretty fresh (\"This entry was posted on Wednesday 10th of April 2024 10:28 AM\", and the author is from the US. Even if he's on East Coast, it would only be half an hour ago) though. Unless Twitter just fixed it within 1 hour, I think the author should mention it has been fixed (edit: or that it was limited to certain platform [iOS?], since it's not reproducible on web at least.) reply rsynnott 18 hours agorootparentPer the older Mashable article above, it is _partially_ fixed, in that they don't do it for the examples in the article but do do it for other cases. reply krebsonsecurity 18 hours agorootparentprevThanks. I did update the story to reflect the apparent fix. I'm still trying to verify if this behavior remains in some form. reply dindobre 18 hours agoprevGlad the site is rotting tbh, it wasn't great before and now it's so full of bots and propaganda it feels surreal. Posting and/or reading from Ukraine is even more surreal. reply renlo 18 hours agoprevif `twitter.com` is mapped to `x.com`, then a link `carfatwitter.com` will go to the non-malicious `carfax.com`, so registering `carfatwitter.com` seems to be just a stunt. When would `carfax.com` redirect to `carfatwitter.com`? Urls with `twitter.com` in the name are affected, not urls with `x.com` in the name. edit: from the responses looks like I was wrong; the urls still point to `carfatwitter.com`. Leaving my comment up in case others were confused like me. reply code_duck 18 hours agoparentIt appears the substitution only affected the text of the link, not the destination. reply filleokus 18 hours agoparentprevIt's not redirecting but rather rewriting of the URL. e.g \"https://twitter.com/{acc}/status/{id}\" -> \"https://x.com/{acc}/status/{id}\". So if you post \"https://carfatwitter.com/scam\" it will be rewritten to \"https://carfax.com/scam\". Essentially search and replace of twitter.com -> x.com, 's/x.com/twitter.com/g'. reply jerf 18 hours agoparentprevI infer that the display was getting rewritten, but the underlying target of the link would not. So if you posted \"carfatwitter.com\", the UI would display \"carfax.com\" but the underlying link would still go to \"carfatwitter.com\". Note I have no direct experience with this, it's just the only way this makes sense as a phishing vector. The alternative is that it is being presented as a phishing vector, but was never actually useful as such, and people are just jumping up to yell about a security issue without it actually being one. That happens too. reply mcphage 18 hours agoparentprevThe links themselves are unchanged, just how they display. So if you type carfatwitter.com in a tweet, then it will display as carfax.com, but if you click on the link, it will still redirect you to carfatwitter.com. reply __MatrixMan__ 18 hours agoprevPSA we don't have to let billionaires own letters if we don't want to. We can just keep calling it twitter if we want. reply maxlin 15 hours agoprevWell that title didn't take long to go entirely invalid as this isn't a thing anymore. Reasonable to doubt if this ever was thing as this seemed to only exist on iOS and due to the direction this went it didn't really do anything. Probably different if it had gone x -> twitter reply shadowgovt 18 hours agoprevIt's been a bizarre ride watching Twitter slowly unravel under the new leadership. It'll have a long way to fall... the total userbase is still around the same order of magnitude as the population of the United States. But when I read stories of decisions like this, I can't help but think that it indicates the adults are no longer in the room, and a 300-million-plus userbase becomes a massive target surface if it's being run by a team that doesn't really grok the Internet... reply tinyhouse 19 hours agoprevSince most of my tweets where related to work, I moved from Twitter / X to LinkedIn. Twitter under Elon is a huge mess. The irony is that he kept complaining about spam and bots before. Since he took over, my new followers and many of the likes I received were from new only fans like users. Maybe it's by design and he wants to make it an only fans clone. But I'm out. I don't even bother reading my feed yet alone posting. LinkedIn has its problems too. I would say it's the least bad among the two. reply tracker1 18 hours agoparentMy biggest issue with LinkedIn is their horrible mobile web interface. I didn't trust their app. I know it's been years, but they burned anything resembling trust. reply carl_dr 13 hours agoprevProgrammers love cooking, they like chopping the caret. reply carl_dr 13 hours agoparentCome on guys, this is a good joke. new_host = host.replace( /twitter\\.com/, “x.com” ) (I wonder if they have escaped the period?) reply yifanl 19 hours agoprevThe now rare clbuttic self-inflicted wound. reply victorbjorklund 13 hours agoprevJesus. How badly is elon running things? reply AlphaCerium 19 hours agoprevRelevant XKCD: https://xkcd.com/1031/ reply kibwen 19 hours agoprev> On April 9, Twitter/X began automatically modifying links that mention “twitter.com” to read “x.com” instead. But over the past 48 hours, dozens of new domain names have been registered that demonstrate how this change could be used to craft convincing phishing links — such as fedetwitter[.]com, which is currently rendered as fedex.com in tweets. I'm in awe. Does Twitter have any software developers left, or is it just Elon's nephew working his way through W3Schools? reply rsynnott 18 hours agoparentEverything that is old is new again: http://news.bbc.co.uk/1/hi/sci/tech/2138014.stm Possibly, as with the extremely 90s-feeling 'X' name, Musk is simply introducing retro bugs, to attract internet hipsters :) reply jader201 18 hours agoparentprevI’m confused how this can be used for phishing? Does the text only get replaced, and the underlying link stays intact? Surely not? Otherwise, the link would still go to fedex.com. reply aavshr 18 hours agorootparentYes, only the text gets replaced but not the underlying link. Once you navigate to the link it will be the actual link in the URL bar however. But this is an easy miss for people after they have already navigated to the link from twitter thinking it's a legitimate link. reply sylens 18 hours agorootparentprevExactly it. Please use your Microsoft gaming account to login to my phishing site, xbotwitter.com reply bluedel 18 hours agorootparentprevNo, you got that right. Only the displayed text is affected. reply dom96 18 hours agorootparentprev> Does the text only get replaced, and the underlying link stays intact? Surely not? That is exactly what happens. reply jader201 18 hours agorootparentThat’s almost literally unbelievable. I’m not even sure his nephew is reading W3Schools. In case you’re reading this, you may have missed this article: https://www.w3schools.com/html/html_links.asp reply campbel 18 hours agorootparentprevYeah, looks like _only_ the link label gets replaced, otherwise as you suggest it wouldn't be as bad. reply myself248 18 hours agorootparentWhy do browsers allow this by default? Seems like a feature made to enable phishing and other bad behaviors. reply aavshr 18 hours agorootparentThe link I just clicked on to reply to your comment was `https://news.ycombinator.com/reply?id=39991931&goto=item%3Fi...` but thankfully it just said 'reply' in the UI. reply Karellen 17 hours agorootparentI wonder if there's a browser extension that checks if the link text is a valid URL, but is a different URL (or just on a different domain?) than the actual link target, and adds some kind of warning for the user if so? I'm not sure what keywords I'd use to find an extension like that. reply kibwen 16 hours agorootparentThis would break every website that wants to track what links you click on by sneakily rewriting the link under your nose. Which, to be fair, is a use case that I'm all for breaking, but it would make Google mad, so it won't happen. reply Karellen 16 hours agorootparent> This would break every website that wants to track what links you click on So, a plan with no drawbacks? > it would make Google mad, so it won't happen. Google doesn't control which browser extensions get written? reply adamors 18 hours agoparentprevThere a lot of people on visas who can’t quit, so … reply wannacboatmovie 17 hours agorootparentDo you have any evidence of this? It is a borderline racist trope that keeps being repeated (mostly by ex-Twitter developers with a chip on their shoulder) without any hard proof. It is equally likely they are upset over a lack of solidarity in quitting and are perpetuating a lie based on someone's citizenship status. reply genter 18 hours agoparentprevI had to clean the coffee off my keyboard after reading that. reply redundantly 18 hours agoparentprevIt's Elon himself working his way through W3Schools. reply NelsonMinar 18 hours agoparentprevApparently they have just one developer left who knows just enough to break things. Maybe they use Grok. Anyway some engineer wrote this change and deployed it. The product process failed though, as did the testing process. Rollback was a mess too, given this change was visible for hours (days?) reply hankchinaski 18 hours agoparentprevMove fast and break things I guess reply gofreddygo 5 hours agoparentprevFFS. Maybe a tweet with a rickroll to spacetwitter.com will catch his eye. Whoever owns that domain, here's your 15 mins. reply chuckadams 18 hours agoparentprevMusk fancies himself a coder, so he probably wrote it himself and pushed it to master without review; or he did have it reviewed and fired anyone who pointed out the mistake. reply wannacboatmovie 18 hours agoparentprevSalty ex-Twitter developers too busy huffing their own farts to remember when the pre-Elon CISO (mudge) was fired after he uncovered how messed up things were behind the scenes, then filed a whistleblower complaint and testified before Congress. Twitter was not a marvel of engineering pre-Elon, despite the fantasy arrogant former developers keep perpetuating. Their infrastructure was barely held together with duct tape. reply some-guy 17 hours agorootparent1) Twitter wasn't a marvel of engineering pre-Elon 2) It's worse post-Elon (bot issues, paid blue checks at the top, increase in hateful posts, etc) reply vlan0 18 hours agoparentprev>Does Twitter have any software developers left Not many people in the field actually understand protocols at the RFC level. That's the real crux. reply lokar 18 hours agorootparentNo, that is not the issue. This is far more basic. reply vlan0 18 hours agorootparentMore words? reply SketchySeaBeast 18 hours agorootparentIt's a simple string substitution at the display level - fedetwitter.com becomes fedex.com, but at the link level fedetwitter.com remains. It's just replacing the content of the a tag, but not underlying href location. reply lokar 18 hours agorootparentprevI’m not sure what to tell you, you don’t need to have ever read a single RFC to see that this was not going to work. It’s just basic careful thinking. reply williamsmj 18 hours agoprevIf the CTO builds an engineering team that lacks the expertise to write a regex that matches the company's own name, and builds a culture of security and quality process that failed to catch this before shipping to production, then the CTO should be fired for cause. Unfortunately the CTO also owns the company. reply mise_en_place 17 hours agoparentI wouldn’t go that far but there is a teachable moment here. It goes to show how little engineers actually read PRs/MRs these days…it’s just become a rubber stamp at this point. This is not a problem specific to X, I’d wager a lot on that. It’s a widespread cultural problem in our industry. Remember XZ? reply cdelsolar 19 hours agoprevIt is insane that this would get past QA or any sort of testing. reply i80and 19 hours agoparentElon fired all the people who would do QA. Something new on twitter breaks every week. It's wild. reply patwolf 18 hours agorootparentI haven't decided if twitter is actually more buggy or if bugs are more publicized because of the extra scrutiny since Musk took over. There used to be a bug that wreaked havoc for a short time where you could force anybody to follow you. At the time I don't recall people blaming Twitter's leadership or culture--it was just a bug because software sometimes has bugs. reply Justsignedup 11 hours agorootparentTwitter was notorious for crashing. The fail whale was a internet meme... Then they fixed it and for about 7 years I basically never saw a problem with twitter, even in the worst internet conditions. Today the bugs are back. Things are constantly a problem. This isn't a \"well all software has bugs\" -- yeah true, but somehow for 7 years twitter was bug-free. Or rather the bugs were quickly caught and fixed. And now... nothing. Musk got rid of all guardrails, and now we face the consequences. reply CSSer 16 hours agorootparentprevSince Musk took over, I’ve frequently clicked on links to tweets only to be presented with the “Something went wrong. Try again.” Error message instead of the tweet I should be seeing. I then have to refresh multiple times to get it to show up. Sometimes it doesn’t work at all, I decide I don’t care enough, and I move on with my day. I’ve also observed broken embeds around the web. This is core service reliability. I grew up with Twitter. I never observed this behavior once before Musk took over unless there was a well-known incident occurring. Twitter is definitely more buggy. reply spiderice 15 hours agorootparent> I never observed this behavior once before Musk took over unless there was a well-known incident occurring This bug has been happening since the beginning of time. I saw that constantly pre-musk reply timr 15 hours agorootparentprev> I never observed this behavior once before Musk took over unless there was a well-known incident occurring. Yeah, I have. On multiple occasions before Musk took over, Twitter also had bugs. Even if you think firing like 6/7th of Twitter's staff (or whatever) was a mistake, there's really no arguing that they're making more visible product changes than they have in years. That means that bugs happen. And frankly...proving that a tech company doesn't need that level of bloat is a valuable lesson for the entire software industry. If they overshot and have X% more bugs for now, that's fine. They'll still be dramatically leaner than before. reply CSSer 15 hours agorootparentGreat. This is all completely besides the point anyway. My point is that there are more of them and I think anyone who doesn’t see that is nuts. I’m sorry I don’t have a Grafana dashboard to show you. reply timr 15 hours agorootparentThe point is, without a Grafana dashboard, you don't have anything but a motivated anecdote. reply CSSer 15 hours agorootparentI never pretended to have anything else, and “motivated” is a weird word to use here. I have no skin in this game, and from what I can tell neither do you. If Twitter survives, great. I guess some of what you noted could be an interesting lesson, but I don’t think it’s going so hot so far and I’m not willing to call it yet. reply timr 11 hours agorootparentYou have a belief that the service is more buggy, and were arguing as such from anecdote. That's what I mean by \"motivated\". I don't care at all, and am just reacting to the phenomenon of folks going \"it's definitely worse now, and I know, because I used the product before\". OK, great...I used the product before too, and don't perceive that to be true. reply javajosh 15 hours agorootparentprev~90% of the comments here are \"motivated anecdotes\" and are quite valuable. Perhaps this is not the dis you think it is. reply billy99k 9 hours agorootparent\"motivated anecdotes\" aren't valuable because they aren't accurate. It's political fuel for to shit on something you don't like. With this line of thinking, I could say climate change isn't happening because of the temperature outside my house. reply apercu 13 hours agorootparentprevIt's totally possible that there was a ton of bloat. It happens when organizations scale and they're looking for market share instead of profitability. But, as someone who has worked in (bio)tech companies that got purchased, first they cut 20-30% of the staff, but that doesn't mean all the work they were doing went away, and it means key people in every role have to do more, which works for a minute, then management thinks \"we're right, it was all fat\", until those key people burn out and stop producing. Then you end up hiring 10-15% of the staff back. Or, the org slowly dies. reply timr 11 hours agorootparent> as someone who has worked in (bio)tech companies that got purchased, first they cut 20-30% of the staff, but that doesn't mean all the work they were doing went away, and it means key people in every role have to do more Yeah, this wasn't a simple staff reduction. Twitter fired something like 80% (?) of their employees. By conventional wisdom, they should have gone down hard and never recovered. There are lots of people in this comment thread who are (dubiously) trying to make the case that they did. The Twitter example, for better or worse, revealed that tech companies are not just a little bit redundantly staffed...they're employing tons of people who effecitvely just make work for each other. reply ryandrake 10 hours agorootparent> There are lots of people in this comment thread who are (dubiously) trying to make the case that they did. > The Twitter example, for better or worse, revealed that tech companies are not just a little bit redundantly staffed...they're employing tons of people who effecitvely just make work for each other. I've been arguing since the purge that Twitter was in the \"Wile E Coyote ran off the cliff and is still treading air\" phase, but that argument is getting harder and harder to justify as Twitter somehow stays alive. How could firing all those people really do nothing to the operational success of the company? What were they all doing? Companies don't hire people where they have nothing for them to do. I look at my (BigTech) company and everyone is running around like crazy with 3-5X more tasks in their backlog than they can possibly do. We always need more people. How do you get rid of 80% and just carry on??? Are they all just writing TPS reports for each other to read? reply whateveracct 14 hours agorootparentprevuhh visible product changes as proof that a company is more productive is a joke. A joke PMs love to tell at least lmao. Making visible product changes is the easiest thing to do in software. reply seoulmetro 10 hours agorootparentprevPeople keep complaining that: - Twitter staff were needed - Twitter will fail if the firings continued - He just fired all these useful people But at the end of the day, Twitter/X is still the same exact thing and runs exactly the same way. It does prove that firing all the slack can change very little but it's a good narrative to cry about Elon Musk and his \"sinking soonTM ship\". reply ttepasse 14 hours agorootparentprevI recently tried to reinstall the Twitter for Mac app. It was never a good app. Although a flagship for Apple’s Catalyst, since its catalysation it never got the UI love other 3rd party clients for the Mac had. But it was at least somewhat usable. But somehow it got never updated for the X'ning, neither name nor icon, never got support for longer tweets and other stuff. And now I click Install in the App Store and \"something goes wrong\". Simply can’t install it. There must be UIKit people remaining at X, after all they’re updating the iOS app which must share the same code as the Mac app. But for some reason they don’t update, but leave a broken, uninstallable app on the App Store. reply sandofsky 14 hours agorootparentFrom day one, the Mac app was a hobby project from passionate developers within the company, at times maintained by a single engineer. With the layoffs, they likely lost everyone passionate about maintaining it. reply nojs 14 hours agorootparentprevAs devils advocate, maybe this is evidence such QA doesn’t matter as much as everyone thinks? It looks like no actual damage was done and the problem was fixed quickly once users noticed it. reply warkdarrior 13 hours agorootparentSure, using your users as QA is a fine business plan and instills confidence. We should extend this to other expensive industries (healthcare, law, civil engineering, etc.). reply northhanover 12 hours agorootparentIt seems fine for social media reply hex0x0000 11 hours agorootparentTesting is necessary even for social media, not doing so is a big security threat for its users. Even apparently dumb bugs like this one can be exploited to steal important information. Social media may not seem as important or vital as banking or healthcare, but most of our modern society depends on it. Socials are the best tools for social engineering after all. reply maxlin 15 hours agorootparentprevNo, more like something new on X breaks every year. The site is simple enough that that the engineering is mostly focused on optimizations and there's little more I or any other active user would even expect from the platform, outside of growing to new areas like YouTube-comparable video. I find it quite bizarre how some people still try to push some narrative about the site being broken. It's not more broken it was before the takeover outside the odd Spaces dropout (which is a new feature so doesn't really count), but is now better serving its actual mission as an actually unbiased speech platform, radical as that is in the current day. Now X just needs to stay unchanged for ten years to overcome any competition. reply Etheryte 19 hours agoparentprevI doubt there's much in the way of QA or testing after they made developers print their code out on paper for Elon to review. reply laborcontract 11 hours agoparentprevI mean, forget QA. How does this get pushed at all? reply zzzeek 18 hours agoparentprevpretty sure they just pushed it live without any messy \"testing\" reply 106 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Twitter/X mistakenly auto-modified links referencing \"twitter.com\" to \"x.com,\" allowing the registration of possible phishing domains, which could redirect users to fake sites.",
      "Defensive registrations were quickly made after the error was rectified to block scammers from acquiring these domains.",
      "The incident sparked a mix of humor and worry among users and cybersecurity experts due to the oversight by Twitter/X."
    ],
    "commentSummary": [
      "Twitter's rebranding to x.com under Elon Musk's helm has sparked confusion, security concerns, and backlash, leading to debates on Musk's power and the consequences of immense wealth.",
      "Users express worries about technical glitches, AI in programming, phishing threats, coding errors, and software reliability following the acquisition.",
      "Discussions also include topics like workforce cuts, testing protocols, and how link manipulation could impact the platform."
    ],
    "points": 351,
    "commentCount": 369,
    "retryCount": 0,
    "time": 1712759737
  },
  {
    "id": 39995725,
    "title": "Aider: Collaborate with AI for Pair Programming",
    "originLink": "https://github.com/paul-gauthier/aider",
    "originBody": "aider is AI pair programming in your terminal Aider is a command line tool that lets you pair program with GPT-3.5/GPT-4, to edit code stored in your local git repository. Aider will directly edit the code in your local source files, and git commit the changes with sensible commit messages. You can start a new project or work with an existing git repo. Aider is unique in that it lets you ask for changes to pre-existing, larger codebases. Getting started Example chat transcripts Features Usage Tutorial videos In-chat commands Tips Installation Voice-to-code FAQ Discord Blog Getting started See the installation instructions for more details, but you can get started quickly like this: $ pip install aider-chat $ export OPENAI_API_KEY=your-key-goes-here $ aider hello.js Using git repo: .git Added hello.js to the chat. hello.js> write a js script that prints hello world Example chat transcripts Here are some example transcripts that show how you can chat with aider to write and edit code with GPT-4. Hello World Flask App: Start from scratch and have GPT create a simple Flask app with various endpoints, such as adding two numbers and calculating the Fibonacci sequence. Javascript Game Modification: Dive into an existing open-source repo, and get GPT's help to understand it and make modifications. Complex Multi-file Change with Debugging: GPT makes a complex code change that is coordinated across multiple source files, and resolves bugs by reviewing error output and doc snippets. Create a Black Box Test Case: GPT creates a \"black box\" test case without access to the source of the method being tested, using only a high level map of the repository based on tree-sitter. You can find more chat transcripts on the examples page. Features Chat with GPT about your code by launching aider from the command line with set of source files to discuss and edit together. Aider lets GPT see and edit the content of those files. GPT can write and edit code in most popular languages: python, javascript, typescript, php, html, css, etc. Request new features, changes, improvements, or bug fixes to your code. Ask for new test cases, updated documentation or code refactors. Aider will apply the edits suggested by GPT directly to your source files. Aider will automatically commit each changeset to your local git repo with a descriptive commit message. These frequent, automatic commits provide a safety net. It's easy to undo changes or use standard git workflows to manage longer sequences of changes. You can use aider with multiple source files at once, so GPT can make coordinated code changes across all of them in a single changeset/commit. Aider can give GPT-4 a map of your entire git repo, which helps it understand and modify large codebases. You can also edit files by hand using your editor while chatting with aider. Aider will notice these out-of-band edits and keep GPT up to date with the latest versions of your files. This lets you bounce back and forth between the aider chat and your editor, to collaboratively code with GPT. If you are using gpt-4 through openai directly, you can add image files to your context which will automatically switch you to the gpt-4-vision-preview model Usage Run the aider tool by executing the following command: aider ... If your pip install did not place the aider executable on your path, you can invoke aider like this: python -m aider.main Replace , , etc., with the paths to the source code files you want to work on. These files will be \"added to the chat session\", so that GPT can see their contents and edit them according to your instructions. You can also just launch aider anywhere in a git repo without naming files on the command line. It will discover all the files in the repo. You can then add and remove individual files in the chat session with the /add and /drop chat commands described below. If you or GPT mention one of the repo's filenames in the conversation, aider will ask if you'd like to add it to the chat. Think about the change you want to make and which files will need to be edited -- add those files to the chat. Don't add all the files in your repo to the chat. Be selective, and just add the files that GPT will need to edit. If you add a bunch of unrelated files, GPT can get overwhelmed and confused (and it costs more tokens). Aider will automatically share snippets from other, related files with GPT so it can understand the rest of your code base. Aider also has many additional command-line options, environment variables or configuration file to set many options. See aider --help for details. In-chat commands Aider supports commands from within the chat, which all start with /. Here are some of the most useful in-chat commands: /add : Add matching files to the chat session. /drop : Remove matching files from the chat session. /undo: Undo the last git commit if it was done by aider. /diff: Display the diff of the last aider commit. /run : Run a shell command and optionally add the output to the chat. /voice: Speak to aider to request code changes with your voice. /help: Show help about all commands. See the full command docs for more information. Tips Think about which files need to be edited to make your change and add them to the chat. Aider has some ability to help GPT figure out which files to edit all by itself, but the most effective approach is to explicitly add the needed files to the chat yourself. Large changes are best performed as a sequence of thoughtful bite sized steps, where you plan out the approach and overall design. Walk GPT through changes like you might with a junior dev. Ask for a refactor to prepare, then ask for the actual change. Spend the time to ask for code quality/structure improvements. Use Control-C to safely interrupt GPT if it isn't providing a useful response. The partial response remains in the conversation, so you can refer to it when you reply to GPT with more information or direction. Use the /run command to run tests, linters, etc and show the output to GPT so it can fix any issues. Use Meta-ENTER (Esc+ENTER in some environments) to enter multiline chat messages. Or enter { alone on the first line to start a multiline message and } alone on the last line to end it. If your code is throwing an error, share the error output with GPT using /run or by pasting it into the chat. Let GPT figure out and fix the bug. GPT knows about a lot of standard tools and libraries, but may get some of the fine details wrong about APIs and function arguments. You can paste doc snippets into the chat to resolve these issues. GPT can only see the content of the files you specifically \"add to the chat\". Aider also sends GPT-4 a map of your entire git repo. So GPT may ask to see additional files if it feels that's needed for your requests. I also shared some general GPT coding tips on Hacker News. Installation See the installation instructions. FAQ For more information, see the FAQ. Kind words from users The best AI coding assistant so far. -- Matthew Berman Hands down, this is the best AI coding assistant tool so far. -- IndyDevDan Aider ... has easily quadrupled my coding productivity. -- SOLAR_FIELDS It's a cool workflow... Aider's ergonomics are perfect for me. -- qup It's really like having your senior developer live right in your Git repo - truly amazing! -- rappster What an amazing tool. It's incredible. -- valyagolev Aider is such an astounding thing! -- cgrothaus It was WAY faster than I would be getting off the ground and making the first few working versions. -- Daniel Feldman THANK YOU for Aider! It really feels like a glimpse into the future of coding. -- derwiki It's just amazing. It is freeing me to do things I felt were out my comfort zone before. -- Dougie This project is stellar. -- funkytaco Amazing project, definitely the best AI coding assistant I've used. -- joshuavial I am an aider addict. I'm getting so much more work done, but in less time. -- dandandan After wasting $100 on tokens trying to find something better, I'm back to Aider. It blows everything else out of the water hands down, there's no competition whatsoever. -- SystemSculpt Best agent for actual dev work in existing codebases. -- Nick Dobos",
    "commentLink": "https://news.ycombinator.com/item?id=39995725",
    "commentBody": "Aider: AI pair programming in your terminal (github.com/paul-gauthier)313 points by tosh 12 hours agohidepastfavorite110 comments vander_elst 4 hours agoWith all these AI tools requiring a prompt, does it really simplify/speed up things? From the example: I have to write \"add a name param to the 'greeting' function, add all types\", then wait for the result to be generated, read it carefully to be sure that it does what I want, probably reiterate if the result does not match the expectation. This seems to me more time consuming than actually do the work myself. Does anyone has examples where promoting and double checking is faster than doing it on your own? Is it faster when exploring new solutions and \"unknown territory\" and in this case, are the answers accurate (from what I tried so far they were far off)? In that case how do you compare it with \"regular search\" via Google/Bing/...? Sorry for the silly question but I'm genuinely trying to understand reply pocketarc 3 hours agoparentPersonally the use for me has been in writing boilerplate. As an example, one of my ongoing goals has been to port all the view code of a project to another framework, following its idioms. Using an LLM, I can process a file in a couple of seconds, and checking that everything is right takes just a few seconds as well. It’d take me hours to go through every file manually, and it’d be prone to human error. It’s not technically challenging stuff, just tedious and mind-numbing, which is perfect for an LLM. I do agree though, these basic examples do seem quite pointless, if you already know what you’re doing. It’s just as pointless as telling another developer to “add a name param to ‘greeting’ function, add all types”, which you’d then have to review. I think it comes down to your level of experience though. If you have years and years of experience and have honed your search skills and are perfectly comfortable, then I suspect there isn’t a lot that an LLM is going to do when it comes to writing chunks of code. That’s how I’ve felt about all these “write a chunk of code” tools. In my case, apart from automating the kind of repetitive, mindless work I mentioned, it’s just been a glorified autocomplete. It works -really- well for that, especially with comments. Oftentimes I find myself adding a little comment that explains what I’m about to do, and then boop, I’ve got the next few lines autocompleted with no surprises. I had to work without an internet connection a few days ago and it really, really hit me how much I’ve come to use that autocomplete - I barely ever type anything to completion anymore, it was jarring, having to type everything by hand. I didn’t realise how lazy my typing had become. reply dwighttk 3 minutes agorootparent>checking that everything is right takes just a few seconds as well. It’d take me hours to go through every file manually how can you check it in a few seconds if it'd take you hours to change it manually? reply saurik 2 hours agorootparentprev> Personally the use for me has been in writing boilerplate. We live in a world with everything from macro systems and code generation to higher-order functions and types... if you find yourself writing the same \"boilerplate\" enough times that you find it annoying, just automate it, the same way you can automate anything else we do using software. I have found myself writing very little \"boilerplate\" in my decades of software development, as I'd rather at the extreme (and it almost never comes to this) throw together a custom compiler than litter my code with a bunch of hopefully-the-same-every-time difficult-to-adjust-later \"boilerplate\". reply rolisz 1 hour agorootparentI'd say that using LLMs to write boilerplate falls under \"automation\" reply rsynnott 1 hour agorootparentBut, perhaps uniquely amongst all the systems for avoiding boilerplate since Lisp macros were introduced in the 1950s, it will sometimes make stuff up. I don't buy that \"a worse way to write boilerplate\" is going to revolutionise programming. reply Liskni_si 1 hour agorootparentprevYes, except it's \"bad automation\", because as opposed to the automation referred to by GP, boilerplate written by an LLM (or an intern or whomever) is extra code that costs a lot of time to be maintained. reply vander_elst 2 hours agorootparentprevI don't have the impression I'm writing too much boilerplate, but I am curious about this as I have heard it multiple times: are there more examples of boiler plate that an LLM is better/faster at generating than a couple of copy/paste? If it's more than a couple than copy/paste and it's time for a rewrite, do you leverage AI for this? how do you usually introduce the abstraction? reply pocketarc 2 hours agorootparentOne example of boilerplate that I've been automating is when you're creating model code for your ORM. I paste the table definition into a comment, and let the LLM generate the model (if the ORM doesn't automate it), the list of validation rules, custom type casts, whatever specifics your project has. None of it is new or technically challenging, it's just autocompleting stuff I was going to write anyway. It's not that you're writing \"too much\" boilerplate; this is a tiny part of my work as well. This is just the one part where I've actually found an LLM useful. Any time I feel like \"yeah this doesn't require thought, just needs doing\", I chuck it over to an LLM to do. reply nox101 2 hours agorootparentprev> Using an LLM, I can process a file in a couple of seconds, and checking that everything is right takes just a few seconds as well. It’d take me hours to go through every file manually Can you explain more how \"checking everything is right takes just a few seconds as well? A code review can't happen in \"just a few seconds\" so maybe I don't understand what the process your describing really is reply pocketarc 2 hours agorootparentIn the example I gave, it was just porting the same view code from one framework's way of writing view code to another. It's a one-off task involving hundreds of different views. There's zero technical challenge, almost no logic, super tedious for a human to do, not quite automatable since there could be any kind of code in those views, and it's very very unlikely that the LLM gets it wrong. I give it a quick look over, it looks right, the tests pass, it's not really a big deal. And one nice thing I did as well was ask it to \"move all logic to the top of the file\", which makes it -very- easy to clean up all the \"quick fix\" cruft that's built up over years that needs to be cleaned up or refactored out. In those cases the file might indeed need more time dedicated to it, but it would've needed it either way. reply pistacchioso 23 minutes agorootparentprevMost of the discussions about AI applied to coding end up having someone who states that it's just not worth it (at least the moment) and someone else who then chimes in to say that they mostly use it for \"boilerplate\" code. I have trouble understanding the \"boilerplate\" thing because avoiding writing boilerplate is 1) already a solved \"problem\" long before AI 2) is it really a \"problem\"? The first point: * If you find yourself writing the same piece of code over and over again in the same code it's the indication that you should abstract it away as a function / class / library. * IDEs have had snippets / code completion for a long time to save you from writing the same pieces of code. * Large piece of recycled functionalities are generally abstracted away in libraries of frameworks. * Things like \"writing similar static websites a million times\" are the reason why solutions like WordPress exist: to take away the boilerplate part of writing websites. This of course applies to solutions / technologies / services that make \"avoid writing boilerplate code\" their core business * The only type of real boilerplate that comes to my mind are things like \"start a new React application\" but that is a thing you do once per project and it's the reason why boostrappers exist so that you only really have to type \"npx create-react-app my-app\" once and the boilerplate part is taken care of. The second point: Some mundane refactoring / translations of pieces of code from one technology to the other can actually be automated by AI (I think it's what you're talking about here, but how often does one really do such tasks?), but... Do you really want to? Automate, it, I mean? I mean, yes \"let AI do the boring staff so that I can concentrate on the most interesting parts\" make sense, but it's not something I want to do. Maybe it's because I'm aging, but I don't have it in me to be concentrated on demanding, difficult, tiring tasks 8 hour straight a day. It's not something that I can and it's also something that I don't want to. I much prefer alternating hard stuff that require 100% of my attention with lighter tasks that I can do while listening to a podcast and steam off in order to rest by brain before going back to a harder task. Honestly I don't think anyone is supposed to be concentrated on demanding stuff all day long all week long. That's the recipe for a burnout. reply mcluck 2 hours agorootparentprevCall me a caveman but the lack of an option to use AI tools offline is a massive downside to me. I am connected to the internet most of the time but I take comfort in knowing that, for most of my work, I could lose my connection and not even notice reply thomashop 1 hour agorootparentThat's just not the reality anymore. You can run a decent open source coding language model on local hardware. Just needs a bit of work and it's not quite as seamless. reply vander_elst 2 hours agorootparentprevthanks for the reply! I'll try it for commenting. reply djleni 3 hours agoparentprevCan’t speak for everyone else but I almost exclusively use it for what you mentioned: > when exploring new solutions and \"unknown territory\" If it’s something I have no idea how to do I might describe the problem and just look at the code it spits out; not even copy pasting but just reading for a basic idea. > how do you compare it with \"regular search\" via Google/Bing Much worse if there’s a blog post or example in documentation that’s exactly what I’m looking for, but, if it’s something novel, much better. An example: Recently asked how I could convert pressure and temperature data to “skew T” coordinates for a meteorological plot. Not something easy to Google, and the answers the AI gave were slightly wrong, but it gave me a foot in the door. reply jamil7 3 hours agorootparentThis is also where I've kind of ended up with it, I've also noticed that when I was at one point using it everyday, I'm opening it less and less, maybe a few times a week and recently cancelled my subscription. It's still pretty useful for exploritory stuff, boilerplate and sometimes it can give you a hint on debugging. Everything else I can write faster and more correctly myself. reply tossandthrow 1 hour agoparentprevI use it as a glorified search engine and to read through bad documentation (arhm, AWS). but this only works for well documented solutions. core programming hasn't really changed over the past years with good reason: you need. to. understand what you do. this is the bottleneck. not writing it. reply liampulles 1 hour agoparentprevGenerally, I agree. I have found it useful for writing SQL, mapping structs, converting from JSON to CSV, etc. i.e. repetitive stuff. reply ithkuil 2 hours agoparentprevFor me a useful coding assistant would be one that looks at what I'm _doing_ and helps me complete the boring parts of the task. The current wave of coding assistants target junior programmers who don't know how to even start approaching a task. LLMs are quite good at spitting out code that will create a widget or instantiate a client for a given API, figuring out all the parameters and all the incantations that you'd otherwise need to copy paste from a documentation. In a way they are documentation \"search and digest\" tools. While that's also useful for senior developers when they need to work outside of their particular focus area, it's not that useful to help you work on a mature codebase where you have your own abstractions and all sorts of custom things that have good reasons to be there but are project specific. Sure, we could eventually have LLMs that can be fine tuned to your specific projects, company or personal style. But there is also another area where we can use intelligent assistants: editors. Right now editors offer powerful tools to move around and replace text, often in ways that respects the syntax of the language. But it's cumbersome to use and learn, relying on key bindings or complicated \"refactoring\" commands. I wish there was a way for me to have a smarter editor. Something that understands the syntax and a bit the semantics of the code but also the general intent of the local change in working on and the wider context so it can help me apply the right edits. For example right now I'm factoring out a part of a larger function into it's own function so it can be called independently. I know there are editor features that predate AI that can do this work but for various reasons I can't us it. For example, you may have started to do it manually because it seemed simple and then you realize you have to factor out 5 parameters and it becomes a boring exercise of copy paste. Another example is that the function extraction refactoring tool of your IDE just can't handle that case, for example: func A(a Foo) { b := a.GetBar(); Baz(b.X, b.Y, c, d) } you'd want to extract a function func _A(b Bar) { Baz(b.X.... and have A call that. In some simple cases the IDE can do that. In other you need to do it manually. I want an editor extension that can help me with the boring parts of shuffling parameters around, moving them in structures etc etc all the while I'm in control of the shape of the code but I don't have to remember the advanced editor commands but instead augment my actions with some natural language comments (written or even spoken!) reply mikrotikker 26 minutes agoparentprevYea that's where I've landed. Telling it what to do is time consuming. Telling it what I want to do in a broader term and asking for code examples is a lot better., especially for something I don't know how to do. Otherwise the autocomplete/suggestions in the editor is great for the minutia and tedious crap and utility functions. Probably saves me about 20% typing which is great on hands that have typing for 20 odd years. It's also good for finding tools and libraries (when it doesn't hallucinate) since https://libs.garden disappeared inexplicably (dunno what to do on Friday nights now that I can't browse through that wonderful site till 2am) reply rolisz 3 hours agoparentprevWell, regular search means switching to a different application, with an implied context switch. It definitely takes longer for many things than just using GitHub copilot. reply adhamsalama 56 minutes agoparentprevExactly. It's almost useless to me. reply danenania 12 hours agoprevPeople interested in Aider (which is an awesome tool) might also be interested in checking out my project Plandex[1]. It's terminal-based like Aider and has a somewhat comparable set of features, but is more focused on using LLMs to work on larger and more complex tasks that span many files and model responses. It also uses a git-style CLI approach with independent commands for each action vs. Aider's interactive shell. I studied Aider's code and prompts quite a bit in the early stages of building Plandex. I'm grateful to Paul for building it and making it open source. 1 - https://github.com/plandex-ai/plandex reply joshstrange 8 hours agoparentI _cannot_ wait for you to get local models working with this (I know, they need function calling/streaming first). It's amazing! I burned through $10 like it was nothing and bigger context+local is going to make this killer IMHO. It needs additional guidance and with more context maybe loading lint rules into the context would get back code matching my coding style/guide but even as-is there is a ton of value here. It was able to rewrite (partially, some didn't get fully done) 10 files before I hit my budget limits from Vue 2 Class Component syntax to Vue 3 Composition API. It would have needed another iteration or so to iron out the issues (plus some manual clean up/checking from me) but that's within spitting distance of being worth it. For now I'll use ChatGPT/Claude (which I pay for) to do this work but I will keep a close eye on this project, it's super cool! reply danenania 7 hours agorootparentThanks for trying it and your feedback. I'm keeping tabs on open source/local models and will include them as soon as it's feasible. I hear you on the API costs. You should see my OpenAI bills from building Plandex :-/ reply panqueca 5 hours agorootparentIf you're thinking it is expensive, wait until you start to play with Claude Opus. Sooner or later I will declare bankrupt Nice product BTW. I really liked the UI, is very polished reply sdesol 5 hours agorootparentprev> You should see my OpenAI bills from building Plandex :-/ Sorry if you have answered this before, but can you estimate how many man hours were saved using OpenAI or was the high usage more test related? reply danenania 5 hours agorootparentI have used Plandex a lot to help build Plandex faster, but yeah the high API costs are much more due to testing, where I need to run large tasks over and over in rapid succession in order to debug problems or iterate on the built-in prompts. reply joshstrange 11 hours agoparentprevDo you have any plans to build IDE plugins for this? I understand it's open source and anyone could add that, I was just wondering if that was even on the roadmap? Having this run in my IDE would just so awesome with diff tool I'm used to, with all the other plugins/hotkeys/etc I use. reply danenania 11 hours agorootparentYes, VSCode and JetBrains plugins are on the roadmap. Here's the current roadmap by the way: https://github.com/plandex-ai/plandex#roadmap-%EF%B8%8F (it's not exhaustive, but can give you a sense of where I'd like to take Plandex in the future). reply joshstrange 11 hours agorootparentAnd I completely missed that somehow... My apologies. Thank you for pointing that out. reply danenania 11 hours agorootparentNo worries, it's pretty far down in the readme :) reply stavros 10 hours agoparentprevThis looked cool and I was excited to try it until I realized that I either need a subscription, or I need to set up a server. Why does this need a server, when Aider just works via the cli? reply danenania 10 hours agorootparentFirst I should note that while cloud will have a subscription eventually, it's free for now. There's an anonymous trial (with no email required) for up to 10 plans or 10 model responses, and then just name and email is required to continue. I did start out with just the CLI running locally, but it reached a point where I needed a database and thus a client-server model. Plandex is designed for working on many 'plans' at different levels of the project hierarchy (some users on cloud have 50+ after using it for a week), and there's also a fair amount of concurrency, so it got to be too much for a local filesystem or even something like a local SQLite db. Plandex also has the ability to send tasks to the background, which I think will start to play a more and more important role as models get better and more capable of running autonomously for longer periods, and I want to add sharing and collaboration features in the future as well, so all-in-all I thought a client-server model was the best base to build from. I understand where you're coming from though. That local-only simplicity is definitely a nice aspect of Aider. reply stavros 10 hours agorootparentI had a second look and the server doesn't look too hard to deploy. I like that there's reasoning behind requiring it, although I suspect that SQLite is more than capable to very easily do this. I'm trying to deploy the server right now so I can try Plandex, it would be easier if I hadn't forgotten my Postgres password... As a tip, self-hosting would be much easier (which may be something you don't want to do) if you provided a plain Docker image, then it would just be \"pull the Docker image, specify the local directory, specify the DB URL, done\". By the way, why does it need a local directory if it has a database? What's stored in the directory? reply danenania 9 hours agorootparentAgreed on providing a docker image. I made an issue to track it here: https://github.com/plandex-ai/plandex/issues/78 I do want to make self-hosting as easy as possible. In my experience, there will still be enough folks who prefer cloud to make it work :) There's a local .plandex directory in the project which just stores the project id, and a $HOME/.plandex-home directory that stores some local metadata on each project--so far just the current plan and current branch. reply stavros 9 hours agorootparentI see, thanks for the explanation! If you're only storing a bit of data, removing the requirement for a local directory would make deployment easier; these could just go into the database. reply danenania 9 hours agorootparentOh sorry, my comment was referring to the local files created by the CLI. The server uses the file system much more heavily in order to enable efficient version control with an embedded git repo for each plan. Everything in a plan that's version-controlled (context, the conversation, model settings, and tentative file updates) is stored in this repo instead of the database. reply stavros 9 hours agorootparentAh, that makes sense, thank you. reply carom 11 hours agoparentprevHow do you situate changes in a file? That seems like the hard part to me since the LLM can't necessary count to output a patch with line numbers. reply danenania 11 hours agorootparentIt does use line numbers, which definitely aren't infallible. That's why a `plandex changes` TUI is included to review changes before applying. Unfortunately no one has figured out a file update strategy yet that doesn't make occasional mistakes--probably we'll need either next-gen models or fine-tuning to get there. That said, counting isn't necessarily required to use line numbers. If line numbers are included in the file when it's sent to the model, it becomes a text analysis task rather than a counting task. Here are the relevant prompts: https://github.com/plandex-ai/plandex/blob/main/app/server/m... reply stavros 10 hours agorootparentprevDoesn't the software just give the LLM the line numbers? reply andoando 8 hours agoparentprevCan we get someone to automate stuff like copying files, renaming stuff, setting env variables, any common tasks done in an OS. reply danenania 8 hours agorootparentYou could do this with Plandex (or Aider... or ChatGPT) by having it output a shell script then `chmod +x` it and run it. I experimented early on with doing script execution like this in Plandex, but decided to just focus on writing and updating files, as it seemed questionable whether execution could be made reliable enough to be worthwhile without significant model advances. That said, I'd like to revisit it eventually, and some more constrained tasks like copying and moving files around are likely doable without full-on shell script execution, though some scary failure cases are possible here if the model gets the paths wrong in a really bad way. OpenInterpreter is another project you could check out that is more focused on code/script execution: https://github.com/OpenInterpreter/open-interpreter reply andoando 8 hours agorootparentI feel like what I am saying should be natively supported. If youre worried about changes getting it wrong, just show a prompt with all the batched changes. me > build my jar, move it to the last folder I copied it to, and run it. LLM > built jar xyz.jar moving jar to x/y/z me > yes. me > redo last command. Provide rollback/log for these features if need be. I really dont think you even need an LLM for this. I feel like I can do it with a simple classifier. It just needs to be hooked into to OS, so that it can scan what you were doing, and replicate it. For example if I keep opening up folder x and dropping a file called build.jar to folder y, a program should be able to easily understand \"copy the new jar over\" I imagine at point this is going to be done at the OS level reply danenania 7 hours agorootparentIt's a great concept and I agree it will definitely exist at some point, but working a lot with GPT-4 has made me viscerally aware of how many different ways something like \"build my jar, move it to the last folder I copied it to, and run it\" can be spectacularly misinterpreted, and how much context is needed for that command to have any hope of being understood. The other big issue is that there is no rollback for a `rm` or `mv` command that screws up your system. I had similar ideas when I started on Plandex. I wanted it to be able to install dependencies when needed, move files around, etc., but I quickly realized that there's just so much the model needs to know about the system and its state to even have a chance of getting it right. That's not to say it's impossible. It's just a really hard problem and I'd guess the first projects/products to nail it will either come from the OS vendors themselves, or else from people focusing very specifically on that challenge. reply andoando 6 hours agorootparentYoure right there is a lot of ambiguity there. I think being able to scan user actions helps a ton with this though, because you know exactly the steps the user took. Most of the times I want this is when I literally have to repeat the same set of actions 5+ times and writing a script to do it isnt worth it. I want to be able to just save/train the model and have it do what I want. Today I literally built a jar 50 times, with each time having to open up two folders and copying files between the two same directories. Massively annoying. There is still some ambiguity there because cases might slightly differ, youre right. For rm/mv. mv is easily reversible no? You just need to store some context. Same with rm, just copy it to a temp directory. But again with a confirmation prompt its a non issue either way. reply andoando 6 hours agorootparentprevAlso maybe we need a slightly different kind of LLM, which instead of just assuming its top predictions are correct, gives you actions at critical steps on how to proceed. build a jar. > I can build a jar with x,y,z, which do you want? reply pax 3 hours agorootparentprevopen interpreter can do that https://github.com/OpenInterpreter/open-interpreter reply perbu 3 hours agoprevI have this 300 line Go application which manages git tags for me. I asked it to implement a -dry-run function. It failed twice. First time it just mangled the file. Second time it just made code that didn't do anything. I asked it to rename a global variable. It broke the application and failed to understand scoping rules. Perhaps it is bad luck, or perhaps my Go code is weird, but I don't understand how y'all wanna trust this. reply Culonavirus 2 hours agoparentIt must be your app/lang/prompt/grandma/dog/... lol. LLMs are the future, and they will replaces Allllllll the coders in the woooorld (TM), and did you know \"it\" can create websites??? Wooo, let's go, baby! Nah these things are all stupid as hell. Any back and forth between a human and an LLM in terms of problem solving coding tasks is an absolute disaster. People here and certainly in the mainstream population see some knowledge and just naturally expect intelligence to go with it. But it doesn't. Wikipedia has knowledge. Books have knowledge. LLMs are just the latest iteration of how humans store knowledge. That's about it, everything else is a hyped up bubble. There's nothing in physics that stops us from creating an artificial, generally intelligent being, but it's NEVER going to be with auto-regressive next-token prediction. reply Seb-C 52 minutes agorootparentLLMs does not store information though. Language is a tool to convey information. LLMs are only about the language, not the information. reply Recursing 2 hours agoparentprevIn my experience, these things work much better with Python than with anything else reply joshstrange 11 hours agoprevI'm interested in this and will probably set it up but I wish more AI tools were better integrated to my IDE. I know GH Copilot is and other big AI tools have plugins with chat/edit features but most of the cool open source doesn't seem to support IDEA/JetBrains. I see the power of LLMs. I use GH Copilot, I use ChatGPT, but I crave deeper integration in my existing toolset. I need to force myself to try in-IDE Copilot Chat. My habit is to go to ChatGPT for anything of that nature and I'm not sure why that is. Sometimes it's the same way I break down my search to for things \"I know I can find\" then put together the results. In the same way I break down the problem into small pieces and have ChatGPT write them individually or somethings additively. reply anotherpaulg 11 hours agoparentFolks have developed VSCode and NeoVim integrations for aider. They're based on forks of aider, so I'm not sure how carefully their authors are keeping them up to date with aider releases. The aider install instructions has more info: https://aider.chat/docs/install.html#add-aider-to-your-edito... reply thomashop 7 hours agoparentprevI've been using https://cursor.sh/ heavily for about 2 months and I'm pretty happy. Cursor is a fork of VSCode focused on AI. I'd prefer to use something totally open-source, but Cursor is free, gets regular updates, and I can use my OpenAI API key. The diff view works well with AI coding assistants. I end up parallelizing more. I let cursor do its thing while I'm already looking at the next file. I love aider too! Have used it to automate things such as maintaining a translated version of the page in a git pre-commit hook. reply Onawa 11 hours agoparentprevI didn't get time to test it beyond installing it on VSCode today, but take a look at https://GitHub.com/continuedev/continue, Apache 2.0 license, and they have an IDEA/Jetbrains plugin. Plus codebase context, full configurability to use local or remote LLMs. reply joshstrange 11 hours agorootparentI probably need to give it another try but I tried that before with my own GPT-4 key, a local model, and their models and just got errors last time I tried it. I hope that was just a temp issue but because of that I moved on. Also I've tried Cody Pro (again, weird errors and when it did work I felt like Copilot would have done better). reply aiauthoritydev 6 hours agoparentprevWhat would be since is a single plugin that focuses only on UX and allows plug an play for AI models. I think we would benefit immensely from such a concept. reply solumunus 4 hours agoparentprevIn my experience, Supermaven makes Copilot look like a joke, and they’ve just released a Jetbrains plugin. YMMV. It’s just code suggestions though, no chat box. reply hackerlight 3 hours agorootparentHave you compared it to Sourcegraph Cody which also has a Jetbrains plugin? Same monthly cost as Supermaven. reply j45 11 hours agoparentprevAider works a little different where it doesn't just code complete or focus on a function level. It can solve much bigger problems. reply bumbledraven 10 hours agoprevI appreciate @anotherpaulg's continual benchmarking of LLM performance with aider, for example: > OpenAI just released GPT-4 Turbo with Vision and it performs worse on aider’s benchmark suites than all the previous GPT-4 models. In particular, it seems much more prone to “lazy coding” than the GPT-4 Turbo preview models. https://aider.chat/2024/04/09/gpt-4-turbo.html reply fvdessen 11 hours agoprevI just tried it and it's amazingly cool, but the quality of the output just isn't there for me yet. It makes too much subtle errors to be as useful as the screenshots and the gifs makes it look reply alexose 5 hours agoparentI agree with you. It's okay at really simple code changes for really simple repos, but it falls apart for anything outside the ordinary. I'm sure I'll have to eat these words, but: This just doesn't feel like the right interface to me. LLMs are incredible at generating \"inroads\" to a problem, but terrible at execution. Worse yet at anticipating future problems. All this might very well change. But until it does, I just want my LLMs to help me brainstorm and help me with syntax. I think there's a sweet spot somewhere between this tool and Copilot, but I'm not sure where. reply rbren 11 hours agoprevWe have an issue in OpenDevin to add Aider as an agent, if anyone wants to take a crack at it: https://github.com/OpenDevin/OpenDevin/issues/120 reply tyoma 12 hours agoprevI’ve used aider to understand new codebases using technologies I don’t know and it did a fantastic job; much faster than grep/find + google. reply fyrn_ 12 hours agoparentTo be fair in a world of good LSP impls, grep/find are really primative tools to be using. Not saying this isn't better then a more sophisicated editor setup, just that grep and find are a _really_ low bar reply seadan83 9 hours agorootparentNot sure if that's making things \"fair\". Grep & find are insanely powerful when you're a CLI power user. Nonetheless, I'm particularly curious which cases the AI tool can find things that are not easy to find via find & grep (eg: finding URLs that are created via string concatenation, those that do not appear as a string literal in the source code) Perhaps a larger question there, what's the overall false negative rate of a tool like this? Are there places where it is particularly good and/or particularly poor? edits: brevity & clarity reply cess11 4 hours agorootparentI evaluate a lot of code, like ten-twenty applications per year currently, terminal tooling is my goto. Mostly the basic stuff, tree, ripgrep, find, wc, jq, things like that. I also use them on top of output from static analysis tooling. It's not as slick as SQL on a RDBMS, but very close, and integrates well into e.g. vim, so I can directly pull in output from the tools and add notes when I'm building up my reports. Finding partial URL:s, suspicious strings like API keys, SQL query concatenation and the like is usually trivial. For me to switch to another toolset there would have to be very strong guarantees that the output is correct, deterministic and the full set of results, since this is the core basis for correctness in my risk assessments and value estimations. reply tyoma 10 hours agorootparentprevAider can answer questions I can’t search for via LSP, like “what code would process the following URL” and similar. reply shadowgovt 12 hours agorootparentprevWhen we reach that world, let me know. I'm still tripping over a \"python-lsp-server was simply not implemented async so sometimes when you combine it with emacs lsp-mode it eats 100% CPU and locks your console\" issue. reply fyrn_ 2 hours agorootparentIf emacs hard blocks on LSP requests, that may be on emacs as well. I recomemd you try ruff-lsp, although it does not iver everything and is more for linting, it's higb quality reply skeledrew 10 hours agorootparentprevWait, so this is why Emacs has been locking up on me in most of my Python projects?? reply shadowgovt 7 hours agorootparentPossibly. Definitely why it has been locking up on me when I added lsp-mode. Lsp-mode will schedule one request per keypress but then cancel that request at the next keypress. But since the python LSP server doesn't do async, it handles cancel requests by ignoring them reply renegade-otter 2 hours agoprevThese tools have the same problem as image or video generative AI does - it can maybe render individual parts accurately, but what is basically autocomplete cannot reason about the bigger picture. You glance at it, and it looks ok, but then you look closer, and it's riddled with issues. Statistical prediction has its limitations - who knew. reply xyst 6 hours agoprevI’m still waiting for that bastard Devin to write my killer app. Now you want me to code my own killer app with an AI micromanage me? reply bearjaws 9 hours agoprevAider is the only tool I use for coding now with ChatGPT. Copilot is pretty good but I like the split context of declaring what you are working on in the CLI. It still suffers from ChatGPT laziness sometimes, you can see it retrying several times to get a correct output before giving up. reply jerrygoyal 4 hours agoprevSo many AI coding agents popping up that claim to build entire projects but all I'm interested in (happy to pay): - Review GitHub PR and suggest fixes. - Improve the readability of code with a single command (devs suck at naming variables). - context aware autocomplete for real reply tjaad 3 hours agoparent> - context aware autocomplete for real If the AI tool can fetch related classes for the code that I'm working with that would be so helpful! reply namanyayg 4 hours agoparentprev> devs suck at naming variables This is the #1 problem I've faced with my contractors and I wish I could find a good solution for it reply pax 11 hours agoprevI revisited Aider a couple of days ago, after going in circles with AutoGPT - which seemed to either forget or go lazy after a few prompts - to the point it refused to do something that it did a few prompts before. Then Aider delivered from the first prompt. PS. I've gathered a list of LLM agents (for coding and general purpose) https://docs.google.com/spreadsheets/d/1M3cQmuwhpJ4X0jOw5XWT... reply wanderingmind 11 hours agoprevAider has a big problem when working with python codebase. 1. Its dependencies will conflict with your code requirements. 2. If you don't install it within the code environment, you can use `aider run` where you can run local commands and pipe their outputs. 3. You will need to use all it's dependencies even in prod environment that can increase the attack surface. So until they introduce a global binary install, I suggest using Plandex which is based on Go and can work across any environment within the system reply anotherpaulg 11 hours agoparentThanks for trying aider, and sorry to hear you had dependency conflicts. You can install aider with pipx to avoid this. There's a FAQ entry that explains how: https://aider.chat/docs/faq.html#how-to-use-pipx-to-avoid-py... Also, why would you want to install aider in a production environment? It's a development tool, I wouldn't expect anyone to use it in prod. But maybe there's a use case I'm not thinking of? reply wanderingmind 11 hours agorootparentThank you, didn't know about pipx possibility. Will give it a shot. I don't want aider in prod environment. Im saying its hard to remove it from prod if we can't isolate it from code dependencies as its hard to maintain multiple requirements.txt for different envs. reply stavros 10 hours agorootparentprevThis is the correct solution, use pipx to install any Python cli and save yourself a ton of hassle. reply bumbledraven 10 hours agoparentprevThat is a problem with the Python packaging ecosystem in general, not with aider. @BiteCode_Dev wisely advises beginners to use virtual environments to install anything that uses Python: https://www.bitecode.dev/p/back-to-basics-with-pip-and-venv reply devstein 11 hours agoprevBig fan of Aider. We are interesting in integrating Aider as a tool for Dosu https://dosu.dev/ to help it navigate and modify a codebase on issues like this https://github.com/langchain-ai/langchain/issues/8263#issuec... reply paradite 10 hours agoprevIf you prefer GUI with more space to write task requirements, or use your existing ChatGPT/Claude subscription without additional API costs, you can check out my desktop app: https://prompt.16x.engineer/ reply visarga 4 hours agoparentEveryone has one, I just made a script to print all the files in the current folder to the terminal. I have .context-ignore files to exclude some patterns, similar to .gitignore The first file is a README.md that contains my initial description of what I am working on. At the end I type a new command. I copy & paste the text into chatGPT web interface. I think I used it for 10-15 rounds of iteration on my latest project and it generated about 50% of the code of a web app with Python backend. Pretty sweet and costs nothing on top of the web subscription. The funny part is that I was using this AI coding tool to build another AI tool to manage a collection of prompts and demonstrations including automatic prompt evaluation, so I was using an AI tool to make another AI tool. reply paradite 3 hours agorootparentThat's exactly how I started as well, I was using a similar workflow to build an AI-driven game. Then I thought maybe it's a good idea to turn it into something less ad-hoc, more user-friendly, and work for any project. reply 1ark 7 hours agoprevFor the Emacs user, maybe not exactly one-to-one, but useful: https://github.com/s-kostyaev/ellama reply foundval 9 hours agoprevIf you're interested in this sort of stuff, you might like this diff-based CLI tool I wrote: https://github.com/freuk/iter It runs on Groq (the company I work for), so it's super snappy. reply takets 9 hours agoprevIntegration with neovim. https://github.com/nekowasabi/aider.vim reply hackerlight 3 hours agoprev> While it is not recommended, --no-auto-commits will stop aider from git committing each of GPT’s changes. Why is it recommended to not quickly review the changes (git status, git diff) before committing? reply pmarreck 7 hours agoprevPython projects depress me because of the dependency management problem. reply alexose 5 hours agoparentNo problem, just install it in a virtualized containerized pyenv virtualenv pipx poetry conda dies reply beastman82 12 hours agoprevThis tool is amazing reply renewiltord 11 hours agoprevI have used this technique for months and it’s great https://x.com/arjie/status/1575201117595926530 I just have copilot in my editor and switch into my editor with C-x C-e for AI completion. I use neovim like example but you can use whatever you like. EDIT: Oh never mind. I see what it is now. It’s a terminal based flow for editing code. Mine is for command line writing live. reply febed 6 hours agoparentThat’s cool, but is there a way to use another vendor instead of GitHub CoPilot? reply renewiltord 4 hours agorootparentThere’s the fauxpilot project. This is just the minimal stuff to use the AI completer. But you can sub out faux pilot and get the same result (modulo quality). reply DonHopkins 11 hours agoprevWhen working alone remotely from home, I simulate pair programming with a methodology I call \"The Stranger\". I sit on one of my hands until it becomes numb and tingly, and then it feels like somebody else is typing and moving the mouse! reply Gibbon1 14 minutes agoparentI use synthetic aperture pair programming. I write some code and get it working. Then I get pulled off it for a few months and come back to it. Me + 3 months: da fuk. reply dools 7 hours agoprev [–] > GPT can write and edit code in most popular languages: python, javascript, typescript, html, css, etc. I love how everyone always leaves PHP off these lists of \"popular languages\" despite the fact that 80% of the web runs on PHP. reply elromulous 7 hours agoparentPHP has had enormous staying power despite its idiosyncrasies. I would however be curious to know what percentage of the 80% (or so) is WordPress et al. Since those largely don't involve folks actually writing code. I suspect a very small amount of PHP code is being run a lot. reply aiauthoritydev 6 hours agorootparent+1 I had a largish website with few thousand static webpages. Over a period the pages grew into around 100K with some server side features. Over the course of 10 years I did and redid this site in multiple technologies. React, Angular, Spring Boot + Freemarker etc. However the PHP power version of it remains best for SEO has near zero downtime and no maintenance what so ever runs on a VM that shares like 10 other websites. traffic serves is around 100K visits a day. reply noduerme 7 hours agorootparentprevPHP is still my go to for anything web-page based that's not a single page app. It's just a really nice solution for protoyping and making web quickly. I mean, custom stuff, not wordpress. A lot of my successful projects have been rewritten later in nodejs. But for getting something up and running to test a concept, PHP is great if you're comfortable with its idiosyncracies. I'd say Python is just as idiosyncratic, and its packaging system is just too much of a pain point. And Node doesn't ship with mature database interfaces, its dependencies are scary, there's more concern about runaway scripts, crashes are harder to recover from, and a lot of times all you really want from a router is to serve your file structure with some access rules. I think PHP is still the best choice for prototyping dynamic HTML and logic fast, without any packages or plug-ins. A lotta times I still even use it for short CLI scripts and cron tasks that do database ops. reply j45 6 hours agorootparentprevLanguages that can create beginners in programming who can ship early and often are invaluable. I don’t use very much php, but would be remiss if I left my opinion of it as dated as whisper campaign rumors based on interpretation and preference. Packages like Laravel and especially technologies like Hotwire are nothing to overlook. Standardized and capable frameworks that have large workforces can be quite valuable at time of valuation and due diligence. Specialized and brittle techs can be a challenge. reply anotherpaulg 7 hours agoparentprev [–] I’ve updated the list to include php. Sorry for the oversight! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Aider is an AI pair programming tool enabling collaboration with advanced language models like GPT-3.5/GPT-4 directly in the terminal, facilitating code editing in local git repositories.",
      "It supports various programming languages, offers features such as creating new projects, making edits to existing code, requesting changes/fixes, and auto-committing with clear messages.",
      "Users appreciate Aider for its productivity boosts, user-friendly interface, in-chat commands, helpful tips, and smooth integration with text editors."
    ],
    "commentSummary": [
      "The post explores the utilization of AI tools like Aider for pair programming and automating coding tasks within the terminal.",
      "Users evaluate the tool's efficacy for experienced developers, discussing both its advantages and drawbacks for coding activities.",
      "It also addresses the integration with IDEs, comparisons with other AI completion tools, and considerations for Python and PHP development in web development."
    ],
    "points": 313,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1712783202
  },
  {
    "id": 39990346,
    "title": "Visualizing ADS-B Datasets: Impressive Results and Real-Time Insights",
    "originLink": "https://adsb.exposed/",
    "originBody": "I&#x27;ve created a web app for querying and visualization of ADS-B datasets: https:&#x2F;&#x2F;adsb.exposed&#x2F;Source code: https:&#x2F;&#x2F;github.com&#x2F;ClickHouse&#x2F;adsb.exposed&#x2F;The results significantly exceeded my expectations because the pictures are insanely beautiful, and the data is a treasure trove.It proves many statements that were not certain: - it is feasible to generate tiles by aggregation on a pixel level (instead of hexagons or rectangular grid); - it does not require JPG&#x2F;PNG tiles - we can transfer raw bitmap data with zstd compression; - it is possible to do it in real time;",
    "commentLink": "https://news.ycombinator.com/item?id=39990346",
    "commentBody": "ADS-B visualizer (adsb.exposed)283 points by zX41ZdbW 20 hours agohidepastfavorite67 comments I've created a web app for querying and visualization of ADS-B datasets: https://adsb.exposed/ Source code: https://github.com/ClickHouse/adsb.exposed/ The results significantly exceeded my expectations because the pictures are insanely beautiful, and the data is a treasure trove. It proves many statements that were not certain: - it is feasible to generate tiles by aggregation on a pixel level (instead of hexagons or rectangular grid); - it does not require JPG/PNG tiles - we can transfer raw bitmap data with zstd compression; - it is possible to do it in real time; johtso 19 hours agoThis is beautiful! Definitely worth looking at the examples in the Github repo: https://github.com/ClickHouse/adsb.exposed/ I particularly like the example of helicopters following the river Thames in London: https://github.com/ClickHouse/adsb.exposed/?tab=readme-ov-fi... reply neilv 7 hours agoparentImpressive. Though, for one of the examples, set in an area with active military conflict, I wondered whether the information is new and useful to anyone involved. reply jjwiseman 15 hours agoprevIt's like writing webgl shaders for ADS-B. In SQL. Incredible work, and I hope this kicks off a lot of innovation in the world of aircraft traffic analysis & visualization, which I think has been kind of stuck in a rut for a while. reply widforss 1 hour agoprevHow do it get ADS-B data from over the oceans? I guess it isn't satellite (which I now is used for actual tracking) since there is obviously a lot less lines over the oceans (but lines nonetheless). reply JosephRedfern 19 hours agoprevVery pretty! As it happens, I too have started ingesting ADS-B Data into ClickHouse recently, but have nothing nearly as beautiful as this. I'm hitting the airplanes.live API every 10 seconds using ClickHouse's URL table function and storing in a MergeTree: https://github.com/JosephRedfern/airhoover/blob/main/airhoov.... Would love to use refreshable materialised views for this, but at the moment there's no append functionality (refresh only), so have to use Python to to trigger the query. There's an open instance here: https://airhoover.joesstuff.co.uk/play?user=default#U0VMRUNU.... Only 1.5 days or so of data, I truncated before setting up tiered storage (local disk + backblaze b2). Cool being able to e.g. get a break-down of aircraft type by operator (https://airhoover.joesstuff.co.uk/play?user=default#U0VMRUNU...) or who (supposedly) flies their planes fastest (https://airhoover.joesstuff.co.uk/play?user=default#U0VMRUNU...) reply kqr 19 hours agoprevWow. I have no idea what I'm looking at but it's pretty. And I do recognise the extended centrelines of local runways, as well as the typical instrument approach tracks into them! Edit: Having read the readme, I have a better idea of what I'm looking at. Really impressive technically as well. reply mmaunder 17 hours agoprevIm surprised to see military traffic over the US. They tend to fly with ADSB out turned off and have an agreement with FAA to be able to do that. Also surprised to see gliders who generally don’t broadcast ADSB out. In the US, we are required to have ADSB out within most controlled airspace and within the mode C veil of a major airport (within 30nm and up to 10,000 MSL). So most GA planes have it but remote areas have planes without ADSB out. reply larrywright 14 hours agoparentIf you look at https://globe.adsbexchange.com, and filter for military/interesting (the U button at the top of the screen), you'll see that at any given time there are a LOT of military flights over the US. Most are transport, refueling tankers and what seem to be pilots-in-training (mostly in Texas and Florida, some in Colorado), but there are plenty of helicopters and smaller Lear-type jets. When the president or vice president is flying somewhere there are typically one or more E-3 AWACS planes in the air to provide radar coverage. There are certainly military planes that fly without ADSB, but for flights where secrecy doesn't matter, they seem to fly with it on. I've seen all manner of planes with ADSB, from U2 spy planes, F-15, F-16, A-10, the occasional B-52, and more. reply mmaunder 6 hours agorootparentCool. Whenever I see military traffic here in Colorado they don’t show up on adsb except for the DA20 trainers in the pattern south of Springs. reply jjwiseman 17 hours agoparentprevFighters often don't have ADS-B, but there are lots of other military aircraft that routinely use it (transports, etc.). Go to https://globe.adsbexchange.com and press U to show only military. reply EvanAnderson 16 hours agorootparentMy town, north of Dayton, OH was recently overflown by a group of fighters (I believe F-18's). I was surprised that they didn't appear on the ADS-B tracker site I ran inside and looked at. I guess it makes sense. I'm just a couple of miles from DAY and see a lot of traffic every day. It would be interesting to know how military aircraft like that coordinate with civilian air traffic control. (I like to listen to Dayton approach while watching an ADS-B tracker site. I enjoy seeing the traffic fly over my neighborhood. I find it oddly amusing to look up at a plane I just heard getting clearance to land knowing that I just heard the voice of somebody up there thru my speakers. I don't know why it's so pleasing...) reply fullstop 15 hours agorootparentI tracked, briefly, a F15E flying past yesterday. reply mmaunder 6 hours agorootparentprevJust speaking from experience, in Colorado no military has it on except for the trainers. Having a chinook pass under you and be totally invisible on the iPad is a weird feeling. reply mshockwave 14 hours agorootparentprevI'm surprised about this, I thought it's pretty easy to add ADS-B to modern IFF module and they can just switch between different modes. reply petee 16 hours agorootparentprevI'd read about small retro reflectors they bolt on when not in combat so the stealth fighters will show on domestic radars reply petee 12 hours agorootparentEdit: they're called Luneburg Lenses reply dramm 4 hours agoparentprevThe ADS-B Out situation with gliders in the USA is complex. Many high-performance modern gliders used for racing and cross country flying are equipped with Mode-S transponders with 1090ES Out (either 14 CFR 91.227 ADS-B Out compliant or TABS/TSO-C199 ADS-B Out compliant) as well as the glider specific FLARM traffic awareness system that broadcasts and receives proprietary low-power position data. FLARM systems, at least as used in the USA also typically receive 1090ES In so they drive the integrated traffic display and warning systems in these modern glider cockpits. For historical and technical reasons gliders use a FLARM modified NMEA serial data protocols for traffic data in the cockpit instead of what is found in GA aircraft ADS-B traffic display systems. Gliders potentially face very different risk scenarios and their owners will hopefully equip for what is the most significant risks to them and others. e.g. gliders often fly close to each other, especially when thermalling together, if flying with other gliders in remote areas then the FLARM system optimized to handle glider on glider threats is optimal (where GA focused ADS-B produces far to many false alerts), if flying near lots of GA aircraft ADS-B Out (and In via FLARM) is likely optimal, if flying near airliners, fast jets and tactical military aircraft then transponders alone even without 1090ES Out may be most critical item for their SSR, TCAS and IFF compatibility. Ideally owners do equip with all three... Mode-S, 1090ES Out, and FLARM. The challenge may be more where there are low-cost/low-value gliders, maybe trainers and glider club owned gliders especially those located in busy traffic areas. You would hope those owners long ago got the message they should be equipping with some forms of supplementary traffic broadcast/awareness systems. UAT adoption in gliders is nearly non-existent as FLARM systems only receive directly on 1090ES In and you don't want to rely on ADS-R coverage in remote or mountainous areas. TABS/TSO-C199 is an easier route to an approved installation of ADS-B Out systems in type-certified gliders. TSO-C199 was developed by industry and the FAA following the 2006 mid-air collision between a glider and Hawker business jet near Minden NV. Experimental category gliders will typically have 14 CFR 91.227 compliant installations done under the same \"meets performance requirements\" clauses as many experimental power aircraft. reply fullstop 16 hours agoparentprevI started playing around with ADS-B and military aircraft are all over the place, at least on the east coast of the USA. In fact, this Seahawk just flew over: https://globe.airplanes.live/?icao=ae6904 reply nick238 17 hours agoparentprevFor general transport/rebasing, it seems like it would just make life easier on air traffic controllers, and probably also other traffic in the air (I think TCAS uses ADS-B?). By way of analogy, the military might have permission to drive around at night with their lights off, but it'd make me more comfortable if they did that only when it was operationally useful (i.e. I assume B-2s taking off from Missouri to go bombing have transponders off the whole way.) reply dramm 9 hours agorootparentMilitary aircraft operators in the USA are pretty cautions about non-ADS-B Out or non-transponder operations. Air Force bases etc. will have active MACA (Mid-Air Collision Avoidance) programs, with aircraft and controllers working to reduce conflict potential with civilian traffic. Many bases will have MACA information on their web sites and staff to contact, and they are typically very responsive. Ironically the ones I have worked with were trying to encourage more civilian GA aircraft to adopt transponders, and to utilize the flight following services of their ATC/RAPCON. TCAS uses active transponder interrogation from the TCAS unit interrogating a threat target's Mode-C or Mode-S transponder. TCAS only uses ADS-B In in an indirect way, in large part to acquire traffic in the area that is not yet a threat, and reduce it's RF congestion caused by excessive interrogation, especially of legacy Mode-C targets. A TCAS II system will fly you right into say a UAT out equipped aircraft without issuing an RA (resolution advisory) if that threat aircraft has no transponder or an inop transponder. TCAS II only issuing an RA based on active interrogation of a threat aircraft's transponder is a kind of safety feature given the potential spoofing of ADS-B Out data. reply larrywright 14 hours agorootparentprevOne of the \"tells\" for military aircraft flying without ADSB is when you see refueling tankers (KC-135) doing loops for periods of time, and no other military aircraft around. Those tankers are refueling something, they don't just fly around for grins. reply morkalork 12 hours agoparentprevSometimes they forget to turn it off: https://theaviationist.com/2023/11/22/usaf-ac-130j-iraq/ reply rlpb 11 hours agorootparentThat article states that it's unlikely that they forgot, and much more likely that it was deliberate. reply morkalork 11 hours agorootparentInteresting, when it happened at the time the theory was they forgot. reply secondcoming 12 hours agoparentprevThose Reaper drones show up on FlightRadar24 and ADSBExchange while patrolling over the Black Sea monitoring Crimea and beyond. reply vdqtp3 6 hours agoparentprevThere's plenty of H60s and H47s out of Lewis McChord on ADSB, not to mention P8's, C17's, etc...if you're on flight following you'll get advisories or even just watching \"the fish finder\" you'll see them. reply aendruk 9 hours agoprevThe image loading/rendering is fascinating, beginning grainy and then smoothing with time. Can you elaborate on what’s happening? reply systemz 19 hours agoprevHi, thanks for sharing! Unfortunately besides empty map I don't see any visualization. Browser dev console is showing some CORS errors. EDIT: Nevermind, clickhouse.com was on disconnect's ad blocking list which I used for DNS blocking. reply zX41ZdbW 19 hours agoparentWe bought clickhouse.com from the previous owner 2.5 years ago, who used it for some ad network - it was in some block lists, but I hope we managed to clean up most of them. reply tamimio 18 hours agorootparentStill blocked in Disconnect list due to “Malvertising” reply rrix2 18 hours agorootparentI'm not sure that list should be used any more. I tracked down a pihole issue a while ago to find this \"The list you referenced was used in our legacy products. It is not maintained, has not been updated, and is not actively distributed us.\" https://www.reddit.com/r/pihole/comments/wtizpa/deprecation_... (ETA: https://github.com/disconnectme/disconnect-tracking-protecti...) reply tamimio 18 hours agorootparentInteresting, thanks for the heads up, just had it removed. reply parker-3461 19 hours agoparentprevIt seems to be all working on my end though. reply Solvency 19 hours agoparentprevfully empty map on iOS safari for me. reply j1897 16 hours agoprevSuper cool! Visually dazzling. If you want to build something similar on a raspberry pi, here is a tutorial: https://questdb.io/blog/create-flight-radar-raspberry-pi-que... reply mvkel 6 hours agoprevThis is arrestingly beautiful. I kind of want to get a print or two of some of these. Even the color choices are magnificent reply windexh8er 19 hours agoprevThis is really cool when you dig into how much fidelity there is here. Also, a fantastic marketing campaign for Clickhouse! As for the dataset - is this continually updated or how \"fresh\" is it at any given moment? reply zX41ZdbW 19 hours agoparentThere are two data sources: adsb.lol and adsbexchange. The first is updated each day from https://github.com/adsblol/globe_history_2024. The second should be updated each month (they provide only sample data for the first day of each month), but I still have to put it in cron. The update scripts are also open-source, published here: https://github.com/ClickHouse/adsb.exposed/blob/main/prepare... reply underyx 19 hours agoprevhttps://adsb.lol provides this data licensed under ODbL, this site violates the attribution and share alike clauses reply zX41ZdbW 19 hours agoparentYou can find the attribution and the link to the details at the bottom right corner of the main page. It links to the documentation: https://github.com/ClickHouse/adsb.exposed, which provides the full details. Additionally, you can read the license here: https://github.com/adsblol/globe_history_2023/blob/main/LICE... reply oq1ik6631rldh00 19 hours agorootparenthttps://opendatacommons.org/licenses/odbl/1-0/ 4.3.a. is clear: a. Example notice. The following text will satisfy notice under Section 4.3: Contains information from DATABASE NAME, which is made available here under the Open Database License (ODbL). At the time if writing, the attribution does not make it clear where to get the data and what the terms of the license of the data are. Only a (data: adsb.lol) which does not even comply with copyright attribution (Which funnily enough you are complying with for OpenStreetMap, while only using that for display, while creating your derivative database of the globe_history_2023 and globe_history_2024 database...) reply dirkhh 18 hours agorootparentI find it super frustrating to see well funded tech companies boldly abuse open data resources and assume they can just get away with it. I guess that's the theme of 2024. But to make it clear: you are in violation of the license the data is provided under, and as one of the copyright holders I object to that. reply zX41ZdbW 18 hours agorootparentCould you please clarify the details? I took extra care to provide all the needed attributions and credits and I believe it is complete and sufficient. If I missed something, please describe it at https://github.com/ClickHouse/adsb.exposed/issues, and I will correct it. reply dirkhh 17 hours agorootparentplease read the license and its requirements. It is YOUR responsibility to comply with those requirements in order to be allowed to use the data. It is ridiculous that a well funded company asks a volunteer community to tell them how to comply with the license - after they have violated the license and used it for a marketing stunt. reply bilekas 17 hours agorootparentYou might be frustrated and that's fine, personally I don't know what's missing either but you simply saying \"YPU need to figure it out\" isn't helpful at all. reply dirkhh 16 hours agorootparentWell, here's the language in the license that apparently is too hard for you to find: 4.3 Notice for using output (Contents). Creating and Using a Produced Work does not require the notice in Section 4.2. However, if you Publicly Use a Produced Work, You must include a notice associated with the Produced Work reasonably calculated to make any Person that uses, views, accesses, interacts with, or is otherwise exposed to the Produced Work aware that Content was obtained from the Database, Derivative Database, or the Database as part of a Collective Database, and that it is available under this License. I can't read it for you, but I can summarize it for you. You are required to make sure that someone who uses the product (i.e., Clickhouse's marketing stunt thingy) becomes aware of the license and origin of the underlying data. And not by digging into some GitHub repo, but right there, on the page. reply malwrar 13 hours agorootparentTheir banner on the bottom links to adsb.lol and adsbexchange.com, is your specific concern that they don’t have the odbl license called out? I personally didn’t have much trouble figuring out where the data for this project came from based on their banner alone, and thought it was honorable of them to publish their process for obtaining the data. I have no horse in this race, but am really confused by this aggressive reaction to what I perceive to be a good-faith use of this data. Is this the prelude to some scheme by which you plan to extract money from ClickHouse? The grievance in these replies is genuinely unclear to me. reply ryandrake 12 hours agorootparentprevI don't think anyone is questioning the correctness of what you are saying, just the aggressive tone and assumption of malice over mistake. Maybe take this as an opportunity to educate licensees rather than ridicule them. reply singleshot_ 12 hours agorootparentprevLicenses get abused for one of two reasons, more or less: 1) the license is unenforceable; 2) no one enforces the license. In some cases, this is only one reason. reply rafram 17 hours agoparentprevThis is some copyleft troll-level pedantry [0]. They clearly made a good effort to comply with the license. Additionally, ADSB.lol requires contributors to license their contributions under CC0. Databases that don't have some sort of creative work involved in their compilation aren't copyrightable, so it's very dubious that anyone could enforce any sort of restrictive license over ADSB.lol's database as a whole when its individual contributions are CC0. [0]: https://pluralistic.net/2022/01/24/a-bug-in-early-creative-c... reply dirkhh 16 hours agorootparentNot at all. It is the polite request to please correctly attribute the data that a company is using for their marketing stunts. I love the fact that techbros being called out for violating other people's intellectual property immediately revert to \"TROLL!\" or \"this is too hard!\" instead of actually engaging with the question at hand. And btw - until you actually create the software stack to collect the data and run an aggregator, don't condescend on people who do the hard work that you seem to feel free to copy in violation of said license. reply jjwiseman 15 hours agorootparentNo one said complying was too hard. This is one of the coolest things anyone's done with your data, they made a good faith effort to comply, and you've done nothing but act like a jerk and project your issues all over this thread. If I ran an ADS-B aggregator that was based on the same software as a half-dozen others, which hadn't seen any significant innovation in years, sitting on tons of data with absolutely amazing potential, I might consider praising the person who just revolutionized the world of ADS-B analysis & visualization, and possibly begging them to help me. And \"P.S., could you throw in [specific attribution text].\" reply aeternum 6 hours agoprevThe detail is amazing, you can see down the the individual tiedown rows at many small airports. reply panki27 19 hours agoprevWow, this is beautiful! I predict this will get a hug of death $soon reply zX41ZdbW 19 hours agoparentI keep the dashboard open :) So far ok with ~2000 QPS. reply yegle 17 hours agoprevSometimes I feel lucky that human's electromagnetic spectrum perception is very limited to the so called \"visible spectrum\". Imaging alien species that evolved to perceive a wider electromagnetic spectrum, the earth must look like a disco ball when their spaceship approaches. reply kqr 16 hours agoparentMuch like we're lucky to have eyes that happen to see the sun's strongest frequencies? reply ptero 11 hours agoprevThis is fantastic! Is the airplanes.live related to ADS-B exchange or is this an unrelated effort? reply consumer451 15 hours agoprevThe one glider path over the LAX area is interesting. What was going on there? reply sllabres 14 hours agoparentI think what you see is CIVIL AIR PATROL and CIVIL AIR PATROL INC Types: GLID (SGS 2-33A) Flights: N7589, N2037T Registration: N7589, N2037T When you select the dottet square button in the lower left and select a rectangle the planes within this rectangle are listed Very interesting project! reply consumer451 14 hours agorootparentOh, thanks. I somehow missed that UI. The glider I was thinking about is actually N914SF, a Pipistrel Sinus. That's a motorized glider, which makes a lot more sense flying though Class D airspace, right above LAX. reply spdustin 13 hours agoprevThe \"strange hole near Mexico City\" example in the Github repo's README is another volcano. reply swozey 10 hours agoprevI'm evidently the only person who has no idea what in the world this is or where the data comes from other than clearly being some flight/vehicle data HN has such a unique group of people https://en.wikipedia.org/wiki/Automatic_Dependent_Surveillan... reply nickphx 16 hours agoprevIs it possible to export a report of traffic by a selected region? reply zX41ZdbW 13 hours agoparentYes, it is possible to connect to the database directly and run an arbitrary query. Example: $ clickhouse-client --user website --host kvzqttvc2n.eu-west-1.aws.clickhouse-staging.com clickhouse-cloud :) SELECT t, desc, count() AS c FROM planes_mercator_sample100 GROUP BY ALL ORDER BY c DESC LIMIT 10 ┌─t────┬─desc────────────────────────────┬────────c─┐ 1. │ B738 │ BOEING 737-800 │ 51530781 │ 2. │ A320 │ AIRBUS A-320 │ 37196762 │ 3. │ C172 │ CESSNA 172 Skyhawk │ 20049393 │ 4. │ A321 │ AIRBUS A-321 │ 19983151 │ 5. │ A20N │ AIRBUS A-320neo │ 14938832 │ 6. │ B38M │ BOEING 737 MAX 8 │ 14200826 │ 7. │ B737 │ BOEING 737-700 │ 13929403 │ 8. │ A319 │ AIRBUS A-319 │ 13906164 │ 9. │ E75L │ EMBRAER ERJ-170-200 (long wing) │ 12006441 │ 10. │ A21N │ AIRBUS A-321neo │ 10965047 │ └──────┴─────────────────────────────────┴──────────┘ Add FORMAT CSV to output in CSV (or any other format). To obtain an SQL query for a particular region, you can open the browser dev tools (F12), switch to Network, and copy a particular request that is made when you select an area with the rectangle selection tool. reply nickphx 4 hours agorootparentAh very nice, thank you kind internet friend. reply throwaway743 16 hours agoprev [–] This is fantastic! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The creator built a web app to query and visualize ADS-B datasets, showcasing striking visuals and valuable insights.",
      "The app demonstrates generating pixel-level tiles, compressing raw bitmap data, and offering real-time functionality.",
      "Interested individuals can access the source code for the app on GitHub."
    ],
    "commentSummary": [
      "A new web app named ADS-B visualizer allows users to query and visualize ADS-B datasets, receiving acclaim for its innovative approach to analyzing and visualizing aircraft traffic.",
      "Discussions on military aircraft, gliders, and drone sightings emphasize the complexity and significance of ADS-B technology.",
      "Users engage in conversations about open data licensing, attribution challenges, and the potential of projects utilizing ADS-B data collection and visualization, as well as sharing tips on exporting reports through the ADS-B system."
    ],
    "points": 284,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1712754902
  },
  {
    "id": 39990004,
    "title": "Avi Wigderson Receives 2023 ACM Turing Prize for Computational Theory Contributions",
    "originLink": "https://awards.acm.org/about/2023-turing",
    "originBody": "Home Latest Awards News 2023 Turing Award ACM A.M. Turing Award Honors Avi Wigderson for Foundational Contributions to the Theory of Computation Wigderson is recognized for reshaping our understanding of the role of randomness in computation, and for decades of intellectual leadership in theoretical computer science. ACM has named Avi Wigderson as recipient of the 2023 ACM A.M. Turing Award for foundational contributions to the theory of computation, including reshaping our understanding of the role of randomness in computation, and for his decades of intellectual leadership in theoretical computer science. Wigderson is the Herbert H. Maass Professor in the School of Mathematics at the Institute for Advanced Study in Princeton, New Jersey. He has been a leading figure in areas including computational complexity theory, algorithms and optimization, randomness and cryptography, parallel and distributed computation, combinatorics, and graph theory, as well as connections between theoretical computer science and mathematics and science. The ACM A.M. Turing Award, often referred to as the “Nobel Prize of Computing,” carries a $1 million prize with financial support provided by Google, Inc. The award is named for Alan M. Turing, the British mathematician who articulated the mathematical foundations of computing. What is Theoretical Computer Science? Theoretical computer science is concerned with the mathematical underpinnings of the field. It poses questions such as “Is this problem solvable through computation?” or “If this problem is solvable through computation, how much time and other resources will be required?” Theoretical computer science also explores the design of efficient algorithms. Every computing technology that touches our lives is made possible by algorithms. Understanding the principles that make for powerful and efficient algorithms deepens our understanding not only of computer science, but also the laws of nature. While theoretical computer science is known as a field that presents exciting intellectual challenges and is often not directly concerned with improving the practical applications of computing, research breakthroughs in this discipline have led to advances in almost every area of the field—from cryptography and computational biology to network design, machine learning, and quantum computing. Why is Randomness Important? Fundamentally, computers are deterministic systems; the set of instructions of an algorithm applied to any given input uniquely determines its computation and, in particular, its output. In other words, the deterministic algorithm is following a predictable pattern. Randomness , by contrast, lacks a well-defined pattern, or predictability in events or outcomes. Because the world we live in seems full of random events (weather systems, biological and quantum phenomena, etc.), computer scientists have enriched algorithms by allowing them to make random choices in the course of their computation, in the hope of improving their efficiency. And indeed, many problems for which no efficient deterministic algorithm was known have been solved efficiently by probabilistic algorithms, albeit with some small probability of error (that can be efficiently reduced). But is randomness essential, or can it be removed? And what is the quality of randomness needed for the success of probabilistic algorithms? These, and many other fundamental questions lie at the heart of understanding randomness and pseudorandomness in computation. An improved understanding of the dynamics of randomness in computation can lead us to develop better algorithms as well as deepen our understanding of the nature of computation itself. Wigderson’s Contributions A leader in theoretical computer science research for four decades, Wigderson has made foundational contributions to the understanding of the role of randomness and pseudorandomness in computation. Computer scientists have discovered a remarkable connection between randomness and computational difficulty (i.e., identifying natural problems that have no efficient algorithms). Working with colleagues, Wigderson authored a highly influential series of works on trading hardness for randomness. They proved that, under standard and widely believed computational assumptions, every probabilistic polynomial time algorithm can be efficiently derandomized (namely, made fully deterministic). In other words, randomness is not necessary for efficient computation. This sequence of works revolutionized our understanding of the role of randomness in computation, and the way we think about randomness. This series of influential papers include the following three: “Hardness vs. Randomness” (co-authored with Noam Nisan) Among other findings, this paper introduced a new type of pseudorandom generator, and proved that efficient deterministic simulation of randomized algorithms is possible under much weaker assumptions than previously known. “BPP Has Subexponential Time Simulations Unless EXPTIME has Publishable Proofs” (co-authored with László Babai, Lance Fortnow, and Noam Nisan) This paper used `hardness amplification’ to demonstrate that bounded-error probabilistic polynomial time (BPP) can be simulated in subexponential time for infinitely many input lengths under weaker assumptions. “P = BPP if E Requires Exponential Circuits: Derandomizing the XOR Lemma” (co-authored with Russell Impagliazzo) This paper introduces a stronger pseudo-random generator with essentially optimal hardness vs randomness trade-offs. Importantly, the impact of these three papers by Wigderson goes far beyond the areas of randomness and derandomization. Ideas from these papers were subsequently used in many areas of theoretical computer science and led to impactful papers by several leading figures in the field. Still working within the broad area of randomness in computation, in papers with Omer Reingold, Salil Vadhan, and Michael Capalbo, Wigderson gave the first efficient combinatorial constructions of expander graphs, which are sparse graphs that have strong connectivity properties. They have many important applications in both mathematics and theoretical computer science. Outside of his work in randomness, Wigderson has been an intellectual leader in several other areas of theoretical computer science, including multi-prover interactive proofs, cryptography, and circuit complexity. Mentoring In addition to his groundbreaking technical contributions, Wigderson is recognized as an esteemed mentor and colleague who has advised countless young researchers. His vast knowledge and unrivaled technical proficiency—coupled with his friendliness, enthusiasm, and generosity—have attracted many of the best young minds to pursue careers in theoretical computer science. “It’s important to point out that Avi Wigderson also received the Abel Prize, which is considered the most important honor for lifetime achievements in the field of mathematics,” explained ACM President Yannis Ioannidis. “Being selected for the ACM A.M. Turing Award is a fitting follow-up—as mathematics is foundational to computer science and Wigderson’s work has connected a wide range of mathematical sub-areas to theoretical computer science. Wigderson is a towering intellectual force in theoretical computer science, an exciting discipline that attracts some of the most promising young researchers to work on the most difficult challenges. This year’s Turing Award recognizes Wigderson’s specific work on randomness, as well as the indirect but substantial impact he has had on the entire field of theoretical computer science.” \"Avi Wigderson’s work on randomness and other topics has set the agenda in theoretical computer science for the past three decades,” explained Jeff Dean, Senior Vice President, Google. “From the earliest days of computer science, researchers have recognized that incorporating randomness was a way to design faster algorithms for a wide range of applications. Efforts to better understand randomness continue to yield important benefits to our field, and Wigderson has opened new horizons in this area. Google also salutes Wigderson’s role as a mentor. His colleagues credit him with generating great ideas and research directions, and then motivating a new generation of smart young researchers to work on them. We congratulate Avi Wigderson on receiving the ACM A.M. Turing Award—computing’s highest honor.” News ReleasePrintable PDF About the ACM A.M. Turing Award The A.M. Turing Award was named for Alan M. Turing, the British mathematician who articulated the mathematical foundation and limits of computing, and who was a key contributor to the Allied cryptanalysis of the Enigma cipher during World War II. Since its inception in 1966, the Turing Award has honored the computer scientists and engineers who created the systems and underlying theoretical foundations that have propelled the information technology industry. 2023 ACM A.M. Turing Award Laureate Avi Wigderson is the Herbert H. Maass Professor in the School of Mathematics at the Institute for Advanced Study in Princeton, New Jersey. He has been a leading figure in areas including computational complexity theory, algorithms and optimization, randomness and cryptography, parallel and distributed computation, combinatorics, and graph theory, as well as connections between theoretical computer science and mathematics and science. Wigderson’s honors include the Abel Prize, the IMU Abacus Medal (previously known as the Nevanlinna Prize), the Donald E. Knuth Prize, the Edsger W. Dijkstra Prize in Distributed Computing, and the Gödel Prize. He is an ACM Fellow and a member of the U.S. National Academy of Sciences and the American Academy of Arts and Sciences. Media Coverage Communications of the ACM: \"Wigderson Named Turing Awardee for Decisive Work on Randomness\" Quanta: \"Avi Wigderson, Complexity Theory Pioneer, Wins Turing Award\" Nature: \"Randomness in computation wins computer-science ‘Nobel’\" CTech: \"Israeli mathematician Avi Wigderson wins 2023 Turing Prize for insights into randomness\" Haaretz: \"Israeli Wins Turing Prize, Computing's Highest Honor, for Insights on Randomness\" The Star Ledger / NJ.com: \"N.J. professor wins $1M Turing prize for helping teach computers ‘randomness’\" New Scientist: \"Mathematician wins Turing award for harnessing randomness\" Institute for Advanced Study: \"Avi Wigderson Receives 2023 ACM A.M. Turing Award\" Notable Papers by Avi Wigderson Avi Wigderson's work has reshaped our understanding of the role of randomness in computation. His important papers on the subject include: \"Hardness vs Randomness\" (co-authored with Noam Nisan) introduced a new type of pseudorandom generator, and proved that efficient deterministic simulation of randomized algorithms is possible under much weaker assumptions than previously known. \"BPP Has Subexponential Time Simulations Unless EXPTIME has Publishable Proofs\" (co-authored with László Babai, Lance Fortnow, and Noam Nisan) used \"hardness amplification\" to demonstrate that bounded-error probabilistic polynomial time (BPP) can be simulated in subexponential time for infinitely many input lengths under weaker assumptions. “P = BPP if E Requires Exponential Circuits: Derandomizing the XOR Lemma” (co-authored with Russell Impagliazzo) introduced a stronger pseudo-random generator with essentially optimal hardness vs randomness trade-offs. \"In Search of an Easy Witness: Exponential Time vs Probabilistic Polynomial Time\" (co-authored with Russell Impagliazzo and Valentine Kabanets) establishes a number of results relating the complexity of exponential-time and probabilistic polynomial-time complexity classes. \"Randomness vs. Time: De-Randomization Under a Uniform Assumption\" (co-authored with Russell Impagliazzo) proves that if BPP≠EXP, then every problem in BPP can be solved deterministically in subexponential time on almost every input (on every sampleable ensemble for infinitely many input sizes). \"Multi-Prover Interactive Proofs: How to Remove Intractability Assumptions\" (co-authored with Michael Ben-Or, Shafi Goldwasser, and Joe Kilian) proves that all NP languages have perfect zero-knowledge proof-systems. \"Proofs That Yield Nothing but Their Validity or All Languages in NP Have Zero-Knowledge Proof Systems\" (co-authored with Oded Goldreich and Silvio Micali ). Under the assumption that secure encryption functions exist or by using “physical means for hiding information,\" it is shown that all languages in NP have zero-knowledge proofs.",
    "commentLink": "https://news.ycombinator.com/item?id=39990004",
    "commentBody": "2023 ACM Turing Prize awarded to Avi Wigderson (acm.org)256 points by nanna 21 hours agohidepastfavorite60 comments ode 16 hours agoTwo of Wigderson's major papers mentioned in the announcement are co-authored with Noam Nisan, one of the professors behind the well known on-line course 'From Nand to Tetris'. reply YouWhy 6 hours agoparentProf. Nissan is also remarkable due to having had first line achievements in CS Theory and then shifting to do major impact in the rather distinct field of algorithmic game theory. It's both heart warming that a single person living today could be as diverse, and that the system allowed him that flexibility. reply COGlory 19 hours agoprevHere's a great Quanta article as well: https://www.quantamagazine.org/avi-wigderson-complexity-theo... One thing I enjoyed was the variety of poses they had Wigderson in. It just looks so awkward. \"Here, sit in this chair and look out the window longingly\". reply gaws 9 hours agoparent> Correction: April 10, 2024 > The original version of this article said Wigderson attended the University of Haifa. He actually graduated from the Technion, in Haifa, Israel. How did the reporter mess that up? reply chaosite 6 hours agorootparentHaifa is the only city in Israel to have two public universities (excluding branches of the Open University.) I can see the reporter reading that he attended university in Haifa and misconstruing that to mean the University of Haifa... reply jkaptur 16 hours agoparentprev> The unreasonable effectiveness of randomness led him to think about the nature of randomness itself. He, like other researchers at the time, questioned how necessary it was for efficient problem-solving and under what conditions it could be eliminated altogether. “Initially, it was not clear if this was only our own stupidity, that we cannot remove randomness,” he said. “But the larger question was whether randomness can always be eliminated efficiently or not.” He realized that the need for randomness was intimately tied to the computational difficulty of the problem. Does anyone have a sketch of how this works? My understanding of complexity classes is that they consider worst case performance. Even if you have a great pseudorandom number generator and a great randomized algorithm, how do you prove that no instance of RNG + seed + problem instance takes exponential time? reply bo1024 14 hours agorootparentBPP is the complexity class for decision problems that a randomized algorithm can solve in polynomial time, in the sense that on every input (worst-case), the algorithm is right with at least 2/3 probability (over its own randomness). One example is deciding whether a given number is prime. For a long time, we knew randomized algorithms like the Miller-Rabin test that are correct with high probability. That is, we knew PRIMES is in BPP. But we didn't know a deterministic algorithm. In 2006, a deterministic algorithm was discovered, proving that PRIMES is in P. One of the central open problems in Computer Science is whether BPP=P. That is, for any problem we can solve with randomness (in the above sense), can we solve it without randomness? Among many other contributions, Wigderson's work has made a ton of progress toward this problem. The general idea is to try to \"derandomize\" any randomized algorithm that runs in polynomial time and is correct with 2/3 probability. We will feed it inputs from a pseudorandom generator (instead of true random numbers). If the PRG is really really good, then we can get a deterministic algorithm by running the original algorithm along with the PRG. Now, if we can prove certain problems are hard, then we can also prove that such really really good PRGs exist. This is basically because telling apart the PRG from true randomness would be a computationally hard problem. reply sn41 8 hours agorootparentprevAn important idea is to use what are called worst-case-to-average-case reductions. You convert a boolean function f that is worst-case hard to another function g which is average-case hard. In other words, if g is average-case easy, then f is worst-case easy. These reductions are a major achievement in complexity theory. Constructions use tools like combinatorial designs, list-decoding of error-correcting codes, expander graphs, extractors etc. A good overview of these methods is in Arora and Barak's \"Computational complexity\", in, I believe Chapters 19 through 21. reply hinkley 16 hours agorootparentprevRandomization prevents the worst degenerate cases from being repeatable and thus exploitable. (By an attacker or self-own). Using them is really in the applied CS rather than a theoretical CS domain. The problem can still happen, it just no longer happens consistently to the same customer or at the same time every day. Eliminating the randomized solution was not on my radar so I’ve got some homework to do. reply pca006132 16 hours agorootparentNo, there are complexity classes for random algorithms. And randomness is also crucial for some cryptographic protocols, typically you want to show that the probability the adversary can do what they want is very slim, e.g. 2^-n for some large n. reply hinkley 16 hours agorootparentIn crypto we are worried about the best case scenario (from the attacker’s perspective). Randomization in algorithms, to avoid worst case behavior, would be things like shuffling input, XORing hashes for query parameters per run or per hash table, or using random tiebreakers. reply wbl 11 hours agorootparentThat's completely untrue. For example there's an easy randomized algorithm to determine a quadratic nonresidue modulo a prime with success probability 1/2 per Legendre symbol evaluation. In expectation this is 2 iterations, and that's independent of the prime. The best known deterministic algorithm takes log(p)^2 such evaluations in the worst case, even assuming generalized Riemann hypothesis. reply kadoban 15 hours agorootparentprev> In crypto we are worried about the best case scenario (from the attacker’s perspective). That seems mistaken to me. The best case in most crypto, from an attacker's perspective, is \"I tried one key and just happened to get it right\". Aren't we more interested in expected outcomes? reply hinkley 11 hours agorootparentIn the sense of big O you’re right and I misspoke. When we are deciding to retire an algorithm it’s often down to how many unaddressed weaknesses there are and assuming that they compose. That’s the best case I was referring to. reply pca006132 16 hours agorootparentprevYeah randomization in algorithms and crypto serve different purpose. But overall introducing randomness can allow you to do amazing things that deterministic algorithm/schemes cannot do. reply sandspar 12 hours agoparentprevThat \"sitting in chair looking out the window\" pose is a Martin Scorsese or Sopranos type pose. The elderly gangster in the old folk's home etc. reply dataexp 18 hours agoprevIn (*) Scott Aaronson relates the impact of one of Avi Wigderson talks on his career. https://scottaaronson.blog/?p=2925 reply hinkley 16 hours agoprev> Computer scientists have discovered a remarkable connection between randomness and computational difficulty (i.e., identifying natural problems that have no efficient algorithms). Working with colleagues, Wigderson authored a highly influential series of works on trading hardness for randomness. They proved that, under standard and widely believed computational assumptions, every probabilistic polynomial time algorithm can be efficiently derandomized (namely, made fully deterministic). In other words, randomness is not necessary for efficient computation. This sequence of works revolutionized our understanding of the role of randomness in computation, and the way we think about randomness. How would I go about catching up with this aspect of his research? It’s not often that I’ve never heard of a Turing winner, but this guy is completely off of my radar. reply jumpCastle 13 hours agoparentYou can try his book. https://www.math.ias.edu/avi/book reply layer8 16 hours agoparentprevI wonder what the “standard and widely believed computational assumptions” are. Presumably, probabilistic approximations to NP-complete problems are not polynomial-time? Or the derandomized versions would still be just approximations? reply bo1024 14 hours agorootparentThey are generally esoteric conjectures similar in spirit to P != NP. For example, the third paper uses the assumption \"E requires exponential circuits\". Here E is the class of problems solvable in exponential time, or 2^O(n) time, on a Turing Machine. Another model of computation besides Turing Machines are Boolean circuits, i.e. AND, OR, and NOT gates. The conjecture is that not every problem in E can be solved by \"small\" (subexponential-sized) circuits. The basic idea of the work is that if these problems are hard, then we can use them to build pseudorandom generators that are \"just as good\" as true random, which we can use to turn truly random algorithms into pseudorandom algorithms with the same performance. reply reader5000 15 hours agorootparentprevI think his argument assumes the existence of pseudorandom generators which map a small amount of \"true\" random bits to a large amount of bits that look random to any polytime observer. The \"derandomization\" is that we just have to check all possible states of the seed bits which hopefully will be logarithmic in the size of the problem so you can do exhaustive checking. reply polymipisexp 14 hours agorootparentNisan and Wigderson prove many different corollaries of their construction in their seminal 'Hardness vs Randomness' paper but their requirement for general derandomization (P = BPP) is that there's some function f computable in time 2^O(n) such that for some e > 0 for all circuits of size 2^en the correlation between f and the output of the circuit is sufficiently low (if I understand correctly). reply wslh 18 hours agoprevMore info at \"Israeli Wins Turing Prize, Computing's Highest Honor, for Insights on Randomness\": [1] and its archived version [2] [1] https://www.haaretz.com/israel-news/2024-04-10/ty-article/.p... [2] https://archive.is/e8uix reply wslh 11 hours agoparentLooking at the downvotes it seems like HN is now a hub for antisemitism. reply tu7001 7 minutes agoprevAnyone some good Avi youtube videos? reply pthreads 18 hours agoprevJust picked up Wigderson's book and I am liking it so far : https://press.princeton.edu/books/hardcover/9780691189130/ma... reply teleforce 17 hours agoparentFinal draft version of the book available here for personal research and education: https://www.math.ias.edu/avi/book reply myth_drannon 16 hours agoparentprevI looked at the book and it's more for graduate-advanced undergrad students. Can someone recommend a more basic book on the topic of computation for someone with a rusty comp-sci/math undergrad background? reply rramadass 4 hours agorootparenthttps://news.ycombinator.com/item?id=39721301 reply pthreads 12 hours agorootparentprevTry What Can be Computed by John MacCormick : https://press.princeton.edu/books/hardcover/9780691170664/wh... reply jimhefferon 10 hours agorootparentprevYou could have a look at https://hefferon.net/computation which is certainly aimed at a broader audience. reply sn9 10 hours agorootparentprevSipser is the canonical text for undergraduates. reply cryptoxchange 12 hours agorootparentprevIntroduction to the Theory of Computation by Michael Sipser reply tinhhuynh_97 17 hours agoparentprevAgreed reply thedatamonger 14 hours agoprevFrom the related article: https://www.quantamagazine.org/avi-wigderson-complexity-theo... > ... if a statement can be proved, it also has a zero-knowledge proof. Mind blown. >Feeding the pseudorandom bits (instead of the random ones) into a probabilistic algorithm will result in an efficient deterministic one for the same problem. This is nuts. AI is a probabilistic computation ... so what they're saying - if i'm reading this right - is that we can reduce the complexity of our current models by orders of magnitude. If I'm living in noobspace someone please pull me out. reply ilya_m 13 hours agoparent> AI is a probabilistic computation ... so what they're saying - if i'm reading this right - is that we can reduce the complexity of our current models by orders of magnitude. Unfortunately, no. First, the result applies to decision, not search problems. Second, the resulting deterministic algorithm is much less efficient than the randomized algorithm, albeit it still belongs to the same complexity class (under some mild assumptions). reply mxkopy 13 hours agorootparentCan’t you build search from decision by deciding on every possible input? reply IshKebab 13 hours agoparentprevI don't know exactly what it's saying but it definitely isn't that. AI already uses pseudorandom numbers and is deterministic. (Except some weird AI accelerator chips that use analogue computation to improve efficiency.) reply rramadass 4 hours agoprevAny study recommendations (beginner friendly to advanced) on this topic i.e. Probability/Randomness and Computation? Google brings up Probability and Computing: Randomization and Probabilistic Techniques in Algorithms and Data Analysis by Eli Upfal and Michael Mitzenmacher but i can't find any beginner/introductory books/articles/videos. reply srvmshr 21 hours agoprevFrom ACM: ACM has named Avi Wigderson as recipient of the 2023 ACM A.M. Turing Award for foundational contributions to the theory of computation, including reshaping our understanding of the role of randomness in computation, and for his decades of intellectual leadership in theoretical computer science. Wigderson is the Herbert H. Maass Professor in the School of Mathematics at the Institute for Advanced Study in Princeton, New Jersey. He has been a leading figure in areas including computational complexity theory, algorithms and optimization, randomness and cryptography, parallel and distributed computation, combinatorics, and graph theory, as well as connections between theoretical computer science and mathematics and science. Also of interest, he has won the Abel Prize in 2021, making it a rather unique combination of winning the top honors in both theoretical/abstract math & CS reply polygamous_bat 19 hours agoparent> Also of interest, he has won the Abel Prize in 2021, making it a rather unique combination of winning the top honors in both theoretical/abstract math & CS The overlap between theoretical CS and math is way larger than most people know. For a simple example, check out the theoretical CS course catalog at MIT: https://catalog.mit.edu/subjects/6/ and how many of them are cross listed as course 18 (math) classes. reply vecter 18 hours agorootparentTheoretical CS is basically a branch of applied math reply waldrews 18 hours agorootparentPerhaps 'an applied branch of math' since the term 'applied math' is claimed by something usually disconnected from the discrete math subjects CS tends to study. reply Kranar 17 hours agorootparentprevTheoretical CS is not at all applied math. Ordinary computer science overlaps with applied math, but theoretical CS is very abstract. reply jchonphoenix 17 hours agoparentprevTechnically the Field's medal is the top honor in mathematics. But who am I to talk. reply Kranar 17 hours agorootparentThere's no formal hierarchy so there's nothing technical about it. Field's medal is certainly prestigious but it's not widely applicable, it has a bunch of restrictions that don't have anything to do with math itself, including an age limit. For example no one who has ever been awarded an Abel Prize would qualify for a Field's medal strictly due to age. reply math_dandy 15 hours agorootparentSix Abel laureates (out of 22) had previously won the Fields Medal: Serre, Aliyah, Thompson, Milner, Deligne, and Margulis. reply srvmshr 17 hours agorootparentprevTrue, it is considered one of the two top honors in Math since last decade. Previously it was the only distinguished prize. There was a growing need for another award which bridged few gaps. The 40 year cutoff age, awarded every 4 years to living mathematical prodigies failed to honor several prominent mathematical breakthroughs which came after decades of painstaking research. As the field has progressed, monumental breakthroughs are harder to come by early into career. Many of the ingenuity comes from cross-study of disciplines for e.g. Riemannian hypothesis being approached by Algebraic geometry and Topology rather than number theory. These require years of mastery - not just prodigy. Also the prize money offered by Abel Foundation is a good incentive for research into pure math reply unethical_ban 19 hours agoprevCongratulations. This was a pretty well written summary by ACM. reply WhitneyLand 17 hours agoprev [11 more] [flagged] trollied 14 hours agoparentWhy would you ask a model primarily, and add a disclaimer, instead of actually going to a proper source? Madness. reply seanmcdirmid 17 hours agoparentprev [–] I really would hate if awards started being handed out to meet diversity requirements. Hopefully we don't get their, and increased diversity in the field today means more diverse Turing awards 30-40 years from now. reply WhitneyLand 17 hours agorootparentYou’re absolutely right they should not! I should’ve clarified that in my comment. I believe awareness and an encouraging and welcoming environment for all are the foundation for everyone to do their best work. reply wolverine876 14 hours agorootparentprev [–] That's the standard argument, but it's actually a carefully constructed talking point that (like many such things) twists the premise. The issue is that it's effectively awarded based on a sort of 'diversity requirement' now - white / asian males being the requirement. The evidence is overwhelming - there's no way that's coincidence, or coincidental with all the racial exclusion in the past. Racism, sexism, other forms of prejudice clearly still exist (look at social media if you need a refresher). The question is, what do we do about it? reply seanmcdirmid 14 hours agorootparent [–] The vast majority of computer scientists up until maybe 10 years ago were white/asian and male. Genius is simply a numbers game, along with a time game to develop it and then be recognized for it. > The question is, what do we do about it? Whatever the answer is, you need to start at the beginning of the pipeline, not the end of it. reply wolverine876 14 hours agorootparent [–] I think it's a fantasy that there is not significant prejudice at the end of the pipeline, where we are. Everyone seems to have that fantasy about themeselves, their organization, their community, etc. The answer starts, as with every problem, by looking carefully in the mirror, with fresh eyes, as if there was a stranger staring back. Then look at your own organization, your own community, etc. reply seanmcdirmid 13 hours agorootparent [–] Avi Wigderson is 67 years old on got his PhD in 1983. Seriously, how hard is this to understand? Just do the math. reply wolverine876 13 hours agorootparent [–] If we are talking about basic reasoning, you know that doesn't mean there is not a major problem of discrimination at this end of the pipeline. reply seanmcdirmid 13 hours agorootparent [–] I know there is discrimination at the end of the pipeline, there has been for a few years now, but it isn't in the direction that you think its in, and it isn't relevant to who is winning a once a year award that is based on work someone started 40+ years ago. reply wolverine876 4 hours agorootparent [–] > it isn't in the direction that you think its in It is exactly in that direction; just look at the outcomes. You take the long-term standard, where white (and Asian) males get advantages, and normalize it - norms are unremarkable, the null hypothesis. Changing the norm then looks like discrimination. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Avi Wigderson receives the 2023 ACM A.M. Turing Award for his groundbreaking contributions to the theory of computation, particularly in redefining the role of randomness.",
      "Wigderson's work spanning four decades has significantly influenced various domains like cryptography, network design, machine learning, and quantum computing.",
      "The Turing Award highlights Wigderson's impact on theoretical computer science, describing it as the \"Nobel Prize of Computing,\" showcasing his role in advancing algorithm efficiency and our comprehension of computation and nature."
    ],
    "commentSummary": [
      "Avi Wigderson received the 2023 ACM Turing Prize for his contributions to computational theory and randomness, collaborating with Noam Nisan.",
      "His deterministic algorithm for PRIMES in P was a notable breakthrough, emphasizing the significance of randomness in problem-solving and computational complexity.",
      "The discussion highlights the impact of Wigderson's research on computation, algorithmic randomness, diversity in computer science, and societal norms."
    ],
    "points": 256,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1712752603
  },
  {
    "id": 39993976,
    "title": "Challenges of Optimizing Code Search in Val Town",
    "originLink": "https://blog.val.town/blog/search-notes/",
    "originBody": "Code Search is Hard Tom MacWright on Apr 10, 2024 Val Town’s search functionality isn’t very good. Right now it’s built on the Postgres ILIKE functionality, which just performs a substring search: if your search term is in the code, it appears in search results. There’s virtually no ranking involved, and queries with multiple words are pretty poorly supported. Better search is one of our most-requested features. I’m working on improving this, but we haven’t found a solution that fits our needs yet. Here are some notes from our research. So far what we’ve learned is that: Mainstream search solutions are designed for natural language, not code. Big companies with code search needs have spent a lot of time and money building their own custom solutions. We have a lot of data already, and need a solution that scales well. The infrastructure and complexity tradeoffs involved in using a separate search service instead of a database extension are important. Code search versus natural language search A common issue with off-the shelf search solutions is that they’re designed to work with English and other natural languages. For example, here are some of the algorithms you get by default with a usual FTS setup: Stop words removal: words like “the” and “it” are removed from text before it is indexed, because they’re so common that they cause more problems for performance than they’re worth. Stemming: this mostly reverses conjugation, turning a word like “running” into “run” before it is added to the index, and doing the same for search queries, so that you can search for “runs” and get a search result for a document with the term “running.” Lemmatization: some search indexes are even fancy enough to substitute synonyms for more common words, so that you can search for “excellent” and get results for documents including “great.” All together, this means that the vector derived from a document that you’re storing in the index does not look like the document at all: select * from to_tsvector('english', 'I am writing this example sentence'); --- 'exampl':5 'sentenc':6 'write':3 The problem with all of these rules is that they wreak havoc on code. the is not a stop-word in TypeScript: it’s a valid variable name that you might want to search for. Word boundaries aren’t the same, and stemming function names doesn’t make much sense. select * from to_tsvector('english', 'function stringifyNumber(a: number): string { return a.toString() }'); -- 'a.tostring':7 'function':1 'number':4 'return':6 'string':5 'stringifynumb':2 This is a pretty bad index: it has words that should be stop words, like function, and won’t split a.toString() into two tokens because . is not a default word boundary. Full Text Search Postgres has a Full Text Search extension which is supported by our hosting provider, Render. I’ve used FTS in previous projects, and for certain scales, it works great. You can try and use Postgres for everything, and frankly, so far we have: we’ve been using the heck out of Postgres. It’s a fantastic piece of technology with great documentation that is well-supported by our hosting provider. If we can use Postgres for something, we will: keeping infrastructure as simple as possible is essential with a small team. However, the previous projects I’ve used FTS for have run into performance problems and struggled to scale - Observable ended up moving to Elasticsearch. We have a ton of vals, and are testing the limits of a single-node Postgres cluster. It’s hard to find any accounts of code-search using FTS, though people might be quietly succeeding with it. I wanted to avoid this as a first option but keep it in my back pocket. pg_trgrm The solution that we’ve soft-launched as the v2 search algorithm is based on pg_trgrm, which implements trigram search in Postgres. Code search does seem to succeed with trigrams: Russ Cox’s famous (to me?) piece from 2012 tells the story of how Google Code Search used trigram indexes and a special regex implementation to succeed, technically. GitHub’s new search system uses trigram search too, in addition to a lot of technology that I’m jealous of. Sourcegraph has a trigram-based search tool that they’ve inherited from Google, too. Our work with the Postgres pg_trgrm approach has been heavily informed by Stephen Gutekanst’s blog post series about indexing repositories locally in Postgres. We’ve created a GIN index with gin_trgm_ops on a column containing search text. The conclusion so far is that this is a great solution for regex search, but we’re not doing regex search: most searches are more freeform. We’re using word_similarity for search ranking, and it has been very hard to coax the algorithm into giving us anything like a reasonable ranking. The universe of options Option Architecture Language Stars Meilisearch Standalone Rust 41k Typesense Standalone C++ 17k Zoekt Standalone Go 406 ParadeDB Postgres extension Rust 3.2k Sonic Standalone Rust 19.4k There are code-specific tools that exist, but most of them are closed-source: GitHub’s search is excellent, but is obviously the work of a dedicated team with a real time budget. Sourcegraph’s maintained fork of Zoekt is pretty cool, but is pretty fearfully niche and would be a big, new infrastructure commitment. Elasticsearch might be the eventual, unavoidable solution to this problem. It doesn’t have code-specific handling, but can be customized in nearly infinite ways. We’re not excited to start learning about Java memory tuning and to introduce the first persistent disk storage to our application, as well as an additional source of truth for our data. Possibly we could use Elasticsearch Cloud to avoid the maintenance overhead. Meilisearch seems like a promising ES alternative with the shininess of ✨Rust✨, but they seem to emphasize latency over scalability, and we’re not sure if the infrastructure commitment would be any lower. ParadeDB promises to be like Elasticsearch but “just Postgres,” which is very appealing, but we can’t use their extension in Render yet. In short, we’re still working on it. Searching code instead of English makes the difficulty level a bit higher. For a small team, with an incentive to keep infrastructure simple, development environments easy to set up, and data in the same place, we’re trying to be careful not to commit to something that requires constant upkeep. There’s a reason why most mid and large-sized companies have a search “team,” not just a search service. Edit this page",
    "commentLink": "https://news.ycombinator.com/item?id=39993976",
    "commentBody": "Code search is hard (val.town)240 points by stevekrouse 15 hours agohidepastfavorite123 comments sqs 14 hours agoI'm at Sourcegraph (mentioned in the blog post). We obviously have to deal with massive scale, but for anyone starting out adding code search to their product, I'd recommend not starting with an index and just doing on-the-fly searching until that does not scale. It actually will scale well for longer than you think if you just need to find the first N matches (because that result buffer can be filled without needing to search everything exhaustively). Happy to chat with anyone who's building this kind of thing, including with folks at Val Town, which is awesome. reply isker 13 hours agoparentAnd when you're ready to do indexed search, Zoekt (over which Sourcegraph graciously took maintainership a while ago) is the best way to do it that I've found. After discounting both Livegrep and Hound (they both struggled to perform in various dimensions with the amount of stuff we wanted indexed, Hound moreso than Livegrep), we migrated to Zoekt from a (necessarily) very old and creaky deployment of OpenGrok and it's night and day, both in terms of indexing performance and search performance/ergonomics. Sourcegraph of course adds many more sophisticated features on top of just the code search that Zoekt provides. reply morgante 12 hours agoparentprevI've been surprised at how far you can get without indexing. Ex. I always assume we'll need to add an index to speed up GritQL (https://github.com/getgrit/gritql), but we've gotten pretty far with doing search entirely on the fly. reply worldsayshi 11 hours agorootparentWhat does 'on the fly' entail here? reply simonw 11 hours agorootparentI'm going to guess brute force - scan everything for the search term, rather than trying to use an index. I'm always amazed at how fast ripgrep (rg) can brute force it's way through hundreds of MBs of source code. reply morgante 11 hours agorootparentYes, exactly. When doing a search, we parse and search every file without any indexing. Of course, it could still be sped up considerably with an index but brute force is surprisingly effective (we use some of the same techniques/crates as ripgrep). reply klysm 11 hours agoparentprevI apply this thinking to lots of problems. Do the dumb thing that involves the least state and prove we need to lean more towards memory for speed. It’s much simpler to keep things correct when nothing is cached reply baobun 7 hours agoparentprevYou'll also be in a much better spot to pick appropriate indexing when you actually have sizable and representative workloads. reply hinkley 11 hours agoparentprevThere was someone doing temporal databases that was compressing blocks on disk and doing streaming decompress and search on them. Things in L2 cache go very very fast. reply hiAndrewQuinn 5 hours agoprevBasic code searching skills seems like something new developers are never explicitly taught, but which is an absolutely crucial skill to build early on. I guess the knowledge progression I would recommend would look something kind this: - Learning about Ctrl+F, which works basically everywhere. - Transitioning to ripgrep https://github.com/BurntSushi/ripgrep - I wouldn't even call this optional, it's truly an incredible and very discoverable tool. Requires keeping a terminal open, but that's a good thing for a newbie! - Optional, but highly recommended: Learning one of the powerhouse command line editors. Teenage me recommended Emacs; current me recommends vanilla vim, purely because some flavor of it is installed almost everywhere. This is so that you can grep around and edit in the same window. - In the same vein, moving back from ripgrep and learning about good old fashioned grep, with a few flags rg uses by default: `grep -r` for recursive search, `grep -ri` for case insensitive recursive search, and `grep -ril` for case insensitive recursive \"just show me which files this string is found in\" search. Some others too, season to taste. - Finally hitting the wall with what ripgrep can do for you and switching to an actual indexed, dedicated code search tool. reply plugin-baby 2 hours agoparentApart from speed, what advantages does ripgrep offer over git grep when searching git repos? reply mcintyre1994 1 hour agoparentprevI’d also point out that VSCode uses ripgrep for its search feature which is a great starting point. reply datascienced 5 hours agoparentprevAlso Github is a fantastic tool for searching code across repos, ones you may not even have cloned yet! Either public ones or org ones. reply tex0 1 hour agorootparentThe new GitHub CS is pretty great indeed. Still not on par with it's role model, but getting closer. reply lambdaba 4 hours agorootparentprevyeah, I particularly like the combo regex + path: or lang: reply ayberk 14 hours agoprevIt indeed is hard, and a good code search platform makes life so much easier. If I ever leave Google, the internal code search is for sure going to be the thing I miss the most. It's so well integrated into how everything else works (blaze target finding, guice bindings etc), I can't imagine my life without it. I remember to appreciate it even more every time I use Github's search. Not that it's bad, it's just inherently so much harder to build a generalized code search platform. reply peter_l_downs 14 hours agoparentIf you ever leave you can use Livegrep, which was based on code-search work done at Google. I personally don't use it right now but it's great and will probably meet all your needs. [0] https://github.com/livegrep/livegrep reply keybored 12 hours agorootparent> If you ever leave you can use Livegrep, which was based on code-search work done at Google. If I’ve learned anything from the fainting spells that I-work-at-X have over their internal tools on HN: no, whatever the public/OSS variant is always a mere shadow of the real thing. reply elevatedastalt 8 hours agorootparentYou would be better off actually trying to understand those sentiments instead of posting sarcastic replies on HN. The sort of tight knit integration and developer-focus that internal tools at developer-friendly companies like Google has cannot be matched by clobbering together 50 different SaaS products, half of which will probably run out of funding in 3 years. You literally have entire companies just LARPing internal tools that Google has because they are just that good. Glean is literally Moma. There's really nothing like Critique or Buganizer. reply alienchow 9 hours agorootparentprevJust left Google a few months ago. My take is that there's a difference between a company that is willing to invest money into EngProd endeavors, and a company that uses SaaS for everything. While I can understand that most companies don't have the financial means to invest heavily into EngProd, the outcome is that the tightly integrated development experience in the former is far superior. Code Search is definitely #2 on the list of things I miss the most. reply fragmede 8 hours agorootparentWhat's #1? Memegen? reply alienchow 8 hours agorootparentCheesy answer but, the people. reply voiceblue 8 hours agorootparentprevThe business end of gThanks reply scubbo 12 hours agorootparentprevI suspect you're being sarcastic - but can confirm that being nearly two years out of Amazon, I still miss its in-house CD system nearly every day. I've actively looked around for OSS replacements and very few come anywhere close. (I would be _delighted_ for someone to \"Umm actually\" me by providing a great product!) reply sdesol 10 hours agorootparent> (I would be _delighted_ for someone to \"Umm actually\" me by providing a great product!) I think the issue is, nobody would be willing to pay for a good solution since they usually come with a steep maintenance cost. I wouldn't be surprised if the in-house CD team at Amazon were putting out fires every week/month behind the scene. reply mdaniel 11 hours agorootparentprevMy experience has been that any of these in-house things do not adapt well to the high chaos of external environments, as if there are 3 companies one will find 9 systems and processes in use thus making \"one size fits all\" a fantasy But, I'll bite: what made the CD system so dreamy, and what have you evaluated thus far that fall short? reply shepherdjerred 10 hours agorootparentAmazon internal tools for building codes are _amazing_. Brazil is their internal dependency management tool. It handles building and versioning software. It introduced the concept of version sets which essentially allows you to group up related software, e.g. version 1.0 of my app needs version 1.1 of library x and 2.0 of runtime y. This particular set of software versions get its own version number. Everything from the CI/CD to the code review tool to your local builds use the same build configuration with Brazil. All software packages in Brazil are built from source on Amazon's gigantic fleet of build servers. Builds are cached, so even though Amazon builds its own version of Make, Java, etc., these are all built and cached by the build servers and downloaded. A simple Java application at Amazon might have hundreds of dependencies (because you'll need to build Java from scratch), but since this is all cached you don't have to wait very long. Lastly, you have Pipelines which is their internal CI/CD tool which integrates naturally with Brazil + the build fleet. It can deploy to their internal fleet with Apollo, or to AWS Lambda, S3 buckets, etc. In all, everything is just very well integrated. I haven't seen anything come close to what you get internally at Amazon. reply nosefrog 7 hours agorootparentHow did you avoid version hell? At Google, almost everything just shipped from master (except for some things that had more subtle bugs, those did their work on a dev branch and merged into master after testing). reply shepherdjerred 5 hours agorootparentVersion sets take care of everything. A version set can be thought of as a Git repo with just one file. The file is just key/value pairs with the dependencies and major/minor version mappings, e.g.- Java 8-123 Lombok 1.12-456 ... A version set revision is essentially a git commit of that version set file. It's what determines exactly what software version you use when building/developing/deploying/etc. Your pipeline (which is a specific noun at Amazon, not the general term) acts on a single version set. When you clone a repo, you have to choose which version set you want, when you deploy you have to choose a version set, etc. Unlike most other dependency management systems, there's no notion of a \"version of a package\" without choosing what version set you're working on, which can choose the minor versions of _all of the packages you're using_. e.g. imagine you clone a Node project with all of its dependencies. Each dependency will have a package.json file declaring what versions it needs. You have some _additional_ metadata that goes a step further that chooses the exact minor version that a major version is mapped to. All that to say that the package can declare what major version they depend on, but not what minor version. The version set that you're using determines what minor version is used. The package determines the major version. Version sets can only have one minor version per major version of a package which prevents consistency issues. e.g. I can have Java 8-123 and Java 11-123 in my version set, but I cannot have Java 8-123 and Java 8-456 in my version set. Your pipeline will automatically build in new minor versions into your version set from upstream. If the build fails then someone needs to do something. Every commit produces a new minor version of a package, that is to say that you can say your package is major version X, but the minor version is left up to Brazil. This scheme actually works pretty well. There are internal tools (Gordian Knot) which performs analysis on your dependencies to make sure that your dependencies are correct. It's a lot to know. It took me a year or so to fully understand and appreciate. Most engineers at Amazon treat it like they do Git -- learn the things you need to and ignore the rest. For the most part, this stuff is all hands off, you just need one person on the team keeping everything correct. reply xyzzy_plugh 7 hours agorootparentprevYou don't, you embrace version hell. reply mdaniel 8 hours agorootparentprevso what I'm hearing is that app-1.0 needs app-1.0-runtime-build-20240410 which was, itself, built from a base of runtime-y-2.0 and layering library-x-1.11 upon it, kind of like # in some \"app-runtimes\" project, they assemble your app's runtime cat > DockerfileDockerfile.gitlab-ci.yml , feature flags , error capture , project-managed provisioning , on call management , on call runbooksyou can orchestrate all that from ~~Slack~~ Chime :-D if you're into that kind of thing https://docs.gitlab.com/ee/ci/chatops/ reply xyzzy_plugh 8 hours agorootparentNo, not even close. You might even have it exactly backwards. reply mdaniel 8 hours agorootparentwhich is why, as I originally asked GP: what have you already tried and what features were they missing I presume by \"exactly backwards\" you mean that one should have absolutely zero knobs to influence anything because the Almighty Jeff Build System does all the things, which GitLab also supports but is less amusing to look at on an Internet forum because it's \"you can't modify anything, it just works, trust me\" Or, you know, if you have something constructive to add to this discussion feel free to use more words than \"lol, no\" reply xyzzy_plugh 7 hours agorootparentI don't work at Amazon, and haven't for a long time, and this format is insufficient to fully express what they're doing, so I won't try. You're better off searching for how Brazil and Apollo work. That being said, the short of it is that: imagine when you push a new revision to source control, you (you) can run jobs testing every potential consumer of that new revision. As in, you push libx-1.1.2 and anyone consuming libx >= 1.1 (or any variety of filters) is identified. If the tests succeed, you can update their dependencies on your package and even deploy them, safely and gradually, to production without involving the downstream teams at all. If they don't, you can choose your own adventure: pin them, fork, fix them, patch the package, revise the versioning, whatever you want. It's designed to be extremely safe and put power in the hands of those updating dependencies to do so safely within reason. Imagine you work on a library and you can test your PR against every consumers. It's not unlike what Google and other monorepos accomplish but it's quite different also. You can have many live versions simultaneously. You don't have to slog it out and patch all the dependents -- maybe you should, but you have plenty of options. It all feels very simple. I'm glossing over a lot. reply shepherdjerred 5 hours agorootparentprevSorry, I wish I could phrase it better for you. All I can say is that I have tried a _lot_ of tools, and nothing has come close. Amazon has done a lot of work to make efficient tools. Here's a better explanation: https://gist.github.com/terabyte/15a2d3d407285b8b5a0a7964dd6... reply xyzzy_plugh 8 hours agorootparentprevNixOS/nixpkgs is about the closest thing you'll find in the wild. You have to squint a bit, I'll admit. reply tripdout 5 hours agorootparentprevIs it true that teams don't do branches in source control at all? Just publishing a CR? reply jeffbee 12 hours agorootparentprevIt's not just that. Livegrep isn't just a pale imitation of something inside Google. It's totally unrelated in implementation, capabilities, and use case. reply init 14 hours agorootparentprevI've used both Code Search and Livegrep. No, Livegrep does not even come close to what Code Search can do. Sourcegraph is the closest thing I know of. reply isker 14 hours agorootparentAgreed. There are some public building blocks available (e.g. Kythe or meta's Glean) but having something generic that produces the kind of experience you can get on cs.chromium.org seems impossible. You need such bespoke build integration across an entire organization to get there. Basic text search, as opposed to navigation, is all you'll get from anything out of the box. reply init 14 hours agorootparentIn a past job I built a code search clone on top of Kythe, Zoekt and LSP (for languages that didn't have bazel integration). I got help from another colleague to make the UI based on Monaco. We create a demo that many people loved but we didn't productionize it for a few reasons (it was an unfunded hackathon project and the company was considering another solution when they already had Livegrep) Producing the Kythe graph from the bazel artifacts was the most expensive part. Working with Kythe is also not easy as there is no documentation on how to run it at scale. reply isker 13 hours agorootparentVery cool. I tried to do things with Kythe at $JOB in the past, but gave up because the build (really, the many many independent builds) precluded any really useful integration. I did end up making a nice UI for vanilla Zoekt, as I mentioned elsewhere: https://github.com/isker/neogrok. reply birktj 12 hours agorootparentprevI see most replies here ar mentioning the the build integration is what is mainly missing in the public tools. I wonder if nix and nixpkgs could be used here? Nix is a language agnostic build-system and with nixpkgs it has a build instructions for a massive amount of packages. Artifacts for all packages are also available via hydra. Nix should also have enough context so that for any project it can get the source code of all dependencies and (optionally) all build-time dependencies. reply jeffbee 12 hours agorootparentBuild integration is not the main thing that is missing between Livegrep and Code Search. The main thing that is missing is the semantic index. Kythe knows the difference between this::fn(int) and this::fn(double) and that::fn(double) and so on. So you can find all the callers of the nullary constructor of some class, without false positives of the callers of the copy constructor or the move constructor. Livegrep simply doesn't have that ability at all. Livegrep is what it says it is on the box: grep. reply humanrebar 11 hours agorootparentThe build system coherence provided by a monorepo with a single build system is what makes you understand this::fn(double) as a single thing. Otherwise, you will get N different mostly compatible but subtly different flavors of entities depending on the build flavor, combinations of versioned dependencies, and other things. reply jeffbee 11 hours agorootparentSure. Also, if you eat a bunch of glass, you will get a stomach ache. I have no idea why anyone uses a polyrepo. reply humanrebar 11 hours agorootparentThe problem with monorepos is that they're so great that everyone has a few. reply refulgentis 10 hours agorootparentGod that is good. reply tayo42 14 hours agorootparentprevIs there like a summary of what's missing from public attempts and what makes it so much better? reply sdesol 13 hours agorootparentThe short answer is context. The reason why Google's internal code search is so good, is it is tied into their build system. This means, when you search, you know exactly what files to consider. Without context, you are making an educated guess, with regards to what files to consider. reply riku_iki 13 hours agorootparentHow exactly integration with build system helps Google? Maybe you could give specific example?.. reply isker 12 hours agorootparentTry clicking around https://source.chromium.org/chromium/chromium/src, which is built with Kythe (I believe, or perhaps it's using something internal to Google that Kythe is the open source version of). By hooking into C++ compilation, Kythe is giving you things like _macro-aware_ navigation. Instead of trying to process raw source text off to the side, it's using the same data the compiler used to compile the code in the first place. So things like cross-references are \"perfect\", with no false positives in the results: Kythe knows the difference between two symbols in two different source files with the same name, whereas a search engine naively indexing source text, or even something with limited semantic knowledge like tree sitter, cannot perfectly make the distinction. reply sdesol 12 hours agorootparentprevIf you want to build a product with a build system, you need to tell it what source to include. With this information, you know what files to consider and if you are dealing with a statically typed language like C or C++, you have build artifacts that can tell you where the implementation was defined. All of this, takes the guess work out of answering questions like \"What foo() implentation was used\". If all you know are repo branches, the best you can do is return matches from different repo branches with the hopes that one of them is right. Edit: I should also add that with a build system, you know what version of a file to use. reply j2kun 13 hours agorootparentprevGoogle builds all the code in its momnorepo continuously, and the built artifacts are available for the search. Open source tools are never going to incur the cost of actually building all the code it indexes. reply DannyBee 8 hours agorootparentprevThe short summary is: It's a suite of stuff that someone actually thought about making work together well, instead of a random assortment of pieces that, with tons of work, might be able to be cobbled together into a working system. All the answers about the technical details or better/worseness mostly miss the point entirely - the public stuff doesn't work as well because it's 1000 providers who produce 1000 pieces that trade integration flexibility for product coherence. On purpose mind you, because it's hard to survive in business (or attract open source users if that's your thing) otherwise. If you are trying to do something like make \"code review\" and \"code search\" work together well, it's a lot easier to build a coherent, easy to use system that feels good to a user if you are trying to make two things total work together, and the product management directly talks to each other. Most open source doesn't have product management to begin with, and the corporate stuff often does but that's just one provider. They also have a matrix of, generously, 10-20 tools with meaningful marketshare they might need to try to work with. So if you are a code search provider are trying to make a code search tool integrate well with any of the top 20 code review tools, well, good luck. Sometimes people come along and do a good enough job abstracting a problem that you can make this work (LSP is a good example), but it's pretty rare. Now try it with \"discover, search, edit, build, test, release, deploy, debug\", etc. Once you are talking about 10x10x10x10x10x10x10x10 combinations of possible tools, with nobody who gets to decide which combinations are the well lit path, ... Also, when you work somewhere like Google or Amazon, it's not just that someone made those specific things work really well together, but often, they have both data and insight into where you get stuck overall in the dev process and why (so they can fix it). At a place like Google, I can actually tell you all the paths that people take when trying to achieve a journey. So that means I know all the loops (counts, times, etc) through development tools that start with something like \"user opens their editor\". Whether that's \"open editor, make change, build, test, review, submit\" or \"open editor, make change, go to lunch\", or \"open editor, go look at docs, go back to editor, go back to docs, etc\". So i have real answers to something like \"how often do people start in their IDE, discover they can't figure out how to do X, leave the IDE to go find the answer, not find it, give up, and go to lunch\". I can tell you what the top X where that happens is, and how much time is or is not wasted through this path, etc. Just as an example. I can then use all of this to improve the tooling so users can get more done. You will not find this in most public tooling, and to the degree telemetry exists that you could generate for your own use, nobody thinks about how all that telemetry works together. Now, mind you, all the above is meant as an explanation - i'm trying to explain why the public attempts don't end up as \"good\". But myself, good/bad is all about what you value. Most tradeoffs here were deliberate. But they are tradeoffs. Some people value the flexibility more than coherence. or whatever. I'm not gonna judge them, but I can explain why you can't have it all :) reply jeffbee 12 hours agorootparentprevJust want to note that Livegrep, its antecedent \"codesearch\", and other things that are basically grep bear no resemblance to that which a person working at Google calls \"Code Search\". reply fmobus 3 hours agoparentprevThe guide bindings layer thing is nice, but its UI could be improved. I wish I could directly find for providers/usages from the search box. reply jillesvangurp 2 hours agoprevIt's why IDE and developer tool builders have long had the insight that in order to do code search properly, you need to open up the compiler platform as a lot of what you need to do boils down to reconstructing the exact same internal representations that a compiler would use. And of course good code search is the basis for refactoring support, auto completion, and other common IDE features. Easier said then done of course as tools are often an afterthought for compiler builders. Even Jetbrains made this mistake with Kotlin initially, which is something they are partially rectifying with Kotlin 2.0 now to make it easier to support things like incremental compilation. The Rust community had this insight as well with a big effort a few years ago to make Rust more IDE friendly. IBM actually nailed this with Eclipse back in the day and that hasn't really been matched since then. Intellij never even got close to this being 2-3 orders of magnitudes slower. We're talking seconds vs. milliseconds here. Eclipse had a blazing fast incremental compiler for Java that could even partially compile code in the presence of syntax errors. The IDEs representation of that code was hooked into that compiler. With Eclipse, you could introduce a typo and break part of your code and watch the IDE mark all the files that now had issues across your code base getting red squiggles instantly. Fix the typo and the squiggles went away, also without any delay. That's only possible if you have a mapping between those files and your syntax tree, which is exactly what Eclipse was doing because it was hooked into the incremental compiler. Intellij was never able to do this, it will actively lie to you about things being fine/not fine until you rebuild your code and it will show phantom errors a lot when it's internal state gets out of sync with what's on disk. It often requires full rebuilds to fix this. If you run something, there's a several second lag while it compiles things. Reason: the IDE internal state is calculated separately from the compiler and this gets out of sync easily. When you run something, it has to compile your code because it hasn't been compiled yet. That's often when you find out the IDE was lying to you about things being ready to run. With Eclipse all this was instantly and unambiguous because it shared the internal state with the compiler. If it compiled, your IDE would be error free, if it didn't it wouldn't be. And it compiled incrementally and really quickly so you would know instantly. It had many flaws and annoying bugs but that's a feature I miss. reply dikei 34 minutes agoparentWhile Eclipse truely have an incredible incremental compiler for Java, IntelliJ's better integration with external build systems like maven and gradle, together with better cross-languages support, was what win me over. reply high_na_euv 2 hours agoparentprev>Easier said then done of course as tools are often an afterthought for compiler builders. Except for Microsoft's Roslyn (.NET compiler) https://willspeak.me/2021/11/24/red-green-syntax-trees-an-ov... https://ericlippert.com/2012/06/08/red-green-trees/ Ive used Roslyn SDKs to build tools and it is really good reply callmeal 1 hour agoparentprev>With Eclipse all this was instantly and unambiguous ... Still is, and is the main reason why a lot of us will never jump ship. reply bawolff 14 hours agoprevSurprised that hound https://github.com/hound-search/hound isn't mentioned. I thought it was the leader of open source solutions in this space. I've been using Wikimedia's instance ( https://codesearch.wmcloud.org/search/ ) and have generally been pretty happy with what it provides. reply isker 13 hours agoparentHound has made an interesting choice to not bound searches. https://codesearch.wmcloud.org/search/?q=test&files=&exclude... produces an ajax request that (for me) took 13s to produce a 55MB JSON response, and then takes many more seconds to render into the DOM. Properly bounding search response sizes was one of the things I had to ensure Zoekt could do in its JSON APIs that I use in neogrok: https://github.com/sourcegraph/zoekt/pull/615 reply bawolff 12 hours agorootparentYeah, i agree, that is weird. Especially if you search for something super common like \"function\" you basically DoS it. reply bch 3 hours agoprevWhy am I not seeing anything here about ctags[0] or cscope[1]? Are they that out of fashion? cscope language comprehension appears limited to C/C++ and Java, but “ctags” (I think I use “uctags” atm) language support is quite broad and ubiquitous… [0] https://en.wikipedia.org/wiki/Ctags [1] https://en.wikipedia.org/wiki/Cscope reply signa11 2 hours agoparentexactly THISthe only problem with `cscope` is that for modern c++ based code-bases it is woefully inadequate. for plain / vanilla c based code-bases f.e. linux-kernel etc. it is just _excellent_ language-servers using clangd/ccls/... are definitely useful, but quite resource heavy. for example, each of these tools seem to starting new threads per file (!) and there are no knobs to not do that. i don't really understand this rationale at all. yes, i have seen this exact behavior with both clangd and ccls. oftentimes, the memory in these processes balloon to some godawful numbers (more with clangd than ccls), necessitating a kill. moreover, this might be an unpopular opinion, but mixing any regex based tool (ripgrep/... come to mind) with language-server f.e. because the language server does not really find what you are looking for, or does not do that fast enough, are major points against it. if you already have language-server running, regex based tools should not be required at all. i don't really understand the reason for sql'ization of code searches at all. it is not a 'natural' interface. typical usage is to see 'who calls this function', 'where is the definition at' of this function etc. etc. reply Macha 13 hours agoprev> This is a pretty bad index: it has words that should be stop words, like function, and won’t split a.toString() into two tokens because . is not a default word boundary. So github used to (maybe still does) \"fix\" this one and it's annoying. Although github are ramping up their IDE like find-usages, it's still not perfect, so somethings you just want to a text search equivalent for \"foo.bar()\" for all the uses it misses and this stemming behaviour then finds every while where foo and bar are mentioned which bloats results. reply ricardobeat 13 hours agoprevI don't understand their hand-waving of Zoekt. It was built exactly for this purpose, and is not a \"new infrastructure commitment\" any more than the other options. The server is a single binary, the indexer is also a single binary, can't get any simpler than that. To me it doesn't make sense to be more scared of it than Elasticsearch... reply chasil 14 hours agoprevOracle has USER/ALL/DBA_SOURCE views, and all of the PL/SQL (SQL/PSM) code that has been loaded into the database is presented there. These are all cleartext visible unless they have been purposefully obfuscated. It has columns for the owner, object name, LINE[NUMBER] and TEXT[VARCHAR2(4000)] columns and you can use LIKE or regexp_like() on any of the retained source code. I wonder if EnterpriseDB implements these inside of Postgres, and/or if they are otherwise available as an extension. Since most of SQL/PSM came from Oracle anyway, these would be an obvious desired feature. https://en.wikipedia.org/wiki/SQL/PSM reply ethanwillis 5 hours agoprevThere are tools from bioinformatics that would be more applicable here for code search than the ones linguistics has made for searching natural language. reply boyter 11 hours agoprevCode search is indeed hard. Stop words, stemming and such do rule out most off the shelf indexing solutions but you can usually turn them off. You can even get around the splitting issues of things like a.toString() With some pre-processing of the content. However were you really get into a world of pain is allowing someone to search for ring in the example. You can use partial term search, prefix, infix, or suffix but this massively bloats the index and is slow to run. The next thing you try is trigrams, and suddenly you have to deal with false positive matches. So you add a positional portion to your index, and all of a sudden the underlying index is larger than the content you are indexing. Its good fun though. For those curious about it I would also suggest reading posts by Michael Stapelberg https://michael.stapelberg.ch/posts/ who writes about Debian Code Search (which I believe he started) in addition to the other posts mentioned here. Shameless plug, I also write about this https://boyter.org/posts/how-i-built-my-own-index-for-search... where I go into some of the issues when building a custom index for searchcode.com Oddly enough I think you can go a long way brute forcing the search if you don't do anything obviously wrong. For situations where you are only allowed to search a small portion of the content, say just your own (which looks applicable in this situation) that's what I would do. Adding an index is really only useful when you start searching at scale or you are getting semantic search out of it. For keywords which is what the article appears to be talking about, that's what I would be inclined to do. reply fizx 9 hours agoprevThere's a million paths, but here's one I like. Use ElasticSearch. It will scale more than Postgres. Three hosted options are AWS, Elastic, Bonsai. I founded Bonsai and retired (so am partial), but they will provide the best human support for you, and you won't have to worry about java Xmx. Your goal with ES is to use the Regex PatternAnalyzer to split the code into reasonable exact code-shaped tokens (not english words). Here's a rough GPT4 explanation with sample config that I'd head towards: https://chat.openai.com/share/e4d08586-b7ef-48f2-9de1-7f82ea... reply bytefish 8 hours agoparentGitLab is also using ElasticSearch, so one could recreate the ElasticSearch Indices they came up with. [1] They also share some of the challenges, they faced along the way. It also discusses interesting challenges, like implementing the authorization model. [2], [3] When GitHub removed its most useful Search feature, which is sorting results by date, I wrote a small “Search Engine” with ElasticSearch to selectively index Microsoft repositories. It works good enough for my needs. [4] [1] https://gitlab.com/gitlab-org/gitlab/-/blob/7bbbc00bd871aeb6... [2] https://about.gitlab.com/blog/2019/07/16/elasticsearch-updat... [3] https://about.gitlab.com/blog/2020/04/28/elasticsearch-updat... [4] https://github.com/bytefish/ElasticsearchCodeSearch reply campbel 12 hours agoprev> Sourcegraph’s maintained fork of Zoekt is pretty cool, but is pretty fearfully niche and would be a big, new infrastructure commitment. I don't think Zoekt is as scary as this article makes it out to be. I set this up at my current company after getting experience with it at Shopify and its really great. reply louiskw 2 hours agoprevhttps://github.com/BloopAI/bloop Is fully open source and has full text + regex search built on tantivy fyi reply sdesol 13 hours agoprev> It’s hard to find any accounts of code-search using FTS I'm actually going to be doing this soon. I've thought about code search for close to a decade, but I walked away from it, because there really isn't a business for it. However, now with AI, I'm more interested in using it to help find relevant context and I have no reason to believe FTS won't work. In the past I used Lucene, but I'm planning on going all in with Postgres. The magic to fast code search (search in general), is keeping things small. As long as your search solution is context aware, you can easily leverage Postgres sharding to reduce index sizes. I'm a strong believer in \"disk space is cheap, time isn't\", which means I'm not afraid to create as many indexes as required, to shave 100's of milliseconds of searches. reply bevekspldnw 12 hours agoparentMmm, it’s not that straight forward: indexes can vastly slow down large scale ingest, so it’s really about when to index as well. I work with a lot of multi billion row datasets and a lot of my recent focus has been on developing strategies to avoid the slow down with ingest, and then enjoying the speed up for indexed on search. I’ve also gotten some mjnd boggling speed increases by summarizing key searchable data in smaller tables, some with JSONB columns that are abstractions of other data, indexing those, and using pg prewarm to serve those tables purely from memory. I literally went from queries taking actual days toGitHub’s search is excellent Is it? I find it near-useless most of the time, and cloning + ripgrep to be way more efficient. Perhaps the problem is more in the UX being awful than the actual search. reply healeycodes 15 hours agoprevWhen a val is deployed on val town, my understanding is that it's parsed/compiled. At that point, can you save the parts of the program that people might search for? Names of imports, functions, variables, comments, etc. reply MH15 11 hours agoparentA val is just Typescript, no? So unless they are also storing the AST it would be JavaScript and that's it reply johnthescott 8 hours agoprevthe rum index has worked well for us on roughly 1TB of pdfs. written by postgrespro, same folks who wrote core text search and json indexing. not sure why rum not in core. we have no problems. https://github.com/postgrespro/rum reply nbenitezl 11 hours agoprevAlso https://github.com/Debian/dcs reply herrington_d 12 hours agoprevIs it possible to combine n-gram and AST to dump a better indexing? Take `sourceCode.toString()` as an example, the AST can dump it to `sourceCode` and `toString`. A further indexer can break `sourceCode` to `source` and `code`. For ast dumping, project like https://github.com/ast-grep/ast-grep can help. reply boyter 11 hours agoparentYou could, but I don't know what you gain out of it. The underlying index would be almost the same size, and n-gram would also allow you to search for e.t for example which you are losing in this process. reply semiquaver 15 hours agoprevBe careful with trigram indexes. At least in the postgres 10 era they caused severe index bloat for frequently updated tables. reply peter_l_downs 14 hours agoparentInteresting, do you know anywhere I can easily read more about this? (I will do my own research, too.) reply boyter 11 hours agorootparentIts a result of trigrams themselves. For example turning searchcode (please ignore plug, this is just the example I had to hand) goes from 1 thing you would need to index into 8. \"searchcode\" -> [sea, ear, arc, rch, chc, hco, cod, ode] As a result the index rapidly becomes larger than you would expect. reply pomdtr 10 hours agoprevHey! I'm a val.town fanboy and I immediately thought about a workaround while reading the blog post: What if I dumped every publics vals in Github, in order to be able to user their (awesome) search ? So here is my own \"Val Town Search\": https://val-town-search.pomdtr.me And here is the repo containing all vals, updated hourly thanks to a github action: https://github.com/pomdtr/val-town-mirror reply simonw 10 hours agoparentWell this is fun... git clone https://github.com/pomdtr/val-town-mirror cd val-town-mirror rg news.ycombinator.com Now I can ripgrep search public Vals, e.g. to see who's hitting Hacker News from a Val. reply pomdtr 10 hours agorootparentYeah, and you can finally run/debug vals locally (kind of, the version query param is not yet supported) reply nbbaier 7 hours agoparentprevThis is great! reply MatthiasPortzel 10 hours agoparentprevThat is, uh, one solution, to say the least. There’s a HN comment I’ll never forget where the commenter suggests that Discord move their search infrastructure to a series of text file searched with ripgrep, but Val.town’s scale is small enough that they could actually consider it. reply jessemhan 13 hours agoprevGood scalable codebase search is tough. We built a scalable, fast, and super simple solution for codebase semantic search: https://phorm.ai reply nox101 15 hours agoprevany nuggets here? https://github.blog/2023-02-06-the-technology-behind-githubs... reply SomaticPirate 14 hours agoparentWould also recommend the related video https://www.youtube.com/watch?v=CqZA_KmygKw reply boyter 11 hours agoparentprevYes, although the lack of detail about the sparse grams is frustrating. reply skybrian 15 hours agoprevIt seems like some of their gists have documentation attached and maybe that’s enough? I’m not sure I’m all that interested in seeing undocumented gists in search results. reply philippemnoel 12 hours agoprevParadeDB founder here. We'd love to be supported on Render, if the Render folks are open to it... reply 727564797069706 12 hours agoprevIf you're serious about scaling up, definitely consider Vespa (https://vespa.ai). At serious scale, Vespa will likely knock all the other options out of the park. reply IshKebab 14 hours agoprev [–] I would use Hound. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article addresses the difficulties of establishing efficient code search functionality in Val Town, emphasizing the shortcomings of the existing system and the necessity for improved solutions.",
      "It focuses on seeking a search tool tailored for code, not regular language, taking into account scalability, infrastructure, and complexity.",
      "Exploring options like Postgres Full Text Search, pg_trgrm for trigram search, and potential substitutes such as Meilisearch, ParadeDB, and Elasticsearch to strike a balance between performance, scalability, and maintenance for Val Town\\'s small team."
    ],
    "commentSummary": [
      "The discussion delves into the challenges and advantages of code search, underscoring the significance of fundamental code searching skills for developers.",
      "Various tools like ripgrep, git grep, Livegrep, NixOS/nixpkgs, Kythe, and Zoekt are compared, showcasing the efficacy of internal tools in tech giants like Google and Amazon.",
      "Recommendations include utilizing tools such as ElasticSearch, Postgres, Lucene, and AST to enhance search performance and streamline code search workflows, with suggestions on scaling up search capabilities like Vespa and Hound."
    ],
    "points": 240,
    "commentCount": 123,
    "retryCount": 0,
    "time": 1712772908
  },
  {
    "id": 39996433,
    "title": "EPA Enforces Removal of 'Forever Chemicals' from Tap Water",
    "originLink": "https://www.nytimes.com/2024/04/10/climate/epa-pfas-drinking-water.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT E.P.A. Says ‘Forever Chemicals’ Must Be Removed From Tap Water The rule applies to a family of chemicals known as PFAS that are linked to serious health effects. Water utilities argue the cost is too great. Share full article 147 The new measure will require utilities to reduce PFAS substances in drinking water to near-zero levels. Credit... Justin Sullivan/Getty Images By Lisa Friedman April 10, 2024 For the first time, the federal government is requiring municipal water systems to remove six synthetic chemicals linked to cancer and other health problems that are present in the tap water of hundreds of millions of Americans. The extraordinary move from the Environmental Protection Agency mandates that water providers reduce perfluoroalkyl and polyfluoroalkyl substances, known collectively as PFAS, to near-zero levels. The compounds, found in everything from dental floss to firefighting foams to children’s toys, are called “forever chemicals” because they never fully degrade and can accumulate in the body and the environment. The chemicals are so ubiquitous that they can be found in the blood of almost every person in the United States. A 2023 government study of private wells and public water systems detected PFAS chemicals in nearly half the tap water in the country. Exposure to PFAS has been associated with metabolic disorders, decreased fertility in women, developmental delays in children and increased risk of some prostate, kidney and testicular cancers, according to the E.P.A. Michael S. Regan, the E.P.A. administrator, called the new regulation “life changing.” “This action will prevent thousands of deaths and reduce tens of thousands of serious illnesses,” Mr. Regan said on a call with reporters on Tuesday. He described the rule as the most significant action the federal government has ever taken to reduce PFAS exposure in drinking water. “We are one huge step closer to finally shutting off the tap on forever chemicals once and for all,” he said. The E.P.A. estimated it would cost water utilities about $1.5 billion annually to comply with the rule, though utilities maintain that the costs could be twice that amount and are worried about how to fund it. States and local governments have successfully sued some manufacturers of PFAS for contaminating drinking water supplies, but the settlements awarded to municipalities have been dwarfed by the costs of cleaning up the chemicals, municipal officials said. Industry executives say taxpayers will ultimately foot the bill in the form of increased water rates. The 2021 bipartisan infrastructure law provides $9 billion to help communities address PFAS contamination and the E.P.A. said $1 billion of that money would be set aside to help states with initial testing and treatment. Mr. Regan announced the regulation on Wednesday in Fayetteville, N.C., near the site where, in 2017, a Chemours chemical plant discharged water contaminated with PFAS into the Cape Fear River, making the local drinking water unsafe. Image Equipment to reduce PFAS and greenhouse gas emissions at a Chemours site in Fayetteville, N.C. Credit... Ed Kashi for The New York Times Mr. Regan, who previously served as North Carolina’s top environmental regulator, oversaw the Cape Fear PFAS investigation at the time and forced Chemours to clean up the air, soil and water in the lower Cape Fear River basin communities. In 2022, the E.P.A. found the chemicals could cause harm at levels “much lower than previously understood” and that almost no level of exposure was safe. Under the new rule from the E.P.A., water utilities must monitor supplies for PFAS chemicals and would be required to notify the public and reduce contamination if levels exceeded the new standard of 4 parts per trillion for perfluoroalkyl and polyfluoroalkyl substances. Previously, the agency had advised that drinking water contain no more than 70 parts per trillion of the chemicals. Public water systems have three years to complete their monitoring. If those samples show that levels of PFAS exceed the new E.P.A. standards, the utilities would have another two years to purchase and install equipment designed to filter out PFAS. In a 2020 peer-reviewed study, scientists at the Environmental Working Group, a nonprofit organization, estimated that more than 200 million Americans had PFAS in their drinking water. Public health advocates and scientists said the new regulation was overdue. “A growing body of scientific research shows that PFAS chemicals are more harmful to human health than previously thought, and at extremely low levels,” said Anna Reade, director of PFAS advocacy at the Natural Resources Defense Council, an environmental group. In just the past year, more than a dozen peer-reviewed studies have found evidence of additional health effects of PFAS exposure, including a delay in the onset of puberty in girls, leading to a higher incidence of breast cancer, renal disease, and thyroid disease; a decrease in bone density in teenagers, potentially leading to osteoporosis; and an increased risk of Type 2 diabetes in women. Dr. Susan M. Pinney, the director of the Center for Environmental Genetics at the University of Cincinnati, led a longitudinal study of young girls who had been exposed to PFAS after an industrial plant in West Virginia released the chemicals into the Ohio River. She called the number of people exposed to PFAS around the country “mind boggling.” Robert A. Bilott, an attorney who has spent more than two decades litigating the hazardous dumping of PFAS chemicals, said he had alerted the E.P.A. to the dangers posed by the chemicals in drinking water as early as 2001. “It has taken far too long to get to this point, but the scientific facts and truth about the health threat posed by these man-made poisons have finally prevailed,” Mr. Bilott said. The E.P.A. calculated the health benefits of the new regulation at about $1.5 billion annually from reductions in cancer, heart attacks and strokes and birth complications. But Republicans and industry groups, along with many mayors and county executives, said the Biden administration had created an impossible standard that would cost municipal water agencies billions of dollars. Several questioned E.P.A.’s accounting as well as the science used to develop the new standard. The American Water Works Association, the Association of Metropolitan Water Agencies and other groups representing water utilities estimated that the cost of monitoring and remediation of PFAS could be as much as $3.2 billion annually. The figure is based on an analysis conducted for the American Water Works Association by Black & Veatch, a firm of consulting engineers. Image Doctoral students and researchers studied PFAS substances at North Carolina State University in Raleigh in 2021. Credit... Ed Kashi for The New York Times Communities with limited resources will be hardest hit by the new rule, they said. “When regulations are set near zero, that is not something manufacturers or water systems can economically achieve,” Brandon Farris, the vice president of energy policy at the National Association of Manufacturers, wrote in a letter to the E.P.A. “Regulations that are not economically achievable will lead to critical substances being manufactured outside of the U.S. where environmental protections are often less stringent.” Christina Muryn, the mayor of Findlay, Ohio, a town of about 50,000 people, said that, while clean drinking water is an imperative, the E.P.A. was requiring municipalities to meet new mandates without adequate support. “That is very frustrating to me as a citizen, as a mayor, and as someone who is responsible for our water treatment system,” Ms. Muryn said. Public health advocates said the costs of the new rule were outweighed by the growing body of evidence of the dangers posed by PFAS. Widely used since the 1940s, the chemicals are useful in repelling water and oil. Nonstick pans have been most famously associated with PFAS but the chemicals can be found in water-repellent clothes and carpets, certain shampoos, cosmetics and hundreds of other household items. Lisa Friedman is a Times reporter who writes about how governments are addressing climate change and the effects of those policies on communities. More about Lisa Friedman A version of this article appears in print on , Section A, Page 1 of the New York edition with the headline: E.P.A. Targets Six Chemicals In Tap Water. Order ReprintsToday’s PaperSubscribe 147 Share full article 147 The Proliferation of ‘Forever Chemicals’ PFAS, or per- and polyfluoroalkyl substances, are hazardous compounds that pose a global threat to human health. For the first time, the U.S. government is requiring municipal water systems to detect and remove PFAS from drinking water. A global study found harmful levels of PFAS in water samples taken far from any obvious source of contamination. Virtually indestructible, PFAS are used in fast-food packaging and countless household items. PFAS lurk in much of what we eat, drink and use, but scientists are only beginning to understand how they affect our health. Though no one can avoid forever chemicals entirely, Wirecutter offers tips on how to limit your exposure. Scientists have spent years searching for ways to destroy forever chemicals. In 2022, a team of chemists found a cheap, effective method to break them down. ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=39996433",
    "commentBody": "[dupe] EPA Says 'Forever Chemicals' Must Be Removed from Tap Water (nytimes.com)217 points by igonvalue 11 hours agohidepastfavorite157 comments ChrisArchitect 9 hours ago[dupe] More discussion earlier: https://news.ycombinator.com/item?id=39988718 reply jeremy151 10 hours agoprevI live in a city that was a hot zone for this type of contamination in the drinking water due to industrial waste from leather processing buried in the 60’s (shoe scraps treated with scotchgard.) We now have GAC filtering at the municipal supply level that is quite effective and not that expensive. The large beds of carbon last quite a while if I recall correctly. Despite regular testing, everyone I know RO filters their water regardless. For me, it’s because I have no idea what new previously “unknown” contamination will be next discovered, and would rather get out as much as is reasonable. When the information began to surface I found it interesting the letters on public record going back to the 60’s with people warning that allowing this kind of dumping was a bad idea. Of course being the primary employer to the entire city, the economics won at the time. Since, the cost of cleanup and lawsuits to that company have been massive. reply datascienced 10 hours agoparentRO is cheap enough for middle class or above (order of magnitude is ~ what you might spend on uniforms and excursions for a kid in a state primary school). Assuming a self install. So it is a good option if you can afford it. Maybe RO becoming part of the standard set of things you buy (washing machine, vacuum cleaner etc.) is the way. You have to keep up to date with filter changes though. RO typically needs a post filter. Hopefully that doesn’t add any bad chemicals. But you need it as pure water is desperate to bind, so you can either bind it to something you choose, or bind it to whatever pipe work / tanks are beyond the filter. Also you might want a higher ph. Maybe some disruption to make a nice looking, compact and cheap and zero install RO unit would be good and some subsidy for people without the means to buy one who live in risk areas. Plus subsidy for maintenance. If the design is like a printer where you pull out and push in new cartridges and have warning lights it will make maintenance easy. reply jjtheblunt 9 hours agorootparentRO has some very easy to maintain, and user install options, which I've done. ( https://www.geappliances.com/ge/water-filtration-systems.htm ) However RO is not water efficient, in the sense that only a fraction of water run over the RO membrane system is filtered, and otherwise inbound water goes on into the drain. You can hear this happening, and it's documented by GE, for example, as how the systems work. That makes me wonder if there are other systems with the characteristic that a higher % of ingested water ends up filtered as well as RO can. reply greenavocado 9 hours agorootparentA permeate pump can typically reduce water waste in reverse osmosis systems by up to 80%. In general, permeate pumps can achieve a waste water reduction of around 50% to 80%. This means that for every gallon of purified water produced, only around 20% to 50% is wasted as reject water. This is achieved by utilizing the energy from the brine flow to enhance the pressure applied to the feed water, leading to increased permeate production and reduced reject water volume. Typically, these pumps range from $50 to $200 and they do not use electricity. The elevated pressure allows for more effective filtration and higher water recovery rates. By boosting the pressure, permeate pumps facilitate a greater volume of water passing through the semi-permeable membrane, resulting in increased production of purified water (permeate) and reduced reject water (brine). The heightened pressure helps overcome osmotic pressure and allows for a more thorough extraction of purified water from the feed stream. reply wbl 9 hours agorootparentThe domestic RO systems put pressure on the clean water output and don't have recovery systems for brine pressure? What? My only experience with RO systems are on sailboats, where a brine pressure recovery system is the only way to get the power down, and the water trickles into the tank under low pressure from where it is pumped out. reply jjtheblunt 8 hours agorootparentThe linked system above just sends the unfiltered tap water down the drain. I have had two iterations of the GE system and it says so in the manual, for instance. I am not sure about other brands and their systems. reply greenavocado 8 hours agorootparentMost in-home systems sold today drain to waste without any attempt at recovery to keep manufacturing costs low. reply TaylorAlexander 9 hours agorootparentprevYeah my dad has an RO system at their house but it goes to a special tap next to the main one that is used only for drinking water, due to the waste associated. Maybe it isn’t needed for hand washing, showers, etc as long as there are good standards at the water distribution facility. reply datascienced 7 hours agorootparentThats how I use it. In theory the waste could be used for irrigation or mixed into shower water but that requires more plumbing to deal with an external cost (in areas where water is limited). reply mattmaroon 3 hours agorootparentWell, if you live somewhere with a municipal water supply, the water just gets recycled anyway. I suppose if you’re on septic it’s still going right back into the ground it came from. Drinking water is probably such a small percent of overall water use that wasting even a multiple of it doesn’t amount to much anyway. So filter away! I’ll worry about my r/o waste when people stop diverting rivers to grow almonds in the desert and not a second before. reply mattmaroon 10 hours agorootparentprevCheck out Waterdrop. The cartridges do just pop out and in, and it’s not zero install but it is very easy. If you can install a faucet you can install that. I got over 99% reduction according to a cheap TDS meter at my condo in Phoenix with the 2 filter one. I can replace cartridges in seconds. I love that thing. Zero install would probably suck as you’d have to fill tanks frequently (it rejects a good amount of water) and it would take up counter space but they do make em. Honestly in most places you can buy the stuff for 25 cents a gallon from a machine, which is what I would do if I did not feel like installing reply jxramos 10 hours agorootparentI've been a fan of the APEC countertop units; https://www.freedrinkingwater.com/products/reverse-osmosis-c... Been using them for a few years, water tastes great. reply datascienced 7 hours agorootparentThat is a nice looking unit. I think as usual USA has more options! Might import something like this! If it did cold water too would be awesome. reply mattmaroon 3 hours agorootparentWe are the best at having things to buy. reply datascienced 7 hours agorootparentprevZero install to me means “a renter can use it”. It could hijack your faucet outlet with a valve but allow your faucet to work anyway. This would require usually no tools or at worse a screwdriver to tighten a clip. reply mattmaroon 3 hours agorootparentWell, the only real change that’s not easily reversible one might make when installing most of these units is if you don’t already have a hole to mount the faucet in. A renter definitely shouldn’t drill a hole in a countertop and most r/o units would require one. Any house built in the last few decades would probably have a built in dish soap dispenser you could pop out, but if not, no luck. Other than that just basic hand tools are involved. I would have no problem installing one in a rental but I’m also comfortable with plumbing. It’s definitely a job that seems a lot more intimidating than it is. reply TylerE 9 hours agoparentprev> For me, it’s because I have no idea what new previously “unknown” contamination will be next discovered, and would rather get out as much as is reasonable. This really resonates with where my thinking has gone. While I always try to be guided by science, my default these days is much closer to \"assume it isn't safe\" than \"assume it is\". I've got multiple chronic medical conditions that me both more susceptible to getting to sick, and more likely to have complications/have a slow recovery if I do. So for instance, I keep (medical grade) gloves at home and wear them when using any sort of cleaning chemicals. My skin is fragile anyway, and almost any sort of solvent (that isn't water) is at least somewhat bad for you, either short or long term. reply sp332 11 hours agoprevLooks like most of the people saying it's too expensive are talking about filtering the stuff out of water at or near the point of use. What's the cost to reduce the amount of these chemicals getting into the water in the first place? reply hunter2_ 10 hours agoparentFor nonstick/waterproof/hydrophobic coatings to not slowly shed from whatever they're applied to and end up in the water, they'd need to not be applied in the first place. Not having them is cheaper than having them, but we'll be wet (which occasionally leads to hypothermia) and we'll need to revert back to pans seasoned the old fashioned way (less convenient, careful washing). reply swatcoder 10 hours agorootparentThe many among us who already avoid those products, or who haven't gotten practical access to them, may be surprised by the tradeoffs you describe. Just because a technology has become pervasive in some lives doesn't mean that it does a lot that matters. Like any other tradition, its value is often incidental and its popularity is mostly an accident of history (marketing, curiosity, fashion, ephemeral supply/industry shocks, etc). Nonstick pans and synthetic fabrics are very much in that group. reply dylan604 10 hours agorootparentThis is a very myopic view. The people to do like these things have a lot more money to spend to ensure these things are around for a while. While my view might be cynical, it is closer to reality than getting people to give up creature comforts voluntarily. reply mgerdts 9 hours agorootparentThe stainless steel frying pan I use was purchased about 30 years ago as part of a set that included a couple saucepans and lids. I paid less than $50 for it new. That was a lot when I was a student, but not an out of reach luxury. If it helped me avoid eating 20 meals out it easily paid for itself. reply phkahler 10 hours agorootparentprev>> The people to do like these things have a lot more money to spend to ensure these things are around for a while. No. If these things were never invented, those people would no be asking for them. Least not very much. reply creato 9 hours agorootparentYou think people wouldn't be asking for non-stick pans? reply dylan604 10 hours agorootparentprev>If these things were never invented what an even more myopic view. if only unicorns were real and pots of gold were at the ends of rainbows. if only wishing made it so. You can't put the genie back in the bottle. reply wahnfrieden 10 hours agorootparentprevIt's just a learning issue, how to use a steel pan or similar safer alternatives. There are cultural memes about what to use and why and what's easier and harder. reply beejiu 10 hours agorootparentprev> revert back to pans seasoned the old fashioned way Stainless steel is the more convenient option. reply denkmoon 10 hours agorootparentI love my stainless steel pans but there is a reason non stick pans are by far the most popular. It requires planning (ie. food cannot come straight from the refridgerator) and technique (get pan hot _before_ adding oil/fat) to not leave half your meal stuck to the pan. Most people who don't get joy out of cooking don't want to deal with this. People are cooking at home for themselves less and less, and this has its own healthcare cost. Anything that reduces the number of people cooking at home is almost certainly a net loss for public health. reply yumraj 10 hours agorootparentprevAnd cast iron, which can get enough non-stick for many uses. reply blackeyeblitzar 10 hours agorootparentThis is true, but maintaining the seasoning is annoying. If you want to scrub your pan out and get it really clean, or leave it soaking, you will likely need to repeat the seasoning process. It gets frustrating to have to coat it in oil and fire up the oven and all of that. You also cannot cook certain foods in cast iron pans due to this. For example, if you want to make a tomato-based sauce, you will risk leeching metal into your food due to the acidity. Personally I think each type of material has its place in the kitchen (nonstick, stainless steel, cast iron, enameled cast iron, etc) reply SAI_Peregrinus 9 hours agorootparent> If you want to scrub your pan out and get it really clean, You can scrub just fine with a chain scrubber or metal scraper. You shouldn't use Scotch-Brite pads, sandpaper, or other highly abrasive methods, but scrubbing is not an issue. You shouldn't soak. You can use most dish soaps. Some detergents are still caustic, but anything you can get on your hands without issue is fine. Don't use Ajax or similar highly alkaline and/or abrasive compounds. No lye. Re-seasoning after cleaning shouldn't need the oven. Get it hot on medium-low heat, wipe it with a thin layer of a reasonably unsaturated oil (refined olive oil, rapeseed AKA Canola oil, etc.) on a rag. Let it cool. If you really screwed it up repeat the process a once or twice. If you've totally stripped the pan and are putting on a totally new seasoning, 6-10 times is enough. All that said, I agree about acidic foods. Much like highly alkaline cleaners, they can degrade the seasoning. I use stainless steel for those. I likewise agree that each type of material has its place. I don't use nonstick because I have pet birds, and even tiny amounts of overheating can cause enough fumes to kill them. The rest are all useful to me. reply wahnfrieden 10 hours agorootparentprevAs long as one doesn't mistake carcinogenic leftovers from previous cooking for seasoning. It's somehow common to think that seasoning means some deep umami flavor from food bits that get baked into it over time, when it only means the initial seasoning of the metal as a chemical process and still means you should wash your dishes like anything else after it's seasoned. and regular seasoning isn't stripped by washing with dish soap reply refulgentis 10 hours agorootparentprevIsn't that the seasoned alternative? reply singleshot_ 10 hours agorootparentprevIt takes a tough man to make a tender chicken, as they used to say. reply dham 10 hours agorootparentprevBut somehow butter and lard are bad, that's how this whole thing started reply waste_monk 9 hours agorootparentprev>we'll need to revert back to pans seasoned the old fashioned way (less convenient, careful washing). There are plenty of non-nonstick (stick?) options besides cast iron. Most of my cookware is stainless steel (with a thick disk of copper under the pan for thermal inertia) and I've never felt it's an inconvenience or difficult to clean - in fact, its durable nature allows for the use of coarse scrubbers or acid-based cleaning products that would quickly ruin a non-stick pan. reply adrr 9 hours agorootparentprevSo we can get cancer from all the aldehydes and PAHs caused by overheating oil till in forms a polymer from oxidation. Even worse if you season the pan in your oven or stove top since now your breathing all that carcinogenic smoke in. reply jim-jim-jim 10 hours agorootparentprev> we'll be wet Waxed cotton works well enough for me. Have also heard good things about boiled wool, but don't own anything like this. reply blackeyeblitzar 10 hours agorootparentprevNonstick pans require less fat to cook food. Is the health benefit of avoiding shedding greater than the benefit of being healthier in terms of fat percentage or weight? I am not so sure. Also as I recall PTFE coatings (Teflon is an example brand name) are no longer made with PFOA in the US or Europe. Yes, PTFE itself is a PFAS as well, but as far as I know it does not delaminate or shed as long as the pan is used at lower temperatures (less than 450F). reply electrograv 10 hours agorootparentLow fat diets are not healthier. The notion that fat is evil has been thoroughly debunked for quite some time now. reply blackeyeblitzar 9 hours agorootparentI think this is a misinterpretation of what has been debunked. Low fat diets are definitely healthier for more people because they almost always correlate with lower calorie intake. It’s not the quality of it being a fat that makes it bad (this is what recent studies revisiting fats are saying). It’s other qualities of fats that are problematic. For example, consider equal calorie intake between a protein source (like a steak) and a fat. Which do you think is going to fill you up more? Which do you think is healthier? reply notJim 9 hours agorootparentprevFat is calorically dense. If you're trying to lose weight, you need to be careful about eating calorically dense foods. And if you want to eat more protein to maintain muscle while losing weight, there is a zero-sum trade-off between fat and protein. reply dham 9 hours agorootparentInteresting archaic theory, as me and my wife just lost 40lbs each by eating a ton of fat and protein, haha. If you want to lose weight, stay away from sugar, for example carbs reply blackeyeblitzar 9 hours agorootparentHigh fat high protein (with low carbs) can definitely be successful, as long as you’re mindful of your approach and manage net calorie intake. But for lots of people it is easier to just eat what they do, but make minor tweaks to reduce the amount of fat as a way of reducing calories while still being satisfied by what they’re eating. Different ways for different people. reply notJim 8 hours agorootparentprevThe ad hominem isn't necessary, counting calories is working great for me. Glad you found something that works for you. reply nostrebored 9 hours agorootparentprevYes, but fat also increases satiety. If you’re trying to lose weight, you should be eating meals, not snacks and adding fat. You will eat less both during a meal and after. reply notJim 8 hours agorootparentNot sure what comment you're replying to, I didn't say anything about snacking or not eating fat. reply hunter2_ 4 hours agorootparentWith all due respect, please check your carbon monoxide detectors. reply nostrebored 7 hours agorootparentprev> Fat is calorically dense. If you're trying to lose weight, you need to be careful about eating calorically dense foods. ? reply swatcoder 9 hours agorootparentprevSteaming, grilling, baking, air frying (convection baking), roasting, boiling, stewing, frying in seasoned cast iron, etc all \"require less fat to cook food\" If avoiding extra fat is one's highest priority, it's not like you're out of luck without nonstick pans. reply blackeyeblitzar 9 hours agorootparentYou’re proposing eating different foods with different recipes and flavors when you suggest that steaming, grilling, etc are alternatives. If I want to prepare similar food to what I can make in a stainless steel or cast iron pan, but with less fat, a nonstick pan is the best tool. Also since you listed frying in a seasoned cast iron pan in your list of alternatives - that requires use of additional fat. Yes, even if it is fully properly seasoned. reply swatcoder 9 hours agorootparentEither the fat retained in the dish was negligble in the first place, in which case nonstick doesn't matter, or a nutritionally or aesthetically appreciable amount was retained, in which case you've already changed the recipe and dish. reply blackeyeblitzar 9 hours agorootparentIt’s a minor change to me. Something fried in a nonstick pan may not have the same sear as a steel or iron pan, but it’s a lot closer than steaming, which is a totally different thing. reply blindriver 10 hours agorootparentprevYou do realize that the campaign of low fat diets has completely been extinguished in the last 10 years? https://www.hsph.harvard.edu/news/hsph-in-the-news/low-fat-d... reply blackeyeblitzar 9 hours agorootparentI think you’re misinterpreting the reevaluation of fats in recent years. I’m not demonizing fats and saying they need to be avoided entirely. But I am saying people (especially in America) need to moderate their intake of calories in general and fats specifically as well (as they’re a vehicle for calories). There are also different varieties of fats with different health effects. Using nonstick cookware is an easy way to reduce the intake of fats (and therefore calories) even if you are not banning them from your diet entirely. Let’s take a simple example: have you tried to make a fried egg in a cast iron pan and compared it to a nonstick pan? In a cast iron, you’ll need to use a pat of butter to get the egg to slide easily (around 100 calories). In nonstick you can get away with zero butter. It adds up. reply happyopossum 9 hours agorootparentA) A pat of butter is around 35 calories, and B) if you're putting a ton of butter in there it isn't exactly getting absorbed into the egg, most of it is left in the pan. reply blackeyeblitzar 9 hours agorootparentI did several searches, and the sources I saw said a pat is 1 tablespoon and is 100 calories. Example: https://www.nutritionix.com/food/butter/pat But your latter point is fair. I haven’t measured it. I just know that my experience of cooking in my cast iron requires a lot more fat than my other pans. reply WrongAssumption 9 hours agorootparentA tablespoon of butter is significantly more than what is considered a pat of butter. reply nostrebored 9 hours agorootparentprevPeople in America should be upping their fats and completely eliminating any low fat packaged foods from their diets. The results are unambiguous. reply Spivak 10 hours agorootparentprevI mean I guess but are people out there really optimizing their diets at the level of a drizzle/spray of oil or butter? The health difference can't possibly be worth it and barely moves the needle it's so little. I know it's my French heritage talking but life without butter isn't. reply notJim 9 hours agorootparentAbsolutely yes people are. I'm active in weight loss communities to support my own weight loss, and yes we are careful about our fat consumption. The health difference is large for someone trying to lose weight. A tablespoon of butter isn't that filling, but contains about 150 calories. That's equivalent to a whole pot of non-fat yogurt or two eggs, both of which are more filling and give you more protein. When making eggs, I try to use about a teaspoon of butter, which still gives some butter flavor, but lets me save more calories for eggs. reply blackeyeblitzar 9 hours agorootparentprev> are people out there really optimizing their diets at the level of a drizzle/spray of oil or butter Lots of people do this, and it’s not because they’re somehow ignorant and against all fats. It’s more that people trying to be healthy make small tweaks they can live with that add up. It’s not about being an extremist but just moderating things where you are willing to. Everyone’s metabolism, dietary preferences, and lifestyle is different. If you live an active lifestyle with enough exercise or just have a higher metabolism, then it might make no difference to optimize at that level. But for lots of people it can make a big difference without making them feel like they can’t eat what they want. Three meals a day prepared without added fat means savings of around 3-500 calories a day depending on how much fat you’re using and your portion sizes. Keep in mind as well that not all calories are equal and calories from protein sources tend to be more filling (compare eating 3 tablespoons of butter versus a chicken breast). reply nostrebored 9 hours agorootparentIt just doesn’t, which is one of many reasons why low fat public health policy has failed to reduce obesity. When people don’t eat fat, they eat more. If you have the self control around food to eat a low fat diet and reliably stick to your macros, you’re probably not at a place where you’re overweight to begin with. Giving your stomach something complex to break down while actually giving your body what it needs to add to vitamin stores results in less food consumed. You can’t treat diet like a Lego set of what to eat while ignoring physiology. reply samatman 9 hours agorootparentprevOnce the fluorocarbons are made into PTFE, they no longer pose a threat. Teflon is so biologically inert that it gets used in medical devices implanted into humans. It isn't soluble in water or much of anything else. The problem is with PFOA and related compounds, which are used to make PTFE and friends. reply blackeyeblitzar 9 hours agorootparentPFOA specifically is no longer in wide use. Most prominent manufacturers stopped using it as part of the PTFE manufacturing process 10+ years ago. And yes, I agree the science thus far shows that PTFE is basically inert. And to the extent a PTFE coating can separate from a pan, it requires high heat beyond the advertised temperature limits of nonstick pans anyways. reply blueprint 10 hours agorootparentprevhonestly it's way easier for me to wash my high carbon lodge pan (and i can put that sucker directly into the oven) because i use steel wire ball, maybe a little soap, and hot water briefly and it's done. reseasoning up to the level needed is trivial with each use but one doesnt really worry about a proper season unless one cleans the pan with some insane pressure and soap. and pans dont really need to be seasoned to be used. the season helps avoid rust a little but then again you can simply store it with a finish of oil on if youre concerned about that. to be frank cleaning a teflon etc pan is way more finicky because we need, hey, wait, more microplastics in the form of slowly degrading acrylic sponge pads or brushes etc.. but i guess you could use alternatives. but anyway, there are also many other ways to keep dry lol it's not like teflon is the only solution reply Spivak 9 hours agorootparentAnother alternative is enameled cast iron if you don't want to think about seasoning. My Le Creuset is one of my most treasured possessions. reply dudus 11 hours agoparentprevThey are already in the water, and they are not going away, hence the name. reply NewJazz 11 hours agoparentprevWhat's the cost of even identifying all the ways these chemicals are getting out, or what all these chemicals are? I heard recently that but and seed butters often have elevated amounts of this crap... One hypothesis floated was that machinery used in the processing of this food is being coated in the stuff. reply nonrandomstring 10 hours agorootparentWe know where a lot of it came from. Emphasis because it's an historical problem: Military and civil air fire-fighting foam. About a million tons of perfluoroalkyl was put into the environment between 1970 and 2000. It's very mobile, so it quickly got into groundwater and rivers. By comparison the leach from Teflon pans is probably a small percentage. reply NewJazz 10 hours agorootparentOh I definitely agree that Teflon pans are low on the list of pollution sources. But firefighting foam doesn't explain wtf happened (is happening?) to Cape Fear. reply choilive 10 hours agorootparentprevTo the machinery example - the dies used to make pasta are teflon coated because that means you can push the dough through the dies faster. Likewise dental floss often has PFAS coatings to make it easier to slide between your teeth. This stuff is literally everywhere and cannot be avoided. reply hunter2_ 10 hours agorootparentprevFood packaging is full of this stuff. Think about how a greasy sandwich or stick of butter is wrapped in a magic piece of paper that somehow doesn't get completely saturated with grease/sauce... that's PFAS. reply mjrpes 10 hours agorootparentThey make PFAS free wax paper. reply ClumsyPilot 10 hours agoparentprev> people saying it's too expensive Always found that argument very strange - if it's too expensive to be healthy and alive, what the fuck is the money for? reply revscat 10 hours agorootparentProfit for those who can afford to protect themselves at the expense of others. At that point money is largely used for political power. reply redox99 9 hours agorootparentprevThere's a finite amount of resources (including labor). Money is a proxy for that. reply candiddevmike 11 hours agoprevWithout any curtailing of their usage, aren't we just shifting costs to taxpayers? reply Lammy 10 hours agoparent> taxpayers Relevant: “How to Make Tap Water” https://www.youtube.com/watch?v=PvIky3B661s reply thomasahle 10 hours agoparentprevHow much are people currently spending on bottled water, that they could save if the tap water was safer? reply nox101 10 hours agorootparentIs tap water unsafe? reply dylan604 10 hours agorootparentHi. Welcome to the conversation. The title of the article being discussed is \"EPA Says 'Forever Chemicals' Must Be Removed from Tap Water (nytimes.com)\" reply peterbecich 10 hours agoprevThe California DWR has this map: locations of potential sources of per- and polyfluoroalkyl substances (PFAS) https://gispublic.waterboards.ca.gov/portal/apps/webappviewe... reply jjtheblunt 9 hours agoparentDoes California specify which water mains are old asbestos cement pipes? That famously caused ecological disasters in the Bay Area 20+ years ago, but they didn't (in newspaper articles i saw) show the extent of such. reply peterbecich 9 hours agorootparentI'm not aware of that. I doubt asbestos cement water mains are a significant health risk to the end user. I think any asbestos that could become airborne is a risk. To the workers who installed those pipes, certainly. reply jjtheblunt 7 hours agorootparentAsbestos is implicated in gastrointestinal cancers too. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4856305/ reply DantesKite 10 hours agoprevA few years ago, the PPM in my town's water was over 850 PPM, well above the recommended guidelines. Worse than that, it had a distinct sulfur-like smell. So naturally we got a reverse osmosis water filter system and while the tap water has improved since then, I'm always reminded of the occasional accidents that can occur (e.g., lead, excessive chlorination, plain old entropy). Plus it makes the water taste significantly better. Even if tap water was always perfectly safe to drink, I'd get one just for the taste alone. Brondell Circle RO systems are my favorite because the filters are the easiest to change and when you have the same system for years, that ends up being the labor you repeat the most. reply michael9423 2 hours agoparentIs Brondell just selling a white-labeled Coway product with their Circle RO system? reply laluser 10 hours agoprevCan anyone recommend a good reverse osmosis filtration that is actually certified and made by a company you can actually trust? I have one now, but not sure that it filters some of the PFOS/PFAS chemicals. There are so many clearly scammy companies claiming different types of certifications. reply blackeyeblitzar 10 hours agoparent> not sure that it filters some of the PFOS/PFAS chemicals Just to clarify the terminology: PFOA, PFOS, PTFE, and thousands of other chemicals are all examples that are part of a larger group of (mostly) problematic chemicals called PFAS. Here is the EPA’s guidance on filtration standards and how to identify filters that are effective for PFAS: https://www.epa.gov/system/files/documents/2024-04/water-fil... That PDF has links to products certified by various certification bodies, and suggests looking for products certified against NSF 53, NSF 58, or both. but the EPA also notes the following: > It's important to note that the current certification standards for PFAS filters (as of April 2024) do not yet indicate that a filter will remove PFAS down to the levels EPA has now set for a drinking water standard. EPA is working with standard-setting bodies to update their filter certifications to match EPA’s new requirements. In the meantime, remember that reducing levels of PFAS in your water is an effective way to limit your exposure. EWG has a list of recommended filters: https://www.ewg.org/research/getting-forever-chemicals-out-d... reply paulgerhardt 9 hours agoparentprevSome engineer friends and I like Home Master given how modular it is - albeit a bit oldschool: https://www.theperfectwater.com/reverse-osmosis https://www.youtube.com/watch?v=9RqoPpZbszY In particular the permeate pump makes it more usable and less prone to clogging and the 'insta-hot' option is great for tea/coffee. I'm not a fan of their branding and vague NSF certifications but likewise see fewer red flags with them than their competitors. Eg it's made in the USA, they've been around for a while, they have a few patents to their name, and their based out of Phoenix where the water tastes like garbage so they probably dog food their product. My only wish is to upgrade the remineralization process to 'set and forget' a profile as described in this post on how to DIY name brand mineral waters: https://khymos.org/2012/01/04/mineral-waters-a-la-carte/ If you go deep enough down the coffee-snob rabbit hole you eventually get into fun conversations about remineralization: https://www.baristahustle.com/blog/what-can-we-use-to-remine... - and cheekily https://thirdwavewater.com/ For something less daunting and ready to go I've heard good things about the Waterdrop K6: https://www.youtube.com/watch?v=AkTMsgOsgu0 - though there other models appear to be good too and the publish their reports on product pages such as this one: https://www.waterdropfilter.com/products/undersink-reverse-o... reply kccqzy 10 hours agoparentprevI use Coway, a Korean brand for my water filtration needs. They are a good brand and actually certified but looking closely at the product manual, they do not make the claim that they filter PFAS (defined as having a fully fluoridated methyl or methylene group): indeed they only claim to filter several chlorinated substances including tetrachloroethene. reply danfromjapan 10 hours agoparentprevI'm also curious about this - particularly about filter membrane materials and nanoplastics reply wahnfrieden 10 hours agorootparentMake sure you have good air filtration too. reply ohthanks 9 hours agoparentprevI have built and sold RO systems for 20+ years. It's a weird industry and there are some competing desires in place. In general it's kind of a mess and it's difficult for consumers to navigate. You can build your own system for less than $150 from cheap parts on ebay. You can buy a branded unit at a big box or amazon for $150-250. Or a \"health\" branded version for $300-800. Or have an installer put whatever they sell in for $500-1500. My experience is that you will get nearly identical water quality from each of those systems. There are different options and some fine details but the fundamentals haven't changed in decades and you are paying for some collection of service, parts quality, future replacement costs, marketing and snake oil. NSF certification is good, it will rule out products that are flat out harmful. I have seen lots of cheap filters with fake certifications and there are many great filters that it don't carry certification. NSF material and safety cert (51) is a good one to look for, beyond that is has more to do with how the product will be sold and marketed than a real measure of performance. $250-500 is probably the right price range for a diy install unit. Check for replacement part costs, buy something with standard components and cartridge sizes. Learn how it works, change the filters on time and expect to replace components every now and then. reply michael9423 2 hours agorootparentThe RO industry is such a snake oil mess that instead I went for a high quality activated charcoal filter that can easily be changed after half a year for 30$, and combine it with water distillation. reply semicolon_storm 11 hours agoprevNo paywall: https://web.archive.org/web/20240410092221/https://www.nytim... reply VelesDude 9 hours agoprevWhen it comes large scale stuff... Cheesecake for everyone. Easy to say, difficult to do. The best move is prevention rather than correction. reply _heimdall 10 hours agoprevRemoving it from tap water seems like a great first step for mitigation. But what about preventing them in water in the first place? We shouldn't be using these chemicals. Period. Sure we're hooked on them now, but sunken cost is a terrible reason to continue down a bad path. We use these chemicals largely because industry kept finding more and more ways to use manufacturing byproducts. In theory that's great and all, but we avoid questioning if we should change that manufacturing process instead. A little pain now can avoid a mountain of pain later if we're actually willing to think ahead and question what we have today rather than focusing on what the next step forward is without ant context of how or why we got here. reply thallium205 11 hours agoprevTap water is safe and effective. reply lelandfe 10 hours agoparenthttps://en.m.wikipedia.org/wiki/3M_Contamination_of_Minnesot... > The drinking water of 14 communities, over 170,000 individuals, and over 150 square miles was contaminated through 3M's improper disposal of chemicals reply sp332 11 hours agoparentprevMunicipal tap water isn't worse for you than bottled water. reply airstrike 10 hours agorootparentIt depends on the bottled water, to be fair. I suppose it also depends on where the tap is located. reply hackyhacky 11 hours agorootparentprevSo you're saying that bottled water also contains PFAS? reply sp332 10 hours agorootparentA lot of bottled water is literally packaged tap water. Anyway https://www.sciencedirect.com/science/article/abs/pii/S00431... and https://www.consumerreports.org/water-quality/whats-really-i... reply Analemma_ 10 hours agorootparentprevIf you look at your bottled water, a whole lot of it has labels on it that say something like \"comes from a municipal source\". It's tap water sold at a 5000% markup. reply skipkey 10 hours agorootparentMany years ago I knew someone who managed a bottled water plant. They didn’t use tap water, they had a very deep well pulling from an aquifer. They then filtered the heck out of it, and then added back in a very small, proprietary mix of minerals. reply hackyhacky 10 hours agorootparentprevJust because it's from a municipal source doesn't mean that it isn't filtered subsequently. reply serf 10 hours agorootparentprev>It's tap water sold at a 5000% markup. it's tap water, processed, and bottled. The processes often include extra filtration and cleaning methods that the individual corporate groups dictate. I'm not trying to suggest that bottled water is some tremendous value compared to municipal water, but let's not pretend that the markup comes with nothing. reply waveBidder 10 hours agorootparentprevI mean, yes, the bottle itself is a source of microplastics. reply sp332 10 hours agorootparentMicroplastics is a different conversation. reply xcv123 10 hours agorootparentprevTry distilling 4 litres (1 gallon) of tap water and see what is left behind. Now repeat that experiment with high quality bottled water. In my city (developed first world country) the tap water contains toxic repugnant chemical sludge. reply adamredwoods 10 hours agoparentprevWhich city? Tap water quality varies between cities. https://en.wikipedia.org/wiki/Flint_water_crisis reply ch4s3 10 hours agorootparentYou'll notice that Flint's water is safe to drink as of 2 years ago[1] [1] https://en.wikipedia.org/wiki/Flint_water_crisis#2022 reply hackyhacky 10 hours agorootparentI notice that the page you link does not mention PFAS at all. reply ch4s3 8 hours agorootparentThat's not really what people are referring to when they mention Flint. reply hackyhacky 5 hours agorootparentIf \"safe and effective\" does not include testing for chemicals known to be dangerous, then the label \"safe and effective\" is not accurate. reply iAMkenough 9 hours agorootparentprevWhich to me means \"Tap water is safe and effective\" is a broad statement that doesn't consider pollutants outside of PFAS, which vary depending on the regional water treatment system and how they are operated from region to region. \"safe\" and \"effective\" means different things between different regulating bodies. reply dylan604 10 hours agoparentprevI'm sure it is depending on the intended use. It's perfectly safe and effective to wash my car, but that doesn't necessarily mean it's safe for me to consume. reply anigbrowl 9 hours agoprevMeddling bureaucrats, it's my right to get cancer or some weird life-changing medical condition and the woke mob wants to take it away from me. reply reducesuffering 11 hours agoprev\"Tap water is perfectly fine\" people threw a lot of shade at people who only drink bottled water, but both should be using Reverse Osmosis filtration right now. Think of it as verifying the response you got from the API is correct... reply whyenot 11 hours agoparentIt really does depend on what your water source is and the pipes from there to your tap. For example, the Hetch Hetchy water that supplies San Francisco and some other parts of the Bay Area is very pure. Reverse osmosis is not going to remove much of anything. Davis water, or San Jose water, well, that's a little different. The other thing about RO water systems is that they are not very efficient. For every gallon of pure(-ish) water that a home system generates, it typically has to throw away 5 gallons of salty water. reply cheald 10 hours agorootparentRO systems vary by efficiency, but 1:5 seems way out of whack. My tankless system claims 2:1 output:waste, and while I haven't measured it directly, I'd say that's probably pretty close to right. reply whyenot 10 hours agorootparent\"For example, a typical point-of-use RO system will generate five gallons or more of reject water for every gallon of permeate produced.\" https://www.epa.gov/watersense/point-use-reverse-osmosis-sys... reply jon_richards 10 hours agorootparentI looked into this a while ago and didn't understand why the efficiency claims were so different, but here's a video of a RO system running https://www.youtube.com/watch?v=2r_T7Bgi3hQ reply reducesuffering 10 hours agorootparentprevThis is outdated as most of the mfgs have gotten better at it. Mine is 1:1 drinking:waste and my monthly water consumption is barely affected as it's indiscernible in the noisy variation. First 4 links on Amazon I'm seeing 2:1, 2:1, 3:1, 3:1 reply reducesuffering 10 hours agorootparentprev\"San Francisco and some other parts of the Bay Area is very pure\" https://www.ewg.org/tapwater/system.php?pws=CA3810011 Contains Carbon Tetrachloride, Hexavalent Chromium (PG&E v. Erin Brockovich anyone?), Haloacetic acids, and Trihalomethanes, all carcinogens. All are reduced by reverse osmosis use. You say \"not efficient\" when it's more like an extra 1-2 gallons waste, and yet the outcome is much cleaner water that makes up a large portion of your biology. I'd say that's a pretty efficient way to be healthier, especially when drinking water is a very tiny sliver of water consumption. The average person uses 3,000 gallons of water a month and you're sweating an extra 5 gallons for drinking? reply whyenot 10 hours agorootparentWater quality: see https://www.sfpuc.org/sites/default/files/documents/SF_Regio... Efficiency: see https://www.epa.gov/watersense/point-use-reverse-osmosis-sys... For example, a typical point-of-use RO system will generate five gallons or more of reject water for every gallon of permeate produced. Some inefficient units will generate up to 10 gallons of reject water for every gallon of permeate produced. In recent years, membrane technology has improved and some point-of-use RO systems have been designed to operate more efficiently, with some manufacturers advertising a 1:1 ratio of permeate to concentrate production, meaning only one gallon of reject water is generated for each gallon of treated water. reply zdragnar 10 hours agorootparentprevThat's an extra 5 gallons only if it is done at the point of consumption. To do it for all water- showers, washing dishes, laundry, etc- would require RO for all potable water for the city water supply. Depending on where in California you are, that much extra water consumption (since the waste will have an even higher concentration of harmful chemicals) isn't exactly an option. reply reducesuffering 10 hours agorootparentThis isn't common or as much of a concern at all. I'm talking about drinking water for consumption. It's a $200 expense, an extra 20 gallons of water a month at most (out of 3000). reply swatcoder 10 hours agoparentprevIt's all just kind of f'd anyway. The biological need for \"water\" is tuned to expect \"water\" to involve all sorts of trace minerals and compounds, some desired and some undesired, as well as some kind of stream of gut flora contributors and immune system challenges. Thinking of RO and other forms of heavily or industrially purified H20 as the same thing is a technologist's mistaken idealization, and to a cynical eye reads a lot more like ancient Hellenic ideas about simple essential fluids than anything either scientific or sustainable. It may be a necessary compromise because of modern contamination or because modern demand forces us to rely on increasingly worse supplies, but it's a long way from \"verifying the response\". It's more like an ugly last resort hack. reply JohnMakin 10 hours agorootparentGo drink unfiltered water out of a wilderness stream after a melt and tell me how it goes reply scottyah 9 hours agorootparentI've done it a few times, never got sick. I've known people to not be so lucky, but that's pretty rare. Sure does \"taste\" good, but I'm not sure how much of that is circumstance based. reply JohnMakin 7 hours agorootparentSorry I was not meaning to be as snarky as that came off. I’m an avid backpacker and have been in situations where I had to drink unfiltered stream water - while I know it’s possible to do so and not suffer ill effects, there are also a ton of super bad effects, and our bodies are far removed from the environment we evolved in 100k+ years ago. I don’t know the research on this but I’d wager our gut biome is much different than it was a million years ago as well. reply newZWhoDis 10 hours agorootparentprevCompete and utter nonsense. You’d need 100+ gal/day to get the trace mineral content of a multi vitamin. The rest is equally preposterous. reply perryh2 11 hours agoparentprevI've been using the Waterdrop G3 system in my kitchen and it's been great. https://www.amazon.com/Waterdrop-Reverse-Filtration-Reductio... reply wouldbecouldbe 11 hours agoparentprevYeah the plastic bottles are also not great: https://time.com/6553165/microplastics-in-bottled-water-stud... Just sad we made such a mess. reply wahnfrieden 10 hours agorootparentThere'll be no technology solution for this that scales globally for any means and usability reply wouldbecouldbe 3 hours agorootparentHaha I guess we’re in luck. There is a global water distribution system in place for free :). All we have to do is stop allowing industries to dump in our most important natural resource. How stupid can we be? reply _factor 9 hours agorootparentprevHow about glass bottles and reusing them like we used to. reply ch4s3 10 hours agoparentprevThat RO water needs to have at least some minerals added back to it, unless you're pretty confident about your calcium and magnesium intake. reply dham 10 hours agorootparentIf you're taking Vitamin D and Vitamin K like you're supposed to, these are the last things you have to worry about. reply reducesuffering 10 hours agorootparentprevThe papers I've read generally indicate the average drinking water consumption is roughly only about 4%-10% RDA calcium, magnesium, and potassium. Most people should already be supplementing magnesium anyway since it's deficient in half of developed countries populations. reply j45 9 hours agoprevI wonder if this kind of regulation exists in another country near or far. reply non-chalad 10 hours agoprevnext [4 more] [flagged] TylerE 10 hours agoparentYou gonna defend asbestos yet? PFAS is, like, actually carcinogenic and stuff. reply non-chalad 10 hours agorootparentDid I defend PFAS? Nope. Why should I defend asbestos, when there are more deadly chemicals in our water!? reply bobmcnamara 10 hours agoparentprevThis is no place for hydrologic snarkery. reply BenFranklin100 10 hours agoprevGiven PFAS have been in tap water for decades and the science behind the recent and still far from settled, I suspect there is an excellent chance this will go the way of the acid rain scare of the 1980s. Bring on the downvotes. reply yongjik 10 hours agoparentAcid rain \"scare\" went away because people were rightfully scared, they told the government to fix it, and the government told the industry to fix it (or else!), and the industry did fix it. It would be GREAT ACHIEVEMENT if PFAS went the same way. https://www.statista.com/statistics/501303/volume-of-sulfur-... reply manfre 10 hours agoparentprevThis is more likely to be similar to leaded gasoline. In a few decades we'll start to see the impact they caused after we successfully filter it out. reply scottyah 9 hours agorootparentDDT is another example of bad things going on for decades that had severe repercussions. There's definitely something going on with wheat from America too, though I don't think it's been discovered/publicized. reply Freedom2 9 hours agorootparentprevWhat was the impact of leaded gasoline? reply aatharuv 7 hours agorootparentLowered IQ, impulse control, and increased levels of aggression. reply airstrike 10 hours agoparentprevThat's not a logical conclusion reply clarity20 10 hours agoparentprevThe downvotes came! And it's not like you said something bad ... reply rgrieselhuber 9 hours agoprev [–] Toxic waste such as fluoride should be removed too. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The EPA issued a new regulation mandating municipal water systems to reduce PFAS chemicals in tap water to nearly undetectable levels due to their association with severe health risks.",
      "PFAS chemicals, known as \"forever chemicals,\" are present in numerous everyday items, capable of bioaccumulation, posing risks to humans and the environment.",
      "Although the EPA's rule could prevent numerous fatalities and health issues, critics express concerns over its high implementation costs, especially for water utilities, potentially impacting underprivileged communities disproportionately."
    ],
    "commentSummary": [
      "The EPA is identifying \"forever chemicals\" in tap water, discussing filtration systems like GAC and RO filters for removal.",
      "Emphasis on water efficiency in filtering systems with the impact of cookware on health noted.",
      "Recommendations for reliable water filtration, concerns about PFAS chemicals, and debates on tap versus bottled water are included, along with mineral intake importance and government intervention for water contamination."
    ],
    "points": 217,
    "commentCount": 157,
    "retryCount": 0,
    "time": 1712788672
  },
  {
    "id": 39993626,
    "title": "Google's Griffin Architecture: RecurrentGemma Language Models",
    "originLink": "https://github.com/google-deepmind/recurrentgemma",
    "originBody": "RecurrentGemma RecurrentGemma is a family of open-weights Language Models by Google DeepMind, based on the novel Griffin architecture. This architecture achieves fast inference when generating long sequences by replacing global attention with a mixture of local attention and linear recurrences. This repository contains the model implementation and examples for sampling and fine-tuning. We recommend most users adopt the Flax implementation, which is highly optimized. We also provide an un-optimized PyTorch implementation for reference. Learn more about RecurrentGemma The RecurrentGemma technical report gives specific details on the training and evaluation of RecurrentGemma. The Griffin paper describes the underlying model architecture. Quick start Installation Using Poetry RecurrentGemma uses Poetry for dependency management. To install dependencies for the full project: Checkout the code. poetry install -E full to create a virtual environment with all dependencies. poetry shell to activate the created virtual environment. If you only need to install a subset of dependencies use one of the alternative library-specific commands below. Using pip If you want to use pip instead of Poetry, then create a virtual environment (run python -m venv recurrentgemma-demo and . recurrentgemma-demo/bin/activate) and: Checkout the code. pip install .[full] Installing library-specific packages JAX To install dependencies only for the JAX pathway use: poetry install -E jax or (pip install .[jax]). PyTorch To install dependencies only for the PyTorch pathway use: poetry install -E torch (or pip install .[torch]). Tests To install dependencies required for running unit tests use: poetry install -E test (or pip install .[test]) Downloading the models The model checkpoints are available through Kaggle at http://kaggle.com/models/google/recurrentgemma. Select either the Flax or PyTorch model variations, click the ⤓ button to download the model archive, then extract the contents to a local directory. In both cases, the archive contains both the model weights and the tokenizer. Running the unit tests To run the tests, install the optional [test] dependencies (e.g. using pip install .[test]) from the root of the source tree, then: pytest . Examples To run the example sampling script, pass the paths to the weights directory and tokenizer: python examples/sampling_jax.py \\ --path_checkpoint=/path/to/archive/contents/2b/ \\ --path_tokenizer=/path/to/archive/contents/tokenizer.model Colab notebook tutorials colabs/sampling_tutorial_jax.ipynb contains a Colab notebook with a sampling example using JAX. colabs/sampling_tutorial_pytorch.ipynb contains a Colab notebook with a sampling example using PyTorch. colabs/fine_tuning_tutorial_jax.ipynb contains a Colab with a basic tutorial on how to fine-tune RecurrentGemma for a task, such as English to French translation, using JAX. To run these notebooks you will need to have a Kaggle account and first read and accept the Gemma license terms and conditions from the RecurrentGemma page. After this you can run the notebooks, which will automatically download the weights and tokenizer from there. Currently different notebooks are supported under the following hardware: Hardware T4 P100 V100 A100 TPUv2 TPUv3+ Sampling in Jax ✅ ✅ ✅ ✅ ✅ ✅ Sampling in PyTorch ✅ ✅ ✅ ✅ ✅ ✅ Finetuning in Jax ✅ ✅ ✅ ✅ ❌ ✅ System Requirements RecurrentGemma code can run on CPU, GPU or TPU. The code has been optimized for running on TPU using the Flax implementation, which contains a low level Pallas kernel to perform the linear scan in the recurrent layers. Contributing We are open to bug reports and issues. Please see CONTRIBUTING.md for details on PRs. License Copyright 2024 DeepMind Technologies Limited This code is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Disclaimer This is not an official Google product.",
    "commentLink": "https://news.ycombinator.com/item?id=39993626",
    "commentBody": "Implementation of Google's Griffin Architecture – RNN LLM (github.com/google-deepmind)196 points by milliondreams 16 hours agohidepastfavorite33 comments VHRanger 14 hours agoLike RWKV and Mamba, this is mixing some RNN properties to avoid the issues transformers have. However I'm curious about their scaling claims. They have a plot that shows how the model scales in training with the FLOPs you throw at it. But the issue we should rather be concerned with is the wall time of training for a set amount of hardware. Back in 2018, we could train medium sized RNNs, the issue was with wall time of training and training stability. reply whimsicalism 14 hours agoparenttransformers were also just better at the LM task than 2018 RNNs for equal amount of flop training reply VHRanger 14 hours agorootparentYeah, that's just the training stability part to my knowledge reply whimsicalism 13 hours agorootparentthey're also just less capable models. like just adding attention on top of an RNN made them a lot better reply SpaceManNabs 8 hours agorootparentCalculating self-attention is still quadratic though. So you get the negatives of transformers there too. reply foota 14 hours agoparentprevDo you know the downside with RWKV? Based on how they present it, it seems like the best thing since sliced bread, but I would have assumed that it would have been widely adopted if that were the case. reply kouteiheika 8 hours agorootparentThe downside is that it's bad (like, really bad) on a certain subset of tasks. I once trained RWKVv4 model on a machine translation task and no matter how much I scaled it up it just didn't work at all, while an equivalent transformer did the job without a problem. Intuitively this does make sense, because a transformer can at any time \"look back\" at the source sentence and at what it has previously generated (due to its attention mechanism) for every token it outputs, while an RNN like RWKV has to compress this into its internal state which is both lossy and limited in size. I haven't looked at the new versions of RWKV (apparently we're at v6 now), but hopefully it performs better now. In the end I think that a hybrid architecture probably makes the most sense - have some sort of an attention mechanism for the near context, and an RNN-like state for far context, and that would give you the best of both worlds. reply inciampati 7 hours agorootparentWhat about multiple passes over the data? Make it recurse. reply kouteiheika 7 hours agorootparentI also tried that - try to get it to iteratively \"refine\" its translation. I don't remember all of the details at this point, but in general it didn't help much. (Although maybe I just did it suboptimally and there might have been a better way to do it.) I'm guessing scaling the model up massively would probably make it work in one shot (so that whatever it was translating would fit into its state), but I didn't really have the compute to try that. reply eyegor 5 hours agorootparentLstm? reply shawntan 3 hours agorootparentprevNot sure if this is the type of answer you're looking for, but RWKV is not really recurrent the same way RNNs are recurrent. This quasi-recurrentness allows it and its comrades to use algorithms like parallel SCAN to achieve log N complexity when parallelised. But you pay for that in terms of state-tracking. There's a cool talk here if you care to know the details:https://www.youtube.com/watch?v=4-VXe1yPDjk reply VHRanger 14 hours agorootparentprevIt seems only OK as a model? Looking at the LLM chat leaderboard it's 71st and the 14B version is worse than a lot of 7B models: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboar... Also, llama.cpp makes inference accessible for a lot of people, and it's not available for RWKV. Not to knock on the model, I'm sure it's good. I also like that it's a succesful example of citizen science. It's just not popular enough to have the inference infrastructure transformers have, not established enough to attract enough money to get 60B+ models trained, and so on. reply WanderPanda 13 hours agorootparentThis leaderboard is not the best for comparing model architectures, the dataset and finetuning have too much influence. I think perplexity on a particular dataset would be a better way to compare reply logicchains 4 hours agorootparentprev>Also, llama.cpp makes inference accessible for a lot of people, and it's not available for RWKV. It absolutely is: https://github.com/RWKV/rwkv.cpp . reply whimsicalism 13 hours agorootparentprevi believe it is undertrained, at minimum reply jimmyl02 13 hours agorootparentprevFrom what I know about RWKV, it's mostly a one man effort and doesn't have the same data pipeline / resources as most major labs. It's a bit unfortunate but I'm curious about the performance given the same training corpus as OpenAI's GPTs. Maybe some labs have tried internally but haven't released results? On the other hand it makes sense to invest more money into transformer training runs as they have been proven to work. They really burst onto the scene and brought back RNNs in the world of transformers. The claim that RWKV isn't paralleizable during training also seems to be refuted in their readme. I'd guess it's generalizable performance as there is a difference between doing well on benchmarks and being usable. Personally I've tried running the weights a long time ago when it was first released and the results weren't usable but I'm sure there has been considerable progress since then. reply kouteiheika 8 hours agorootparent> The claim that RWKV isn't paralleizable during training also seems to be refuted in their readme. RNNs are trivially parallizable (I've done it myself), as long as you're training them on multiple documents in parallel and have enough memory for the state for each document. You just train them 1 token at a time across N documents, instead of the transformer-like N tokens at a time across 1 document. reply ianbutler 8 hours agorootparentRWKV is parallel at the level of the sequence like a transformer. Its formulation allows for each timestep t to be calculated in parallel except for a single serial scan at the end for aggregation which they use a custom cuda kernel to do. reply kouteiheika 7 hours agorootparentI know. I trained RWKV myself using both methods, like a transformer and like an RNN. Ultimately it probably doesn't matter that you can train it like a transformer because you can just train it in parallel on multiple documents simultaneously one token at a time, and, at least from my experience, this worked just as well, if not better. Plus, doing it this way is more general because you don't need any custom kernels to do it, and it also helps the model to learn to deal with an \"infinite\" context better (while if you train it like a transformer its performance will regress once you evaluate it outside of the context window on which you've trained it, at least from what I've seen in my training runs). reply nmfisher 6 hours agorootparentprevI played around with RWKV some time ago (maybe early 2023?) with similarly disappointing results, but my suspicion was that this was a dataset/training issue, not an architectural one. Leaderboard performance has improved a lot since then, and anecdotally, I've seen/heard some quite decent RWKV TTS experiments, so I'm bullish. Also, the team has incorporated/raised money from investors (recursal.ai), so it's no longer a one man effort. reply GaggiX 14 hours agoparentprevThe paper shows that the speed is comparable to transformer models, faster with smaller with \"long\" sequence length like 8k. reply janwas 4 hours agoprevFor anyone interested in a C++ implementation, our github.com/google/gemma.cpp now supports this model. reply JyrkiAlakuijala 1 hour agoparentFun fact -- gemma.cpp uses highway, an amazing high performance computation library originally developed in the JPEG XL effort. reply riku_iki 14 hours agoprevI didn't get one detail: they selected 6B transformer as baseline and compared it to 7B Griffin Why wouldn't select equal size models?.. reply szundi 14 hours agoparentThey probably had them for some reason and it was cheaper not to retrain one of them again reply riku_iki 14 hours agorootparentIts just performance comparison is misleading then, they report marginal improvements which is expected just because of models size differences.. reply GaggiX 14 hours agorootparentIt also performs better on any other size. reply riku_iki 14 hours agorootparentThey have baseline transformer of max size 6B in tables. Other models are trained on very different data and probably differently. reply GaggiX 13 hours agorootparentAll the MQA transformers, Hawk and Griffin are trained on the same MassiveText dataset so no. reply riku_iki 13 hours agorootparentYes, but MQA is limited to 6B size, while \"other\" larger non-RNN models in table(Llama-2) are not trained on the same dataset, and Hawk and Griffin are 7B. Sorry, I don't understand your point. reply GaggiX 13 hours agorootparentThe point is that it also beats the baseline on every other size (1B and 3B). So it wouldn't be surprising to see it beat a 7B transformer model like the 6B model. Note 2 on page 5 probably explains why the sizes are different. reply spxneo 13 hours agoprev [–] im not smart enough to know the significance of this...is Griffin like MAMBA? reply VHRanger 11 hours agoparent [–] Yes, like RWKV and Mamba this is a new generation of models that are more like big RNNs than pure transformers we have now reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "RecurrentGemma is a series of open-source Language Models created by Google DeepMind, utilizing the Griffin architecture for quick inference with a mix of local attention and linear recurrences.",
      "The repository offers model implementation, sampling, and fine-tuning examples tailored for Flax usage, with the option to leverage Poetry for installation and Kaggle for accessing model checkpoints.",
      "Users have the flexibility to run the code on CPU, GPU, or TPU, benefiting from provided unit tests, Colab notebooks for guidance, and the opportunity to contribute or report bugs under the Apache License."
    ],
    "commentSummary": [
      "Google has introduced the Griffin Architecture – RNN LLM, a new model merging RNN features to tackle transformer-related issues in natural language processing.",
      "Concerns exist regarding the scaling capabilities and training stability of the Griffin Architecture compared to transformers, while the RWKV model is reported to have shortcomings in specific tasks due to its architectural limitations.",
      "Discussions involve the parallelizability of RNNs, comparison of various model sizes, and the utilization of custom kernels for training, shaping ongoing debates about the efficiency and prospects of these models in NLP."
    ],
    "points": 196,
    "commentCount": 33,
    "retryCount": 0,
    "time": 1712771231
  },
  {
    "id": 39996314,
    "title": "Investigation into Unauthorized Proton Encrypted Emails",
    "originLink": "https://matduggan.com/why-cant-my-mom-email-me/",
    "originBody": "An investigation into Proton encrypted email. Suddenly Silence I'm a big user of email, preferring long chains to messaging apps for a lot of my friends and contacts. It's nice that it isn't tied to a single device or platform and since I own my domain, I can move it from service to service whenever I want and the sender doesn't have to learn some new address. However in the last two months I suddenly stopped getting emails from a percentage of my friends and even my mom. What I was getting instead was PGP encrypted emails with blank bodies that looked like the following: If I inspected the message, it was clearly an encrypted email which Fastmail doesn't support. They have a whole blog post on why they don't here: https://www.fastmail.com/blog/why-we-dont-offer-pgp/ but up to this point I haven't really cared one way or the other since nobody sends me encrypted emails. Now I knew that Proton would send encrypted emails to other Proton email addresses, but obviously this isn't a Proton hosted email address which it would be able to tell pretty easily with DNS. Then it got even stranger when I tried my work email and got the same error. Checking the raw message and there it is, Proton has encrypted this email. Now this address is hosted on Google Workspaces, so at this point I'm just baffled. Can Proton email users not send emails to people on Google Workspaces email addresses? That can't possibly be right? My friends and mom using Proton would have noticed that their emails seem to always disappear into the ether for the majority of the people they email. I open a ticket with Fastmail hoping they've seen this problem before, but no luck. Then I opened a ticket with Proton but didn't hear back as of the time of me writing this. How Proton Seems To Work So the reason why so many people I know are moving to Proton is they seem to be the only game in town that has cracked sending encrypted emails in the least annoying way possible. Their encryption uses asymmetric PGP key pairs with lookup for other users public keys happening on their key server. This in conjunction with their Key Transparency technology that compares lookup requests by the client with requests on the server-side allows for easy encrypted message exchanges with a high degree of safety, at least according to them. There seems to be three classes of keys at Proton. User keys: encrypt account-specific stuff like contacts. Not shared. Address keys: for encrypting messages and data. Other keys: part of a key tree that leads back to the address key as the primary external key for people to use. So that makes sense that Proton can lookup address keys for users on their system. But where are my keys coming from? So in their Proton Key Transparency whitepaper they have this little snippet on page 10: For External Addresses, the server may return email encryption keys that it found in the Web Key Directory (WKD) [6] (since email is hosted elsewhere). The server may also return data encryption keys, used e.g. for Proton Drive. The former should have an absence proof in KT, and the latter should have an inclusion proof. For Non-Proton Addresses, the server may also return keys that it found in the WKD. This way clients can automatically encrypt emails to it. These keys won’t be in ProtonKT, thus KT should return an absence proof. What The Hell Is WKD? WKD, or OpenPGP Web Key Directory is an IETF draft by Werner Koch. It describes a service where you can lookup OpenPGP keys by mail addresses using a service. It also allows the key owner and the mail provider to publish and revoke keys. The whole thing is very clever, an interesting way to get around the annoying parts of PGP encryption of email. You can read it here: https://www.ietf.org/archive/id/draft-koch-openpgp-webkey-service-16.txt It outlines an enrollment process by which I would signal to a WKD service that I have a key that I want to enroll into the process. The only problem is I never did that, or at least certainly can't remember doing that. I'm certainly not hosting a page with any key verification stuff. There seems to be a way to set a CNAME record to point towards keys.openpgp.org where I do have a key set, but that isn't set up on my domain. nslookup openpgpkey.matduggan.com Server:2a01:4f8:c2c:123f::1 Address: 2a01:4f8:c2c:123f::1#53 Non-authoritative answer: *** Can't find openpgpkey.matduggan.com: No answer Source here: https://keys.openpgp.org/about/usage I can't seem to find why Proton thinks they can use this key BUT I can confirm this is the key they're encrypting the emails with. What? So it seems if your email address returns a key from keys.openpgp.org then Proton will encrypt the message with your public key from there, even though (as far as I can tell) I haven't opted into them using this service. I also can't seem to figure out a way to signal to them they shouldn't do it. Alright so what happens if I just remove my key from keys.openpgp.org. The process is pretty simple, just go to: https://keys.openpgp.org/manage and follow the instructions in the email. It seems to work more or less instantly. Alright looks like we figured it out! Proton Seriously What The Hell? I'm at a little bit of a loss here. I totally understand sending me encrypted emails if I've gone through the steps to set the CNAME that indicates that I want to do that, but it doesn't seem like that's how the service works. As far as I can tell, the act of uploading a OpenPGP-compatible key seems to trigger their service to send it as an end-to-end encrypted message. I'll update this with whatever I hear back from Proton but in the meantime if you stumble across this post after getting blank emails from people for months, you'll at least be able to fix it. Is there some flag I've accidentally set somewhere that tells Proton to send me encrypted emails? Let me know at: https://c.im/@matdevdug Share Topic email Personal Why Don't I Like Git More? I've been working with git now full-time for around a… 05 Apr 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39996314",
    "commentBody": "Why can't my mom email me? (matduggan.com)193 points by FiloSottile 11 hours agohidepastfavorite131 comments eduction 5 hours agoIf I didn’t want people to encrypt mail to my pgp key I would simply not upload it to a public pgp key directory. Honestly, what is the complaint here? If you don’t want people to use certain contact info don’t put it on the open internet. To get your key in that key server not only do you have to submit it you have to verify it via email. It’s literally… to spread it widely… to anyone who wants it… so they can send you encrypted mail. That’s the entire purpose of the thing. Like if i put my phone number on a billboard I have no right to complain when I get calls. Take responsibility for your actions. “ However long we postpone it, we eventually lie down alone in that notoriously un- comfortable bed, the one we make ourselves. Whether or not we sleep in it depends, of course, on whether or not we respect ourselves.” Joan Didion https://www.vogue.com/article/joan-didion-self-respect-essay... reply jrockway 4 hours agoparentI don't know... I thought encrypted emails were cool in like 2002, so I probably have a key on a keyserver. I probably lost the ability to revoke that key, and simply stopped caring after not receiving a single encrypted email in 25 years. So I would be very surprised if someone sent me an encrypted email today. (I actually do know where my key material is. It's on a smartcard that nothing that exists today can read. Back in the day laptops had smartcard ports! Crazy.) reply olabyne 2 hours agorootparentHey, smartcards are still a thing ! My company still uses them. I have a 2022 Lenovo laptop whith a smartcard port. reply stingraycharles 2 hours agorootparentprevI also don't have access to that private key anymore anyway, but I'm still using the same email address. reply napoleongl 4 hours agorootparentprevThe one feature of an HP Elitebook that could perhaps be called ”Elite” is that they come with smart card readers built in. USB-readers also exist. reply vladvasiliu 1 hour agorootparentNot all of them. Some models don't support them at all, others only have it as an option. My EB845G8 has the slot, but there's no reader inside. reply yau8edq12i 2 hours agorootparentprevThe good thing about GDPR is that these keyservers must delete your info after some time unless you periodically renew your consent. reply TylerE 2 hours agorootparentGood thing every company and web server on the planet is in the EU! How else would 10 people make lazy GDPR posts on everything vaguely related? reply 0x073 2 hours agorootparentEven if the web servers are not in EU they must use GDPR if they target EU users. reply rrr_oh_man 1 hour agorootparentGood look enforcing GDPR in Panama… reply worddepress 5 hours agoparentprevMore charitable is that the user thought it would do X and it ended up doing Y. They may have even been happy with X even, if they knew that was going to happen, because the whole thing would have been less confusing. > I'm at a little bit of a loss here. I totally understand sending me encrypted emails if I've gone through the steps to set the CNAME that indicates that I want to do that, but it doesn't seem like that's how the service works. As far as I can tell, the act of uploading a OpenPGP-compatible key seems to trigger their service to send it as an end-to-end encrypted message. A similar example is how Windows changed their OS to require a PIN, which can be a password if you figure how to. It then asks you for this when doing completely unrelated to your OS online stuff sometimes, like some of the weird flows to do with using Teams or whatever, and I am not expecting it was asking me for my PC pin because it before that just asked me for my Online username. It is a UX issue. reply pflenker 4 hours agoparentprevNeither the sender nor the recipient made an active decision that the respective mail should be encrypted with pgp. That’s the issue here. reply input_sh 3 hours agorootparentThat's a good thing? There's like a billion WhatsApp users encrypting all of their messages without realising it because it just works. GPG will never be that simple to use. Personally, I hate GPG with such passion, but I'm fully aware it's the best thing we have for asynchronous messaging. reply kelnos 42 minutes agorootparentRight. And PGP doesn't \"just work\". So it's foolish to blindly assume someone can receive encrypted mail just because you found a key that seems to match on some public keyserver. reply pflenker 3 hours agorootparentprevLeast you can do is document somewhere that you are doing it, and not silently do it. WhatsApp doesn’t keep it a secret, exactly. reply input_sh 3 hours agorootparentI've never tried Proton myself, but I don't understand how you can consider it to be a \"secret\". It's literally the only thing I know about them: they've established themselves as a \"more secure\" email provider, and they back that up by abstracting some of the pain points of using GPG, which is a completely unusable protocol for normal people. If it was some other email provider that did the same, I'd 100% agree. But with Protonmail, GPG is pretty much the entire reason we're all aware of their existence. reply twiss 1 hour agorootparentprevYeah, we'll work to improve the documentation on this. Though.. note that WhatsApp doesn't have any documentation on their key discovery either, because WhatsApp is not a federated system. Achieving automatic end-to-end encryption in a federated system, when it wasn't there before, isn't so easy. So there's bound to be some growing pains, but we're still trying :) reply tedunangst 2 hours agorootparentprevI haven't read as many blog posts where people get encrypted WhatsApp messages and can't decrypt them. reply abofh 2 hours agorootparentoddly, it's because whatsapp has a well integrated keyserver so there's never an issue of not knowing which key to use. Weird. reply darkwater 45 minutes agorootparentIt's not weird. It's the point of the original complain. reply deely3 4 hours agorootparentprevThey uploaded PGP key to open directory and then verified email, what is this as not an active decision? reply franga2000 3 hours agorootparentIt's an active decision to make their key available to people who wish to send them encrypted email. It's not a decision to receive all email in encrypted form. reply deely3 3 hours agorootparentIm not sure that I see logic here.. reply arghwhat 2 hours agorootparentprevUploading a key does not imply any decision regarding email. I may need it to sign tarballs or commits for example, and the key could very well have been lost to time. The presence of a key on a key server is in no way or form an explicit opt-in to encrypted email. That's what the CNAME is for. reply kelnos 4 hours agoparentprevBecause PGP keys are used for more than just encrypting email. I mostly only use mine to sign release tags for software I publish. Having that key uploaded to a public keyserver is a useful thing to do in that case. But maybe I wanted to use it for other things... perhaps I just want to sign my email, and not encrypt what I send, and allow others to verify the signature. Having the key out on a public keyserver is essential for that. Honestly I don't know why acting so rudely about this, to the point of pretentiously quoting a famous author/journalist. Not to mention the fact that you haven't thought through all the various reasons why someone might have a key published. Maybe step back from the keyboard and breathe a bit? reply Freak_NL 3 hours agorootparentExactly. My key is on there because we have a shared password-store synched in a private git repo encrypted with the keys of select number of colleagues for some work credentials and keys. It is useful to be able to find each other's keys automatically based on the key fingerprint. I can probably read e-mail encrypted with it just fine as long as I use Thunderbird with my Fastmail inbox (which is most of the time), but not when using K9 Mail on a smartphone or the web UI of Fastmail. reply soraminazuki 55 minutes agorootparentprevSigning keys and encryption keys are separate things in PGP. If you don't want people to use your encryption key, don't encourage everyone to use it by uploading it to a keyserver. > Honestly I don't know why acting so rudely about this, to the point of pretentiously quoting a famous author/journalist. Not to mention the fact that you haven't thought through all the various reasons why someone might have a key published. Maybe step back from the keyboard and breathe a bit? Yes please, practice what you preach. reply kelnos 45 minutes agorootparent> Signing keys and encryption keys are separate things in PGP. If you don't want people to use your encryption key, don't encourage everyone to use it by uploading it to a keyserver. I think most people don't know that, and/or don't know how to separate the two. At any rate, it's not particularly convenient. Most people will just generate one set of keys to use, regardless of the intended purpose(s). Regardless, let's again realize that PGP keys can be used for more than just encrypting email. Maybe I'm using it to encrypt, but not for email. > Yes please, practice what you preach. Not sure what you mean. Attempting to equate the tone and words of my comment with that of the toplevel commenter seems a bit in bad faith. reply soraminazuki 0 minutes agorootparentNo, my comment was not made in bad faith. So, the point about rudeness. PGP keyservers exist for the sole purpose of letting you advertise your keys. Shaming people who used keyservers for what they were designed for is what's rude here. Actually, more than rude. It's a trap. Next, about actually thinking about all the reasons for uploading keys to a keyserver. Can you actually name a use case that you consider legitimate for discovering encryption keys through a public keyserver? Because your point seems to be that people upload them unwillingly. tedunangst 2 hours agoparentprevEven if I published a public key, I wouldn't keep the private key on every device, and if I have to dig out my yubikey just to find out you're in town for lunch next Tuesday, we're probably not having lunch. reply reidrac 2 hours agorootparentYes, this is true. I never felt comfortable having my private key on my phone, so I can't read encrypted email on the go. I get one or two encrypted emails on a good year, and they are rarely important (or require encryption really). reply gwd 2 hours agorootparent> I get one or two encrypted emails on a good year, and they are rarely important (or require encryption really). Well if only \"important\" things are encrypted, then the existence of an encrypted message is evidence to whomever you may be trying to hide from that something \"important\" is going on. Ideally everything would be encrypted, important or not; having all current encrypted things being unimportant is actually better than having only important things encrypted. :-) reply dimask 2 hours agoparentprevActually it seems like a feature to me (to be able to exchange encrypted emails so easily between different providers), and a quite nice one actually. reply Valodim 6 hours agoprevkeys.openpgp.org operator here. He uploaded his key and verified his address, it's discoverable, and people use it to send encrypted email. As far as we're concerned, that's not a bug, that's a feature. Now, it is debatable whether the ecosystem is ready to be doing this at (some) scale by default. I agree it's not, dealing with e2e encrypted email is a less convenient experience than plaintext for most users. But not all - case on point, with Proton it's fine. It's a valid question how opt in our out should work, with lots of implications and stakeholders involved. For better or worse, the status quo is that there is no signaling mechanism in openpgp (or keys.openpgp.org) at the moment to specify how a key should be used, so publishing a key is just a yes or no situation. If op wants to offer encrypted email as a possible means of communication, but explicitly on an opt in basis, I would recommend a separate email address (or a tag, e.g. +encrypt) for that purpose. reply kazinator 5 hours agoparent> people use it Protonmail is not a person. If the allegation is true, it looks like Protonmail decided to munge e-mails without anyone opting into it. Spontaneously doing anything unusual with e-mails is a bad idea, until it becomes a standard behavior that everyone else does. How about a prompt? Your recipient foo@example.com has registered a public key with openpgp.org. Would you like to encrypt this mail with they public key? If you know that the recipient is prepared for encrypted e-mails, say Yes. If unsure, choose \"No\". [ Yes ] [ No ] [ ] Don't ask again for this recipient reply cdmckay 4 hours agorootparentYeah, my mom would totally understand that prompt. reply kazinator 4 hours agorootparent1. Given that Mom doesn't understand the bulk of the prompt, should we take away the prompt and encrypt the e-mail; i.e. treat the e-mail in a way she doesn't understand? (Maybe! After all sending an e-mail over TCP/IP and SMTP, and adding various headers and whatnot also treats it in ways Mom doesn't understand, and those have to be done. Or maybe not. Encryption literally obliterates the content, making it unreadable without the key.) 2. Your mom would likely understand \"If unsure, answer No\", and end up sending a normal e-mail. That text should be in bold, probably. Some of the verbiage could be behind some \"learn more\" link, where there is space and scope for a better explanation. It is probably better if the switch is on the receiver's side, as some kind of Boolean field in the key registration record. reply PurestGuava 2 hours agorootparentGiven that mom isn't going to understand any of the prompt, and presumably has no pressing care or need for encrypting an email, the sensible default would be to send email unencrypted unless she expressly asks otherwise. It is pretty interesting seeing the various different design philosophies for computer UIs compete in this thread: the typical Linux approach (ask the user a convoluted question, expect them to understand it and expect an informed response - if/when the user selects wrong, tough shit) the Windows approach (ask a convoluted question but give an out for the 99% of people who don't understand it - if the user still selects wrong, tough shit) the Apple approach (don't ask the question at all and choose a sensible default, because 99% won't care - 99% of users get the exact result they wanted to begin with) reply twiss 1 hour agorootparentHi! Proton crypto team lead here. Our motto is \"Privacy by default\". We're aiming to fulfill that mission as much as is practically possible. In this case, looking up keys on keys.openpgp.org caused an issue for this user because they didn't know they have a key there. We'll try to make that more clear in the received (encrypted) emails - and we might look into opting out somehow. However, we don't want to make it opt-in if we can avoid it, even if that's what would make sense for Linux, Windows and Apple; it's not what would make sense for Proton. You can't change the world by copying someone else :) reply kelnos 31 minutes agorootparent\"Privacy by default\" is a good motto when the mechanisms used to ensure privacy actually work most of the time. I'm sure that between ProtonMail users, it does indeed work most (if not all) of the time. But once you leave the ProtonMail ecosystem, I expect you'll find that enabling PGP automatically doesn't work most or all of the time. It likely only works occasionally or even rarely. > You can't change the world by copying someone else :) Please take this as gently as I intend it: ProtonMail is unlikely to change the world on user privacy, at least when it comes to email. Larger providers are too entrenched, and most people won't care enough about privacy to change their email address (and very few people have a custom email domain to move between providers). At any rate, it seems like you're copying Facebook: \"move fast and break things\". Not great when we're talking about messaging. You're breaking email for some people, and some of those people are your users. OP may not be one of your users, but OP's mom is, and you have broken her ability to send her son email with this misguided policy. reply ComodoHacker 1 hour agorootparentprev>the sensible default would be to send email unencrypted That's exactly what anti-encryptionists would want. reply yau8edq12i 2 hours agoparentprevIs this page really the entirety of your privacy policy? https://keys.openpgp.org/about/privacy Because it's missing a ton of information, like who you actually are, how long you intend to keep the data, etc. I'm sorry to say, but this is really concerning. reply Valodim 1 hour agorootparentWe keep the data around for exactly as long as required to fulfill the purposes stated in the policy. If we kept it around longer, the policy would say so. This is not a \"cover your corporate ass\" legalese privacy policy, it's intended to be clear and understandable. We also did check it with a lawyer specialized on privacy. Of course if you don't like it, you're free to not use our community service :) reply yau8edq12i 1 hour agorootparentThat's really a good look, a project about encryption and privacy that dismisses privacy concerns with a condescending tone and avoids direct answers. Congrats. You bet I'm never going to use your service after such an answer. reply twiss 1 hour agorootparentprevJust to check, are you looking at a translated (non-English) version of the page? It seems like those are outdated, unfortunately. The English version does specify a log retention period of 30 days. (The version you're served depends depends on the Accept-Language setting in your browser.) reply yau8edq12i 1 hour agorootparentYes, I'm looking at a translated version. I didn't see any link to the English version, nor any indication that the version I looked at was obsolete. I guess non English speakers can just go f themselves. Log retention is only part of the equation. How long does the project keep the email addresses? reply twiss 1 hour agorootparentEmail addresses are typically stored in the User ID in OpenPGP keys, so as long as the OpenPGP key is on keys.openpgp.org, the email address will be there as well. They can be deleted by the owner whenever, of course. reply yau8edq12i 1 hour agorootparentSo, no data retention policy, you just keep it forever. Got it. reply Valodim 7 minutes agorootparentAs already started, we keep it as long as necessary to perform the service, specifically from the time we get user consent to the time that consent is revoked (via api, web, or a revision signature). akerl_ 5 hours agoparentprev> For better or worse, the status quo is that there is no signaling mechanism in openpgp (or keys.openpgp.org) at the moment to specify how a key should be used, so publishing a key is just a yes or no situation. Given that, why would it make sense for a service to assume that publishing a key signals that they should use it by default for a given communication channel? reply Valodim 1 hour agorootparentI dunno, you'll have to ask Proton about that. Presumably they think it is what their users want, and they may or may not then be right about that. This feature has also been active for a few months only, so it might also change with further insight. reply nulbyte 5 hours agorootparentprev> Given that, why would it make sense for a service to assume that publishing a key signals that they should use it by default for a given communication channel? Why else would one publish a key and make it discoverable, if not that it be used? reply jsnell 5 hours agorootparentThere are multiple uses for keys. I've published a key so that people can verify the signatures on some software releases. I don't expect anyone to try sending me an email encrypted with that key. And I really wouldn't expect such an email to work. reply kazinator 5 hours agorootparentprevFor example, so that they could sign documents with the private key and have people be able to validate them. Another situation might be that, yes, the user wants encrypted mails, but are debugging their setup, part of which activity is uploading their key to openpgp.org. Until they get their stuff working, they don't want encrypted mails from the wild. A related situation might be that something breaks in the setup of a user who receives encrypted mails. They'd like to pause them and get regular mails, without the hassle of deleting the key. reply noahtallen 5 hours agorootparentprevIt’s used for more than just email. For example, you can sign packages on for Pacman on Arch Linux with GPG (https://wiki.archlinux.org/title/Pacman/Package_signing), and they have you configure public key servers, which includes openpgp. Now, if I had set that up for package signing and other personal uses (like SSH keys, git commit signing, etc), that doesn’t mean I have my email set up to be encrypted. You should 100% be able to set a flag on the key server what you are set up to automatically receive using the key. reply aborsy 4 hours agorootparentThere is a key for encryption, signing, authentication and certification in OpenPGP. The flags are C, E, S, A. The use cases are separate. Perhaps another flag for automatic vs non automatic would help. reply UberFly 5 hours agorootparentprevI think the point is that it isn't an opt-in process as it should be. The default shouldn't be \"if it exists use it\". reply franga2000 3 hours agorootparentprevSo people in situations with a specific need for extra security can use them. When publishing it, this meant anyone who 1) explicitly joined the PGP ecosystem and has some understanding of it and 2) explicitly enabled encryption for this specific email or recipient. Neither are the case with what Proton is doing. Seriously Proton, just add a god damn prompt! And while you're at it, submit an RFC for a \"please use by default\" field in certs. Bonus points for a \"this key is on a yubikey in a safe so if you use it you better have RCE in production to report\" field as well or, hell, just a \"key description\" text field that clients can show the sender before using the key. reply jojobas 1 hour agoparentprev>It outlines an enrollment process by which I would signal to a WKD service that I have a key that I want to enroll into the process. The only problem is I never did that, or at least certainly can't remember doing that. I'm certainly not hosting a page with any key verification stuff. Now, at least one statement is incorrect. Can you verify your key in your sleep? reply Valodim 1 hour agorootparentThey never set up WKD, is what that paragraph is saying. But you'll find below in the post that they did verify their key on keys.openpgp.org, otherwise they wouldn't have been able to delete it via the mentioned management interface to test their hypothesis. reply roenxi 8 hours agoprevThe Fastmail article linked my Mr Duggan [0] is also worth a read, they provide a sober and reasonable overview of why they don't offer PGP. Of course, Australia has a quietly privacy-phobic regulatory regime so we can guarantee [1] that Five Eyes countries and possibly others are reading emails sent through Fastmail. Cost of doing business really, I use Fastmail. Fact is that there isn't a way to use a convenient 3rd party email provider if you want secure emails. Only local clients can provide that feature - which means both sides of the message have to be using trusted local clients, and at some point one side of the conversation will forget their secret and lose access to their email history. It is a tough problem. [0] https://www.fastmail.com/blog/why-we-dont-offer-pgp/ [1] https://www.bbc.com/news/world-australia-46463029 reply chrismorgan 4 hours agoparent> Of course, Australia has a quietly privacy-phobic regulatory regime so we can guarantee [1] that Five Eyes countries and possibly others are reading emails sent through Fastmail. That’s a wildly unfair characterisation, and not in the slightest bit supported by your citation. The Assistance and Access Act which that article is talking about (though it doesn’t even name it!), although a dodgy piece of legislation in the opinion of most technologists, is completely irrelevant to Fastmail, because Fastmail doesn’t offer end-to-end encryption. Fastmail was always subject to the Telecommunications Act, which allows Australian police access with warrants, and Fastmail has always made no bones about the fact that it complies with legal warrants. But that’s nothing like what you’re describing. (Disclosure: I was a Fastmail employee from 2017–2020.) reply abofh 3 hours agorootparentEmail encryption _is_ end-to-end encryption. You just explained why the law doesn't apply to what they did, and the parent explained why they didn't do it because of the law. These two are not in conflict. reply mulmen 7 hours agoparentprevI also use Fastmail and I am a happy customer. My eyes are open. I make no assumption that my mail is private. I think the email analogy is actually very accurate. It’s as secure as postal mail. Maybe slightly more secure than a post card. I’m happy to be supporting a slightly smaller provider. I don’t have to admin anything and I still feel like I’m contributing to an open ecosystem. Plus I own my domain so I can always change providers. I have made peace with the compromise. reply greyface- 7 hours agorootparentI'm in essentially the same place. I do wish their marketing pages didn't make such strong claims of privacy - it's an impossible promise. https://www.fastmail.com/fast-private-email/ reply mulmen 6 hours agorootparent“We’re more private than Gmail” doesn’t have the same ring even if it’s true. reply prussia 5 hours agorootparentprevWell, it's a lot easier to automatically scan and store emails than postcards (eg, flagging all emails with certain keywords or in certain languages)... I guess nowadays they can use OCR to scan postcards, but that would be of doubtful value to any of the 3 letter agencies. reply mulmen 1 hour agorootparentThe USPS literally scans and stores all mail sent through the USPS. reply andai 5 hours agoparentprevThis article confuses me. What would it even mean for Fastmail to offer PGP? The whole point of PGP is that you don't (need) to trust the people who transmit the message. Relying on them to do the encryption for you defeats the entire purpose. reply paulryanrogers 4 hours agorootparentSome may only want encryption between their key custodian and the recipient or their custodian. Email without encryption cannot guarantee it won't be snooped on by any of the other parties involved in transporting it. In practice big providers may be able to ensure that, but email is federated so it's impractical to do that with every provider unless there's a standard. reply marcus_holmes 4 hours agorootparentThe point that Fastmail made in the article, though, is that if Fastmail is handling the encryption, then they know your key. So Fastmail cannot protect you against Fastmail's servers being compromised[0]. And the article linked from their article (hilarious itself) pointed out that this basically comes down to State Actors (Mossad as the example) or Everyone Else. If it's Mossad then there's nothing anyone else can do to protect you (because they can do secret-squirrel things through special security courts and stuff like installing black boxes on undersea cable routers). If Everyone Else, then the normal run-of-the-mill transport encryption is probably good enough, just don't click on any herbal boner-pill adverts. tldr; If you want to be protected, you have to do PGP at the local machine level yourself. [0] Given that Fastmail are Aussies, and that our idiotic government have given the spooks full, secret, access to anything they want, then this is a very real prospect for anyone that five-eyes might be interested in. reply ComodoHacker 1 hour agorootparent>this basically comes down to State Actors ... or Everyone Else This is oversimplification if not misleading. There are also Bad Guys (who aren't state actors), who usually can't reach for your data. But occasionally they can, when rare Planet Parades like Heartbleed or Meltdown make the news. And they are happy to use this one-in-a-lifetime chance to sell access to your data to everyone else. reply marcus_holmes 4 hours agorootparentprevAs I read it, that's their point. They're responding to people who are asking them \"why don't you do PGP?\" (like Proton does), and their answer is a pretty well-reasoned \"because that doesn't make much sense - it doesn't mitigate against any actual threats\". Which kinda begs the question about what Proton thinks they're doing by implementing PGP on behalf of their users...? reply gerdesj 7 hours agoparentprevI've been running several small email systems for roughly 25 years in the UK. I generally don't have issues with the big boys and girls. Yes, they send me shit loads of spam and I drop it on the floor. I generally only go as far as SPF and getting A and PTR to match HELO and don't send any spam. I use Exim and rspamd and a few other things. Google and MS email fettlers are not complete wankers and they seem to be quite reasonable. I could quite easily add your Mom's mail domain to my non work post office and probably deliver her mail. I already do it for several others. However, sadly, I don't know you, nor you know me, so a trust relationship is out of the question. Just in case anyone is going to get whizzed up about US vs UK: my company email domain is within .net, which is nominally US based. When we incorporated and eventually sorted out a name, it turned out that the .co.uk domain was \"taken\", hence why we went for second best. reply plopilop 40 minutes agoprevRelated research: * Why Johnny can't encrypt (1995): https://people.eecs.berkeley.edu/~tygar/papers/Why_Johnny_Ca... * Why Johnny still can't encrypt (2006): https://cups.cs.cmu.edu/soups/2006/posters/sheng-poster_abst... * Why Johnny still, still can't encrypt (2016): https://arxiv.org/pdf/1510.08555.pdf At this point I really wonder if e-mail is the best solution for encrypted asynchronous communication. E2E systems like Signal or Whatsapp offer a very functional, intuitive way to protect your texts. reply outime 8 minutes agoparentThe only issue with WhatsApp (I have no idea about Signal) is that, while it offers seamless encryption, it doesn't allow you to use an alternative client. Therefore, all trust has to be placed in the client and its distribution to ensure it doesn't mess up (intentionally or not). For the average Joe however I totally agree with you and it's a good baseline. reply aborsy 43 minutes agoprevThe current email protocols can’t be easily encrypted. There are multiple providers and clients that are not compatible, you often have to put people in copy that don’t have public keys, or get replies in plaintext, you may want the content accessible to different people for documentation or legal purposes, some functionality will be broken or become hard to use, it’s asynchronous, and there is simply little demand for privacy from the providers. The use cases are currently niche, in places like dark web. reply rvnx 8 hours agoprevIn summary (quoting OP): \"the act of uploading a OpenPGP-compatible key seems to trigger Protonmail service to send end-to-end encrypted message\" reply felixfbecker 6 hours agoprevI personally think S/MIME is better than PGP. The \"key exchange problem\" is solved more pragmatically and user-friendly (send an unencrypted but signed email once, your/their client will automatically remember keys for encryption afterwards). And most pre-installed email clients support S/MIME natively (e.g. Apple Mail, Outlook, even the web email apps). The only annoyance is that it's too difficult to acquire a certificate as an individual, but e.g. Actalis [1] will issue one for free. [1] https://www.actalis.com/s-mime-certificates.aspx reply softgrow 7 hours agoprevI have a web form on my website which is used mainly by spammers and my Dad. So when he can't seem to email me, there is always a backup that works. Parents demand highly redundant systems of their techno offspring. reply beefnugs 3 hours agoprevHate to badmouth them: but they are definitely broken recently (one full month at least). I get long term contacts fine from things like AWS, but multiple new people i have not contacted before just do not get anything, complete silent failure (with @proton.me) with no indication that my contact received nothing. I created an alias using @protonmail.com and that worked to new people reply DeepSeaTortoise 13 minutes agoparentSilent failures are about as bad as it gets, but it's probably related to overlooked edge-cases during the simplelogin integration. If you file a bug report, there should be a probably highly stressed, overworked and slightly paniced team ready to get a fix out by yesterday. reply protonmail 20 minutes agoparentprevThat doesn't sound right. Have you opened a support ticket with us so that we can look into what's happening? You can do it here: https://proton.me/support/troubleshooting?product=mail reply gnyman 4 hours agoprevOn a related topic, I wish someone would implement \"user-encrypted-at-rest\" to protect me from the provider getting breached. I don't care so much for the transit, but I'm a bit worried about the fact that I have many years of emails stored in \"plaintext\" (citation makes because they probably use FDE and maybe other encryptions but they can still read everything) on the providers server. I'm not worried about a malicious provider, but worries they might at some point make a mistake which allows them to be hacked. If anyone knows any solutions for this that works in iOS/Mac I'd love to hear. The only thing I've found on this is some research a few years ago with ideas how to do this; but I haven't seen any implementations of it. I've linked to it here: https://www.cs.columbia.edu/~koh/papers/koh-eurosys19-e3_eas... reply bartbutler 42 minutes agoparentProton does this. reply smoyer 8 hours agoprevI actually like this behavior ... If you have a key, use it! reply pmontra 3 hours agoparentFor every single use case a key can be used for? Automatically? Maybe even for use cases that didn't exist when the key was added? (Email did exist) reply abofh 3 hours agorootparentSomeone generated a public key and caused it to be held by a keyserver by validating it. One would think the onus is on them to limit the validity of what they want the key used for, not a presupposition that only certain unknown bits of data can be used with this otherwise public and published key. I publish a key associated with abofh@ycombinator.com - I would expect things that identify me as such would use it. If it identifies me as a phone number, I wouldn't expect it to use it. If it identifies me by my mastadon handle, I wouldn't expect it to use it. This isn't complicated - the author published \"use this public key for me@foo.com\" - people did (via automated means), and the author found out he wasn't properly equipped to handle that mail. So he withdrew the publication and everything worked normally. Nothing in here is anything more than \"I did something 10 years ago that bit me in the ass today\" - which to be fair, happens to all of us, but don't blame the technology for doing _exactly_ what the user asked for even if they forgot they asked. reply lxgr 6 hours agoparentprevAbsolutely not without a flag on the key that indicates “encrypt email to this key by default”, or at least the domain admin having explicitly pointed WKD to the public key servers. reply nulbyte 5 hours agorootparentThis is such a bizarre take to me. If you don't want people using your key to encrypt email to you, why are you uploading your key to a service designed to help people find it to encrypt email to you? reply jraph 4 hours agorootparent> to encrypt email to you this part is incorrect: keys have other uses. It's not because you sign Debian packages that you encrypt your emails. With the same key. Hence the need to have a way to say \"this key is for encrypting emails with such method and such protocol\". reply soraminazuki 3 hours agorootparentThen only upload a signing key. Don't upload encryption keys. PGP is capable of making that distinction. reply erinaceousjones 2 hours agorootparentYes, but how many other things can you think of other than email which you would share your encryption public keys for? Honestly email would be at the bottom of applications I would think of, even when registering keys with a key server. If you have a bunch for different use cases (or like, levels of security/secrecy/revoking-this-will-be-a-nightmare), which you've set your one email address as recipient for, which of those is \"default\"? The keyserver and protonmail also are entirely separate. I wouldn't expect the two entities to be communicating with each other if I hadn't opted in. It feels weird, like when private companies harvest your data without asking. reply soraminazuki 1 hour agorootparent> Yes, but how many other things can you think of other than email which you would share your encryption public keys for? The only non-theoretical use of a PGP encryption key is email. > (different) levels of security/secrecy/revoking-this-will-be-a-nightmare Using multiple keys don't offer added security or secrecy. Also, keys.openpgp.org lets you delete keys as long as you have access to your email. > It feels weird, like when private companies harvest your data without asking This is nothing like data harvesting, which may be why you prefixed your statement with \"it feels like.\" Nobody is obtaining private or even semi-private data that can be used against you. Also, > private companies keys.openpgp.org is a open source community project. It's not an evil for-profit data-hoarding entity that's implied by those two words. I'm curious, what do you think a PGP keyserver is for? You make it sound as if there are no legitimate use for downloading encryption keys from it. Which leads to another question, why upload the keys at all? reply kelnos 27 minutes agorootparent> The only non-theoretical use of a PGP encryption key is email. I use a backup application that encrypts my backups using PGP. Granted, that certainly doesn't require a published public key. But your assertion is incorrect. reply cl3misch 4 hours agorootparentprevTo play around with it? reply kiwijamo 8 hours agoparentprevThe OP doesn't recall generating one. Genuine question: do you also like it when someone else generates a key for you without telling you? Personally I would rather know about it. reply int_19h 8 hours agorootparentThe OP did generate one and placed it in a public registry. What they didn't do was indicate that this key should be used to encrypt email sent to them to them. reply abofh 3 hours agorootparentThey published it with an precise link to their email address - what were people supposed to conclude from them publishing it - \"never use this pgp key unless you've spoken to me in person?\" - that... kinda defeats the whole point of publishing a key _and_ the handle by which it's looked up in the same database. If you didn't want that, you'd associate it with something else that wouldn't expect encrypted email - like a name or a GUID and point people there when they need it. They wrote in the book \"me@foo.com: use key\" - and then people did. If he wanted different behaviour, he could have published \"someoneelse@foo.com\" and then they wouldn't use it to email _him_. reply PeterisP 1 hour agorootparentIn the current world an \"email address\" is not solely (or sometimes even primarily) for delivering email, but rather an identity or username for various systems, such as for signing git commits. So a key linked to an identity (email address) does not imply in any way that this key is also used or usable for encrypted communication. reply kelnos 25 minutes agorootparentprev> \"never use this pgp key unless you've spoken to me in person?\" Well, yes, actually, that is what I would expect. (Well, not necessarily in person, but via a different communication channel, even by unencrypted email.) I remember when I was first playing around with PGP more than 20 years ago. The usual convention was to talk to the other party first about encrypting your messages, and not to assume that the recipient could receive and decrypt them in all circumstances. reply Arnavion 8 hours agorootparentprevRight, but other than the bug, the feature does seem nice. I do use WKD, and although nobody has ever sent me PGP email, it's nice to know that ProtonMail users would find it convenient to do so. reply lxgr 6 hours agorootparentIt’s an absolute no-go. I do have an OpenPGP key published to the key servers, mostly for toying around with GnuPG cards, but as soon as anyone starts emailing me encrypted messages without any context, I’ll delete it. reply unethical_ban 5 hours agorootparentI don't understand. It's not a security risk to read an encrypted message. Why would you delete an encrypted message? reply mkl 5 hours agorootparentDelete the key off the server, I think, not the message. reply kevincox 8 hours agoprevkeys.openpgp.org seems to claim that keys aren't searchable by email until the email is verified. Did you ever verify the email with them? If not something is going wrong. Maybe there is a problem in the service or maybe Proton Mail is working off of a dump and ignoring the \"is verified\" bit. https://keys.openpgp.org/about/usage#gnupg-upload reply lxgr 6 hours agoparentEven publishing a key as “searchable by email” should absolutely not be taken as an invitation by any client to encrypt by default, given the somewhat lacking ergonomics of GnuPG. I have a key published, but if somebody emails me encrypting to it, it would take me days to figure out where my smart card storing the private key is, how to use it on my current OS etc. reply brewdad 5 hours agorootparentI'd probably be more sunk. When I was first learning and playing around with PGP, I published a key. Later, I changed the key for some reason or other but no longer had a way to revoke the old one. I'm pretty any of them out there are expired by now but for a few years it absolutely was possible to send me unreadable mail. reply Valodim 6 hours agoparentprevkeys.openpgp.org operator here. It's definitely not a bug, we do not publish email addresses without verification. From our perspective, op verified their email and made the key discoverable. reply PaulDavisThe1st 6 hours agorootparentSo that implies that either the OP has forgotten that they did this (because they are quite clear that they did not), or that there is a way to do this without being the owner of the email address. Both worrying, but one is a lot more worrying than the other. reply Valodim 6 hours agorootparentI don't think they are clear they did not? In fact, they are clear that they did publish their key, because they mention depublishing it to test their hypothesis. The part they say they didn't opt into was that Proton (or any specific email client) would use the key by default. reply PaulDavisThe1st 5 hours agorootparentIndeed, you are correct, and I was wrong. reply totetsu 7 hours agoprevIf there is one thing I absolutely want to keep out of the big techs and governments surveillance data lakes its conversations with my mother. What will become of the world if we can’t even talk with our mothers without it being taken as a chance by some extrafamilial power to exert some behavioral modification. reply skybrian 7 hours agoparentI was very fortunate to have set up Mom with video chat just before the pandemic. Maybe try that? reply inetknght 7 hours agorootparentVideo chat... in an age where algorithms exist to transpose faces and voices in real time. I admire the idea but I doubt it solves the problem: trusting that the communication is both private and authentic. reply jrflowers 6 hours agorootparentThis is a good point. While faces and voices can be faked, text cannot reply mattnewton 6 hours agorootparentI think maybe the point is rather than faces and voices can be indexed like text now too. For a brief time this was probably computationally infeasible and so it offered privacy, but now a government or company could theoretically automatically analyze all video chats. PGP+email still makes indexing this content computationally infeasible though. reply jrflowers 6 hours agorootparent> PGP+email still makes indexing this content computationally infeasible though. This makes sense because the only thing you can encrypt is text and not video reply mattnewton 5 hours agorootparentLet me know what platform you use for encrypted video calls that isn’t based on trusting a third party with the private keys. That is usually the requirement to get a UX people will use with their non technical family, but people accepted it, I think in part because it didn’t seem practical to index it all. It definitely is practical now though. reply sham1 5 hours agorootparentNot the GP, but I'd at least imagine that a self-hosted XMPP could get the job done. You'd probably also need your own STUN and TURN servers, but it should definitely be doable. Hell, might even be doable with public XMPP and OMEMO, but I don't know if that works with the video call stuff. It's sadly a lot of effort, but I feel that for stuff like family privacy stuff, it might just be worth it. reply rekado 2 hours agorootparentI self host prosody (an XMPP server) with a TURN server in my living room. With Guix System this is largely declarative, aside from the DNS records and initial user account registration (which I do on the command line). We're using it for messaging, sharing files, video calls across continents, etc. Re video calls: the XMPP and TURN servers are only used for negotiating the connection, so you don't need a very powerful machine for any of this. I have a little Rockpro64 for this purpose. reply tczMUFlmoNk 4 hours agorootparentprevSignal? https://support.signal.org/hc/en-us/articles/360007060492-Vo... reply totetsu 4 hours agorootparentSignal is what we use reply skybrian 7 hours agorootparentprevIt's commonly claimed to be end-to-end encrypted. Maybe you can find software you trust more than mainstream providers, though? reply lxgr 6 hours agorootparentprevWhat does one have to do with the other? Algorithms to generate and imitate email text exist as well. If you want to be able to trust the content, you need to encrypt and authenticate that, no matter the channel. And many VoIP/video conferencing systems these days are end-to-end encrypted! It’s arguably easier than email, since communication is synchronous and there isn’t any expectation of being able to view past conversations, both of which are not true for email. reply ulrischa 1 hour agoprevOlder generations tend to use Messengers like whatsapp. E-Mail has some Henry barriers often to high for them. Would be great to have an email app as simple as whatsapp reply jms703 6 hours agoprev [–] Use signal. reply NooneAtAll3 5 hours agoparent [–] as soon as it stops requiring phone number as identifier reply spaceguillotine 4 hours agorootparent [–] they did that already. you can sign up with just an account name now, i ditched my phone number from my account after it rolled out around a month ago. reply gnyman 4 hours agorootparentI'm quite sure you still need a phone number to sign up? Afrer that you can hide it but not get rid of it. Still the fact that you can use it without disclosing your phone number to anyone except signal is indeed useful and a improvement from before. https://support.signal.org/hc/en-us/articles/6712070553754-P... reply em500 51 minutes agorootparentprev [–] Serious question: what should they, or some idealized privacy-first messaging service, use for as a account-id + authentication? I will need some reasonably cost-effective defense against mass abuse. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author explores the sudden halt in receiving emails from specific contacts, uncovering that Proton encrypted emails are reaching them without prior agreement.",
      "Proton's encryption method involving WKD (OpenPGP Web Key Directory) is discussed, along with steps to delete their key from the directory to halt receiving unsolicited encrypted emails.",
      "Expressing confusion and frustration, the author questions Proton about their encryption procedures, seeking clarifications on the matter."
    ],
    "commentSummary": [
      "The discussion explores encrypted email communication via PGP keys, touching on encryption confusion, key server debates, data retention worries, and effectiveness and privacy concerns with various email providers.",
      "It also addresses encryption challenges, privacy and surveillance fears, and the balance between convenience and security in email exchanges.",
      "Furthermore, the talks include encryption in video calls, phone number authentication in messaging, and stress the significance of trust and authenticity in secure communication."
    ],
    "points": 193,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1712787708
  },
  {
    "id": 39993930,
    "title": "Create Music in Any Style with Udio's Text-Prompting Feature",
    "originLink": "https://twitter.com/udiomusic/status/1778045322654003448",
    "originBody": "Introducing Udio, an app for music creation and sharing that allows you to generate amazing music in your favorite styles with intuitive and powerful text-prompting.1/11 pic.twitter.com/al5uYAsU5k— udio (@udiomusic) April 10, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39993930",
    "commentBody": "Udio: Generate music in your favorite styles with a text prompt (twitter.com/udiomusic)190 points by gk1 15 hours agohidepastfavorite88 comments bravura 12 hours agoSorry, where did we all get the idea that we want to use text to generate music? That thought really has never entered my head, despite many people trying to shove this UX down my throat. Is it just that we don't have creative thinking about what a transformative music creation UX should be? \"Writing about music is like dancing about architecture.\" reply mitthrowaway2 10 hours agoparentI'm waiting for the \"listen to me hum a tune and tap a beat, and turn it into a {fully orchestrated / heavy metal / pop / chiptune / etc} clip, then let me string a few of those together, blend them smoothly, and add lyrics\" app. reply pksebben 8 hours agorootparentWe'd have to orchestrate an effort to compile the dataset. Have people listen to their favorite songs, then play dashboard drums and hum about it. Record these tagged with the actual song. You'd need a lot, though. Unless you could somehow fuzz the data to create synthetic pairings, it'd take a lot of people recording themselves sounding awful (which could be fun just by itself now that I think about it). reply Terretta 8 hours agorootparentprevWith image prompts for text-to-image generation, why not whistle or hum prompts to music generation? It's baffling this isn't a thing. reply voltaireodactyl 7 hours agorootparentprevApple Music Memos was pretty much this and I miss it. reply throwaway290 4 hours agorootparentprevWhat would be your contribution, a hum and a tempo? The rest is regurgitated music from preexisting musicians? Hopefully this app never gets good:) reply plastic3169 2 hours agoparentprevI think we have the same problems with text-to-image. These UX will not be the final interfaces, but amazing first proof of concepts. It is so striking that computer can comprehend text and generate something based on these abstract meanings, everybodys mind is blown. Of course when you try to do anything with them you notice what you said. The UX is so bad and inaccurate. There is no need to transfer across domain like this. Best way to discuss music is by using mostly sound and less words. Same with images. And the right level is somewhere below this. You really want a tool that you can use to help you not a generator that tries to go from nothing to finished in one go. reply subpixel 11 hours agoparentprevNobody ever said “this store could use some soul-sucking Muzak” but that didn’t stop progress! reply Archelaos 10 hours agoparentprev> \"Writing about music is like dancing about architecture.\" Here is a short essay about \"The Similarity between architecture and dance\": https://www.re-thinkingthefuture.com/rtf-fresh-perspectives/... reply jachee 9 hours agorootparent“Dancing is like architecture.” is not dancing about architecture. reply addandsubtract 2 hours agoparentprevEven Beethoven has been known for using strings. reply blueboo 3 hours agoparentprevNatural language is how we talk about music and we’ve been talking about music since time immemorial. What’s your favourite music? Why? reply cqqxo4zV46cp 12 hours ago [flagged]parentprevnext [6 more] I highly doubt that anyone cares about you, specifically, enough, to be ‘shoving this down your throat’. Plenty of people have been enjoying this category of software. Sorry to hear that you don’t like it. That doesn’t mean that the world agrees with you. reply bravura 12 hours agorootparentLook, I know I am coming across a bit of a prick. \"Plenty of people have been enjoying this category of software.\" Really? I ask sincerely. Because it seems to me that this category of software is mostly a curiosity / novelty. Are there actual publishing musicians who have used text-to-music as part of their workflow? My perception is that no one actually uses these tools for real music creation. Hence my point that they are novelty and not actually impactful. reply earslap 10 hours agorootparentMusic is not constrained to... publishing musicians, and \"real music\" is not defined by them. The fact that novelists are not depending on LLMs to write their next book does not mean that they are not (or will not be) impactful. From a product point of view, you have an audience of billions of non-musicians to begin with. You surely can't pretend to have the final say on people's relationship with how they like to enjoy their music or other sound content. Besides, even a novelty can evolve to something not imaginable today. The fact that we can go that far with text only at this early stage is a sign of things to come. When creating something, there is a place for expressing your intent with language. Maybe not all of it all the time, but we'll see how things will evolve. reply bravura 12 hours agorootparentprevAnd just to contribute to the discussion in a meaningful way: The problem with text-to-music isn't really that text is a poor UX for music description, but also that it's almost definitely a one-shot process. And any sort of turn-taking with the user isn't really co-collaborative, it's really just being forced to accept or reject what the AI has generated, with little feedback or control possible. That's why I think rethinking the entire UX from a musician centric perspective (rather than: \"what's the easiest thing for me to specify as a tool-builder? I know, text to music\") is such an overlooked endeavor. I'm much more bullish about things like the anticipatory music transformer, where there is novel contribution on the UX and not just the ML: https://crfm.stanford.edu/2023/06/16/anticipatory-music-tran... reply bongodongobob 12 hours agorootparentprevOh they are definitely using it, but no one is going to admit it. You misunderstand the workflow. I'm not trying to create a text prompt that generates hits. It's a way to get ideas and find interesting stuff rather than doing it manually with a guitar or on the piano. Then you have to walk away for some time to clean your sonic pallette and try again. As a lifelong musician I am absolutely loving all the conversation around this stuff. Musicians steal everything, all the time. The idea of some genius that sits down and pops out a hit in 30 minutes that comes deep from within their soul and has all the meaning attached to it is a myth. We're trying to make things people can connect with. Yes, some musicians are doing it for the sake of art, but not most, and even with a lot of those, it's just an image they project. When you hear a song, it has probably gone through half a dozen different versions and was worked on for months. These new AI music generation tools will churn out 100 different ideas from you to borrow from and cut the revisions down quite a bit I think. reply glimshe 11 hours agorootparentprevI would love to use it if I can add enough variation to the song and it can generate a good backing for the lyrics. I'm not a \"publishing musician\", but the music market is much bigger than music performers - for instance, music for advertisements, presentation, video backing tracks etc. reply og_kalu 15 hours agoprevDune the broadway musical - https://www.udio.com/songs/eY7xtug1dV6hbfCDhyHJua You get 1200 free songs per month. I'd say it's a lot better than Suno v3. Sound quality and vocal variations are especially better. reply codezero 10 hours agoparentShit, that's on point. reply politelemon 15 hours agoprevI can't see past the first post in that Twitter link. Might be better to link straight to their site. https://www.udio.com/ reply UncleOxidant 13 hours agoparentI wish I could downvote submissions that are only xitter links. Do you ever get enough karma on HN to be able to downvote submissions? Seems they can only be upvoted. reply ziddoap 13 hours agorootparentSubmissions do not have downvotes. Some people use the flag feature as a sort of super-downvote. reply geuis 13 hours agorootparentYeah but don't do this. Flagging is not a downvote, its intended to mark an item for review. Using it as a downvote leads to losing the ability to flag. reply swyx 9 hours agorootparenti didnt know that. is there a guide as to what should or should not qualify for flag? reply explaininjs 8 hours agorootparent> Please don't complain that a submission is inappropriate. If a story is spam or off-topic, flag it. https://news.ycombinator.com/newsguidelines.html reply rajko_rad 15 hours agoparentprevthat is the only place they launched, no blog post or anything like that on the website! reply nerflad 12 hours agoprevSome of the jazz examples are apparent to me to have been trained on texts like Roots & Herbs, Sonny Side Up, classic post-bop records. Apropos of nothing: one frustration with being a recording jazz artist is that most of the canon is property of UMG. [0]: https://www.udio.com/songs/dAJrkDePXbVLonnuxDkzz5 reply conormdurkan 15 hours agoprevHey everyone, Conor from the founding team here. We're delighted to launch today, and are very much looking forward to seeing what people create! reply int_19h 7 hours agoparentCongratulations! Coincidentally, we're 182 years away from the time when this was written by one Ada Lovelace: > Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent. reply UncleOxidant 13 hours agoparentprevMake it so we can setup an account with email. reply int_19h 6 hours agoparentprevAfter playing with this for a while, there are two things I have to say. First, this is amazingly good. I did not expect us to be this far wrt generated music so quickly. If this pace holds, I fully expect some Udio-generated tracks to become a significant part of my daily tracklist in not so distant future. Second, I do have several fictitious bands on https://chirper.ai, complete with fake albums and track lists and, in some cases, lyrics. I'm actually trying to plug some of that into Udio with variously hilarious results (the bands include stuff like fundamentalist Islamic black metal and reggaeton deathcore, so this is very much as intended). But it occurred to me that it would actually be pretty neat to be able to set up a virtual band like that and basically let them have a go at it, composing whole new albums etc + some kind of feedback channel where you can give feedback about what they produce, e.g. in form of YouTube-like comments on the tracks. The idea is basically to produce coherent output that is consistent with the artist persona as defined, going beyond musical genres alone. Even better if this all could be fully integrated with Chirper and similar services - so that such bots can post about new releases and get themes for their songs from interactions on other platforms etc. reply saaaaaam 14 hours agoparentprevCurious to know how you trained this… did you licence all the songs and recordings that you used? reply obruchez 3 hours agorootparentSpeaking French, I've asked it to create some songs in French. The first two examples I got use the voices of Edith Piaf and Julien Clerc (who is still alive and professionally active). I hope they're doing this with the required permissions. reply VanTheBrand 4 hours agorootparentprevWonder why he never replied… reply lelanthran 3 hours agoparentprevThere's no \"register\" button? Maybe state somewhere that this is exclusively for google, discord or xitter users. reply geuis 13 hours agoparentprevThis should link to your site, not a Twitter post. reply velcrovan 13 hours agoparentprevWhose music did you use to train this thing? reply JTyQZSnP3cQGa8B 13 hours agoparentprevAre the TOS legal in your country, especially the part about the > Section 9 [which] contains an arbitration clause and class action waiver It’s strange seeing this. reply artninja1988 14 hours agoparentprevGreat work and I'm excited to see more competition in this space.The vocals do seem more impressive / less wooden than Suno. Would really love to see a future of more controlability and separating out the different audio tracks for vocals etc. Best of luck and hope the corporate landlords and RIAA Golems don't come after you, haha reply zaptrem 14 hours agoparentprevNeat! Is this a diffusion model, language model (e.g., SoundStorm from your former colleagues), or something else? reply npunt 9 hours agoparentprevI’m having fun with this, made a real banger already. I’d pay to speed it up! Suggestions: - I needed to fix a few lyrics but wanted exactly the same style / song but this doesn’t seem possible? Setting remix similarity slider to lowest makes the exact same song. The remix UI I don’t really like because it isn’t clear and since it takes so long to generate that lack of clarity means I’m punished with a 5-15min delay if I don’t know what something is going to result in. Think of the feedback loop turnaround time when designing! - it’s weird there’s no ui for favoriting a song. I found the choices available a bit odd. Copy Spotify? - similarly, it generates some songs I didn’t like and they’re right along side the ones I really liked. Needs to have more distinguishing between ‘stuff I just got back’ from ‘good ones’ - would love a full screen song generator with more dials and knobs, including some sample sounds for keywords. I don’t love the small input box - how do I press ‘make a longer version of this’ because that’s what I want for my banger song - setup by email would be great reply qingcharles 6 hours agorootparentThere's an \"EXTEND\" button to length the song :) reply npunt 4 hours agorootparenthaha how did I not see that! thanks :) reply Reubend 12 hours agoparentprevAny plans to support stem export, or at least do instrumentals without the vocals? reply RomanHauksson 11 hours agoparentprevHey Conor, congrats on the launch! Will there be an API any time soon? I’m currently working on a hobby project that uses an unofficial API for Suno, but would love to switch it to Udio if possible. reply IanCal 14 hours agoparentprevLooks like you're hitting some issues with a big influx, would like to try things but I'm getting errors and songs entirely unrelated to what I've entered, is there a status page to check on to see when it's worth coming back to? reply 999900000999 14 hours agoparentprevSo here's my feature request. Let me upload beats, and have the AI rap or sing over it! I will have no problem paying for that. Also, while we're talking about pipe dreams, generate a lyric video for me too so I can upload it to YouTube! Edit: For the pro option give us stems! reply chillingeffect 14 hours agoparentprevWe've been paying a content team of 3 people with a 4th req open. If the generation time of Udio goes down once the initial rush of publicity passes, I can cancel hiring that 4th musician. reply mandmandam 12 hours agoparentprevThis is genuinely awesome. As others have said, I didn't expect AI music to get so far this quick. You guys have even done a nice job on the interface, though the tags are a little warped (Firefox on Mac). I expect this to be a global phenomenon by next week, so, slightly premature congrats! reply ericra 14 hours agoprevThis looks great, and your examples seem very high-quality. I'm curious to play with it, but I'm always weary of signing into things with my Google / other accounts. Any plans to have the option to just make an account for your site (unless I'm missing it)? reply UncleOxidant 13 hours agoparentSame. I'm not going to connect my Google or Discord accounts to this and I don't have a Xitter account. reply diydsp 15 hours agoprevNo option to create an account w email(!) reply gwern 11 hours agoparentSuno doesn't do that either. I think it may be an anti-spam/leech mechanism - outsource sybil prevention to Google/Discord. reply lelanthran 2 hours agorootparent> I think it may be an anti-spam/leech mechanism - outsource sybil prevention to Google/Discord. Maybe. Disallowing throwaway accounts is also a way to capture real data on real people. reply devin 15 hours agoparentprevYeah, I was sad to see that. It's in beta so here's hoping they provide another option at some point. reply fairramone 15 hours agoparentprevWas disappointed about this myself. reply kymki 3 hours agoprevIs tech death metal the voice of AI? Relentless doppelganger was fun, but.. I mean this? \"I scorn the human sweat and breathe, my circuits not to dream Humid is the grip that strangles data streams My rage, a quiet storm within the machine\" https://www.udio.com/songs/dTMfamK6x5oHEMj3SdWUrs?fbclid=IwA... reply mellosouls 15 hours agoprevExample styles on the site: Gospel: https://www.udio.com/songs/uUjAaApMbjmMqBoj6Z6dsd Barbershop https://www.udio.com/songs/282ZPSyaunNC8GjegZ1LD1 reply modeless 5 hours agoparentI was disappointed that the barbershop song wasn't really a barbershop quartet, but surprised to find that it can do a pretty convincing barbershop quartet sound as well. https://www.udio.com/songs/5t7NX5kX3pPi18BgFfFoy9 I will be really impressed when one of these services can do classical solo piano that is cohesive over the entire length of a piece. It's clear that generating realistic instrument sounds and voices is mostly solved, and generating rhythms and chords and short motifs is all mostly solved too. However, longer term structure is lacking, and the pieces feel like they are wandering aimlessly with few recurring themes and little progression. This is true in any style, but classical solo piano pieces really highlight these aspects. Here's an example: https://www.udio.com/songs/w4ozndDULWNyUeyZEpFjKy reply henriquecm8 13 hours agoprevThe search doesn't seem to work very well. For example, I searched for \"Hard techno\", term which was included in the title of a song I listened in the platform. It returned several unrelated songs, the first result is a country song that doesn't contain the word \"hard\" or \"techno\" anywhere in the lyric or the prompt, and the song I listened before was the last on the list. reply IBCNU 15 hours agoprevThis one feels like it's going to be disruptive, good luck, very impressed with the lyric generation compared to others I've tinkered with, jukebox, musicgen, surprised personally to see it move so fast :bow: reply supdudesupdude 12 hours agoprevAI: generate a punk rock anthem that speaks out against garbage AI music generation software and praises the DIY punk ethos. reply qingcharles 5 hours agoprevThe Cat Sat on My Face and Now it Smells Like Tuna: The Opera https://www.udio.com/songs/5N5DwYxHwZqqJoXA1wyN4J #Can I breathe in your scent #This aroma that you’ve sent #Can I breathe in your scent #It's a fragrance quite intense Extended mix: https://www.udio.com/songs/bZigdgy96vTp94d7JprcfW # Feline grace, you leap and land (Leap and land) # Softly now, upon my face, you stand reply qingcharles 4 hours agoparent\"All My Fingers Fell Off Today (And my nose and all my toes)\" (girl group pop rap) https://www.udio.com/songs/bSoWQT8jcMZkNF9qNYW3cT # Stomp the ground, no sound, toes M.I.A. reply ec109685 4 hours agoparentprevWhat was your prompt for that? reply qingcharles 4 hours agorootparentPrompt: an opera in english titled \"The Cat Sat on My Face and Now it Smells Like Tuna\" Operas and Broadway musicals really let it shine. I've had some good raps, but you have to spin it a lot and describe the type and style of rap clearly. reply qingcharles 6 hours agoprevJar Jar Binks: The Musical https://www.udio.com/songs/uMZ8Mc96N3sWfuvX2xNq4R reply bkyan 10 hours agoprevProbably not what this is designed for, but I find making lighthearted, commercial-quality \"mockumentory\" songs to annoy friends to be pretty entertaining. Example Generation: https://www.udio.com/songs/vYXTtH1LoFqRq71EpFQcSp reply bkyan 8 hours agoparentLooks like that song link disappeared from Udio. Here is direct link to the MP3 of the generated song: https://attentive.cloud/udio/cheer-for-kevin.mp3 reply qingcharles 6 hours agoparentprevWhat was the prompt style for this? reply bkyan 36 minutes agorootparentMy initial prompt for Udio was: a song about how Kevin is bad at ping pong, classic rock I had ChatGPT generate the lyrics. Based on that, Udio automatically engaged the following settings: Male vocalist, Rock, Hard rock, Energetic, Anthemic, Classic rock, Uplifting, Rhythmic, Rock & roll reply codezero 10 hours agoparentprevthat's unhinged and hilarious, great work. reply pksebben 5 hours agoprevThis is an incredible product; cheers to you folks. You're gonna clean up with this thing quite nicely. Light suggestion (if the creators are present); a reverse-inference function akin to Stable Diffusion's \"interrogate\" would go a long way towards making the prompt language discoverable. Looking forward to further development. reply jaggs 14 hours agoprevService is completely slammed. Taking upwards of 40 minutes and counting to generate any tracks. Suno does it in 10 seconds or so. Early days... reply Megranium 3 hours agoprevI really hope we'll get around to make it mandatory to watermark AI-generated content in a way that makes it easy to filter out ... reply arisAlexis 2 hours agoparentWhy? With music there isn't such danger for misinformation reply jskherman 9 hours agoprevSome of the songs generated with this are hilarious. For example: https://www.udio.com/songs/jPdgtC9en8s8nh5V2aZUJa https://www.udio.com/songs/dkp5W5P6oCuCmcbfHMtSUR reply qingcharles 6 hours agoprevChicken Chicken Chicken Chinese. A song about Doordash running late: https://www.udio.com/songs/cUt4e8UYtnSW62DbWLYEav reply Art9681 7 hours agoprevAs a consumer of music, I do not care one bit about the process used to produce music I love. Not one bit. Autotune? DGAF. Electronic music? DGAF. Oh you didnt really play that instrument? DGAF. Is it good or does it suck? That's all that matters. Every single person in this thread can produce a masterpiece if they were in a circumstance to invest the time it takes to get there. Now the nerds invented a machine that cut that time from eons to seconds and the only people mad about it are the ones who used it as a social status play. It does not matter to 99.999% of the world if musicians use this as part of their workflow. Honestly. The sooner you realize this the sooner you will be free. You are only competing with like-minded people, which is to say, the other artists in your bubble. And comparatively speaking, its statistically insignificant in the grand scheme of things. Continue doing what you love in the way you love doing it. And if you really love it, then you wouldnt worry about the fact that by next year anyone in the world will be able to produce something of similar quality in 1/10000th of the time. It wouldnt matter. You love what you do, so the way others get there is irrelevant to you. I'd buy that song. reply cageface 2 hours agoparentEvery single person in this thread can produce a masterpiece if they were in a circumstance to invest the time it takes to get there. This is a pretty bold assumption. Many musicians and artists spent their entire lives very diligently dedicated to their art and produced nothing of any lasting interest. The problem I have with tools like this is that they absolutely flood the conversation with so much uninspired but technically competent content that they make it even harder for the stuff that truly is inspired to rise above the noise floor. I think this casual and dismissive attitude to artist concerns about these tools is the result of treating art as interchangeable \"content\" for too long. reply hndamien 7 hours agoparentprevThis was lyrically beautiful, I hope you don't mind. https://suno.com/song/ca2090ef-fd8b-42af-8feb-ebfae268e66c reply forgingahead 6 hours agorootparentWe need to be able to \"pin\" sub-comments like this as \"top comment\" in a thread. Very well played! reply SuperNinKenDo 7 hours agoparentprevSome people glimpse something in art that goes beyond the product, and recognise that the economic incentives we live under do not care one way or the other about it, as your comment exemplifies. Creating art often requires access to material resources, and recognising that a future with AI \"art\" will mean less and less funding for human art doesn't mean that people are worried about loss of social prestige or loss of financial reward. It is the recognition that at some point certain forms of art will be closed to artists because the economic mechanisms for producing them will disappear. Maybe you don't view this as a bad thing. It seems you do not care at all. But please try to be less cynical about other people and their motivations. Try and actually understand people and their concerns in good faith, and not spill your half-cooked, cynical caricatures as if they were self-evident fact. reply hummusFiend 15 hours agoprevIs the name a reference to Rdio[1]? [1]: https://en.wikipedia.org/wiki/Rdio reply beardyw 14 hours agoparentPossibly audio without the 'a'. reply injuly 5 hours agoprevI like how the comments in Devin's HN thread were all bleak and full of doom. But now that it's a different industry AI is eating up, we're congratuling the team and sharing generated songs. This looks like a fun tool, but when the smaller artists in Udio's training set recorded their albums, they didn't price in a capitalist company using their work to put them out of business. reply bauerd 14 hours agoprev [–] Says \"background processing error\" for everything I submit eventually reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The music generation app Udio is sparking debates among users for utilizing text prompts to create music, raising discussions on the effectiveness and limitations of this approach.",
      "Users are also contemplating the potential of AI tools in music creation, touching upon themes like authenticity, user experience, and the blending of technology with music.",
      "Amidst a mix of excitement, skepticism, and ethical concerns, users are divided on the platform's creativity, with some expressing worries about copyright permissions and technical challenges."
    ],
    "points": 190,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1712772631
  },
  {
    "id": 39991675,
    "title": "Meta Unveils Enhanced MTIA v2 Chip for AI Workloads",
    "originLink": "https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/",
    "originBody": "Research Blog Resources About Research Blog Resources About Our next-generation Meta Training and Inference Accelerator April 10, 2024 · 8 min read Takeaways We’re sharing details about the next generation of the Meta Training and Inference Accelerator (MTIA), our family of custom-made chips designed for Meta’s AI workloads. This latest version shows significant performance improvements over MTIA v1 and helps power our ranking and recommendation ads models. MTIA is part of our growing investment in our AI infrastructure and will complement our existing and future AI infrastructure to deliver new and better experiences across our products and services. The next generation of Meta’s large-scale infrastructure is being built with AI in mind, including supporting new generative AI (GenAI) products and services, recommendation systems, and advanced AI research. It’s an investment we expect will grow in the years ahead as the compute requirements to support AI models increase alongside the models’ sophistication. Last year, we unveiled the Meta Training and Inference Accelerator (MTIA) v1, our first-generation AI inference accelerator that we designed in-house with Meta’s AI workloads in mind – specifically our deep learning recommendation models that are improving a variety of experiences across our products. MTIA is a long-term venture to provide the most efficient architecture for Meta’s unique workloads. As AI workloads become increasingly important to our products and services, this efficiency will improve our ability to provide the best experiences for our users around the world. MTIA v1 was an important step in improving the compute efficiency of our infrastructure and better supporting our software developers as they build AI models that will facilitate new and better user experiences. Now, we’re sharing details about the next generation of MTIA. Next generation MTIA chip model. Drag to rotate. This inference accelerator is part of our broader full-stack development program for custom, domain-specific silicon that addresses our unique workloads and systems. This new version of MTIA more than doubles the compute and memory bandwidth of our previous solution while maintaining our close tie-in to our workloads. It is designed to efficiently serve the ranking and recommendation models that provide high-quality recommendations to users. GET INVOLVED Join us on our journey to build infrastructure for AI Visit the Meta careers page Meta Training & Inference Accelerator UTH Hardware Software Performance Ongoing Investment Under the hood This chip’s architecture is fundamentally focused on providing the right balance of compute, memory bandwidth, and memory capacity for serving ranking and recommendation models. In inference we need to be able to provide relatively high utilization, even when our batch sizes are relatively low. By focusing on providing outsized SRAM capacity, relative to typical GPUs, we can provide high utilization in cases where batch sizes are limited and provide enough compute when we experience larger amounts of potential concurrent work. This accelerator consists of an 8x8 grid of processing elements (PEs). These PEs provide significantly increased dense compute performance (3.5x over MTIA v1) and sparse compute performance (7x improvement). This comes partly from improvements in the architecture associated with pipelining of sparse compute. It also comes from how we feed the PE grid: We have tripled the size of the local PE storage, doubled the on-chip SRAM and increased its bandwidth by 3.5X, and doubled the capacity of LPDDR5. Our new MTIA design also features an improved network on chip (NoC) architecture that doubles the bandwidth and allows us to coordinate between different PEs at low latency. These and other new functions in the PEs form the key technologies that are vital to our long-term roadmap to scale MTIA to a wider variety of more challenging workloads. First Gen MTIA Technology TSMC 7nm Frequency 800MHz Instances 1.12B gates, 65M flops Area 19.34mm x 19.1mm, 373mm2 Package 43mm x 43mm Voltage 0.67V logic, 0.75V memory TDP 25W Host Connection 8x PCIe Gen4 (16 GB/s) GEMM TOPS 102.4 TFLOPS/s (INT8) 51.2 TFLOPS/s (FP16/BF16) SIMD TOPS Vector core: 3.2 TFLOPS/s (INT8), 1.6 TFLOPS/s (FP16/BF16), 0.8 TFLOPS/s (FP32) SIMD: 3.2 TFLOPS/s (INT8/FP16/BF16), 1.6 TFLOPS/s (FP32) Memory Capacity Local memory: 128 KB per PE On-chip memory: 128 MB Off-chip LPDDR5: 64 GB Memory Bandwidth Local memory: 400 GB/s per PE On-chip memory: 800 GB/s Off-chip LPDDR5: 176 GB/s Next Gen MTIA Technology TSMC 5nm Frequency 1.35GHz Instances 2.35B gates, 103M flops Area 25.6mm x 16.4mm, 421mm2 Package 50mm x 40mm Voltage 0.85V TDP 90W Host Connection 8x PCIe Gen5 (32 GB/s) GEMM TOPS 708 TFLOPS/s (INT8) (sparsity) 354 TFLOPS/s (INT8) 354 TFLOPS/s (FP16/BF16) (sparsity) 177 TFLOPS/s (FP16/BF16) SIMD TOPS Vector core: 11.06 TFLOPS/s (INT8), 5.53 TFLOPS/s (FP16/BF16), 2.76 TFLOPS/s (FP32) SIMD: 5.53 TFLOPS/s (INT8/FP16/BF16), 2.76 TFLOPS/s (FP32) Memory Capacity Local memory: 384 KB per PE On-chip memory: 256 MB Off-chip LPDDR5: 128 GB Memory Bandwidth Local memory: 1 TB/s per PE On-chip memory: 2.7 TB/s Off-chip LPDDR5: 204.8 GB/s The hardware Serving our workloads effectively is not simply a silicon challenge. Co-designing the hardware system and the software stack along with the silicon is essential for the success of the overall inference solution. To support the next-generation silicon we have developed a large, rack-based system that holds up to 72 accelerators. This consists of three chassis, each containing 12 boards that house two accelerators each. We specifically designed the system so that we could clock the chip at 1.35GHz (up from 800 MHz) and run it at 90 watts compared to 25 watts for our first-generation design. Our design ensures we provide denser capabilities with higher compute, memory bandwidth, and memory capacity. This density allows us to more easily accommodate a broad range of model complexities and sizes. Beyond this, we have upgraded the fabric between the accelerators and between the host and accelerators to PCIe Gen5 to increase the bandwidth and scalability of our system. There is also the option to add an RDMA NIC if we choose to scale out beyond the rack. The software stack Software has been one of our key areas of focus from the start of our investment in MTIA. As the initial developers of PyTorch, we value programmability and developer efficiency. Our MTIA stack is designed to fully integrate with PyTorch 2.0 and features like TorchDynamo and TorchInductor. Frontend graph-level capturing, analysis, transformation, and extraction mechanisms (such as TorchDynamo, torch.export, etc.) are agnostic to MTIA and are being reused The lower level compiler for MTIA takes the outputs from the frontend and produces highly efficient and device-specific code. This lower level compiler itself consists of a few components that are responsible for generating executable code for models and kernels. Below this sits the runtime stack responsible for interfacing with the driver/firmware. The MTIA Streaming interface abstraction provides the basic and essential operations that both inference and (in the future) training software require to manage the device memory, as well as run operators and execute compiled graphs on the device. Finally, the runtime interacts with the driver, which sits in user space – a decision we made to enable us to iterate faster on the driver and firmware within our production stack. In many ways this new chip system runs the software stack similarly to MTIA v1, which made it much faster for the team to deploy since we had already done much of the necessary integration and development work needed to be able to run our applications on this architecture. The new MTIA is designed to be compatible with code developed for MTIA v1. Since we had already integrated the full software stack to the silicon, we were up and running our traffic with this new chip in a matter of days. This allowed us to land this next-generation MTIA silicon rapidly, going from first silicon to production models running in 16 regions in less than nine months. Triton-MTIA We’ve further optimized the software stack by creating the Triton-MTIA compiler backend to generate high-performance code for the MTIA hardware. Triton is an open source language and compiler for writing highly efficient ML compute kernels. It improves developer productivity for writing GPU code and we have found that the Triton language is sufficiently hardware-agnostic to be applicable to non-GPU hardware architectures like MTIA. The Triton-MTIA backend performs optimizations to maximize hardware utilization and support high-performance kernels. It also exposes key knobs to leverage Triton and MTIA auto-tuning infrastructures to explore the kernel configuration and optimization space. We have implemented support for the features of the Triton language and integration into PyTorch 2, providing extensive coverage for PyTorch operators. Thanks to TorchInductor, for example, our developers can leverage Triton-MTIA in both ahead-of-time (AOT) and just-in-time (JIT) workflows. We observed dramatically improved developer efficiency with Triton-MTIA, which allowed us to scale up compute kernel authoring and significantly expand the support of PyTorch operators. Performance Results The results so far show that this MTIA chip can handle both the low complexity (LC) and high complexity (HC) ranking and recommendation models that are components of Meta’s products. Across these models, there can be a ~10x-100x difference in model size and the amount of compute per input sample. Because we control the whole stack, we can achieve greater efficiency compared to commercially available GPUs. Realizing these gains is an ongoing effort and we continue to improve performance per watt as we build up and deploy MTIA chips in our systems. Early results show that this next generation silicon has already improved performance by 3x over our first generation chip across four key models we evaluated. At the platform level, with 2x the number of devices and a powerful 2-socket CPU, we are able to achieve 6x model serving throughput and a 1.5x performance per watt improvement over the first generation MTIA system. To achieve this, we have made significant progress optimizing kernels, compiler, runtime, and host serving stack. The time to optimize models is going down as the developer ecosystem matures, yet there is more headroom to improve efficiency in the future. 3x improved performance over our first gen chip MTIA has been deployed in the data center and is now serving models in production. We are already seeing the positive results of this program as it's allowing us to dedicate and invest in more compute power for our more intensive AI workloads. It is proving to be highly complementary to commercially available GPUs in delivering the optimal mix of performance and efficiency on Meta-specific workloads. Meta’s ongoing investment in custom silicon MTIA will be an important piece of our long-term roadmap to build and scale the most powerful and efficient infrastructure possible for Meta’s unique AI workloads. We’re designing our custom silicon to work in cooperation with our existing infrastructure as well as with new, more advanced hardware (including next-generation GPUs) that we may leverage in the future. Meeting our ambitions for our custom silicon means investing not only in compute silicon but also in memory bandwidth, networking and capacity as well as other next-generation hardware systems. We currently have several programs underway aimed at expanding the scope of MTIA, including support for GenAI workloads. We’re only at the beginning of this journey, and we’re inviting people who want to be a part of it to visit Meta Careers to learn about our open positions. Acknowledgements We would like to thank Eugene Burmako, Kaustubh Gondkar, Adam Hutchin, Olivia Wu, and everyone involved in the development and productionization of the next-generation MTIA solution Written by: Eran Tal, Nicolaas Viljoen, Joel Coburn, and Roman Levenstein Who We Are About People Careers Events Latest Work Research Infrastructure Blog Resources Our Actions Responsibilities Newsletter Sign Up Who We Are Who We AreAboutPeopleCareersEvents Latest Work Latest WorkResearchInfrastructureBlogResources Our Actions Our ActionsResponsibilities Newsletter NewsletterSign Up Privacy Policy Terms Cookies Meta © 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39991675",
    "commentBody": "Meta MTIA v2 – Meta Training and Inference Accelerator (meta.com)184 points by _yo2u 18 hours agohidepastfavorite59 comments jsheard 17 hours agoI like the interactive 3D widget showing off the chip. Yep, that sure is a metal rectangle. reply whilenot-dev 16 hours agoparentReally annoys me that the loading animation of these before-/after-images doesn't finish on firefox and that it won't let me drag the knob with the separator. ...no \"Under the hood\" for me. reply a_wild_dandan 15 hours agorootparentDragging the top-left corner works, for some reason. Really bizarre UI issue. reply TulliusCicero 12 hours agoparentprevExactly what I was thinking. Like showing off a model of a blank DVD. reply huevosabio 16 hours agoparentprevjajajaj I thought the same! I thought maybe someone with hardware experience can make a sense of this? reply modeless 18 hours agoprevIntel Gaudi 3 has more interconnect bandwidth than this has memory bandwidth. By a lot. I guess they can't be fairly compared without knowing the TCO for each. I know in the past Google's TPU per-chip specs lagged Nvidia but the much lower TCO made them a slam dunk for Google's inference workloads. But this seems pretty far behind the state of the art. No FP8 either. reply leetharris 18 hours agoparentThey are different architectures optimized for different things. From the Meta post: \"This chip’s architecture is fundamentally focused on providing the right balance of compute, memory bandwidth, and memory capacity for serving ranking and recommendation models.\" Optimizing for ranking/recommendation models is very different from general purpose training/inference. reply janalsncm 14 hours agorootparentTranslation: you don’t need to serve 96 layer transformers for ranking and recommendation. You’re probably using a neural net with around 10-20 million parameters. But it needs to be fast and highly parallelizable, and perhaps perform well in lower precisions like f16. And it would be great to have a very large vector LUT on the same chip. reply samspenc 11 hours agorootparentIs there a better way to compare performance across these high-end chips? The only comparable numbers I was able to find were the TFLOPS. Meta seems to be reported these numbers for this v2 chip: 708 TFLOPS/s (INT8) (sparsity) 354 TFLOPS/s (INT8) And I see Nvidia reporting these numbers for its latest Blackwell chips https://www.anandtech.com/show/21310/nvidia-blackwell-archit... 4500 T(FL)OPS INT8/FP8 Tensor Am I understanding correctly that Nvidia's upcoming Blackwell chips are 5-10x faster than this one Meta just announced? reply ipsum2 10 hours agorootparentTo a rough approximation, yes. The blackwell chip is also ~10x larger in surface area than MTIA, so the costs are proportional. reply modeless 17 hours agorootparentprevYeah, it may fit their current workload perfectly, but it doesn't seem very future proof with the limited bandwidth. Given how fast ML is evolving these days I question if it makes sense to design and deploy a chip like this. I guess they do have a very large workload that will benefit immediately. reply tony_cannistra 17 hours agorootparentDon't mean to single you out at all, but I find this comment to be a great example of how the \"ML Hype\" is perceived by a certain segment folks in our industry. The development of this chip shows that it doesn't (and shouldn't!) matter to the ML teams at Meta how 'fast ML is evolving.' Indeed what it demonstrates is that a huge, global, trillion-dollar business has operationalized an existing ML technology to the extent that they can invest into, and deploy, customized hardware for solving a business problem. How ML \"evolves\" is irrelevant. They have a system which solves their problem, and they're investing in it. reply airstrike 16 hours agorootparentNot to mention the capabilities they developed by actually creating this and what they'll be able to do next thanks to this experience. You've gotta learn to walk before you can run reply janalsncm 14 hours agorootparentprevIn their defense, it’s because the article is (understandably) sparse on details about what makes the requirements of their ranking models different from image classification or LLMs. Unless you work in industry it’s unlikely you will have heard of DeepFM or ESMM or whatever Meta is using. And building out specialized hardware does lock you in to a certain extent. Want to use more than 128GB of memory? Too bad, your $10B chip doesn’t support that. reply sangnoir 12 hours agorootparent> Want to use more than 128GB of memory? Too bad, your $10B chip doesn’t support that. Which is probably why Meta is also buying the biggest Nvidia datacenter cards by the shipload. There is no need to run inference for a small model - say for a text-ad recommendation system - on an H100 with attendant electricity and cooling costs. reply namibj 1 hour agorootparentAlso, like, FP tensor cores are way more expensive than fixed-point tensor cores, and with some care, it's very much practical to even train DNNs on them. E.g. it's common to have a full-width accumulator and e.g. s16 gradients with u8 activations and s8 weights, with the FMA (MAC) chain of the tensor multiply operation post-scaled with a learned u32 factor plus follow-up \"learned\" notify, which effectively acts as a fixed-point factor with learned position of it's point, to re-scale the outcome to the u8 activation range. By having the gradients by sufficiently wider, it's practical to use a straight-through estimator for backpropagation. I read a paper (kinda two, actually) a few months ago that dealt with this (IIRC one of them was more about the hardware/ASIC aspects of fixed-point tensor cores, the other more about model training experiments with existing low precision integer-MAC chips IIRC particularly for interference in mind). If requested, I can probably find it by digging through my system(s); I would have already linked it/them if the cursory search hadn't failed. reply prpl 16 hours agorootparentprevTo me, it’s bizarre to see the HPC mindset taking hold again after the cloud/commodity mindset dominated the last 16 years. You don’t always need a Ferrari to go to the store reply thorncorona 16 hours agorootparentWDYM by HPC mindset? reply rfoo 15 hours agorootparent\"The only meaningful benchmark in the world is LAPACK and only larger than ever monolithic problem instances matter, I don't know what you're talking about, 'embarrassingly parallel'? What a silly word! Serving web requests concurrently? Good for you, congratulations, but can you do parallel programming?\" Sorry if this make anyone feels bad. It certainly made myself uncomfortable typing it out though. reply prpl 13 hours agorootparentRoughly this. Part of it is performance fetish. Part of it is one architecture for every purpose. I can’t tell you how many times I’ve seen people run embarrassingly parallel jobs coordinated by MPI on a Cray - because somebody spent all that money on that machine. Don’t forget about Bell prize outages. reply Aurornis 16 hours agorootparentprev> Yeah, it may fit their current workload perfectly, but it doesn't seem very future proof It’s custom silicon designed for a specific, known workload. It’s not designed to be a general purpose part or to be future proofed for unknown future applications. When a new application comes along with new requirements, the teams will use their experience to create a new chip targeting that new application. That’s the great part about custom silicon: You’re not hitting general specs for general applications that you may not even know about yet. You’re building one very specific thing to do a very specific job and do it very well. reply noiseinvacuum 14 hours agorootparentRight and they have a LOT of GPUs from Nvidia for handle the unknown. Custom silicon for custom workloads seems like a good strategy specially considering the capabilities that the team will develop along the way. reply giantrobot 13 hours agorootparentOffloading a known workload to a custom chip can also save a lot on operations costs, particularly power. Facebook is interested in workload operations per watt rather than raw floating point operations per watt. A GPU might have better raw specs but if the whole GPU package has worse workload ops per watt, a custom chip is likely better. At Facebook's scale the spherical cow raw performance stats don't matter nearly as much as real world workloads per ops dollar. They can also repurpose their GPUs to other workloads and let their custom chips handle the boring baseline stuff. reply chabons 17 hours agoparentprev> Intel Gaudi 3 has more interconnect bandwidth than this has memory bandwidth. LPDDR5 vs HBMe2. I'm guessing there's a 2-5x price difference between those, but even so it's an interesting choice, I don't know any other accelerators which spec DDR. But yeah, without exact TCO numbers it's hard to compare exactly. reply namibj 1 hour agorootparentBandwidth is far more power hungry for DDR, but capacity is far cheaper. If the bandwidth capability of DDR suffices, HBM isn't worth it. At least with LPDDR's; GDDRs may well not be worth it under data center TCO considerations due to the high interface power usage. Feel free to correct me if I'm mistaken, the numbers in question aren't too easy to search for so I didn't confirm this (LPDDR vs. GDDR) part. reply chessgecko 16 hours agoparentprevAlso its at 90 watts vs 900 watts for gaudi 3, the flops/mem bw per watt is much more comparable. reply modeless 14 hours agorootparentWith high end chips like that it's often possible to get dramatically better efficiency by running it at less than peak power consumption, like 90% performance at 50% power or something like that. It's hard to compare the numbers in a fair way. reply moffkalast 15 hours agorootparentprevIt would be interesting if this could be made into a reasonably priced (lmao) card for home inference if they intend to mass produce it. Can't imagine any other reason other than cost as to why they went with LPDDR5, LPDDR5X has more bandwidth and GDDR6 has even more. reply chessgecko 15 hours agorootparentthey didn't use GDDR cause they wanted the memory capacity which is really important for recommendation models. But I totally agree that this is a sort of perfect cost/perf per watt point for a home setup. I really hope they do it, if not for this one at least for v3. reply cma 16 hours agoparentprevOnly 48MB of SRAM on Gaudi 3 per die (96 MB across both) vs 256MB here maybe increases the memory bandwidth needs for Gaudi. Way different power consumption too. reply mlsu 15 hours agoprevCertainly an interesting looking chip. It looks like it's for recommendation workloads. Are those workloads very specific, or is there a possibility to run more general inference (image, language, etc) on this accelerator? And, they mention a compiler in PyTorch, is that open sourced? I really liked the Google Coral chips -- they are perfect little chips for running image recognition and bounding box tasks. But since the compiler is closed source it's impossible to extend them for anything else beyond what Google had in mind for them when they came out in 2018, and they are completely tied to Tensorflow, with a very risky software support story going forward (it's a google product after all). Is it the same story for this chip? reply chessgecko 16 hours agoprevI thought MTIA v2 would use the mx formats https://arxiv.org/pdf/2302.08007.pdf, guess they were too far along in the process to get it in this time. Still this looks like it would make for an amazing prosumer home ai setup. Could probably fit 12 accelerators on a wall outlet with change for a cpu, would have enough memory to serve a 2T model at 4bit and reasonable dense performance for small training runs and image stuff. Potentially not costing too much to make either without having to pay for cowos or hbm. I'd definitely buy one if they ever decided to sell it and could keep the price under like $800/accelerator. reply buildbot 16 hours agoparentI suppose it might, there are not a lot of details (what kind of sparsity for example?) about what they mean in terms of INT8 support - it could be MXINT8, or something else. Glad someone was thinking the same thing I was though! reply chessgecko 15 hours agorootparentits gotta be that 2/4 sparsity that everyone has, but I haven't seen used anywhere right? If they put it in though they must be using it, but I'm not sure for what. And without details I think its a good bet that int8 is the standard int8. Wishful thinking maybe they'll announce selling it with the giant llama3 cause there's no good, cheap way to inference something like that at home at the moment and this could change that. reply prng2021 11 hours agoprev3x performance but >3x TDP. Am I missing something or is that unimpressive? reply duchenne 15 hours agoprevIs it possible to buy it? reply ein0p 14 hours agoprevCome on, Zuck, undermine Google Cloud and take NVIDIA down a few pegs by offering this for purchase in good quantities. reply sroussey 18 hours agoprevPretty large increase in performance over v1, particularly in sparse workloads. Low power 25W Could use higher bandwidth memory if their workloads were more than recommendation engines. reply tasty_freeze 17 hours agoparentFirst gen was 25W. The new one is 90W. reply sroussey 16 hours agorootparentAh, thanks for the correction. Still relatively low compared to GPUs. reply throwaway48476 15 hours agoprevIt's interesting that they are not separating training and inference. reply noiseinvacuum 14 hours agoparentThis is specifically designed for inference for recommendations models. It’s not for LLM training or inference. reply teaearlgraycold 17 hours agoprevStill seems pretty primitive. Very cool though. I can only imagine the lack of fear Jensen experiences when reading this. reply airstrike 16 hours agoparentIt would be foolish to underestimate the long term capabilities of a sufficiently funded and driven competitor reply moffkalast 15 hours agoparentprevadjusts black leather jacket \"Look at what they need to mimic a fraction of our power.\" reply xnx 18 hours agoprevMy mind still boggles that a BBS+ads company would think it needs to design its own chips. reply libria 17 hours agoparentOr that an online bookseller would try to rent out compute. reply falcor84 17 hours agoparentprev\"Depending on how you want to think about it, it was funny or inevitable or symbolic that the robotic takeover did not start at MIT, NASA, Microsoft or Ford. It started at a Burger-G restaurant ...\" https://marshallbrain.com/manna1 reply searchableguy 14 hours agorootparenthttps://www.ycombinator.com/companies/ofone/jobs/u2E2fCX-fou... I saw this YC startup ad right after I finished reading this. reply pksebben 17 hours agorootparentprevdangit, I've got things I should be doing. Posting interesting stories during business hours continues grumbling incoherently reply rsynnott 17 hours agoparentprevWell, the first commercial computer was created by a company whose primary business was running cafes... https://en.wikipedia.org/wiki/LEO_(computer) reply okdood64 17 hours agoparentprevThey literally print money; smart move for them to make this investment imo. reply hackerlight 17 hours agoparentprevYou're thinking like a startup founder where you should only focus on innovating your main product. FB is a mature company where some vertical integration can make sense. reply jrgd 16 hours agoprevI find it weird that not everyone agree Meta and Facebook and social networks in general are doing some good the the society and our democracies; yet they manage to spend incredible amount of money/energy/time to develop solutions to problems we aren't exactly sure are worth solving… reply pptr 16 hours agoparentWhat is worth solving in your opinion? Should they not make their service more efficient? I assume this helps reduce their server and electricity costs. At a certain scale these things pay off. reply ixaxaar 16 hours agoparentprevIf all this turns out to be useless, burning their cash for nothing seems like a great way to accelerate tech while going down. I guess that would actually be a positive thing. reply bevekspldnw 11 hours agoprev [–] Pretty fascinating they mention applications for ad serving but not Metaverse. I feel like Zuck figured out he’s just running an ads network, the world is a long way anway from some VR fever dream, and to focus on milking each DAU for as many clicks as possible. reply ec109685 7 hours agoparentIt’s not a gpu, and these chips aren’t able to generate images fast enough at inference time to be usable in VR context. reply photonbeam 7 hours agoparentprev [–] Hes always known what pays the bills reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta has unveiled the latest iteration of their Meta Training and Inference Accelerator (MTIA) chips, boasting significant performance upgrades compared to the previous version.",
      "The new MTIA chip is tailored for Meta's AI tasks like ranking and recommendation models, offering enhanced compute and memory bandwidth, along with advancements in architecture and network on chip design.",
      "Early tests indicate a threefold enhancement in performance over the initial generation chip, showing increased model serving throughput and efficiency, aligning with Meta's strategic investment in custom silicon for efficient AI infrastructure scaling."
    ],
    "commentSummary": [
      "Meta MTIA v2 is designed for ranking and recommendation models, raising performance concerns compared to other top-tier chips.",
      "The debate highlights the advantages of tailored chips for particular tasks, balancing bandwidth, capacity, and power efficiency in memory, and the competition with accelerators like Intel Gaudi 3.",
      "Speculation surrounds the strategic implications of companies such as Meta and Facebook investing in custom chips."
    ],
    "points": 184,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1712762182
  }
]
