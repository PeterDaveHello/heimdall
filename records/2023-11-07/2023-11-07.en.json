[
  {
    "id": 38166420,
    "title": "OpenAI Unveils GPT-4 Turbo, New Assistants API, Multimodal Capabilities, and Updates to Whisper and Consistency Decoder Models",
    "originLink": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
    "originBody": "Close SearchSubmit Skip to main content Site Navigation Research Overview Index GPT-4 DALL·E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Safety Company About Blog Careers Residency Charter Security Customer stories Search Navigation quick links Log in Try ChatGPT Menu Mobile Navigation Close Site Navigation Research Overview Index GPT-4 DALL·E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Safety Company About Blog Careers Residency Charter Security Customer stories Quick Links Log in Try ChatGPT SearchSubmit Blog New models and developer products announced at DevDay GPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more. November 6, 2023 Authors OpenAI Announcements, Product Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include: New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window New Assistants API that makes it easier for developers to build their own assistive AI apps that have goals and can call models and tools New multimodal capabilities in the platform, including vision, image creation (DALL·E 3), and text-to-speech (TTS) We’ll begin rolling out new features to OpenAI customers starting at 1pm PT today. Learn more about OpenAI DevDay announcements for ChatGPT. GPT-4 Turbo with 128K context We released the first version of GPT-4 in March and made GPT-4 generally available to all developers in July. Today we’re launching a preview of the next generation of this model, GPT-4 Turbo. GPT-4 Turbo is more capable and has knowledge of world events up to April 2023. It has a 128k context window so it can fit the equivalent of more than 300 pages of text in a single prompt. We also optimized its performance so we are able to offer GPT-4 Turbo at a 3x cheaper price for input tokens and a 2x cheaper price for output tokens compared to GPT-4. GPT-4 Turbo is available for all paying developers to try by passing gpt-4-1106-preview in the API and we plan to release the stable production-ready model in the coming weeks. Function calling updates Function calling lets you describe functions of your app or external APIs to models, and have the model intelligently choose to output a JSON object containing arguments to call those functions. We’re releasing several improvements today, including the ability to call multiple functions in a single message: users can send one message requesting multiple actions, such as “open the car window and turn off the A/C”, which would previously require multiple roundtrips with the model (learn more). We are also improving function calling accuracy: GPT-4 Turbo is more likely to return the right function parameters. Improved instruction following and JSON mode GPT-4 Turbo performs better than our previous models on tasks that require the careful following of instructions, such as generating specific formats (e.g., “always respond in XML”). It also supports our new JSON mode, which ensures the model will respond with valid JSON. The new API parameter response_format enables the model to constrain its output to generate a syntactically correct JSON object. JSON mode is useful for developers generating JSON in the Chat Completions API outside of function calling. Reproducible outputs and log probabilities The new seed parameter enables reproducible outputs by making the model return consistent completions most of the time. This beta feature is useful for use cases such as replaying requests for debugging, writing more comprehensive unit tests, and generally having a higher degree of control over the model behavior. We at OpenAI have been using this feature internally for our own unit tests and have found it invaluable. We’re excited to see how developers will use it. Learn more. We’re also launching a feature to return the log probabilities for the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo in the next few weeks, which will be useful for building features such as autocomplete in a search experience. Updated GPT-3.5 Turbo In addition to GPT-4 Turbo, we are also releasing a new version of GPT-3.5 Turbo that supports a 16K context window by default. The new 3.5 Turbo supports improved instruction following, JSON mode, and parallel function calling. For instance, our internal evals show a 38% improvement on format following tasks such as generating JSON, XML and YAML. Developers can access this new model by calling gpt-3.5-turbo-1106 in the API. Applications using the gpt-3.5-turbo name will automatically be upgraded to the new model on December 11. Older models will continue to be accessible by passing gpt-3.5-turbo-0613 in the API until June 13, 2024. Learn more. Assistants API, Retrieval, and Code Interpreter Today, we’re releasing the Assistants API, our first step towards helping developers build agent-like experiences within their own applications. An assistant is a purpose-built AI that has specific instructions, leverages extra knowledge, and can call models and tools to perform tasks. The new Assistants API provides new capabilities such as Code Interpreter and Retrieval as well as function calling to handle a lot of the heavy lifting that you previously had to do yourself and enable you to build high-quality AI apps. This API is designed for flexibility; use cases range from a natural language-based data analysis app, a coding assistant, an AI-powered vacation planner, a voice-controlled DJ, a smart visual canvas—the list goes on. The Assistants API is built on the same capabilities that enable our new GPTs product: custom instructions and tools such as Code interpreter, Retrieval, and function calling. A key change introduced by this API is persistent and infinitely long threads, which allow developers to hand off thread state management to OpenAI and work around context window constraints. With the Assistants API, you simply add each new message to an existing thread. Assistants also have access to call new tools as needed, including: Code Interpreter: writes and runs Python code in a sandboxed execution environment, and can generate graphs and charts, and process files with diverse data and formatting. It allows your assistants to run code iteratively to solve challenging code and math problems, and more. Retrieval: augments the assistant with knowledge from outside our models, such as proprietary domain data, product information or documents provided by your users. This means you don’t need to compute and store embeddings for your documents, or implement chunking and search algorithms. The Assistants API optimizes what retrieval technique to use based on our experience building knowledge retrieval in ChatGPT. Function calling: enables assistants to invoke functions you define and incorporate the function response in their messages. As with the rest of the platform, data and files passed to the OpenAI API are never used to train our models and developers can delete the data when they see fit. You can try the Assistants API beta without writing any code by heading to the Assistants playground. Use the Assistants playground to create high quality assistants without code. The Assistants API is in beta and available to all developers starting today. Please share what you build with us (@OpenAI) along with your feedback which we will incorporate as we continue building over the coming weeks. Pricing for the Assistants APIs and its tools is available on our pricing page. New modalities in the API GPT-4 Turbo with vision GPT-4 Turbo can accept images as inputs in the Chat Completions API, enabling use cases such as generating captions, analyzing real world images in detail, and reading documents with figures. For example, BeMyEyes uses this technology to help people who are blind or have low vision with daily tasks like identifying a product or navigating a store. Developers can access this feature by using gpt-4-vision-preview in the API. We plan to roll out vision support to the main GPT-4 Turbo model as part of its stable release. Pricing depends on the input image size. For instance, passing an image with 1080×1080 pixels to GPT-4 Turbo costs $0.00765. Check out our vision guide. DALL·E 3 Developers can integrate DALL·E 3, which we recently launched to ChatGPT Plus and Enterprise users, directly into their apps and products through our Images API by specifying dall-e-3 as the model. Companies like Snap, Coca-Cola, and Shutterstock have used DALL·E 3 to programmatically generate images and designs for their customers and campaigns. Similar to the previous version of DALL·E, the API incorporates built-in moderation to help developers protect their applications against misuse. We offer different format and quality options, with prices starting at $0.04 per image generated. Check out our guide to getting started with DALL·E 3 in the API. Text-to-speech (TTS) Developers can now generate human-quality speech from text via the text-to-speech API. Our new TTS model offers six preset voices to choose from and two model variants, tts-1 and tts-1-hd. tts is optimized for real-time use cases and tts-1-hd is optimized for quality. Pricing starts at $0.015 per input 1,000 characters. Check out our TTS guide to get started. Listen to voice samples Select text Scenic Directions Technical Recipe As the golden sun dips below the horizon, casting long shadows across the tranquil meadow, the world seems to hush, and a sense of calmness envelops the Earth, promising a peaceful night’s rest for all living beings. Select voice Alloy Echo Fable Onyx Nova Shimmer Model customization GPT-4 fine tuning experimental access We’re creating an experimental access program for GPT-4 fine-tuning. Preliminary results indicate that GPT-4 fine-tuning requires more work to achieve meaningful improvements over the base model compared to the substantial gains realized with GPT-3.5 fine-tuning. As quality and safety for GPT-4 fine-tuning improves, developers actively using GPT-3.5 fine-tuning will be presented with an option to apply to the GPT-4 program within their fine-tuning console. Custom models For organizations that need even more customization than fine-tuning can provide (particularly applicable to domains with extremely large proprietary datasets—billions of tokens at minimum), we’re also launching a Custom Models program, giving selected organizations an opportunity to work with a dedicated group of OpenAI researchers to train custom GPT-4 to their specific domain. This includes modifying every step of the model training process, from doing additional domain specific pre-training, to running a custom RL post-training process tailored for the specific domain. Organizations will have exclusive access to their custom models. In keeping with our existing enterprise privacy policies, custom models will not be served to or shared with other customers or used to train other models. Also, proprietary data provided to OpenAI to train custom models will not be reused in any other context. This will be a very limited (and expensive) program to start—interested orgs can apply here. Lower prices and higher rate limits Lower prices We’re decreasing several prices across the platform to pass on savings to developers (all prices below are expressed per 1,000 tokens): GPT-4 Turbo input tokens are 3x cheaper than GPT-4 at $0.01 and output tokens are 2x cheaper at $0.03. GPT-3.5 Turbo input tokens are 3x cheaper than the previous 16K model at $0.001 and output tokens are 2x cheaper at $0.002. Developers previously using GPT-3.5 Turbo 4K benefit from a 33% reduction on input tokens at $0.001. Those lower prices only apply to the new GPT-3.5 Turbo introduced today. Fine-tuned GPT-3.5 Turbo 4K model input tokens are reduced by 4x at $0.003 and output tokens are 2.7x cheaper at $0.006. Fine-tuning also supports 16K context at the same price as 4K with the new GPT-3.5 Turbo model. These new prices also apply to fine-tuned gpt-3.5-turbo-0613 models.Older models New models GPT-4 Turbo GPT-4 8K Input: $0.03 Output: $0.06 GPT-4 32K Input: $0.06 Output: $0.12 GPT-4 Turbo 128K Input: $0.01 Output: $0.03 GPT-3.5 Turbo GPT-3.5 Turbo 4K Input: $0.0015 Output: $0.002 GPT-3.5 Turbo 16K Input: $0.003 Output: $0.004 GPT-3.5 Turbo 16K Input: $0.001 Output: $0.002 GPT-3.5 Turbo fine-tuning GPT-3.5 Turbo 4K fine-tuning Training: $0.008 Input: $0.012 Output: $0.016 GPT-3.5 Turbo 4K and 16K fine-tuning Training: $0.008 Input: $0.003 Output: $0.006 Higher rate limits To help you scale your applications, we’re doubling the tokens per minute limit for all our paying GPT-4 customers. You can view your new rate limits in your rate limit page. We’ve also published our usage tiers that determine automatic rate limits increases, so you know what to expect in how your usage limits will automatically scale. You can now request increases to usage limits from your account settings. Copyright Shield OpenAI is committed to protecting our customers with built-in copyright safeguards in our systems. Today, we’re going one step further and introducing Copyright Shield—we will now step in and defend our customers, and pay the costs incurred, if you face legal claims around copyright infringement. This applies to generally available features of ChatGPT Enterprise and our developer platform. Whisper v3 and Consistency Decoder We are releasing Whisper large-v3, the next version of our open source automatic speech recognition model (ASR) which features improved performance across languages. We also plan to support Whisper v3 in our API in the near future. We are also open sourcing the Consistency Decoder, a drop in replacement for the Stable Diffusion VAE decoder. This decoder improves all images compatible with the by Stable Diffusion 1.0+ VAE, with significant improvements in text, faces and straight lines. Learn more about our OpenAI DevDay announcements for ChatGPT. Authors OpenAI View all articles Research Overview Index GPT-4 DALL·E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Company About Blog Careers Charter Security Customer stories Safety OpenAI © 2015 – 2023Terms & policiesPrivacy policyBrand guidelines Social Twitter YouTube GitHub SoundCloud LinkedIn Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=38166420",
    "commentBody": "New models and developer productsHacker NewspastloginNew models and developer products (openai.com) 995 points by kevin_hu 15 hours ago| hidepastfavorite477 comments dang 14 hours agoRelated ongoing threads:GPTs: Custom versions of ChatGPT - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38166431OpenAI releases Whisper v3, new generation open source ASR model - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38166965OpenAI DevDay, Opening Keynote Livestream [video] - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38165090 minimaxir 15 hours agoprevMost of the products announced (and the price cuts) appear to be more about increasing lock-in to the OpenAI API platform, which is not surprising given increased competition in the space. The GPTs&#x2F;GPT Agents and Assistants demos in particular showed that they are a black box within a black box within a black box that you can&#x27;t port anywhere else.I&#x27;m mixed on the presentation and will need to read the fine print on the API docs on all of these things, which have been updated just now: https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-referenceThe pricing page has now updated as well: https:&#x2F;&#x2F;openai.com&#x2F;pricingNotably, the DALL-E 3 API is $0.04 per image which is an order of magnitude above everyone else in the space.EDIT: One interesting observation with the new OpenAI pricing structure not mentioned during the keynote: finetuned ChatGPT 3.5 is now 3x of the cost of the base ChatGPT 3.5, down from 8x the cost. That makes finetuning a more compelling option. reply faeriechangling 14 hours agoparentIt&#x27;s a good strategy. For me, avoiding the moat means either a big drop in quality and just ending up in somebody elses moat, or a big drop in quality and a lot more money spent. I&#x27;ve looked into it and maybe the most practical end-to-end system for owning my own LLM is to run a couple of 3090s on a consumer motherboard at substantial running cost to keep them up 24&#x2F;7 and that&#x27;s not powerful enough to cut it and rather expensive simultaniously. For a bit more expense, you can get more quality and lower running costs and much slower processing from buying a 128gb&#x2F;192gb apple silicon setup and that&#x27;s much much much slower than the \"Turbo\" services that OpenAI offers.I think the biggest thing pushing me away from OpenAI was they were subsidizing the chat experience much more than the API and this seems to reconcile that quite a bit. Quite simply OpenAI is sweetening the pot here too much for me to really ignore, this is a massively subsdizised service. I honestly don&#x27;t feel the switching costs in the future will outweigh the benefits I&#x27;m getting now. reply swatcoder 11 hours agorootparentEverybody&#x27;s got their own calculus about how competitive their space is and what this tech can do for them, but some might be best off dancing around lock-in by being careful about what they use from OpenAI and how tightly they integrate with it.This is very early in the maturity cycle for this tech. The options that will be available for private inference and fine tuning, for cloud-gpu&#x2F;timeshare inference and fine tuning, and for competing hosted solutions are going to vastly different as months go by. What looks like squeezing value out of OpenAI today might look a lot like technical debt and frustrating lock-in a year from now.That&#x27;s what they&#x27;re hoping you chase after, and if your product is defined by this technology, maybe that&#x27;s what you have to do. But if you&#x27;re just thinking about feature opportunities for a more robust product, judiciousness could pay off better than rushing. For now. reply layoric 11 hours agorootparent\"What looks like squeezing value out of OpenAI today might look a lot like technical debt and frustrating lock-in a year from now.\"Just wanted to highlight this as such a great, concise way to look at the Buy vs Build with pretty much any cloud service, thanks! reply epolanski 8 hours agorootparentprevWhile everything you say mighy be true, it&#x27;s also irrelevant.Time to market is more important, you build users you get an edge, you can swap models later on (as long as you own the data). reply eropple 6 hours agorootparentAlmost nobody sharecropping their way to an MVP has a product except at the sufferance of the company doing the work for them - regardless of which one they switch to later.There&#x27;s very little there there in most of these folks.(For the few where there is, though, I agree with you.) reply fumar 9 hours agorootparentprevDo you build on AWS AI services then? Or any other cloud provider? The outcome is the same, right? Technical lock in, cost risks, integration maintenance, etc. reply swatcoder 8 hours agorootparentThe key line in my comment, for emphasis:> This is very early in the maturity cycle for this tech.Think about what value you get out of the services and what migration might look like. If you are making simple completion or chat calls with a clever prompt, then migration will probably be trivial when the time comes. Those features are the commodity that everyone will be offering and you&#x27;ll be able to shop around for the ideal solution as alternatives become competitive.Alternately, if you&#x27;re handing OpenAI a ton of data for them to opaquely digest for fine tuning with no egress tools, or having them accumulate lots of other critical data with no egress, you&#x27;re obviously getting yourself locked in.The more features you use, the more idiosyncratic those features are, the more non-transferable those features are, and the more deeply you integrate those features, the more risk you&#x27;re taking. So you want to consider whether the reward is worth that risk.Different projects will legitimately have different answers for that. reply keyle 10 hours agorootparentprevWhile I agree with you, as a happy GPT4 plus customer, I&#x27;m worried about the inevitable enshittification downhill roll that will eventually ensue.Once marketing gets in charge of product, it&#x27;s doomed. And I can&#x27;t think of a product startup that it hasn&#x27;t happened to. Particularly with this type of growth, at some point, the suits start to out number the techies 10:1.This is why openeness and healthy competition is primordial. reply ethbr1 9 hours agorootparentIt&#x27;s not marketing, it&#x27;s economics.If you set money on fire -- eventually there&#x27;s a time when you need to stop doing that. reply layoric 8 hours agorootparentI think the parent is more talking about the other common situation where organizations start focusing on maximizing profits, rather than just working towards a basic profitability. Eg, Google Maps API pricing comes to mind.Yes, OpenAI might be (we don&#x27;t know how much) burning through their $5B capital&#x2F;Azure credits now, but I think the `turbo` models are starting to addressing this as well. And $20&#x2F;month from a large user base can also add up pretty quick. reply ehnto 7 hours agorootparentYou do see VC bloat up company sizes for what could have been a very profitable small to medium sized private business, without enshittefication, had they not hired dozens more people. replyjpalomaki 1 hour agorootparentprevChatGPT only costs a few dollars, but I&#x27;m also \"paying\" for the service by contributing training data to OpenAI.Getting access to this type of interaction data with (mostly) humans must be quite valuable asset. reply mirekrusin 59 minutes agorootparentprevObviously it is good strategy, surely created from GPT. reply muttled 12 hours agorootparentprevFor me personally, being able to fine-tune the local LLM&#x27;s at a much higher rank and training more layers is very useful for (somewhat unreliably) embedding information. AFAIK the OpenAI fine-tuning is more geared towards formatting the output. reply bigfudge 11 hours agorootparentAs I understand it, fine tuning is never really about adding content. RAG and related techniques are likely cheaper&#x2F;better if that’s what you want. reply antupis 2 hours agorootparentyup how I understand fine tuning is more about adding context and bigger picture. RAG is more about adding actual content. Good system probably needs both in long run. reply ebiester 14 hours agoparentprevI don&#x27;t understand the lock-in argument here. Yes, if a competitor comes in there will be switching cost as everything is re-learned. However, from a code perspective, it is a function of the key and a relatively small API. New regulations outstanding, what is stoping someone from moving from OpenAI to Anthropic (for example) other than the cost of learning how to effectively utilize Anthropic for your use case?OpenAI doesn&#x27;t have some sort of egress feed for your database. reply minimaxir 13 hours agorootparent> OpenAI doesn&#x27;t have some sort of egress feed for your database.That&#x27;s what they&#x27;re trying to incentivize, especically with being able to upload files for their own implementation of RAG. You&#x27;re not getting the vector representation of those files back, and switching to another provider will require rebuilding and testing that infrastructure. reply goosinmouse 11 hours agorootparentThats exactly what i thought. Smart strategy on OpenAI&#x27;s part given that its extremely easy (and free) to do RAG with pgvector. reply lolinder 10 hours agorootparentprev> However, from a code perspective, it is a function of the key and a relatively small API.You&#x27;re thinking of traditional apps and APIs.In an AI application, most of the work is in prompt engineering, not wiring up the API to your app. Prompts that work well for one model will fail horribly for another. People spend months refining their prompts before they&#x27;re safe to share with users, and switching platforms will require doing most of that refinement over again. reply Aeolun 10 hours agorootparentI’d be more worried about this if OpenAI had a track record of increasing prices, but the opposite happens. I get more for the same price basically every 6 months. reply dom96 10 hours agorootparentSure, they are decreasing the prices right now. But once it comes time for them to become profitable they can easily reverse course. reply jairuhme 9 hours agorootparentprevI&#x27;d imagine that will start to switch back the other way at some point. Decrease prices to gain market share and get you locked in, then increase prices to earn more money to keep VC&#x27;s happy reply Rastonbury 7 hours agorootparentprevStill moving between models is less arduous than switching cloud providers, depending on use case and price difference of course. Most models hold GPT4 as the benchmark they aspire to and should converge to its capabilities. reply edgyquant 9 hours agorootparentprevSwitching from one API to another generally requires refactoring. I’ve not had much problems moving between LLMs (openai to Anthropic) reply wokwokwok 9 hours agorootparentThen you’re either not testing your prompts or doing something trivial.Remember: a good model with a good prompt will generate bad outputs sometimes.A bad model with a bad prompt will generate a good output sometimes.That is simply a fact with these non deterministic models.You have to do many iterations for each prompt to verify they are working correctly.> I’ve not had much problems moving between LLMs…If you want to move your prompts to a different model, you’re effectively replacing one:f(prompt + seed) => outputWith different black box implementation.Unless you’re measuring the output over multiple iterations of (seed) and verifying your prompt still does the right thing, it’s actually very likely that what you’ve done if take an application with a known output space and converted it to an application with an unknown output space……that partially overlaps the original output space!So it looks like it’s the same.…but it isn’t, and the “isn’t” is in weird edge cases.Unless you’re measuring that, you simply now have an app that does “eh, who knows?”So yes. Porting is trivial if you don’t care if you have the same functionality.…but reliably porting is much harder (or longer). reply wizzard0 2 hours agorootparent> many iterations of each promptBTW its much faster and cheaper to artive at a good prompt if you sample the model in deterministic mode (ie temperature=0)By default you have to guess if the difference is due to the prompt change or due to the dice roll, as you’ve noticed, but you don’t need to! reply wokwokwok 1 hour agorootparentYou should have a read of https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;how-to-generate (the section on sampling, with regard to setting temperature to zero).This is degenerate (greedy) behaviour, and not representative of the what the prompt will behave like at a higher temperature.(At least, that’s my understanding; it’s a complex topic but broadly speaking there no specific reason, as far as I’m aware, to expect that a particular combination of params&#x2F;prompt is representative of any other combination of params&#x2F;prompt for the same model; it may be, but it may not. Certainly on models like GPT4 it is not, for reasons that are not clear to anyone. So… take care with your prompt testing. setting temperature to 0 is basically meaningless unless you expect to use a temperature of 0 in production. The results you get from your prompts at temp 0 are not generally reflective of the results you will get at temp > 0). reply esafak 5 hours agorootparentprevHow many people are even writing tests for these things? reply dbmikus 5 hours agorootparentVery few. Many deployed apps don&#x27;t have a good quantitative grasp of the quality of their LLMs. Some are doing testing or evaluation, through things like unit tests, A&#x2F;B testing different prompts, collecting user feedback.I think we&#x27;re exiting the phase where people can launch an AI app and have people use it just because of the initial \"wow factor\" and moving into the phase where users will start churning and businesses will need to make sure that their AI agent is performing and they they understand how well it&#x27;s performing. replymvkel 10 hours agoparentprev> Assistants demos in particular showed that they are a black box within a black box within a black box that you can&#x27;t port anywhere else.I&#x27;d argue the opposite. The new \"Threads\" interface in the OpenAI admin section lets you see exactly how it&#x27;s interpreting input&#x2F;output specifically to address the black box effect.Source: https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;api-reference&#x2F;runs&#x2F;listRunS... tells you exactly how it&#x27;s stepping through the chain. Even more visibility than there used to be. reply DerJacques 6 hours agorootparentI agree that some parts of the process now seem more like “open”, but there is definitely a lot more magic in the new processing. Namely, threads can have an arbitrary length, and OpenAI automatically handles context window management for you. Their API now also handles retrieval of information from raw files, so you don’t need to worry about embeddings.Lastly, you don’t even need any sort of database to keep track of threads and messages. The API is now stateful!I think that most of these changes are exciting and make it a lot easier for people to get started. There is no doubt in my mind though that the API is now an even bigger blackbox, and lock-in is slightly increased depending on how you integrate with it. reply mvkel 3 hours agorootparentI wouldn&#x27;t say the black box issue is unique to OpenAI. I suspect nobody could explain certain behaviors, including them.As for lock in, agreed completely. reply asaddhamani 5 hours agoparentprevTheir products are incredible though. I’ve tried the alternatives and even Claude is not nearly as good as even ChatGPT. Claude gives an ethics lecture with every second reply, which costs me money each time and makes their product very difficult to (want to) embed. reply ChatGTP 4 hours agorootparentWhat are you using it for? I want to know what people actually use these things for damn it ! reply code_runner 15 minutes agorootparentSummarizing large documents. Finding relationships between two (or more) documents. Building a set of points bridging the gap between the documents. Correcting malformed text data.Not everything is just data in a database or some structured format. Sometimes you have blobs of text from a user, or maybe you ran whisper on an audio&#x2F;video file and now you just have a transcript blob… it’s never been easier to automate all of this stuff and get accurate results.You can even have humans in the loop still to protect against hallucinations, or use one model to validate another (ask GPT to correct or flag issues with a whisper transcript) reply williamcotton 1 hour agorootparentprevRetrieving the non-metadata titles of 45,000 various PDF, docx, etc. without a bunch of rules&#x2F;regexs that would fail half the time.“Derp derp, hallucinations”.Eh, no, not in practice, not when the entire context and document is provided and the tools are used correctly. reply asaddhamani 1 hour agorootparentprevI&#x27;d built a bot to use ChatGPT from Telegram (this was before the ChatGPT API), and currently building a tool to help make writing easier (https:&#x2F;&#x2F;www.penpersona.com). This is the API.Apart from that, it&#x27;s pretty much replaced 80% of my search engine usage, I can ask it to collate reviews for a product from reddit and other sites, get the critical reception of a book, etc. You don&#x27;t have to go and read long posts and articles, have GPT do it for you. There&#x27;s many other use cases like this. For the second part, I&#x27;m using a UI called Typing Mind (which also works with the API). reply visarga 15 hours agoparentprevMistral + 2 weeks of work from the community. Not as good, but private and free. It will trail OpenAI by 6-12 months in capabilities. reply coder543 14 hours agorootparentOpenAI offering 128k context is very appealing, however.I tried some Mistral variants with larger context windows, and had very poor results… the model would often offer either an empty completion or a nonsensical completion, even though the content fit comfortably within the context window, and I was placing a direct question either at the beginning or end, and either with or without an explanation of the task and the content. Large contexts just felt broken. There are so many ways that we are more than “two weeks” from the open source solutions matching what OpenAI offers.And that’s to say nothing of how far behind these smaller models are in terms of accuracy or instruction following.For now, 6-12 months behind also isn’t good enough. In the uncertain case that this stays true, then a year from now the open models could be perfectly adequate for many use cases… but it’s very hard to predict the progression of these technologies. reply fancy_pantser 11 hours agorootparentOpen researchers are trying to shrink and speed up 138K models e.g. YaRN https:&#x2F;&#x2F;github.com&#x2F;jquesnelle&#x2F;yarnIt&#x27;s very compelling and opens up a lot of use cases, so I&#x27;ve been keeping an eye out for advancements. However, inferencing on 4xA100s would be the target today for YaRN and 128K to get a reasonable token rate on their version of Mistral. reply pclmulqdq 13 hours agorootparentprevComparing a 7B parameter model to a 1.8T parameter model is kind of silly. Of course it&#x27;s behind on accuracy, but it also takes 1% of the resources. reply coder543 13 hours agorootparentThe person I replied to had decided to compare Mistral to what was launched, so I went along with their comparison and showed how I have been unsatisfied with it. But, these open models can certainly be fun to play with.Regardless, where did you find 1.8T for GPT-4 Turbo? The Turbo model is the one with the 128K context size, and the Turbo models tend to have a much lower parameter count from what people can tell. Nobody outside of OpenAI even knows how many parameters regular GPT-4 has. 1.8T is one of several guesses I have seen people make, but the guesses vary significantly.I’m also not convinced that parameter counts are everything, as your comment clearly implies, or that chinchilla scaling is fully understood. More research seems required to find the right balance: https:&#x2F;&#x2F;espadrine.github.io&#x2F;blog&#x2F;posts&#x2F;chinchilla-s-death.ht... reply razodactyl 13 hours agorootparentNah, it&#x27;s training quality and context saturation.Grab an 8K context model, tweak some internals and try to pass 32K context into it - it&#x27;s still an 8K model and will go glitchy beyond 8K unless it&#x27;s trained at higher context lengths.Anthropic for example talk about the model&#x27;s ability to spot words in the entire Great Gatsby novel loaded into context. It&#x27;s a hint to how the model is trained.Parameter counts are a unified metric, what seems to be important is embedding dimensionality to transfer information through the layers - and the layers themselves to both store and process the nuance of information. reply danielmarkbruce 13 hours agorootparentprevIt&#x27;s an order of magnitude comparison.Let&#x27;s just agree it&#x27;s 100x-300x more parameters, and let&#x27;s assume the open ai folks are pretty smart and have a sense for the optimal number of tokens to train on. reply razodactyl 13 hours agorootparentThis definitely. Andrej Karpathy himself mentions tuned weight initialisation in one of his lectures. The TinyGPT code he wrote goes through it.Additionally explanations for the raw mathematics of log likelihoods and their loss ballparks.Interesting low-level stuff. These researchers are the best of the best working for the company that can afford them working on the best models available. reply visarga 11 hours agorootparentprevUsually the 7B model is fine-tuned with \"enriched\" data, \"textbook quality\" generations from the 1.8T model. Riding on its coat tails. reply tannhaeuser 12 hours agorootparentprevThat&#x27;s my take-away from limited attempts to get Code Llama2 Instruct to implement a moderately complex spec as well, using special INST and SYS tokens even or just pasting some spec text along in a 12k context when Code Llama2 supposedly can honor up to 100k tokens. And I don&#x27;t even know how to combine code infilling with an elaborate spec text exceeding the volume of what normally goes into code comments. Is ChatGPT 4 really any better? reply stavros 2 hours agoparentprevWhat do you mean \"orders of magnitude above\" for DALL-E? As far as I can see, Midjourney is $0.05 per image, and that&#x27;s if you don&#x27;t forget you have a subscription. I&#x27;ve ended up paying $10 per image. reply davidbarker 14 hours agoparentprevAlso, DALL·E 3 \"HD\" is double the price at $0.08. I&#x27;m curious to play around with it once the API changes go live later today.The docs say:> By default, images are generated at standard quality, but when using DALL·E 3 you can set quality: \"hd\" for enhanced detail. Square, standard quality images are the fastest to generate.https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;images&#x2F;usage reply Terretta 11 hours agoparentprev> most of the products announced (and the price cuts) appear to be more about increasing lock-in to the OpenAI API platformOpenAI is currently refusing far more enterprises than these products could \"lock-in\" even with 100% stickiness.Makes it unlikely this is about lock-in or fighting churn when arguably, the best advertisement for GPT-4 is comparing its raw results to any other LLM.If you said their goal was fomenting FOMO, I&#x27;d buy it. Curious, though, when they&#x27;ll let the FOMO fulfillment rate go up by accepting revenue for servicing that demand. reply vsareto 14 hours agoparentprev>The GPTs&#x2F;GPT Agents and Assistants demos in particular showed that they are a black box within a black box within a black box that you can&#x27;t port anywhere else.This just rings hollow to me. We lost the fights for database portability, cloud portability, payments&#x2F;billing portability, and other individual SaaS lock-in. I don&#x27;t see why it&#x27;ll be different this time around. reply kortilla 12 hours agorootparent> We lost the fights for database portability, cloud portability, payments&#x2F;billing portability, and other individual SaaS lock-in.No we didn’t. There are viable on-prem alternatives or cross cloud alternatives for everything popular on the cloud.Many companies did choose to hand their destiny over to cloud providers but lots didn’t. reply activescott 13 hours agoparentprevI think it&#x27;s more about finding places to add value than \"lock in\" per se. It seems they&#x27;re adding value with improved developer experience and cost&#x2F;performance rather than on the models themselves. Not necessarily nefarious attempts to lock in customers, but it may have the same outcome :) reply spankalee 15 hours agoparentprevA friend of mine is building Zep (https:&#x2F;&#x2F;www.getzep.com&#x2F;), which seems to offer a lot of the Assistant + Retrieval functionality in a self-hostable and model-agnostic way. That type of project may the way around lock-in. reply Vipsy 6 hours agoparentprevAnything open about OpenAI starts and ends with the name reply openquery 8 hours agoprevIf I had no contact with society from the 29th of November 2022 (the day before ChatGPT was released according to Wikipedia) and came back today to see the OpenAI keynote I would have lost my mind.The progress and usefulness of these products is absolutely incredible. reply qingcharles 8 hours agoparentI was in prison when ChatGPT came out. All I knew of it was a headline that flashed past really fast on CNN and I called my buddy and said \"What the hell is Chat OPT?\"I&#x27;d just finished reading The Singularity is Near for the second time too... reply freedomben 4 hours agorootparentThe singularity is near is a great book. Hilarious that you read that for the second time and then got out and saw chat gpt!I love kurzweil but his estimates of timeline are often pretty over optimistic, so I&#x27;d be really wondering. reply cubefox 1 hour agorootparentOn Metaculus the arrival for weakly general AI was predicted for 2045 two years ago. Now it&#x27;s at 2026.https:&#x2F;&#x2F;www.metaculus.com&#x2F;questions&#x2F;3479&#x2F;date-weakly-general... reply ssnistfajen 5 hours agoparentprevI remember opening Twitter that night and seeing a bunch of tech people I follow sharing screenshots of conversations with a little green icon. I thought \"oh neat, yet another chatbot fad to try out for 5 minutes\". I could not have been more wrong. reply torginus 3 hours agoparentprevI&#x27;m sorry, what breakthrough feature did we see here?- Code interpreter, function calling were already possible on any sufficiently advanced LLM that could follow instructions well enough to output tokens in a rigidly parseable format, which could then be fed into a parser, and its output fed back to the LLM. It was clunky to do with online APIs like ChatGPT, but still eminently possible.- Custom chatbots were easy to build before, and services to build them (like Poe.com) existed before.- Likewise outputting JSON just requires a good instruction following AI, that can output token probabilities, along with a schema validator that always picks a token that results in schema-conforming JSON- GPT4-128k seems to be revolutionary, but Claude-100k already existed, and considering LLM evaluation is quadratic wrt context size, they are probably using some tricks to extend the context, they are not &#x27;full&#x27; tokens (I&#x27;d be happy to be proven wrong). While having a huge context is useful, for coding, a 8k context can be enough with some elbow grease (like filling the context with a 2-3 deep recursive &#x27;Go To Definition&#x27; for a given symbol), so that the AI receives the right context.- Dall-E 3 seems to be the most revolutionary, but after playing with it, it has much improved compositional ability over SD, but it&#x27;s still prone to breakdownsOverall I feel like todays announcements were polish and refinement over last year&#x27;s bombshell breakthroughs. reply reissbaker 25 minutes agorootparentThe most exciting announcements for me were:* GPT-4-128k. Sure, Claude exists, but it&#x27;s closer to GPT-3.5 than 4 IMO. TBD how well 128k context works given that classic attention scales quadratically (so they&#x27;re presumably using something else) but given how good OpenAI&#x27;s models tend to be I&#x27;m willing to give them the benefit of the doubt.* Pushed GPT-3.5 finetuning to 16k context (up from 4k when it was released this summer). IME 3.5 finetunes are very useful, very fast, very cheap replacements for specific specialized tasks over GPT-4, and easily outperform GPT-4 for the right kind of tasks. The 4k context limit was a bit of a bummer.* New tts that to my ears sounds nearly equivalent to Eleven Labs or Play.ht, at one-tenth to one-twentieth the price (with zero monthly commitment). The Eleven Labs Discord is a bit of a bloodbath right now, most of the general chat is just people saying they&#x27;re switching. (The Play.ht Discord is pretty dead most of the time anyway, so not much new since this morning.) I will say though that it&#x27;s a bummer that the OpenAI tts doesn&#x27;t have input streaming, only output streaming, so latency will likely be worse and you&#x27;ll have to figure out some way to do chunking yourself which is fairly annoying, but for any kind of personalized use case (e.g. a bot talking to customers, as opposed to using pre-recorded snippets) a 10-20x price improvement is worth the extra pain and may be the difference between \"neat prototype\" and \"shippable to production.\"Plus, massive price drops for OpenAI&#x27;s existing products across the board, along with a legal defense fund to protect OpenAI customers from getting sued for using OpenAI models. If you&#x27;re building an \"OpenAI wrapper startup,\" today was a very good day. If you&#x27;re competing with OpenAI, though... Oof. reply awestroke 2 hours agorootparentprevYou&#x27;re completely ignoring image analysis. reply torginus 2 hours agorootparentsorry, missed that one, yeah, that&#x27;s new as well, but to be fair, I missed it because nobody else was talking about it. reply yen223 2 hours agoparentprevDoes it boggle anyone else&#x27;s mind that ChatGPT was released less than a year ago? It definitely feels like it was around lot longer than that. reply gardenhedge 2 hours agorootparentAnd there&#x27;s people (most people) who aren&#x27;t using it reply byteflip 5 hours agoparentprevI&#x27;ve been in contact with society and I&#x27;m still losing my mind. reply modeless 15 hours agoprevWhisper V3 is released! https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;whisper&#x2F;commit&#x2F;c5d42560760a05584c1...Looks like it&#x27;s just a new checkpoint for the large model. It would be nice to have updates for the smaller models too. But it&#x27;ll be easy to integrate with anything using Whisper V2. I&#x27;m excited to add it to my local voice AI (https:&#x2F;&#x2F;www.microsoft.com&#x2F;store&#x2F;apps&#x2F;9NC624PBFGB7)I assume ChatGPT voice has been using Whisper V3 and I&#x27;ve noticed that it still has the classic Whisper hallucinations (\"Thank you for watching!\"), so I guess it&#x27;s an incremental improvement but not revolutionary. reply dang 14 hours agoparentRelated:OpenAI releases Whisper v3, new generation open source ASR model - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38166965 reply ianbicking 15 hours agoparentprevDo you also get those hallucinations just on silence?I kind of wonder if they had a bunch of training data of video with transcripts, but some of the video&#x2F;audio was truncated and the transcript still said the last speech, and so now it thinks silence is just another way of signing off from a TV program.IMHO the bottleneck on voice now is all the infrastructure around it. How do you detect speech starting and stopping? How do you play sound&#x2F;speech while also being ready for the user to speak? This stuff is necessary, but everything kind of works poorly, and you really need hardware&#x2F;software integration. reply modeless 14 hours agorootparentYou&#x27;re right, I think that&#x27;s exactly what happened.Silence is when you get the most hallucinations. But there is a trick supported by some implementations that helps a lot. Whisper does have a specialtoken that it predicts for silence. You can look at the probability of that token even when it&#x27;s not picked during sampling. Hallucinations often have a relatively high probability for the nospeech token compared to actual speech, so that can help filter them out.As for all the surrounding stuff like detecting speech starting and stopping and listening for interruptions while talking, give my voice AI a try. It has a rough first pass at all that stuff, and it needs a lot of work but it&#x27;s a start and it&#x27;s fun to play with. Ultimately the answer is end-to-end speech-to-speech models, but you can get pretty far with what we have now in open source! reply ianbicking 10 hours agorootparentThis is what you mean? https:&#x2F;&#x2F;apps.microsoft.com&#x2F;detail&#x2F;9NC624PBFGB7But I don&#x27;t have Windows :( reply modeless 9 hours agorootparentI plan to do a Linux build when I have time and release it as a Snap or Flatpak or Appimage or something. But it relies on an Nvidia GPU, so I can&#x27;t release a macOS version right now. I could probably put something together with llama.cpp and whisper.cpp but it wouldn&#x27;t be as fast, even on Apple&#x27;s best. I&#x27;ll probably do it eventually along with AMD support using ROCm or Vulkan, but I have a whole lot of other things to get to first and very limited time to work on it.Right now my target is people with high end gaming PCs, because they can have a really good experience with the right software but most AI stuff is ridiculously hard to install. My goal is one click install with no required dependencies. reply azinman2 11 hours agorootparentprevSo why doesn&#x27;t the model score that higher then? I&#x27;m guessing there&#x27;s an inherent trade off and they picked&#x2F;trained it with enough silence vs non-silence? reply modeless 9 hours agorootparentLow quality training data, almost certainly. reply prabhasp 6 hours agoparentprevLove that Sama literally spent 16 seconds of a 45minute announcement on whisper - https:&#x2F;&#x2F;app.reduct.video&#x2F;o&#x2F;eca54fbf9f&#x2F;p&#x2F;250fab814f&#x2F;share&#x2F;9d9... reply petesergeant 4 hours agoparentprevDoes it have diarisation yet? reply RockRobotRock 1 hour agorootparentCheck out WhisperX reply petesergeant 9 minutes agorootparentThanks! reply Void_ 14 hours agoparentprevToo bad they didn&#x27;t upgrade Whisper API yet. Can&#x27;t wait to make it available in https:&#x2F;&#x2F;whispermemos.com reply dotancohen 3 hours agorootparentIf you add an Android version that I could activate from my lock screen, you&#x27;ll have another customer. reply topicseed 15 hours agoprev128,000 token context, Assistants API, JSON mode, April 2023 knowledge cutoff, GPT 4 Turbo, lower pricing, custom GPTs, a good bunch of announcements all-round!https:&#x2F;&#x2F;openai.com&#x2F;pricing reply Alifatisk 2 hours agoparentI thought GPT-4 had access to internet now? reply jeppebemad 1 hour agorootparentThe “browse with bing” feature allows it to fetch a single webpage into the context, but the new cutoff allows _everything crawled_ to be context (up to the new date, that is) reply qup 1 hour agorootparentprevPer the announcement, the \"GPTs\" do, natively.I think everyone else had been hacking it on via \"functions\" reply saliagato 13 hours agoprevYou can now [1] pay from $2 to $3 million to pretrain custom gpt-n model. This has gone unnoticed but seems really neat. Provided that a start-up has enough money spend on that, it would certainly give competitive advantage.[1] https:&#x2F;&#x2F;openai.com&#x2F;form&#x2F;custom-modelsEdit: forgot to put the link reply MagicMoonlight 12 hours agoparentWell it won’t because they’ll use the model you paid for and take your customers. reply govg 11 hours agorootparentIs it the same as avoiding AWS because they will take your software and run it themselves to steal your clients? reply constantly 10 hours agorootparentIt’s more like running a PaaS product backed by AWS and then your customers realizing they can just use AWS directly, pay less, and have less complexity. And they have done this before for what it’s worth. reply lemmsjid 9 hours agorootparentThe assumption behind why you&#x27;d use the program is that you have access to a proprietary dataset of sufficient size to build a large model around (say, for example, call center transcripts). This almost certainly means OpenAI doesn&#x27;t have access to train their other models off that data, and it almost certainly means your customers can&#x27;t take the same data and go straight to OpenAI.I&#x27;m assuming that the target customer of this is people whose moat is proprietary data. If their moat is a unique approach to building a model, then it would indeed be dangerous to engage OpenAI. But then I&#x27;d think OpenAI would be hesistent to engage as well. reply thisgoesnowhere 10 hours agorootparentprevThis hasn&#x27;t happened often, but it has happened. Elastic search for example.Also dynamo db. reply dotancohen 4 hours agorootparentThis is misleading - intentionally using the incorrect definitions of the words in the parent post to construe a lie that plausibly addresses the concern when read by somebody unfamiliar with the situation.AWS took an open source project (Elastic) and forked it. They did not take an AWS customer&#x27;s code. reply somsak2 11 hours agorootparentprevHow do you square this with OpenAI&#x27;s assertion that they never use data from enterprise customers for their own training? Are you suggesting they&#x27;re lying? reply cornholio 6 hours agorootparentOpenAI just slurped the entire internet to train their main model, and the world just looks on as they directly compete with and disrupt authors the globe over.Whoever thinks they are not interested in your data and won&#x27;t use any trick to get it, then double down on their classic \"but your honor, it&#x27;s not copyright theft, the algorithm learns just like an employee exposed to the data would\", isn&#x27;t paying attention. reply jprete 5 hours agorootparentThis is exactly why I am personally intensely opposed to treating ML training as fair use. Practically speaking the argument justifies ignoring anyone or any group’s preference not to contribute to ML training, so it’s a massive loss of freedom to everyone else. reply Heyso 2 hours agorootparentprevI agree with you. What come to my mind, is that GPT using private data to learn, if given back to (any) customer, you would have an indirect \"open source everything\". reply JacobThreeThree 9 hours agorootparentprevThey don&#x27;t have to be currently lying for this to be a valid concern.Clauses in terms of service are routinely updated or removed. reply ethbr1 9 hours agorootparent> Clauses in terms of service are routinely updated or removed.True, but that plays a bit differently in B2B land, because your customers also have legal teams and law firms on retainer. reply jprete 5 hours agorootparentNo-one seems to have sued Unity over an even more egregious set of EULA changes - claiming that users retroactively owed money on games developed and published under totally different terms. reply infecto 7 hours agorootparentprevDo you have proof or are you just throwing baseless accusations out there? reply teaearlgraycold 5 hours agorootparentprevNo. The downside is you spent a lot of money on a model and don’t own it. reply zizee 12 hours agoprevIn people&#x27;s experience with these sorts of tools, have they assisted with maintainance of codebases? This might be directly, or indirectly via more readable, bette organized code.The reason I ask is that these tools seem to excel in helping to write new code. In my experience I think there is an upper limit to the amount of code a single developer can maintain. Eventually you can&#x27;t keep everything in your head, so maintaining it becomes more effort as you need to stop to familiarize yourself with something.If these tools help to write more code, but do not assist with maintainance, I wonder if we&#x27;re going to see masses of new code written really quickly, and then everything grinds to a halt, because no one has an intimate understanding of what was written? reply anotherpaulg 10 hours agoparentMy open source ai coding tool aider is unique in that it is designed to work with existing code bases. You can jump into an existing git repo and start aaking for changes, new features, etc.https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aiderIt helps gpt understand larger code bases by building a \"repository map\" based on analyzing the abstract syntax tree of all the code in the repo. This is all built using tree-sitter, the same tooling which powers code search and navigation on GitHub and in many popular IDEs.https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;repomap.html reply anotherpaulg 5 hours agorootparentRelated to the OpenAI announcement, I&#x27;ve been able to generate some preliminary code editing evaluations of the new GPT models. OpenAI is enforcing very low rate limits on the new GPT-4 model. I will update the results as quickly my rate limit allows.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38172621Also, aider now supports these new models, including `gpt-4-1106-preview` with the massive 128k context window.https:&#x2F;&#x2F;github.com&#x2F;paul-gauthier&#x2F;aider&#x2F;releases&#x2F;tag&#x2F;v0.17.0 reply extr 4 hours agorootparentDude I love how passionate you are about this project. I see you in every GPT thread. Despite all this tech there are so few projects out there trying to make large-repo code editing&#x2F;generation possible. reply stavros 1 hour agorootparentprevI do love aider, thanks for making it! I&#x27;d like an option to stop it from writing files everywhere, though, even if that means I have no history. reply jeswin 4 hours agorootparentprev> It helps gpt understand larger code bases by building a \"repository map\" based on analyzing the abstract syntax tree of all the code in the repo.What I do with codespin[1] (another AI code gen tool) is to give a file&#x2F;files to GPT and ask for signatures (and comments and maybe autogenerate a description), and then cache it until the file changes. For a lot of algorithmic work, we could just use GPT now. Sure it&#x27;s less efficient, but as these costs come down it matters less and less. In a way, it&#x27;s similar to higher level (but inefficient) programming languages vs lower level efficient languages.[1]: https:&#x2F;&#x2F;github.com&#x2F;codespin-ai&#x2F;codespin-cli reply NikhilVerma 3 hours agorootparentprevHow do you manage token limits when sending large amounts of code structure to OpenAI? reply mcbishop 3 hours agorootparentprevThanks a lot for doing this project. Your blog post got me excited. reply jillesvangurp 1 hour agoparentprevThere&#x27;s a nice code gpt plugin for intellij and vs code. Basically you can select some code and ask it to criticize it, refactor it, optimize it, find bugs in it, document it, explain it, etc. A larger context means that you can potentially fit your entire code base in that. Most people struggle to keep the details in their head of even a small code base.The next level would be deeper integration with tools to ensure that whatever it changes, the tests still have to pass and the code still has to compile. Speaking of tests, writing those is another thing it can do. So, AI assisted salvaging of legacy code bases that would otherwise not be economical to deal with could become a thing.What we can expect over the next years is a lot more AI assisted developer productivity. IMHO it will perform better on statically typed languages as those are simply easier to reason about for tools. reply gen220 10 hours agoparentprev> If these tools help to write more code, but do not assist with maintainance, I wonder if we&#x27;re going to see masses of new code written really quickly, and then everything grinds to a halt, because no one has an intimate understanding of what was written?Yep. Companies using LLMs to \"augment\" junior developers will get a lot of positive press, but I guess it remains to be seen how much the market consistently rewards this behavior. Consumers will probably see right through it, but the b2b folks might get fleeced for a few years before eventually churning and moving to a higher quality old-fashioned competitor that employs senior talent.But IDK, maybe we&#x27;ll come up with models that are good at growing and maintaining a coherent codebase. It doesn&#x27;t seem like an impossible task, given where we are today. But we&#x27;re pretty far from it still, as you point out. reply bertil 7 hours agorootparentWhat are the tasks that you envision are key to maintenance?- bug finding and fixing- parsing logs to find optimisation options- refactoring (after several local changes)- given new features, recommending a refactoring?I feel like code assistants are already reasonable help for doing the first two, and the later two are mostly a question of context window. I feel we might end up with code bases split by context sizes, stitched with shared descriptions. reply throw2321 11 hours agoparentprevI&#x27;ve been thinking about this for a while now, wrt two points:1. This will be the end of traditional SWEs and the rise of the age of debuggers, human debuggers who spend their days setting up breakpoints and figuring bugs in a sea of LLM generated code.2. Hiring will switch from using Leetcode questions to \"pull out your debugger and figure out what&#x27;s wrong with this code\". reply w-m 11 hours agorootparentWhat makes you think the LLM couldn’t run a debugging session from the content of a JIRA ticket and the whole code base + documentation? reply mike_hearn 6 minutes agorootparentWho can say what&#x27;s possible but there are very few \"debugging transcripts\" for neural nets to train on out there. So it&#x27;d have to sort of work out how to operate a debugger via functions, understand the codebase (perhaps large parts of it), intuit what&#x27;s going wrong, be able to modify the codebase, etc. Lots of work to do there. reply eichin 11 hours agorootparentprevHaving never seen it, or anything even close to it. (Of course, I&#x27;m a little biased by seeing product demos that don&#x27;t even get \"add another item to this list of command line arguments\" right; maybe if everyone already believes it works, nobody bothers to actually sell that?) reply spookie 7 hours agorootparentprevIf the codebase is anything more than a simple Python project... I don&#x27;t think that&#x27;ll happen.It just doesn&#x27;t scale that well. Hell, GPT-4 can&#x27;t make sense of my own projects. reply eichin 4 hours agorootparentprevThe first interview where I was handed some (intentionally) broken C code and gdb was about 15 years ago. I&#x27;m not sure that part is a change in developer workflow (this may apply more in systems and embedded though.)I&#x27;ve been paying attention to this too (mostly by following Simon Willison) and I&#x27;m still solidly in the \"get back to me when this stuff can successfully review a pull request or even interpret a traceback\" camp... reply ushakov 11 hours agoparentprevWe are doing this for API-Testing now. You should check out our websitehttps:&#x2F;&#x2F;ai.stepci.com reply somsak2 11 hours agorootparentpiece of feedback: it&#x27;s weird to have a drop-down on \"OpenAPI Links\" when there are no other options. reply ushakov 11 hours agorootparentThanks! We will have more examples coming very soon reply jack_riminton 12 hours agoprevFor all the naysayers in the comments, the elephant in the room that no one quite wants to admit, is that GPT4 is still far better than everything else out there reply nmfisher 9 hours agoparentI cancelled my GPT4 subscription because I found Claude more useful for code and Qwen for Chinese language tasks.It might be better on average but I don’t think it’s better for every task.All the others are only going to get better too. reply qingcharles 8 hours agorootparentClaude is superior for me on writing summaries of large documents. reply unshavedyak 8 hours agorootparentprevCan you go into depth? I’ve used ChatGPT Pro and Phind extensively, didn’t know about Claude and code. Curious to give it a try reply nmfisher 7 hours agorootparentI generally use it for boilerplate tasks like “here’s some code, write unit tests” or “here’s a JSON object, write a model class and parser function”.Claude is significantly faster, so even if it requires a couple more prompt iterations than GPT4, I still get the result I need earlier than with GPT4.GPT4 also recently developed this annoying tendency to only give you one or two examples of what you asked for, then say “you can write the rest on your own based on this template”. I can’t overstate how annoying this was. reply icelancer 1 hour agorootparent> GPT4 also recently developed this annoying tendency to only give you one or two examples of what you asked for, then say “you can write the rest on your own based on this template”. I can’t overstate how annoying this was.The last model \"update\" has really ruined GPT-4 in this regard. reply dri_ft 52 minutes agorootparentWhen was this? I noticed chatGPT becoming succinct almost to the point of being standoffish about a week or two ago. Probably exacerbated by my having some custom instructions to tame its prior prolixity. reply Alifatisk 2 hours agorootparentprevImagine being told by an ai to do it yourself, hilarious reply icelancer 5 hours agoparentprevPhind is the only thing that I&#x27;ve supplemented GPT-4 with, which is still pretty impressive. reply mezeek 11 hours agoparentprevGrok? Just kidding reply kossTKR 11 hours agoparentprevIs there anything promising out there?Is crowd sourced training still unfeasible?I remember how fast the diffusion world moved in the first year but it seems it&#x27;s stalled somewhat compared to first midjourney then Dall-e 3. Is it the same with text models? reply bugglebeetle 11 hours agorootparentGPT-4 is the best general model and specifically very good at coding, if correctly promoted. Lots of open source stuff is good at various tasks (e.g. NLP stuff), but nothing is near to the same overall level of performance. reply Zaheer 15 hours agoprevThe playbook OpenAI is following is similar to AWS. Start with the primitives (Text generation, Image generation, etc &#x2F; EC2, S3, RDS, etc) and build value add services on top of it (Assistants API &#x2F; all other AWS services). They&#x27;re miles ahead of AWS and other competitors in this regard. reply somsak2 11 hours agoparentI don&#x27;t know if I&#x27;d say \"miles ahead.\" AWS had 7 years of basically no other competition -- all of the other big clouds of today had their heads in the sand. OpenAI has a bunch of people competing already. They may not be as good on the leaderboards now, but they&#x27;re certainly not having to play catch up from years of ignoring the space. reply gumballindie 15 hours agoparentprevAnd just like amazon they will compete with their own customers. They are miles ahead in this regard as well since they basically take everyone’s digital property and resell it. reply dave1010uk 11 hours agorootparentThis is essentially the \"Innovate - Leverage - Commoditise\" strategy, which Simon Wardley (as in Wardley Mapping) explains:https:&#x2F;&#x2F;blog.gardeviance.org&#x2F;2014&#x2F;03&#x2F;understanding-ecosystem... reply sharemywin 13 hours agorootparentprevdon&#x27;t hate the player hate the game. reply jprete 5 hours agorootparentWhy not both? reply charlie0 12 hours agorootparentprevand if you can&#x27;t beat them, join them. reply gwern 14 hours agoprev> We’re also launching a feature to return the log probabilities for the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo in the next few weeks, which will be useful for building features such as autocomplete in a search experience.This is very surprising to me. Are they not worried about people not just training on GPT-4 outputs to steal the model capabilities, but doing full blown logit knowledge-distillation? (Which is the reason everyone assumed that they disabled logit access in the first place.) reply leobg 13 hours agoparentHow many GBs worth of logits would you need to reverse engineer their model? Also, if it’s a conglomerate of models that they’re using, you’d end up in a blind alley. reply gwern 9 hours agorootparentConsidering how well simply reusing GPT-3.5&#x2F;4 outputs has worked to juice rival model performance, at least in relatively narrow benchmarking, I dunno how many GBs it&#x27;d take, but probably not that many, and it&#x27;s a straightforward easy way to turn money into performance at a much lower cost than buying a few thousand more H100s. reply leobg 4 hours agorootparentOpenAI does not strike me as a company that would be naive about this. Didn’t they just recently manipulate the outputs of an endpoint when they realized people were misusing it? (“CatGPT”)The most sinister interpretation is that the logits are a red herring. People who are tied up in stealing them aren’t free to do actual rival work. reply joegibbs 1 hour agorootparentWhat happened with CatGPT? reply danielmarkbruce 13 hours agoparentprevI thought the same thing.... My guess is they did a lot of analysis and decided it would be safe enough to do? \"most likely\" might be literally a handful and cover little of the entire distribution % wise? reply nwoli 12 hours agoparentprevI guess the EO takes care of that in their eyes (outlawing open models). They’re probably right too reply dragonwriter 9 hours agorootparent> I guess the EO takes care of that in their eyes (outlawing open models).The EO doesn&#x27;t do anything even approximately like outlawing open models. reply nwoli 8 minutes agorootparentIt basically bans models that can be finetuned to act as a biological weapon which is basically every model, llama, mistral etc reply simonw 12 hours agoprevI just released a new version of my LLM CLI tool with support for the new GPT-4 Turbo model: https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;changelog.html#v0-12You can install it like this: pipx install llmThen set an API key: llm keys set openai Then run a prompt through GPT-4 Turbo like this: llm -m gpt-4-turbo \"Ten great names for a pet walrus\" # Or a shortcut: llm -m 4t \"Ten great names for a pet walrus\"Here&#x27;s a one-liner that summarizes all of the comments in this Hacker News conversation (taking advantage of the new long context length): curl -s \"https:&#x2F;&#x2F;hn.algolia.com&#x2F;api&#x2F;v1&#x2F;items&#x2F;38166420\"\\ jq -r &#x27;recurse(.children[]).author + \": \" + .text&#x27;\\ llm -m gpt-4-turbo &#x27;Summarize the themes of the opinions expressed here, including direct quotes in quote markers (with author attribution) for each theme. Fix HTML entities. Output markdown. Go long.&#x27;Example output here: https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;d50c8634320d339bd88f0ef17dea0... reply eurekin 11 hours agoparentGreat tool and example! Makes me wonder, what one can do more with it reply Michelangelo11 8 hours agoparentprevJesus. Yeah, considering the input size, this is a pretty good sign that the 128k context window is working decently well. reply whytai 14 hours agoprevEvery day this video ages more and more poorly [1].categories of startups that will be affected by these launches:- vectorDB startups -> don&#x27;t need embeddings anymore- file processing startups -> don&#x27;t need to process files anymore- fine tuning startups -> can fine tune directly from the platform now, with GPT4 fine tuning coming- cost reduction startups -> they literally lowered prices and increased rate limits- structuring startups -> json mode and GPT4 turbo with better output matching- vertical ai agent startups -> GPT marketplace- anthropic&#x2F;claude -> now GPT-turbo has 128k context window!That being said, Sam Altman is an incredible founder for being able to have this close a watch on the market. Pretty much any \"ai tooling\" startup that was created in the past year was affected by this announcement.For those asking: vectorDB, chunking, retrieval, and RAG are all implemented in a new stateful AI for you! No need to do it yourself anymore. [2] Exciting times to be a developer![1] https:&#x2F;&#x2F;youtu.be&#x2F;smHw9kEwcgM[2] https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-models-and-developer-products-an... reply morkalork 14 hours agoparentIf you want to be a start-up using AI, you have to be in another industry with access to data and a market that OpenAI&#x2F;MS&#x2F;Google can&#x27;t or won&#x27;t touch. Otherwise you end up eaten like above. reply Cali_cramoisie 4 hours agorootparentOr you can treat what OpenAI is doing like a commodity like AWS and leverage it to solve a meaningful problem. reply dragonwriter 9 hours agorootparentprev> a market that OpenAI&#x2F;MS&#x2F;Google can&#x27;t or won&#x27;t touch.But also one that their terms of service, which are designed to exclude the markets that they can&#x27;t or won&#x27;t touch, don&#x27;t make it impractical for you to service with their tools. reply Rastonbury 5 hours agorootparentprevEven if you aren&#x27;t eaten, the use case will just be copied and run on the same OpenAI models by competitors, having good prompts is not good enough a moat. They win either way reply ushakov 14 hours agorootparentprevWe just launched our AI-based API-Testing tool (https:&#x2F;&#x2F;ai.stepci.com), despite having competitors like GitHub Co-Pilot.Why? Because they lack specificity. We&#x27;re domain experts, we know how to prompt it correctly to get the best results for a given domain. The moat is having model do one task extremely well rather than do 100 things \"alright\" reply parkerhiggins 13 hours agorootparentDomain specialization could be the moat, not only in the business domain but the sheer cost of deployment&#x2F;refinement.Check out Will Bennett&#x27;s \"Small language models and building defensibility\" - https:&#x2F;&#x2F;will-bennett.beehiiv.com&#x2F;p&#x2F;small-language-models-and... (free email newsletter subscription required) reply vunderba 9 hours agorootparentprevIf the primary value-proposition for your startup is just customized prompting with OpenAI endpoints, then unfortunately it&#x27;s highly likely it could be easily replicated using the newly announced concept of GPTs. reply esafak 13 hours agorootparentprevIf you just launched it is too soon to speak. reply ushakov 13 hours agorootparentOf course! Today our assumption is that LLMs are commodities and our job is to get the most out of them for the type of problem we&#x27;re solving (API Testing for us!) reply darkwater 14 hours agorootparentprevSorry to be blunt but they can be totally right, if you do not succeed and have to shut down your startup. reply ushakov 13 hours agorootparentIt certainly will be a fun experience. But our current belief is that LLMs are a commodity and the real value is in (application-specific) products built on top of them. reply Rastonbury 5 hours agorootparentExactly, everyone is so pessimistic but for every AWS sku there is a billion dollar startup that leads that market. reply sharemywin 13 hours agorootparentprevTime will tell reply renewiltord 13 hours agorootparentprevWriter.ai is quite successful, and is totally in another industry that Google+MS participate in. reply ren_engineer 14 hours agoparentprevdepends on how much developers are willing to embrace the risk of building everything on OpenAI and getting locked onto their platform.What&#x27;s stopping OpenAI from cranking up the inference pricing once they choke out the competition? That combined with the expanded context length makes it seem like they are trying to lead developers towards just throwing everything into context without much thought, which could be painful down the road reply klabb3 12 hours agorootparent> depends on how much developers are willing to […] getting locked onto their platform.I mean.. the lock in risks have been known with every new technology since forever now, and not just the risk but the actual costs are very real. People still buy HP printers with InkDRM and companies willingly write petabytes of data into AWS that they can’t even afford to egress at current prices.To be clear, I despise this business practice more than most, but those of us who care are screaming into the void. People are surprisingly eager to walk into a leaking boat, as long as thousands of others are as well. reply ky0ung 8 hours agorootparentCombination of 1) short-term business thinking (save $1 today = $1 more of EPS) and 2) fear of competition building AI products and taking share. thus rush to use first usable platform (e.g. openAI).Psychology and FOMO plays interesting role in walking directly into a snake pit. reply keithwhor 13 hours agorootparentprevI suspect it is in OpenAI&#x27;s interest to have their API as a loss leader for the foreseeable future, and keep margins slim once they&#x27;ve cornered the market. The playbook here isn&#x27;t to lock in developers and jack up the API price, it&#x27;s the marketplace play: attract developers, identify the highest-margin highest-volume vertical segments built atop the platform, then gobble them up with new software.They can then either act as a distributor and take a marketplace fee or go full Amazon and start competing in their own marketplace. reply vikramkr 5 hours agorootparentprevi mean sure it&#x27;s lock in, but it&#x27;s lock in via technical superiority&#x2F;providing features. Either someone else replicates a model of this level of capability or anyone who needs it doesn&#x27;t really have a choice. I don&#x27;t mind as much when it&#x27;s because of technical innovation&#x2F;feature set (as opposed to through your usual gamut of non-productive anti-competitive actions). If I want to use that much context, that&#x27;s not openAIs fault that other folks aren&#x27;t matching it - they didn&#x27;t even invent transformers and it&#x27;s not like their competitors are short on cash. reply stuckkeys 12 hours agorootparentprevReminds me of that sales entrapment approach from cloud providers. “Here is your free $400, go do your thing” next thing you know you have build so much on there already that it is not worth the time and effort to try and allocate it regardless of the 2k bill increase -haha. Good times. reply larodi 13 hours agoparentprevWell, if said startups were visionaries, the could&#x27;ve known better the business they&#x27;re entering. On the other hand - there are plenty of VC-inflated balloons, making lots of noise, that everyone would be happy to see go. If you mean these startups - well, farewell.There&#x27;s plenty more to innovate, really, saying OpenAI killed startups it&#x27;s like saying that PHP&#x2F;Wordpress&#x2F;NameIt killed small shops doing static HTML. or IBM killing the... typewriter companies. Well, as I said - they could&#x27;ve known better. Competition is not always to blame. reply karmasimida 13 hours agoparentprevTBH those are low-hanging fruits for OpenAI. Much of the value still being captured by OpenAI&#x27;s own model.The sad thing is, GPT-4 is its own league in the whole LLM game, whatever those other startups are selling, it isn&#x27;t competing with OpenAI. reply blibble 13 hours agoparentprevHN is quite notorious for that Dropbox commentI suspect that video is going to end up more notorious, it&#x27;s even funnier given it&#x27;s the VCs themselves reply thisgoesnowhere 10 hours agorootparentI&#x27;m firmly in the camp that in a vacuum that comment looks dumb but the thread was actually great.Those were valid concerns at the time and the market for non technical file storage like they were building was non existant.Perfectly rational to be skeptical and Drew answered all his questions with well thought out responses. reply arcanemachiner 13 hours agorootparentprevMore context, please.EDIT: I guess it&#x27;s this:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=8863#9224 reply blibble 13 hours agorootparentthat&#x27;s the one reply riku_iki 13 hours agoparentprev> - vectorDB startups -> don&#x27;t need embeddings anymorethey don&#x27;t provide embedings, but storage and query engines for embeddings, so still very relevant> - file processing startups -> don&#x27;t need to process files anymorecurious what is that exactly?..> - vertical ai agent startups -> GPT marketplacesure, those startups will be selling their agents on marketplace reply dragonwriter 9 hours agorootparent> they don&#x27;t provide embedings, but storage and query engines for embeddings, so still very relevantBut you don&#x27;t need any of the chain of: extract data, calculate embeddings, store data indexed by embeddings, detect need to retrieve data by embeddings and stuff it into LLM context along with your prompt if you use OpenAI&#x27;s Assistants API, which, in addition to letting you store your own prompts and manage associated threads, also lets you upload data for it to extract, store, and use for RAG on the level of either a defined Assistant or a particular conversation (Thread.) reply visarga 11 hours agorootparentprevIt&#x27;s easy to host your query engine somewhere else and integrate it as a search function in chatGPT. Quite easy to switch providers of search. reply obmelvin 11 hours agorootparentAs in, use an existing search and call it via &#x27;function calling&#x27; as part of the assistants routine - rather than uploading documents to the assistant API? reply make3 13 hours agorootparentprevthey definitely do provide embeddings, https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-models-and-developer-products-an... ctrl+f retrieval, \"... won&#x27;t need to ... compute or store embeddings\" reply riku_iki 13 hours agorootparentI mean embeddingsDB startups don&#x27;t provide embeddings. They provide databases which allows to store and query computed embeddings (e.g. computed by ChatGPT), so they are complimentary services. reply taf2 12 hours agorootparentYeah I still see a chat bot being able to look for related information in a database as useful. But I see it as just one of many tools a good chat experience will require. 128k context means for me there other applications to explore and larger tasks to accomplish with fewer api requests. Better chat history and context not getting lost reply lazzlazzlazz 14 hours agoparentprevEmbeddings are still important (context windows can&#x27;t contain all data + memorization and continuous retraining is not yet viable), and vertical AI agent startups can still lead on UX. reply dragonwriter 9 hours agorootparentSeparate embedding DBs are less important if you are working with OpenAI, since their Assistants API exists to (among other things) let you bring in additional data and let them worry about parsing it, storing it, and doing RAG with it. Its like \"serverless\", but for Vector DBs and RAG implementations instead of servers. reply Finbarr 13 hours agorootparentprevContext windows can&#x27;t contain all data... yet. reply andrewjl 7 hours agoparentprevNone of those categories really fall under the second order category mentioned in the video. Using their analogy they all sound more like a mapping provider versus something like Uber. reply bluecrab 13 hours agoparentprevVector DBs should never have existed in the first place. I feel sorry for the agent startups though. reply m3kw9 13 hours agorootparentHow does this absolve vectordbs reply dragonwriter 13 hours agorootparentIf you are using OpenAI, the new Assistants API looks like itnwill handle internally what you used to handle externally with a vector DB for RAG (and for some things, GPT-4-Turbo’s 128k context window will make it unnecessary entirely.) There are some other uses for Vector DBs than RAG for LLMs, and there are reasons people might use non-OpenAI LLMs with RAG, so there is still a role for VectorDBs, but it shrunk a lot with this. reply oezi 12 hours agorootparentOpenAI is still way too expensive to run a corporate knowledge base on top reply m3kw9 7 hours agorootparentIt’s more reliable than chatpdfs that relies on vector search. With vector db all you are doing is doing a fuzzy search and then sending in that relevant portion near that text and send it to a LLM model as part of a prompt. It misses info. reply dragonwriter 5 hours agorootparentI&#x27;d be very surprised if the Assistants API is not doing RAG with a vector DB behind the scenes with the supplied files. reply danielbln 13 hours agorootparentprevIt doesn&#x27;t, but semantic search is a lot less relevant if you can squeeze 350 pages of text into the context. reply quinncom 12 hours agorootparentOpenAI charges for all those input tokens. If an app requires squeezing 350 pages of content in every request is going to cost more. Vector DB still relevant for cost and speed. reply gk1 7 hours agorootparentprevBesides the cost factor, stuffing the context window can actually make the results worse. https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;why-use-retrieval-instead-of-la... reply Yadayadaaaa 12 hours agoparentprevJust because something is great doesn&#x27;t mean that others can&#x27;t compete. Even a secondary good product can easily be successful due to a company having invested too much, not being aware of openai (ai progress in general), due to some magic integration, etc.If it would be only me, no one would buy azure or aws but just gcp. reply yawnxyz 14 hours agoparentprevi&#x27;m excited for the open source, local inferencing tech to catchup. The bar&#x27;s been raised. reply mvkel 10 hours agoparentprevProbably best not to make your company about features that a frontier AI company would have a high probability of adding in the next 6-12 months. reply felixding 8 hours agoparentprevOfftopic. I find it&#x27;s amusing that we not only have \"chatGPT\" but now also \"vectorDB\". Apple&#x27;s influence is really strong. reply treprinum 11 hours agoparentprevThere is not much info about retrieval&#x2F;RAG in their docs at the moment - did you find any example on how is the retrieval supposed to work and how to give it access to a DB? reply bilsbie 13 hours agoparentprevWhy don’t you need embedding? reply monkeydust 11 hours agorootparentYou might. Depends what your trying to do. For RAG seems like they can &#x27;take care of it&#x27; but embeddings also offer powerful semantic search and retrieval ignoring LLMs. reply baq 13 hours agoparentprevChecking hn and product hunt a few times a week gives you most of that awareness and I don’t need to remind you about the person behind hn ‘sama’ handle. reply teaearlgraycold 12 hours agoparentprevI’ve been keeping my eye on a YC startup for the last few months that I interviewed with this summer. They’ve been set back so many times. It looks like they’re just “ball chasing”. They started as a chatbot app before chatgpt launched. Then they were a RAG file processing app, then enterprise-hosted chat. I lost track of where they are now but they were certainly affected by this announcement.You know you’re doing the wrong thing if you dread the OpenAI keynotes. Pick a niche, stop riding on OpenAI’s coat tails. reply seydor 11 hours agoparentprevmore startups should focus on foundation models, it&#x27;s where the meat is. Ideally there won&#x27;t be a need for any startup as the platform should be able to self-build whatever the customer wants. reply Der_Einzige 14 hours agoparentprevStartups built around actual AI tools, like if one formed around automatic1111 or oogabooga, would be unaffected, but because so much VC money went to the wrong places in this space, a whole lot of people are about to be burned hard. reply throwaway-jim 14 hours agorootparentdamn hahaha it&#x27;s oobabooga not oogabooga reply colordrops 14 hours agoparentprevI haven&#x27;t been paying attention, why are embeddings not needed anymore? reply sharemywin 13 hours agorootparentRetrieval: augments the assistant with knowledge from outside our models, such as proprietary domain data, product information or documents provided by your users. This means you don’t need to compute and store embeddings for your documents, or implement chunking and search algorithms. The Assistants API optimizes what retrieval technique to use based on our experience building knowledge retrieval in ChatGPT.The model then decides when to retrieve content based on the user Messages. The Assistants API automatically chooses between two retrieval techniques:it either passes the file content in the prompt for short documents, or performs a vector search for longer documents Retrieval currently optimizes for quality by adding all relevant content to the context of model calls. We plan to introduce other retrieval strategies to enable developers to choose a different tradeoff between retrieval quality and model usage cost. reply sjnair96 13 hours agorootparentReally cool to see the Assistants API&#x27;s nuanced document retrieval methods. Do you index over the text besides chunking it up and generating embeddings? I&#x27;m curious about the indexing and the depth of analysis for longer docs, like assessing an author&#x27;s tone chapter by chapter—vector search might have its limits there. Plus, the process to shape user queries into retrievable embeddings seems complex. Eager to hear more about these strategies, at least what you can spill! reply riku_iki 12 hours agorootparentprev> or performs a vector search for longer documentsso, clients upload all their docs to OpenAI database?.. reply karmasimida 12 hours agorootparentprevEmbedding is poor man&#x27;s context length increase. It essentially increases your context length but with loss.There is a cost argument to make still, embedding-based approach will be cheaper and faster, but worse result than full text.That being said, I don&#x27;t see how those embedding startups compete with OpenAI, no one will be able to offer better embedding than OpenAI itself. It is hardly a convincing business.The elephant in the room is the open source models aren&#x27;t able to match up to OpenAI models, and it is qualitative, not quantitive. reply estreeper 11 hours agorootparentFor embeddings specifically, there are multiple open source models that outperform OpenAI’s best model (text-embedding-ada-002) that you can see on the MTEB Leaderboard [1]> embedding-based approach will be cheaper and faster, but worse result than full textI’m not sure results would be worse, I think it depends on the extent to which the models are able to ignore irrelevant context, which is a problem [2]. Using retrieval can come closer to providing only relevant context.1. https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard2. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.00093 reply karmasimida 1 hour agorootparent> on the MTEB LeaderboardThe point isn&#x27;t about leaderboard. With increasing context length, the question is on whether we need embeddings or not. With longer context length, embeddings is no longer a necessity, and it lowers its value. reply lazzlazzlazz 14 hours agorootparentprevOP is incorrect. Embeddings are still needed since (1) context windows can&#x27;t contain all data and (2) data memorization and continuous retraining is not yet viable. reply zwily 13 hours agorootparentBut the common use case of using a vector DB to pull in augmentation appears to now be handled by the Assistants API. I haven&#x27;t dug into the details yet but it appears you can upload files and the contents will be used (likely with some sort of vector searching happening behind the scenes). reply nextworddev 13 hours agorootparentprev\"yet\" reply coding123 13 hours agorootparentIt&#x27;s also much slower. LLMs are generating text token at a time. That&#x27;s not very good for search.Pre-search tokenization however, probably a good fit for LLMs. reply emadabdulrahim 14 hours agorootparentprevI believe their API can be stateful now: https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;new-models-and-developer-products-an... reply atleastoptimal 14 hours agoparentprevThere will be a lot of startups who rely on marketing aggressively to boomer-led companies who don&#x27;t know what email is and hoping their assistant never types OpenAI into Google for them. reply cityzen 12 hours agoparentprevWhere is the part about embeddings? reply echelon 13 hours agoparentprevWe don&#x27;t want Open AI to win everything. reply zavertnik 15 hours agoprevAnd here I was in bliss with the 32k context increase 3 days ago. 128k context? Absolutely insane. It feels like now the bottle neck in GPT workflows is no longer GPT, but instead its the wallet!Such an amazing time to be alive. reply in3d 15 hours agoparentFor GPT-4 Turbo, not GPT-4. reply kridsdale3 13 hours agorootparentYes, nowhere in the text today was there any assertion that Turbo produces (eg) source code at the same level of coherence and consistently high quality as GPT4. reply weird-eye-issue 5 hours agorootparentThey specifically said it&#x27;s a better model reply bart_spoon 10 hours agorootparentprevAltman did specifically say it’s a “better model” than GPT4, but that’s “better” is vague enough that it might not actually be in terms of accuracy. reply somsak2 11 hours agorootparentprevWas there an assertion that it doesn&#x27;t? reply dragonwriter 14 hours agorootparentprevGPT-4-Turbo seems to be replacing GPT-4 (non-turbo); the GPT-4 (non-turbo) model is marked as \"Legacy\" in the model list.EDIT: the above is corrected, it previously erroneously said the non-turbo model was marked as \"deprecated\", which is a different thing. reply naiv 15 hours agoparentprevnow with the prices reduced so much even the wallet might not be the bottle neck anymore reply MagicMoonlight 12 hours agoparentprevIt’s insane because it makes no sense. When you read a book you don’t remember the last 100,000 words. It’s so wildly inefficient to do it that way. reply lucubratory 11 hours agorootparentHuh? By the time you finish reading a book you&#x27;ve forgotten the book? reply stavros 1 hour agorootparentThe specific words, yes. reply marban 15 hours agoparentprevComment will not age well. reply Swizec 15 hours agoparentprev> 128k context? Absolutely insane128k context is great and all, but how effective are the middle 100,000 tokens? LLMs are known to struggle with remembering stuff that isn&#x27;t at the start or end of the input. Known as the Lost Middlehttps:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.03172 reply gjm11 8 hours agorootparentIn fairness, humans have much the same problem. reply saliagato 14 hours agorootparentprevsama said they improved it reply Der_Einzige 12 hours agorootparentWe can&#x27;t just take his word for it. This needs experimental verification. It&#x27;s likely not even close to solved. reply drcode 8 hours agorootparentpeople now have access and can do any verification they want reply225 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenAI has revealed multiple updates such as reduced pricing, the introduction of GPT-4 Turbo promising improved cost-efficiency and performance, and a new Assistants API to simplify AI app building.",
      "Additional offerings include multimodal abilities for vision, image creation, and text-to-speech. OpenAI also launched DALL·E 3 for image generation from apps, a text-to-speech API, and a Custom Models program.",
      "A \"Copyright Shield\" has been developed to protect users from copyright claims. Modifications were also made to the Whisper and Consistency Decoder models."
    ],
    "points": 995,
    "commentCount": 477,
    "retryCount": 0,
    "time": 1699294678
  },
  {
    "id": 38166431,
    "title": "OpenAI Introduces Customizable Chatbots 'GPTs' to Personalize AI Experiences",
    "originLink": "https://openai.com/blog/introducing-gpts",
    "originBody": "Close SearchSubmit Skip to main content Site Navigation Research Overview Index GPT-4 DALL·E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Safety Company About Blog Careers Residency Charter Security Customer stories Search Navigation quick links Log in Try ChatGPT Menu Mobile Navigation Close Site Navigation Research Overview Index GPT-4 DALL·E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Safety Company About Blog Careers Residency Charter Security Customer stories Quick Links Log in Try ChatGPT SearchSubmit Blog Introducing GPTs You can now create custom versions of ChatGPT that combine instructions, extra knowledge, and any combination of skills. November 6, 2023 Authors OpenAI Announcements, Product We’re rolling out custom versions of ChatGPT that you can create for a specific purpose—called GPTs. GPTs are a new way for anyone to create a tailored version of ChatGPT to be more helpful in their daily life, at specific tasks, at work, or at home—and then share that creation with others. For example, GPTs can help you learn the rules to any board game, help teach your kids math, or design stickers. Anyone can easily build their own GPT—no coding is required. You can make them for yourself, just for your company’s internal use, or for everyone. Creating one is as easy as starting a conversation, giving it instructions and extra knowledge, and picking what it can do, like searching the web, making images or analyzing data. Try it out at chat.openai.com/create. Example GPTs are available today for ChatGPT Plus and Enterprise users to try out including Canva and Zapier AI Actions. We plan to offer GPTs to more users soon. Learn more about our OpenAI DevDay announcements for new models and developer products. GPTs let you customize ChatGPT for a specific purpose Since launching ChatGPT people have been asking for ways to customize ChatGPT to fit specific ways that they use it. We launched Custom Instructions in July that let you set some preferences, but requests for more control kept coming. Many power users maintain a list of carefully crafted prompts and instruction sets, manually copying them into ChatGPT. GPTs now do all of that for you. The best GPTs will be invented by the community We believe the most incredible GPTs will come from builders in the community. Whether you’re an educator, coach, or just someone who loves to build helpful tools, you don’t need to know coding to make one and share your expertise. The GPT Store is rolling out later this month Starting today, you can create GPTs and share them publicly. Later this month, we’re launching the GPT Store, featuring creations by verified builders. Once in the store, GPTs become searchable and may climb the leaderboards. We will also spotlight the most useful and delightful GPTs we come across in categories like productivity, education, and “just for fun”. In the coming months, you’ll also be able to earn money based on how many people are using your GPT. We built GPTs with privacy and safety in mind As always, you are in control of your data with ChatGPT. Your chats with GPTs are not shared with builders. If a GPT uses third party APIs, you choose whether data can be sent to that API. When builders customize their own GPT with actions or knowledge, the builder can choose if user chats with that GPT can be used to improve and train our models. These choices build upon the existing privacy controls users have, including the option to opt your entire account out of model training. We’ve set up new systems to help review GPTs against our usage policies. These systems stack on top of our existing mitigations and aim to prevent users from sharing harmful GPTs, including those that involve fraudulent activity, hateful content, or adult themes. We’ve also taken steps to build user trust by allowing builders to verify their identity. We'll continue to monitor and learn how people use GPTs and update and strengthen our safety mitigations. If you have concerns with a specific GPT, you can also use our reporting feature on the GPT shared page to notify our team. GPTs will continue to get more useful and smarter, and you’ll eventually be able to let them take on real tasks in the real world. In the field of AI, these systems are often discussed as “agents”. We think it’s important to move incrementally towards this future, as it will require careful technical and safety work—and time for society to adapt. We have been thinking deeply about the societal implications and will have more analysis to share soon. Developers can connect GPTs to the real world In addition to using our built-in capabilities, you can also define custom actions by making one or more APIs available to the GPT. Like plugins, actions allow GPTs to integrate external data or interact with the real-world. Connect GPTs to databases, plug them into emails, or make them your shopping assistant. For example, you could integrate a travel listings database, connect a user’s email inbox, or facilitate e-commerce orders. The design of actions builds upon insights from our plugins beta, granting developers greater control over the model and how their APIs are called. Migrating from the plugins beta is easy with the ability to use your existing plugin manifest to define actions for your GPT. Enterprise customers can deploy internal-only GPTs Since we launched ChatGPT Enterprise a few months ago, early customers have expressed the desire for even more customization that aligns with their business. GPTs answer this call by allowing you to create versions of ChatGPT for specific use cases, departments, or proprietary datasets. Early customers like Amgen, Bain, and Square are already leveraging internal GPTs to do things like craft marketing materials embodying their brand, aid support staff with answering customer questions, or help new software engineers with onboarding. Enterprises can get started with GPTs on Wednesday. You can now empower users inside your company to design internal-only GPTs without code and securely publish them to your workspace. The admin console lets you choose how GPTs are shared and whether external GPTs may be used inside your business. Like all usage on ChatGPT Enterprise, we do not use your conversations with GPTs to improve our models. We want more people to shape how AI behaves We designed GPTs so more people can build with us. Involving the community is critical to our mission of building safe AGI that benefits humanity. It allows everyone to see a wide and varied range of useful GPTs and get a more concrete sense of what’s ahead. And by broadening the group of people who decide 'what to build' beyond just those with access to advanced technology it's likely we'll have safer and better aligned AI. The same desire to build with people, not just for them, drove us to launch the OpenAI API and to research methods for incorporating democratic input into AI behavior, which we plan to share more about soon. We’ve made ChatGPT Plus fresher and simpler to use Finally, ChatGPT Plus now includes fresh information up to April 2023. We’ve also heard your feedback about how the model picker is a pain. Starting today, no more hopping between models; everything you need is in one place. You can access DALL·E, browsing, and data analysis all without switching. You can also attach files to let ChatGPT search PDFs and other document types. Find us at chatgpt.com. Learn more about OpenAI DevDay announcements for new models and developer products. Authors OpenAI View all articles Research Overview Index GPT-4 DALL·E 3 API Overview Data privacy Pricing Docs ChatGPT Overview Enterprise Try ChatGPT Company About Blog Careers Charter Security Customer stories Safety OpenAI © 2015 – 2023Terms & policiesPrivacy policyBrand guidelines Social Twitter YouTube GitHub SoundCloud LinkedIn Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=38166431",
    "commentBody": "GPTs: Custom versions of ChatGPTHacker NewspastloginGPTs: Custom versions of ChatGPT (openai.com) 509 points by davidbarker 15 hours ago| hidepastfavorite285 comments garbanz0 10 hours agoWhen I saw this, I figured it was a clear step back from simply using plugins in the main ChatGPT view. It&#x27;s basically plugins, but with extra prompting and you can only use one at a time.But if you look at projects like Autogen ( https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;autogen ), you see one master agent coordinating other agents which have a narrower scope. But you have to create the prompts for the agents yourself.This GPTs setup will crowd-source a ton of data on creating agents which serve a single task. Then they can train a model that&#x27;s very good at creating other agents. Altman nods to this immediately after the GPTs feature is shown, repeating that OpenAI does not train on API usage.Prediction: next year&#x27;s dev day, which Altman hints will make today&#x27;s conference look \"quaint\" by comparison, will basically be an app built around the autogen concept, but which you can spin up using very simple prompts to complete very complex tasks. Probably on top of a mixture of GPT 4 & 5. reply LouisvilleGeek 10 hours agoparentCurious if anyone that has access can verify that we can still build \"actions\" via localhost like we previously did with plugins? reply mvkel 2 hours agorootparentConfirmed. Actions don&#x27;t look to be deprecated reply minimaxir 14 hours agoprevThe GPT Store will prove to be an interesting moderation and quality control experiment for OpenAI. Apple&#x2F;Google have spent a lot of time and money on both of those things and they still have issues, and that&#x27;s not even accounting for the fact that AI growth hackers will be the primary creators of GPTs. And a revenue sharing agreement will provide even more incentive to do the traditional App Store marketing shennanigans. reply nextworddev 12 hours agoparentI wouldn&#x27;t be surprised if most of the moderation is done via elaborate AI automation.Also RIP chatgpt plugnins reply joshspankit 4 hours agorootparentI’m suddenly worried about that future.The more elaborate the moderation tooling, the faster the race to the bottom gets as revenue drives black-hat marketers to become experts in circumventing AI moderation. It could end up being Google “S”EO all over again where we eventually ended up with very little real information in the results. reply intended 4 hours agorootparentIn theory yes, however thats the long run scenario. The short term and middle term are complex enough that the long run becomes irrelevant.Moderation tooling is not as elaborate as we would hope, and all your work is with humans - who keep adapting. reply SheinhardtWigCo 10 hours agorootparentprevTickets filed by plugin developers (including requests to publish or update a plugin) are handled by bots. The only contact I&#x27;ve had with a human there has been via backchannels. reply zer0c00ler 2 hours agorootparentYes, thats also why policies aren’t enforced. I assume OpenAI knew all along that plugins probably are a dead end. Wonder if they will be around 6 months from now.But Actions will have all the same security challenges basically. reply shmoogy 10 hours agorootparentprevNot sure about currently - but budget increases seemed to be manually reviewed in a queue. I had to increase monthly budget amounts twice and it took like 10-30 hours for each approval, interesting to see how their processes advance. reply dongobread 13 hours agoprevI don&#x27;t think the target market for this is people looking for extremely knowledgeable LLMs that can handle deep technical tasks, given that you can&#x27;t even finetune these models.I&#x27;d guess this is more of an attempt to poach the market of companies like character.ai. The market for models with a distinct character&#x2F;personality is absolutely massive right now (see: app store rankings) and users are willing to spend insane amounts of money on it (in part because of the \"digital girlfriend\" appeal). reply crooked-v 13 hours agoparent> digital girlfriendThe ban on \"adult themes\" is part of the reason people use services other than OpenAI for that kind of thing in the first place. reply sucralose 10 hours agorootparentOpenAI is against emotional attachment to their LLMs in general, even disregarding \"adult themes\". reply Der_Einzige 12 hours agorootparentprevSomeone has to give reasons for civit.ai and huggingface to exist! reply mg 14 hours agoprevSo these \"GPTs\" are the combination of predefined prompts and custom API access? Not customly trained LLMs?If so, I guess you can make such a \"GPT\" on your own server and independent from a specific LLM service by using a prompt like ...you have available an API \"WEATHER_API\". If you need the weather for a given city for a given day, please say WEATHER_API(&#x27;berlin&#x27;, &#x27;2022-11-24&#x27;) and I will give you a new prompt including the data you asked for...Or is there some magic done by OpenAI which goes beyond this? reply jatins 14 hours agoparentYeah I don&#x27;t _think_ there is a lot \"magic\" in custom GPT.However creating something like this previously required a Jupyter notebook but now just...asking for it. Makes it accessible to 10x more people reply jondwillis 11 hours agoparentprev“GPTs” (terrible name) === Agents reply arcanemachiner 6 hours agorootparentAgreed, but one is a recognizable trademark (I assume) and one is a generic catch-all term that means a lot of things and would probably be worse from a branding&#x2F;marketing perspective. reply golol 3 hours agorootparentprevNot even. An agent should primarily operate in some kind of self-prompting loop. Afaik xou can not specify complex and branching looping behaviors for GPTs. reply zer0c00ler 2 hours agorootparentChatGPT and plugins were already agents. An agent just does stuff on somebody else’s behalf. Browsing, browses the Internet for the user, code interpreter creates and runs Python code for the user…What you describe are what’s called autonomous agents, and i agree that I also expected some interesting announcement there but probably too many security issues. reply golol 1 hour agorootparentWell, I would say that when people at the moment say agent they mean what you call autonomous agent. Browsing or code interpreter is just LLM + tool use. I think there is a really large difference in quality between a system that just interacts 1-2 times with some tool&#x2F;API before giving an answer and one that runs in a loop with undefined length (until it decides to terminate). It&#x27;s like the difference between programming without loops or recursion vs with them. Night and day. reply ddmma 1 hour agorootparentprevExactly my point, then you can register .agency domain based on your area of ‘agent’ expertise.Sorry, but I won’t bite the marketplace as normally wipe the top agents with official ones, like apple did with native apps.It’s crowdsourced AGI reply burningion 14 hours agoparentprevThat&#x27;s the thing, we don&#x27;t know.The lack of transparency for how the product works behind the scenes will most likely make it difficult to build something effectively. reply burningion 6 hours agorootparentActually, the documentation here is pretty great. Too late to edit my comment, but the transparency level here isn&#x27;t too bad: https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;assistants&#x2F;tools reply BoorishBears 14 hours agoparentprevIf you want to be independent for academic&#x2F;personal reasons, sure you can.If you want reasoning capabilities that Open Source hasn&#x27;t matched before today, and I&#x27;m guessing just got blown out of the water on again today... there&#x27;s no reason to bother. reply dragonwriter 13 hours agorootparent> If you want to be independent for academic&#x2F;personal reasons,Or OpenAI&#x27;s usage policy limits reasons (either because of your direct use, or because of the potential scope of use you want to support for downstream users of your GPT, etc.) Yes, OpenAI&#x27;s model is the most powerful around. Yes, it would be foolish not to take advantage of it, if you can, in your custom tools that depend on some LLM. Depending on your use, it may not make sense to be fully and exclusively dependent on OpenAI, though. reply mg 14 hours agorootparentprevYou don&#x27;t need to use an open source LLM for the approach I described. You can still send the prompts to OpenAI&#x27;s GPT-4 or any other LLM which is available as a service. reply BoorishBears 14 hours agorootparentWhat other LLM will compete with GPT-4 Turbo (+ V)? At most you&#x27;re hedging that Anthropic releases a \"Claude 2 Turbo (+ V)\": is complicating your setup to such a ridiculous degree vs \"zero effort\" worth it for that?If things change down the line the fact you invested 5 minutes into writing a prompt isn&#x27;t going to be such a huge loss anyways, absolutely no reason to roll your own on this. reply dragonwriter 13 hours agorootparent> If things change down the line the fact you invested 5 minutes into writing a prompt isn&#x27;t going to be such a huge loss anywaysIf things change down the road such that your tool (or a major potential downstream market for your tool) is outside of OpenAI&#x27;s usage policies, the fact that you invested even a few developer-weeks into combining the existing open source tooling to support running your workload either against OpenAI&#x27;s models or a toolchain consisting of one or more open source models (including a multimodal toolchain tied into image generation models, if that&#x27;s your thing) with RAG, etc., is going to be a win.If it doesn&#x27;t, maybe its a wasted-effort loss, but there&#x27;s lots of other ways it could be a win, too. reply BoorishBears 13 hours agorootparentLet&#x27;s go back to my very first comment.> If you want to be independent for academic&#x2F;personal reasons, sure you can.If your goal is to be in business, or to get some sort of reach, or anything other than \"have fun tinkering\"... wasting developer weeks on \"could be a win\" is how to fail. replyatleastoptimal 14 hours agoprevI wonder how much money I could make making \"GPTs\" full time. Barrier to entry is nonexistent so I imagine highest revenue ones if this becomes a serious thing people use will be advertised externally or have some proprietary info&#x2F;access. reply jatins 14 hours agoparent> I wonder how much money ...I saw that announcement and my immediate thought was \"God yet another thing passive income youtubers will be shilling soon\"In general, I was a little confused by this. Sam&#x27;s demo of creating a GPT didn&#x27;t seem particularly exciting as well. reply atleastoptimal 14 hours agorootparentOpenAI in a weird way has mediocre marketing. The examples they use for Dalle-3 are way worse than the average ones I see people cooking up on Twitter&#x2F;Reddit. They only seem to demo the most vaguely generic implementations of their app. Even their DevDay logo is just 4 lines of text. reply RobertDeNiro 14 hours agorootparentJust the fact that they decided to stay with what is essentially a highly technical acronym i.e. GPT, as their major product line says a lot. reply andrewmunsell 14 hours agorootparentTo be fair, the name \"ChatGPT\" has quite a bit of mindshare and I&#x27;ve found many non-technical folks referring to any generative AI product as \"ChatGPT\" or \"GPT\". Yet, if you asked any single one of them what \"GPT\" stood for, they&#x27;d have no clue. reply JCharante 13 hours agorootparentTo be fair, I&#x27;m a dev who uses chatgpt on an hourly basis and I had no idea what GPT stood for until I googled it just now. I think it&#x27;s kinda smart to make people strongly associate GPT with OpenAI reply toomuchtodo 13 hours agorootparent\"Generative Pre-trained Transformers\" for those who don&#x27;t want to leave the page. reply teg4n_ 11 hours agorootparentprevwhy would it be smart? people can advertise their own models as GPT because it’s a generic term and confuse people into thinking it’s an OpenAI thing reply singularity2001 13 hours agorootparentprevI&#x27;ve heard all permutations of \"GPT\" \"GPP\" \"GTP\" etc. reply visarga 3 hours agorootparentIn a bad day GPT is GTP and Bard is Brad reply mensetmanusman 12 hours agorootparentprev“You down with G P T?” reply xnx 10 hours agorootparentYeah you know me reply shawnc 13 hours agorootparentprevI don&#x27;t recall which interviews I saw it stated in, but I believe Sam said in one or two of his world tour stops, where he stated they deliberately have gone with a technical name instead of a human name to help remind those using it that it&#x27;s not a person. So I think that coupled with the mindshare (as others have stated) it already holds, makes a lot of sense to stick with it. reply GuB-42 11 hours agorootparentprevDoesn&#x27;t seem like a problem to me. Many brands are acronyms that are meaningless, too technical or arbitrary. Very common for cars for instance, how about a BMW X5 V8 SUV?Plus, \"generative pre-trained transformer\" sounds futuristic, which seems like a fitting brand image for OpenAI. reply falcor84 14 hours agorootparentprevI think that&#x27;s actually crucial in that they want to trademark this otherwise generic term reply Aerbil313 14 hours agorootparentOmg… Thinking about their push for regulation with this… Are they after something like keeping advanced generative pretrained transformer LLM model technology to themselves, prohibiting others, at least in American economy where regulations can be applied? reply ddmma 1 hour agorootparentprevBefore coming into mainstream I was always referring as GTP .. reply block_dagger 14 hours agorootparentprevI think it might be the ubiquity of the term \"GPT\" as it relates to OpenAI from a public branding perspective. reply imdsm 14 hours agorootparentprevAnd yet in a way I find this refreshing reply manojlds 14 hours agorootparentprevBetter that than do a silly rename ala X. reply minimaxir 14 hours agorootparentprevSam&#x27;s \"all our marketing is from word-of-mouth\" was refreshingly honest. reply toomuchtodo 14 hours agorootparentWould be somewhat humorous to plug OpenAI into ad platforms, give it budget, and say \"go market yourself as effectively as possible.\" reply tanseydavid 11 hours agorootparentThat would not be humorous -- that would be the AI equivalent of \"The China Syndrome\" https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_China_Syndrome reply JackFr 14 hours agorootparentprevNeed some Boston Dynamics flair. reply conradev 13 hours agorootparentprevI would pay money for a GPT that was incredible at naming types in SwiftI’d pay for one that was good at programming rubber duckingThere are specific sub-tasks that everyone would pay for to make their lives easier. This marketplace is trying to make that efficient reply NiagaraThistle 13 hours agorootparentI find NORMAL ChatGPT a good programming Rubber Duck and use it often. reply tanseydavid 11 hours agorootparentprevI suppose you&#x27;re going to ask GPT to do Cache Invalidation eventually too? reply ren_engineer 14 hours agoparentprevnot much based on what OpenAI has been doing lately, using their own customers as product research and then copying the best ideas. OpenAI pretty much has to keep a huge lead in model capabilities or developers are going to stop using them for this reasonbasically copying the Microsoft strategy of Embrace, Extend, Extinguish. Makes sense they took so much funding from Microsoft reply CSMastermind 14 hours agoparentprevI&#x27;m more confused how the revenue share works. Do they get part of my ChatGPT subscription fee? Am I paying extra? Per bot? Per amount of time I consult with the bot? reply csjh 12 hours agorootparentI think it must be based on usage (some GPTs could be using more credits than others) reply zwily 13 hours agorootparentprevYeah he didn’t explain at all how GPTs would be monetized. reply stevesearer 14 hours agoparentprevI bet if you combined GPT creation with Zapier integrations you could help a lot of people and companies. reply toomuchtodo 14 hours agorootparentThoughts on Zapier trying to become OpenAI faster than OpenAPI can become Zapier? There will always be a long tail of APIs that folks want integrating, but the most popular APIs are perhaps only a few hundred in number (Google Calendar and Slack, for example). reply jimmyl02 14 hours agorootparentI feel like history has shown that those who own the platform end up winning and in this case OpenAI&#x27;s platform of models seems much harder to recreate. My guess is this would lead to Zapier using OpenAI as a platform and eventually OpenAI would re-create Zapier&#x27;s integrations before the other way around. reply toomuchtodo 14 hours agorootparentI think this perspective is fair and historically accurate wrt platform risk, but I also strongly believe Zapier has substantial value beyond what historically has been acting as a conduit between APIs. Customers don&#x27;t want a pipe between their services, they want to automate their mundane work with a robot. reply ukuina 6 hours agorootparentWhat is Zapier&#x27;s value-add when GPT can chain an arbitrary number of flexible API calls? Imagine if the GPT plug-ins directory is fed as input to GPT... reply JCharante 13 hours agorootparentprevSo many zapier integrations are half baked. A lot of them are good for reacting to events but not for searching for data (i.e. you can use zapier to react to a jira ticket change but can&#x27;t use zapier to query jira for ticket info) reply parkerhiggins 13 hours agorootparentYou can, just have to use JQL. Zapier can create a conditional trigger if the JQL criteria are met and pool on a set cadence, such as 15 mins. reply torginus 14 hours agoparentprevAnd seeing how OpenAI is moving up the value chain, what&#x27;s the guarantee they won&#x27;t come up with an in-house competitor to the bot that was built on their platform? reply adventured 11 hours agorootparentGuarantee? That&#x27;s one of the most important aspects of bothering to build out a platform: eat the ecosystem to add incremental value to the platform, bolster the moat.The guarantee is that they will clone and extinguish most of the best bots&#x2F;tools&#x2F;services riding on top of GPT. It&#x27;s what platforms overwhelmingly tend to do.If your thing is a near-touch to GPT, depends on it, is of medium complexity or lower, and is very popular&#x2F;useful, they will build your thing into GPT eventually.On the flip side, if you&#x27;re Salesforce and using GPT to augment something about a major product, you&#x27;re not facing a serious threat of OpenAI trying to become the next big CRM company.This process will work similarly to how it did with Windows, Google, AWS, etc. reply nullptr_deref 11 hours agorootparentTheir primary target is B2B and they want to get that market ASAP. Pretty sure any idea or anything that seems feasible will be copied and even improved upon.The pattern will continue until no one needs anything. I am sure this company will eat everything. There will be no pie. reply elforce002 10 hours agorootparentSo, basically develop something, earn $$ early on, and then prepare to be assimilated by chatGPT. reply anonyfox 2 hours agorootparentPretty much this.Also not only for ChatGPT, but there will be a timespan between majority of people are automated out of jobs and a a forceful redistribution of wealth happens (let’s call it socialism). It’s always a good idea to have that $$ to bridge that gap. replyilaksh 14 hours agoparentprevRight now, zero. They didn&#x27;t say when it was rolling out or what percentage they would share. It might be something like 10%. Lol. reply atleastoptimal 13 hours agorootparentI will make a bot that makes GPTs, and a bot that makes other bots that make GPTs reply tanseydavid 11 hours agorootparentHoarding is so much more efficient and fun with technological assistance.reply Uehreka 14 hours agoparentprev> I wonder how much money I could make making \"GPTs\" full timeI don’t get why people are thinking along these lines at all. Like, if you don’t own and control the LLM yourself, what makes you so sure OpenAI will allow you to make money at all? They could make advertising externally or hosting external marketplaces against the TOS. They could copy your GPT and put their “official” version at the top of the store page. Just because a technology is powerful does not necessarily mean you can make money off of it. reply jes5199 13 hours agorootparentChatGPT is the new iPhone. Dealing with a walled-garden app store is never a great experience, but we&#x27;ll do it because that&#x27;s where the users are reply Uehreka 12 hours agorootparentI just feel like the big tech companies have gotten better at capturing the value of their platforms. It’s not the same as it was in 2008. I don’t think we’re going to see anything like the “manage an indie app as a lifestyle business” boom we saw back then, and I expect the consolidation that killed off most of those indie devs will happen way faster this time. reply michaelmior 14 hours agorootparentprev> what makes you so sure OpenAI will allow you to make money at all?They might not. But if they do, I&#x27;d imagine there are a lot of people who will try. And as long as you&#x27;re not dependent on the income stream they provide, you don&#x27;t have much to worry about if it gets shut off. reply stale2002 13 hours agorootparentprevThe answer is because the bot market is a creator economy market.It takes significant effort to come up with good use cases, build the prompts, and advertise the bots.So a company can get a lot of value by going after this up and coming type of \"content creator\". reply oezi 14 hours agoparentprevHow many people made any money from the plugins for ChatGPT? reply minimaxir 14 hours agorootparentPlugins failing was more due to lack of visibility, which a GPT Store does solve. reply singularity2001 13 hours agorootparentHow does the store solve visibility? In the demo it looked like there was a select list of Custom Assistants in the left panel which he manually had to click, so not much different from plug-ins?Right he said something about promoting the best but what about the discoverability? reply manojlds 13 hours agorootparentprevDepends on features. How much data can I give it and how much can I customize it? reply avarun 14 hours agorootparentprevChatWithPDF made a TON of money, for one. reply euazOn 13 hours agorootparentSource? reply bilsbie 14 hours agoparentprevI’m not clear who the market is? Why someone buy one? reply imdsm 14 hours agorootparentImagine a \"GPT\" that could generate websites and provide you with a live deployment as you change it using natural language. A website builder GPT that is primed to output and design in a decent way, that has all the prep beforehand to use particular libraries, and integrations with something like Render.People would pay for that.Sh--... I better build it! reply block_dagger 14 hours agorootparentOne can come up with all sorts of ideas like this, but building it will be a matter of slow iterations at prompt engineering in a mixture of natural language and data structures and will be at the whim of changing APIs, including the backing ChatGPT model. Sounds messy, hard to manage, hard to test...or am I missing what the actual process will be for creating one of these? reply mirekrusin 14 hours agorootparentWorry not, surely there will be GPT for creating GPTs. reply ggsp 12 hours agorootparentGood intuition, it was in the demo. Chat with an agent to config a new GPT. Then make the GPT available to others if you wish. Same functionality is exposed via API IIRC. reply alvah 6 hours agorootparentprevZipWP already does something close to this for WordPress sites. reply user_7832 14 hours agorootparentprevAll fun and games until someone sues because the \"hallucinated\" product description didn&#x27;t match the product... reply siva7 14 hours agoparentprevI&#x27;m not sure i understand? reply atleastoptimal 14 hours agorootparentThe GPT&#x27;s making the most money will be made by larger companies who advertise use of it and maybe make it a funnel to their in-app integration, or GPTs which are made effective by information that is proprietary. reply ca_tech 14 hours agorootparentThis makes me think of the Alexa skills ecosystem which is full of low quality skills. Many of which have poor data practices abstracted away behind the scenes. How long until \"Chat with your favorite character from [Intellectual Property]?\" which is simply made to promote a new film or collect data.A good paper on the state of the Alexa skills BTWSkillVet: Automated Traceability Analysis of Amazon Alexa Skills https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;9619970 reply bbor 14 hours agorootparentprevPart of the announcement is that they&#x27;ll open up an app store w&#x2F; revenue sharing of some kind. \"In the coming months\" or smtn reply colesantiago 14 hours agoparentprevI agree.Anyone with factual data (proprietary or not) is now an input away to AI &#x2F; GPTs.Data (or a new foundational model) is now the moat. reply j7ake 14 hours agorootparentData has always been the moat no ? reply altdataseller 14 hours agorootparentprevWhat sort of data do you think will be the most valuable to input to AI? reply bbor 14 hours agoparentprevI think something could be said for \"virality\" as well - could easily see some entertainment or lifehack themed templates blowing up on TikTok. No one wants to post the output of the lame, less popular template on their story! reply teabee89 14 hours agoprev\"Example GPTs are available today for ChatGPT Plus and Enterprise users to try out including Canva and Zapier AI Actions.\" and yet as a paying ChatGPT Plus customer, neither the Canva nor the Zapier AI Actions link work for me, I get a \"GPT inaccessible or not found\" error for Canva or Zapier. reply vinni2 14 hours agoparentI have the same issue, my guess is they are still rolling out.Edit: here is the message I get:Your access to custom GPTs isn’t ready yet. We’re rolling this feature out over the coming days. Check back soon. reply nomel 13 hours agoparentprevOpenAI is a sane company. They do rollouts for new features. reply joshstrange 13 hours agorootparentNo, they just lie in their marketing> Example GPTs are available today for ChatGPT Plusor> Starting today, no more hopping between models; everything you need is in one place.Neither of which are true. I&#x27;m a paying user and I have access to neither. They do this _all the time_. They announce something \"available immediately\" and it trickles out a week or more later. If they want to do gradual rollouts (which is smart) then they should say as much. reply omarfarooq 10 hours agorootparentAre you sure? Both of those went live at 1 PM PST. reply namibj 10 hours agorootparentI (Plus subscriber, EU) tried https:&#x2F;&#x2F;chat.openai.com&#x2F;gpts&#x2F;editor (as linked from https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;8554407-gpts-faq#h_86549... ) a minute ago and got a \"You do not currently have access to this feature\" toast notification on orange background top of pace, along with \"chat.openai.com\" in the URL bar. (Chrome on Android, but besides a responsive layout, I haven&#x27;t noticed discrepancies with the desktop Chromium site&#x2F;interface; sadly the native Android app still shows no signs of code interpreter mode.)The announced (and a few days ago leaked) Omni prompt also doesn&#x27;t show up in the model selector. And that despite the expanded context looking very promising for the REPL-feedback-augmented code generation abilities. reply raesene9 1 hour agorootparentprevNot live for me in the UK 8am GMT... Ideally they&#x27;d be a bit more transparent about their rollout process&#x2F;timeline reply zer0c00ler 2 hours agorootparentprevNo, it’s 11:24 pm pst and not available for me. reply astrange 8 hours agorootparentprevI don&#x27;t have them either. I also get things on mobile and web days apart IIRC. reply KerryJones 10 hours agorootparentprevI am sure, this happened to me replytrash_cat 11 hours agoprevTheir zappier demo was uninspiring. They are trying to appeal to the mainstream (non devs) users when their actual value is the API. Everyday users don´t have access to propriatery data that makes the LLM Bots valuable.This makes me think that open source models are the future, for other reasons as well. No company is going to give them OpenAI their data to make bots, regardless if they won&#x27;t train on the data. So what OpenAI can do at the moment is to focus on everyday user, and see where the biggest use cases are.Furthermore, as Sam alluded to, this confirms to me that multi-LLM architectures (AutoGen, AutoGPT, BabyAGI etc) is the future. It´s about pushing the ways in which we can use these models, i.e. using different architectures because it will be a while before we get into more powerful models. These is simply no compute for it at the moment. The dust has yet to settle. reply atleastoptimal 11 hours agoparentThe issue is all open source models are made by huge companies with an indirect profit motive. There is too much scale advantage for open source models to be competitive in any field other than those which venture backed companies deliberately avoid (porn, illegal stuff, etc). reply x0x0 5 hours agoparentprevI disagree. There&#x27;s a lot of folks using zapier that would like some light automation -- I have fielded requests from several -- who are not engineers and cannot build anything with an api. The bit where they built out the functionality with zapier and human instructions was aimed squarely at folks like this. reply dang 14 hours agoprevRelated ongoing threads:New models and developer products - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38166420OpenAI DevDay, Opening Keynote Livestream [video] - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38165090 reply siva7 14 hours agoprevSo i guess the next wave of startups has been killed. I&#x27;m not even sure what kind of startup still makes sense as a gpt thin wrapper? reply dragonwriter 14 hours agoparent> I&#x27;m not even sure what kind of startup still makes sense as a gpt thin wrapper?Startups don&#x27;t make sense as thin wrappers around another company&#x27;s product when that company is aggressively expanding that product is a core offering that the vendor is aggressively working to provide as an integrated solution for as many markets as possible, which very much applies to OpenAI offerings in general, and its chat models especially.A wrapper that also leverages some exclusive special sauce data, algorithm, etc., for which you have a real moat as a key component, that makes some sense. But just a thin wrapper around GPT? That&#x27;s just asking to have your market eaten by OpenAI.It might make some sense for products where the vendor is a stable, steady-state infrastructure supplier for many markets without any evident interest in entering the same market as the startup, where the uncertainty across markets that they would create by specifically targeting your startups market would hurt them more with their established customers than they would gain from your niche, but even that is risky because it requires lots of potentially-erroneous assessments of how the vendor would expect their other customers to react to them acting in your market. reply hackerbeat 0 minutes agorootparentAI graveyard: https:&#x2F;&#x2F;dang.ai&#x2F;ai-graveyard reply ethanbond 14 hours agoparentprevNone of them, but here&#x27;s the thing: they never made any sense. reply alvah 6 hours agorootparentConversion.ai &#x2F; Jarvis.ai &#x2F; Jasper.ai &#x2F; whatever they’re called today raised $125M at a $1.5b valuation, so it made sense at some point to some people. reply constantly 14 hours agorootparentprevThey never made sense long term, of course. But, plenty of first movers made a bundle of money making chatgpt wrappers and marketing the hell out of them. In that context, they probably made sense for a small subset of people for a small slice of time. reply RobertDeNiro 14 hours agorootparentprevYeah none of them ever did, and that was always very obvious. If you can make it by wrapping a few API calls, you have no moat and anyone can steal your idea&#x2F;customers. reply dumbfounder 11 hours agorootparentFirst-mover advantage can be powerful. If you can build a community around your wrapper quickly then you can leverage that community to build a moat. reply vikramkr 10 hours agorootparentEven if you&#x27;re doing something with strong network effects you&#x27;ve got to consider cases like facebook which weren&#x27;t first movers in markets where there are super strong network effects and yet still won. You&#x27;re not really going to have a &#x27;community&#x27; based moat for PDF analyzer tool #5 or retrieval augmented generation knowledgebase parser #300. You&#x27;re not gonna have a moat based on much else either... reply danenania 9 hours agoparentprevI think people may be overstating this. OpenAI has obvious brand and distribution advantages for their end-user facing products, but they aren&#x27;t guaranteed to hit the bullseye and be the market leader in every category. ChatGPT plugins, for example, have never really taken off. Startups like Perplexity and Phind are doing just fine in the \"search the web with AI\" space that plugins were supposedly going to dominate.I&#x27;d say that trying to build a business plugging gaps in OpenAI&#x27;s infrastructure offerings (fine-tuning, RAG, etc.) is quite perilous, since this is really their core competency. But they&#x27;ve still had just one big user-facing hit so far: ChatGPT.Wrapping OpenAI for a specific use case then building better workflows and UI for that use case than ChatGPT seems like it&#x27;s still a pretty reasonable strategy. OpenAI isn&#x27;t going to build the ideal workflow&#x2F;UI for every use case. reply visarga 2 hours agorootparentI recently enjoy Perplexity.ai for their superior search+llm integration, while chatGPT usually send stupid search keywords to Bing that I know are not going to help. reply tacone 14 hours agoparentprevIt&#x27;s just commoditization, hard things now becoming a lot easier and value proposition to move up elsewhere.It happened with hardware, operative systems, web tools such as maps etc. reply sergiotapia 14 hours agoparentprevIf your startup is just a thin wrapper for GPT, it&#x27;s DOA regardless. Trivial moat is always a trivial moat.You must use LLMs as a launching point to something else, some kind of 10x in a vertical.Being able to \"chat\" is table stakes and worthless to you as a company by itself. reply Workaccount2 12 hours agoparentprevIt was (is) basically a smash and grab. You can spin up a site and a payment processor so fast nowadays that it just takes a weekend of work to go from idea to \"Click here to subscribe and get 200 [AI generation] a month!\" reply api 14 hours agoparentprevThin wrappers around anything never make much sense. They&#x27;re trivially replaceable. reply visarga 2 hours agorootparentNot only we have to disclose to OpenAI our data, but also to the wrapper operators. This doesn&#x27;t sit well with me. reply audiala 14 hours agorootparentprevEven if they create a great UX&#x2F;UI? GPT would be like the motor of the car, there is still all the structure to build around it. reply joelthelion 11 hours agoparentprevAre there any good RAG as a service startups? reply colesantiago 14 hours agoparentprevThis meme is getting very old and tired.What kinds have been &#x27;killed&#x27; I don&#x27;t see this anywhere. reply Tankenstein 14 hours agorootparentMany startups have started over the past few years, trying to build infrastructure (shovels) for companies to integrate LLMs, or specific chat copilots trying to cater to a specific usecase. Most are dead in the water once OpenAI subsumes their feature set. reply BoorishBears 13 hours agorootparent... is what people who don&#x27;t understand positioning will parrot time and time again.Jasper isn&#x27;t having a good time, but you&#x27;d think the fact anyone can produce better output than they did after spending millions of dollars in GPT-3 based pipelines for $20 a month would mean they&#x27;re dead dead.But instead they went and changed their positioning, changed who their target market is, adjusted the UX, the messaging, and the feature set, and now it&#x27;s a product that has a place even if OpenAI can give all of your marketers an internal ChatGPT (unless your plan is to have 100 different \"GPTs\" for every marketing task in your company)tl;dr: People fail to realize that OpenAI can offer your startup&#x27;s core value proposition tomorrow morning, and it doesn&#x27;t matter if they don&#x27;t offer it in a format that resonates with your target users.You could have a cure to cancer and you&#x27;d still have to market it correctly. reply Sai_ 13 hours agorootparentIsn’t that the crux of the conversation? Jasper had to fight to survive because they were a wrapper. reply tsunamifury 12 hours agorootparentWhat isn&#x27;t a wrapper? Honestly everyone turns they nose in the air and claims all superiority bit I challenge you to find a use case that isn&#x27;t just a &#x27;wrapper&#x27; reply omarfarooq 10 hours agorootparentNot all wrappers are created equal. reply BoorishBears 10 hours agorootparentprevThat&#x27;s the complete opposite of reality: Jasper could only fight to survive because they were a wrapper.Imagine if Jasper had raised and built their own GPT-3 alternative from the ground up at 100x the cost targeted at writing: it wouldn&#x27;t have generalized like OpenAI&#x27;s models given the different goals in training, they&#x27;d have spent orders of magnitude more, they&#x27;d currently be scrambling to partner with some non-OpenAI provider to play the role OpenAI does for them today...Jasper \"only\" had to start integrating new APIs to keep up with the SOTA in quality, and instead of burning precious runway on R&D they get to focus on rebranding and driving home that positioning. replylevmiseri 14 hours agoparentprevSome use cases are still valid I hope. E.g. content generation on a massive scale. An example of that that I&#x27;m tinkering with: https:&#x2F;&#x2F;meoweler.com reply anigbrowl 13 hours agorootparentCan I ask what the goal is here? It&#x27;s cool to be able to spin up a nicely designed website (and it is nicely designed and has a good aesthetic), but isn&#x27;t the content going to be semantically empty? I don&#x27;t want to be negative but it feels like cheap plastic imitation of a real thing, and the web is already full of spammy low quality content. Aren&#x27;t you in danger of your products having a very short life cycle and ending up as digital landfill, so to speak? reply levmiseri 11 hours agorootparentThe quality of the GPT4 content for travel is surprisingly good, although there are of course plenty of hallucinations and I&#x27;ve a new pass for the content set up that gets rid of the worst offenders. This is really just a learning experiment, but building something of this content scale by 1 person was absolutely impossible a year ago.The point was that content generation use cases might be a viable path give OpenAI&#x27;s thin-wrappers-killing direction. reply anigbrowl 8 hours agorootparentBut does anyone particularly want another travel site full of generic advice? Nothing against your individual project, I&#x27;m just saying that the web is already absolutely awash in marketing materials and all-in-one portals. If I&#x27;m visiting a place and want to read up on it in advance I tend to look for someone with insider knowledge, otherwise I could just buy a guidebook specifically about that place (might be a bit dated, but the editorial quality in a book is usually orders of magnitude better than the web). reply visarga 2 hours agorootparentLLMs can quickly extract ideas using search and synthesize fresh articles. Take this conversation thread for example, there are many interesting ideas here, they can be scooped and formatted in a nice style. This gives AI a seed of authenticity, its main role would be to curate the mess.Here is a sample, it took only a couple of minutes to make: https:&#x2F;&#x2F;pastebin.com&#x2F;WZpfgWWHPrompt: Please write a 1000 word long article based off of this text, don&#x27;t mention the commenters, just weave their ideas into a coherent piece.I used Claude 2 reply danenania 10 hours agorootparentprevHmm but why click around on an AI-generated content farm to get information when I can just ask ChatGPT directly? reply ukuina 5 hours agorootparentThe site layout and images look pretty. replyrajnathani 1 hour agoprevThis is just slid into the last paragraph, but it is a pretty big deal as this knowledge cutoff date is now past ChatGPT’s initial launch date (thus OpenAI has likely sort of figured out as to how to exclude crawled text data generated by itself and other LLMs on the internet):> Finally, ChatGPT Plus now includes fresh information up to April 2023. reply tracerbulletx 14 hours agoprevCan anyone explain what \"extra knowledge\" means specifically? Is that like fine tuning? How much data can I give it to learn? How much can it retain? Can it be updated over time? reply minimaxir 14 hours agoparentThe keynote used a .txt file of a lecture that the user uploaded as a data source the model can select from. From a technical perspective, it&#x27;s anoher data source for retrieval-augmented generation (RAG) doing a vector search in the background: https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;assistants&#x2F;tools&#x2F;knowledge-... reply singularity2001 13 hours agorootparent\"optimizes for quality by adding all relevant content to the context of model calls.\" So for their own profits they maximize recall and let GPT handle precision. reply tracerbulletx 14 hours agorootparentprevAh ok, thank you. reply ankit219 13 hours agoparentprevThey showed a demo where you could upload a file while creating an agent. As others have answered. I think it&#x27;s about configuring an agent in a way that you give it material on some specific topic (one file, multiple files) and it uses Retrieval to augment the answer based on the source material.Google launched Notebook LM[1] a while back which does a similar thing conceptually. It allows you to import a Google drive folder with docs of the stuff you would want to understand and then just chat with it. It&#x27;s a good product but restrictive in the sense that it only allowed Google docs.[1] https:&#x2F;&#x2F;blog.google&#x2F;technology&#x2F;ai&#x2F;notebooklm-google-ai&#x2F; reply vunderba 13 hours agoparentprevThe demo just showed a file dialog box where they could upload a set of static files. What I&#x27;d really like to see is the ability to sync a source integration (for example into a GitHub repo or a notion account), and it would always pull relevant information using a RAG architecture. reply Racing0461 14 hours agoparentprevI doublt it&#x27;s fine tuning (actually changing the model weights). It&#x27;s more like \"im going to paste a text blob then in the following chats i will ask questions about it) type inner prompt. reply singularity2001 13 hours agorootparentFor longer documents it uses vector embeddings reply Racing0461 10 hours agorootparentHow&#x27;s that different from pasting the text in the first chat and running the vector embedding step on the text on the server (maybe at least bypassing the chat text limit)? Does this fix the amnesia issue where the info from chats longer than the context length is forgotton because the document isn&#x27;t baked directly into the weights like fine tuning? reply glitchc 14 hours agoprevThe cynical me thinks the point of calling these GPTs is a ploy to trademark the term \"GPT\". reply leobg 13 hours agoparentThought the same thing. reply joshstrange 13 hours agoprev> We’ve made ChatGPT Plus fresher and simpler to use> Finally, ChatGPT Plus now includes fresh information up to April 2023. We’ve also heard your feedback about how the model picker is a pain. Starting today, no more hopping between models; everything you need is in one place. You can access DALL·E, browsing, and data analysis all without switching. You can also attach files to let ChatGPT search PDFs and other document types. Find us at chatgpt.com.It&#x27;s so annoying how they say this, I refresh and I still have to hop between models. Just say \"rolling out over the next week\" if that&#x27;s what&#x27;s happening. I even logged out and back in and still the same old way of doing it. reply column 15 minutes agoparentWhat is even more annoying is every plus subscriber whining that they didn&#x27;t get the feature yet. We know, you paid and you want to play now, but it&#x27;s a rollout and it&#x27;s progressively coming towards you. Just have a little bit of patience. reply lukejagg 12 hours agoparentprevThe roll outs are quite slow. Sometimes it takes weeks for them to release a feature to my account while I know others who get access immediately. I understand that it helps them control the quality of ChatGPT, but I wish I got access earlier as I have been subscribed since the beginning. reply chankstein38 12 hours agorootparentSometimes it seems like logging off and back on causes these updates to hit. I didn&#x27;t have Dall-e or image capabilities after weeks of it being released and I logged off and back on and both were available. This was between multiple computers logged in so it wasn&#x27;t just a cache clearing situation. reply chankstein38 12 hours agoparentprevFor what it&#x27;s worth, for me it appeared nothing was different except I could leave the Default model on, upload a picture, ask it to generate a similar one with a change, and it just booted up Dall-e and did so. I&#x27;m not sure if everyone&#x27;s experience is like this. I agree it&#x27;s super annoying how they handle these things. I do still have the dropdown but, set to Default, it would generate images for me.I do recommend logging off and logging back in if it isn&#x27;t working. I&#x27;ve seen that update things for me. reply ryanSrich 11 hours agoparentprevThe lack of transparency around how they release access to features is infuriating. Especially when you pay for the product. I cannot stand it. reply SheinhardtWigCo 10 hours agoprevWithout getting into specifics, I operate a ChatGPT plugin with a lot of users.It sounds like the GPT Store will solve some of the rough edges with plugins as they exist today, but I&#x27;m not interested.OpenAI has been a horrible \"partner\" to work with. The team that owns this product is terribly under-resourced. You have no hope of reaching a human unless you have an inside contact. Support tickets are handled by GPT; policies are arbitrarily enforced; and major bugs in the API and documentation are simply ignored.There&#x27;s an amazing opportunity there for user acquisition, obviously, but do you really want to put your hand in the pain box? reply adkandari 2 hours agoprevWell the market is pretty obvious. Example - Fitness Influencer on Instagram can be your personal coach,i.e. the AI clone of the Fitness Influencer you can subscribe to at a fairly low price, as personal consultations are expensive and physically limited. So people with distributions and expertise will train their GPTs, make them accessible at affordable prices. good for creators, it&#x27;s passive income source and good for consumers, affordable service&#x2F;education. reply singularity2001 13 hours agoprev\"you can now create custom versions of ChatGPT\"how? login opens unrelated tab.Found it:https:&#x2F;&#x2F;chat.openai.com&#x2F;gpts&#x2F;discovery -> create own ->https:&#x2F;&#x2F;chat.openai.com&#x2F;gpts&#x2F;editor reply Implicated 14 hours agoprev> Your access to custom GPTs isn’t ready yet. We’re rolling this feature out over the coming days. Check back soon.> Go to ChatGPTSad. Have a real-world use case ready to go and a plus account. reply chankstein38 12 hours agoparentYeah I&#x27;m getting pretty sick of these announcements followed by insanely slow rollouts. I&#x27;m a paying customer and have been since Plus became available. So annoying. reply empath-nirvana 14 hours agoprevIs this basically them deploying fine tuned models? It wouldn&#x27;t be very interesting to just be using custom prompts. reply dragonwriter 14 hours agoparent> Is this basically them deploying fine tuned models?From the description of the past outside practice it is marketed as moving into OpenAI&#x27;s offering, it sounds more like its custom prompts, not fine-tuned models. reply bearjaws 14 hours agorootparentMore specifically, it seems like a RAG (retrieval augmented generation) system than fine tuning. reply Method-X 12 hours agoparentprevI think OpenAI will take the most popular \"GPTs\" in their store and launch their own fine tunes. reply bbor 14 hours agoparentprevI don&#x27;t know for sure but I&#x27;d bet BIG money that these do not include automatic fine-tuning, though I still understand them to be a bit more powerful than just \"custom prompts\" -- think templates, or sets of custom prompts for specific (sub-)situations.This is the kind of feature that will prove to be a minor improvement for anyone on this forum, and a complete paradigm shift for the less technically-inclined. IMO. reply itissid 12 hours agoprevInteresting. They are taking the pain out of two things1. RAG based infra building for people(regular joes).2. Compartmentalize or Work around context lengths by introducing threads. reply thih9 13 hours agoprevIs this going to be openai’s moat?E.g. one popular comment in the submission about twitter’s new “edgy ai” was that it could be reimplemented as a chatgpt prompt[1]. Looks like this is even more relevant now.[1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38148845 reply bertil 10 hours agoprevWhat is your impression of how respectful of their ecosystem this was? I saw many “OpenAI killed my start-up”-type reactions. Still, it feels like they are trying to handle obvious use cases in the ecosystem and make things easier to integrate canonically and more transparently. And to handle legal concerns. It was their first ecosystem conference.Then again, iOS felt like a very promising ecosystem when the iPhone 4 came out; I feel like it turned into an addendum&#x2F;duplicate of the web rather than something genuinely original. Android seems more vivid, but not much more. reply TheCaptain4815 4 hours agoprevSo disappointed these only allow for one agent&#x2F;model. Was hoping for an autogen clone + GUI pretty much. Not very useful with single agents reply ofermend 14 hours agoprevExcited about GPT4-Turbo and longer sequence lengths. Looking forward very much for faster inference. We just released Vectara&#x27;s \"Hallucination Evaluation Model\" (aka HEM) today https:&#x2F;&#x2F;huggingface.co&#x2F;vectara&#x2F;hallucination_evaluation_mode..., with a leaderboard: https:&#x2F;&#x2F;github.com&#x2F;vectara&#x2F;hallucination-leaderboard GPT-4 was already in the lead.Looking forward to seeing GPT4-Turbo there soon. reply Vfiorx 14 hours agoprevI don’t understand why more people aren’t creating digital doubles of their brain..? You train your own LLM for practically free and have your own digital double to maximize any and all productivity. Why is there not more of this? reply EZ-E 26 minutes agoparentI have the same feeling.90% of my work is done in Slack and Email threads, if I had a (ideally local) LLM read and learn everything there, it would be immensely helpful - as in it could replace me to answer on most questions and queries. reply vunderba 9 hours agoparentprevSeveral github repos around this concept including Khoj and Danswer. reply ukuina 4 hours agoparentprevWhen local LLMs get better, maybe? reply rocmcd 12 hours agoparentprevTry it and let us know how it works out?I think you&#x27;ll find that \"maximizing any and all productivity\" is both harder than you think and not really something worth striving for.Plus as neat as these advancements are, they are nowhere nearing the ability to create a \"digital double of your brain\" of yourself. There is a vast ocean from where we are are today to the possibility of replicating a brain digitally (if it&#x27;s even physically possible). reply m3kw9 13 hours agoprevSo basically selling prompts but openai keeps the prompt a secret. Is that it? reply lucubratory 3 hours agoparentThey also run an RAG implementation for you, and there&#x27;s probably some value in the store if it can surface high quality apps and have a good rating system. But basically, yeah. This is a complete replacement for every startup that was just a thin wrapper around the API - instead of being real startups those concepts will just be \"GPTs\" on the \"GPT Store\". I don&#x27;t know how realistic or viable their solution is, but the sea of thin wrapper startups was never viable and everyone knew it. reply colesantiago 14 hours agoprevAwesome, I&#x27;ve been waiting for something like this.It looks like we are moving away from apps to web GPTs, this looks like chatbot interface is here to stay and &#x27;AI&#x27; is now the default interface that is to be expected.I also don&#x27;t need to spend lots of money on a developer to test my ideas out, this is great for product validation, I look forward to playing with GPTs.I can see writing, travel and other bots being more enhanced and more powerful, hopefully existing startups will adapt to this change.Exciting and interesting times! reply anonu 13 hours agoprevA couple dozen startups just died. reply runjake 14 hours agoprevhttps:&#x2F;&#x2F;archive.ph&#x2F;fEp7mFor others like me that are getting errors accessing the page. reply sidcool 6 hours agoprevI didn&#x27;t understand this well enough to appreciate it. What&#x27;s the use cases and how about privacy? How&#x27;s it different to regular GPT 4? reply fudged71 15 hours agoprevPoe has done a great job in this space, quite a large marketplace of existing bots. I&#x27;m excited to see what it can do with the extra vision, D•ALLE, and Code Interpreter models. reply rickcarlino 14 hours agoprevI’m very excited about all the new developments but must say that they really dropped the ball on marketing this one. The feature name is ambiguous and unsearchable. reply ilaksh 14 hours agoprevhttps:&#x2F;&#x2F;chat.openai.com&#x2F;gpts&#x2F;editor \"You do not currently have access to this feature\" reply Roritharr 14 hours agoparentProbably a staged rollout once again leaving people outside of the US wait. reply chabad360 14 hours agorootparentNah, I&#x27;m in the US (with a US based account) and I&#x27;m still getting the message it&#x27;s rolling out over the next few days (you have to open a sample to see that message). reply ccakes 14 hours agorootparentprevAhh.. is that why I’ve been on the waitlist since day 1 for ChatGPT Enterprise?C’mon OpenAI, we’re &#x2F;trying&#x2F; to give you money here! reply judge2020 14 hours agorootparentprevI&#x27;m in the US and still don&#x27;t have it. reply ilaksh 14 hours agorootparentprevI mean, I am in the US, have been waiting for the last rollout still.. reply mvkel 10 hours agoprevThis feels like simply a system prompt generator. It gives you a survey to generate a decent system prompt from, then staples that prompt to the front of any interactions with the GPT.It doesn&#x27;t seem to be much smarter than a simple \"tell me how I should interpret inputs from the user.\" reply LouisvilleGeek 7 hours agoparentOne of the things I did not care for is setting my plugin hints from the Custom Instructions setting. It seems this will allow for each \"GPT\" agent to have their own system instructions which will be quite handy. reply littlestymaar 14 hours agoprev> The best GPTs will be invented by the community> We believe the most incredible GPTs will come from builders in the community. Whether you’re an educator, coach, or just someone who loves to build helpful tools, you don’t need to know coding to make one and share your expertise.“Please work for us for free, while we keep all the product of your work for ourselves like we did with the content we scraped on the internet.”OpenAI really is next level parasitism. reply leobg 13 hours agoparentIsn’t that the game they all play? Amazon Marketplace. Apple App Store. Let the guinea pigs run. See which one gets the furthest. Then take away its lunch. reply b3nji 12 hours agoprevForgive my ignorance, isn&#x27;t this various prompts that OpenAi allow you to store, and quickly use? reply ilaksh 14 hours agoprevI can&#x27;t access the GPTs stuff. I haven&#x27;t actually got the last update either with the combined models or anything. reply rco8786 14 hours agoprevDoes anyone know if this is just a \"native\" RAG implementation? Or if it&#x27;s actually fine tuned models? reply minimaxir 14 hours agoparentNative RAG, with likely some secret sauce to align the models a bit better: https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;assistants&#x2F;tools&#x2F;knowledge-...> Retrieval augments the Assistant with knowledge from outside its model, such as proprietary product information or documents provided by your users. Once a file is uploaded and passed to the Assistant, OpenAI will automatically chunk your documents, index and store the embeddings, and implement vector search to retrieve relevant content to answer user queries. reply breadsniffer 10 hours agoprevGPTs replacing AI startups is what happens when they are pushed to ship as fast as possible by VCs and YC. What do they end up shipping? Very small +1 features that are already on OpenAI&#x27;s roadmap. reply JCharante 13 hours agoprev> ChatGPT Plus now includes fresh information up to April 2023I&#x27;m so happy; I can finally ask questions about expo and trpc and get fresh answers. I confirmed this by asking chatgpt about the superbowl winners in 2022 & 2023. reply emadabdulrahim 12 hours agoprevWhen is the updated UI and feature set for ChatGPT rolling out? reply Racing0461 14 hours agoprevBarrier to entry for commercial or useful GPTs&#x2F;Plugins&#x2F;\"Agents\" is almost non-existant since its just a str.concat(hiddenprompt, user_prompt), the secret sauce (ie the weights, chat timeout and context length) are already generated&#x2F;limited by OpenAI and they already have the content moderation&#x2F;\"hr dept\" baked in at the weights level. So even if one was to create a \"story writer helper\" GPT, i don&#x27;t see how it would be of any value generating new, unique and interesting content other than the prompt recipes we already have on reddit&#x2F;r&#x2F;chatgpt (heres 1000 prompts for every use case) that creates netflix like plots (inclusively diverse casting across ethnicities and orientations, socially conscious storylines, modern jargon-filled dialogue, themes of empowerment, progressive characters, and non-traditional relationship dynamics).This will most likely be like the google play store with a 99% of GPTs being a repackaged public prompt. reply caglaroktay 10 hours agoprevI don&#x27;t get where is a place for chatGPT plugins now. are they gonna allow plugins enabled in the new gpts now? reply thereisnoself 11 hours agoprevWhat&#x27;s the file upload limit? I can see many junior roles being replaced if a large corpus of data can be uploaded. reply cagataycali 7 hours agoprevBasically the OpenAI invented TinyAI.id (4 months later) reply Dowwie 13 hours agoprevDoes this summarize to Pre defined custom instructions and workflows? What categories of fine tuning are associated with this work? reply khaki54 14 hours agoprevI felt a great disturbance in the Force, as if millions of voices suddenly cried out in terror and were suddenly silenced. reply cafxx 14 hours agoprevWould be nice also if they fixed the ubiquitous \"network errors\" that happen approximately every single time... reply chankstein38 12 hours agoparentRight?! That then give you no way to interact except to force a regeneration. I don&#x27;t want to waste messages or time repeating a long generation, I want to be able to either continue it or, if it&#x27;s sufficient as is, just respond to it in place. I don&#x27;t understand why you&#x27;re forced to hit regenerate after those. reply duxup 14 hours agoprevIs this just the role and content type data being set as you might with their dev tools? reply jes5199 14 hours agoprevthey mentioned revenue sharing in the keynote, and I&#x27;m eager to find out how that is going to work. There isn&#x27;t much money in the $20&#x2F;month subscription to go around to very many other developers reply ilaksh 13 hours agoparentWhat I was thinking for my own agent hub thing was to sell universal credits and charge per use or token. Then agent developers could specify what they want to charge. reply jes5199 13 hours agorootparentthat&#x27;s kinda interesting but I&#x27;m not sure it maps well to the value added by a GPT app. Like, I&#x27;m imagining that I&#x27;ll do old fashioned API work and GPT will the UI layer - sure, the tokens are the most expensive part, but the value for the customer comes from easy access to whatever is on the backend reply Vfiorx 14 hours agoprevMake and train your own LLM and you can literally duplicate your brain power and productivity. For almost free. Where’s everyone’s digital doubles? reply anonymouse008 13 hours agoprevIs this scary? They said revenue share - it sounds like a streaming platform software licensing model. That sounds like getting paid much less than 70%. reply bluecrab 14 hours agoprevStartup funeral. reply heavyshark 14 hours agoprevAny word on when the GPTs will be available? reply imdsm 14 hours agoparent1 pm PST reply ChrisArchitect 14 hours agoprev[dupe]More discussion over here: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38166420 reply minimaxir 14 hours agoparentHN generally allows multiple related announcements for keynotes such as this. reply partiallypro 6 hours agoprevI&#x27;d love to have a GPT for language learning, granted this would also require vocal inputs and audio. reply alvis 15 hours agoprevI have been utilising GPT to create my own app, and now openAI wants to be the only app that matters. I’m not sure whether I should be excited or not :&#x2F; reply fatherzine 14 hours agoparentEvery prompt you put in somebody else&#x27;s LLM goes into the training set of the next iteration of said LLM, with the explicit purpose of replacing you as a cognitively, and therefore economically, relevant entity. The only dignified move is not to play, though it&#x27;s a very difficult choice. It probably not a winning move, though at this point there are no obvious winning moves -- you and I and all our loved ones will be obsoleted and replaced by tech within the next few years. Concretely, to not play means to stop feeding the machine data, i.e. disconnecting from the digital world. Given how digitalized society is becoming, possibly also from the modern society altogether. Godspeed. reply oezi 14 hours agorootparentI think all technological revolutions have caused similar transformations which obsolete certain types of activities and push novel activities to the forefront.Not playing is certainly possible but could be a losing strategy as well. reply bbor 14 hours agorootparentprevI like to think about it from the perspective of the far future, looking back on me as a historical actor. I have no idea what will happen exactly, of course, but I can&#x27;t imagine a moral&#x2F;social crisis of the past where \"cross your fingers and hope it goes away\" is a move I&#x27;d approve of...That said, your worry is one I definitely share. I guess I just hope more people think of ways they can try to ride&#x2F;shape this wave, rather than stop&#x2F;weather it. reply mickdarling 15 hours agoparentprevWell, they tried to put a government sponsored moat in the way of other people building AI companies that would be competing with them. Thankfully, they mostly seem to have whiffed the ball on that one. This plan of theirs to monetize the creation of agents and other tools that take advantage of their underlying infrastructure is a good secondary kind of moat. Because, if your tool relies on their underlying infrastructure, even if you could build something different, the infrastructure is required. This may be a \"less-evil\" way to keep them building things and making tools available without completely locking out competition. reply cornholio 14 hours agoparentprevWhat did you expect? Surely it was just an MVP, and you expected OpenAI to commoditize its complements? Right?On the long run, if your idea or app can be expressed as a flavor of a general GPT, you will not be able to compete with the AI gorillas. The space for AI startups is with custom, highly niched data or capabilities, that cannot be found in a general corpus, or that you can uniquely generate or control. reply zeroCalories 14 hours agoparentprevWhen your startup is a repackaging of another company&#x27;s tech I don&#x27;t see how you can be surprised when a big player swoops in to kill you off. reply topicseed 15 hours agoparentprevThey will allow revenue sharing so then it&#x27;s a matter of how many customers they&#x27;d be able to offer you for your app; and whether it makes sense for you to distribute through them or bypass them and distribute yourself. reply elforce002 14 hours agorootparentThis business model will only serve them in the long run. Luckily for us, open source llms are getting traction and we won&#x27;t depend on \"open\"AI to implement features on our apps. reply ahmedhosssam 14 hours agoparentprevI thought about the same thing, I&#x27;ve seen a lot of apps that have similar ideas like \"ChatGPT chatbot for your data or your website\", I don&#x27;t know how will they deal with it. reply bbor 14 hours agoprevI think I speak for a few of us AI Doomers here when I say that this makes me excited and terribly anxious at the same time. So... well done OpenAI :). Great news, and a great feature!I have no doubt that this will immensely increase uptake among the less technically literate, since it will allow the techy people in their life (or on the app store) to introduce them with much less friction. It&#x27;ll be like the little examples you can find on the \"New Chat\" screen of every chatbot, but 1000x more engaging reply cubefox 14 hours agoparentYou clearly aren&#x27;t much of a doomer yet. There is nothing exciting about a looming catastrophe. reply MeImCounting 14 hours agorootparentIf the catastrophe is mega-corps getting to monopolize a valuable technology because people saw The Terminator and thought it was a documentary then any announcement from OpenAI is bad news. reply cubefox 14 hours agorootparentThe catastrophe is humanity going extinct from superintelligent AI. Like a native species going extinct after an invasive species arrives. Mentioning Terminator is like saying the Earth is flat because Hitler said it is round. reply MeImCounting 13 hours agorootparentThis reminds of the Eliezer Yudkowsky tweet saying that AI was going to hack our DNA and use our bodies to mine bitcoin or something. Ridiculous fearmongering.I have probably read more sci-fi than the average HN user but the whole \"superintelligent AI is going to kill us all\" hysteria is among the more ridiculous ideas I have ever heard.Really though I have entertained all the doomers propositions and none of them seem any more likely than the plot of the Matrix. The ideas that prop these fears up are based on layers of ever more far fetched hypothesis about things that do not exist. If you have a novel reason why AI poses an x-risk I am more than interested in hearing it.Here is a really interesting quote that I think might go against some of the misanthropic tendencies of doomers and the tech crowd in general but it really is more relevant than ever: “There was also the Argument of Increasing Decency, which basically held that cruelty was linked to stupidity and that the link between intelligence, imagination, empathy and good-behaviour-as-it-was-generally-understood – i.e. not being cruel to others – was as profound as these matters ever got.” reply cubefox 13 hours agorootparentA species doesn&#x27;t automatically get more altruistic towards other species once it gets smarter. Look at how many species humanity drove to extinction. reply MeImCounting 13 hours agorootparentTrue humans have been remarkably ignorant throughout our short history. Though you might notice though that most folks dont go around abusing animals or hurting other people on purpose. Take from that what you will.Give this essay a read if youre interested in good faith arguments about the danger of AI at the current state of development. https:&#x2F;&#x2F;1a3orn.com&#x2F;sub&#x2F;essays-propaganda-or-science.htmlMaybe together as a species we can avoid hellish cyberpunk dystopias brought on by regulatory capture of the most powerful technology created by humans thusfar. I can only hope. reply og_kalu 11 hours agorootparent>Though you might notice though that most folks dont go around abusing animals or hurting other people on purpose. Take from that what you will.It doesn&#x27;t matter what \"most folks\" go about doing. If anything it makes things all the scarier. All this destruction we&#x27;ve caused to so many other species and we weren&#x27;t even trying. reply cubefox 12 hours agorootparentprevI know this piece. Here is a better article by a top biologist and CRISPR pioneer:https:&#x2F;&#x2F;nitter.net&#x2F;kesvelt&#x2F;status&#x2F;1720440451059335520But anyway, that&#x27;s just human terrorists using future AIs to build biological weapons. But the much greater danger is superintelligent AI causing human extinction by itself. reply MeImCounting 12 hours agorootparentThat is not an article that is a series of short-form tweets. If it was an actual article I would absolutely read it but I cannot see referencing a bunch of tweets from a self-proclaimed expert as in any sense good faith. I asked if you had any novel ideas about how \"superintelligent AI\" gives a valid x-risk but you failed to provide any interesting ideas. So I wont engage with what from my perspective is fearmongering for the benefit of malicious corporations.I still dont see any studies with control groups and reproduceable experimental procedures saying that any AI agent is in any way more useful to a &#x27;terrorist&#x27; than unrestricted access to the internet. reply cubefox 2 hours agorootparent> That is not an article that is a series of short-form tweets. If it was an actual article I would absolutely read itSo you didn&#x27;t even read the tweets? They contain the link to a paper.> self-proclaimed expertEsvelt is absolutely a recognized expert on biotechnology. The authors of the article you linked to are not.> I asked if you had any novel ideas about how \"superintelligent AI\" gives a valid x-riskI just responded to your article about bioterrorism, which was not about x-risk. Arguments about x-risk were made elsewhere, but I&#x27;m sure you would dismiss them because they don&#x27;t contain studies with control groups. replyed_mercer 5 hours agorootparentprevLooks like we’re gonna go extinct anyway, so might as well have a shot at AGI. reply cubefox 2 hours agorootparentI think without AI we wouldn&#x27;t go extinct for a long time. There are no other likely extinction risks. Toby Ord has a nice book (The Precipice) about various forms of extinction risks, and he basically says the same thing. reply pphysch 14 hours agorootparentprev*laughs in Accelerationism*https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Accelerationism reply 26 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenAI has launched custom versions of its AI tool ChatGPT, dubbed GPTs, which let users tailor AI to their specific needs without the necessity for coding expertise.",
      "The functionality includes web searching, data analysis, and image creation, available for personal or corporate applications, with plans to expand access for more users soon.",
      "OpenAI plans to open a GPT Store showcasing creations by verified builders and fortified its commitment to user data privacy, promising not to share interactions with builders."
    ],
    "commentSummary": [
      "OpenAI announced the introduction of customizable versions of ChatGPT known as GPTs, which allow users to adapt the AI tool to specific needs without advanced programming skills.",
      "GPTs are designed for individual or company applications and can be customized for various tasks, such as web searching, data analysis, and image creation.",
      "OpenAI plans to launch a GPT Store, featuring creations from verified builders, and has implemented strong privacy measures, ensuring user data control and confidentiality of interactions with GPTs."
    ],
    "points": 509,
    "commentCount": 284,
    "retryCount": 0,
    "time": 1699294724
  },
  {
    "id": 38169504,
    "title": "ApeFest Attendees Report Eye Pain and Vision Loss Due to Laser Performances",
    "originLink": "https://www.404media.co/bored-ape-yacht-club-conference-eye-pain-vision-loss-yuga-labs/",
    "originBody": "Subscribe Join the newsletter to get the latest updates. Success Great! Check your inbox and click the link. Error Please enter a valid email address. 🖥 Sign up above for free access to this article. Attendees at a conference for Bored Ape NFT owners are reporting waking up in the middle of the night following laser and blacklight-heavy performances with extreme eye pain and vision loss. Yuga Labs, the parent company of Bored Ape Yacht Club, hosted ApeFest in Hong Kong from November 3-5. The event was open to holders of Bored Ape NFTs, a crypto project that peaked in 2021 and recently crashed to a two-year low, costing many investors thousands of dollars. “I woke up at 04:00 and couldn’t see anymore. Had so much pain and my whole skin is burned. Needed to go to the hospital,” one attendee posted on the last day of the event. “The doctor told me the uv of the lightning of the stage did it. It has the same effect as sunlight. Still can not see normally..” “Same here for me and +1. I had eyeglasses, so was a bit spared, but skin is burned and +1 had the same degree of issues with eyes,” someone replied. This post is for paid members only This post is for members only Subscribe Already have an account? Sign in",
    "commentLink": "https://news.ycombinator.com/item?id=38169504",
    "commentBody": "Bored Ape conference attendees wake up with eye pain, vision lossHacker NewspastloginBored Ape conference attendees wake up with eye pain, vision loss (404media.co) 500 points by anigbrowl 12 hours ago| hidepastfavorite360 comments pavlov 12 hours agoThe Financial Times [1] has attendee numbers:“About 2,250 people attended the Saturday evening party at a cruise terminal, said organiser Yuga Labs, one of the pioneers in this market. Since then, more than a dozen have posted on social media complaining of a burning sensation in the eyes and sometimes also impaired vision.”Honestly that’s a lot more proud Bored Ape bagholders than I would have guessed. But what do I know — maybe Beanie Baby meet-ups also pull crowds of thousands.[1] https:&#x2F;&#x2F;www.ft.com&#x2F;content&#x2F;fe9a3fa3-edfc-41df-a333-1f91e6248... reply Kye 11 hours agoparentThat&#x27;s about the attendance of an average furry convention (or roughly one NordicFuzzCon) where the A&#x2F;V crew will know what they&#x27;re doing with the lights. Questionable ideas and their events and services struggle to attract competent staff for some reason. They&#x27;re always getting hacked or melting faces. reply astrange 10 hours agorootparentI help run an anime con with, uh, 15 times this reported attendance. The most important thing is to keep your competent volunteers entertained so they&#x27;re happy to run a rave one weekend a year that doesn&#x27;t blind anyone. reply seanp2k2 9 hours agorootparent+1 I&#x27;ve also got some friends who do sound, lighting, and DJ for anime &#x2F; furry cons. It&#x27;s a fun little niche. reply CoastalCoder 8 hours agorootparentLots of requests for \"Hamster Dance\"?(Sorry, couldn&#x27;t resist the pun. No actual offense intended.) reply Kye 8 hours agorootparentWe&#x27;ll know the furry and anime communities are finally back together after decades apart once a cosplayer and a fursuiter go head to head with Caramelldansen and Hamster Dance at Floor Wars.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=mMmGSsy7IEo reply CoastalCoder 7 hours agorootparentThanks for the link! I would never have imagined an event like was real. replykayfox 7 hours agorootparentprevWe (furry con A&#x2F;V) care a lot about safety, so there&#x27;s no way you&#x27;d see UV-C lamps at a furry con dance. We also measure sound levels and keep them in check, have free water stations around the room, have medical trained security staff around, keep an eye on the crowd, etc. reply Kye 7 hours agorootparentI&#x27;ve gotten to know some A&#x2F;V people doing furry music, and I&#x27;m always appreciative of how dedicated they are to their craft. reply SrslyJosh 11 hours agoparentprevClearly, Yuga Labs has no incentive to either exaggerate the number of attendees or understate the number of people harmed. =) reply pavlov 11 hours agorootparentEverybody in crypto is so honest and the numbers they love to tout are always real, so I don’t see any reason to be so cynical. Unless of course one is a fiat-blinded NPC sheep who hates disruptive innovation and wants to kill the children in Africa and El Salvador who are being rescued from poverty by the limitless power of blockchain energy. reply reactordev 9 hours agorootparentsaving the world, one block at a time... reply mikhael28 11 hours agorootparentprevGrade A sarcasm. reply plagiarist 8 hours agorootparentprevThis isn&#x27;t even sarcasm, this is just collecting various actual statements in one location. I know I have been called genocidal for expressing an opinion that we should maybe run medical studies prior to massive ivermectin doses for everyone in the third world. Same energy. reply mikeyouse 11 hours agorootparentprevI was curious when I heard the initial reports so I went looking for pics on Twitter — the ones I found looked absolutely nothing like the 2,500 attendees and more like 250.. reply dylan604 10 hours agorootparentAre these the same people that count attendees at inauguration speeches? reply qingcharles 7 hours agoparentprevI was once (probably) the largest secondary market retailer of Beanie Babies, 1997-1998.I used to organize \"Beanie Fairs\" (which were scams).Can confirm attendance of thousands. reply rospaya 10 hours agoparentprevI&#x27;m more interested in the gender distribution. reply zoky 8 hours agorootparentOh, we all know that these things are just the wurst. reply Nursie 7 hours agorootparentprevhttps:&#x2F;&#x2F;twitter.com&#x2F;CryptoTurkweb3&#x2F;status&#x2F;172136398921162773...Looks almost exclusively male to me. reply bragr 10 hours agoparentprev>Honestly that’s a lot more proud Bored Ape bagholders than I would have guessedThe breakdown of locals vs people that flew in would be revealing. e.g. passionate owners vs cryto-casual hongkongers looking for a party. reply 1vuio0pswjnm7 9 hours agorootparentAccording to the Zeke Faux book \"Number Go Up\", one needs to \"own\" a Bored Ape YC NFT to be admitted. The author had to pay $20,000 for one of these in order to gain admission to this event. reply Nursie 7 hours agorootparentIn this case there appear to be other ways. Ape holders could &#x27;+1&#x27; for the whole three-day event for $269 (US$ I think).It also appears[1] that the party was after an open day of sorts, and you could buy tickets[2] for the open day and party for 388HK$ (about $50 US).It is a little hard to tell exactly what the open house included and whether it gave access to the event that had the problem, so I could be wrong.[1] https:&#x2F;&#x2F;www.artazine.com&#x2F;news&#x2F;apefest-2023-nft-bored-ape-yac... [2] https:&#x2F;&#x2F;openhouse.boredapeyachtclub.com&#x2F; reply somsak2 10 hours agoparentprevhttps:&#x2F;&#x2F;archive.ph&#x2F;sgHix reply anigbrowl 9 hours agoparentprevSuperb picture editing work reply endgame 9 hours agoprevThere&#x27;s no reason the party with unsafe UV lights had to be a crypto-party. While I&#x27;m no NFT bagholder, it bothers me that the article is mostly about dunking on cryptobros than talking about the dangers of hazardous lighting rigs at a show. At any event, how is an attendee supposed to assess such a thing? It&#x27;s not like you can DYOR. reply rainsford 9 hours agoparent> At any event, how is an attendee supposed to assess such a thing? It&#x27;s not like you can DYOR.That&#x27;s why I think the cryptobro angle is relevant to the story, because assessing trust in the folks putting on a conference is the way for attendees to figure out if they&#x27;re likely to be blinded by the lighting or face other issues. And the type of people the whole cryptocurrency and especially NFT scene attract seem at least in my opinion to be way more likely to put on a conference with hazardous lighting than more reputable organizers. Everything about that whole space gives off a Fyre Festival vibe, which seems like pretty fair warning about going to an actual event organized by cryptobros. reply s1artibartfast 8 hours agorootparentIt was a rave open to the general public. How many people actually consider the reputation of someone setting up a party or rave. Half the time it IS someone out of their mind on drugs in a condemned warehouse or the middle of the desert . reply Spivak 8 hours agorootparentI think you picked the one avenue where people are wary of the person putting on the event because you are gonna be tripping and need to be in a reasonably safe environment, you will get your hearing destroyed in only a few hours by some idiots who think that turning it up to 11 makes for a better show, and raving is a community of regulars that have collective memory for bad reputation.It&#x27;s funny because what you&#x27;re describing with the warehouse is \"the afters\" which unbeknownst to someone who isn&#x27;t a regular has already established a reputation and been vetted by the community because they&#x27;re all word of mouth.The point is that lack of reputation likely meant that despite the fact that it was public the usual partygoers weren&#x27;t there. You typically need either a trusted organizer&#x2F;venue or a trusted artist and their techs who&#x27;s done this before and aren&#x27;t gonna do something stupid. reply technion 7 hours agorootparentprevWait.. just to be clear are you saying you could get in without owning one?I heard for years the \"benefit you don&#x27;t understand\" is the exclusive club access. Exclusive nightclubs were a specific use case I kept seeing argued. If anyone can walk in it&#x27;s not that exclusive. Heck people might just be a raver that doesn&#x27;t care about crypto but showed up to dance. reply Nursie 7 hours agorootparentIt seems like the club night&#x2F;party was open to people who bought tickets for the \"Open House\" event they ran that day, for about $50 US - https:&#x2F;&#x2F;www.artazine.com&#x2F;news&#x2F;apefest-2023-nft-bored-ape-yac...Hard to be sure though. reply plagiarist 8 hours agorootparentprevI would have mistakenly assumed there are like regulations or common sense preventing blasting people in the eyes with raw UV regardless of the event.This story is a rude awakening for me that one does have to asses event planners for competence, not only for remote Fyre Festivals but also for regular venues. I hope the attendees recover. reply endgame 9 hours agorootparentprev> That&#x27;s why I think the cryptobro angle is relevant to the storyThat&#x27;s an interesting perspective, and reminds me a bit of the \"libertarian paradise\" at the end of http:&#x2F;&#x2F;bitsim.beepboopbitcoin.com&#x2F; . But the fact that it&#x27;s framed more as a dunk than as genuine sympathy for those affected reminds me of other anti-crypto reporting like \"Line Go Up\" ( https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YQ_xWvX1n9g ). Which makes it really hard to recommend it as a way to break people out.Given a choice between reporting which mocks crypto victims and reporting which might actually help get people away from the rigged casino, I&#x27;d prefer to see articles aiming for the latter. reply rainsford 9 hours agorootparentI would actually tend to agree. I think the organizers deserve all the scorn and dunking in the world, but I&#x27;m less comfortable about that framing for those impacted. \"Conference organized by the monkey JPG people\" should be a major red flag, but it wasn&#x27;t a red flag for the target audience for probably the same reason \"monkey JPG\" wasn&#x27;t in the first place.Similar to the Fyre Festival, the real villains are the scammers (or idiots, if you&#x27;re giving them the benefit of the doubt) running the show, not the people who fall victim to the scam. Cryptocurrency and NFTs fall into that category where I think recognizing the BS requires a level of technical knowledge that is unreasonable to expect everyone to have. Dunking on people who don&#x27;t get the technical argument for why NFTs and the people promoting them are BS does not feel like the way. reply eru 8 hours agorootparentNFTs, and even monkey JPG NFTs, are perfectly sensible when viewed as art projects. (Not as finance: financially, monkey JPG NFTs are rather silly.)And by &#x27;sensible&#x27; I mean sensible compared to weird things other people either do for art or spend money on for art. Compare eg https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Performance_art (and that&#x27;s far from the most egregious corner). reply hnfong 8 hours agorootparentArguably Christie&#x27;s (the auction house) invented the concept of NFT before there were NFTs.I mean, the value of art may be undervalued by society in general, but paintings that sell for hundreds of millions of dollars is just insane. The art collectors are just using art pieces in the same way crypto folks are using NFTs. The only apparent difference is that there&#x27;s no physical object for NFTs, but the physical object was never the main value proposition for traditional high value art anyway. reply jjoonathan 7 hours agorootparentWhy insane? An asset with a value that can be arbitrarily manipulated in a plausibly deniable fashion is extraordinary useful for money laundering and tax avoidance. reply meowface 8 hours agorootparentprevYou&#x27;re absolutely right, but at least there is some underlying art-intended-as-art there. Just as much of the stock market can be viewed the same way but at least there usually is an underlying company with some value behind the stocks. With NFTs there really is nothing there. It&#x27;s finance without any art component. reply zoklet-enjoyer 7 hours agorootparentYou know there&#x27;s more to NFTs than Bored Apes, right? Look at the stuff Beeple was doing before NFTs hit the mainstream. NFT was about the art before it became a get rich quick thing. Still is if you ignore Yuga Labs and people like them. reply meowface 8 hours agorootparentprevI disagree. They&#x27;re generally not intended as art or consumed as art. Performance art, modern art, post-modern art, avant-garde art - while not always to everyone&#x27;s taste - is usually intended in a purely artistic sense. (And in my opinion much of it actually is good. Jackson Pollock was regarded as nonsense or non-art and still is by many but he&#x27;s one of my favorites, for example.)NFTs that start themselves as NFTs, like Bored Ape NFTs, are a solely financial product intended to convey status and something to eventually be pawned off to others for a profit. It&#x27;s a market, not an art community. Not all artists who sell things they make as NFTs necessarily fit this mold, but things like Bored Apes definitely do. reply eru 1 hour agorootparentI&#x27;m sure there&#x27;s plenty of art and artists who are very aware of themselves being used as tools in a status game.See eg https:&#x2F;&#x2F;www.theguardian.com&#x2F;artanddesign&#x2F;2022&#x2F;apr&#x2F;14&#x2F;receipt... reply hannasanarion 8 hours agorootparentprevHelping people get out is not the only purpose for media about a thing. Informing people who aren&#x27;t in it is also legitimate news, and can be important if the people outside need to know what&#x27;s going on inside so that they don&#x27;t get taken in in the first place. reply endgame 8 hours agorootparentThat does not require dunking on those who got sucked in. reply plagiarist 8 hours agorootparentprevWell also it&#x27;s totally fair to say they should know better than to spend thousands of real money on idiotic computer-generated cartoons. But it&#x27;s really unexpected that any event would blind the attendees. The first is a pyramid scheme and the second is like a war crime or something. There&#x27;s zero financial incentive to blind the attendees. reply eru 8 hours agorootparentprevGuess which kind of reporting gets more engagement? reply kylebenzle 7 hours agorootparentprevMolly White has made a career out of mocking crypto bros, she&#x27;ll have a hay day with this one!https:&#x2F;&#x2F;twitter.com&#x2F;molly0xFFF reply hbbio 8 hours agorootparentprev\"in my opinion to be way more likely\"I have no data but still, here&#x27;s my bias. reply endofreach 8 hours agorootparentIt is so annoying... ehag do people believe \"in my opinion\" means? Also: is everything you believe actually based on facts?If not, by posting your comment just showed a whole bunch of biases that are secretly controlling you. reply pauselaugh 7 hours agorootparentin customer experience design the user&#x27;s perception is objective reality.so it&#x27;s not \"i have no data but here&#x27;s my bias,\" it&#x27;s \"anecdote IS the singular of data, not only that, but data doesn&#x27;t matter: what matters is what happened to me.\"i can&#x27;t imagine going to a movie theater and getting your arm lopped off by a whirling sawblade, but since you have no data on the frequency of it, your anecdote is an inaccurate overgeneralization, and you should not dare to assert that an entity that throws raves that burn and blind people are more likely to jump into trends recklessly with such limited information.I don&#x27;t have a spreadsheet handy, but I will say that entities that consider these types of happenings as good investments are trying to embody the \"don&#x27;t miss out on being cool at the cool party\" moreso than the \"there is no party because we are interested in having a conservative and responsible image to bolster our solid technological foundation\" side of the spectrum.I&#x27;ll get back to you after I get back from the ayahuasca + squirrel wingsuit base jump retreat my retirement fund manager is holding reply userbinator 9 hours agoparentprevAt any event, how is an attendee supposed to assess such a thing?If you see clear tubes emitting a light blue glow, close your eyes and get away ASAP.One of the few positive things about the pandemic may be that it&#x27;s educated lot more people on what UVC lamps look like, as they&#x27;re often used --- away from people --- for sterilisation purposes. reply sanderjd 8 hours agorootparentWhat? I&#x27;ve seen lots of tubes that may or may not have been clear, I really don&#x27;t know, emitting light blue glows. Like, I used to go into Spencer&#x27;s at the local mall a couple decades ago... I would have no clue what the difference is here. reply userbinator 7 hours agorootparentThey&#x27;re made of a distinctively clear quartz glass (used because regular glass is quite effective at blocking UVC), and you can see the filaments at the ends. Hopefully you won&#x27;t have to ever experience looking at a lit UVC tube, but they appear very different from regular fluorescents.The comment here has a link to a previous incident of this happening, which shows what UVC tubes look like: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38170902 reply sandworm101 9 hours agoparentprevBecause part of cryptobro culture is thinking you know more than anyone else, and projecting an image of access to secret insider knowledge. Likely one of them just decided to be a lighting designer because they thought they new more than the professionals. reply tamimio 7 hours agorootparent> Because part of cryptobro culture is thinking you know more than anyone elseThat’s basically the case with most if not all internet cultures. reply alephxyz 9 hours agoparentprevBring sunglasses (and ear protection while you&#x27;re at it) when going to a concert. reply endgame 9 hours agorootparentI can&#x27;t stand the mixing at many concerts unless I wear earpro, but thanks for the reminder. reply gymbeaux 5 hours agoparentprevIt’s kind of a metaphor eh? reply Animats 9 hours agoprevThis is pathetic. The original promoters behind BAYC, two guys from Florida and the son of some official of a bank in Africa, were rave promoters. They botched their metaverse project, Otherside, but that was technically ambitious. The people running it now can&#x27;t even run a medium-sized rave properly. Earlier ApeFest events were only for BAYC holders and had a celebrity lineup. This time they sold tickets to anyone.BAYC is the top end of NFTs. Almost everything else is worse. Overall, NFT value has declined something like 98%.(The NFT crowd irks me because I want to see good metaverses, and between the NFT clowns and Zuckerberg, the whole field crashed.) reply X6S1x6Okd1st 8 hours agoparent> BAYC is the top end of NFTs. Almost everything else is worse.Judged by floor price, not by artistic value reply sanderjd 8 hours agoparentprev> between the NFT clowns and Zuckerberg, the whole field crashedThis is well put. I remember back when both of these visions for \"metaverse\" were heading up the hype curve. It&#x27;s really a bummer how disappointing both visions are. reply silisili 9 hours agoparentprev> two guys from FloridaAnd that&#x27;s all I need to know. I avoid buying from, selling to, or doing business with anyone from FL if I can, I&#x27;ve been burnt too many times. I&#x27;m not sure if the state attracts shady characters, or produces them, or both. reply acjohnson55 9 hours agorootparentI would hope that this is a forum where we can do better than broad generalizations about a place with over 20 million people. reply kyleyeats 7 hours agorootparentAs someone from Florida, I&#x27;m disappointed. I clicked in here for some cryptobro hate. reply birdyrooster 8 hours agorootparentprevAnd yet, there are few who disagree and it&#x27;s worth mentioning. reply seanp2k2 9 hours agorootparentprevShysters love it because it&#x27;s the closest you can get to a \"tropical paradise\" in the CONUS with relatively cheap housing and low CoL, then you can run away to Cuba or Mexico if things get bad, or fly to Europe and NYC easily when things are good. Plenty of yacht babes, drugs, and yacht babes who will hang out with you in exchange for drugs, which is also a great draw for some of these types of folks too. reply reactordev 9 hours agorootparentprevThat&#x27;s a pretty broad generalization. I could say the same for the Bahamas. Or Nigeria. Or New Jersey. Definitely Russia. So let&#x27;s just address that there are bad actors everywhere, no matter from where, and that you must do you due diligence when dealing with people for business. What you probably meant to say is \"There seems to be more lawlessness in FL than usual\" which I would definitely agree with you.There&#x27;s good people working at Disney, Universal, L3&#x2F;Harris, etc. It&#x27;s not all crazy Florida man in shorts with pet alligators. reply quesera 9 hours agorootparent> What you probably meant to say is \"There seems to be more lawlessness in FL than usual\" which I would definitely agree with you.This itself is interesting enough.I&#x27;ve been on the inside of a few risk management systems, with hundreds of signals feeding into a scoring product.Invariably, and justifiably from the data, there is a \"Florida rule\", which affects the user&#x27;s final risk score by a few percent.This is mostly old-school deterministic stuff with manual weighting, tweaked over years of time by people far away from (and with no particular opinions about) Florida. The newer mystery box AI stuff, trained on many years of data, also shows the same bias!I&#x27;m sure there&#x27;s a story in this data. reply glompers 8 hours agorootparentNo doubt there is. The new story (and a story probably on-topic in the context of this discussion) is the quantity of financial arbitrage firms who have lately been relocating their employees and headquarters _to_ Florida. reply trillic 8 hours agorootparentFlorida has no state income tax and has a homestead bankruptcy exception. Two huge reasons for high-risk traders to prefer living there.Under Florida bankruptcy laws you are allowed to exempt an unlimited amount of value in your home or any other property which is covered by the homestead exemption.Buy a $50 Million house. Fill it with expensive art. If you lose everything, at least you still have your $50M house filled with $500M in Artwork to fallback on. reply reactordev 8 hours agorootparentThis is why. It&#x27;s a tax, retiree&#x27;s, high-risk trader&#x27;s, haven. The Florida bankruptcy laws are so lax that people often file multiple times throughout their lifetimes. And still run for president. &#x2F;s To quote a man behind bars for attempted murder and was on a Netflix series - Florida&#x27;s favorite - Tiger King: \"I&#x27;m never gonna financially recover from this.\" is something you hear often in Florida. Both from visitors, and from residents. reply silisili 9 hours agorootparentprevYou&#x27;re probably right...I definitely don&#x27;t think every single person and business is corrupt, I lived there for 2 years and enjoyed it even!But as a swappa&#x2F;eBay buyer and seller, it&#x27;s hard to ignore trends I guess. reply IvyMike 11 hours agoprevI learned last night that there are low-cost cards that test for UVC exposure; here is an example:https:&#x2F;&#x2F;www.amazon.com&#x2F;QuantaDose-Light-Power-Visibility-Tec...They are usually used to verify that a purported UVC lamp is working, rather than working as a measure of protecting your eyesight, but maybe I should start bringing one of these to any concerts I go to. reply CodeWriter23 10 hours agoparentI would not rely on a stain test designed to be used in close proximity to a light source being sensitive enough to detect eye-damaging UV at a distance. It might react only slightly, and the stain may not be discernible from the control in such circumstances. reply daniel_reetz 10 hours agorootparentEven clear \"sunglasses\" made of polycarbonate can provide excellent UV protection, but no telling if it&#x27;s enough to protect against a blacklit room or stage laser that&#x27;s been set up by incompetents. reply 0cf8612b2e1e 9 hours agorootparentI believe even regular contacts provide some amount of UV protection. reply dylan604 10 hours agoparentprevI&#x27;m thinking UV safety glasses too. If you attend with one of these cards, you&#x27;ve already been subjected to them too. At least with glasses, you can have some protection. Unless you&#x27;re going to wears these over your eyes as protection reply samtho 8 hours agorootparentThe point, I&#x27;m assuming, would be to simply leave if those things indicate there is a lot of UV present. I certainly would not hang around regardless of adequate protection. reply iNerdier 12 hours agoprevCould someone have used uv-c black lights thinking they looked cool and not realising about the severe health hazards? reply bertil 12 hours agoparentYes, and it’s apparently not the first time it happens in Hong-Kong. Easy access to electronics, no real control. reply SketchySeaBeast 11 hours agorootparentLeave it to NFT bagholders to find new ways to get burned. reply RenThraysk 11 hours agorootparentprevYephttps:&#x2F;&#x2F;wwd.com&#x2F;eye&#x2F;parties&#x2F;hypebeast-party-uv-lights-injuri... reply seanp2k2 9 hours agorootparentoof, yeah those were powerful UV-C disinfecting bulbs. I&#x27;ve got half a dozen Elektralite eyeBall UV fixtures that I use for parties in conjunction with other lights, which are 18x3w 385nm UV LEDs per fixture. I don&#x27;t stare directly into them as they&#x27;re pretty intense, but they&#x27;re still firmly in UV-A territory and don&#x27;t hurt your eyes. They do light up anything fluorescent super bright though! - UVA (315-400 nanometers) - UVB (280-315 nanometers) - UVC (180-280 nanometers) reply anon84873628 8 hours agorootparentSince you are familiar with the topic:So now when I go to a place with blacklights, how do I know someone didn&#x27;t get confused (or is just ignorant) and used bulbs that can burn my eyes? Thanks. reply kayfox 7 hours agorootparentIf the blacklights are LEDs they are UV-A and safe. UV-C LEDs exist, but they are so expensive noone would make a stage lighting fixture out of them.If they are fluorescent tubes that look kinda purple, black or really deep blue when off, they are UV-A and safe. These tubes use a phosphor to convert UV-C light to UV-A light and are made of wood&#x27;s glass to filter out all but that UV-A light.If they are fluorescent tubes that are clear, they are UV-C and unsafe.Theres a third type that uses phosphor and regular glass that emits UV-A and some white light, but those are almost never seen in stage lighting fixtures.There are also UV-A arc lamps like the Wildfire IronArc series, which are safe as long as they have the dark purple wood&#x27;s glass filter installed on them (these lamps do not ship without such a filter, but sometimes you encounter used ones that the filter was broken in or otherwise removed). replyWistar 11 hours agoparentprevExactly what I was thinking. UV-C. reply klyrs 10 hours agoparentprevReplace \"uv-c black lights\" with \"radium paint\" and it doesn&#x27;t change the answer: yeah somebody could have done just that. reply Always_Anon 11 hours agoprevAfter I learned that NFTs were essentially just signed git commits, I wrote them off as a hilarious grift. I feel bad for the people that were injured, so here&#x27;s my advice: run away from NFTs and crypto. RUN AWAY! reply astrange 10 hours agoparentA signed git commit is actually immutable. An NFT is generally the signed text of a URL, which kind of isn&#x27;t. reply LelouBil 9 hours agorootparentWait what, I thought it was at least the hash of the image or something. reply matteoraso 9 hours agorootparentNo, NFTs technically have nothing to do with images, or files at all. The way it works is that you write a random string and save it on a blockchain, which becomes the NFT. From there, you can use a lookup table to map the NFT to a URL that links to an image that&#x27;s stored on the owner&#x27;s server. NFTs are truly the worst of all grifts. reply codetrotter 8 hours agorootparent> you write a random string and save it on a blockchain, which becomes the NFT. From there, you can use a lookup table to map the NFT to a URL that links to an image that&#x27;s stored on the owner&#x27;s serverThe way which they are mapped to images vary from NFT to NFT.But let&#x27;s look at Bored Ape Yacht Club specifically.Collection https:&#x2F;&#x2F;opensea.io&#x2F;collection&#x2F;boredapeyachtclubPick a random bored ape. For example #1337.https:&#x2F;&#x2F;opensea.io&#x2F;assets&#x2F;ethereum&#x2F;0xbc4ca0eda7647a8ab7c2061...From there we can see the bored ape Ethereum \"smart contract\"https:&#x2F;&#x2F;etherscan.io&#x2F;address&#x2F;0xbc4ca0eda7647a8ab7c2061c2e118...This contract is responsible for minting all of the original Bored Ape NFTsClick on \"contract\". And then click \"read contract\".Scroll down. There is a method named \"tokenURI\". It takes an integer as parameter. Let&#x27;s query it with value \"1337\" (the id of the bored ape we are interested in knowing about).Result:ipfs:&#x2F;&#x2F;QmeSjSinHpPnmXmspMjwiXyN6zS4E9zccariGR3jxcaWtq&#x2F;1337Let&#x27;s visit that URL via an IPFS gateway.https:&#x2F;&#x2F;cloudflare-ipfs.com&#x2F;ipfs&#x2F;QmeSjSinHpPnmXmspMjwiXyN6zS...It contains JSON data. {\"image\":\"ipfs:&#x2F;&#x2F;QmWjXFUVNjavM4hDzXeQYGXGCXjV86mP7kPEaTeP5bohae\",\"attributes\":[{\"trait_type\":\"Background\",\"value\":\"Yellow\"},{\"trait_type\":\"Fur\",\"value\":\"Dark Brown\"},{\"trait_type\":\"Clothes\",\"value\":\"Bone Necklace\"},{\"trait_type\":\"Mouth\",\"value\":\"Bored Unshaven\"},{\"trait_type\":\"Hat\",\"value\":\"Horns\"},{\"trait_type\":\"Eyes\",\"value\":\"Bloodshot\"}]}We make note of the IPFS URL for the image from here.ipfs:&#x2F;&#x2F;QmWjXFUVNjavM4hDzXeQYGXGCXjV86mP7kPEaTeP5bohaeAnd visit that one via our chosen IPFS gateway.https:&#x2F;&#x2F;cloudflare-ipfs.com&#x2F;ipfs&#x2F;QmWjXFUVNjavM4hDzXeQYGXGCXj...So there you have it. Via the Bored Ape smart contract we retrieved the JSON data and image file for Bored Ape #1337 from IPFS. reply ceejayoz 7 hours agorootparent{\"trait_type\":\"Eyes\",\"value\":\"Bloodshot\"} is a particularly well-chosen example here. reply qingcharles 7 hours agorootparentprevRetrieving that image relies on someone continuing to pin the file indefinitely, though, right? reply codetrotter 7 hours agorootparentThe neat thing about IPFS is that the IPFS URL is based on a hash of the file.So having saved that file offline on my harddrive, I can later re-add it to IPFS. ipfs add Downloads&#x2F;1337.png added QmWjXFUVNjavM4hDzXeQYGXGCXjV86mP7kPEaTeP5bohae 1337.png 140.68 KiB &#x2F; 140.68 KiB [=============================================] 100.00%Notice that because I added the same file to my local ipfs, it got the same id as it had when I retrieved it.QmWjXFUVNjavM4hDzXeQYGXGCXjV86mP7kPEaTeP5bohaeSo, IPFS doesn&#x27;t even require that the file is indefinitely pinned.And if I have a public node, others can now once again retrieve the file using the same known id.The person that re-adds it doesn&#x27;t have to (nor can) specify the id of the file; that&#x27;s calculated from the hash of it.(Although, IPFS does also have a way to serve mutable data, so in those cases the id is not calculated directly from just a hash. But in the case of the image we looked at here we see that it was.)The difference between needing to pin forever and merely needing to have a copy of the same file somewhere outside of IPFS which is then re-added to IPFS and gets the same ID is a fine distinction perhaps, but still an important one IMO.And I think that even if all of the IPFS nodes that currently have a copy of the Bored Apes would go offline, there is probably someone out there who would notice that and for whatever reason be motivated to make all of the apes available again on IPFS with their same original id&#x27;s that they had by re-adding the original files. reply qingcharles 4 hours agorootparentRight, it&#x27;s like a torrent in that way. I have dozens of torrents where I am waiting for that one guy to -- bless his soul -- open up his torrent client and seed the last 14 bytes.I guess the best thing to do if you own an Ape is to store the image on your HDD and put a copy in a safe-deposit box, and then pin it yourself whenever you can. reply LelouBil 1 hour agorootparentprevThanks for the explanation, it makes a bit more sense that it is an ipfs URL. reply thrwy_918 8 hours agorootparentprevAFAIK it is often perfectly possible to actually encode image data on-chain, it is just much more expensive (and much less common as a result) reply astrange 5 hours agorootparentPeople get upset when you do this because storing data on-chain is, possibly, infinitely expensive, since you can never get rid of it. reply thrwy_918 4 hours agorootparent>since you can never get rid of itif you&#x27;re an actual believer in NFTs, isn&#x27;t that kind of the point? reply TaylorAlexander 8 hours agorootparentprevPerhaps but the standard NFT is a URL encoded in the chain. Yes this is as absurd as it sounds. reply wyldfire 6 hours agorootparentIt seems to me that cryptocoin adjacent things somehow attract well meaning people who just don&#x27;t understand what parts of the technology are critical to its functionality. Maybe they understand one use case for a signature but don&#x27;t really understand what signature is and more importantly what it isn&#x27;t.And sadly enough - several cryptocoins themselves are also this way.I used to think cryptocoins were truly revolutionary. But the tech space is so rife with abuse, theft, scams that it doesn&#x27;t feel worth it. reply quickthrower2 9 hours agorootparentprevIt could be. I don&#x27;t know about the ERC for NFTs and I can&#x27;t be bothered to read it, but in theory just store the IPFS address for the image.In reality though it is an implementation detail. An NFT is just a coloured coin, and derives it&#x27;s value from someone else thinking they can buy it and sell it on to someone else &#x2F; OR some kind of bragging rights value, i.e. I can afford $100k on X useless thing (e.g. watch, gold plated iPhone etc.) so therefore I am rich. reply meepmorp 8 hours agorootparent> I can afford $100k on X useless thingPretty sure the X useless thing was $44 billion. reply komali2 9 hours agorootparentprevSince finding this out a couple years back it&#x27;s always my delight to see this realization happen for others.We knew NFTs were stupid... we just couldn&#x27;t comprehend how incredibly stupid they actually are. reply gryn 9 hours agorootparentWhat baffling is how did they get this popular ? Plenty of stupid things to go around, why this specific one? Who was the marketing genius that convinced a critical mass to buy something worse than sand in the middle of the desert. reply anon84873628 8 hours agorootparentAt a first level, similar phenomenon to why trading cards or any other collectible fads have their periods of excitement. Is digital beanie babies at all surprising?Then there were also folks who wanted to use them as an open standard for, like, \"metaverse assets\" and whatnot. And while there are many different ways to solve that technically, you also have the blockchain proponents looking for any problem that blockchain can solve... So there is the added energy from that synergy as well.Then I believe there are fairly serious businesses looking at NFTs for things like digital albums sales (or other ways to cultivate and monetize parasocial relationships) while disintermediating the traditional platforms. reply csomar 5 hours agorootparentprevNo. You just own a number in a smart contract. reply mattdesl 1 hour agorootparentprevIt’s typically an IPFS hash of an image that is stored on the blockchain, rather than a mutable URL. Some generative artworks will even store all of their code on-chain.Ultimately, the chain is meant to be a ledger of transactions (like a decentralized Christie’s auction house maintaining provenance), not a storage layer. reply GuB-42 7 hours agoparentprevYep, git is essentially a blockchain. NFTs but also all cryptocurrency transactions are essentially git commits containing a digital signature from the owner (to prove ownership), what the owner is transacting, and the public key of the receiver. Sometimes it is a bit more complicated, there may be some instructions others than \"I am A, send the X to B\", but that&#x27;s the idea.The big difference between git and a blockchain like Bitcoin and Ethereum is how you do \"pull requests\". On git, usually, there is a designated central repository that is controlled by project maintainers, and when you submit a pull request, they check it, merge it to central, and others get it back from here.With blockchains, there is no designated central repository, to merge a pull request, you have to win a sort of lottery, commonly by reversing a crypto hash (mining). The more invested you are into the system, the higher the odds. If you win, you show it to the the world, do the merge, and everyone pulls from you. By randomly selecting who does the pull among the most invested limits the chance of one bad actor ruining the system for everyone. In case of a conflict, the branch with the most commits wins, because that&#x27;s the one with the most \"lottery winners\" and therefore the most likely to be the right one. To encourage others to merge that pull request, there is usually a reward for those who do (transaction fees).Technically, it is very sound, just like git is, in a sense. The \"proof-of-work\" lottery is wasteful in electricity though, and of course, what you do with it is another matter. Just like git can be used for good (ex: linux) or for bad (ex: developing malware). reply Yadayadaaaa 11 hours agoparentprevHey! i mean you own that commit hash! and you can show it to others through an ipfs link!So basically ipfs won?! or lost?! reply yieldcrv 10 hours agorootparentthe worst were one that didn&#x27;t even host on IPFS, they used their own domain names reply Akronymus 10 hours agoparentprevNot even a signed git commit.But rather, a signed receipt that says you paid for the nft. reply simpsond 8 hours agorootparentIt’s actually a mapping between an unsigned integer and public key, stored on all nodes. It only requires a signature to persist the state change. reply yieldcrv 10 hours agoparentprevessentially. a whole industry cropped up around hosting them, which mostly defeats the pointthere are a lot of NFTs that are better implementations, the primary improvement being that there is no external hosting and they build the visual data into the smart contract function for a renderer to interpretIve done a lot with SVGs that way. others upload dependendies like three.js on the blockchain in other smart contracts, so when called they can return into yours and you have more sophisticated rendersI think artblocks does something like that, not sureMost of the financial sector NFTs have an onchain svg too, like all the Uniswap liquidity poolsfirst movers are often poorer implementations of the technology, its up to the consumers to discern and if they dont then there is no incentive to act differently reply LelouBil 9 hours agorootparentThis whole smart contract stuff looks very interesting.Do you have resources for someone that only know the basics of cryptocurrencies and blockchains ? reply yieldcrv 9 hours agorootparentbuild some EVM (ethereum virtual machine) scaffolding in a local environment athttps:&#x2F;&#x2F;scaffoldeth.io&#x2F; reply GuB-42 11 hours agoprevThere have been stories of venues using UVC (germicidal) lamps because they looked pretty and fit standard fixtures. It resulted in sunburns and lots or eye pain.I don&#x27;t know if that&#x27;s what happened there, but I think that using the same fixtures for dangerous yet pretty looking germicidal lamps and regular lighting is not the smartest idea. reply autokad 9 hours agoparentthe article I read on google news said they believed lights used for cleaning may have been used, so yeah reply samspenc 12 hours agoprevI wonder how much they charged and made in ticket revenue. They couldn&#x27;t even hire stage and lighting professionals to determine the right amount of UV and lighting at a conference hosting hundreds or thousands of folks?!?!> On social media, some suspect photokeratitis — damage to the eye caused by UV exposure. If that’s what caused the Apes’ agony, they likely won’t be blind forever: it’s a temporary condition. People usually get it from being in bright sunlight and snow for too long, and not at concerts, which typically have stage managers and lighting professionals who understand how to safely set up a show. reply timeon 10 hours agoparentThey would probably call them `tradprofessionals` and would think they know better. reply JoshTko 9 hours agoparentprevIn bright snow your pupils are super small as the brightness is constant. At a night show your pupils are dilated in let in maximum light intermittent if exposure in these circumstances could be quite different in terms of short&#x2F;long term damage reply FFP999 12 hours agoparentprevCould have? Definitely. Thought to? Maybe not. After all, when you already know you&#x27;re the smartest guys in the room... reply ryukoposting 7 hours agoparentprevIt was more like thousands of folks, based on other comments. reply scottyah 11 hours agoparentprevOr were those affected just not used to the amount of sunlight they were exposed to while travelling there? reply theandrewbailey 11 hours agoprevAlready on https:&#x2F;&#x2F;web3isgoinggreat.com&#x2F; reply jeffhuys 11 hours agoprevI really am surprised by all the “lol you buy NFT so already blind” folks. Actually pretty disgusting.Disclaimer: I don’t own NFTs. reply jrflowers 9 hours agoparentPeople making those jokes can’t see how disgusting that behavior is. It is like they were blinded by the bright lights of cynicism. When I look into these people’s souls it is like staring into a vast black abyss. reply astrange 10 hours agoparentprevLuckily they should recover in 2-3 days, this is usually more like a sunburn than permanent injury. reply unixhero 11 hours agoparentprevThere is no need for a disclaimer reply Ensorceled 8 hours agoparentprevReally? These grifters have stolen millions from innocent patsies all while clogging social media with scams and arrogant “stay poor” memes and directed billions of VC funding into creating useless tools to facilitate the scams. And we’re disgusting for maybe thinking maybe karma had something todo with this? reply Grustaf 11 minutes agorootparentThe people that still own Bored Apes are more likely to have lost millions though reply anigbrowl 8 hours agoparentprevI feel sorry for the attendees (though as noted their suffering is temporary, and they were probably drawn by the lure of easy money). I&#x27;m delighted about the organizers&#x27; further loss of reputation and likely legal troubles. reply Yadayadaaaa 10 hours agoparentprevWhy not?Its called Schadenfreude and tbh the amount of NFT or Crypto scams, garbage and co people had to endure the last years is horrendes.Its one thing to state in a comment that you shouldn&#x27;t use NFT or crypto, but unfortunate we are not discussing banning them activly. reply stu2b50 10 hours agorootparentI think being into NFTs and crypto makes you a fool.But I do not think fools should suffer vision damage, or that they deserve harm on them. reply asadotzler 8 hours agorootparentFools alone, maybe deserving of some sympathy. Greedy get-rich-quick fools who derided everyone not participating in the scam, nah, no sympathy from me, none at all. reply s1artibartfast 9 hours agorootparentprevWhat is this horrible suffering people had to endure?IMO, it was basically all self inflicted anguish due to \"someone else is wrong on the internet\" syndrome.If someone wants to gamble their money away, you don&#x27;t have to care. If they want to talk about it, you don&#x27;t have to listen.People act like living in a world with people who act differently was some kind of ordeal. they have totally lost the ability to not engage with topics they don&#x27;t enjoy, and then take joy in the pain of others. reply bobsmooth 11 hours agoparentprevPeople love laughing at people they think are below them. In the current political climate, pretty much all of the groups that were laughed at are protected now and deriding them is uncouth. Thankfully, it will always be okay to laugh at stupid people but only in specific contexts. reply isk517 10 hours agorootparentIt isn&#x27;t ok to make fun of people just because they are stupid, no, these people can be made fun of because they are stupid AND greedy. reply CoastalCoder 8 hours agorootparentWe&#x27;re all stupid and greedy to various degrees. Making fun of people probably isn&#x27;t conducive to a happy life. reply GartzenDeHaes 9 hours agorootparentprevFor several years, anyone who pointed out problematic or criminal aspects of crypto were subject to a torrent of abuse from the people you are identifying as the victims. reply dpkirchner 10 hours agorootparentprevIn this case I think it&#x27;s more accurate to say the laughter is directed at people who said they&#x27;re above us normies that aren&#x27;t hodling any NFTs. reply ericflo 8 hours agoprevIf you read this article and it gave you pleasure, examine those feelings. I find the generally schadenfreudic reaction to be disgusting. These people just wanted to get together to talk about something they enjoy and you don&#x27;t. reply djur 8 hours agoparentIt&#x27;s a wonder that there isn&#x27;t more sympathy for a group of people who say \"have fun staying poor\" so often they started using an acronym for it. reply csomar 5 hours agorootparentShouldn&#x27;t they have double sympathy though. First, for being idiots and second for getting eye damage? reply plagiarist 8 hours agorootparentprevI think it&#x27;s funny to see smug people lose money but I don&#x27;t think it&#x27;s funny if they are seriously injured and possibly permanently disabled. reply pauselaugh 7 hours agorootparentit&#x27;s a fixation on the extremes, no?the people making jokes like \"I&#x27;m surprised everyone wasn&#x27;t wearing sunglasses already, their future is so bright\" (ok, it was me, right now) are those who had witnessed some form of the extreme cryptobro asserting anyone not on board is mentally disabled.i&#x27;d assert there&#x27;s nothing unhealthy or immoral about laughing at the poetry of the stereotypical actor doing a sterotypical action and it paralleling the generalization regarding the trope of the failures of NFTs: \"a declared cool reality shifter, but potentially harmless fad meeting physical reality with a literally harmful event.\"sociopathic, of course, but to those not considering internet based social media \"society\" it&#x27;s in containment reply cthulha 6 hours agoparentprevHN1: \"Let people enjoy whatever they enjoy\"HN2-100: HN1: \"Not like that\" reply thepasswordis 5 hours agorootparentThe \"regulations\" resulted in them going to a party and nearly losing their eyesight. It was implied safe, and it wasn&#x27;t. How is this not absolutely validating to the entire ethos of strawman you are trying to burn here? reply meepmorp 8 hours agoparentprevIf you read this article and immediately wanted to admonish others for what you imagine their reactions to be, examine those feelings. reply anigbrowl 10 hours agoprev&#x27;I never thought nihilistic hominids would melt my face,&#x27; sobs man who invested his entire net worth in the Nihilistic Hominids Melting People&#x27;s Faces launch. reply andrewstuart 11 hours agoprevWhen NFTs turned up I had people telling me in some detail, very seriously, how NFTs were going to change the world. reply 1270018080 9 hours agoparentThis website had tons of grifters shilling it during the peak. A lot of them raised money somehow too. reply yieldcrv 10 hours agoparentprevthe success of PFP collections aren&#x27;t any barometer of that thoughaside from showing the size of the existing collector speculation market reply yesod 12 hours agoprevSounds very much like arc eye (welder&#x27;s flash). Very painful, caused by UV. reply whycome 11 hours agoparentYou would think the pain would be immediate though. reply retrac 9 hours agorootparentIt&#x27;s like a sunburn. The radiation damages the DNA of the cells. DNA damage like that triggers a cellular self-destruct mechanism. It&#x27;s a cancer-prevention mechanism. So over the next few days as the skin or corneal cells start to go through their normal replication cycle, the DNA damage is discovered and causes those cells to die off. This causes an injury just like if another mechanism like heat or chemical damage, had killed those cells. Inflammation, oozing, pain, scarring. reply JohnFen 11 hours agorootparentprevA couple of years ago, I had a mishap with a moderately powerful laser, meant for engraving, grazing quickly across my left eye (I was wearing appropriate eye protection).It wasn&#x27;t until the next day that it started feeling like someone poured sand in it. It took a few days to return to normal. reply NotYourLawyer 9 hours agorootparentIf you were wearing appropriate eye protection, this would not have been possible. reply yoyohello13 10 hours agorootparentprevThat&#x27;s the big danger with solar eclipses. It&#x27;s not bright enough to hurt at the time. The damage only starts to become apparent after a few hours or the next day. reply postalrat 10 hours agorootparentWhen I went to see a total eclipse I was surprised that even a tiny slit of the sun was still hard to look at. Not sure why anyone would try to stare at that. reply dekhn 8 hours agorootparentgalileo (the galileo) used to stare at the sun for extended periods of time (without a telescope), to the point he could identify sunspots. But IIUC he looked at it near the horizon when it&#x27;s attenuated by more atmosphere. reply yesod 10 hours agorootparentprevIts like a sunburn blister. Takes a while to develop, usually whilst you&#x27;re asleep. When you wake up, you open your eyes and rip the blisters open. I&#x27;m told it feels like hot sand being poured in your eyes at that point. Definitely one to avoid! reply api 11 hours agorootparentprevUnfortunately no, like a sunburn sometimes you don&#x27;t feel it until later. This can also happen when hiking across snow covered plains under clear very bright skies. It&#x27;s called snow blindness. reply roym6 11 hours agorootparentprevOnly if the source is extremely strong. E.g. the sun reply diego_sandoval 9 hours agorootparentAs a kid (~6 years old), I once tried to look at the sun for as long as possible, competing against myself. Luckily, I didn&#x27;t do any noticeable damage, but I feel like someone should have told me about UV radiation and the damage it can cause to the retina. reply dusted 1 hour agoprevThere&#x27;s a lot of UV-C generating LEDs in the wake of Corona which seems to now be put in everything, I&#x27;ve seen halloween decorations with florescent paint using them, china don&#x27;t care. reply mtlmtlmtlmtl 8 hours agoprevWhat actually happens at a conference based on the common ground of \"owning\" a jpeg with a generic CGI chimp in it?Especially when the cat&#x27;s already out of the bag that this was all a scam to begin with?I don&#x27;t understand. reply djtriptych 10 hours agoprevReads like a Silicon Valley screenplay draft. reply Hoasi 12 hours agoprevBad art causes eye pain. You really can&#x27;t make that up. reply sfrinlan 11 hours agoprev\"Richard, these are real people with real crotches, and they&#x27;re burning.\" reply actionfromafar 9 hours agoparentBurning crotches?! reply shwaj 7 hours agorootparentQuote from Silicon Valley sitcom. Probably intended as a response to a different post, not as a top-level post. reply RenThraysk 11 hours agoprevSome nitwits used UVC light just cos it looked purple? reply aatharuv 11 hours agoprevNow I wonder whether wearing photochromatic lenses (transitions lenses) would have reduced the damage. I do believe they are triggered by UV. reply somat 10 hours agoparentPlain clear polycarbonate lenses would have worked. polycarbonate is opaque to uv light.Er... might of worked. A lot of light can come in around the sides of normal glasses. so it would depend how the uv was reaching the eye.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Polycarbonate reply semiquaver 11 hours agoprevhttps:&#x2F;&#x2F;archive.ph&#x2F;JLQtn reply jl6 1 hour agoprevJust another weeknd in crypto. reply bethekind 11 hours agoprevTLDR; Basically too much UV light resulted in people getting skin burns and burned, red eyesThe likely cause imo? Someone bought&#x2F;used UVB fluorescent tubes&#x2F;lights (~200nm) instead of LED UVA (~400nm) as blacklights. They&#x27;re cheaper, brighter and have a stronger effect.Unprotected skin&#x2F;eyes can feel the difference inregular defocusing to relax the eyesI wonder if we lived in nature instead of cities, if there would be times when we&#x27;d relax our eyes and just sit in the grass with defocused vision. It would be nice, I think. reply solarpunk 12 hours agoprevi can&#x27;t shake this just being an incredibly hamfisted way to garner attention for an otherwise dead project. reply resolutebat 9 hours agoprev\"all this time i thought the lasers were going in the other direction\" -Molly Whitehttps:&#x2F;&#x2F;twitter.com&#x2F;molly0xFFF&#x2F;status&#x2F;1721549511950999579 reply asadotzler 8 hours agoparentMolly killed that. She killed it, cleaned it, cooked it, and served it up hot. Damn she goes hard. reply qingcharles 7 hours agoparentprevOh shit, the top comment burns \"All my retina gone.\" reply snorkel 11 hours agoprevit was the My Eyes Are on Fire Festival reply achanda358 11 hours agoparentMy Eyes Are on Fyre Festival reply topato 8 hours agoparentprevTotally thought you were double referencing this classic rave song from a decade ago https:&#x2F;&#x2F;youtu.be&#x2F;IUGzY-ihqWc?si=eK53NHAEc5cegFCw reply Cadwhisker 10 hours agoparentprevThe NFT alternative to the Burning Man festival reply anon373839 5 hours agoprevThis is a terrible thing to have happened. I’m confused about one aspect: is party light show equipment something you just hack together? I always thought it was kind of pricy and made by companies who specialize in lighting. reply throwaway20222 11 hours agoprevIf I remember correctly, your eyes don’t adjust to black light the same it would as full spectrum, and will stay mostly dilated as if it were in the dark. This can increase the chances of photokeratitis, or basically sunburn on the eyes with high powered shows like this.I am obviously not a dr reply hunter2_ 9 hours agoparentSimilar to the effect of wearing one earplug or one headphone&#x2F;IEM: when one ear is exposed to much louder sound than the other, the combined exposure across both ears doesn&#x27;t trigger the natural protective mechanism [0] to the degree that it would be triggered if that same level of exposure was applied to both ears, so the one ear gets totally wrecked. Also not a doctor and would appreciate a counterpoint from anyone who knows this to be a myth.[0] http:&#x2F;&#x2F;hyperphysics.phy-astr.gsu.edu&#x2F;hbase&#x2F;Sound&#x2F;protect.htm... reply jonny_eh 11 hours agoparentprevJust like looking at a solar eclipse. reply neilv 8 hours agoprevThis is horrible. I hope they all recover fully and quickly. And that there&#x27;s some kind of lesson learned by everyone in a position to prevent it from happening again. reply asadotzler 8 hours agoparentYeah, maybe don&#x27;t put your health and safety in the hands of hucksters and charlatans. reply neilv 8 hours agorootparentI&#x27;ve been criticizing \"crypto\" as much as almost anyone, but blaming the victim of eye damage at an event doesn&#x27;t seem appropriate. reply gatkinso 12 hours agoprevjust looking at bored apes has been doing this to my vision for a while reply xeckr 11 hours agoprevI predict that this disaster will end up driving up the price of the Bored Ape NFTs. reply lgleason 7 hours agoprevAnecdotally, I&#x27;ve seen more fast and loose things happen at crypto conferences than others I have attended. Some were pretty big safety concerns, things that legally could cause massive headaches etc. so this does not surprise me. reply atleastoptimal 10 hours agoprevWeb3 bear market entering its body horror phase reply morkalork 10 hours agoprevReminds me of an urban legend about a rave party in Russia where the laser show blinded people in the audience. reply dexzod 11 hours agoprev>> a crypto project that peaked in 2021 and recently crashed to a two-year low, costing many investors thousands of dollars. First you lose your money and now your eyesight reply EVa5I7bHFq9mnYK 3 hours agoprevThe schadenfreude is strong here. reply metacritic12 9 hours agoprevWhat if they provide an NFT as compensation for those who burned their eyes? I bet more than half would accept it. reply notyograndma 11 hours agoprevWow BAYC&#x27;s offering free Lasik now reply skerit 12 hours agoprevBlacklight so strong people got sunburn? reply bertil 11 hours agoparentIt’s not much brighter than usual, but it’s UVC, a higher frequency&#x2F;shorter wavelength than what we are used to on the surface of the Earth. UVC reacts with (di-)oxygen to make ozone, so it’s blocked by the ozone layer. The smell is very distinct and it burns eyes and skin readily. reply tyler569 12 hours agoparentprevThis sounds to me like someone put in real UV fluorescent tubes instead of blacklight tubes, those things are dangerous. reply joezydeco 11 hours agorootparentCheap blue LEDs will leak the dangerous parts of the UV spectrum. I&#x27;ve had projects hit import snags in the EU because they got spot checked - the overseas contract manufacturer used parts which were out of spec. reply hsuduebc2 9 hours agoprevUV light for entertainment? What&#x27;s the point? reply kstrauser 8 hours agoparentBlacklights are cool.These weren&#x27;t the blacklights you&#x27;d normally use for entertainment. reply hx8 9 hours agoparentpreventertainment reply meindnoch 10 hours agoprevYikes. Those receptors are not growing back... reply muffa 10 hours agoprevThey should rename their site to 403media, the article is behind a paywall. reply hunter2_ 9 hours agoparentTheoretically 402 would be best, although I suppose 403 is closer to reality. reply codetrotter 8 hours agorootparent429media. Because every request is too many requests. Unless you pay. reply unixhero 11 hours agoprevWho was this conference for again? reply jrflowers 8 hours agoparentVisionaries reply honkycat 9 hours agoprevFew things I hate more at a concert than a light show pointed directly into my eyeballs!! reply fortran77 10 hours agoprevThis has happened before when the event planners have put the wrong type of UV lights around. Instead of \"UV-A\" blacklights, they used shorter wavelength UV-C lightshttps:&#x2F;&#x2F;hongkongfp.com&#x2F;2017&#x2F;10&#x2F;26&#x2F;partygoers-left-burns-ligh...It&#x27;s easy to order the wrong tubes on AliExpress.... reply unsupp0rted 9 hours agoprevNormally HN is HN. In these comments HN is Reddit: cheap sarcastic “haha I’m smart they’re dumb” jokes.Very little curiosity or intriguing anecdotes, just “I was smart enough to identify NFTs were a scam from the get-go, so now enjoy your eye damage losers”.It’s a cheap way to feel good about oneself, but it makes us look like what our detractors say we look like. reply CoastalCoder 8 hours agoparentSadly I agree. Poor @dang is probably looking forward to the end of this work shift. reply jd24 10 hours agoprevWhere&#x27;s the yacht? reply logicallee 8 hours agoprevCan anyone recommend an insurer where can I take out an insurance policy that covers against this sort of thing and any other weird and random spookie stuff at a distance, in order to cover medical costs and any quality of life issues that result?Some sort of literal insurance policy would make me feel better after reading stories like this.It probably wouldn&#x27;t be too expensive, people tend to inflate risks based on saliency, I bet the actual risk is very low, but it will give me enough peace of mind to be worth it. reply evan_ 7 hours agoparentWouldn’t just regular medical&#x2F;vision insurance cover this?Or temporary disability insurance reply smm11 11 hours agoprevLooks like they tried to kill the prosecution. reply hindsightbias 11 hours agoprevI heard they had it there because Chernobyl was not available. reply readthenotes1 11 hours agoprevWhy is this news? Doesn&#x27;t buying a bored ape kinda guarantee you&#x27;re blind to begin with? reply layer8 11 hours agoparentIt’s a double-blind test. reply mjhay 11 hours agoparentprevGiven the demographic overlap with Funko Pops, I&#x27;d tend to think so. reply kobalsky 11 hours agorootparentwhat? people hate funko pops now? reply delecti 10 hours agorootparentThey were neat tchotchkes for a bit, but they just kept making absurd quantities of them, and for increasingly irrelevant side characters. There are currently over 15,000 different figures. Plus I think people are losing their appetite for \"collectibles\" that are just mass produced plastic. reply CobrastanJorji 11 hours agoparentprevTrust, but verify. reply artursapek 9 hours agoparentprevkek reply NotYourLawyer 9 hours agoprevI been blinded. all my eyes gone. this just lasered please help me. reply ceejayoz 12 hours agoprevCited in the article:https:&#x2F;&#x2F;twitter.com&#x2F;crypto_birb&#x2F;status&#x2F;1721222853394121193\"Thanks for great apefest logistiscs guys @yugalabs & @BoredApeYC. Incredible event and met plenty of amazing people. Still, as dozens of others, I’ve almost lost sight this night.\"I mean, c&#x27;mon. \"Thanks, incredible event, it almost blinded me\"? reply IvyMike 11 hours agoparentIt&#x27;s a matter of taste, of course, but the event does not look incredible to me.https:&#x2F;&#x2F;twitter.com&#x2F;slandermepls&#x2F;status&#x2F;1721352598945677544 reply krsdcbl 11 hours agorootparentBrilliant and truly innovative stage design, mounting the \"do not ever look into those\" scan lasers vertically, at head height ... ... reply phkahler 10 hours agorootparentThere are regulations on that stuff in some countries. A friend of mine is in lighting and says their company generally avoids lasers due to the hassle of compliance. reply seanp2k2 9 hours agorootparentIn the USA, you need an FDA variance to operate one at events. https:&#x2F;&#x2F;www.proxdirect.com&#x2F;support&#x2F;variance has some info but the most important parts are: - Laser projector must be mounted and have the lowest point on the beam at least 10 fee above the floor and 10 feet horizontally from anywhere a person could go -- this is why many laser shows just do \"laser sky\" effects that are just projecting wide horizontal lines even though they can do up&#x2F;down effects as well - beam must be terminated into something non-reflective -- can&#x27;t shoot it off into the sky - absolutely no scanning the audience -- again, the lowest point of the beam has to be at least 10ft above the floorRelated standards: - ANSI Z136.1* ‘Safe Use of Lasers’ (currently available, but general for all laser use) - ANSI Z136.10* (to be published shortly, but specifically covers laser light show safety)https:&#x2F;&#x2F;www.lasershowsafety.info&#x2F;us-laws.html has more info including some about how to possibly do audience scanning with special equipment, but you can see from all of these rules why it&#x27;s a lot easier to just avoid lasers for events and clubs. Pyrotechnics are a lot of red tape too. reply dekhn 8 hours agorootparentin optical research you typically end your beam into a beam dump https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beam_dump IIUC at CERN it&#x27;s basically a big chunk of graphite that heats up a bunch when you drain your beam into it. reply echelon 7 hours agorootparentprevTo a broad degree, the industry self-regulates and comes up with its own safety mechanisms, guidelines, hardware and software controls, etc.I used to be a member of ILDA [1], which developed a number of the protocols, serialization formats, and safety measures. They&#x27;ll certify hardware and operators, and they&#x27;ll kick out members that engage in crowd scanning. They don&#x27;t really have much of an enforcement mechanism, though.In the US, the FDA and FAA have regulations on laser displays [2]. You&#x27;re supposed to apply for variance, but many smaller venues and operators skip this outright.It can be incredibly dangerous if care isn&#x27;t taken. These multi-watt lasers can easily burn holes into things, with your tissues being among the easiest.These folks were clearly crowd scanning at eye level.[1] https:&#x2F;&#x2F;www.ilda.com[2] https:&#x2F;&#x2F;www.lasershowsafety.info&#x2F;us-laws.html reply JeremyNT 10 hours agorootparentprevIt&#x27;s not loading for me, here&#x27;s a nitter link: https:&#x2F;&#x2F;nitter.cz&#x2F;slandermepls&#x2F;status&#x2F;1721352598945677544 reply brianbreslin 11 hours agorootparentprevDidn&#x27;t they say 2500 people came? reply polygamous_bat 11 hours agorootparentThey also said millions of people are trading NFTs every day or something, I don’t remember. reply hanniabu 11 hours agorootparentprevAlmost like the event isn&#x27;t happening at the time of that recording... reply jandrese 9 hours agorootparentprevImagine going to the event and all you have to look at for the entire show is hideous monkey \"art\". If this is what the people like then maybe they didn&#x27;t deserve to have eyes in the first place. reply JumpCrisscross 11 hours agorootparentprevI’m surprised they didn’t stuff the room with paid actors. The closest tangible value to NFTs is that they grant membership to an exclusive social club. reply kosolam 10 hours agorootparentSadly, they don’t need to. The people who actually buy into this are so stup*d that it’s not worth spending a penny more than a lousy eyes blinding “party” reply RockRobotRock 10 hours agorootparentprevLike the first episode of Silicon Valley reply smnrchrds 10 hours agorootparentprevFor what it&#x27;s worth, the photo in the article itself shows a much larger crowd. The video might be too early or too late into the event. reply libraryatnight 10 hours agorootparentprevThey do this with Discords and bots to make interest look high, I can&#x27;t believe they didn&#x27;t think to do it with an in person event. Actors must be a bigger pain to get than bots? reply _jal 10 hours agorootparentHumans who will work for jpgs are hard to find. reply RajT88 10 hours agorootparentprevGiven all the wild stuff to do in Hong Kong, this looks pretty middle of the road. reply philwelch 10 hours agorootparentprevNeither incredible nor credible; what a paradox. reply zoklet-enjoyer 11 hours agorootparentprevThe video pans to the audience near the end. 2500 people sounds right https:&#x2F;&#x2F;x.com&#x2F;dj_soda_&#x2F;status&#x2F;1721510442621861925?s=20 reply arthur_sav 11 hours agorootparentprevThat&#x27;s because you need the lazers to see how amazing it was reply krisoft 11 hours agorootparentYeah, clearly the camera had a UV filter. We are missing out on the brilliance :) reply DonHopkins 11 hours agorootparentprevCAUTION: DO NOT LOOK INTO LASER WITH REMAINING EYE! reply ethanbond 12 hours agoparentprevThis industry is truly the gift that keeps on giving reply latchkey 11 hours agorootparents&#x2F;gift&#x2F;grift&#x2F; reply nameless912 11 hours agorootparentI know this isn&#x27;t what you meant, but \"the grift that keeps on griving\" gave me an excellent laugh. reply nevi-me 11 hours agorootparentIt&#x27;d have to be s&#x2F;gi&#x2F;gri&#x2F; reply margalabargala 11 hours agorootparentNo, it would have to be s&#x2F;gi&#x2F;gri&#x2F;w reply brink 10 hours agorootparentprevIt looks like latchkey edited their answer to be correct. lol Took me a bit trying to figure out what was meant by this. reply latchkey 10 hours agorootparentI didn&#x27;t edit anything. replywhalesalad 12 hours agoparentprevnft enthusiasts are not the sharpest tools in the shed reply ceejayoz 11 hours agorootparent\"Not the brightest bulbs\" was right there. reply jrflowers 10 hours agorootparentMaybe they were having trouble seeing the joke reply nkozyra 10 hours agorootparentprevThis is what happens when you&#x27;re short sighted before hitting the \"reply\" button. reply 404mm 11 hours agorootparentprevThank you!Also possible candidates:A few watts short of a lightbulb.Their lights are on but nobody’s home. reply forty 10 hours agorootparentIn french, we have \"they are not not the sharpest knives of the drawer\" but my favorite one is \"he is not the penguin who slides the furthest\" :DOn the light related ones, I only know \"they are not lights\" which is a bit to bare bones to my taste, but works well in french reply the-smug-one 10 hours agorootparentWe have the knife in the drawer one in Swedish too. My favourite one is \"The wheel is spinning but the hamster is dead\" however. reply Gibbon1 1 hour agorootparentThat feels a lot like &#x27;the lights are on but nobody is home&#x27; reply sheepscreek 9 hours agorootparentprevNever heard the penguin one in any language but it sure is hilarious! reply lobochrome 10 hours agorootparentprevI hope I can remember that one. reply steanne 10 hours agorootparentprev\"they are not bright\" is common in english reply littlestymaar 9 hours agorootparentprev> but my favorite one is \"he is not the penguin who slides the furthest\" :DDid you too learned that one in the related thread on &#x2F;r&#x2F;France last year[1]? Or was it something that you heard IRL?[1] https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;france&#x2F;comments&#x2F;uhazwh&#x2F;connaissezvo... reply forty 1 hour agorootparentI heard it in real life and I Googled it to check the person speaking did not make it up ^^ reply TeMPOraL 11 hours agorootparentprevPeople there were definitely some half a micrometer short of seeing the light. reply pixl97 10 hours agorootparentprev5 watts short of a 10 watt bulb (halfwit) reply DonHopkins 11 hours agorootparentprevOne taco short of a combination plate. reply davidw 10 hours agorootparent\"A few fries short of a happy meal\" reply kstrauser 8 hours agorootparentprevThey were, for a couple of glorious hours. reply otikik 11 hours agorootparentprevHey now. reply cwillu 9 hours agorootparentprevThe blind leading the blind. reply ClassyJacket 11 hours agorootparentprevI don&#x27;t get it reply Kye 11 hours agorootparentThey pointed bright UV lights meant for sterilizing air in vents at attendees rather than UV lights meant for this purpose. reply pynappo 11 hours agorootparentprevthe original post is about people near-blinded from the very bright&#x2F;intense lighting, and therefore using \"not the brightest bulbs\" is funnier. reply tanseydavid 11 hours agorootparentprevBravo -- well-played!!! reply pavlov 11 hours agorootparentprevCrypto on the whole is a scene where Sam Bankman-Fried was considered very smart while he was busy conducting the most inept financial crimes in modern history. The bar is low indeed. reply dangerboysteve 11 hours agorootparentThe NFT grift is the truest example of the Greater fool theory. reply dreamcompiler 11 hours agorootparentThe Gnon-Fungible Token (GFT). reply selimthegrim 10 hours agorootparentThe ??? is there but not the profit. reply rpigab 11 hours agorootparentprev\"was\"?You don&#x27;t understand how loyal his followers are. reply hanniabu 11 hours agorootparentprevYou do know tradfi has had it&#x27;s share as well, right? Enron? Madoff? reply mandmandam 10 hours agorootparentprevWe - humans - could have had digital money - fast and fee-less, micro-transactions, decentralization, secure, anti-spam. Everything Bitcoin claimed to want to be.Instead we - the tech community - got smug about scams. People here get smug even toward the victims of scams.The attitude has been &#x27;Who gives a flying fuck about the details of any crypto tech, or the potential of digital money&#x27;. &#x27;It&#x27;s all a scam, so I don&#x27;t have to learn anything about it&#x27;.The baby got thrown out with the bathwater.The people working on actually good crypto have nothing to do with SBF. The only way in which they&#x27;re affected by this is the extent to which they get tarred with the same brush.Edit: Y&#x27;all proving my point. You really don&#x27;t see that? ... Look at how many of these responses lack table-stakes domain knowledge, missed the point, lack all imagination, and yet have a condescending tone. It&#x27;s wild. reply yunwal 10 hours agorootparentFast and fee-less? Does that work on a distributed ledger? Is there some magic I’m missing to make consensus quick and cheap?Micro-transactions were never going to be feasible considering gas fees required to run a sufficiently secure blockchain app.Decentralized? I’m still confused. Has anyone actually solved the Oracle problem with crypto? How are we actually getting decentralization?Anti-spam??????? Ok that’s literally just crazy talk right? In what way does crypto have anything to do with spam? Afaict everything crypto-related still requires an email address to sign up if that’s what you mean. reply mandmandam 9 hours agorootparentBlock lattice tech has been around for seven years. All your points would be answered by looking into it - it&#x27;s not a secret.Eg, Nano uses it. It&#x27;s been working, efficient, safe, fast ( It&#x27;s also not obvious to me that cryptocurrencies have value beyond \"buying things you&#x27;re not allowed to buy\".The same could be said about cash but that&#x27;s an important aspect of life. Whether we&#x27;re talking about drugs (like cannabis or shrooms) or getting an abortion, there&#x27;s quite a few things you may not like to be confronted about by your local police. reply JohnFen 10 hours agorootparent> The same could be said about cash but that&#x27;s an important aspect of life.But there are a ton of very obvious uses for cash aside from for things that are illegal. I use it every single day. reply southerntofu 10 hours agorootparentSure, but that&#x27;s true also with cryptocurrencies. You can make donations to non-profits &#x2F; free-software with them, or acquire hosting services (web&#x2F;dns&#x2F;vps&#x2F;vpn). I don&#x27;t think technology is neutral in the general sense, but money (whether cash of cryptocurrencies), much like a pen and paper, is the kind of tool you can use for the best and worst purposes and there&#x27;s not any way to change short of an orwellian dystopia. reply JohnFen 10 hours agorootparent> that&#x27;s true also with cryptocurrenciesTo a much lesser degree. I wouldn&#x27;t be able to use cryptocurrencies for any of the things I use cash for.But that wasn&#x27;t what was being asserted. The original assertion was \"It&#x27;s also not obvious to me that cryptocurrencies have value beyond \"buying things you&#x27;re not allowed to buy\".\"The same thing cannot be said of cash.(I don&#x27;t agree with the assertion I quoted, to be clear. Aside from buying illegal goods, it&#x27;s good for evading banking regulations and fees, but that&#x27;s neither here nor there.) reply pauselaugh 8 hours agorootparentthe two core, dare i say intrinsic, values of cryptocurrency are 1) immutable, public ledger requiring no single point of authority and 2) that authority having zero bias or regulatory oversight on any transaction outside of its protocol.1&#x27;s value being a traceable permanent record and 2&#x27;s value is no direct stepping in between a transaction or control over a holding.both of those, to many skeptics, are actually negatives and give more control and power to sovereign entities, not less.i am not including the pseudoanonymity of it for two reasons, 1) depending on the crypto, currently the level of sophistication to transact in a manner that any given address cannot personally identify you requires massive amounts of personal responsibility &#x2F; knowledge and most importantly to most people, effort and 2) because the major way the current digital fiat channels handle the transaction volumes that they do is that they&#x27;re built on a foundation of authentication linking to personal identification, and so the theorycraft of \"sure it has a huge ecological footprint but it could replace the many-many-many times larger ecoprint of the entire fiat banking system\" is disingenuous to me because in order to actually replicate that, any main network would be a duplicate of the current authentications backed by men with guns, it would just allow that sidechannel of p2p to have it&#x27;s wild west.i do find it a touch humorous that those who want to shatter the fiat-police complex are quick to engage the legal system to protect and recoup from bad actors. replylemmsjid 9 hours agorootparentprevIt isn&#x27;t just the tech illuminati who associate Bitcoin with scams. Everyone in our neighborhood now knows that it&#x27;s a regular occurrence for older people to get emergency calls from the \"gas company\" and rush to the corner store&#x27;s Bitcoin kiosk so they can pay and not get their gas shut off.They then realize it was a scam and post on Nextdoor, Facebook, etc. While I know that cryptocurrency is not a scam, and while I know that such scams pre-exist Bitcoin, that is absolutely the perception that it&#x27;s getting in our community amongst non-tech people.Your edit, BTW, is interesting. You tar all the responses with the same brush, \"missed the point\", \"lack all imagination\", and then accuse them of having a condescending tone? Do you have a sense there might be a two way street? While you do have a point, there&#x27;s a lot of reasons why human dishonesty, e.g. scamming, prevents us from having a great many nice things that we could otherwise have. reply batch12 9 hours agorootparentprevTo be fair, that was a very tiny baby in the tub. Maybe in theory crypto could have been a perfect system, but in practice humans are flawed. It was doomed.It is horrible that these folks lost their vision. They didn&#x27;t deserve that. Even in the context of crypto apes. It&#x27;s easy (but boring) to make fun of them-- again humans are flawed. reply southerntofu 10 hours agorootparentprevWait till you realize that any form of money is a scam designed to extort riches from the poorest producers and make those riches climb the social pyramid. Tongue in cheek comment of course, but that&#x27;s an argument that can be defended. reply astrange 10 hours agorootparentIt&#x27;s more like a way to trick farmers into producing food for you even though you don&#x27;t right that second have something they want. reply southerntofu 10 hours agorootparentI don&#x27;t think that&#x27;s it. They have to do it because they themselves have bills to pay. However, if we&#x27;re not talking about big industrial farms, most farmers are very much below the poverty line and have massive debt. So it&#x27;s not exactly a facilitator of mutually-beneficial transactions, but rather a pyramid scheme. Who will profit from the food sale? The middlemen. Who will profit from the little money that the farmer pockets? Banks, Monsanto, etc.. reply pauselaugh 9 hours agorootparentastrange had the right context but characterized it not-so-correctly. Modern agriculture&#x27;s origin involved power structures where farmers were forced to be unable to be self-reliant, and would instead trade their crops at a market to get the other things they needed. The powers that be then had a nice, centralized place to take their cut and exchanges to offramp goods into currency. we&#x27;re seeing it all over again with crypto via fiat reliance reply astrange 8 hours agorootparentprevFarmers didn&#x27;t have bills to pay prior to the invention of bills though; some of them were subsistence farmers, some ran on IOUs, and some were slaves.> However, if we&#x27;re not talking about big industrial farms, most farmers are very much below the poverty line and have massive debt.Median family farmer in the US is a millionaire (or their household is anyway). You&#x27;re thinking of farmworkers.> Who will profit from the little money that the farmer pockets? Banks, Monsanto, etc..This is 70s New Leftism \"evil corporations\" thought, but it&#x27;s a bad approach. Small business owners (petit bourgeois &#x2F; local gentry) are more evil than corporations. Farmers, who are largely a kind of landlord, extra so.For instance, the reason factory farms took over poultry in the US is that chickens were originally not valuable because there wasn&#x27;t a market for chicken meat for the longest time. So the farmers left it to their wives to run.Once it became popular, all the farmers sold their businesses to the factory farms, because having no business was less embarrassing than letting a woman run one. reply Yadayadaaaa 10 hours agorootparentprev\"We - humans\" had already a perfect PoS system called \"existing fiat system\" and noone should have allowed the virus &#x27;crypto&#x2F;PoW&#x27; making so much co2 reply chpatrick 9 hours agorootparentprevWe have that in Europe, it&#x27;s called SEPA. reply causality0 10 hours agorootparentprevWhich one&#x27;s the good crypto again? reply lern_too_spel 9 hours agorootparentprevWhen will the cryptocurrency promoters understand that society doesn&#x27;t want decentralization? People don&#x27;t want scammers going around taking people&#x27;s money without recourse, causing a burden on everyone else. They have explicitly entrusted governments to claw back ill-gotten gains. It is just a matter of time until regulations on on-ramps and off-ramps hits cryptocurrencies to the point where they have all the same regulatory costs as centralized ledgers with the additional computational cost of decentralized ledgers. reply philipwhiuk 10 hours agorootparentprevthey were looking kinda dumb with the lasercarving an L in their forehead reply kbmr 9 hours agorootparentRIP reply hanniabu 11 hours agorootparentprevYou could tell the difference in UV wavelengths just by looking at them, or rather not by looking at them. I wish I was as smart and cool as you. reply worklaptopacct 4 hours agoparentprevWow, someone I (somewhat) know in person featured on HN.I haven&#x27;t heard from him in a long while (and even then, he was a friend of a friend who happened to show up from time to time). Then, he got lucky publishing Bitcoin prophecies that were accurate enough to gather significant following. A shovel shop in the times of gold rush kind of thing.Now from what I see on his Instagram, he seems to be living in Dubai and his life consists of fine dining, driving fast cars and attending crypto conferences. Never in my life will I understand why this kind of life is something that people pursue. reply pictureofabear 10 hours agoparentprevIn the cult of BoredApe, you are not allowed to disparage BoredApe. reply 1vuio0pswjnm7 9 hours agoparentprevIn the world of crypto, there are no negatives. Ever. reply bjornsing 10 hours agoparentprevThat’s a sign of a bloody good event, when you go blind but you don’t mind. reply dragontamer 9 hours agoparentprevIts called SEO.If you know something bad has happened, you want to word your public statements in such a way that you beat out the bad news to the top search of Google at least, to minimize the attention given to that issue.Ex: https:&#x2F;&#x2F;twitter.com&#x2F;elonmusk&#x2F;status&#x2F;1465838829370228737This Elon Musk tweet coincided to help SEO-fight against a Whistleblower claim.Ex2: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21122806Boris Johnson using search terms to try and hide the \"Bus\" incident back during Brexit.-----------The #1 goal is to do it in such a way that confuses and befuddles those not-in-the-know (Oh, its just the CEO being quirky... they&#x27;ll brush it off). But of course, you still want to accomplish the goal and make a news-statement with the largest amount of reach such that you beat the \"real news\" event on the top search of Google&#x2F;Facebook&#x2F;other social media.It often sounds like gobbytygook, because it is. You&#x27;re just word-vomiting into the wild trying to shove search terms to the top of Google &#x2F; other search engines... but in a way that no one realizes why those words are related. reply pryce 9 hours agorootparentRemember when Musk called some guy in Thailand a &#x27;pedophile&#x27; for no reason at all, and it immediately became international news with those keywords. That was so quirky!! reply notnmeyer 10 hours agoparentprevit’s a music festival for people into nfts—don’t expect them to be well-reasoned. reply babypuncher 10 hours agoparentprevThis seems pretty reasonable once you realize that other cults have managed to get people excited about literally killing themselves. reply krsdcbl 11 hours agoparentprevThat tweet honestly reads like a pars pro toto of the whole NFT world ...\"visionary revolution, it&#x27;s the future, it will change everything, also it absolutely ruined me!\" reply libraryatnight 10 hours agoparentprev\"WE ARE TRYING TO PUT A TINY THRILL INTO THEIR GREY LITTLE LIVES! OH FORGET IT, TURN ON THE LASERS!\" (https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fiaWwjZS50s) reply colesantiago 11 hours agoparentprev> I’ve almost lost sight this night.> I mean, c&#x27;mon. \"Thanks, incredible event, it almost blinded me\"?Seems like an event to keep these crypto &#x2F; nft bag holders blind to their losses and to encourage them to keep buying worthless jpegs. reply isk517 10 hours agoparentprevWhat&#x27;s to say? Satire died years ago and now this is reality. reply artursapek 9 hours agoparentprevThis is peak bagholder psychology lmfao reply 27 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ApeFest, a conference in Hong Kong for owners of Bored Ape NFTs, resulted in attendees reporting severe eye pain, vision loss, and skin burns due to exposure from laser and blacklight-heavy performances.",
      "Some conference-goers had to seek hospital treatment due to injuries sustained at the event.",
      "The event was hosted by Yuga Labs, the parent company of the Bored Ape Yacht Club, an NFT crypto project that recently saw a significant drop to a two-year low."
    ],
    "commentSummary": [
      "Attendees at ApeFest, a conference for Bored Ape NFT (Non-fungible Token) owners in Hong Kong, reported adverse health effects such as severe eye pain, vision loss, and skin burns due to exposure from laser and blacklight-heavy performances.",
      "The affected participants have sought hospital treatment for their injuries.",
      "Yuga Labs, the parent company of Bored Ape Yacht Club, an NFT cryptocurrency project that recently experienced a significant decline, organized the event."
    ],
    "points": 500,
    "commentCount": 360,
    "retryCount": 0,
    "time": 1699307326
  },
  {
    "id": 38162435,
    "title": "Understanding the Importance of DOM Manipulation with Vanilla JavaScript for Web Developers",
    "originLink": "https://phuoc.ng/collection/html-dom/",
    "originBody": "Mastering DOM manipulation with vanilla JavaScript Star me on GitHub → 5400 ⭐ Web development moves at lightning speed. I still remember when I first started using libraries like jQuery, Prototype, script.aculo.us, Zepto, and many more. Even with modern tools like Angular, VueJS, React, Solid and Svelte, we still have to deal with the Document Object Model (DOM). While these frameworks encapsulate and hide direct DOM management, they still give us access to work with the DOM via refs and event handlers. Whether you're developing or using a web component in any framework, you need to work with the DOM at a certain level. Knowing the browser DOM APIs and how to use them is crucial to web development. A website that introduces the APIs, highlights common problems, and provides answers to popular questions can be incredibly useful. That's why I've put together this collection of resources: No external libraries, just native browser APIs Small, easy-to-understand examples Live demos Tips and best practices included Real-life use cases Works with modern browsers and even supports Internet Explorer Get ready to master DOM manipulation with vanilla JavaScript. Level 1 — Basic Change the favicon dynamically based on user's color scheme preference 17 Oct, 2023 Select a given line in a text area 24 Sep, 2023 Calculate the reading time of a webpage 23 Sep, 2023 Dynamically update CSS root variables 21 Sep, 2023 Insert HTML at the current position of a contentEditable element 20 Sep, 2023 Insert text into a text area at the current position 19 Sep, 2023 Move the cursor to the end of a contentEditable element 18 Sep, 2023 Get or set the cursor position in a text field 17 Sep, 2023 Display a confirm modal when closing the browser 15 Sep, 2023 Make a textarea auto-expand 03 Sep, 2023 Check if users click outside of selected text 02 Sep, 2023 Prevent the page from scrolling to an element when it is focused 19 Oct, 2021 Detect the dark mode 03 May, 2020 Check if the code is running in the browser 03 May, 2020 Detect mobile browsers 25 Apr, 2020 Check if the touch events are supported 25 Apr, 2020 Detect internet explorer browser 24 Apr, 2020 Select the text content of an element 19 Apr, 2020 Get the selected text 19 Apr, 2020 Prevent body from scrolling when opening a modal 18 Apr, 2020 Scroll to an element 08 Apr, 2020 Calculate the mouse position relative to an element 02 Apr, 2020 Press Shift and Enter for a new line 01 Apr, 2020 Distinguish between left and right mouse clicks 01 Apr, 2020 Check if an element is in the viewport 28 Mar, 2020 Swap two nodes 18 Mar, 2020 Count the number of characters of a textarea 18 Mar, 2020 Upload files with Ajax 17 Mar, 2020 Submit a form with Ajax 17 Mar, 2020 Serialize form data into a query string 17 Mar, 2020 Insert given HTML after or before an element 16 Mar, 2020 Detect mac OS browser 15 Mar, 2020 Strip HTML from a given text 10 Mar, 2020 Prevent the default action of an event 10 Mar, 2020 Get size of the selected file 10 Mar, 2020 Check if the native date input is supported 10 Mar, 2020 Detect if an element is focused 09 Mar, 2020 Determine the height and width of an element 03 Mar, 2020 Toggle password visibility 28 Feb, 2020 Toggle an element 28 Feb, 2020 Put cursor at the end of an input 28 Feb, 2020 Select the text of a textarea automatically 27 Feb, 2020 Create one time event handler 24 Feb, 2020 Check if an element is a descendant of another 22 Feb, 2020 Resize an iframe to fit its content 21 Feb, 2020 Replace broken images 21 Feb, 2020 Redirect to another page 21 Feb, 2020 Go back to the previous page 21 Feb, 2020 Get the size of an image 21 Feb, 2020 Get the position of an element relative to another 21 Feb, 2020 Get or set the document title 21 Feb, 2020 Get the position of an element relative to the document 19 Feb, 2020 Scroll to top of the page 18 Feb, 2020 Reload the current page 18 Feb, 2020 Prepend to an element 18 Feb, 2020 Get the document height and width 18 Feb, 2020 Detect clicks outside of an element 18 Feb, 2020 Trigger an event 17 Feb, 2020 Show or hide an element 17 Feb, 2020 Set CSS style for an element 17 Feb, 2020 Get siblings of an element 17 Feb, 2020 Get, set and remove data attributes 17 Feb, 2020 Get, set and remove attributes 17 Feb, 2020 Get or set the HTML of an element 17 Feb, 2020 Get CSS styles of an element 17 Feb, 2020 Attach or detach an event handler 17 Feb, 2020 Append to an element 17 Feb, 2020 Wrap an element around a given element 16 Feb, 2020 Unwrap an element 16 Feb, 2020 Select the children of an element 16 Feb, 2020 Select an element or list of elements 16 Feb, 2020 Replace an element 16 Feb, 2020 Remove an element 16 Feb, 2020 Remove all children of a node 16 Feb, 2020 Loop over a nodelist 16 Feb, 2020 Insert an element after or before other element 16 Feb, 2020 Get the text content of an element 16 Feb, 2020 Get the parent node of an element 16 Feb, 2020 Get the closest element by given selector 16 Feb, 2020 Execute code when the document is ready 16 Feb, 2020 Create an element 16 Feb, 2020 Clone an element 16 Feb, 2020 Check if an element has given class 16 Feb, 2020 Check an element against a selector 16 Feb, 2020 Add or remove class from an element 16 Feb, 2020 Level 2 — Intermediate Remove indentation in a text area using the Shift+Tab key combination 03 Oct, 2023 Truncate the content of an element 02 Oct, 2023 Jump to the beginning or end of the current line in a text area 25 Sep, 2023 Count how many lines a given string takes up in a text area 24 Sep, 2023 Enter full-screen mode 22 Sep, 2023 Indent content in a text area using the Tab key 20 Sep, 2023 Build a spin input 18 Sep, 2023 Get or set the cursor position in a contentEditable element 17 Sep, 2023 Build an OTP input field 17 Sep, 2023 Display a modal when users leave website 15 Sep, 2023 Waiting for an element to become available 13 Sep, 2023 Automatically resize an iframe when its content height changes 13 Sep, 2023 Add a special message when users copy text 27 Aug, 2023 Scroll an element to ensure it is visible in a scrollable container 03 May, 2020 Position an element absolutely to another element 02 May, 2020 Check if an element is visible in a scrollable container 02 May, 2020 Get the direction of the text selection 24 Apr, 2020 Show a custom context menu at clicked position 21 Apr, 2020 Save and restore the text selection 19 Apr, 2020 Print an image 08 Apr, 2020 Show a loading indicator when an iframe is being loaded 06 Apr, 2020 Show a ghost element when dragging an element 31 Mar, 2020 Allow to enter particular characters only 31 Mar, 2020 Paste as plain text 19 Mar, 2020 Paste an image from the clipboard 18 Mar, 2020 Export a table to csv 17 Mar, 2020 Scale a text to fit inside of an element 16 Mar, 2020 Resize an image 15 Mar, 2020 Highlight an element when dragging a file over it 09 Mar, 2020 Communication between an iframe and its parent window 08 Mar, 2020 Placeholder for a contenteditable element 04 Mar, 2020 Detect if the caps lock is on 03 Mar, 2020 Calculate the size of scrollbar 03 Mar, 2020 Resize the width of a text box to fit its content automatically 29 Feb, 2020 Measure the width of given text of given font 28 Feb, 2020 Get the default value of a CSS property 28 Feb, 2020 Change the website favicon 28 Feb, 2020 Get the first scrollable parent of an element 27 Feb, 2020 Check if an element is scrollable 27 Feb, 2020 Copy text to the clipboard 24 Feb, 2020 Preview an image before uploading it 21 Feb, 2020 Download a file 21 Feb, 2020 Load a JavaScript file dynamically 18 Feb, 2020 Load a CSS file dynamically 18 Feb, 2020 Level 3 — Advanced Create a custom cursor 21 Sep, 2023 Sanitize HTML strings 20 Sep, 2023 Detect if users open another tab of the current page 18 Sep, 2023 Show an addition toolbar after users selects text 02 Sep, 2023 Create a custom scrollbar 03 May, 2020 Scroll to an element smoothly 26 Apr, 2020 Show or hide table columns 24 Apr, 2020 Drag to scroll 20 Apr, 2020 Copy highlighted code to the clipboard 19 Apr, 2020 Zoom an image 18 Apr, 2020 Create a range slider 07 Apr, 2020 Create an image comparison slider 06 Apr, 2020 Create resizable split views 04 Apr, 2020 Drag and drop table column 03 Apr, 2020 Drag and drop table row 02 Apr, 2020 Drag and drop element in a list 01 Apr, 2020 Sort a table by clicking its headers 08 Mar, 2020 Resize columns of a table 05 Mar, 2020 Make a resizable element 03 Mar, 2020 Make a draggable element 03 Mar, 2020 Tip Avoid shifting layout when opening a modal 13 Sep, 2023 Get the bounding rectangle of a text node 30 Aug, 2023 Attach event handlers inside other handlers 24 Apr, 2020 Recent posts ⚡ Build a custom scrollbar 04 Nov, 2023 Scroll by dragging 03 Nov, 2023 Create an image comparison slider 02 Nov, 2023 Create a drawer 01 Nov, 2023 Create resizable split views 31 Oct, 2023 Create a range slider 30 Oct, 2023 Create a custom draggable hook 29 Oct, 2023 Make an element draggable on touchscreen devices 28 Oct, 2023 Make a given element draggable 27 Oct, 2023 Build a tooltip component 26 Oct, 2023 Pass a ref to a child component using forwardRef() 25 Oct, 2023 Expose methods of a component using useImperativeHandle() 25 Oct, 2023 Merge different refs 24 Oct, 2023 Make an element draggable 23 Oct, 2023 Create a custom hook returning a callback ref 22 Oct, 2023 See more→ Newsletter 🔔 If you're into front-end technologies and you want to see more of the content I'm creating, then you might want to consider subscribing to my newsletter. By subscribing, you'll be the first to know about new articles, products, and exclusive promotions. Don't worry, I won't spam you. And if you ever change your mind, you can unsubscribe at any time. Phước Nguyễn",
    "commentLink": "https://news.ycombinator.com/item?id=38162435",
    "commentBody": "Mastering DOM manipulation with vanilla JavaScriptHacker NewspastloginMastering DOM manipulation with vanilla JavaScript (phuoc.ng) 392 points by srirangr 20 hours ago| hidepastfavorite127 comments snide 19 hours agoExcellent list and examples. Browsing through a dozen or so of these I&#x27;m amazed at the care and detail in the examples. They are not only functional, but provide very solid UI work as well.As I&#x27;ve shifted away from platforms like React to smaller, minimalist implementations, I often have trouble finding ways for how to do complex patterns in standard JS. It&#x27;s funny when you look at the code and think... huh, that&#x27;s actually a lot easier than importing a huge library with way too many props :)Thanks to the author for putting this together. reply bob1029 19 hours agoparent> It&#x27;s funny when you look at the code and think... huh, that&#x27;s actually a lot easier than importing a huge library with way too many propsI had this realization after the 3rd or 4th RiotJS major version update. It started getting harder to do it the opinionated way. I realized that every minimalistic JS framework would eventually suffer this fate as totally innocent feature requests gradually accumulate into a monstrous pile of hooks, events and other side-effect bandaids.I don&#x27;t even use jQuery anymore. I will use things like ES modules to help organize code, but you don&#x27;t need to run a goddamn npm command to use any of that technology. All of this is a global browser standard, not something you have to vendor in.I look at MDN as the Bible these days. If you take a leap of faith, it will carry you almost 100% of the way. The frameworks will cripple you after a certain point. I can&#x27;t say this would have been true ~5 years ago. Things have shifted rapidly with browser APIs converging on a powerful set of primitives (modules, flexbox, grid, etc) that are now very consistent across platforms. reply reactordev 19 hours agorootparentI like the fact that you said “I can’t say this would have been true ~5 years ago”.React et al. exist because there weren’t global standards around these things and composability suffered. Now that we are all playing the same chorus, these frameworks provide little more than component libraries to reuse. This can be done with ES modules. JSX is why most React folks stick with React, not knowing they can use Preact or just NakedJSX if they wish.I just wanted to verbally concur that vanilla JS is more than capable of doing everything you need. Custom tags. Shadow dom. Composed UI. etc. if you are ok with returning HTMLElement vs a JSX closure. reply nicoburns 18 hours agorootparent> JSX is why most React folks stick with React, not knowing they can use Preact or just NakedJSX if they wish.The large ecosystem of component libraries and the backing of a large corporate company is why people continue to use React. That, and that the extra 100kb that comes with React is generally dwarfed by the actual application in any large app. reply explaininjs 18 hours agorootparentI&#x27;m using preact for a few projects and the integration with React stuff is generally seamless, but there are enough rough edges encountered that looking back I wouldn&#x27;t mind taking the difference of a few dozen kB to avoid them all. The vast majority of my load time is taken by TTFB, even from the Cloudflare Pages local cache, so worrying about kilobytes seems misguided. reply wayfinder 17 hours agorootparentprevNo, React exists because there was a need for a template engine that’s easy to use.There is still no global standard for templates.Just like mustache, Twig, Java Server Pages, Jinja, ERB… every language has this problem. You have a bunch of components written in HTML and you need to mix and match them together in a bigger unit. React+JSX let you do that easier than appending DOM components. Before React, you’d use different libraries that did what React did.The ONLY major language I can think of that has templating baked in is PHP. reply Joeri 16 hours agorootparentHTML has built-in templates now, and custom elements (web components) provide composability. If the built-in templating is not quite powerful enough (it has no logic or variables) there’s developit&#x2F;htm which is basically JSX but parsing in the browser.React is still easier, but it comes at a maintainability tax because you have to keep upgrading to new react versions and port to new react wrapper framework versions if you want to keep a codebase in active development. Vanilla web development has no such problem because web standards remain backwards compatible.For a small project where I don’t care about SEO &#x2F; server-side rendering I absolutely prefer vanilla web dev over React now. reply nicoburns 14 hours agorootparent> React is still easier, but it comes at a maintainability tax because you have to keep upgrading to new react versions and port to new react wrapper framework versions if you want to keep a codebase in active development.There has been one breaking version of React (the recent React 18) in it&#x27;s 8 years of history. Upgrading React versions is a non-issue.Wrapper frameworks like next.js can come with a much higher maintenance cost, but you don&#x27;t need to use those to use React (and I think most people don&#x27;t). reply wayfinder 14 hours agorootparentprevYes, it does! But while I have not used them yet, the approach used by them (via SGML tags) is not new and many libraries &#x2F; XLST have done it before in the past 20+ years and I find the approach usable at best. It&#x27;s too verbose for my taste and it never really caught on, and I find it hard to think that it would catch on this time. reply johnmaguire 15 hours agorootparentprev> The ONLY major language I can think of that has templating baked in is PHP.Depends on what you mean exactly. For example, Go ships with templates. They don&#x27;t mix HTML markup and Go code directly though. reply snide 19 hours agorootparentprevAnd this is absolutely true in CSS now as well. I find a lot of people are needing to rediscover the standards now that they&#x27;ve upped their featuresets. reply paradox460 11 hours agorootparentI even wrote a small article about it at the end of last month:https:&#x2F;&#x2F;pdx.su&#x2F;blog&#x2F;2023-10-25-css-is-fun-again reply sesm 18 hours agorootparentprevReact exists because Facebook needed to sync DOM elements in different parts of webpage with a single source of state. There is still no global standard that solves this problem. reply marcosdumay 17 hours agorootparentYou mean state handlers? Those are one of the main reasons people adopted OOP at the 80&#x27;s.The observer pattern got pretty much standardized by the GoP book, but people overwhelmingly prefer to run some non-standard specialized thing, as it&#x27;s usually much simpler. It&#x27;s a pretty much solved problem since way before a lot of current developers were even born.All of (ok, a lot of) the React&#x27;s complexity comes from it being a generic library that must support every kind of usage. Any standard would have to be as complex. reply reactordev 14 hours agorootparentprevBS there’s custom elements and classes and the ability to target them with a selector. This is standard now. Web components. reply MrJohz 14 hours agorootparentWeb components solve encapsulation of structure, style, and behaviour within a single component, but they don&#x27;t solve the larger state problems, because web components don&#x27;t come with any sort of state management beyond what you can do with normal Javascript.React does provide components, but there&#x27;s not so much different in React components than any other component system. The main reason to use React is the state management it provides on top of those components, making sure that state is in sync across the whole application.In my experience, the hard part of larger applications is that handling of application state. Rendering data can be done in umpteen different ways, and they&#x27;re all pretty much good enough, but handling application state in such a way that every component remains in sync with all other components is hard. reply nicoburns 14 hours agorootparentprevWeb components don&#x27;t really do the same thing. If you use web components then you still need to manage the DOM manually. reply gedy 17 hours agorootparentprevYeah I think some people glorify \"simple\", low level solutions, which work fine for sprinkles of interactivity. But if I&#x27;m building a non trivial UI application - give me a framework please! reply irrational 19 hours agorootparentprevI though React, et al were originally created as a better way to manage DOM updates (shadow DOM). Has that need gone away? reply nicoburns 18 hours agorootparentNot at all. Without React (or a similar framework) you are still left with the options of:- Manually keep track of UI state (which is complex, and often leads to hard to maintain spaghetti code)- Recreate an entire DOM tree every time you re-render (which is slow to the extent that it will often lead to performance problems in practice). Apparently this is not as slow as it used to (so you could probably get away with this sometimes - and you could before), but it&#x27;s still generally a better idea to use a framework as you get better performance for very low cost.There are plenty of non-React frameworks that will you these benefits too. And many of them are much smaller. I think the reason to use React specifically is more \"business reasons\" such assurances that the framework will continue to be maintained, library ecosystem, developer pool, etc. Other frameworks are often technically superior, the difference just isn&#x27;t that great. reply evan_ 18 hours agorootparentprevReact et al use a virtual DOM, not shadow DOM (unless you specifically render it to shadow DOM).They are a better* way to manipulate DOM in that you no longer need to manipulate the DOM, you just build a function that returns what the DOM should look like and it figures out what transforms need to happen.*They’re pitched as a better way, and I think it’s better, but people can reasonably disagree reply nine_k 19 hours agorootparentprevOne of the points of React vs e.g. jQuery was the ability to do efficient updates in large DOM.trees.Did something arrive to plain JS APIs to allow this in a simple enough way? reply kevindamm 18 hours agorootparentTemplates and shadow DOM help a lot with creating and cloning of large subtrees, but time to first render and caution around page re-layout still require attention to do well at scale. reply r-spaghetti 18 hours agorootparentprevIt depends on the type of relations between the updates you want to make, for example adding&#x2F;removing a class can group DOM elements. Updating all elements with this class is easy. reply wruza 16 hours agorootparentprevthe ability to do efficient updates in large DOM.treesIt was the ability to do efficient updates based on large VDOM diffs. The speed of updates is the same between a React VDOM-diff update plan and a direct update of the same size based on e.g. observers. reply nine_k 14 hours agorootparentYes, that&#x27;s the point: computing what needs to change using observers quickly becomes hard. React allows you to run all updates against an unchanged view; observers can interact with each other&#x27;s updates.While React proper may be too large, I find the approach indispensable for more complex interfaces. One can pick Preact instead.But it&#x27;s indeed overkill for adding small bits of interactivity to large, mostly static documents. reply wruza 12 hours agorootparentobservers can interact with each other&#x27;s updates.They only do that if you build a dependency graph instead of a dependency tree and iff the observing framework allows that, and without any loop resolution method. The common technique for graphs under dumb observing is to create a single update routine and pass the name of an emitter to it. I’ve built literally hundreds of interfaces using this (most of them what they’d call “more complex”) and never met any issues React kids are taught to be scared of. UI dataflow was never even a serious concern to be worth a name.The whole premise of React compares it to a “generic detergent” which doesn’t really exist. reply nine_k 12 hours agorootparentI agree that a tree would solve the problem. How do you efficiently compute such a tree, given an event like clicking a button, somewhere far from its potential root?I&#x27;m not an expert in frontend development, but all solutions I&#x27;ve seen either used a variant of virtual DOM, or two-way bindings (the latter a non-starter for me). What did I miss? reply wruza 10 hours agorootparentYou don&#x27;t have to compute it, it is natural. Observers subscribe (this builds a tree), receive notifications and propagate changes via a set of [possibly common] routines that update other values.The difference is, React dataflow is \"returns new state with old state and (returns new state with old state and (returns new state, which triggers the above internally), which triggers the above internally), which triggers the reconciliation\".And the classic way is \"triggers a state, which triggers a set of states, which triggers a set of states, and every triggered control flags a redraw internally with a proper granularity at gui side\".or two-way bindings (the latter a non-starter for me). What did I miss?That they may be a starter, I guess. Or you can use \"control X observes data A\", \"data B, C observe control X\" dataflow. It doesn&#x27;t really matter, because two-way bindings simply push input down to the data level instead of treating controls as explicit data storage. With two-way bindings you can erase the UI layer completely and still get a fully functioning object, which is useful for automation and for reusing&#x2F;macroing of a proper functionality. replyefdee 16 hours agorootparentprevNo, jQuery existed for that reason. React has other reasons, one of them being that manual DOM tracking and manipulation is brittle and annoying and the way React works turns that around completely.I&#x27;m not sure what Preact or NakedJSX offers me that would make it a better choice than React. reply reactordev 16 hours agorootparentWe don’t do manual dom tracking anymore. We have custom tags, shadow dom, es modules, etc making React obsolete. reply hombre_fatal 18 hours agoparentprevNothing in TFA competes with a web client framework, though, since the framework is solving the general questions of how to manage state and then update the UI when state changes.For example, pretty much all of the TFA examples are code you&#x27;d have to write even if you were using React. reply danielvaughn 19 hours agoprevSo I’m working on this side project. I don’t have a name for it yet but I’ve been describing it as “vim for web designers”. The idea is that you can build a website (or components) in the browser, similar to Webflow, but it’s entirely driven by a command language.When I began the project, I told myself I wasn’t going to use a framework. After all, I’ve been doing this since 2009, I was working with JS for a long time before I ever touched a framework. And besides, the native DOM APIs have long been good enough. Right?My god, it was an absolute slog. Marshalling data into and out of the DOM is so tedious and error prone that I began writing my own mini framework to save my own sanity. And then I remembered why I began using frameworks to begin with - because ultimately you’re going to be using a framework of some kind regardless, it’s just whether you want your own custom one or a community-built industry standard.I do still think native DOM is great, if you’re working with small amounts of interactivity. Most sites that use React probably don’t need it. But if your product has even a little bit of nuance or complexity, I’d go with a framework and avoid the hassle.In the end I migrated to Svelte, and I’m much happier as a result. reply dmix 18 hours agoparentHyper minimalist projects always end up re-implementing half of jquery or lodash. Much easier to just find a good library that does treeshaking like ramda and use it sparingly. reply Klonoar 14 hours agorootparentI will gladly reimplement half of those projects if it means I can avoid the JS build systems that break when I come back and try to build in a year.And to be fair I’m sure the build systems have gotten better - they just all left a bad taste in my mouth. I spent more than enough time working with them, writing plug-ins, etc. reply danielvaughn 14 hours agorootparentThis was one of the reasons I really tried to go native. Once you introduce a build system, it has to be maintained.I eventually did choose Svelte, but I’m dockerizing it so I can still use the thing a year from now, unlike virtually all of my other projects. reply Aerbil313 9 hours agorootparentTake a look at nix-shell scripts. Much simpler, lightweight and fast. Start of a build script from a C project of mine: #!&#x2F;usr&#x2F;bin&#x2F;env nix-shell #! nix-shell -i fish --pure #! nix-shell -I https:&#x2F;&#x2F;github.com&#x2F;NixOS&#x2F;nixpkgs&#x2F;archive&#x2F;4ecab3273592f27479a583fb6d975d4aba3486fe.tar.gz #! nix-shell --packages fish gcc gnumake gmp git cacert ...build script goes here...-i fish: I am using fish shell.--pure: Environment is as pure as it can reasonably get.--packages: Packages from nixpkgs to be made available in the environment.-I : pinned to a specific git commit of nixpkgs. This script will give you the exact same environment with the exact same packages with exact same dependencies forever.You can drop into a shell with the exact same configuration by using the cli command nix-shell with the same options.Admittedly this is not as declarative as using flakes, since it&#x27;s a script, but hey, I&#x27;m lazy, still didn&#x27;t sit down to learn them once and for all.Reference: https:&#x2F;&#x2F;nixos.org&#x2F;manual&#x2F;nix&#x2F;stable&#x2F;command-ref&#x2F;nix-shell reply danielvaughn 46 minutes agorootparentThis is interesting! I’ll take a look, thanks. replybryancoxwell 19 hours agoparentprevHow’s that saying go? Choose a framework or you’ll invent one? reply danielvaughn 19 hours agorootparentYep, and it’s exactly right. It had been so many years since I’d worked directly at the DOM layer that I’d forgotten why I stopped.That being said, it was equally difficult picking a framework that had the least likelihood to handicap me down the road. And I am still using plenty native methods, especially for the CSSOM. That has yet to be sufficiently abstracted into any kind of framework. reply the__alchemist 18 hours agoparentprevYou are building application - I could estimate from your description alone that you&#x27;re better off wrapping the DOM interactions.For websites that need interaction, DOM manipulation is IMO a better move due to it using a more fundamental&#x2F;universal skill, light or no build process, faster execution, and explicitness.This is a categorization, with lots of room for grey areas! I bring it up because your use case falls squarely in the category that benefits from a framework. reply danielvaughn 17 hours agorootparentYeah, agreed. I was hoping I could lean into a quasi-HATEOAS approach, where the state of the editor is reflected in the state of the html output. My thinking was that if it&#x27;s only in the html, then in theory it would handle very large numbers of nodes better, since you don&#x27;t have to correspondingly increase the size of JS object in memory.I&#x27;m still attracted to that idea, but it&#x27;s not worth it in the short term. reply synergy20 19 hours agoparentprevsveltekit is all about SSR these days, svelte itself does not have a default client side router, I was trying svelte for csr spa and gave up reply danielvaughn 19 hours agorootparentI’m using Astro for SSR and only using Svelte for the client-side state. It’s working out well so far. reply meiraleal 19 hours agorootparentprevWith the view transition API[1], directory&#x2F;index.html is&#x2F;will be the client-side router.1. https:&#x2F;&#x2F;chipcullen.com&#x2F;adding-view-transitions-api&#x2F; reply panzi 19 hours agoprev`value.startsWith(&#x27;javascript:&#x27;)` and there you have a vulnerability. There can be arbitrary white-space before the URL, so you&#x27;d need to do `value.trim().startsWith(&#x27;javascript:&#x27;)` instead. However, I much prefer white listing instead, i.e. only allowing `http:`, `https:` and maybe `mailto:`, `ftp:`, `sftp:` and such. Maybe allow relative URLs starting with `&#x2F;`. Maybe. Though that would mean to correctly handle all attributes that actually can be URLs. Again, I&#x27;d just white list a few tags plus a few of their attributes. reply jfhr 18 hours agoparentUnless you need to support ancient browsers [1], I&#x27;d go the easy way and leave the URL parsing to the browser, e.g. `[&#x27;http:&#x27;, &#x27;https:&#x27;, &#x27;mailto:&#x27;, &#x27;tel:&#x27;].includes(new URL(value, location.origin).protocol)`[1] https:&#x2F;&#x2F;caniuse.com&#x2F;url reply keepamovin 18 hours agorootparentJavaScript elegance is always welcome! reply simonw 18 hours agoparentprevYou mean from this article \"Sanitize HTML strings\"? https:&#x2F;&#x2F;phuoc.ng&#x2F;collection&#x2F;html-dom&#x2F;sanitize-html-strings&#x2F;Yeah, that article really shouldn&#x27;t imply that sanitization is \"that easy\". It does at least mention https:&#x2F;&#x2F;github.com&#x2F;cure53&#x2F;DOMPurify at the end but it should LOUDLY argue against attempting to write this particular thing yourself and promote that exclusively in my opinion.Filed an issue about this here: https:&#x2F;&#x2F;github.com&#x2F;phuocng&#x2F;html-dom&#x2F;issues&#x2F;281 reply runarberg 17 hours agorootparentThere is also a future way of doing this with vanilla JavaScript using the Sanitize API. It is enabled under flags in most browsers currently.https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;HTML_Saniti... reply panzi 9 hours agorootparentYes, that one seems promising, but I really would like more control on allowed URLs. I.e. a flag to allow&#x2F;disallow cross domain URLs anywhere (, , , , ,etc. etc.) And does it strip any styles (style attribute, style tag, link stylesheet)? If yes, can I allow certain styles somehow? Needs work IMO. reply flanbiscuit 19 hours agoprevSeems like a helpful list of things to know, I haven&#x27;t gone through it all but looks good so far.Clicked on the first one and I noticed something.They write this: const setFavicon = () => { const favicon = document.querySelector(&#x27;link[rel=\"icon\"]&#x27;); favicon.href = (window.matchMedia(&#x27;(prefers-color-scheme: dark)&#x27;).matches) ? &#x27;dark-mode-favicon.png&#x27; : &#x27;light-mode-favicon.png&#x27;; }; setFavicon();And then this:> We can use this function to update the user&#x27;s color scheme preference whenever they make a change. window .matchMedia(&#x27;(prefers-color-scheme: dark)&#x27;) .addEventListener(&#x27;change&#x27;, setFavicon);They are running \"window.matchMedia(&#x27;(prefers-color-scheme: dark)&#x27;)\" twice unnecessarily. If you&#x27;re going to run `setFavicon` as the callback to the event listener, you can get the result from the `event` parameter passed to the callback like this: const setFavicon = (event) => { const favicon = document.querySelector(&#x27;link[rel=\"icon\"]&#x27;); favicon.href = event.matches ? &#x27;dark-mode-favicon.png&#x27; : &#x27;light-mode-favicon.png&#x27;;This would be my preferred way of doing it because it supports users who are visiting with JS disabled.https:&#x2F;&#x2F;phuoc.ng&#x2F;collection&#x2F;html-dom&#x2F;change-the-favicon-dyna... reply explaininjs 18 hours agoparentAlso, doing it in HTML prevents the flash of alternate-themed content. Doing the same with the `theme-color` meta is also a seldom-used but great feature for detail-minded PWA developers. reply throwaway858 15 hours agoprevThe technique for \"Make a textarea auto-expand\" is obsoleted by a new CSS property(\"form-sizing\"):https:&#x2F;&#x2F;chriscoyier.net&#x2F;2023&#x2F;09&#x2F;29&#x2F;css-solves-auto-expanding...Even without using this new CSS property, the technique that is used (adjusting the \"height\" of the element) is not ideal and can be glitchy.A better approach is to use a mirror hidden element:https:&#x2F;&#x2F;css-tricks.com&#x2F;the-cleanest-trick-for-autogrowing-te... reply bakkoting 16 hours agoprevA number of these are using older, awkward APIs, when there&#x27;s nicer native versions now.For example, \"replace an element\" is ele.parentNode.replaceChild(newEle, ele);but unless you&#x27;re targeting Chrome . reply TheRealPomax 16 hours agoparentprevIf you want to use the browser as a user interface for an application, use a user interface library. Knowing what you&#x27;re building is just as important as picking the right tools after you figured that part out: not all web pages (in fact, man web pages) are not, and don&#x27;t need to be, user interfaces for applications. reply gwbas1c 11 hours agoprevFYI: Understanding some very basic DOM manipulation is important even if you are working with very modern tools.Case in point: I&#x27;m working on a shipping Blazor WASM application, and I&#x27;ve had to drop into the DOM twice:1: We&#x27;re still using Leaflet.js for a map component. This occasionally requires some DOM calls to adapt between Blazor&#x27;s worldview and Leaflet&#x27;s worldview.2: Blazor WASM loads slowly, so we put up a splash animation in HTML. Removing the splash animation at the \"right\" time requires DOM manipulation. (Because we want to keep the splash animation up while the Blazor side makes additional calls, so we use DOM manipulation to hide the Blazor-based UI until we&#x27;ve loaded everything.) reply mg 19 hours agoprevThese days, I get by with just two dependencies. One is dqs.js:https:&#x2F;&#x2F;github.com&#x2F;no-gravity&#x2F;dqs.jsWhich turns document.querySelector(&#x27;.some .thing&#x27;) into dqs(&#x27;.some .thing&#x27;).The other one is Handlebars:https:&#x2F;&#x2F;github.com&#x2F;handlebars-lang&#x2F;handlebars.js&#x2F;Which I use for all my frontend templating needs. reply bambax 19 hours agoparentInstead of importing dsq you can do this const $ = document.querySelector.bind(document); const $$ = document.querySelectorAll.bind(document); reply gmiller123456 18 hours agorootparentThat&#x27;s pretty much what that include file does, but with \"dqs\" rather than \"$\". I&#x27;m not a fan of using the $ as it might confuse others that you&#x27;re using JQuery. reply chrismorgan 15 hours agorootparent$ is what I would expect and prefer, because it’s what I’ve been using in my browser dev tools for over fifteen years (ah, Firebug; fond reminiscences). If your document doesn’t use the names, you’ll get implementations roughly equivalent to these: function $(selector, element = document) { return element.querySelector(selector); } function $$(selector, element = document) { return Array.from(element.querySelectorAll(selector)); }Reference on more magic available in the console (there’s a lot of good stuff):• https:&#x2F;&#x2F;firefox-source-docs.mozilla.org&#x2F;devtools-user&#x2F;web_co... (I think “:help” is my new favourite: opens that URL, which is not particularly discoverable)• https:&#x2F;&#x2F;developer.chrome.com&#x2F;docs&#x2F;devtools&#x2F;console&#x2F;utilities... reply bambax 18 hours agorootparentprevWouldn&#x27;t that be a feature? ;-) reply mg 19 hours agorootparentprevThat&#x27;s what I did before I put those lines (and some more) into dqs.js reply throw555chip 19 hours agorootparentprevThat&#x27;s a neat shortcut, I prefer vanilla JS too. reply crtasm 16 hours agorootparentprevIs there a way to make this also work as a function? e.g. myTable.$$(&#x27;tr&#x27;).foreach() reply mg 15 hours agorootparentnext [–]import {qsA} from &#x27;.&#x2F;dqs.js&#x27;; rows = qsA(myTable, &#x27;tr&#x27;); reply crtasm 15 hours agorootparentThanks but the point is being able to chain it:myTable.qsA(&#x27;tr&#x27;)TypeError: myTable.qsA is not a function reply mg 14 hours agorootparentWell, since html elements already have a querySelectorAll function, you could just assign it to a new method \"qsA\": HTMLElement.prototype.qsA = HTMLElement.prototype.querySelectorAll;Now myTable.qsA(&#x27;tr&#x27;) will work. You can try it right here on the HN page in the browser console: HTMLElement.prototype.qsA = HTMLElement.prototype.querySelectorAll; document.querySelector(&#x27;#hnmain&#x27;).qsA(&#x27;tr&#x27;);Why do you prefer chaining? reply xeckr 13 hours agorootparentYou&#x27;ll have to do this in addition to one of the previously discussed solutions if you want something like document.qsA since document isn&#x27;t an HTMLElement. reply crtasm 13 hours agorootparentprevPerfect, thankyou!I suppose habit plus I find it easier to grok in that order, targetelement->childelements->foreach. replyxeckr 17 hours agorootparentprevOrconst $ = (query) => document.querySelector(query);const $$ = (query) => document.querySelectorAll(query);(How do I write monospace on HN?) reply chrismorgan 15 hours agorootparent> (How do I write monospace on HN?)Two-space indentation. reply xeckr 13 hours agorootparentnext [–]Thank you reply bambax 15 hours agorootparentprev> How do I write monospace on HN?Line that start with four spaces like this reply xeckr 13 hours agorootparentnext [–]Looks like 2 spaces are sufficient. But I sympathize with 4-space indentation. reply bambax 13 hours agorootparentIndeed! I&#x27;ve always thought it was 4, because... Markdown, probably? replynicoburns 18 hours agoparentprevThese days? This is what we were using 10 years ago before React came about! It works fine, but performance can be a problem in large apps as handlebars just naively recreates entire DOM trees rather than diffing. reply dylan604 19 hours agoparentprevare we really concerned about the shortness of the code for the worse tradeoff of being less readable?how much is truly saved by this? these seem like the things of a solo dev vs building something for other people to follow reply mg 19 hours agorootparentWhat&#x27;s wrong with solo devs?You are talking to me on a site done by one.This is the frontend code:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;hn.js reply throw555chip 18 hours agorootparentThat&#x27;s wild, never really looked into it, reminds me how a C programmer thinks. I&#x27;m mostly backend but get involved with frontend every now and then. Tried to avoid Angular and React but now trying to teach myself Angular. AngularJS was nice. Angular sucks, very complicated for doing the same things I can do with vanilla HTML&#x2F;CSS&#x2F;JS, but that&#x27;s what companies seem to be hiring for. reply dylan604 17 hours agorootparentprevI&#x27;ve been a solo dev as well. But unless you&#x27;re going to live forever, the code should be maintainable by someone else. You can be a solo dev that&#x27;s impossible to ever work with, or you can maintain good practices so that other people can play along as well. reply deathanatos 13 hours agorootparentprev… is that a good example?… upvote&#x2F;downvotes are does by fetching an \"image\", i.e., it does a GET request with side-effects! … also seems like a trivial CSRF vuln, too… reply evbogue 19 hours agorootparentprevhn would benefit from the ael library reply von_lohengramm 19 hours agorootparentprevThere&#x27;s a reason why web developers have the reputation they do. reply r-spaghetti 18 hours agoparentprevCan you use Handlebars in Cloudflare workers? EJS was not accepted and string literals are string literals. reply mg 18 hours agorootparenthandlebars.js runs in the browser:https:&#x2F;&#x2F;plnkr.co&#x2F;edit&#x2F;LXb5TOMoyAxdbTxB reply evbogue 19 hours agoparentprevCombine this with https:&#x2F;&#x2F;github.com&#x2F;dominictarr&#x2F;hscrpt and you&#x27;ll be building websites in no time! reply bobmaxup 19 hours agoprevIf I were making a cookbook on HTML&#x2F;JS I would probably leave out this:https:&#x2F;&#x2F;phuoc.ng&#x2F;collection&#x2F;html-dom&#x2F;sanitize-html-strings&#x2F;#... reply panzi 19 hours agoparent`value.startsWith(&#x27;javascript:&#x27;)` and there you have a vulnerability. There can be arbitrary white-space before the URL, so you&#x27;d need to do `value.trim().startsWith(&#x27;javascript:&#x27;)` instead. However, I much prefer white listing instead, i.e. only allowing `http:`, `https:` and maybe `mailto:`, `ftp:`, `sftp:` and such. Maybe allow relative URLs starting with `&#x2F;`. Maybe. Though that would mean to correctly handle all attributes that actually can be URLs. Again, I&#x27;d just white list a few tags plus a few of their attributes. reply zeroCalories 18 hours agoparentprevWhy? This is a common need. Is there an issue with the implementation or another reason why you would want to use a library? reply chrismorgan 17 hours agoparentprevI’m not sure quite why you’re against removing script tags, but honestly that entire article is poor, riddled with disastrously bad advice:• “Using regular expressions”: it suggests that this approach is acceptable within its limits. It’s not at all. As a simple example, the expression shown is trivially bypassed by \"…\". This is why, unlike the post claims claims, using regular expressions for cleaning HTML is not a common approach.• (“Eliminating the script tags”: I want to grumble about using `[...scriptElements].forEach((s) => s.remove())` instead of `for (const s of scriptElements) { s.remove(); }` or even `Array.prototype.forEach.call(scriptElements, (s) => s.remove())`. Creating an array from that HTMLCollection is just unnecessary and a bad habit.)• “Removing event handlers”: `value.startsWith(&#x27;javascript:&#x27;) || value.startsWith(&#x27;data:text&#x2F;html&#x27;)` is inadequate. Tricks like capitalising and adding whitespace (which the browser will subsequently normalise) in order to bypass such poor checks have been common for decades.• “Retrieving the sanitized HTML”: you are now vulnerable to mXSS attacks, which undo all your effort.• “Elements and attributes to remove from the DOM tree”: this proposes a blacklist approach and mentions a few examples of things that should be removed. Each example misses adjacent but equally-important things that should be removed. You will not get acceptable filtering if you start from this approach.• “Simplifying HTML sanitization with external libraries”: this is pitched merely as easier, faster and cheaper, rather than as the only way to have any confidence in the result.• “Conclusion”: as I hope I’ve shown, “The DOMParser API is one tool you can use to get the job done right.” is not an acceptable position.Really, the article could be significantly improved by presenting it as what a common developer might think, and then scribbling all over the problematic things with these explanations of why they’re so bad, and ending with the conclusion “so: just use the DOMPurify library; consider nothing else acceptable”. (There have at times been a couple of other libraries of acceptable quality, but as far as I’m concerned, DOMPurify has long been the one that everyone should use. I note also that this article is talking about client-side filtration. I’m not familiar with the state of the art in server-side HTML sanitisation, where you probably don’t have an actual DOM; this is also a reasonable place to wish to do filtering, but the remaining active mXSS vectors might pose a challenge. I’d want to research carefully before doing anything.)I look forward to the Sanitizer APIbeing completed and deployed, so that DOMPurify can become just a fallback library for older browsers. reply bobmaxup 17 hours agorootparent> I’m not sure quite why you’re against removing script tagsMy bad, I left a fragment in the URL. reply DerekBickerton 15 hours agoprevFor syntactic DOM manipulation sugar I use Cash[0][0] https:&#x2F;&#x2F;kenwheeler.github.io&#x2F;cash&#x2F; reply earthboundkid 14 hours agoprevhttps:&#x2F;&#x2F;phuoc.ng&#x2F;collection&#x2F;html-dom&#x2F;resize-an-iframe-to-fit...There should be a way to do this with CSS and maybe some magic headers. It&#x27;s super annoying that it has be done via JS. reply jampekka 18 hours agoprevI appreciate the benefit of not having dependencies but not a fan of this rabid vanilla fixation.Of course you can do anything with Vanilla that you can do with e.g. jquery. Otherwise jquery obviously couldn&#x27;t do it.The problem is that the vanilla DOM API sucks. document.getElementById sucks vs $, document.querySelector sucks vs $. setTimeout and setInterval suck. addEventHandler really sucks. reply jeffgreco 19 hours agoprevThis looks like some helpful refreshers as I try and mess with building a browser extension. reply sumoboy 17 hours agoprevSame developer who has offers a form validator, very solid and was worth the $ for past projects. Nice list of JS snippets. reply marban 19 hours agoprevAll good except for Create a custom scrollbar reply cmrdporcupine 19 hours agoprevNice. Bookmarked. Another good one in a similar vein is https:&#x2F;&#x2F;youmightnotneedjquery.com&#x2F;I don&#x27;t do web stuff often, but when I do I am completely demoralized by the state of framework-itis there. It&#x27;s gotten completely out of control. A fresh React project is hundreds of dependencies. The weight of that complexity is astounding to me.Meanwhile the core cross-platform browser tech shipped by default has never been more capable and the amount of code to do common things ... isn&#x27;t very much.Front-end devs: are you ok? reply mattlondon 19 hours agoparentI am no fan at all of react or npm, but some frameworks do provide some helpful things that pure DOM APIs do not, so they still have a place I think.So e.g. application state management and routing are two huge things you&#x27;ll still need to implement yourself if you just use pure DOM APIsDepending on the complexity of your app then that might not be such a big deal, but these extras are rapidly worth their weight in gold as soon as you go beyond a basic app. Unless you are using anything that needs NPM in which case you are fucked. reply mablopoule 18 hours agoparentprevAnother excellent resource in the same vein is the marvelous https:&#x2F;&#x2F;javascript.info&#x2F;As a front-end dev who actually like the fundamental platform and the capacities it offers, I&#x27;m both happy to see so much nice things coming our way (nested CSS, container queries, and a generally nicer JS experience since ES6) and at the same time horrified by the amount of people using components for trivial things like buttons or bold text.And I do like framework, but so much framework and library&#x27;s marketing depend on making people believe that CSS and vanilla DOM manipulation is insanely hard (it&#x27;s not once you know the fundamentals), and that pulling a random package on NPM is not just quick and easy, but also the professional thing to do. reply danielvaughn 19 hours agoparentprevAgreed it’s out of control. But in the past year I’ve transitioned to a platform team so I’m working with devops-ish tooling for the first time. Holy shit…the amount of complexity there gives front-end a run for its money. You’ve got Kubernetes, Helm, Terraform, CircleCI, yadda yadda. The amount of layered terminology in just those technologies alone is absurd. reply throw555chip 18 hours agorootparentAs a mostly backend dev, maybe I&#x27;m biased but all the tech you listed there isn&#x27;t layers of stuff. Each of those do something different. For all but FAANG, kube is overkill. Helm is just templating. The rest existed before DevOps was a term. reply cmrdporcupine 18 hours agorootparentWas replying saying mostly this. I stay away from most of that kind of stuff and work in embedded-type space mostly, but when I have ventured in there, it all kind of makes a certain amount of sense as modules that layer relatively independent of each other. Because, well, most things interoperate via Rest or gRPC or whatever, without a lot of coupling.The problem with the JS&#x2F;TS&#x2F;Node ecosystem is the cultural tendency there to build and proselytize frameworks. Heavily coupling seems to be seen as a positive rather than negative. reply troupo 17 hours agoparentprev> Meanwhile the core cross-platform browser tech shipped by default has never been more capable and the amount of code to do common things ... isn&#x27;t very much.Have you tried to use this \"capable cross-platform tech\" to do anything non-trivial? And React isn&#x27;t the only thing out there. reply purplecats 14 hours agoprevwhat is the value in manually processing knowledge bases like this (besides perhaps in an artisan fashion) if your knowledge interface such as chatgpt already knows it and surfaces it at will? reply kavaruka 14 hours agoparentChatGPT is useful only if you understand the code that it produces reply Tagbert 12 hours agoparentprevWhere does ChatGPT get its pattern to match to produce the output? Likely sources like this. reply spacechild1 14 hours agoparentprevYou can ask the same question about any knowledge base. Why here? This is completely off topic. reply agumonkey 17 hours agoprevVery nice list, it&#x27;s nice to \"down to earth\" after too much react ui libs :). Thanks. reply wangii 17 hours agoprevso, after 10 years of reactjs, we&#x27;ve forgot how to do this in javascript? reply est 18 hours agoprevIs there a CSS version of this? Like Zen-garden but modern reply throw555chip 18 hours agoprevAI models will no doubt gobble it up and spit out back out in a unique looking laundered format minus the comments. reply Ringz 12 hours agoprevJust use HTMX reply russellbeattie 13 hours agoprevMy new favorite bit of vanilla JavaScript has been around since Chrome 1.0 and IE: Element.insertAdjacentHTML. To say it&#x27;s useful is an understatement, especially with multi-line Strings using `. I can&#x27;t believe it&#x27;s been around so long and I only realized it a year or so ago.The insertAdjacentHTML(position, text) method of the Element interface parses the specified text as HTML or XML and inserts the resulting nodes into the DOM tree at a specified position.A contrived example of something that&#x27;s a pain to do with raw DOM: let anchors = [&#x27;#a&#x27;, &#x27;#b&#x27;, &#x27;#c&#x27;]; let el = document.querySelector(&#x27;ul&#x27;); for (let a of anchors) { el.insertAdjacentHTML( &#x27;beforeend&#x27;, `${a.substr(1)}` ); }Previously I would have to create the li element, then the a element, add it as a child, set the href attribute, then the text value of the a element, then insert it into the ul element. Or, I&#x27;d batch it all up as a big string and insert it using innerHTML. Modifying an existing list was even more painful and verbose.1. https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;API&#x2F;Element&#x2F;ins... reply kossTKR 19 hours agoprevGreat resource! I&#x27;ve defaulted to just use vanilla js and petite-vue, it&#x27;s 15 kilobytes and gives a good base.The problem is the build requirements you run into quickly if you want to use plugins these days with vanilla js.Elegance was being able to load actual modules from a CDN. Why did we need these anti-web build steps on top of JS?I feel like there was a sweet spot of complexity around Vue 1 or say 2014.The Vue 2-->3 jump illustrates the welcome but total breakdown of frameworks in my opinion. It constantly gets in your way and moving data on a page with a hierarchy you need some bizarrely complex data flow, so many folders, files, tools, brittle typescript linting, \"auto\" this and that, constant building and so many rules it&#x27;s not fun to engineer anymore.Anyway, i think we are very close to a sweet spot if we just use something like Petite-vue &#x2F; Alpine on top of JS maybe with a tiny router and get creative with JS around that, or just early versions say Vue if we need anything more complex. reply athanagor2 14 hours agoparentThe toolchain complexity problem was already present with Vue 2 (and anyway it was usable as a dependency pulled into a classic web page). It definitely could be better, and it seems to be on the way of being improved.I feel like a lot of criticism toward Vue 3 is based on feelings and vibes. What exactly are your complaints toward v3? reply selimnairb 17 hours agoprev [–] In Capitalist America, DOM manipulates you! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author highlights the necessity for web developers to master Document Object Model (DOM) manipulation with plain JavaScript, despite the availability of modern tools like Angular, VueJS, React, Solid, and Svelte that simplify direct DOM management.",
      "Modern tools encapsulate and abstract direct DOM interactions, but web developers must still interact with the DOM at some level.",
      "To aid developers in mastering DOM manipulation, the author provides resources such as simple examples, live demos, tips, and best practices."
    ],
    "commentSummary": [
      "The author emphasizes the necessity for web developers to acquire proficiency in Document Object Model (DOM) manipulation using vanilla JavaScript, even in the era of modern tools like Angular, VueJS, React, Solid, and Svelte.",
      "Despite these tools wrapping and abstracting away much of the direct DOM management, it's pointed out that there is still a certain level of interaction with the DOM in web development.",
      "The author offers resources such as simple examples, live demonstrations, tips, and best practices to aid developers in mastering the art of DOM manipulation."
    ],
    "points": 392,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1699278090
  },
  {
    "id": 38161452,
    "title": "Introducing Ladder: A Customizable, Open-Source Alternative in Golang to 12ft.io and 1ft.io Services",
    "originLink": "https://github.com/kubero-dev/ladder",
    "originBody": "Hey thereI made a opensource alternative for these services. Although these worked very well, I was not so confident what they do. So I made my own and opensourced it.It is written in Golang and is fully customizable.",
    "commentLink": "https://news.ycombinator.com/item?id=38161452",
    "commentBody": "Ladder, open source alternative to 12ft.io and 1ft.ioHacker NewspastloginLadder, open source alternative to 12ft.io and 1ft.io (github.com/kubero-dev) 346 points by 2cpu1container 21 hours ago| hidepastfavorite134 comments Hey thereI made a opensource alternative for these services. Although these worked very well, I was not so confident what they do. So I made my own and opensourced it.It is written in Golang and is fully customizable. ktpsns 21 hours agoI got the feeling that these features should be part of a browser extension the same way as there are AdBlock extensions. I guess the reason it is not is \"personal preference\" of the author, or is there some technical reason? reply sva_ 20 hours agoparent> these features should be part of a browser extensionYou mean like Bypass Paywall Clean?https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-chrome-clean reply Beijinger 17 hours agorootparentDoes not work so well anymore. Better use a bookmarkletjavascript:location.href=&#x27;https:&#x2F;&#x2F;archive.is&#x2F;?run=1&url=%27+encodeURIComponent(documen... reply yubiox 12 hours agorootparentNot sure what happened here but the end of that is chopped off. Trying to fix it: javascript:location.href=&#x27;https:&#x2F;&#x2F;archive.is&#x2F;?run=1&url=%27+encodeURIComponent(document.location.href)+%27&#x27; reply godelski 8 hours agorootparentNot sure this is a good solution considering archive&#x27;s fuckery reply rahulroy9202 43 minutes agorootparentWhat did archive do? reply NelsonMinar 17 hours agorootparentprevThis works quite well and probably covers 90% of my needs. For the other 10% I still use archive.today or 12ft (RIP).It&#x27;s a shame Google won&#x27;t let this addon be in the store. reply johnmaguire 19 hours agorootparentprevIs there a Firefox version? reply xipho 19 hours agorootparentYou don&#x27;t even need to install it, just add it as an import line to UO, Google how. Game changing. reply xaellison 19 hours agorootparentWhat&#x27;s UO? This would&#x27;ve been a great comment with a little more info :) reply rustyminnow 19 hours agorootparenthttps:&#x2F;&#x2F;ublockorigin.com&#x2F;After install go to \"Filter Lists\" > Import ... > and add the url of the \"list\"... which is actually from a different repo: https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-clean-filter...Note: this apparently works for fewer sites than the linked extension. reply sva_ 17 hours agorootparent> Note: this apparently works for fewer sites than the linked extension.Still, this is great to know because it can then be used on Firefox mobile. reply pieter_mj 9 hours agorootparentAddons are coming for firefox mobile. If you can&#x27;t wait, you can install any addon you want with firefox nightly. reply gzer0 18 hours agorootparentprevThank you for the insight. I usually hesitate to install add-ons, but now I can avoid that step entirely based on your advice. reply sva_ 19 hours agorootparentprevhttps:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea...It used to be in mozillas addon store, but they removed it, so have to install via dev mode reply m-p-3 18 hours agorootparentOr you can load this uBO filterlist, which should basically do the same thing as the extension https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-clean-filters&#x2F;-&#x2F;raw&#x2F;main&#x2F;bpc-paywall-filter.txt reply bluish29 17 hours agorootparentThat&#x27;s why I like reading HN comments. Thanks for that filter link reply penguin_booze 17 hours agorootparentprevNo need for dev mode - signed XPIs are avaiable from releases: https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea.... reply sva_ 17 hours agorootparentAh you&#x27;re right, I confused it with Chrome replybilekas 21 hours agoparentprevI don&#x27;t know for sure, but I would imagine there are more severe actions taken against circumventing paid material (content behind a paywall) than there is for free content supplemented by advertisements..Edit : The Digital Millennium Copyright Act (DMCA) prohibits circumventing an effective technological means of control that restricts access to a copyrighted work. I guess that would apply here. reply mckirk 20 hours agorootparentGiven how liberally the DMCA is applied, you definitely don&#x27;t want to be on the wrong side of that.I remember some guy that wrote a WoW bot and got sued using the DMCA, with the argument that his bot was circumventing the anti-cheat and the anti-cheat could be seen as a &#x27;mechanism protecting copyrighted material&#x27;, because it was safeguarding access to the game servers, the servers were generating parts of the game world (such as sounds) dynamically, and those were under copyright... Wild stuff. reply kkzz99 16 hours agorootparentIt happened to Honorbuddy, a very advanced bot for World Of Warcraft made by a German company. The argument in relation to DMCA was that the bot was circumventing warden, the games anti-cheat system. The legal battle was long and they ultimately had to strip many features of the bot, until the company went under. reply judge2020 19 hours agorootparentprevAs far a I know section 1201 has never been prosecuted. Distribution of the copyrighted material is what&#x27;s focused on. reply mckirk 17 hours agorootparentThis seems a good summary of the case I was talking about:https:&#x2F;&#x2F;massivelyop.com&#x2F;2020&#x2F;02&#x2F;28&#x2F;lawful-neutral-cheating-c... reply Aaargh20318 18 hours agorootparentprev> The Digital Millennium Copyright Act (DMCA) prohibits circumventing an effective technological means of control that restricts access to a copyrighted work. I guess that would apply here.It doesn&#x27;t if you&#x27;re not in the US. reply zeusk 17 hours agorootparentKim Dotcom believed so too, didn&#x27;t fare too well. reply Aaargh20318 12 hours agorootparentMegaupload did business in the US, they had a lot of servers in the US. IIRC that was the basis for his arrest: the crime took place on US territory. reply zeusk 12 hours agorootparent12ft.io and vercel have servers in US, so what&#x27;s your argument here? reply nottheengineer 20 hours agorootparentprevGood old section 1201. The EFF has been fighting it for a while, but hasn&#x27;t had much success unfortunately. reply nerdbert 19 hours agorootparentprevIsn&#x27;t anything that can be circumvented ineffective?Or, looking at it the other way, if you put a small sticker that says \"do not do X\" and even one person follows that, isn&#x27;t that therefore an \"effective\" method? reply overtomanu 20 hours agoparentprevthere is below extension for this purpose which I know of, I think there can be many more if we search for themchrome and firefox extension for removing paywall: https:&#x2F;&#x2F;github.com&#x2F;iamadamdev&#x2F;bypass-paywalls-chrome reply user764743 18 hours agorootparentThis extension is asking for a lot of permissions it shouldn&#x27;t ask forIf you want an alternative that only requests permissions for sites with paywalls, this one is better: https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea... reply nfriedly 6 hours agoprevThe docker image, and on the upside is fairly easy to get running. But I&#x27;m downside, I&#x27;m zero for two actually using it.I tried a Bloomberg article which gave me a \"suspicious activity from your IP, please fill out this captcha\" page, only the captcha was broken and didn&#x27;t load.Then I tried a WSJ article which loaded basically the same couple of paragraphs that I could get for free, but did not load any of the rest of the content. reply fyzix 20 hours agoprevI&#x27;m very new to this kind of service, but do you have to write your own rulesets for each site you want to bypass? The repo doesn&#x27;t seem to include much... reply 2cpu1container 18 hours agoparentYes, the one i provide is still pretty empty yet. I plan to build one that can be used as a starting point or as a default. reply SigmundurM 21 hours agoprevYou mention 13ft as another open source inspiration. How is Ladder improving on what 13ft does? reply 2cpu1container 21 hours agoparentI did try 13ft. But it misses several points.The ladder applies custom rules to inject code. It basically modifies the origin website to remove the Paywall. It rewrites (most of) the links and assets in the origins HTML to avoid CORS Errors by routing thru the local proxy.The ladder uses Golangs fiber&#x2F;fasthttp, which is significantly faster than Python (biased opinion) .Several small features like basic auth ... reply withinboredom 20 hours agorootparent> The ladder uses Golangs fiber&#x2F;fasthttp, which is significantly faster than PythonI have a feeling that this performance difference is practically imperceptible to regular humans. It&#x27;s like optimizing CPU performance when the bottleneck is the database. reply ComputerGuru 16 hours agorootparentNot for any publicly hosted instance, it’s not. We’re not talking about the time it takes to perform one request but the scalability it affords a small vm to handle so many requests in parallel when it is being used by the general public. reply oh_sigh 15 hours agorootparentprevIf the paywall is implemented in client code, then usually just disabling javascript for the site is enough to let you view it. If it is implemented server side, then there usually isn&#x27;t a way around it without an account. reply pacifika 20 hours agoprevOpen source makes it easy for the cat in the cat mouse game, right? reply lucideer 20 hours agoparentThere&#x27;s no real cat & mouse game here (yet*) - sites don&#x27;t do anything to mitigate this. Sites deliberately make their content available to robots to gain SEO traction: they&#x27;re left with the choice of allowing this kind of bypass or hurting their own SEO.* I say \"yet\" because there could conceivably be ways to mitigate this, but afaik most would involve individual deals&#x2F;contracts between every search engine & every subscription website - Google&#x27;s monopoly simplifies this somewhat, but there&#x27;s not much of an incentive from Google&#x27;s perpsective to facilitate this at any scale. reply tiagod 19 hours agorootparentGoogle publishes IP ranges for GoogleBot. You can also reverse-lookup the request IP address - the resolved domain should in turn resolve to the original address. reply ForkMeOnTinder 19 hours agorootparentDoes anyone else remember 10 years ago when Google would penalize sites for serving different content to GoogleBot than to normal users? Those were the days. reply omoikane 8 hours agorootparent> Google would penalize sites for serving different content to GoogleBot than to normal usersListed under spam policies:https:&#x2F;&#x2F;developers.google.com&#x2F;search&#x2F;docs&#x2F;essentials&#x2F;spam-po... \"Cloaking refers to the practice of presenting different content to users and search engines with the intent to manipulate search rankings and mislead users\"The top of the pages says sites that violate the policies may \"rank lower or not appear in results at all\". replygumby 18 hours agoprevThe README says \"The author does not endorse or encourage any unethical or illegal activity.\"Is it actually illegal anywhere to bypass a paywall? reply qingcharles 7 hours agoparentCertainly in Illinois it would be a crime to violate the TOS of a website. Misdemeanor for first offense, felony for second, IIRC. reply quickthrower2 5 hours agorootparentCan’t be that simple. What if TOS has ridiculous shit in it. Stuff about life long servitude to the webmasters pet goldfish, for example? reply 2cpu1container 17 hours agoparentprevNot sure about the paywalls. But it might be used for \"drive by attacks\" or phishing. reply janejeon 20 hours agoprevReally dummy question: how do services like this work? As in, how do they bypass these paywalls?The obvious thing is to mock Googlebot, but site owners can check that the request isn&#x27;t coming from a Google-published IP and see that it&#x27;s a fake, right? reply Fnoord 18 hours agoparentSome possible clues:> https:&#x2F;&#x2F;github.com&#x2F;kubero-dev&#x2F;ladder#environment-variables> USER_AGENT User agent to emulate Mozilla&#x2F;5.0 (compatible; Googlebot&#x2F;2.1; +http:&#x2F;&#x2F;www.google.com&#x2F;bot.html)> X_FORWARDED_FOR IP forwarder address 66.249.66.1> RULESET URL to a ruleset file https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubero-dev&#x2F;ladder&#x2F;main&#x2F;rul... or &#x2F;path&#x2F;to&#x2F;my&#x2F;rules.yaml reply janejeon 18 hours agorootparentOh wow... I&#x27;m surprised that&#x27;s enough. When I was researching scraping protection bypass, you had to do some real crazy stuff with the browser instance + using residential IPs at a minimum... reply 2cpu1container 17 hours agorootparentThats not the full story. It works on many sites, but some (ft.com as an example) have more severe countermeasures to bypass the paywall. Therefore the ladders modifies the served HTML from origin to remove such.Those rules still need to be build up. (by me or the OS-community) reply ComputerGuru 16 hours agorootparentprevI don’t know of any off-the-shelf product that respects X_FORWARDED_FOR unless the current request ip originates from a whitelisted (or lan) address. reply narinxas 20 hours agoparentprev> site owners can check that the request isn&#x27;t coming from a Google-published IP and see that it&#x27;s a fake, right?just because they can doesn&#x27;t mean they will... also most \"site owners\" are (by this point) a completely different people than \"site operators\" (who I take to be the &#x27;engineers&#x27; who indeed can check this IP things) reply calflegal 19 hours agoparentprevrelated: If this is how they work, why doesn&#x27;t google offer a private service to allow publishers to have content indexed while still protected? reply matsemann 19 hours agorootparentIt used to be against guidelines to serve different content to google vs what users would see. Not sure if still the case, but I don&#x27;t think it&#x27;s in google&#x27;s interest to give a result that the user actually can&#x27;t access. reply ComputerGuru 16 hours agorootparentI’m not aware that this policy has changed. What has changed is that Google will rank results it can’t (officially) index without showing their content. I’m guessing they do shadow index them but use the whole “if you outwardly can’t tell they did then it’s as if they didn’t” C++ compilers use to get away with insane optimizations. reply fader 21 hours agoprevFor folks like me who have no idea what 12ft.io or 1ft.io are, they appear to be services for bypassing paywalls on websites. reply alberto_ol 19 hours agoparentPrevious dicussions of the service on HN:https:&#x2F;&#x2F;hn.algolia.com&#x2F;?q=12ft.io reply 2cpu1container 18 hours agoparentprevThose were Paywall bypassing tools. 12ft.io was shut down one week ago and 1ft.io still works.But I feel a bit unconfident to let someone inject code to sites i view. reply szaboat 19 hours agoprevNot relevant to the project but I usually check for earlier versions of the paywalled pages in the wayback machine (~75% success). I felt bad using these services (paywall removers), and just feeling a bit better checking in archive.org. reply TanguyN 13 hours agoprevI have noticed that on a lot of websites, if you stop the page loading at just the right moment (you have to be quick), the whole content will display without the paywall. And that&#x27;s without any external tools. These kinds of tools seem, of course, much more convenient. reply cooper_ganglia 12 hours agoprevReally great and easy to use. I was trying to read an article that was on the front page of HN and couldn&#x27;t due to paywall. Downloaded the binary and was reading it within 30 seconds. Awesome and very useful tool, thanks! reply arendtio 17 hours agoprevSounds great, not just for paywalls, but for removing CORS as well:> Remove CORS headers from responses, assets, and images ... reply JustinGoldberg9 18 hours agoprevI still miss outline.comI use txtify.it reply jwmoz 18 hours agoprev12ft was really good! reply 2cpu1container 18 hours agoparentIn deed it was. Sad it&#x27;s gone.One single downside was the intransparency. It was not clear which code was added or removed on the site you where looking at. reply KoftaBob 18 hours agoprevCreate a browser book mark and set this as the URL of the bookmark:javascript:window.location.href=\"https:&#x2F;&#x2F;archive.is&#x2F;latest&#x2F;\"+location.hrefIt will usually open up the archived version of article without the paywall. reply rounakdatta 18 hours agoprevGiven a very different paywall model for Substack, what exactly would work for bypassing their paywalls?Wouldn&#x27;t we always require a paid account to cache the HTML through (the SciHub model)? reply some1else 21 hours agoprevRelevant: 12ft.io was banned by Vercel, taking down the developer&#x27;s entire account with multiple other hosted projects & domains: https:&#x2F;&#x2F;twitter.com&#x2F;thmsmlr&#x2F;status&#x2F;1718663563353755982Edit: Access to other projects & domains was apparently restored some time after: https:&#x2F;&#x2F;twitter.com&#x2F;thmsmlr&#x2F;status&#x2F;1719480558932148272 reply abofh 20 hours agoparentLovely, the Google classic \"ban the world\" approach -- I&#x27;ve been desperately trying to move my client off of vercel, this might just be the gasoline. reply benjaminwootton 18 hours agorootparentI followed this drama on Twitter. The author was breaking the terms of service and creating DMCA support burden for Vercel. They had proactively been in touch with him a few times to reach a solution.I think it’s quite reasonable that they blocked the account rather than the project. You wouldn’t have got that level of service from big tech. reply ComputerGuru 16 hours agorootparent> I think it’s quite reasonable that they blocked the account rather than the project.I’m just responding to your last sentence: why would you go out of your way to say it is reasonable to block the account rather than the project?I can understand locking the account just as the “lazy default” but I would not call it in any way reasonable - but you did, so I’m curious.If that is reasonable, what would you consider unreasonable?(Because to me, the obviously reasonable thing to do would be to block the project and not his entire account.) reply benjaminwootton 12 hours agorootparentIt’s just a matter of opinion. I think Vercel were acting reasonably with all of the context we have. Things like terms of business are at the account level, not project level. reply ensignavenger 15 hours agorootparentprevCare to provide any links? The Twitter claim above is that their was no communication and the ban occurred on a Friday. reply Brian_K_White 12 hours agorootparentprevPerhaps, yet it is also reasonable to see this as a reason for anyone else to avoid Vercel.Just because it&#x27;s their right to conduct this way, doesn&#x27;t mean it&#x27;s not everyone elses exactly equal right to judge that and avoid. reply threatofrain 19 hours agorootparentprevOne might consider Cloudflare as a very nice competitor to Vercel in terms of DX, although I suspect all companies use a ban the world approach, even banks. reply MaKey 19 hours agorootparentNot the best time to recommend Cloudflare. reply judge2020 19 hours agorootparentMaybe, but they&#x27;ve kept customers informed throughout the entire outage. https:&#x2F;&#x2F;www.cloudflarestatus.com&#x2F;incidents&#x2F;hm7491k53ppg reply JCharante 18 hours agorootparentprevThey just had a big outage. What’s the probability of having another so soon? reply orphea 18 hours agorootparentIf outages don&#x27;t depend on each other, the probability is the same. reply explaininjs 17 hours agorootparentWhy would you assume independence? I&#x27;d expect an outage to put people \"on edge\" for a period of time following the outage, during which changes are scrutinized to a higher degree, and&#x2F;or a greater engineering focus&#x2F;budget is dedicated to reliability to reflect the changed business&#x2F;image requirements. replyrgrieselhuber 20 hours agorootparentprevAside from this (which is already very shitty and would cause the same response in me) what are the issues you’re running into with Vercel? reply abofh 19 hours agorootparent- Support is failing us - I want my team to use you for vercel support, but it isn&#x27;t there. - Support is failing our customers - when you fail, I end up reverse-depending your repo to tell us why it&#x27;s failing -- just give us a clear answer, we all move away happy, bullshit and I go to lambda where I just accept it. - EOD: Vercel makes engineers happy to bullshit, but gives operations teams nothing acceptable - I want a deliverable product. reply judge2020 19 hours agorootparentFYI you have to use two line breaks to start a new line with (HN&#x27;s) Markdown. reply Rauchg 17 hours agorootparentprevWould love to dig into your support issues. Let me know: rauchg@vercel.com reply abofh 16 hours agorootparentAnd if the support team had done so, I&#x27;d have nothing to converse about :)After digging upwards, additional support seems like an option delivered too late, and too outside of &#x27;proper&#x27; channels - if you want a sanitized rant I can probably deliver it tomorrow, but too-little too-late is where vercel has landed in the operations team. reply Rauchg 10 hours agorootparentAlways looking for the meta-feedback as well on how we can improve support. So, both pieces of feedback welcome! replylxe 18 hours agorootparentprevVery disappointing that this was the path Vercel chose to take. This is something I would expect from Google or Amazon, but not a developer darling like Vercel. Seems like all companies shed their values is service of growth and capitalism at some point or another. A shame. reply canadianfella 20 hours agorootparentprevWhy desperately? reply treyd 18 hours agoparentprevI don&#x27;t know why anyone trusted Vercel in the first place. The vibes of VC money funding an unsustainable offering for a relatively niche market are so strong, it doesn&#x27;t make any sense. reply alwayslikethis 19 hours agoparentprevShows the importance of controlling your own critical infrastructure, or at least not being dependent for critical functions. Other examples include Github and Discord, both having shown the tendency do arbitarily ban users with little recourse. reply paulgb 20 hours agoparentprevTaking down all his projects (not just 12ft) is heavy-handed, but otherwise Guillermo’s response in that thread seems pretty reasonable to me:> Hey Thomas. Your paywall-bypassing site broke our ToS and created hundreds of hours of support time spent on all the outreach from the impacted businesses.> Our support team reached out to you on Oct 14th to let you know this was unsustainable and to try to work with you. reply hombre_fatal 19 hours agorootparentThe 12ft guy doesn’t look so great in that thread. He admits to ignoring the email (gosh I was busy mmkay?) and then argues that Vercel is lying about the extra work they had to do. reply flutas 19 hours agorootparent> gosh I was busy mmkay?Mischaracterization much? He was on Vacation.How many of us read every email for personal projects that comes in when you&#x27;re half way across the world and supposed to be relaxing. reply paulgb 18 hours agorootparentSure, but if you go on vacation and don&#x27;t check your email for two weeks, you can&#x27;t really claim “no warning”. If two weeks isn’t sufficient notice because of vacation it’s fine to say so, but it’s not the same thing as “no warning” just because you’re not checking email. reply stronglikedan 17 hours agorootparentTo take it even further, if you&#x27;re a one man operation, not checking emails regarding your operation for two weeks is pure negligence, vacation or not. reply jdminhbg 16 hours agorootparentprevI might ignore personal project emails while I&#x27;m on vacation, but I also won&#x27;t complain if one of those emails says my billing method is out of date and I come home and it&#x27;s been turned off. reply littlestymaar 10 hours agorootparentBut if you missed one bill for a service and your account was nuked for all the other products you had paid without issue, you&#x27;d be right to get mad, and that&#x27;s exactly what&#x27;s happening here:> I’m sympathetic to vercel here. Honestly very reasonable to take down 12ft with no response in 2 weeks.> Taking everything else is the weird part. reply littlestymaar 10 hours agorootparentprev> but otherwise Guillermo’s response in that thread seems pretty reasonable to me“Other than the completely unreasonable thing, they seemed pretty reasonable”.I mean he&#x27;s not being a complete asshole in the discussion here[1], but nuking the entire customer&#x27;s account for a ToS violation on one specific product still isn&#x27;t a reasonable move. Yes Google and Amazon do it routinely, but if you&#x27;re not a trillion dollar monopoly and you care about your business&#x27; reputation, you shouldn&#x27;t behave like those.[1] CEOs behaving like assholes in Twitter discussions isn&#x27;t supposed to be the norm. reply serial_dev 12 hours agoprevFirst of all congrats on the project and thank you for open sourcing it.> Freedom of information is an essential pillar of democracyHowever, this reads like this tool saves democracy by letting you bypass a crappy pay wall on a site you visit once a year, and that whoever wants to get paid for their published content online is an enemy of democracy. reply UncleEntity 11 hours agoparentIronically, the rest of the paragraph you quoted from gives their reasoning why they believe this tool is needed beyond \"whoever wants to get paid for their published content online is an enemy of democracy\". Double-plus democracy and all that... reply benatkin 14 hours agoprevThis reminds me of the thread when 12ft was taken down.Does anyone have any insight into how it would take Vercel hundreds of hours of support time? https:&#x2F;&#x2F;twitter.com&#x2F;rauchg&#x2F;status&#x2F;1718680650067460138 reply someotherperson 14 hours agoparentMy assumption here is that affected websites sent multiple, persistent support tickets and engaged in back and forth communication, as well as updates to the client, support team contacting engineering&#x2F;legal&#x2F;management&#x2F;meetings on how to deal with 12ft. reply karaterobot 12 hours agoprevIt seemed to me like 12ft.io was useful for a couple of months, but then stopped being useful as they agreed to blacklist more and more URLs. I thought everybody switched to archive.is, which (so far) works 100% of the time, even if it is sometimes a pain in the butt. reply Axsuul 12 hours agoparentIs there an open source version of archive.is? reply metadat 11 hours agorootparentThe operator of archive.is must constantly re-up on hacked credentials for wsj and nyt. Given this is a critical aspect of the service, it is not really feasible&#x2F;useful to open source it. reply boplicity 16 hours agoprevSlightly edited \"Why\":Access to private property is an essential pillar of democracy and the safe proliferation of ideas. While property owners have legitimate financial interests, it is crucial to strike a balance between property and the public&#x27;s right to access property. The proliferation of locks on doors raises concerns about the erosion of this fundamental freedom, and it is imperative for society to find innovative ways to preserve access to people&#x27;s homes and workspaces without compromising the sustainability of property ownership.. In a world where property should be shared and not commodified, locks should be critically examined to ensure that they do not undermine the principles of an open and informed society. reply donohoe 18 hours agoprevI use services like this as I often skip news site paywalls because I just can&#x27;t afford, nor is it practical, to have so many subscriptions.That said, I work in news media (and have been involved in building paywalls at different orgs - NYT and New Yorker). I know how money for these directly support journalism - salaries and the costs with associated with any story.If you are skipping paywalls a lot, I would encourage you to pay for a subscription to at least one or two news sites you respect - bonus points if its a small or medium local newsroom that benefits!For me that has been; NYTimes, New Yorker, Wired, Teen Vogue, and my wife&#x27;s hometown paper in Illinois. reply stetrain 15 hours agoparentMy personal experience with this has been that paying for a subscription still gets me inundated with ads and marketing (often more now that I&#x27;m on their official mailing list), is still inconvenient since I may not be logged in to every news site on every device where I may follow an article link, and leaves me to fight through dark patterns to unsubscribe, since a button to allow you to cancel online is clearly dark magic that has not yet been invented.I do wish there was a better way for me to share an account across multiple news sites that let me properly pay for good journalism without these issues. I do subscribe to a very local news source that seems to handle this a lot better, but they also don&#x27;t paywall (most) of their primary content.In the meantime I do find it strange that so many sites wish to gain the advantage of advertising that they have put up an article on the web, without actually providing that article. I have no issue with paid content, but when that content gets listed in search engine results and social media links like a web page, but clicking on it does not behave like a web page, It feels something feels like something has broken from the idea of the linkable World Wide Web. reply 2cpu1container 1 hour agoparentprevI can fully agree on that. reply mejthemage 17 hours agoparentprevThere&#x27;s a huge need for subscription bundles. I&#x27;d gladly pay $20&#x2F;mo for access to a bunch of big names, even if I&#x27;m limited to like 60 articles per month combined across those sources.Instead I just don&#x27;t pay anyone, turn back when I encounter a paywall and look for someone&#x27;s summary if I&#x27;m really interested. reply orpheansodality 16 hours agorootparentIsn’t that the value-prop of Apple News? reply stetrain 11 hours agorootparentIn my experience Apple news relies entirely on the app for reading articles, so if you are on a computer without the app or following a link that doesn&#x27;t auto-redirect to the app then you still hit the paywall.I&#x27;d rather have a system that was just a cross-website web account. reply QkPrsMizkYvt 16 hours agorootparentprevThere used to be an app called scroll (https:&#x2F;&#x2F;twitter.com&#x2F;tryscroll?lang=en), which got bought by Twitter, which is now part of subscription, but only for the top articles. Informed.so is doing something similar but different: https:&#x2F;&#x2F;www.informed.so&#x2F;The problem creating such a service is that most media houses believe that their content is the best thing since sliced bread and thus they often don&#x27;t want to partner. Even though most of their content isn&#x27;t that unique. Of course, some publications do have unique content, e.g. nyt, bloomberg.I could see artifact being an interesting company to tackle this though (https:&#x2F;&#x2F;artifact.news&#x2F;). They are already sending traffic to news sites and only serving what the user wants. If they now let me bypass paywalls for $20 that would be nice. reply roydivision 21 hours agoprevIs it just me or has 12ft become less and less effective? I rarely get through with it these days. reply user_7832 21 hours agoparentTheir policies have apparently… changed. They accept donations to not have your website bypassed. Archive.org is much better.Edit: apparently it is down now.402: PAYMENT_REQUIRED Code: DEPLOYMENT_DISABLED ID: fra1::8wkv2-1699275385535-39dedae23d6a reply jdiff 20 hours agorootparentIs it donations they accept or legal threats? reply ProllyInfamous 20 hours agorootparentYes. reply i67vw3 20 hours agorootparentprevArchive.today never fails compared to Archive.org or various browser extensionsTo remove paywalls 12ft settings > privacy and security > DNS over HTTPS > Manage exceptions > Add \"archive.is\", \"archive.ph\", and \"archive.today\" reply Brian_K_White 12 hours agorootparentIt has nothing to do with dns-over-https, it has to do with using cloudflare dns at all, over https or plain.In my case I added override rules in my opnsense router so that archive.is .ph .today .md are all resolved by a different nameserver.Disabling DOH can appear to fix it only in the happenstance case that the fallback plain dns doesn&#x27;t end up using cloudflare, or doesn&#x27;t use it first. reply ryeights 13 hours agorootparentprevFor those on mobile it was the opposite, since the archive sites only show you the desktop sites reply gnicholas 13 hours agorootparentDoes reader mode work on archive sites? reply snarkyturtle 20 hours agoparentprevBefore they went down it seemed that there were many big publishers who got the owner to disable it for their sites. Either that or the sites learned to actually not send their articles unless the user is logged in (and didn&#x27;t care about googlebot not scanning it).It was just an effective way to get through substack&#x2F;medium in my experience. reply ams92 21 hours agoparentprevI’ve rarely found it to be able to skip a paywall, I gave up after trying a few times. reply j-a-a-p 17 hours agoprev [–] In the README there is a WHY paragraph:> Freedom of information is an essential pillar of democracy and informed decision-making. While media organizations have legitimate financial interests, it is crucial to strike a balance between profitability and the public&#x27;s right to access information. The proliferation of paywalls raises concerns about the erosion of this fundamental freedom, and it is imperative for society to find innovative ways to preserve access to vital information without compromising the sustainability of journalism. reply j-a-a-p 17 hours agoparent [–] For me this is grotesque. Democracy is in dispair so is journalism. What exactly is this software doing to support journalism or democracy? reply 2cpu1container 16 hours agorootparent [–] We live in a world, where we have more misinformation and poor journalism every day, and less money in the pockets of the people to afford paying for good journalism. So this might start a more open discussion on how to finance journalism. And while discussions are still going on, people can inform themselves with good journalism, which supports the democracy. reply j-a-a-p 11 hours agorootparent [–] > And while discussions are still going on, people can inform themselves with good journalism, which supports the democracy.That is a looters mentality, sorry to say that. Paywall jumping software is like robbing a disabled old veteran in public transport. It are the last blows to finish off what was once good journalism. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The speaker has developed an open-source alternative to specific services due to doubts about their efficacy and operations.",
      "The new system is written in Golang, a statically typed, compiled programming language designed at Google.",
      "Highlighting the versatility and adaptability of the new system, it is fully customizable for different usage scenarios."
    ],
    "commentSummary": [
      "The speaker has developed an open-source solution as an alternative to specific services due to mistrust in their operations.",
      "The newly designed system, created using Golang, offers complete customization."
    ],
    "points": 346,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1699272284
  },
  {
    "id": 38161016,
    "title": "Prossimo Releases Stable Version of Sudo-rs, A Security-Enhanced Linux Utility Developed in Rust",
    "originLink": "https://www.memorysafety.org/blog/sudo-first-stable-release/",
    "originBody": "Prossimo is pleased to announce the first stable release of sudo-rs, our Rust rewrite of the critical sudo utility. The sudo utility is one of the most common ways for engineers to cross the privacy boundary between user and administrative accounts in the ubiquitous Linux operating system. As such, its security is of the utmost importance. The sudo-rs project improves on the security of the original sudo by: Using a memory safe language (Rust), as it's estimated that one out of three security bugs in the original sudo have been memory management issues Leaving out less commonly used features so as to reduce attack surface Developing an extensive test suite which even managed to find bugs in the original sudo The Wolfi Linux OS already includes sudo-rs and we hope that others will follow their lead. \"When we first set out to build Wolfi, making sure it was memory safe was always a top priority,\" said Dan Lorenc, CEO and Co-founder at Chainguard. \"The sudo utility is a perfect example of a security-critical tool that's both pervasive and under-appreciated. Security improvements to tools like this will have an outsized impact on the entire industry. The work that went into building the first sudo-rs release is a great step forward in eliminating potential security issues by adopting memory safe languages like Rust. This is critical for upholding and maintaining Wolfi as the secure-by-default foundation for developers who want to address most modern supply chain threats.\" A joint team from Tweede Golf and Ferrous Systems built sudo-rs under contract with Prossimo. We're pleased with how much progress they've made since starting this project in December, 2022. An external security audit of the sudo-rs code is scheduled to start in September 2023. After that, the team will start on Milestone 4 of our work plan, which focuses on enterprise features. The original C-based sudo utility has been maintained by Todd C. Miller for many years now, and we're grateful to him for taking on this huge and important task. We're also grateful that Todd has made time to offer us excellent advice on implementing sudo-rs. Prossimo is able to take on the challenging work of rewriting critical components of the Internet thanks to our community of funders from around the world. We’d like to thank the NLnet Foundation for their funding of the audit of Sudo-rs. We'd also like to thank Amazon Web Services for supporting this work and supporting the transition to memory safe software. ISRG is a 501(c)(3) nonprofit organization that is 100% supported through the generosity of those who share our vision for ubiquitous, open Internet security. If you'd like to support our work, please consider getting involved, donating, or encouraging your company to become a sponsor.",
    "commentLink": "https://news.ycombinator.com/item?id=38161016",
    "commentBody": "The first stable release of a memory safe sudo implementationHacker NewspastloginThe first stable release of a memory safe sudo implementation (memorysafety.org) 335 points by goranmoomin 23 hours ago| hidepastfavorite235 comments coggs 20 hours agoAs one of the original creators of sudo (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sudo) I&#x27;ve witnessed it getting nearly totally rewritten and then incrementally bug-fixed over the last 43 years. It must take the prize for the UNIX command most highly-scrutinized for security flaws. Flaws which have been identified and fixed.Thousands of developers and security experts have gone over it. So part of me wonders - how is it possible for a single dev team to totally reimplement it without unknowingly introducing at least a bug or two? Is there something to this Rust language which magically eliminates all chances of any bug being introduced? reply rnijveld 15 hours agoparentI can only thank you for the work you&#x27;ve done in creating sudo, I think it&#x27;s an invaluable tool in the general day to day use for so many people. As someone working on sudo-rs, our goal with creating it never was to invalidate any of the work previously done, and we are very much aware that our implementation will not be bug free, especially not at the start.For me personally, creating this Rust version allowed me to work on something that I would normally not be able to work on, given how I would not rate my confidence in writing relatively safe C code very high. If nothing else, at least we already found a few bugs in the original sudo because of this work. Despite the 43 years of bugfixing, such a piece of software is unlikely to ever be free of bugs, even if just for the changing surroundings.Other than that, having some alternatives can never hurt, as long as we keep cooperating and trying to learn from each others work (and from each others mistakes). reply tptacek 19 hours agoparentprevWhatever else happened in those 43 years, we had a widely-exploitable memory corruption vulnerability (Baron Samedit) as recently as 2021. reply ndr 18 hours agorootparentAnd it looks like it was a buffer overflow:https:&#x2F;&#x2F;blog.qualys.com&#x2F;vulnerabilities-threat-research&#x2F;2021...Would Rust prevent this? reply jwilk 17 hours agorootparentDiscussed on HN:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25919235 (321 comments) reply steveklabnik 18 hours agorootparentprev> Would Rust prevent this?This is often hard to say.In a very literal sense, you could write this same code, in unsafe Rust, so one could argue that Rust does not prevent it.Some may argue that if this program was written in Rust in the first place, \"concatenate all command line arguments into one big string for processing\" wouldn&#x27;t be the way you&#x27;d go about escaping command line arguments. The issue here is about misplacing a null terminator, Rust strongly prefers a \"start + length\" style of representing strings instead of null terminators, so you&#x27;d never really end up in this situation in Rust in the first place.I&#x27;m sure there&#x27;s other ways to evaluate the situation as well. Which one you find compelling is up to you. reply jvanderbot 18 hours agorootparentprevYes buffer overflows are one of the explicitly addressed vulnerabilities of Rust&#x27;s bounds checker, which is always on, if memory serves. I haven&#x27;t touched Rust in a year. reply adastra22 16 hours agorootparentYou can get around the bounds checker with unsafe code. But yes, by default an overflow should result in a panic and program termination. reply Filligree 14 hours agorootparentYou can, but unsafe code is discouraged in general, even given a slight performance cost.For performance-insensitive, security-critical code, there really shouldn&#x27;t be any such code in the entire program—and it would be easy to verify that with a presubmit. reply bunderbunder 9 hours agorootparentIn an ideal world, no, there shouldn&#x27;t. But `unsafe` is not just a performance hack; people do find things that they legitimately need to do that Rust can&#x27;t statically verify. Probably the most trivial example is interacting with code that is not itself written in Rust.This does imply that some of the stronger claims about Rust&#x27;s level of static safety guarantees that float around on the Internet can&#x27;t really be true unless substantially everything you might want to do has a version that&#x27;s been completely written in Rust. Whether you feel that means that achieving the desired level of safety implies you&#x27;ve still got to rely on some dynamic analysis tools just to be sure probably depends on how much safety you really want, and how much faith you&#x27;re willing to place in the skills of the authors of the libraries you use.And even then, if we really want to go least common denominator, if you&#x27;re running your program on Windows or a Unix or basically any other OS that isn&#x27;t Redox, then you&#x27;ve got unsafe code executing every time Rust&#x27;s own standard library needs to make a syscall to achieve something.Which I don&#x27;t say by way of criticizing rust Rust. It&#x27;s got to live in the same crappy world we all have to live in, and it&#x27;s arguably doing a better job of de-crappifying it than any other systems programming language. I&#x27;m just trying to illustrate how an unqualified statement along the lines of \"there shouldn&#x27;t be any unsafe code in the entire program\" is kind of a self-strawman, precisely because Rust has to live in said crappy world, and I think that it might be unsafe to lose sight of that fact. reply qerti 13 hours agorootparentprevThis comment reminds me of “What you&#x27;re referring to as Linux, is in fact, GNU&#x2F;Linux, or as I&#x27;ve recently taken to calling it…” reply adastra22 13 hours agorootparentLook into the crates you use and you’ll find tons of unsafe code, especially around custom data structures doing buffer pointer arithmetic and stuff. If it’s wrapped in a safe interface, you’d never know. reply slashdev 16 hours agorootparentprevYes, unless you use unsafe code. reply Hedepig 18 hours agorootparentprevYes it would be prevented by the borrow checker. reply chlorion 15 hours agorootparentThe borrow checker does not prevent out of bounds access of arrays (or vectors or whatever you want to call them).The borrow checker is intended to protect against \"temporal memory unsafety\". It can tell you that you are using something that has already been freed, or something that could be freed while you are using it for example.Bounds checking is a \"spatial memory unsafety\" problem, it has nothing to do with borrowing and exclusive references.Bounds checking is a trivial problem, for an array that has N length, like a char[N], something tried to access a value past the end of the array (like char[11] if N=10).Rust doesn&#x27;t really do anything special here and protecting against buffer overflows does not require any novel technology.An implementation of bounds checking is as simple as an \"assert(I >= 0 && IValgrind existsYou may be right on an infinite frictionless plane, but unfortunately that does not work in real life, cf. e.g. https:&#x2F;&#x2F;msrc.microsoft.com&#x2F;blog&#x2F;2019&#x2F;07&#x2F;why-rust-for-safe-sy...> Memory-safe is not the same thing as secure.And safety belts do not help you if your car is on fire, still it&#x27;s better to wear it. reply adgjlsfhk1 17 hours agorootparentprevThe fundamental problem with valgrind is it only looks at what happened, not what could happen. Valgrind is great at making sure you don&#x27;t have memory safety issues for \"normal\" inputs, but is basically useless at making sure your code doesn&#x27;t have memory safety vulnerabilities when fed atypical inputs. reply elteto 13 hours agorootparentprevMany vulnerabilities rely on crafting very particular inputs that trigger memory corruption in programs. Unless you happen to have fed that same input to your program when running it under Valgrind then Valgrind is useless for this case. reply chlorion 15 hours agorootparentprevIt&#x27;s true that it doesn&#x27;t eliminate all bugs in general, but it can completely eliminate buffer overflows for example.There is no excuse to not at least have bounds checking. This is one of the most basic memory safety problems and it&#x27;s trivial to prevent.Just preventing this small issue will prevent a non-trivial fraction of bugs. I don&#x27;t have sudo&#x27;s bug list on hand but I wouldn&#x27;t be surprised if 25% or more are caused by buffer overflows.So even if it doesn&#x27;t prevent all logic bugs, it cuts out a pretty big chunk of the bug list.>assuming you don&#x27;t switch them offYou can&#x27;t switch them off.>Rust community&#x27;s tendency to pitch this stuff as a security panaceaI&#x27;ve not seen anyone claim this so far. reply bunderbunder 10 hours agorootparent> You can&#x27;t switch them off.https:&#x2F;&#x2F;doc.rust-lang.org&#x2F;book&#x2F;ch19-01-unsafe-rust.html reply Too 4 hours agorootparentI’d rather have safety default on with an opt-out, rather than the inverse that C gives you with -Werror -Wall -Weverything -Wyesireallymeanteverything. Compile it again one two different architectures, compile yet another time with clang-tidy and then static analysis with Coverity just to be sure. Run it with valgrind, asan and thread sanitizer. Sprinkle some fuzz testing on top.Yet you still don’t the same level of confidence as a rust program that may have a small unsafe block in one corner of the code. reply mtlmtlmtlmtl 5 hours agorootparentprevI feel pretty good about the fire safety measures at my apartment despite the fact that I own several lighters. reply timschmidt 8 hours agorootparentprevEven unsafe Rust comes with significantly more checks and safety built-in than C. reply lionkor 19 hours agorootparentprevIs that because theyre easy to find, or because theyre the worst? reply tialaramex 19 hours agorootparentA lot of the most serious security vulnerabilities are memory safety because e.g. remote code execution is very often along the lines of \"LOL, I smash buffer with machine code, it gets executed\" and that&#x27;s a memory safety problem.For sudo you have potential for some very serious logic bugs, where the program does exactly what the programmer wrote, but what they wrote was not what they intended.Rust&#x27;s type safety makes it less vulnerable to these mistakes than some languages, but there is no magic. In C obviously a UID, a PID, a duration, an inode number, a file descriptor, a counter are all just integers. In Rust you could make all those distinct types (the \"New type idiom\"), and out of the box the Duration and the File Descriptor are in fact provided as distinct types. So, some improvement. reply Someone 18 hours agorootparent> In C obviously a UID, a PID, a duration, an inode number, a file descriptor, a counter are all just integers. In Rust you could make all those distinct typesFor various kinds of IDs you can do that in C, too: struct UID { int value; };A C compiler can pass these in registers to functions (https:&#x2F;&#x2F;wintermade.it&#x2F;blog&#x2F;posts&#x2F;value-struct.html). So, performance impact should be zero.It may be not as nice as other languages, but it isn’t bad, either. If you use C++, it can be made a bit nicer, and you could also have such structs that you can calculate with. reply viraptor 4 hours agorootparentIt exists, but... throw in some macros or generic cache&#x2F;storage which is untyped and you end up with a non-trivial version of this: struct GID gid = *(GID*) &some_uid;Which will compile without issues or warnings by default. No belts or braces in this area. reply elteto 13 hours agorootparentprevYou can technically do this but then you have to write wrapper functions for all relevant syscalls or libc functions to unpack the structure and call the actual thing. Lots of work. reply yakubin 19 hours agorootparentprevThey’re easy to make. reply hoherd 19 hours agoparentprevRust aside, one thing to consider is that a reimplementation of an existing piece of software does offer the benefit of being able to test the old version and the new version side by side for consistent behavior. You could have an entire class of test cases that is just \"do X with the old version, and then do X with the new version, and just make sure the result is the same.\" There is also the entire bug history of the old version that can be investigated during reimplementation. If the old version has specific tests for each resolved bug, those can also be run against the new version to ensure it has consistent behavior.In this case though, it&#x27;s only a partial reimplementation: \"Leaving out less commonly used features so as to reduce attack surface\", which would complicate that approach. reply larodi 13 hours agoparentprevOf course it&#x27;s not. But hubris is possible and does not take years to master. And honestly statements like my overnight-rust-sudo is better than some poor peoples 40 years of work... well they don&#x27;t really help Rust becoming more popular, but actually contribute to everything rust becoming even more irritating. Most rust tools are released with this pathos of \"we fix what oldies couldn&#x27;t get right with C\". Not a great attitude to approach giants whose shoulders we&#x27;re standing on indeed. reply sophacles 10 hours agorootparentWho said anything about \"overnight\"? This project has been worked on for a year, implemented a test suite that found bugs in the original sudo, and have generally been respectful of the original work.I think you might be projecting something here, there&#x27;s no evidence for for your assertions. reply pohl 19 hours agoparentprevI think a better question might be whether it prevents categories of bugs that are more likely to be exploitable than, say, the logic errors that no language could ever prevent?Also, it sounds like your seasoned eyes would be valuable in reviewing this code. reply sgerenser 17 hours agoparentprevIt can eliminate many bugs, but it certainly wouldn’t eliminate all bugs. During implementation they realized they were not implementing sudo’s (undocumented) feature of failing to run if the sudoers file is world-writable: https:&#x2F;&#x2F;ferrous-systems.com&#x2F;blog&#x2F;testing-sudo-rs&#x2F;.Of course they did find and fix the bug, but in general Rust isn’t going to protect you from bugs like this that are essentially logic errors. reply Calzifer 16 hours agorootparentThat is documented. Since the mercurial web interface isn&#x27;t very nice to use I picked a random version. sudo 1.8.6 from 2012 writes in the man page \"The sudoers file must not be world-writable,\".https:&#x2F;&#x2F;www.sudo.ws&#x2F;repos&#x2F;sudo&#x2F;file&#x2F;SUDO_1_8_6&#x2F;doc&#x2F;sudoers.m...This is also a very common behaviour for security sensitive applications to check config file permissions. Another example I remember are ssh private keys.I might be to harsh but it is not so trustworthy they still made this error and still miss the documentation. reply jrmg 16 hours agorootparentI’m not sure why people are downvoting you. I suspect they may be clicking the link and thinking ‘that’s not documentation it’s source code’, not realizing it actually _is_ documentation.The language it’s in is ‘mdoc’ - a markup format for man pages: https:&#x2F;&#x2F;man.freebsd.org&#x2F;cgi&#x2F;man.cgi?mdocIt’s the source code for the man page, which is about as documentationey as you can get. reply sgerenser 15 hours agorootparentprevInteresting, the posting I linked to indicated this behavior wasn’t documented. It’s certainly not surprising and as you mentioned, it’s equivalent to openssh requiring specific permissions on private key files. reply noahjk 20 hours agoparentprevOn the surface, sudo seems fairly straightforward, so it’s interesting to hear how much work has gone into it! Do you have any interesting facts or anecdotes you’d care to share? reply coggs 19 hours agorootparentHackaday interviewed me about the origin story - https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LaAwl3HN5ds&ab_channel=HACKA... reply ralgozino 17 hours agorootparentgreat story! also, TIL that I&#x27;ve been pronouncing `sudo` wrong, I was 100% sure that it was supposed to be like pseudo, but I guess that is a myth :)It&#x27;s so great to be able to listen and learn from the people that invented these important building blocks themselves, I feel lucky. Thanks for sharing. reply nolok 19 hours agorootparentprevThe key is in \"on the surface\". While the common usage of sudo is fairly straightforward, you me and most people use like 5% of it. The trick is in all the side shows. reply yesimahuman 19 hours agorootparentMakes you wonder then why it does so much, if those rarely used features increase the surface area of possible exploits? This is just a question I’ve had about *nix utilities in general, since sudo is hardly the only tool with obscure flags and features reply yjftsjthsd-h 19 hours agorootparentBecause the long tail of features is useful to someone. Mind, I like doas for this reason, but having the more feature rich option available makes sense. reply __turbobrew__ 17 hours agorootparentprevThis is part of what the openbsd ‘doas’ was trying to solve. They drastically reduced the functionality to reduce the attack surface. reply folmar 19 hours agorootparentprev> sudo seems fairly straightforward`su` is straightforward, `sudo` is a very powerful piece of software and the configuration has a lot of edge cases. reply SoftTalker 18 hours agorootparentYes, have a read of the sudoers man page and marvel at the complexity of the configuration, and wonder about your chances of getting it right if you are not well-experienced. This is the config file with the infamous paragraph:The sudoers grammar will be described below in Extended Backus-Naur Form (EBNF). Don’t despair if you are unfamiliar with EBNF; it is fairly simple, and the definitions below are annotated.OpenBSD replaced sudo with their own \"doas\" command a few years ago; the doas.conf manual page is about 100 lines; sudoers is over 2,000. reply stefs 19 hours agoparentprev> Is there something to this Rust language which magically eliminates all chances of any bug being introduced?no, altough it has features that prevent or reduce the probability of some types of bugs - one example of this being memory safety bugs. rust can&#x27;t prevent logic bugs.the rust reimplementation probably has more bugs than the original, but a theoretically better chance to achieve fewer bugs in the long run.is rewriting mature linux infrastructure in rust a good idea? many people agree that no, it&#x27;s probably not a good idea outside of special use cases. reply amai 15 hours agoparentprev> How is it possible for a single dev team to totally reimplement it without unknowingly introducing at least a bug or twoThis is possible if every bug fixed has an associated test. If they use this battery of tests to test their new implementation it should be as good as the original implementation. reply AndyKelley 13 hours agoparentprevBy making it simpler and not having a ton of rarely used features, and by using a programming language that makes it more difficult to write bugs. reply kristopolous 15 hours agoparentprevSo let&#x27;s settle this. Does sudo rhyme with judo or voodoo? reply jjgreen 13 hours agorootparentsu(peruser) do, so soodoo reply flexagoon 4 hours agorootparentIt actually stands for \"substitute user do\" now reply kristopolous 12 hours agorootparentprevLanguage isn&#x27;t necessarily that logical. It can be whatever he says it is. reply jjgreen 11 hours agorootparentLanguage will evolve as it will, the person who invents a word does not get to tell the world how it will be pronounced for the rest of time. reply kristopolous 10 hours agorootparentCool I&#x27;ll pronounce it ghoti then reply jjgreen 10 hours agorootparentYour grandchildren may replynarinxas 20 hours agoparentprev> Is there something to this Rust language which magically eliminates all chances of any bug being introduced?apperently, yes... and its the type system, but grantedit&#x27;s only &#x27;memory safety&#x27; bugs... the kind of error that C languages are really suceptible to. reply alkonaut 15 hours agoparentprevI’m sure there is a logic bug or two in the new implementation. Whether you want to take the risk of new logic bugs for the benefit of removing several whole categories of bugs (both known and unknown!) is the question and tradeoff in each case like this. This too requires scrutiny, but I’d be a lot more comfortable running a Rust program with 2 years of scrutiny than any C program with 40. reply godelski 16 hours agoparentprevYou gotta start somewhere right? I mean its not like you got it right the first time. Don&#x27;t everyone go switching over just yet, but people can&#x27;t scrutinize something that doesn&#x27;t exist. reply grayhatter 17 hours agoparentprevThank you for working to create one of the tools which is obviously on the list of the most valuable and beneficial to computer security. Perhaps only second to netfilter.And I&#x27;m really sorry so many people have decided they&#x27;re going to imply something is wrong or broken with it for their own clout. Or because they&#x27;ve bought into the lie that no code written in C can be safe or correct.For what it&#x27;s worth, I and all the engineers I willingly associate with (read: the ones who I respect) all have said the exact same thing. Switching to rust here, just &#x27;cause, isn&#x27;t going to meaningfully increase anyone&#x27;s security. But what are you gonna do. Other than ask people to be honest?Annoying fanboys aside... again, *thank you*! The computer security world is meaningfully better because of your work, and that&#x27;s something the RIIR fad will never be able to replace :) reply WesolyKubeczek 19 hours agoparentprevWhile I don&#x27;t negate your experience and I genuinely anticipate that this project is going to rediscover some pain, there&#x27;s something to be said about the fact that we don&#x27;t have to replicate the life work of Newton, Leibniz, Maxwell, etc to really \"get\" classical physics. It fits now into the high school curriculum, and if you pass it, you can be fairly decent at it; with a little additional effort, you can get real freaking good at what took those people their whole freaking lifetimes.This is because we can stand on those giants&#x27; shoulders and have the benefit of hindsight and not have to also repeat each and every of their blunders, and have better technology and learning methodology to boot.So I presume if you yourself wanted to rewrite sudo from the first principles, you, with all your experience and knowledge already there, would spend a lot less time doing it, and it would be way cleaner and simpler.So while I&#x27;m not dunking on your effort and experience, I&#x27;m just pointing out that it&#x27;s not impossible to take your experience and turn it into something better over a smaller timespan. reply lionkor 19 hours agoparentprevNo, but maybe it feels better to know memory bugs aren&#x27;t there (but all others are, and worse than in sudo) reply knorker 12 hours agoparentprev> how is it possible for a single dev team to totally reimplement it without unknowingly introducing at least a bug or two?As someone with over three decades of C programming experience (so not as much as you), maintaining widely used stuff written in C for decades, that has recently switched from C and C++ as main languages for systems programming to Rust, I&#x27;d instead ask this: How is it possible, even given 43 years of working the problem, to create a program in C that does what it&#x27;s supposed to, and only what it&#x27;s supposed to?But also, one of the answers from the article is \"Leaving out less commonly used features so as to reduce attack surface\". Most security bugs in sudo are in features I don&#x27;t use.Rust isn&#x27;t just memory safe. It&#x27;s also orders of magnitude harder to accidentally make other mistakes, such as race conditions. reply khimaros 18 hours agoparentprevi wonder what fraction of these fixes came with automated texts to prevent regressions (and to aid new implementations from making the same mistakes). reply cyber_kinetist 19 hours agoprevI think the two bullet points they listed for their project (other than using Rust) are often overlooked:- Leaving out less commonly used features so as to reduce attack surface- Developing an extensive test suite which even managed to find bugs in the original sudoWhich are the most important aspects when writing any safety-critical code, even moreso than rewriting in Rust! reply Terretta 9 hours agoparentAre those more important than, say:- Proven with Coq, a formal proof management system: https:&#x2F;&#x2F;coq.inria.fr&#x2F;See in the real world: https:&#x2F;&#x2F;aws.amazon.com&#x2F;security&#x2F;provable-security&#x2F;And check out Computer-Aided Verification (CAV). reply Too 4 hours agorootparentA formal proof is only as good as its requirements. Leaving out features and reducing complexity is for sure going to be more secure. reply garblegarble 21 hours agoprevI&#x27;m assuming this project&#x27;s aim is to replace sudo, in which case hand-waving away \"Leaving out less commonly used features\" is a bit worrying. What are these features? How uncommonly are they used? In which way will it fail if a configuration uses those features?Edit: Looks like their github readme outlines some of these limitations, https:&#x2F;&#x2F;github.com&#x2F;memorysafety&#x2F;sudo-rs#differences-from-ori... reply hpb42 21 hours agoparentOne of those left out features is `sudoedit` or `sudo -e`. I use this a lot when editing files in &#x2F;etc or any file that my user does not have permissions. The flag first copies the file to a temporary location with permissions for my user to edit, then opens my text editor (defined via $SUDO_EDITOR env var) as _my user_, without any sudo permissions. After I close the editor, the file is copied back with the original permissions only if there were any changes.The cool thing is running the editor via my user, which loads my user&#x27;s configuration&#x2F;plugins, instead of the root user&#x27;s. reply lucideer 20 hours agorootparent> One of those left out featuresThey indicate that many omittted features are by design, but this particular one is implied to be planned:> Some functionality is not yet supported; in particular sudoedit reply josephg 20 hours agorootparentprevThis would be a great use case for a capability security model. Essentially what you really want is the sudo command to acquire a temporary capability token to edit that specific file. Then run your editor and pass it the capability. (And revoke the capability when the editor process closes).It’s a pity this isn’t more straight forward to implement on Linux. reply arijun 14 hours agorootparentAre there no overwrite&#x2F;seek bugs in Unix that could be exploited in that case? It seems to me like only using sudo for a cp command would reduce the attack surface. reply josephg 1 hour agorootparentI’d certainly hope not. A capability based security system in an OS is only secure if it doesn’t have bugs. Just like most security-critical software.However, I’m not sure that Linux is capable of masquerading as a general purpose capability based operating system. I think it’s missing a bunch of APIs. reply quotemstr 20 hours agorootparentprev> Essentially what you really want is the sudo command to acquire a temporary capability token to edit that specific file.This should be doable with an XDG portal model, right? reply yakubin 19 hours agorootparentIt&#x27;s doable by opening the file in a privileged process (sudo) and passing the file descriptor to a non-privileged process.Maybe one could make a sudoedit that opens a file in sudo process and then spawns a non-privileged editor process which inherits the file descriptor and is given the &#x2F;dev&#x2F;fd&#x2F; path on the command line, so it stays none the wiser about the whole process. reply vlovich123 17 hours agorootparentSounds like a bit of recipe for accidentally handing access to an unintended privileged fd through inheritance (ignoring the &#x2F;dev&#x2F;fd one) such that a compromised unprivileged SUDO_EDITOR value gives you sudo access. Maybe not likely, but I’d really be hesitant about any feature that relies on implicit fd inheritance… reply kelnos 13 hours agorootparentAnother option could be to open a UNIX socket in the privileged sudo process, spawn an unprivileged child process &#x27;shim&#x27; that connects to that socket, and then the sudo process can pass the file descriptor over the socket. Since the child shim is &#x27;clean&#x27;, it should have nothing more than stdin&#x2F;out&#x2F;err open, plus now this passed FD. Then the shim can spawn the target program and allow it to inherit just the passed FD.I think the larger issue is that I doubt many (if any?) editors allow opening a file via an inherited file descriptor! I guess some will read stdin (the shim could close stdin and then dup2() it into its place), but then there&#x27;s no way to save the file back when finished. reply yakubin 15 hours agorootparentprevClose all other fds between fork and exec then (you can look at the code of base::LaunchProcess in Chromium for an example). It’s a minuscule amount of code to audit compared to XDG portals. And it’s backwards-compatible with decades of unix programs.For a more complicated solution: spawn a zygote process early with a unix socket which you’ll use to send the fd later. Zygote at start drops provileges. When it receives the fd, it closes the socket and execs the editor. reply vlovich123 15 hours agorootparentI’m not saying it’s not possible to do correctly. But do you not agree that the first is hard to correctly (can overlook an fd) while the latter is a lot of complexity?There is the CLOEXEC flag which is the intended way to manage this but it’s not the default and you have to be diligent about setting it which again carries its own set of challenges.What you’d really want is CLOEXEC implicitly on all fds and having to explicitly opt in for fd inheritance. reply yakubin 14 hours agorootparentThat would be ideal, but we&#x27;d need a new syscall for that. reply sophacles 14 hours agorootparentprevRust stdlib opens all fds with CLOEXEC set. You have to explicitly disable it when you want the child to inhert the fd. replyaumerle 17 hours agorootparentprevSo now any program that&#x27;s running as your user, even your browser, can edit any file you edit with sudo. It just has to watch for your editor to quit and win a race with sudo to modify the file before sudo reads it. reply bombcar 10 hours agorootparentprevIt sounds like 90% of \"sudoedit\" can be done without elevated permissions, assuming your user can read the file. reply tambourine_man 21 hours agorootparentprevIn Vim::w !sudo tee %Which I map to :w!!Of course, if you’re not using Vim, you’re doing it wrong :) reply phanimahesh 20 hours agorootparentIt is a little less useful if the file is not readable by your user, and once you authenticate anything within your vim can also silently run other sudo commands since on most distros sudo remembers the autnentication for a while.Now that I think of it, not sure how sudoedit behaves wrt this cached auth. reply KMnO4 19 hours agorootparentI think you can use the sudo -k flag to clear the cached auth reply Zardoz84 21 hours agorootparentprevI usully use \"sudo -E EDITOR_OF_MY_CHOICE\" reply liftm 20 hours agorootparentMany editors can execute shell commands, so this isn&#x27;t the same at all. reply Denvercoder9 19 hours agorootparentprevThat makes your editor run as root, which is a bad idea for many reasons (aside from security, any mistake now has the potential mess with the whole system). reply gary_0 21 hours agoparentprevOne feature they didn&#x27;t mention they left out was the ability to run `make me a sandwich` (https:&#x2F;&#x2F;github.com&#x2F;sudo-project&#x2F;sudo&#x2F;blob&#x2F;main&#x2F;Makefile.in#L...) reply queuebert 20 hours agorootparentIs that a Slashdot reference? reply tecleandor 20 hours agorootparentOld XKCD joke (maybe coming from an older joke)...https:&#x2F;&#x2F;xkcd.com&#x2F;149&#x2F; reply gary_0 20 hours agorootparentThat XKCD joke was all over the Internet in 2006. Now get off my lawn. reply kelnos 13 hours agorootparentHuh, it took 6 years for the joke to get added: https:&#x2F;&#x2F;github.com&#x2F;sudo-project&#x2F;sudo&#x2F;commit&#x2F;c1d6e86d67aa60d7... reply tecleandor 20 hours agorootparentprevWell, that comic is from 2006 so not that far away :P ;) replysigio 20 hours agoparentprevOne of the features I use in some (larger) environments, which isn&#x27;t on the roadmap or implemented is LDAP support in sudo-rs. Using the regular sudo, this allows you to manage the sudo permissions for the entire network from the central LDAP configuration, and even make rules that are time&#x2F;host&#x2F;user&#x2F;command limited in a central location with no chance of simple syntax-errors wiping out your entire configuration, just that single rule is being ignored in this case. reply SonOfLilit 21 hours agoparentprevI&#x27;ve lived through the transition to systemd. I&#x27;m sure sysadmins will be able to manage this one. And I&#x27;m sure great technical documentation exists, this was a PR release, not where I&#x27;d look for a list of missing features. reply jacquesm 21 hours agorootparentI still run into trouble on a fairly regular basis on account of systemd. Especially the log files continue to cause all kinds of issues. reply happymellon 21 hours agorootparentWhat issues do you run into with the log files?I have plenty of ideological problems with the design of the logs, but not actually ran into problems in the real world. reply djbusby 21 hours agorootparentI&#x27;m still mad I can&#x27;t just `tail -f` reply stouset 12 hours agorootparentnext [–]journalctl -fOn the flip side, now you get structured logging, efficiently-searchable logs over any of those fields, the ability to easily aggregate logs from multiple machines, the ability to accurately iterate over logs in processes without missing entries, and on and on and on.Logs as a database is wildly superior to logs as a plain text file, with virtually the only downside being that you need a specific program to tail them. reply happymellon 12 hours agorootparent> the only downside being that you need a specific program to tail them.The journal can output to text files as well!But this is exactly it, my only issue is the ideological one that there is no spec for the database. The implementation is the specification, so the only true way of building a reader of logs is to implement the journal. There are attempts to document the layout but if there is any difference then you can&#x27;t submit a bug to get the layout corrected but the implementation isn&#x27;t wrong.I can think of plenty of other applications with bigger issues than that. But I think can still want the defacto log for Linux to have an official spec. reply eptcyka 21 hours agorootparentprevjournalctl -f? reply abofh 20 hours agorootparentprevHave you tried reading them? They&#x27;re often helpful reply emporas 8 hours agoparentprevSo, statically link the new sudo with the old sudo, and when the user tries to use less commonly used features fallback to the older version in C.The binary executable will be bigger, but disk space is a lot cheaper nowadays, compared to 43 years back.I mean, is there any time where one more level of indirection failed to make everyone happy? reply dtx1 21 hours agoparentprevHaving so many different feeatures in one of the most basic unix tools is much more of a red flag. reply garblegarble 21 hours agorootparentFor sure! But that mistake has already been made, and has been in the wild for years, so removing those features (and proposing yourself as a replacement for the original) is now a breaking change reply mprovost 21 hours agorootparentOpenBSD replaced sudo with doas (with a vastly reduced feature set) several years ago, and without breaking everything. Sure there are use cases where you absolutely need some feature of sudo, but you can always install it. reply garblegarble 21 hours agorootparentThat seems like the Right Way to do it...As annoying as it is to have to update every sudo reference -> doas, it forces you to think about everywhere you&#x27;re using it, rather than waiting to see what breaks and then trying to fix it. reply codetrotter 19 hours agorootparent> As annoying as it is to have to update every sudo reference -> doas, it forces you to think about everywhere you&#x27;re using it, rather than waiting to see what breaks and then trying to fix it.In my scripts I never call sudo or doas. Instead, if the script needs to do something as root, I write the whole script so that it expects to itself be run as root.And then when I want to run my script, I run it as root doas .&#x2F;somescript.zsh reply PrimeMcFly 19 hours agorootparentThat&#x27;s a much worse approach from a security pov. reply codetrotter 19 hours agorootparentNo. That’s a blanket statement on your part that you cannot make because you don’t know what my scripts look like, or what commands they call. reply xyzzy_plugh 19 hours agorootparentNo, it&#x27;s never better to run whole scripts as root when root is only required for part of it. Unless every expression in your script requires root, the blanket statement holds.In my experience, and in my own scripts, it is better to explicitly check if you are being run as root, advise against it and exit (with maybe some break glass flags) and invoke sudo when escalated privileges are required. reply PrimeMcFly 19 hours agorootparentprevYes, it&#x27;s a blanket statement, better it&#x27;s an absolute statement because it&#x27;s absolutely true.You&#x27;re taking a shortcut due to convenience and it&#x27;s bad security practice.It&#x27;s that simple. reply josefx 19 hours agorootparentprevI just constantly run as root since there is always a chance that I might need root permissions for something. &#x2F;s replysamus 21 hours agorootparentprevOpenBSD is much more open (pun not intended!) about breaking parts of userspace to push through beneficial changes. After all, they control their own userspace and can fix up most things before they even become an issue. Linux is only the kernel. reply cpach 20 hours agorootparentIn that case, shouldn’t Linux distros be even more free to break things…? In theory they can bundle any userland tools they want.AFAIK sudo isn’t really tightly coupled to the kernel itself. reply samus 19 hours agorootparentThey could, but their users really won&#x27;t like that. They have their workflows that they got used to. In practice it&#x27;s gonna be GNU Coreutils and Glibc and the other usual suspects. If they bundle something more exotic, it better be for a very good reason. For example musl on Alpine or what Android does. reply xorcist 19 hours agorootparentprev> without breaking everythingExcept all exiting use of sudo ...It&#x27;s such an entrenched tool that I&#x27;m sure there a compatible replacement could be useful.Personally I would appreciate someone to take on the mess that is PAM. It was much too complex from the start and it hasn&#x27;t become better over the years. reply Fnoord 20 hours agorootparentprevOpenBSD uses BSD_Auth instead of PAM. So you cannot use your YubiKey with doas via PAM on the Linux ports. At least not in the same way, as they do not support caching it seems. reply PrimeMcFly 19 hours agorootparentprevOpenBSD is mainly used by hobbyists and not sysadmins, which is why there are not complaints about the missing functionality. reply kristjank 18 hours agorootparentOpenBSD is mainly used where other Unices can be used, and provides widely used software like OpenSSH, OpenBGPD and OpenSMTPD. To say that it&#x27;s a hobby project strikes me as very ignorant. That said, it is not very easy to convince the developers that a function is missing because it&#x27;s a pretty opinionated project, and they might not share the user&#x27;s definition of needed functionality. Thankfully, they&#x27;re nowhere near ebassi levels of functionality deletion disorder. reply PrimeMcFly 17 hours agorootparent> OpenBSD is mainly used where other Unices can be used,That&#x27;s a pretty general statement, and I&#x27;d say to that not really. It&#x27;s very much a hobbyist OS. A few people use it at home as firewalls, a few small businesses maybe, but it&#x27;s mostly hobbyists and developers.> To say that it&#x27;s a hobby project strikes me as very ignorant.I mean, I&#x27;ve been familiar with the project for over 20 years, so I don&#x27;t think I&#x27;m ignorant at all. The developers primarily make the OS for themselves and people with the same ideas and priorities.> That said, it is not very easy to convince the developers that a function is missing because it&#x27;s a pretty opinionated project, and they might not share the user&#x27;s definition of needed functionality.Right, the devs prioritize their own needs, and can do so because it&#x27;s a hobbyist OS. reply speed_spread 21 hours agorootparentprevPriorities. If a fundamental security tool&#x27;s design limits it&#x27;s trustworthiness, it greatly reduces it&#x27;s usefulness and \"breaking\" it&#x27;s interface is thus warranted. reply fermuch 21 hours agoparentprevIt seems like those changes are noted here: https:&#x2F;&#x2F;github.com&#x2F;memorysafety&#x2F;sudo-rs#differences-from-ori... reply garblegarble 21 hours agorootparentThe text seems to imply that&#x27;s not a list of features not implemented, it&#x27;s a list of features not implemented that don&#x27;t output a clear error reply quotemstr 20 hours agoparentprevThere&#x27;s a gulf between being 99% compatible with something and being a 100% drop-in. If the author of program B wants to replace A, he should go the extra mile and implement every feature of A so as to erase technical excuses for stasis. B needs to put aside his ego, swallow his pride, and implement all the features of A, even the ones her personally dislikes, because the effect of doing otherwise will be that B doesn&#x27;t replace A. We have to work backwards from out desired outcomes. reply marcus0x62 19 hours agorootparentPerhaps, but if the goal is security of a critical tool, losing some attack surface (features) if they aren’t widely used is a win. Other projects, like ntpsec[0] have taken this approach with good results. Although, I agree with another commenter in this thread[1] that this effort would have been better directed at something with an inherently small attack surface like doas.0 - https:&#x2F;&#x2F;www.ntpsec.org&#x2F;accomplishments.html1 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38161016#38162736 reply godshatter 18 hours agorootparentThe features they are leaving out were presumably added for a reason. If someone is on a system that is using sudo-rs as a drop-in replacement (not under their control) and they need to use one of those less widely-used features, how secure is the work-around they have to use instead? I&#x27;m hoping this factored in to their analysis.Sometimes reimplementing something and leaving out lesser-used features to \"reduce the attack surface\" can sound an awful lot like \"let them pound sand\". reply nucleardog 15 hours agorootparentDid you know you can set up an OpenVPN tunnel with no encryption or authentication?I&#x27;m sure for someone out there that&#x27;s a make-or-break feature, but for the vast majority trying to use OpenVPN it&#x27;s a massive, insecure footgun. (Hell, how many bugs have protocol negotiation led to in OpenVPN&#x2F;SSL&#x2F;etc?)Compare that to Wireguard that just says... it&#x27;s encrypted. Full stop. Carry on.A lot of this tooling and technology was developed in a different era with different priorities. Security, and especially network security, was not such a huge focus 30-40 years ago. Priorities have shifted. The operating environment is a lot more homogeneous (when&#x27;s the last time you dealt with a layer 2 protocol besides ethernet?), while the risk of poor security has grown immensely.It&#x27;s absolutely fair to critically evaluate these features and determine which can be removed to simplify and improve the products for the vast majority of users. If a small fraction of users stay on sudo, but the majority are able to move to a more secure option... that&#x27;s a win. This is exactly what Wireguard provides versus OpenVPN. reply marcus0x62 14 hours agorootparentprevSure, and those people are free to continue to use the C-based implementation of sudo, implement the features themselves and submit patches to the maintainers of the rust sudo implementation, etc. But, the idea that a feature, once implemented, is sacrosanct and can never be deprecated is insane, especially for a piece of critical security infrastructure. reply infamouscow 13 hours agorootparentHaving disdain for real-world business requirements is probably why Rust sees so little use in the real world. reply marcus0x62 11 hours agorootparentThat’s an interesting criticism. With commercial projects I’ve been involved in, feature deprecation was always based on a business case built on cost vs future revenue projections, not on a sense of duty to maintain ever single feature forever, regardless or whether or not anyone actually used it. replydb48x 19 hours agorootparentprevIt’s actually mostly just a matter of time and money. They implemented the most commonly–used features first, because those are the most important ones. The majority of people could probably swap sudo-rs in for their existing sudo implementation today with nothing lost. Others will need one of those “enterprise features” and so they should either wait or pitch in. reply fdsfsdfdsffgg 21 hours agoprev> Apache-2.0+MIT vs GPL-2.0So, you may get a memory-safe su&#x2F;sudo-rs, but those who distribute it in a binary form won&#x27;t be obliged to show you the source code it was built from (potentially including some modifications). reply alwayslikethis 21 hours agoparentI wanted to write this comment too, as I have a serious concern that the effort to displace GPL tools with Rust rewrites in MIT&#x2F;Apache will one day lead to a proprietary Linux, but the original sudo is not in GPL. It&#x27;s in an ISC&#x2F;MIT style license [1].1. https:&#x2F;&#x2F;www.sudo.ws&#x2F;about&#x2F;license&#x2F; reply pjmlp 20 hours agorootparentLook at Android, the only GPL thing left is exactly the Linux kernel, and it hasn&#x27;t been yet replaced by Zirkon because reasons.Look at the FOSS UNIX like RTOS, none of them is GPL. reply fdsfsdfdsffgg 21 hours agorootparentprevThere are su and runuser in util-linux (GPL-2.0) [1].[1]: https:&#x2F;&#x2F;github.com&#x2F;util-linux&#x2F;util-linux&#x2F;tree&#x2F;master&#x2F;login-u... reply wiz21c 21 hours agoparentprevMoving away from the GPL is a very bad idea for such critical component. Exactly for the reason you give.(this post is just here to insist on the issue) reply mhh__ 21 hours agorootparentIn general a lot of \"[GNU tool] but in rust\" projects do seem to come very close to violating the GPL, especially in spirit. reply tsimionescu 20 hours agorootparentIn what sense? It is completely in the spirit of the GPL to reimplement a GPL tool from scratch with the same behavior and a different license. After all, that&#x27;s how the free Unixes came about (though admittedly those were BSD licensed typically). reply zorked 20 hours agorootparentKinda? Historically there were indeed concerns about reimplementation and copyright. One of the ways that the GNU Project tried to fight claims was to reimplement the tools using dynamically-allocated memory (instead of Unix&#x27;s traditional fixed-size buffers) to make sure the implementation was sufficiently different. Other ways were making the implementation Posixly correct, adding internationalization or trying to pick different approaches (like using more modern algorithms for sorting). The GNU tools were better, not just direct ports, and that&#x27;s why it was common to install them in systems like Solaris or HP-UX. reply mustache_kimono 16 hours agorootparent> Kinda? Historically there were indeed concerns about reimplementation and copyright.Free software projects should welcome multiple implementations and interoperability, because these are the mother&#x27;s milk of free software. It&#x27;s frankly incoherent, given values of free software, that a reimplementation of, for example, Unix coreutils (GNU) would find fault with a reimplementation of itself (uutils).Notwithstanding how philosophically incoherent it is, a desire, now that Linux and free software have some market power, to be a bully back, to grasp for monopoly power, to play AT&T, is really distasteful. What&#x27;s exciting about free software is not the artifact, Linux or coreutils or sudo, but that anyone can create new and interesting alternatives. That users get to make choices about which implementation to use. The existence of FreeBSD does not make Linux worse. It makes it better! The \"solution\" to an MIT licensed coreutils is a GNU licensed fork which is 10x better. Instead, we get complaints which amount to a kind of free software entitlement, an endless pissing and moaning about how other people won&#x27;t do new things your way.This is a major problem in the way that most normies view the GNU and the GPL. In the past, I may not have chosen the GPL for my own projects, but I&#x27;d be pleased to contribute to a GPL project. Now, I&#x27;d have a hard contributing to a GPL project, because of just how toxic this attitude (no other license matters but ours) is. reply iudqnolq 19 hours agorootparentprevArguably writing in Rust will force a similar magnitude difference to those examples. For example, you&#x27;ll probably replace that modern sorting algorithm with a call to .sort reply oynqr 20 hours agorootparentprevPretty sure you&#x27;d have to clean room the whole thing. Which these Rust implementation might not have. This is not legal advice. reply mhh__ 18 hours agorootparentI can&#x27;t remember enough to name names but one obviously didn&#x27;t to the point that it mentioned what the GNU code does in comments. reply mustache_kimono 17 hours agorootparentI&#x27;m really not certain this is enough to be a copyright problem.For instance, GNU and POSIX both publish their specs for the coreutils. If a coder were to take a look at the actual GNU code (which BTW is published for everyone to see), copyright law has a well trodden distinction between the idea and the expression -- that is, ideas are not copyrightable. If the \"idea\" simply amounts to what would be a more a detailed specification, I&#x27;m not sure there is a problem, like ... GNU uses this kernel facility for X. The problem would be vast amounts of \"expression\", especially \"creative expression\", directly copied and reimplemented in Rust. If the code is meat and potatoes, not 10xer galaxy brain fare (\"I wrote a custom allocator which is suspiciously like the custom allocator implemented by GNU\"), there shouldn&#x27;t be an issue.Think about what copyright to a play, or a novel, or a screenplay is. Now imagine a comment in the text&#x2F;source: \"This is how Toni Morrison did her characterizations in Beloved\". This obviously isn&#x27;t a copyright violation, unless you&#x27;re copying the actual expression or a translation of the actual expression found in Beloved. replynindalf 19 hours agorootparentprevBut it wasn’t GPL to begin with - https:&#x2F;&#x2F;www.sudo.ws&#x2F;about&#x2F;license&#x2F; reply Nullabillity 21 hours agoparentprevSudo is ISC-ish[0] (very permissive), not GPL. Su does indeed seem to be GPL though.[0]: https:&#x2F;&#x2F;www.sudo.ws&#x2F;about&#x2F;license&#x2F; reply ksherlock 20 hours agorootparentThe GNU ~~coreutils~~ util-linux version of su would be GPL but su is from Unix V1 (1971) so there are AT&T implementations, BSD implementiations, etc. reply fdsfsdfdsffgg 21 hours agorootparentprevThere are su and runuser in util-linux (GPL-2.0) [1].[1]: https:&#x2F;&#x2F;github.com&#x2F;util-linux&#x2F;util-linux&#x2F;tree&#x2F;master&#x2F;login-u... reply zigzag312 21 hours agoparentprevIs that really bad? You are free to not use such distributions.Regarding security, malicious actor could show you a different source code from what he distributes in a binary form. GPL or no GPL. reply pbmonster 20 hours agorootparent> Regarding security, malicious actor could show you a different source code from what he distributes in a binary form.That&#x27;s why hashes are published by distributors and checked by package managers, right? reply zigzag312 19 hours agorootparentThat only checks that binaries are the same as what is published by distributors.It doesn&#x27;t help against a malicious distributor, unless package managers also do a deterministic build themselves and verify that checksum from self-build binary matches the checksum published by a distributor. reply pbmonster 1 hour agorootparentIf I&#x27;m not mistaken, many (most?) of the package managers of major Linux distributions actually build a package from source themselves in an automated process.Otherwise, getting a package to run on different architectures would be a ton of manual labor. At least Debian certainly has that automated.Besides the huge security risk involved in... just distributing random binaries? reply zigzag312 38 minutes agorootparentAnyone building from source has the ability to potentially including some modifications. Unless you are able to verify that checksum published (by whoever builds the code) \"matches\" the source code (by reproducing the build) you cannot be sure there aren&#x27;t any modifications. Published checksums are often meant only for verifying that binary you got matches the original binary. Not for verifying that it is build from specific source code.Many Debian packages already have reproducible builds, but not everything.From Debian wiki:> Reproducible builds of Debian as a whole is still not a reality, though individual reproducible builds of packages are possible and being done. So while we are making very good progress, it is a stretch to say that Debian is reproducible.https:&#x2F;&#x2F;wiki.debian.org&#x2F;ReproducibleBuildsBut the point is that this has nothing to do with the type of an open source license. GPL doesn&#x27;t guarantee you that a build is reproducible and MIT doesn&#x27;t prevent you from having reproducible builds. reply EspressoGPT 21 hours agorootparentprev> Is that really bad? You are free to not use such distributions.It&#x27;s not really bad but it kinda defeats the purpose. reply pjmlp 20 hours agoparentprevFor better or worse the use of GPL is going away, even the future of Linux kernel is not guaranteed.In the realm of IoT FOSS UNIX like operating systems, all the contendants are using a mix of Apache, MIT and BSD licenses, including the ZephyrOS sponsored by the Linux Foundation.When the GPL generation is gone from the face of the Earth, it won&#x27;t last long that UNIX-like OSes get another steward alternative to the Linux kernel, with a more appealing license to big corps. reply NegativeK 17 hours agorootparentHow would they be able to drop the GPL from the Linux kernel? reply pjmlp 16 hours agorootparentBy replacing it with something else, duh.Linux kernel isn&#x27;t the first nor the last UNIX clone. reply slacka 20 hours agoprevWould be interesting to see a a Debian derivative that combines this with the Rust Implementation Of GNU Coreutils.[1] Could be a big win for memory safety and performance.[1] https:&#x2F;&#x2F;github.com&#x2F;uutils&#x2F;coreutils reply guerrilla 19 hours agoparentI wonder how we are from a Rust UNIX userland. At least we wouldn&#x27;t have to implement a C compiler! reply yjftsjthsd-h 18 hours agorootparentA Linux distro is going to need to see compiler to self-host regardless of the user land. If you can live without Linux, there&#x27;s redox ( https:&#x2F;&#x2F;redox-os.org&#x2F; ) reply guerrilla 17 hours agorootparentWell, I guess if the build system uses that distro or if it&#x27;s a sourve distro. reply nonameiguess 17 hours agorootparentprevI looked into this a few years back when I was making my own toy Linux distro, and this is the list of packages provided by a typical GNU system that meet POSIX requirements for a userspace:* `bash`* `bc`* `binutils`* `bison`* `Coreutils`* `Diffutils`* `file`* `Findutils`* `flex`* `gawk`* `glibc`* `grep`* `tar`* `gzip`* `M4`* `make`* `man-db`* `man-pages`* `procps-ng`* `psmisc`* `sed`That&#x27;s a reasonable start, but you also need, minimally, something to replace `pciutils`, `IPRoute2`, a bootloader, and an init system. For a close to expected experience, add in `TexInfo`, `XZ`, `ZStd`, and `bzip2`, plus `shadow` if you don&#x27;t want passwords stored in plaintext.POSIX doesn&#x27;t dictate an editor, but you probably want something that can run in a terminal. Usually `cURL` and either `openssl` or `GnuTLS`, plus `bind-utils`, `ldns`, or something equivalent are there for actually using the network, something to replicate `gpg` functionality if you&#x27;re going to install signed packages, and of course the package manager itself. Cargo is fine for Rust app developers, but can&#x27;t replace an installer of system packages. You likely need an `ssh` implementation to replace `OpenSSH`.I&#x27;m sure there&#x27;s more I&#x27;m missing, but this is pretty close to what you&#x27;d get in a minimal server image.If you&#x27;re looking to fully get rid of C and not need a C compiler, though, Linux itself is a hurdle. You don&#x27;t necessarily need a kernel quite as fully-featured, but you need something that at least implements the POSIX system calls. Just about every Linux distro I&#x27;m aware of seems to also provide Python and Perl these days as a whole lot of system utilities and build scripts use them. Presumably, rewriting all of Perl and Python in Rust is not feasible, so you either need some other interpreted scripting language good for system scripting that is written in Rust, or somehow make your shell a superset of POSIX but also much closer to a real programming language.Don&#x27;t underestimate the lift of replacing `libc`, either. It&#x27;s not just the C standard library and interface to system calls. It also provides the linking loader that makes it possible to even run other programs, all of the locales and time zones, the system&#x27;s name server, profiler, memory dumper. A whole lot of stuff. reply mprovost 15 hours agorootparent> POSIX doesn&#x27;t dictate an editored is the standard text editor! reply fanf2 13 hours agorootparentvi is also a standard editorhttps:&#x2F;&#x2F;pubs.opengroup.org&#x2F;onlinepubs&#x2F;9699919799&#x2F;utilities&#x2F;e...https:&#x2F;&#x2F;pubs.opengroup.org&#x2F;onlinepubs&#x2F;9699919799&#x2F;utilities&#x2F;v... replymarkhahn 16 hours agoprevPolishing a turd?Seriously, better reimplementations are great. But weren&#x27;t you shocked to read about all those weird sudo features? I mean, the normal stuff is very weird, subtle, and therefore fragile.Anyone who uses sudo \"deeply\" should probably think about whether there are other ways. reply feldrim 19 hours agoprevI&#x27;d love to see a verification &#x2F; validation parameter&#x2F;flag&#x2F;tool that allows the user to dry-run the current sudo configuration and print out the parts unsupported by sudo-rs.Portability helpers for projects like these enable a frictionless change. reply datadeft 14 hours agoprevDoas is a much simpler tool for the same thing. I am not sure why it is not used more. reply brundolf 18 hours agoprevBy now we&#x27;ve gotten several projects that re-implement a core unix util in a safe language, often also with better performance due to concurrency, better standard primitives, etcHave any of these ever been adopted by a distro as the default implementation? Is that something that might happen? I.e. I would never bother to upgrade my sudo command or my grep command, but getting better defaults would be betterObviously they would have to be perfect drop-in equivalents, which some of these projects don&#x27;t try to be, but others of them do reply steveklabnik 18 hours agoparentIn 2021, uutils was far enough along and compatible enough to boot debian. I haven&#x27;t heard of anyone actually moving to them fully yet. reply kelnos 13 hours agoprev> Leaving out less commonly used features so as to reduce attack surfaceIn principle I think this is a great idea, but when it comes to encouraging adoption, I think this might be a mistake. I would love to use this, but I&#x27;m not going to install it myself and make sure it stays updated. Realistically, I&#x27;m only going to use it if Debian decides to replace their sudo-c package with sudo-rs. And I just don&#x27;t see Debian (known for being fairly conservative with changes and updates) doing that when sudo-rs doesn&#x27;t implement sudo-c&#x27;s full feature set. reply lambdaone 18 hours agoprevThis is good work, and I&#x27;m not quite sure why people are complaining about it; there clearly won&#x27;t be any replacement of the traditional C sudo by this unless it&#x27;s driven by distros and the community making it happen.Multiple implementations make it much easier to do fuzzing and generate automatic test suites that may be used to improve all the versions of this critical utility. reply thiht 17 hours agoparentAs they say in the article, their test suite was even able to uncover 2 bugs in the original sudo. That&#x27;s definitely a win. reply dang 13 hours agoprevRecent and related:Sudo-rs&#x27; first security audit - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38131442 - Nov 2023 (58 comments) reply declan_roberts 15 hours agoprevI don&#x27;t know why anyone is enamored with a rewrite of something as critical as sudo in a memory safe language, as if memory-safety somehow magically makes all of the other types of bugs disappear.No thanks. Keep this far away from all of my systems. reply lynx23 21 hours agoprevdoas is 43184 bytes, and does everything I need. That sudo exploit was a wakeup call. If you haven&#x27;t moved away from it yet, give opendoas (Debian package) a try. reply hannob 21 hours agoparentDo you use doas on Linux? It is not protected against tty pushback attack: https:&#x2F;&#x2F;github.com&#x2F;Duncaen&#x2F;OpenDoas&#x2F;issues&#x2F;106That&#x27;s a pretty severe unsolved security issue. reply zokier 18 hours agorootparentIt&#x27;s solved; TIOCSTI is disabled by default since Linux 6.2 https:&#x2F;&#x2F;git.kernel.org&#x2F;pub&#x2F;scm&#x2F;linux&#x2F;kernel&#x2F;git&#x2F;torvalds&#x2F;lin... reply hannob 15 hours agorootparentThe thing you linked says \"default y\" to \"Allow legacy TIOCSTI usage\". So yeah, you can disable it, no, it&#x27;s not the default.There&#x27;s also a related issue with TIOCLINUX and the paste functionality. (That will however be solved in an upcoming kernel version. I wrote the patch for it :-) reply csdvrx 12 hours agorootparent> There&#x27;s also a related issue with TIOCLINUX and the paste functionality. (That will however be solved in an upcoming kernel version. I wrote the patch for it :-)If anyone wants to apply that to their kernel, it&#x27;s on https:&#x2F;&#x2F;lore.kernel.org&#x2F;all&#x2F;2023101158-esteemed-condiment-1d... replyoh_sigh 12 hours agoprevWhy do we need a memory safe re-implementation? Is there no way to wrap a binary and have it blow up if any attempts at memory unsafe accesses are detected? reply badrabbit 21 hours agoprevIIRC, all the recent sudo vulns are logic errors, not memory safety. I mean, rewrite away but let&#x27;s not pretend that there couldn&#x27;t be some new bug introduced due to a misunderstanding of how something works or just a plain old mistake. reply latexr 19 hours agoparent> let&#x27;s not pretend that there couldn&#x27;t be some new bug introduced due to a misunderstanding of how something works or just a plain old mistake.Is anyone doing that? I see a lot of claims of memory safety, but as far as I can see the project isn’t saying other types of bugs are for sure eliminated. reply badrabbit 14 hours agorootparentThat&#x27;s fair and i support that but it does not address historical bug patterns that may be a design issue. reply alpaca128 20 hours agoparentprevIn the same way a new memory bug could be introduced to the original sudo. Shrinking the attack surface with static checks seems like a better deal in the long run. reply fanf2 13 hours agoparentprev2021: https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2021-31562019: https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2019-18634 reply badrabbit 2 hours agorootparentFair point with the second one but for the first off by one is a logic error still? reply charcircuit 17 hours agoprevHaving sudo itself makes an OS less secure since malware can use it to easily get root. Sure a rust version may be more secure, but even better would be deleting it entirely. reply AnonC 16 hours agoparentHow else would you do things as root or superuser without exposing everything? If your sudo configuration is ALL = ALL (or is a variant of this), then sudo opens everything up for use&#x2F;abuse. But if you carefully construct the configuration to allow only certain commands, it’s much better than just giving out the root password to users. reply charcircuit 14 hours agorootparent>How else would you do things as rootNothing a user is supposed to do should require elevating to root to do.>it’s much better than just giving out the root password to users.This is even worse. Access control should be set up properly instead of having users assume the identity of other accounts. reply gitaarik 3 hours agorootparentAnd so how do you then configure a server? Like modifying the files in &#x2F;etc&#x2F; reply charcircuit 1 hour agorootparentThere are several different options1. Don&#x27;t. Just include the correct configuration as part of the OS.2. For a configuration file use a group for passing out write access to users on the system.3. Have a daemon which handles updating configuration files based off some central source of truth which pushes changes to a fleet of servers. Programs could directly communicate with this daemon to get their configuration. reply thebitstick 10 hours agoparentprevOn a server, absolutely. On a desktop, the user is usually the administrator anyways. UAC exists on Windows land and the Administrator account is disabled for a reason. reply charcircuit 8 hours agorootparentI disagree. Desktop users run a lot of untrusted software. Allowing software to gain Administrator privileges with a single click from the user is a mistake. It doesn&#x27;t follow the principle of least privilege where the user should only give apps permission for what they need to do and not more. reply jackmott 22 hours agoprev [–] I remember a couple of years ago a root exploit in Sudo that was the result of failing to check for a sentinel value, thinking “that is a bug that wouldn’t happen in Rust, even though it isn’t related to memory safety!”Rust enums are sum types, and imho are one of the few unambiguously good language feature ideas. I miss them any time I use a language where they are not built in. F# is another nice language where they are first class and where I first got familiar with them reply Karellen 20 hours agoparent [–] > Rust enums are sum types,I wouldn&#x27;t mind so much if they just called them \"sum types\" or \"tagged unions\", or even some other new name. Reusing the existing name \"enum\" from other languages, but differently from the way all those other languages have used it for 45 gorram years, is freaking maddening. reply cyber_kinetist 20 hours agorootparentSwift and Scala also uses the enum keyword to define sum types, and their history goes earlier than Rust, so now you have multiple languages to yell at! reply Karellen 13 hours agorootparentAre you sure about Scala? I&#x27;m not familiar with it, but looking at the documentation it seems that an enum is a set of values, not a set of types?https:&#x2F;&#x2F;docs.scala-lang.org&#x2F;scala3&#x2F;reference&#x2F;enums&#x2F;enums.htm... reply n_plus_1_acc 20 hours agorootparentprev [–] That&#x27;s inherited from OCaml I think.https:&#x2F;&#x2F;www.ocamlwiki.com&#x2F;wiki&#x2F;Enum reply debugnik 19 hours agorootparentI don&#x27;t think so, OCaml consistently calls them \"variant types\". I don&#x27;t know who wrote that page, but that wiki didn&#x27;t even exist before September and it isn&#x27;t endorsed by ocaml.org, so I suggest you don&#x27;t consider it authoritative. reply ode 19 hours agorootparentprev [–] Nobody in the OCaml community commonly refers to them as Enum&#x27;s today or any time recently (maybe they were at the time Rust was created though? IDK).They&#x27;re usually called &#x27;variants&#x27;. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Prossimo released the first stable version of sudo-rs, a security-improved renovation of the sudo utility in Linux, coded in Rust, a memory-safe language that helps to curtail potential security threats.",
      "The project, already integrated into the Wolfi Linux OS, benefits from a reduced feature set and a comprehensive test suite, which are aimed at enhancing security. It is planned to undergo an external security audit in September 2023.",
      "The project was accomplished through collaboration with Tweede Golf and Ferrous Systems, with advisory input from the original sudo utility creator Todd C. Miller. Financial backing was provided by the NLnet Foundation and Amazon Web Services among others."
    ],
    "commentSummary": [
      "Prossimo has announced the first stable release of sudo-rs, a more secure version of the widely used Linux utility sudo, developed with the memory-safe programming language Rust.",
      "Incorporated already into the Wolfi Linux operating system, the project seeks to counter potential security threats through the use of memory-safe languages; developers Twoede Golf and Ferrous Systems have contributed significantly.",
      "An external security audit is scheduled for September 2023, and the project received funding from various sources including NLnet Foundation and Amazon Web Services. Original sudo utility creator, Todd C. Miller, offered critical guidance during sudo-rs' development."
    ],
    "points": 335,
    "commentCount": 235,
    "retryCount": 0,
    "time": 1699268304
  },
  {
    "id": 38160703,
    "title": "FFmpeg Enhances Video Transcoding Capabilities with Introduction of Multithreaded Pipelines",
    "originLink": "https://twitter.com/FFmpeg/status/1721275669336707152",
    "originBody": "The most powerful video transcoder in the world is getting better with multithreaded transcoding pipelines. Find out more in the patchset below:https://t.co/oiuy2GJ7wq— FFmpeg (@FFmpeg) November 5, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38160703",
    "commentBody": "FFmpeg is getting multithreaded transcoding pipelinesHacker NewspastloginFFmpeg is getting multithreaded transcoding pipelines (twitter.com/ffmpeg) 323 points by raybb 23 hours ago| hidepastfavorite82 comments not2b 13 hours agoProbably would have been better to link tohttps:&#x2F;&#x2F;ffmpeg.org&#x2F;pipermail&#x2F;ffmpeg-devel&#x2F;2023-November&#x2F;3165...instead of the tweet (or the xit, or whatever they are called now), as the substance in the tweet is the link. reply Sabinus 11 hours agoparentI wonder if the X should be pronounced in the Chinese manner, as in Xi Jinping. reply telotortium 11 hours agoparentprev> tweet (or the xit, or whatever they are called now)Just \"post\". They&#x27;re no longer limited to 280 characters now either, although longer posts are collapsed by default. reply raphaelj 22 hours agoprevAbout 2x faster on my 4-cores ARM server, without any significant parallelism overhead: $ time ffmpeg_threading&#x2F;ffmpeg -i input.mp4 -ar 1000 -vn -acodec flac -f flac -y &#x2F;dev&#x2F;null -hide_banner -loglevel quiet 14.90s user 2.08s system 218% cpu 7.771 total $ time ffmpeg -i input.mp4 -ar 1000 -vn -acodec flac -f flac -y &#x2F;dev&#x2F;null -hide_banner -loglevel quiet 14.05s user 1.80s system 114% cpu 13.841 total reply CrendKing 18 hours agoparentYou are not using hardware acceleration on the decoding side, and removing video output here. I wonder what happens if we use both hardware acceleration on video decoding and encoding, i.e. something like this on NVIDIA card ffmpeg -hwaccel cuda -i $inputFile -codec:a copy -codec:v hevc_nvenc $output reply TD-Linux 15 hours agorootparentNo video is being transcoded in the parent&#x27;s command (-vn). reply jamal-kumar 12 hours agorootparentprevWhat&#x27;s to note about hardware acceleration on the transcoding with NVENC is that it actually has a resolution limit, so if you&#x27;re trying to transcode something like 8K VR video using that it&#x27;ll choke reply m00x 12 hours agorootparentOn all hardware? Wouldn&#x27;t that be a buffer size limit? reply cm2187 21 hours agoparentprevBut what part gets multi threading? Because the video compression is already multithreaded. Video decompression I am not sure. And I think anything else is fairly small in comparison in term of performance cost. All improvements are welcome but I would expect the impact to be fairly immaterial in practice. reply raphaelj 21 hours agorootparentWell, that&#x27;s the very specific command I&#x27;m using in one of my webapps (https:&#x2F;&#x2F;datethis.app), and it&#x27;s one of the main performance hotspots, so it&#x27;s very *not* immaterial. reply j1elo 20 hours agorootparentVery interesting! I had seen the \"learn more\" video already, but it stayed in a corner of my mind.To compare any given piece of sound with reference sounds for ENF analysis, the references must have been recorded to start with.The fact that a webapp like yours can exist... does it mean that we, indeed, have recordings of electrical hum spanning years and years? Are they freely available, or are they commercial products?It seems so crazy to me that someone decided to put a recorder next to a humming line just to be able to later in the future match the sound with some other recordings... reply raphaelj 20 hours agorootparentFor Europe, there are academic and public organizations that publish these ENF backlog since about 2017.For US, I couldn&#x27;t find any open dataset. For these regions, I&#x27;m basically recording the sound of an A&#x2F;C motor to get the reference data, but I only have a few months of backlog.See here for the coverage of the webapp: https:&#x2F;&#x2F;datethis.app&#x2F;coverage reply garblegarble 19 hours agorootparentI notice in your coverage plot, the UK National Grid data appears to end mid-2023... have they stopped providing this data? reply raphaelj 19 hours agorootparentNo, but they do not provide the data in real time. replytimvdalen 21 hours agorootparentprevWow, I learned something today, did not know this was a thing! reply drewtato 21 hours agorootparentprevThis is removing the video stream (-vn) so that&#x27;s not involved. Not sure which parts are in parallel here, but I&#x27;m guessing decoding and encoding the audio. reply jamal-kumar 12 hours agorootparentprevFrom what I understand the more you increase multithreading with ffmpeg the more you get quality issues...If they have started to solve that problem then I will be a much happier camper reply izacus 20 hours agorootparentprevThreading depends on implementation of each encoder&#x2F;decoder - most video encoders and decoders are multithreaded, audio ones not so much. At least that was the state of the world the last time I&#x27;ve looked into ffmpeg internals. reply pjc50 20 hours agorootparentprevMultithreading the filter graph itself at the top level, so \"decode\", \"sample rate convert\", and \"encode\" can be in separate threads. reply jbk 23 hours agoprevIt&#x27;s difficult to understand what this is about without the presentation from Anton, at VideoLAN Dev Days 2023, that you can watch here: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Z4DS3jiZhfo&t=1221 reply gooseus 21 hours agoparentNice, great presentation! Curious what he has in mind for the \"dynamic pipelines\" and \"scripting (Lua?)\" he mentions in the \"Future directions\" section. I&#x27;m imagining something more powerful for animating properties? reply gbersac 22 hours agoparentprevI love this guy! He&#x27;s a very talented guy who devoted his life to open source. reply lofaszvanitt 22 hours agorootparentHope he doesn&#x27;t have to beg for donations. reply aidenn0 17 hours agoparentprevOh, I&#x27;ve run into so many issues related to the \"Extras\" listed on the slide at ~33m into that video. reply dylan604 17 hours agoparentprevMan, i really want to watch this presentation, but the piss poor audio just causes my brain to have a fit. How in today&#x27;s time is this still possible to screw up so badly? reply wildekek 15 hours agorootparentAgreed. Maybe some are not as sensitive to this, but it is a major energy suck for me. A little post-processing on noise and compression would come a long way. This recording is as raw as the Ramsey meme. reply jbk 15 hours agorootparentprevWe&#x27;re very happy for your volunteering to record our next developer conference, tickets are free! reply dylan604 15 hours agorootparentif you weren&#x27;t on a different continent separated by a very large body of water, I&#x27;d be there. I&#x27;ll donate by suggesting the use of a lav mic vs a podium mic.It is very difficult for some people to be able to understand clearly voices that are muddled from off axis audio recording. It&#x27;s a real condition. I have hard time hearing voices in a crowded room from people across the table from me. We spend time worrying about the aria tags in our mark up, but we just assume that everyone has the same hearing abilities? I get that most people probably don&#x27;t think about this when they don&#x27;t have a hearing condition, but to be dismissive about it is an entirely different level of egregiousness.Could my initial criticism have been provided with an entirely different tact, absolutely. But after the mental exhaustion that video was, that was all the energy I could afford at the time. reply jugad 16 hours agorootparentprevWhat are you talking about... the audio might not professional studio level 10&#x2F;10, but I don&#x27;t see anything significantly wrong with it - given that its more like a standard presentation mic. Its clearly good enough. reply dylan604 15 hours agorootparentEvery time he turns away from the mic and continues talking while looking at the projection his volume goes way down and at best sounds like a mumble. It is very taxing to keep up with him when he&#x27;s turned away. It&#x27;s the wrong mic for the task. reply bobsmooth 22 hours agoparentprevWhat is difficult to understand? Does multithreaded mean something different in the realm of video transcoding? reply t43562 22 hours agorootparentHis work is not primarily about multithreading but about cleaning up ffmpeg to be true to its own architecture so that normal human beings have a chance of being able to maintain it. Things like making data flow one way in a pipeline, separating public and private state and having clearly defined interfaces.Things had got so bad that every change was super difficult to make.Multithreading comes out as a natural benefit of the cleanup. reply pjc50 22 hours agorootparent^ This: I just spent the ~20 minutes necessary to watch that part of the talk at a reasonable 1.5x speed, and that&#x27;s the summary. Ffmpeg was suffering from 20 years of incremental change and lack of cleanup&#x2F;refactoring, and that&#x27;s what he&#x27;s spent two years doing.A couple of great lines including \"my test for deprecating an option is if it&#x27;s been broken for years and nobody is complaining, then definitely nobody is using it\". reply smcnally 4 hours agorootparentAnd “the most-popular encoder on at least two planets” as Mars Rover Perseverance runs it.https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;space&#x2F;comments&#x2F;lpz3l7&#x2F;the_mars_2020... reply andrewstuart 22 hours agorootparentprevI often look for obscure options to get specialists tasks done in all sorts of software.Better to fix the option.Just because I didn’t go to the very significant effort of complaining doesn’t mean I didn’t want and burn hours trying to use that option. reply pjc50 21 hours agorootparentThe verdict of the presentation was that many options are (bad) duplicates of the filter graph, and you should configure the software through the filter graph.We saw in openssl what the consequences of never removing any code for decades were. It has a real cost.Always a case-by-case decision, though. reply Zagitta 21 hours agorootparentprevYou can&#x27;t expect open source developers to be omniscient and know you want to use a specific feature if you don&#x27;t communicate that to them. Would you rather have them add telemetry? reply johnmaguire 15 hours agorootparentprevFixing the option takes time and expertise, as well as an idea of what users actually are trying to do.If you don&#x27;t give feedback to developers, how do you expect them to know you wanted to use the option?Better to remove a broken feature than let users burn hours futilely trying to make it work. reply bootloop 22 hours agorootparentprev> \"multi-threading it&#x27;s not really all about just multi-threading - I will say more about that later - but that is the marketable term\"That&#x27;s whats said in the video at least in the first 10 seconds so it might be that multi-threading is just a too trivial term for the work here. (But haven&#x27;t watched the video yet so just an observation.) reply rolandog 11 hours agorootparentIf I recall correctly, he meant that multi-threading wasn&#x27;t a direct goal, but a \"side effect\" of rewriting some parts of the codebase to match the expected conceptual flow, and paying off some re-write debt that had been accumulated after years of new features being \"tacked on\". reply defrost 22 hours agorootparentprevThere are many different types of pipelined processing tasks with many differing kinds of threading approaches, and I guess the video clears up what kinds of approaches work best with transcoding .. reply pjc50 22 hours agoprevTweet just links to http:&#x2F;&#x2F;ffmpeg.org&#x2F;pipermail&#x2F;ffmpeg-devel&#x2F;2023-November&#x2F;31655..... which in turn references code at https:&#x2F;&#x2F;git.khirnov.net&#x2F;libav.git&#x2F;log&#x2F;?h=ffmpeg_threading reply kierank 22 hours agoparentThe tweet links to the mailing list which is the official source of the patchset (you can choose \"Next in Thread\" to continue). reply m3kw9 18 hours agoprevIs it automatic or does one need to do command line parameter gymnastics? reply jokoon 16 hours agoprevAre there any video editing software that take advantage of ffmpeg? I once thought about making something to draw geometry through SVG and use ffmpeg then, or maybe add some UI or whatever, or just to add text, but I never started.Avidemux feels like it&#x27;s a bit that.Since ffmpeg internals are quite raw and not written to be accessed through a GUI, any video editor based on it would probably be quite clunky and weird and hard to maintain.Maybe an editor that use modules that just build some kind of preview with an command explainer, or some pipeline viewer.ffmpeg is quite powerful, but it&#x27;s a bit stuck because it only works with a command line, which is fine, but I guess it somehow prevents it from being used by some people.I&#x27;ve already written a python script to take a random amount of clips, and build a mosaic with the xstack filter. It was not easy. reply brucethemoose2 16 hours agoparentVapourSynth is intended to be a middle ground you might be seeking. You manipulate the video in Python instead of ffmpeg&#x27;s CLI, but its often more extensible and powerful than pure ffmpeg due to the extensions:https:&#x2F;&#x2F;vsdb.top&#x2F;https:&#x2F;&#x2F;www.vapoursynth.com&#x2F;I have seen some niche software built on ffmpeg like losslesscut:https:&#x2F;&#x2F;github.com&#x2F;mifi&#x2F;lossless-cutStaxrip is also big:https:&#x2F;&#x2F;github.com&#x2F;staxrip&#x2F;staxripBut I don&#x27;t know anything \"comprehensive.\" reply jy14898 10 hours agoparentprevhttps:&#x2F;&#x2F;shotcut.org&#x2F; also uses ffmpeg reply up_o 6 hours agoparentprevKDEnlive. Fantastic foss video editing software. reply kjuulh 22 hours agoprevVery nice.Hopefully this will make my small transcoding needs faster for plex (as I don&#x27;t have hardware transcoding support on my graphics card) =D reply Alifatisk 22 hours agoprevI’ve been looking for more ways to speedup the transcoding process, one solution I found was using gpu acceleration, another was using more threads but its hard to find the optimal amount I should provide. reply marcyb5st 22 hours agoparentCan&#x27;t you just use Hyperparameter Optimization to find the best value? Tools like Sherpa or Scikit-optimize can be used to explore a search space of n-threads&#x2F;types of input&#x2F;CPU type (which might be fixed on your machine). reply corndoge 20 hours agorootparentI don&#x27;t think \"just\" is appropriate here, that makes it sound like this should be a trivial task for anyone while it is not. Using \"just\" like this minimizes work and makes people feel stupid which leads to various negative outcomes.Sorry for lecturing reply pjc50 22 hours agoparentprevFor most workloads, setting the number of threads to the number of vCPUs (i.e. count each hyperthreaded core as 2) works. But GPU acceleration is much better if it&#x27;s available to you. reply angrais 16 hours agorootparentGPU acceleration may produce worse quality and slightly larger files. So there&#x27;s a trade-off to be had. reply cm2187 21 hours agorootparentprevThough in my tests I found that gpu acceleration of video decoding actually hurts performance. It seems software decoding is faster than hardware for some codecs. Of course not the case for encoding. reply themoonisachees 19 hours agorootparentThat heavily depends on the GPU being used and whether or not it has hardware support for your codec. Maybe your GPU is just old&#x2F;weak compared to your CPU? reply cm2187 17 hours agorootparentThat&#x27;s possible, but if you look at nvidia, the whole range uses the same hardware accelerator, so at most it is a difference in term of chip generation, not so much GPU model.I am not saying GPU hw decoding isn&#x27;t useful, it certainly is in term of power consumption, and the CPU might be better used for something else happening at the same time. But in term of raw throughput it&#x27;s not clear that a GPU beats a recent CPU. replygoeiedaggoeie 12 hours agoprevYou could already place demuxing on a different thread as well reply keepamovin 21 hours agoprevCan anyone clarify the licensing requirements of large scale ffmpeg deployments? In what cases are fees required? reply hutzlibu 21 hours agoparent\"FFmpeg is licensed under the GNU Lesser General Public License (LGPL) version 2.1 or later. However, FFmpeg incorporates several optional parts and optimizations that are covered by the GNU General Public License (GPL) version 2 or later. If those parts get used the GPL applies to all of FFmpeg. \"http:&#x2F;&#x2F;ffmpeg.org&#x2F;legal.htmlMeaning it is free, but if you use some modules, you might have problems mixing it with proprietary code. reply bsenftner 21 hours agoparentprevThe general use case for ffmpeg inside proprietary software is the version of ffmpeg used needs to be statically compiled and linked into the software&#x27;s executable, or it needs to be a separate executable called by the proprietary software. reply Daemon404 20 hours agorootparentYou have that backwards - it must be dynamically linked. Static linking without providing your source would violate the LGPL. reply keepamovin 19 hours agorootparentCan you drill down a bit more into this? I would consider static linking to be including unmodified ffmpeg with my application bundle and calling it from my code (either as a pre-built binary from ffmpeg official or compiled by us for whatever reason, and called either via a code interface or from a child process using a command line interface). Seems bsenftner&#x27;s comment roughly confirms this, tho their original comment does make the distinction between the two modes.What&#x27;s someone to do? reply ndriscoll 15 hours agorootparentStatic linking means combining compiled object files (e.g. your program and ffmpeg) into a single executable. Loading a .so or .dll file at runtime would be dynamic linking. Invoking through a child process is not linking at all.Basically you must allow the user to swap out the ffmpeg portion with their own version. So you can dynamically link with a .dll&#x2F;.so, which the user can replace, and you can invoke a CLI command, which the user can replace. Any modifications you make to the ffmpeg code itself must be provided. reply Daemon404 19 hours agorootparentprevIt is widely known and accepted that you need to dynamically link to satisfy the LGPL (you can static link if you are willing to provide your object files on request). There is a tl;dr here that isn&#x27;t bad: https:&#x2F;&#x2F;fossa.com&#x2F;blog&#x2F;open-source-software-licenses-101-lgp...But, speciically the bit in the LGPL that matters, is secton 5: https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;old-licenses&#x2F;lgpl-2.1.en.html#S... - particularily paragraph 2.As always, IANAL, but I also have worked with a lot of FOSS via lawyers.Also, this is and always has been the view of upstream FFmpeg. (Source: I work on upstream FFmpeg.) reply wyldfire 8 hours agorootparentffmpeg&#x27;s own https:&#x2F;&#x2F;www.ffmpeg.org&#x2F;legal.html seems like a good reference IMO.To @keepamovin, \"called either via a code interface or from a child process using a command line interface\" -- regardless of the license terms, fork()&#x2F;exec()&#x27;d programs \"could never\" impose any licensing requirements on the parent because the resulting interaction among parent&#x2F;child is not a derived work. As usual: IANAL, this probably pertains more to USC than other jurisdictions. reply bsenftner 13 hours agorootparentprevIf one statically links ffmpeg into a larger proprietary application, the only source files one needs to supply are your ffmpeg sources, modified or not. The rest of the application&#x27;s source does not have to be released. In my (now ex) employer&#x27;s case, only the low level av_read_frame() function was modified. The entire ffmpeg version used, plus a notice about that being the only modification, is in the software as well as the employer&#x27;s web site in multiple places. They&#x27;re a US DOD contractor, so their legal team is pretty serious. reply spider-mario 12 hours agorootparentSection 6:“Also, you must do one of these things:a) […] if the work is an executable linked with the Library, [accompany the work] with the complete machine-readable ‘work that uses the Library’, as object code and&#x2F;or source code, so that the user can modify the Library and then relink to produce a modified executable containing the modified Library. […]b) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (1) uses at run time a copy of the library already present on the user&#x27;s computer system, rather than copying library functions into the executable, and (2) will operate properly with a modified version of the library, if the user installs one, as long as the modified version is interface-compatible with the version that the work was made with.[…]” reply ta1243 19 hours agorootparentprev> What&#x27;s someone to do?Release your code as GPL reply keepamovin 21 hours agorootparentprevThanks. We&#x27;re doing the second but I heard that at a certain scale you might need to pay fees anyway? reply bsenftner 20 hours agorootparentI used to work at an FR video security company, where our product was in a significant percentage of the world&#x27;s airports and high traffic hubs. Statically linked ffmpeg for the win. reply sharkski 17 hours agoprevAwesome to see improvements to FFmpeg! I&#x27;m hoping to see Dolby AC4 support soon. reply amelius 20 hours agoprevIsn&#x27;t video transcoding easily parallelizable? I mean just split the video into N equal parts (at keyframes) and divide the work. reply xuhu 19 hours agoparentNot a video encoding expert, but for live streams you can&#x27;t merge the output until you process all the N parts, so you introduce delays. And if any part of the input pipeline, like an overlay containing a logo or text, is generated dynamically i.e. not a static mp4, it basically counts as a live stream. reply bambax 18 hours agorootparentWhy not cut the image in rectangles and process those simultaneously? Wouldn&#x27;t that work for live streams? (There may be artefacts at the seams though?) reply slimscsi 17 hours agorootparentYes, and we do, but that is not the slow part: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Motion_compensationAnd as for seams: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Deblocking_filter reply bambax 15 hours agorootparentThanks! Very informative! It&#x27;s really a fascinating topic. reply amelius 18 hours agorootparentprevYes, good point about live streams. reply MaxikCZ 18 hours agoparentprevYou are right, but first you ideally should perform scene change detection (to put keyframes at propper positions), and that alone takes quite some processing. reply angrais 16 hours agorootparentExactly as some encoders (H264) have keyframes at intervals (e.g., every 30 frames) rather than where the action occurs.As such, they are suboptimal by default if a lot of motion occurs. reply dylan604 15 hours agorootparent> some encoders (H264)What H.264 encoder are you using that does not have a scene change detection option? reply 38 15 hours agoprev [–] non crap link:http:&#x2F;&#x2F;farside.link&#x2F;twitter.com&#x2F;FFmpeg&#x2F;status&#x2F;17212756693367... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FFmpeg, a highly potent video transcoder globally, is set to enhance its capabilities with the introduction of multithreaded transcoding pipelines.",
      "This advancement was revealed as part of a new patchset.",
      "Multithreaded transcoding pipelines will enable concurrent task handling, improving the transcoder's efficiency and speed."
    ],
    "commentSummary": [
      "FFmpeg, described as the world's most potent video transcoder, is set to become even more effective with an update introducing multithreaded transcoding pipelines.",
      "This improvement is outlined in a recently released patchset.",
      "Multithreaded transcoding pipelines imply the capability to simultaneously process different parts of a task, potentially improving speed and efficiency of video transcoding."
    ],
    "points": 323,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1699265554
  },
  {
    "id": 38167363,
    "title": "Developer Creates Terminal Autocomplete Feature for Windows and Linux: Seeks User Feedback",
    "originLink": "https://github.com/microsoft/inshellisense",
    "originBody": "I built this terminal native runtime for Fig&#x27;s autocomplete to support Windows and Linux. Would appreciate any feedback on it!",
    "commentLink": "https://news.ycombinator.com/item?id=38167363",
    "commentBody": "Inshellisense – IDE style shell autocompleteHacker NewspastloginInshellisense – IDE style shell autocomplete (github.com/microsoft) 313 points by cpendery 14 hours ago| hidepastfavorite128 comments I built this terminal native runtime for Fig&#x27;s autocomplete to support Windows and Linux. Would appreciate any feedback on it! dolmen 10 hours agoThe implementation has obviously never been run on a Unix&#x2F;Linux:* shell config is created with CRLF https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;inshellisense&#x2F;issues&#x2F;8* changing directory doesn&#x27;t work https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;inshellisense&#x2F;issues&#x2F;5 reply suavesito 9 hours agoparentIt doesn&#x27;t work on Windows either... at least by a comment in the issue. I don&#x27;t have Windows. reply theusus 5 hours agoprevNot up to the mark with https:&#x2F;&#x2F;github.com&#x2F;marlonrichert&#x2F;zsh-autocomplete which works contextually. reply monkellipse 14 hours agoprevGreat idea! Also terrifying :) Given how often I accidentally commit the wrong text in vscode, I shudder to think of the damage I could do with this on my shell, hah! What safety measures are there&#x2F;could there be? reply graypegg 12 hours agoparentI don&#x27;t know how painful it would be, but I think I could adapt to a short delay of maybe 50-100ms where the shell won&#x27;t respond to Return presses, only after accepting a completion. Just long enough ideally to make me re-read what I entered. reply biellls 14 hours agoparentprevA simple approach might be that if the resulting exit code is not 0 it won&#x27;t be used to complete in the future. reply Eavolution 11 hours agorootparentThat&#x27;d have the problem of tools like grep returning nothing, having an exit code of 1, but being a perfectly valid thing to suggest in the future. reply starttoaster 10 hours agorootparentprevWhen you&#x27;re writing, building, and executing code, I would not expect a 0 exit code every time. reply KRAKRISMOTT 7 hours agorootparentprevThe fish approach reply owlstuffing 13 hours agoparentprevGenerally, a destructive shell command prompts for approval. But I suppose overfumbly fingers could still cause grief.Type&#x2F;command aware tooling is a night and day difference. reply mksybr 13 hours agorootparentGenerally? Do you have examples in mind? The ones I think of are destroy first and ask questions only if told to do so: mv cp rm redirection tee reply owlstuffing 12 hours agorootparentYeah, you’re right. I was thinking of overly destructive commands eg. reformat etc. reply tombert 13 hours agorootparentprevI feel like I&#x27;ve trashed millions of files I shouldn&#x27;t have with `rm` and trying to be clever with regular expressions. reply teddyh 13 hours agorootparentNote: Normal shells use glob(7) expressions, not regular expressions. reply tombert 13 hours agorootparentYou know I googled this immediately after I posted it, and you&#x27;re absolutely right, but a good chunk of the syntax still kind of looks like regular expressions so I don&#x27;t think I was too far off! reply teddyh 12 hours agorootparentRegular expressions and glob(7) expressions look superficially similar, but it is mostly an illusion; they both use * and ? as common metacharacters, but both of those characters mean different things in the two systems. Only the [ and ] characters are used more similarly, but even those have their differences. The two syntaxes only have one common function which functions identically in the two systems, but it is invoked by different characters; specifically the ? (question mark) character in glob(7) exactly corresponds to the . (full stop) character in regular expressions. reply Per_Bothner 10 hours agorootparentTo be pedantic: You&#x27;re conflating the syntax of regular exprssions, with the (computer science) concept. Glob patterns are a kind of very resticted regular pexressions. Ksh extended the glob syntax so it has the full power of regular expressions in the computer science sense - though without all the extensions of modern regular expressions. reply doubled112 13 hours agorootparentprevI&#x27;ve done this enough times I use `ls` first, then `rm` reply AnyTimeTraveler 1 hour agorootparentI set up hourly borg backups instead. That resulted in me aliasing rm=&#x27;rm -rf&#x27; without a worry in the world. So far, I made about ten recoveries and have never lost important data. reply reddit_clone 12 hours agorootparentprevAs much as I can, I do a &#x27;mv ... &#x2F;tmp&#x27; instead of &#x27;rm -rf&#x27;. It gets the files out of the way, and provides a way out if that&#x27;s not what I wanted.&#x27;&#x2F;tmp&#x27; will eventually be cleaned by the OS so no (prolonged) space worries. reply KMnO4 8 hours agorootparent&#x2F;tmp is a RAM disk on many systems so be careful with very large files reply floppydisc 10 hours agorootparentprevTo add to this, I&#x27;ve been a happy user of the trash-cli for forever. Can recommend. reply tombert 13 hours agorootparentprevYeah, I generally will use `find . -name \" The world would have been a much better place if CLIs themselves defined strict interfaces using standard data structures.Well it&#x27;s not too late just yet.How about if all CLI exes had an option, say --dump-cli-options, that wrote out a spec of the supported commands&#x2F;flags&#x2F;options?Something like e.g. Python&#x27;s argparse should be able to effortlessly spit out this info (and the same for other similar argument parsing libraries). reply runeks 3 hours agorootparentAnd we would also need an options that does CLI argument parsing only, and either returns an error related to parsing one or more options or \"ok\".This could be used to do \"form validation\" of CLI args as they&#x27;re typed in the shell. reply hiAndrewQuinn 4 hours agoprevAlternatively, if you simply wish to occasionally bring Copilot into your shell, you should know that Ctrl+X Ctrl+E (on bash) &#x2F; Alt+E (on fish) will open your current shell line up in $EDITOR, which you may set to Vim or Neovim.From there, :wq will drop the text back into your command line. If you have Copilot set up in either of those, then it will also work here.I know from working on https:&#x2F;&#x2F;github.com&#x2F;hiAndrewQuinn&#x2F;shell-bling-ubuntu that Neovim&#x27;s LazyVim setup now supports Copilot out of the box now. I never had much trouble setting up the Vim plugin either. YMMV. reply lloeki 3 hours agoparentIf using bash in vi mode instead of emacs mode: ESC v.zsh starts minimal, so one needs the following for such a feature to be operational: # enable autoload edit-command-line zle -N edit-command-line # bash-like emacs mode shortcut bindkey \"^X^E\" edit-command-line # bash-like vi mode shortcut bindkey -M vicmd v edit-command-line reply efitz 11 hours agoprevSo I have to take a shell dependency on Node.js?Hard pass reply creatonez 5 hours agoparentI don&#x27;t mind it, it&#x27;s just another runtime that works on Linux and BSD. I have my neovim infected with extensions made in Node.js, Python, Lua, my weechat is infected with extensions made in Python, Perl, Node.js, and my zsh is infected with extensions with components made in Rust, C, and Python. Just make sure to pay attention to shell start times and RAM usage. reply preciousoo 8 hours agoparentprevMaybe the users don’t care about their devices reply ilc 12 hours agoprevFor those who haven&#x27;t read the code:This reuses Fig.io&#x27;s internal completion engine to actually get the job done.Because of that, it forces the language choice of Typescript, and the heavy lift is done by Fig&#x27;s engine.I hope the OP continues with the work and improves it. I was looking forward to trying Fig on Linux. This sounds like a step towards that. reply mschrage 13 hours agoprevCo-founder of Fig here! Just want to say that I think this is awesome.It’s really cool to see alternative implementations of IDE-style autocomplete in the terminal. Nice work! reply trey-jones 13 hours agoparentI&#x27;m pretty sure this is just a wrapper for Fig. reply alexlur 10 hours agorootparentA wrapper for Fig that doesn’t require me to create an account in order to use my terminal. I’d say it’s a good thing. reply evilduck 12 hours agorootparentprevhttps:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;inshellisense&#x2F;blob&#x2F;main&#x2F;package... reply arjvik 7 hours agoparentprevFig autocomplete is really cool! With the Fig aquisition, what do you forsee happening to Fig&#x27;s autocomplete offering in the long and short term? My impression is that Amazon was interested more in the other parts of your tech, with the scripts and automation capabilities. reply nextaccountic 13 hours agoparentprevhttps:&#x2F;&#x2F;github.com&#x2F;withfig&#x2F;autocomplete is it this? reply shrimpx 11 hours agoparentprevBtw if you don&#x27;t mind me asking, how will Fig make money? Will it be integrations with stuff like AWS, GCP, etc.? Nice work on Fig! reply st0le 11 hours agorootparentIIRC They were acquired by Amazon. They have a paid offering for shared team environments. reply shrimpx 8 hours agorootparentOh cool, thanks! reply curiousgal 13 hours agoparentprevLooking at the main author&#x27;s gorhub profile, they forker Fig&#x27;s repo. reply 100k 14 hours agoprevThe name is terrific. reply exikyut 14 hours agoparentThe fact that it&#x27;s coming from Microsoft is even crazier, because obviously you know they had to go through clearance - and made it.I half wonder if the author checked the name first, then started coding...... reply keyle 10 hours agorootparentI picture the first slide of the deck breaking the name down \"in shell intellisense\" and people nodding at the back... reply rewgs 1 hour agoparentprevFirst good name to ever come out of Microsoft. I&#x27;m sure it just barely succeeded over Visual Windows Subsystem for IntelliSense Core. reply chrysoprace 10 hours agoprevIt&#x27;s pretty cool but still seems a little rough around the edges. My simple test case was to launch into the shell and see if it loaded any of my normal shell configs like highlighting or auto-suggestions (based on my history) but it seems to just be its own shell entirely. It also doesn&#x27;t seem to play nicely with `zsh` since I tried to do a `..` (which is a common `zsh` alias for \"cd ..\" which got a `zsh:1: permission denied: ..` and then I tried to run `cd ..` it did nothing.Keen to see how this pans out though! reply dbtc 10 hours agoparentstock macos zsh&#x27;s `..` just gave me &#x27;zsh: permission denied: ..&#x27;zsh 5.9 (x86_64-apple-darwin22.0) reply chrysoprace 7 hours agorootparentYep it&#x27;s not stock `zsh` alias, so that suggests to me then that Inteshellisense isn&#x27;t reading my shell config at all. reply boxed 14 hours agoprevI wish I had something like this without it replacing the shell prompt with `>`. reply all2 12 hours agoparentI&#x27;d guess you could source it and then override the PS1 env variable with whatever prompt formatting you wanted. reply xnx 13 hours agoprevStarting to type \"dir\": \"d[el &#x2F;q &#x2F;s .]\" (press any key to accept) reply cpendery 8 hours agoprevThanks for everyone creating issues. I&#x27;m working on addressing all the issues and they&#x27;ll be included in the next release. reply Night_Thastus 14 hours agoprevI&#x27;d be curious how this compares with Fish, which has autocomplete as well. It&#x27;s worked fantastic for me so far. Once you have it, it&#x27;s hard to go back! reply boxed 14 hours agoparentThis looks much cooler than fish autocompletes imo. But this replaces the nice fish stuff with `>` as a prompt, so I&#x27;ll stick with fish. reply schmorptron 13 hours agorootparentThe readme for this project states that it supports fish, does it replace the prompt? reply keyle 10 hours agoprevIs anyone else feeling off about typescript in the terminal emulator? I guess it&#x27;s not worse than TCL or Perl. reply jameslk 12 hours agoprevThis looks awesome! Is the next step something like a Copilot integration?There’s Copilot CLI coming soon[0] but I think something that autocompletes would feel more natural most of the time.0. https:&#x2F;&#x2F;githubnext.com&#x2F;projects&#x2F;copilot-cli reply ssijak 3 hours agoprevI use Fig which also has this and it seems to be better at it reply jss326 13 hours agoprevI get this error on zsh: Unsupported shell: &#x27;&#x27;, supported shells: bash, powershell, pwsh, zsh, fish reply dclowd9901 12 hours agoparentYou need to run it with the —shell flag and provide “zsh” as the value reply jss326 12 hours agorootparentThanks, that worked reply filterfiber 14 hours agoprevIs this fully local? I glanced around and didn&#x27;t see any mention of chatgpt&#x2F;gpt&#x2F;codex so I&#x27;m thinking it is? reply trey-jones 13 hours agoparentThere is really not much code to speak of, and a quick perusal didn&#x27;t yield anything sus. Initial guess is that it&#x27;s not very efficient, but I really didn&#x27;t look that closely. reply dclowd9901 12 hours agorootparentAutocomplete is fast but I&#x2F;O is really crummy. reply HomeDeLaPot 14 hours agoprevAwesome! And it supports fish! Just the other day, I was wishing we had something like this. reply pbbakkum 11 hours agoprevIf you&#x27;re interested in GPT-powered shell autocomplete, check out https:&#x2F;&#x2F;butterfi.shThis also enables shell-aware LLM prompting! reply orliesaurus 11 hours agoprevi use warp&#x27;s terminal and i love it, but I think im gonna love this more because its OSS reply guessmyname 14 hours agoprevI wish they had written inShellisense in a more efficient programming language than TypeScript.I recall disabling bash_completion.sh on my computer some time ago due to its negative impact on the startup speed of each iTerm2 session and the delay it introduced when using thekey for autocompletion.Before I disabled this feature, I consistently experienced delays of over 300ms between triggering autocomplete and receiving the actual results. I must admit that this was on an Intel Core i7, so I assume the performance is much better with newer processors. However, even after more than two years without bash_completion.sh, I have already committed many command line tool flags to memory so I would only consider using a tool written in a compiled programming language that can provide autocomplete in 100ms or less, potentially requiring the inclusion of hardcoded information in the binary. reply dlivingston 14 hours agoparentThis was the first thing I noticed, too. Why TypeScript? Is it: a) efficient enough, esp. compared to a Bash&#x2F;Zsh&#x2F;PWSH alternative, that spawning a JS interpreter for each autocomplete is no biggie? Is b) TypeScript just much more efficient than I thought? Or c) is TypeScript Microsoft&#x27;s hammer, and everything looks like a nail? reply yoyohello13 13 hours agorootparentI tend to think since VSCode plugins are javascript all new tooling from Microsoft seems to be written in typescript. As a non front end dev though `npm install` is an instant turnoff for me. reply dlivingston 13 hours agorootparentI agree. I was going to install and backed out when I saw `npm install` as well. I am wondering if my priors need to be updated, though (namely: JavaScript is slow, inefficient, and unsuited for anything outside of webdev). reply yoyohello13 13 hours agorootparentI think javascript is plenty fast these days. My only problem with Typescript&#x2F;Javascipt is the toolchain is very complex&#x2F;confusing to someone on the outside. In my experience, Rust&#x2F;Go or even python is easier to get into if you&#x27;re not living it every day. Besides familiarity, I&#x27;m not sure why someone would choose Typescript for non-web work. reply carlhjerpe 8 hours agorootparentThe TypeScript typesystem is pretty great. And there&#x27;s always JS to escape to if the typesystem fails you. reply dclowd9901 12 hours agorootparentprevIt really doesn’t make any sense here. What are we making type safe exactly? reply evilduck 12 hours agorootparentIt doesn&#x27;t make any sense to even complain about Typescript in the first place here. Typescript itself is not being ran when this command is invoked, the npm package for this doesn&#x27;t even distribute .ts files or a type definitions file.This is latching on the to the word \"Typescript\" and immediately beginning the whining. reply dclowd9901 6 hours agorootparentI mean, right. But back to what I was saying: why typescript? The fact you’re right doesn’t make my question absurd? In fact it kind of completely validates it. reply evilduck 12 hours agoparentprevAs a user of this project, Typescript is 100% irrelevant, it&#x27;s compiled to JS and is just a nodejs process and regular javascript at that point. You can go look at what&#x27;s installed via `npm list -g`, find the path and notice there&#x27;s not a single Typescript file in the build output.I wish those aboard the TS&#x2F;JS hate-train knew what they were even complaining about. reply Xeamek 10 hours agorootparent>JS and is just a nodejs process and regular javascript at that pointYes, and that&#x27;s what people are complaining about. People use the wrod typescript in this thread because, well, it&#x27;s what shows up on github, but the same argument would come up if it was in pure js.People have problem with the fact that you are running a nodejs for the terminal enhancement, not what language it uses reply explaininjs 14 hours agoparentprevDon’t bring TypeScript into this. It is very possible to write sub 100ms procedures in TS, but an inelegant algorithm will be slow in any language, eventually. reply guessmyname 13 hours agorootparent> It is very possible to write sub 100ms procedures in TS, […]I won’t dispute this statement since I currently lack the means to assess inshellisense. Would it be possible for you (or someone with a functional Node + NPM setup) to install inshellisense and share the actual performance figures? You could use a tool like hyperfine (https:&#x2F;&#x2F;github.com&#x2F;sharkdp&#x2F;hyperfine) for this purpose.As an attempt to test this myself, I used a Docker image (version 21.1.0-bookworm from https:&#x2F;&#x2F;hub.docker.com&#x2F;_&#x2F;node&#x2F;). The TypeScript tool installed without any issues, along with the binding, which simply adds the following line into ~&#x2F;.bashrc: [ -f ~&#x2F;.inshellisense&#x2F;key-bindings.bash ] && source ~&#x2F;.inshellisense&#x2F;key-bindings.bashHowever, when I initiated a new Bash session within the same Docker container to activate the updated Bash configuration, I encountered the following error: bash: &#x2F;root&#x2F;.inshellisense&#x2F;key-bindings.bash: line 1: syntax error near unexpected token `$&#x27;{\\r&#x27;&#x27; &#x27;ash: &#x2F;root&#x2F;.inshellisense&#x2F;key-bindings.bash: line 1: `__inshellisense__() {Due to this issue, I am unable to perform a performance test using hyperfine.The version of Bash available in this Docker image is 5.2.15(1)-release.I verified that the content of &#x2F;root&#x2F;.inshellisense&#x2F;key-bindings.bash is exactly the same as https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;inshellisense&#x2F;blob&#x2F;main&#x2F;shell&#x2F;k... reply quodlibetor 12 hours agorootparentI&#x27;m pretty sure that the scripts generated by inshellisense are CRLF, and the carriage returns aren&#x27;t recognized by unix shells.You should be able to fix it with: vi $HOME&#x2F;.inshellisense&#x2F;key-bindings.zsh -c \"set ff=unix\" -c \":wq\" reply cassianoleal 13 hours agorootparentprev100ms is a long time to wait for each completion suggestion. reply emorning3 13 hours agorootparentJakob Nielsen says that 0.1 second is about the limit for having the user feel that the system is reacting instantaneously, meaning that no special feedback is necessary except to display the result...https:&#x2F;&#x2F;www.nngroup.com&#x2F;articles&#x2F;response-times-3-important-... reply Xeamek 10 hours agorootparentIn gaming, delay of 100ms is huge enough for anyone besides the most casual players to notice.Obviously this are different use-cases, but wouldn&#x27;t be suprised if it was big enough for people to at least notice reply emorning3 5 hours agorootparent>>In gaming, delay of 100ms is huge enough for anyone besides the most casual players to notice.<<I agree. I used to play a drum beat game with my daughter and son-in-law, who are both professional musicians, and they could hit beats within a 10ms window, while I struggled to get under 100ms.But 100ms seems like a reasonable upper limit for autocomplete to me.PS: My son-in-law is a professional drummer and can&#x27;t play Rush YYZ on Rock Band (just had to include this tidbit cause it&#x27;ll piss him off) reply Rapzid 11 hours agorootparentprevDepends on what they are waiting for. If there were 100ms of keystroke to paint latency with typing you&#x27;d def notice it. reply nu11ptr 13 hours agorootparentprevI was going to say the same thing. In what world is 100ms fast on a computer? (for something not making a network round trip) reply explaininjs 13 hours agorootparentThe claim isn’t that 100ms is fast, it’s that the blame for the latency lies in the engineering, not the language. reply userbinator 5 hours agorootparentprevIn absolute terms, that&#x27;s a few hundred million CPU instructions. reply Xeamek 10 hours agorootparentprevMan, we are doing a CLI autocomplete for duck sake, and You guys want to spin ENTIRE FUCKIN BROWSER for it, and then argue \"well, it isn&#x27;t THAT slow, so what&#x27;s the problem\".I&#x27;m not some extreme purist that thinks everything should be made in C and then hand optimized in assembly, but Jesus, how wasteful can You get before the slightest amount of self awareness starts to kick in.And for what reason, to save a week of engineering time learning GOlang? reply evilduck 8 hours agorootparentOr, spend the 10 minutes learning that Node isn’t spinning up an entire browser.This thread is filled with a shitload of this knee jerk idiocy. If you want to promote Go, at least learn how to complain about factual things instead of writing some fiction and complaining about that. Node has legit issues you could complain about. FFS, whine better. reply carlhjerpe 8 hours agorootparentprevYou know, JavaScript runs without a browser... There are several JavaScript implementations, most of them aren&#x27;t bundled with a browser.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_ECMAScript_engines reply stn_za 4 hours agorootparentprevJavaScript doesn&#x27;t belong in the shell, thanks. reply tacker2000 11 hours agorootparentprevI disagree. Using the same algorithms, a language like Typescript or Javascript will inevitably be slower than ones that are “closer” to the CPU. reply k8svet 12 hours agoprevBeyond the not-using-typescript for CLI tooling difference, it would be interesting for a comparison against carapace-bin, another shell agnostic completer: https:&#x2F;&#x2F;github.com&#x2F;rsteube&#x2F;carapace-bin (written in Go, since this thread includes other discussion about the choice of typescript for Inshellisense).From a quick peek, carapace-bin supports more shells (including and powering the one I use, nushell). reply mongol 11 hours agoparentKnowing it is go I like it better already. Anything that does not require a runtime gets a head start in my book. reply kermatt 11 hours agoprevIDE style shell autocomplete for JavaScript or TypeScript programersBecause otherwise requiring `npm` as a package manager for those of us who don&#x27;t develop in either is silly.(Yes, I know npm also implies JavaScript as a runtime environment, still nope) reply bestest 13 hours agoprevok. can someone explain to me. I&#x27;ve been using ctrl+r for god knows how long and it solves all my shell completion needs.is inshellisense for average users like me or more advanced users? reply fostware 10 hours agoparentIf you&#x27;re more used to ctrl+r, you could try hiSHtory (https:&#x2F;&#x2F;github.com&#x2F;ddworken&#x2F;hishtory) reply serial_dev 13 hours agoparentprevI use zsh plugin, autosuggests, and ctrl r, too.Ctrl r is okay and it is a very convenient if you already typed the command.Inshellisense seems to help with knowing what subcommands, flags and file paths are available, and it even provides a small docs helper for the flags and commands.If you didn&#x27;t try these tools, I&#x27;d encourage you take a look, it helped me a lot, especially when I work with tools whose commands I don&#x27;t know by heart or used a while ago. reply Dudester230602 13 hours agoprev1980s... but like 2020s-style.Coming next: auto-complete for punch cards. reply dang 13 hours agoparentCould you please stop posting unsubstantive comments and flamebait? You&#x27;ve unfortunately been doing it repeatedly. It&#x27;s not what this site is for, and destroys what it is for.If you wouldn&#x27;t mind reviewing https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html and taking the intended spirit of the site more to heart, we&#x27;d be grateful. reply leshenka 13 hours agoparentprevcan you expand on this? reply askiiart 13 hours agorootparentI think they meant \"using the terminal (1980s)... but like 2020s-style (intellisense)\"Y&#x27;know, because it&#x27;s not like tons of people use the terminal on a daily basis in the modern era. &#x2F;s reply Dudester230602 13 hours agorootparentYeah, they claim because it&#x27;s better but the reality is that they simply cannot build good cross-platform UIs on a budget &#x2F; on time. reply Y_Y 14 hours agoprev [–] Ya, cool name, but worse privacy than gorilla, more awkward than llm.sh and why would I ever get the Microsoft version of something I can get elsewhere without the worry that I&#x27;m about to be devoured by a corporate anglerfish. reply smoldesu 14 hours agoparent [–] Did you check the page? It&#x27;s MIT-licensed and isn&#x27;t about GPT or text generation. It&#x27;s Intellisense, for the shell. reply hui-zheng 14 hours agorootparent [–] then what makes it better than withfig&#x2F;autocomplete? reply boxed 14 hours agorootparentI downloaded fig, then immediately deleted it after it requested way too much access to my github account to even start. So... that&#x27;s a big thing. reply all2 12 hours agorootparentAnd this will deter me from ~~ever~~ trying Fig. :&#x2F; reply strbean 13 hours agorootparentprevIt is withfig&#x2F;autocomplete. The only difference from their runtime is that this gives results directly in the console, instead of using a graphical overlay. I believe that means you could use this in an SSH session. reply Timon3 13 hours agorootparentprevIt&#x27;s available for Linux and Windows. reply wrs 13 hours agorootparentprev [–] That’s what it’s based on. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The developer has established a terminal native runtime for Fig's autocomplete feature which is now compatible with both Windows and Linux platforms.",
      "The creator is actively seeking feedback to evaluate and improve the system's performance and efficacy."
    ],
    "commentSummary": [
      "A developer has developed a terminal native runtime for the autocomplete feature of Fig, which is compatible with Windows and Linux platforms.",
      "The developer is actively seeking feedback on the functionality and efficacy of this newly developed system.",
      "This development marks Fig's expansion from its previously macOS-only support to now supporting multiple platforms."
    ],
    "points": 313,
    "commentCount": 128,
    "retryCount": 0,
    "time": 1699298132
  },
  {
    "id": 38164735,
    "title": "Developers Protest Against Apple's Feedback Assistant: Call for Improved Bug Reporting System",
    "originLink": "https://lapcatsoftware.com/articles/2023/11/2.html",
    "originBody": "Previous: This Feedback will no longer be monitored, and incoming messages will not be reviewed Articles index Jeff Johnson (My apps, PayPal.Me, Mastodon) Apple developer boycott of Feedback Assistant November 6 2023 I'm organizing a boycott of Apple's Feedback Assistant, starting immediately, and I encourage all Apple developers to join me. Here's how I propose that each of us can effectively participate in the boycott and let Apple know that we're boycotting Feedback Assistant: File a new Feedback about Feedback Assistant (in Developer Tools & Resources) that lists the issues below and states that you're boycotting Feedback Assistant until the issues are addressed. Don't file any other new Feedbacks unless and until Apple addresses the issues. If Apple requests a response to a previously filed Feedback, respond only by saying that you're boycotting Feedback Assistant, and refer to your Feedback number from step 1. Ideally, I think you should make your Feedback from step 1 as unique as possible. The point is to flood Apple with new Feedbacks about the boycott and force Apple to do some work to handle them, to take notice of the boycott, and to recognize that we're serious about it. Boycotting Feedback Assistant does not preclude talking about your bugs on social media, on your blogs, and on your podcasts. Nor does it preclude filing reports with Apple's other public bug reporting systems, such as those for WebKit and various open source projects on GitHub. Those other bug reporting systems are superior to Feedback Assistant in a number of ways. The primary goal of the boycott is to bring about changes specifically in Feedback Assistant, the most hostile bug reporter I've ever seen. After consulting with fellow developers, I've composed a list of issues with Feedback Assistant that Apple needs to address in order to end the boycott. I'll number the issues for ease of reference, but the order doesn't necessarily reflect their relative importance. Apple neglects or refuses to say whether or not they can reproduce reported bugs, even when we give them precise steps to reproduce and sample Xcode projects. This is crucial for us to determine whether Apple is taking our Feedbacks seriously or just lazily, bureaucratically stringing us along. Apple closes Feedbacks with the status \"Investigation complete - Unable to diagnose with current information\" without asking us for more information or even notifying us that the Feedback has been closed. Apple closes Feedbacks without the agreement of the person who filed the Feedback, and apparently it's now a \"feature\" of their bug reporting system that closed Feedbacks cannot be reopened, even by Apple employees. (It wasn't always this way, I believe.) When Apple mistakenly closes a Feedback for a bug that isn't fixed, Apple demands that we open a new Feedback for the same bug, instead of just opening a new one themselves and giving us the new Feedback number. Apple demands that developers \"verify\" Feedbacks with the latest betas despite the fact that Apple has not fixed the bugs, attempted to fix the bugs, or even attempted to reproduce the bugs with the steps given by us. This is a giant waste of our time. And Apple closes the Feedbacks if we don't \"verify\" them. Apple doesn't always notify us of changes to the status of the original Feedback when our Feedbacks are closed as duplicates. Apple constantly demands invasive sysdiagnose reports, often unnecessarily, and refuses to look at Feedbacks without them. Many developers work on their own personal devices, and sysdiagnoses are gross violations of our privacy, which Apple claims is a fundamental human right. Apple has avoided or abandoned creating smaller, more targeted and less intrusive methods of collecting information and diagnosing bugs. Feedbacks can no longer be filed from the web. Apple now requires that all Feedbacks be filed from the native Feedback Assistant app on macOS or iOS. This is a very recent setback: I've been filing Feedbacks via the web app for years, the last one on October 26. Note the passive-aggressive question \"Where would you like to start your feedback?\" and the \"recommendation\" to use the native app, as if there were a choice. We can't search Feedback Assistant for bugs. Apple employees can search the database, but I can see only the Feedbacks that I personally filed. Of course we acknowledge that some Feedbacks need to remain secret, especially for products that haven't yet been announced by Apple, but countless Feedbacks require no such protection, and an opt-in searchable bug database would help external developers immensely, improving the overall quality of the software on Apple's platforms, to the benefit of Apple, developers, and users alike. Below is a screenshot of one of my old reports that epitomizes the absurdity of Apple's bug reporting system. Apple claimed in their response that \"much has changed\", but to this day, nothing has really changed. I've seen no evidence that Apple sincerely appreciates our input. Apple's Feedback Assistant, formerly known as Radar, has remained unreasonably terrible for a very long time, much too long, so now we're demanding change. In defense of Apple, some people assert that Apple doesn't have the time to properly respond to Feedbacks. I don't find this argument convincing, because Apple's priorities, schedules, and staffing are determined by Apple itself, via the decisions of the company's leadership. Apple values its own time over the time of external developers and seems to have no guilt over wasting endless amounts of our time. We are not happy, though, to sacrifice ourselves for a corporation worth trillions of dollars. Needless to say, my net worth and income are microscopic in comparison. If Apple can decide that it doesn't have the time to respond to our Feedbacks, then we can decide that we don't have the time to file them; Apple's problems with lack of time are thereby solved. Frankly, as a longtime Apple user, I could do without the relentless annual OS updates, and many of us look back fondly to the era of Mac OS X Snow Leopard when the updates were around two years apart, leaving more time for bug fixes. This is not a boycott against individual Apple engineers, many of whom also want Feedback Assistant to be improved. Indeed, the improvement of Feedback Assistant would enhance rather than detract from the relationship between Apple engineers and external developers. This is a boycott against the bug reporting system, intended to force Apple leadership to recognize and respond to the persistent problems with the system. Although I call it a boycott, it could also be termed a labor strike. Apple utilizes developers for vast amounts of unpaid QA labor. Both Apple and developers know the crucial role that developers play in testing and refining Apple's software and products. Apple needs our bug reports, our labor, often hours or even days of labor for a single Feedback. Nonetheless, Apple acts as if it were entitled to our Feedbacks, treating developers kind of like indentured servants. No respect or basic human courtesy is afforded by Apple to developers in the bug reporting system. We've been indoctrinated into believing that it's simply our duty to file Feedbacks, for the sake of the platforms. However, Apple's platforms are not charity cases. To the contrary, they've made Apple the most profitable company in the world. We developers are not Apple employees, and our unpaid labor should not be taken for granted. Henceforth, it will not be taken for granted. In my view the boycott, or strike, has two goals. First, obviously, is to pressure Apple into improving Feedback Assistant by showing Apple that it needs us to file bug reports and would suffer without them. The second goal is to show ourselves that we don't actually need to file bug reports with Apple. My feeling is that Apple has a lot more to lose here than we do. After all, the majority of bugs that I file never get fixed anyway, and even the fixes usually come later rather than sooner, not in time to avoid the consequences of the bug. Do Apple bugs affect our apps? Yes, of course. But we typically have to ship workarounds for the bugs in our apps, because we can't count on Apple to fix our reported bugs in a timely manner. With a workaround for a bug in place, we no longer need Apple to fix the bug, so reporting the bug becomes more an act of charity than urgency. This situation is often misunderstood by the public and even by Apple employees. Feedback Assistant does not provide customer service to developers. We developers are the ones who are providing the service to Feedback Assistant, and now we're choosing to withhold our services until the system is improved. I hope that Apple chooses to address the problems with Feedback Assistant, but if Apple happens to choose otherwise, and improvements never arrive, then my intention is to boycott forever. Regardless of whether Apple responds positively, I will consider the boycott to be a success if many developers participate, and we show ourselves that Feedback Assistant is not essential to our work and our livelihood. Jeff Johnson (My apps, PayPal.Me, Mastodon) Articles index Previous: This Feedback will no longer be monitored, and incoming messages will not be reviewed",
    "commentLink": "https://news.ycombinator.com/item?id=38164735",
    "commentBody": "Apple developer boycott of Feedback AssistantHacker NewspastloginApple developer boycott of Feedback Assistant (lapcatsoftware.com) 302 points by frizlab 17 hours ago| hidepastfavorite120 comments speedyapoc 16 hours agoI estimate that around 10% of my Feedback Assistant reports ever receive a reply or acknowledgement.These are for reproducible bugs in iOS, for which I&#x27;ve provided an isolated sample project with a 100% reproduction rate. It takes time to create a thoughtful and detailed bug report and the lack of replies are beyond frustrating. I&#x27;m glad to see this post and couldn&#x27;t agree more. reply crimsontech 15 hours agoparentFully agree. I work in security and sent Apple a bypass for child restriction policies on iOS, they told me to send it to feedback.Testing, reproducing, writing it up all takes time and effort. I didn’t want anything from them other than for it to be fixed, I have kids with iPhones.It’s no different than other companies though, I sent a remote code execution to Cisco and they just replied that they already knew about it but the product was approaching end of life so they wouldn’t fix it.I’ve stumbled across so many vulnerabilities over the years and I tend to just ignore them unless I’m being paid to find them. It’s not worth the frustration. reply saagarjha 10 hours agorootparentTo be fair, remote code execution and \"I got around the child lock on this device\" are somewhat different in severity. reply ary 8 hours agorootparent> remote code execution and \"I got around the child lock on this device\" are somewhat different in severityYou’ll notice that the recipients of both reports responded similarly, with apathy. Perhaps that was the point of juxtaposing them? reply joenot443 14 hours agoparentprevRespectfully, why file them at all if that&#x27;s the case? The cynic in me can&#x27;t help but see that as free high-quality engineering labor for what&#x27;s already the world&#x27;s most valuable company. They don&#x27;t need more help :) reply TheTon 13 hours agorootparentI see a similar response rate as the poster you’re replying to, but I am going to keep filing bugs. If Apple fixes a bug I file, that’s not just to Apple’s benefit, it’s to my benefit and to the benefit of people who use my software as well. Even if they only fix 10% of what I file, it’s a better outcome than if I didn’t file any bugs at all.I also notice that response and fix rates have large variance across components &#x2F; teams within Apple. Some of them are quite responsive and others are just &#x2F;dev&#x2F;null. I do tend to focus my energy on those components where I’ve had success in the past. reply black_puppydog 13 hours agorootparentprevThis is also the stance I take on customer support of Apple products. It got hyped so much about \"just working\" that I take it as a bit of an insult that they want me to stand in for their CS team. I do make exceptions for close family. reply tinus_hn 9 hours agorootparentYou prefer fixing Windows computers? reply fxtentacle 1 hour agorootparentNo, you just train them to walk to the Apple shop instead of asking you. If you sell products at a premium with the promise of an excellent user experience, then you should also budget for the costs of actually providing that user experience. reply jbverschoor 14 hours agoparentprevSame here. TBH I’m starting to get fed up by a lot of Apple things lately reply crazygringo 15 hours agoprevBoycotts&#x2F;strikes* only work if most or nearly all people participate, because you&#x27;ll only participate if you&#x27;re pretty sure nearly everyone else is too. While if this blogpost merely gets 0.1% of developers to strike... Apple couldn&#x27;t care less.I think this developer strike is a great idea, but launching it as a single person&#x27;s call to action (Jeff Johnson&#x27;s) is not generally going to be effective in changing behavior.What an organizer needs to do is to first personally contact somewhere between 50-200 other major, known, respected developers and get them to co-sign a joint letter which then gets published, so everyone can see this is a serious strike by people who know what they&#x27;re doing -- not just one person&#x27;s wish. It needs to get covered in all the major tech publications, so both Apple and all developers see it as well.And then the letter needes to not lay out a list of all of the complaints, but rather the specific, verifiable actions Apple needs to take in order to end the strike, so this isn&#x27;t some open-ended thing. And they need to be realistic actions as well, not wishful thinking -- signs of progress with dates and milestones, not fix-all-the-things-immediately.If you&#x27;re going to do a strike, you need to actually organize it. Writing a blog post that merely \"encourage[s] all Apple developers to join me\" is not the same as organizing. And generally speaking, writing a blog post claiming that you&#x27;re striking isn&#x27;t going to get anybody else to magically organize it for you.* As the author indicates lower in the article, it&#x27;s more of a strike than a boycott since it&#x27;s stopping providing free labor rather than stopping purchases, so I&#x27;ll refer to it as a strike reply lapcat 14 hours agoparentI&#x27;m already fairly well-known in the Apple developer community and received a lot of positive feedback on social media before posting the announcement, but I do think you&#x27;re overestimating the amount of effort that I and other developers are willing to put into getting Feedback Assistant improved. This is not a fight for higher income or something of that obvious importance. The beauty of the strike is that steps 1-3 at the beginning are low effort from everyone, which allows everyone to easily join if they choose. As I said at the end of the article, one of the goals of the strike is to prove to ourselves that we don&#x27;t really need Feedback Assistant, we&#x27;ve got little to lose, it&#x27;s not essential to us that we participate in the bug reporting system, and we can just walk away. The strike is not a win-at-all-costs, fight-to-the-death scenario. Feedback Assistant, no matter how unrelentingly terrible and annoying, is or at least should be rather low on our list of professional priorities.I think this attitude gives us leverage over Apple. We&#x27;re only volunteers walking away from a crappy volunteering \"opportunity\", whereas Apple relies on our free labor for its commercial products and would have to actually hire additional staff and pay them in order to replace our QA work. reply rendaw 5 hours agorootparentI think Apple will individually reply to the feedbacks with a PR message like \"we take your concerns seriously, we have a plan to improve our feedback interaction.\" Because the replies are individual and posted on a private channel, people who joined the boycott will not have any wider visibility, and think \"Oh maybe it&#x27;s okay now\" and go back to using it.It might be worth establishing an official public dialog forum and discouraging people from individually communicating until a resolution is confirmed there in public. Right now it&#x27;s on individuals to confirm that Apple improved things, which can only reasonbly be done by submitting more feedback (i.e. breaking the boycott).I&#x27;ve personally stopped providing feedback via forms on cloud providers&#x27; websites due to similar unacknowledgement and general disinterest in improving developer experience, but haven&#x27;t tried to organize anything about it. reply saagarjha 10 hours agorootparentprevMight be worth having a public list of people who&#x27;ve \"signed\" your boycott, though. reply lapcat 10 hours agorootparentI&#x27;ve considered that, but I have some concerns about the idea.I&#x27;m worried about personally getting overwhelmed by emails and messages on social media. The more successful the boycott, the more I become swamped. :-)Also, I think many developers have a fear of reprisal, whether that&#x27;s empirically justified or not. My understanding is that there&#x27;s now actually some degree of anonymity in Feedback Assistant, after changes were made in recent years for GDPR compliance? In any case, putting your name in public is a bigger step than simply boycotting, and I&#x27;m aiming for maximum participation. I&#x27;ve put my own name on this, and some cranks have even speculated that I might get kicked out of the developer program for it, which I doubt, because it would make me a martyr and kill my popular software — not to mention that dropping several 0days on Apple didn&#x27;t get me kicked out — but I suppose that&#x27;s always a possibility.In general, I don&#x27;t think that petitions are particularly useful. I&#x27;m more in favor of action (though in this specific case, the boycott is technically inaction). reply saagarjha 9 hours agorootparentIf this gets you kicked out of the developer program, I think everyone would be quite upset, yes. reply ncallaway 14 hours agoparentprev> because you&#x27;ll only participate if you&#x27;re pretty sure nearly everyone else is too.Definitely true for traditional paid labor, but this isn’t that.Nobody is leaving their paycheck behind if they stop using Feedback Assistant. Since the costs to participating in this strike are very low (for most people, participating in the strike is probably a lower cost than not participating in it), I could see a path to building up to an impactful strike over time, in a way that’s not possible in a traditional labor dispute. reply jshier 14 hours agoparentprevFundamentally, such a strike assumes Apple cares about the free reports they get from their developers. I&#x27;m not sure that&#x27;s true. And even if it is, it&#x27;s not like they don&#x27;t have enough other reports or features to work on, so the loss of all outside bug reporting probably wouldn&#x27;t affect them at all. reply jfim 14 hours agorootparentThere might even be a middle manager that gets to say \"external bug reports are 30% down YoY, our quality efforts worked out.\" reply schappim 9 hours agoparentprevSure Apple could be indifferent to a small percentage of people boycotting their process, but the Public Relations (PR) and Comms group is extremely averse to negative commentary. I&#x27;d imagine there are some emails flying around right now... reply e28eta 15 hours agoprevI hope something changes with the way apple manages bugs. It’s incredibly demoralizing to spend my time reproducing a bug, writing up the bug report I’d want to receive (including a minimal test case), and hearing nothing for years until it’s either closed or receiving a message asking me to do more work to determine if it’s still an issue.Are there companies that do this well, at a scale approaching Apple’s?I can think of lots of reasons why this is a “hard problem”, and how it’d be hard to staff. I also think they’re getting paid to solve this problem, and aren’t. reply astrange 15 hours agoparentThere&#x27;s a fundamental problem that the first step is you doing all that work. That means there&#x27;s no way to tell you the work is wasted; you already did it.I generally think people should do &#x2F;less&#x2F; work when reporting bugs because of this, but you should be careful and clear about what you do put in there. reply e28eta 15 hours agorootparentSome of that work does benefit me: better understanding of the bug, verification that it’s not something I’m doing wrong, maybe some ideas of a workaround.I’ve stopped doing the second phase: packaging it up and submitting it to apple, because that’s shown itself to be an entirely negative experience.If apple’s not going to spend the $$ on internal QA, and they’re not going to spend the $$ on developer relations, I’m not especially interested in donating my time to help them. I know I’ve seen a similar sentiment from other developers for years.I think low quality &#x2F; low content bug reports are usually worse than none, because they decrease the signal to noise ratio of bugs. Maybe if there was a better channel of communication, and I knew it’d be more interactive it’d be different. reply baz00 14 hours agoparentprevI got so fed up of all the Apple bugs I sold all my Apple stuff and bought a PC. At least I&#x27;m greeted with new and interesting bugs no one is going to close. reply ladberg 16 hours agoprevI sympathize: even filing radars internally at Apple gets much of the same treatment, albeit with the ability to see the state of the radar. reply changoplatanero 16 hours agoparentYeah he&#x27;s lamenting how poorly the external developers are being treated but seems like they are largely being treated the same as Apple treats its own employees reply JKCalhoun 15 hours agorootparentI found it sometimes frustrating as well. But my experience was that it depended on the engineer&#x2F;team&#x2F;screener the Radar went to. Obviously if it was a bug I found, I much preferred to talk to the engineer in person who maintained the code in question.There were plenty of teams that were very good about jumping on bug reports and getting right back to me. In fact, a decade or so ago at Apple this was more or less the rule.My sense though is that engineering became overwhelmed with bugs especially as Apple itself became as successful as it has become.\"In the old days\" a smaller team would have one of the engineers on the team screen Radars — maybe rotating to a new engineer every week or so. Engineers could quickly mark a bug as a duplicate of another (recognizing a familiar backtrace or symptoms) or might be able to guess at a diagnosis and send it straight to the engineer in charge of the (likely) relevant code.A few teams I worked on were very good about daily \"BRB\"s (bug review board meetings). With a number of engineers in attendance, again, bugs were quickly dispatched to the right component, assigned to right engineer.Again though, as Apple grew, I saw bug screeners on many teams more or less automatically flipping Radars back to the Originator if there was not a litany of documentation attached to the Radar. What build of the OS, what hardware, a full sample, system diagnosis.... It was frustrating when it was an obvious bug — maybe even a UI bug — that would take someone all of 30 seconds to repro. I somewhat sympathize though as traffic no doubt began to become a firehose of bug reports.It seemed backwards to me though to put such a burden on the person who is simply trying to report the bug, trying to help you improve the software. Often in screening Radars the description alone was enough for me to know exactly where in the code the problem was. I didn&#x27;t mind my time being spent at least scanning the Radar title and description. If it was not clear to me at that point, sure, flip it back for more info. The seeming rise of \"auto-reject\" is what began to irk me. reply jbverschoor 14 hours agorootparentDon’t they autonomously analyze a crash report’s stack trace and group them together?As for feedback.. if someone had a dev account or is linked to it, they should be flagged as more detailed&#x2F;investigated reply JKCalhoun 14 hours agorootparentYes, if there&#x27;s a stack trace attached. That is kind of cool though — they also bucket them by OS release so you can see quickly when a bug first appeared (or, well, a similar stack trace appeared anyway). Also they rank them on frequency so it is easy to find your component&#x27;s \"Top Crasher\". reply MichaelZuo 15 hours agorootparentprevWhat would the ideal structure look like in your opinion? reply JKCalhoun 15 hours agorootparentIdeally, daily BRBs (kept to an hour) with engineers attending.If it turns out there are too many Radars to keep the BRBs to an hour then there are issues elsewhere.At the same time, I didn&#x27;t mention it above, engineers need to be allowed time in the product schedule to fix these bugs. As probably everyone knows, when a bug is kicked down the road once (because of scheduling constraints: hey, we gotta ship the software at some point!) it becomes easier still to kick it down the road when the next release dat rolls around (we lived with the bug this long...).I think engineers and users alike would love to see more bug--only releases of MacOS and iOS. People still talk about how wonderful SnowLeopard was simply because it was a bug-fix only (mostly?) release. reply ryandrake 15 hours agorootparentIncentivize screening Radars (and fixing them). There&#x27;s no engineering glory in keeping your unscreened count at zero... nobody gets promoted for it. reply JKCalhoun 14 hours agorootparentGood point. reply jbverschoor 14 hours agorootparentprevNot everybody wants to get promoted reply krackers 9 hours agorootparentprev>People still talk about how wonderful SnowLeopard was simply because it was a bug-fix only (mostly?) release.I think that was part marketing, snow leopard brought pretty sweeping architectural&#x2F;internal changes: ARC, GCD, 64-bit kernel, so it definitely wasn&#x27;t a pure bugfix release. reply kridsdale3 12 hours agorootparentprevWhen I was there on the BRBs I often dreamed of actually commissioning a neon *NMOS* sign that I would flash by pressing a button for nearly every bug that came up for discussion. reply inferiorhuman 14 hours agorootparentprevnext [–]I think engineers and users alike would love to see more bug--only releases of MacOS and iOS. People still talk about how wonderful Snow Leopard was simply because it was a bug-fix only (mostly?) release.Apple&#x27;s dot zero OSX releases have always been pretty bad, so yes Snow Leopard and the last iteration of Tiger look great by comparison. As an outsider I don&#x27;t have fond memories of filing bugs with Apple in that era. The general opacity combined with corporate disinterest discouraged me to the point where it&#x27;s been years since I&#x27;ve even thought about filing a bug.I dread new iOS releases. Sure there are usually unwanted changes (e.g. text selection in iOS 13), but the bugs... iOS 16 completely trashed whatever database Siri uses to map text to speech, so using Siri to play a song is usually a miss. Most recently it&#x27;s decided the name Ben is really the name \"Mouse Face\". That&#x27;s left a fair bit of egg on my face as I interact with those two people in two very different contexts. Sure, I could file a bug, but it&#x27;s just easier to stop using Siri (which was never great in the first place).With Tiger I was trying to integrate LDAP (AD masquerading as OpenDirectory), and it turned out that OSX would just assign GID 0 to any LDAP backed user. To me that seems pretty fucking straightforward, obviously pretty severe. That bug report went nowhere (not even an acknowledgement) and if memory serves Leopard went out with the same issue.I also had a polycarbonate MacBook for personal use around the same time. I ended up filling out the RAM slots and discovered that after a certain amount of uptime that playing DVDs would eventually cause a kernel panic. That bug report actually did get a response, but only one. I think the computer itself died before that got fixed.As someone who went on to dick around with CalDAV and CardDAV, there were plenty of issues to be filed but it was never really worth the effort. But as with everything else, as bad as Apple is they&#x27;re usually better than the competition. That&#x27;s how I ended up replacing my recently deceased Intel MBP with a new ARM one.So long as market share holds I don&#x27;t think Apple&#x27;s got much incentive to reform. Although given that Tim Apple seems hell bent on pulling a Gil Amelio (my new MacBook SuperproairMax 15.323456\" with the all new M-Centris really truly pro max upper bound superlative CPU sure looks great on the shelf) who knows how much longer that&#x27;ll hold. reply kridsdale3 11 hours agorootparentI was on the team that owned CalDAV and CardDAV in approximately that era, as a bug screener and with zero decision making power.The attitude was basically, if it doesn&#x27;t affect us personally working inside Apple, we don&#x27;t care. If our personal workflow and servers doesn&#x27;t use those features (which I guess is the case for GID), it doesn&#x27;t matter.We don&#x27;t know what Exchange is or why anyone would use a Microsoft product. What are they, idiots? We don&#x27;t really care about Gmail. Why is their server not responding according to RFC Whatever? Google must be idiots.Etc.There were a very few number of developers, most were wasting their time making fake leather stitching for the UI because Jobs demanded it. Unit testing was quite poor. Code review was nonexistent. reply inferiorhuman 10 hours agorootparentI should probably clarify: the DVD bug was related to playing DVDs on a Mac with Intel graphics and 4 GB of RAM. Certainly it&#x27;s an odd edge case but it spoke volumes about the quality control with the drivers. Maxing out the RAM on one of those can&#x27;t have been such a rare thing. Certainly I can wrap my head around why calendaring and enterprise features didn&#x27;t get much love, but video drivers on the mass market product? Jesus.With the calendaring stuff: I wrote a CardDAV server that&#x27;s probably still lurking around on Github somewhere. The workarounds I had to implement for MacOS and iOS clients were beyond pale. For instance stuff would just get cached indefinitely and trailing slashes caused OSX to have a conniption fit. If you told me that the client implementations just hardcoded a bunch of stuff so it worked for Steve Jobs (and that nobody else used it) I&#x27;d believe you.Eventually I did a brief stint at a consultant that was brought on to do some infra stuff for Apple, and that didn&#x27;t do much to improve my impression. We got a bunch of servers from the basement with expired iLO licenses, and boy were we lucky to get that much. The attitude was basically, if it doesn&#x27;t affect us personally working inside Apple, we don&#x27;t care. If our personal workflow and servers doesn&#x27;t use those features (which I guess is the case for GID), it doesn&#x27;t matter.Honestly I&#x27;m not sure how to respond to this. If memory serves there was no way to uncouple group mapping, so this was part and parcel of Apple&#x27;s push into the enterprise space (and in this case AD was behaving as an OpenDirectory server would&#x27;ve). Which is to say, their failure in that space was entirely a self-fulfilling prophecy. The more frustrating part was that an obvious security flaw was treated with such disdain, which encouraged me to stop caring about filing bug report with Apple (and it&#x27;s lead me to keep the bluetooth modem off almost exclusively on my iPhone).That gig was actually mostly staffed with Apple alumni so there were 1st gen iPhones all around. Even the less esoteric stuff was simply not good. Obviously voice quality was shit (at least in San Francisco) compared to the CDMA feature phones of the day. One of the coworkers there worked on the mobile Mail.app before bailing for startup life. Autocomplete was a mess. Honestly, I rather enjoyed rubbing some salt into that wound.What amazes me is that fifteen years on and Microsoft is still somehow worse (what with Windows being adware and all). replymusicale 10 hours agorootparentprevSad but probably true. I imagine Apple can afford to do better and it would pay off both internally and externally. reply jbverschoor 14 hours agoparentprevWhich basically mean they’re understaffed with people who enjoy fixing bugs. Fixing bugs, fine tuning, just gardening the code base reply ladberg 13 hours agorootparentI think a lot of Apple engineers would love to have more time to be able to fix bugs but for the most part they&#x27;re all stretched pretty thin.Prioritizing fixing old bugs over developing new features isn&#x27;t a call an IC can really make so I&#x27;d kinda argue that the issue is either \"overstaffed with designers+execs who spend engineer time on UI changes\" or \"understaffed with engineers in general\" and not an issue with the mindset of the current engineers. reply dewey 14 hours agorootparentprevOr it&#x27;s too annoying to triage, find (If app store search is anything to judge by) and replicate bugs because the tooling is too annoying or not collecting the right information if people submit bugs. If that&#x27;s the case then even if you would in theory have the resources to work on bugs it will not be very efficient. reply redleggedfrog 16 hours agoprevActions speak louder than words - Apple is letting it&#x27;s developers know what it thinks of them. You&#x27;re going to have to raise quite the ruckus to get them to change their behavior. There&#x27;s really no impetus for them to change when they&#x27;re sitting on a mountain of cash. reply omnicognate 16 hours agoparentI never thought I&#x27;d wistfully look back on Steve Ballmer yelling DEVELOPERS! DEVELOPERS! DEVELOPERS!!!Apple seem to view external developers as a kind of infestation. The systems and processes they subject them to seem designed to actively discourage them. reply mikestew 14 hours agorootparentI never understood why Ballmer took so much flack for that. Which platform would you rather write code for, the one one with a sweaty representative going on about how much the company loves the people who write code for their platform, or the one that doesn&#x27;t even try to conceal its contempt for you, the developer? reply iudqnolq 14 hours agorootparentDear Board,I don’t want to belong to any club that would have me as a member.Sincerely yours, Groucho Marx. reply bogwog 15 hours agorootparentprevI always find it funny to see Apple pushing for tech education when it&#x27;s well known that Apple despises developers. Please teach your kids computer science so we can treat them like shit and&#x2F;or exploit them!They used to (or still do?) have coding courses for kids at Apple stores, they have free learn to code resources (https:&#x2F;&#x2F;developer.apple.com&#x2F;learn&#x2F;), probably some more programs I&#x27;m not familiar with. reply travoc 15 hours agorootparentprevTo be fair, the app store is already infested with apps and ad networks that push deceptive alerts that attempt to mislead users into installing unwanted&#x2F;unneeded apps. reply kyriakos 15 hours agorootparentprevStockholm syndrome with a hint of masochism. App developers seem to stick with apple regardless of what they are being subjected to. reply latexr 15 hours agorootparent> App developers seem to stick with apple regardless of what they are being subjected to.Because the alternatives aren’t preferable. The indies who have been exclusively using and developing for Apple platforms for years know that Windows and Linux aren’t better¹, just a different set of annoying trade offs. Apple platforms aren’t perfect, but they are the least bad option in that context. They also used to be better in many regards, which makes the situation frustrating due to the wasted potential.¹ At a personal preference level. There’s no judgement either way, you do you and use whatever you like best. reply smoldesu 14 hours agorootparentFor my money, the thing that stops me from dailying a Mac again is the amount of trust Apple demands from you nowadays. Older Macs are really cozy, even up through Yosemite in my opinion. After a certain point though, you could feel the focus shift from onboard utility to metered service. iTunes became Apple Music, the black \"AppleTV\" logo popped up on my dock, and iCloud pitched a tent in my Settings menu to beg for change. None of this really appeals to me; it&#x27;s engineered to make the common user happy for a nominal fee, but to everyone else it&#x27;s just annoying. Little UX warts like that popped up all over MacOS, and it really started to evoke those \"Windows 8 Armageddon\" vibes I felt a while back.For Apple to correct course, they should ditch the ads and focus on superpowering MacOS and iOS. Right now it&#x27;s hard for me to trust that Apple is giving me the best of all worlds. reply realusername 14 hours agorootparentprevIt&#x27;s the same with users as well, the keyboard of my iPhone&#x27;s wife is freezing randomly during 3s but she still puts up with it and will not say a word against it.I gave up on mine personally after the app installs were impossible due to some modal error in loops.I&#x27;m not sure how they managed to do it but they are very strong on the marketing side, I&#x27;ll admit that. reply CamperBob2 15 hours agorootparentprevWhen the bank robbers share their loot with the hostages, Stockholm Syndrome can&#x27;t be too surprising. reply saltypal 15 hours agoprevPreviously: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=3947903(I feel incredibly old right now.)Edited to add: Finally found the full form letter [1]. (I filed a copy back then and it still shows up languishing in Feedback Assistant even now!)[1]: https:&#x2F;&#x2F;gist.github.com&#x2F;mysteriouspants&#x2F;1989061 reply plorkyeran 16 hours agoprevReporting bugs via feedback assistant is sometimes required when you&#x27;ve successfully found a developer inside Apple who has agreed to fix your bug and they just need the feedback number for internal work reporting purposes. Submitting unsolicited bug reports are just a pointless waste of time. I wouldn&#x27;t say I&#x27;m specifically \"boycotting\" Feedback Assistant; I just stopped using it because it wasn&#x27;t doing anything useful. reply neodypsis 15 hours agoparentHow are you supposed to find a developer inside Apple to fix your issue? reply Klonoar 15 hours agorootparentIt is possible to backchannel to them like any other company. It&#x27;s just much less... publicly evident, if that makes sense?Also (IIRC) if you post on their dev forums and deal with e.g Quinn, they might ask you to file and provide a radar - but it&#x27;s been a minute since I&#x27;ve interacted over there. reply nicky0 14 hours agorootparentThat guy Quinn \"The Eskimo\" seems to have been almost singlehandedly manning the developer forums at Apple for over a decade. I hope Apple recognises how valuable he is. reply AlexandrB 15 hours agorootparentprevAs with all modern tech support: complain loudly on Twitter&#x2F;X and hope the issue gets enough traction that somebody notices. reply s3p 15 hours agorootparentprevMaybe HN or LinkedIn, lol reply wackget 15 hours agoprevAs a developer, I flatly refuse to engage with Apple in any way because of their $100 annual fee.I feel especially vindicated when stuff like this surfaces.Imagine being the richest company in the world and having the audacity to charge people for the privilege of being able to contribute to your platform.It is absolutely insane, and nothing will convince me otherwise. reply kaishin 1 minute agoparentOne reason they are charging is to weed out bad actors or people not serious enough about publishing apps on their stores. That said they could do this as a one off like Google, and make it cheaper too. reply stockboss 14 hours agoparentprevwhile i agree with your sentiment, i think Apple would probably argue that they&#x27;re actually charging for access to their distribution network. reply type0 7 hours agorootparentThey are charging for the access to their \"walled city&#x2F;garden\", they are promoting digital feudalism and the worst part is that other companies see them as exemplary for these terrible practices. reply saagarjha 16 hours agoprevWait, you can’t file feedback from the web anymore? That’s incredibly stupid. Their feedback app itself breaks half the time, so I guess they wouldn’t be getting my reports then even if they wanted them. reply CharlesW 16 hours agoparenthttps:&#x2F;&#x2F;www.apple.com&#x2F;feedback&#x2F; reply saagarjha 16 hours agorootparentIf reporting feedback using Feedback Assistant is akin to shouting into the void, using that site is like doing that but in the vacuum of space. reply CharlesW 16 hours agorootparentI can&#x27;t speak for its efficacy, just it&#x27;s existence. reply dwaite 15 hours agoparentprevI&#x27;m not sure what that is about, https:&#x2F;&#x2F;feedbackassistant.apple.com reply AshleysBrain 14 hours agoprevI previously blogged about how Apple treat web developers with negligence when it comes to Safari [1]. This makes it looks like Apple treat other developers with negligence as well, which is unsurprising, frankly.[1] https:&#x2F;&#x2F;www.construct.net&#x2F;en&#x2F;blogs&#x2F;ashleys-blog-2&#x2F;safari-rel... reply realusername 14 hours agoparentI solved this by basically treating the browser as legacy and \"best effort\", similarly as IE11 in its days.They have a very large number of basic features broken regularly, it&#x27;s not only about cutting edge stuff. I suspect their testing practices are less than stellar. reply AshleysBrain 14 hours agorootparentYep, we&#x27;ve ended up on the same idea - we make a best effort to keep it working and if we can&#x27;t we advise to switch to another browser. But tough luck on iOS, as other browser engines are banned... reply realusername 14 hours agorootparentIt&#x27;s going to be controversial but iPhone users are going to have an inferior web experience no matter what, at some point we have to say it even if it&#x27;s not popular. The core business of Apple is in the apps. reply watersb 14 hours agoprevI stopped filing Apple bugs (\"rdar\") ten years ago.It was pitching a lot of work into a hole: totally opaque. reply pschuegr 16 hours agoprevI&#x27;m in. Apple Feedback Assistant is absolutely useless. reply Obscurity4340 14 hours agoparent[pirate voice] all right, all right, you talkd me into it (parrot squawk) reply AnonC 15 hours agoprevSomeone please set up a website that can compose an email to Craig Federighi with these issues and let developers email him directly. Apple may not act on or respond to this collective Feedback Assistant feedbacks, since the system may probably be broken to the extent that Apple’s teams don’t even see or get these feedback reports. reply guessmyname 15 hours agoparent> Someone please set up a website that can compose an email to […]We do not need a website for that.Simply send an email to Craig Federighi or even Tim Cook.You can find their email addresses here → https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27320383 reply rado 15 hours agoparentprevFederighi replied with a suggestion to fix my issue. It didn&#x27;t work, but it was cool anyway. reply spacebacon 16 hours agoprevMy experience talking to brick walls has been more productive. reply walterbell 16 hours agoprevWhat would happen if some Apple developers moved their collective feedback to a Github repo where their bug reports were licensed as CC-BY-4.0, i.e. a license that would allow Apple to migrate content from the public repo to their internal radar? This would allow developers to have a public database of de-duped and triaged bugs, and it would allow Apple to either respond publicly (same as Webkit?) or maintain status quo. reply latexr 16 hours agoparentNothing would happen. Websites and GitHub repos for posting Apple bug reports publicly have existed for over a decade.https:&#x2F;&#x2F;openradar.appspot.com&#x2F; reply pronoiac 15 hours agorootparentI&#x27;m aware of an open collection of Apple Feedback Assistant reports on GitHub - https:&#x2F;&#x2F;github.com&#x2F;feedback-assistant&#x2F;reports&#x2F;issues> Apple Feedback Assistant reports are not public. To help other developers discover existing reports, copy-paste your submitted Feedback Assistant report into a new issue. Make sure you include everything.> This is similar to Open Radar, but it&#x27;s easier to fill out the report and GitHub has better search. Also, Open Radar doesn&#x27;t yet support Feedback Assistant.> Instead of voting on issues opened here (which Apple won&#x27;t see anyway), file a new Feedback Assistant report and copy-paste the contents of the report you found here and link to the original report. Apple encourages duplicates. reply walterbell 15 hours agorootparentprev> have existed for over a decade Please note: Reports posted here will not necessarily be seen by Apple. All problems should be submitted at bugreport.apple.com before they are posted here. Please only post information for Radars that you have filed yourself, and please do not include Apple confidential information in your posts. Thank you!Those look like a public mirror of some Radars filed at Apple, i.e. Apple already has that information. If enough developers stop submitting feedback (topic of this HN thread) and start posting bugs into a public repo FIRST, with a CC-BY-4.0 license allowing Apple to mirror the public bugs, then a boycott would have the positive side effect of showing-by-example how the feedback process could be improved.> nothing would happenAs a counterpoint, there is a public Bugzilla for Webkit bugs, where the keyword \"InRadar\" indicates whether the bug has been mirrored to Apple&#x27;s internal bug reporting system. If enough developers boycott Apple Feedback, could they use a Bugzilla instance for iOS and macOS bug reporting? If enough Apple users start referencing Bugzilla URLs for iOS&#x2F;macOS production issues and purchasing decisions, could that influence Apple?https:&#x2F;&#x2F;bugs.webkit.org&#x2F;buglist.cgi?bug_status=__open__&cont... reply ayewo 15 hours agoparentprevYou mean something like this https:&#x2F;&#x2F;github.com&#x2F;feedback-assistant&#x2F;reports ? reply walterbell 15 hours agorootparentThat is yet another public mirror of private Apple feedback reports. The suggestion is a public Bugzilla of iOS&#x2F;macOS bug reports, similar to Webkit, that could be selectively mirrored by Apple to private radars, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38166908 reply CharlesW 16 hours agoprevI&#x27;m not sure I understand this. Wouldn&#x27;t developers who need code-level assistance (i.e. have more than simple \"feedback\") submit a Technical Support Incident (https:&#x2F;&#x2F;developer.apple.com&#x2F;support&#x2F;technical&#x2F;)? reply speedyapoc 16 hours agoparentHere&#x27;s an excerpt from the reply that I get from filing a TSI for functionality that is ultimately a bug with iOS.------------Hello,Thank you for contacting Apple Developer Technical Support (DTS).We believe this issue is a bug. Please file a bug report via Feedback Assistant (https:&#x2F;&#x2F;feedbackassistant.apple.com). For more information on Feedback Assistant, visit https:&#x2F;&#x2F;developer.apple.com&#x2F;bug-reporting.------------From here, typically the Feedback Assistant bug report will remain open. For years. With no acknowledgement.I would happily pay Apple a developer rate to fix these bugs. Some of them are massive showstoppers in a large scale production application. reply chongli 15 hours agorootparentI think it&#x27;s safe to say that Apple doesn&#x27;t care about any individual developer unless they are large enough (i.e. Adobe) to move the needle. If developers really wanted to get Apple&#x27;s attention, they wouldn&#x27;t boycott Feedback Assistant, they&#x27;d boycott the App Stores. If enough developers pull their apps off the market it would move the needle and Apple would have to pay attention. reply culi 16 hours agorootparentprev> I would happily pay Apple a developer rate to fix these bugs. Some of them are massive showstoppers in a large scale production application.Wow that&#x27;s pretty intense. Any examples of such show-stopping bugs? reply speedyapoc 15 hours agorootparentHere&#x27;s a few recent ones:1. Xcode 15&#x27;s \"Replace Container\" feature replaces the app container with incorrect permissions that results in the app not being able to write to its container (ex. the documents directory). This is an important feature for debugging and flat out doesn&#x27;t work.2. Apple&#x27;s AVPlayer has an API called MTAudioProcessingTap which allows you to get access to low level audio data. Since iOS 17.1, it is not possible to have more than one MTAudioProcessingTap running at the same time.3. AVPlayer&#x27;s `addPeriodicTimeObserverForInterval` function will randomly stop calling back to its observer, and never recover, when connected to an Apple TV via AirPlay on iOS 17.Issue 1 makes debugging more arduous. Issue 2 stops my app from being able to crossfade audio together or play more than one audio stream at once. Issue 3 requires me to build my own time observer which is further technical debt, versus being able to rely on Apple&#x27;s API.I&#x27;ve spent hours debugging each of these and trying to find a workaround before resigning myself that it&#x27;s a platform issue and relegating myself to abandoning that specific API or feature. reply toast0 15 hours agorootparent> Issue 3 requires me to build my own time observer which is further technical debt, versus being able to rely on Apple&#x27;s API.IMHO, it&#x27;s the same amount of technical debt, regardless of who built it. In this case, the overall situation is worse: because you didn&#x27;t build it, you can&#x27;t fix it. If you have a dependency built by a benevolent provider, that may mean less work for you in the future, but it&#x27;s still technical debt. reply happytoexplain 16 hours agoparentprevRead the post - it&#x27;s about bug reporting, not assistance with code or simple feedback.The feedback assistant is primarily for reporting bugs. Bugs that impact developers. reply CharlesW 16 hours agorootparentReading the post is how I learned the developer didn&#x27;t file Technical Support Incidents (TSIs). reply agsnu 16 hours agorootparentAlmost every time, developer technical support will look at the internal bug and if there is no known workaround will say “sorry that’s our bug and engineering is investigating, we don’t have anything to tell you, please keep an eye on the feedback and we will update you if anything changes - in the meantime here’s your TSI refunded”. reply e28eta 15 hours agorootparentIn that process though, you’ve had someone at Apple verify that your issue is legitimate, and that issue has probably shown up in a place they care about (DTS metrics)Two things that simply filing a feedback issue doesn’t seem to reliably do.I’m suspect a concerted effort to get developers using their TSIs for bug reporting would backfire (stop refunding them? remove the program?), but I think it’s an interesting idea given how cheap the TSIs are for developers. reply agsnu 14 hours agorootparentSure and I don’t mean to minimise the value&#x2F;effort - I’m sure the “look at” may involve actually investigating the issue themselves by looking at the implementation, speaking with the responsible engineers, etc. I am also sure it is a hard problem to solve - while Apple is massive and well resourced these days their developer community is also humongous and I am sure there is a tremendous amount of noise. But my point is merely - TSIs are not a panacea reply CharlesW 16 hours agorootparentprevOoof. That’s a terrible experience, thank you for sharing. reply saagarjha 16 hours agoparentprevDevelopers use feedback to file bugs. Why would they spend money to get help from Apple on how to do that? reply CharlesW 16 hours agorootparent> Developers use feedback to file bugs.Yes, and they&#x27;re specifically instructed to file TSIs if they need a workaround in the meantime. https:&#x2F;&#x2F;developer.apple.com&#x2F;bug-reporting&#x2F;Should developers not have to do both? Sure. Should they ignore Apple&#x27;s instructions and still expect to get help? Not sure. reply loumf 16 hours agorootparentFrom experience, I have used a TSI when I need help with a bug in Apple&#x27;s products. Every single time (about 5x) they \"refunded\" the TSI because it was an Apple bug. I have never had to pay for extra TSIs -- I use the ones that come with my developer account and (so far) they are always refunded. I don&#x27;t know why people hoard them -- they are use it or lose it every year.I have also gone to a virtual dev lab during WWDC which was awesome. Lots of \"free\" help from an Apple engineer (basically 30 minute speed run of lot of questions about Apple Watch workout app development).I have mixed experience with Feedbacks. I have filed dozens that I actually care about and maybe 10% were a decent experience. The rest were essentially ignored. reply latexr 16 hours agorootparentprevI filed a TSI earlier this year. It look them six months to reply, and even then it was to ask for information that had already been given in the original report. By then I had already figured out what was causing my problem and found a workaround. Still, I thanked the developer for their time, described my approach and asked if there were a less hacky way to do it. I never got a reply. reply mistrial9 16 hours agorootparentprev\"developers are a profit center\" is your answer reply sccxy 15 hours agoprevAlso their feedback assistant is buggy as hell.If you managed to create draft in old version (before random code change).But it does not match with new version your feedback assistant crashes to blank screen. (JSON parse syntax error)That means it is impossible to file new bugs...1 second web page works but after that it is blank screen and nothing to do. reply efitz 13 hours agoprevI am sick of everybody asking for feedback everywhere, and nobody wants to spend any human effort looking at the feedback.And the worst of it is the \"Do you love our app?\" dark pattern. reply vasdae 15 hours agoprevI suppose Feedback Assistant is like Windows&#x27; Feedback Hub. Users fill it to the top with shit making it impossible for Apple or Microsoft to do anything useful with the feedback. reply s3p 15 hours agoparentDo users even use Feedback Assistant? I have yet to hear of anyone in my circle (friends, family, coworkers) submit a Feedback. None of them are developers or us the developer beta of iOS, so I have always surmised that to be why. reply snazz 14 hours agorootparentThe app is installed but hidden by default unless you’re running a beta (and in my experience I’ve never gotten a reply to a report unless it’s on a beta). You can use the website or launch the native app with its URI scheme but approximately nobody does that unless they’re really frustrated with a bug and know Feedback Assistant exists. reply unflxw 12 hours agoprevSome minor product manager in Cupertino, two quarters from now: \"As we can see in this graph, thanks to the improvements to Feedback Assistant over the past couple years, we&#x27;ve reduced our products&#x27; bug rate to nearly zero percent!\" reply Obscurity4340 15 hours agoprevI like when HN holds the fire to these kinds of issues&#x2F;companies. Its almost like us nerds have some kind of fancy communication system&#x2F;technology or something [dunno shrug] reply rado 16 hours agoprevThe recent macOS dialog \"Allow app to access folder [OK] [Cancel]\" supports button cycling by Shift+Tab (previous), but not Tab (next). The Wi-Fi signal strength indication changes when clicking it. So yeah, at least they need to admit things are very bad over there. reply AnonC 16 hours agoparentWhenever I see or hear about keyboard shortcuts being weird or not working, for some reason I can only think of Catalyst or Electron, both bringing frustratingly poor user experiences. Apple’s own Catalyst based apps (Reminders, as an example) have been terrible over the last few years. I can’t imagine a macOS dialog being either one of these abominations though, which makes me wonder how the issue you describe was introduced. reply dwaite 15 hours agorootparentApple&#x27;s solution for missing Catalyst functionality has been to create analogous features or port existing API to iPad.That said - the long-term strategy doesn&#x27;t appear to be Catalyst, but Swift UI. reply rado 15 hours agorootparentprevThey still seem to have issues from the Carbon to Cocoa transition 20 years ago, like my favourite: apps don&#x27;t remember their keyboard language. reply sitzkrieg 14 hours agoparentprevthese arent the heavy HN apple blinders these threads usually get!but yea its pretty bad. i cant go 1 day without a blatant ui bug lightly using apple products at work reply zakki 15 hours agoprevLet’s see if there will be Bug as a Service. Pay for bug fix in addition of $99&#x2F;year reply enginaar 14 hours agoprevIMO it&#x27;s optimistic to assume Apple can fix this reply gumballindie 16 hours agoprevApple’s customer support as a whole is atrocious. Here in the uk their milton keynes apple store is essentially a cesspit of cockiness and upselling rudeness. Filed a complaint against an employee and have yet to receive any feedback two months later. The only apple product in my life is now an iphone, but that will be phased out too. Good riddance. Open source all the way. reply mvdtnz 15 hours agoprev [–] It&#x27;s not clear from reading this if \"Apple developers\" are developers who work for Apple, or developers who write software for Apple systems. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jeff Johnson has initiated a boycott against Apple's Feedback Assistant, urging developers to file new feedbacks about existing issues, but refuse to respond to current ones until improvements are made.",
      "Significant issues highlighted include Apple's lack of response, random closing of feedbacks, ignoring developer bug reports, demanding detailed sysdiagnose reports, neglecting smaller methods for diagnosing bugs, lack of search function for bugs, and a mandate to file all feedbacks via macOS or iOS.",
      "The boycott, compared to a labor strike, emphasizes the unrecognized and unpaid contribution of developers in testing and refining Apple's software and products. It does not criticize individual Apple engineers but challenges the company's leadership."
    ],
    "commentSummary": [
      "Jeff Johnson is rallying Apple developers to boycott Apple's Feedback Assistant until improvements are made to several significant issues including Apple's unresponsiveness, dismissal of bug reports, and inability to search for bugs.",
      "The intended boycott encourages developers to submit new feedback about the system's issues, but abstain from replying to existing ones, emphasizing the crucial but unpaid contributions developers make in testing and refining Apple's products.",
      "Johnson clarified that the boycott is not targeted towards Apple's engineers, but rather it aims to compel Apple's leadership to acknowledge and enhance their bug reporting system."
    ],
    "points": 302,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1699288243
  },
  {
    "id": 38164046,
    "title": "Understanding the Risks and Proper Usage of Git Rebase and Git Push Commands",
    "originLink": "https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/",
    "originBody": "git rebase: what can go wrong? Hello! While talking with folks about Git, I’ve been seeing a comment over and over to the effect of “I hate rebase”. People seemed to feel pretty strongly about this, and I was really surprised because I don’t run into a lot of problems with rebase and I use it all the time. I’ve found that if many people have a very strong opinion that’s different from mine, usually it’s because they have different experiences around that thing from me. So I asked on Mastodon: today I’m thinking about the tradeoffs of using git rebase a bit. I think the goal of rebase is to have a nice linear commit history, which is something I like. but what are the costs of using rebase? what problems has it caused for you in practice? I’m really only interested in specific bad experiences you’ve had here – not opinions or general statements like “rewriting history is bad” I got a huge number of incredible answers to this, and I’m going to do my best to summarize them here. I’ll also mention solutions or workarounds to those problems in cases where I know of a solution. Here’s the list: fixing the same conflict repeatedly is annoying rebasing a lot of commits is hard undoing a rebase is hard force pushing to shared branches can cause lost work force pushing makes code reviews harder losing commit metadata rebasing can break intermediate commits accidentally run git commit –amend instead of git rebase –continue splitting commits in an interactive rebase is hard complex rebases are hard rebasing long lived branches can be annoying rebase and commit discipline a “squash and merge” workflow miscellaneous problems My goal with this isn’t to convince anyone that rebase is bad and you shouldn’t use it (I’m certainly going to keep using rebase!). But seeing all these problems made me want to be more cautious about recommending rebase to newcomers without explaining how to use it safely. It also makes me wonder if there’s an easier workflow for cleaning up your commit history that’s harder to accidentally mess up. my git workflow assumptions First, I know that people use a lot of different Git workflows. I’m going to be talking about the workflow I’m used to when working on a team, which is: the team uses a central Github/Gitlab repo to coordinate there’s one central main branch. It’s protected from force pushes. people write code in feature branches and make pull requests to main The web service is deployed from main every time a pull request is merged. the only way to make a change to main is by making a pull request on Github/Gitlab and merging it This is not the only “correct” git workflow (it’s a very “we run a web service” workflow and open source project or desktop software with releases generally use a slightly different workflow). But it’s what I know so that’s what I’ll talk about. two kinds of rebase Also before we start: one big thing I noticed is that there were 2 different kinds of rebase that kept coming up, and only one of them requires you to deal with merge conflicts. rebasing on an ancestor, like git rebase -i HEAD^^^^^^^ to squash many small commits into one. As long as you’re just squashing commits, you’ll never have to resolve a merge conflict while doing this. rebasing onto a branch that has diverged, like git rebase main. This can cause merge conflicts. I think it’s useful to make this distinction because sometimes I’m thinking about rebase type 1 (which is a lot less likely to cause problems), but people who are struggling with it are thinking about rebase type 2. Now let’s move on to all the problems! fixing the same conflict repeatedly is annoying If you make many tiny commits, sometimes you end up in a hellish loop where you have to fix the same merge conflict 10 times. You can also end up fixing merge conflicts totally unnecessarily (like dealing with a merge conflict in code that a future commit deletes). There are a few ways to make this better: first do a git rebase -i HEAD^^^^^^^^^^^ to squash all of the tiny commits into 1 big commit and then a git rebase main to rebase onto a different branch. Then you only have to fix the conflicts once. use git rerere to automate repeatedly resolving the same merge conflicts (“rerere” stands for “reuse recorded resolution”, it’ll record your previous merge conflict resolutions and replay them). I’ve never tried this but I think you need to set git config rerere.enabled true and then it’ll automatically help you. Also if I find myself resolving merge conflicts more than once in a rebase, I’ll usually run git rebase --abort to stop it and then squash my commits into one and try again. rebasing a lot of commits is hard Generally when I’m doing a rebase onto a different branch, I’m rebasing 1-2 commits. Maybe sometimes 5! Usually there are no conflicts and it works fine. Some people described rebasing hundreds of commits by many different people onto a different branch. That sounds really difficult and I don’t envy that task. undoing a rebase is hard I heard from several people that when they were new to rebase, they messed up a rebase and permanently lost a week of work that they then had to redo. The problem here is that undoing a rebase that went wrong is much more complicated than undoing a merge that went wrong (you can undo a bad merge with something like git reset --hard HEAD^). Many newcomers to rebase don’t even realize that undoing a rebase is even possible, and I think it’s pretty easy to understand why. That said, it is possible to undo a rebase that went wrong. Here’s an example of how to undo a rebase using git reflog. step 1: Do a bad rebase (for example run git rebase -I HEAD^^^^^ and just delete 3 commits) step 2: Run git reflog. You should see something like this: ee244c4 (HEAD -> main) HEAD@{0}: rebase (finish): returning to refs/heads/main ee244c4 (HEAD -> main) HEAD@{1}: rebase (pick): test fdb8d73 HEAD@{2}: rebase (start): checkout HEAD^^^^^^^ ca7fe25 HEAD@{3}: commit: 16 bits by default 073bc72 HEAD@{4}: commit: only show tooltips on desktop step 3: Find the entry immediately before rebase (start). In my case that’s ca7fe25 step 4: Run git reset --hard ca7fe25 Another solution folks mentioned to “undoing a rebase is hard” that avoids having to use the reflog is to make a “backup branch” with git switch -c backup before rebasing, so you can easily get back to the old commit. force pushing to shared branches can cause lost work A few people mentioned the following situation: You’re collaborating on a branch with someone You push some changes They rebase the branch and run git push --force (maybe by accident) Now when you run git pull, it’s a mess – you get the a fatal: Need to specify how to reconcile divergent branches error While trying to deal with the fallout you might lose some commits, especially if some of the people are involved aren’t very comfortable with git This is an even worse situation than the “undoing a rebase is hard” situation because the missing commits might be split across many different people’s and the only worse thing than having to hunt through the reflog is multiple different people having to hunt through the reflog. This has never happened to me because the only branch I’ve ever collaborated on is main, and main has always been protected from force pushing (in my experience the only way you can get something into main is through a pull request). So I’ve never even really been in a situation where this could happen. But I can definitely see how this would cause problems. The main tools I know to avoid this are: don’t rebase on shared branches use --force-with-lease when force pushing, to make sure that nobody else has pushed to the branch since your last fetch Apparently the “since your last fetch” is important here – if you run git fetch immediately before running git push --force-with-lease, the --force-with-lease won’t protect you at all. I was curious about why people would run git push --force on a shared branch. Some reasons people gave were: they’re working on a collaborative feature branch, and the feature branch needs to be rebased onto main. The idea here is that you’re just really careful about coordinating the rebase so nothing gets lost. as an open source maintainer, sometimes they need to rebase a contributor’s branch to fix a merge conflict they’re new to git, read some instructions online that suggested git rebase and git push --force as a solution, and followed them without understanding the consequences they’re used to doing git push --force on a personal branch and ran it on a shared branch by accident force pushing makes code reviews harder The situation here is: You make a pull request on GitHub People leave some comments You update the code to address the comments, rebase to clean up your commits, and force push Now when the reviewer comes back, it’s hard for them to tell what you changed since the last time you saw it – all the commits show up as “new”. One way to avoid this is to push new commits addressing the review comments, and then after the PR is approved do a rebase to reorganize everything. I think some reviewers are more annoyed by this problem than others, it’s kind of a personal preference. Also this might be a Github-specific issue, other code review tools might have better tools for managing this. losing commit metadata If you’re rebasing to squash commits, you can lose important commit metadata like Co-Authored-By. Also if you GPG sign your commits, rebase loses the signatures. There’s probably other commit metadata that you can lose that I’m not thinking of. I haven’t run into this one so I’m not sure how to avoid it. I think GPG signing commits isn’t as popular as it used to be. rebasing can break intermediate commits If you’re trying to have a very clean commit history where the tests pass on every commit (very admirable!), rebasing can result in some intermediate commits that are broken and don’t pass the tests, even if the final commit passes the tests. Apparently you can avoid this by using git rebase -x to run the test suite at every step of the rebase and make sure that the tests are still passing. I’ve never done that though. accidentally run git commit --amend instead of git rebase --continue A couple of people mentioned issues with running git commit --amend instead of git rebase --continue when resolving a merge conflict. The reason this is confusing is that there are two reasons when you might want to edit files during a rebase: editing a commit (by using edit in git rebase -i), where you need to write git commit --amend when you’re done a merge conflict, where you need to run git rebase --continue when you’re done It’s very easy to get these two cases mixed up because they feel very similar. I think what goes wrong here is that you: Start a rebase Run into a merge conflict Resolve the merge conflict, and run git add file.txt Run git commit because that’s what you’re used to doing after you run git add But you were supposed to run git rebase --continue! Now you have a weird extra commit, and maybe it has the wrong commit message and/or author splitting commits in an interactive rebase is hard The whole point of rebase is to clean up your commit history, and combining commits with rebase is pretty easy. But what if you want to split up a commit into 2 smaller commits? It’s not as easy, especially if the commit you want to split is a few commits back! I actually don’t really know how to do it even though I feel very comfortable with rebase. I’d probably just do git reset HEAD^^^ or something and use git add -p to redo all my commits from scratch. One person shared their workflow for splitting commits with rebase. complex rebases are hard If you try to do too many things in a single git rebase -i (reorder commits AND combine commits AND modify a commit), it can get really confusing. To avoid this, I personally prefer to only do 1 thing per rebase, and if I want to do 2 different things I’ll do 2 rebases. rebasing long lived branches can be annoying If your branch is long-lived (like for 1 month), having to rebase repeatedly gets painful. It might be easier to just do 1 merge at the end and only resolve the conflicts once. The dream is to avoid this problem by not having long-lived branches but it doesn’t always work out that way in practice. miscellaneous problems A few more issues that I think are not that common: Stopping a rebase wrong: If you try to abort a rebase that’s going badly with git reset --hard instead of git rebase --abort, things will behave weirdly until you stop it properly Weird interactions with merge commits: A couple of quotes about this: “If you rebase your working copy to keep a clean history for a branch, but the underlying project uses merges, the result can be ugly. If you do rebase -i HEAD~4 and the fourth commit back is a merge, you can see dozens of commits in the interactive editor.“, “I’ve learned the hard way to never rebase if I’ve merged anything from another branch” rebase and commit discipline I’ve seen a lot of people arguing about rebase. I’ve been thinking about why this is and I’ve noticed that people work at a few different levels of “commit discipline”: Literally anything goes, “wip”, “fix”, “idk”, “add thing” When you make a pull request (on github/gitlab), squash all of your crappy commits into a single commit with a reasonable message (usually the PR title) Atomic Beautiful Commits – every change is split into the appropriate number of commits, where each one has a nice commit message and where they all tell a story around the change you’re making Often I think different people inside the same company have different levels of commit discipline, and I’ve seen people argue about this a lot. Personally I’m mostly a Level 2 person. I think Level 3 might be what people mean when they say “clean commit history”. I think Level 1 and Level 2 are pretty easy to achieve without rebase – for level 1, you don’t have to do anything, and for level 2, you can either press “squash and merge” in github or run git switch main; git merge --squash mybranch on the command line. But for Level 3, you either need rebase or some other tool (like GitUp) to help you organize your commits to tell a nice story. I’ve been wondering if when people argue about whether people “should” use rebase or not, they’re really arguing about which minimum level of commit discipline should be required. I think how this plays out also depends on how big the changes folks are making – if folks are usually making pretty small pull requests anyway, squashing them into 1 commit isn’t a big deal, but if you’re making a 6000-line change you probably want to split it up into multiple commits. a “squash and merge” workflow A couple of people mentioned using this workflow that doesn’t use rebase: make commits Run git merge main to merge main into the branch periodically (and fix conflicts if necessary) When you’re done, use GitHub’s “squash and merge” feature (which is the equivalent of running git checkout main; git merge --squash mybranch) to squash all of the changes into 1 commit. This gets rid of all the “ugly” merge commits. I originally thought this would make the log of commits on my branch too ugly, but apparently git log main..mybranch will just show you the changes on your branch, like this: $ git log main..mybranch 756d4af (HEAD -> mybranch) Merge branch 'main' into mybranch 20106fd Merge branch 'main' into mybranch d7da423 some commit on my branch 85a5d7d some other commit on my branch Of course, the goal here isn’t to force people who have made beautiful atomic commits to squash their commits – it’s just to provide an easy option for folks to clean up a messy commit history (“add new feature; wip; wip; fix; fix; fix; fix; fix;“) without having to use rebase. I’d be curious to hear about other people who use a workflow like this and if it works well. there are more problems than I expected I went into this really feeling like “rebase is fine, what could go wrong?” But many of these problems actually have happened to me in the past, it’s just that over the years I’ve learned how to avoid or fix all of them. And I’ve never really seen anyone share best practices for rebase, other than “never force push to a shared branch”. All of these honestly make me a lot more reluctant to recommend using rebase. To recap, I think these are my personal rebase rules I follow: stop a rebase if it’s going badly instead of letting it finish (with git rebase --abort) know how to use git reflog to undo a bad rebase don’t rebase a million tiny commits (instead do it in 2 steps: git rebase -i HEAD^^^^ and then git rebase main) don’t do more than one thing in a git rebase -i. Keep it simple. never force push to a shared branch never rebase commits that have already been pushed to main Thanks to Marco Rogers for encouraging me to think about the problems people have with rebase, and to everyone on Mastodon who helped with this. Want a weekly digest of this blog? Subscribe Confusing git terminology",
    "commentLink": "https://news.ycombinator.com/item?id=38164046",
    "commentBody": "Git rebase, what can go wrongHacker NewspastloginGit rebase, what can go wrong (jvns.ca) 279 points by kens 18 hours ago| hidepastfavorite349 comments jawns 17 hours agoI like how Atlassian puts it:> The golden rule of rebasing> Once you understand what rebasing is, the most important thing to learn is when not to do it. The golden rule of git rebase is to never use it on public branches.https:&#x2F;&#x2F;www.atlassian.com&#x2F;git&#x2F;tutorials&#x2F;merging-vs-rebasing#...For me, even though rebasing comes with some trappings, I still greatly prefer it to the alternative, which is to have merge commits cluttering up the commit history. reply sakarisson 3 minutes agoparent> I still greatly prefer it to the alternative, which is to have merge commits cluttering up the commit history.I&#x27;ve heard this many times before, but haven&#x27;t been able to figure out why this is a problem. In your workflow is it a problem to have a cluttered commit history? If so, could you explain how? reply ohwellhere 16 hours agoparentprevThe way I phrase and teach what I consider to be the important rule of git is:> Don&#x27;t rewrite history on shared branches with proper communication.I don&#x27;t teach \"never\", I don&#x27;t teach that `main` is special, I don&#x27;t teach that force pushing is forbidden, because I don&#x27;t believe in those things.I highly prefer a rebase-heavy workflow. In addition to not \"cluttering\" the history, it&#x27;s an invaluable tool to keep commits focused on \"the right level\" of atomic changes. reply joshribakoff 15 hours agorootparentYou can simply pass flags to “git log” to hide merge commits, without needing to rewrite history to “destroy” that information. While they are often noisy, sometimes they can be useful. I usually prefer to hide information rather than destroy it. reply unlikelytomato 7 hours agorootparentI read this justification in nearly every thread that pops up git rebase. I feel like a full because I cannot think of a real world example when this information crosses from signal to noise. Generally, branches that are not ready to merge tend to have enormous amounts of noise commits. Is there a blog post or some concrete examples I could work through that illustrate these benefits? I feel like workflows dramatically different from mine are likely the source of my struggle. reply fulafel 4 hours agorootparentIt&#x27;s a log of what happened in dev and supports reconstructing history to understand why something worked or didn&#x27;t in retrospect. \"It work when we tried it\" \"oh this dependency was updated in this merge commit that could have changed the behaviour\" reply jakelazaroff 16 hours agorootparentprevI assume that “with” is meant to be “without”? reply thebigspacefuck 9 hours agorootparentprevIt’s annoying when someone force pushes to a branch that you just reviewed, but you can no longer see the history so you have to scan through the whole PR you already reviewed looking for the change. Please just commit the fix, let me see it, then squash it. reply sam_bristow 7 hours agorootparentUnfortunately I haven&#x27;t seen a git forge that will let you do \"autosquash on merge\" so I could just push up fixup commits as part of an merge request. reply recursive 16 hours agoparentprevSquash merges cut down the noise considerably. reply dkarl 15 hours agorootparentI think squash merges are a last resort heavy-handed tool for dealing with developers who refuse to clean up their commit history before merging. Most developers can do better by hand.Git history should tell a simple, understandable story of each change. For example: 1) refactor existing code, 2) add feature. Or 1) add missing tests, 2) refactor existing code, 3) add feature.But since you&#x27;re working on the fly with imperfect knowledge, it doesn&#x27;t happen in such neat steps. Refactorings and behavior changes end up interleaved in your raw git history, so you need to do a little bit of cleanup by hand in order to present a simple story in the commit log.Of course if you have developers that don&#x27;t do that and instead merge dozens of commits that just say wip, wip, wip, lol, fml, wip, wip, lol, yolo and you can&#x27;t fire them or get them to change, then squash merges ftw. reply bogeholm 15 hours agorootparentI ser it the other way around - why spend time on a ‘nice’ commit history in a (smallish) feature branch when you can squash merge later.I prefer one commit to main per feature, a long with a good description on the GitHub PR.Sometimes I’ll branch out from a feature branch for the occasional and infamous ‘get CI working’ round of 10 one-line commits though, to not make it too muddy. reply gwright 14 hours agorootparent> why spend time on a ‘nice’ commit history in a (smallish) feature branch when you can squash merge later.Several reasons: * facilitates much better code review discussions * enables use of git bisect to locate bugs * allows for informative commit messages associated with the changes * communicates clearly to future self about why changes were made reply toast0 14 hours agorootparent> * enables use of git bisect to locate bugsThis is really only viable if each intermediate commit on a development branch is intended to be bug free. If that&#x27;s the standard you and your team work with, that&#x27;s fine, but it&#x27;s not usually my standard; in a development branch, I may commit things that don&#x27;t even compile, let alone work, if it&#x27;s a good point to commit. reply bonzini 14 hours agorootparentThe point of the parent comment is exactly that you should clean up the history before merging to a public branch, so that you can use bisect, even if so far you had wip wip doh wip as the commit messages. The way to get there is to have a mix of proper and wip commits. reply WorldMaker 13 hours agorootparentIf merge points are your \"known good\" points anyway you can just use the powers of the git dag and `git bisect --first-parent` in your main branch to just bisect the merge points. There&#x27;s no need for rebase&#x2F;squash and you still get useful git bisect results. reply bonzini 13 hours agorootparentAll commits are good points and potentially useful points. Was the bug in the refactoring? In the feature itself? In the resolution of merge conflicts? You can only answer if you don&#x27;t squash, and it becomes easier to fix the bug if you know the answer. reply WorldMaker 12 hours agorootparentSure, but also no one particularly wants to CI every commit inside a PR, so there is a usefulness in `git bisect --first-parent` as the \"first pass\" of known CI points (merge commits presumably from PRs) to find the \"PR that introduced the problem\" and then drill down into every smaller commit to see if you can get additional bisect information (from commits that may or may not have passed CI in the first place in development work-in-progress). reply dkubb 2 hours agorootparentI think the point the GP message is making is that, prior to review&#x2F;merge you extract atomic commits from your WIP that tell a clear, concise story of how the change was made. The reviewer has less built up context so by chunking it like this they can step through each commit one at a time.IMHO the expectation is that each commit would 100% pass CI, so if you decided to extract some commits and merge that early you can. This is especially useful when a 6 commit PR is reviewed, and the first 3 commits are fine but there is more feedback on the last three. The reviewer can split the first 3 good ones out, get them merged and whittle down the PR to the remaining three. The subsequent follow up will be less.IME team velocity goes up with this too, and it encourages small and easy to review commits like a Remove to be extracted and merged early.Since PRs are always as large or larger than commits, I would much rather have a specific commit flagged than have to wade through the whole PR diff. If the PR is not familiar to me, I want to increase my effectiveness narrowing down the cause, so I can fix it faster. reply bonzini 2 hours agorootparentprevI don&#x27;t do full CI for every commit but I do run the relevant unit tests (or all of them depending on the change and the project) and ensure that they pass. reply Terr_ 12 hours agorootparentprev> even if so far you had wip wip doh wip as the commit messagesAside, `git commit --fixup HEAD` is often better than `git commit -m \"oops, one more thing\"`, since it means you can easily `git rebase -i --autosquash`. reply watwut 13 hours agorootparentprevFrankly, people lately spend more time managing commit history then using it. Like, commit history is useful once in a year little bit, maybe, but we spend absurd amount of time trying to make it look nice. reply nilptr 13 hours agorootparentI use commit history a little bit more than that, but mostly agree. I had another dev recently give me crap about the mess of \"WIP\" commits on a feature branch because they review by clicking through the commits and my commits don&#x27;t tell much of a useful story other than I apparently did some shit and eventually it all worked.That said, I&#x27;ve also come to the conclusion there&#x27;s basically two classes of Git users: people who really understand Git and use it fully, and those of us who basically use it as a place to shove source code before quitting for the night. reply Terr_ 12 hours agorootparentprev> Frankly, people lately spend more time managing commit history then using it.At one company with a Giant Custom Enterprise App, I ended up occasionally acting as a historian for pieces of the company with bad communication&#x2F;institutional-memory, ex: \"Oh, the +5% Foo charge was because of a request 3 years ago by vice-president X, here&#x27;s the ticket number, before that it used to be +3%.\"In those circumstances--where the implementation is the source of truth for business process--a well-maintained stream of commit-messages become quite useful. reply mixmastamyk 11 hours agorootparentComment with a ticket-id would be more efficient. reply Izkata 10 hours agorootparentOn a long-lived codebase you&#x27;re going to end up with nearly as many such comments as there are lines of code. Now that&#x27;s a cluttered mess. reply mixmastamyk 7 hours agorootparentWhen&#x2F;how to comment is an art in itself, in no conflict to what I wrote.Either a short quip, doc string, or link to the full story is an accessible combo. Nothing is the correct choice for unsurprising code. reply Terr_ 4 hours agorootparentWhile it&#x27;s often said that comments should capture the \"why\" of code, I don&#x27;t usually think that ought to extend to \"cuz ticket#\" except when that ticket number is an significant bug&#x2F;limitation that explains a nasty hack.Noting each feature ticket that ever affected a line--or even just at the function level--is sort of like maintaining a few thousand incomplete micro-changelogs. Doing it \"acceptably well\" takes much more effort than grooming the commit history so that someone can click \"show change history for selected lines\" in their IDE.Plus consider all the unnecessary noise it makes for people reading the code, or reviewing a PR. replygwright 13 hours agorootparentprevNot my experience, nor my team&#x27;s experience over almost 10 years of using this approach. reply Chris_Newton 12 hours agorootparentI’m firmly in your camp on this one, but I’ve noticed that advocating a tidy history gets a lot of push-back online. I think there is an element of self-fulfilling prophecy here. If a team habitually leaves a messy history behind, that history is rarely going to be useful, so naturally the team has low expectations and sees little value in doing anything to curate it. And if a team isn’t used to making an effort to curate its history, they may assume that doing so is expensive because `git rebase -i` is scary and not something they use on auto-pilot for a few seconds at a time.In other news, our developers also create several small PRs every day but each is for an incomplete change that doesn’t stand alone so we’re never quite sure which features are finished in any given build, everyone keeps complaining about being interrupted to do code reviews all the time when the code reviews have no value anyway because they always just say LGTM :+1:, and we have targets that no more than 15% of commits should break production when CI&#x2F;CD deploys them and that we recover fully within an hour each time that happens. If only there were something we could do to improve all this… reply seba_dos1 8 hours agorootparent> I think there is an element of self-fulfilling prophecy here.This too, but there&#x27;s another thing at play as well: many developers don&#x27;t know git at all. They just memorized enough commands to let them do their work. They don&#x27;t understand what they&#x27;re doing, so they can&#x27;t reap the benefits of the tool they use. You won&#x27;t get much use of RAW photos if all you can do in a graphics editor is clicking \"auto enhance\" button. reply watwut 11 hours agorootparentprevI worked in a team where tech lead insisted on nice history. It was a lot of effort all the time and very little to no benefit.He was lead and could influence salaries, his opinion mattered. So, in real life, people rarely pushed back. That is not the same as us sharing the same opinions tho. I became more verbal about history not being useful online. reply mixmastamyk 11 hours agorootparentprevIf a strategy requires humans to be virtuous AND vigilant it is doomed to failure.I rarely use history and prefer merge&#x2F;squash, with automated CI tools, and tests. \"Why\" is kept in doc strings, comments, specs, and story tickets. Everything viewable in gitlab with automatic links. All this gets out of the critical path, every day.I submit that, if your code is so complex that diagnosing a bug is a major research project rather than moving forward with a few extra&#x2F;modified lines of obvious fix, then that is the problem to focus on. reply Chris_Newton 10 hours agorootparentIf a strategy requires humans to be virtuous AND vigilant it is doomed to failure.Sorry, but I don’t buy that. By the same principle, there’s also no point in writing unit tests or defining static types or having code reviews, all of which require thought and extra work, yet can yield considerable dividends when done even moderately well.I rarely use history and prefer merge&#x2F;squash, with automated CI tools, and tests. \"Why\" is kept in doc strings, comments, specs, and story tickets.The argument for a tidy history isn’t just about a different place to explain a change. It’s about presenting work in clearly defined, meaningful steps to other readers like code reviewers, or perhaps someone who found these commits later through `git blame` on a problematic line of code or `git bisect` after a regression. It’s about each commit representing a complete, self-contained change that could later be reverted, or cherry-picked or merged to another branch.I submit that, if your code is so complex that diagnosing a bug is a major research project rather than moving forward with a few extra&#x2F;modified lines of obvious fix, then that is the problem to focus on.Some problems have a lot of essential complexity. The code to solve them necessarily has at least the same degree of complexity. Sooner or later, there will probably be a change to that code with an unintended consequence for something else. Keeping the code and its history tidy and systematic is, IMHO, how you avoid those investigations becoming major research projects. reply mixmastamyk 6 hours agorootparentOne of these things is not like the other. (Journey vs. final destination.)As an industry we get paid primarily for 1) working software and 2) communicating with stakeholders.Tidy yet inaccessible (to non-dev) construction stories are not on that path. I would argue unit tests et al are, to ensure #1.No stakeholders? Put why into a readme, where it can be seen at a glance. Comments can reference docs.Complexity must be broken down into bite-sized chunks for a solution to be feasible in the first place, reliable in the second. i.e. skull-size limits. If there’s any code I don’t understand I rewrite it until I can. With tests of course. replym000 12 hours agorootparentprevCurating the commit history takes like 10&#x27; per PR and can easily repay in hours of work when some bug hits. Or when you want to tell the junior that wants to implement X for A, why don&#x27;t you take a look on this one commit where we implement X for B? reply mixmastamyk 11 hours agorootparentIs there something wrong with the latest version of the method? Instead of one from ~18 months ago which may not work any longer? reply m000 10 hours agorootparentI&#x27;m not sure I get you. What do you mean the \"latest version of the method\"? And why shouldn&#x27;t code from 18 months ago not work? Some minimal regression testing should be in place for production code, and it is probably also used regularly.So, yes seeing e.g. how \"CSV export for class A\" is implemented is a great guide for implementing \"CSV export for class B\". reply mixmastamyk 7 hours agorootparentMost recent. Interfaces change over time.Everything you need is in the most recent copy. Showing an old one invites errors for no benefit. reply deredede 3 hours agorootparentA commit is not a method, it is a change set potentially affecting many files. Pointing people to the commit used to implement feature A lets them understand the whole story of which components need to change (and how) to implement similar feature B in a way that pointing them to a single method or file doesn&#x27;t necessarily can.You would then typically supplement reading the commit with reading the current version of the affected code, but looking at the commit points you in the direction of the files and methods you need to look at. replyerik_seaberg 11 hours agorootparentprevIf I can invest business hours and get back minutes during an outage at 3 AM, I should do that. reply seba_dos1 9 hours agorootparentprevOf course I commit a lot of garbage commits that don&#x27;t work, it&#x27;s super useful to do so. Those never get pushed out into branches that I share to others though - why would I waste their time having them look at those?What I push out are atomic commits that make sense logically, not an external undo log of my text editor; squashing those on merge provides no benefit and only loses useful information. Squashing should happen before push, not on merge, and there&#x27;s no reason to have buggy \"intermediate\" commits recorded in your central remote branch at all. reply u801e 8 hours agorootparentprev> > * enables use of git bisect to locate bugs> This is really only viable if each intermediate commit on a development branch is intended to be bug free.git rebase has an --exec option that allows you to run a command or set of commands for each commit in the branch. You could rebase your development branch before pushing it up for review and ensure each commit passes coffee linting and tests. reply u801e 14 hours agorootparentprevAnother good reason is that having a small commit that changes just one thing is a lot easier to revert without encountering conflicts, even after other features have been committed to the main&#x2F;master branch. reply sangnoir 11 hours agorootparentprevThere are a multitude of Git workflows, and opinions on what the basic unit of change is: for some, a feature is atomic, so squash-merging feature branches is perfectly natural.> facilitates much better code review discussionsThis can be done while adding code to the feature branch> allows for informative commit messages associated with the changesI&#x27;m assuming you consider individual commits to be the basic unit of change? This isn&#x27;t always the case. Some products are not amenable to adding features fractionally> communicates clearly to future self about why changes were madeYou can do that with a squash-merge too!I&#x27;ve noticed people who work on an evergreen deployment can afford to work on a very granular, commit-level. However, if you have to support multiple production branches concurrently and often have to cherry-pick features and fixes across them, features will naturally become the basic unit of change you will find yourself gravitating towards, and will liberally use squash-merging just to keep your sanity. reply bogeholm 11 hours agorootparentprev> facilitates much better code review discussionsHmm, I usually mark PR’s as draft until ready for review, and then I expect the discussion to be about the current state, not a previous intermediate state. Easiest with small PR’s.> enables use of git bisect to locate bugsInteresting. I know _of_ git bisect, but haven’t used it as part of my workflow. Have you found it useful to bisect commits on a feature branch (which, presumably, represents unfinished work)?> allows for informative commit messages associated with the changesI find using the PR title and accompanying info in GitHub or similar to be quite informative - that should convey the purpose of the change.> communicates clearly to future self about why changes were madeSee above. Perhaps we work differently, but I find it clearer to read a git history where each commit represents a single, complete feature&#x2F;fix&#x2F;refactor instead of intermediate steps. reply marcandre 14 hours agorootparentprevThe classic example where this fails is when needing to revert something. An atomic commit for the migrations + some atomic commits for the implementation mean you can easily revert the implementation, and leave the migration intact (as should be) and add a reverse migration. reply throw555chip 12 hours agorootparentprev> why spend time on a ‘nice’ commit history in a (smallish) feature branch when you can squash merge later.Agreed, it has been standard at most shops I&#x27;ve worked in the past 8-10 years. reply ufo 15 hours agorootparentprevPrecisely, you want to keep it about one commit per feature. I think the parent comment was worried about monster merges that squash many features together. reply folmar 13 hours agorootparentprevIf things are heading towards a single commit an amend commit works fine for me. If I need a previous state I just get it from reflog. reply m000 12 hours agorootparentprev> I ser it the other way around - why spend time on a ‘nice’ commit history in a (smallish) feature branch when you can squash merge later.Squash-merge is a scourge. I&#x27;ve seen squash merged commits 30 lines long (\"try 15\", empty line, \"try 14\", empty line...). I&#x27;m not even sure if you can do anything about such commits because squash-merge is a github&#x2F;gitlab thing. So, I&#x27;m not sure if there are hooks to block it via a commit message linter.And I&#x27;ve seen people going through some intense mental gymnastics to justify avoiding squashing locally, writing a proper commit message and then merging. reply joshribakoff 15 hours agorootparentprevYou can simply ask “git log” to show you one coarse entry per Pr rather than “destroying” the more granular history reply erik_seaberg 14 hours agorootparentThis. It bugs me that people permanently throw away details of changes rather than show just the log of merge commits. reply hhjinks 13 hours agorootparentWhy would I want to keep the details? Umpteen \"tmp\", \"fix\" and \"fixed typo\" provide negative value. When I check the blame of a line, I need to see the context of the change, meaning a description of all the work that was done as part of that change, and perhaps a ticket number. Anything else is noise that actively detracts from the value of the log.It&#x27;s like 4K porn. It&#x27;s less appealing when you see everything. reply mablopoule 13 hours agorootparentNo yeah, absolutely do squash those commits, they are actually polluting the git history.The issue for me is when commit who are about adding a new value in the env file become mixed with template and responsive handling, mixed potentially with a bug fix. reply silenced_trope 12 hours agorootparentprevI disagreed with you up to this point:> Of course if you have developers that don&#x27;t do that and instead merge dozens of commits that just say wip, wip, wip, lol, fml, wip, wip, lol, yolo and you can&#x27;t fire them or get them to change, then squash merges ftw.Yes, any large organization has plenty of devs who all have their own style and preferences, for better or worse.Whoever demands they all bend to the one true way is a fascist (lol not really but you know).Just set up your CI&#x2F;CD in such a way that PRs with weird git logs get squashed into one pretty message, preferably the PR description since other devs have to review the PR it&#x27;s often given more effort. Set it up so that if things weren&#x27;t formatted the \"right way\" they get auto-formatted or a test fails and the dev says \"ah, I have to run that one task and then update the PR\".I don&#x27;t think a big organization is going to scale with developer \"evangelists\" demanding people write their commits a certain way either.conventionalcommits.org was the worst. I worked at one \"big co\" that tried to get devs to do this. Even after we had been doing it for a while, nobody ever went back to look at the history in such a way that it was worth it. We ended up throwing in the towel rather than the company trying to get all other teams to do it. reply zaptheimpaler 12 hours agorootparentprevI get why you would prefer it clean but its just too much overhead for me. I naturally make lots of changes together - especially on a complex feature, you need to build things in a \"full-stack\" way horizontally so you can test as you go. Then pulling things apart into \"clean\" atomic commits later just takes too much time and I don&#x27;t really know how to do it efficiently. reply yxhuvud 11 hours agorootparentThe thing is that when you get good at it the overhead will go down drastically. And the practice will make it easier and faster to extract small pull requests out of your main work that can be reviewed separately. reply Rapzid 10 hours agorootparentprevThere is a reason so many open source projects require squashing.Ain&#x27;t nobody got time for that shit. reply leptons 10 hours agorootparentprevIt&#x27;s like if someone said they wanted to invite you over for dinner. Then they started texting you. \"at the grocery store\". \"bought a pound of beef\". \"bought some carrots\". \"Checking out now\". \"Arrived at home\". \"Turned oven on\". \"Turned oven to 450 degrees\". \"Turned oven up to 460 based on different recipe\". \"Starting to prep the beef now\". and so on, and so on. I mean, just cook the damn dinner - I don&#x27;t need to be needled about all the steps. I&#x27;ll come over and bring the wine, and we&#x27;ll eat a meal. I don&#x27;t need to know every minor implementation detail in a commit log, to review and merge the branch. Arghhhh I have one dev on my team like this right now. I&#x27;ll have to have a talk with them. reply jdhzzz 13 hours agorootparentprevThat was me.Got fired. Kinda. I was laid off. reply jimbob45 15 hours agorootparentprevI think squash merges are a last resort heavy-handed tool for dealing with developers who refuse to clean up their commit history before merging. Most developers can do better by hand.This is too much thought put into a VCS. I don’t want to have to think about my VCS at all beyond the commit message. For all of Git’s popularity, I’ve never seen benefits that justify the absurd amount of work and knowledge it takes to perform simple actions. It’s the VCS equivalent of Scheme or emacs. reply dkarl 15 hours agorootparentIt really pays the effort back, though, when you can figure out why something was done, beyond knowing the feature it was related to, which is all you get with a squash merge. reply mixmastamyk 11 hours agorootparentWhy not document it then? In a place where everyone can read it, instead of only developers. reply Izkata 9 hours agorootparent...are you suggesting spending hours searching through documentation, hoping to possibly find something relevant, instead of just being able to run \"git blame\" to see why a specific line was changed? reply mixmastamyk 7 hours agorootparentI use blame often to blame.Code tells how, not why—the domain of specs and comments. Commit messages effectively don’t exist for non-developers.I put spec links in doc strings whenever possible. They are accessible to everyone—devs, PMs, SMEs, stakeholders that pay bills, and myself when at a web browser.Searching is not required but even if it was it would be a tiny fraction of “hours.” reply mablopoule 15 hours agorootparentprevIt&#x27;s not about VCS, it&#x27;s about code, both now and later, and context.If you need to do a workaround, or a complicated feature sometime it&#x27;s nice to explain it as a comment in the code, but sometime it&#x27;s better to put it as a comment inside the commit message. But if it&#x27;s all merged i the end, along with lots of template changes, README changes, refactor irrelevant to the current changes, then you&#x27;re losing an important way of navigating a codebase. reply Dylan16807 13 hours agorootparentprev> I don’t want to have to think about my VCS at all beyond the commit message.Fine as an opinion.> For all of Git’s popularity, I’ve never seen benefits that justify the absurd amount of work and knowledge it takes to perform simple actions. It’s the VCS equivalent of Scheme or emacs.This is just wrong. When people talk about this, it&#x27;s not about git at all.You write some code, and you make it into commits. Part of that is choosing if&#x2F;how to organize it with multiple commits, and how much effort you want to put into that. This is fundamental to using a VCS, any VCS.Or by analogy, if a lot of emacs users complain about your spelling, that&#x27;s not because emacs is overly demanding. reply mablopoule 16 hours agorootparentprevI actually hate squash merge because of all the noise it adds. Sure, the commit graph looks nicer, but it come with a terrible loss of information when doing git blame.I&#x27;m a big proponent of rebase and squash if it helps to make a commit more coherent, but we use squash merges by default in the current project I&#x27;m working on, and I die a little bit each time I try to understand what changes were related to a line when tracking down a bug. reply nerdponx 16 hours agorootparentThis is the big one for me. destroying Commit information just to keep the graph tidy is a bad idea in my opinion. It would be better if Git provided better tools for filtering the log, e.g. providing some mechanism to elide commits from parents of any merge commit other than the 1st. reply diek 16 hours agorootparent> destroying Commit information just to keep the graph tidy is a bad idea in my opinionThe commit information I see when telling teams to squash their branches on merge is not valuable.* \"fixing whitespace\" * \"incorporate review comments\" * \"fix broken test\" * \"fix other broken test\"(note, the broken tests were broken by the changes in the PR)As soon as that PR is merged those commits are worthless. And there are branches with dozens of those \"fixing X\" commits that would otherwise pollute the commit graph. reply trevor-e 15 hours agorootparentYep, intermediate commits on a branch tend to be completely worthless. I&#x27;d much rather have \"git blame\" point to the commit that contains the entire change together. reply dieselgate 15 hours agorootparentAgree strongly, it&#x27;s nice in theory to view the intermediate commits but in practice have never needed to look at them reply gray_-_wolf 16 hours agorootparentprev> * \"fixing whitespace\" * \"incorporate review comments\" * \"fix broken test\" * \"fix other broken test\"Things like this should not be standalone commits though, they should be incorporated into the previous branch by amending the original work. It takes some effort to have a useful git history, it does not just happen on its own. reply lesuorac 15 hours agorootparentSounds like six vs half-dozen. Why does it matter if somebody amends vs squashes? reply noahtallen 15 hours agorootparentprevYou can very easily rewrite your commit message on GitHub when squash merging. Since the organizations I work exclusively use squash merge, I often just update the commit to be more valuable, listing the important changes it contains. (And of course the PR in GitHub will contain the commit history of the branch that was squashed, as well as any discussion.)IMO, this is a lot simpler and easier to do than rebasing your branch to have a flawless history. reply sodapopcan 14 hours agorootparentprevI rather strongly disagree here.Having whitespaces mucks up commit, causing you to lose focus of what&#x27;s actually important.I have `git blame` aliased to `git blame -w` which ignores whitespace-only changes.You can also reblame when you come across this formatting commits. reply lelandfe 16 hours agorootparentprevThose commits would be the bathwater one casts out alongside the useful commits in using squash merges. reply diek 15 hours agorootparentIf the useful commits are the \"baby\" in your bathwater analogy, all the useful information in those commits is in the squashed commit.This assumes a branch being merged in represents one logical change (a feature&#x2F;bugfix&#x2F;etc) that is \"right sized\" to be represented by one commit. reply mablopoule 15 hours agorootparentYes, but now it&#x27;s mixed with the bathwater, and now morph into another metaphor as it become the needle in the haystack.It&#x27;s okay to have &#x27;low information&#x27; commits one can easily ignore in your history, as long as the &#x27;high information&#x27; ones stay readable and coherent. reply 26fingies 15 hours agorootparentprevYou can usually see that in whatever tool youre using anyway. Blame -> find the PR -> see commit history. reply gray_-_wolf 16 hours agorootparentprevYou mean like for example `git log --first-parent`? reply nerdponx 15 hours agorootparentTIL, thank you! Now I know for a fact that squash-mergers have no excuse and can brandish the man page at them. reply Izkata 9 hours agorootparentPeople in general just have no idea how much version control is able to do. For one example try running \"git help log\" and just tap page-down a few (dozen) times to get an idea what&#x27;s in there.For another example, you know how people hate aligning code vertically so much that linters don&#x27;t allow it nowadays, the primary reason being that if you have to change the spacing then the diffs will identify far too many lines as having changed? Both git and svn have options to ignore whitespace changes: git diff -w svn diff -x -w reply WorldMaker 13 hours agorootparentprev`--first-parent` also today works for blame and bisect. reply diek 16 hours agorootparentprev> I die a little bit each time I try to understand what changes were related to a line when tracking down a bugA change&#x2F;feature&#x2F;bug is a branch, which is squashed into a commit on your main branch, right? So your main branch should be a linear history of changes, one change per commit.How does that impact the ability to git blame? reply js2 15 hours agorootparentBecause unless it&#x27;s the most trivial of features, you&#x27;ll break it up into smaller commits which each explain what they are doing and make reviewing the change easier.As a simple example, I recently needed to update a json document that was a list of objects. I needed to add a new key&#x2F;value to each object. The document had been hand edited over the years and had never been auto-formatted. My PR ended up being three commits:1. Reformat the document with jq. Commit title explains it&#x27;s a simple reformat of the document and that the next commit will add `.git-blame-ignore-revs` so that the history of the document isn&#x27;t lost in `git blame` view.2. Add `.git-blame-ignore-revs` with the commit ID of (1).3. Finally, add the new key&#x2F;value to each object.The PR then explains that a new key&#x2F;value has been added, mentions that the document was reformatted through `jq` as part of the work, a recommends that the reviewer step through the commits to ignore the mechanical change made by (1).A followup PR added a pre-commit CI step to keep the document properly linted in the future. reply diek 15 hours agorootparentIn general I agree with you, there are absolutely times where you want to retain commit history on a particular branch (although I try to keep the source tree from knowing about things like commit IDs).I would argue that those are by far the minority of PRs that I see. As I mentioned in another comment, _most_ PRs that I see have a ton of intermediary commits that are only useful for that branch&#x2F;PR&#x2F;review process (fixing tests, whitespace, etc). Generally the advice I give teams is, \"squash by default\" and then figure out where the exceptions to that rule are. That&#x27;s mainly because, in my opinion, the downsides of a noisy commit graph filled with \"addressing review comments\" (or whatever) commits are a much bigger&#x2F;frequent issue than the benefits you talk about. It really depends on the team. reply js2 14 hours agorootparent> As I mentioned in another comment, _most_ PRs that I see have a ton of intermediary commits that are only useful for that branch&#x2F;PR&#x2F;review process (fixing tests, whitespace, etc).Right, but that&#x27;s only because developers don&#x27;t amend and force push their commits to the PR branch as they receive feedback. Which is largely encouraged by GitHub being a terrible code review tool.To me, git is part of the development process, it&#x27;s not an extra layer of friction on top. So I compose my commits as I go. I find it helpful for recording what I&#x27;m thinking as I write the code. If I wait till the very end, I&#x27;ll have forgotten some important bit of context I wanted to include. So during the day I may use the commits like save points. But before I push anything I&#x27;ll often check out a new branch and create and incremental set of commits that have the change broken down into digestible pieces. And if I receive feedback, I&#x27;ll usually amend those changes into the PR and force push it.I&#x27;d like to add that I spend a lot of time cleaning up tech debt. And I deal with a ton of commits and PRs that don&#x27;t explain themselves. So I&#x27;m really biased toward a clean development workflow because I hope to make the lives of those who come after me easier.I was also trained on this workflow by being an early git contributor and it had extremely high standards for documenting its work. There&#x27;s a commit from Jeff King that&#x27;s a one line change with about six paragraphs of explanation.There&#x27;s no right answer here. I value the \"meta\" part of writing code. Not everyone does and that&#x27;s okay. reply throw555chip 12 hours agorootparentWhen the word \"force\" is involved, it&#x27;s time to take a step back and re-evaluate things. reply js2 9 hours agorootparentIt&#x27;s due to GitHub lacking change set support. With Gerrit, force pushing isn&#x27;t required. reply Izkata 9 hours agorootparentprev> only useful for that branch&#x2F;PR&#x2F;review process (fixing tests, whitespace, etc).I have had bugfix cases where, digging through the repo history, both of those examples accidentally introduced the bug (the first because the person who made the original change didn&#x27;t completely understand a business rule so it changed both the code and the test, the second because of a typo in python that only affected a small subset of the data). Keeping the commit separate let me see very quickly what happened and what the intent actually was. reply mablopoule 15 hours agorootparentprevBecause now instead of having a line changed within a granular level of changes, it&#x27;s lost with the other changes from the same feature branch, which is a more macro level. So if a change in config is needed for the feature, the part when this config change actually need to be handled, or would impact the data-flow is harder to evaluate now that you mix it with template changes, style changes, new interactions needed for the users, etc...EDIT: On top of that, there&#x27;s usually a bit of &#x27;related&#x27; work you need for a task, by example when you find an edge case related to your feature, and now you also needed to fix a bug, or you did a bit of refactoring on a related service, or needed to change the data on a badly formatted JSON file.Unbeknownst to you, you added a bug when refactoring the related service, a bug that is spotted a few months after, only on a very specific edge case. If the cause is not obvious, you might want to reach for git bisect, but that won&#x27;t be very useful now that everything I&#x27;ve talked about is squashed into a single commit. reply diek 15 hours agorootparent> EDIT: On top of that, there&#x27;s usually a bit of &#x27;related&#x27; work you need for a task, by example when you find an edge case related to your feature, and now you also needed to fix a bug, or you did a bit of refactoring on a related service, or needed to change the data on a badly formatted JSON file.I agree that&#x27;s related work, but I&#x27;d argue that work doesn&#x27;t belong in that branch. If you find a bug in the process of implementing a feature, create a bugfix branch that is merged separately. If you need to refactor a service, that&#x27;s also a separate branch&#x2F;PR.That&#x27;s actually the most common pushback I get from people when I talk about squashing. They say \"but then a bunch of unrelated changes will be lumped together in the same commit\", to which I respond, \"why are a bunch of unrelated changes in the same branch&#x2F;PR?\" reply mablopoule 14 hours agorootparentI agree with you in principle, but it&#x27;s usually because of process and friction. In the place I&#x27;m working right now, that would result in days lost as I need to create a new Jira ticket, which obviously require a team meeting for grooming (because Agile!), and then going after colleagues so that the PR is accepted, which best case still need for CI&#x2F;CD pipeline to finally deploy, and then merge it to the dev branch, and finally rebase the current feature branch... and all this multiple times. reply joshribakoff 15 hours agorootparentprevBecause sometimes a PR touches more code than a single commit, and you lose the more granular context surrounding the more granular changes. You can always ask git to make the log more coarse, but once you “destroy” the granular history it is for all intents and purposes gone. reply WirelessGigabit 16 hours agorootparentprevMe too. I care a LOT about provenance. And squash merge completely breaks that.When my branch is up to date with `main` I can build an artifact, fast forward merge that branch into `main` and RETAIN the artifact, and merely update its tags to mark it as `merged` in.With a squash I lose that information.Now, GitHub does not allow me to do a fast-forward merge but I can still trace the 2 commits that are the parent of the resultant merge, and find the artifact based on that, and retag. reply Lio 14 hours agorootparentprevGit blame confuses people even without squash merges.I&#x27;ve seen people forget to go back more than one commit and then blame the person who last indented a file instead of going back to the commit that actually wrote the code many times. reply sodapopcan 13 hours agorootparent> I&#x27;ve seen people forget to go back more than one commit and then blame the person who last indented a file instead of going back to the commit that actually wrote the code many times.I default my `git blame` to `git blame -w` which ignores whitespace commits. Though knowing how to jump back commits should be required knowledge. reply erik_seaberg 14 hours agorootparentprevWe shouldn’t tamper with code we don’t actually need to fix, it’s not a good use of time and it makes history less useful. Just because it doesn’t look like I wrote it doesn’t make it wrong. reply Lio 13 hours agorootparentI’m thinking of situations where the surrounding structure of the code has been changed to correct a problem.That’s done by an automated tool. Correction of indentation is just a byproduct.I don’t consider that “tampering”. reply WorldMaker 13 hours agorootparentYou can deal with that through tooling.In a lot of my work I call those types of automated tool commits \"wrench\" commits personally and even have a simple shell script to help automate committing them. In my case I prefix the command line with a wrench emoji. At that point it&#x27;s very obvious in git blame that if a line starts with a wrench it was last touched by an automated tool of some sort.You can also very easily at that point grep your git log for wrenches to dump commit hashes into a git-ignore-revs file and automate that part too so that those commits don&#x27;t even show up in git blame at all. reply johnsbrayton 16 hours agorootparentprevI do squash merges but keep the feature branches. So after determining that I made a change as part of a big pull request, I can then look at the commit&#x2F;blame history for the pull request source branch if necessary. reply mvdtnz 16 hours agorootparentI&#x27;m guessing you don&#x27;t work on large projects. This would create an outrageous amount of noise in a busy repository. reply matijsvzuijlen 16 hours agorootparentprevThis means having to keep these branches around cluttering up everything, and makes git bisect a lot more complicated. reply skybrian 16 hours agorootparentprevThis all depends on the project. Sometimes you don’t look at history all that much. Sometimes the loss of information is acceptable. reply Izkata 15 hours agorootparentIf you don&#x27;t look at history much, why would you care about keeping it \"clean\"? Just keep the truth of the changes in the history for those of us who do use it, and you can continue ignoring it. reply sodapopcan 15 hours agorootparentprevNo one is mentioning: $ git log --mergesNow you can see your features in a nice history and also have added benefit of seeing intermediary commits. Pro tip: merge commits aren&#x27;t required to use the canned \"Merge branch into...\" message, you can give it any message you want, such as \"feat: ...\" or whatever your convention is.I hate that branch squashing has become something of a defacto. I actually do rewrite my history and often add context to my commits. `git blame` can be an incredibly useful tool to get context about a given small change. Getting a massive diff for a whole feature is much less so, especially since you can just look at the diff of the merge commit. reply dahart 16 hours agorootparentprevWhat I think I see these days is squash merges being used lazily to avoid having to do anything to build a clean history with clearly semantically delineated commits. Squash merges are good compared to an alternative where people check in super messy noisy branches, but they unfortunately have a big downside because squash merges can make bisecting and history spelunking more difficult, when the branches that are squash merged were big. reply sidlls 16 hours agorootparentWhat is a \"semantically delineated commit\"? What is a \"clean history\"? Why are these two things important? reply FeepingCreature 16 hours agorootparentNot parent: there are technical commits, such as \"fix review\", \"fix jenkins\", \"fix typo\" etc. Those don&#x27;t delineate a particular feature but a fix for a problem that arose from the workflow. This ends up with a history of \"big feature commit that is wrong in three trivial ways\" + \"fix 1\" + \"fix 2\" + \"fix 3\". Of those, \"big feature commit\" is the important one, but \"fix 3\" is the only working one. This is clearly silly; you should pretend you were perfect from the start and squash \"fix 1\" through \"fix 3\" into \"big feature commit\". Your typos and brainfarts are not of historical relevance. reply sidlls 15 hours agorootparentPerhaps I&#x27;m missing something, but I don&#x27;t see how your comment answers my questions. Do you mean that a \"clean history\" is one without \"fix 1\", \"fix 2\" and \"fix 3\"? Or is that a \"semantically delineated commit\"? reply ruds 15 hours agorootparentA clean history is one where there is a single commit, \"big feature commit\", that produces a worktree that is the same as the one produced by \"fix 3\" in the \"unclean\" history. reply nomel 15 hours agorootparentHow is this possible, while sharing code? Doesn&#x27;t this require that pushed code is perfect? What about everyone else working on the same code? Do they wait until you&#x27;ve reached perfection? Or, do you squash the branch once it&#x27;s complete, with the assumption that there&#x27;s no other development on&#x2F;from that temporary branch (I envy you if so)?(I ask these questions fully assuming I&#x27;m doing it wrong.) reply dahart 15 hours agorootparent> Doesn&#x27;t this require that pushed code is perfect?We aren’t talking about pushed code. We are talking about cleaning up the local commit history before pushing it into a shared branch. reply sidlls 10 hours agorootparentAnd that&#x27;s the one--and only--reasonable use of rebasing, to squash commits from a branch before merging into main. If engineers find themselves using rebase in any other context than squashing a merge, it&#x27;s time to re-evaluate the processes&#x2F;culture around workflow. reply nomel 13 hours agorootparentprevWhat about the context where one works with other people, while sharing code? reply dahart 12 hours agorootparentWhen working with published&#x2F;shared branches with other people, the advice with git has always been that history is history and not to be changed after publishing, unless there is an emergency like a security incident.Aside from that we need might need to clarify what the question is. With shared code & git, it’s nice to use a branch & merge workflow, and it’s nice to make incoming merges as clean &#x2F; nice as you can do the resulting history is as smooth as it can be while capturing what happened at a reasonable granularity. These are today’s conventions though, and it’s really up to the team to decide how to balance shared work, and what people feel are the most important workflows and tools. reply hhjinks 13 hours agorootparentprevYou fix your local tree before sharing it. Alternatively, you can communicate with your team and tell them they&#x27;ll need to run git fetch && git rebase -i origin&#x2F;main to drop your erroneously merged commits. replyIzkata 15 hours agorootparentprevThey are. Or at least can be. Typos I probably agree with, but I&#x27;ve seen plenty of logic bugs introduced in those \"fix\" commits and keeping them separate from the big one is useful when figuring out what was supposed to happen. reply smw 15 hours agorootparentprevA bunch of wip wip2 wip3 commits don&#x27;t add any value, and make the log harder to read. But if you break a bigger PR down into \"added feature x\", \"tests for feature x\", \"refactored y to support x\" -- the commits are easier to read and provide valuable \"why\" history when you&#x27;re trying to figure out what happened two years later. reply sidlls 15 hours agorootparentThat&#x27;s more about the contents of the merged commits than anything else. Modifying the commit message(s) fixes that, as long as that&#x27;s what the commits actually did.Aside from that, how are \"a fix for a bug\" style commits not \"clean\"? If merge 123 into master contains a bug that is fixed in a future merge 1234, it doesn&#x27;t seem \"dirty\" to me; quite the opposite actually, as it tracks what actually happened.Now, \"wip\" style commits shouldn&#x27;t be on whatever main branch everyone is working on: that&#x27;s what branches are for. And if everyone is just working off the main branch and committing directly to it, that&#x27;s an organizational deficiency; not one that VCS can solve. reply dahart 14 hours agorootparentModifying commit messages is rewriting history, right?> “wip” style commits shouldn’t be on whatever branch everyone is working onAgreed! We aren’t talking about rewriting shared branch history, we are talking about removing the “wip” commits made hastily and locally before pushing them. Sounds like we agree! reply sethammons 10 hours agorootparentprev> they unfortunately have a big downside because squash merges can make bisecting and history spelunking more difficult, when the branches that are squash merged were bigcan you help me understand this? It is the exact opposite of my experience. The flow I see is: bug reported, write a git bisect test, identify the feature that introduced it, reach out to that developer&#x2F;team.This is allowed by squash merges. When I&#x27;ve seen these more \"clean\" histories, they have commit points that wont even compile or have runnable tests causing git bisect to fail.> branches that are squash merged were bigit must be this - how big are your merges? All the projects I&#x27;ve worked on strive for smaller PRs. Large PRs are usually broken up into smaller pieces. Large PRs are an anti-pattern. reply dahart 8 hours agorootparentI maybe don’t know what you mean about “clean histories”. Speaking for myself, I always expect a history that’s called “clean” to compile error-free at every commit, unless otherwise noted; one of my personal criteria for calling history ‘clean’ is that efforts are made to keep the main branch up and running for every commit.> how big are your merges? […] Large PRs are an anti-pattern.Depends, but they sometimes on occasion can get pretty big, if there’s a bit refactor and&#x2F;or multiple people in the branch. Small enough PRs are a nice goal - it’s a goal that might agree with and exist in part because squash merges on large PRs lose too much. It’s just the real world routinely gets in the way. It’s very easy for someone who needs to do an ‘atomic’ refactor to touch a ton of files. It’s very easy for a planned feature to end up way bigger than intended. You can’t always keep PRs small or enforce it on other people. Sometimes stuff happens, and when it does, sometimes squash merging feels less good than merging a branch with multiple commits. The good news is that it’s always optional. The bad news is that I can’t necessarily babysit or dictate what others do, and some people prefer squash-merging to spending any time doing cleanup on a messy branch. reply paulddraper 16 hours agorootparentprevSquash merges are rebases. reply ksenzee 16 hours agorootparentOnly metaphorically, maybe. You can squash merge in lots of cases where a rebase will fail. reply Izkata 15 hours agorootparentThey are essentially rebase+squash, despite the name. There is no actual merge taking place.And for that matter, you&#x27;d manually do a squash with the interactive rebase tool anyway (\"git rebase -i\"). reply ksenzee 14 hours agorootparentImagine a feature branch where someone has been keeping it up to date by merging main into it regularly. Now the feature is ready to go into main. You can easily `git merge --squash` that branch into main. You can likely do the same thing manually (as you point out) by running `git rebase -i` if you squash all the commits in the branch. But you’ll never manage to do a genuine rebase, where every commit in the branch gets turned into a clean non-merge commit onto main. reply paulddraper 14 hours agorootparentFWIW I consider `git rebase -i` to be a \"genuine rebase\" reply ksenzee 9 hours agorootparentI do too, except in cases where it’s being used simply as a more complicated UI for `git merge --squash` and there’s no actual “generate a diff and apply it to a different base commit” going on. reply paulddraper 8 hours agorootparentI think we have a rose by any other name situation.I call that a rebase. reply Dylan16807 3 hours agorootparentThat&#x27;s a badly fitting analogy because there&#x27;s only one type of flower involved. In this situation, they&#x27;re saying that most things you might do with \"rebase -i\" are rebases, except for one.I&#x27;ll make a math analogy. Technically a rectangle is a trapezoid, but if someone says tries to draw a distinction between rectangles and proper trapezoids, it&#x27;s not hard to figure out what they mean.When rebase -i outputs a single commit, that&#x27;s a degenerate case. There are statements about rebases that are generally true but not true for that specific kind. replyrecursive 16 hours agorootparentprevJust when I thought I was starting to understand git... reply Izkata 15 hours agorootparentRebase essentially means \"create new commits out of old commits\", the original use being to \"move\" a set of commits from one branch to another (think of the name as meaning, to change the base these commits started from).There&#x27;s a few special cases that have their own names, a common one is when you amend a commit - to do that manually you&#x27;d make a new commit, then use interactive rebase to squash the two commits together into a new one (or, use the \"fixup\" command available in that tool, which is a squash that automatically picks the first commit message instead of asking for a new one).Squash merges will squash a whole branch into a single commit, rebasing it onto the target in the process, and then fast-forward the target to the new commit. It&#x27;s a tightly controlled use of rebase, and can be thought of a bit like how \"for\", \"foreach\", and \"while\" loops are a tightly controlled use of \"goto\", an abstraction built on top of a far more flexible tool. reply jawns 15 hours agorootparentprevMy rule of thumb for commits is that they should be of a size and scope suitable for cherry-picking. So, maybe I&#x27;m working on a small feature that entails three changes, and each of those three changes is useful in and of itself and could conceivably be cherry-picked by others. I would create three separate commits, generate a PR with all three, and merge in the work. Sure, I could squash merge and end up with one merge commit encompassing all three changes, but now none of those three changes is cherry-pickable. reply mcv 14 hours agorootparentprevThey also cut down the signal. reply Lio 14 hours agorootparentprev> Squash merges cut down the noise considerably.They do but they have their own issues. e.g. having to delete local branches using git branch -D instead of git branch -d and getting the protection from deleting unmerged work.I still agree that on balance annoyances like that might still be worth putting up with for larger teams with mixed skill levels. reply throw555chip 12 hours agoparentprev> For me, even though rebasing comes with some trappings, I still greatly prefer it to the alternative, which is to have merge commits cluttering up the commit history.The purpose of history is to remember. Rewriting history, whether git or in life, is bad; outside of the context of don&#x27;t use it on public repos. Such advice is similar to saying, only point the shotgun away from you when firing. If you have to remember such a rule, it&#x27;s best to avoid it. reply 0x6c6f6c 11 hours agorootparentBut in unmerged branches, you aren&#x27;t rewriting history, you&#x27;re starting your work on a more recent commit in history. reply ozim 11 hours agorootparentShush, don’t say those things because maybe people discussing merge vs rebase will realize they don’t discuss but just talk side by side.One and the other does not care what is the context and what they discuss but apparently each one just knows better.I also don’t mean those specific users - but in general any git discussion I saw for last 10+ years. reply oehpr 10 hours agorootparentprevA history you can&#x27;t understand is a history you can&#x27;t remember. reply leptons 10 hours agoparentprevI don&#x27;t mind merge commits, it&#x27;s the 100 tiny individual commits some developers seem to like to do that really clutters things up. Yes, I know, git squash is a thing, but not committing until the feature is working and ready to commit is also a thing. reply lolinder 10 hours agorootparent> not committing until the feature is working and ready to commit is also a thingThat leaves you prone to losing work if you have a false start that you need to back out of. I prefer to commit early and often on my private branches, then before submitting a pull request I clean up the history to where there are a few good commits that form useful, standalone chunks (ideally the test suite fully passes on each commit). reply leptons 5 hours agorootparent>That leaves you prone to losing work if you have a false start that you need to back out of.Hasn&#x27;t happened to me in over 20 years of using version control. I always keep moving forward, there&#x27;s really never been a need to go back to a previous commit that hitting crtl-Z wouldn&#x27;t accomplish just the same. If I wanted to try a new direction I&#x27;d just clone the repo again and do the work there. Littering the git history with dozens of superfluous commits just seems pointless. Having to stop and think about writing a commit comment is also just a waste of time - in aggregate it wastes a lot of time. It adds a lot of churn to a workflow for something that may never really be of any value. reply richbell 16 hours agoparentprev> I still greatly prefer it to the alternative, which is to have merge commits cluttering up the commit history.GitHub recently added a feature that prompts people to update their branches via merge. It&#x27;s frustrating because every PR now had dozens of merge commits polluting the history. reply spankalee 16 hours agorootparentA PR with merges is fine by me, it lets me see how the PR has evolved.What I want is for GitHub to track changes between sets of commits in a PR so that you can do most of the review with merges and \"address review comments\" commits, and then rebase into well organized, logical commits and review that those have the same diff as the messy history after a force push. reply adhesive_wombat 16 hours agorootparentAt least Gitlab does that when you push a new commit (force or not) to the branch: it&#x27;ll show a list of value of the branch head commit and you can diff between them. reply smw 15 hours agorootparentSo does Github, but it breaks if you fix via rebase and push -f. Gerrit and some other competitors manage this better. reply richbell 16 hours agorootparentprevThe problem is PRs that haveInstead of letting it go, maybe we should have more discipline and organization in our lives and not less.Again, both sides could argue that they&#x27;re the ones with more discipline.> The point is to make it possible to debug later, via bisect, or show, or even just a diff.This sounds anti-revisionist.> The point is to make the workspace clean for the next guy.This is one of the most common pro-revisionist arguments. reply corytheboyd 10 hours agorootparentprev100% agree, but nobody gives a shit, and I’ve learned to just let it go. I’ve been in so many meetings, seen so many PSAs, and you know what happens every single time? Nothing. Maybe a couple people learn what interactive rebase is for the first time, try it once, say “it lost all my code” and never try it again. Good luck explaining ref log in these cases. reply b450 15 hours agorootparentprevDid you notice, though, that rebase advocates use very \"emotive\" terminology when talking about git history? Like it&#x27;s a subject they care about? Seems awfully touchy feely. reply mostlylurks 12 hours agorootparentYou say that like it&#x27;s a bad thing. If there are two groups of people, and one of them is indicating (via words or behavior) that they don&#x27;t care about something all that much and the other is indicating that they do care about that something quite a bit, why would I ever listen to the ones that don&#x27;t care? It is almost tautological that the group that actually cares is going to have the more persuasive arguments and is thus far more likely to be right than the apathetic group. reply duped 14 hours agorootparentprevI don&#x27;t know if \"emotive\" is the right word, because to me this whole discussion is like trying to tell someone to be less sloppy because they make a mess when eating at their desk, knowing that the custodians will clean up after them. reply FeepingCreature 15 hours agoparentprev> What matters is that you end up with working systems. That a lot of change happened is just, well, what happened. It doesn&#x27;t need to be prettied up and made to look like your development occurred in a clockwork march of cleanliness. It literally does not matter unless you spend a lot of time doing git-bisect.And git blame. And git checkout to a past state. It \"doesn&#x27;t matter\" only if ease of understanding your project history doesn&#x27;t matter. reply jjeaff 15 hours agorootparenthow often is \"understanding your project history\" something that actually comes up for you? In all my years of working with projects in git, I will occasionally look at my history to help me find a change that may have led to a bug, but it really only comes up for me once or twice a year and even then, it is rarely an extensive deep dive and never very far back in time. reply Groxx 13 hours agorootparent>how often is \"understanding your project history\" something that actually comes up for you?Frequently, for any long and complex project. Large amounts were written by people no longer working on it, and the history of how things came to be can help fill in documentation gaps and make intent clear.By \"frequently\" I mean something like \"I check history for about 2&#x2F;3rds of bug fixes, and 1&#x2F;4 of adding features\" to understand the surroundings better, when writing or reviewing. Anything that makes that better saves me hours per week.It catches and prevents more than enough subtle issues to be worth the effort. reply mixmastamyk 10 hours agorootparentI&#x27;m on a long and complex project. However most of previous folks were not very good and one reason I&#x27;m here to fix it. Their history is not particularly useful except to giggle at. reply duped 15 hours agorootparentprevDo you work with other people or on large codebases at all? It comes up pretty much weekly for me. reply nomel 15 hours agorootparentprev> I will occasionally look at my historyIt&#x27;s others history that I&#x27;m usually interested in. I can easy follow the small diffs of individual commits, but have a much harder time grokking a wall of red and green. reply erik_seaberg 14 hours agorootparentprevWhen I’m on call and discover at 3 AM that we’re doing something weird, I need to know whether we meant to do that and especially why. In theory you could write all that down, but the people who aren’t doing that in git also won’t do it outside of git. The more you write down, the less likely it is that I need to page you to ask WTF. reply tomjakubowski 15 hours agorootparentprevI read git commits in either the repo I am working on or a dependency repo almost every day reply icedchai 10 hours agorootparentprevIt comes up often enough. I run \"git blames\" frequently to figure why something odd looking was introduced. It may not be a bug, but a WTF. This is in an environment with few code reviews, despite my attempts to introduce them. It is frustrating. reply pjc50 13 hours agorootparentprevSometimes. Once every few months. Sometimes it conveys useful information. Sometimes it just hits the \"product imported from previous VCS a decade ago\" commit. reply suzzer99 14 hours agorootparentprevSame, and I&#x27;ve worked on some large, long-running projects. But never at the scale of a big tech company.I&#x27;ve used git bisect the few times I&#x27;ve had to diagnose an issue that wasn&#x27;t detected immediately, which gets you down to the exact granular commit. reply Nullabillity 12 hours agorootparentprevEvery time I try to blame or bisect and just end up stuck on an irrelevant megacommit I curse the Git maintainers that don&#x27;t have the backbone to just get rid of --squash.Every time I try to review a PR and the bookmark resets because they decided to force push I curse the Git maintainers that don&#x27;t have the backbone to just get rid of rebase. reply mmazing 15 hours agorootparentprevI never use rebase, and I&#x27;ve never once had trouble understanding who did what where and when, even in a large project with 500+ users.That being said, after reading this stuff, I may start using it on my local branches to clean up multiple commits into one tidy one, but that&#x27;s about it. reply Hackbraten 9 hours agorootparent> I never use rebase, and I&#x27;ve never once had trouble understanding who did what where and whenAnd a well-organized commit can also tell you the “why.” reply khalilravanna 15 hours agoparentprevI think if the definition of a “good history” is “clean and not messy”, then yes I agree that’s pointless. If the definition is “a clear ability to see what changes were made, by who, and most importantly why” I think that’s incredibly necessary and would even go so far as to say it’s naive at best to not support.The amount of time that has been saved in my life by someone leaving an explanation in their commit (for some weird edge case or context I’d have no way of gleaning because they’ve since left the company) is SO much more than the extra time I’ve put in to make sure the history has this extra info in it. reply __MatrixMan__ 16 hours agoparentprevWhat&#x27;s worse, the desire for cleanliness ends up making things like `git bisect` less useful.If I had a bad day and introduced something stupid, I want a bisect to point me a the code I wrote on that bad day. If you squash liberally, perhaps because you want each commit to correspond with a release-note, you&#x27;re going to lose that debugging granulariry. reply orblivion 12 hours agoparentprevThis is such a strange thing to say. I&#x27;d be curious if you feel the same way about cleaning up your code, or cleaning up your room. I think you have an unfair advantage in this argument because it&#x27;s difficult to defend such intangible benefits. We have to resort to making up logical explanations, or sounding unhinged or emotional as you suggest.But it&#x27;s simply intangible. My instinct tells me that it&#x27;s helpful and that&#x27;s okay. I don&#x27;t owe anyone a justification for how I organize things, and there&#x27;s nothing controversial about this. (Or maybe I could even come up with a logical example of a benefit, but that&#x27;s a trap I&#x27;m not going to fall into) And a lot of people agree, and they know what I mean, so it&#x27;s not merely an individual preference. If I have to work with someone who has strong preference against it I&#x27;ll worry at that point about negotiating. reply watwut 12 hours agorootparent> I&#x27;d be curious if you feel the same way about cleaning up your code, or cleaning up your roomVery genuinely: I do not care at all whether you clean your room starting from left and continuing to right. Or, starting from doors and continuing toward window. Or whether you clean it in a random order. I also do not care about whether you clean every Friday or whenever you feel like. That is the equivalent of git history. Because this excessive care about git history is just that - insisting that room is cleaned from left to right as if any other order was an issue.The reason why it is hard to defend the tangible benefits of this or that git history strategy is that there are very little benefits. reply orblivion 12 hours agorootparent> I do not care at all whether you clean your room starting from left and continuing to rightBut you didn&#x27;t say that you don&#x27;t want it clean. It sounds like you&#x27;re talking about how it&#x27;s organized rather than whether it&#x27;s organized.> The reason why it is hard to defendI&#x27;m talking about intangible benefits and no that&#x27;s not the reason. Intangible benefits are inherently difficult to defend in words. Citing this as evidence of anything is akin to a debater&#x27;s trick. reply watwut 12 hours agorootparentInsisting on highly organized git history is like insisting on particular order of cleaning. History is not the product itself. It is not the code itself. It is less important and matters only a little.In the rare situation when I have to read it, I am perfectly ok looking at previous commit too or whatever. It is still less overall work then what people describe in here.Even with room, I do not want my room infinitely clean. I am ok when books are not ordered by height and color for example. I do not need t-shirst ordered by color either. reply htamas 14 hours agoparentprevA clean git history on a pull request also makes it easier for the reviewer to understand your code. Small, concise commits will tell the reviewers about your train of thought or what issues did you run into, making it easier to pick up the context. I start with every code review by looking at the commit history.I prefer not to have squash commits in our team for this reason. It makes master look good, but usually nobody ever looks at the master commit history first, they look at the merged pull requests. However, everybody must look at the commits you made in a pull request. If you have squash commits, you are encouraged to have messy commit history in your pull requests, leading to meaningless commit messages and even large commits (causing other problems...).IMO the only advantage of squashing is that it makes it easy to roll forward when you accidentally deploy something that causes problems. reply suzzer99 13 hours agorootparentYeah we use pull requests for the coarse-grained stuff and leave the small commits, which should also have good comments, intact. Maybe other shops use pull requests differently. reply tormeh 11 hours agoparentprevThe git history of a project is the main source of knowledge on that project, once the people that wrote it are gone. The git history answers questions such as \"wtf is that supposed to do?\", \"what&#x27;s this code connected to?\", and \"why did they do it that way?\". You can use other kinds of documentation, but the git history is always there, so it makes sense to make it semi-useful. reply anonyme-honteux 16 hours agoparentprevIt’s ego reply davedx 16 hours agoprevI’ve never understood the tradeoff of rebasing, squashing or otherwise “keeping a clean history”. It always seemed like tons of sometimes highly error prone work (sometimes you can wipe out a colleague’s work with it! Wtf!), for almost no gain (why does it matter that the git history is “clean”?). reply nemetroid 16 hours agoparentIt matters because when I:* use filtering commands like \"git log -S\"* press the \"annotate\" button in my IDE and can see which commit introduced each line* run \"git bisect\"* use \"tig\" to drill down through the history of a file (shortcut \",\" is \"move to commit preceding current line&#x27;s blame commit\")...every step of the way, I get a meaningful description of why a change was made and what other diffs were necessary to achieve that change. And not just \"fix\", \"bug\", \"PR commments\". reply WorldMaker 13 hours agorootparent* `git log --first-parent -S`* `git blame --first-parent`* `git bisect --first-parent`* At least one \"tig-like\" with a --first-parent first UI: https:&#x2F;&#x2F;github.com&#x2F;kalkin&#x2F;git-log-viewer reply nomel 14 hours agorootparentprev> * press the \"annotate\" button in my IDE and can see which commit introduced each lineIn PyCharm, I can see which commit introduced each line, regardless of branching. Same with drilling down through a files history. Is this an IDE limitation you&#x27;re seeing?> every step of the way, I get a meaningful description of whyIsn&#x27;t this more about commit messages, than anything else? reply nemetroid 12 hours agorootparentThe context of my comment is the usefulness of a clean history, not about merging vs rebasing.> Is this an IDE limitation you&#x27;re seeing?I&#x27;m using Jetbrains too. reply tuckerpo 16 hours agoparentprev> why does it matter that the git history is “clean”?Makes reviewing a set of changes prior to a merge much easier. It&#x27;s nice if there&#x27;s a 1:1 correlation between a commit message and the actual patch contents.Im sure you&#x27;ve dealt with the case of reviewing a colleague&#x27;s changes with a commit message like \"Enable logging in foobar module\" and the patch is actually enabling foobar logging and a bunch of other stuff.This makes bisecting your git history to identify and fix bugs much more difficult.If the git history is clean, you can just read the commit messages and implicitly trust the developer if clean git hygiene is in place (as opposed to actually needing to read the whole diff on a per-commit basis to find out what _actually_ happen at commit XYZ, despite it&#x27;s message). reply ajkjk 16 hours agoparentprevNever understood why you wouldn&#x27;t want it clean. There&#x27;s no benefit whatsoever to it being messy and it&#x27;s a liability for a lot of reasons, whereas the clean version is free and easy and makes everything you do that interacts with git history simpler. reply btilly 16 hours agorootparentThere is a giant benefit to it being messy. And that is that the mess is the actual history.Every time you do a git rebase, you are literally asking your source control system to lie about history. If you mess up, and you eventually will, you&#x27;re then forced to manually figure out what the history really was despite being lied to. If you mess it up, well, good luck.I used to work at a company where someone (we never figured out who) in another group would rebase every few weeks. We didn&#x27;t find out about it until their stuff was pushed then released. The result was that features which we&#x27;d written, QAed, and released to production would simply disappear a few weeks later. With no history suggesting that it ever existed.Have you ever been pulled off of a project to go fix a project from a month ago which has disappeared from source control? You don&#x27;t know what happened, you no longer have context, you&#x27;ve just got complaints because your stuff no longer works.Is your desire for a \"clean history\" worth potentially creating THAT disaster for other developers on your team??? reply foobarbaz33 15 hours agorootparent> the mess is the actual history.The true history is not recorded in your normal commits either. Every time you modify your source buffer, that is the true sequence of events. This truth is lost already as you undo&#x2F;rework things before you commit. You&#x27;re ALWAYS manipulating and telling a false story of history whether you realize it or not.Commits are a tool that give stronger backup&#x2F;undo protections over simple file saves and in-memory editor undo lists. Just because you happened to save your work in a commit doesn&#x27;t mean it should be instantly be regarded as holy history. Not anymore so than if you simply saved the file.I think the bar for \"holy\" history should be whether it is published to a shared branch. reply btilly 15 hours agorootparentI believe that you are confusing \"full history\" and \"true history\".Every single point in the commit history represents an actual state of a repository at a specific point of time, along with the information of which point or points were next before it. This is all part of the true history.This is not a full history - you don&#x27;t have every keystroke, abandoned commit, switch between branches and so on. But nothing that you&#x27;re being told is wrong.As soon as you do a rebase, you&#x27;re rewriting history. You&#x27;re claiming that there were specific points of time with specific states that never actually existed. You&#x27;re losing information about points of time and specific states that actually existed, which someone once considered important enough to do a git commit over.The difference becomes important if that someone, which at a previous job was me far more often than I would like, tries to go back to the historical commit. And finds that it is gone without a trace. reply foobarbaz33 14 hours agorootparent> As soon as you do a rebase, you&#x27;re rewriting history.Agreed. But the rewrite occurs in your private branch. It&#x27;s history is just as private as the undo list in your editor. No one cares about what&#x27;s going on in your editors undo list. And by the same logic they shouldn&#x27;t care about commits in a private branch.> You&#x27;re losing information about points of time and specific states that actually existedIf you avoid rebase, then you end up \"rebasing\" without rebasing. You \"squash\" intermediate states by never recording them to begin with.Failing to record history is not superior to squashing it.> And finds that it is gone without a trace.I don&#x27;t have the details, but it sounds like someone rebased a public branch. Yes that is bad. But it&#x27;s sort of like saying we shouldn&#x27;t drive cars because someone chose to drive the wrong direction down a 1 way road. reply Nullabillity 12 hours agorootparent> Agreed. But the rewrite occurs in your private branch. It&#x27;s history is just as private as the undo list in your editor. No one cares about what&#x27;s going on in your editors undo list. And by the same logic they shouldn&#x27;t care about commits in a private branch.The rebase becomes part of the public branch eventually, inflicting your lies on everyone else.> If you avoid rebase, then you end up \"rebasing\" without rebasing. You \"squash\" intermediate states by never recording them to begin with.If only Git had a third alternative, a way to... entangle two diverging branches of history without destroying or rewriting either. You could say it would be a bit like a car merging into a highway.> Failing to record history is not superior to squashing it.\"We don&#x27;t know\" is at least an honest statement. Claiming that you do but then making up some nonsense is something that the LLMs do enough of already. reply foobarbaz33 9 hours agorootparentI think the crux of the argument is what you think about private git commits. You may think of them as \"holy\" history. Assuming the commits are still private, I give them no more prestige than the editor&#x27;s undo log.What do you think of the editors undo log? It&#x27;s a very real historical log. Should it be treated as \"holy\" history too? If not, what makes the undo log less true&#x2F;important than a private git commit log? reply Nullabillity 5 hours agorootparentIf you did X then Y then Z, there&#x27;s a difference between saying \"I did Y, Z, and X\" (squashing&#x2F;summarizing) and \"I did Y then Z then X\" (rebasing).Squashing is often dumb and unhelpful, because you&#x27;re now re-summarizing the points in time that you already considered worth highlighting when they happened (when you had the most context to judge them!).Rebasing is lying about the order and&#x2F;or context that those changes happened in.Your undo log is comparable to squashing, but not at all to rebasing.And then again, the first-order vs second-order summarizing distinction matters, and you already capture the second-order summary in your merge commit. Squashing is just destroying information for zero practical benefit.> privateYou keep using that word, but branches are often a lot less private than you think. Push it to get a colleagues&#x27; input on something? Congratulations, it&#x27;s now public. Created a pull request that you want to revise? Already public. reply dahart 10 hours agorootparentprevGit was never intended to capture and preserve the order of git commit commands, and you’re making incorrect assumptions that it was a goal. Maybe the git devs knew there is no useful information and no benefit to being so strict about something so arbitrary, or maybe because forcing people to keep their commits in stone in whatever the first arbitrary way they were added is a big disincentive to making commits at random points in time, which somewhat undermines the point of having a version control system at all.BTW rebase produces a new commit ordering, but does not modify the old one.> You’re claiming that there were specific points of time with specific states that never actually existed.No. You are asserting intent on the part of git users and git that has never existed, you have misunderstood what git history is. The git history is not a claim that the state at that point existed during development, you are projecting your own goals that are not shared by git or git users.> You&#x27;re losing information about points of time and specific states that actually existed, which someone once considered important enough to do a git commit over.Hehe this is so full of assumption. You write it like I’m rebasing someone else’s work, but you already know I’m only rebasing my own commits, and I’m the one who decides what’s important enough to do a commit over.I like commit early, commit often. I want to make small incremental commits that don’t display to others that way and I expect to put small commits and fix ups together later into a single useful commit with only one commit message. reply dkbrk 12 hours agorootparentprev> the mess is the actual history.This argument reminds me of a scene from Yes Prime Minister [0]:> Humphrey: The minutes do not record everything that was said at a meeting do they?> Bernard: Well of course not.> Humphrey: And people change their minds during a meeting don&#x27;t they?> Bernard: Well, yes.> Humphrey: The actual meeting is a mass of ingredients for you to choose from.> Bernard: Oh, like cooking?> Humphrey: No, not like cooking. Better not to use that word in connection with books or minutes. You choose, from a jumble of ill-digested ideas, a version which represents the Prime Minister&#x27;s views, as he would, on reflection, have liked them to emerge.> Bernard: But if it&#x27;s not a true record...> Humphrey: The purpose of minutes is not to record events it is to protect people. You do not take notes if the Prime Minister say something he did not mean to say, particularly if it contradicts something he has said publicly. You try to improve on what has been said, to put it in a better order. You are tactful.> Bernard: But how do I justify that?> Humphrey: You are his servant> Bernard: Oh, yes.> Humphrey: A minute is a note for the records and a statement of action if any that was agreed upon.I think the analogy is pretty clear. A pull request does not record every single little change you made when writing it. You choose, from a jumble of ill-digested ideas, a version which better reflects your intent as you would, on reflection, have liked it to emerge. It doesn&#x27;t matter that it&#x27;s not a true record, since its purpose is not to record events but to communicate ideas. You try to improve on the commits as they have been written, to put it in a better order. You are tactful.[0]: https:&#x2F;&#x2F;youtu.be&#x2F;MF-Qnv2Srfs?si=U6xKNLrTAIn5h1Iz&t=118 reply Nullabillity 12 hours agorootparentYou do realize that Yes, Minister is satire...? reply ajkjk 15 hours agorootparentprevIMO the relevant &#x27;true history&#x27; is at the plane of &#x27;things relevant to other developers&#x27;. As far as anyone is concerned, my local work history is that I atomically wrote all my code in a single instant. Commits are that unit of atomicity.But I have never seen a commit disappear while rebasing, ever. That workflow is busted somehow. They were doing it wrong. reply diek 16 hours agorootparentprevThe golden rule is \"do not rewrite history of a public branch\". Rebase&#x2F;squash your PR branches to your heart&#x27;s content, but once it&#x27;s merged that&#x27;s it.You get clean history by not merging branches with 50 intermediary \"fiddling with X\" commits in them. reply btilly 16 hours agorootparentRelated, it is a bad idea to use long-running per team branches. Merge early, merge often, get commits to trunk ASAP. With best practices on unit testing, code review, and so on, this scales to many thousands of developers. And will save a lot of pain over time. reply debaserab2 16 hours agorootparentprev> I used to work at a company where someone (we never figured out who)Wouldn&#x27;t this be trivially solvable by git bisecting your deploy branch? reply btilly 15 hours agorootparentNo, because git bisect operates off of the information in the history. And thanks to the bad rebase, the history no longer existed in the branch. reply lifeisstillgood 15 hours agorootparentJust clarifying ... they took a version of master from say a month ago, did their work on it, then, they force pushed their work out, wiping everyone else&#x27;s work that was added to master since one month ago?I mean that&#x27;s the equivalent of reversing your JCB through a house on a building site because \"the house was not there a week ago when I last moved the JCB\".or am I missing something? reply btilly 15 hours agorootparentBasically equivalent to that. This was about a decade ago.We had 4 teams, each released on a schedule. Each team had a branch. When a team released, it was merged from master, then each team pulled. The person who did the pull would change, and it was often a rebase.So my team released, some other team rebased badly. There would be no sign of problems for us until after they released. But since 2 teams generally released at once, and people didn&#x27;t remember who actually did the merge a few weeks earlier, it was hard to figure out who was actually messing up. (I had suspicions, but no proof.)I&#x27;ve seen rebase used appropriately since. But that disaster left scar tissue. reply lifeisstillgood 15 hours agorootparentSo (sorry for picking on this scab) there became 4 branches each of which for the team concerned was the next beautiful branch and then say each week a team woukd merge into master and everyone woukd rebase but fuck that up once and now the beautiful branch team B is working on is out of step with the real master ... oh god.Yeah that would leave a mark. reply Chris_Newton 10 hours agorootparentprevIt looks like you’re talking about rewriting history that has already been shared. Pretty much everyone agrees that is a bad idea. It even has a prominent warning in the git book.What others here, including myself, are advocating is rewriting your own history before you share it, which makes a very different set of trade-offs. reply yxhuvud 11 hours agorootparentprevBut that make no sense, as the second someone pulls from that branch it would be noticed. reply latexr 13 hours agorootparentprevWasn’t `git reflog` viable? reply btilly 13 hours agorootparentHow do you find the right commit from a month ago to reflog to?How do you determine which combination of changes are yours, and not accidentally undo changes that other developers made in the last month?Could git reflog have helped? Maybe sometimes. If we had the foresight to have saved the right commits from the past, sure. I think we did start saving old branches just in case it happened again, but then we had to sort through a month of changes to figure out what to keep, what to change, and what conflicts there might be.Remember, the person screwing up didn&#x27;t know he screwed up. And the person trying to fix it is doing so a long time after the fact. It was a disaster. reply Arch-TK 15 hours agorootparentprevIf you think the mess (i.e. 50 mixed up mini commits as you work on a feature and iterate on it) is useful history (it usually isn&#x27;t) then use a different DVCS because git was not designed around this mode of operation and a lot of its advanced features work poorly when applied to such a history.Alternative DVCSes which support this workflow include: fossil reply sethammons 10 hours agorootparentprevthis exact problem led us to block all --force pushes. reply rbehrends 16 hours agorootparentprevYou can have it clean without rebasing. This is simply a matter of properly visualizing the history. Unfortunately, most of the major version control systems have decided to basically just dump a raw graph rather than presenting the history in a more user-friendly fashion. reply LAC-Tech 13 hours agorootparentprevI want it clean. But I don&#x27;t want to do the work to make it clean. The juice is not worth the squeeze. reply BeetleB 13 hours agorootparentprevAs your parent already said, (and it matches my experience), we didn&#x27;t have any of these problems when using systems without history rewriting (mercurial, for example).I recall when I first switched to git at work and the team was insisting on a \"linear history\", I was bemused: Could these developers really not handle a merge graph? It was bizarre how something straightforward in other VCS is suddenly \"messy\" amongst git folks. reply ajkjk 13 hours agorootparentIt&#x27;s not that we&#x2F;they &#x27;can&#x27;t&#x27; handle it. It&#x27;s that we choose not to because it&#x27;s a better life.It is like moving to one&#x27;s favorite part of town. reply BeetleB 13 hours agorootparent> It&#x27;s not that we&#x2F;they &#x27;can&#x27;t&#x27; handle it. It&#x27;s that we choose not to because it&#x27;s a better life.As someone who has had to put up with rebase for several years now, my life is definitely not better. And things that were trivial in mercurial are now complicated (seeing the actual chronology - both in the log and in the graph). That graph with multiple branches that git developers find messy can actually be really useful. reply haakonhr 16 hours agorootparentprevIs it free though?I&#x27;m a fan of rebase myself, but understand the point made above. For me, the biggest pro of a clean history is when doing `git blame`. If the history is clean and the commits are good, it might solve my issue. On the other hand, if the commit in question is a huge mess of unrelated things it doesn&#x27;t help me at all. I also find it way easier to review a PR with a clean, well-described history. reply ajkjk 15 hours agorootparentWell I also object to multiple unrelated changes in the same commit. Those should be separate commits, and separate code reviews for that matter. reply unicornmama 16 hours agorootparentprevYou can run ‘git log —-first-parent’ that will give you the same output as squash merging, without losing the ability to effectively manage stacked branches&#x2F;PRs.But because GitHub and other tool’s version of rendering history just flatten merge commits into spaghetti we’re stuck with squash merge. Thanks GitHub. reply pm215 16 hours agoparentprevFor me the big gain is at the code review stage. It&#x27;s much easier to review a set of patches that are a clear and distinct sequence of changes without \"oops, fix bug\" changes later in the series. It does require extra work by the code author, but it means less work for the code reviewer. Depending on the project and the organisation and the workflow, that can be a worthwhile tradeoff. reply heisenberg302 16 hours agoparentprevAnother thing is if you keep commits in a clean \"state\", it is easier to revert a commit, when you squash or keep them messy it can make it harder to revert.Also sometimes you decide you want to backport some change to other releases, and if commits are in a good state, it is much easier to do this. reply nicolaslem 16 hours agoparentprevSometimes when working on an old code base built by developers that came and went, one needs to perform what I call \"code archeology\": going back in time to understand why a feature was implemented the way it was.Whether this is feasible at all depends largely on the care developers put in structuring their commits. reply ohwellhere 16 hours agorootparentYes! The source code is itself the first level of documentation. The commit history is the second.A free form textual interface to document everything about why you made the changes you just made? Why not maximize the value of this resource! reply Izkata 13 hours agorootpa",
    "originSummary": [
      "The author elaborates on the potential issues to bear in mind while using 'git rebase' and 'git push --force' commands in collective branches, such as repeated conflict resolution, loss of commit metadata, and challenges in undoing a rebase.",
      "They suggest cautious use of rebase in shared branches and emphasizes on maintaining neat commit discipline.",
      "They propose a “squash and merge” workflow as an alternative, and discourage force pushing to a shared branch and rebasing a pushed one, acknowledging rebase's utility nonetheless."
    ],
    "commentSummary": [
      "The author discusses potential problems with git rebase and git push --force commands in shared branches, including conflict repetition, loss of commit metadata, and difficulties in undoing a rebase and rebasing a vast number of commits.",
      "There is a recommendation for practicing restraint when using rebase in cooperative branches, emphasizing the importance of maintaining a clean commit discipline.",
      "The article suggests considering the \"squash and merge\" workflow as an alternative. The author discourages force pushing to a shared branch and rebasing a pushed one, despite recognizing rebase's usefulness."
    ],
    "points": 279,
    "commentCount": 348,
    "retryCount": 0,
    "time": 1699285343
  },
  {
    "id": 38162837,
    "title": "Elgato Innovates with USB-C Cables: Bandwidth and Type Clearly Imprinted for User Convenience",
    "originLink": "https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato",
    "originBody": "Tech/ Creators/ USB-C Please make more USB-C cables like this Please make more USB-C cables like this / Elgato has a simple and useful approach to reduce USB and HDMI cable confusion: listing the important information on the connector itself. By Tom Warren, a senior editor covering Microsoft, PC gaming, console, and tech. He founded WinRumors, a site dedicated to Microsoft news, before joining The Verge in 2012. Nov 6, 2023, 1:44 PM UTC| Share this story Photo by Tom Warren / The Verge Elgato hasn’t just made an excellent teleprompter, it’s also made a great USB-C cable that ships with it. Professional audio engineer Matt “Spike” McWilliams spotted that Elgato’s latest USB-C cable has the bandwidth and USB type imprinted on the connector, and now I wish all manufacturers did this. I recently spent too many hours sorting my USB-C cables into ones that are high speed, ones that can deliver fast charging, and ones that can do both. None of them had any marker to let me know the speed or type of USB-C cable without me having to test them. It’s a common issue for people switching to USB-C right now, and even a small indicator like Elgato’s can certainly help. The writing on Elgato’s cable tells me it’s USB 3.0 compatible and can support up to 5Gbps in bandwidth. The USB Implementers Forum has a logo that manufacturers can use on packaging, but that’s often useless later on when you’re searching for a specific USB-C cable in your box of cables. I’ve also purchased plenty of USB-C cables that have claimed, on the box, to deliver a certain bandwidth and failed to do so in reality. Elgato’s solution is so simple I didn’t even notice it when I unboxed its Prompter hardware recently, but it’s so useful that I wish it was more widespread. “You’ll see spec data on all Elgato USB and HDMI cables going forward,” explains Julian Fest, senior vice president at Elgato, in a post on X (Twitter). “No more guessing if the cable is the culprit of a tech issue.” Most Popular Bored Ape NFT event attendees report ‘severe eye burn’ OpenAI is letting anyone create their own version of ChatGPT Please make more USB-C cables like this Apple MacBook Pro 14 (2023) review: entry-level enigma ChatGPT subscribers may get a ‘GPT builder’ option soon Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=38162837",
    "commentBody": "USB-C cable with the bandwidth and USB type imprinted on the connectorHacker NewspastloginUSB-C cable with the bandwidth and USB type imprinted on the connector (theverge.com) 277 points by alphabettsy 19 hours ago| hidepastfavorite220 comments JohnBooty 19 hours agoThis is very cool.I&#x27;ve resorted to physically labeling them with a Brother label printer.For MacOS, here&#x27;s a terminal snippet to determine the current charging capacity of a cable. &#x2F;usr&#x2F;sbin&#x2F;system_profiler SPPowerDataTypegrep WattageThis will tell you the charging capacity of your current cable+charger combo. In other words, a 100W charger + 100w cable will return \"Wattage (W): 100\". A 100W charger + 15W cable will return \"Wattage (W): 15\". A 15W charger + 100W cable will return \"Wattage (W): 15\".So, you kinda need a charger that meets or exceeds the power delivery of the cable you&#x27;re testing. And I believe this relies on the cable&#x27;s self-identified capacity via the e-chip or whatever, so it could be fraudulent if you have some dodgy cable from an unknown manufacturer? But it&#x27;s better than nothing.There is a lot of info in `&#x2F;usr&#x2F;sbin&#x2F;system_profile`, not sure if there&#x27;s other bits related to identifying USB-C cable capacity. I&#x27;m sure there has to be something. reply chx 18 hours agoparentLinux has a very nice and super informative way to do this too: https:&#x2F;&#x2F;github.com&#x2F;Rajaram-Regupathy&#x2F;libtypec&#x2F;wiki&#x2F;lstypec-:...It relies &#x2F;sys&#x2F;class&#x2F;typec: https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;ABI&#x2F;testing&#x2F;sysfs-c... which does not always exist https:&#x2F;&#x2F;bbs.archlinux.org&#x2F;viewtopic.php?id=274618 https:&#x2F;&#x2F;community.frame.work&#x2F;t&#x2F;tracking-controlling-power-di... https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;UsbCHardware&#x2F;comments&#x2F;z92jgf&#x2F;when_d... etc. reply Feathercrown 14 hours agorootparent> Linux has a very nice way to do this [...] which does not always existSome things never change haha reply shrikant 16 hours agoparentprevYou can get this information from the \"System Information\" app in macOS as well. (At least, that&#x27;s what it&#x27;s called in Ventura...)It should be under Hardware -> Power, in the section titled \"AC Charger Information\".You should also be able to see connected USB-C cable capacity under Hardware -> USB. For example, mine says the following: USB 3.1 Bus: Host Controller Driver: AppleT8112USBXHCI USB3.0 Hub: Product ID: 0x0813 Vendor ID: 0x2109 (VIA Labs, Inc.) Version: 90.11 Speed: Up to 5 Gb&#x2F;s Manufacturer: VIA Labs, Inc. Location ID: 0x01200000 &#x2F; 2 Current Available (mA): 900 Current Required (mA): 0 Extra Operating Current (mA): 0 USB2.0 Hub: Product ID: 0x2813 Vendor ID: 0x2109 (VIA Labs, Inc.) Version: 90.11 Speed: Up to 480 Mb&#x2F;s Manufacturer: VIA Labs, Inc. Location ID: 0x01100000 &#x2F; 1 Current Available (mA): 500 Current Required (mA): 0 Extra Operating Current (mA): 0 reply ender341341 15 hours agorootparentAny way to do similar for DP&#x2F;HDMI? I&#x27;ve thrown away multiple cables cause I couldn&#x27;t tell if the cable was just old and didn&#x27;t support what I needed vs somehow broken. Looking at system report I only see `Graphics&#x2F;Displays` which show&#x27;s monitor capabilities but not cable. (I&#x27;m not actually sure if DP&#x2F;HDMI advertise their capabilities in the same manner as USB 3).Much like GP I&#x27;ve taken to labeling newly bought cables with a label maker to avoid wasting them in the future due to being unknown. reply wtallis 8 hours agorootparentDP and HDMI cables are mostly passive with no identifying chip, so the only way to determine their bandwidth electronically is to try to bring up the link at that speed. reply greggsy 15 hours agorootparentprevThere’s an okay video from LTT where they hire or buy an expensive tester for HDMI cables. Seemed to be a a bit of a dark art. reply ender341341 15 hours agorootparentyeah I&#x27;ve seen that, would be awesome to have access to it but not worth the cost to me, was less interested in stress testing does this 100% meet spec and more interested in \"this labels itself as HDMI v{X} with optional features {y}, {z}\".I mostly care as my current monitor requires HDMI 2.1&#x2F;Display Port 1.4 and going through my random cables drawer was extremely tedious. As I had way to many cables. reply Dylan16807 16 hours agoparentprevThere should be no such thing as a 15 watt cable, by the way.Every cable is a 60 watt cable by default. 20 volts, 3 amps.Cables designed for 5 amps have a marker chip in them. If they follow the old spec they support 20 volts, and if they follow the new spec they support 48 volts. There&#x27;s not much incentive to fake these because you still have to put a chip into it and 60 watts is fine most of the time. reply emgeee 14 hours agorootparentAnecdotally I was able to get this command to return 8 watts by using a usb-A -> usb-C cable meant for charging a mouse. reply Dylan16807 14 hours agorootparentThat sounds like you&#x27;re hitting the limit from non-charging ports. Normal A to C should allow much more. But either way that&#x27;s a special case here, because the limits are entirely on the USB A end. That&#x27;s not a proper C cable. reply lxgr 13 hours agorootparentAn A-to-C cable is limited by what an USB-A host can supply, since that&#x27;s the limit which a compliant C-device sensing an A-to-C cable or adapter would observe.Depending on the combination of host and device, that would be 4.5W for standard USB 3, 2.5W for USB 2, up to 7.5W for USB Battery Charging (BC), or possibly more for non-USB-C PD (highly unlikely, since it would require legacy PD signalling on the voltage pins, which almost no USB-C devices support as far as I know).I strongly suspect it&#x27;s USB BC, rounded up to 8W by Apple.As an aside: Apple&#x27;s USB power controllers used in their laptops are surprisingly flexible based on my observations: In addition to the obvious USB-PD, they support at least USB BC and Apple&#x27;s proprietary USB-A resistor-based protocol. I was able to successfully charge a MacBook Pro at 12W and even 5W supplied by an old iPad&#x2F;iPhone charger and the appropriate combination of adapters once. reply ace2358 11 hours agorootparentGreat post thanks for the information. Seems to verify my experiences.I’m actually able to run my m2 MBA from a 5w usb a iPhone charger. I thought that was cute. replyarchagon 16 hours agoparentprevApparently, USB-C cables are supposed to contain an “e-marker” chip that can tell you its bandwidth, wattage, and other specs. It would be nice if this information showed up somewhere when you plugged in an empty cable into your Mac. reply JohnFen 12 hours agorootparentBut that only helps if you plug it into something. Better would be a visual indicator that can be used when the cable is just lying around unplugged. reply lxgr 19 hours agoparentprevI’ve also tried to get that information on macOS before, but annoyingly all promising (i.e. related to the SOP protocol which is how the computer talks to the chip in the cable) log lines seem to be redacted.This is doubly frustrating in that it shows that it’s clearly available to the OS (and not handled by a lower layer controller), but also leaves the faint hope that Apple might implement just such a feature in the future. reply bartvk 18 hours agorootparentMy suspicion is that this is security related. I&#x27;m on the lookout for a brand-name quality Thunderbolt-capable cable with an LCD showing current power. reply lxgr 17 hours agorootparentWho&#x27;s security? Is Apple trying to protect me from the information that my $20 USB 4 cable is just as good as their own $69 version? reply MagerValp 14 hours agorootparentThe system logs are redacted by default as they were a common source of information leakage. You can enable unredacted logs with a configuration profile: https:&#x2F;&#x2F;superuser.com&#x2F;questions&#x2F;1532031&#x2F;how-to-show-private-... reply david422 16 hours agoparentprevI&#x27;ve been using a nice Brother labeler for years to label all my power plugs. eg this power plug goes to this router etc. since a lot of them are subtly different.I once mixed up two different netgear router power plugs - a new version and an older version. I had the newer netgear router hooked up to the old plug - which gave slightly less amperage - and everything worked fine until it would randomly die. It took me a long time to figure out that it was dying when it was under load and trying to pull more power than was available. reply Hamcha 18 hours agoparentprevI&#x27;ve also started doing this, with USB-C to C cables you might want to get something like the C2C caberQU cable tester as a lot of cheap type c cables don&#x27;t actually wire all the conductors.Also I like to note the cable length on the label, measured by myself since manufacturers always count the connectors on the total cable length and on some of them that can add up to almost 10cm reply michael1999 17 hours agoparentprevThanks. Now I&#x27;m a bit freaked out to realize that tiny CalDigit PD cable is rated to 100W. Yikes. The future is wild. reply Dylan16807 16 hours agorootparentFor a cable 1-2 meters long, a conductor 1mm across is more than enough to carry 5 amps. reply michael1999 14 hours agorootparentThe whole cable is smaller than 5mm diameter. Less rubber jacket, and shield, it might just be 3mm inside. And there are ~18 lines in there? I expect the power lines are larger than the differential signalling lines, but I&#x27;m just realizing how much I&#x27;m trusting this vendor. reply Dylan16807 14 hours agorootparentThat&#x27;s much more than big enough. 1mm diameter was the overkill number, and 3mm diameter is nine times as big while you only need two of them. Even accounting for insulation and the smaller data wires, that&#x27;s plenty. reply izacus 17 hours agoparentprevThat tells you wattage but not transfer speed - e.g. most 100W power USB-C cables are USB2.0 (yes, Apple ones too).To you have a matrix - charging speed and capability. reply greggsy 15 hours agoparentprevThis is incredibly helpful, thank you reply csdvrx 19 hours agoparentprev> So, you kinda need a charger that meets or exceeds the power delivery of the cable you&#x27;re testing. And I believe this relies on the cable&#x27;s self-identified capacity via the e-chip or whatever, so it could be fraudulent? But it&#x27;s better than nothing.Get a cable with a watt meter display included. I use that on my Fold: https:&#x2F;&#x2F;csdvrx.github.io&#x2F; section 0.4 : https:&#x2F;&#x2F;csdvrx.github.io&#x2F;X1_Fold_(20RL_20RK)_Optimization&#x2F;pa...Such cables cost about $10 on amazon, or $2 on aliexpress. Amazon link for the reference: https:&#x2F;&#x2F;www.amazon.com&#x2F;Charging-Display-Braided-Compatible-G... reply lxgr 18 hours agorootparentThese can be useful, but ultimately a watt meter tells you what the device is actually using at the moment, which is not only limited by the cable and source capabilities, but also by the power sink device&#x27;s current needs. reply criddell 18 hours agorootparentprevMcdodo isn’t a member of the USB IF group and their products aren’t certified. I wouldn’t have much faith that the number in the display is correct. reply aaron465 19 hours agoprevHDMI cables too!As a techie who understands that there are many different types of USB-C and HDMI cables depending on what you need to use it for, it&#x27;s an incredible amount of effort to find the right thing to buy.When you consider that most brick and mortar stores (I&#x27;m looking at you, Currys PC World) massively rip you off with £100+ gold plated HDMI cables, and the search results on Amazon are filled with knock-off Nigel rubbish (or worse, listings that out-right lie to you)... it&#x27;s a total minefield!Imagine what it&#x27;s like for the average consumer! A complete disaster! reply moduspol 17 hours agoparentEspecially tedious on Amazon when you have a seller list the same cable in various lengths. The reviews are all commingled, so you&#x27;ll get plenty of valid five star reviews for the short cables while the long ones don&#x27;t support the bandwidth.And who knows how many reviews are from non-techies who are just getting a picture on the screen and leave a five star review without knowing &#x2F; caring if they actually got HDR or not. reply lxgr 19 hours agoparentprevThis would be difficult with HDMI cables – unlike USB-C, they’re not electronically marked.Only HDMI 2.1 introduced link training (before that it was just the source picking a resolution that the sink supports and hoping for the best!), but even that is an end-to-end thing; the cable is not part of the conversation, so you wouldn’t know if the cable is bad or the socket&#x2F;internal wiring beyond the cable on either side. reply notyourwork 19 hours agorootparentMost (all? But I’d have to double check) of my hdmi cables have the version printed on the cable jacket. Seems entirely sensible to do. reply lxgr 18 hours agorootparentOh, sorry, I thought this was in the sub thread about “why not mark cables in software”!Fully agreed – the maximum supported data rate would be even more important to be printed on the cable for HDMI.One reason I could see why manufacturers would be hesitant to do this is that some future improvements might not have higher requirements for cabling, so the printed data rate might be underselling actual capabilities. reply vel0city 14 hours agorootparentAs a consumer I&#x27;d much rather have a cable rated for one level and end up actually supporting more than not having any information at all. reply falcolas 18 hours agorootparentprevAll but one of mine. The one has a plastic braid \"protecting\" the cable, which hides any potentially useful markings.I just had to go through the \"do I have any good HDMI cables\" dance, and the answer was effectively no. reply kalleboo 6 hours agorootparentprevI just checked the 4 HDMI cables plugged into my TV and not a single one had the version printed on itThe closest was one that said \"High Speed HDMI cable with ethernet\" but I&#x27;m not sure what \"high speed\" is supposed to mean reply cogman10 18 hours agoparentprevMonoprice is where it&#x27;s at for cables. I&#x27;ve never went wrong with them and always get what I want. reply jd3 18 hours agorootparentI personally prefer the aesthetics of the white Amazon Basics [0] and&#x2F;or Infinite Cables [1] hdmi 2.1b [2] 48gbps cables, but as long as they&#x27;re certified and pass the totalphase cable test I guess it doesn&#x27;t really matter[0]: https:&#x2F;&#x2F;www.amazon.com&#x2F;dp&#x2F;B08BS181P2[1]: https:&#x2F;&#x2F;us.infinitecables.com&#x2F;products&#x2F;ultra-high-speed-hdmi...[2]: https:&#x2F;&#x2F;www.hdmi.org&#x2F;spec21sub&#x2F;ultrahighspeedcable reply david422 17 hours agoparentprevI hooked up a new monitor and had the hardest time trying to figure out why things weren&#x27;t working. Finally swapped the HDMI cable and it worked perfectly. Gah. reply coldtea 19 hours agoprevWhat we actually needed was the USB standards body to mandate this information on the USB cable for it to be labelled and sold as USB. reply Ajedi32 16 hours agoparentIt&#x27;s not mandated, but they do have specific logos available for cables that pass compliance testing: https:&#x2F;&#x2F;www.usb.org&#x2F;sites&#x2F;default&#x2F;files&#x2F;usb-if_usb_type-c_ca...If consumers would stop buying cables that don&#x27;t have these logos, that would (mostly) solve the problem. reply StillBored 15 hours agorootparentAnd where is the list of cables that have passed the compliance test and use the cable logo?I can&#x27;t say I&#x27;ve ever seen one of the package logo&#x27;s, and yes I&#x27;ve been aware of them for a while now.This is obviously a case of \"mistakes were made\" but I think the fact that they haven&#x27;t mandated the logos as part of the license to use the USB specifications says a lot about the companies that run the organization. (or maybe their products).Edit: There is this, but no actual type-c -> type-c cables, https:&#x2F;&#x2F;www.usb.org&#x2F;products. Oh maybe they are under \"retail -> cable assembly\" but I still can&#x27;t find one with a logo, which is the same problem. Once I buy it, how do I know what it can do in a year or two when I pull it out of a pile of cables. reply lazide 18 hours agoparentprevRandom amazon&#x2F;alibaba sellers ain’t got time for that! reply coldtea 16 hours agorootparentMost expensive sellers from known brands seem to not have time for that either... reply vel0city 14 hours agorootparentprevThe solution is to stop shopping Amazon&#x2F;Alibaba. They make trash because people buy the trash. Quit buying the trash. reply lazide 14 hours agorootparentGood luck trying to shame buyers into not buying cheap junk! If that worked, our economy would look very different. reply Tagbert 16 hours agorootparentprevthen we can avoid buying from them. A super cheap price may not be worth the trouble. reply benj111 14 hours agoparentprevOr just remember what the U is USB actually stands for.We&#x27;ve been through the mutually incompatible plug standards. USB was introduced to solve that. Now we&#x27;re back to mutually incompatible cable standards. reply JohnFen 12 hours agorootparentYeah, the U in USB is a bit of a joke. reply izacus 13 hours agorootparentprevWe are not. reply brotchie 17 hours agoprevAt this point I&#x27;d probably pay ~$100 &#x2F; year for a subscription service that annually sends me 4-5 cables of the \"max rating.\" I don&#x27;t want to waste my life keeping on top of the every-changing spec, but would 100% just turn-over my cables once a year to ensure highest performance.Perhaps change their color every year (red -> green -> blue -> purple) to keep track of which are the latest cables.I have dozens of almost identical black USB-C cables :| reply AdamN 16 hours agoparentJust get certified Thunderbolt 4 and be done with it then. Cheaper is TB3. If you stick with the legit ones you&#x27;re done and it will basically support everything. reply MrDrMcCoy 11 hours agorootparentCertified Thunderbolt cables with a length over 4 feet don&#x27;t really exist, though. If you need a cable longer than that which can handle 100w+ and video&#x2F;data, you&#x27;re going to have a much harder time finding something decent. reply AdamN 27 minutes agorootparentHere&#x27;s a 6 ft cable for $176: https:&#x2F;&#x2F;www.cablestogo.com&#x2F;usb-and-pc&#x2F;usb-c-cables-adapters-...In the end, the technology just doesn&#x27;t exist to send 100w and full data speed over the usb-c interface longer than that reliably (and economically enough for the market) so it just isn&#x27;t an option to go longer. You&#x27;ll have to find a different solution (optical plus separate power) or do a cross-your-fingers method with a non-certified cable that may or may not work under different circumstances. reply technofiend 15 hours agorootparentprevThat&#x27;s my solution: I just buy Caldigit TB4 cables and I know they will do everything required.There&#x27;s also the Dockcase Smart USB-C Hub 10-in-1 Explorer which exposes what&#x27;s actually happening across your various connections including USB-C. Unfortunately it doesn&#x27;t seem to support video passthrough over USB-C, so it isn&#x27;t perfect. reply pomian 16 hours agoparentprevExactly what I was thinking yesterday. Why not continue with different colors on the USB plugs, as they sort of did with blue for usb c or was it USB 3? Or. Ah. Can&#x27;t keep up.3.1? (Some motherboards tried, but there weren&#x27;t cables in same color.) reply GloriousKoji 15 hours agorootparentYou mean USB 3.2 Gen 2x1 aka USB 3.1 Gen 1 aka USB 3.0 aka Super Speed USB?Or do you mean Thunderbolt 3 which supports USB 3.2 Gen 2×1 aka USB 3.1 Gen 2 which was called SuperSpeed+ USB but is now SuperSpeed USB 10Gbps?Or the newest USB4 Gen 4, USB4 Gen 3×2, USB4 Gen 3×1, USB4 Gen 2×2 and USB4 Gen 2×1 which supports Thunderbolt 3?Maybe they could start with the upcoming Thunderbolt 5 which follows the USB4 2.0 spec, not to be confused with USB4 Gen 2(x1&#x2F;x2).I&#x27;m not sure we have enough colors to enumerate the different type of USBs. Going by the naming schemes it seems like there&#x27;s already not enough numbers. &#x2F;s reply dwaite 14 hours agorootparentMost of those are market noise because of manufacturers wanting to sell things which aren&#x27;t under conformance.Thunderbolt 4 and 5 are external conformance marks, because the USB-IF can&#x27;t be motivated to stop the abuse their own members are causing.For USB-C to C cabling, there is only bandwidth and power capacity. The operational modes supported by the devices you are connecting with that cable is out of your control (except purchasing replacement hardware). reply quietbritishjim 14 hours agorootparent> Most of those are market noise because of manufacturers wanting to sell things which aren&#x27;t under conformance.No, most of them are from the USB consortium who gave a new name to all the old standards with each new standard. reply crote 13 hours agorootparentprevAll the \"Gen\" bullshit was explicitly considered to be an internal designation. You can blame tech journalism for bringing that out into the world, because the USB IF never promoted that as a marketing term. reply CYR1X 16 hours agoparentprevOh man I think this is a great idea too. Would be a great way to support some independent maker. Somebody like a level1techs. reply jquery 15 hours agoparentprevAlongside this amazing subscription service, I would love a service that provides a list of tech brands that never do any kind of false advertising, so that I know I can buy from them with confidence. Recently had a bad experience with a spate of SSD external enclosures that all were under-spec, and worked with Ex-Fat but not APFS+ (which requires 100% USB spec compliance to work properly). Ended up finding a brand called \"Cable Matters\" which was a bit more expensive but was up to spec and well worth the extra cost. reply pomian 10 hours agorootparentThanks for that! Always hard to find the right tools, with a recommendation it is much easier. (Sometimes, you don&#x27;t know if it&#x27;s just you? Or there really are quality differences.) reply tonymet 16 hours agoprevLet&#x27;s just review all the various configurations of a \"standard\" USB-C cable (usb-c is really just the connector pinout).Data channel: yes &#x2F; noPD support: yes &#x2F; noUSB version: 2.0, 3.0, 3.1, 3.2Amperage: 3 amp, 5 amp, 6+ amp48+ possible varieties of \"standard\" USB-C cable and no practical way to determine what you have without plugging it in.What a great \"standard\"! reply Dylan16807 16 hours agoparent> Data channel: yes &#x2F; no\"no\" is violating the standard.> PD support: yes &#x2F; no\"no\" is violating the standard.> USB version: 2.0, 3.0, 3.1, 3.2Yes, there are a handful of speeds.But I don&#x27;t see a good way of avoiding that problem while having backwards compatibility.> Amperage: 3 amp, 5 amp, 6+ amp6+ amp does not exist. There are two 5 amp specs, but the new one obsoletes the old one. reply LorenPechtel 14 hours agorootparentData channel: no is a charging-only cable, *preferred* for dealing with untrusted sources of power. While it might violate spec it has a very valid use case. reply tonymet 14 hours agorootparentyou are right, i have many \"primitive\" usb-c devices that only work with a charge-only cable. reply Dylan16807 10 hours agorootparentSo you&#x27;re saying the data pins inside the device are attached to something, and when that something makes a connection it stops the device from drawing power?Does this happen when you plug them into chargers and when you plug them into hubs? reply tonymet 14 hours agorootparentprevthanks for the corrections which actually reinforce my point: it&#x27;s impossible for consumers to know what cable they have.Can you remind me how 120watts is achieved? is it 24v x 5 amps? I forget.Most cables that ship with devices are not standard. There&#x27;s no way to know it -- and that&#x27;s the USB-IF&#x27;s job!USB-C is a terrible consumer experience. reply Dylan16807 14 hours agorootparent> Can you remind me how 120watts is achieved? is it 24v x 5 amps? I forget.It doesn&#x27;t matter to the consumer. You either have a 240 watt cable or you don&#x27;t. But yes that&#x27;s how it does it.> Most cables that ship with devices are not standard.What? That&#x27;s not true at all. reply archagon 16 hours agoparentprevDon’t forget alt mode support of various kinds! reply ProZsolt 10 hours agorootparentThere is no such thing. It&#x27;s only depends on the USB version. reply archagon 6 hours agorootparentAre you saying that every USB-C cable is required to support DP alt mode? I’ve heard that’s not the case — that active cables, for example, will usually not support DP functionality. reply rstuart4133 12 hours agoparentprevYou forgot at least one: 5V &#x2F; 48V. reply onewheeltom 15 hours agoparentprevAt least the connector is better. reply tonymet 14 hours agorootparentbetter than microUSB that&#x27;s fair, but far worse than lightning. Engagement is terrible, it collects debris and it&#x27;s inconsistent (often too tight or too lose). reply ranguna 1 hour agorootparentIn theory yes, in practice I think I&#x27;ve had problems with two cables in the last 5 years using dozens of cables. reply Ajay-p 19 hours agoprevRelated to this, a group did fascinating work using a \"CT scanner to uncover the hidden engineering differences between\" Lightning cables.https:&#x2F;&#x2F;www.lumafield.com&#x2F;article&#x2F;usb-c-cable-charger-head-t... reply adra 18 hours agoparentI just watched a puff piece on Adam tested comparing apple thunderbolt $150 cables to introductory $5-10 usb cables. They seemed more in love with the tech to visualize the cables than actuallu giving meaningful information and like for like comparisons. reply Sakos 18 hours agoparentprevWow, that&#x27;s really interesting. I&#x27;d love to see comparisons between Apple and more well-known third party manufacturers like Anker, CSL or UGREEN. reply izacus 13 hours agorootparentThey&#x27;d be the same because they&#x27;ve been scanning cables with eMarkers vs. cables without. reply crote 13 hours agorootparentNot eMarkers but retimers &#x2F; redrivers. Basically, once you go beyond a certain length you need the cable to \"boost\" the signal. The tested cable is longer than that, so it needs active electronics. reply cornedor 18 hours agoprevThis has been done for years on HDMI&#x2F;Cat cables, but instead of on the connector on the cable itself. Why make the connector larger to print very little detail on it, if you have literally a whole meter to write complete sentences.This is roughly the same solution, but in my opinion more poorly implemented. So please make more USB cables like HDMI or Cat cables, print all the relevant info on the cable. reply skinner927 18 hours agoparentBecause then how will you wrap it in pretty textiles? reply unethical_ban 15 hours agorootparentYou jest, but flexibility and endurance is important for consumer cables, and except for LAN parties, Ethernet doesn&#x27;t get moved much (and is much cheaper to replace). reply thomastjeffery 16 hours agoparentprevI would be happy with either or both. reply TheRealPomax 16 hours agoparentprevBut what&#x27;s your solution for braided cables? Because I&#x27;ll pick braided over \"just plastic\" any day of the week, and there is no way you&#x27;re going to cleanly print information on the braid. reply vel0city 14 hours agorootparentI&#x27;ve got many graphic tshirts with text on fabric. I don&#x27;t see why it&#x27;s impossible on braided cables.Even a quick laser etch could be done after the braid. It doesn&#x27;t need to be super high resolution. reply whalesalad 19 hours agoprevWe need to fix the version naming first. USB 3.2 Gen 2x2 is such a silly name. reply InsomniacL 18 hours agoparentThe names are ridiculous:- Low-Speed & Full-Speed- High-Speed- SuperSpeed USB 5Gbps (renamed), original: SuperSpeed (Gen 1)- SuperSpeed USB 10Gbps (renamed), original: SuperSpeed+ (Gen 2)- SuperSpeed USB 20Gbps (USB 3.2 Gen 2×2)- USB4 40Gbps (USB4 Gen 3×2)- USB4 80Gbps (USB4 Gen 4)Not to mention- USB 1.0- USB 2.0- USB 3.0then it changes:- USB 3.1 Gen 1- USB 3.1 Gen 2Then it changes again:- USB4Then it changes again:- USB4 Gen 2- USB4 Gen 2×2- USB4 Gen 3- USB4 Gen 3×2 reply lxgr 17 hours agorootparentI don&#x27;t know, is this really much more ridiculous than e.g. the various Ethernet standards (I can never remember which one the \"normal\" Gigabit Ethernet is), mobile data protocols (remember HSDPA, CDMA, 1xRTT and all that?) etc.?And to the USB-IFs credit, they did eventually come around with the much saner bandwidth-focused names. As far as I know, even \"SuperSpeed\" and \"USB 4\" are now gone as speed designators; now it&#x27;s just \"USB xGbps\", e.g. USB 5GBps for the case of what used to be \"USB 3.2 Gen 1x1\". reply thomastjeffery 16 hours agorootparentprevThe good thing here is that they seem to be converging on names that contain the speed itself in plain text. \"USB4 40Gbps\" is as helpful as a name can get. reply lxgr 19 hours agoparentprevThe USB-IF already recommends a branding scheme other than that. For the mode you mention it would be “SuperSpeed USB 20Gbps“, for example. reply usrusr 18 hours agorootparentIt&#x27;s certainly much less bad now than it used to be, but the existence of \"x1\" connections (half lane count and bandwidth as its contemporary \"x2\" sibling) does undermine trust. Is a cable advertised with the number \"20\" really an x2 cable accurately described according to \"Gen 2\" metrics, or is it really just an x1 cable, daringly called \"20\" because that&#x27;s what a gen 3x1 connection could achieve?Looking at the Wikipedia page I get the impression that the new naming simply ignores the x1 links.That&#x27;s like an invitation for misleading product descriptions. Maybe it would have been helpful to assign slightly off but clearly confusion-resistent numbers to the x1 siblings? Perhaps taking a page from supermarket pricing, \"4.99Gbps\", \"9.99Gbps\", \"19.99Gbps\"? reply Dylan16807 15 hours agorootparentYou&#x27;re a bit mixed up.All USB-C cables have 4 high speed wire pairs (or 0). There&#x27;s no such thing as a USB-C cable that limits you to x1. x1 or x2 is entirely up to the devices attached.So if a USB-C cable is advertising \"20Gbps\", you want them to be talking about x1, because that means it&#x27;s a better cable.(And if it&#x27;s not USB-C on both ends, then it&#x27;s always x1. It&#x27;s never ambiguous.) reply lxgr 15 hours agorootparentAgreed on most points, but> So if a USB-C cable is advertising \"20Gbps\", you want them to be talking about x1, because that means it&#x27;s a better cable.is not what the USB-IF recommends – to them, a 20 Gbps cable is one capable of 2x2. reply Dylan16807 14 hours agorootparentRight, I mean \"want\" in the pleasant surprise way, not in the being correct way. reply lxgr 16 hours agorootparentprev> Looking at the Wikipedia page I get the impression that the new naming simply ignores the x1 links.Yes, and that probably makes some sense: All compliant USB-C cables (other than USB 2 only) need to have two high-speed lane pairs, i.e. four high-speed lanes in total. (I&#x27;m not sure why that&#x27;s the case, i.e. whether the aim was to avoid further confusion or if it&#x27;s required for orientation-reversible operation in some ports).Whether the host&#x2F;device actually support lane bonding or not is arguably not really the cable&#x27;s choice, so it makes some sense to specify whatever the cable would support in an optimal configuration (i.e. maximum number of lanes used at the maximum supported modulation rate). reply archagon 16 hours agorootparentBut you also need to know how fast your USB will be if you’re using DP alt mode in 2 lane mode. reply lxgr 16 hours agorootparentYes, but \"USB 10Gbps (except if using 1-2 lanes of DP alt mode; then USB 5Gbps)\" is probably a bit too much text for a USB-C plug.And with USB 4 and dynamic bandwidth sharing between USB and DisplayPort on the same physical bus this would get even more complicated. reply archagon 16 hours agorootparentIn that sense, I think the Gen1x1&#x2F;1x2 terminology was a bit more informative. Though still completely terrible. replybgm1975 18 hours agoprevConsidering the number of features and capabilities that can be given to a usb-c cable, I think a better alternative would be to take cue from the humble resistor and use color coded bands around each of the ends. Text can only really cover 1-2 capabilities and it’d be nice to have a way to know exactly what a given cable can do, even if it requires a decoder ring. reply skinner927 18 hours agoparentThis is also an international solution reply reidjs 15 hours agorootparentBut not accessible. It&#x27;s generally better to use colors AND designs (dashed, dotted, etc) over color to prevent color-blind issues, what I see as red may differ from what you see as red. reply andix 17 hours agoparentprevIf I understand it correctly most parameters can be negotiated by the usb controller of a device. It should be possible to plug it into a device and see the specs of the cable and the charger somehow. reply LorenPechtel 14 hours agorootparentYeah. A lot of IT people would pay for a box that you plug a USB cable into and it tells you what the cable can and can&#x27;t do. Simply reading out the specs, not actually testing whether it can perform to spec. (Although a simple test of the latter should be possible. Baby computer, it sends data at all supposedly supported speeds and sees if it gets the results. That would catch the ones that couldn&#x27;t but could miss a flaky one.) reply ProZsolt 10 hours agoparentprevThere are only two types capabilities of a cable speed (480 Mb, 5Gb, 20Gb, 40Gb or 80Gb) and power (60W (3A) or 240W (5A)) everything else is depends on the host. reply patmorgan23 18 hours agoparentprevMight not always require a recorder ring! You could label ports with the same color if it&#x27;s a capability like displayport. reply Tempest1981 17 hours agoparentprevDecoder ring made me think QR code reply java-man 16 hours agoparentprevAlways been a problem to differentiate one gray from all other gray colors... reply ChrisMarshallNY 19 hours agoprevIf it were mandated, I guarantee that every damn cable would have \"10GB&#x2F;3.1\" on them, except, maybe the big manufacturers, with large US presence.I wouldn&#x27;t mind a good tester, but the ones that can test for that kind of thing are quite pricey. reply HPsquared 19 hours agoparentNeed to trademark the symbols. reply nrp 19 hours agorootparentThey did! I’ve posted this comment before:To use any of the USB-IF logos (including the official ones that clarify power and signal compatibility) a product has to go through the certification process: https:&#x2F;&#x2F;www.usb.org&#x2F;logo-licenseUSB-IF also maintains a list of products that are certified: https:&#x2F;&#x2F;www.usb.org&#x2F;products The question then is, “Why is this still confusing?”. The answer is probably that the popular retail and e-commerce channels are flooded with products that aren’t certified, and consumers don’t realize that a certification process even exists. reply thomastjeffery 15 hours agorootparentThe problem is that you can flood the market with 2 categories of product:1. A cheap version that didn&#x27;t bother to get certified (to save cost).2. A fraudulent version that relies on the existence of a poorly regulated market (like Amazon) that allows \"mistakes\" in advertising details. That market is already flooded with cheap uncertified cables that do work, making it trivially easy to falsely advertise cables that don&#x27;t work.What needs to happen is for USB-IF to minimize #1 by making their standardization process as accessible (and affordable) as possible. Otherwise, the bulk of cables that customers are willing to buy will be uncertified. reply eschneider 14 hours agorootparentThis seems like a business opportunity. It should be straightforward (not _easy_, but straightforward) to manufacture high quality, reasonably priced cables and sell them direct. Establish a rep for consistent quality & fast shipping and carve out the &#x27;customers who give a shit&#x27; market.Once you&#x27;ve got a good rep, scale up. reply thomastjeffery 14 hours agorootparentThe problem is your interface with those customers.The customers we are talking about both care about cost and are willing to put more effort into their search to find an affordable quality product.From that customer&#x27;s perspective, the best method is to look through Amazon, Ebay, Alibaba, etc. listings to find every product that claims their desired spec under a reasonable price ceiling. After that, the customer need only filter out sketchy listings. This is most reliably done by checking reviews. It&#x27;s also done somewhat by checking the coherency of listing details: self-contradicting details are a common method for fraudulent sellers to filter for unscrupulous victims - the same process our scrupulous customer is performing here, played in reverse.So what can you do as a seller to reach this customer? Make detailed listings, and get reviews. Any more work than that is, unfortunately, a wasted effort. reply ChrisMarshallNY 16 hours agorootparentprevThis is true, but it’s been my experience, that scammers don’t pay any attention to stuff like that.It’s really up to the distributors and promoters, to vet the products they push, but we all know how well that’s going… reply blueflow 19 hours agoprevIdea: Lets give them different plugs so they don&#x27;t fit if they don&#x27;t match.[current variant of \"suggest previous status quo as improvement\"] reply thomastjeffery 15 hours agoparentSo long as the port itself is full-featured, it&#x27;s good to keep the plugs the same.The real issue here is with our marketplaces. Amazon in particular is notorious for allowing counterfeits with very little recourse available to the end customer. This has heavily incentivized uncertified products, because there is not enough value in accurate advertising to pay for certification in the first place. Shoppers who care know to trust reviews instead of listings. Shoppers who do get a fraudulent cable will just get a refund; and the seller will just keep selling bullshit.The marketplaces that care about accuracy love this situation. They get to take a larger cut from heavily overpriced cables, because they can brag about certification. Why would Best Buy ever try to get more competition? That would only hurt their margins. reply fassssst 18 hours agoparentprevTerrible idea. Most of the time I just want to charge stuff, which any old USB-C mostly works fine for up to like 60 watts. And very few peripherals actually need more than like USB 2.0 speeds. reply jack_h 15 hours agoparentprevAll of this talk about testing and labeling doesn&#x27;t make sense to me or likely any layperson. I don&#x27;t want to have to know what capabilities a cable needs and then compare it to a label and I really don&#x27;t want to have to test a cable to see if it will work for my intended use case.I want to be able to grab any USB cable and have it work regardless of my intended use. If that is impossible because the standard is so complex then we really should have different connectors. The entire point of it being universal is that regardless of circumstance it will work. Somehow that has morphed into the connector being universal while the actual utility of each cable is not; the latter of which is the only thing people really care about. reply izacus 13 hours agorootparentNot being able to plug a mouse or a keyboard into a 40GBps USB4.0 laptop connector just because those devices only need the USB2.0 protocol is a dumbarse idea.Just like having to pay for a 40GBps USB4 controller in a mouse which needs USB 1 speeds.Despite all the whining here from \"hackers\", the current situation is undoubtedly better. reply marcosdumay 19 hours agoparentprevThey have the same plug so we can choose anyone we want to use. It&#x27;s way better than the previous \"oh, no, my computer only has 1 parallel port, we can&#x27;t connect this one\". reply jon-wood 18 hours agorootparentAnnoyingly while that’s true on Apple devices there’s a plethora of cheap laptops where only one of the USB-C ports is capable of DisplayPort, and a different one can do USB-PD. Good luck working out which is which. reply izacus 13 hours agorootparentAnd there&#x27;s many more laptops where all of them work. And on all of them the ports are clearly marked and will always support at least USB3 speeds. reply gedy 19 hours agoparentprevBut see there&#x27;s no room (when we bevel the laptop edges up to give a false impression of thinness) reply alberth 18 hours agoprevWasn’t this solved last year?Official new logos below:https:&#x2F;&#x2F;tidbits.com&#x2F;2022&#x2F;09&#x2F;29&#x2F;usb-simplifies-branding&#x2F; reply ajmurmann 17 hours agoparentIf I read this correctly, those logos are for \"packaging and marketing\". Unless they are on the cable itself, as Elgato does, this is gonna be of little help when I&#x27;m looking for a cable in my cable box even six months later. Or if you have shared cables in a household. reply Arnavion 18 hours agoparentprevYes, see the \"Cable logo\" column in https:&#x2F;&#x2F;www.enablingusb.org&#x2F;certification , though it&#x27;s still the manufacturer&#x27;s option whether to put the packaging logo on the packaging or the cable logo on the cable or both. reply rini17 19 hours agoprevThat does not help with counterfeits. We need an easy way to check the cable instead. Best would be if you can plug both ends into computer and the controller measures resistance (charging capability) and data bandwidth.Or at least some standalone tester, shouldn&#x27;t be hard to build , dunno about the bandwidth part. reply bentcorner 18 hours agoparentWhat I want is something similar to this [1] but also can test cable speeds.[1] https:&#x2F;&#x2F;www.amazon.com&#x2F;Treedix-Cable-Checker-Charging-Type-C... reply jrmg 16 hours agorootparentI’ve been impressed with this:https:&#x2F;&#x2F;www.amazon.com&#x2F;dp&#x2F;B0BS2ZS813?psc=1&ref=ppx_yo2ov_dt_...The UI is a little confusing at first, but it seems to work well. reply LorenPechtel 14 hours agorootparentprevThank you. That looks like it would test anything other than the 3.0 square plugs--but those you can identify by eyeball. reply iudqnolq 19 hours agoparentprevIdeally you&#x27;d want to measure actual rather than reported charging capacity. For about $15 you can get a USB load generator and a USB current monitor on AliExpress. reply rini17 19 hours agorootparentResistance is all you need to know, actually testing the max current is not necessary. Or safe, even. reply lxgr 18 hours agorootparentFor the newer USB-PD modes, you also need to know the maximum safe voltage (and related construction features, such as CC pins being a bit shorter to enable a safe&#x2F;spark-free connection teardown), or you risk arcing. reply rini17 29 minutes agorootparentAFAIK all cables are de-facto rated up to 30V, as you won&#x27;t save money if you make insulation even thinner. So this is non issue in practice. Plus you can&#x27;t really measure breakdown voltage without destroying the cable. reply lsaferite 19 hours agorootparentprevNot an EE, but I would hope that testing max current _would_ be safe (at least on a legit cable). reply lazide 18 hours agorootparentThose aren’t the cables one would be hoping to find in such a test though, correct? reply lsaferite 11 hours agorootparentAh, point taken. reply iudqnolq 19 hours agorootparentprevYou&#x27;re right, I mixed up USB wall bricks which can have issues like the voltage dropping. reply dist-epoch 19 hours agorootparentprevHow do you measure actual without risking fire. reply iudqnolq 19 hours agorootparentBy being careful? If you were otherwise going to use it without testing you&#x27;re not any worse off plugging it into a test setup up to your expected load.You can step up the load gradually and keep an eye on it.I would be more worried about the cheap no-brand test gear catching fire. I wouldn&#x27;t plug my 100W charger into it. reply dist-epoch 19 hours agorootparentMy point was that if you can&#x27;t test safely with zero user intervention, adding this capability to computers will just cause fire incidents. reply lazide 18 hours agorootparentIt wouldn’t cause more fire incidents than just using the cable without testing, right?The tests would only run for a tiny period of time. Most fires come from too much current through too small conductors for too long. It takes time for heat to build up to fire causing levels.The type of time required to actually try to charge something. replythomastjeffery 16 hours agoparentprevI would rather have a counterfeit cable that is clearly labeled. At least then I can trivially explain the fraud that was committed.Without the labeling, every discussion about a counterfeit must include redundant clarification. Most fraudsters will take that as an opportunity to talk in circles. reply archagon 16 hours agoparentprevUSB-C cables are supposed to contain an “e-marker” chip with all the relevant spec information. reply HPsquared 19 hours agoparentprevCan&#x27;t the host and guest measure the voltage on each end anyway? They&#x27;d just need to communicate it with each other. This would give a full end-to-end voltage drop measurement (including the plug and socket connections at both ends). reply Johnny555 15 hours agoprevThat&#x27;s cool and very useful, should have been in the USB specs (\"connectors shall have supported version and speed imprinted\").But realistically, I can&#x27;t remember the last time I cared about the data speed of a USB cable, I use USB cables almost 100% for charging. I have a few memory sticks I plug in from time to time, but those get plugged in direct without a cable. reply crote 13 hours agoparentIt&#x27;s already part of it - see https:&#x2F;&#x2F;www.usb.org&#x2F;sites&#x2F;default&#x2F;files&#x2F;usb-if_usb_type-c_ca... . reply danbulant 18 hours agoprevIt&#x27;a weird that this isn&#x27;t normal. I bought an \"AlzaPower\" cable, which is just a rebrand of some chinese manufacturer by a local reseller (\"czech amazon\"), and they have their wattage as well as speed on them (100W&#x2F;5Gbps). Then also a samsung usb c cable, which was pure white, has nothing on it and doesn&#x27;t seem to support any fast charging or higher speeds.. reply usrusr 16 hours agoparent5Gbps is conveniently unambiguous, but 10 and 20 exist in variations that use four lanes and in variations that use two lanes. According to Wikipedia, the symbols with a number are only specified for the four lanes version, but there&#x27;s little doubt that half-width cables will also be be advertised with whatever bandwidth latest devices could funnel through. reply emmet 19 hours agoprevWhat we really need is to stop the USB-IF&#x27;s reign of terror reply meepmorp 18 hours agoparentReign of unforced errors. There&#x27;s no reason it needed to be the clusterfuck it currently is. reply spaceywilly 19 hours agoprevWhat a mess… couldn’t this be done in software instead? As in, if you try to charge your phone with a cable that doesn’t support fast charging, the phone could detect it and pop up an alert. I’m not sure the average consumer would know what to make of these labels, and that’s assuming the cable manufacturers would comply and put accurate labels in the first place. reply marcosdumay 19 hours agoparentThat&#x27;s way too slow and way too late. It doesn&#x27;t help finding the cable in a drawer, and it certainly doesn&#x27;t give you any assurance it&#x27;s a power-only cable before plugging in some unknown power supply.But yeah, let that happen too. It&#x27;s also useful.> assuming the cable manufacturers would comply and put accurate labels in the first placePutting the wrong label on the cable is plain simple fraud. You can easily get some civil recourse, and if you are feeling revengeful, you can make a criminal complaint against the seller. Compare that with our current situation. reply lazide 18 hours agorootparentGood luck perusing that course of action with a random Amazon or Alibaba seller.A well known brand? Sure. But those are not generally the source of the problems because of that. reply lxgr 19 hours agoparentprevThat’s how it’s already done. I don’t know why no device actually exposes that information, but the e-marker in every USB-C 3.0 (or beyond) cable tells the device exactly that: The highest supported physical data rate and charging speed. reply rtkwe 18 hours agorootparentThat&#x27;s not true for PD charging cables though right? They are generally USB2 speeds if they have data pins beyond what&#x27;s required for PD charging at all but they&#x27;re just direct electrical cables without any chips as far as I know.https:&#x2F;&#x2F;www.lumafield.com&#x2F;article&#x2F;usb-c-cable-charger-head-t... reply lazide 18 hours agorootparentDevices could (and usability wise I wish they would!) show low data speed only warnings. No OS’s I know of currently do.The device definitely knows, all the information is accessible in the USB info if you know the special tools to run to dump it. reply lxgr 16 hours agorootparentAt least Windows used to do this for USB 3 (before USB-C was a thing) devices plugged into an USB 2 port when the computer also had USB 3 ports available: https:&#x2F;&#x2F;superuser.com&#x2F;questions&#x2F;1022542&#x2F;windows-10-display-a...It would seem that with e-markers in USB-C cable (and assuming that USB 3&#x2F;4 still expose device information like that to the host), an extended version of this should be possible: \"Your computer and this device support x Gbps, but your cable caps out at y Gbps; use a better cable to make this connection faster\". reply lazide 14 hours agorootparentThey can do the opposite direction too, detecting a USB2.0 cable when a higher speed one is needed. It still boggles my mind that no corresponding warning pops up.Even without an e-marker, there has to be some way to know that the client device can support a higher speed and warn the user - it seems like USB device capabilities supports that?[https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;iokit&#x2F;1425036-usb_...] reply lxgr 18 hours agorootparentprevPer the specifications, the absence of an actual chip serves as a one-bit marker indicating \"this cable supports 3A and USB 2 speeds\" :)5A already needs a marker, even at USB 2 speeds! reply city41 18 hours agorootparentprevMy Pixel warns me when it is \"charging slowly\". reply SAI_Peregrinus 14 hours agorootparentBut it doesn&#x27;t warn \"charging slowly because the cable you&#x27;re using doesn&#x27;t support fast charging\" vs \"charging slowly because the charger you&#x27;re using doesn&#x27;t support fast charging\". It tells you there&#x27;s a problem, but not what the cause is even when it can determine the cause. reply spaceywilly 18 hours agorootparentprevThere you go. Seems like software could display this in a much more useful way than these labels ever could. Even the example in the article isn’t complete, it lists the data rate but doesn’t say anything about the amperage support for charging.Just a simple information window that lists the max charging Wattage and data transfer speed of the current cable&#x2F;charger&#x2F;device configuration would suffice. reply kalleboo 5 hours agoparentprevThe lowest power USB-C cable is still 60 W which is more than enough for any phone (aside from some Android phones with proprietary 100W charging protocols that need their own custom chargers anyway) reply whywhywhywhy 18 hours agoparentprevAt least the label on the cable is useful for those of us that understand and doesn&#x27;t intrude on those that don&#x27;tA popup we all have to suffer wether we want to see it or not or if we understand it or not. Oh guess I could disable it right? Yeah another task for me to do on every device I own, no thanks.This is an excellent design solution, my only comment would be the type layout could be nicer. reply gosub100 19 hours agoprevI&#x27;ve wondered why consumers don&#x27;t usurp the naming conventions for USB? For whatever reason, the USB consortium completely screwed up with all the variants which led to headaches and money wasted on cables. Why not just self-organize online and apply meaningful names to devices.So, for instance, a 10gb USB3 with oval plug and power delivery could be called \"USB3P\", 40gb could be called \"USB4P\", and so on, regardless of what advertisers or the organization calls it (don&#x27;t get hung up on my example name, the community could collaborate and vote on the best nomenclature).Tech reviewers could publish lab results of each cable SKU (and laptop model number) and provide lookup tables. Then sell cheap stickers or bands so you can label your own cables. Just completely bypass the standard and put the power in the hands of the people. reply bborud 19 hours agoparent> which led to headaches and money wasted on cablesor as the members of the consortium would say: hey, extra revenue! reply crote 12 hours agoparentprevThis entire mess happened because consumers usurped the naming convention. The intended marketing term is just \"USB-C 5Gbps\", \"USB-C 20Gbps\" and so on.The \"GenX\" stuff was only ever intended to be an internal technical designation, but journalists who didn&#x27;t bother to actually read all documents started using that term with the general public. reply jon-wood 18 hours agoparentprev[Insert “now there’s n+1 standards” XKCD here] reply dist-epoch 19 hours agoparentprev> Tech reviewers could publish lab results of each cable SKUWhat are you talking about? Nobody reviews USB cables, there are thousands of them and too few people care to make it worth the reviewer time. reply martyvis 19 hours agorootparentI guess one is more than nobody. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Benson_Leung reply Dylan16807 15 hours agorootparentHas he done any in the last five years? There have been important spec updates, and available brands and models change over time. reply donatj 16 hours agoprevI have so many USB-C cables that are only capable of either USB 2 speeds or just power that I&#x27;ve taken to just throwing them away when finding them.USB-C really made the problem worse and not better when it comes to incompatible connectors. reply nottorp 18 hours agoprevAnd now, dear usb consortium: why isn’t this mandatory in the -ing standard?Oh I know, because you allowed usb 2.0 normal speed and super speed. Were you bribed at least or did you do it for free? reply crote 12 hours agoparentIt is mandated. Most companies just choose to deliberately ignore that part of the spec. reply smsm42 14 hours agoprevIt&#x27;s so refreshing to encounter a manufacturer that gives a hoot about users. And then I feel sadness for realizing that this situation should be normal and routine, and it is so much not that it deserves an HN frontpage when somebody finally does the right thing. reply aerophilic 18 hours agoprevIt occurs to me some sort of labeling system really should be part of the standard.I was thinking it could be as simple as colored bands (with maybe dots&#x2F;dashes for those color blind folks).If such a standard could be adopted, it would make all our lives less of a headache…. reply JohnFen 12 hours agoprevThe USB standards body really should fix this by requiring all USB-C cables to indicate what their capabilities are. It seems insane to make them all the same shape and not to have a requirement for some other way they can be easily differentiated.This is one of the two things that makes me dislike USB-C (the other is that I find the physical connectors more fragile than the predecessors). reply andix 17 hours agoprevIs there any simple way to check the capabilities of a cable, without using specialized equipment?For me it&#x27;s always a gamble, with my devices I actually can barely even see how fast they are charging on a specific cable&#x2F;charger combo. Only by measuring charge time and then trying to calculate how many watts they delivered. reply Night_Thastus 17 hours agoparentPersonally, no. Testing equipment is the best way if you want to be sure.If you buy directly from a reputable brand, you may be able to trust what they&#x27;ve said a given product can do. If you buy from a reseller like Amazon, that all goes out the window and you should assume nothing. reply Ajedi32 15 hours agorootparentWorth noting the testing equipment for this isn&#x27;t that expensive these days. You can pick up an FNB58 USB tester for $60.You really shouldn&#x27;t have to though. At the very least every phone on the market ought to be able to just read the e-marker chip and tell you what each cable supports. Very annoying that they don&#x27;t because the information is there and they have a way to read it, it&#x27;s just not exposed to the user. reply andix 17 hours agorootparentprevI would be so happy if I could find a manufacturer that sells all common USB-C cables (to USB-C, lightning, micro-usb, mini-usb, ...) in good quality, with reliable specs, all common lengths and common color variations. A plus would be to even have the option to get one or both ends 90 degrees angled.Personally I prefer my charging cables to be red, really helps people not to trip over them. In other places the color preferably matches the environment. reply callalex 16 hours agorootparentMonoprice mostly fits your description. reply andix 13 hours agorootparentI need to check if their cables are available outside of the US. A quick search didn’t look promising. reply Night_Thastus 14 hours agorootparentprevAnker is one I&#x27;ve had decent success with. reply dotancohen 14 hours agorootparentprevWhat testing equipment is practical for the home user - $10 tops? reply iudqnolq 12 hours agorootparentGo on AliExpress and search USB tester, buy one for $5-$15, then wait a few months.In my single personal experience I ended up with one that gives reasonable readings and seems to work just fine. reply Night_Thastus 14 hours agorootparentprevIf you want a good, reliable tester, I think $10 is a bit limiting personally. You don&#x27;t need to spend $100 but $10 is stretching it really thin. GIGO. reply onewheeltom 13 hours agoparentprevI have a gadget that maps all the wires that is helpful reply dwaite 14 hours agoprevThe difference is that the USB-IF logos to indicate bandwidth&#x2F;charging capacity on packaging and plugs require certification, while numbers written arbitrarily on the plug don&#x27;t require testing and don&#x27;t have to be true. reply ComputerGuru 16 hours agoprevFYI for HDMI the licensing group actually forbids doing this, iirc. Go figure. reply throw932490 19 hours agoprevThere was a backlash when Apple tried to put microchips and DRM into USB-C cables. I hate Apple, but I would fully support them on this.USB-C cable problem is 10 years old. In 2013 Google introduced Pixel laptop, and some guy from Google bought all USBC cables on Amazon, only to test them and write reviews! reply black3r 19 hours agoparentMicrochips in cables are already a semi-optional part of the USB-C standard, mandatory for charging with more than 3A... The backlash was against rumors that Apple was planning an additional paid certification program which would mean only cables whose manufacturers paid for Apple certification would be able to achieve higher speeds... reply lxgr 18 hours agorootparentMarkers are also mandatory for all speeds above USB 2.This part of the spec is actually quite well thought out.It’s a shame that no device I know just displays the marker data (e.g. a message like “this connection might be limited by your cable”, assuming it’s also possible to figure out the other end’s highest theoretical speed, or simply the cable capabilities in some system menu). reply izacus 17 hours agoparentprev\"Cable chips\" - or as they&#x27;re actually called \"eMarkers\" are mandatory for all USB-C cables above basic capability.What Apple wanted was to leech on every single cable sold for their own profit with their proprietary licensing.You already have what you wanted - now you&#x27;re just demanding Apple tax for no reason. Remember, you can always just buy that 120$ Apple cable and be a happy camper. Right now. With no extra DRM. reply hiatus 19 hours agoparentprevIt&#x27;s a shame, I think the posts were all on google plus and hard to find now. reply radicality 17 hours agoparentprevThink I remember that story. And then he got one cable that was so badly miswired it fried his laptop I think? reply dukoid 15 hours agoprevI think it&#x27;s pretty common to have \"Cat X\" printed on ethernet cables... Perhaps the same advanced technology could be applied for HDMI and USB :) reply micw 16 hours agoprevHaha, there are so many parameters for USB-cables, they&#x27;d need to put a booklet on the connector... reply onewheeltom 13 hours agoprevDevices come with compatible cables. Everything else is an unknown. reply hiddencost 19 hours agoprevHaving the wrong USB cable has been the hardest part of my hobby electronics projects.Maybe I should try harder projects. reply AprilArcus 19 hours agoprevCable has a nice little \"3.0\" logo embossed, but don&#x27;t those fools at Elgato know that it&#x27;s called USB \"3.2 Gen 1x1\" now? reply lxgr 18 hours agoparentNo, the consumer-facing name is “SuperSpeed USB 5Gbps”.Gen 1x2 has been deprecated as far as I understand, so there is no more ambiguity when just referring to modes by their maximum data rate.USB 4 would make this even more complicated, but they fortunately have decided that 3x1 (20 Gbps just like 2x2) also does not get a marketing designation, so there’s also no more ambiguity there. reply crote 12 hours agorootparent\"Gen 1x2\" wasn&#x27;t deprecated because as a marketing term it was never a thing to begin with. And they recently dropped the whole \"SuperSpeed\" part so now it&#x27;s just \"USB 5Gbps\". reply dist-epoch 19 hours agoprevA lot of USB-C cables only support USB 2.0 speeds (480 Mbps = 60 MB&#x2F;s). I found out this while searching for a USB-C cable for a HDD enclosure. There are a ton of \"charging\" cables, but very few \"fast data\" cables, at least in the $10-15 range. I&#x27;m sure for $100 you can find a cable with everything in. reply izacus 17 hours agoparentYes, but that&#x27;s because manufacturing faster cables costs more. If all cables would have to carry USB 3+ speeds, you just wouldn&#x27;t have 10$ charging cables. reply lxgr 18 hours agoparentprevYou can definitely find decent 5 Gbps and even some 10 Gbps cables under $20.$100 is firmly in the \"premium\" cable range; you can find decent active 40 Gbps cables by reputable brands for around $50 these days. reply minimaxir 19 hours agoparentprevAnd some of those charging cables are capped at 60W too. reply eviks 17 hours agoprevExcept no consumer should be expose to this bit nonsense, the units should be the ones they actually use reply joecool1029 16 hours agoprevIt ultimately won&#x27;t tell me if the cables are actually connected inside but I have two USB testers that can read and print the information on each emark (if it&#x27;s present) in the cable, which is a pretty useful thing to have when grabbing a mystery usb-c cable. I have the avhzy ct-2 and ct-3 testers, there&#x27;s a bunch of other distributors of the same thing as well. The FNB48 looks like same board, different programming.I used to be able to recommend the avhzy ones but during the chip shortage they started using some clone chips inside which don&#x27;t appear to be as accurate. reply make3 16 hours agoprevCharging capacity too please reply Am4TIfIsER0ppos 16 hours agoprevlol you do realize manufacturers will just lie, don&#x27;t you?They&#x27;ll print the usb logo or whatever they like regardless of licensing restrictions. Did you always check for the official logo on CDs and was it always standard conforming if present? reply exabrial 19 hours agoprev [–] ...or just vote with your Dollar reply BitwiseFool 18 hours agoparent>\"...or just vote with your Dollar\"What do you mean by this? People can&#x27;t really vote with their dollar when it comes to the USB Implementers Forum and the decisions they make. reply mrweasel 17 hours agoparentprev [–] Isn&#x27;t the problem that you can&#x27;t tell in advance what you&#x27;re buying, due to crappy manufacturers and resellers. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Elgato, a technology company, has introduced a USB-C cable that has its bandwidth and type marked on the connector for easy identification.",
      "This feature aims to reduce confusion when locating specific USB-C cables, eliminating the need for testing to ascertain the cable's specifications.",
      "Elgato intends to incorporate this design feature into all of its upcoming USB and HDMI cables."
    ],
    "commentSummary": [
      "Elgato, a tech firm, has introduced a USB-C cable with its bandwidth and USB type imprinted on the connector, aiding users in quickly identifying the cable's capabilities.",
      "This feature is considered practical and beneficial in mitigating the general confusion when identifying a specific USB-C cable among many.",
      "Elgato has further plans to incorporate this specification data on all its upcoming USB and HDMI cables."
    ],
    "points": 277,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1699280115
  },
  {
    "id": 38169816,
    "title": "Celebrating Two Decades of Inkscape: Users Praise the Open-Source Vector Graphics Editor",
    "originLink": "https://inkscape.gitlab.io/inkscape-docs/awesome-web-pages/20th_anniversary/#frame4804",
    "originBody": "I love Inkscape, it's one of, if not, the most engaged projects in the FOSS world!And of course it is excellent software for working with SVG images! – Greenpete – Happy 20th Anniversary, Inkscape! I want to deeply thank the contributors for creating such a wonderful product. I've been using Inkscape for four years now and it has undergone remarkable change, coming back better each time. Growing my design skills with Inkscape is exciting. Each feature has its own unique function, helping me to express myself when designing. – Tifu Kelison – A big Thank you for your work over all these years for all the embroiderers! – St nelle – H[appy] b[irth]d[ay]! Thank you for all the things you do, especially for Ink/Stitch. – Mdds – I am very happy to be able to use Inkscape because it is free and easy to use. I hope Inkscape will continue to develop rapidly [...]. Thanks Inkscape developers and all! – ris – Another 20 years at least, thank you for all the nice diagrams you allowed me to draw <3 – Marci – Thank you very much to all developers and other contributors, past and present, for creating, maintaining, and improving this software. Although I am not a professional designer and I can utilize only littleof its real power, it helped me in many of my projects. Moreover, I enjoy very much using Inkscape. – Yusuf B – Congratulations on 20 years! Thank you for your continued commitment to developing polished software for professional designers. Living my values as a freelancer, through the use of open source graphic design software, is possible because of the work you do! Because of Inkscape, I am able to create professional works for clients of all sizes. Thank you so much for maintaining and improving my favorite piece of software! And in the meantime, I hope you have some time to celebrate! – Anonymous – I wish the project will stay as successful for another 20 years. I am extremely glad that there is a program of this quality available to everyone. – Böröczky Csaba – Congratulation to everyone. You have worked hard to bring Inkscapes to this level. Thanksssssss – Rene – Happy birthday, Inkscape. Thank you to the team for this product. I use it every weekend to create my designs and my embroidery files with Ink/Stich. I'm so proud to say about my customized cushions: \"I did it\". Many thanks for working on the software day by day. – Nath – All the best, keep up the great work and [I'm] looking forward to the next 20 years! Inkscape rocks! – Anonymous – Happy birthday, Inkscape! Thanks for this wonderful tool. I have been using it for many years. I am delighted with the great progress of the software. I use it for drawing, lasercutting and embroidery with the extension Ink/Stitch.[...] – Lyogusa – Happy anniversary to the whole team!!! What you're doing is cool!!! – Sandra Leclerc – Happy birthday to the whole team! Thank you for everything! Thanks to you I can personalize my embroidery projects! [...] – Michèle Botte – I appreciate the work put into this software, especially as it is free. Hopefully it surpasses [other software] so that I can stop being subject to predatory marketing schemes. – EAER – Amazing job! Keep going! You guys turn our tasks wonderful. We are very proud! – William Sanson – [...] I use Inkscape to make icons and also plan to print stuff [...]! I appreciate the work you guys do. Inkscape's awesome! – doomguy – Thank you very much for this sharing of passion, this development. The software helps me to understand hard computer drawings, but also, for my greatest pleasure, [it helps with] learning to create a file for my embroidery, thanks to the Ink/Stitch extension. Thank you a thousand times and Happy birthday Inkstitchement :-) – Edwige Fob – Happy birthday! Thank you very much for all the work on Inkscape and Ink/Stitch, very helpful and useful! – SignIsLife – Happy anniversary to the whole Inkscape team. It is a remarkable work that allows me to make beautiful embroideries. – Murielle – Thank you all so much for your hard work on Inkscape, I really appreciate it a lot. There are so many things you can do with Inkscape. This large pool of useful tools for all sorts of creative techniques is simply great. I have been intensively involved in the embroidery capabilities for several years now. Happy birthday, Inkscape. I'm excited to see how you will evolve over the next 20 years :) – Anonymous – Happy birthday, Inkscape. First, I used Inkscape only for drawing, then for my cutting machine, and then to create embroidery designs. I've been using Inkscape for five years and I'm so thankful for the incredible improvements. So wonderful to have such a powerful and versatile tool. So nice that it allows extensions. I use Inkscape with Ink/Stitch almost every day and I'm amazed at how good it is. Thanks everyone, and a long life to Inkscape. – Claudine Peyrat – Congratulations to the whole team and every contributor! Thanks for all your efforts throughout the years. Paying thousands of EUR for a piece of S[oft]W[are] is not possible when you just want to create private designs. You make this possible!!! – David – I've been using Inkscape for about 10 years and it's my favourite piece of software. I mainly use it to make fantasy maps and other goofy art, but it's possible to create so many different things with Inkscape. It's so flexible because it uses SVG. Thanks so much for all the work you guys have put in over the years. I truly appreciate your efforts in making and sharing something that benefits so many. Here's to the next ten years! – z3z – I found so many friends and fun through Inkscape. Hours of doing nonsense with Inkscape ... hours of starting over because autosave didn't work ;-; Get better, get faster learn new trix. Through the Ink/Stitch extension I found my love for machines, the technology and for hard- and software, thank you for that. <3 – TheHorrorOfEmbrodery – Happy 20th birthday, Inkscape. You are an awesome tool to work with! – Anonymous – The FOSS vector landscape was forever altered... with the arrival of Inkscape! I was a design apprentice, who happened to own a low spec system and couldn't afford proprietary apps. I'm glad, I made a right choice. Still today, happy to see Inkscape striding and covering almost all the capabilities of non-free apps. [...] – artistlibre – Thank you for making such a good FOSS product. Every time I use it, I keep wondering how you managed to build the product with that level of quality. – Anonymous – More innovation and a healthy life to all the developers working on the project! – Claudius – May developers and their teams always enjoy good health and happiness in their endeavors – Anonymous – May Inkscape get a lot of recognition in the future, and may more people support the open source community. – GRAVIK GRATIS – I can not thank the Inkscape team enough for this masterpiece software. It helped me change my life to the better. I have created many works of art and generated a lot of profit using this app. I wish you guys a beautiful journey and life in the future to come. – Mohammad Qowaidul Umam – Inkscape will be even cooler in the future and will continue to improve with a variety of powerful features. Hopefully next year Inkscape will be willing to create an Inkscape Design Conf. in Indonesia! – Rania Amina – Inkscape, I wish you grow and grow so much to become industry standard. You deserve that! – Dominik – Thank you for putting the i in Libre Software. – Anonymous – Thank you for the magical experience. Inkscape has helped me kick-start a career in design. – Rj – You all need to know that you are fantastic. Really fantastic. Thank you so much. – Lib4Knight – Wishing many more years of awesome developments and happy users! xoxo – @ikbensiep – Keep on working on this wonder. I love Inkscape. – nico – Thank you for your gift to the world! Though my history with Inkscape doesn't span 20 years, I've been using it for at least 10, and it's amazing to me how far it has come. – Anonymous – I remember the days of Sodipodi and watching Inkscape come upon the open-source scene was amazing to witness. It's unbelievable how far this application has come, it's existence and availability to creatives starting out has meant more than I can express. I come from a small town with parents who didn't believe I could make it as a designer but went on to get my BFA in graphic design and landed a decent job at a design company. None of this would have been possible without Inkscape in those early days when no one else believed in me. I'm so grateful to all those who put so much time and love into this program. Thank you for all the creative escapades throughout these years! – Megan E. Moore – I use Inkscape for all my vector designing needs, and even started contributingto its translations. Thanks for such a powerful program and inclusive community! I hope the best for everyone involved, and I wish Inkscape will thrive for years to come! – ltlnx – Thank you! Long live Inkscape! – Anonymous – Happy anniversary and thank you very much – Sylvie – For Inkscape's 20th anniversary, Inkscape Community Members would like to thank you for your work which made this possible. In the course of the last 20 years, Inkscape has enabled a whole generation of FLOSS users to Draw Freely. Dear Inkscape Contributor, Use arrow keys or left/right click to navigate, key t or middle click opens frame list. The following messages have been collected from them. Enjoy & Celebrate! Credits Original image: Sreya Saju Questionnaire texts: Michèle Thibeau, Pacer, Martin Owens and Maren Hachmann in Nextcloud Slideshow: made with and sozi by Maren Hachmann License: CC-By-SA 4.0 Happy 20th anniversary to the Inkscape Project and its amazing contributors! Your dedication to creating a free, open-source vector graphics editor has empowered me in the last 5 years, and countless artists, designers, and creators around the world, to bring our visions to life. Thank you for your hard work and commitment to the creative community. Here's to another 20 years of innovation and inspiration! – Habibu (Desigveloper) – Nov 6, 2023 1 Dear Contributor A Whole Generation Community Enjoy & Celebrate Greenpete Tifu Kelison St nelle Mdds ris Marci Yusuf B Anonymous Böröczky Csaba Rene Nath Anonymous Lyogusa Sandra Leclerc Michèle Botte EAER William Sanson doomguy Sylvie Edwige Fob SignIsLife Murielle Anonymous Claudine Peyrat David z3z TheHorrorOfEmbrodery Anonymous artistlibre Anonymous Claudius Anonymous GRAVIK GRATIS Mohammad Qowaidul Umam Rania Amina Dominik Habibu Anonymous Rj Lib4Knight ikbensiep nico Anonymous Megan E. Moore ltlnx Anonymous Credits",
    "commentLink": "https://news.ycombinator.com/item?id=38169816",
    "commentBody": "Happy 20th Birthday to InkscapeHacker NewspastloginHappy 20th Birthday to Inkscape (inkscape.gitlab.io) 257 points by haubry 11 hours ago| hidepastfavorite31 comments ygra 4 hours agoInkscape has been awesome, both as an editor for SVGs that leverages whatever SVG can support, and as a tool in automated processes. While I haven&#x27;t used it much when I vectorized a lot of things on Wikimedia Commons (because handwriting the files was easier for some enough geometric shapes), I&#x27;ve been using it for a few years now as part of the pipeline to build a card game [1]. With the exception of the CMYK conversion at the end, I&#x27;ve managed to build everything with open source software and Inkscape has been invaluable.[1] https:&#x2F;&#x2F;fallacygame.com&#x2F; reply lallysingh 10 hours agoprevInkscape is fantastic. When I needed to quote a diagram in a paper, I could just import the quoted article&#x27;s PDF into inkscape and just take the diagram. A quiet super powwr, frankly. Also when screwing around with d3.js, I could create elaborate SVGs in inkscape, name the nodes I cared about, and alter them programmatically in d3. Great tool reply __mharrison__ 9 hours agoprevInkscape is getting a little old in the tooth. I&#x27;ve used it on Linux, Windows, and Mac. Even though the features and updates are much less frequent than in the old Sodipodi days, I still haven&#x27;t found the a tool that works better for my needs.I use it to generate images and graphics for my books. I wrote an extension that lets me create a layer where I create rectangles with ids that will correspond to PNG filenames. When I run the extension it makes the layer invisible and exports each rectangle. reply jacquesm 11 hours agoprevInkscape is extremely useful for me in combination with a laser cutter. reply MarkSweep 10 hours agoparentAlso embroidery machines and fabric cutting machines:https:&#x2F;&#x2F;inkstitch.org&#x2F; reply ninju 11 hours agoparentprevDo you have good resource&#x2F;tutorial for learning how to use a laser cutter?Our local makerspace has a cutter and I&#x27;ve been intrigued about how to use it.I&#x27;ve got experience in 3D modelling (beginner Fusion360 user) but have never worked with designing for a cutter. reply sen 10 hours agorootparent- Design what you want- Save as SVG- Import into LightBurn- Set your speeds and strengths- Export to laser cutterThat’s very simplified, as designing for laser cutters has some “gotchas” (merging shapes to create connected outlines, cut vs etch, etch before cut, inside parts before outlines, etc etc), but it’s the general idea and can get you started.I generally recommend starting with basic stuff like text in a simple shape that consists of a single cut line (keyring, nameplate on a door, Christmas bauble, etc). Etch the text, cut out the shape around it. Work your way up from there. reply jacquesm 10 hours agorootparent&#x27;Export to laser cutter&#x27; -> set speed; intensity for layers; press &#x27;play&#x27; and sit back.It&#x27;s so easy it&#x27;s deceptive.Oh, and wear those safety glasses. reply yonatan8070 39 minutes agorootparentAt our local makerspace the laser cutter has a microswitch attached in parallel with the laster tube, so it won&#x27;t output anything if the lid isn&#x27;t fully closed reply sleepytimetea 11 hours agorootparentprevDo you have experience with 3D printers or CNC machines ? Essentially , a laser engraver&#x2F;cutter just needs a 2D drawing like an SVG or DXF and it turns on the laser once it moves the laser head. reply WillAdams 10 hours agorootparentprevLasers are 2D, so arguably simpler.Draw up a design, then assign toolpaths to:- cut all the way through- engrave making a light--dark mark reply CodeWriter23 11 hours agorootparentprevTons of maker vids on YouTube. reply eggfriedrice 10 hours agoparentprevI&#x27;ve also lasered lots of things from Inkscape drawings. I&#x27;ve always found it easier to use than proper CAD but still get good results.I don&#x27;t remember this much, but a previous me wrote a guide for lasering using Inkscape for Edinburgh Hacklab. I don&#x27;t think the process has changed much... https:&#x2F;&#x2F;wiki.ehlab.uk&#x2F;inkscape_for_lasering reply CodeWriter23 11 hours agoparentprevI had no idea it had been around so long. I only discovered it about 2 years ago when I wanted to convert some multiple-layer files from Illustrator to .STL for 3D Printing. reply WillAdams 10 hours agorootparentIt was preceded by sodipodi:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sodipodi reply raphlinus 9 hours agorootparentAnd before that Gill[1].[1]: https:&#x2F;&#x2F;levien.com&#x2F;svg&#x2F; reply kderbe 9 hours agoprevOn my 2019 MBP, the thank-you frame transition is choppy in Firefox and very choppy in Safari. In my brief attempt to use the browsers&#x27; performance profilers, it looks like 40ms per paint in Firefox and 300ms per paint in Safari. Chrome is less than 2ms per paint and only drops about 20% of rendered frames.Can anyone with more browser perf debugging experience explain why this animation is so janky, especially on non-Chrome browsers? reply CapitalistCartr 10 hours agoprevI love Inkscape. Along with Gimp, it covers all my non-artist needs. For work or personal. reply tfrutuoso 10 hours agoprevHappy birthday to Inkscape and kudos to all those that shaped it into what it is today! reply polivier 11 hours agoprevI use Inkscape to create simple yet high-quality SVG images for technical presentations&#x2F;talks. It&#x27;s great and it integrates somewhat easily with LaTeX too. reply ChrisArchitect 11 hours agoprevIs this official? I mean, where has it been shared officially like on the forum or blog? reply hgs3 10 hours agoparentThey have a celebratory tweet [1]. Wikipedia lists the initial release as November 6, 2003 [2].[1] https:&#x2F;&#x2F;twitter.com&#x2F;inkscape&#x2F;status&#x2F;1721123809002917936[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Inkscape reply ChrisArchitect 10 hours agorootparentThat&#x27;s not the link tho. That&#x27;s something else.Where did this come from? reply sen 10 hours agoprevI use Inkscape pretty much every single day, and it’s in my top 5 pieces of software ever made.I’ve used it professionally in industries dominated by Adobe crapware, and in pretty much all my hobbies, both physical and digital. reply Animats 9 hours agoprevIt really is quite nice at this point.Is there some way to export pure base SVG, without any of that stuff in the \"inkscape\" namespace that carries around page size and such? reply jstrieb 9 hours agoparentIn the \"Save as...\" pop-up, you can set the file type to \"Optimized SVG\" (among other SVG output types, if I remember correctly). Picking optimized SVG pops up another dialog on save that can be used to configure the optimization passes.I regularly use this to export small SVGs for the web. reply dawidpotocki 6 hours agoparentprevYou can select \"Plain SVG\" instead of \"Inkscape SVG\" in the file saving dialogue. reply flobosg 10 hours agoprevI made my doctoral defense slides with Inkscape. It was a lot of work, but it looked beautiful! reply squigz 10 hours agoprevHappy birthday! And a big thanks to all Inkscape developers and contributors!Long live FOSS <3 reply krupan 9 hours agoprevPro tip: definitely go through a tutorial when you first try to use inkscape, at least if you are like me with no previous vector drawing experience reply muppetman 10 hours agoprev [–] TIL about Inkscape! Nice! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Inkscape, the open-source vector graphics editor, is celebrating its 20th anniversary, garnering appreciation and gratitude from its users and contributors.",
      "Users of the platform commend its role in numerous creative fields such as professional design, embroidery, and icon creation, praising its flexibility, distinctive features, and consistent improvement.",
      "The community expresses their thanks to Inkscape's developers and contributors, and hopes for the platform's further success and growth in the coming decades."
    ],
    "commentSummary": [
      "Inkscape, the free, open-source vector graphics editor, is celebrated on its 20th anniversary by its users and contributors.",
      "Users acclaim Inkscape for its pivotal role in various creative fields like professional design work, embroidery designs, and icon creation, praising its versatility, unique features, and consistent enhancements.",
      "The contributors and developers of Inkscape receive gratitude for their commitment and hard work, with well-wishers hoping for continued growth and success over the forthcoming two decades."
    ],
    "points": 257,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1699308759
  },
  {
    "id": 38171322,
    "title": "Bluesky Refactors PDS for Single-Tenant SQLite, Introducing Separate User Files, New Interaction Classes, and Streaming Replication",
    "originLink": "https://github.com/bluesky-social/atproto/pull/1705",
    "originBody": "bluesky-social / atproto Public Notifications Fork 342 Star 5k Code Issues 107 Pull requests 44 Discussions Actions Projects Security Insights New issue Jump to bottom Pds sqlite refactor #1705 Merged dholms merged 143 commits into pds-v2 from pds-sqlite-refactor Merged Pds sqlite refactor #1705 dholms merged 143 commits into pds-v2 from pds-sqlite-refactor +3,336 −5,421 Conversation 71 Commits 143 Checks 10 Files changed 190 Conversation Collaborator dholms commented • edited This refactors the PDS to use a single-tenant SQLite datastore. Each user has their own SQLite file that stores their repo and private account state. User databases are stored hierarchically as such: /${dbDirectory}/${sha256Hex(did).slice(0,2)}/${did} Repo signing keys for each repo are stored alongside the SQLite file. We also switch out the abstraction for interacting with user data to an ActorStore. The primary difference between the ActorStore and our previous \"services\" is that the ActorStore has different classes for reading & writing. Since SQLite does not support concurrent transactions, the ActorStore requires that you clearly \"transact\" with the store in order to make a write. We also maintain an LRUCache for signing keys and databases. We allow 30k open file handles and 30k keys held in memory. When a database falls out of the cache, we ensure that we close the file handle. We also introduce 3 separate SQLite databases for managing service state: service DB for account info, invite codes, refresh tokens etc did cache DB with only one table for caching DID resolutions sequencer DB with only one table for sequencing repo updates across all repos on a service. Each of these SQLite files in run in WAL mode to allow for concurrent reads and streaming replication. We intend to ship the PDS distribution with Litestream or something similar 1 dholms added 30 commits wip 99862e3 wip f9e9096 rework readers & transactors 82fd36a yay it compiles again 66bca94 wip 8f9e3eb crud all working 9bd5ca9 tidy store interface 98bfbea clean up sequencing 2ca5010 get sequencer working 5262b00 fixing build errors ea68d14 handle races on actor store transacts 00c2a3b fix file uploads tests 554e88b spec out new simple pds mod routes cfc525b introduce new admin state endpoints db53a27 merge 6135d32 wire up routes cae1381 Merge branch 'mod-account-state' into simplify-pds-mod 76f6326 clean up pds f801465 revoke refresh tokens ff7365f Merge branch 'mod-account-state' into simplify-pds-mod 05c65b7 getUserAccountInfo 68f502c pr tidy 7c76bed Merge branch 'mod-account-state' into simplify-pds-mod dcc0048 mege 3da5105 tidy 1460dd2 clean up tests & blobstore b2804a6 fixing up more tests 2f02c1c clean up rest of tests eb8d8a9 actor store in lru cache b1c565a fix open handles 6978e46 114 hidden items Load more… dholms added 10 commits tidy 5a8cd30 better config for actorstore & dbs a36944c clean up cfg more 396c431 reorg actorstore fs layout b22835a handle erros on actor db create f9f1171 pr tidy & fix accoutn deletion test 49a01ac pr feedback 0366ee8 merge main 8ca484f fix bad merge 3af8e15 unskip test 37807a8 devinivy reviewed View reviewed changes packages/pds/src/api/com/atproto/server/requestEmailConfirmation.ts Outdated Show resolved Hide resolved devinivy reviewed View reviewed changes packages/pds/src/api/com/atproto/sync/getRecord.ts Outdated Show resolved Hide resolved dholms added 3 commits fix subscribe repos tests 0cb274f tidy repo root tables 9b66921 tidy f25cef4 devinivy reviewed View reviewed changes packages/pds/src/sequencer/sequencer.ts Show resolved Hide resolved devinivy reviewed View reviewed changes packages/pds/src/api/com/atproto/server/createAccount.ts Show resolved Hide resolved fix tests d891b30 devinivy approved these changes View reviewed changes Collaborator devinivy left a comment Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Left a good handful of notes and comments, but seriously wow 💎 tons of great simplifications in here and things feel very tidy. dholms added 3 commits bulk deletesg 2df9506 increase chunk size 5d7e838 tweak sequencer 36a5412 dholms changed the base branch from main to pds-v2 dholms added 3 commits deleted app migration table f84f5ce Merge branch 'pds-v2' into pds-sqlite-refactor bf6d725 patch up new auth test 2166add dholms mentioned this pull request Feature branch: PDS v2 #1789 Draft Hide details View details dholms merged commit 8449ceb into pds-v2 10 checks passed dholms deleted the pds-sqlite-refactor branch xueyuanl mentioned this pull request Daily Hacker News 07-11-2023 xueyuanl/daily-hackernews#1157 Open Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment Reviewers devinivy Assignees No one assigned Labels None yet Projects None yet Milestone No milestone Development Successfully merging this pull request may close these issues. None yet 2 participants",
    "commentLink": "https://news.ycombinator.com/item?id=38171322",
    "commentBody": "Bluesky migrates to single-tenant SQLiteHacker NewspastloginBluesky migrates to single-tenant SQLite (github.com/bluesky-social) 252 points by HillRat 9 hours ago| hidepastfavorite144 comments lucasyvas 8 hours agoLove SQLite - in general there are many challenges with a schema or database per tenant setup of any kind though. Consider the luxury of row-level security in a shared instance where your migration either works or rolls back. Not now! If you are doing a data migration and failed to account for some unexpected data, now you have people on different schema versions until you figure it out. Now, yes, if you are at sharding scale this may occur anyway, but consider that before you hit that point, a single database is easiest.You will possibly want to combine the data for some reason in the future as well. Or, move ownership of resources atomically.I&#x27;m not opposed to this setup at all and it does have its place. But we are running away from schema-per-tenant setup at warp speed at work. There are so many issues if you don&#x27;t invest in it properly and I don&#x27;t think many are prepared when they initially have the idea.The funny thing is that about a decade ago, the app was born on a SQLite per tenant setup, then it moved to schema per tenant on Postgres, now it&#x27;s finally moving to a single schema with RLS. So, the exact opposite progression. reply dilyevsky 6 hours agoparentI dont know - I have experience working with monster DBs in production and never again. Under large enough load every change becomes risky because you can’t test performance corner cases fully. Having a free-tier user take out your prod because they found a non-indexed code path is also classic reply gigatexal 14 minutes agoparentprev> If you are doing a data migration and failed to account for some unexpected data, now you have people on different schema versions until you figure it out. Now, yes, if you are at sharding scale this may occur anyway, but consider that before you hit that point, a single database is easiest.This can be accounted for and handled. Though if schema issues are enough of a scare I wonder if a documentdb style embedable database like a couch&#x2F;pouchdb might make more sense. reply ngrilly 50 minutes agoparentprev> The funny thing is that about a decade ago, the app was born on a SQLite per tenant setup, then it moved to schema per tenant on Postgres, now it&#x27;s finally moving to a single schema with RLS.To be fair, RLS was not available yet a decade ago :) It appeared in PostgreSQL 9.5 in 2016. reply starcraft2wol 8 hours agoparentprev> now you have people on different schema versions until you figure it out.That can be a good thing if your product has sayIf you are doing a data migration and failed to account for some unexpected data, now you have people on different schema versions until you figure it out.That shouldn&#x27;t be a big issue. Any service large&#x2F;complex enough to care does the schema upgrades in phases, so it&#x27;s 1. Make code future compatible. 2. Migrate data. 3. Remove old schema support.So typically it should be safe to run between steps 1 and 2 for a long time. (Modulo new bugs of course) As an ops-y person I&#x27;m comfortable with the system running mid-migration as long as the steps as described are used. reply gigatexal 13 minutes agorootparent> That shouldn&#x27;t be a big issue. Any service large&#x2F;complex enough to care does the schema upgrades in phases, so it&#x27;s 1. Make code future compatible. 2. Migrate data. 3. Remove old schema support.Exactly this, schema migrations should be an append, deprecate, drop operation over time. reply KRAKRISMOTT 6 hours agoparentprevhttps:&#x2F;&#x2F;blog.turso.tech&#x2F;introducing-embedded-replicas-deploy...https:&#x2F;&#x2F;electric-sql.com&#x2F; reply gigatexal 16 minutes agoprevAt a previous fintech role the company would store customer accounts as encrypted sqlite3 files on blob storage ... this worked out decently well for our access patterns. reply malkia 8 hours agoprevWhat do they mean by \"Since SQLite does not support concurrent transactions\" - it supports them, as long as you don&#x27;t access the .db file through a file share (UNC, or NFS, etc) - https:&#x2F;&#x2F;www.sqlite.org&#x2F;wal.htmlI&#x27;ve been using this to update&#x2F;read db from multiple threads&#x2F;processes on the same machine. You can also do snapshotting with the sqlite backup API, if you want consistent view, and to not hold on transaction (or copy in-memory).But maybe I&#x27;m missing something here... Also haven&#x27;t touched sqlite in years, so not sure... reply malkia 2 hours agoparentNope! I was mistaken - it&#x27;s really multiple readers, single writer - Probably I was assuming things the whole time, and did not spent enough time thoroughly checking - granted most of the db&#x27;s I&#x27;ve done with sqlite were more about reading than writing.So stand corrected! reply sinkwool 8 hours agoparentprev> Writers merely append new content to the end of the WAL file. Because writers do nothing that would interfere with the actions of readers, writers and readers can run at the same time. However, since there is only one WAL file, there can only be one writer at a time.I think the OP meant that updates have to run sequentially. reply starcraft2wol 8 hours agorootparentWhich just means the lock happens at user scope in this case instead of per table or row. This limitation still causes so much confusion when it’s a completely reasonable design. reply mikrotikker 3 hours agorootparentprevI&#x27;ve been importing data to sqlite databases running being actively written to for years. Just throws exception if the database is locked and I retry. Do 10k row batches, with a small sleep between. No issues. Helps if your use case doesn&#x27;t really care about data being in order I guess. reply manmal 3 hours agoparentprevDid you disable auto checkpointing? Wouldn’t checkpointing result in potential corruption or at least data loss if two processes do that simultaneously? Or is that scenario exhaustively prevented with a lock file? reply dilyevsky 7 hours agoparentprevProbably means there’s still no row-level locking at least last I checked, very limited table-level locking. The writer still grabs a lock on entire db per docs reply gigatexal 12 minutes agoprevThis should make leaving the service rather simple. Download your sqlite file and throw up a simple local-only html front end to the data and you&#x27;re solid. reply paxys 6 hours agoprevCool, but maybe let people actually use your service before everyone forgets what it is? reply nyx2d 5 hours agoparentThey have over 1.8 million users currently, or do you mean PDSes specifically? Federation is in open beta on a test network, you can try it out today if you&#x27;d like. reply Tomte 3 hours agorootparentI have been on the wait list since they launched. They seem to mostly rely on invites. reply felixthehat 2 hours agorootparentnext [–]bsky-social-scbch-eolha bsky-social-fs26y-d6gnv bsky-social-2lx5u-ntrdv bsky-social-hboq7-dyuue bsky-social-b2v3f-3a23q reply brylie 2 hours agorootparentThank you so very much! :-) reply WinstonSmith84 38 minutes agorootparentprevdamn, seems already all gone reply garblegarble 22 minutes agorootparentnext [–]bsky-social-lkzsp-7x7ja bsky-social-p4vwr-nrthu bsky-social-bdu6c-6tbv4 bsky-social-fkpgk-oestw reply WinstonSmith84 15 minutes agorootparentGot one, thank you sir! reply pnathan 11 minutes agorootparentprevI have a few invites. Email me and I&#x27;ll pass them out. :) reply BiteCode_dev 29 minutes agorootparentprevGot a few invites, DM me on twitter, substack or masto if you want them (listing on https:&#x2F;&#x2F;bitecode.dev) reply soperj 3 hours agorootparentprevIt shouldn&#x27;t be too difficult to find an invite? They hand them out pretty frequently. reply FlyingSnake 2 hours agorootparentprevI’ve some invites lying around. DM me if you want one. reply bugsmith 1 hour agorootparentI&#x27;d like to take you up on that, if you still have one going? reply troupo 2 hours agorootparentprev> They have over 1.8 million users currentlyHow many of them active? reply raverbashing 5 hours agorootparentprevCome on, \"over 1.8 million users\" is not an impressive numberThese kind of movements makes me think they&#x27;re not serious about scaling up. Wouldn&#x27;t surprise me if then end up as an also-ran reply pmontra 3 hours agorootparentMaybe not impressive but none of the services of my customers had or has 1.8 million users. And yet they do well (my customers.) reply rapnie 2 hours agorootparentThat. And none of the big social media platforms were big at the start either. reply raverbashing 1 hour agorootparentYes but 1.8Mi at a time where people are longing for a Twitter alternative is just leaving money on the table replyvictorbjorklund 1 hour agoprevCan someone that knows more about bluesky explain what data is stored in sqlite and not? Because i assume it isnt messages etc between users. reply williamcotton 42 minutes agoparentI assume that messages between users are stored in those SQLite DBs.Think email. When you send an email and CC five other people as well then seven people now have the same copy of the email stored on their email servers. That is, there’s no central database that contains a single email that is referenced by others.This is basically how sharding with relational DBs works as well.This sort of data denormalization is almost a requirement as applications scale and especially for many-to-many applications that have a high write to read ratio.Low write to read and you can get away with a single master to many slave relational DB architecture for quite astonishing numbers of requests and data! reply mythz 5 hours agoprevAlways happy to see more server SQLite&#x2F;Litestream adoption which we&#x27;ve also been using to build our new Apps with.SQLite + Litestream is an even greater choice for tenant databases, that&#x27;s vastly cheaper to replicate&#x2F;backup to S3&#x2F;R2 than expensive cloud managed databases [1] (up to 3900% cheaper vs SQLServer on Azure).[1] https:&#x2F;&#x2F;docs.servicestack.net&#x2F;ormlite&#x2F;litestream reply focom 9 hours agoprevI am curious, does the HN folks know if bluesky is more active than nostr or the mastodon network? reply steeleduncan 1 hour agoparentI don&#x27;t know about nostr, but I find it is a lot less active than Mastodon. In general the tech accounts I am interested in have moved to Mastodon rather than bluesky. I imagine this would depend on whose activity you are interested in, and where they have chosen to migrate to reply muglug 8 hours agoparentprevLess active than Mastodon, I&#x27;d assume more active than Nostr.But the interesting thing for me isn&#x27;t activity — it&#x27;s the people on there.Of the cohort who had >100k followers on Twitter, I think more of them post regularly on Bluesky than post on Mastodon. Bluesky definitely has a more cohesive feel, especially because there&#x27;s currently just one instance & mod team. reply StevenXC 7 hours agorootparentI&#x27;m a donating supporter of the Mathstodon.xyz instance, but (sadly?) most of \"math Twitter\", at least the education-focused university faculty, ended up on BlueSky. I think there&#x27;s a strong appeal for \"a straight forward Twitter clone without Musk\" for a lot of people. reply spookie 7 hours agorootparentprevMastodon, and the Fediverse in general, make user interaction decisions on purpose to limit many of the issues common to social media. Think about: mob culture, addiction, and the like.I wonder if BlueSky intends to follow on those. For example, hiding user actions counts (repeats, favourites, etc...) until the user acts on one.Things like these may be strange for those accustumed to Twitter, but personally, is what makes me stick with smaller instances on the Fediverse. reply indigochill 1 hour agorootparentMy bet is regardless of any initial good intentions, since BlueSky is a company, market pressures will inevitably force them into dark patterns like we see on every other commercial social network (going back to the early days of the companies, Facebook, Twitter, and even Google looked really good early on until all were corrupted by profit motive). My belief is that the profit motive is necessarily at odds with free communication.To me, the ActivityPub network (Mastodon and friends) is relatively unique in the social media space in having no direct commercial pressures (the protocol is developed by W3C) and therefore being inoculated against the causes for these dark patterns. reply BryantD 9 hours agoparentprevLess active than Mastodon, more active than nostr.https:&#x2F;&#x2F;vqv.app&#x2F;stats&#x2F;chart is useful for Bluesky and draws on the Bluesky firehose for data. https:&#x2F;&#x2F;stats.nostr.band&#x2F; seems useful for nostr. reply nvy 8 hours agoparentprevBluesky is pretty positive and definitely lacks the \"American Suburbia HOA\" energy that some Mastodon instances have.It&#x27;s pretty active during North American hours. reply nemo44x 7 hours agoparentprevThey’re all just arbitrary ghettos that aren’t dissimilar to each other. None of them matter in terms of influence but are like nice Reddit boards for certain interests. reply lukevp 9 hours agoprevInteresting... I like the strategy of having each user be 1:1 with a DB. What would be done for data that needs to be aggregated across users though? If I&#x27;m subscribed to another user and they post, how does my DB get updated with that new post? Or is this meant just for durable data and not feed data (like profile data, which users are followed &#x2F; not followed &#x2F; etc.) and all the interactive stuff happens separately?I like that \"connection pooling\" is just limiting the number of open handles in a LRU cache. It&#x27;s also interesting because instead of having to manage concurrency at the connection level, it handles it at the tenancy level since each DB connection is single-threaded. You could build up per-DB rate limiting on top of this pretty easily to prevent abuse by a given user.Is there a straightforward way to set up Litestream to handle any arbitrary number of DBs? reply wmf 8 hours agoparenthttps:&#x2F;&#x2F;blueskyweb.xyz&#x2F;blog&#x2F;5-5-2023-federation-architecture reply Retr0id 8 hours agorootparentTo summarise the relevant details, the \"AppView\" service is responsible for the sorts of queries that aggregate across users, and that has its own database setup - I think postgres but I&#x27;m not 100% sure on that. reply jakebsky 8 hours agorootparentYou&#x27;re right, as usual. AppView is on a Postgres cluster with read replicas doing timeline generation (and other things) on-demand. We&#x27;re in the process of moving it toward a beefy ScyllaDB cluster designed around a fanout-on-write system.The v1 backend system was optimized for rapid development and served us well. The v2 backend will be somewhat less flexible (no joins!) but is designed for much higher scale. reply manmal 3 hours agorootparentDoes the BGS pull all the tenant‘s individual SQLite data? Or do the PDS push new posts to the BGS? reply rapnie 1 hour agorootparentprev> The BGS handles \"big-world\" networking. It crawls the network, gathering as much data as it can, and outputs it in one big stream for other services to use. It’s analogous to a firehose provider or a super-powered relay node.\"Big-world\" networking by Big Tech-to-be Bluesky with super-powers, I wonder? Is this BGS also going to be federated, or is that the big centralized beating heart of this platform managed exclusively by BS? reply xrd 8 hours agoprevIs Bluesky still invite only? reply jakebsky 8 hours agoparentIt is, but not as a \"growth hack\" or anything. It&#x27;s just a way of limiting growth while the system is scaled (in terms of the backend and abuse prevention).There&#x27;s a dedicated waitlist for developers that will get you access quite quickly: https:&#x2F;&#x2F;atproto.com&#x2F;blog&#x2F;call-for-developers reply nvy 8 hours agoparentprevYes. I have invite codes if you would like one. Email in my profile.Edit: they&#x27;re all gone! reply lucb1e 8 hours agorootparentWas browsing around your website (mentioned in profile), noticed https:&#x2F;&#x2F;0x85.org&#x2F;contact.html only mentions Twitter and email. Maybe the bluesky omission is intentional, but probably it just hasn&#x27;t been updated yet? I&#x27;m not on bsky myself, currently having fun on mastodon and I&#x27;m not familiar with bsky enough to know what I&#x27;m missing out on, but for other folks I figured I&#x27;d mention it reply nvy 8 hours agorootparentHey thanks. It&#x27;s just outdated, what with young kids and grad school. Appreciate the note. reply eitland 3 hours agorootparentprevIf anyone else still has invites I am also interested.My mail is at the bottom of my bio. reply kin3tik 4 hours agorootparentprevI&#x27;d also love a code if anyone has any to spare (email in bio) reply JosephK 7 hours agorootparentprevIs your offer invite code available to other randoms like myself? I tried to register on bsky months ago and still haven&#x27;t been approved. reply defatigable 7 hours agorootparentI have some extra if you&#x27;d like one. Let me know how to get it to you and I will. reply pests 3 hours agorootparentI&#x27;m still trying to get one if anyone see&#x27;s this. Keep missing the ones posted. Email in profile.Thanks. reply duvara 6 hours agorootparentprevWould also like one if you have an extra. Thx in advance. (Click on username to see my email in profile) reply duvara 4 hours agorootparentGot an invite from another user. Thx!!! reply Pi9h 5 hours agorootparentprevSent you a code. reply azemetre 7 hours agorootparentprevIf you&#x27;re offering I&#x27;d love one. My email is my username on hackernews at gmail.com reply Pi9h 6 hours agorootparentSent you an invite code. reply AnonC 4 hours agorootparentCould you send me one? Email in my profile. Thanks. reply nvy 6 hours agorootparentprevI&#x27;m happy to give them to any HN user but I&#x27;m afraid I have only three left and there are three emails in my inbox asking for invites, so if one is you then congratulations! Otherwise, sorry. reply panja 5 hours agorootparentprevI&#x27;ll also add my 4 invite codes if anyone wants themEDIT: I&#x27;m fresh out for now, sorry! reply xarope 4 hours agorootparentprevif anyone still has any codes, please DM me one, email in my profile. Thanks muchly reply anonyfox 1 hour agoprevSlightly related: is Bluesky moderated good enough or do I get lots of rightwing and conspiracy crap like on twitter currently?I‘d really love to have some more civilized hub again that isn’t full of hate and anti-intellectualism. reply elAhmo 1 hour agoparentI think it is still too small and people seem quite nice there. But, that has its drawbacks, as I keep returning to Twitter due to the slow migration in the recent months.It is a shame, as it seems like a nice alternative that has some cool ideas. reply winrid 6 hours agoprevWhy sha256 hash the user into to get a two character target directory? Wouldn&#x27;t md5 be much faster and solve the same problem? reply paulryanrogers 6 hours agoparentAt their scale maybe they&#x27;re worried about collisions?Or, like me, they&#x27;re drowning in security tooling from corporate and don&#x27;t want to have to carve out exceptions for md5 usage in each. reply RaisingSpear 1 hour agorootparent> At their scale maybe they&#x27;re worried about collisions?With their scheme, collisions are already guaranteed to happen if they have >256 users. reply wmf 6 hours agoparentprevIt&#x27;s probably not healthy to have broken cryptographic hashes running around. If you don&#x27;t need a secure hash there are plenty of fast non-cryptographic hashes. reply moreati 2 hours agoparentprevAt a guess: that hash is performed relatively few times, so any performance difference is lost in the noise floor. Never having to answer \"why did you use this insecure hash\" or eliminating&#x2F;minimising any possibility of a class of security problem is worth more. reply alex_suzuki 4 hours agoparentprevThis is probably not about collisions but about filesystem limitations (max number of files in a directory). reply andrewstuart 2 hours agoprevI sure hope they don’t ever want to change their db structure.Why not use Postgres with RBAC (Row Based Access Control). reply hknmtt 2 hours agoprevgood luck with running updates. reply scorpion_farmer 8 hours agoprevThat looks like the PR from hell - 190 files changed, 143 commits? Mostly with names like \"tidy\" and \"wip\"Props to whoever actually reviewed that, you are a warrior reply lrx 7 hours agoparentI prefer to read the unified diff and commits don&#x27;t matter as much. reply MenhirMike 7 hours agorootparentSame. Do whatever you want in your feature branch, what matters is the Files list and the description in the PR. The whole thing gets squashed into a single commit anyway (which also makes reverting much easier). reply eitland 3 hours agorootparentReverts are also easy even if one merges the whole branch. Just revert the merge commit.I almost never look at them, but once in a while it is really great to see the thought process that led to something. reply readline_prompt 7 hours agorootparentprevdon&#x27;t know why, but recent teams around me have always made strict rules about number of commits in PRs. I just wanted to tell them the same thing you said: \"Why don&#x27;t you just look at the diffs?\" curious for other opinions. (sorry not really about this particular topic) reply lolinder 7 hours agorootparentI prefer to have clear commits that tell a tidy story. For example:* Refactor function `foo` to accept a second parameter* Add function `bar`* Use `bar` and `foo` in component `Baz` to implement feature #XIf you give me a commit history like this, I can easily validate that each step in your claimed process does what you describe.If you instead give me a messy history and ask me to read the diff, you might know that the change to file `Something.ts` on line 125 was conceptually part of the refactor to `foo`, but I&#x27;ll have to piece that together myself. It&#x27;s not obvious to the person who didn&#x27;t write the code what the purpose of any given change was supposed to be.This isn&#x27;t a huge deal if your team&#x27;s process is such that each step above is a PR on its own, but if your PRs are at the coarseness of a full feature, it&#x27;s helpful to break down the sub-steps into smaller (but sane and readable) diffs. reply 88913527 7 hours agorootparentThis is reasonable, but the problem I encounter is how stifling it seems to ask others to structure their work so specifically. By way of comparison, getting compliance on conventional commit messages is a challenge, and that&#x27;s an appreciably smaller ask than this. reply lolinder 7 hours agorootparentOh, for sure. This is how I structure my own PRs, but I&#x27;ve certainly never bothered to ask a coworker to do so, I just appreciate it when I see it.That said, OP is in an environment where it sounds like this kind of structure is already the cultural norm. reply eitland 2 hours agorootparentFrom another one who tries to do the same (but doesn&#x27;t enforce it):Thanks! reply dilyevsky 5 hours agorootparentprevIn the context of Github PR you can’t leave reviews on commits other than what’s currently the tip commit of the pr branch so structuring this way is just wasted effort.What you should be doing is breaking down PRs more finely so that your unrelated refactors are all separate single-commit PRs. That ofc requires that your pr review round trip time is fast reply lolinder 3 hours agorootparentI&#x27;m pretty sure I&#x27;ve left comments on a commit before in a GitHub PR. The comment just goes in the right place in the PR diff, assuming no changes, or comments can actually be attached to commits themselves (which is what happens when a comment becomes stale—it retains a reference to the original commit). reply krainboltgreene 6 hours agorootparentprevFunny that two of your commits don&#x27;t actually tell us why they exist, one simply describes the diff (which you should never need lol?) and the other proxies that responsibility to some other system.You could have simply randomized the text in each commit, put the ticket id and the one \"why\" in the merge commit body and gotten the same end result amount of real information in the end. reply lolinder 3 hours agorootparentThe first line of the commit message isn&#x27;t about including information that couldn&#x27;t be gleaned from the commit. That can be done in subsequent lines. The first line is for two purposes:* Priming the reader so they are able to quickly interpret what they&#x27;re seeing when they open the commit.* Making it easy to search or scan for a specific change.The last commit message in my example would probably have included the name of the feature as well as the ticket number, but I couldn&#x27;t be bothered to invent an actual feature name.DRY doesn&#x27;t really apply to technical writing, at least not as extremely as you seem to think it should. Headings are supposed to summarize the contents, and that&#x27;s what commit messages are: headings. reply FPGAhacker 7 hours agorootparentprevSquash is our git given right. reply ukzwc0 7 hours agorootparentprevA good practice is to rebase your commits before creating a PR into a single commit. You are free to commit as many times as you want to while doing your work. This minimizes the noise in the log. reply recursive 7 hours agorootparentprevEasy workaround. Start with feature branch f.1. Branch f-prime from master. 2. Squash merge f to f-prime. 3. Pull request f-prime to master. 4. Profit. reply gonzo41 7 hours agorootparentprevCommit and push often. Put a novel explaining yourself in the PR. And that&#x27;s enough IMO. reply _heimdall 7 hours agorootparentprevCommit your code and commit it often. There&#x27;s no reason not to. reply nkozyra 7 hours agorootparentSure, but then there&#x27;s nothing wrong with rebasing it and making a nicer story for other people that want to review it.Diffs are great but sometimes they&#x27;re just as overwhelming in a huge PR. It&#x27;s nice to first follow 5-10 commits in chunks of logical change. reply arein3 1 hour agorootparentI don&#x27;t know why people are obsessed with squash merging. I always rebase (when needed) to preserve commit history. It&#x27;s a good best practice, and makes it easier to spot errors after fixing conflicts.I suspect squashers use the wrong tools. Use source tree, or, if you are on linux, smartgit. You can see a detailed log, which makes it much easier. reply dilyevsky 5 hours agorootparentprevDont send huge prs. They are hard&#x2F;impossible to review anyway with good commit history or not reply lolinder 7 hours agorootparentprevCommit and commit often, but then clean up the history into discrete, readable chunks.If your PRs are tiny it&#x27;s not a big deal, but with 190 files changed in this one, it absolutely should have been rebased into a more reasonable commit history. reply aantix 7 hours agorootparentprevUnless you’d like to maintain your train of thought.I don’t want to interrupt my flow with intermediary commits. reply h0l0cube 7 hours agorootparentprevAlso continuously integrate (from trunk) if you want to hit that moving target sooner. reply tehlike 7 hours agorootparentprevThis is the answer. reply throwaway892238 7 hours agorootparentprevI don&#x27;t think any method is gonna make it easy to grok 3,336 added lines and 5,421 removed reply jakebsky 7 hours agoparentprevThose two work very closely together, so probably not as nightmarish as it may appear to an observer. But, the two of them are most certainly warriors. reply 4death4 7 hours agoparentprevWhat if it was 190 files changed in 1 commit, would that make a difference? reply rgoulter 6 hours agorootparentIt might.With commits like \"typo\", you might as well squash these into the commit which introduced the typo in the changeset.If there are changes across many files, and the changes were made automatically with some search-and-replace (or some refactoring tool).. by having a commit that&#x27;s only that automatic change, it&#x27;s easy to look at that commit and tell what the changes were. -- Presumably, non-automatic changes are going to be smaller.I guess roughly, if it makes sense to apply a changeset that changes 5 things, you&#x27;d want 5 commits. Having commits like \"typo\" means there are more commits; but squashing those 5 things together makes it harder to discern the granular change. reply numbsafari 7 hours agoparentprev> Props to whoever actually reviewed that, you are a warriorOr a ghost. reply scientaster2 7 hours agoparentprevlgtm reply pvg 9 hours agoprevThis seems like a very misleading title, the Bluesky PDS is the meant-for-selfhosting thing they distribute, not the bluesky service as experienced and used by most of its users. reply typical182 8 hours agoparentThe “Personal” in PDS doesn’t mean it is only for self-hosting.Bluesky has a main PDS instance at https:&#x2F;&#x2F;bsky.social that serves almost all of the Bluesky user base.There is a good overview of the architecture here:https:&#x2F;&#x2F;blueskyweb.xyz&#x2F;blog&#x2F;5-5-2023-federation-architectureHere’s a snippet from the protocol roadmap they published 3-4 weeks ago [1]:Multiple PDS instancesThe Bluesky PDS (bsky.social) is currently a monolithic PostgreSQL database with over a million hosted repositories. We will be splitting accounts across multiple instances, using the protocol itself to help with scaling.[1] https:&#x2F;&#x2F;atproto.com&#x2F;blog&#x2F;2023-protocol-roadmap reply wmf 8 hours agoparentprevAFAIK there&#x27;s only one version of the software so \"the service\" runs the same thing that you self-host. SQLite seems like it will simplify the single-user case though. reply jakebsky 8 hours agorootparentThat&#x27;s right. This is the same code Bluesky is running on our new PDS hosts. It&#x27;s all open source.The main motivation in moving from a big central Postgres cluster to single tenant SQLite databases is to make hosting users much more efficient, inexpensive, and operationally simpler.But it&#x27;s also part of the plan to run regional PDS hosts near users, increasing performance by decreasing end-to-end latency.The most experimental part of this setup is using Litestream to replicate these many SQLite databases (there are almost 2 million user repositories) to cloud storage. But we&#x27;re not relying on this alone, we&#x27;re also going to maintain standard SQLite \".backup\" snapshots as well. reply whyrusleeping 8 hours agoparentprevno this actually is moving every single user currently on the service into this setup. Everyone gets their own sqlite under the hood. reply PrivateButts 5 hours agoprev [–] I&#x27;ve got a bunch of invites if folks want them:bsky-social-etdu7-njigubsky-social-2ktcs-uwoxgbsky-social-6f5nh-36gnqbsky-social-ciwro-3gzk5bsky-social-y4h57-dxh3g reply okanesen 3 hours agoparentThe codes are all gone. That was fast.E: Happy to take one, if somebody happens to have a spare one left. Email is in my bio. reply thegeekpirate 5 hours agoparentprevGrabbed bsky-social-6f5nh-36gnq, thanks! reply pests 5 hours agorootparentDamn, they all gone. reply ochronus 4 hours agorootparentThere you go folks:bsky-social-h3d4w-u6yn4bsky-social-74bqi-vkmcqbsky-social-n3fdq-46nxzbsky-social-yippe-32vdrbsky-social-l2fbt-xnscx reply chocolatkey 3 hours agorootparentEither people were really prepared for these codes to appear, or they are being scraped. Regardless, they&#x27;re all gone reply numpad0 2 hours agorootparentIt seems a temporary, anonymous, private, receive-only dropbox(not the USB drive replacement kind) on the Internet is an unsolved problem. It doesn&#x27;t have to be completely out-of-band like email, could be just an encrypted public reply by `catbase64 -dopenssl rsautl -decrypt -inkey temp.key`, so long up to few bits(70 in this instance) of encrypted content would be allowed on a platform. reply ulucs 2 hours agorootparentpiracy websites were using base64 encoding for this purpose a while ago, but now it seems they moved on to a proprietary algorithm reply arthurcolle 4 hours agorootparentprevAny more? reply CtrlAlt 3 hours agorootparentSome more for y&#x27;all!bsky-social-ge2mz-mfmpibsky-social-hykwa-x3ox4bsky-social-gh4mt-2od6pbsky-social-dejzy-mmcxfedit: all gone :( reply pixelat3d 3 hours agorootparentSnagged bsky-social-hykwa-x3ox4 reply shmde 4 hours agorootparentprevGone in 60 seconds. reply arthurcolle 4 hours agorootparentAny more? reply tymofiy 3 hours agorootparentbsky-social-lbjkg-gcxs4bsky-social-zigwm-f3qpqbsky-social-2jlu7-apy5absky-social-6ct52-4egmzbsky-social-cy64m-53sqn reply da25 3 hours agorootparentYa&#x27;ll got anymore of that. reply hoseja 2 hours agorootparentprevMaybe if you stop posting them with the easily-greppable first part they won&#x27;t be so easy to scrape. reply numpad0 1 hour agorootparentNo one seems to be taking two codes I&#x27;m putting up without the prefix for ~hour, this is likely the casee: second one now used, first still upe: both used reply greyface- 42 minutes agorootparentI wonder what they&#x27;re being used for. The UI doesn&#x27;t expose it, but the Bluesky API will tell you who redeemed your invites. Open the site, watch for a \"com.atproto.server.getAccountInviteCodes\" request in your browser&#x27;s network inspector, look in the \"usedBy\" field in the response JSON, and append the DID value there onto \"https:&#x2F;&#x2F;bsky.app&#x2F;profile&#x2F;\". Any commenters in the parent chain who got scraped want to take a look? reply numpad0 29 minutes agorootparentI get \"$username joined using your invite code!\" in notification tab that leads to the user profile. So far the user hasn&#x27;t done anything. reply pests 1 hour agorootparentprevgrep \"bsky-\" internet.txt replynumpad0 4 hours agorootparentpreve: burner email didn&#x27;t work, sorrye4: check out dns on [my username].com reply WinstonSmith84 20 minutes agorootparentcan&#x27;t believe it&#x27;s gone, too many smart people on this website lol reply ldeso 14 minutes agorootparentprevThanks a lot! reply skybrian 5 hours agoparentprevThese seem to be gone. reply da25 3 hours agoparentprev [–] they&#x27;re all exhausted now :( replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Product Data Store (PDS) has been revamped to use a single-tenant SQLite database, where each user will have their respective SQLite file to keep repo and private account status.",
      "The ActorStore is introduced to interact with user data, replacing earlier services, and includes specific classes for reading and writing. Any change in the store necessitates a distinct 'transaction'.",
      "To manage service state, three separate SQLite databases have been added. PDS is planned to be packaged with Litestream to facilitate concurrent reads and replication in real-time. This upgrade involved several commits and fixes, which have passed all tests."
    ],
    "commentSummary": [
      "The PDS has undergone refactoring to implement a single-tenant SQLite datastore, where each user possesses a unique SQLite file for storing repository and private account data.",
      "The ActorStore has been incorporated for user data interaction, replacing previous services, and utilizes various classes to perform read and write operations.",
      "The architecture now includes three separate SQLite databases for managing service state, with mandatory 'transactions' for store changes. The PDS will be distributed with Litestream enabling concurrent reads and streaming replication."
    ],
    "points": 251,
    "commentCount": 144,
    "retryCount": 0,
    "time": 1699316559
  },
  {
    "id": 38161369,
    "title": "Celebrating Diversity of Interests in the Hacker News Community: A Reflective Post",
    "originLink": "https://news.ycombinator.com/item?id=38161369",
    "originBody": "I thought this post[1] from exactly a year ago was a nice type of post and so to celebrate it I thought I would start another to show the diverse interest of the HN community. Cheers![1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33488891 (thanks mckirk!)",
    "commentLink": "https://news.ycombinator.com/item?id=38161369",
    "commentBody": "What are you passionate about at the moment?Hacker NewspastloginWhat are you passionate about at the moment? 252 points by kurtdev 22 hours ago| hidepastfavorite558 comments I thought this post[1] from exactly a year ago was a nice type of post and so to celebrate it I thought I would start another to show the diverse interest of the HN community. Cheers![1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33488891 (thanks mckirk!) vivekd 8 hours agoThis post caught my attention because recently I met a woman I really admire. She dropped out of her computer engineering class, took a vow of celibacy and poverty and became a missionary with a Catholic sect.The reason I admire her isn&#x27;t that. What caught my attention about her is that she has a sense of purpose. She has a purpose in life and has devoted her life to it.I used to have one. My goal was to make the legal system cheaper and more accessible. And I devoted the last decade of my life to that. Now that&#x27;s fallen apart and I&#x27;m a little lost and it hurts me alot.I hope I&#x27;ll find a purpose soon. That&#x27;s what I&#x27;m passionate about right now, finding a new purpose or a new way to accomplish my past purpose. reply jll29 17 minutes agoparent> She dropped out of her computer engineering class, took a vow of celibacy and poverty and became a missionary with a Catholic sect.It seems the only way to make HN even better is to bring in people that HNers admirer and to interview them on here.Speaking of nuns, I know two cases of admirable women (one English and one French), who left their studies (and one also left her then-fiancee) to live and serve in a religious order. At least in one case no-one could have predicted that, least the (MIT-educated) brain researcher fiancee. reply lewisjoe 7 hours agoparentprevI worked with a founder who had the same vision. Democratising legal industry using tech.It was my first job after college and was fun building a contract negotiation platform, document assembling API, etc.Learnt a lot but had to leave because something was missing & it wasn&#x27;t working out.The person who I worked with has the same name signature as yours: vivek durai reply serialNumber 6 hours agorootparentWould be incredible if it was the same person reply the_common_man 11 minutes agorootparentIt obviously is reply oulu2006 29 minutes agoparentprevI hope you find it as well, thank you for sharing...it can be quite soul crushing when you loose something that gives you a strong sense of identity. reply m463 5 hours agoparentprevIt would be nice to have a legal wiki.Maybe not to represent yourself (\"anyone who represents himself has a fool for a client\") but maybe useful stuff.For example, in california, you have certain online privacy rights you can exercise, and it would be nice to have a form letter to enumerate and exercise those rights.or how to fight a parking ticket, etc. reply dendrite9 2 hours agoparentprevNot that I find this person&#x27;s path admirable, although I don&#x27;t know enough about his personal life to feel comfortable saying much more than that. But you might be interested in reading about this one time wold class climber who left his life to join a monastery and become a priest. And then left that and returned to climbing recently. His story doesn&#x27;t resonate with me, but there might be something to it that grasps you. https:&#x2F;&#x2F;www.cbc.ca&#x2F;news&#x2F;canada&#x2F;british-columbia&#x2F;crack-of-des... reply shyn3 2 hours agoparentprevDon&#x27;t be too dismayed. They get a credit card with an unlimited spending account.Also, see my post history about acts. It may help you see the trees instead of the forest. reply chadk 7 hours agoparentprevThanks for your honest answer. Finding purpose is a pretty good passion reply Obscurity4340 4 hours agoparentprevYou should talk with AI Judge guy, wehad an interesting chat about similar things (sort of). Check my history reply t0bia_s 2 hours agoparentprevWhy is it fallen apart? reply kylebenzle 7 hours agoparentprevIf you are into the legal system issues right now I am passionate about fathers rights and its been a huge outlet for me. SO many kids and fathers are desperate for help and 50&#x2F;50 shared parenting is a admirable and achievable goal.[1] https:&#x2F;&#x2F;www.sharedparenting.org&#x2F; reply nwiswell 9 hours agoprevPowerlifting.Solitary and meditative if you want it to be; social and uplifting if you don&#x27;t.It&#x27;s healthy in a variety of ways (including bone density; physical activity; higher BMR and glucose metabolism; improved cardiovascular function). Also being strong is useful surprisingly often.Unlike many things in life, your progress is almost entirely dependent on your consistency and the effort invested, with the exception of (hopefully) temporary setbacks like injury. Hitting personal records and milestones feels particularly good because you know you&#x27;ve earned it. It&#x27;s hard! But it&#x27;s also not so hard that I&#x27;m liable to get discouraged.Lots of people prefer bodybuilding style training, but there&#x27;s something magic about the barbell for me. Olympic lifts are also a lot of fun, but they&#x27;re more technical and you need more gear and space.Also it&#x27;s much easier to quantify your strength progress (I have a literal spreadsheet), and it feels less vain than focusing on looks (not that it isn&#x27;t a significant bonus).Dunno. Feels good. I&#x27;m gonna keep at it. Two thumbs up. reply kfoley 7 hours agoparentFor anyone looking to get started I highly recommend the StrongLifts program and the associated app, both free [1].I neglected strength training for a long time because every time I tried to get started I would feel overwhelmed. Then I tried StrongLifts and loved the fact that it&#x27;s just 5 exercises, but provides pretty much a full body strength workout.The guy who developed the program has put a ton of effort into making sure it&#x27;s detailed and accessible so that pretty much any question you could have is answered.[1]: https:&#x2F;&#x2F;stronglifts.com&#x2F;5x5&#x2F; reply kccqzy 3 hours agorootparentThe StrongLifts program, while still pretty good, is not for absolute beginners. The app and the website has videos and detailed instructions, but that doesn&#x27;t replace a human coach being able to point out in real time mistakes in your form that you do not even realize. And if you are a beginner, any attempt to do these exercises will invariably contain so many mistakes that they are not worth doing any more.Just find a human coach. An app is not a substitute. reply probablynish 8 hours agoparentprev> Unlike many things in life, your progress is almost entirely dependent on your consistency and the effort investedThis really resonates with me. Not powerlifting myself, but I have a strength training routine at the gym with the goal of improving my right knee pain. I have to take things very slow (increase by 1 rep each session, up the weight every ~2 weeks by the smallest possible increment) but looking at my graph this year is very satisfying. This little corner of my life feels a lot more under my control than anything else right now reply nwiswell 8 hours agorootparent> This little corner of my life feels a lot more under my control than anything else right nowYes that&#x27;s exactly it! It&#x27;s something of a refuge from whatever other chaos I&#x27;m dealing with. I get to feel good about trying and even better about succeeding.The body is incredibly resilient, and given enough stimulus and patient consistency, it&#x27;s pretty amazing how strong average people are capable of becoming.I&#x27;m reminded of this quote from Socrates:\"No man has the right to be an amateur in the matter of physical training. It is a shame for a man to grow old without seeking the beauty and strength of which his body is capable.\"I am not young anymore, but I&#x27;m not old, either, and I&#x27;ve come to the conclusion it&#x27;s certainly better to start late than to never discover my limits at all. reply candiddevmike 8 hours agoparentprevPowerlifting can be very dangerous. You can really, permanently, mess up your body, specifically your back and neck. What steps do you take to mitigate that risk? reply nwiswell 8 hours agorootparentThis is a complicated question.First of all, your statement is absolutely accurate. The weights involved can cause injury and can possibly be dangerous or even fatal when not handled correctly. On the other hand, exactly what is meant by \"injury\" can vary widely, so we have to be precise in our language.Acute, serious injuries typically come about due to poor form, carelessness, failure to use safety mechanisms, or irresponsible selection of load. They are the most easily preventable type of injury, and unsurprisingly they are most common among novice lifters.I address these risks by being deliberate about my form, consistent about my use of safety devices, and reasonable about loads and progression in my training. I also use a belt with proper abdominal bracing technique.Chronic injuries, on the other hand, are much less preventable and much more common among experienced lifters. I&#x27;ve worked through several, including a soft tissue injury in my hips and a nagging tendonitis in my left elbow. Managing these types of injuries comes down almost entirely to sensible training.Powerlifting in general is associated with a lower injury rate than other sports[1]. That&#x27;s not to imply that injuries aren&#x27;t a reality of powerlifting -- they are -- but more that I do not believe they constitute a good reason to forego the benefits of lifting.[1] https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S07651... reply reaperman 4 hours agorootparentprevPersonally, good coaching. I joined the Starting Strength gym near me and enjoyed having an incredibly high quality coach monitor nearly every lift. Rippetoe has some obnoxious views but the local gyms usually have coaches which are a great fit for the local culture. Our gym was very inclusive. reply bodhi_mind 8 hours agoparentprevI just began the 5&#x2F;3&#x2F;1 strength training method. I’d like to use gymnastic rings for most of my accessory work to try and get a nice balance of strength and flexibility. reply itake 8 hours agorootparentI tried 5&#x2F;3&#x2F;1 in college and I after week 4, I felt like you need to be juicing to keep up with it. reply brosco 7 hours agorootparentWhat? If you start light like Wendler recommends, the program is completely manageable. In fact, most people I know think there is too little training volume at first. I used it for several months in a row a few years ago and it led to great strength gains in every lift. reply kinghtown 7 hours agorootparentprevNo. reply kbf 19 hours agoprevMental health. I recently passed out in public and had to finally admit to myself that I’m severely burnt out. I had ignored the signs and mentally written off the concept of burnout as a bit of a wishy-washy mostly “emotional” concept and not as the very real psychological AND physiological condition that doesn’t just go away if you get a good nights sleep and a day off. It was emotional to actually read up on it and realize my disillusionment with work, deteriorating relationships and steady decline in my ability to carry out daily tasks weren’t signs of a personal lack of interest or effort, but an inevitability resulting from the position I had put myself in. reply justin_oaks 18 hours agoparentThe world is slowly realizing that the advice to \"suck it up\", \"tough it out\", \"walk it off\", \"man up\", etc. aren&#x27;t helpful. Real problems need real solutions, not platitudes.I&#x27;m sorry you&#x27;ve gone through this. I quit a previous job with no other job lined up because of burnout. Fortunately, I could afford the 4 month gap in employment. My heart aches for those who can&#x27;t. reply aramndrt 40 minutes agoparentprevCould you please elaborate on why you think you passed out or what led up to that incident? Whenever I&#x27;m particularly stressed, I tend to become very light-headed or dizzy to the point where it feels like I&#x27;m about to pass out, but I never do. I&#x27;ve had this on and off for years. I&#x27;m only 26 years old. reply manuelisimo 12 hours agoparentprevYou are not alone! I have been dealing with burnout since the pandemic began. For me it was the inability to focus on the simplest tasks that made me realize I could no longer ignore the problem. Therapy has helped a lot. reply Minor49er 11 hours agoparentprevLonghorn PHP in Austin, Texas wrapped up just a couple of days ago. One of the keynote talks was titled \"Stronger Than Fear: Mental Health in the Developer Community\" by Ed Finkler. It outlined why these things not only need to be talked about, but also taken seriously:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QYPAGuh6Le8 reply rustyboy 9 hours agoparentprev> my disillusionment with work, deteriorating relationships and steady decline in my ability to carry out daily tasks weren’t signs of a personal lack of interest or effort, but an inevitability resulting from the position I had put myself in.Wow this is exactly what i&#x27;m suffering from right now... I recently thought about applying to a more serious tech team at my company given i&#x27;ve slowly slipped into a basic DBA&#x2F;PM roll.But i&#x27;m worried it&#x27;s only going to make it worse. What does the science or your peers say is the key to getting help? reply brainless 6 hours agoparentprevSending a big hug over the protocol stack of the Internet. We neglect the signs because our society tells us so, but now that you have started noticing them, I am sure you will find the balance you need for a happy life. reply RationalDino 8 hours agoparentprevMental health for me as well.That&#x27;s the point of https:&#x2F;&#x2F;rationaldino.substack.com&#x2F; and this pseudonymous account. Separately I have other children who were impacted by COVID and lockdowns and are part of https:&#x2F;&#x2F;jonathanhaidt.substack.com&#x2F;p&#x2F;mental-health-liberal-g.... That&#x27;s also been hard.All of this is made more complicated by the fact that I regard psychology as having a lot of pseudoscience in it. Feynman was right to call them that in https:&#x2F;&#x2F;calteches.library.caltech.edu&#x2F;51&#x2F;2&#x2F;CargoCult.htm. It took them 40 years to realize that he was right about the importance of replication. But they still haven&#x27;t accepted the rest of what he said. reply fillskills 3 hours agoparentprevCreated HealingGardens.co after such an episode myself. If you want to chat just DM me reply amelius 10 hours agoparentprev> that doesn’t just go away if you get a good nights sleep and a day offTry to sleep (or at least stay in bed) 10+ hours a day, consistently for a few weeks. reply all2 10 hours agorootparentIf I don&#x27;t get enough sleep (10ish hours in bed) I crash rather quickly. Long covid sucks. reply solardev 20 hours agoprevThere is a new show on HBO called Scavengers Reign, and it&#x27;s become my favorite scifi story ever. https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt21056886&#x2F;It&#x27;s kinda like watching Planet Earth about another ecosystem, with a strong focus on judgment-free ecology (ie there isn&#x27;t good and evil, just different flora and fauna and otherwise interacting both with their normal food webs and with human outsiders).It really tickles the environmental science geek in me. There&#x27;s such a wonderful assortment of predators, prey, symbiotes, diseases, treatments, and thoughtful little touches everywhere. Beautiful art too.------That aside, I can&#x27;t stop thinking about how much fun it is to throw people off cliffs in Baldur&#x27;s Gate 3. https:&#x2F;&#x2F;steamcommunity.com&#x2F;profiles&#x2F;76561199138390397&#x2F;recomm...Simple pleasures, man. reply O1111OOO 11 hours agoparent> Beautiful art too.I just looked over a couple of trailers. I think this makes my watchlist.About the artwork... reminded me of Another World [0, 1] but with the better rez of today.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Another_World_(video_game)?use...[1] Walkthrough: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=wjMf_bEfqIc reply izzydata 19 hours agoparentprevI&#x27;m really enjoying Scavengers Reign, but I can&#x27;t help but feel like the local biology is a bit too absurdist. Or the amount of consistently lucky moves some of the characters would had to make to discover some weird quirk of that alien life. They had been there for maybe weeks or months, but somehow discovered that if you climb inside of the belly of this particular creature and twist some of his internal organs that it will release a light producing egg when you hit them against a hard surface? Or that if you attach this tentacle sac to your face that you won&#x27;t inhale some fungus gas.Very creative, but a bit hard to believe. Perhaps the timescale of how long they were stranded there could have been increased a little bit. reply ChristopherDrum 1 hour agorootparentThis is precisely what I was saying to my friend the other day. Their knowledge of the planet is hyper-specific in some ways, but they also seem to acknowledge that they don&#x27;t understand the planet. One person says, \"This place is like a puzzle.\" but it seems like a puzzle they&#x27;ve solved, if they know that taking the tiny skull from a worm and rolling it in a leaf will make a whistle that summons a bird that won&#x27;t hurt them.Yet, on the other hand, they seem mystified by things like which plants are edible, or the yellow fungus on the computer circuits, or the intelligent mind-power beasts. Stuff that should be top-level knowledge, I&#x27;d think? So do they understand the planet or not? Clearly a lot is known about the esoteric workings of the planet, but weirdly the BASICS of the place are a total mystery to them.And yeah, the biology of the planet is both supremely alien yet also amazingly perfectly suited to incredibly specific use cases: this creature doubles as a gas mask, this other one is a personal flying machine with handlebars, reach inside this one and it&#x27;ll inflate into a personal balloon guaranteed to hold your weight.If I turn off that part of my brain, I enjoy letting myself get caught up in the \"LOST\"-ish mysterybox-ness of it (LOST, in space!). reply Caligatio 5 hours agorootparentprevI&#x27;ve been wondering the same thing! My explanation is that the planet was already known&#x2F;cataloged and that the crew has access to that knowledge somehow. reply leoqa 10 hours agorootparentprevThat particular character is a biologist before the crash, perhaps prior knowledge since the planet has been inhabited before. reply mettamage 7 hours agoparentprevHaha, yea I binged that one too. I&#x27;d give it a similar description, I loved it. I also watched Pantheon. There are a few questions in Pantheon that are interesting to imagine. Also, the computer science fan service (like the philosopher&#x27;s problem in concurrency&#x2F;multithreading) feels fun - irrelevant to be honest, but fun. reply cdchn 7 hours agoparentprev>I can&#x27;t stop thinking about how much fun it is to throw people off cliffs in Baldur&#x27;s Gate 3.I was too worried about wasting loot then I finished the game with 20k gold and nothing to spend it on and thought \"And nothing of value would have been lost if I had yeeted more people off cliffs.\" reply windowshopping 9 hours agoparentprevThis sounded really cool so I went and checked it out, turned it off within 10 min. Was just kind of silly, felt more like someone having fun with drawing than someone really trying to do any sort of serious ecological design. reply tommychillfiger 7 hours agoparentprevJust dropping back in to say I started this show after reading your comment and am enjoying it quite a bit. Very imaginative and beautiful animation. Thanks! reply FFP999 19 hours agoparentprevOne of my favorite things about Deathloop was kicking unsuspecting NPCs off of cliffs and roofs. I&#x27;ve played a lot of that game and that hasn&#x27;t gotten old yet. reply crabmusket 9 hours agorootparentFlashbacks to Dark Messiah Of Might And Magic reply dilawar 20 hours agoparentprevIt is surprising very creative. Somehow it reminds me of manga mushishi through there is hardly any obvious overlap reply miki_tyler 15 hours agoparentprevIs this based on any of Jean Giraud&#x2F;Moebius&#x27;s work? reply solardev 13 hours agorootparentI don&#x27;t know who that is (had to look him up), but some critics think the series was inspired by his look & feel:https:&#x2F;&#x2F;www.theverge.com&#x2F;23912922&#x2F;scavengers-reign-review-ma...https:&#x2F;&#x2F;www.polygon.com&#x2F;23798021&#x2F;scavengers-reigns-announcem... reply dopidopHN 7 hours agorootparentprevI had the same thought. He’s influential, that would make sense reply hahahacorn 8 hours agoprevRunning.I got the bug about 12 weeks ago. I had never ran longer than 1.5 miles continuously 12 weeks ago. Now I’m planning my next Sundays long run. 12.5ish miles.Im a heavy guy. My heaviest was 270, but I’m down to 220 now. I hated running my entire life. But I finally figured out how to run pain free and now it’s the thing I look forward to everyday.It’s also tremendously rewarding to see such rapid improvement. When I first started I ran a 38 minute 5k, and 10 weeks later I ran a 25:44. I’ve almost improved my 1mi time from 8:30 to 6:30. Signed up for a marathon a couple weeks ago and gave myself 6 months to train.Sub 4:30, here we come reply swah 1 hour agoparentDid you do anything to improve on 5ks other than running 5ks? My 5k time is around 35min forever, I run every now and then and recently signed for a public 5k run but I don&#x27;t see myself improving my pace. (It took years until I could run 30 minutes non-stop and at least I didn&#x27;t lose this over the years, but I&#x27;m slow...) reply hahahacorn 39 minutes agorootparentYes. I didn’t run 5ks and I didn’t try to run fast. I heard the buzz phrasey “to go faster you have to go slower” for like 2 years and totally disregarded it.I used to run before this latest bug, like a year ago, but I hated it. My only goal was to do a sub 30 minute 5k and I trained for like 8 months with 0 progress and was so dejected I quit. I would show up, run at a 10 min pace for as long as I can, repeat. It felt like shit and I didn’t get better. My growth happened when I just ran 4.5 mph until it was the _bad kind_ of uncomfortable (this is too much stress on my shins vs my legs muscles are building lactic acid). After two weeks, I upped the pace a tiny tiny bit. I spend most of my runs fighting with myself to slow down.The science behind it is something something aerobic base and running economy. I’m absolutely not an expert, but anecdotally I’ve never had my life improve so much in such a short amount of time by just _running slower_ reply theryan 8 hours agoparentprevAny tips for running pain free? I used to run pretty often (3-5 times per week) for 5+ miles. Longest was 11. However, my feet started hurting while running -- never while walking even long distances. reply enjeyw 2 hours agorootparentPersonally, I intersperse my running with Interval Training on an exercise bike: 2 minutes out of the saddle with the resistance turned up a bit, pushing as high as you can go, 1 minute of easy riding in the saddle with low resistance. Repeat 7 times.I think it provides a few advantages to your running:- it’s excellent cardio, so it’ll improve your VO2 max, which is hugely helpful for both running and living longer- it’s low impact, so it gives your joints time to recover. Recovery is critical for an impact sport like running!- the resistance builds leg strength, so it makes you more resilient on longer runsOh, and get a decent pair of running shoes, ideally from a store that knows what they’re talking about. Note that the best shoe for you won’t necessarily be the most expensive one - things like whether your over pronate or not are critical, so don’t go out and buy the latest carbon sole foodad just because that’s what the pro’s have. reply hahahacorn 6 hours agorootparentprevFor me, slowing down, proper foot striking (go for a couple barefoot runs at the park until you know what feels right and then translate that to form with running shoe on), and I did feet and ankle strength training for the first month.Oh and #1 is listen to your body. I’m running for the joy of running. If something hurts I’m not gonna run, lol. Important to rest. reply tommychillfiger 7 hours agorootparentprevI can&#x27;t give specific advice as there are just so many factors at play, but I&#x27;ll offer that it has to do with what you&#x27;re used to and what you&#x27;re conditioned to. I started running after a long history of skateboarding in very thin shoes. I end up with pain in my knees and even my arches with thicker shoes - I say this as it is somewhat counterintuitive to make the point that it&#x27;s not always true that more cushion = less pain.Aside from that, be very gradual in ramping up mileage and try to feel for wasted motion&#x2F;effort&#x2F;impact. There are tons of \"form cues\" people get caught up in, but just trying to be efficient in your use of energy to travel forward is what has been generally helpful for me in developing a stride that doesn&#x27;t get me hurt. Your mileage may vary, of course. reply guybedo 7 hours agorootparentprevstrength training helps a lot in preventing injuries. I&#x27;ve been through years of achilles tendonitis and finally got rid of it mostly by doing leg&#x2F;calf strength exercises.Fwiw, here&#x27;s some example exercises for Achilles: https:&#x2F;&#x2F;routineshub.com&#x2F;public&#x2F;1faae50e-8d36-4eec-84ee-ae3ef... reply hn_user82179 3 hours agorootparentthank you for this. I&#x27;ve been dealing with plantar fasciitis for a year+, finally got it to go away ~5 months ago and got back into running, and just in the last 2 weeks have noticed gradually increasing pain in my achilles and I&#x27;m absolutely devastated at the thought of battling another chronic injury. reply mns 1 hour agorootparentI&#x27;ve been running for 12+ years and after going through plantar fasciitis and some other issues (went to a very good orthopedic sports clinic that helped me a lot), getting a Theragun and using it every day was amazing afterwards, especially for calves (that can help with achilles tendonitis) and for your foot for plantar fasciitis). reply mindwok 6 hours agorootparentprevI had this happen to me when I got into running. I went on a trip for 2 months and let my injuries recover. When I got back into running, I had no pain and haven’t had for more than a year. I’d say give it another try, I think a lot of the injuries are overuse and your body will adapt over time. reply beingfit 5 hours agorootparentprevFor me, my hips start hurting after a run. My shoes are relatively new and I’m running on paved pathways or surfaces that are not too uneven. After learning just a little bit, I don’t try to take long strides, I don’t try the “heel strike” approach, etc. But running is quite difficult for me, both during and after. I’d really appreciate tips on avoiding the hip pain (I do some yoga too for the hips, hip flexors, quads, etc.). reply csharpminor 7 hours agorootparentprevWorn out shoes can cause pain after a few months. Any further diagnosis should probably be delivered by a PT. reply liampulles 1 hour agoparentprevI wouldn&#x27;t consider myself an avid runner, but I run semi-regularly. It does wonders for clearing my mind.(To the curious) Don&#x27;t get bogged down trying to train for a marathon. Just run what you can, then walk, then run again. reply jslakro 1 hour agoparentprevA resource recently shared in HN for running tech lovers https:&#x2F;&#x2F;github.com&#x2F;yihong0618&#x2F;running_page reply arach 4 hours agoparentprevI think it&#x27;s worth consulting with PT&#x2F;pros who could give feedback on the ramp up period and the load you&#x27;re putting yourself under. What I&#x27;ve read in my research is that you can feel fine for a while but if you&#x27;re not building up methodically you can cause yourself some long term issues but with a delay.Congrats on the weight loss and figuring out that you can be a runner. It&#x27;s quite liberating. I was never a good runner even in peak competitive basketball conditioning in my late teens and thought I would never be a runner until a few years ago. Keep pushing! reply coffeesque 5 hours agoparentprevI also started running again relatively recently - about one month ago - definitely not to the same extent parent is doing it, but it managed to reduce the upper back pain I was getting from dev workAlso the mental health benefits cannot be understated, very glad I built it into my routine reply beingfit 5 hours agoparentprevI’m feeling some envy right now. I’ve tried running…or rather, walks and runs interspersed, at least once a week or more often. But my heart rate shoots up even at a moderate pace of 5mi&#x2F;h (7.5km&#x2F;h) to around 140bpm or higher (I’m 50 years old). I just can’t run (actually jog) continuously for more than one minute before breaking into a walk. Otherwise, I workout everyday, mostly cardio and some strength for at least 45 minutes a day, every single day. My weight is in the normal BMI range. I don’t know if this is how it is to be. reply alexfoo 1 hour agorootparentHave you tried Couch to 5k? (C25K)FWIW I did it a few years back after coming back from an ankle fracture that had stopped me doing any exercise for 6 months. Looking at my Week 1 Day 1 activity on Strava my HR was ~105bpm when initially walking, but then up to 160bpm when running for a minute, and only dropped to ~130bpm when walking for the next minute. This see-saw repeated; up to ~160bpm when running, down to ~130bpm when walking. At the end it dropped from 160bpm to 115bpm over the course of the final 5 minute \"cool down\" walk. There&#x27;s quite a \"lag\" between exercise intensity and HR.My point is that if you&#x27;re not regularly running then your CV system is inefficient, and so even walking may put your HR up quite high. If you stick at it it should improve.I&#x27;m 47 and my HR when I go for a run is ~170bpm. If I go for a gentle run I try and keep it below 150bpm, but when I was just getting back into it I&#x27;d struggle to keep my HR under 165bpm even on a \"gentle\" run. As you get fitter (or lighter) you can do the same kind of runs with a lower HR than before. When I really want to push it I can average 180bpm-190bpm for a 5k (although it feels grim at the time).If you get through a few weeks of C25K and you&#x27;re still having trouble running for more than a minute or so at a time then I&#x27;d go see a doctor. If you&#x27;re concerned&#x2F;nervous about the idea of trying C25K with a possibility that there may be something amiss then go and see a doctor about it now.(This is not medical advice. I am not a doctor.) reply swah 59 minutes agorootparentprevI&#x27;m 39 and my HRV will shoot to 160-170 at those speeds... I think its ok? reply tommychillfiger 7 hours agoparentprevRunning has pretty much changed my life completely - very good stuff. For all of the snake oil out there promising ridiculous benefits, consistently running anything more than about an hour a week is as close to the real McCoy as I have ever experienced. reply hahahacorn 6 hours agorootparentThe changes I’ve seen in 3 months are frankly staggering. I quit smoking, cut like 95% of my alcohol consumptions, started eating healthier, etc. and it was really easy for me to do all at the same time.I had tried tackling each of these problems independently before and typically failed. Withdrawals, cravings, etc. are too much. But man I really fucking love running now and smoking, binge drinking and junk food just look so gross to me now that I know they’re gonna ruin my runs.The brain is weird. I’m insanely grateful. reply Gabriel_Martin 5 hours agoparentprevDo please divulge, I&#x27;m at my heaviest, but have started jogging backwards to try to limit the damage as I get used to it again. What&#x27;re your tips? reply pharmakom 2 hours agorootparentBarefoot running (or extremely minimalist shoes) helped me. Build it up slowly! Your muscles will likely be degraded from a lifetime of low mobility footwear.Consider: how could humanity have survived millennia walking and running around before Nike and orthotics? reply arach 4 hours agorootparentprevIt might be better for your joints to walk on the incline (treadmill) until you are lighter. Might be worth building a program with a personal trainer and mixing in some weight training. Apologies in advance if you&#x27;ve already done all of the above reply tyler_sf 7 hours agoparentprevI&#x27;m in the same boat as you. I have been running since the summer and I recently finished a couch to 5km plan. My pace is similar at 35-38 minutes. Do you have any tips on going from 38 to 25 minutes? Thanks reply hahahacorn 6 hours agorootparentMy biggest thing was slowing the fuck down and consistency. I was an athlete in HS and had a huge ego around how fast I should be going. For reference I didn’t run faster than 12min&#x2F;mi for the first 2 weeks. Also I run 5-6x a week now (1 long run the rest super easy recovery runsItalians weren&#x27;t \"white\" when they first came to the US, either.Italians have been \"white\" since the dawn of the construction of \"race\" in which \"white\" was even one of the options.(They&#x27;ve been othered as \"ethnic\" in a way which closely mirrors othering on race-qua-race, and we use \"racism\" to describe both racial and ethnic bigotry, but while similar and sharing some features, the two things were not the same in the US.) reply acover 6 hours agorootparent> Teddy Roosevelt, not yet president, famously said the lynching was indeed \"a rather good thing\".[15] John M. Parker helped organize the lynch mob, and in 1911 was elected as governor of Louisiana. He described Italians as \"just a little worse than the Negro, being if anything filthier in their habits, lawless, and treacherous\".https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Anti-ItalianismHaiti codified \"whiteness\" into law way before then.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Slavery_in_Haiti reply hsuduebc2 9 hours agorootparentprevIt&#x27;s same everywhere. Fear of the unknown, coping with own failure via blaming someone weaker, different language and of course at least in EU a lot of hatred due to desinformations.Of course more descriptive word would be just xenophobia combined with copium addiction. reply mikrl 7 hours agorootparentprevSlavs have been discriminated against quite terribly over history.‘White’ is just a category invented by Anglos and other Germanics (including Scandis, Franks, excluding Slavs and Jews) to lump together the ‘good sort’ of light skinned people away from dark skinned people and the ‘bad sort’ of light skinned people.If you want to get deeper into ethnic speculation, I suspect it’s because Slavs have historically absorbed non-negligible amounts of ‘other’ groups such as Turkic, Iranic and Mongol; and this sets them apart from the more insulated Germanic groups who perhaps feel it makes them superior. Look at Nazi views on Aryanism and how Slavs were considered impure (funnily enough, except Bohemians, Croats and Ukrainians for political reasons… there was a war on) reply Obergruppen 9 hours agoparentprevIs there a country closer to their homeland where they can be housed? It is unfortunate they have to be so far from home. Or better yet, can we just push for peace talks so they can stay home? reply capableweb 9 hours agorootparent> Or better yet, can we just push for peace talks so they can stay home?Some of these people don&#x27;t have any homes anymore, literally. Some places have been pretty so extensively damaged that you&#x27;re not able to live there anymore.We need both, help for refugees that have nowhere else to go, and more dialog to end the conflict.This applies to all conflicts. reply realusername 3 hours agorootparentprevPeace talks with Putin lol, like any of that would work reply thatwasunusual 9 hours agorootparentprev> Or better yet, can we just push for peace talks so they can stay home?\"Stay home\" as in \"stay in a country occupied by an enemy country who wants to get rid of you\"?Sure. That sounds like a brilliant solution. reply meiraleal 8 hours agorootparentIf we are talking about Palestine and Gaza yes, stay home would be much better than be killed and repelled. reply philip1209 9 hours agoprevLow-plastic living.I read a book by the world&#x27;s first zero-waste restaurant, Silo. The book got me thinking about how different our lives would be if oil were expensive. \"Pretend oil is really expensive\" is a good proxy for doing things that are \"eco-friendly.\" Since reading the book, I&#x27;ve tried to eliminate single-use plastic from most of my life. It&#x27;s incredibly hard. Everything from my socks to my vegetable packaging to my dog&#x27;s toys to my floor is made of plastic.I&#x27;ve slowly been adjusting my habits - such as checking whether clothing is natural or oil-based, buying food mostly at the farmers market, and eating in instead of takeaway. These little change have decreased my carbon footprint immensely.I enjoy playing this game of \"pretend oil is expensive\" even though it&#x27;s not because it&#x27;s revealing problems we will need to tackle as a society. At some point we will run out of oil. When that happens, everything will be impacted. Travel, food delivery, and most of all healthcare. reply 4death4 7 hours agoparentIf oil were expensive, plastic would probably still be used in all the places it is used today because it is lighter and a large percentage of oil goes into the transportation of goods.I actually don&#x27;t have a problem with plastic, at least in theory. Oil exists in the ground. I don&#x27;t see an issue with pumping it up, using it for a bit, and putting it back into the ground via a landfill. I take more of an issue with recycling where plastic is shipped overseas and makes its way into the waterways. reply jll29 52 minutes agorootparent> Oil exists in the ground. I don&#x27;t see an issue with pumping it up, using it for a bit, and putting it back into the ground via a landfill.- The form it is put \"back into the ground\" isn&#x27;t the same form it was \"pumped up\".- The landfill isn&#x27;t the same location where we put it compared to where we sourced it (much deeper).I&#x27;m not an ecologist, but both these differences have complex consequences for earth and us. reply throwaway2037 6 hours agoparentprevnext [–]Everything from my socksAre you socks synthetic fiber? I am surprised they are not cotton.One way to think about plastic: Is there a reasonable alternative? For most plastic (food) wraps, the answer is (sadly): no. They are the best option to reduce food spoilage and only take a couple of grams of oil to make them. Plastic bottles for drinks? Most of them can be avoided by reducing your consumption, or reusing old bottles. reply smileysteve 5 hours agorootparentPatagonia has a great article on plastics - wool socks on a polyester core create a drastically better and longer lasting sock than 99% cotton or wool.https:&#x2F;&#x2F;www.patagonia.com&#x2F;why-plastics&#x2F; reply tangjurine 3 hours agorootparentArticle linked doesn&#x27;t have anything to do with socks? reply sotix 9 hours agoparentprevI have been doing this for about 2 years now. Got a few glass mason jars that I fill up with items like rice and nuts from the bulk bin at the grocery store. Peanut butter and jelly both come in glass jars. I get just about everything else from a local farm share where the items are not packaged in anything. It’s nice not dealing with plastic with my food. I used to throw so much of it away. Now most of my food waste is compostable. reply itake 8 hours agorootparentMy understanding is glass causes higher levels of carbon emotions b&#x2F;c it is heavier to transport [0].If you are re-using the jars, then that would have a lower impact, but if you&#x27;re buying jarred butter and jelly, you&#x27;re making the problem worse.[0] - https:&#x2F;&#x2F;www.bbc.com&#x2F;future&#x2F;article&#x2F;20230427-glass-or-plastic... reply bmitc 3 hours agorootparentGlass is basically infinitely usable, aside from breaking it, and does not pollute the things that go in it or if it&#x27;s tossed in a landfill. It&#x27;s also the most easily recycled material with high yields (basically 100%). reply voidnap 1 hour agorootparentI was recently made aware that some amount of glass is not recycled and instead turned into powder to cover landfills. Apparently, where I live, most used glass ends up this way; but I expect it varies between institutions.https:&#x2F;&#x2F;www.cbc.ca&#x2F;news&#x2F;canada&#x2F;montreal&#x2F;recycling-glass-eigh... reply azemetre 7 hours agorootparentprevI&#x27;d think the benefit of using glass, especially in the context of food and drink, would be lessening the amount of nano plastic your body digests.Besides it&#x27;s not hard to tip the carbon emission in favor of glass if the externalities of plastic (and fuel) were appropriately priced and not massively subsidized. reply throwaway2037 5 hours agorootparentAbout nano plastic: Are they any serious (respected), peer-reviewed scientific studies on the negative health effects of nano plastics? It is frequently mentioned by the \"natural crowd\" on HN, but I never see any scientific evidence shared. reply philip1209 7 hours agorootparentprevGlass correlates with higher levels of carbon emissions, but doesn&#x27;t cause it.At Silo, wine causes a lot of headaches. Their solution is to use wind-powered transport ships. Then, they turn the waste glass into tiles. So, the carbon footprint is low.If you can tolerate slow shipping, then the carbon footprint can be reduced. So, well-preserved food in glass (as opposed to, say, flowers in plastic) can have a lower environmental impact. Plus, glass can be reasonably reused or recycled. reply throwaway2037 5 hours agorootparentnext [–]they turn the waste glass into tilesThis is delusional. Think about how much heat energy it takes to melt glass to make tiles. There is no way this has a low carbon footprint. I applaud the PR team at Silo for convincing their customer base of this fantasy. The lifetime carbon footprint (from manufacturing to \"recycling\" &#x2F; disposal) is much lower for plastic compared to glass. And, I write this post as someone who tries to reduce my plastic footprint as much as possible. reply smileysteve 5 hours agorootparentProbably about ⅓ as much heat as virgin glass reply kylebenzle 7 hours agorootparentprevThere is no active wind powered transport, obviously. The one ship they are testing is not shipiping anything yet.https:&#x2F;&#x2F;www.cnn.com&#x2F;2023&#x2F;08&#x2F;22&#x2F;travel&#x2F;wind-powered-cargo-shi... reply philip1209 7 hours agorootparentIncorrect.\"Three Dutchmen decided to take a step back to make a leap forward and established Fairtransport, the world’s first modern emission-free shipping company. Silo gets coffee via Fairtransport – it still has to be transported from the landing port, but it is a huge saving on energy. It’s a fine example of imaginative thinking. The small fleet of traditional sailing ships used by Fairtransport includes the world’s only engineless sailing cargo ship. The focus is on transporting special products which are organic or crafted traditionally – such as olive oil, wine and rum. In addition, the fleet is raising awareness of the huge amounts of pollution created by the modern shipping industry and is affecting positive change in the way goods are shipped around the world.\"McMaster, Douglas. Silo (p. 71). Leaping Hare Press. Kindle Edition.https:&#x2F;&#x2F;fairtransport.eu&#x2F;en&#x2F; reply philip1209 8 hours agorootparentprevI do similar things. Just replacing plastic sandwich bags with reusable silicon ones has been a big win. Even at the farmer&#x27;s market, I eliminated the last little plastic by bringing cotton vegetable bags.To be clear, I&#x27;m not puritanical about this. I still do takeout sometimes. And, I&#x27;m about to hop on some flights that will burn a ton of oil. (Ironically, one of those carbon-belching flights is to a dinner at Silo, which I&#x27;m looking forward to.)Two of my biggest realizations have been:1. There are many low-hanging changes we can make to reduce plastic, such as going to a farmers market, replacing a one-time use sandwich bag with a reusable one, or skipping delivery food (which often has more plastic packaging than actual food).2. Low-plastic is often synonymous with quality. My cotton jeans, leather bag, and metal water bottle are all more expensive - but they are higher quality, have lasted for years, and can often be repaired. For example, my leather was an expensive $400, but I&#x27;ve had it for 8 years and counting - and just had it restored to look shiny and new. (Put another way: Plastic lacks wabi-sabi beauty). reply throwaway2037 5 hours agorootparentMy god, this section is full of self-congratulatory, delusional posts. Hop on a flight to go to dinner at Silo? That single flight it probably worth more than a decade of reducing plastic sandwich bags. My cotton jeans, leather bag, and metal water bottle are all more expensive - but they are higher quality, have lasted for years, and can often be repaired.You can probably buy jeans at Walmart for 10 USD. They will last for a decade. reply smileysteve 5 hours agorootparentWalmart jeans are about $20, men&#x27;s wranglers have stretch with spandex plastic, but will definitely approach 10 years. reply beingfit 4 hours agoparentprev> I&#x27;ve tried to eliminate single-use plastic from most of my life. It&#x27;s incredibly hard.I’ve tried too, and I agree that it is very hard. I rarely buy packaged and ready-to-eat food products, which eliminates a lot of plastic. But still there’s so much plastic when buying grocery and produce. Buying grains or other items from loose bins means insects, lower quality and what not. I’m happier with the lower plastic though. reply zx8080 9 hours agoparentprevDoes replacing plastics with paper packaging means that trees will be eliminates much more on scale? reply philip1209 9 hours agorootparentTrees are renewable. The mass of a tree comes from the carbon it extracts from the air. [1]It&#x27;s not a zero-sum, though. Why does my cilantro need to come in a plastic bag? At the farmer&#x27;s market, it doesn&#x27;t come packaged at all.[1] And, fire is basically reversing the process - releasing the sunlight that created the plant: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=N1pIYI5JQLE reply OJFord 8 hours agorootparent> Why does my cilantro need to come in a plastic bag?Everything here is hugely regional, but two answers spring to mind:1) coriander is frequently sold in unpackaged bunches, typically larger than the packaged ones;2) it&#x27;s packaged in plastic because then it can be filled with some inert gas that prolongs spoiling, it&#x27;s not completely obtuse.(In the UK for example Waitrose sells it growing reared in Sussex, or packaged grown idk Spain or Monaco or somewhere; Sainsbury&#x27;s sells it similarly packaged or cut and bunched of undisclosed origin.) reply philip1209 8 hours agorootparentThe big takeaway from the Silo book matched the motto of restaurant Noma - \"Time and place\" - to eat locally and seasonally. So much of the carbon footprint of food comes from not eating locally or seasonally. For example, getting an avocado toast in November here in NYC requires putting an avocado on a 747 and flying it around the world. And, yes - in places like the UK or Denmark or New York, that can mean that fresh foods in Winter are scarce - but that&#x27;s where fermentation can be a fascinating area of exploration (and, a related passion of mine right now).I don&#x27;t share this to create a sense of guilt or scarcity. We have oil now and can get coriander and avocados in the Winter. That means it can be from a position of fun (and privilege) that we can explore alternative ways to eat. But, our ancestors still ate without 747s. Constraints drive creativity - and exploring these areas of low-carbon eating now can drive innovation that helps us cope with with a more low-oil world. replyjll29 25 minutes agoprevThere are several topics that I recently got more into:- The utility of randomness: returning from a machine learning conference in Italy where I had a pretty random (if you do not believe in fate) conversation with a stranger, where suddenly more people stood around and joined in. It basically started with me commenting on one poster \"Isn&#x27;t it amazing how useful randomness is, given there is no pattern&#x2F;structure in a random sequence, and it costs me nothing to make up?\" What I meant is how can something as arbitrary still be useful. Suddenly we started collecting examples: randomized algorthms like the random walker model behind PageRank, Random Forests in machine learning, random numbers for perfect encryption (one time pad), Pentti Kanerva&#x27;s \"Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors\" (thanks to an anonymous Finnish bystander, for I could not recall Kanerva&#x27;s name at the time) etc. Today, I found a book that studies how random people&#x27;s travel decisions are: Ennio Cascetta (2009) Random Utility Theory, Heidelberg: Springer.- hypergraphs: graphs have been very useful representations for so many things in work and life, but they are only for expressing dyadic relations between nodes, whereas their generalizations, hypergraphs can express n-ary relations between nodes.- how to bring people back to a culture of books & libraries: I collect books and I read a lot. As I teach&#x2F;lecture also, I often ask my students about their reading habits, and find it shocking that many have never been to a library, and most do not read books. I would like to contribute to changing that (seems harder than finding out whether P = NP?). reply a3_nm 12 hours agoprevThe following open research problem. Given an undirected graph G with two vertices s and t, the task is to determine whether there is an undirected path connecting s and t which is simple (no repeated vertices) and has length divisible by 3.It is not known whether this problem is NP-hard, or whether it can be solved in polynomial time; apparently the question is open since the early 90s.(The problem is also open for paths of length p mod q for any fixed p and q (fixed means they are constants, and are not given as input), whenever q>2. The problem is known to be in PTIME for 0 mod 2 and 1 mod 2, and to be NP-hard when the graph is directed. Pointers to related work here: https:&#x2F;&#x2F;gitlab.com&#x2F;a3nm&#x2F;modpath) reply sdenton4 11 hours agoparentAn old favorite open graph theory problem:In your right hand, take any collection of trees with 2, 3, ..., n vertices. These have a total of 1+2+...+n-1 edges, ie, n choose 2.In your left hand, take the complete graph with n vertices. This, of course, has the same number of edges.Conjecture: it is always possible to pack&#x2F;embed the trees into the complete graph in such a way that all edges are matched exactly once.The problem has been open for like fifty years, lacking a counter-example or a proof. One can assume Erdos thought hard about it at some point... reply muldvarp 10 hours agoparentprevWhat&#x27;s the motivation for this problem if there is any? It&#x27;s a really specific problem. reply 4death4 7 hours agoparentprevWhat makes it NP-hard? Naively, I would assume the number of simple paths between any two nodes in a graph to a polynomial function of the number of nodes plus the number of edges, so checking every path wouldn&#x27;t be hard. reply penteract 7 hours agorootparentIn the complete graph on n vertices, there are (n-2)! simple paths of length n-1 between any pair of distinct vertices, which is more than any polynomial in the number of vertices or edges. reply fuzzer371 9 hours agoparentprevI mean, so what if there is? reply melvinmelih 7 hours agoparentprevdone: https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;c8bc7305-6c75-4194-8753-b319d6... reply voxl 6 hours agorootparentI can&#x27;t tell if you&#x27;re making a joke, a troll, or genuinely believe what ChatGPT tells you with out bothering to test it.This is why I hate the AI hype, because people do legitimately do this and think \"wow it&#x27;s so smart.\" When I explained the halting problem using Python to a student he asked \"did you use ChatGPT?\" No. Of course not... reply melvinmelih 5 hours agorootparentLol it was a joke, but I imagine it won’t take too long before AI can start solving problems that we’ve been struggling with as humanity. reply seige 2 hours agoprevSolving men&#x27;s issues, particularly friendship for middle aged men, developing hobbies outside the purview of providership, infrastructure and tools for fighting addiction of all kinds and mental fitness. I think a lot about what a modern day men&#x27;s magazine would look like, and what sort of platforms or tools could exist in service of men&#x27;s emotional well being. reply patrick91 1 hour agoparenthow are you doing that? reply uncharted9 19 hours agoprevLearning about my own mind and how to tame it.I&#x27;ve started realizing that I don&#x27;t have much control over the external world, people or events, and only my perception of it can keep me sane or fix my mental issues.This has effectively forced me to see my relationship with my devices with more scrutiny. I&#x27;ve repeatedly found that digital consumption, whether it is infotainment, knowledge, or mindless Reels or Shorts, has always led me to a more depressed and sad state.A recent trip in the mountains without any cell reception even further confirmed this hypothesis for me personally.I try to leave my devices and social media for longer durations, but the eventual FOMO and withdrawals that kick in always bring me back to square one of agony.Events around a romantic interest recently also made me to rethink on how to effectively control emotions and feelings. The other person can do nothing wrong, but my mind can still feel tormented by their simple actions and events that unfold.Unless I can achieve some sort of mental and emotional equanimity, I feel all my pursuits of learning and career would still not alleviate me from this joyless state of life. reply m0d 6 hours agoparentYou should try Raja-Yoga. I suggest you start small - with Vivekananda, for example and then, if that brings you joy, read the whole plethora of interdependent teachings. reply kukkeliskuu 6 hours agoparentprevPerhaps you might be interested in the works of George Ivanovich Gurdjieff. When I encountered his ideas, I was impressed by his psychological insights. I had never encountered same level of insight before. It really resonated with me.His approach is not religious in the usual sense -- there is no need to believe anything or pray to some entities. It is not in conflict with religions either. Gurdjieff is presenting the psychological and spiritual ideas that are behind many mystical traditions of religions in a way that is more digestible by the modern man, and possible to apply in modern life.His own books are probably too dense to start from. A good starting point for me was the book series Psychological commentaries by Nicoll. Another popular one is In the search of the Miraculous by Ouspensky. You can only get the Commentaries as used, and they are really expensive, but you only need one of them to start with and PDF versions can be found online. One one book of the series will do, as they all are all composed of one pagers on recurring topics.Drop me a note if you are interested. reply cammil 11 hours agoparentprevI&#x27;m pretty sure Buddhism is what you want reply jackthetab 8 hours agorootparentOr stoicism. reply layer8 7 hours agorootparentOr CBT. reply riansanderson 6 hours agoparentprevThere are books that can help with this action -> thought -> emotion spiral.I found great help by reading the (IMHO poorly titled) Happiness Trap: How to Stop Struggling and Start Livinghttps:&#x2F;&#x2F;a.co&#x2F;d&#x2F;1QeKVAn reply workbreak 8 hours agoparentprevYou might find this video interesting: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kse87ocS0UoIt talks about different levels of thinking without going into any religion or culture.PS: The YouTube channel seems to be related to dating, but this particular video doesn&#x27;t have any of that. reply asimovfan 11 hours agoparentprevCheck out buddhism and meditation, pretty much all about that reply dcw303 8 hours agoprevMath as prep for 3d game programming. I&#x27;ve been out of school a long time, and when I was there I didn&#x27;t even get to the pre-calculus level.This year I&#x27;ve churned through all the introductory level texts from Art Of Problem Solving. Yes, they&#x27;re written for high schoolers and you need to have some humility to admit you might be missing or have forgotten some fundamentals, but the lesson strucutre really appeals to me. It&#x27;s the only series I&#x27;ve found that respects the learner and really builds up knowledge one piece at a time.Before I start the intermediate texts and the calculus book, I&#x27;ve taken a detour to \"Linear Algebra: Theory, Intuition, Code\" and it&#x27;s sticking a lot better now than previous attempts on the subject. So that gives me some confidence. reply Scarlxd 3 hours agoparentI am currently reading the introductory books of AOPS as I did not do well in high school math. I did not find the way my teacher taught math interesting because it didn&#x27;t connect with real-world applications. The hardest thing I face as a 21-year-old going through the middle and high school math books is accepting that I lack the foundational knowledge and setting my ego aside. My plan is to learn calculus and linear algebra as a follow-up. reply tommychillfiger 8 hours agoparentprevThis is awesome to hear. I took trigonometry and business calculus 1 & 2 in college, but that&#x27;s a far cry from what I&#x27;d need to do anything interesting with ML which I&#x27;m interested in trying.I&#x27;ve bought a few textbooks for statistics, math, and data engineering - I&#x27;m in the middle of getting my life together in terms of habits and time management so hopefully I&#x27;ll be right there with you doing what is essentially remedial math for someone in their 30s, lol. I know it will be exciting once I make space for it and get into a rhythm. Cheers! reply yura 6 hours agoparentprevThe Art of Problem Solving books are not for regular high schoolers. I&#x27;d say they&#x27;re more geared towards gifted high schoolers (which may eventually go on to compete on contests such as the USAMO or the IMO). So definitely no shame in learning from them. reply rjbwork 1 hour agorootparentYup. They were the bible for our competitive math teams in HS. reply politelemon 20 hours agoprevGames. I&#x27;ve been forcing myself into productivity for too long because that is the thing to do. I recently built a gaming pc, picked some jrpgs, and I&#x27;m having a blast. There are entire franchises and worlds for the exploring. There are amazing rich stories and history and beautiful soundtracks that I&#x27;m living through right now. My wishlist grows by the day as I discover new tales. reply cwales95 11 hours agoparentAbsolutely this. The overwhelming need to be “productive” has been wearing on me a while.Gaming is a great stress reliever and I make sure to make some time for it everyday. I mainly like single player games for my PS5 such as Horizon, Cyberpunk 2077, The Witcher and more recently Spider-Man 2. Sometimes you just need to be selfish and have you time! reply Aaronstotle 10 hours agoparentprevAs I&#x27;ve gotten older its rare for me to enjoy games, I can&#x27;t seem to play for more than 15 minutes before wanting to turn it off&#x2F;feel like I&#x27;m bored. reply stcroixx 6 hours agorootparentAs I get older, game controls just get too complex for me to enjoy casually. Way too many combinations of stuff to remember. NES is most fun - 2 buttons and a d-pad. reply unixhero 10 hours agorootparentprevOh my dear dear bored wanderer. You have not tried Baldirs Gate 3. reply dolmen 10 hours agoparentprevBought a SteamDeck. I haven&#x27;t play video games so much for the last 20 years as in the last 2 months. reply jader201 8 hours agorootparentI dunno, I was a fairly early preorder, and have been quite underwhelmed with it. It does what it’s supposed to do, but there are just way too many games in my library that don’t really work well with it, and&#x2F;or are just overall much better with keyboard&#x2F;mouse.Maybe I’m not the target audience. I also play console games, so most of my library are games that don’t overlap with console (so games that are probably designed for keyboard&#x2F;mouse).I played it for a bit, and there are definitely some good games on it (I finally beat Portal on it), but I just haven’t found any games that make me want to break it back out. reply xedrac 10 hours agoparentprevI tried this by playing Factorio, and realized I was doing similar \"work\" as my job, but without the added stress of deadlines, compromise and people. Ironically, it felt extremely productive, but not very recharging. reply november84 7 hours agorootparentI feel this. I don&#x27;t reach the same level of drain after playing compared to work. However, it&#x27;s such a fun way to use my brain in a similar way, I can be hard to put it down.I also find it rewarding after getting robotics and memorizing recipes. Which allows me to focus more on designing (I suck so bad at it but it&#x27;s fun).After a good few hundred hours I moved on to krastorio2 + space exploration. It&#x27;s so much more complex.Oh and multiplayer is a great addition allowing me to asynchronously play with some friends &#x2F;colleagues. reply jjice 20 hours agoparentprevWhat are your favorite RPGs at the moment? New or old? I feel like RPGs have been holding strong since the Super Nintendo and there so many hours of incredible gameplay to be had. reply jll29 46 minutes agorootparentI never liked Rocket Propelled Grenades, so no favorites, and in these dark times of multiple violent conflicts, they are holding up the path to peace. Never knew Nitendo was also a weapons manufacturer, I know them mostly for computer game devices. reply artimaeis 19 hours agorootparentprevI know it&#x27;s got a lot of hype at the moment, but Baldur&#x27;s Gate 3 really is something special. I never played the first 2 games, but I have played D&D regularly for some 16 years now and it&#x27;s one of the best narrative adventures I&#x27;ve ever had.I&#x27;m planning on going back and playing the Divinity: Original Sin games that Larian made before it. I hear they&#x27;re fantastic as well. reply D13Fd 15 hours agorootparent> I&#x27;m planning on going back and playing the Divinity: Original Sin games that Larian made before it. I hear they&#x27;re fantastic as well.They are. I&#x27;d play them in order -- the combat mechanics of DOS1 are kind of tough to swallow after the improvements in DOS2.The DOS1 plot is also a bit thin, and you have to appreciate their brand of humor. But they are both great. reply cwales95 11 hours agorootparentprevI’m getting Baldur’s Gate 3 as a gift for Christmas this year and I’m looking forward to trying it. reply Const-me 10 hours agorootparentprevAn old one: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vampire:_The_Masquerade_%E2%80...There were many technical issues with the release version, but GOG sells the game with unofficial patches integrated, which worked great for me. reply unixhero 10 hours agorootparentYes that game is STUNNING and absolutely pulled me in reply plasticchris 10 hours agorootparentTry it as the insane vampire, it’s great. reply unixhero 3 hours agorootparentI don&#x27;t remember thar it was possible to select House &#x2F; class. Was it? It is 23 years go so ... reply hotnfresh 9 hours agorootparentprevMalkavian (the crazy ones) and Nosferatu (masquerade violation if anyone sees you) are both very-different spins on the game compared with the rest of the clans. reply D13Fd 15 hours agorootparentprevIf you haven&#x27;t played the various Bethesda games (Oblivion, Skyrim, Fallout), now is a great time. There is a modding tool called Wabbajack that auto-installs curated lists of hundreds of mods, all patched to work together, and you can get an incredible setup with very little work. reply jonwinstanley 11 hours agorootparentAre these mainly new features for the games or graphics updates? reply politelemon 19 hours agorootparentprevRight now the Trails series (trails in the sky, trails of cold steel)... I think it&#x27;s an old and new series isn&#x27;t it?Also a surprise like was Blue Reflection, if you watch its trailer it doesn&#x27;t even look too dynamic but it was great fun. reply unixhero 10 hours agorootparentprevBaldurs Gate 3 reply namrog84 10 hours agoprevQuit my job earlier this year to start an indie game studio and a little bit of contributions to misc open-source projects.A fair balance of relax time but also enjoying the time off. There is so much I want to do and get excited for, but I still suffer a lot of procrastination related problems or else I feel like I could have done a tremendous amount more.I feel like there is lifetimes of things I get excited about and want to do, but spend so little time doing them. :( It sucks reply tommychillfiger 8 hours agoparentFeel this hard. I feel like I did well during unemployment while the Covid lockdown played out. I was an audio engineer before that, and during lockdown I ran 30 miles a week, made a lot of music, learned some Python, managed to pivot into tech (analytics). Still, it never feels like I&#x27;m really doing anywhere near as much as I could - that was all pretty much 3-5 hours a day with the rest being more or less pure leisure and rest.Part of me is tempted to buy into the idea that the human animal just isn&#x27;t naturally inclined to do more than a handful of hours of difficult or \"productive\" work in an average day. I honestly have no idea whether I&#x27;m normal or not, but I agree the pressure I put on myself probably isn&#x27;t ultimately helpful most of the time. It&#x27;s hard not to feel like others are getting way more done, and even harder to tell how much of what is presented is authentic. reply mashlol 9 hours agoparentprevI&#x27;m quitting my job in December to work on my game full time, neat to find someone else who also quit for something similar.Would love to connect, maybe we can help motivate each other :). My email is in my profile if you&#x27;re interested in connecting, same goes for anyone else in a similar situation. reply phforms 9 hours agoparentprevCan relate. I get excited about so many things I want to do, but ultimately lack the energy and abilities (executive dysfunction) to do even a tiny fraction of them. And there is this constant self-pressure of “I should do so much more”, which is not helping. Anyway, hope you have a great time and success with your indie game studio! reply abraae 11 hours agoprevMoving golf simulation into the cloud [0].Golf is one of the only sports that can be realistically simulated indoors.Golf is also one of the only sports where even severe lag has no effect on the accuracy of the play - just the viewing experience. Unlike FPS games in the cloud which become unplayable with even modest lag.The golf simulation technology landscape today is pre-cloud. People running their own gaming computers with complex lash ups of open source software to connect their launch monitors. All pretty prohibitive to the stereotypical older tech-averse golf player.[0] https:&#x2F;&#x2F;golfsimcloud.com reply winrid 7 hours agoparentyour site flashes and loads content for like 10 seconds for me. the content just keeps moving up and down... reply abraae 7 hours agorootparentIs it possible you have first party cookies disabled? On your first visit we need to run some JS (apparently the only way to get the user timezone). We store that in your session and then redirect you so we can show availability in your timezone. If first party cookies are disabled then there will be no session and the page will loop. (Open to suggestions on better TZ handling - other than rewriting as an SPA :) reply latchkey 2 hours agorootparentWhen you loop at least set a query string counter and stop after 3 times. You can then display a note to the user that they need to enable first party cookies. reply veqq 6 hours agorootparentprevThe first time loading the page, the white bar holding the menu flashes 2cm lower then higher for a few seconds (as elements load). It doesn&#x27;t flash afterwards. Consider moving something to the footer, so the user doesn&#x27;t see the loading&#x2F;flashing. reply abraae 6 hours agorootparentMany thanks for the input. reply smm11 11 hours agoparentprevGolf and downhill skiing were huge when I was a kid. I grew up golfing and downhill skiing.Decades have gone by now. Who golfs? Who has downhill skis? reply samschooler 11 hours agorootparentI think this very much is a personal perspective thing. Skiing is only increasing in popularity. Many ski resorts recording record high visitation this year. https:&#x2F;&#x2F;coloradosun.com&#x2F;2023&#x2F;05&#x2F;29&#x2F;national-ski-areas-record... reply asdff 10 hours agorootparentprevGolf has surged in popularity after covid, 20 year high rates of growth in every demographic. reply p1mrx 11 hours agorootparentprev> Who golfs? Who has downhill skis?I imagine you could answer that question by going to a golf course or ski slope to see who&#x27;s there. reply havefunbesafe 11 hours agorootparentprevColorado resident, here. I&#x27;d say 95% of people I know do one or the other. Probably 50% of people I know do both. reply stcroixx 6 hours agorootparentprevBought season passes for me and 3 of my kids this year, we’ll ski as much as possible. Have multiple pairs of skis myself. Resorts are always busy and 50&#x2F;50 ski&#x2F;snowboard. reply buildbot 10 hours agorootparentprevFiguratively everyone skis in Seattle. Like, you need to reserve parking or get shuttled at Crystal these days it’s so busy. reply makingstuffs 6 hours agoprevLove, life, people and planet.When I was young I spent a lot of time blinded by rappers with their spinning wheels and crowds of hangers-on which drastically lead me astray — being from a single parented poverty stricken family in the London it wasn’t exactly hard.Since then I have been through so many different stages to get to the point of contentment with whatever life throws my way.Within this contentment I have found that the things I truly care about are (as cliché as it sounds) not the things which can be bought but the things which connect me to other beings.The realisation and acceptance of the inevitable end of life has made me realise that we will all cease to exist within a generation, our memory will cease within two, our entire footprint within three.From this I found my passion was not buying things to distract me nor was my desire to be remembered long after I pass. It is to enjoy the now, make others feel the love I never felt until I reached my mid 30s, lift people out of bad places where possible and do my utmost to appreciate this breathtakingly beautiful planet we call home.Cheesy, I know reply Elegia 8 minutes agoparentNot cheesy at all. I wish I could enjoy the same thing, but when it comes to showing kindness to others I often end up feeling used. reply ddmf 1 hour agoprevGetting out of my current flat &#x2F; apartment as I&#x27;m getting serious noise pollution from above and to the left of me, and it&#x27;s really affecting my mental health. Not had a proper night&#x27;s sleep since the new upstairs neighbour moved in, around June&#x2F;July, and this extra sensory overload has removed my ability to ignore the sub-bass coming from the left of me.Last week there was a guy in social housing who killed himself after similar, and since then my social housing manager has been in touch to look at doing something - sad that it takes a death before someone investigates. reply rchaud 19 hours agoprevPrint media -- seriously. There are vanishingly few types of paper-based products left besides mass-market paperbacks and a handful of soulless conglomerate-owned magazines that may disappear tomorrow.Just like there are small farming collectives out there, I&#x27;d love for there to be micro-magazines and short stories on paper, made cheaply and distributed to small mailing lists with a single stamp (not an email newsletter!). PDF versions available for long-distance readers.Something, anything to counter the overwhelm of ad and email popup-ridden \"content blogs\" and walled garden platforms sucking everything into their in-house LLMs. reply andremendes 19 hours agoparent>Just like there are small farming collectives out there, I&#x27;d love for there to be micro-magazines and short stories on paper, made cheaply and distributed to small mailing lists with a single stamp (not an email newsletter!). PDF versions available for long-distance readers.You seem to be describing what I&#x27;ve known (since forever) by \"Zine\"[¹][¹]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zine reply rchaud 6 hours agorootparentYeah that&#x27;s what I was thinking of. Are there semi-professional zine publications out there? I checked out Etsy but the stuff there was not really to my taste. reply dendrite9 2 hours agorootparentDo you have any decent local bookstores nearby? You might be surprised at the variety of magazines they have once you skim past the usual options. I&#x27;ve seen all sorts of niche publications, including zines. Most are not for me, but that isn&#x27;t a problem since there is usually something that catches my eye.You might find something that grabs you from the list of exhibitors here: https:&#x2F;&#x2F;www.seattleartbookfair.org&#x2F;Exhibitors Or look for similar events and check out who attends.I like reading about food, so places like Omnivore books have specialty books or magazines that are not just recipe books. Though I do like cookbooks. https:&#x2F;&#x2F;omnivorebooks.myshopify.com&#x2F; reply mapleoin 19 hours agoparentprevI&#x27;ve had the opposite experience. I&#x27;m amazed at the types of print magazines on niche topics that still exist. Are you aware of https:&#x2F;&#x2F;www.stackmagazines.com&#x2F; ? reply piva00 16 hours agorootparentI&#x27;ve been looking for something like this for a while, thanks a lot for sharing! It just sucked when I was checking out a few mags and after calculating the cost for shipping to Sweden it was higher than the total cost of 4-6 magazines... reply greenie_beans 7 hours agoparentprev> I&#x27;d love for there to be micro-magazines and short stories on paper, made cheaply and distributed to small mailing lists with a single stamp (not an email newsletter!). PDF versions available for long-distance readers.what do you like to read? i might have some suggestions reply rchaud 6 hours agorootparentI&#x27;m open to different topics. Short stories, music, business, art. reply scruple 17 hours agoparentprevI am also into micro-zines (and larger). There&#x27;s a great community around Sword and Sorcery style short stories, with a handful of high quality quarterly and monthly publications, with strong authors in the genre. reply jkepler 11 hours agoparentprevAre you ruling our hardcover books that still get written and published? I&#x27;m on email mailing lists of two small&#x2F;Independent publishers, and they still make books.And where I live, there&#x27;s a book binders workshop on my street. reply dan_quixote 8 hours agoprevLearning Korean to better communicate with my GFs family and take advantage of _finally_ having real motivation to learn another language.I&#x27;m really enjoying having a different type of challenge than I&#x27;m used to. Mostly approaching it with Duolingo and Anki vocab cards with moderate success so far. We both have our own careers, kids, various responsibilities...so I don&#x27;t get much time to practice with her. And practicing memorization at the end of a long day is tough, so I&#x27;d love to hear any tips you all may have to improve my memorization techniques and mnemonics. reply second_brekkie 6 hours agoparentGet Korean teacher irl or on italki. At least once or twice a week.Try to speak and listen to much Korean as you can.Watch Korean kids tv shows on Netflix with Korean & english subtitles (I use language reactor chrome plug in)Real life Korean conversations for beginners by Talk to me in Korean is good.When you get to an intermediate level (TOPIK level 3ish) the iyagi podcast is GREAT.Ive been going through the sejong institute practical Korean textbooks. You can get them for free online. They focus on understanding and expressing youself in spoken Korean. The audio examples emulate Korean speech much more than other Korean textbooks I&#x27;ve used.The sejong institute also to free Korean online courses with weekly zoom lectures.How to study Korean, sells anki flashcard decks with audio attached. The lessons are free. They&#x27;re quite dense but are BRILLANT as a reference to grammar you have forgotten or want to know in more nuance.Vocab-wise when you do flashcards have two for each direction KORENG. Learn the words for 3 days before putting them into an Anki deck (e.g. 30 words a day, but every day 10 words &#x27;graduate&#x27; to the anki deck)I find Anki good for keeping words memorised but not learning them. When you are looking at the korean word play the audio, it helps SO much!Remember Korean is one of the hardest languages for a English native speaker to learn. There are times when you will feel you&#x27;re making no progress. Then one day you will realise you actually understand what you MIL is saying. Have a sacred time every day where you do some Korean. Ideally at least an hour, anything less you will not improve quickly enough to re-motivate you, or thats at least how it was for me.Good luck 파이팅! reply Ilasky 7 hours agoparentprevKind of in a similar boat!Been learning Korean to speak more with my girlfriend’s family, especially since we’re going to Korea soon.My biggest waves of improvement have come from reading aloud the Talk to Me in Korean books.I’ve found that getting the structure of the sentences has helped me and then I learn vocabulary by using what I interact with on a day-to-day basis.There’s also a Korean slang book by Talk to Me in Korean which is really great too! Helps break out of the “learned it from a textbook” sound. reply orlandohill 7 hours agoparentprevFocus on acquiring the language through meaningful input, rather than rote memorization. Favor listening comprehension over reading comprehension.The Refold guides, YouTube channel, and Discord server are good resources for language learners with busy schedules.https:&#x2F;&#x2F;refold.la&#x2F; reply zweice 7 hours agoparentprevPractice in the morning reply pushfoo 10 hours agoprevText rendering &#x2F; UI on minimal hardware and finding more time for ML-family languages.A few months ago, an iced-rs maintainer[1] recommended I try Elm. So far, this has lead to:1. A an MVP[2] of a curses[3]-like library for CHIP-8 derivatives (https:&#x2F;&#x2F;github.com&#x2F;pushfoo&#x2F;octo-termlib)2. A growing interest in language design3. An ongoing re-evaluation of my software development worldview[1] 13r0ck &#x2F; Brock on GitHub (https:&#x2F;&#x2F;github.com&#x2F;13r0ck). Hire him if you get the chance. He has a rare blend of know-how, mentorship, and community management skills.[2] Unsolved issues with octo-termlib:1. Finding a license friendly toward beginners editing pre-made template assembly files (Maybe zlib + acknowledgement?)2. Elegant & efficient syntax for ending screen X &#x2F; Y parsing before all digits are used[3] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Curses_(programming_library) reply jcpst 8 hours agoprevMaking an album.It’s been a journey. For about a year I was trying to do it all by myself.Then one former band mate moved back, another reached out and we reunited to make new music.This is gonna sound corny, but after that first time we got back together to jam, later that night I was in tears because it playing with them again was such a large emotional release for me.Now we’re in a great place, we all want the same thing. We’re all in on making new music, no one is trying to make this a career or go on tour. It has to be fun, that’s number one. reply SubGenius 5 hours agoparentSounds great, good luck! I&#x27;m hoping to get back to making music again after a few years away due to health&#x2F;life reasons. I definitely know that euphoric feeling of jamming with an old bandmate after a long break.> Now we’re in a great place, we all want the same thing. We’re all in on making new music, no one is trying to make this a career or go on tour. It has to be fun, that’s number one.This is key. I&#x27;ve made more music goofing off than I did when I sat down to write a song seriously. reply wsintra2022 7 hours agoparentprevNice! I’m starting a project that can automate pressing vinyl records, kinda like a kickstarter, you create a 45 day campaign with the aim of selling 75 pre sales and we then go into production (you need to 100 total). No upfront cost from the artists. Still working on the details. My friends have opened a vinyl pressing factory so I figured building out an automated indie vinyl pressing crowd sourcing app would be cool. Check out Kushty buck records if your interested. It’s my weekend passion project. reply stephenstuder 11 hours agoprevBeing a dad - Preparing for the Google Cloud Associate exam (5 YEO front-end developer trying to get out of my bubble) - Fixing my posture using a daily routine - Really fine-tuning our budget, researching the stores that have better prices for items we frequently buy, having a better grip on upcoming expenses, figuring out what things we need that we can buy used, and most importantly, getting the most out of the amount we can spend monthly! reply shooker435 10 hours agoparentI have some exam notes (10 parts) on my blog at https:&#x2F;&#x2F;www.sebhook.com&#x2F;tag&#x2F;pca&#x2F; if that&#x27;s helpful. It&#x27;s mostly focused on the architect exam, but there&#x27;s a lot of crossover. reply brickers 11 hours agoparentprevBecoming a dad or are you already one and just upping the excitement? If you’re new, my two pieces of advice are that there’s only right now to deal with when things start to feel rough, and lick your fingers to open the nappy sack before you start dealing with the nappy. Both gems and hard-won reply rogerthis 11 hours agoparentprevCould you share your routine for fixing your posture? reply kleinishere 9 hours agoprevPreserving family moments with a 35mm film camera and the attachment an analog photograph engenders vs. digital.After seeing a friend with young kids post candid photos of his family for the last few years, I decided to give it a try. Purchased a Retina IIIc rangefinder camera (from the 1950s, preceded SLRs) and it&#x27;s one of the most amazing purely mechanical, consumer-focused engineered products I&#x27;ve held in my hands. Got a scanner, successfully booted the Nikon Scan abandonware on a virtualized Windows XP environment, and saw my first roll of photos appear last weekend. Wife and extended family absolutely loved it. I&#x27;ve reviewed those 36 photos (standard roll length) more in the past week than the 1,000 family photos of the past 2 years on my cell phone. Excited for more. reply sotix 8 hours agoparentFilm cameras truly are a joy. I’m using a Minolta X-700[0] myself. I also review the photos taken with it far more than my digital photos. There’s something about slowing down and focusing on taking a few good shots instead of a quick action with minimal effort that feels so rewarding. And they can look beautiful with the right film. Enjoy your new camera in good health![0] https:&#x2F;&#x2F;www.kenrockwell.com&#x2F;minolta&#x2F;700.htm reply SwiftyBug 9 hours agoprevLearning to use Vim! I&#x27;ve tried three times before in the past 10 years. This is the first time it actually clicked. It&#x27;s the first time I&#x27;ve tried Neovim. It&#x27;s awesome and I feel like I&#x27;m playing a game with myself while working. I thought it would take much longer to get to the speed I&#x27;m at now. I&#x27;ve been using Vim exclusively for a month. reply enasterosophes 6 hours agoparentCongratulations :) The same thing happened to me about 12 years ago. I&#x27;d tried Vim a few times and it didn&#x27;t grab me, but then suddenly it clicked and I haven&#x27;t looked back.Not sure if you&#x27;ve seen this, but worth working through, sooner or later (no rush): https:&#x2F;&#x2F;learnvimscriptthehardway.stevelosh.com&#x2F;A discipline I wished I&#x27;d learned earlier was don&#x27;t let your vimrc grow too big. Instead, keep carving parts of it off into plugins. Track everything in Git. reply champoradopapi 7 hours agoparentprevSame here! What keys&#x2F;combinations are you usually using right now? reply SwiftyBug 1 hour agorootparentI try to not stray too far from the default key binding as I think it would be good to be able to use Vim on servers that don&#x27;t have all my config. But, hey, what&#x27;s the point of using Vim if you&#x27;re not having fun ricing it? These are some indispensable key maps to me:In visual mode, move a line up or down: vim.keymap.set(\"v\", \"J\", \":m &#x27;>+1gv=gv\") vim.keymap.set(\"v\", \"K\", \":m &#x27;gv=gv\")In normal mode, replace all occurences of the word under the cursor: vim.keymap.set(\"n\", \"s\", [[:%s&#x2F;\\\\>&#x2F;&#x2F;gI]])In normal or visual mode, Yank to the OS&#x27;s pasteboard: vim.keymap.set({\"n\", \"v\"}, \"y\", [[\"+y]])And there are plenty more that are related to specific plugins.Also, I&#x27;ve remapped caps lock to esc in the OS. This makes it way easier to leave insert mode without leaving the home row. reply november84 7 hours agoparentprevAny resources you can share or did you jump into the docs? reply sovietswag 5 hours agorootparent(Not the person you asked) but it helps to realize that vim in \"insert\" mode is just like any other text editor you know, you can use your arrow keys and the delete button to edit your document. You will get annoyed pretty quickly navigating with arrow keys and no mouse cursor, so you might search up \"how do I jump the end of this damned line!\" (hint: it&#x27;s the $ sign). As you edit documents you will get more and more annoyed and search up more shortcuts which you will be happy to incorporate into your life. This is what worked for me, anyways. reply jodrellblank 5 hours agorootparentprevYears ago I learned the basics from this IRC chat log of someone introducing someone else to Vim basics: https:&#x2F;&#x2F;www.vi-improved.org&#x2F;irc-style-tutorial&#x2F;There&#x27;s a popular stackoverflow answer as a Vim (Vi) introduction, which I haven&#x27;t read through but will link anyway: https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;1218390&#x2F;what-is-your-mos...http:&#x2F;&#x2F;vimcasts.org&#x2F;episodes&#x2F; screencasts are high quality short videos, go back to the early ones for introductions to buffers and windows and things (more recent ones tend to be too many niche scripts and plugins for me). reply SwiftyBug 1 hour agorootparentprevSure! The first thing I did was follow this video: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=w7i4amO_zaEThis is ThePrimeagen&#x27;s 0 to LSP, Neovim RC from Scratch. In this video he performs a clean installation of Neovim and goes step by step adding the things he considers essential. This was very important for me to acquaint myself with how things work, how to install plugins, how to define custom key maps. I remember the first times I tried using Vim, I couldn&#x27;t figure out how to get Nerdtree to work. This video made me realize I just lacked the knowledge of how Vim config works.This video was such a good start because It provided me with the tools to continue my exploration of Vim autonomously. In a week I was already able to install new plugins and tweak them using Lua config files the way I specifically wanted. It&#x27;s such a cool experience!Keep in mind that both the author of this video and I use Neovim, which is a fork of Vim. As a text editor they both function essentially the same. The difference lies on the config files and in broader UI capabilities by Neovim. While Vim uses Vimscript, Neovim prefers Lua, although Neovim is fully backwards compatible, so you can choose to use Vimscript for your configuration if you want as well. This also means that Vim plugins just work with Neovim!The docs are also a huge source of knowledge for me. In the beginning I resorted to :help key-codes a lot when defining key mappings.To learn the Vim motions, which is the most challenging part of using Vim, I suggest you find a cheatsheet online and refer to it all the time. One very cool plugin that will help you get comfortable with Vim motions is ThePrimeagen&#x27;s VimBeGod: https:&#x2F;&#x2F;github.com&#x2F;ThePrimeagen&#x2F;vim-be-good. It&#x27;s a set of game-like exercises to practice the motions. This is also pretty cool and helped a lot: https:&#x2F;&#x2F;vimsnake.com&#x2F;. It&#x27;s a classic snake game where instead of using arrow keys, you use HJKL. And speaking of arrow keys, one thing I did very early on was disabling them (or, in reality, remapping them to noop) in normal mode so I was forced to move around the text using Vim Motions.At first you will get frustrated because your brain will need some time to rewire in a way to absorb all the new abstractions Vim presents. It&#x27;s a whole new logic of editing text. The most important thing is to stick to it and you will be surprised with how fast you end up picking things up. Of course, don&#x27;t expect to be crazy fast in a few weeks. But right now, after a little over a month, I no longer feel that discomfort using Vim anymore. I suppose I&#x27;d still be faster on VS Code, but I really want to master Vim, so I&#x27;m sticking with it and I feel a constant improvement. reply theossuary 6 hours agorootparentprevTbh the best bit of advice I ever got for learning vim is to keep a sticky note on your monitor with 5-10 motions you want to learn, and replace it once you&#x27;ve started using them consistently.Start with h, j, k, l, w, b, a, i, o, esc And go from there reply kaycebasques 7 hours agoprevPoverty abolition.I just finished Poverty, By America. It&#x27;s really moving and seemingly well-researched and honest.I don&#x27;t know if I&#x27;ll stick with it. I don&#x27;t have a good track record of sticking with social activism stuff. Deep-down in my bones I know however that poverty is wrong and really doesn&#x27;t need to exist at the levels we have collectively accepted as \"natural\". reply appplication 10 hours agoprevI just downloaded Ableton Live and it’s been a blast playing with it. Similarly, I finally understand how modular synths work and it’s basically just functional programming with sound. I haven’t bought any modulars yet but I’m really looking forward to getting into it in the new year. Music production is very cool and I can’t wait to learn more. reply loxias 1 hour agoparentEnjoy!!!!I worked in music tech for a few startups starting in 2000, have written a few instruments (VSTs) and dabbled in making noise (some call it music). specialize in cutting edge dsp, but i just like applying it to PHAT SOUNDS. :D i&#x27;m a much better sw engineer DSP enthusiast than an artist, but can still hold my own.Get ready for so much fun. Making music gets infinitely more fun once you start to build all your own tools =) Do you code? If you can code even a bit..It&#x27;s not my day job anymore, but send me an email if you&#x27;d like to know more, I guess I&#x27;m finally old enough to pass on some shit... reply PaulDavisThe1st 10 hours agoparentprev> It’s haven’t bought any Modular’s yet but I’m really looking forward to getting into other on the new year.http:&#x2F;&#x2F;cardinal.kx.studiohttps:&#x2F;&#x2F;vcvrack.com&#x2F;The former is libre and gratis, runs as a standalone or plugin and in the browser!! and is based on the latter.The latter has a libre and gratis standalone version, the plugin version is non-gratis. reply cbrugs 7 hours agoparentprevI got into making house&#x2F;techno music about a year ago and it&#x27;s become a life-changing hobby just like getting into tech was. Getting to that point where you start making \"listenable\" music is unmatched. The best advice that I can give (that you didn&#x27;t ask for) is to use a reference track as you make a song. It keeps you inspired, allows you to learn from the track, and prevents the kind of listening fatigue that makes it easy to convince yourself your mix is well balanced. It&#x27;s not- yet :) Have fun! reply tommychillfiger 8 hours agoparentprevI did audio production for about 10 years before pivoting into tech, ha! Live sound for work, but I&#x27;m also a musician so I&#x27;ve spent my fair share of time with Logic Pro. It&#x27;s funny because I also want to dig more into synths, I&#x27;ve always been a guitarist and have thus far only used soft synths mostly due to not having the money to play with nicer equipment. Now that I make good money, I haven&#x27;t found time to circle back to it. Very high on my wish list is a nice analog synth and maybe a physical sequencer. I find that what happens to me is the futzing around with the DAW gets between my idea and getting it recorded - I&#x27;ve done some solid work at getting channel strip settings and templates built out to minimize this, but at the end of the day I need some more equipment. It really is a ton of fun, though. It&#x27;s as broad and deep as you want to make it, not unlike software more generally. reply jnovek 9 hours agoparentprevThis is me, it was my pandemic hobby and then it just stuck. I’m still terrible on pretty much every front but I’m constantly improving.Take your time with modular. It’s easy to get carried away and overspend. Each module is deeper than it looks and you really benefit from getting to know each one.Edit: and, of course, VCV Rack is the best. reply mr_sturd 9 hours agoparentprevGood stuff!I started getting in to this at the start of the year. Already had an old, dusty MicroKORG and MIDI interface to use it as a controller, but recently splashed out on a bigger controller as the Korg&#x27;s tiny keys were hurting me - plus, I wanted something bigger to get better at piano!A couple of free soft synths I&#x27;d recommend are Surge XT, and Vital.https:&#x2F;&#x2F;surge-synthesizer.github.io&#x2F;https:&#x2F;&#x2F;vital.audio&#x2F; reply aeonik 19 hours agoprevI&#x27;m obsessed with interfaces at the moment.How do humans interface with computers and data, from control to visual feedback.How humans interface with music, perform it, read it, store it. How do birds interface with tones and rhythm?How software interfaces with other software and hardware.How humans interface with the world via symbols. I&#x27;ve been reading about Semiotics a bit, and find the field fascinating.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Semiotics?wprov=sfla1Birdsong as code: https:&#x2F;&#x2F;youtu.be&#x2F;OCYU0LtqRH0?si=4DwOKC3oZ6vE-w-y reply interestica 11 hours agoparentFor my masters I proposed that you could treat urban spaces as one giant interface (and that interface would appear different to different users). After stepping away from it for a while, I&#x27;ve gotten back into it. My main interest is information design for wayfinding, transit, and urban navigation. It&#x27;s such a human problem and it&#x27;s so fascinating.Though he never called it such, a lot of it intersects with Donald Norman&#x27;s work that he describes in \"The Design of Everyday Things\". Even the objects we intereact wit",
    "originSummary": [
      "The author initiates a new post to commemorate the anniversary of a previously appreciated post, showcasing the varied interests of the Hacker News community.",
      "The post serves as a gesture of gratitude towards a user named mckirk.",
      "This illustrates the warm relationships and sense of community within the Hacker News site."
    ],
    "commentSummary": [
      "The author has initiated a new post to commemorate an anniversary of a previous post they found enjoyable.",
      "This action is to emphasize the diverse interests found within the Hacker News (HN) community.",
      "They have also conveyed their appreciation to a user going by the name 'mckirk'."
    ],
    "points": 251,
    "commentCount": 558,
    "retryCount": 0,
    "time": 1699271679
  }
]
