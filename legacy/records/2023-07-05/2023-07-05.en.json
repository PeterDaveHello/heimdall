[
  {
    "id": 36586632,
    "timestamp": 1688478602,
    "title": "Sao Paulo: A city with no outdoor advertisements (2013)",
    "url": "https://www.amusingplanet.com/2013/07/sao-paulo-city-with-no-outdoor.html",
    "hn_url": "http://news.ycombinator.com/item?id=36586632",
    "content": "Sao Paulo: The City With No Outdoor AdvertisementsKAUSHIK PATOWARY JUL 20, 2013 19 COMMENTSIn September 2006, the mayor of S\u00e3o Paulo passed the so-called \u201cClean City Law\" that outlawed the use of all outdoor advertisements, including on billboards, transit, and in front of stores. Within a year, 15,000 billboards were taken down and store signs had to be shrunk so as not to violate the new law. Outdoor video screens and ads on buses were stripped. Even pamphleteering in public spaces has been made illegal. Nearly $8 million in fines were issued to cleanse S\u00e3o Paulo of the blight on its landscape. Seven years on, the world's fourth-largest metropolis and South America\u2019s most important city remains free of visual clutter and eye sore that plagues the majority of cities around the world.When the law was passed, it triggered wild alarm among city businesses and advertisement groups. Critics worried that the advertising ban would entail a revenue loss of $133 million and 20,000 people would lose jobs. Others predicted that the city would look like a bland concrete jungle with the ads removed.Photo credit\"I think this city is going to become a sadder, duller place,\u201d said Dalton Silvanom, the lone councilman to vote against the law, and who (unsurprisingly) is in the advertising business. \u201cAdvertising is both an art form and, when you're in your car or alone on foot, a form of entertainment that helps relieve solitude and boredom,\" Silvanom added.Despite the forebodings, S\u00e3o Paulo\u2019s economy didn\u2019t run aground although the city did look alien and war-torn for a few months following the tear down. The breakneck speed at which the law was enacted caused the city to resemble a battlefield strewn with blank marquees, partially torn-down frames and hastily painted-over storefront facades.In a survey conducted in 2011 among the city\u2019s 11 million residents, 70 percent found the ban beneficial. Unexpectedly, the removal of logos and slogans exposed previously overlooked architecture, revealing a rich urban beauty that had been long hidden. \u201cMy old reference was a big Panasonic billboard,\u201d said Vinicius Galvao, a reporter with Folha de S\u00e3o Paulo, Brazil\u2019s largest newspaper, in an interview with NPR. \u201cBut now my reference is an art deco building that was covered [by the massive sign]. So you start getting new references in the city. The city\u2019s now got new language, a new identity.\u201dPhotographer Tony de Marco documented the transformation the city underwent in 2007 in a sequence of images published on Flickr.Sao Paulo isn\u2019t the only city to have banned outdoor advertisements. Bans on billboards exist in other parts of the world, such as Vermont, Alaska, Hawaii, and Maine in the US, as well as some 1,500 towns. In Europe, the Norwegian city of Bergen does the same and many others have imposed severe restrictions on billboards or declared no-billboard zones within the city.Sources: Newdream, Businessweek, Adbusters, EconomistBRAZILCITYLANDMARKSMore On Amusing Planet{{POSTS[0].LABEL}}{{posts[0].title}}{{POSTS[0].DATE}} {{POSTS[0].COMMENTSNUM}} {{MESSAGES_COMMENTS}}{{POSTS[1].LABEL}}{{posts[1].title}}{{POSTS[1].DATE}} {{POSTS[1].COMMENTSNUM}} {{MESSAGES_COMMENTS}}{{POSTS[2].LABEL}}{{posts[2].title}}{{POSTS[2].DATE}} {{POSTS[2].COMMENTSNUM}} {{MESSAGES_COMMENTS}}{{POSTS[3].LABEL}}{{posts[3].title}}{{POSTS[3].DATE}} {{POSTS[3].COMMENTSNUM}} {{MESSAGES_COMMENTS}}CommentsRed is BadJuly 22, 2013 at 9:10\u202fPMYes ... red is bad ...ReplyDeleteUnknownJuly 23, 2013 at 11:11\u202fAMLooks very eerie...Like the show \"Life After People\"ReplyDeletejessifierJuly 31, 2013 at 5:30\u202fPMBanners, billboards and signage\u2019s are for the advertisements and I thing with these signage\u2019s it becomes easy for the customer\u2019s to locate the plaices. I think if laws are there not to hide the architectures, then sign boards can be used. Free standing sign boards are usually placed at a distance from the shop areas and these signs are actually for the customers so, that they can easily find what they want. I think these signs are essential to attract customers to your business. They need not to ask people to find your business.ReplyDeleteAnonymousJuly 31, 2013 at 8:46\u202fPMWell the Blockbuster store would've been closed by now anyway. (4th picture).ReplyDeleteAdSystemsAugust 7, 2013 at 6:39\u202fPMI don't think think this is a nice move by local govt. Due to this many of people becomes job less and lost their only source of earning.ReplyDeleteREPLIESAnonymousDecember 20, 2014 at 10:12\u202fPM\"AdSystems\" thinks this is a bad idea...DeleteAnonymousNovember 3, 2015 at 7:42\u202fPMit helps the small business thoughDeleteReplyAnonymousSeptember 10, 2013 at 12:34\u202fAMThat looks so peaceful. Ads really aren't beauty since they tug at our attention and attempt to invade and persuade. This might be good for small business that doesn't have a million dollar ad budget.ReplyDeleteAnonymousNovember 25, 2013 at 6:19\u202fAMI think this law needs to be pass on the Philippine government, they are countless numbers of celebrity billboards around the city of Manila, advertising God knows what, which is causing big tragedy, when there is storm, strong winds etc.. causing the billboards to fall down on peoples houses, blocking the street and causing many lives.ReplyDeleteUnknownMarch 25, 2014 at 12:25\u202fAMYES!!!!!! I LIVE IN SAO PAULO!!!!!that is actually AWESOME!!!!! we love itthe visual pollution used to be unbearable now... THIS looks more like a city and less like a formula 1 suit!this law should be considered world wide, really increased our life quality, even though this is still brazil (YES small b, I hate this place!)ReplyDeleteAnonymousAugust 7, 2014 at 5:43\u202fAMIt looks grim, like something from Eastern Europe or USSR during the Cold War.ReplyDeleteAnonymousOctober 29, 2014 at 8:40\u202fAMIt's not that creepy looking, the pictures give that impression because it's focused on the blanks where were there were ads... But the city still have tons of graffiti art and color.ReplyDeleteAnonymousDecember 13, 2014 at 4:22\u202fAMHow do you find anything while driving? Some of those aren't even advertisements, they're signs for the businesses. Where do you draw the line? Barber poles? Cafe blackboard menus? Hopefully they at least took all the billboard structures down and painted over everything, not that the buildings themselves don't look like crap anyway. Gloomy grey stone everywhere.ReplyDeleteREPLIESAnonymousOctober 5, 2015 at 8:42\u202fPMSTREET SIGNS. The architecture now has to stand out more than before.DeleteReplyUnknownFebruary 17, 2015 at 1:14\u202fAMIt looks beautiful; peaceful; harmonious; more human. It allows for your imagination to flourish. I'd say if worldwide cities began adopting this that over the years creativity would increase for the human race. There are enough ads online at this point, the last thing we need is more ads cover our eyes as we walk out own streets.ReplyDeleteUnknownOctober 21, 2015 at 5:39\u202fAMSo how are you supposed to know what is inside any of those businesses? You have to know the exact address in advance?ReplyDeleteUnknownMarch 29, 2017 at 9:43\u202fPMYou guys needed BEFORE and AFTER pics. Just showing the AFTER pics doesn't really do much.ReplyDeleteUnknownOctober 20, 2017 at 5:02\u202fAMThe stores are allowed small signs.ReplyDeleteUnknownDecember 31, 2020 at 8:11\u202fAMThe pictures concentrated in the removed outdoor ads and banners, but, believe me, the city looks cleaner and tidier without all that visual pollution. In southeast and south Brazil regions, things look much nicer than many other countries. Thankfully, Rio de Janeiro is not a city that represents Brazil anymore.ReplyDelete",
    "summary": "- In 2006, the mayor of S\u00e3o Paulo passed a law that banned all outdoor advertisements in the city, including billboards, transit ads, and store signs. This resulted in the removal of 15,000 billboards and the shrinking of store signs to comply with the new law. \n- Despite concerns that the ban would lead to a loss of revenue and jobs, the city's economy did not suffer, and a survey conducted among residents found that 70% believed the ban had a positive impact. The removal of advertisements also revealed previously overlooked architectural beauty in the city. \n- S\u00e3o Paulo is not the only city to ban outdoor advertisements, as other places in the US and Europe have imposed similar restrictions. This move sparked interest because it challenged the traditional presence of advertisements in urban environments and highlighted the potential benefits of a more visually clean cityscape.",
    "hn_title": "Sao Paulo: A city with no outdoor advertisements (2013)",
    "original_title": "Sao Paulo: A city with no outdoor advertisements (2013)",
    "score": 618,
    "hn_content": "- S\u00e3o Paulo has a law that bans outdoor advertisements and billboards. \n- Instead of traditional outdoor advertisements, some stores cheat by using glass fa\u00e7ades with LEDs inside.\n- Since the advertising ban, there has been an increase in full building graffiti, which is not banned. \n- In Brazil, graffiti is considered street art and is referred to as \"grafite.\"\n- This post sparked a conversation about the distinction between murals and graffiti, as well as the different meanings of graffiti in various countries.\n- Some people appreciate the street art and murals in S\u00e3o Paulo, while others find it ugly and prefer no advertisements at all. \n- Other cities, such as Berlin and certain parts of the US, also have restrictions on outdoor advertising.- Introduction of the Clean City Law led to the removal of billboards and advertising structures in S\u00e3o Paulo\n- Some people argue that the absence of advertising makes the city look abandoned and suggest alternative ways to add color and character\n- Others view the law as revealing the true nature of cities as concrete landscapes and prefer a more natural aesthetic\n- Advertising can be intrusive and disruptive, as seen in the experience of closing video ad overlays and flashing ads while reading the article\n- The law has had mixed effects, with some appreciating the clean architecture and absence of advertising, while others believe it has destroyed small businesses and led to the construction of lifeless buildings\n- Similar laws regulating signage and color saturation exist in other cities like Kyoto, Japan\n- Some readers have implemented desaturation filters or browse on e-ink displays to avoid high-saturation colors online",
    "hn_summary": "- S\u00e3o Paulo has a law that bans outdoor advertisements and billboards, leading to the increase in street art and murals.\n- There is a conversation about the distinction between murals and graffiti and the differing opinions on street art in S\u00e3o Paulo.\n- Some appreciate the clean architecture and absence of advertising, while others believe it has destroyed small businesses and led to lifeless buildings."
  },
  {
    "id": 36583906,
    "timestamp": 1688460076,
    "title": "Companies must stop using Google Analytics",
    "url": "https://www.imy.se/en/news/companies-must-stop-using-google-analytics/",
    "hn_url": "http://news.ycombinator.com/item?id=36583906",
    "content": "IndividualsOrganisationsContact usListenP\u00e5 svenskaSearchStartNews in EnglishFour companies must stop using Google AnalyticsSvensk versionFour companies must stop using Google AnalyticsPublished: 3 July 2023The Swedish Authority for Privacy Protection (IMY) has audited how four companies use Google Analytics for web statistics. IMY issues administrative fines against two of the companies. One of the companies has recently stopped using the statistics tool on its own initiative, while IMY orders the other three to also stop using it.IMY has audited how four companies transfer personal data to the US via Google Analytics, which is a tool for measuring and analysing traffic on websites. The companies audited are CDON, Coop, Dagens Industri and Tele2. The audits concerns a version of Google Analytics from 14th of August 2020.The audits are based on complaints from the organisation None of Your Business (NOYB) in the light of the Schrems II ruling by the European Court of Justice (CJEU). The complaints allege that the companies, in violation of the law, transfer personal data to the United States.According to the data protection regulation, GDPR, personal data may be transferred to third countries, i.e. countries outside the EU/EEA, if the European Commission has decided that the country in question has an adequate level of protection for personal data that corresponds to that within the EU/EEA. However, the CJEU ruled through the Schrems II ruling that the United States could not be considered to have such an adequate level of protection at the time of the ruling.In its audits, IMY considers that the data transferred to the US via Google's statistics tool is personal data because the data can be linked with other unique data that is transferred. The authority also concludes that the technical security measures that the companies have taken are not sufficient to ensure a level of protection that essentially corresponds to that guaranteed within the EU/EEA.\u2013 By the fact that IMY has decided on these cases at the same time, it is made clear what requirements are placed on technical security measures and other measures when transferring personal data to a third country, in this case the United States, says legal advisor Sandra Arvidsson, who led the audits of the companies.If there is no decision on an adequate level of protection by the European Commission, data may be transferred based on standard contractual clauses that the European Commission has decided on. However, according to the CJEU, such standard contractual clauses may need to be supplemented with additional safeguards if it is necessary for the protection that the clauses are intended to provide to be maintained in practice.All four companies have based their decisions on the transfer of personal data via Google Analytics on standard contractual clauses. From IMY's audits, it appears that none of the companies' additional technical security measures are sufficient. IMY issues an administrative fine of 12 million SEK against Tele2 and 300,000 SEK against CDON, which has not taken the same extensive protective measures as Coop and Dagens Industri. Tele2 has recently stopped using the statistics tool on its own initiative. IMY orders the other three companies to stop using the tool.\u2013 These decisions have implications not only for these four companies, but can also provide guidance for other organisations that use Google Analytics, says Sandra Arvidsson.The decisionsRelated linksRead the decision against CDON in Swedish (pdf, 433 kB)Read the decision against Coop in Swedish (pdf, 423 kB)Read the decision against Dagens Industri in Swedish (pdf, 746 kB)Read the decision against Tele2 in Swedish (pdf, 399 kB)For more information contactLegal advisor Sandra Arvidsson, telephone 08-515 154 14Press service, telephone 08-515 15 415Latest update: 4 July 2023PrintPage labelsData protectionMore news on this topicAdministrative fee against Spotify13 June 2023Data protection officers point to problems applying GDPR31 January 2023Administrative fine against Klarna after investigation31 March 2022Administrative fine to the Swedish Customs for deficient routines16 March 2022See more newsAbout IMYOm ossV\u00e5rt uppdragContact usPhone:08-657 61 00E-mail: imy@imy.seHow to contact usAbout our siteCookiesHow we process personal dataAbout this websiteFollow usLinkedInTwitterRSS",
    "summary": "- The Swedish Authority for Privacy Protection (IMY) has audited four companies' use of Google Analytics for web statistics and has issued fines against two of them.\n- The audits were conducted based on complaints from the organization None of Your Business (NOYB) regarding the transfer of personal data to the US in violation of the law.\n- The audits found that the companies' technical security measures were not sufficient to ensure a level of protection equivalent to that within the EU/EEA, leading to fines and orders to stop using Google Analytics.",
    "hn_title": "Companies must stop using Google Analytics",
    "original_title": "Companies must stop using Google Analytics",
    "score": 605,
    "hn_content": "- People are overly dependent on specific tools like Google Analytics, and if those tools are banned, they can become economically useless.\n- Training exclusively on specific tools instead of learning first principles can be a foolish decision.\n- Data analytics is a general area of expertise that is not bound to Google Analytics. There are other tools and skills involved.\n- Dependence on specific tools is not unique to the tech industry. It happens in other fields as well, such as using Microsoft products in business settings.\n- There is a divide between IT workers and \"run-of-the-mill business drones,\" with both sides having negative opinions about the other's work.\n- Many meetings in corporate environments are seen as pointless and time-wasting.\n- Good communication and relationship building are important skills for IT workers, as well as understanding the value of other roles in the organization.\n- The intelligence required for a job is not necessarily an indication of a person's overall intelligence.\n- Many tasks in corporate environments can be automated or made more efficient with better communication and understanding between IT workers and other employees.\n- The intelligence of workers in non-tech roles is often underestimated.\n- Specialization in specific tools can lead to being left behind as technology evolves, but it can also be beneficial in the short term for certain job opportunities.\n- Retraining from one tool to another does not necessarily take 6-12 months. Depending on the similarities between the tools, the transition can be much faster.\n- Learning first principles and primitives is important, but specialization in specific tools is often necessary for job opportunities.\n- People who only know one specific tool can still have value and expertise in their work, but it is important to continuously learn and adapt to new tools and technologies.\n- Companies should not be solely reliant on a single tool or platform for their operations, as they can become vulnerable if the tool is banned or discontinued.\n- It is important to have a diversified toolkit and not rely on a single tool or platform for success.- The use of popular analytics tools like Google Analytics is in violation of the European GDPR.\n- People are now questioning the usefulness and reliability of these analytics platforms.\n- Self-hosted analytics solutions like Matomo are seen as more privacy-friendly alternatives.\n- The debate centers around the importance of data privacy and the trade-offs between convenience and security.\n- Many users are advocating for more control over their own data and the ability to roll their own analytics solutions.\n- The role of analytics in business decision making is being reevaluated, with a focus on the specific data that is needed for actionable insights.\n- The post highlights the need for individuals and businesses to consider the implications of using third-party analytics tools and explore alternatives.\n- The rise of GDPR and privacy regulations has sparked a discussion about data protection and the ethics of data collection and usage.\n- The complexity and cost of using analytics platforms are seen as barriers to entry for smaller businesses and developers.\n- The existing platforms are criticized for being bloated, unreliable, and providing excessive data that isn't necessarily useful.\n- The potential for self-hosted solutions to provide more control, privacy, and cost-effectiveness is an area of interest for many readers.- The discussion revolves around the topic of accessing and analyzing web server logs and the preference for using Google Analytics.\n- Some users suggest that writing a small script to parse logs is a viable solution if one has access to the logs.\n- The idea of not having access to logs is questioned by other users who argue for the importance of structured logging and log aggregators like Splunk or DataDog.\n- One user explains their decision not to convert to Google Analytics 4, instead opting for a websocket to measure active users, page views, and user counts.\n- The conversation touches on topics such as honoring regulations like HIPAA and GDPR and the use of log analysis for trend identification and analysis.\n- Overall, the discussion highlights different approaches to tracking and measuring website traffic and the benefits of using web server logs versus analytics tools like Google Analytics.",
    "hn_summary": "- Dependence on specific tools like Google Analytics can lead to economic uselessness if those tools are banned or discontinued.\n- The importance of learning first principles in data analytics and not relying solely on specific tools.\n- The need for companies to diversify their analytics toolkit and consider alternatives to popular platforms like Google Analytics."
  },
  {
    "id": 36586346,
    "timestamp": 1688477261,
    "title": "More than 75% of Steam games tested are playable or verified on the Steam Deck",
    "url": "https://mastodon.cloud/@boilingsteam/110655979942850128",
    "hn_url": "http://news.ycombinator.com/item?id=36586346",
    "content": "mastodon.cloud is part of the decentralized social network powered by Mastodon.\u200c\u200c\u200cADMINISTERED BY:\u200c\u200c\u200cSERVER STATS:\u200c\u200cLearn moremastodon.cloud: About \u00b7 Profiles directory \u00b7 Privacy policyMastodon: About \u00b7 Get the app \u00b7 Keyboard shortcuts \u00b7 View source code \u00b7 v4.1.1ExploreLocalFederatedSign in to follow profiles or hashtags, favourite, share and reply to posts. You can also interact from your account on a different server.Sign inCreate account",
    "summary": "- mastodon.cloud is a part of a decentralized social network called Mastodon.\n- It allows users to create accounts, follow profiles or hashtags, favorite, share, and reply to posts.\n- Users can also interact from their accounts on different servers.",
    "hn_title": "More than 75% of Steam games tested are playable or verified on the Steam Deck",
    "original_title": "More than 75% of Steam games tested are playable or verified on the Steam Deck",
    "score": 602,
    "hn_content": "- Steam Deck is receiving positive reviews from users who find it surprisingly comfortable and enjoyable to use.\n- More than 75% of tested Steam games are playable or verified on the Steam Deck.\n- Users are pleased with the quality of the device, finding it more comfortable than expected.\n- Steam sales are a topic of discussion, with users sharing their experiences and opinions on pricing and discounts.\n- Indie games are highly recommended, with users sharing their favorites and ways to discover new ones.\n- The Steam Deck offers a seamless gaming experience, allowing players to pause and resume games easily and quickly.\n- The ability to play games on the go and switch between devices seamlessly is a major benefit for many users.\n- Some users have found limitations, such as lower graphics settings or shorter battery life, but overall, the Steam Deck is well-received.\n- The Steam Deck is seen as a culmination of Valve's previous hardware attempts and a successful execution of a gaming handheld.\n- The device has generated excitement and interest among tech-savvy individuals, particularly due to its unique features and potential for gaming on the go.- The Steam Deck is a portable gaming device that can play most games from the Steam library.\n- It has a similar form factor to the Nintendo Switch and offers the convenience of playing games on the go.\n- The device runs on SteamOS, which is based on Linux, but is compatible with Windows games thanks to the Proton compatibility layer.\n- Users are impressed with the sleep/resume function, which allows for quick and easy gaming sessions.\n- The hardware is praised for its performance, but some users have noted issues with heat management and fan noise.\n- Emulation is also supported on the Steam Deck, making it an attractive option for retro gaming enthusiasts.\n- The device offers a high level of customization, with the ability to map controls for each game and install non-Steam games.\n- Some users have reported discomfort during extended gaming sessions due to the size and weight of the device.\n- Overall, the Steam Deck is seen as a major innovation in portable gaming and has rekindled the passion for gaming for many users.",
    "hn_summary": "- More than 75% of tested Steam games are playable or verified on the Steam Deck, making it compatible with a wide range of games.\n- Users are highly satisfied with the device's quality, comfort, and seamless gaming experience, especially for on-the-go gaming and switching between devices.\n- The Steam Deck has generated excitement and interest among tech-savvy individuals as a culmination of Valve's previous hardware attempts and a successful execution of a gaming handheld."
  },
  {
    "id": 36591032,
    "timestamp": 1688496475,
    "title": "How to build a website without frameworks & tons of libraries",
    "url": "https://www.kodingkitty.com/blog/how-to-build-a-website/",
    "hn_url": "http://news.ycombinator.com/item?id=36591032",
    "content": "LightDarkHow to build a website without frameworks and tons of librariesAn innocent question was posted on IndieHackers the other day: \"I'm asking what did you use to build your website\".The question also had a second part. So just to be fair, here it is in its entirety: \"I'm asking what did you use to build your website, I really love the design and feeling of kodingkitty.\"But this post is not about people loving our website. Although we're obviously happy about that.Too lazy to read? Check out Koding Kitty's simple toolchain.The shocking truthThe question sparked an idea to write about the tech stack that KodingKitty team uses for its own web. And we warn you beforehand: it's so simple that some of you might find it shocking.You have been warned.Hype is just a coincidenceOur tech stack has nothing to do with the current hype about lightweight or simplified web development. Not everyone rides that wave, and that's fine.On the other hand, we believe that in many cases, \"back to the roots\" or \"simplicity is the ultimate perfection\" is a better solution for customers than convoluted web of overused frameworks and libraries.To put it another way, the solution we designed for our website and the current hype is just a coincidence. Our stack is simple because we wanted something pragmatic with minimal friction for the developers.And even though we wanted Koding Kitty website to have a distinctive yet subtle design, it is important to mention that it could have been achieved in many other ways.Comparing optionsOur website and the way it is built comes from our past experience.We had the chance to work with different web frameworks in the past. We know this is a luxury not everyone has. It has allowed us to compare different technologies and decide what works best for us.From the beginning we agreed to start with a static website. After all, what does a kitty's web really need? A front page, a showcase, a blog ... and probably not much more. At least in the beginning.Our requirementsWe set the following set of requirements:Fast websiteFast to developInexpensive hostingMinimal complexityWhat are the options today to have a website with the above requirements?Wordpress (and similar CMS)Nothing against WordPress, it's done a tremendous job for the internet but ...... honestly, it's a bit overkill for our modest needs.We don't need to store our content in a database. We don't need to deal with plugins. We don't need a visual editor to write our content.No-CodeYes, that's one area where there's a lot of the hype these days.But to be a web coder and use a no-code tool? Seriously? It sounds weird.We want to own our content. We want to do whatever we want with our web. We can code a website faster than we can build it in the no-code WYSIWYG editor.The last point is that such tool still requires initial (and then ongoing) learning.FrameworksWe have an experience with frameworks.But, did we want to do the whole setup? Although it can be done quickly, we would end up with too many parts. Like a database, configuration, libraries, admin, and so on. Yes, it is a fully customizable solution, but we would end up with something that resembles WordPress.And that's not what we wanted.Jamstack (aka Site generators)Static site generators look like a tempting option for our needs, don't they?Just pick a programming language, do some coding, press a button (or run a script) and voila ...... your fast static site is generated.Unfortunately, Jamstack tools require an initial setup and an initial (and continuous) learning.We understand, that writing a blog in markdown is convenient, but taking all things into consideration we decided for simpler solution.KittyStackOur solution is obvious, when you review our requirements (repeated below for your convenience):Fast websiteFast to developInexpensive hostingMinimal complexityLet's discuss them one by one:Fast websiteIs a static website fast?Yes, it is!Fast developmentNow, you may be thinking: \"If you're not writing your content in markdown, you're not using a fancy editor, then what are you using?\"We write our content in HTML!Take a look at our website. For the kind of posts we publish there, HTML is ideal. We mix the text with HTML/CSS/JS widgets, so it makes sense to have a post written directly in HTML.Inexpensive hostingIs hosting a static site inexpensive?It depends on the provider and all the bells and whistles you decide you need.We decided to go with the absolute minimum, because hosting static sites doesn't require a massive hardware setup.Mind you, Hetzner webhosting starts at 2 EUR! How crazy is that?And if you are worried that your site will not survive a massive spike in traffic - like when your post hits the front page of Hacker News - it probably will. At least our site did.Minimal complexityWe insisted on minimal complexity, but some of our pages and posts have repeating patterns.While it is possible to use a copy+paste routine, it can quickly get out of control. Using some form of loops would help a lot. Also, the ability to include some blocks of code - like header, footer, and the alike - would be beneficial.Templating to the rescueAnd so we come to the last piece of the puzzle.Meet Jinja templates, which make our life much easier. Jinja allows us to use loops, include files (navigation bar, footer, etc.) and much more.Plus, we didn't have to learn a new system as we were already using frameworks based on Jinja before.A simple toolchainNow you might be wondering how we generate the final, static page.We use a short Python script with exactly 45 lines of code. Including comments and blank lines.In summary, this is the simple toolchain for building our web:Developer updates index.src.htmlWatchdog.py detects the file change and ...... renders Jinja template into index.html and ...... calls Tailwind CSS CLI to generate styles.min.css.index.src.htmlwatchdog.pyrenders Jinja templatecalls Tailwind CLIindex.htmlstyles.min.cssIt's also important to mention following:During development, Live Server serves files and reloads them as they change.When it's time to publish, the files are uploaded manually via FTP.ConclusionWeb development can be kept simple without sacrificing too much.In fact, it can be liberating to limit yourself to just a few options. And based on what we've seen so far, many customers would benefit from having solutions that are faster, simpler and cheaper.But don't just take our word for it. Take a look around the web and ask yourself: could the same result be achieved using a less complex tech stack?Turn design into HTML and CSSBloghello@kodingkitty.com\u00a9 2023 Koding KittyAll Rights Reserved",
    "summary": "- The Koding Kitty team uses a simple tech stack to build their website, which is fast, inexpensive, and minimally complex.\n- They considered options like WordPress, no-code tools, and frameworks, but ultimately chose a static website approach using Jinja templates.\n- The team uses a short Python script and Jinja templates to generate their final static pages. They also use Tailwind CSS for styling.",
    "hn_title": "How to build a website without frameworks and tons of libraries",
    "original_title": "How to build a website without frameworks and tons of libraries",
    "score": 423,
    "hn_content": "- A developer shares their approach to building websites without frameworks and libraries.\n- They write everything as straight HTML without templates and upload it to a server.\n- They use regular expressions to handle shared UI elements like navigation.\n- They generate image variants using their RAW editor instead of using libraries.\n- They compare their approach to putting together a photo book where more effort is required upfront but the final product is stable.\n- Other users suggest using server-side includes and different frameworks for easier implementation.\n- Users discuss the pros and cons of using Vapor and other frameworks.\n- Readers express interest in building their own websites without frameworks and ask for advice and resources.\n- Some comments discuss the best tools and approaches for static site generation.\n- Users share their experiences with different tools and libraries for building websites.\n- There is debate about the use of dark mode in websites and the implementation challenges associated with it.\n- Readers suggest other tools and resources for building simple websites.\n- Some users talk about the benefits of using platforms like GitHub and GitLab for managing and deploying static sites.\n- Developers discuss the balance between simplicity and complexity in web development, along with the appropriateness of using frameworks and libraries.- Some users express their dissatisfaction with modern web development tools, such as Dreamweaver and Adobe Creative Cloud, due to issues like phoning home, reliance on the cloud, and incompatibility with older versions.\n- The use of a locked down environment (container or VM) is suggested as a potential workaround to running Adobe products without network access.\n- The post describes a simple toolchain for building a website using a short Python script (45 lines of code), Jinja templates for rendering, and Tailwind CSS CLI for generating styles.\n- Some readers criticize the post for not providing step-by-step code or a repository, questioning the simplicity and widespread use of the approach.\n- Jinja templates are explained as a way to simplify web development by allowing for loops and including files like navigation bars and footers.\n- Some users argue that building without frameworks allows for a better understanding of the platform and problem-solving abilities, while others believe frameworks offer valuable tools and abstractions.\n- The complexity and bloat of modern web development tools are criticized, with suggestions to focus on native browser features, like Web Components, and avoid unnecessary abstractions.\n- Various static site generators and methods, such as Next.js, Vercel, pandoc, and plain HTML/CSS/JS, are mentioned as alternatives for building websites.\n- Some users express nostalgia for the simpler days of HTML-based development and criticize the overuse of frameworks and libraries.\n- GitHub Pages is suggested as a simple and straightforward hosting option for static websites.",
    "hn_summary": "- The article discusses a developer's approach to building websites without frameworks and libraries, using straight HTML and regular expressions.\n- Other users suggest using server-side includes and different frameworks for easier implementation.\n- Readers express interest in building their own websites without frameworks and ask for advice and resources."
  },
  {
    "id": 36584656,
    "timestamp": 1688466062,
    "title": "Sourcegraph is no longer open source",
    "url": "https://github.com/sourcegraph/sourcegraph/blob/main/CHANGELOG.md",
    "hn_url": "http://news.ycombinator.com/item?id=36584656",
    "content": "- Added support for LSIF upload authentication against GitLab.com on Sourcegraph Cloud\n- Introduced a redesigned repository page\n- Improved search-based code navigation for various programming languages\n- Added support for management of batch changes on Bitbucket Cloud\n- Improved performance and stability of search queries and code insights\n- Updated versions of Git, Docker images, and other dependencies\n- Deprecated and removed certain features and settings\n- Fixed various bugs and issues in Code Insights, batch changes,- The new version of Sourcegraph (3.27.0) includes several new features and improvements.\n- Password reset link expiration can now be customized via the site config.\n- Panes in the Sourcegraph monitoring dashboards now include links to relevant- The new version of Sourcegraph (3.21.x) includes various improvements and bug fixes\n- Some important updates include a new GraphQL API query field to make it easier to look up the user or organization with a given name\n- Changesets created by campaigns now include a link back to the campaign in their body text\n- Users can now preview commits that are going to be created in their repositories in the campaign preview UI\n- Users will be sent an email when important account information is changed (e.g., password, email, access tokens)\n- A subset of changesets can now be published by setting the published flag in campaign specs to an array, allowing specific changesets within a campaign to be published based on the repository name\n- Fixed issues with sourcegraph/server Docker deployments that caused server closed idle connection errors and prevented syntax highlighting from working correctly.\n- Improved performance of the /site-admin/repositories page to prevent timeouts.\n- Fixed issues where Gitolite repositories would become inaccessible to non-admin users after upgrading to a newer version.\n- Repository names are now treated as case-sensitive to prevent duplicate key errors.\n- Repositories containing submodules not on Sourcegraph will now load without error.\n- HTTP metrics in Prometheus/Grafana now distinguish between different types of GraphQL requests.\n- Improved wording in the site-admin onboarding process.\n- Added the ability to disable updates to critical configuration on the management console.\n- Improved performance of the /site-admin/repositories page significantly.\n- Fixed issues with repository queries and out-of-bounds errors in GraphQL queries.\n- Added support for specifying custom paths for gitolite.host and EXTSVC_CONFIG_FILE in external service configuration.\n- Added support for overriding critical, site, and external service configurations via files.\n- Added support for excluding specific repositories in Gitolite and AWS CodeCommit in external service configuration.\n- Added support for specifying Git credentials for AWS CodeCommit repositories in external service configuration.\n- Added support for specifying custom paths for CRITICAL_CONFIG_FILE, SITE_CONFIG_FILE, and EXTSVC_CONFIG_FILE on the front-end container.\n- Fixed issues with site-admin onboarding and user/org/global settings from older versions.\n- Improved syntax highlighting error handling and fallback behavior.\n- Fixed issues with file search suggestions and search scopes.\n- Added support for specifying repositoryPathPattern for custom repository names.\n- Added support for disabling patch version release update alerts on the site.\n- Deprecated fields related to repository enablement and global saved searches.\n- Added various performance improvements and bug fixes to code intelligence and search functionality.",
    "summary": "- The new version of Sourcegraph (3.27.0) includes several new features and improvements, such as LSIF upload authentication, a redesigned repository page, and improved code navigation for different programming languages.\n- There are also bug fixes and updates to dependencies, as well as the option to customize password reset link expiration.\n- Sourcegraph version 3.21.x includes various improvements and bug fixes, including a new GraphQL API query field, campaign enhancements, and improved performance and stability.",
    "hn_title": "Sourcegraph is no longer open source",
    "original_title": "Sourcegraph is no longer open source",
    "score": 415,
    "hn_content": "- Sourcegraph, a code search and navigation tool, has announced that it is no longer open source.\n- The company has decided to separate its code search and Cody (code AI) products into two separate repositories.\n- Cody will remain open source, but the code search variant will be closed source.\n- Sourcegraph claims that very few individual developers or companies were using the limited-feature open-source code search variant.\n- The change will not affect individual developers who can still use Sourcegraph code search for free on public code and on their self-hosted free tier for private code.\n- Critics argue that the company did not effectively communicate the licensing change and that the OSS build was difficult to use and frequently broken.\n- Other alternatives for code search include livegrep and OpenGrok.- Sourcegraph, a code search and navigation tool, recently changed their licensing model for future versions, causing a stir in the open-source community.\n- Some users have expressed disappointment, citing concerns about the sustainability of open-source projects.\n- Sourcegraph's decision to release their code under a restrictive license has led to discussions about the benefits and drawbacks of open-source software.\n- The company has also recently released Cody, an AI-powered code assistant, as a competitor to GitHub Copilot.\n- Code search tools like Sourcegraph offer powerful features for searching and navigating code repositories, making development more efficient.\n- Other alternatives to Sourcegraph, such as Livegrep and Opengrok, have also been recommended by users.\n- Open-source projects face challenges in funding and sustainability, especially for smaller startups that may not have the resources of larger companies like Microsoft and Google.\n- The debate over open-source versus source-available licenses continues, with proponents arguing for a balance between free use and the need for revenue to support development.",
    "hn_summary": "- Sourcegraph, a code search and navigation tool, has decided to separate its code search and Cody (code AI) products into two separate repositories, with the code search variant becoming closed source.\n- Critics argue that the company did not effectively communicate the licensing change and that the open-source code search build was difficult to use and frequently broken.\n- This decision has sparked discussions about the sustainability and funding challenges faced by open-source projects, and the balance between free use and the need for revenue to support development."
  },
  {
    "id": 36580837,
    "timestamp": 1688432545,
    "title": "Learning needs to be effortful to be effective",
    "url": "https://giansegato.com/essays/edutainment-is-not-learning",
    "hn_url": "http://news.ycombinator.com/item?id=36580837",
    "content": "How to Learn Better in the Digital Age$ when \u2022 November 2020Before I got into productivity and performance, I used to spend many hours online ingesting vast troves of digital content. My information diet ranged from inspirational TED talks to specialized podcasts, from blog posts found on Hacker News to ebooks shared on Twitter.I\u2019m deeply curious, and I gave in to new content as much as I could. What could be the harm?\u2014I thought. I loved spending my time this way. It felt useful, it was fun, and it nurtured my self-image as a \u201csmart guy\u201d \u2014 all at the same time. Truly, a learning hack.Turns out I wasn't hacking anything: The learning wasn\u2019t real.A few months ago, doubts began to creep into my mind about the effectiveness of my habits.While I\u2019d amped up my information consumption, I wasn\u2019t retaining most of it. My memory was behaving like a leaky bucket. Sure, I was spending tens of hours listening to politics on the radio. But when I tried to use any of those points in a conversation, I found that I didn't actually know enough to make a coherent argument. I knew the surrounding context, but the moment I needed to get specific my argument would crumble. Same for many other topics: the more technical they were, the less retention I had.Where did all that information go?The problem lied in how I was seeing learning, and therefore how I was approaching it.Learning is what turns information consumption into long-lasting knowledge. The two things are different: while information is ephemeral, true knowledge is foundational. If knowledge were a person, information would be its picture.It\u2019s easy to think of learning in accretive, cumulative terms: if I stack up enough information, it will eventually turn into knowledge. We tend to judge the world in material terms, and if data were tangible, an indefinitely growing memory might be reasonable to assume. The more information I consume, the more information I store, the information data I can later retrieve. The more business newsletters I\u2019ll read, the more I\u2019ll know business.However, this line of thinking wasn\u2019t really applicable to my case: I was undoubtedly consuming many business newsletters each week, but that wasn\u2019t translating into long-term business knowledge.I spent the last eight months trying to find an answer to this riddle. It took me deep into the topic of meta-learning: How do humans learn? And how can we learn better in the digital information age?Learning must be effortfulUnfortunately for us, human memory does not resemble storage, and \u201cpassive accumulation\u201d isn\u2019t how learning happens.The truth is that we retain information only when we put serious effort into the process of learning. The intrinsic effortfulness of learning is not just a byproduct of the core activity, like shortness of breath during running. On the contrary: it\u2019s what actually enables it. The relationship is causal.I didn\u2019t find a learning hack to avoid effort because there\u2019s no such thing as easy learning: learning must be effortful in order for it to happen.What surprised me the most is that learning is far more grounded in the physical world than I was comfortable admitting.The most literal meaning of effort is physical effort (think of weight lifting at the gym). The same holds true with information retention: it works best when the process of assimilating it is physically effortful. Our memory shines when our learning is physical, visceral, and obvious, like the aching in your hands after a morning spent hand-writing.Since they\u2019re passive, easy, and exclusively digital, after this realization all my podcasts, e-books, audiobooks, newsletters, blog posts, videos, live webinars were suddenly deprived of their \u201clearning status\u201d. Instead, they assumed their proper place in my schedule as pure entertainment activities.The fact of the matter is that digital products make it uniquely easy to trick yourself into thinking that you\u2019re learning when you are actually being entertained.What I still didn\u2019t know was why our mind works like this. Is this just the current state of digital learning and teaching, or there\u2019s actually a margin for easy learning to be found somewhere?The neurology of learningI\u2019m no expert in medicine, let alone neurology, but I did want to roughly understand what happens when we \u2014 as humans \u2014 create knowledge. Luckily I didn\u2019t need profound medical expertise to get the gist of the matter.Our brain is made of a web of interconnected neurons. The links between these neurons are called axons: long, slender projections of nerve fibers that transmit electrical impulses.Around these axons, there\u2019s an insulating membrane called myelin. It covers many neuronal axons and facilitates the propagation of electrical signals along neuronal circuits. The more myelin around an axon, the stronger and more connected the signal transmission will be.Myelin is to neural transmissions as oxygen is to fire. It allows rapid information transfer over long distances, and it greatly increases the speed of propagation of electric signals in our brain.See it as water flowing through a pipe with dynamic, changing capacity. Pipes with greater capacity can move more water, more quickly than a small pipe or a slow drip. The more myelin supporting a neural connection, the easier it is to use that connection \u2014 and thus to use the skill or remember the topic associated with that connectionA key aspect of myelin is that it\u2019s highly dynamic. It\u2019s an integral component of our brain plasticity. So the question becomes: how is myelin generated, and why?When we come across a new topic, new regions of the brain start activating. The more we use those new regions, the more myelin is synthesized, the easier that topic (or activity) gets.We all know the old saying practice makes perfect. The more we use a certain region of our brain, the more our brain \"prioritizes\" and \"hones\" it. That is what leads to myelin: activity induces myelination, which leads to increased strength of connectivity and efficiency along those very neurons. It\u2019s a self-reinforcing process.In other words, it compounds.See now why it\u2019s so hard to learn? To learn anything we must make active use of unexplored regions of our brain before they're ready. It's, quite literally, getting out of the comfort zone. The more we use them, the more they get better. Learning is structurally hard.The truly mesmerizing thing about myelination is that it is correlated with active use of motor neurons. It looks like human cognition is fundamentally grounded in sensory-motor processes: we retain information better when we associate some physical activity to it. The general intuition is that movement provides additional cues we can use to retrieve knowledge.We can see this effect happening when we take notes. A larger corpus of research is suggesting that taking notes physically \u2014 that is, by hand-writing them \u2014 is far more effective than using a laptop. Keyboarding does not provide tactile feedback to the brain that the contact between pencil and paper does: this contact, this raw feedback, is the key to creating the neurocircuitry in the hand-brain complex, that evidence shows supports memory and retention.All of this means we need to radically reassess digital learning. We haven\u2019t evolved to store information by passively watching Masterclass videos: that\u2019s just not how our minds work.However, the other side of this coin is that we\u2019re living in times of unprecedented information surplus. This is an opportunity that we should learn to seize.Creative learning in a digital worldThe best way to describe my information diet before discovering that effort is instrumental to learning would be edutainment.Edutainment mixes education topics with entertainment methodologies. Even if edutainment optimizes for passive attention instead of effortful engagement (the opposite of learning), it\u2019s not just \u201cmere fun.\u201d Deleting Twitter and unsubscribing from newsletters, as suggested by Deep Work advocates like Cal Newport, can actually end up preventing learning.I see edutainment as preparation for learning: it\u2019s a powerful explorative tool that can provide ideas and motivation to learn. And yet, it\u2019s also not learning itself, in the same way as buying running shoes is not running.Within this framework, \u201cmindless\u201d browsing online can be transformed into scouting for learning opportunities. It\u2019s yet another searching problem where it\u2019s key to balance the exploration of new opportunities with the commitment to the existing ones \u2014 a topic I wrote about at length in another essay. It\u2019s about balancing the time spent \u201cscouting\u201d for interesting topics online with the offline effort needed for long-term retention and integration.Pragmatically, I solved this trade-off with a powerful tool: a learning inbox.A learning inbox is a to-do list for stuff I\u2019d like to actually learn. I picked up the idea from Andy Matuschak \u2014 legendary ed-tech expert \u2014, who used a similar concept as a tool for capturing possibly-useful references. The learning inbox is a system that forces me to be mindful about what content is learning, and what is at the end of the day just entertainment.Everything interesting I find on my way is sent to my learning inbox and from there gets triaged, be it a paper, an online essay, a blog post, a YouTube video, or a podcast. When an item ends up in there, there are three things that can happen: I either decide to actively engage with it, to file for future interest, or just trash it. Active engagement is exactly what it sounds like: I need to take effortful action to consume the content in the list, otherwise I automatically bucket it as entertainment.In other words, I need to do something with it. To create something. Write a blog post about it, use it in a new project, test it on the field, teach it at a meetup. That\u2019s why I speak at many conferences: it\u2019s a learning tool.In GTD fashion, permanence in this list is temporary. It's a release valve, not a procrastination tool.For example, I recently came across a tweet during the last Election day mentioning a video about computational democracy. I\u2019m extremely interested in the intersection between politics and data, so I sent the link to my learning inbox (a task on Things 3) \u2013 and then promptly forgot about it.A few days later, during one of my ritual learning sprints, I took out my notepad and watched the whole video while taking handwritten notes. I then reviewed and transcribed what I had jotted down to an evergreen digital note in my personal knowledge base. The whole process took twice as long as watching the video, and it\u2019s not even a done deal: I would still need some kind of experimentation, tinkering, iteration, application in different contexts, and generally something more hands-on than just note-taking to significantly solidify my knowledge on the topic. That\u2019s what learning takes.The process takes a lot of time and effort, which means it\u2019s not something I can afford to do with every piece of content I find online. Most of the time I trash the links I find, upon further review. Sometimes they end up in my learning wish list.The core idea is trying my best to not kid myself: when my engagement with a piece of content is active and effortful then it\u2019s learning, when it\u2019s passive it\u2019s entertainment. When I create I learn. When I consume I just relax.Bottom line: we need to engage with what we encounter if we wish to absorb it long term. In a smartphone-driven society, real engagement, beyond the share or like or retweet, got fundamentally difficult \u2013 or, put another way, not engaging got fundamentally easier. Passive browsing is addictive: the whole information supply chain is optimized for time spent in-app, not for retention and proactivity.Luckily, the other side of the coin is that finding new topics and new reasons to learn got dramatically easier, with such an abundance of content and stimuli.We just need to be proactive with how we engage with all of the streams of content available to us. To go out and build, write, talk, teach, explain, create \u2014 effortful actions, that lead to meaningful growth.That's for sure what I\u2019m going to do.---",
    "summary": "- Learning requires effort and active engagement in order to be effective.\n- Simply consuming digital content without actively engaging with it does not result in long-term knowledge retention.\n- Balancing exploration of new topics with the commitment to actively engage with and apply the content is crucial for effective learning in the digital age.",
    "hn_title": "Learning needs to be effortful to be effective",
    "original_title": "Learning needs to be effortful to be effective",
    "score": 350,
    "hn_content": "- Effortful learning is more effective than passive consumption when it comes to acquiring knowledge.\n- The process of actively engaging with and transforming information leads to better understanding and retention.\n- The field of education studies teaching and learning, exploring various theories and strategies for knowledge acquisition.\n- Education science has its limitations, such as a focus on constructivism and limited statistical sophistication.\n- Some programs and communities of practice, such as learning engineering, emphasize controlled experiments and data mining for effective teaching.\n- Education programs vary across schools, with some embracing neuroscience and others lacking incentives to keep up with the latest research.\n- Good learning techniques include spaced repetition, reflection, and varied practice.\n- Learning a language requires both conscious and subconscious effort, with the need for comprehension and time playing a significant role.\n- Individuals have different learning preferences and styles, some benefiting more from passive consumption while others need active engagement and repetition.\n- Taking notes, asking questions, and summarizing content can enhance understanding and retention.\n- AI chatbots can provide patient and unbiased feedback, assisting with learning and comprehension.\n- Engaging in active learning and active creation can lead to a deeper understanding of the subject matter.\n- Effortful learning is essential for long-term mastery, but it's crucial to find ways to make the learning process enjoyable and engaging.\n- Boredom can sometimes foster learning, as it creates a desire for new knowledge and understanding.\n- Effortful learning doesn't mean that all passive consumption is entertainment and all active creation is work.\n- Carefully curated and relevant information is valuable for effective learning.\n- Repetition and manipulation of knowledge can lead to better retention.\n- Effortful learning is not just about quantity but also about insights gained from the content.\n- Eye-tracking research suggests that making content harder to read can enhance memory and retention.\n- Taking the time to critically evaluate and understand the content is more important than mere consumption.\n- Curating information and engaging in active learning practices can enhance comprehension and understanding.\n- Actively participating in conversations and debating ideas helps in developing a deeper understanding of the subject matter.\n- Unlearning can be challenging, making it important to critically evaluate information before accepting it.\n- Education should focus on relevance and curration due to the vast amount of available content.- The author suggests that in order to learn effectively in the digital age, one should engage in effortful learning rather than passive consumption.\n- The author points out that the physical act of writing can help burn knowledge into the brain and lead to better retention.\n- The commenters discuss different learning techniques and strategies, such as creating mental scaffolding, deliberate practice, and implementing what has been learned.\n- One commenter mentions the importance of finding the subject matter fun or meaningful in order to learn effectively.\n- Another commenter disagrees and suggests that formal education can provide valuable knowledge and skills if one engages actively with the material.\n- The connection between physical movement and learning is mentioned, with pen and paper being suggested as a tactile learning method.\n- The commenters discuss the impact of technology on learning, including the use of educational apps and the potential downsides of relying too heavily on automated tools.\n- Some commenters express their frustration with ineffective learning methods and the feeling of wasting time or not retaining information.\n- The importance of effort and practice in learning is emphasized, with examples given of how certain activities and exercises can enhance learning.\n- The need for a balance between active engagement and relaxation is mentioned, as well as the idea that learning should be enjoyable rather than a burden.\n- One commenter shares their experience of learning more through entertainment than traditional educational methods, suggesting that finding engaging ways to present information can enhance learning.\n- The commenters highlight the importance of finding meaning and enjoyment in the learning process, as well as the role of curiosity in driving deeper learning.",
    "hn_summary": "- Effortful learning is essential for effective acquisition and retention of knowledge.\n- Different learning techniques and strategies are discussed, such as spaced repetition, reflection, and active engagement.\n- Engagement in active learning practices, critical evaluation, and meaningful content are crucial for deep understanding and comprehension."
  },
  {
    "id": 36588514,
    "timestamp": 1688486804,
    "title": "Turning my hobby into a business made me hate it",
    "url": "https://shant.nu/turning-my-passion-hobby-into-a-business-made-me-hate-it/",
    "hn_url": "http://news.ycombinator.com/item?id=36588514",
    "content": "Skip to contentShantnu's Silent SiteHomeAboutTurning my Passion/Hobby into a Business Made Me Hate ItThis is a problem many people who love doing creative stuff (writing, music, programming, painting, pottery, calligraphy, anything creative and original) will face:\u201cThis is good, why don\u2019t you make money from it?\u201dAs if the only value anything has is by how much dollars it makes.You could create the most beautiful painting, the most moving music, the most engaging novel, but if you don\u2019t have the big dolla\u2019s in your bank account, you are a failure.And all of it is complicated by the fact that, yes, like most creatives, I do want to make money from my writing. Maybe not millions, but a little would help. Especially when the day job is going shit, and you know the fantasy that online entrepreneurs sell:Quit the day job! Start your own business! Don\u2019t depend on one person for all your income! Blah blah blah! Now buy my book/course, so at least one of us can get rich.It doesn\u2019t help with all these online \u201cgurus\u201d who claim you can make the big $$$ by selling anything (yes sir, ANYTHING!) online, if only you buy their expensive course.One guru (and why name him, since they all do the same) says you can make money online (BIG MONEY!) even if you are selling things like crotchet kits or dog training. All you have to do is (in thunderous manly voice):FOLLOW MY FORMULA!His formula was, in a nutshell: Create training courses and sell them to others, just as he was doing. And at least one of us made big money (hint, it wasn\u2019t me).My storyI loved writing fiction\u2013 novels, short stories, interactive fiction. At first, I would just self-publish them on Amazon, no care whether anyone bought them.Then I joined a few writer groups, and they were all like \u201cYou need to market your work\u201d.And so I spent hours every week building my email list, running ads, asking for reviews (as you need them to sell books), blogs/podcasts.I found the type of fiction that sells is in a few mainstream genres, and in series (so you write a dozen books, all with a hard-boiled detective, for example). That\u2019s why every book nowadays is in a series because publishers (traditional and indie)have figured out this is the \u201cformula\u201d to success.I like to experiment and play\u2014one genre I love writing is comedy-horror. This is one of those super niche genres that have a few hardcore fans but isn\u2019t mainstream. Think Shaun of the Dead in movies, or one of my favourite books John Dies at the End.Other books I wrote were supernatural detective series, a fantasy comedy (a teddy bear that solves crime). Now, there are books like these and sell (and where do you think I got ideas from?), but they are not mainstream.If you want to make the big money, you at least have to try writing for the \u201cmarket\u201d or trying to write in a way that appeals to the mainstream, or at least a large number of people.So that\u2019s what I did. Why not? Everyone else was doing it. They were pasting screenshots of how much money they were making online in Facebook groups.And while all the above I tried worked in the short term (and yes, it DOES work), soon I began to hate what I was doing and one day, I just couldn\u2019t type anymore.And one day, I couldn\u2019t take it anymore. Couldn\u2019t type one more word. Quit 2 years ago, after 7-8 years of writing.Have tried to restart many times, but each time, I feel disgusted and quit.Trying to make money from your \u201cpassion\u201d is hardJust follow your passion and you won\u2019t have to work a day!Sure, if your passion is online marketing or making WordPress sites for cash-rich, time-poor business owners.When I was writing, I hit all the clich\u00e9s about \u201cpassion\u201d:I would be lost for hours in writingI loved doing itI did it for several years non-stop, including training to become betterAnd yet, in spite of my passionate \u201cpassion\u201d, I never made enough money to quit the day job. I followed my passion and all it gave my was wrist pain.And it is a field where people make money\u2014 but like many other creative fields (music, sports, art), it\u2019s a winner-take-all field. 5-10% of authors make 90% of the money. And that\u2019s fine, I knew the statistics before I started.But I was still hoping to make some money. To be honest, I do, but it\u2019s never been more than a fancy night out each month.I never cracked the Amazon algorithm; I never got 10,000 fans on Facebook; I never got a huge email list of people Oh gosh! Just waiting to buy my book.And in trying to hit these targets, I started hating what I was actually doing \u2014 writing.My advice is: If you love someone, do it for love, don\u2019t always try to make money from it.It all comes back to doing it for the love and accepting that perhaps this will always remain a hobby. Do it with a craftsman\u2019s mindset, always improving for the sake of improving.And don\u2019t let anyone shame you if you don\u2019t want to make money.Note: I wrote this post 2\u20133 years ago, and it had been sitting on my hard drive since then. I have given up fiction writing, though my books are still online I don\u2019t do anything to promote them, nor am I writing new ones. I wrote this post as a warning: If you love something, do it for the love and don\u2019t let the Get-Rich-Types make you lose your focus.Read more posts on Creativitysignup formSign up to recieve exclusive member content and special offersYes Please!No spam ever, and I will never share your emailShantnu's Silent SiteProudly powered by WordPress",
    "summary": "- Many people who turn their hobbies into a business end up hating it because the focus on making money takes away from the enjoyment and creativity of the activity.\n- Online \"gurus\" claim that anyone can make big money by selling courses or products online, but the reality is that only a small percentage of people actually experience financial success.\n- The author's advice is to do what you love for the love of it, rather than always trying to make money from it, and not to let others shame you for not pursuing financial gain.",
    "hn_title": "Turning my hobby into a business made me hate it",
    "original_title": "Turning my hobby into a business made me hate it",
    "score": 333,
    "hn_content": "- The author turned their hobby into a business and ended up hating it.\n- They tried to cater to the market rather than focusing on what they truly wanted to create.\n- The author believes that it's important to know if your hobby is marketable before turning it into a business.\n- Following the market and not being innovative can result in creating products that are just like what large corporations are doing.\n- Many entrepreneurs focus on what they are interested in rather than what the market wants, which can lead to building something nobody wants.\n- Finding a perfect intersection between your interests and what the market wants is uncommon but ideal.\n- The author's experience highlights the challenges and trade-offs of turning a hobby into a business.\n- It is possible to turn your hobby into a profitable venture if you are relatively competent and the hobby is marketable.\n- Prioritizing and compartmentalizing different activities can help balance work and hobbies.\n- Subsidizing your hobby business with a more stable source of income can be a smart approach.\n- The author's experience should serve as a warning to others about the potential pitfalls of monetizing a hobby.- Monetizing your passion is important, and if you can't find a way to do it, you may be doing something wrong\n- However, if you're only pursuing your passion for money and not genuinely interested in it, others are likely to see through you\n- It's important to consider factors like internet connection, language, and availability of time and money for potential users of your product\n- Pursuing a PhD in a subject you love may not always be a good idea, as it can lead to hating it afterwards\n- Turning a hobby into a job can sometimes make you lose your passion for it\n- It's essential to consider the business aspects of your hobby, like marketing, customer service, and management, which may not be enjoyable for everyone\n- There is no one-size-fits-all formula for success in art or any other field, as it depends on individual circumstances and talents\n- It's challenging to get rich quickly, and many get-rich-quick schemes are scams\n- It's important to find a balance between turning your passion into a business and preserving the enjoyment and fulfillment you derive from it\n- Running a business based on your hobby is not the same as enjoying the hobby itself, as it involves various other tasks and responsibilities\n- Programming can be both a job and a hobby, but it depends on individual preferences and how the work is approached\n- Money is a means of controlling the masses, and the unequal distribution of wealth makes it difficult for most people to achieve financial success\n- Being successful in business often requires operating on a large scale or coming up with innovative ideas\n- Libraries could be a potential technology for reducing wealth disparities and achieving a true meritocracy\n- Programming is not completely artless and can be considered as a form of art by some\n- It's important to have other hobbies and interests outside of work to avoid burning out on your passion\n- Programming is different when it becomes a job rather than a hobby, as the projects and motivations may not align with personal interests\n- Envy of others who have successfully turned their passion into a job is not productive; it's better to focus on finding an activity that you love\n- The author made a deliberate choice not to pursue programming professionally because they believed they would lose their love for it by doing it full-time\n- It's important to find a balance between pursuing your passion and considering the marketplace demand for it\n- The author's dislike for programming may be due to their inability to make money from it, but it's not necessarily a universal experience\n- The value of a talent should not be determined solely by its marketability; there are intrinsic rewards in pursuing one's passion, regardless of financial success.",
    "hn_summary": "- Turning a hobby into a business can lead to hating the hobby if the focus is solely on catering to the market instead of personal interest.\n- Finding a balance between personal interest and market demand is crucial when monetizing a hobby.\n- Prioritizing and compartmentalizing different activities, as well as subsidizing the hobby business with a stable source of income, can help maintain a healthy balance between work and hobbies."
  },
  {
    "id": 36586127,
    "timestamp": 1688475859,
    "title": "Dutch rules will soon prevent schoolchildren from having a phone in classroom",
    "url": "https://nltimes.nl/2023/07/04/dutch-rules-will-soon-prevent-schoolchildren-phone-classroom",
    "hn_url": "http://news.ycombinator.com/item?id=36586127",
    "content": "ImageTeenagers on their phones - Credit: creatista / DepositPhotos - License: DepositPhotosPOLITICS SCHOOL PHONE BAN PRIMARY SCHOOLS SECONDARY SCHOOLS DENNIS WIERSMATUESDAY, 4 JULY 2023 - 14:26SHARE THIS:Dutch rules will soon prevent schoolchildren from having a phone in the classroomChildren will soon be prevented from bringing mobile phones into Dutch classrooms. Sources close to the Cabinet confirmed that schools will have until October 1 figure out how they can arrange the restriction on their own. If that fails, national rules may be introduced to restrict the use of telephones in schools.Unnamed sources also confirmed the story to ANP, after it was first reported by AD and RTL Nieuws.Smartphones may still be used if they are needed for class, or if there is a medical need. One example of the latter is if a student with diabetes needs to measure sugar levels.Schools and teachers have been asking for rules to restrict the use of mobile phones in the classroom for some time. The debate about this issue gained momentum at the end of last year when CDA Member of Parliament Ren\u00e9 Peters advocated on behalf of a ban. The largest opposition party in the Tweede Kamer, PVV, has been outspoken in favor of a ban for an extended period, and has joined forces with the CDA, a coalition party.Initially, the plea from the CDA and PVV was warmly received in the Tweede Kamer, the lower house of Parliament, with a degree of support from two other coalition parties. These include Prime Minister Mark Rutte's VVD, and the smallest party in the coalition, ChristenUnie. However, former Education Minister Dennis Wiersma and the two parties thought it was better for the schools themselves to set limits in this regard.Gradually, the number of supporters seemed to increase. A large group of teachers appeared to support the proposal, teachers union AOb announced after commissioning a poll.Due to persistent signals from teachers that they are unable to keep smartphones out of the classroom on their own, Wiersma left the door open at the beginning of this year to examine the policy. He promised discuss the issue with schools and said he would be open to a ban if there was a great need for it.Reporting by ANP",
    "summary": "- Dutch schools will soon be implementing rules to prevent students from bringing mobile phones into classrooms.\n- Schools will have until October 1st to figure out their own restrictions, but if they fail, national rules may be introduced.\n- Smartphones may still be allowed if they are needed for class or if there is a medical requirement, such as a student with diabetes needing to measure sugar levels.",
    "hn_title": "Dutch rules will soon prevent schoolchildren from having a phone in classroom",
    "original_title": "Dutch rules will soon prevent schoolchildren from having a phone in classroom",
    "score": 324,
    "hn_content": "- Dutch rules will soon prevent schoolchildren from having a phone in the classroom.\n- The measure is based on research that shows adolescents are more susceptible to the negative effects of smartphones.\n- The main adverse effect is a \"crumbling brain\" with a short attention span.\n- Students using smartphones often score lower on tests.\n- The coalition has decided on an \"urgent advice\" to ban smartphones, tablets, and smartwatches in secondary education starting in 2024.\n- Schools have the freedom to implement the ban in different ways, such as banning them in the entire building or just in classrooms.\n- The ban is sparking debate about whether children should learn how to handle smartphones or be protected from their dangers.\n- Some argue that schools could provide specially designed devices for educational purposes.\n- The ban is part of a broader discussion about the role of smartphones in education and the potential negative consequences.\n- The post highlights the divided opinions on the topic and the ongoing debates and research surrounding it.\n- This development is of interest to readers who are new to the industry and want to understand the potential effects of smartphones on education.- Schools in the Netherlands will soon ban students from bringing mobile phones into classrooms.\n- Teachers have been asking for rules to restrict the use of mobile phones in the classroom.\n- Smartphones can still be used if they are needed for class or if there is a medical need.\n- Some parents are concerned about the addictive and distracting nature of smartphones.\n- Critics argue that the ban is unnecessary and that students should be taught responsible use instead.\n- The ban is aimed at secondary education and up, not primary schools.\n- The implementation of the ban will be up to individual schools.\n- There is currently no specific law in place, but the ban is being supported by the Dutch government.",
    "hn_summary": "- Dutch schools will soon prohibit students from having phones in classrooms, based on research showing negative effects on adolescents.\n- The ban is sparking debate on whether children should learn to handle smartphones or be protected from their dangers.\n- Schools will have the freedom to implement the ban in different ways, raising discussions about the role of smartphones in education."
  },
  {
    "id": 36585371,
    "timestamp": 1688471014,
    "title": "A curated list of uBlock origin filters",
    "url": "https://letsblock.it/filters",
    "hn_url": "http://news.ycombinator.com/item?id=36585371",
    "content": "Template listHelpAboutContributingNewsLoginSponsor the project on Open CollectiveVisit the project on GitHubContribute:Suggest a new templateTemplate sourcesSponsor us:Help us run this project by sponsoring us on OpenCollective or Github Sponsors.Filter by tag:amazonbravecustomduckduckgogithubgooglehackernewskagilinkedinlinuxfrnebulasearxsoundcloudstartpageyoutubeThis website is a collaborative repository of uBlock content filters you can customize and sync across your browsers. Learn more about it and create an account to start building your filter list.Available filter templatesCheck these new filters and customize them for your use:Amazon: filter out products by nameGitHub: interface cleanupsGoogle Search: interface cleanupsHacker News: unofficial dark modeLinkedin: filter out posts by link targetLinuxFr: interface cleanupsNebula: filter out videos by creatorSearch engines: filter out results by websiteSoundcloud: hide repostsYouTube: filter out ShortsYouTube: filter out mixes and radiosYouTube: filter out upcoming videos and streamsYouTube: filter out videos by channelYouTube: filter out videos by titleYouTube: filter out videos you already watchedYouTube: hide video recommendationsYouTube: search interface cleanupsYouTube: video playback interface cleanupsAdd custom blocking rulesRemove tracking URL parameters from links",
    "summary": "- This website is a repository of uBlock content filters that you can customize and sync across your web browsers.\n- The website provides pre-made filter templates for popular platforms like Amazon, GitHub, Google Search, Hacker News, and more, allowing users to easily filter out specific content.\n- Users can also add their own custom blocking rules and remove tracking URL parameters from links.",
    "hn_title": "A curated list of uBlock origin filters",
    "original_title": "A curated list of uBlock origin filters",
    "score": 258,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginA curated list of uBlock origin filters (letsblock.it)258 points by spacebuffer 22 hours ago | hide | past | favorite | 63 commentsxvello 19 hours ago | next [\u2013]Project maintainer here, thanks for posting it!As described in https://letsblock.it/help/about, this project is a home for rules that cannot be included in the default content blocking lists, because everyone has their own definition of low-value content.Our goal is to curate a list of templates that allow you to tune up the signal/noise ratio and avoid distractions. You can just copy-paste rules in your uBlockOrigin / AdGuard settings, but the project is best used by signing-up to create your customized rule list. This gives you automatic updates when rules are improved and updated by the community, and the ability to use that list on all your devices.replym3affan 18 hours ago | parent | next [\u2013]Awesome work. It's sad to see the internet being ridden by cancerous ads more and more.replyabwizz 16 hours ago | root | parent | next [\u2013]cancerous ads are an integral part of the internet since the dot-com boomso is ad-blockingreplyFatnino 8 hours ago | parent | prev | next [\u2013]Add a rule to block the distracting other communities links on stackoverflowreplyOldManRyan 21 hours ago | prev | next [\u2013]I use the extension BlockTube to filter out videos by channel and I think it works pretty well since I only need to right click on a video and can block the entire channel. That means you have to trust another extension.That being said, it is an understatement to say I can't tolerate the internet without uBlock Origin. I get so much value out of this project I would donate 10-20% of my salary to the project if they allowed donations.replyromseb 21 hours ago | parent | next [\u2013]For YouTube, I go the other way and instead of blacklisting channels, I use https://unhook.app to only see my subscriptions and nothing else. No Home feed suggestions, no sidebar, no end screen cards, etc.replyAlready__Taken 15 hours ago | root | parent | next [\u2013]there are some gems the YT homepage can bring you. be sure to delete the dumb meme recap you watched at 3am from the watch history and the algorithm can work for you.replylysp 8 hours ago | root | parent | next [\u2013]I've done that for a while too.Often if I want to watch a one-off video topic, I'll view it in private mode, so it doesn't get added to history. Then I don't really mess up the algorithm.replymosquitobiten 14 hours ago | root | parent | prev | next [\u2013]Oh, wow. Didn't know I can do that. Thanks, it was getting really hard to train the algo again with the things I want recommended on a certain account after accidentally searching for something of interest for another account.replyxdrosenheim 13 hours ago | root | parent | next [\u2013]Be sure to also mark videos as \"Not interested\" on your homepage, if you also did not know about that. You can even tell them not to recommend an entire channel.replymosquitobiten 12 hours ago | root | parent | next [\u2013]The not interested button doesn't really work for me, it just blocks that particular video but other videos in that category still show up no matter what.replyshortcake27 9 hours ago | root | parent | next [\u2013]The \u201cDon\u2019t recommend channel\u201d button also doesn\u2019t really work for me. It used to work, but now the channels keep coming back. YouTube has been trying to get me to watch videos from \u201cDr.\u201d Sten Ekberg. After watching one video a couple years ago I quickly realised he\u2019s full of shit. I\u2019ve clicked the \u201cDon\u2019t suggest channel\u201d button so many times, but after a short while his videos return to my feed.replyOldManRyan 20 hours ago | root | parent | prev | next [\u2013]Didn't know this extension existed, thank you. I don't actually subscribe to any Youtube channels and mainly search for what I want at the time but this may be a healthier way to use Youtube.replymistermann 19 hours ago | root | parent | prev | next [\u2013]Any idea if this is smart enough to show all new content from your subscriptions or does it only show the select choices that YouTube makes for you (another infuriating YouTube design choice).reply2Gkashmiri 19 hours ago | parent | prev | next [\u2013]This is my standard response. Internet without ubo is like having unprotected sexreplysomsak2 11 hours ago | root | parent | next [\u2013]this is such a bad analogy, unprotected sex is objectively way better just riskier. ads on the web are in no way better, and maybe marginally riskier than no adsreplyBizarreByte 17 hours ago | root | parent | prev | next [\u2013]So exciting and dangerous? I may have to give the ad infested internet a try /sreplyxpil 18 hours ago | root | parent | prev | next [\u2013]Like, with another person?reply2Gkashmiri 17 hours ago | root | parent | next [\u2013]No, like digital stimulation of your member.replyTao3300 18 hours ago | root | parent | prev | next [\u2013]while sandyreplysmall_coconut 15 hours ago | parent | prev | next [\u2013]I can't use Youtube without Blocktube. Currently at 11,000 blocked channels and still going strong - it's amazing how the same junk keeps cropping up just under different names.replymosquitobiten 14 hours ago | root | parent | next [\u2013]I wish I knew about it sooner, it took me almost a year to \"train\" the algo to recommended me only certain topics of interest. I could've saved some scrolling time at the beginning.replyyuumei 20 hours ago | prev | next [\u2013]In-case anyone needs it, the following uBlock blocks some of the changes Microsoft made to github, like the cursor and symbol viewer. github.com##.code-navigation-cursor github.com###symbols-pane:upward(1)Unfortunately searching is still broken. This greasemonkey script blocks the capture of keyboard shortcuts like / ctrl space: keycodes = [191, 17, 32]  document.addEventListener('keydown', function(e) {   if (keycodes.indexOf(e.keyCode) != -1)   {     e.cancelBubble = true;     e.stopImmediatePropagation();   }   return false; });replyck2 18 hours ago | parent | next [\u2013]I wish ublock could do basic url rewrites without having to use greasemonkey/scripts/pluginsThings like rewriting www.reddit to old.reddit without anything extra.replyGrom_PE 18 hours ago | root | parent | next [\u2013]After switching browsers, I never actually reinstalled Greasemonkey as uBlock Origin has the capability to inject custom JavaScript.Check \"I am an advanced user\", then in \"Advanced settings\" set the parameter to something like:  userResourcesLocation file://localhost/home/user/stuff/scriptlets.jsAnd in the scriptlets.js file, text between the line \"/// ScriptName.js\" and an empty line is considered a separate scriptlet, which you can add to an URL in \"My filters\" like:  news.ycombinator.com##+js(ScriptName.js)The annoying bit is that scriptlets are cached, and to update them, you have to edit the \"userResourcesLocation\" setting, for that reason I made \"scriptlets1.js\" a symlink to the original file and switch back and forth whenever I edit it.replyffpip 13 hours ago | root | parent | next [\u2013]Thanks for this!But is a file URL working for you on Windows? http:// URLs are working but file:// urls are not being parsed for me.replyGrom_PE 13 hours ago | root | parent | next [\u2013]It certainly worked for me when I used Windows. Try either of these:  file:///C:/stuff/scriptlets.js  file://localhost/C:/stuff/scriptlets.jsI must say I wouldn't want to add a remote 3rd party URL to this setting, what if it updates with bad code?replyffpip 12 hours ago | root | parent | next [\u2013]Are you using Firefox?Those URLs do not work for me when using Firefox + Windows. But the code works perfectly fine when I upload it to github gists and use the raw url.And yeah, like you said do not want to inject remote JS into every site I visit so won't be using a remote URL.replyGrom_PE 9 hours ago | root | parent | next [\u2013]No, I'm using Vivaldi. Chromium-based browsers have an option \"Allow access to file URLs\" for extensions. In Firefox this functionality is missing. But it might be possible to patch omni.ja to get it anyway:https://bugzilla.mozilla.org/show_bug.cgi?id=1266960#c43replyffpip 45 minutes ago | root | parent | next [\u2013]Yeah, figured the file access permission was the issue. Thanks for the help!replyPannoniae 20 hours ago | prev | next [\u2013]I find it sad how many websites, or even programs have atrociously low information density. In our quest to flat and minimalistic design\u2122, we have managed to lower information density so much that very simple websites require lots of scrolling and menu-opening just to get simple information.replyJTon 20 hours ago | parent | next [\u2013]I read a comment on a forum where the user had a job to audit google home voice command and search result alignment. Meaning, did the google home user get what they wanted or not. The forum user made a side comment on how a surprising number of people have horrible diction. Makes me think, maybe low information density is the optimal design for the majority of users.replyajsnigrutin 19 hours ago | root | parent | next [\u2013]Maybe it's users intentionally simplifying their speech, because google doesn't understand more complex sentances.replymartin_a 18 hours ago | root | parent | next [\u2013]I find myself doing this (and feeling stupid doing so) with my Amazon Echo Dots. They seem to understand very little, so I simplify and reduce like crazy. Nevertheless, lots of searches/commands go wrong.Good example probably: I've tried to play music from German rap artist \"Disarstar\" which is pronounced like \"disaster\". I did not get Alexa to play music from this artist from Spotify, it only searched for \"disaster\" and played music it found... Not a good experience.replyOldGuyInTheClub 17 hours ago | root | parent | prev | next [\u2013]Techmoan nailed it here: https://www.youtube.com/watch?v=NvNPMlATA9greplyxedrac 19 hours ago | root | parent | prev | next [\u2013]My wife is one of these people - an artistic ADHD type whose brain bounces around in a very non-linear way. The commands she speaks to Google leave me completely confused.replyDalewyn 19 hours ago | root | parent | prev | next [\u2013]Information density as we had it in the 90s and 2000s was far too dense, let's be real. We might have liked it, but most people essentially saw an insurmountable column of text and immediately keeled over, eyes glazed. The response to reduce information density in and of itself is reasonable.What isn't reasonable is how low information density has gotten. Yes, information was too dense before, but now we have the opposite problem: It's not dense enough. There is a fine balance in density that designers seemingly can't seem to find.replyanthk 19 hours ago | root | parent | next [\u2013]Not too dense if you were used to read newspapers and magazines.replynofunsir 17 hours ago | root | parent | next [\u2013]Or, ya know, books.replyanthk 16 hours ago | root | parent | next [\u2013]Magazines and newspapers interleaved articles and news; books were just either straight stories or short tale compilations, there were no short tales in the middle of a page or at the bottom/top/edge placed sides.reply5e92cb50239222b 20 hours ago | parent | prev | next [\u2013]Now I wonder if that's the reason we ended up with 7-inch smartphone displays.replydpifke 17 hours ago | prev | next [\u2013]I use uBlock Origin for a banlist of sites and keywords that are repeat offenders for off-topic[0] HN posts: https://pifke.org/hn.txtMy rules hide the title line but not the number of comments/points, and sometimes I'll click through stories that have been highly upvoted, to see what I'm missing (rarely anything of interest to me).[0]: I consider \"off-topic\" to be anything \"they'd cover on TV news,\" per https://news.ycombinator.com/newsguidelines.html. A lot of the blocked sites are quite literally TV news sites.replyxvello 17 hours ago | parent | next [\u2013]Heya, you can take inspiration the rules I posted in https://news.ycombinator.com/item?id=35676840 for hiding the second line and the separator. I messed up the last newline, the bottom block is the following two lines:  news.ycombinator.com##html:not([op=\"item\"]) tr.spacer + tr:not(.athing):remove()  news.ycombinator.com##html:not([op=\"item\"]) tr.spacer + tr.spacer:remove()I'll make it a letsblock.it template one day.replyanonymousab 17 hours ago | prev | next [\u2013]I wish there was a maintained filter of \"video players on news sites\".replyxvello 17 hours ago | parent | next [\u2013]Agreed! That would be a great template to have in letsblock.it's corpus! Contributions are welcome if you have a ruleset to get us kickstarted.replyrx_tx 16 hours ago | prev | next [\u2013]Another really useful rule I like is for disabling the recommendation overlay that shows up on youtube when you pause a video, which is really annoying. youtube-nocookie.com,youtube.com##.ytp-pause-overlay, .show-video-thumbnail-button, #rvidreplyHeckFeck 21 hours ago | prev | next [\u2013]> YouTube: search interface cleanupsAh, finally someone has done it. YouTube search without the 'For you' and 'from your history' guff.replyvidyesh 19 hours ago | parent | next [\u2013]I have disabled YouTube Watch history and I really enjoy seeing YouTube struggle to provide me anything meaningful in the Home feed. Most suggestions are just based on my subscriptions and I see a lot of repeated suggestions as it doesn't know what I have watched.This keeps me away from the echo chamber that it creates for most people.reply2Gkashmiri 19 hours ago | parent | prev | next [\u2013]Why do you use YouTube while signed in ?All the past 12-14 years I've used YouTube, it was always in a private window. With or without login. Earlier I had an account but when google signup was mandated, I quit and since then I just manually go and search for stuff.I have newpipe and libretube on my phone so I have some semblance of \"subscriptions and channels\" but not much.replybutz 18 hours ago | prev | next [\u2013]What sad state of internet are we in, when users have to jump through hoops just to make browsing websites a little bit more convenient.replybaal80spam 13 hours ago | parent | next [\u2013]Eh, for the vast majority of websites I'd say: \"usable at all\", not just \"a little bit more convenient.\"replysomsak2 11 hours ago | parent | prev | next [\u2013]interesting point of view. i think content creators would look at it the other way -- never has it been so easy to steal content as it is on an ad-supported internet.replyOldGuyInTheClub 18 hours ago | prev | next [\u2013]I've never gotten the hang of writing uBO rules for myself. There's a lot more to it than is in the tutorial. Learned about letsblock.it last week through a support request to the uBO project and it has been a great help. It is very good at filtering out low quality search results sites[0]. Additionally it takes the pain out of blocking sites or domains I don't want to see.[0] https://news.ycombinator.com/item?id=29794372replyTekMol 21 hours ago | prev | next [\u2013]I wonder if uBlock origin has overshot the mark.It even blocks self-hosted analytics scripts from the same domain. By default.I don't see how that is a good thing. It just makes the lives of people who run websites harder. When I visit a website, I have some sympathy towards the person who runs it.Disabling scripts which talk to self-hosted analytics software makes it hard to figure out how users use a website. Especially when the site is using a CDN. So people enable tracking on the CDN level. Which means now CloudFlare, Amazon etc store that data again. Lose-lose for everybody.replyjabbany 19 hours ago | parent | next [\u2013]> makes it hard to figure out how users use a websiteIsn't that the whole point? A user with the no tracking filters turned on in uBO is intentionally trying to opt out. I don't have much sympathy for site owners unless they also offer a first party opt-out option (which I've never seen so far even given the cookie banners). Site owners are no more entitled to track than a site user is to block even first party trackers. (Also wouldn't a site owner be able to use server logs anyways?)As for defaults, I think when it comes to the point that someone is installing uBO specifically, they're usually sophiscated enough to configure filters. Most of the people I know (even those in tech) don't use any form of adblock or tracking blocking. (I don't know how they can manage to always be vigilant and avoid all the dark patterns, but to each their own.)replynathanlied 21 hours ago | parent | prev | next [\u2013]It is an unfortunate reality of how the Internet is built.There's quite a few people like you, that are fine with self-hosted analytics, either because you believe the principles of the websites you visit, or because you've done so-called \"good\" analytics, and so disable that kind of blocking, hoping your trust won't be abused.Problem is, some of us don't believe those principles hold, and/or have seen people doing vacuum-style analytics. I've listened to conversations of otherwise well-intentioned devs who are otherwise anti-ads and anti-unnecessary data collection ask for more data to be collected in analytics because \"we might need it\". Leaves a very sour taste in my mouth. So I block it all - what I can, of course. If they find ways around it that I can't block, at least I've done my best.reply5e92cb50239222b 20 hours ago | parent | prev | next [\u2013]Same domain doesn't always mean your information won't get leaked to wherever. For example, Sentry supports sending data through a proxy hosted on the same domain used by the website. If you don't block it, your data ends up on sentry.io anyway (in most cases; some users probably self-host their own Sentry instance, but how many? It's quite painful from my personal experience.)https://docs.sentry.io/platforms/javascript/troubleshooting/...replymcpackieh 18 hours ago | parent | prev | next [\u2013]> It just makes the lives of people who run websites harder.User agent. The browser is meant to serve the user's interests. The wants or desires of people who run websites are their own problems, not the user agent's problems.I don't even run first party javascript by default, only on a whitelist basis. Most of the time, even first party javascript only makes the website worse from my perspective as a user. Javascript's most common purpose is to implement annoyances and spyware, legitimate functionality comes third.replyqingcharles 17 hours ago | parent | prev | next [\u2013]It is frustrating. At the end of the day uBlock can only block what it can see. If I'm an asshole web host I can still take your IP and every other bit of data that I can gobble up from the HTTP headers and sell those off to the highest bidder and uBlock can't do anything about that. So, nefarious actors still be nefarious actors.Disclaimer: I use uBlock.replybunga-bunga 20 hours ago | parent | prev | next [\u2013]I share this sentiment. I use content blockers to avoid annoyances and trackers, but I'm ok with healthy ads and other local stats. The problem is not being able to distinguish them at scale.replysomsak2 11 hours ago | root | parent | next [\u2013]> I'm ok with healthy adsno such thing as a healthy ad!replywahnfrieden 19 hours ago | parent | prev | next [\u2013]I think we put up with a lot because we give smaller businesses a pass on bad practices and focus our energy on the \"bad\" bigger players. But I don't think being an underdog means that society should accept PII surveillance - society is made up of underdogs.replynipperkinfeet 7 hours ago | prev [\u2013]This is my go-to filter for blocking YouTube Shorts garbage.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- \"letsblock.it\" is a project that curates a list of uBlock origin filters for custom content blocking\n- The goal is to improve the signal/noise ratio and avoid distractions on the internet\n- Users can sign up to create customized rule lists and receive automatic updates when rules are improved and updated by the community"
  },
  {
    "id": 36588240,
    "timestamp": 1688485616,
    "title": "ZFS 2.2.0 (RC): Block Cloning merged",
    "url": "https://github.com/openzfs/zfs/pull/13392",
    "hn_url": "http://news.ycombinator.com/item?id=36588240",
    "content": "Skip to contentProductSolutionsOpen SourcePricingSign inSign upopenzfs/zfsPublicNotificationsFork 1.6kStar 9.2kCodeIssues988Pull requests126DiscussionsActionsProjects6WikiSecurityInsightsNew issueBlock Cloning #13392Mergedbehlendorf merged 1 commit into openzfs:master from pjd:brt on Mar 10Mar 10, 2023on Mar 10+3,480 \u2212120Conversation 172Commits 1Checks 8Files changed 51ConversationContributorpjd commented on Apr 30, 2022Apr 30, 2022 \u2022editedMotivation and ContextBlock Cloning allows to clone a file (or a subset of its blocks) into another (or the same) file by just creating additional references to the data blocks without copying the data itself. Block Cloning can be described as a fast, manual deduplication.DescriptionIn many ways Block Cloning is similar to the existing deduplication, but there are some important differences:Deduplication is automatic and Block Cloning is not - one has to use a dedicated system call(s) to clone the given file/blocks.Deduplication keeps all data blocks in its table, even those referenced just ones. Block Cloning creates an entry in its tables only when there are at least two references to the given data block. If the block was never explicitly cloned or the second to last reference was dropped, there will be neither space nor performance overhead.Deduplication needs data to work - one needs to pass real data to the write(2) syscall, so hash can be calculated. Block Cloning doesn't require data, just block pointers to the data, so it is extremely fast, as we pay neither the cost of reading the data, nor the cost of writing the data - we operate exclusively on metadata.If the D (dedup) bit is not set in the block pointer, it means that the block is not in the dedup table (DDT) and we won't consult the DDT when we need to free the block. Block Cloning must be consulted on every free, because we cannot modify the source BP (eg. by setting something similar to the D bit), thus we have no hint if the block is in the Block Reference Table (BRT), so we need to look into the BRT. There is an optimization in place that allows to eliminate majority of BRT lookups that is described below in the \"Minimizing free penalty\" section.The BRT entry is much smaller than the DDT entry - for BRT we only store 64bit offset and 64bit reference counter.Dedup keys are cryptographic hashes, so two blocks that are close to each other on disk are most likely in totally different parts of the DDT. The BRT entry keys are offsets into a single top-level VDEV, so data blocks from one file should have BRT entries close to each other.Scrub will only do a single pass over a block that is referenced multiple times in the DDT. Unfortunately it is not currently (if at all) possible with Block Cloning and block referenced multiple times will be scrubbed multiple times.Deduplication requires cryptographically strong hash as a checksum or additional data verification. Block Cloning works with any checksum algorithm or even with checksumming disabled.As mentioned above, the BRT entries are much smaller than the DDT entries. To uniquely identify a block we just need its vdevid and offset. We also need to maintain a reference counter. The vdevid will often repeat, as there is a small number of top-level VDEVs and a large number of blocks stored in each VDEV. We take advantage of that to reduce the BRT entry size further by maintaining one BRT for each top-level VDEV, so we can then have only offset and counter as the BRT entry.Minimizing free penalty.Block Cloning allows to clone any existing block. When we free a block there is no hint in the block pointer whether the block was cloned or not, so on each free we have to check if there is a corresponding entry in the BRT or not. If there is, we need to decrease the reference counter. Doing BRT lookup on every free can potentially be expensive by requiring additional I/Os if the BRT doesn't fit into memory. This is the main problem with deduplication, so we've learn our lesson and try not to repeat the same mistake here. How do we do that? We divide each top-level VDEV into 64MB regions. For each region we maintain a reference counter that is a sum of all reference counters of the cloned blocks that have offsets within the region. This creates the regions array of 64bit numbers for each top-level VDEV. The regions array is always kept in memory and updated on disk in the same transaction group as the BRT updates to keep everything in-sync. We can keep the array in memory, because it is very small. With 64MB regions and 1TB VDEV the array requires only 128kB of memory (we may decide to decrease the region size in the future). Now, when we want to free a block, we first consult the array. If the counter for the whole region is zero, there is no need to look for the BRT entry, as there isn't one for sure. If the counter for the region is greater than zero, only then we will do a BRT lookup and if an entry is found we will decrease the reference counters in the entry and in the regions array.The regions array is small, but can potentially be larger for very large VDEVs or smaller regions. In this case we don't want to rewrite entire array on every change. We then divide the regions array into 128kB chunks and keep a bitmap of dirty chunks within a transaction group. When we sync the transaction group we can only update the parts of the regions array that were modified. Note: Keeping track of the dirty parts of the regions array is implemented, but updating only parts of the regions array on disk is not yet implemented - for now we will update entire regions array if there was any change.The implementation tries to be economic: if BRT is not used, or no longer used, there will be no entries in the MOS and no additional memory used (eg. the regions array is only allocated if needed).Interaction between Deduplication and Block Cloning.If both functionalities are in use, we could end up with a block that is referenced multiple times in both DDT and BRT. When we free one of the references we couldn't tell where it belongs, so we would have to decide what table takes the precedence: do we first clear DDT references or BRT references? To avoid this dilemma BRT cooperates with DDT - if a given block is being cloned using BRT and the BP has the D (dedup) bit set, BRT will lookup DDT entry and increase the counter there. No BRT entry will be created for a block that resides on a dataset with deduplication turned on. BRT may be more efficient for manual deduplication, but if the block is already in the DDT, then creating additional BRT entry would be less efficient. This clever idea was proposed by Allan Jude.Block Cloning across datasets.Block Cloning is not limited to cloning blocks within the same dataset. It is possible (and very useful) to clone blocks between different datasets.One use case is recovering files from snapshots. By cloning the files into dataset we need no additional storage. Without Block Cloning we would need additional space for those files.Another interesting use case is moving the files between datasets (copying the file content to the new dataset and removing the source file). In that case Block Cloning will only be used briefly, because the BRT entries will be removed when the source is removed.Note: currently it is not possible to clone blocks between encrypted datasets, even if those datasets use the same encryption key (this includes snapshots of encrypted datasets). Cloning blocks between datasets that use the same keys should be possible and should be implemented in the future.Block Cloning flow through ZFS layers.Note: Block Cloning can be used both for cloning file system blocks and ZVOL blocks. As of this writing no interface is implemented that allows for ZVOL blocks cloning.Depending on the operating system there might be different interfaces to clone blocks. On FreeBSD we have two syscalls: ssize_t fclonefile(int srcfd, int dstfd); ssize_t fclonerange(int srcfd, off_t srcoffset, size_t length, int dstfd, off_t dstoffset);Even though fclonerange() takes byte offsets and length, they have to be block-aligned.Both syscalls call OS-independent zfs_clone_range() function. This function was implemented based on zfs_write(), but instead of writing the given data we first read block pointers using the new dmu_read_l0_bps() function from the source file. Once we have BPs from the source file we call the dmu_brt_addref() function on the destination file. This function allocates BPs for us. We iterate over all source BPs. If the given BP is a hole or an embedded block, we just copy BP. If it points to a real data we place this BP on a BRT pending list using the brt_pending_add() function.We use this pending list to keep track of all BPs that got new references within this transaction group.Some special cases to consider and how we address them:The block we want to clone may have been created within the same transaction group as we are trying to clone. Such block has no BP allocated yet, so it is too early to clone it. In this case the dmu_read_l0_bps() function will return EAGAIN and in the zfs_clone_range() function we will wait for the transaction group to be synced to disks and retry.The block we want to clone may have been modified within the same transaction group. We could potentially clone the previous version of the data, but that doesn't seem right. We handle it as the previous case.A block may be cloned multiple times during one transaction group (that's why pending list is actually a tree and not an append-only list - this way we can figure out faster if this block is cloned for the first time in this txg or consecutive time).A block may be cloned and freed within the same transaction group (see dbuf_undirty()).A block may be cloned and within the same transaction group the clone can be cloned again (see dmu_read_l0_bps()).A file might have been deleted, but the caller still has a file descriptor open to this file and clones it.When we free a block we have additional step in the ZIO pipeline where we call the zio_brt_free() function. We then call the brt_entry_decref() that loads the corresponding BRT entry (if one exists) and decreases reference counter. If this is not the last reference we will stop ZIO pipeline here. If this is the last reference or the block is not in the BRT, we continue the pipeline and free the block as usual.At the beginning of spa_sync() where there can be no more block cloning, but before issuing frees we call brt_pending_apply(). This function applies all the new clones to the BRT table - we load BRT entries and update reference counters. To sync new BRT entries to disk, we use brt_sync() function. This function will sync all dirty top-level-vdev BRTs, regions arrays, etc.Block Cloning and ZIL.Every clone operation is divided into chunks (similar to write) and each chunk is cloned in a separate transaction. To keep ZIL entries small, each chunk clones at most 254 blocks, which makes ZIL entry to be 32kB. Replaying clone operation is different from the regular clone operation, as when we log clone operation we cannot use the source object - it may reside on a different dataset, so we log BPs we want to clone.The ZIL is replayed when we mount the given dataset, not when the pool is imported. Taking this into account it is possible that the pool is imported without mounting datasets and the source dataset is destroy before the destination dataset is mounted and its ZIL replayed.To address this situation we leverage zil_claim() mechanism where ZFS will parse all the ZILs on pool import. When we come across TX_CLONE_RANGE entries, we will bump reference counters for their BPs in the BRT and then on mount and ZIL replay we will just attach BPs to the file without bumping reference counters.Note it is still possible that after zil_claim() we never mount the destination, so we never replay its ZIL and we destroy it. This way we would end up with leaked references in BRT. We address that too as ZFS gives as a chance to clean this up on dataset destroy (see zil_free_clone_range()).How Has This Been Tested?I have a test program that can make use of this functionality that I have been using for manual testing.Types of changesBug fix (non-breaking change which fixes an issue)New feature (non-breaking change which adds functionality)Performance enhancement (non-breaking change which improves efficiency)Code cleanup (non-breaking change which makes code smaller or more readable)Breaking change (fix or feature that would cause existing functionality to change)Library ABI change (libzfs, libzfs_core, libnvpair, libuutil and libzfsbootenv)Documentation (a change to man pages or other documentation)Checklist:My code follows the OpenZFS code style requirements.I have updated the documentation accordingly.I have read the contributing document.I have added tests to cover my changes.I have run the ZFS Test Suite with this change applied.All commit messages are properly formatted and contain Signed-off-by.Sorry, something went wrong.16lin72h, jittygitty, satmandu, shodanshok, kapitainsky, ninewan, cyphar, WRMSRwasTaken, ms-jpq, NavinF, and 6 more reacted with thumbs up emoji24rincebrain, rustybird, behlendorf, lin72h, szubersk, lukts30, szpght, thesamesam, jittygitty, amotin, and 14 more reacted with hooray emoji14lin72h, edillmann, jittygitty, amotin, deepbluev7, kapitainsky, GauntletWizard, Alanaktion, mmatuska, ms-jpq, and 4 more reacted with heart emoji10lin72h, thesamesam, jittygitty, darkdragon-001, kapitainsky, cyphar, ms-jpq, kwinz, network-shark, and artob reacted with rocket emojilin72h commented on May 3, 2022May 3, 2022It's finally happening, Thanks for the hard workContributorrincebrain commented on May 3, 2022May 3, 2022First, thanks for this, it's been something I've looked forward to since I heard about it on the drawing board.A couple of thoughts, before I go play with this on my testbeds and have more informed ones:I think BRT winds up being strictly better than dedup for almost every use case (excepting if people don't have the time/initial additional space to burn to examine their data to be BRT'd after the fact). Consequently, one thing I've thought would be nice since I heard about the proposal was if we could eventually implement transparent \"migrate to BRT\" from a deduplicated pool - but I believe deferring to the DDT if the DDT bit is set would break the ability to do that later without another feature flag change later. Does that seem like a worthwhile idea/reason to consider ways to allow it to work without breaking changes? (I also just really would like to see existing dedup use dwindle to zero after BRT lands, just so my biases are clear.)In a sharply contrasting but complimentary thought, another unfortunate loss if one uses BRT is the fact that you pay full size for your data on send/recv, until you perhaps go do an additional pass on it afterward...but that doesn't help your snapshots, which may be a substantial penalty. So another thing I've been curious about your thoughts on the feasibility of would be implementing additional hinting for zfs send streams that zfs send on its own would not generate, for \"this received record should be a clone of this block\", with the intent that one could implement, say, a zstream pipeline for being handed a periodically-updated-by-userland dedup table, and checking incoming blocks against it.Is this a userland dedup implementation, similar to the one that just got ejected from the codebase? Yes. Could you mangle your bytes if it incorrectly claimed a block was a duplicate? Also yes. Would allowing people to pay the computational cost to optionally BRT their incoming send streams rather than live with the overhead forever be worth it, IMO? A third yes.(Not suggesting you write it, for this or in general, just curious about your thoughts on the feasibility of such a thing, since \"all your savings go up in smoke on send/recv\" seems like one of the few pain points with no mitigations at the moment...though I suppose you could turn on dedup, receive, and then use idea 1 if that's feasible. Heh.)You said currently scrub will iterate over BRT'd blocks multiple times - IANA expert on either BRT or the scrub machinery, but it seems like you could get away with a BRT-lite table where any time a block would get scrubbed, you check the BRT-lite table, and if it's not there, you check the BRT, and if it has an entry, you queue it and add an entry to the aforementioned lite table? (You could even just drop it all on the floor if the machine restarts, rather than persisting it periodically, and accept scrubbing more than once if the machine restarts.)Come to think of it, I suppose if that works, you could just do that for dedup entries too and stop having to scan the DDT for them, if you wanted...which might make someone I know happy. (You might wind up breaking backward compatibility of resuming scrubs, though, I suppose. Blech.)Likely missing some complexity that makes this infeasible, just curious why you think this might not be a thing one could implement.Thanks again for all your work on this, and for reading my uninformed questions. :)behlendorf added the Status: Code Review Needed Ready for review and testinglabel on May 3, 2022May 3, 2022rincebrain mentioned this pull request on May 3, 2022May 3, 2022Feature request: moving datasets #13393OpenContributorAuthorpjd commented on May 4, 2022May 4, 2022 \u2022edited@rincebrain Here are my thoughts on easier manual deduplication and preserving deduplication during send/recv.Let's say you would like to manually deduplicate the data on your pool periodically. To do that you would need to scan your entire pool, read all the data and build some database that you can check against to see if the there is another copy of the given block already.Once you have such database, then you could use zfs diff to get only the files that has changed and scan only them. Even better if we had a mechanism to read all BPs of the given file without reading the data - that would allow us to determine if the given block is newer than the snapshot and only read the data of the blocks that are newer (and not all the data within modified file).One could teach zfs recv to talk to this database and deduplicate the blocks on the fly.Note that this database is pretty much DDT in userland.Converting DDT to BRT is still doable. I think we would just need to teach ZFS that even though the BP has the D bit set, it may not be in the DDT.As for the scrub, of course everything is doable:) Before you start the scrub you could allocate a bitmap where one bit represents 4kB of storage (2^ashift). Once you scrub a block you set the corresponding bit, so the next time you want to scrub the same block, you will know it already has been done. Such bitmap would require 32MB of RAM per 1TB of pool storage. With this in place we could get rid of the current mechanism for DDT. This bitmap could be stored in the pool to allow to continue scrub after restart (it could be synced to disk rarely as the worst that can happen is that we scrub some blocks twice).3lin72h, rincebrain, and edillmann reacted with thumbs up emojijittygitty commented on May 17, 2022May 17, 2022 \u2022edited@pjd Wow, I just noticed this (you work in stealth mode? :) ), surprised at the progress, thanks! I had a question related to what @rincebrain asked about zfs send/receive and was curious if you knew how BTRFS was doing their reflink send preservation?(I think the -c and -p options? But I've also read on email lists that supposedly btrfs can preserve reflinks over send/receive.)https://blogs.oracle.com/linux/post/btrfs-sendreceive-helps-to-move-and-backup-your-dataSo was wondering how they do it. thxHow can we donate anonymously? (to a specific dev, or openzfs earmarked for specific developer?)Also need to know if \"donations\" will be tax-exempt for recipient or not. If they are not, sender should deduct as biz expense.(Personally, I want my unsolicited donations to be accepted as \"Tax-Exempt gifts\" by recipients, but double taxation of the same money needs to be avoided, so donor and gift recipient don't both pay taxes on donated funds, that's reason for question.)1zfsbot reacted with thumbs down emojiamotin reviewed on May 20, 2022May 20, 2022View reviewed changesMemberamotin left a commentThank you, Pawel. It is a cool feature. I've only started reading the code, so only few first comments so far:1lin72h reacted with thumbs up emojimodule/os/freebsd/zfs/zfs_vnops_os.cOutdated td->td_retval[0] = done; return (done > 0 ? 0 : error);}Memberamotin on May 20, 2022May 20, 2022As I see, FreeBSD already has sys_copy_file_range()/kern_copy_file_range() and even linux_copy_file_range(). Aren't you duplicating?ContributorAuthorpjd on May 20, 2022May 20, 2022 \u2022editedTo be honest I was under impression that copy_file_range(2) always copied the data, just avoiding copying the data to and from userland, but Linux's manual page does mention that reflinks can be used. So, yes, that is a good candidate to use. I'd still prefer to have a flag controlling this behavior, but on Linux, I guess, reflink is used when available. One problem is that if offsets and length are not recordsize-aligned, we would still need to implement the actually copy of the non-aligned bits.All in all, please ignore the syscalls for now, they were added so it can be tested. This will be done separately, maybe under sponsorship of the FreeBSD Foundation.module/zcommon/zpool_prop.cOutdated@@ -116,6 +116,9 @@ zpool_prop_init(void) zprop_register_number(ZPOOL_PROP_DEDUPRATIO, \"dedupratio\", 0,   PROP_READONLY, ZFS_TYPE_POOL, \"<1.00x or higher if deduped>\",   \"DEDUP\", B_FALSE, sfeatures); zprop_register_number(ZPOOL_PROP_BRTRATIO, \"brtratio\", 0,   PROP_READONLY, ZFS_TYPE_POOL, \"<1.00x or higher if cloned>\",   \"BRT\", B_FALSE, sfeatures);Memberamotin on May 20, 2022May 20, 2022\"brtratio\" is not very informative. Why not \"cloneratio\", for example?ContributorAuthorpjd on May 20, 2022May 20, 2022I agree this is not the best name... I didn't want to use \"clone\" as it may be confused with 'zfs clone' and blockcloneratio seems too long. But I agree this should be changed. If people are ok with cloneratio, then it's fine by me.Thank you Alex for looking at this!Memberamotin on May 26, 2022May 26, 2022I am actually curios what does it measure and how useful? It seems like the average number of references to each block that has more than one. For dedup the attribute had some more sense, since it counted all blocks in DDT, which included all blocks written since dedup enable.Contributorallanjude on May 26, 2022May 26, 2022blockrefratio is better, but maybe a bit longContributorAuthorpjd commented on May 20, 2022May 20, 2022@pjd Wow, I just noticed this (you work in stealth mode? :) ), surprised at the progress, thanks! I had a question related to what @rincebrain asked about zfs send/receive and was curious if you knew how BTRFS was doing their reflink send preservation?(I think the -c and -p options? But I've also read on email lists that supposedly btrfs can preserve reflinks over send/receive.) https://blogs.oracle.com/linux/post/btrfs-sendreceive-helps-to-move-and-backup-your-dataSo was wondering how they do it. thxSorry, but I don't know how they do it.How can we donate anonymously? (to a specific dev, or openzfs earmarked for specific developer?) Also need to know if \"donations\" will be tax-exempt for recipient or not. If they are not, sender should deduct as biz expense. (Personally, I want my unsolicited donations to be accepted as \"Tax-Exempt gifts\" by recipients, but double taxation of the same money needs to be avoided, so donor and gift recipient don't both pay taxes on donated funds, that's reason for question.)I'd recommend donating to the FreeBSD Foundation, which is fully tax-deductible in US and FF is supporting OpenZFS development.2lin72h and devnullity reacted with thumbs up emojijittygitty commented on May 21, 2022May 21, 2022@pjd I wish freebsd all the best, but I wanted to donate to a 'specific person', I only use linux anyway. I would not mind 'paying taxes' on donation, without any tax-deduction, so that the individual/recipient does not, since it is a gift. Also via the foundation it seems you must give foundation full name/address etc, the anonymity they offer is just not to list your name on their website.I'll try do a bit of research to see if I can find a more direct way than FreeBSD Foundation, or if that's the only way.As far as btrfs reflinks over send/receive, I think they 'replay' a sort of playbook of actions on extents, that playback on receive.(I could be wrong since details escape me now, I'll have to take a peek again.)Many thanks again for all your hard work and surprising us with this pull-request for feature many of us long wished for!amotin reviewed on May 21, 2022May 21, 2022View reviewed changesMemberamotin left a commentFew more thoughts in order of appearance. Still reading.module/zfs/brt.cOutdated spa_config_enter(brt->brt_spa, SCL_VDEV, FTAG, RW_READER); vd = vdev_lookup_top(brt->brt_spa, brtvd->bv_vdevid); size = vdev_get_min_asize(vd) / brt->brt_rangesize + 1;Memberamotin on May 20, 2022May 20, 2022Aren't you allocating here one extra if vdev_get_min_asize(vd) is multiple of brt->brt_rangesize?ContributorAuthorpjd on May 24, 2022May 24, 2022 \u2022editedI am. Thanks.Memberamotin on May 24, 2022May 24, 2022I was thinking about DIV_ROUND_UP() AKA howmany().module/zfs/brt.c brt_vdev_sync(brt, brtvd, tx); if (brtvd->bv_totalcount == 0)  brt_vdev_destroy(brt, brtvd, tx);Memberamotin on May 20, 2022May 20, 2022Do you need brt_vdev_sync() in case of brt_vdev_destroy()?ContributorAuthorpjd on May 24, 2022May 24, 2022It is not strictly necessary, but in brt_vdev_destroy() I assert that all the counters are 0. If brt_vdev_sync() wouldn't be called then we would need to resign from the asserts and I'd prefer not to do that, especially that this won't happen often. Do you agree?Memberamotin on May 24, 2022May 24, 2022I just meant that writing something just to delete it next line is a waste of time. I agree about the assertions point though, but it is more of implementation detail.module/zfs/brt.cOutdated if (bre1->bre_offset < bre2->bre_offset) return (-1); else if (bre1->bre_offset > bre2->bre_offset) return (1);Memberamotin on May 20, 2022May 20, 2022Lots if not all AVL compare functions were rewritten to TREE_CMP().1pjd reacted with thumbs up emojimodule/zfs/brt.cOutdated /* * Entries to sync. */ avl_tree_t *bv_tree;Memberamotin on May 20, 2022May 20, 2022Does it need to be a pointer? It is a very small structure, why add another pointer dereference?1pjd reacted with thumbs up emojimodule/zfs/brt.cOutdated if (bre1->bre_vdevid < bre2->bre_vdevid) return (-1); else if (bre1->bre_vdevid > bre2->bre_vdevid) return (1);Memberamotin on May 20, 2022May 20, 2022I see brt_entry_compare() is used only within one vdev. So I guess you could just assert (bre1->bre_vdevid == bre2->bre_vdevid).ContributorAuthorpjd on May 24, 2022May 24, 2022Good catch! This is a leftover from when all BRT entries were together. We can actually eliminate bre_vdevid (which I just did). Thanks.module/zfs/brt.cOutdated if (error == ENOENT) { BRTSTAT_BUMP(brt_decref_entry_not_on_disk); brt_entry_free(bre);Memberamotin on May 20, 2022May 20, 2022Allocating and immediately freeing memory for every freed block potentially present in BRT is a bit of waste. Could probably be optimized.ContributorAuthorpjd on May 24, 2022May 24, 2022Good point. Fixed.module/zfs/brt.cOutdated brt_entry_fill(&bre_search, bp); brt_enter(brt);Memberamotin on May 20, 2022May 20, 2022I haven't profiled frees recently, but have feeling that global pool-wide lock here will cause congestion. It would be good to have it at least per-vdev.ContributorAuthorpjd on May 24, 2022May 24, 2022For now I've modified brt_lock to rwlock. This allows to use read-lock in brt_may_exists() and this is the hot function called on every free.module/zfs/brt.cOutdated { \"decref_no_entry\",  KSTAT_DATA_UINT64 }};#define BRTSTAT_BUMP(stat) atomic_inc_64(&brt_stats.stat.value.ui64)Memberamotin on May 20, 2022May 20, 2022Would be good to use wmsum_add() to not touch global variables with atomic under another global (now) lock.1pjd reacted with thumbs up emojiallanjude reviewed on May 24, 2022May 24, 2022View reviewed changesmodule/zfs/brt.cOutdated /* * TODO: Walk through brtvd->bv_bitmap and write only dirty parts. */ dmu_write(brt->brt_mos, brtvd->bv_mos_brtvdev, 0,Contributorallanjude on May 24, 2022May 24, 2022Brian pointed out this might need to be byte swapped so it works properly across endian-nessContributorallanjude on May 26, 2022May 26, 2022Should we have a header to go with this? to store BRT_RANGE_SIZE and anything else that might change in the future?Otherwise we are likely to have compatibility issues if we ever want to change the range sizeContributorAuthorpjd on May 27, 2022May 27, 2022You won't be able to change range size once the pool is created. Well, it at least will be hard, as you would need to reread the entire BRT table and recreated refcount table. Currently I store range size in metadata to be able to change the define in the future and don't affect existing pools.amotin reviewed on May 26, 2022May 26, 2022View reviewed changesMemberamotin left a commentI've got to the end of the patch, so the last batch for now.module/zcommon/zpool_prop.cOutdated@@ -116,6 +116,9 @@ zpool_prop_init(void) zprop_register_number(ZPOOL_PROP_DEDUPRATIO, \"dedupratio\", 0,   PROP_READONLY, ZFS_TYPE_POOL, \"<1.00x or higher if deduped>\",   \"DEDUP\", B_FALSE, sfeatures); zprop_register_number(ZPOOL_PROP_BRTRATIO, \"brtratio\", 0,   PROP_READONLY, ZFS_TYPE_POOL, \"<1.00x or higher if cloned>\",   \"BRT\", B_FALSE, sfeatures);Memberamotin on May 26, 2022May 26, 2022I am actually curios what does it measure and how useful? It seems like the average number of references to each block that has more than one. For dedup the attribute had some more sense, since it counted all blocks in DDT, which included all blocks written since dedup enable.module/zfs/brt.cShow resolvedmodule/zfs/dmu.c dl->dr_override_state = DR_OVERRIDDEN; if (BP_IS_HOLE(bp)) {  dl->dr_overridden_by.blk_phys_birth = 0;  dl->dr_overridden_by.blk_birth = 0;Memberamotin on May 26, 2022May 26, 2022Is this correct? This case is equivalent of hole being punched after being written, so as I understand it should get birth time of this TXG to replicate correctly. Exception can be made only if it was hole and still remains a hole, in which case we may keep the pointer intact.ContributorAuthorpjd on Nov 1, 2022Nov 1, 2022 \u2022editedDoes that look better?--- a/module/zfs/dmu.c+++ b/module/zfs/dmu.c@@ -2272,12 +2272,11 @@ dmu_brt_addref(objset_t *os, uint64_t object, uint64_t offset, uint64_t length,        dl->dr_override_state = DR_OVERRIDDEN;        if (BP_IS_HOLE(bp)) {            dl->dr_overridden_by.blk_phys_birth = 0;-            dl->dr_overridden_by.blk_birth = 0;        } else {            dl->dr_overridden_by.blk_phys_birth =              BP_PHYSICAL_BIRTH(bp);-            dl->dr_overridden_by.blk_birth = dr->dr_txg;        }+        dl->dr_overridden_by.blk_birth = dr->dr_txg;        /*         * When data in embedded into BP there is no need to createMemberamotin on Nov 1, 2022Nov 1, 2022Probably, but I am not completely confident in this db_dirty_records area. Additionally if both source and destination are holes, I guess this could be skipped completely to not increase metadata and replication traffic, but with the same caution.module/zfs/zfs_vnops.c   length, RL_WRITER); srclr = zfs_rangelock_enter(&srczp->z_rangelock, srcoffset,   length, RL_READER); }Memberamotin on May 26, 2022May 26, 2022Should here also be a case of srczp == dstzp, comparing srcoffset to dstoffset to avoid deadlock if somebody try to do it simultaneously in opposite directions.1pjd reacted with thumbs up emojiContributorAuthorpjd on May 27, 2022May 27, 2022Good catch!module/zfs/zfs_vnops.cOutdated   length, RL_READER); } srcblksz = srczp->z_blksz;Memberamotin on May 26, 2022May 26, 2022I feel somewhere here must be a check that destination block size is equal to source or the destination file consist of only one block or it will be overwritten completely. zfs_grow_blocksize() used below will silently do nothing otherwise, that will probably result in data corruption.1pjd reacted with thumbs up emojimodule/zfs/zio.cOutdated return (zio); } if (BP_GET_TYPE(bp) != DMU_OT_PLAIN_FILE_CONTENTS &&   BP_GET_TYPE(bp) != DMU_OT_ZVOL) {Memberamotin on May 26, 2022May 26, 2022Should here be BP_IS_METADATA() instead for symmetry with dmu_brt_readbps()?1pjd reacted with thumbs up emojimodule/zfs/zfs_log.c lr->lr_length = len; lr->lr_blksz = blksz; lr->lr_nbps = partnbps; memcpy(lr->lr_bps, bps, sizeof (bps[0]) * partnbps);Memberamotin on May 26, 2022May 26, 2022Do we have some guaranties that blocks from different dataset won't be freed before the replay? For example in case of encrypted dataset (I just don't remember whether non-encrypted datasets are replayed strictly on import or may be sometimes later).Contributorallanjude on May 26, 2022May 26, 2022IIRC, they are replayed at mount, so it could be lateContributorAuthorpjd on Nov 1, 2022Nov 1, 2022For now I changed the code to only use ZIL when we clone within a single dataset. When sync=always is set on the destination dataset and we are cloning between separate datasets we call txg_wait_synced() before returning.Memberamotin on Nov 1, 2022Nov 1, 2022I suspect unless the file is really big it may be faster to return error and just copy it than wait for TXG commit.behlendorf reviewed on May 27, 2022May 27, 2022View reviewed changesContributorbehlendorf left a commentI've only started working my way through the PR, but I thought I'd go ahead and post at least some initial feedback. I'll keep working my through it as I can. For other reviewer I'd suggest starting with the Big Theory comment at the top if brt.c. It does a great job explaining the high level design.cmd/zdb/zdb_il.cOutdated   tab_prefix, (u_longlong_t)lr->lr_foid, (u_longlong_t)lr->lr_offset,   (u_longlong_t)lr->lr_length, (u_longlong_t)lr->lr_blksz); for (i = 0; i < lr->lr_nbps; i++) {Contributorbehlendorf on May 26, 2022May 26, 2022Suggested change for (i = 0; i < lr->lr_nbps; i++) { for (unsigned int i = 0; i < lr->lr_nbps; i++) {1pjd reacted with thumbs up emojiinclude/sys/ddt.hOutdatedShow resolvedmodule/icp/include/sys/bitmap.hOutdated@@ -0,0 +1,183 @@/*Contributorbehlendorf on May 26, 2022May 26, 2022Is there a reason this needs to be added to the icp include directory? It'd be a bit more convenient to place it under the top level include/sys/ path. It would also be nice to either prune the header down to what's actually used, or add the missing bitmap.c source so all the publicly defined functions exist.1pjd reacted with thumbs up emojiContributorbehlendorf on Feb 24Feb 24, 2023This still should be moved. It builds fine on Linux under include/sys/.1pjd reacted with thumbs up emojimodule/zfs/brt.cOutdatedShow resolvedmodule/zfs/brt.cOutdated *  algorithm or even with checksumming disabled. * * As mentioned above, the BRT entries are much smaller than the DDT entries. * To uniquely identify a block we just need its vdevid and offset. We alsoContributorbehlendorf on May 26, 2022May 26, 2022I'd suggest using vdev id instead of vdevid throughout.1pjd reacted with thumbs up emojimodule/zfs/brt.cOutdatedShow resolvedmodule/zfs/brt.cOutdatedShow resolvedmodule/zfs/brt.cOutdated * references? To avoid this dilemma BRT cooperates with DDT - if a given block * is being cloned using BRT and the BP has the D (dedup) bit set, BRT will * lookup DDT entry and increase the counter there. No BRT entry will be * created for a block that resides on a dataset with deduplication turned on.Contributorbehlendorf on May 26, 2022May 26, 2022I think you mean no BRT entry will be created for a block that is already in the dedup table, right? Not that this is based on the current setting of the dedup property. It seems to me this also requires that once a block has been added to the BRT it can't be added to the DDT. Or put another way, the contents of the BRT and DDT are required to be mutually exclusive.ContributorAuthorpjd on May 27, 2022May 27, 2022I'll try to clarify that. You are right that we may have a block residing on dedup-enabled dataset and not having the D bit set (ie. it was created before dedup was turned on).Also note that it is impossible to set the D bit after block creation, so \"once a block has been added to the BRT it can't be added to the DDT\" cannot happen as there is no way to add a block to DDT after it was created.module/zfs/brt.c * ssize_t fclonefile(int srcfd, int dstfd); * ssize_t fclonerange(int srcfd, off_t srcoffset, size_t length, *           int dstfd, off_t dstoffset); *Contributorbehlendorf on May 26, 2022May 26, 2022For reference, on Linux the interfaces are:  ssize_t copy_file_range(int fd_in, off64_t *off_in, int fd_out, off64_t *off_out, size_t len, unsigned int flags);  int ioctl(int dest_fd, FICLONERANGE, struct file_clone_range *arg);  int ioctl(int dest_fd, FICLONE, int src_fd);https://man7.org/linux/man-pages/man2/copy_file_range.2.htmlhttps://man7.org/linux/man-pages/man2/ioctl_ficlone.2.htmlAt least on Linux there's no requirement that the start and length offsets need to be block aligned. If unaligned we'll have to copy the beginning and end at least on Linux. My feeling is it'd be best to handle this in the common code so all the platforms could benefit from it. But if could be platform specific if we need too.I haven't yet gone through all the expected errno's from the man pages above, but let's make sure they match up with what zfs_clone_range() returns if possible.module/zfs/brt.cOutdated * - The block we want to clone may have been modified within the same *  transaction group. We could potentially clone the previous version of the *  data, but that doesn't seem right. We treat it as the previous case and *  return an error.Contributorbehlendorf on May 26, 2022May 26, 2022For this case, and the one above I think we're going to have to handle waiting on the txg sync and retrying somewhere in the ZFS code. Based on established errno's from the man pages any existing callers aren't going to know what to do with EAGAIN.2pjd and edillmann reacted with thumbs up emojirincebrain mentioned this pull request on May 28, 2022May 28, 2022Fast Copy Between Related Datasets #13516ClosedHaravikk commented on May 28, 2022May 28, 2022Excellent to see this feature being worked on! I just posted a request for \"fast-copying\" between datasets (#13516) because I had assumed any reflink style support wouldn't initially handle it, but it's good to see it mentioned here. In that case my issue can probably be closed.To be clear, in my issue I'm assuming automatic \"fast-copying\" (cloning) rather than having to explicitly call cp --reflink or similar, because a lot of file copying/moving is done by programs that won't know to do this, so it's better for it to be automatic. For move/copy between datasets though this will obviously depend upon zfs knowing when that's the case (I'm not super familiar with the system calls themselves so don't know how easy that would be to detect?), but clearly it would be better if ZFS recognises as many cases where cloning is beneficial as possible, without having to be told.One thing I'd like to discuss from my issue is copying between datasets where certain settings differ, with the main ones being copies, compression and recordsize.If a target dataset has a copies setting that differs from the source then ideally copies should be created or ignored to match. So if the source has copies=2 and the target is copies=1 then the second copy won't be cloned, while if we flip that around (copies=1 -> copies=2) then we still need to create a new copy in addition to cloning the first, so that we don't end up with \"new\" data that is less redundant than an ordinary copy would have been. I didn't see any mention of copies being discussed, and can't tell if this is already implemented or not.Meanwhile compression and recordsize represent settings that will mean that a cloned file will not be an exact match for the file as it would have been had it been copied normally, as the source dataset may not have compression but the target does, target may use a larger recordsize and so-on.To cover these I propose a new setting, probably better named blockcloneto for this feature, which will control cloning between datasets. I proposed three basic settings:on: block-cloning is always used where possible when this dataset is the target.off: block-cloning is never used (files are copied normally in all cases).exact: block-cloning is used when compression and recordsize for the source dataset matches the target.There may be other settings that need to be considered, but these are the ones I use that seem applicable to cloning.The aim is to allow users to ensure that cloning is not used to produce files in a dataset that are less redundant, less compressed etc. than files that are copied normally.Contributorrincebrain commented on May 29, 2022May 29, 2022So, I tried a first pass at wiring up the Linux copy/clone range calls, and found a problem.You see, Linux has a hardcoded check in the VFS layer for each of these calls that: if (file_inode(file_in)->i_sb != file_inode(file_out)->i_sb) return -EXDEV;That is, it attempts to explicitly forbid cross-filesystem reflinks in a VFS check before it ever gets to our code.The options I see for dealing with this are all pretty gross.Lie about having the same i_sb for all datasets on a pool, and work around doing this anywhere we actually consume this value (eww, and seems like it could have weird side effects)Write our own ioctl reimplementing the equivalent functionality of copy_file_range, without this restriction (ewwww, cp et al wouldn't use it so we'd need zfs_cp or similar, though we might be able to eventually convince coreutils to support it directly...maybe...)I don't really have any other suggestions.Thoughts?Haravikk commented on May 29, 2022May 29, 2022The options I see for dealing with this are all pretty gross.Lie about having the same i_sb for all datasets on a pool, and work around doing this anywhere we actually consume this value (eww, and seems like it could have weird side effects)Write our own ioctl reimplementing the equivalent functionality of copy_file_range, without this restriction (ewwww, cp et al wouldn't use it so we'd need zfs_cp or similar, though we might be able to eventually convince coreutils to support it directly...maybe...)I don't really have any other suggestions.If the aim is to clone as many copy/move operations as possible then should we be worrying about explicit cloning commands in the first place?I think the ideal scenario is to avoid the need for those calls entirely, by detecting when a file is being copied between datasets in the same pool (including the same dataset) so we can use cloning instead, this way programs don't need to make any explicit cloning calls, since many don't/won't, so picking this up automatically on the ZFS side would be best, as it would mean more files \"deduplicated\" with no extra work by users or programs.Contributorrincebrain commented on May 29, 2022May 29, 2022The options I see for dealing with this are all pretty gross.Lie about having the same i_sb for all datasets on a pool, and work around doing this anywhere we actually consume this value (eww, and seems like it could have weird side effects)Write our own ioctl reimplementing the equivalent functionality of copy_file_range, without this restriction (ewwww, cp et al wouldn't use it so we'd need zfs_cp or similar, though we might be able to eventually convince coreutils to support it directly...maybe...)I don't really have any other suggestions.If the aim is to clone as many copy/move operations as possible then should we be worrying about explicit cloning commands in the first place?I think the ideal scenario is to avoid the need for those calls entirely, by detecting when a file is being copied between datasets in the same pool (including the same dataset) so we can use cloning instead, this way programs don't need to make any explicit cloning calls, since many don't/won't, so picking this up automatically on the ZFS side would be best, as it would mean more files \"deduplicated\" with no extra work by users or programs.Changing the scope of this from \"userland generated reflinking of ranges\" to \"reimplementing dedup inline\" would significantly impact how useful it is, IMO. In particular, it would go from \"I'd use it\" to \"I'd force it off\".The goal here is not (I believe) \"maximize dedup on everything\", the goal here is \"allow userland to specify when something is wanted to be a CoW copy of something else, because that's where most of the benefit is compared to trying to dedup everything\".6lukts30, modzilla99, adamdmoss, scineram, Ranlvor, and thesamesam reacted with thumbs up emojiHaravikk commented on May 29, 2022May 29, 2022 \u2022editedChanging the scope of this from \"userland generated reflinking of ranges\" to \"reimplementing dedup inline\" would significantly impact how useful it is, IMO. In particular, it would go from \"I'd use it\" to \"I'd force it off\".This seems like a huge overreaction; ZFS provides more than enough tools to control redundancy already, so there's simply no advantage to maintaining multiple copies of the same file beyond that (so long as the target dataset's settings are respected if necessary, see above).I'm very much of the opposite position; if this feature is manual only then I guarantee you I will almost never use it, because so many tools simply do not support cloning even on filesystems that support it. If I have to jump to the command line to do it manually outside of a tool, or discard and replace copies after the fact, then that's an excellent way to guarantee that I'll almost never do it.While there's certainly potential for offline deduplication scripts to make this a bit easier, e.g- scan one or more datasets for duplicates and replace with clones of one of them, why force it to be done retroactively? Otherwise it's limiting to just those with more specific needs, like cloning VMs or similar (where cloning datasets isn't suitable).And if the cloning system calls aren't going to work for cross-dataset cases anyway, then automatic may be the only way to go regardless which is why I mentioned it; at the very least you could still turn it off by default, then turn it on temporarily when you want to clone to another dataset.3modzilla99, scineram, and Ranlvor reacted with thumbs down emojiContributorrincebrain commented on May 29, 2022May 29, 2022I don't know if I'd say they're not going to work...$ ls -ali /turbopool/whatnow/bigfile2 /turbopool/whatnow2/bigfile3ls: cannot access '/turbopool/whatnow2/bigfile3': No such file or directory3 -rw-r--r-- 1 root root 10737418240 May 28 12:20 /turbopool/whatnow/bigfile2$ sudo cmd/clonefile/clonefile /turbopool/whatnow/bigfile2 /turbopool/whatnow2/bigfile3$ sudo cmd/zdb/zdb -dbdbdbdbdbdb turbopool/whatnow 3 > /tmp/file1$ ls -ali /turbopool/whatnow/bigfile2 /turbopool/whatnow2/bigfile3128 -rw-r--r-- 1 root root 10737418240 May 29 17:02 /turbopool/whatnow2/bigfile3 3 -rw-r--r-- 1 root root 10737418240 May 28 12:20 /turbopool/whatnow/bigfile2$ sudo cmd/zdb/zdb -dbdbdbdbdbdb turbopool/whatnow2 128 > /tmp/file2$ grep ' 200000  L0' /tmp/file{1,2}/tmp/file1:     200000  L0 DVA[0]=<0:15800223c00:20000> [L0 ZFS plain file] edonr uncompressed unencrypted LE contiguous unique single size=20000L/20000P birth=377L/377P fill=1 cksum=1746b656272237d9:cd37f4f0b1f655f5:699bc3e57a9d0e06:72ebf1ea28603be2/tmp/file2:     200000  L0 DVA[0]=<0:15800223c00:20000> [L0 ZFS plain file] edonr uncompressed unencrypted LE contiguous unique single size=20000L/20000P birth=4158L/377P fill=1 cksum=1746b656272237d9:cd37f4f0b1f655f5:699bc3e57a9d0e06:72ebf1ea28603be2$ df -h /turbopool/whatnow2/bigfile3 /turbopool/whatnow/bigfile2Filesystem     Size Used Avail Use% Mounted onturbopool/whatnow2 1.8T  21G 1.8T  2% /turbopool/whatnow2turbopool/whatnow  1.8T  41G 1.8T  3% /turbopool/whatnowIt's more inconvenient, but it's certainly not unworkable.I'm not saying there's no use for anyone if it's used to replace the existing inline dedup implementation - further up the thread, I advocate for doing just that, and implementing similar functionality to allow you to do inline dedup on send/recv with this.But personally, the performance tradeoffs of inline dedup aren't worth it to me, especially compared to doing it as postprocessing later as needed, or with explicit notification, so if that was the only functionality, I would not be making use of it.lukts30 commented on May 29, 2022May 29, 2022 \u2022editedSo, I tried a first pass at wiring up the Linux copy/clone range calls, and found a problem.You see, Linux has a hardcoded check in the VFS layer for each of these calls that:My understanding from reading the man page and kernel source code is that this is indeed a problem with all ioctl-based file clone/dedup calls but not with the newer more generic copy_file_range syscall.copy_file_range explicitly has supports for cross-filesystem copies since 5.3.If the src and dst file are from zfs but not on the same sb with copy_file_range zfs still has the chance to implement\"copy acceleration\" techniques.torvalds/linux@5dae222Still not optimal since most tools that claim to be reflink compatible will first use the ioctl and only might fall back to copy_file_range.E.g. gnu coreutil cp and syncthing would still try copy_file_range but other tools might not.https://dev.to/albertzeyer/difference-of-ficlone-vs-ficlonerange-vs-copyfilerange-for-copy-on-write-support-41lmContributorrincebrain commented on May 29, 2022May 29, 2022 \u2022editedcoreutils (8.30) on 5.4 was where I saw EXDEV without ever executing our copy_file_range or remap_file_range, and looking in the source, 8.30 doesn't know about copy_file_range at all.So even if the kernel provides copy_file_range, plenty of older platforms aren't going to play, unfortunately.(Not that I'm not ecstatic to be wrong and that we can have our cake and eat it too, and also amused that Oracle originated the patch, but I think we still get to have a fallback, albeit a much simpler one than what I implemented, for those cases.)e: anyway, i'll polish up the thing I have for making this play nice under Linux with cp --reflink and extending clonefile to use copy_file_range, and then I'll post it probably tomorrow for people to use or not as they like.e: I actually just tried coreutils git and it also refused cp --reflink with EXDEV, so I'm guessing their autodetection handling is...incorrect, because actually calling copy_file_range works great.1leelists reacted with heart emojijittygitty mentioned this pull request on May 29, 2022May 29, 2022Support for ZFS on top of VDO inline deduplication and best practices etc? #13462OpenContributorrincebrain commented on Jun 18, 2022Jun 18, 2022Huh, did I really never post anything more in here? Rude of me.It seems the consensus is Linux should change here, and that coreutils can't do anything to make the behavior I write about there better without violating correctness, but I put the odds of that as infinitely approaching 0 without quite getting there, so I suppose we'll be stuck with clonefile and/or documenting cp --reflink=always not behaving as expected, unless there's some detail I'm overlooking.I've been busy and haven't done what I wanted to, which was extend the clonefile command to do smaller than full range copies and write a bunch of tests to exercise that and report back. Maybe I'll get that done in the next few days, we'll see.1adamdmoss reacted with thumbs up emojiIvanVolosyuk commented on Jun 19, 2022Jun 19, 2022Lie about having the same i_sb for all datasets on a pool, and work around doing this anywhere we actually consume this value (eww, and seems like it could have weird side effects)i_sb is a superblock of filesystem. Do you think it is actually lying, if the data can be linked freely across the pool? I wonder if it will be interesting to find btrfs discussion on the topic - why they implemented it that way.1mqudsi reacted with thumbs up emojiContributorrincebrain commented on Jun 19, 2022Jun 19, 2022It's probably worth reading about some side effects of how btrfs handles subvolumes before thinking that might be a good route.2IvanVolosyuk and Haravikk reacted with thumbs up emojiHaravikk commented on Jun 19, 2022Jun 19, 2022 \u2022editedSo if leveraging the system clone call isn't an option, what information do we have to work with?My thinking was that if we can know the source dataset, target dataset, and the file/inode being copied or moved, then this should still be enough for ZFS to decide whether to clone or not, without having to be explicitly told?I don't know enough about the file system calls to know how easy it is to get that information though; I guess I was just thinking that if we know a file is being copied within the same dataset, or copied/moved between two datasets under the same (or no) encryption root then ZFS can simply decide to clone automatically based on whatever rules are set on the target (e.g- always clone where possible, only clone if compression/recordsize etc. is the same, or never clone).It's never really made sense to me why an explicit call should be required to clone in the first place, because if you trust a filesystem enough to store your data then you shouldn't need to create extra redundancy by default, especially on ZFS with mirror or raidz vdevs, copies=2 etc., if those aren't enough redundancy for you, then you could still disable cloning (either permanently or temporarily).It should still probably be an opt-in feature, as administrators know their pools the best, but the three main options I outlined should be sufficient? Then if the clone system calls are improved in future, we could add a fourth setting \"on\" meaning cloning is permitted, but only when the appropriate system call is used? Actually thinking about it, this option could also be available immediately, as even if the system calls won't work for now, we could still presumably have a ZFS specific command to begin with (e.g- something under zdb) to handle the cloning entirely via ZFS? Not ideal, but for those that want manual cloning only, it would still make it possible in the interim.lukts30 commented on Jun 19, 2022Jun 19, 2022My thinking was that if we can know the source dataset, target dataset, and the file/inode being copied or moved, then this should still be enough for ZFS to decide whether to clone or not, without having to be explicitly told?I don't know enough about the file system calls to know how easy it is to get that information though;The whole reason why reflink exists and why it is is an explicit operation is because:To the filesystem, a cp isn't a copy -- it's one process reading from one file and writing to another. Figuring out that that is supposed to be a copy is very non-trivial and expensive, especially when taking into account metadata operations which aren't part of the regular file stream.https://lwn.net/Articles/332076/And IMHO if you want to do things implicitly ZFS already has dedupe.Block Cloning/ Reflink is purposely built around being an explicit operation so that the kernel knows of the intention of userspace.There is no guessing game that some open read and write syscall all correlate together.2adamdmoss and scineram reacted with thumbs up emojiHaravikk mentioned this pull request on Jun 19, 2022Jun 19, 2022Lightweight Deduplication (for Copying) #13572Openproblame suggested changes on Jun 22, 2022Jun 22, 2022View reviewed changesContributorproblame left a commentI did another ZIL-focussed review.Apologies for the formatting, I did it offline and then copy-pasted into GitHub.2adamdmoss and lin72h reacted with thumbs up emojimodule/zfs/zfs_log.cOutdatedShow resolvedmodule/zfs/zfs_replay.cShow resolvedmodule/zfs/zfs_vnops.cOutdatedShow resolvedmodule/zfs/zfs_vnops.cOutdatedShow resolvedmodule/zfs/zfs_vnops.cOutdatedShow resolvedmodule/zfs/zfs_vnops.cOutdatedShow resolvedmodule/zfs/zfs_vnops.cOutdatedShow resolvedmodule/zfs/zfs_vnops.cShow resolved34 hidden itemsLoad more\u2026behlendorf approved these changes on Mar 10Mar 10, 2023View reviewed changesContributorbehlendorf left a commentThanks for working through feedback! This looks ready to me. We've got a few changes we'll need to follow up with, but I don't think they need to hold the rest of this up.We'll want to further address the ZIL replay issue mentioned in the feedback and commented on above zvol_replay_clone_range. But that can be handled in its own PR.This is wired up for FreeBSD, but not yet for Linux. Again there are some platform specific details there which can be worked out in a follow up PR.module/zfs/zil.cShow resolvedbehlendorf added Status: Accepted Ready to integrate (reviewed, tested)and removed Status: Code Review Needed Ready for review and testinglabels on Mar 10Mar 10, 2023View detailsbehlendorf merged commit 67a1b03 into openzfs:master on Mar 10Mar 10, 202314 of 16 checks passedContributoradamdmoss commented on Mar 10Mar 10, 2023 \u2022editedApologies since I lost track; did this land in a state compatible with existing Linux copy_file_range/reflink support (i.e. cp --reflink) or is a separate flag/syscall/api needed to utilize this?Thanks!2Alanaktion and edillmann reacted with eyes emojiMemberamotin commented on Mar 10Mar 10, 2023did this land in a state compatible with existing Linux reflink support (i.e. cp --reflink) or is a separate flag/syscall/api needed to utilize this? Thanks!@adamdmoss It landed without Linux support yet. Only FreeBSD for now, but hopefully not for long.Contributoradamdmoss commented on Mar 10Mar 10, 2023did this land in a state compatible with existing Linux reflink support (i.e. cp --reflink) or is a separate flag/syscall/api needed to utilize this? Thanks!@adamdmoss It landed without Linux support yet. Only FreeBSD for now, but hopefully not for long.Cool - thanks for the clarification.Contributorcyphar commented on Mar 12Mar 12, 2023Is anyone else already working on wiring this up on Linux? I'd be willing to give it burl if no-one else is working on it.2thesamesam and edillmann reacted with heart emojiContributorryao commented on Mar 13Mar 13, 2023@cyphar Currently, @rincebrain is doing some experiments with it.Memberamotin commented on Mar 13Mar 13, 2023We also have a guy looking into it.2thesamesam and edillmann reacted with heart emojimcmilk pushed a commit to mcmilk/zfs that referenced this pull request on Mar 13Mar 13, 2023Implementation of block cloning for ZFS \u2026913e1bblundman pushed a commit to openzfsonwindows/openzfs that referenced this pull request on Mar 16Mar 16, 2023Implementation of block cloning for ZFS \u202662f5d52jittygitty mentioned this pull request on Mar 28Mar 28, 2023FAST-Tracking REFLINK and Offline Deduplication, first for LINUX only #13349OpenMajiir mentioned this pull request on Apr 4Apr 4, 2023Feature Request - online split clone #2105OpenContributormjguzik commented on Apr 11Apr 11, 2023Given the above commentary I take it this was not tested on Linux?We got reports of data and pool corruption in FreeBSD after the import which landed the feature, for example:Initially I copied my /usr/obj from my two build machines (one amd64.amd64 and an i386.i386) to my \"sandbox\" zpool.Next, with block_cloning disabled I did cp -R of the /usr/obj test files. Then a diff -qr. They source and target directories were the same.Next, I cleaned up (rm -rf) the target directory to prepare for theblock_clone enabled test.Next, I did zpool checkpoint t. After this, zpool upgrade t. Pool t now has block_cloning enabled.I repeated the cp -R test from above followed by a diff -qr. Almostevery file was different. The pool was corrupted.I restored the pool by the following removing the corruption:slippy# zpool export tslippy# zpool import --rewind-to-checkpoint tslippy#It is recommended that people avoid upgrading their zpools until theproblem is fixed.There is also a report claiming corruption can show up even with the feature disabled, I don't know if it is related yet.1grahamperrin reacted with eyes emojiContributorryao commented on Apr 11Apr 11, 2023Given the above commentary I take it this was not tested on Linux?It was not. There are also no test cases in ZTS for this. We really should get those implemented sooner rather than later.A few bugs have already been found in this from a mix of static analysis and people attempting to adapt it for use on Linux.1grahamperrin reacted with eyes emojiContributormjguzik commented on Apr 11Apr 11, 2023Given the above I don't think the feature is ready for prime-time and consequently should be reverted for the time being.Contributorryao commented on Apr 11Apr 11, 2023The ones reported have already been fixed:ce0e1cc5b5f518That said, we still need test cases in the ZTS for this.1lin72h reacted with thumbs up emojiContributormjguzik commented on Apr 11Apr 11, 2023we have these commits, they came with the merge. as noted above, issues are still there.thesamesam commented on Apr 11Apr 11, 2023Let's start with a bug report in a separate issue.2ryao and edillmann reacted with thumbs up emojiContributorAuthorpjd commented on Apr 11Apr 11, 2023Let me summarize what I found, as a lot of FUD is being spread.Reported corruptions when block cloning was disabled, AFAIU, were due to EXDEV leaking to userland's cp(1) and not being handled in the kernel. This resulted in cp(1) creating empty files. Not a data corruption inside ZFS. This was due to OpenZFS being merged to FreeBSD before plumbing was finished:https://reviews.freebsd.org/D38803@mjguzik should know that, as he was involved in the review.I can confirm there is data corruption when block cloning is enabled. It happens for very small files were embedded blocks are used. I'm overwriting blk_phys_birth which for embedded blocks is part of the payload.The corrupted file has 8 bytes overwritten at position 60. zpool status -v might report checksum errors in those files. After removing problematic files and running scrub everything comes back to normal.I've created pull request with a fix:#14739I'm also working on adding tests, but I'd like to be able to have a more complete coverage and currently there is no userland tool apart from cp(1) that uses copy_file_range(2). Maybe integrating pjdfstest would be beneficial? I could extend fstest tool to allow testing copy_file_range(2) with different offsets and sizes.I hope that helps.PS. I'm currently in the middle of spring break with my family, mostly traveling, so I can answer with delays. If someone wants to commit the fix to FreeBSD directly, be my guest.10jittygitty, olegsidorkin, lin72h, behlendorf, ryao, grahamperrin, thesamesam, edillmann, thushan, and rrbutani reacted with thumbs up emojibehlendorf pushed a commit that referenced this pull request on Apr 12Apr 12, 2023Fix data corruption when cloning embedded blocks \u2026Verifiedc71fe71Contributoredillmann commented on Apr 24Apr 24, 2023Hi @pjd,Any new on the porting of this to linux (client side) ?Thanksandrewc12 pushed a commit to andrewc12/openzfs that referenced this pull request on Apr 30Apr 30, 2023Fix data corruption when cloning embedded blocks \u20263d61173andrewc12 pushed a commit to andrewc12/openzfs that referenced this pull request on Apr 30Apr 30, 2023Fix data corruption when cloning embedded blocks \u202656405aebehlendorf added a commit that referenced this pull request 5 days agoJun 30, 2023Tag 2.2.0-rc1 \u2026009d328darkdragon-001 mentioned this pull request 4 days agoJul 1, 2023COW cp (--reflink) support #405Openjacky1234 mentioned this pull request 9 hours agoJul 5, 2023HackerNews Top 10 @2023-07-05 jacky1234/blogPages#106Opengithub-actions bot mentioned this pull request 3 hours agoJul 5, 2023Hacker News Daily Top 30 @2023-07-05 meixger/hackernews-daily#290Openxueyuanl mentioned this pull request 1 hour agoJul 5, 2023Daily Hacker News 05-07-2023 xueyuanl/daily-hackernews#1031OpenSign up for free to join this conversation on GitHub. Already have an account? Sign in to commentReviewersallanjudeallanjude left review commentsamotinamotin approved these changesadamdmossadamdmoss left review commentsproblameproblame requested changesbehlendorfbehlendorf approved these changesAssigneesNo one assignedLabelsStatus: AcceptedReady to integrate (reviewed, tested)ProjectsNone yetMilestoneNo milestoneDevelopmentSuccessfully merging this pull request may close these issues.None yet17 participantsFooter\u00a9 2023 GitHub, Inc.Footer navigationTermsPrivacySecurityStatusDocsContact GitHubPricingAPITrainingBlogAbout",
    "summary": "- ZFS 2.2.0 (RC) has introduced a new feature called Block Cloning, which allows users to clone a file (or a subset of its blocks) into another file without copying the data itself.\n- Block Cloning is different from deduplication, as it requires manually using a dedicated system call to clone the file, whereas deduplication is automatic.\n- Block Cloning operates exclusively on metadata and does not require reading or writing data, making it extremely fast and efficient.",
    "hn_title": "ZFS 2.2.0 (RC): Block Cloning merged",
    "original_title": "ZFS 2.2.0 (RC): Block Cloning merged",
    "score": 255,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginZFS 2.2.0 (RC): Block Cloning merged (github.com/openzfs)255 points by turrini 18 hours ago | hide | past | favorite | 133 commentsalbertzeyer 17 hours ago | next [\u2013]Also see this issue: https://github.com/openzfs/zfs/issues/405> It is in FreeBSD main branch now, but disabled by default just to be safe till after 14.0 released, where it will be included. Can be enabled with loader tunable there.> more code is needed on the ZFS side for Linux integration. A few people are looking at it AFAIK.replyludde 14 hours ago | prev | next [\u2013]A M A Z I N GHave been looking forward to this for years!This is so much better than automatically doing dedup and the RAM overhead that entails.Doing offline/RAM+in memory dedup size optimizations seem like a really good optimization path. In the spirit of also paying only what you use and not the rest.Edit: What's the RAM overhead of this? Is it ~64B per 128kB deduped block or what's the magnitude of things?replymlyle 12 hours ago | parent | next [\u2013]> Edit: What's the RAM overhead of this? Is it ~64B per 128kB deduped block or what's the magnitude of things?No real memory impact. There's a regions table that uses 128k of memory per terabyte of total storage (and may be a bit more in the future). So for your 10 petabyte pool using deduping, you'd better have an extra gigabyte of RAM.But erasing files can potentially be twice as expensive in IOPS, even if not deduped. They try to prevent this.replyludde 10 hours ago | root | parent | next [\u2013]If I manually deduplicate/copy a 1GB file, how many such offset/refcount tuples would be created in RAM? Just one for the whole file, or one per underlying 128kB block?replymlyle 3 hours ago | root | parent | next [\u2013]They don't go in ram. You have one 64 bit refcount number for every 64MB of storage irrespective of how much is deduplicated; this is a reduction factor of 8M, or 128k per terabyte stored.This largely exists so you can avoid doing extra work on free.replyrincebrain 10 hours ago | root | parent | prev | next [\u2013]n.b. I am not pjd, this is from memory and may be wrong.The answer to that is messy, but basically, there's a table that should be kept in memory for \"is there anything reflinked at all in this logical range on disk\", and that covers large spans, so how many entries would depend on how contiguous your data logically was on disk; the actual precise mapping list per-vdev doesn't need to be kept continuously in memory, just the more coarse table, so that saves you a fair bit on memory requirements.replylockhouse 17 hours ago | prev | next [\u2013]Anyone here using ZFS in production these days? If so what OS and implementation? What have been your experiences or gotchas you experienced?replytrws 16 hours ago | parent | next [\u2013]I\u2019m not a filesystem admin, but we at LLNL use OpenZFS as the storage layer for all of our Lustre file systems in production, including using raid-z for resilience in each pool (order 100 disks each), and have for most of a decade. That combined with improvements in Lustre have taken the rate of data loss or need to clear large scale shared file systems down to nearly zero. There\u2019s a reason we spend as many engineer hours as we do maintaining it, it\u2019s worth it.LLNL openzfs project: https://computing.llnl.gov/projects/openzfs Old presentation from intel with info on what was one of our bigger deployments in 2016 (~50pb): https://www.intel.com/content/dam/www/public/us/en/documents...replymuxator 12 hours ago | root | parent | next [\u2013]If I'm not mistaken the linux port of ZFS that later became OpenZFS started at LLNL and was a port from FreeBSD (it may have been release ~9).I believe it was called ZFS On Linux or something like that.Nice how things have evolved: from FreeBSD to linux and back. In my mind this has always been a very inspiring example of a public institution working for the public good.replyrincebrain 11 hours ago | root | parent | next [\u2013]FreeBSD had its own ZFS port.ZoL, if my ancient memory serves, was at LLNL, not based on the FreeBSD port (if you go _very_ far back in the commit history you can see Brian rebasing against OpenSolaris revisions), but like 2 or 3 different orgs originally announced Linux ports at the same time and then all pooled together, since originally only one of the three was going to have a POSIX layer (the other two didn't need a working POSIX filesystem layer). (I'm not actually sure how much came of this collaboration, I just remember being very amused when within the span of a week or two, three different orgs announced ports, looked at each other, and went \"...wait.\")Then for a while people developed on either the FreeBSD port, the illumos fork called OpenZFS, or the Linux port, but because (among other reasons) a bunch of development kept happening on the Linux port, it became the defacto upstream and got renamed \"OpenZFS\", and then FreeBSD more or less got a fresh port from the OpenZFS codebase that is now what it's based on.The macOS port got a fresh sync against that codebase recently and is slowly trying to merge in, and then from there, ???replydrewg123 17 hours ago | parent | prev | next [\u2013]We use ZFS in production for the non-content filesystems on a large, ever increasing, percentage of our Netflix Open Connect CDN nodes, replacing geom's gmirror. We had one gotcha (caught on a very limited set of canaries) where a buggy bootloader crashed part-way through boot, leaving the ZFS \"bootonce\" stuff in a funky state requiring manual recovery (the nodes with gmirror were fine, and fell back to the old image without fuss). This has since been fixed.Note that we do not use ZFS for content, since it is incompatible with efficient use of sendfile (both because there is no async handler for ZFS, so no async sendfile, and because the ARC is not integrated with the page cache, so content would require an extra memory copy to be served).replybakul 16 hours ago | root | parent | next [\u2013]Is integrating ARC with the page cache a lost cause? If not, may be Netflix can fund it!replypostmodest 15 hours ago | root | parent | next [\u2013]I would expect page cache to be one of those platform-dependent things that prevent OpenZFS from doing that. Especially on Linux, and especially because AFAIK the Linux version has the most eyes on it.replyrincebrain 14 hours ago | root | parent | next [\u2013]My understanding, not having dug into it, is that it's possible but just work nobody has done yet, though I'm not sure what the relevant interfaces are in the Linux kernel.One thing that makes the interfaces in Linux much messier than the FreeBSD ones is that a lot of the core functionality you might like to leverage in Linux (workqueues, basically anything more complicated than just calling kmalloc, and of course any SIMD save/restore state, to name three examples I've stumbled over recently) are marked EXPORT_SYMBOL_GPL or just entirely not exported in newer releases, so you get to reimplement the wheel for those, whereas on FreeBSD it's trivial to just use their implementations of such things and shim them to the Solaris-ish interfaces the non-platform-specific code expects.So that makes the Linux-specific code a lot heavier, because upstream is actively hostile.reply5e92cb50239222b 2 hours ago | root | parent | next [\u2013]Sun has picked the license specifically to make the project incompatible with the Linux kernel. If you start out with hostility, what treatment do you expect in return?replydepr 1 hour ago | root | parent | next [\u2013]Surely you have a source for that that isn't someone else's comment.replyDylan16807 13 hours ago | root | parent | prev | next [\u2013]> SIMD save/restore stateI wish someone would come in and convince the kernel devs that \"hey, if you want EXPORT_SYMBOL_GPL to have legal weight in a copyleft sense then you can't just slap it onto interfaces for political reasons\"replyrincebrain 13 hours ago | root | parent | next [\u2013]I don't think they care about it having legal weight, that ship sailed long ago when they started advocating for just slapping SYMBOL_GPL on things out of spite; I think they care about excluding people from using their software.IMO Linus should stop being half-and-half about it and either mark everything SYMBOL_GPL and see how well that goes or stop this nonsense.reply5e92cb50239222b 2 hours ago | root | parent | next [\u2013]We see how well \"including people into using your software\" works out for pushover licenses almost every week now, FreeBSD itself being the prime example, having lost three major companies one after another from its list of users, sponsors, and developers a couple of years ago. Not that there were many to begin with. What Linus and the gang are doing have been working out great over the long term (much better than any competition, at least).replyFilligree 12 hours ago | root | parent | prev | next [\u2013]I just don't understand why they're so anti-ZFS. I want my data to survive, please...replyrincebrain 12 hours ago | root | parent | next [\u2013]My impression is that some of the Linux kernel devs are anti-anything that's not GPL-compatible, of any sort, regardless of the particulars.Linus himself also made remarks about ZFS at one point that were pretty...hostile. [1] [2]> The fact is, the whole point of the GPL is that you're being \"paid\" in terms of tit-for-tat: we give source code to you for free, but we want source code improvements back. If you don't do that but instead say \"I think this is _legal_, but I'm not going to help you\" you certainly don't get any help from us.> So things that are outside the kernel tree simply do not matter to us. They get absolutely zero attention. We simply don't care. It's that simple.> And things that don't do that \"give back\" have no business talking about us being assholes when we don't care about them.> See?Note that there's at least one unfixed Linux kernel bug that was found by OpenZFS users, reproducible without using OpenZFS in any way, reported with a patch, and ignored. [3]So \"not giving back\" is a dubious claim.[1] - https://arstechnica.com/gadgets/2020/01/linus-torvalds-zfs-s...[2] - https://www.realworldtech.com/forum/?threadid=189711&curpost...[3] - https://bugzilla.kernel.org/show_bug.cgi?id=212295replyzokula 3 hours ago | root | parent | prev | next [\u2013]ZFS was made to be Linux hostile, the kernel devs are just returning the favor.replyolgeni 1 hour ago | root | parent | next [\u2013]Was ifconfig made to be hostile too? :D(/s)replycolonwqbang 12 hours ago | root | parent | prev | next [\u2013]Why don't you think it has legal weight? Or did you mean something else?As far as know the point of EXPORT_SYMBOL_GPL was to push back on companies like Nvidia who wanted to exploit loopholes in the GPL. That seems to me like a reasonable objective.Relevant Torvalds quote: https://yarchive.net/comp/linux/export_symbol_gpl.htmlreplyrincebrain 12 hours ago | root | parent | next [\u2013]Sure, and that alone isn't an unreasonable premise - as he says, intent matters.But if you're marking interfaces as GPL-only, or implementing taint detection that means if you use a non-SYMBOL_GPL kernel symbol which calls a GPL-only function it treats the non-SYMBOL_GPL symbol as GPL-only and blocks your linking, it gets a bit out of hand.Building the kernel with certain kernel options makes modules like OpenZFS or OpenAFS not link because of that taint propagation - because things like the lockdep checker turn uninfringing calls into infringing ones.Or a little while ago, there was a change which broke building on PPC because a change made a non-SYMBOL_GPL call on POWER into a SYMBOL_GPL one indirectly, and when the original author was contacted, he sent a patch reverting the changed symbol, and GregKH refused to pull it into stable, suggesting distros could carry it if they wanted to. (Of course, he had happily merged a change into -stable earlier that just implemented more aggressive GPL tainting and thereby broke things like the aforementioned...)replyPlutoIsAPlanet 12 hours ago | root | parent | next [\u2013]The Linux kernel has never supported out of tree modules like how ZFS works out of tree.All ZFS needs to do is just have one of Oracles many lawyers say \"CDDL is compatible with GPL\". Yet, they Oracle don't.replyrincebrain 12 hours ago | root | parent | next [\u2013]\"All.\"It's explicitly not compatible with GPL, though. It has clauses that are more restrictive than GPL, and IIRC some people who contributed to the OpenZFS project did so explicitly without allowing later CDDL license revisions, which removes Oracle's ability to say CDDL-2 or whatever is GPL-compatible.So even if someone rolled up dumptrucks of cash and convinced Oracle that everything was great, they don't have all the control needed to do that.replyDylan16807 12 hours ago | root | parent | prev | next [\u2013]To have legal weight, it has to be a signal that you're implementing something that is derivative of kernel code. That's the directly stated intent of EXPORT_SYMBOL_GPL.But \"call an opaque function that saves SIMD state\" is obviously not derivative of the kernel code in any way. The more exports that get badly marked this way, the more EXPORT_SYMBOL_GPL becomes indistinguishable from EXPORT_SYMBOL.replycolonwqbang 12 hours ago | root | parent | next [\u2013]I see it as just a kind of \"warranty void if seal broken\". Don't do this or you may be in violation of the GPL. Maybe a legal court in $country would find in your favour (I'm not convinced it's as clear cut as you imply). Maybe they would find that you willfully infringed, despite the kernel devs clearly warning you not to do it.The main \"legal effect\" I see is that you are not willing to take that risk, just like Oracle isn't.replybakul 12 hours ago | root | parent | prev | next [\u2013]I suspect the underlying issues for not unifying the two have more to do with the ZFS design than anything to do with Linux. It may be the codebase is far too large at this stage to make such a fundamental change.replyrincebrain 12 hours ago | root | parent | next [\u2013]I don't think so. The memory management stuff is pretty well abstracted; on FBSD it just glues into UMA pretty transparently, it's just on Linux there's a lot of machinery for implementing our own little cache allocating because Linux's kernel cache allocator is very limited in what sizes it will give you, and sometimes ZFS wants 16M (not necessarily contiguous) regions because someone said they wanted 16M records.The ZoL project lead said at one point there were a variety of reasons this wasn't initially done for the Linux integration [1], but that it was worth taking another look at since that was a decade ago now. Having looked at the Linux memory subsystems recently for various reasons, I would suspect the limiting factor is that almost all the Linux memory management functions that involve details beyond \"give me X pages\" are SYMBOL_GPL, so I suspect we couldn't access whatever functionality would be needed to do this.I could be wrong, though, as I wasn't looking at the code for that specific purpose, so I might have missed functionality that would provide this.[1] - https://github.com/openzfs/zfs/issues/10255#issuecomment-620...replybakul 11 hours ago | root | parent | next [\u2013]Behlendorf's comment in that thread seems to be talking about linux integration. My point was this is an older issue, going back to the Sun days. See for instance this thread in where McVoy complains about the same issue! https://www.tuhs.org/pipermail/tuhs/2021-February/023013.htm...replyrincebrain 11 hours ago | root | parent | next [\u2013]That seems more like it's complaining about it not being the actual page cache, not it not being counted as \"cache\", which is a larger set in at least Linux than just the page cache itself.But sure, it's certainly an older issue, and given that the ABD rework happened, I wouldn't put anything past being \"feasible\" if the benefits were great enough.(Look at the O_DIRECT zvol rework stuff that's pending (I believe not merged) for how a more cut-through memory model could be done, though that has all the tradeoffs you might expect of skipping the abstractions ZFS uses to minimize the ability of applications to poke holes in the abstraction model and violate consistency, I believe...)replythe8472 12 hours ago | root | parent | prev | next [\u2013]Could the linux integration use dax[0] to bypass the page cache and go straight to ARC?[0] https://www.kernel.org/doc/Documentation/filesystems/dax.txtreplyxmodem 14 hours ago | root | parent | prev | next [\u2013]If you can share, what type of non-content data do the nodes store? Is this just OS+application+logs?replygigatexal 16 hours ago | root | parent | prev | next [\u2013]This is amazing. A detail of Netflix that, I a plebe, wouldn\u2019t know if not for this site.replyComputerGuru 16 hours ago | root | parent | next [\u2013]Actually, Drew's presentations about Netflix, FreeBSD, ZFS, saturating high-bandwidth network adapters, etc. are legendary and have been posted far and wide. But having him available to answer questions on HN just takes it to a whole 'nother level.replydrewg123 15 hours ago | root | parent | next [\u2013]You're making me blush.. But, to set the record straight: I actually know very little about ZFS, beyond basic user/admin knowledge (from having run it for ~15 years). I've never spoken about it, and other members of the team I work for at Netflix are far more knowledgeable about ZFS, and are the ones who have managed the conversion of our fleet to ZFS for non-content partitions.replyComputerGuru 13 hours ago | root | parent | next [\u2013]I\u2019ve devoured your FreeBSD networking presentations but I guess I must have confused a post about tracking down a ZFS bug in production written by someone else with all the other content you\u2019ve produced.Back to the topic at hand, it\u2019s actually scary how few software expose control over whether or not sendfile is used, assuming support is only a matter of OS and kernel version but not taking into account filesystem limitations. I ran into a terrible Samba on FreeBSD bug (shares remotely disconnected and connections reset with moderate levels of concurrent ro access from even a single client) that I ultimately tracked down to sendfile being enabled in the (default?) config - so it wasn\u2019t just the expected \u201cperformance requirements not being met\u201d with sendfile on ZFS but even other reliability issues (almost certainly exposing a different underlying bug, tbh). Imagine if Samba didn\u2019t have a tubeable to set/override sendfile support, though.replygigatexal 13 hours ago | root | parent | prev | next [\u2013]Have they ever blogged or spoke at conferences about it? I soak up all that content -- least I try to.replythrow0101c 16 hours ago | root | parent | prev | next [\u2013]Any use of boot environments for easy(er?) rollbacks of OS updates?replydrewg123 15 hours ago | root | parent | next [\u2013]Yes. That's the bootonce thing I was talking about. When we update the OS, we set the \"bootonce\" flag via bectl activate -t to ensure we fall back to the previous BE if the current BE is borked and not bootable. This is the same functionality we had by keeping a primary and secondary root partition in geom a and toggling the bootable partition via the bootonce flag in gpart.replylifty 16 hours ago | root | parent | prev | next [\u2013]what do you use for the content filesystem?replydrewg123 16 hours ago | root | parent | next [\u2013]FreeBSD's UFSreplymgoetzke 3 hours ago | parent | prev | next [\u2013]We use it with SmartOS on hypervisors running customer Bhyve machines. ZFS (and smartos) works really really well. .The fact how I can replicate live data of arbitrary size (many TB sized filesystems) in small chunks to other hosts every minute greatly increased my deep sleep quality. Of course databases with multi-host write etc are nice, but in our use case, all customers are rather small with just lots and lots of files (medical and otherwise), the database itself is rather small and doesn't need replication.Best thing, on the receiver side of the backup ZFS ensures due to its architecture that the diff is directly applied on top of the existing filesystem, while in normal differential backups one might find out months or years later that one diff snapshot was damaged in transfer or is not accessible.zfs scrubbing with S.M.A.R.T monitoring also helps a lot to ensure drive quality over time.# GotchasZFS:- There is no undo etc, this is unix, so beware of wrong commands. - ZFS Scrubbing can be stopped (it does sometimes affect io speed), but ZFS resilvering cannot. This can lead to performance issues. - There must be enough RAM for the caching to work well and synchronous workloads do well with good write cache drives (ZIL) - Data usage patterns should fit well with the Append Log schema of ZFS. E.g databases such as LevelDB worked really well. Others are not slow, but need a good ZIL more then when the pattern fits.SmartOS: Some minor gotchas with how vmadm deletes zfs filesystems or in general with SmartOS, e.g when having too many snapshots, but everything quite predictable.replynoaheverett 17 hours ago | parent | prev | next [\u2013]Running in production for about 3 years with Ubuntu 20.04 / zfs 0.8.3. ZFS is being used as the datastore for a cluster of LXD/LXC instances over multiple physical hosts. I have the OS setup on its own dedicated drive and ZFS striped/cloned over 4 NVMe drives.No gotchas / issues, works well, easy to setup.I am looking forward to the Direct IO speed improvements for NVMe drives with https://github.com/openzfs/zfs/pull/10018edit: one thing I forgot to mention is, when creating your pool make sure to import your drives by ID (zpool import -d /dev/disk/by-id/ <poolname>) instead of name in case name assignments change somehow [1][1] https://superuser.com/questions/1732532/zfs-disk-drive-lette...replythrow0101c 16 hours ago | root | parent | next [\u2013]> I am looking forward to the Direct IO speed improvements for NVMe drives with https://github.com/openzfs/zfs/pull/10018See also \"Scaling ZFS for NVMe\" by Allan Jude at EuroBSDcon 2022:* https://www.youtube.com/watch?v=v8sl8gj9UnAreplynoaheverett 15 hours ago | root | parent | next [\u2013]Sweet, I appreciate the link!replydoctor_eval 9 hours ago | root | parent | prev | next [\u2013]Sorry if this is a newbie question - but what are you using to share the ZFS file system between physical hosts?I\u2019ve been out of this particular game for a long time.replyDrybones 16 hours ago | parent | prev | next [\u2013]We use ZFS on every server we deployWe typically use Proxmox. It\u2019s a convenient node host setup and usually has a very up to date zfs and it\u2019s stableI just wouldn\u2019t use the Proxmox web ui for zfs configuration. It doesn\u2019t have up to date options. Always configure zfs on the clireplytlamponi 1 hour ago | root | parent | next [\u2013]> I just wouldn\u2019t use the Proxmox web ui for zfs configuration. It doesn\u2019t have up to date options.What options are missing for you?Would be great if you could open an enhancement request over at https://bugzilla.proxmox.com/ for tracking this (no promises on (immediate) implementation though).replymnw21cam 1 hour ago | parent | prev | next [\u2013]Yeah, a few years ago I steered our university research group towards using ZFS for a couple of new servers with 1.5PB storage each (2.0PB before raidz2), on RHEL (would have preferred Debian). Absolutely no problems with the system. The IT guys were a bit doubtful whether it would work well, for instance they insisted that XFS would be better at managing parallel requests, but when the system happily saturates a 10Gb ethernet with plenty of bandwidth to spare I don't think there's a problem. We have had ZFS flag up one hard drive so far that was returning false data, which was then hot-swapped out. Other filesystems would probably have just suffered data corruption without us knowing it.replyopk 1 hour ago | parent | prev | next [\u2013]My workplace has been using ZFS for years. I've yet to see an alternative come close to being compelling. We use OpenZFS on all our RHEL servers and even workstations with XFS for root partitions and smaller clients. Our primary file server is Solaris 11 but is coming to the end of its long life. The plan is to use FreeBSD on the replacement. ZFS has become so valuable to us that it alone can be a major factor in directing our other choices.Is nice to see it advancing in useful ways. Hopefully this leads to offline dedup without the runtime memory costs.replybenlivengood 11 hours ago | parent | prev | next [\u2013]\"production\" at home on Debian 11, previously on FreeBSD 10-13. The weirdest gotcha has been related to sending encrypted raw snapshots to remote machines[0],[1]. These have been the first instabilities I had with ZFS in roughly 15 years around the filesystem since switching to native encryption this year. Native encryption seems to be barely stable for production use; no actual data corruption but automatic synchronization (I use znapzend) was breaking frequently. Recent kernel updates fixed my problem although some of the bug reports are still open. I only moved on from FreeBSD because of more familiarity with Linux.A slightly annoying property of snapshots and clones is the inability to fully re-root a tree of snapshots, e.g. permanently split a clone from its original source and allow first-class send/receive from that clone. The snapshot which originated the clone needs to stick around forever[2]. This prevents a typical virtual machine imagine process of keeping a base image up to date over time that VMs can be cloned from when desired and eventually removing the storage used by the original base image after e.g. several OS upgrades.I don't have any big performance requirements and most file storage is throughput based on spinning disks which can easily saturate the gigabit network.I also use ZFS on my laptop's SSD under Ubuntu with about 1GB/s performance and no shortage of IOPS and the ability to send snapshots off to the backup system which is pretty nice. Ubuntu is going backwards on support for ZFS and native encryption uses a hacky intermediate key under LUKS, but it works.[0] https://github.com/openzfs/zfs/issues/12014 [1] https://github.com/openzfs/zfs/issues/12594 [2]https://serverfault.com/questions/265779/split-a-zfs-clonereplynightfly 17 hours ago | parent | prev | next [\u2013]Yes, Ubuntu 20.04 and 22.04. But we've been running ZFS in some form or other for 10+ years. ACL support not as good/easy to use as Solaris/FreeBSD. Not having weird pathological performance issues with kernel memory allocation like we had with FreeBSD though. Sometimes we have issues with automatic pool import on boot, so that's something to be careful with. The tooling is great though, and we've never had catastrophic failure that was due to ZFS, only due to failing hardware.replyDatagenerator 3 hours ago | parent | prev | next [\u2013]In production since the first Release of FreeBSD in our Research company. We layered our storage to disconnect hardware Lifecycles from OS. The spinning disks are pooled together using ScaleIO and provided to FreeBSD on ESX as physical devices which live forever through each EOL cycle of the hardware every five up to eight years. We have been serving ten PetaBytes for 15 years without hiccups. We expose SMB3/NFS4 and S3 and have nightly DR to mirror sites using ZFS binary difference Send and Receive cronjobs which are fully automated and monitored. Basically the ScaleIO SDS servers can be replaced during daytime while the upper layer keeps going on for HPC and thousands of other jobs. Since hardware provides more capacity after each Lifecycle we can increase the ZFS pool on the fly and have the most profound 24/7 storage environment without any major interruptions for 15 YEARS and counting. Our partners are having multiple outages due Lustre and other problems, my 2creplyenneff 12 hours ago | parent | prev | next [\u2013]I use ZFS on Debian for my home file server. The setup is just a tiny NUC with a couple of large USB hard drives, mirrored with ZFS. I\u2019ve had drives fail and painlessly replaced and resilvered them. This is easily the most hassle free file storage setup I\u2019ve owned; been going strong over 10 years now with little to no maintenance.replyrsaxvc 9 hours ago | root | parent | next [\u2013]Pretty much same setup here except I used a Lacie atom NAS, added a USB stick to the motherboard for booting.replyjustinclift 16 hours ago | parent | prev | next [\u2013]TrueNAS (www.truenas.com) uses ZFS for the storage layer across it's product range (storage software).They have both FreeBSD and Linux based stuff, targeting different use cases.replyexplorer83 4 hours ago | root | parent | next [\u2013]I use TrueNAS (FreeBSD) version. ZFS makes keeping data snapshots and external backups a fairly easy process. It really cut down on server data management time. The only issue I found with it is there was a now long patched SMB bug a few years ago in the TrueNAS software that caused my storage pool to become corrupted after a large transfer. And there was no way for me to recover the data. It was all lost despite the disks still spinning and presumably still having most and the 1 and 0s recoverable.replyjustinclift 6 minutes ago | root | parent | next [\u2013]Hopefully you had backups?replycrest 16 hours ago | parent | prev | next [\u2013]I use it as my default file system on FreeBSD. It was rough in FreeBSD 7.x (around 2009), but starting with FreeBSD 8.x it has been rock solid to this day. The only gotcha (which the documentation warns about) has been that automatic block level deduplication is only useful in a few special applications and has a large main memory overhead unless you can accept terrible performance for normal operations (e.g. a bandwidth limited offsite backup).replySkyMarshal 16 hours ago | parent | prev | next [\u2013]Not in production, but using ZoL on my personal workstations. https://zfsonlinux.org/Some discussion: https://www.reddit.com/r/NixOS/comments/ops0n0/big_shoutout_...replyshrubble 16 hours ago | parent | prev | next [\u2013]The gotcha on Proxmox is that you can't do swapfiles on ZFS, so if your swap isn't made big enough when installing and you format everything as ZFS you have to live with it or do odd workarounds.replytlamponi 1 hour ago | root | parent | next [\u2013]That's not a gotcha on Proxmox projects but a gotcha everywhere, SWAP on ZFS needs a lot of special care to avoid that it needs to allocate memory during swapping memory out (e.g., if low on memory), causing hard-freezes.It can be made somewhat[0] stable with using no (or a simple compression like ZLE), avoiding log devices and caches it can work, but it's way simpler and guaranteed stable to just use a separate partition.[0] even setting the myriad of options there still exist reports about hangs, like https://github.com/openzfs/zfs/issues/7734#issuecomment-4167...replycrabbone 10 hours ago | parent | prev | next [\u2013]We don't use ZFS as a filesystem, we use it to aggregate multiple EBS into a single pool, and present it to... stock Ubuntu (one found in AWS marketplace) as a single big block device.We also use ZFS for testing of our own block device. My biggest problems with it are related to this activity. Hot-plugging or removing block devices from the pool often leads to unrepeatable pool, and the tests have to be scrambled because the whole system needs to be rebooted.replyDvdGiessen 14 hours ago | parent | prev | next [\u2013]In production on SmartOS (illumos) servers running applications and VM's, on TrueNAS and plain FreeBSD for various storage and backups, and on a few Linux-based workstations. Using mirrors and raidz2 depending on the needs of the machines.We've successfully survived numerous disk failures (a broken batch of HDD's giving all kinds of small read errors, an SSD that completely failed and disappeared, etc), and were in most cases able to replace them without a second of downtime (would have been all cases if not for disks placed in hard-to-reach places, now only a few minutes downtime to physically swap the disk).Snapshots work perfectly as well. Systems are set up to automatically make snapshots using [1], on boot, on a timer, and right before potentially dangerous operations such as package manager commands as well. I've rolled back after botched OS updates without problems; after a reboot the machine was back in it's old state. Also rolled back a live system a few times after a broken package update, restoring the filesystem state without any issues. Easily accessing old versions of a file is an added bonus which has been helpful a few times.Send/receive is ideal for backups. We are able to send snapshots between machines, even across different OSes, without issues. We've also moved entire pools from one OS to another without problems.Knowing we have automatic snapshots and external backups configured also allows me to be very liberal with giving root access to inexperienced people to various (non-critical) machines, knowing that if anything breaks it will always be easy to roll back, and encouraging them to learn by experimenting a bit, to the point where we can even diff between snapshots to inspect what changed and learn from that.Biggest gotchas so far have been on my personal Arch Linux setup, where the out-of-tree nature of ZFS has caused some issues like a incompatible kernel being installed, the ZFS module failing to compile, and my workstation subsequently being unable to boot. But even that was solved by my entire system running on ZFS: a single rollback from my bootloader [2] and all was back the way it was before.Having good tooling set up definitely helped a lot. My monkey brain has the tendency to think \"surely I got it right this time, so no need to make a snapshot before trying out X!\", especially when experimenting on my own workstation. Automating snapshots using a systemd timer and hooks added to my package manager saved me a number of times.[1]: https://github.com/psy0rz/zfs_autobackup [2]: https://zfsbootmenu.org/replycsdvrx 10 hours ago | root | parent | next [\u2013]> Systems are set up to automatically make snapshotsI do that with sqlite to keep a selection of snapshots from the last hours, days etc.https://github.com/csdvrx/zfs-autosnapshotreplyunixhero 16 hours ago | parent | prev | next [\u2013]Yes. Using latest ZFS ZFS On Linux distrib on Debian. Using Proxmox. Never had and problems ever, ever.replycyrnel 7 hours ago | parent | prev | next [\u2013]I use it in Kubernetes via https://github.com/openebs/zfs-localpvThe PersistentVolume API is a nice way to divvy up a shared resource across different teams, and using ZFS for that gives us the snapshotting, deduplication, and compression for free. For our workloads, it benchmarked faster than XFS so it was a no-brainer.replyyjftsjthsd-h 16 hours ago | parent | prev | next [\u2013]Sure; we get good mileage out of compression and snapshots (well, mostly send-recv for moving data around rather than snapshots in-place). I think the only problems have been very specific to our install process (non-standard kernel in the live environment; if we used the normal distro install process it would be fine).replyquags 12 hours ago | parent | prev | next [\u2013]I have been using zfs for years from ubuntu 18. Easy snapshots, monitoring, choices for raid levels, and ability to very easily copy a dataset remotely with resume or incremental support is awesome. I mainly use it for kvm systems each with their own dataset. Coming from mdadm + lvm from my previous set up is night and day for doing snapshots and backups. I do not use zfs on root for ubuntu instead I do a raid1 software set up for the os and then a zfs set up on other disks - zfs on root was the only gotcha. For FreeBSD zfs on root works fine.replymattjaynes 12 hours ago | parent | prev | next [\u2013]ZFS on Linux has improved a lot in the last few years. We (prematurely) moved to using it in production for our MySQL data about 5 years ago and initially it was a nightmare due to unexplained stalling which would hang MySQL for 15-30 minutes at random times. I'm sure it shortened my life a few years trying to figure out what was wrong when everything was on fire. Fortunately, they have resolved those issues in the subsequent releases and it's been much more pleasant after that.replyszundi 16 hours ago | parent | prev | next [\u2013]Yes and it is awesome, no issues.replygigatexal 16 hours ago | parent | prev | next [\u2013]Any of the enterprise customers of klara Systems are likely ZFS production folks.https://klarasystems.com/?ampreplymiohtama 17 hours ago | prev | next [\u2013]What are applications that benefit from block cloning?replymustache_kimono 17 hours ago | parent | next [\u2013]Excited, because in addition to ref copies/clones, httm will use this feature, if available (I've already done some work to implement), for its `--roll-forward` operation, and for faster file recoveries from snapshots [0].As I understand it, there will be no need to copy any data from the same dataset, and this includes all snapshots. Blocks written to the live dataset can just be references to the underlying blocks, and no additional space will need be used.Imagine being able to continuously switch a file or a dataset back to a previous state extremely quickly without a heavy weight clone, or a rollback, etc.Right now, httm simply diff copies the blocks for file recovery and roll-forward. For further details, see the man page entry for `--roll-forward`, and the link to the httm GitHub below:  --roll-forward=\"snap_name\"  traditionally 'zfs rollback' is a destructive operation, whereas httm roll-forward is non-destructive. httm will copy only the blocks and file metadata that have changed since a specified snapshot, from that snapshot, to its live dataset. httm will also take two precautionary snapshots, one before and one after the copy. Should the roll forward fail for any reason, httm will roll back to the pre-execution state. Note: This is a ZFS only option which requires super user privileges.[0]: https://github.com/kimono-koans/httmreplyvovin 17 hours ago | parent | prev | next [\u2013]This is huge. One practical application is fast recovery of a file from past snapshot without using any additional space. I use ZFS dataset for my vCenter datastore (storing my vmdk files). In case of need to launch a clone from a past state one could use a block cloning to bring past vmdk file without the need to actually copy the file - it saves both space and time to make such clone.replybithavoc 14 hours ago | root | parent | next [\u2013]Can you elaborate a bit more on how you use ZFS with vCenter? How do you mount it?replyredundantly 9 hours ago | root | parent | next [\u2013]They're likely using a NAS (e.g., FreeNAS/TrueNAS) that uses ZFS for the underlying storage then sharing that storage by either NFS or iSCSI with their vSphere cluster. In some rare cases FC is being used instead of NFS or iSCSI.replyphilsnow 17 hours ago | parent | prev | next [\u2013]It seems kind of like hard linking but with copy-on-write for the underlying data, so you'll get near-instant file copies and writing into the middle of them will also be near-instant.All of this happens under the covers already if you have dedup turned on, but this allows utilities (gnu cp might be taught to opportunistically and transparently use the new clone zfs syscalls, because there is no downside and only upside) and applications to tell zfs that \"these blocks are going to be the same as those\" without zfs needing to hash all the new blocks and compare them.Aditionally, for finer control, ranges of blocks can be cloned, not just entire files.I can't tell from the github issue, can this manual dedup / block cloning be turned on if you're not already using dedup on a dataset? Last time I set up zfs, I was warned that dedup took gobs of memory, so I didn't turn it on.replyrincebrain 15 hours ago | root | parent | next [\u2013]It's orthogonal to dedup being on or off, and as someone else said, it's more or less the same underlying semantics you would expect from cp --reflink anywhere.Also, as mentioned, on Linux, it's not wired up with any interface to be used at all right now.replynabla9 17 hours ago | root | parent | prev | next [\u2013]Gnu cp --reflink.>When --reflink[=always] is specified, perform a lightweight copy, where the data blocks are copied only when modified. If this is not possible the copy fails, or if --reflink=auto is specified, fall back to a standard copy. Use --reflink=never to ensure a standard copy is performed.\"replythrill 17 hours ago | parent | prev | next [\u2013]FTFA: \"Block Cloning allows to clone a file (or a subset of its blocks) into another (or the same) file by just creating additional references to the data blocks without copying the data itself. Block Cloning can be described as a fast, manual deduplication.\"replydanudey 16 hours ago | parent | prev | next [\u2013]As others have said: block cloning (the underlying technology that enables copy-on-write) allows you to 'copy' a file without reading all of the data and re-writing it.For example, if you have a 1 GB file and you want to make a copy of it, you need to read the whole file (all at once or in parts) and then write the whole new file (all at once or in parts). This results in 1 GB of reads and 1 GB of writes. Obviously the slower (or more overloaded) your storage media is, the longer this takes.With block cloning, you simply tell the OS \"I want this file A to be a copy of this file B\" and it creates a new \"file\" that references all the blocks in the old \"file\". Given that a \"file\" on a filesystem is just a list of blocks that make up the data in that file, you can create a new \"file\" which has pointers to the same blocks as the old \"file\". This is a simple system call (or a few system calls), and as such isn't much more intensive than simply renaming a file instead of copying it.At my previous job we did builds for our software. This required building the BIOS, kernel, userspace, generating the UI, and so on. These builds required pulling down 10+ GB of git repositories (the git data itself, the checkout, the LFS binary files, external vendor SDKs), and then a large amount of build artifacts on top of that. We also needed to do this build for 80-100 different product models, for both release and debug versions. This meant 200+ copies of the source code alone (not to mention build artifacts and intermediate products), and because of disk space limitations this meant we had to dramatically reduce the number of concurrent builds we could run. The solution we came up with was something like:1. Check out the source code2. Create an overlayfs filesystem to mount into each build space3. Do the build4. Tear down the overlayfs filesystemThis was problematic if we weren't able to mount the filesystem, if we weren't able to unmount the filesystem (because of hanging file descriptors or processes), and so on. Lots of moving parts, lots of `sudo` commands in the scripts, and so on.Copy-on-write would have solved this for us by accomplishing the same thing; we could simply do the following:1. Check out the source code2. Have each build process simply `cp -R --reflink=always source/ build_root/`; this would be instantaneous and use no new disk space.3. Do the build4. `rm -rf build_root`Fewer moving parts, no root access required, generally simpler all around.replythe8472 12 hours ago | parent | prev | next [\u2013]Any copy command. On-demand deduplication managed by userspace.https://man7.org/linux/man-pages/man2/ioctl_fideduperange.2.... https://man7.org/linux/man-pages/man2/copy_file_range.2.html https://github.com/markfasheh/duperemovereplyaardvark179 17 hours ago | parent | prev | next [\u2013]It can be a really convenient way to snapshot something if you can arrange some point at which everything is synced to disk. Get to that point, make your new files that start sharing all their blocks, and then let your main db process (or whatever) continue on as normal.replyikiris 17 hours ago | parent | prev | next [\u2013]I think the big piece is native overlayfs so k8 setups get a bit simpler.replyrossmohax 14 hours ago | prev | next [\u2013]Does ZFS or any other FS offer special operations which DB engine like RocksDB, SQLite or PostgreSQL could benefit from if they decided to target that FS specifically?replymagicalhippo 14 hours ago | parent | next [\u2013]Internally, ZFS is kinda like an object store[1], and there was a project trying to expose the ZFS internals through an object store API rather than through a filesystem API.Sadly I can't seem to find the presentation or recall the name of the project.On the other hand, looking at for example RocksDB[2]:File system operations are not atomic, and are susceptible to inconsistencies in the event of system failure. Even with journaling turned on, file systems do not guarantee consistency on unclean restart. POSIX file system does not support atomic batching of operations either. Hence, it is not possible to rely on metadata embedded in RocksDB datastore files to reconstruct the last consistent state of the RocksDB on restart. RocksDB has a built-in mechanism to overcome these limitations of POSIX file system [...]ZFS does provide atomic operations internally[1], so if exposed it seems something like RocksDB could take advantage of that and forego all the complexity mentioned above.How much that would help I don't know though, but seems potentially interesting at first glance.[1]: https://youtu.be/MsY-BafQgj4?t=442[2]: https://github.com/facebook/rocksdb/wiki/MANIFESTreplydark-star 14 hours ago | prev | next [\u2013]Wow, I was under the impression that this had long been implemented already (as it's already in btrfs and other commercial file systems)Awesome!replymgerdts 13 hours ago | parent | next [\u2013]It has been in the Solaris version of zfs for a long time as well. This came a few years after the Oracle-imposed fork.https://blogs.oracle.com/solaris/post/reflink3c-what-is-it-w...replyrincebrain 10 hours ago | root | parent | next [\u2013]reflink is implemented on Oracle Solaris; how it works internally I don't think Oracle has ever commented, since implementing post-facto dedup on a filesystem that assumes data once written is unmodified is a bit Spicy(tm) - you can look at the BRT talk from the OpenZFS dev summit or other talks about it at the leadership meetings to get some idea why it's so exciting.replyuvatbc 15 hours ago | prev | next [\u2013]Technically, yes: through the use of Truenas that gives us API access to iscsi on ZFS.replyGauntletWizard 16 hours ago | prev | next [\u2013]> Note: currently it is not possible to clone blocks between encrypted datasets, even if those datasets use the same encryption key (this includes snapshots of encrypted datasets). Cloning blocks between datasets that use the same keys should be possible and should be implemented in the future.Once this is ready, I am going to subdivide my user homedir much more than it already is. The biggest obstacle in the way of this has been that it would waste a bunch of space until the snapshots were done rolling over, which for me is a long time (I keep weekly snapshots of my homedir for a year).replytecleandor 3 minutes ago | parent | next [\u2013]> Block Cloning is not limited to cloning blocks within the same dataset. It is possible (and very useful) to clone blocks between different datasets.But this is already a great advance, I love it :)replyyjftsjthsd-h 16 hours ago | parent | prev | next [\u2013]Is there a benefit to breaking up your home directory?replysomeplaceguy 15 hours ago | root | parent | next [\u2013]Not sure why you'd want to do that to your home directory usually, but it depends on what you store in it and how you use it, really.In general, breaking up a filesystem into multiple ones in ZFS is mostly useful for making filesystem management more fine-grained, as a filesystem/dataset in ZFS is the unit of management for most properties and operations (snapshots, clones, compression and checksum algorithms, quotas, encryption, dedup, send/recv, ditto copies, etc) as well as their inheritance and space accounting.In terms of filesystem management, there aren't many downsides to breaking up a filesystem (within reason), as most properties and the most common operations can be shared between all sub-filesystems if they are part of the same inherited tree (which doesn't necessarily have to correspond to the mountpoint tree!).As far as I know, the major downsides by far were that 1) you couldn't quickly move a file from one dataset to another, i.e. `mv` would be forced to do a full copy of the file contents rather than just do a cheap rename, and 2) in terms of disk space, moving a file between filesystems would be equivalent to copying the file and deleting the original, which could be terrible if you use snapshots as it would lead to an additional space consumption of a full new file's worth of disk space.In principle, both of these downsides should be fixed with this new block cloning feature and AFAIU the only tradeoffs would be some amount of increased overhead when freeing data (which should be zero overhead if you don't have many of these cloned blocks being shared anymore), and the low maturity of this code (i.e. higher chance of running into bugs) due to being so new.replyGauntletWizard 15 hours ago | root | parent | prev | next [\u2013]Controlling the rate and location of snapshots, mostly. I've broken out some kinds of datasets (video archives) but not others historically (music). It doesn't matter that much, but I want to split some more chunks out.replyyjftsjthsd-h 15 hours ago | root | parent | next [\u2013]Fair enough. I've personally slowly moved to a smaller number of filesystems, but if you're actually handling snapshots differently per-area then it makes sense (indeed, one of the reasons I'm consolidating is the realization that personally I'm almost never going to snapshot/restore things separately).replyvlovich123 17 hours ago | prev | next [\u2013]Do Btrfs or ext4 offer this?replywtallis 17 hours ago | parent | next [\u2013]This feature is basically the same as what underpins the reflink feature that btrfs has supported approximately forever and xfs has supported for at least several years.replymustache_kimono 17 hours ago | root | parent | next [\u2013]Does anyone know whether btrfs or XFS support reflinks from snapshot datasets?replyDylan16807 17 hours ago | root | parent | next [\u2013]I can confirm BTRFS yes, but note that source and destination need to be on the same mount point before kernel 5.18replydanudey 16 hours ago | root | parent | prev | next [\u2013]XFS doesn't have snapshot support, so the short answer there is no.replymustache_kimono 16 hours ago | root | parent | next [\u2013]Shows what I know about XFS. Thanks!replyPlutoIsAPlanet 12 hours ago | root | parent | next [\u2013]You can get psuedo-snapshots on XFS with a tool like https://github.com/aravindavk/reflink-snapshotBut, it still has to duplicate metadata which depending on the amount of files may cause inconsistency in the snapshot.replyplq 12 hours ago | root | parent | prev | next [\u2013]This is only a tangent given we are talking about snapshots and reflink, but just wanted to mention that LVM has snapshots, so if you need XFS snapshots, create the XFS filesystem on top of an LVM logical volume.replyComputerGuru 16 hours ago | root | parent | prev | next [\u2013]XFS doesn't have native snapshot support, though?replydsr_ 17 hours ago | parent | prev | next [\u2013]You can get a similar effect on top of any file system that supports hard links with rdfind ( https://rdfind.pauldreik.se/ ) -- but it's pretty slow.The Arch wiki says:\"Tools dedicated to deduplicate a Btrfs formatted partition include duperemove, bees, bedup and btrfs-dedup. One may also want to merely deduplicate data on a file based level instead using e.g. rmlint, jdupes or dduper-git. For an overview of available features of those programs and additional information, have a look at the upstream Wiki entry.Furthermore, Btrfs developers are working on inband (also known as synchronous or inline) deduplication, meaning deduplication done when writing new data to the filesystem. Currently, it is still an experiment which is developed out-of-tree. Users willing to test the new feature should read the appropriate kernel wiki page.\"replysomeplaceguy 16 hours ago | root | parent | next [\u2013]> You can get a similar effect on top of any file system that supports hard links with rdfind ( https://rdfind.pauldreik.se/ ) -- but it's pretty slow.It's a similar effect only if you don't modify the files, I think.If you \"clone\" a file with a hard link and you modify the contents of one copy, the other copy would also be equally modified.As far as I understand this wouldn't happen with this type of block cloning: each copy of the file would be completely separate, except that they may (transparently) share data blocks on disk.replythrtythreeforty 17 hours ago | parent | prev | next [\u2013]Btrfs yes, ext4 no (but I believe xfs does).This should end up being exposed through cp --reflink=always, so you could look up filesystem support for that.replydanudey 16 hours ago | root | parent | next [\u2013]XFS does, I've used it for specifically this feature before.replyKirillPanov 4 hours ago | prev | next [\u2013]Didn't btrfs have this like a ~decade ago?replyLeoPanthera 3 hours ago | parent | next [\u2013]Yes, although be careful, because running \"defragment\" will un-clone and therefore duplicate all your identical blocks.replyXorNot 10 hours ago | prev | next [\u2013]Finally! This is exactly what I want for handling photos and media on my home server. We have phone's dumping their camera rolls onto the machine via Syncthing, but the easiest way to share content safely is just to make copies of the file and go \"storage is cheap\". But the reality has always been closer to \"storage should handle deduplication\" - with this feature landing that can finally be a reality.EDIT: Not to mention, this should make ZFS best-in-class for handling container filesystems. Deduplicating all the files in the individual layers means you can pretty much dispense with awkwardly trying to maintain inheritance trees.replytecleandor 1 minute ago | parent | next [\u2013]I also was thinking about using this for something similar: organization. I have datasets with unorganized data dumps: photos, music, torrents...Organizing that in shares means duplicating files...replythinkloop 8 hours ago | parent | prev | next [\u2013]Would you mind expanding on that - what mechanism/app on the phone would share the files?replyPxtl 16 hours ago | prev [\u2013]Filesystem-level de-duplication is scary as hell as a concept, but also sounds amazing, especially doing it at copy-time so you don't have to opt-in to scanning to deduplicate. Is this common in filesystems? Or is ZFS striking out new ground here? I'm not really an under-the-OS-hood kinda guy.replywrs 13 hours ago | parent | next [\u2013]MacOS and BTRFS have had it for several years. In fact I believe it\u2019s the default behavior when copying a file in MacOS using the Finder (you have to specify `cp -c` in shell).replynijave 13 hours ago | root | parent | next [\u2013]Windows Server has had dedupe for at least 10 years, tooreplyyjftsjthsd-h 16 hours ago | parent | prev [\u2013]> Filesystem-level de-duplication is scary as hell as a conceptWhat's scary about it? You have to track references, but it doesn't seem that hard compared to everything else going on in ZFS et al.> Is this common in filesystems? Or is ZFS striking out new ground here? At least BTRFS does approximately the same.replycesarb 16 hours ago | root | parent | next [\u2013]> > Filesystem-level de-duplication is scary as hell as a concept> What's scary about it?It's scary because there's only one copy when you might have expected two. A single bad block could lose both \"copies\" at once.replyopk 59 minutes ago | root | parent | next [\u2013]You can do `zfs set copies=2` for a ZFS dataset if you think there's value in the extra copies. That's better than a copy of the file because with a single bad block, the checksum will fail and ZFS will retrieve the data from the good block and repair both files. In practice, using disk mirrors is better than setting the copies property.replyFilligree 15 hours ago | root | parent | prev | next [\u2013]Disks die all the time anyway. If you want to keep your data, you should have at least two-disk redundancy. In which case bad blocks won't kill anything.replymagicalhippo 13 hours ago | root | parent | prev | next [\u2013]Besides what all the others have mentioned, you can force ZFS to keep up to 3 copies of data blocks on a dataset. ZFS uses this internally for important metadata and will try to spread them around to maximize the chance of recovery, though don't rely on this feature alone for redundancy.replygrepfru_it 15 hours ago | root | parent | prev | next [\u2013]The file system metadata is redundant and on a correctly configured ZFS system your error correction is isolated and can be redundant as wellreplystormking 10 hours ago | root | parent | prev | next [\u2013]That's what data checksumming and mirroring is for.replyphpisthebest 14 hours ago | root | parent | prev | next [\u2013]3-2-1..3 Copies2 Media1 offsite.If you follow that then you would have no fear of data loss. if you are putting 2 copies on the same filesystem you are already doing backups wrongsreplyForkMeOnTinder 15 hours ago | root | parent | prev | next [\u2013]Copying a file isn't great protection against bad blocks. Modern SSDs, when they fail, tend to fail catastrophically (the whole device dies all at once), rather than dying one block at a time. If you care about the data, back it up on a separate piece of hardware.replyPxtl 15 hours ago | root | parent | prev [\u2013]> What's scary about it?Just that I'm trusting the OS to re-duplicate it at block level on file write. The idea that block by block you've got \"okay, this block is shared by files XYZ, this next block is unique to file Z, then the next block is back to XYZ... oh we're editing that one? Then it's a new block that's now unique to file Z too\".I guess I'm not used to trusting filesystems to do anything but dumb write and read. I know they abstract away a crapload of amazing complexity in reality, I'm just used to thinking of them as dumb bags of bits.replytonoto 1 hour ago | root | parent | next [\u2013]ZFS is COW with checksums so it wouldn't edit the same blocks and there is the possibility for sending snapshots to another pool (which may/may not use deduplication). Although, deduplication comes with a performance cost. I had all my photos spread out on various disks and external media, sometimes extra copies (as I did not trust certain disks) - if I remember it correctly I went from 3.6T to 2.2T usage by consolidating all my photos to a deduplicated pool. All fine, but the zpool wanted way more RAM and felt slower than my other pool. After I removed duplicates (with help of https://github.com/sahib/rmlint ), I migrated my photos to an ordinary zpool instead.replyDylan16807 13 hours ago | root | parent | prev [\u2013]If you're on ZFS you're probably using snapshots, so all that work is already happening.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- ZFS 2.2.0 (RC) introduces block cloning, a feature that allows files to be copied without actually duplicating the data, saving both time and space.\n- Block cloning is not limited to the same dataset and can be used to clone blocks between different datasets\n- Some applications, like file recovery from snapshots or fast copying of files, can benefit greatly from block cloning."
  }
]
