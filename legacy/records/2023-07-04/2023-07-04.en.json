[
  {
    "id": 36580192,
    "timestamp": 1688427104,
    "title": "Threads, an Instagram app",
    "url": "https://apps.apple.com/us/app/threads-an-instagram-app/id6446901002",
    "hn_url": "http://news.ycombinator.com/item?id=36580192",
    "content": "AppleStoreMaciPadiPhoneWatchAirPodsTV & HomeEntertainmentAccessoriesSupport0App Store PreviewThis app is available only on the App Store for iPhone.Threads, an Instagram app 12+Share ideas & trends with textInstagram, Inc.Pre-OrderFreeThis content may change without notice, and the final product may be different.iPhone ScreenshotsDescriptionSay more with Threads \u2014 Instagram\u2019s text-based conversation appThreads is where communities come together to discuss everything from the topics you care about today to what\u2019ll be trending tomorrow. Whatever it is you\u2019re interested in, you can follow and connect directly with your favorite creators and others who love the same things \u2014 or build a loyal following of your own to share your ideas, opinions and creativity with the world.moreApp PrivacySee DetailsThe developer, Instagram, Inc., indicated that the app\u2019s privacy practices may include handling of data as described below. For more information, see the developer\u2019s privacy policy.Data Linked to YouThe following data may be collected and linked to your identity:Health & FitnessPurchasesFinancial InfoLocationContact InfoContactsUser ContentSearch HistoryBrowsing HistoryIdentifiersUsage DataSensitive InfoDiagnosticsOther DataPrivacy practices may vary, for example, based on the features you use or your age. Learn MoreInformationSellerInstagram, Inc.Size254.3 MBCategorySocial NetworkingCompatibilityiPhoneRequires iOS 14.0 or later.iPod touchRequires iOS 14.0 or later.LanguagesEnglish, Croatian, Czech, Danish, Dutch, Finnish, French, German, Greek, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Malay, Norwegian Bokm\u00e5l, Polish, Portuguese, Romanian, Russian, Simplified Chinese, Slovak, Spanish, Swedish, Tagalog, Thai, Traditional Chinese, Turkish, Ukrainian, VietnamesemoreAge Rating12+Infrequent/Mild Alcohol, Tobacco, or Drug Use or ReferencesInfrequent/Mild Profanity or Crude HumorInfrequent/Mild Sexual Content and NudityInfrequent/Mild Mature/Suggestive ThemesCopyright\u00a9 2023 Instagram, LLC.PriceFreeApp SupportPrivacy PolicyMore By This DeveloperInstagramPhoto & VideoLayout from InstagramPhoto & VideoMore ways to shop: Find an Apple Store or other retailer near you. Or call 1-800-MY-APPLE.Choose your country or regionCopyright \u00a9 2023 Apple Inc. All rights reserved.Privacy Policy Terms of Use Sales and Refunds Legal Site Map",
    "summary": "- Threads is a new text-based conversation app developed by Instagram.\n- It allows users to connect with their favorite creators and discuss various topics of interest.\n- Users can follow others and share their own ideas and opinions with the world.",
    "hn_title": "Threads, an Instagram app",
    "original_title": "Threads, an Instagram app",
    "score": 715,
    "hn_content": "- Meta's new app, Threads, is predicted to have a significant impact on Twitter and potentially replace it.\n- Some users believe that Meta understands what made Twitter great, while others believe they do not.\n- Meta's use of Instagram branding and logins suggests they may be focused on acquiring user data.\n- The success of Meta's previous new standalone social apps, like Poke, is questionable.\n- Meta is targeting the features that made Twitter successful, such as easy-to-digest brands and direct access to news from journalists and celebrities.\n- Twitter has different user cohorts, including scrollers, \"industrial producers,\" and \"communitarians.\"\n- Mastodon, a social platform similar to Twitter, is preferred by some users due to its focus on tribe-specific content.\n- User safety and anonymity are concerns for some groups, especially the LGBT community and pro-Ukraine users.\n- Meta's successful advertising platform may help drive the success of Threads, but some users are not interested in following brands on the platform.\n- Twitter has faced criticism for being a \"disgusting totalitarian political censorship and propaganda tool\" and for shadow-banning accounts.\n- Meta's track record of copying features from competitors and their dominance in the advertising space may contribute to the success of Threads.\n- The future success of Threads is uncertain, as the growth of Twitter has stagnated, and Meta may struggle to attract new users.\n- The potential impact of Threads on Twitter's revenue and the possibility of antitrust issues are points of contention.\n- Users are divided on whether Musk's involvement will benefit or harm Twitter, with some accusing him of suppressing liberal discourse.\n- Meta's focus on profitability and ability to run experiments quickly may contribute to the success of Threads.\n- Some users believe that Meta understands what made Twitter great and will successfully launch Threads, while others doubt their ability to do so.\n- The success of Threads may come from its integration with the existing Instagram user base, but there are doubts about its appeal to new users.\n- Users are interested in seeing if Meta can create a new Twitter that captures what made it great in the past.\n- Several factors, such as the quality of the advertising platform and Meta's track record of launching new products, will contribute to the success or failure of Threads.",
    "hn_summary": "- Meta's new app, Threads, is predicted to have a significant impact on Twitter and potentially replace it.\n- Some users believe that Meta understands what made Twitter great and will successfully launch Threads, while others doubt their ability to do so.\n- The success of Threads may come from its integration with the existing Instagram user base, but there are doubts about its appeal to new users."
  },
  {
    "id": 36575003,
    "timestamp": 1688400663,
    "title": "Goodreads was the future of book reviews, then Amazon bought it",
    "url": "https://www.washingtonpost.com/technology/2023/07/01/amazon-goodreads-elizabeth-gilbert/",
    "hn_url": "http://news.ycombinator.com/item?id=36575003",
    "content": "Amazon's Kindle Scribe in 2022. (Chris Velazco/The Washington Post)Listen9 minComment621Gift ArticleShareSAN FRANCISCO \u2014 Goodreads \u2014 an Amazon-owned review site beloved by the bookish \u2014 has grown beleaguered.The site is built on outdated technological infrastructure, which made the cost of overhauling and updating it a challenge that was ultimately not worth it for the e-commerce giant, according to former employees who spoke on the condition of anonymity to discuss sensitive matters. Meanwhile, limited manual content moderation and a lack of protective features allow users to engage in targeted harassment known as \u201creview bombing\u201d \u2014 behavior that has resulted in the cancellation of books and their authors.Tech is not your friend. We are. Sign up for The Tech Friend newsletter.Former employees said Amazon seemed happy to mine Goodreads for its user-generated data and otherwise let it limp along with limited resources. In Amazon\u2019s more than 20-year history, the company has made dozens of acquisitions, and it is not unusual for it to try to cheaply acquire properties in markets that it wants to dominate, only to let them languish. Until recently, Amazon owned Book Depository and camera-enthusiast favorite DPReview, and it still operates discount marketplace Woot, collectibles website AbeBooks and movie database IMDb.AdvertisementGoodreads \u201chasn\u2019t been all that well maintained, or updated, or kept up with what you would expect from social communities or apps in 2023,\u201d said Jane Friedman, a publishing industry consultant. \u201cIt does feel like Amazon bought it and then abandoned it.\u201dAmazon spokesperson Ashely Vanicek said that \u201cBy joining Amazon, Goodreads has accelerated their mission to delight customers with the help of Amazon\u2019s resources and technology.\u201d\u201cWe continue investing and growing Goodreads as a community for readers and authors,\u201d she said in a statement, \"and have created opportunities for Amazon and Goodreads to invent new services for readers and authors alike.\u201dThe most recent high-profile incident involving Goodreads came when Elizabeth Gilbert \u2014 author of the best-selling memoir \u201cEat, Pray, Love\u201d \u2014 canceled the release of her forthcoming novel after it was flooded with one-star ratings. None of the people leaving negative reviews had actually read the book, which wasn\u2019t slated to come out until February of next year.But because some readers felt the book\u2019s setting \u2014 1930s Russia \u2014 was inappropriate in light of the ongoing war in Ukraine, they used Goodreads ratings as a way to express their frustration. Gilbert later announced on Instagram that it\u2019s \u201cnot the time for this book to be published.\u201dAdvertisementIt\u2019s common practice in the publishing world to release early copies of books to both readers and professional critics to generate pre-publication buzz. But Goodreads allows any user, not just those who\u2019ve received advance copies, to leave ratings months before books are released. Authors who\u2019ve become targets of review-bombing campaigns say there\u2019s little moderation or recourse to report the harassment. Writers dealing with stalkers have pointed to the same problem.Unlike Amazon\u2019s marketplace, Goodreads \u201cis designed so you don\u2019t have to buy a product to review a book,\u201d said former Amazon employee Kristi Coulter, who worked in publishing and is the author of a forthcoming memoir, \u201cExit Interview,\u201d about her experience. \u201cThat makes it ripe for abuse.\u201dAmazon bought Goodreads in 2013 for a reported $150 million with the hope that the online community of book lovers and the data they created about books would advance its mission of selling everything to everyone.AdvertisementMore than a decade later, even as other social platforms have undergone multiple reinventions, little has changed about Goodreads, a beige website where readers can rate books from one to five stars, write reviews and talk to other readers on old-school forums, some of which have tens of thousands of members.\u201cIn its heyday, [Goodreads] was more of a help selling books than a hindrance,\u201d said Maris Kreizman, an author who hosts a podcast about books for Literary Hub.But Goodreads has remained so clunky in its design and is so difficult to use, Kreizman said, that it is no longer fulfilling the promise it once had of \u201cbringing book lovers together and making new communities.\u201d\u201cI feel like the trajectory was, Goodreads was innovating and doing good things, it was exciting,\u201d she said. \u201cAnd then Amazon bought it. The end.\u201dAdvertisement(Amazon founder Jeff Bezos owns The Washington Post. Interim CEO Patty Stonesifer sits on Amazon\u2019s board.)The first thing Amazon wanted Goodreads to do after acquiring it was build an app for the Kindle, according to one of the former employees.The Kindle, first released in 2007, was an extremely popular device for reading books, and Amazon was eager to broaden its possibilities by bringing in a social component. At the time, the company was trying to get a foothold in social media, where it had trailed its competitors in the tech world, a former Amazon executive said.The Goodreads app allowed readers to highlight passages and share them with other readers, a feature that sparked excitement in academia about the possible future for \u201csocial reading.\u201dThe social aspect of Goodreads was also attractive to publishers, who had access to data for the first time about what kinds of books ignited conversation among readers, the Los Angeles Times argued in a 2010 series on digital reading. At the time, it seemed possible that Goodreads\u2019s data could power a personalized recommendation algorithm akin to what Spotify built for music.AdvertisementCoulter worked at Amazon from 2006 to 2018 and recalled attending a dinner in New York where executives discussed Amazon\u2019s purchase of the site.\u201cThere was interest in using Goodreads data to help power Kindle features,\u201d she recalled. \u201cThere was all this excitement and momentum behind it.\u201dBut after Amazon bought Goodreads, it gradually became clear that the technology was old and the data not well organized, and that a significant investment would be required to bring the site up to speed, according to two former Goodreads employees.The reason \u201cit feels stuck as a product,\u201d one of the former Goodreads employees said, is because \u201cit was painfully slow to create change.\u201d As a result, proposed features like a recommendation algorithm or a news feed for the Kindle powered by Goodreads were never built.AdvertisementAmazon largely let Goodreads operate independently. The company could still get valuable data from Goodreads users, like reviews and genre labels, as well as advertising revenue without needing to commit much in the way of resources to the site, three former employees said.There was also a concern that any major changes to the platform could scare people away. One former employee compared Goodreads to Reddit, an 18-year-old internet forum where users are revolting because of modifications to the site. \u201cPeople feel like they can\u2019t anger the community,\u201d the former employee said.Ultimately, Amazon is pulled in so many directions that it\u2019s common for teams to get pulled onto \u201canother shiny object,\u201d Coulter said. \u201cIt would be in line with what I\u2019ve seen many times at Amazon,\u201d she said. \u201cPeople just get kind of distracted.\u201dAdvertisementAnd as Amazon pursued other goals, authors, publishers and readers said Goodreads became increasingly toxic.In 2014, author Kathleen Hale published a book of essays, called \u201cKathleen Hale Is A Crazy Stalker,\u201d about the lessons she learned after showing up uninvited at the home of someone who left her a bad review on Goodreads. Authors have reported being extorted for money by scammers who will bomb a book\u2019s rating unless the author sends them money. Just this year, young-adult author Sarah Stusek\u2019s publisher reportedly dropped her after she insulted a reader who left a four-star review on her Goodreads page.Stusek said her comments were meant to be humorous but were misinterpreted, \u201cwhich led to retaliation on Goodreads.\u201d\u201cMy publisher and I had been having problems for a while, and when they asked me make a public apology to them, I thought it was best we go our separate ways,\u201d she said via email.Advertisement\u201cFor a number of reasons, including but not limited to attacking a reviewer and multiple others online, we have decided to part ways with one of our authors,\u201d her publisher, Brooke Warner of SparkPress, tweeted at the time.Goodreads does have moderators; their official job titles are \u201cGoodreads experts,\u201d per LinkedIn and public job listings. Moderators are supposed to remove posts that violate Goodreads\u2019s rules \u2014 for example, reviews that attack an author but aren\u2019t actually about their book. But the moderation is manual, and the queue for flagged reviews is long.Suzanne Skyvara, a Goodreads spokesperson, said the company \u201ctakes the responsibility of maintaining the authenticity and integrity of ratings and protecting our community of readers and authors very seriously.\u201d\u201cWe listen to feedback from readers, authors, and publishers, and invest in tools and support teams to improve our ability to quickly detect and stay ahead of content and accounts that violate our reviews or community guidelines,\u201d Skyvara said in an emailed statement.But authors who feel unsafe or unfairly treated often turn to other social platforms like Twitter to get the company\u2019s attention.\u201cGoodreads really needs a mechanism for stopping one-star attacks on writers,\u201d author Roxane Gay tweeted after Gilbert announced her book\u2019s cancellation. \u201cIt undermines what little credibility they have left.\u201dFriedman, the publishing consultant, says any decent publisher will present a marketing plan that involves two to three stages of advanced reviews on Goodreads to generate buzz.\u201cI think it\u2019s a powerful tool for publishers,\u201d she said. \u201cBut there\u2019s a real double-edged sword where it gets used as a weapon.\u201d621CommentsGift this articleGift ArticleView more",
    "summary": "- Goodreads, an Amazon-owned review site for books, has faced criticism for its outdated infrastructure and lack of protective features, leading to targeted harassment and \"review bombing\" incidents.\n- Amazon reportedly did not invest in updating or maintaining Goodreads, using it primarily for user-generated data.\n- The acquisition of Goodreads by Amazon in 2013 was seen as a promising opportunity for the site to innovate and create new services for readers and authors, but little has changed since then, resulting in disappointment among users and authors alike.",
    "hn_title": "Goodreads was the future of book reviews, then Amazon bought it",
    "original_title": "Goodreads was the future of book reviews, then Amazon bought it",
    "score": 540,
    "hn_content": "- Amazon's acquisition of Goodreads was seen by many as anti-competitive and a defensive move to prevent competition.\n- The fear was that Amazon would stifle innovation and potential new features that Goodreads could have developed.\n- Some argue that stronger and enforced anti-trust laws could prevent such acquisitions from happening.\n- Others believe that banning all M&A activity could be a solution, as mergers often reduce competition and come at the expense of employees and customers.\n- Goodreads was considered a valuable platform for book reviews and recommendations, with a large user base.\n- The sentiment among some tech-savvy individuals is that startups often get bought to prevent them from becoming competitors.\n- The acquisition of Goodreads by Amazon has disappointed some users who expected more growth and innovation from the platform.\n- Goodreads' API has been sunsetted by Amazon, leading to frustration among developers and further hindering the platform's potential for innovation.\n- The discussion around this acquisition highlights broader concerns about monopolies and the need for stronger regulations in the tech industry.- Goodreads was a profitable business based on affiliate marketing links mostly to Amazon, and Amazon acquired it to save money.\n- Amazon owns a number of other companies, including AbeBooks, Twitch, Audible, and Whole Foods.\n- Amazon's acquisition of Goodreads was seen as anticompetitive by some users.\n- Goodreads provides a catalog of user reviews, which is an important feature for an eCommerce site like Amazon.\n- The acquisition was seen as a defensive move by Amazon to prevent anyone else from acquiring or partnering with Goodreads and potentially using its database of books and reviews to compete with Amazon's book business.\n- The acquisition also allowed Amazon to save costs on affiliate sales.\n- Some users on the forum express frustration with Amazon's acquisition and the impact it had on certain apps or features, such as Stanza.\n- The free market versus government regulation is a debated topic among users, with some arguing that big companies acquiring smaller ones limits consumer choices and stalls innovation, while others believe in the benefits of a free market system without excessive government intervention.\n- The cultural differences in rating systems are discussed, with examples given from the Netherlands and Germany, where lower ratings are seen as more acceptable.\n- The limitations of the star rating system are acknowledged, such as the tendency for ratings to cluster around the higher end. There are suggestions for alternative rating systems, such as a binary recommend/do not recommend option or an Elo-like system that compares books to each other.\n- The problem of fake or paid reviews is mentioned as a bigger issue than the distribution of ratings.\n- The idea of using relative rankings, where users compare two books they have read, is proposed, but concerns are raised about comparing books across genres.\n- The suggestion of using a pain scale with faces for rating systems is mentioned as an alternative approach.",
    "hn_summary": "- Amazon's acquisition of Goodreads was seen as anti-competitive and a defensive move to prevent competition.\n- The sentiment among some tech-savvy individuals is that startups often get bought to prevent them from becoming competitors.\n- The acquisition of Goodreads by Amazon has disappointed some users who expected more growth and innovation from the platform."
  },
  {
    "id": 36568192,
    "timestamp": 1688352886,
    "title": "55 GiB/s FizzBuzz (2021)",
    "url": "https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630",
    "hn_url": "http://news.ycombinator.com/item?id=36568192",
    "content": "- The post discusses the FizzBuzz challenge, which involves writing a program that prints numbers from 1 to a given number, replacing numbers divisible by 3 with \"Fizz\", numbers divisible by 5 with \"Buzz\", and numbers divisible by both with \"FizzBuzz\".\n- The challenge is to write a FizzBuzz implementation that can handle an arbitrarily large number of iterations and do it as fast as possible.\n- The post includes a naive implementation in C and invites the community to come up with clever ideas to push the throughput to its limits.\n- The challenge allows all programming languages and encourages parallel implementations and architecture-specific optimizations.\n- The post includes results from different authors using various programming languages, showing the throughput achieved by their implementations.\n- One notable implementation achieved a throughput of 54-56 GiB/s, which is the best score so far.\n- The post includes information on how to build and run an optimized assembly language implementation using AVX2 instructions.\n- The assembly implementation aims for maximum single-threaded performance and utilizes SIMD instructions to generate FizzBuzz output.\n- The assembly implementation is extensively optimized and avoids copying output data by using the vmsplice system call to place a reference to the output buffer into a pipe.\n- The assembly implementation aims to output 64 bytes of FizzBuzz output per four clock cycles in its main loop.\n- The main loop interprets bytecode instructions stored in memory to generate the FizzBuzz output.\n- The assembly implementation shows significant performance improvements compared to simple implementations using write-like system calls.",
    "summary": "- The post discusses the FizzBuzz challenge, which involves writing a program that prints numbers with certain conditions.\n- The challenge is to create a FizzBuzz implementation that can handle a large number of iterations and execute quickly.\n- A notable implementation achieved a throughput of 54-56 GiB/s, which is the best score so far.",
    "hn_title": "55 GiB/s FizzBuzz (2021)",
    "original_title": "55 GiB/s FizzBuzz (2021)",
    "score": 509,
    "hn_content": "- A discussion on Hacker News about the speed of the FizzBuzz program in different languages.\n- Commenters are impressed by the efficiency of Linux in keeping data in L2 cache and avoiding main memory access.\n- CPU caches, TLB invalidation, and page table contention are discussed in relation to program performance.\n- The conversation touches on high-frequency trading and the use of custom hardware and assembly language in HFT systems.\n- There are jokes and banter between commenters about the FizzBuzz program and performance optimizations.\n- Some comments delve into the performance of different programming languages, including Java and C.\n- The relevance and importance of optimizing code and performance are debated.\n- The limitations of I/O speed are mentioned, particularly in the context of large-scale data processing.\n- The discussion highlights the trade-offs between performance optimization, developer time, and business requirements.- Web pages can be laggy due to a combination of factors such as using libraries that don't optimize performance and not using tools for optimization.\n- VS Code, although popular, can be laggy on older devices, while Sublime Text performs better.\n- There is a debate about whether Electron is the cause of software inefficiency or if other frameworks would also result in similar issues.\n- Some argue that the debate about slow apps is more ideological than based on reality, and that backend/network latency may be mistaken for front-end slowness.\n- The choice to use Electron may be due to its popularity and the fact that it works for many developers.\n- The software engineering process involves making trade-offs based on constraints, and the economic incentives may affect the prioritization of performance optimization.\n- FizzBuzz is a simple programming problem that is often used in job interviews to assess coding skills.\n- The story of a candidate failing the FizzBuzz test highlights the importance of aligning job requirements with the skills needed for the role.\n- Some commenters argue that UI/UX designers may not need to know FizzBuzz, while others emphasize the minimal coding proficiency expected in the role.\n- The story resonates with many who have encountered job descriptions that seem to combine multiple roles into one position.",
    "hn_summary": "- Comments discuss the efficiency of different programming languages in the FizzBuzz program and the importance of code optimization and performance.\n- The limitations of I/O speed and trade-offs between performance optimization, developer time, and business requirements are mentioned.\n- The relevance of aligning job requirements with the skills needed for the role is highlighted in a discussion about the FizzBuzz test in job interviews."
  },
  {
    "id": 36575784,
    "timestamp": 1688403698,
    "title": "Joins 13 Ways",
    "url": "https://justinjaffray.com/joins-13-ways/?a=b",
    "hn_url": "http://news.ycombinator.com/item?id=36575784",
    "content": "Justin JaffrayblognotesJoins 13 Ways03 Jul 2023Relational (inner) joins are really common in the world of databases, and one weird thing about them is that it seems like everyone has a different idea of what they are. In this post I\u2019ve aggregated a bunch of different definitions, ways of thinking about them, and ways of implementing them that will hopefully be interesting. They\u2019re not without redundancy, some of them are arguably the same, but I think they\u2019re all interesting perspectives nonetheless.Table of ContentsA join is a lookupA join is a nested loop over rowsA join is a nested loop over columnsA join is compatible alternate realitiesA join is flatMapA join is the solution to the N+1 problemA join is paths through a graphA join is a minimal modelA join is typecheckingA join is an operation in the Set monadA join is the biggest acceptable relationA join is a\u2026joinA join is a ring productA join is a lookupThe first and most practical way to see a join is that it\u2019s \u201clooking something up,\u201d or adorning some data with some additional, redundant data.I think the first place people typically encounter joins is when some guy on the internet has told them to normalize their tables. Meaning they\u2019ve been told to stop storing data like this:user country country_codeSmudge Canada CASissel Canada CAPetee United States USThis is \u201cbad,\u201d because there\u2019s redundancy: country_code doesn\u2019t change between rows with the same country. If we were storing something more volatile than this, we\u2019d have to make sure that any changes to the data were reflected everywhere, which is error prone and inefficient.The correct way to do this is to normalize the table. Create a table which only relates country and country_code:country_id country country_code1 Canada CA2 United States USand then reference that in the \u201cfact table:\u201duser country_idSmudge 1Sissel 1Petee 2Then a join is the operation that lets us recover the original table, which we might want to do some computations: to join these two tables we\u2019d write:SELECT user, country, country_code FROM  users   INNER JOIN  countries   ON users.country_id = countries.country_idGoing forward, we will often adopt that convention that we implicitly join \u201con\u201d any columns of the two relations that have the same name. But we will play a bit fast and loose with this, don\u2019t sweat the details too much.A join is a nested loop over rowsGiven some predicate p, the join of two sets R and S is:def join(R, S, p): output = [] for r in R:   for s in S: if p(r, s):     output.push((r, s)) return outputIf the cross product of two collections is all concatenations of rows from the two of them, their join is a subset of that.A join is a nested loop over columnsThe domain of a column is the set of possible values that can appear in it.If I have a relation R whose columns are a and b, and S whose columns are b and c, then the join of R and S is:def join(R, S): output = [] for a in domain(a):  for b in domain(b):   for c in domain(c):    if R.contains(a, b) and S.contains(b, c):  output.push([a, b, c]) return outputA join is compatible alternate realitiesHere\u2019s our first weird one.John and Sally are standing around the corner from each other, each has a pet, and each can see a stray animal on the corner, but they can\u2019t see each other.John and Sally both exist in a handful of alternate realities at once. For instance, there\u2019s one reality where John has a dog, and another where he has a cat. We can summarize the realities that are possible for John in a relation:john\u2019s pet straydog dogcat dogcat mouseSimilarly, Sally also has a pet, and can also see the stray:sally\u2019s pet straydog mousecat mousemouse dogWe only have this imperfect information, because John can\u2019t see Sally\u2019s pet, and Sally can\u2019t see John\u2019s pet.We can still make some inferences though: it can\u2019t be the case that John has a dog while Sally has a cat, because then they would disagree on what the stray was (whenever John has a dog, the stray is a dog, but whenever Sally has a cat, the stray is a mouse).By this logic, we can list out all the combinations that might exist:john\u2019s pet stray sally\u2019s petdog dog mousecat dog mousecat mouse dogcat mouse catThis is the join of the two tables on stray.A join is flatMapThe flatMap function in many programming languages operates on arrays. It computes a new array for every element of the original, and concatenates the results.> [1, 2, 3].flatMap(x => new Array(x).fill(x))[ 1, 2, 2, 3, 3, 3 ]This can implement a join. This:SELECT * FROM r INNER JOIN s ON pbecomes this:r.flatMap(x => s.filter(y => p(x, y)))Some SQL variants support a LATERAL construction which turns joins into flatMaps:pg=# SELECT * FROM (VALUES (1), (2), (3)) r(x), LATERAL (SELECT * FROM generate_series(1, x)) u; x | generate_series---+----------------- 1 |        1 2 |        1 2 |        2 3 |        1 3 |        2 3 |        3(6 rows)Whenever the right-hand side of such a flatMap doesn\u2019t contain any references to the left-hand side, it\u2019s equivalent to a cross product (this is the crux of how query decorrelation is done, utilizing successive rewrites to remove column references from the right-hand side).A join is the solution to the N+1 problemA common problem that occurs when using ORMs is called the \u201cN+1 problem.\u201d This happens when you need to do a query for each row in a result set. It ends up looking something like this:pets = run_query(\"SELECT * FROM users\")for pet in pets: country_code = run_query(  \"SELECT country_code FROM countries WHERE country_id = %d\" % pet.country_id ) print(pet.name, country_code)This is a really common problem that shows up when people aren\u2019t yet used to using relational databases. The problem is that in databases that use connections, like Postgres (this is not so much of a problem for in-process databases like Sqlite), there\u2019s a high fixed cost to an individual query. Thus, you might want a way to tell the database \u201cplease do all these lookups for me,\u201d and the result turns out to be exactly a join:SELECT name, country_code FROM users INNER JOIN countries ON users.country_id = countries.idA join is paths through a graphA relation is so named because it \u201crelates\u201d two sets. In the case of our users table, it relates the set of usernames with the set of country IDs. We can visualize this relationship as a graph:And similarly, we relate the set of country IDs to the set of two-letter country codes:Since the right-hand side of the first graph, and the left-hand side of the second graph share a vertex set, it makes sense to consider them together:If we enumerate all the paths that start in the left set of this graph, go to a vertex in the middle set, and end on a vertex in the right set, we will construct exactly the join of these two relations.A join is a minimal modelIn formal logic, a model of a set of sentences is a set of facts which make all of the sentences true. In this setting, a relation is a predicate. The users relation is the predicate that satisfies the following:users(\"Smudge\", 1).users(\"Sissel\", 1).users(\"Petee\", 2).and the country relation is the predicate that satisfies:country(1, \"Canada\", \"CA\").country(2, \"United States\", \"US\").Now consider the following implication:users(a,b)\u2227countries(b,c,d)\u2192Q(a,b,c,d)read \u201cwhenever users(A, B) and countries(B, C, D), then Q(A, B, C, D).\" A model of this sentence is a set of facts which Q is true for such that this sentence is true.One possible model is:Q(\"Smudge\", 1, \"Canada\", \"CA\").Q(\"Smudge\", 2, \"United States\", \"US\").Q(\"Sissel\", 1, \"Canada\", \"CA\").Q(\"Sissel\", 2, \"United States\", \"US\").Q(\"Petee\", 1, \"Canada\", \"CA\").Q(\"Petee\", 2, \"United States\", \"US\").Another isQ(\"Smudge\", 1, \"Canada\", \"CA\").Q(\"Smudge\", 2, \"United States\", \"US\").Q(\"Sissel\", 1, \"Canada\", \"CA\").Q(\"Sissel\", 2, \"United States\", \"US\").Q(\"Petee\", 2, \"Canada\", \"CA\").Q(\"Petee\", 1, \"United States\", \"US\").Q(\"Banana\", 1, \"Banana\", \"Banana\").It\u2019s not particularly satisfying that this definition means we can have multiple possible models. We want something canonical. That\u2019s why we ask for the smallest model that works. It turns out that for sentences like this, such a model always exists, and it\u2019s the intersection of all models.Here it\u2019sQ(\"Smudge\", 1, \"Canada\", \"CA\").Q(\"Sissel\", 1, \"Canada\", \"CA\").Q(\"Petee\", 2, \"United States\", \"US\").which is the join of users and country.A join is typecheckingML-style type systems bear a lot of similarities to joins (mostly because they very strongly resemble Prolog and Datalog). First, let\u2019s define our relations as Rust traits:trait Users {}trait CountryCode {}Now define our values, they\u2019re Rust concrete types:struct Smudge;struct Sissel;struct Petee;struct Canada;struct UnitedStates;struct CA;struct US;impl Users for (Smudge, Canada) {}impl Users for (Sissel, Canada) {}impl Users for (Petee, UnitedStates) {}impl CountryCode for (Canada, CA) {}impl CountryCode for (UnitedStates, US) {}Now we define the join itself. A triple (A, B, C) is in the join when (A, B) is in Users, and (B, C) is in CountryCode.trait UserCountryCode {}impl<A, B, C> UserCountryCode for (A, B, C)where  (A, B): Users,  (B, C): CountryCode,{}Finally, we can check if something is in the join if our program typechecks. This typechecks:fn test<X: UserCountryCode>() {}fn main() {  test::<(Smudge, _, CA)>()}While this doesn\u2019t:fn test<X: UserCountryCode>() {}fn main() {  test::<(Smudge, _, US)>()}error[E0277]: the trait bound `(Canada, US): CountryCode` is not satisfied --> src/main.rs:32:12  |32 |   test::<(Smudge, _, US)>()  |      ^^^^^^^^^^^^^^^ the trait `CountryCode` is not implemented for `(Canada, US)`  |  = help: the following other types implement trait `CountryCode`:       (Canada, CA)       (UnitedStates, US)note: required for `(Smudge, Canada, US)` to implement `UserCountryCode` --> src/main.rs:22:15  |22 | impl<A, B, C> UserCountryCode for (A, B, C)  |        ^^^^^^^^^^^^^^^   ^^^^^^^^^...25 |   (B, C): CountryCode,  |       ----------- unsatisfied trait bound introduced herenote: required by a bound in `test` --> src/main.rs:29:12  |29 | fn test<X: UserCountryCode>() {}  |      ^^^^^^^^^^^^^^^ required by this bound in `test`A join is an operation in the Set monadLet\u2019s define an option type (in JavaScript).let Some = x => ({  map: f => Some(f(x)),  // Sometimes called `bind`.  andThen: f => f(x),  inspect: () => `Some(${JSON.stringify(x)})`,});let None = () => ({  map: () => None(),  andThen: () => None(),  inspect: () => `None()`,});Now, say we have some records, but they\u2019re all optional. As in, we might not have them, so we have to wrap them in Some:let user = Some({ name: 'Smudge', country: 'Canada' });let country1 = Some({ country: 'Canada', code: 'CA' });let country2 = Some({ country: 'United States', code: 'US' });Now we\u2019ll write a function that takes two of these records and returns Some of their merging if they\u2019re compatible, and None otherwise:let merge = (user, country) => {  if (user.country === country.country) {    return Some({ name: user.name, country: user.country, code: country.code });  } else {    return None();  }}But since our actual records are optional, we need to wrap them in calls to andThen:let combine = (user, country) => {  return user.andThen(user => {    return country.andThen(country => {      return merge(user, country);    })  });}Now we can see the results of calling this function with various values:console.log(combine(user, country1).inspect());console.log(combine(user, country2).inspect());console.log(combine(None(), country1).inspect());console.log(combine(user, None()).inspect());One interesting thing about the way we\u2019ve set this up is that we can implement our \u201ccontainer\u201d type differently, but keep the implementation.Let\u2019s implement a different container, called Rel, which stores a set of records. Now our map operates on every row, and our andThen returns another relation, which all get concatenated to the new relation.let Rel = x => ({  map: f => Rel(x.map(f)),  andThen: f => Rel(x.flatMap(v => f(v).list())),  list: () => x,})Now we can instantiate some data:let users = Rel([  { name: 'Smudge', country: 'Canada' },  { name: 'Sissel', country: 'Canada' },  { name: 'Petee', country: 'United States' },]);let countries = Rel([  { country: 'Canada', code: 'CA' },  { country: 'United States', code: 'US' },]);And try running the same function combine from before:console.log(combine(users, countries).list());And we get the join of the two relations:[ { name: 'Smudge', country: 'Canada', code: 'CA' }, { name: 'Sissel', country: 'Canada', code: 'CA' }, { name: 'Petee', country: 'United States', code: 'US' }]A join is the biggest acceptable relationFor two relationsR andS, say a third relationT which has all the columns from both is \u201cacceptable\u201d if it doesn\u2019t invent any new information.By that I mean, if you look at any row inT and restrict it to just the columns inR, the resulting row exists inR, and the same is true forS.For instance, say our tables are:Ruser countrySmudge CanadaSissel CanadaPetee United StatesScountry country_codeCanada CAUnited States USThisT is unacceptable:user country country_codeSmudge Canada USBecause if we restrict to the columns ofS,\u27e8country,country_code\u27e9, we getcountry country_codeCanada USWhich doesn\u2019t exist inS.Notably, the empty relation is acceptable. The largest acceptable relation, here, isuser country country_codeSmudge Canada CASissel Canada CAPetee United States USThis is the join of the two relations.A join is a\u2026joinA partial order is a set equipped with a binary relation\u2264 satisfying the following properties:Reflexivity:a\u2264a for alla,Antisymmetry: Ifa\u2264b andb\u2264a, thena=b, andTransitivity: Ifa\u2264b andb\u2264c, thena\u2264c.(I provide this definition to be complete, but if it\u2019s new to you, don\u2019t expect the above to give you the intuition you\u2019d need to actually be able to think about it.)In a partial order, if two elementsa,b always have a least upper bound (that is, a smallestx whicha andb are both\u2264), then that is called their join and is writtena\u2228b.Define the following partial order on relations:R\u2264Q if:Q contains all the columns inR, andrestricting any row inQ to just the columns inR gives a row inR.Then for two relationsR andS,R\u2228S exists, and it\u2019s their join (in both senses of the word).A join is a ring productYou might know from high school how to manipulate polynomials. Ifa,b, andc are all unknowns, the following expressions are all equivalent:a(b+c)=ab+ac=ba+ca=ca+ba=(c+b)aWe can represent a relation algebraically in this way. A row is the product of its columns. We represent the rowuser country_idSmudge 1as the product of two (column name, column value) pairs:[user=Smudge][country_id=1]A relation is the sum of its rows:R=++[user=Smudge][country_id=1][user=Sissel][country_id=1][user=Petee][country_id=2]We also have the special value 1, satisfying1x=x for allx.We then add the following rules for simplifying these expressions:Idempotence:[x=y][x=y]=[x=y]and Contradiction:[x=y][x=z]=0 if y\u0338=z.our lookup table here isS=+[country_id=1][country=Canada][country_code=CA][country_id=2][country=United States][country_code=US]Then something interesting happens if we take the product of these two expressions, obeying the normal rules of polynomial rewriting such as distributivity:RS=\u239d\u239c\u239b++[user=Smudge][country_id=1][user=Sissel][country_id=1][user=Petee][country_id=2]\u23a0\u239f\u239e(+[country_id=1][country=Canada][country_code=CA][country_id=2][country=United States][country_code=US])If you apply the laws of distributivity and commutativity here, you\u2019ll wind up with the following:++[user=Smudge][country_id=1][country=Canada][country_code=CA][user=Sissel][country_id=1][country=Canada][country_code=CA][user=Petee][country_id=2][country=United States][country_code=US]Which is precisely the join of the two relations (I\u2019m told this is a tensor contraction).",
    "summary": "- A join in databases is a way to combine data from multiple tables based on a common column or attribute.\n- Joins help eliminate redundancy in data storage by creating separate tables for related information and linking them using a join operation.\n- Join operations can be implemented in various ways, such as lookups, nested loops, graph paths, or typechecking, depending on the context and requirements of the data.",
    "hn_title": "Joins 13 Ways",
    "original_title": "Joins 13 Ways",
    "score": 465,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginJoins 13 Ways (justinjaffray.com)465 points by foldU 17 hours ago | hide | past | favorite | 63 commentsbob1029 15 hours ago | next [\u2013]Once I started thinking about joins in terms of spatial dimensions, things got a lot easier to reason with. I like to think of the inner join like the scene from Stargate where they were describing how gate addressing works. Assume you have a contrived example with 3 tables describing the position of an off world probe in each dimension - X, Y and Z. Each table looks like: CREATE TABLE Dim_X (  int EntityId  float Value )Establishing a point in space for a given entity (regardless of how you derive that ID) is then a matter of: SELECT Dim_X.Value AS X, Dim_Y.Value as Y, Dim_Z.Value as Z FROM Dim_X, Dim_Y, Dim_Z WHERE Dim_X.EntityId = Dim_Y.EntityId --Join 1 AND Dim_Y.EntityId = Dim_Z.EntityId --Join 2 AND Dim_X.EntityId = @MyEntityId --The entity we want to find the 3D location ofYou will note that there are 2 inner joins used in this example. That is the bare minimum needed to construct a 3 dimensional space. Think about taking 3 cards and taping them together with 2 pieces of rigid metal tape. You can make a good corner of a cube, even if it's a bit floppy on one edge. Gravity doesn't apply inside the RDBMS, so this works out.This same reasoning can be extrapolated to higher, non-spatial dimensions. Think about adding time into that mix. In order to find an entity, you also now need to perform an inner join on that dimension and constrain on a specific time value. If you join but fail to constrain on a specific time, then you get a happy, comprehensive report of all the places an entity was over time.The other join types are really minor variations on these themes once you have a deep conceptual grasp (i.e. can \"rotate\" the schema in your mind).Playing around with some toy examples can do wonders for understanding. I sometimes find myself going back to the cartesian coordinate example when stuck trying to weld together 10+ dimensions in a real-world business situation.replyquasarj 7 hours ago | parent | next [\u2013]I now understand joins less than I did before.replybarrkel 2 hours ago | root | parent | next [\u2013]It's more a description of how dimensions work in OLAP and it's incomplete without measures.replymichaelmior 14 hours ago | parent | prev | next [\u2013]This reminds me a lot of HyperDex[0], which hashes values into a multidimensional hyperspace based on their attributes for indexing purposes.[0] https://dbdb.io/db/hyperdexreply1970-01-01 12 hours ago | root | parent | next [\u2013]OLAP and MOLAPhttps://www.ibm.com/topics/olaphttps://en.wikipedia.org/wiki/Online_analytical_processing#M...replytoyg 7 hours ago | root | parent | next [\u2013]That Wikipedia article, and it's offshoot pages, are so awfully out of date...But yes, OLAP.replyrjbwork 11 hours ago | parent | prev | next [\u2013]This is like, a hypernormalization of the data though, isn't it? I think in standard BCNF you'd just leave your table as CREATE TABLE EntityPosition (  int EntityId,  float X,  float y,  float z )It does remind me of data warehouse stuff though, given we're working with aggregates and piecing together bits of various dimensions.replyjagged-chisel 11 hours ago | root | parent | next [\u2013]> \u2026a hypernormalization of the data \u2026Well, yeah - it\u2019s an example to get the point across, not an exercise in finding the right level of normalization.replyfarkanoid 10 hours ago | parent | prev | next [\u2013]Great. Now you've made me want to re-watch Stargate!replyroywiggins 13 hours ago | prev | next [\u2013]The 0th would be \"it's an operator in relational algebra.\"https://en.m.wikipedia.org/wiki/Relational_algebra(\"The result of the natural join [R \u22c8 S] is the set of all combinations of tuples in R and S that are equal on their common attribute names... it is the relational counterpart of the logical AND operator.\")\u22c8 amounts to a Cartesian product with a predicate that throws out the rows that shouldn't be in the result. Lots of SQL makes sense if you think of joins this way.replyjimwhite42 11 hours ago | parent | next [\u2013]In the functional interpretation of relational theory, a join is function composition, surprised that one was left out.replygpderetta 12 hours ago | parent | prev | next [\u2013]The cross-product plus predicate is referenced in \"A join is a nested loop over rows\".replyroywiggins 12 hours ago | root | parent | next [\u2013]That's true, but I think the benefit of treating a Cartesian product as fundamental is that it lets you stop thinking about loops at all, or which one is the inner or outer loop, or of it as an iterative process at all.Boxing all that up into the Cartesian product is a really useful concept, and the whole idea of relational algebra is to find convenient formalisms for relational operations, so it seems like it deserves a separate mention.replycode_biologist 10 hours ago | root | parent | next [\u2013]1,000%. I learned discrete/set math in college and then later SQL on the job. Helping a few analysts moving beyond Excel skills to learn SQL was interesting. They struggled with some things that clicked quickly for me because I immediately had \"oh, these are set operators\" intuition. Stuff like when to use an outer join vs a cross join, or how to control cardinality of output rows in fancy ways (vs slapping `DISTINCT ON` on everything).I passed on the set understanding where it explained an unintuitive SQL behavior and I hope it helped 'em.replycmrdporcupine 10 hours ago | root | parent | prev | next [\u2013]Absolutely. One of the biggest dis-services that SQL does is getting people thinking of relations as \"tables\" with \"rows\" which ends up making them think in an iterative, sequential, tabular model for something that I think they'd be better off thinking more abstractly about.The whole relational model clicked for me a whole lot more once I started thinking of each tuple as factual propositions (customer a's name is X, phone number is Y), and then all the operations in the relational algebra start to look more like \"how would it be best to ask questions about this subject?\"... \"I'm interested in facts about...\"replymaxdemarzi 13 hours ago | prev | next [\u2013]The 14th way is \u201cmulti way joins\u201d also called \u201cworst case optimal joins\u201d which is a terrible name.It means instead of joining tables two at a time and dealing with the temporary results along the way (eating memory), you join 3 or more tables together without the temporary results.There is a blog post and short video of this on https://relational.ai/blog/dovetail-join and the original paper is on https://dl.acm.org/doi/pdf/10.1145/3180143I work for RelationalAI, we and about 4 other new database companies are bringing these new join algorithms to market after ten years in academia.replygavinray 8 hours ago | parent | next [\u2013]Justin also has a post on WCOJ that's really solid:https://justinjaffray.com/a-gentle-ish-introduction-to-worst...replynamibj 11 hours ago | parent | prev | next [\u2013]Negating inputs (set complement) turns the join's `AND` into a `NOR`, as Tetris exploits.The worst case bounds don't tighten over (stateless/streaming) WCOJ's, but much real world data has far smaller box certificates.One thing I didn't see is whether Dovetail join allows recursive queries (i.e., arbitrary datalog with a designated output relation, and the user having no concern about what the engine does with all the intermediate relations mentioned in the bundle of horn clauses that make up this datalog query).Do you happen to know if it supports such queries?replymirekrusin 14 hours ago | prev | next [\u2013]For several days I'm having trouble finding good resources on _implementation_ for query execution/planning (predicates, existing indices <<especially composite ones - how to locate them during planning etc>>, joins etc).Google is spammed with _usage_.Anybody has some recommendations at hand?ps. the only one I found was CMU's Database Group resources, which are greatreplygavinray 13 hours ago | parent | next [\u2013]There's an entire 700-page book about this you can access for free here:\"Building Query Compilers\"https://pi3.informatik.uni-mannheim.de/~moer/querycompiler.p...EDIT: You also might get use out of TUM's course \"Database Systems on Modern CPU Architectures\"The year that contains all the video lectures is 2020:https://db.in.tum.de/teaching/ss20/moderndbs/?lang=enreplyhobofan 3 hours ago | root | parent | next [\u2013]I'd also recommend \"How Query Engines Work\" for a good practical exercise in the topic: https://andygrove.io/how-query-engines-work/ . I think one should go already have a high-level overview of what individual parts make up a query engine and what there purpose is, as it only touches on that lightly.replysobellian 13 hours ago | parent | prev | next [\u2013]I don't know if this has the level of detail you're looking for, but you might want to check https://www.sqlite.org/optoverview.html and https://www.sqlite.org/queryplanner.html.replySesse__ 2 hours ago | root | parent | next [\u2013]The SQLite optimizer is really primitive. You generally don't want to be using it as a template if you can avoid it; it's good for OLTP but can't handle many-way joins well.replySesse__ 2 hours ago | parent | prev | next [\u2013]Generally, this is a specialist topic, so you're unlikely to find e.g. good textbooks. It depends a bit on how deep you want to go and what specific parts you're interested in, though. (Query execution is pretty much entirely separate from planning, for one.)The best join optimizer paper I know of is still the original Selinger paper (https://courses.cs.duke.edu/compsci516/cps216/spring03/paper...). It doesn't support outer joins and there are more efficient techniques by now, but anyone looking at a System R-like optimizer can read this and feel right at home. (There is also the Volcano/Cascades school of optimizers, which is rather different from System R, but I've found even less information on it.)As others have said, Postgres' source and documentation is good. In particular, src/backend/optimizer/README contains a number of interesting things that you won't find a lot of other places. (The Postgres optimizer doesn't always use the latest fancy designs, but it's generally very polished and pretty easy to read.)I can also echo Andy Pavlo's courses (at CMU), they're pretty much the only ones who will explain this stuff to you online. The \u201cBuilding Query Compilers\u201d PDF is rather incomplete and there's a lot of stuff I never cared for in there, but it contains several key papers from Moerkotte et al if you actually want to implement the modern stuff (DPhyp etc.). In general, most of the modern System R-like stuff (how to efficiently deal with outer joins, how to deal with interesting orders, how to deal with large queries) comes from the groups of Moerkotte and/or Neumann; all of these things had solutions before them, but less efficient and/or powerful and/or elegant.Finding an applicable index isn't hard (you generally just try to see if it hits a predicate\u2014a so-called \u201csargable predicate\u201d, for SARG = Search ARGument). Estimating selectivity is hard. Estimating selectivity through joins is perhaps the hardest problem in optimizers, and nobody has truly solved it. This was one of the things I never really got to; there are so many hard subproblems. Like, for something that sounds really simple but isn't, what do you do if you have predicates A=x AND B=y AND C=z and you happen to have indexes on (A,B) and (B,C) (with selectivity/cardinality information) and want a selectivity for all three combined? There are papers that literally require you to build a \u201csecond-order cone programming\u201d solver to solve this problem :-)replyaidos 13 hours ago | parent | prev | next [\u2013]My go to pointer for this is to read the Postgres docs (and / or source - which is also super readable).https://www.postgresql.org/docs/current/planner-optimizer.ht...replymrkeen 13 hours ago | parent | prev | next [\u2013]I just tried adding 'relational algebra' in front of my 'query planning' query, and at a glance it skews more towards implementation.replydattl 13 hours ago | parent | prev | next [\u2013]I find the Advanced Databases Course from CMU an excellent resource. https://15721.courses.cs.cmu.edu/spring2023/schedule.htmlYou might want to look into academic papers, e.g., T. Neumann, Efficiently Compiling Efficient Query Plans for Modern Hardware, in VLDB, 2011 https://www.vldb.org/pvldb/vol4/p539-neumann.pdfreplymalfist 13 hours ago | parent | prev | next [\u2013]It seems content farms, farming keywords with just fluff has taken over google. Can't find how to do anything anymore, just pages and pages of people talking about doing something.You could try your query on a different search engine. I've had good luck with kagi.replyskywhopper 13 hours ago | parent | prev | next [\u2013]It\u2019s out of date and probably has less detail than I remember but I got a lot out of \u201cInside Microsoft SQL Server 7.0\u201d which does deep dives into storage architecture, indices, query planning etc from an MSSQL specific POV. The book was updated for SQL Server 2003 and 2008. Obviously the book also has a ton of stuff about MS specific features and admin functionality that\u2019s not really relevant, and I\u2019m sure there are better resources out there, but I\u2019ve found the background from that book has helped me understand the innards of Oracle, MySQL, and Postgres in the years since.replyttfkam 16 hours ago | prev | next [\u2013]Excellent! We need more articles like this that demonstrate the subtleties of the relational model to primarily app-level developers. The explanations and explorations in terms of functional programming are both concise and compelling.replytmpfile 5 hours ago | prev | next [\u2013]Great article. Side note: his normalization example reminded me how I used to design tables using a numeric primary key thinking they were more performant than strings. But then I\u2019d have a meaningless id which required a join to get the unique value I actually wanted. One day I realized I could use the same unique key in both tables and save a join.Simple realization. Big payoffreplyroselan 2 hours ago | parent | next [\u2013]I still like to have a unique id field per table. It helps logging and it doesn't care about multi fields \"real\" key.However I keep an unique index on the string value and more importantly point integrity constraints to it, mainly for readability. It's way easier to read a table full of meaningful strings rather than full of numerical id or uuids.replysrcreigh 12 hours ago | prev | next [\u2013]Another missed chance to educate on the N+1 area. Join on unclustered index is still N+1, it\u2019s just N+1 on disk instead of N+1 over the network and disk.replytourist2d 11 hours ago | parent | next [\u2013]\"Another missed chance to talk about X problem I find interest in and would bloat the article\"replyconor-23 9 hours ago | prev | next [\u2013]This dudes blog is fire. Very nice explanations of complex database topics.replygavinray 8 hours ago | parent | next [\u2013]Justin Jaffray is a gemreplysmif 12 hours ago | prev | next [\u2013]An inner join is a Cartesian product with conditionals added.replyRobinL 12 hours ago | parent | next [\u2013]There's a big performance difference between creating the Cartesian product and then filtering for the conditional, and creating the conditionals directly. An inner join with equi join conditions creates the conditions directly; any non equi join conditions actually have to be evaluatedreplysmif 12 hours ago | root | parent | next [\u2013]That's true, but that is an implementation detail. In abstract terms, you can think of an inner join like that.Also, while what you say is true in general for modern DB's, there are some implementations like old Oracle versions where the only way to create the effect of an inner join was in terms of a Cartesian product.replycharles_f 14 hours ago | prev | next [\u2013]Very nice explanation!> The correct way to do this is to normalize the tableThis is true for transactional dbs, but in data warehouses it's widely accepted that some degree of denormalization is the way to goreplyTechBro8615 6 hours ago | prev | next [\u2013]This is a nice way of explaining a concept, and could probably be applied to any complex topic. As someone who learns best by analogy, concepts usually \"click\" for me once I've associated them with a few other concepts. So I appreciate the format, and would personally enjoy seeing more explainers like this (and not just about database topics).replyfifilura 6 hours ago | parent | next [\u2013]I think the next article could be about group by.Like (only intuitively sofar...)A group by from A rows to B rows - is a map-reduce job - is an AxB linear transformation matrix from your linear algebra course - is...replybenjiweber 14 hours ago | prev | next [\u2013]Joins as a relational AND.https://benjiweber.co.uk/blog/2021/03/21/thinking-in-questio....replyBoppreH 11 hours ago | prev | next [\u2013]Fantastic post, I always enjoy reading about different computation mental models.And under \"A join is a\u2026join\", there's a typo in the partial order properties. It currently reads:  1. Reflexivity: a\u2264b,And I'm pretty sure it should be  1. Reflexivity: a\u2264a,instead (i.e., every element is \u2264 to itself).replyfoldU 11 hours ago | parent | next [\u2013]You are correct! I will fix it when I get home, thank you for the correction!replywaynecochran 11 hours ago | prev | next [\u2013]Can join also be thought of as unification? Similar to type checking.https://www.cs.cornell.edu/courses/cs3110/2011sp/Lectures/le...replyAnon4Now 11 hours ago | prev | next [\u2013]A couple things:This is admittedly a bit pedantic, but in E.F. Codd's original paper, he defined \"relation\" as the relation between a tuple (table row) and an attribute (table column) in a single table - https://en.wikipedia.org/wiki/Relation_(database). I'm not sure of the author's intent, but the user table example (user, country_id ) might imply the relationship between the user table and the country table. It's a common misconception about what \"relational\" means, but tbh I'm fine with that since it makes more sense to the average developer.If you ever need to join sets of data in code, don't use nested loops - O(n^2). Use a map / dictionary. It's one of the few things I picked up doing Leetcode problems that I've actually needed to apply irl.replyzzleeper 11 hours ago | parent | next [\u2013]I think it depends on the data.If it's presorted by the join variable then rolling the loop is faster. Also, if the index is too big for memory, then it might be faster to loop.replyAnon4Now 9 hours ago | root | parent | next [\u2013]Yeah. Many database tables are too large for an in memory hash join.My comment was a not very well fleshed out tangential remark on the value of practicing DS&A problems. I know a lot of devs hate Leetcode style interviews. I get it. It's not fun. But contrary to what some people say, I have run into a fair number of situations where the practice helped me implement more efficient solutions.replylopatin 14 hours ago | prev | next [\u2013]Very useful. Something like this but for Flink\u2019s fancy temporal, lateral, interval, and window joins would be great too.replydanbruc 13 hours ago | prev | next [\u2013]  1. cross join  2. natural join  3. equi join  4. theta join  5. inner join  6. left outer join  7. right outer join  8. full outer join  9. left semi join 10. right semi join 11. left anti semi join 12. right anti semi join 13. ???replyiaabtpbtpnn 12 hours ago | parent | next [\u2013]13. lateral join :)replyboredemployee 11 hours ago | prev | next [\u2013]I always thought venn diagram was a good representation but I think I was wrong.Edit: Why did I get down voted? :)replyericHosick 9 hours ago | parent | next [\u2013]I would also like to know why you are getting downvoted.Even wikipedia uses a Venn diagram to explain JOIN https://en.wikipedia.org/wiki/Join_(SQL) .Not trying to use an argument from authority but just pointing out that this is not unheard of.replyradiospiel 3 hours ago | root | parent | next [\u2013]Venn diagrams are a terrible way to describe join types (and, tbh, I don\u2019t understand why Wikipedia has these) because it makes it look like applying an 1:1 relationship. In a M:N relationship \u201eartefacts\u201c (for lack of better words) of both tables would appear multiple times, and the venn diagram obscures this factreplysomat 3 hours ago | parent | prev | next [\u2013]Because a venn diagram does not describe join mechanics well. A venn diagram does however describes the UNION, INTERSECT, EXCEPT part of sql.https://blog.jooq.org/say-no-to-venn-diagrams-when-explainin...And more meta, it is an innocent slightly incorrect statement, stuff like that should not be down voted, reply with a correction. Save down votes for outright malicious posts.replykadenwolff 11 hours ago | prev | next [\u2013]The title of this post sounds like an advertisement for a cultreplycaptaintobs 13 hours ago | prev | next [\u2013]Very cool work!replynickpeterson 14 hours ago | prev [\u2013]I imagine the title \u2018thirteen ways of looking at a join\u2019 was taken?replylbrindze 14 hours ago | parent [\u2013]Dunno why this was downvoted, I came here to make a similar comment about the possible Wallace Steven\u2019s reference.replylcnPylGDnU4H9OF 13 hours ago | root | parent [\u2013]Obviously I'd only be able to say for sure if I was the downvoter but I sometimes observe lighter text on comments which make a reference without calling out the reference. It's similar to using an acronym without defining it, though possibly more confounding if it's a niche enough reference. In this case, I did not get the reference and would have wondered why the parent commenter expected that title to have already been used.At best, such a comment is referencing something topical which most readers will get (and ostensibly be entertained by); at worst, it's a distracting non sequitur. It generally ties back to a community preference that comments are curiosity-satisfying before entertaining.replylbrindze 13 hours ago | root | parent [\u2013]Wouldn\u2019t an allusion to 20th century American poetry fall into the category of \u201ccuriosity satisfying\u201d? Given they were not the only person who got the reference it feels kind of arbitrary to say this is frivolous entertainment when another person in the community (in this case, me) found it curious and also wondered if there was an allusion there.If it was an intentional allusion, then it may actually add depth/meaning to the conversation but we may not know since it was already downvoted\u2026replylcnPylGDnU4H9OF 12 hours ago | root | parent [\u2013]It could be if it was called out as such. Without the explicit callout, one runs the risk of it \u201cgoing over the readers\u2019 heads\u201d so to speak. Anyway, I\u2019m not intending to justify any behavior; just offering my interpretation based on past observations.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- The article provides an analogy for understanding joins in terms of spatial dimensions, using the example of a Stargate scene.\n- The concept of joins in higher, non-spatial dimensions, such as adding time, is discussed.\n- The article introduces the idea of worst-case optimal joins, which allows joining three or more tables without temporary results."
  },
  {
    "id": 36575116,
    "timestamp": 1688401123,
    "title": "Where in the USA is this?",
    "url": "https://pudding.cool/games/where/",
    "hn_url": "http://news.ycombinator.com/item?id=36575116",
    "content": "How to PlayThere are five photos from the same place. You have five guesses to figure out where. A new photo is revealed after each guess. The tile color is how close your guess was:0miles100miles200miles200+milesBy Russell Samora. A new puzzle is released daily at midnight, EST.START",
    "summary": "- 'Where in the USA is this?' is a daily puzzle game where players have to guess the location of a place based on a series of five photos.\n- Each guess reveals a new photo and provides a color-coded tile that indicates the proximity of the guess to the actual location, ranging from 0 miles to 200+ miles.\n- The puzzle is released daily at midnight EST and can be accessed on 'The Tech Times' website.",
    "hn_title": "Where in the USA is this?",
    "original_title": "Where in the USA is this?",
    "score": 424,
    "hn_content": "- Many users found it difficult to understand that all the photos in the game were from the same location, despite the instructions clearly stating it.\n- Some users suggested improving the sentence structure or using better UX design to ensure users grasp the concept without reading instructions.\n- There was a discussion around whether users should read instructions or rely on intuition when interacting with software.\n- The game is similar to other guessing games like GeoGuesser and Wordle, which have gained popularity in recent years.\n- Some users noticed issues with the scale and radius of the game, suggesting improvements to make it more accurate.\n- The game sparked a sense of nostalgia for computer manuals from the 90s, with users sharing their fond memories of reading them.\n- Users shared their experiences and strategies for playing the game, including guessing based on the look and feel of different locations in the US.\n- Some users encountered technical issues with the game, such as unresponsive buttons or difficulty zooming in on photos.- Users are discussing a game or puzzle that involves guessing the location of a picture based on clues provided.\n- Some users prefer the challenge of not knowing if the clues will be helpful or not.\n- There are pictures with signs in them, contradicting a claim made earlier.\n- Users share their success in guessing the location, with some getting as close as 0 miles.\n- One user figured out the locations by viewing the source of the photos.\n- Another user jokes that their brain has been trained by the internet to guess the locations accurately.\n- Users discuss strategies like opening the image in a new tab and examining the filename for clues.\n- A user comments on the game's US-centric focus.\n- There is speculation about whether all users received the same set of pictures and how the game works.\n- The game involves five images in the same location, and a new set is released daily.\n- Clicking on the images tells you how far your guess is from the correct location.\n- The game is compared to Wordle.\n- The post concludes with technical information about guidelines, FAQ, lists, API, security, legal, applying to YC, and contact information.\n\nThe most important thing people should know about this post is the discussion about a game or puzzle that involves guessing the location of a picture based on clues. Users share their strategies, success in guessing, and opinions about the game's design. The post is special because it offers insights into how people approach and enjoy this game. It may be exciting for readers who are interested in puzzles or games and want to learn more about this particular one.",
    "hn_summary": "- Users discuss a game or puzzle that involves guessing the location of a picture based on clues provided.\n- Users share their strategies, success in guessing, and opinions about the game's design.\n- The post offers insights into how people approach and enjoy this game, which may be exciting for readers interested in puzzles or games."
  },
  {
    "id": 36580417,
    "timestamp": 1688428574,
    "title": "Leaking Bitwarden's Vault with a Nginx vulnerability",
    "url": "https://labs.hakaioffsec.com/nginx-alias-traversal/",
    "hn_url": "http://news.ycombinator.com/item?id=36580417",
    "content": "nginxHunting for Nginx Alias Traversals in the wildDaniel (Celesian) MatsumotoJul 3, 2023 \u2022 10 min readNginx, a versatile web server pivotal to numerous internet infrastructures, has held a dominant market share since its inception in 2004, with widespread adoption across websites and Docker containers. This article delves into the intricacies of Nginx, focusing on the location and alias directives that are central to how Nginx handles specific URLs. We also explore potential vulnerabilities arising from misconfigurations and demonstrate how they can lead to security exploits, drawing on research presented at the BlackHat 2018 conference by Orange Tsai.The guide further illustrates these points through a thorough examination of popular open-source repositories using GitHub Code Search to identify potential Nginx configuration vulnerabilities. Real-world case studies involving Bitwarden and Google's HPC Toolkit highlight the significant risk of data exposure if these vulnerabilities are not addressed. Additionally, we introduce NavGix, an automated tool designed to detect these vulnerabilities in a black-box manner, providing comprehensive insights into Nginx's complexities, vulnerabilities, and potential misconfigurations.Brief Overview of NginxNginx is a versatile web server that can also function as a reverse proxy, load balancer, mail proxy, and HTTP cache. The software was created and publicly released in 2004.As per W3Tech's data, as of June 2022, Nginx holds the highest market share among web servers, with 33.6% of websites on the internet utilizing it. Additionally, according to Docker, Nginx is the most deployed technology in their containers. This significant popularity makes vulnerabilities related to Nginx all the more critical and intriguing.Location and Alias DirectivesThe location directive is a block directive that can contain other directives and is used to define how Nginx should handle requests for specific URLs, they can be defined.It is often used in conjunction with the alias directive to map URLs to specific file locations on the server. the directives can be defined in the nginx.conf file or in a separate configuration file.The syntax for the location directive is as follows:location [modifier] /path/to/URL {  # other directives}The modifier is optional and can be one of the following:=: Exact match~: Case-sensitive regular expression match~*: Case-insensitive regular expression match^~: Prefix match (stop searching if this matches)Here is an example of how to use the location directive in the nginx.conf file:location /assets/ { # defines a location block for requests matching /assets    alias /opt/production/assets/; # maps the request to the assets folder  }Identifying MisconfigurationsAt the BlackHat 2018 conference, Orange Tsai presented his research on breaking URL parsers. Among other impressive findings, he demonstrated a technique discovered in a 2016 CTF challenge from HCTF, created by @iaklis.For the technique to be applicable, the following conditions must be met:The location directive should not have a trailing slash in its path;An aliasdirective must be present within the location context, and it must end with a slash.Achieving ImpactIn the vulnerable example above, Nginx will match any URLs that start with /img and serve whatever follows that slash with the alias path /var/images/ prepended.This implies that both a request for /img/profile.jpg and a request for /imgprofile.jpg would return the same file. Because since the alias directive ends with a trailing slash, an additional slash is not necessary after the matched location.Taking into account that we can access the target folder through any request URL starting with /img, we can attempt to access the ever-present .. directory, thereby reaching the parent directory of the target directory by issuing a request to /img.. for the given example.If we receive a redirection response from Nginx, we can assume that Nginx has located the directory and is attempting to redirect us to /img../, as it commonly does when accessing a directory.And we do!Consequently, this implies that any file or child directory within the parent directory of the target folder will be accessible to us, and Nginx will readily serve them. In our lab example, this means we could access all files in the /var/ folder, given that the target folder in the configuration is /var/images/. This allows us to utilize simple payloads, such as a GET to, /img../log/nginx/access.log to download a log file located on /var/log/nginx/access.log.We can even see our previous tests!The severity of this vulnerability can fluctuate significantly depending on the project, extending from a negligible impact to a critical one. The degree of its repercussions is primarily determined by whether the exposed directory holds sensitive data that may facilitate additional attacks or result in the disclosure of private information.Achieving Impact Without a Slash on AliasSame vulnerability, but without a trailing slash on alias.A question that may arise is whether this vulnerability can still be exploited without the trailing slash on the alias directive. The answer is yes, but it would mean that using traversal sequences to escape the directory would no longer be possible. This is because everything after the matched location is appended to the alias, and appending a .. sequence to a path without a trailing slash would only result in a non-existent folder name. e.g /var/images../However, we can still exploit this misbehavior to access other directories that have a name starting with the target directory name. As a result, we might not be able to access it /var/images/../log/, but we could still access a directory /var/images_confidential by making a GET request to /img_confidential.In this case, Nginx appends _confidential to the /var/images target path, effectively serving URLs from the combined path of /var/images and _confidential which results in /var/images_confidential.Hunting Open-Source RepositoriesAs a starting point in our search for this vulnerability, we chose to explore popular GitHub repositories that displayed this issue. Identifying this specific vulnerability in environments with access to source code becomes significantly more feasible, primarily due to two main factors:Detection: Utilizing straightforward code analysis tools, such as regular expression searches, allows us to effectively pinpoint potentially vulnerable Nginx configuration files within these projects.Exploitation: Having knowledge of the exact target directory that has been aliased empowers us to set up a local instance, examine the aliased directories using a local shell, and determine which files can be accessed through the vulnerability.GitHub Code SearchGitHub Code Search is a feature available on GitHub, the web-based platform for version control and collaboration using Git. This feature enables users to search for code across all public repositories hosted on the platform, making it especially useful for developers seeking examples, libraries, or solutions to specific coding challenges.Additionally, GitHub Code Search can be employed to search for snippets of vulnerable code within popular projects. This can be accomplished through a variety of methods, such as simple string matches, regular expressions, path filters, and more. For example, to search for the Nginx Alias Traversal vulnerability, the following regular expression can be used:/location \\/[_.a-zA-Z0-9-\\/]*[^\\/][\\s]\\{[\\s\\n]*alias \\/[_.a-zA-Z0-9-\\/]*\\/;/Upon examining the search results for this query, it becomes evident that a significant number of repositories contain this specific vulnerability.Due to the inherent limitations of regular expressions, they may not be ideally suited for matching code syntax. For instance, this particular regular expression will not match vulnerable configuration files containing comments between the directives. However, it serves as a starting point for our analysis.Case Study #1: Leaking Bitwarden's vault, logs, and certificates.Bitwarden is an open-source password manager that helps users securely store and manage their passwords, credentials, and other sensitive information. It offers features such as password generation, autofill, and synchronization across devices. Bitwarden supports various platforms, including Windows, macOS, Linux, Android, iOS, and web browsers through extensions. Users can access their data through a web vault, desktop apps, mobile apps, or browser extensions.Bitwarden also offers a self-hosted option for those who want to maintain their own server, which is the one we are going to examine.If we search for ways to create a self-hosted instance of the Bitwarden server, one of the presented ways is through the Unified docker method, which is a setup made for simplifying the deployment of the platform.100K+ downloads!Since our regular expression gave a match for Bitwarden's repository, we started dwelving into the code base searching for potentially vulnerable Nginx configurations, which is when we found the following inside the folder for the unified docker setup.If this config is used, then it means all files on /etc/bitwarden/attachments/ will be accessible on the URL /attachments, but looking back at the exploit details, this also means that any files present on /etc/bitwarden/ will be downloadable also.But let's not get ahead of ourselves, we first must assess the impact of the exposure. To do that, we can either explore the code base or fire up a local instance and list the directory with a local shell.Looking into the Dockerfile for that image, we find more info about that directory and what files it can contain:[...]ENV BW_DB_FILE=\"/etc/bitwarden/vault.db\"[...]DockerfileIt seems like this environment variable sets where the vault database is saved, and we can see that vault.db is located on /etc/bitwarden.Bitwarden only saves the database in that location if the user chooses to use SQLite as a database provider.Therefore, if we issue an unauthenticated request to http://<instance>/attachments../vault.db, we will download the entire Bitwarden SQLite3 database.We can also fetch log files, which have a predictable filename and can all be downloaded by accessing the following paths:/attachments../logs/api.log/attachments../logs/admin.log/attachments../logs/identity.log/attachments../logs/notifications.logAnd, of course, the certificate file was also exposed in that folder. To access it, all you would need to do is issue a request to /attachments../identity.pfx.The data protection keys were also accessible, but these did not have a predictable filename, therefore, leaking them was not possible.This vulnerability has been disclosed to Bitwarden and has since then been fixed. Bitwarden issued a US$6000 bounty, which is the highest bounty they issued on their HackerOne program.Case Study #2: Google HPC Toolkit - Leaking Google Cloud CredentialsDuring our foray through GitHub, we chanced upon a software solution developed by Google, known as the Cloud HPC Toolkit. This was introduced in 2022, designed as a robust framework to facilitate the deployment of high-performance computing (HPC) environments on Google Cloud.The HPC Toolkit boasts a Django-based web application front-end, granting users the ability to manage their HPC environments conveniently via a web interface.Upon scrutinizing the configuration section identified by our regular expression, we discovered that a vulnerable path had indeed been defined. This path was aliased to ../hpc-toolkit/community/front-end/website/static/, implying that issuing a request to /static../ would provide us with access to the website folder.Interestingly, we found that the SQLite database was also exposed within this folder and could be accessed through a specific URL:curl http://<frontend URL>/static../db.sqlite3 -OMoreover, the Django secret key was accessible at the following location:curl http://35.204.135.69/static../.secret_keyGaining access to this database is highly critical for the application, as the primary function of the HPC Toolkit appears to be the orchestration of large-scale Google Cloud resources. In the event of a compromise, an attacker could potentially gain control over a victim's GCP account credentials, which are stored in the SQLite database.sqlite> select * from ghpcfe_credential;1|production key|{ \"type\": \"service_account\", \"project_id\": \"andunduaindadaww\", \"private_key_id\": \"3acb9f[... redacted from report ...]7c69\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEv[... redacted from report ...] 5Kdkvg=\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"adwaw[...].com\", \"client_id\": \"105114036295455180401\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://oauth2.googleapis.com/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/adwaw1f13f1f13-tkfe-sa%40andunduaindadaww.iam.gserviceaccount.com\"}|1sqlite>The Google VRP Team recognized our work by awarding us a $500 reward for uncovering this vulnerability. They believed the impact on the application wasn't severe enough to warrant a larger reward. I toyed with the idea of debating the reward amount with them, but ultimately decided against it. After all, the recognition of our efforts was a reward in itself, and that was more than enough for us.NavGix: Detecting the vulnerability in a black-box mannerThere are several methods to detect this vulnerability without necessitating access to the Nginx configuration file. Initially, potential location aliases can be identified by extracting links from the HTML source-code on the website's main page. Subsequently, directory traversal can be attempted using the techniques outlined in the earlier sections of this report.In the absence of extractable links, it is feasible to perform a minor brute-force attack targeting common aliases. The ensuing list has demonstrated promising results during our testing phase: var dictionary = []string{ \"static\", \"js\", \"images\", \"img\", \"css\", \"assets\", \"media\", \"lib\", }An automated tool, NavGix, has been created to aid in the enumeration and testing of aliased directories for traversal vulnerabilities. It is publicly available for download and use on its respective GitHub page.Sample usage of navgixUpon the identification of a directory susceptible to traversal by NavGix, it becomes possible to employ additional tools to fuzz for other accessible folders or files within the traversed directory. The primary objective here should be to locate files of interest that could potentially provide further impact, such as a configuration file containing secrets, log files, or source-code.ConclusionWrapping up, while Nginx is a robust and incredibly versatile tool that fuels a significant portion of the internet, it's easily susceptible to certain inconsistencies. These potential pitfalls are often a result of misconfigurations, which can inadvertently transform this reliable powerhouse into a possible weak link. Nginx's approach to security leaves a significant onus on developers to avoid hazardous configurations, underscoring the importance of thorough understanding and cautious implementation.Our journey through the world of open-source repositories and real-life case studies like Bitwarden and Google's HPC Toolkit underlines just how significant these vulnerabilities can be. It's a sobering reminder that even the most reliable systems can have their Achilles' heel.",
    "summary": "- Nginx, a popular web server, has potential vulnerabilities related to misconfigurations in the location and alias directives.\n- These vulnerabilities can lead to data exposure, allowing unauthorized access to sensitive files and directories.\n- Automated tools like NavGix can help identify and test for these vulnerabilities, making it important for developers to understand and implement secure configurations.",
    "hn_title": "Leaking Bitwarden's Vault with a Nginx vulnerability",
    "original_title": "Leaking Bitwarden's Vault with a Nginx vulnerability",
    "score": 397,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginLeaking Bitwarden's Vault with a Nginx vulnerability (hakaioffsec.com)397 points by celesian 10 hours ago | hide | past | favorite | 116 commentsevgpbfhnr 8 hours ago | next [\u2013]FWIW gixy (nginx configuration checker) catches this: https://github.com/yandex/gixy/blob/master/docs/en/plugins/a...(and nixos automatically runs gixy on a configuration generated through it, so the system refuses to build <3)replytsak 1 hour ago | parent | next [\u2013]Thank you. I didn't know about gixy and ran it on my home server which found a vulnerability ($uri in a 301 redirect)replywredue 8 hours ago | parent | prev | next [\u2013]I just gave nix a go and so far it seems great.But do you know, if they\u2019re a nicer options finder? The one I found where you just search all several thousand options kinda sucks. I want to just see my package (say, ssh) and just the ssh options, but the results get littered with irrelevancy.replyevgpbfhnr 8 hours ago | root | parent | next [\u2013]When I roughly know what I'm doing I use search.nixos.org; if you give it the full services.foo prefix it's usually relevant enough, e.g. for ssh you'd want \"services.openssh\", which you can find skimming through the results of just searching 'ssh' first:https://search.nixos.org/options?channel=unstable&from=0&siz...For anything I'm not 100% sure will be obvious I search through a local clone of the nixpkgs repo directly, but I'll be honest and say I just never took time to search for a better toolreplylicebmi__at__ 5 hours ago | root | parent | prev | next [\u2013]I would suggest using man and searching like any piece of documentation. Specifically you are looking for `man configuration.nix`replySuperSandro2000 2 hours ago | root | parent | next [\u2013]That's actually worse search experience and slower because less is struggling with the amount of lines.replyJasonSage 8 hours ago | root | parent | prev | next [\u2013]My main usage of Nix is on non-NixOS machines, and I use Home Manager, and while it has a similar problem, just searching the options in the packages it provides configuration for is a smaller issue.Not sure if this helps you at all or not, it really depends on your usage of Nix, but for managing user configuration I do recommend Home Manager.replysmoldesu 5 hours ago | root | parent | prev | next [\u2013]> if they\u2019re a nicer options finder?https://mynixos.com/> I want to just see my package (say, ssh) and just the ssh optionshttps://mynixos.com/nixpkgs/options/programs.sshreplyGlitchMr 2 hours ago | parent | prev | next [\u2013]NixOS doesn't run Gixy anymore, see https://github.com/NixOS/nixpkgs/pull/209075.replySuperSandro2000 2 hours ago | root | parent | next [\u2013]NixOS core maintainer here. That's about nginx' own test. Gixy is still run when writing any nginx config file with the writer helper function https://github.com/NixOS/nixpkgs/blob/b6cc06826812247fe54655...reply542458 9 hours ago | prev | next [\u2013]At risk of asking a dumb question, is there any good reason that you\u2019d want nginx to allow traversing into \u201c..\u201d from a URL path? It just seems like problems waiting to happen.Edit: Actually, I\u2019m a bit lost as to what\u2019s happening in the original vuln. http://localhost/foo../secretfile.txt gets interpreted as /var/www/foo/../secretfile.txt or whatever\u2026 but why wouldn\u2019t a server without the vulnerability interpret http://localhost/foo/../secretfile.txt the same way? Why does \u201c..\u201d in paths only work sometimes?replylyu07282 9 hours ago | parent | next [\u2013]That has been a known issue in nginx for a very long time and its a common attack vector at CTFs:https://book.hacktricks.xyz/network-services-pentesting/pent...replymagicalhippo 2 hours ago | root | parent | next [\u2013]There is a LFI vulnerability because:  /imgs../flag.txtTransforms to:  /path/images/../flag.txtI've only implemented a handful of HTTP servers for fun, but I've always resolved relative paths and constrained them. So I'd turn \"/path/images/../flag.txt\" into \"/path/flag.txt\", which would not start with the root \"/path/images/\" and hence denied without further checks.Am I wrong, or, why doesn't nginx do this?replyhanikesn 1 hour ago | root | parent | next [\u2013]It does when you use the root directive. Alias should hardly be used if possible for those reasons.replypravus 1 hour ago | parent | prev | next [\u2013]The problem is that a URL isn't actually a path. It's an abstract address to a resource which can be a directory or file (or an executable or stream or ...).In this case part of the URL is being interpreted by nginx as a directory (http://localhost/foo) due to how that URL is mapped in the configuration to the local filesystem. Apparently it references a directory, so when nginx constructs the full path to the requested resource, it ends up with \"${mapped_path}/../secretfile.txt\" which would be valid on the local filesystem even if it doesn't make sense in the URL. Notice how the location of the slashes doesn't matter because URLs don't actually have path elements (even if we pretend they do), they are just strings.This is a very common problem that I have noticed with web servers in general since the web took off. Mapping URLs directly to file paths was popular because it started with simple file servers with indexes. That rapidly turned into a mixed environment where URLs became application identifiers instead of paths since apps can be targeted by part of the path and the rest is considered parameters or arguments.And no, it generally doesn't usually make sense to honor '.' or '..' in URLs for filesystem objects and my apps sanitize the requested path to ensure a correct mapping. It's also good to be aware that browsers do treat URLs as path-like when building relative links so you have to be careful with how and when you use trailing '/'s because it can target different resources which have different semantics on the server side.replySahAssar 9 hours ago | parent | prev | next [\u2013]Not in any \"normal\" use-case, no. It'd make sense to make this behavior opt-in, like having a `allow_parent_traversal on;` flag in the location.replyaidenn0 9 hours ago | parent | prev | next [\u2013]Just guessing, but NginX probably either checks for \"/foo/bar/..\" and disallows it, or normalizes it to \"/foo/\" but \"/foo/bar..\" is a perfectly valid file name, so it doesn't get caught by the net checking for this.replydumpsterdiver 6 hours ago | parent | prev | next [\u2013]> Why does \u201c..\u201d in paths only work sometimes?That fully depends upon the file permissions. In this case, let's assume that a user has permissions to read files all the way from the web index directory (../index.html) back to the root directory (/). At that point, since they have permission to traverse down to the root directory, they now have permission to view any world viewable file that can be traversed to from the root directory, for instance /etc/passwd.In other words, imagine a fork with three prongs, and your web server resides on the far right prong. Imagine that the part of the fork where the prongs meet (the \"palm\" of the fork) is the file system. If your web server residing on the far right prong of that fork allows file permission to files and directories that lead all the way to the palm of the fork, at that point you could continue accessing files on other prongs once you have reached the palm.replykomali2 6 hours ago | root | parent | next [\u2013]Isn't setting correct permissions for www-data like, the first note in a bunch of \"secure your web server\" tutorials? I thought if read is only set for the directory with actual public files, and not for the parent directory, there should be no traversal possible like this?replyqwertox 9 minutes ago | prev | next [\u2013]What would I need to grep my nginx logs for to see if my possibly misconfigured servers were exploited? [^/]+\\.\\. (not adding a question mark after that regex even though I'm asking if that one would be ok)replytechnion 7 hours ago | prev | next [\u2013]OK hear me out: a Linux capability like option that removes the .. option from the kernels file name parser.Like web apps have been seen various bypasses involving somehow smuggling two dots somewhere since we were on dial up modems. It's time to look for a way to close this once and for all, as the Linux kernel has done with several other classes of user land bugs.replyilyt 16 minutes ago | parent | next [\u2013]  /some/../path should pretty much 100% of the time be disallowed, there is no sensible use case that is not \"someone wrote ugly code\"../some/path makes sense sometimes at least... but I'd imagine it wouldn't as useful as you think it is, because many apps resolve .. before passing it to the OSreplyloeg 7 hours ago | parent | prev | next [\u2013]https://man7.org/linux/man-pages/man2/openat2.2.html RESOLVE_BENEATH(FreeBSD has this in ordinary openat(2) as O_RESOLVE_BENEATH.)replyjunon 1 hour ago | parent | prev | next [\u2013]That makes no difference. Code often normalizes paths before they ever touch the filesystem APIreplym00x 5 hours ago | parent | prev | next [\u2013]That would break so many things that it would be insane to do.You could just run nginx as a separate user with very limited rights, or just run it on Docker. This, plus updating regularly usually fixes 90% of security issues.replyarchi42 2 hours ago | root | parent | next [\u2013]Most (I hope all) distributions already run nginx as a separate user. It's best practice.But that won't help if you alias to \"/foo/bar/www\" and the the application has a SQLite database at \"/foo/bar/db.db\", which the nginx user has to have access to. Same if you run it in a container (or lock down permissions using systemd).replymartinflack 5 hours ago | root | parent | prev | next [\u2013]But the issue is -- would it break the things a web server is doing? It doesn't have to be a universal solution.replyikekkdcjkfke 3 hours ago | parent | prev | next [\u2013]It's something else in the kernel, there we have the permission system which we rely on.If you are serving files to web from the folder, the web framework should handle not taversing the public root folder it was tasked to serve. If are rolling your own, well now you have to consider all kinds of stuff, including this.replyamluto 9 hours ago | prev | next [\u2013]How is this not seen as a vulnerability in nginx? This behavior is utterly absurd, seems to have no beneficial purpose, and straightforwardly exploitable.replyphendrenad2 7 hours ago | parent | next [\u2013]It's done for speed. Straightforward text replacement is so much faster than checking to see if a path is properly terminated by a slash. And remember that Nginx became popular due to benchmarks that showed that it was more \"web scale\" than Apache2.replyamluto 6 hours ago | root | parent | next [\u2013]I find it hard to believe that searching for \u201c..\u201d would even show up in a benchmark.In any case, it seems that nginx does try to search for .. but has a bug in the corner case where the \u201clocation\u201d doesn\u2019t end with a slash. I assume there\u2019s some kind of URL normalization pass that happens before the routing pass, and if the route matches part of a path component, nothing catches the ..If I\u2019m right, this is just an IMO rather embarrassing bug and should he fixed.replyD13Fd 6 hours ago | root | parent | next [\u2013]Yeah, this whole thing reads to me like a bug in nginx. There is no obvious reason users would need that functionality.replym3affan 5 hours ago | root | parent | next [\u2013]What are the alternarives though?replyseanw444 4 hours ago | root | parent | next [\u2013]To Nginx? HAProxy. Or Caddy if you're just doing web stuff.replyterom 49 minutes ago | root | parent | next [\u2013]HAProxy does not serve static files, so it's a poor alternative for this specific case.replysofixa 3 hours ago | root | parent | prev | next [\u2013]> And remember that Nginx became popular due to benchmarks that showed that it was more \"web scale\" than Apache2.More like because it was much faster out of the box, and came with many batteries included while Apache2 required mods to be separately install.replyrobertlagrant 1 hour ago | root | parent | next [\u2013]And the config was nicer to read and write.replyDecabytes 9 hours ago | prev | next [\u2013]Glad that the leaks are still encrypted. Even companies that specialize in this sort of stuff are not immune to leaks, so this is honestly the best case scenario.replywhiskeymikey 8 hours ago | prev | next [\u2013]This is probably a dumb question but why would Bitwarden allow unauthenticated requests to /attachments at all? Even with the Nginx bug, wouldn\u2019t the request have failed if that URL required authentication?replySomeone1234 7 hours ago | parent | next [\u2013]This is an exploit against the web server's configuration, so never executes Bitwarden's authentication code or any Bitwarden code at all. It isn't unusual or incorrect for projects to use their own authentication rather than Nginx or a module.It is still Bitwarden's responsibility since they shipped a dangerous configuration via Docker. Which they seemingly acknowledge and have since fixed.replyautoexec 5 hours ago | root | parent | next [\u2013]> It is still Bitwarden's responsibility since they shipped a dangerous configuration via Docker. Which they seemingly acknowledge and have since fixed.The screenshot makes it look like the docker setup option was still in beta and the page had warnings all over it saying there could be possible issues. I can't really judge Bitwarden too harshly here for releasing something in beta that was later found to have a vulnerability in it.replywhiskeymikey 7 hours ago | root | parent | prev | next [\u2013]Ahh okay. That explanation makes sense. Thanks!replykibwen 9 hours ago | prev | next [\u2013]If all you need is a simple way to serve static files that minimizes resource consumption and is reliably secure, what is the state of the art these days? In the past I would probably reach for Nginx, but I wonder if a more focused/less configurable tool would be preferable from a security standpoint.replycyrnel 5 hours ago | parent | next [\u2013]I use https://static-web-server.net/Cross-platform, written in Rust, straightforward configuration, secure defaults, also has a hardened container image and a hardened NixOS module.I wouldn't recommend Caddy. Their official docker image runs as root by default [1], and they don't provide a properly sandboxed systemd unit file [2].[1]: https://github.com/caddyserver/caddy-docker/issues/104[2]: https://github.com/caddyserver/dist/blob/master/init/caddy.s...EDITED: phrasingreplyfrancislavoie 9 hours ago | parent | prev | next [\u2013]Shameless plug: Caddy does a great job here. Automatic HTTPS, written in Go so memory safety bugs are not a concern, has a solid file_server module.replyprincevegeta89 7 hours ago | root | parent | next [\u2013]+1 to Caddy. Just tried it recently and I was very happy to forget all the nginx jargon the next moment.replyalexalx666 3 hours ago | root | parent | prev | next [\u2013]Im using caddy, it's great!replyusername135 8 hours ago | root | parent | prev | next [\u2013]Isn't everything forced to https nowreplyehnto 8 hours ago | root | parent | next [\u2013]Browsers try to navigate you to HTTPS but no, http the protocol is still working as it always has. Both nginx and Apache will require configuration to serve HTTPS.You might still use HTTP on an internal network in a DMZ or other trusted network.replyhousemusicfan 8 hours ago | root | parent | prev | next [\u2013]No.Some things were never meant to be, like downloading CRLs over HTTPS.replypepa65 6 hours ago | parent | prev | next [\u2013]I have used Caddy for years, automatic SSL certificates, does file serving, does reverse proxy, very easy and clear to configure. Single-binary (Go) so easy to \"install\", single configfile.replydylan604 6 hours ago | parent | prev | next [\u2013]how is a static site served from S3 considered in these parts of the interweb? i've never done this, but see it as an option, yet i never really hear others using it either.replysofixa 3 hours ago | root | parent | next [\u2013]In my view, it's perfect (okay, maybe slightly less than perfect, and dedicated platforms taking ot to the next level like Netlify, CloudFlare Pages, Firebase Hosting, etc are for their added related services and tools, as well as their generous free tiers). It's pay as you go, scales from zero to infinite, and has zero attack surface or maintenance.I've run a couple of websites (WordPress or Hugo based, including my personal blog) like that and it's great.replycrote 3 hours ago | root | parent | prev | next [\u2013]You probably want some kind of CDN to avoid a HN frontpage link from making you go bankrupt, but it's a pretty decent solution.I personally prefer something like Github Pages, though - it doesn't get much more hands-off than that!replychrisweekly 5 hours ago | root | parent | prev | next [\u2013]Good Q. Using S3 as origin behind Cloudfront seems like a pretty standard AWS CDN setup for static assets... but S3 isn't a traditional web server.replyadventured 9 hours ago | parent | prev | next [\u2013]Caddy is pretty simple to configure and serve static files from.replyhousemusicfan 8 hours ago | parent | prev | next [\u2013]Merecathttps://github.com/troglobit/merecat/replypepa65 6 hours ago | root | parent | next [\u2013]Last release 2016??replyhousemusicfan 5 hours ago | root | parent | next [\u2013]OP wanted a simple web server for serving static content. Are you aware of open CVEs? No? It's possible for software to be done you know. Just because something isn't a rolling release of change for the sake of change (like most Google crapware) doesn't mean it isn't fit for purpose.replycrote 3 hours ago | root | parent | next [\u2013]Considering the vast majority of commit were made after 2016, I don't think it is \"done\".And a C program, written by a single developer, with only 27 issues ever being filed? With all due respect, that's guaranteed to have some nasty bugs in there.replycalvinmorrison 6 hours ago | parent | prev | next [\u2013]werc, shttpd, etc.Treat any web request like you would a real user on a Linux system you'd need to give access to to download files via scp. Chroot, strict permissions, etc. Can't escape what you can't escape. A ../ should return the same as expected in the shell, permission deniedreplyilyt 19 minutes ago | prev | next [\u2013]Don't let web server access app's code, soo many security problems solved...replygostsamo 4 hours ago | prev | next [\u2013]The title is significantly editorialized. The post title is:Hunting for Nginx Alias Traversals in the wildand the hn submission highlights the bitwarden vulnerability while there is a google one discussed as well.replyphendrenad2 7 hours ago | prev | next [\u2013]This has nothing to do with bitwarden. This is a generic directory traversal attack (enabled by Nginx's configuration language being full of serious gotchas).replyComputerGuru 7 hours ago | parent | next [\u2013]It does have to do with BitWarden: they wrote and shipped the buggy config.replyautoexec 5 hours ago | root | parent | next [\u2013]It looks like they did say it was still beta and warned there could be issues though. I'll give them credit for that much.replybrigandish 8 hours ago | prev | next [\u2013]The article didn't mention permissions, would this still work if the nginx user is denied permissions on things like `/var/log`? I suspect it wouldn't but isn't the most common cause of security flaws going to be unchecked assumptions?As an aside, I didn't know Github code search accepted regex.replyVWWHFSfQ 7 hours ago | parent | next [\u2013]no it wouldn't work if the user nginx is running as didn't have read access to the directory or filesreplykomali2 6 hours ago | root | parent | next [\u2013]Ah then I just realized, it probably does have access to all nginx log directories, because nginx needs write permissions to them anyway, right? Now I really want to go double check all my permission setups...replycrote 3 hours ago | root | parent | next [\u2013]It depends on how nginx is designed. In theory you could separate log writing into a different process, and drop those permissions from the worker process.Or just write to stdout and have systemd handle the logging for you, that'd work too.replyandrewstuart 8 hours ago | prev | next [\u2013]I dropped nginx because it was really fiddly to configure and misconfiguration has potentially bad consequences.replydylan604 6 hours ago | parent | next [\u2013]what webserver did you not just describe?replypepa65 6 hours ago | root | parent | next [\u2013]Caddy?replyjand 5 hours ago | prev | next [\u2013]Please excuse the silly question: Would proper directory and file ownerships not prevent this traversal?If nginx does not run as root, how can it read other files than the ones explicitly assigned to the nginx user?replyilyt 9 minutes ago | parent | next [\u2013]It would absolutely prevent it. Run app as one user, nginx as other, go-rwx on all app files, set the group of the \"static\" files as www-data and g+r on them and now web server can't access app files.It's LITERALLY app hosting 101 and people did it that way 20+ years ago.replyPhilipRoman 2 hours ago | parent | prev | next [\u2013]Ah the wonders of 022 umask. Personally I would always recommend making files unreadable to other users. If not for all files then at least significant directories like everything under /home, etc.It may require more fiddling with group memberships, but it's well worth it.replyoefrha 4 hours ago | parent | prev | next [\u2013]Typical umask is 022 so most things are readable by nginx workers but not writable, they don\u2019t need to be explicitly assigned (e.g. to www-data). If your application generates sensitive data of course you should probably use a 077 umask.replyJolter 3 hours ago | root | parent | next [\u2013]You could make an argument that bitwarden vaults constitute sensitive information.replyNoMoreNicksLeft 4 hours ago | parent | prev | next [\u2013]I don't know about everyone else, but at this point I'm no longer doing a proper installation of nginx for personal stuff. I always just spin up a docker image... and I'm not checking if it runs as root or not, really.Probably really screwing things up. Ouch.replyfrays 4 hours ago | parent | prev | next [\u2013]You are correct.Unfortunately, nginx (and other web servers) generally need to run as root in normal web applications because they are listening on port 80 or 443. Ports below 1024 can be opened only by root.A more detailed explanation can be found here: https://unix.stackexchange.com/questions/134301/why-does-ngi...replyduijf 4 hours ago | root | parent | next [\u2013]> Ports below 1024 can be opened only by root.Or processes running with the CAP_NET_BIND_SERVICE capability! [1]Capabilities are a Linux kernel feature. Granting CAP_NET_BIND_SERVICE to nginx means you do not need to start it with full root privileges. This capability gives it the ability to open ports below 1024Using systemd, you can use this feature like this:  [Service]  ExecStart=/usr/bin/nginx -c /etc/my_nginx.conf  AmbientCapabilities=CAP_NET_BIND_SERVICE  CapabilityBoundingSet=CAP_NET_BIND_SERVICE  User=nginx  Group=nginx(You probably also want to enable a ton of other sandboxing options, see `systemd-analyze security` for tips)[1]: https://man7.org/linux/man-pages/man7/capabilities.7.htmlreplyoefrha 4 hours ago | root | parent | prev | next [\u2013]Nginx workers shouldn\u2019t run as root and certainly don\u2019t on any distro I know. Typically you have a www-data user/group or equivalent. Dropping privilege is very basic.replyguraf 4 hours ago | root | parent | prev | next [\u2013]Nginx is started as root but it does not run as root, it changes its user after opening log files and sockets. (unless you use a lazy docker container and just run everything as root inside it).replyjand 4 hours ago | root | parent | next [\u2013]Even in (the official) docker image, a nginx user is created: (latest, layer 6)/bin/sh -c set -x && groupadd --system --gid 101 nginx && useradd --system --gid nginx --no-create-home --home /nonexistent --comment \"nginx user\" --shell /bin/false --uid 101 nginx .....[1] https://hub.docker.com/layers/library/nginx/latest/images/sh...replykentt 8 hours ago | prev | next [\u2013]If I understand correctly, this is a vulnerability in self-hosted Bitwarden only. Is that correct?replyemaciatedslug 4 hours ago | parent | next [\u2013]Yes, per the article: \"Bitwarden also offers a self-hosted option for those who want to maintain their own server, which is the one we are going to examine.\"replyzeeZ 3 hours ago | parent | prev | next [\u2013]This is for the single image self-hosted setup method, which is still in beta. The current supported self-hosted setup is a script that creates a bunch of individual containers for the different services.replysneak 9 hours ago | prev | next [\u2013]Note that this leaks the vault with secrets encrypted - a leak of the cyphertext.> This vulnerability has been disclosed to Bitwarden and has since then been fixed. Bitwarden issued a US$6000 bounty, which is the highest bounty they issued on their HackerOne program.That's a ridiculously low payout.replyandersa 9 hours ago | parent | next [\u2013]Small companies can't just give out $50k bounties, even if it would be deserved.replylyu07282 8 hours ago | root | parent | next [\u2013]They raised 100 million $ last yearhttps://siliconangle.com/2022/09/06/bitwarden-reels-100m-ope...replyandersa 4 hours ago | root | parent | next [\u2013]Huh. First time hearing about this. No longer a small company then.replydghlsakjg 9 hours ago | parent | prev | next [\u2013]I don\u2019t know enough about bounty programs to comment on the amount, but my understanding is that leaking encrypted secrets isn\u2019t really dangerous?replyNoZebra120vClip 9 hours ago | root | parent | next [\u2013]It's generally a question of time.If you want to play the long game and collect a lot of encrypted data now, you can simply wait until it is possible to trivially decrypt, and/or start cracking now and let the years work on it.Most encryption decisions are framed as a tradeoff of the time and resources it would currently take to brute-force your way through it, and how many years before a simple attack becomes feasible, vs. your $5 wrench attacks in the present day.replynyolfen 9 hours ago | root | parent | next [\u2013]BW uses 100K rounds of PBKDF2 for the master password so I don't think that will be any time soonreplySV_BubbleTime 8 hours ago | root | parent | next [\u2013]BW now uses Argon2 over PBKDF. I can\u2019t remember if that is by default, opt-in, or new accounts. But barring an argon vuln, this is even less of a concern.Also, I think BW has been using more than 100k for some time now. Last I saw 600K was the recommendation.replyemaciatedslug 7 hours ago | root | parent | next [\u2013]The default for new Bitwarden accounts from Feb 2023 on is PDBFK2 HMAC SHA 256 setting at 600,001 iterations on the client and 100,000 on the server with the option to use Argon2id. These settings are above current OWASP recommendations. https://cheatsheetseries.owasp.org/cheatsheets/Password_Stor... https://bitwarden.com/help/kdf-algorithms/replyNoZebra120vClip 6 hours ago | root | parent | next [\u2013]All the replies have given random statistics, but these don't shed much light on the length of time it may take an attacker to brute-force a password, or find a chink in the armor of the vault's encryption algorithm.Now as I said, a significant threat actor with lots of time in their future plans can collect encrypted stuff such as vaults and bide their time. Someday, the decryption may be cost-effectively cheap. Someday, a flaw may be uncovered in the cryptography. Someday, a vault owner's secret key(s) may leak and can be correlated.As I said, it's just a question of time, and the ability to hold on to your cards for long enough that they can be played in the proper manner. It may take 5 years, 10 or 20, but if the payoff is valuable enough, it's worth the wait for the threat actor.replyrcxdude 7 hours ago | root | parent | prev | next [\u2013]a password vault contains a lot of long-lived secrets protected by a human-provided key, so it's really not something you want out there, even encrypted.replyBluecobra 6 hours ago | root | parent | next [\u2013]I would assume most people that are doing self-hosted are securing it behind a VPN like Wireguard instead of opening it to the whole web. (at least I hope so)replydiarrhea 4 hours ago | root | parent | next [\u2013]I am not. Working well so far. My instance is behind Caddy, behind a secret URL path. To talk to the instance, this \u201cpre-shares secret\u201d needs to be known first. So far I haven\u2019t seen any abnormal hits. I\u2019m closing in on 3 years of using it in this setup, via Vaultwarden.I\u2019m aware that this is security through obscurity. The instance\u2019s accounts use strong passwords and MFA.replydonutshop 5 hours ago | root | parent | prev | next [\u2013]I thought so too. But then did a quick search on Shodan and found these:https://www.shodan.io/search?query=bitwardenhttps://www.shodan.io/search?query=vaultwardenreplyberkes 2 hours ago | root | parent | prev | next [\u2013]I'm afraid not. I've seen some really dumb setups of BW when helping selfhosted.I do think that while selfhosting is admirable, in the case of your password vault, it's not. It's one thing where I'd always advice against selfhosting or DIY, because the downside risk is just too big.The chance of fng up may be tiny, bit if you fck up, it's bad. Potentially bankruptcy or jail bad.replydw33b 9 hours ago | parent | prev | next [\u2013]not compared to the $500 Google gave themreplygostsamo 4 hours ago | root | parent | next [\u2013]Not sure why your comment is last in the page. Google have significantly more resources and the authors looked to disagree with the amount awarded for the google vulnerability.replyem1sar 2 hours ago | prev [\u2013]Okay so I self-host Vaultwarden, what do I need to do to fix the vulnerability? The article mentions another flavor of the self hosted docker image though.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Bitwarden's vault can be leaked through a Nginx vulnerability in its configuration language.\n- The vulnerability allows for directory traversal attacks, exposing sensitive files.\n- Bitwarden has acknowledged and fixed the issue, and even issued a bounty for its discovery."
  },
  {
    "id": 36573871,
    "timestamp": 1688396452,
    "title": "Ask HN: Who is hiring? (July 2023)",
    "url": "",
    "hn_url": "http://news.ycombinator.com/item?id=36573871",
    "content": "",
    "summary": "- The post is titled \"Ask HN: Who is hiring? (July 2023)\" and it is asking for information about job openings in the tech industry.\n- The post is from July 2023, so it is up-to-date information about current job opportunities.\n- People who are looking for a job in the tech industry would be interested in this post because it provides a platform for employers to share job openings and for job seekers to find potential opportunities.",
    "hn_title": "Ask HN: Who is hiring? (July 2023)",
    "original_title": "Ask HN: Who is hiring? (July 2023)",
    "score": 387,
    "hn_content": "- Hacker News is hosting a \"Who is hiring?\" thread for July 2023, where companies can post job openings.\n- Companies from various locations are hiring for remote, onsite, and intern positions.\n- Some notable companies include Proton in Switzerland/EU, seeking a Python Engineer for their VPN Linux client; Nascent in Bangalore/India, looking for a Research Engineer in ML-imaging for academic curricula redesign; Hypertune in the UK, seeking a Founding Full Stack Software Engineer for their code configuration platform; and HRL Laboratories in Los Angeles, searching for a Senior Software Engineer or Software Engineering Manager experienced in Common Lisp to work on quantum computing.\n- Other companies with job openings include Journee in Berlin, Quatt.io in Amsterdam working in climate tech, Y Combinator in the Bay Area looking for an Infrastructure Software Engineer, and more.\n- These job postings are ideal for university students and entry-level software engineers who are new to the industry and are interested in working on cutting-edge technologies and innovative projects in different locations around the world.",
    "hn_summary": "- Hacker News is hosting a \"Who is hiring?\" thread for July 2023, featuring job openings from various companies.\n- Notable companies include Proton, Nascent, Hypertune, and HRL Laboratories, each seeking specific positions in different locations.\n- Ideal for university students and entry-level software engineers interested in cutting-edge technologies and innovative projects globally."
  },
  {
    "id": 36567918,
    "timestamp": 1688350649,
    "title": "Ask HN: Are people in tech inside an AI echo chamber?",
    "url": "",
    "hn_url": "http://news.ycombinator.com/item?id=36567918",
    "content": "",
    "summary": "- This post asks if people in the tech industry are only exposed to information and opinions that reinforce their own beliefs about artificial intelligence (AI).\n- The author questions whether tech professionals are living in an \"echo chamber\" where they only hear ideas that align with their existing views.\n- The post explores the potential consequences of this echo chamber phenomenon and encourages tech professionals to seek out diverse perspectives on AI.",
    "hn_title": "Ask HN: Are people in tech inside an AI echo chamber?",
    "original_title": "Ask HN: Are people in tech inside an AI echo chamber?",
    "score": 367,
    "hn_content": "- Some people in the tech industry may be living in an echo chamber when it comes to AI.\n- The conversation revolves around the perception and adoption of AI technologies like ChatGPT.\n- AI is often seen as overhyped, with many viewing it as similar to the hype around cryptocurrencies.\n- While AI has made advancements in areas like speech to text, language translation, and computer vision, it still has limitations and is not always reliable.\n- There is a debate about the impact of AI on productivity and job displacement.\n- Productivity numbers haven't seen significant growth despite the advancements in AI.\n- AI has the potential to create wealth redistribution rather than new economic growth.\n- Tech has embedded AI in various processes, but its impact on overall economic prosperity is still uncertain.\n- The discussion touches on the potential negative effects of AI, such as distraction and privacy concerns.\n- The focus on AI as the next big thing attracts the attention of both tech-savvy individuals and those new to the industry.\n- The hype around AI can overshadow actual progress and practical applications.\n- The environmental impact of training large AI models is a growing concern.\n- Comparisons to crypto highlight the skepticism and disillusionment that can arise from overhyped technologies.\n- The potential utility of crypto, particularly in financial services, is acknowledged.\n- The disruptive nature of crypto is contrasted with the limited impact on certain industries.\n- The potential for AI and crypto to bring about significant changes in society is questioned, highlighting the need for realistic expectations.- Laypeople are highly interested in AI, particularly ChatGPT, seeing it as a world-changing technology.\n- Tech professionals tend to be more skeptical and critical of its abilities and impact.\n- Non-tech individuals are using ChatGPT for tasks like writing emails and brainstorming ideas, finding it valuable and time-saving.\n- The technology has potential in areas like natural language processing and document search, but it's crucial to have accurate prompts and manage its limitations.\n- Some people believe the hype around AI is similar to past trends like cryptocurrency, while others see it as a real and practical tool with tangible applications.\n- AI and LLMs can make certain tasks more efficient and accessible, but they don't replace the need for expertise and understanding in complex fields like programming.\n- It's important to maintain a culture of skepticism and critical thinking around new technologies. False claims and exaggerated expectations can discredit the industry and lead to negative consequences.- Ticketmaster was selling tickets online for the 2004 Olympic Games, even though the web had been invented 11 years prior. This highlights the slow adoption of new technologies in certain industries.\n- The Bitcoin paper was published in 2008, but blockchain has still not been widely adopted for ticket sales, despite its potential advantages.\n- Selling digital event tickets directly from the venue is possible, but it would require technical and operational expertise, which many venues do not have the resources for.\n- There could potentially be an open-source solution for digital ticketing with lower operating costs than Ticketmaster.\n- NFTs (non-fungible tokens) have not been widely used for ticket sales due to complex reasons that go beyond simply expecting immediate success.\n- Bitcoin and other cryptocurrencies have legitimate use cases, such as making purchases that payment processors might deny or politicize.\n- Bitcoin mining has had a significant impact on electricity consumption, contributing to environmental concerns.\n- Bitcoin has been used to buy cars and has various legitimate use cases, although they may not be immediately apparent to affluent individuals in the United States.\n- Crypto skeptics argue that it is inefficient and less practical compared to traditional payment methods.\n- AI and cryptocurrency have different implications in the tech industry, with AI being seen as a more efficient solution while crypto is viewed as less efficient.\n- Crypto has seen both hype and skepticism within the tech world, with some important figures voicing support and others remaining skeptical.\n- Cargo culting, or mindlessly imitating tech skills and talk, is a concern in the industry and can hinder genuine innovation.\n- The slow adoption of new technologies in certain industries and the potential for cargo culting highlight the need for critical thinking and careful evaluation when it comes to new tech trends.",
    "hn_summary": "- The conversation in the tech industry revolves around the perception and adoption of AI technologies like ChatGPT, with some viewing AI as overhyped.\n- There is a debate about the impact of AI on productivity and job displacement, and the potential negative effects of AI, such as distraction and privacy concerns, are also discussed.\n- The slow adoption of new technologies in certain industries and the need for critical thinking and careful evaluation when it comes to new tech trends is highlighted."
  },
  {
    "id": 36571110,
    "timestamp": 1688380869,
    "title": "Data-Oriented Design (2018)",
    "url": "https://www.dataorienteddesign.com/dodbook/dodmain.html",
    "hn_url": "http://news.ycombinator.com/item?id=36571110",
    "content": "Next: Contents  ContentsOnline release of Data-Oriented Design :This is the free, online, reduced version. Some inessential chapters are excluded from this version, but in the spirit of this being an education resource, the essentials are present for anyone wanting to learn about data-oriented design.Expect some odd formatting and some broken images and listings as this is auto generated and the Latex to html converters available are not perfect. If the source code listing is broken, you should be able to find the referenced source on github. If you like what you read here, consider purchasing the real paper book from here, as not only will it look a lot better, but it will help keep this version online for those who cannot afford to buy it. Please send any feedback to support@dataorienteddesign.comData-Oriented DesignRichard FabianContentsData-Oriented DesignIt's all about the dataData is not the problem domainData and statisticsData can changeHow is data formed?The frameworkConclusions and takeawaysRelational DatabasesComplex stateThe frameworkNormalising your dataNormalisationPrimary keys1st Normal Form2nd Normal Form3rd Normal FormBoyce-Codd Normal FormDomain Key / KnowledgeReflectionsOperationsSumming upStream ProcessingWhy does database technology matter?Existential ProcessingComplexityDebuggingWhy use an ifTypes of processingDon't use booleansDon't use enums quite as muchPrelude to polymorphismDynamic runtime polymorphismEvent handlingComponent Based ObjectsComponents in the wildAway from the hierarchyTowards managersThere is no entityHierarchical Level of DetailExistenceMementosJIT mementosAlternative axesThe true measureBeyond spaceCollective LODSearchingIndexesData-oriented LookupFinding low and highFinding randomSortingDo you need to?MaintainingSorting for your platformOptimisationsWhen should we optimise?FeedbackKnow your limitsA strategyDefine the problemMeasureAnalyseImplementConfirmSummaryTablesTransformsSpatial setsLazy evaluationNecessityVarying length setsJoins as intersectionsData-driven techniquesSIMDStructs of arraysHelping the compilerReducing order dependenceReducing memory dependencyWrite buffer awarenessAliasingReturn value optimisationCache line utilisationFalse sharingSpeculative execution awarenessBranch predictionDon't get evictedAuto vectorisationMaintenance and reuseCosmic hierarchiesDebuggingLifetimesAvoiding pointersBad StateReusabilityReusable functionsUnit testingRefactoringWhat's wrong?The harmMapping the problemInternalised stateInstance oriented developmentHierarchical design vs changeDivisions of labourReusable generic codeAbout this document ...Richard Fabian 2018-10-08",
    "summary": "- The post is an online release of a book called \"Data-Oriented Design\" which focuses on the importance of data in software development and provides insights on various topics related to data-oriented design.\n- The book emphasizes the significance of properly structuring and organizing data for efficient processing, including topics such as normalizing data, different types of processing, the use of components and managers, and optimizing performance.\n- It highlights the need to understand database technology, various techniques for data processing and manipulation, and the importance of debugging and maintaining software that is built with a data-oriented approach.",
    "hn_title": "Data-Oriented Design (2018)",
    "original_title": "Data-Oriented Design (2018)",
    "score": 334,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginData-Oriented Design (2018) (dataorienteddesign.com)334 points by DeathArrow 23 hours ago | hide | past | favorite | 116 commentsinopinatus 21 hours ago | next [\u2013]Some of the best advice I ever got for writing composable, high-performance code was \u201cwork on structs of arrays, not arrays of structs\u201d. I hear many echoes of that advice in this text. Turns out that entity-component architectures work well in line-of-business applications too, not just games.Alas, many developers in enterprise are rusted onto a record-keeping CRUD model and struggle to think in columns rather than rows. The idea of inserting an entity id into a \u201cpublished\u201d table, instead of setting a boolean \u201cpublished\u201d field to true, doesn\u2019t always come naturally. Yet once you realise how readily polymorphic this is, you may start wanting to use such approaches to data for everything. Rich new opportunities then arise from cross-pollinating component data. Some may question why it is structurally permissible that, say, a network interface can have a birthday, or why an invoice has an IPv6 address, why my cat is in the DHCP pool, whilst limegreen is deleted and $5 on Tuesdays. This of course is half the fun.I don\u2019t accept that it\u2019s wholly incompatible with OO, though, a thesis you\u2019ll see dotted around the place. I\u2019ve even taken this approach with Ruby using Active Record for persistence; not normally a domain where the words \u201chigh performance\u201d are bandied about. That worked particularly because Ruby\u2019s object system, being more Smalltalk-ish than C++/Java-ish, strongly favours composition over inheritance.replygnuvince 19 hours ago | parent | next [\u2013]> I don\u2019t accept that it\u2019s wholly incompatible with OOIt's not incompatible with the mechanics of OO, but it does require that programmers change how they approach problems. For instance, a common way to write code in an OO language is to focus solely on the thing you want to think about (a user, a blog post, a money transaction, what have you) and to implement it in isolation of everything else, to hide all of its data, and then to think about what methods need to be exposed to be useful to other parts of the system. The idea of encapsulation is quite strong.In DOD, it is more common for data related to different domains to be accessible and let the subsystems pick and choose what they need to do their work. Nothing about Java or Ruby would prevent this, but programmers definitely have mental barriers.replyandsoitis 19 hours ago | parent | prev | next [\u2013]> high-performance code was \u201cwork on structs of arrays, not arrays of structs\u201dWikipedia's article \"Array of Structure (AoS) and Structure of Arrays (SoA)\" explains the trade off between performance (SoA) and intuitiveness/lang support (AoS): https://en.wikipedia.org/wiki/AoS_and_SoAThey also get into software support for SoA, including data frames as implemented by R, Python's Panda's package, and Julia's DataFrames.jl package which allows you to access SoA like AoS.replygeokon 39 minutes ago | root | parent | next [\u2013]I'd also mention Clojure also has a very performant 'tech.ml.dataset' equivalent to dataframesMaybe I'm wrong, but AoS VS SoA seems like an old C false dichotomy that's been already effectively resolvedIf you need to choose between the two then the answer is probably neither - use an appropriate table datastructureThere is an extra layer of abstraction and software, but I don't really see any downsides. I'd love to hear some arguments againstreplywinter_blue 1 hour ago | parent | prev | next [\u2013]Wouldn\u2019t structs of arrays affect cache locality adversely, if say you\u2019re say reading and processing the data in sequential order? You\u2019d have to jump between memory addresses that are pretty far apart to read in a single record / set of fields, and potentially trigger cache misses since they\u2019re likely spread out across different memory pages and you end up filling your L1/L2/L3 cache?replycellularmitosis 1 hour ago | root | parent | next [\u2013]it depends on the access pattern. If you are performing an operation where you need to access every field on each object, then yes you'd be better off with array of structs. But at least in games, it seems more common that you'd need to e.g. get just the x-y coords of every monster, in which case struct-of-arrays is better.replyrawgabbit 19 hours ago | parent | prev | next [\u2013]In the data side of the world, \"structs of arrays\" translate to column based indexes i.e. Snowflake and OLAP. \"Arrays of structs\" translate to relational databases with its page/row based indexes.FWIW, I am a big fan of Snowflake and think it will eat everyone else's lunch. I also find it amusing that Snowflake \"supports\" foreign keys but don't enforce it. In other words, Snowflake is as \"nosql\" as I care to go.replyOJFord 9 hours ago | root | parent | next [\u2013]> \"supports\" foreign keys but don't enforce itAs in, you can have a sort of 'link' to another table, that not only mignt be null, but might be a value which doesn't exist over there?What does the 'support' add over just having a column in which you tell yourself (but not your DBMS) you're storing values which correspond to keys in another table?(You can take it further - s/keys/values in a column/ - many to one relationships without an intermediary table! Amazing! ...more 'NoSQL' than I care to go I think, for almost anything.)replyrawgabbit 9 hours ago | root | parent | next [\u2013]https://docs.snowflake.com/en/sql-reference/constraints-over...Note Snowflake supports defining and maintaining constraints, but does not enforce them, except for NOT NULL constraints, which are always enforced.replynerdponx 18 hours ago | root | parent | prev | next [\u2013]Or an in-memory \"data frame\" like in R, Python's Pandas, and Polars.replyDeathArrow 19 hours ago | parent | prev | next [\u2013]As I see it, there are two types of DOD, one that you've mentioned and where you \u201cwork on structs of arrays, not arrays of structs\u201d.The other one means just giving up of encapsulation, separate the data from the methods that work with the data, think the whole app in terms on how the data flows through it and model it so everything is easy to understand and change. For added correctness, you can use immutable data structures and pure functions.replybutterNaN 2 hours ago | root | parent | next [\u2013]Isn't this the very definition of functional programming?replytsss 13 minutes ago | root | parent | next [\u2013]No. Functional programming is programming with functions as values. It is not incompatible with data hiding (for example through modules).replyhohohmm 7 hours ago | root | parent | prev | next [\u2013]This is exactly right. Good summary.replyglun 6 hours ago | parent | prev | next [\u2013]This architecture completely ruins your write performance. Theres a reason double databases, one OLTP and one OLAP, is the norm.replyhu3 17 hours ago | parent | prev | next [\u2013]> The idea of inserting an entity id into a \u201cpublished\u201d table, instead of setting a boolean \u201cpublished\u201d field to true, doesn\u2019t always come naturally.It doesn't come naturally because now you need a JOIN in your SQL just to fetch what was before a column. Or two queries instead of one.Not to mention having closely related data spread in different tables increases cognitive load.You just added a layer of indirection for what gain precisely?replyxupybd 11 hours ago | root | parent | next [\u2013]I think they're arguing that you get a performance gain. Personally the systems if deal with wouldn't gain from such and optimisation as there is too little data. So I'll stick to the simplest approach.replyhu3 9 hours ago | root | parent | next [\u2013]I agree with you. Splitting db tables can be a net positive in very specific cases but it smells like premature optimization.replyOvid 19 hours ago | parent | prev | next [\u2013]I've tried to push entity-component-systems (ECS) for non-game applications. A financial company in London took that advice to manage the complexity of their system since it was such a good fit.For those who are curious, here's a very brief introduction to ECS: https://dev.to/ovid/the-unknown-design-pattern-1l64replyF-W-M 2 hours ago | root | parent | next [\u2013]Would you mind to elaborate a bit on your system? I tried to do something similar for an algo trading system, but in the end I hadn't enough entities to justify the approach.replywhstl 20 hours ago | parent | prev | next [\u2013]\"I don\u2019t accept that it\u2019s wholly incompatible with OO, though, a thesis you\u2019ll see dotted around the place.\"Agreed. Arrays of Structs is not the part that is really incompatible.The part that clashes a bit with traditional OOP is ECS, where data and code are meant to be kept separate. But of course I'm talking about very traditional OOP. It is entirely possible to use an OOP languages + classes to implement ECS. It's just not gonna be \"traditional\".EDIT: To quote your other reply: \"records don\u2019t have to identify strongly with a class hierarchy\".replygalaxyLogic 1 hour ago | root | parent | next [\u2013]Working with JavaScript has caused me to ask this question: \"When should I create a Class, and when a Function?\"This is not a trivial question because class-instances are basically collections of Functions, and Functions can return class-instances. So a trivial answer would be: \"It does not matter, you can do everything with both or either one\".No, you shouldn't do everything with both, you should do some specific types of things with classes and other types of things with functions. But what?I've come to this Rule-of-Thumb: Use classes only to represent and encapsulate data, use functions to process and transform class-instances.In practice this means that in most cases classes should not have methods which take arguments. Exceptions: You can have setters. You can have a new() method which creates a new instance with data that differs from the data of the recipient. But it should be all about data-creation and extraction.Classes are still truly useful over plain records, because a class abstracts over what data is stored, and what are the names by which it is accessed, and whether the methods return a stored field or a calculated one.The idea that \"Everything is an instance of a Class\" as in Smalltalk is a bit flawed, or at least too trivial an advice. It is of course also true that (even in JavaScript) Functions are objects. And in Smalltalk you can use BlockClosures, which really are \"functions\".This division of design makes it easier for me to think about the structure of the whole application. I no longer have classes for everything. Instead I have classes which represent and provide access to and creation of data, and a set of functions which do the processing of those class-instances.Why is this good? Because classes are, and easily become complicated, since they can have many methods, they can have both static and instance-methods, and methods can be local or inherited and you can even use the 'super' to add to the confusion. So therefore if you can find a way to force your classes to be as simple as possible, do that. Don't make them do complicated things since they are complicated to start with.Dividing the design into two parts, classes + functions, divides the complexity of the whole app into two parts, each of which contains about 1/2 of the complexity of the whole app. The complexity is now data-complexity PLUS function-complexity whereas with use-classes-for-everything the app-complexity would be more like data-complexity TIMES function complexity (I conjecture).Keep data (= classes) and functions separate. Divide and conquer complexity.replywhstl 26 minutes ago | root | parent | next [\u2013]Oh, yes. I totally agree with that philosophy.The part about classes only acting on themselves resonates heavily with me. To me it's the same with components in frontend apps: the more independent they are, the better.I'm probably never going to be copy-pasting my classes and components into other apps, but a good class/component should be copy-paste-able without needing many dependencies. This avoids the \"banana gorilla jungle problem\", as named by Joe Armstrong.\"Free Functions\" also help with keeping those classes even more self-contained and decoupled: you can use Function types for callbacks and things like that, instead of having to couple the class to some other class or interface. The only thing the class has to worry about is that the function conforms to the type.-\"Keep data (= classes) and functions separate. Divide and conquer complexity.\"Yep, 100% agree. Amen to that.replyloup-vaillant 11 hours ago | root | parent | prev | next [\u2013]Not sure what you all mean by OOP to be honest. Is this a case of OOP actually meaning \"good programming\", then morphing into the most fashionable (and hopefully best) practices of the day?It wouldn't be the first time\u2026 https://loup-vaillant.fr/articles/deaths-of-oopreplywhstl 2 hours ago | root | parent | next [\u2013]Well, I made the case to emphasize the word traditional. By \"traditional\" I mean OOP that follows the popular practices and pedagogy.Using ECS means totally eschewing things like encapsulation and inheritance. You stop grouping functions and methods together. You basically have \"free records\" and \"free procedures\" (or \"free methods\" if the language requires classes, Kingdom of Nouns style). ECS is a procedural/functional paradigm.You can argue that this is not OOP anymore, and guess what: I'll probably support you on that! :)replyryukoposting 18 hours ago | parent | prev | next [\u2013]Very insightful. I never thought about it like this, but it's common practice in embedded systems to approach problems in this way. One module may provide a statically-sized pool of like objects, and other modules extend behavior in relation to those objects by carrying buffers of pointers and/or indexes into that pool. It might feel inefficient due to the storage of so many pointers/indexes, but you're optimizing the size of the largest thing: the object pool itself.replyAndrewKemendo 19 hours ago | parent | prev | next [\u2013]>The idea of inserting an entity id into a \u201cpublished\u201d table, instead of setting a boolean \u201cpublished\u201d field to true, doesn\u2019t always come naturally. Yet once you realise how readily polymorphic this is, you may start wanting to use such approaches to data for everything.Wow thank you for this, it's a heuristic I didn't think about but is really powerful.I need to think through some of the implications, cause I think there are some risks to this approach - namely co-mingling production and non-production data in the same infra. That means that there are data at rest and in transport that are following the same data pathways but have different production criticality. It puts a lot of risk on the filter working perfectly, rather than not even being available on the same infra.Just spitballing:Is the ostensible \"prod_live_bool\" flag manually set? If not then a bug in any automation would certainly cause a nasty data exposure issueAre you doing column level security tokens? Wouldn't that need some kind of intra-table RBAC? In other words, it seems like if you want any RBAC within your table, then you have to bring it with you every time from the beginning because you have no idea what level of data sensitivity you'll have eventually, and refactoring an increasingly expanding table to inherit RBAC later ---- omg I can't even think of the amount of work that would be. O(n^2) level of manual work??Does it lead to a canonical \"live or not\" lookup table/dict at scale?I think the data security risks would prevent me from using this design pattern for critical applications in MOST cases, but I do love some of the patterns here and will be exploring this in the future for sure!replyTwisol 17 hours ago | root | parent | next [\u2013]I interpreted `published` along the lines of blog posts, in which an article can be either a draft (visible only to the author) or published (visible to the world). This seems different from dev/prod venueing, where having separate databases altogether makes sense.I understood the column-first approach more as an alternative to putting all columns for an entity in one table, especially when rows often don't populate every column. From that perspective, what's being described is a strong separation of concerns; applying this to dev/prod would be a weakening of this separation, and so probably not what is desired.replyfoobarbaz33 16 hours ago | root | parent | prev | next [\u2013]I don't think you're talking about the same thing as the parent comment.replym_mueller 20 hours ago | parent | prev | next [\u2013]> I don\u2019t accept that it\u2019s wholly incompatible with OOI don't think it is, not even in Java. If you use its primitive array and value types, you can easily wrap this in a data wrapper class with functional interfaces, which then can be used quite elegantly in the rest of your code in an OO way - you just don't box all the records into objects.replyinopinatus 20 hours ago | root | parent | next [\u2013]Agreed. Honestly I think the hardest part here is programmer mindset - when you suggest that identity is fluid and records don\u2019t have to correspond to a class hierarchy some folks get a panicked look going on, like you just claimed to eat their petsreplym_mueller 17 hours ago | root | parent | next [\u2013]Yep. Funny story, this was actually a coding interview homework for a question with order of millions of data rows from a file. The exercise required 10s timing requirement, so I decided to do it data centric from start. Interviewers found my style \u201anon idiomatic\u2018, but probably never thought about why my version was done in fractions of a second.replyzx8080 5 hours ago | root | parent | next [\u2013]Did you pass the interview?replym_mueller 2 hours ago | root | parent | next [\u2013]I did and got the offer. In the end it didn't work out however (better offer elsewhere).replyincrudible 17 hours ago | parent | prev | next [\u2013]> you may start wanting to use such approaches to data for everythingPlease, dont. Arrays of structs is more natural for a reason, SOA is not universally faster and it has some of its own performance problems. If you really care about performance, AOSOA may well be your best bet. Again, please dont use it for everything.> a dhcp pool full of cats is half the funReplacing the weird code that the last guy wrote for his own amusement is not fun.replyCapricorn2481 19 hours ago | parent | prev | next [\u2013]> Alas, many developers in enterprise are rusted onto a record-keeping CRUD model and struggle to think in columns rather than rows. The idea of inserting an entity id into a \u201cpublished\u201d table, instead of setting a boolean \u201cpublished\u201d field to true, doesn\u2019t always come naturally. Yet once you realise how readily polymorphic this is, you may start wanting to use such approaches to data for everythingI don't really understand what's Polymorphic about this, or even beneficial. It seems like everytime I've had a boolean column in a long-standing application, it eventually needed to turn into something elsereplybob1029 22 hours ago | prev | next [\u2013]> Is your data layout defined by a single interpretation from a single point of view?I think this might be the most important question at technology selection and architecture time. Answering it usually requires talking to the business and customers.If you are certain there is exactly 1 valid \"view\" of the data that will be used throughout, then perhaps enshrining it in code makes sense. If you are even a tiny bit uncertain of this, a relational-style model probably works better. SQL is the end game for most businesses once they realize the game theory around this one...I am curious what HN thinks as major reasons for why everyone seems to have moved away from 1 big SQL database. From my perspective, yeah we have \"web scale\" edge cases that threaten vertical scalability on writes, but most businesses will never touch this, including members of the F100.replyjwestbury 21 hours ago | parent | next [\u2013]At a previous F100 company -- a tech company whose products are widely used, we'll say -- we received guidance that RDBMS was verboten except with explicit approval. This had nothing to do with the best ways to model a given dataset, or achieving the best performance, and everything to do with schema flexibility and a history of outages caused by fucking up schema migrations. These problems weren't occurring in our NoSQL designs, and whatever benefits SQL databases offered didn't counter the huge benefits we gained from NoSQL's lack of rigid schema.Of course, bad uses of key-value stores can have massive performance impacts, and huge monetary costs when leveraging cloud platforms like DynamoDB -- I've seen a lot of cases where people didn't properly structure their data for DDB, and ended up performing loads of scans and sending costs through the roof.replyisoprophlex 21 hours ago | root | parent | next [\u2013]I read that as \"We don't want things to crash immediately when the data model changes. We want things to keep chugging along until the last possible moment, when we will realize we've been silently corrupting everything\"replybob1029 21 hours ago | root | parent | prev | next [\u2013]> schema flexibilityIf the business is of the notion that the schema is \"flexible\", then it is probably time to bring all of the MBAs into a conference room and have a come-to-jesus conversation about the limitations of information theory and human suffering.At a certain point, when someone says \"Widget\", everyone in the organization needs to be on the same page. This goes well beyond any specific technology.replygeophile 20 hours ago | root | parent | prev | next [\u2013]And then there are silent query failures. Want to change \u201cname\u201d: \u201cJohn Smith\u201d to \u201cname\u201d: {\u201cfirst\u201d: \u201cJohn\u201d, \u201clast\u201d: \u201cSmith\u201d}? Easy! No schema migration!But you have to modify all your queries to support both old and new formats, or stop the world and change all the data (after modifying all your code, including dynamically generated queries).And if you don\u2019t, your queries fail silently.replybarrkel 21 hours ago | root | parent | prev | next [\u2013]Hybrid solutions are possible; e.g. JSONB in Postgres, where you can still index and join with decent performance.replylittlestymaar 21 hours ago | root | parent | prev | next [\u2013]> and everything to do with schema flexibility and a history of outages caused by fucking up schema migrations. These problems weren't occurring in our NoSQL designs, and whatever benefits SQL databases offered didn't counter the huge benefits we gained from NoSQL's lack of rigid schemaYikesreplylisasays 39 minutes ago | root | parent | next [\u2013]\"Data consistency issues? We'll catch those during integration\"replylayer8 16 hours ago | root | parent | prev | next [\u2013]Sounds like a company whose end users are mostly not their customers.replylovasoa 21 hours ago | parent | prev | next [\u2013]> why everyone seems to have moved away from 1 big SQL databaseHacker News is probably not representative of the whole tech ecosystem. I think a majority of applications still uses one big SQL database.I recently released an open-source framework [1] that is entirely based on Data-Oriented Design. I have received a lot of comments from people for whom it was the right design. Having all your data in the same place makes so many things easier![1] https://sql.ophir.devreplythebruce87m 17 hours ago | root | parent | next [\u2013]Web technologies are also not representative of all the whole tech ecosystem. I can\u2019t fit an SQL database on my microcontroller.replybob1029 16 hours ago | root | parent | next [\u2013]> I can\u2019t fit an SQL database on my microcontroller.Perhaps not SQL Server or Oracle, but unless we are talking about a very limited device there are likely options.replydelusional 20 hours ago | parent | prev | next [\u2013]>I am curious what HN thinks as major reasons for why everyone seems to have moved away from 1 big SQL database.We haven't moved away from it, but we have run into a certain class of problems that seem related to the 1 big SQL database architecture. We're a really old enterprise, with a lot of non-technical people creating a lot of technical solutions back in the day that have become calcified and therefore have to keep existing. One of the things we have is 5 levels of SQL data transformations from the operational database (the one that actually has applications) into different generations of datamodel as the \"type\" of business we did changed.The problem is that as we accumulate ever more layers, we keep building on the layers before. The application that was built 10 years ago on abstraction layer 2 now needs some data from layer 4, let's create a new script that loops that data back into the previous layer and keep going. Eventually we've ended up with a huge amount of interdependent tables that all load data from other tables/views in weird and unintuitive ways, and the project to sort out the mess was deemed too expensive and postponed until the 2030's.I think it's understandable that people see those problems and consider how we could have avoided them. Unfortunately, for reasons I don't fully grasp, it seems impossible to apply anything to software engineers that require discipline, and we have to somehow make it impossible to create the spaghetti. That's where separation comes in. If you can't read the data from some other service, then it's impossible to create a spaghetti mess that kills velocity for both parties.The vertical separation of application becomes a software solution to the people problem of poor engineering discipline in enterprises.replybigger_cheese 6 hours ago | root | parent | next [\u2013]I can relate to this where I work we have similar issues the data has grown beyond what the original database schema was designed around. Ours is older than 10 years though - it is 27 years old (originally from 1996).We originally had a (in my opinion) pretty nicely designed schema and set of tables to represent the data but over time as more data has been added additional tables have grown in a hodge-podge fashion. We now have a Frankenstein's monster of a database. Similar to you there are all sorts of weird interdependencies across various tables.An additional complication is We have a lot of legacy programs/apps that Read/write to the database so changing types, field widths etc. Is basically impossible because you would have to recompile legacy code / Rewrite Messaging queues etc.The modern solution to this problem seems to be something called \"data lake\" which ingests data from the source DB but also applies transformations onto it. I don't really understand how it works something like a SQL view but also keeps the underlying data accessible as well. I don't know all the details has a lot of traction for our IT people at the moment.replyDeathArrow 21 hours ago | parent | prev | next [\u2013]>I am curious what HN thinks as major reasons for why everyone seems to have moved away from 1 big SQL databaseFor the places I worked:1. We transitioned to microservices2. Performance, 1 BIG database slows that3. Ops/maintenance is very hard in a huge DB4. In a huge DB there can be a lot of junk no one uses, no one remembers why is there, but no one is certain whether that junk is still needed5. We had different optimization strategies for reads and writes6. Teams need to have ownership on databases/data stores so we can move fast instead waiting for DBAs to reply to tickets.replylisasays 19 hours ago | root | parent | next [\u2013]4. In a huge DB there can be a lot of junk no one uses, no one remembers why is there, but no one is certain whether that junk is still neededOf course no one knows how to even begin to come up with a way of addressing that problem.So the only viable option is to keep on masking it. And keep propagating the junk data and zombie schemas ever forward.reply0wis 21 hours ago | parent | prev | next [\u2013]I'll second @viraptor and @hops answers : It is the same cause as the rise of microservices and DevOps adoption, easier politics. I worked for a big old company, and most of the problems were political and administrative. One big SQL database is quite efficient until the entity that owns it does not agree with the new CTO strategy and any another critical part of the business. Add an incident that shows the low resilience of the model and it quickly becomes a political headache, while the technical solution still seems evident to everyone.replyviraptor 21 hours ago | parent | prev | next [\u2013]> reasons for why everyone seems to have moved away from 1 big SQL databaseI'm sure there are going to be other answers for the code side of things, but for ops:Depends a lot on the size of the service, but in some cases: We got enough data that 1 big SQL store makes ops hard. (Took me 3 days to drop a table recently in a way that wouldn't affect the users) And splitting data became easier than before with specialised backends. (A sharded 2nd layer cache of live data seems way simpler to achieve than say 2 decades ago)replyrobertlagrant 21 hours ago | parent | prev | next [\u2013]For me it's parallelisable delivery (or fragility, depending on how you look at it). If a team owns its own data store, it can make whatever changes it needs to and not have to worry about any other part of the software being broken by those changes.replyhcarvalhoalves 21 hours ago | root | parent | next [\u2013]The trade-off being having a separate team trying to integrate all the data back together with ETL.replyrobertlagrant 19 hours ago | root | parent | next [\u2013]Well, if you need to integrate it. If you have some hideous dashboards you might need to, it's true, but at that point it's worth investing in a data person whose job it is to keep up with all the breaking data warehouse integrations. They'd have to anyway with any data approach.replyChicagoDave 13 hours ago | parent | prev | next [\u2013]Because over time the one big relational database turns into a big ball of mud. Change becomes expensive and has a large blast radius.Contextual business domains should be the foundation of any complex architecture. You reduce complexity and change blast radius and speed up agility and feature adoption.replyhobs 21 hours ago | parent | prev | next [\u2013]For the same reason they went with microservices - its easier to service the technical boundaries you control, and is a political solution rather than a technical one.Getting something based a DBA in change control was hard, but shipping some IaC templates can be done in a sprint!replymarcosdumay 19 hours ago | prev | next [\u2013]The entire advice is context-dependent.Games just happen to have a lot of operations that need column-based access; but that's not true for all domains. When you go and blindly push the game best practices into other domain, you are just making everybody's life hard and most systems worse.reply10000truths 15 hours ago | parent | next [\u2013]It's not just column-based access. Formatting your data into a struct of arrays exposes opportunites to pack your data more efficiently and greatly reduce your application's memory usage. Boolean struct fields can become bitsets. Nullable struct fields can become sparse (or dense) maps. Pointer/reference struct fields can become arrays of smaller-width integers that index into a pool. And so on. When everything runs on CPUs that frequently stall on memory accesses, the impact of these sorts of changes cannot be understated - the latency difference between L3 cache and RAM can be on the order of ~10x.replytorginus 13 hours ago | parent | prev | next [\u2013]I feel like this is increasingly the only way to write high-performance code.With newer hardware, the only thing that's expected to scale is logic density - SRAM (and cache sizes) have stopped scaling with the latest lithographies - and RAM bandwidth hasn't really been scaling for quite a while (I'd think it's even possible that per-core bandwidth has been decreasing) - memory access has been the bottleneck for a while.replyflohofwoe 18 hours ago | parent | prev | next [\u2013]> Games just happen to have a lot of operations that need column-based access.And that's not even true for many code areas in typical games, only where there's at least a few thousands 'things' to process (e.g. particle systems or navigation/collision systems).DOD makes a lot of sense within specific subsystems, but not necessarily in high level gameplay code (outside specific genres at least).replylyu07282 7 hours ago | root | parent | next [\u2013]You can see that in bevy, it feels like they are slowly reinventing a relational database and query language, every time they discover another limitation in their pure ECS architecture they add another macguffin to make it work.replypaulddraper 16 hours ago | parent | prev | next [\u2013]Databases are another common cause.replyjackmott42 17 hours ago | parent | prev | next [\u2013]The advice of keeping the data you access frequently contiguous in memory applies to everything on modern hardware. If there is a program where performance is an issue at all, probably this will be one way to make sure performance is good.replymarcosdumay 17 hours ago | root | parent | next [\u2013]Well, if you want to generalize, it's about keeping data with correlated accesses close to each other and aligned inside memory pages, and failing that, yes to keep it at least contiguous. It's not exactly about access frequency, except that you want to optimize the things you access more.Yes, that's a generic advice for high performance applications that is at least generic enough to apply on anything that is close to a normal computer. You will still need further details if you are talking about things like HPC (ironically) or mainframes, but it's general enough to say people should do it without qualifications.reply_dain_ 13 hours ago | parent | prev | next [\u2013]>Games just happen to have a lot of operations that need column-based access; but that's not true for all domains.This was not at all obvious when ECS first came around. It took a lot of time to convince people away from the OOP way.replyforrestthewoods 16 hours ago | parent | prev | next [\u2013]Hrmmm. Not sure this line of thinking makes sense.Game data access patterns are quite brutal. OOP for games results in extremely inefficient cache use, lots of random access, and lots of pointer chasing.ECS isn\u2019t a \u201cnatural\u201d fit for games. It\u2019s quite difficult and ECS systems are still far from a solved problem.The two most popular game engines, Unreal and Unity, are decisively non-ECS for almost everything they do.In any case, I think the underlying principles of DOD apply to all programs. Specific solutions vary, as always.replyDeathArrow 7 minutes ago | root | parent | next [\u2013]> The two most popular game engines, Unreal and Unity, are decisively non-ECS for almost everything they do.Unity is promoting Data-Oriented Technology Stack which includes ECS. They even made some tools to help translate between Gameobject based workflows and ECS based workflows.replymarcosdumay 16 hours ago | root | parent | prev | next [\u2013]> In any case, I think the underlying principles of DOD apply to all programs.Yeah, I do agree with that.replyGartzenDeHaes 18 hours ago | parent | prev | next [\u2013]It's for high-performance computing with current CPU designs that are dependent on data locality for performance.I agree that it's a harmful design for business data. Programmers want to push their runtime data model into the database and they have no interest in the operational, maintenance, and performance problems this causes. When someone suggests this kind of thing, I'll ask them \"how do we diagnose performance problems with this technology when there are 100,000 concurrent users and millions of data elements?\" The rows-and-columns people can answer this question.replyfeoren 15 hours ago | root | parent | next [\u2013]> When someone suggests this kind of thing, I'll ask them \"how do we diagnose performance problems with this technology when there are 100,000 concurrent users and millions of data elements?\"I don't understand; the exact same performance diagnostics work in both cases. Why is this different? There's nothing intrinsically less performant about this approach. You really think your checkerboard tables and long lists of columns with names like \"VALUE12\" and \"VALUE13\" and multiple different kinds of key/value pairs you jammed in there for different clients -- you think those are better performance!?> 100,000 concurrent usersDo you actually have 100,000 concurrent users? Really? You don't, do you? You just kinda hope you will eventually. And again: this approach is not worse for that.> millions of data elementsThis is absolute peanuts for any modern database system. It's weird that this is your extreme example.replyfeoren 17 hours ago | parent | prev | next [\u2013]But it is true, much much more often than you probably realize. Just look at your tables and think about how repetitive they are. The reason you can't come up with a lot of \"column-based\" (as you put it, which is still narrow-minded IMO) operations is because you've never looked for them before. Of course you haven't: you've been stuck in the traditional mode where such things are basically impossible.Do most of your tables have Name / Description type fields? Here's some \"column-based operations\": Allow translation of everything in your database. Generate natural-sounding text in a report, inserting these names and descriptions (from multiple different tables, of course). Free-text search of all your important database concepts. Detect similar names to the one the user is wanting to add, to prevent duplication. Clean whitespace. That's five off the top of my head.Do most of your tables have Archived / Status / Soft-delete type fields? Allow a user to archive a record. Choose whether to include archived records in a query or not. Delete archived records after X days.Do most of your tables have Comments fields? Allow multiple comments. Track who made a comment and when. Track responses to comments.Do most of your tables track who last modified the record? Track all modifications. Show a list of recent modifications to any records.The list goes on and on and on. You call these \"column-based operations\", which again, is short-sighted. They're more like \"concern-based operations\". And it turns out everything is a cross-cutting concern. You're shooting this idea down without nearly understanding it.replyfeoren 14 hours ago | root | parent | next [\u2013]For those who disagree, please give an example of a domain that is devoid of important \"concern-based\" operations.replyAcumen321 19 hours ago | prev | next [\u2013]Mike Acton's talk \"Data-Oriented Design and C++\" from CppCon 2014 is the best programming talk ever given in my opinion. A must watch:https://youtu.be/rX0ItVEVjHcreplyjrvarela56 6 hours ago | parent | next [\u2013]Beat me to it, was about to post this talk.For those reading, check the video out if you want to get a gist how to world-class performance is implemented.Most of my career has been writing web apps and this talk showed me 'why would someone use C?'replywrapperup 16 hours ago | parent | prev | next [\u2013]It's fantastic, and also my favorite. And for those who might not know, he was the one who really mainstreamed Data-oriented design and ECS architecture in my eyes.He previously was also leading the charge on Unity DOTS, though unfortunately it seems Unity is having a tailspin at the moment. The work on DOTS is solid, if incomplete.replycempaka 19 hours ago | prev | next [\u2013]Andrew Kelley gave an informative and entertaining talk on how DOD had inspired a lot of his work on the Zig compiler: https://vimeo.com/649009599replyGuestHNUser 7 hours ago | parent | next [\u2013]Thanks for sharing this! Hands down the most practical talk I have seen about DOD. That made so many things click for me.replydebanjan16 19 hours ago | prev | next [\u2013]Even beginners can learn to program in a data oriented way from the beginning.Two books that teach this style of programming to beginners are:1. How to Design Programs - https://htdp.org/2. A Data-Centric Introduction to Computing - https://dcic-world.org/replymorelisp 18 hours ago | parent | next [\u2013]Those are not this.replydebanjan16 17 hours ago | root | parent | next [\u2013]Can you elaborate a bit more on the reason?replyjrvarela56 6 hours ago | root | parent | next [\u2013]I think this may be more akin to what you original comment meant: https://blog.klipse.tech/dop/2022/06/22/principles-of-dop.ht...I got confused by the terms (Data Oriented Design and Data Oriented Programming) and watched Mike Acton's talk by mistake https://www.youtube.com/watch?v=rX0ItVEVjHc (and what a lucky mistake)replymorelisp 15 hours ago | root | parent | prev | next [\u2013]The books you presented are, roughly speaking, introductions to programming with a focus on data science, functional programming, and common structures/ideas used in those which are, in other texts, not usually considered introductory material. \"Data\" in this sense means, like, collected facts about the world and how to model them.Data-oriented design is a particular way of designing your programs where you focus on efficiently laying out your \"data\" - in a different sense, meaning \"whatever it is I've got in storage\" - within that storage - to compute with it as fast as possible.The industry-standard tools used for the first thing are often using techniques developed in the second of the second thing, but that's not relevant for the pedagogical framing. The tools they are teaching (Scheme and Pyret) actually make it very hard to play with low-level data layout details. And the emphasis in these texts on \"real [as in, world] data\" is in direct contradiction to the DOD axiom that \"data is not the problem domain... The data-oriented design approach doesn't build the real-world problem into the code.\"A rule of thumb: Is anyone talking about GPUs, SIMD, or CPU cache sizes? If not, you're looking at something about data modeling or data science, not data orientation.And this, sorry, is all super fucking obvious if you actually read the intro to all three things.replygjadi 22 hours ago | prev | next [\u2013]Found an online review of the book: https://gist.github.com/seece/25ed1b2108cf5782718b026382f2c5...replyvictor106 19 hours ago | parent | next [\u2013]Thanks for the link.Found this interesting and against common advice.\"The bane of many projects, and the cause of their lateness, has been the insistence on not doing optimisation prematurely. The reason optimisation at late stages is so difficult is that many pieces of software are built up with instances of objects everywhere, even when not needed.\"There are definitely applications where performance is of primary concern (maybe only a few) and others where they are not. In apps where it is, this gives me thought that maybe premature optimization is okay? Am I reading that right?There's also this called Data-Oriented Programming https://www.manning.com/books/data-oriented-programming.Are both these concepts the same?replycrabmusket 12 hours ago | root | parent | next [\u2013]No, DOP is basically just functional programming. There are some overlaps (e.g. separate code from data) but they're not related.replyoaiey 20 hours ago | prev | next [\u2013]Data Oriented Design is more beginner friendly. Because it does not deal with people and business but only with purity of data modelling.When I was young, my first step in a new project was painting the Entity Relationship Model. That gave me the foundation for everything else.Nowadays, I try to understand the problems and the domain, work on capabilities and how to group/box them before I start doing data models.replychrisgd 20 hours ago | parent | next [\u2013]I like the content of your comment. Everyone who has experience recognizes that our love of data and programming often gets sideswiped by business needs. I think though this article tries to say that if we focus on gathering data needs from the beginning, it might make the business needs conversation mootreplydosshell 22 hours ago | prev | next [\u2013]One key concept when I use DoD is to not abstract away the data. Less is more.But when quickly reading the intro text, I found it doing the opposite, it talks too much and abstracts away the key concepts. Only me who found it a bit ironic of not drinking the wine?replyBonitaPersona 20 hours ago | prev | next [\u2013]I love this book and have been very influenced by it.However, it should definitely be called: Data-Oriented Design FOR GAME DEVELOPMENT.replyloup-vaillant 11 hours ago | parent | next [\u2013]Are you suggesting the advice in there isn't applicable in other domains? Or just that it uses games as an example?replyBonitaPersona 3 hours ago | root | parent | next [\u2013]The second, more or less.The advice is certainly applicable in other domains, however the explanations and examples used in the book are heavily focused on software that will be executing repetitive code in a frequency-controlled loop, that loops over arrays of arrays. This also applies to simulation software.There are some key characteristics on this kind of software that may or may not be present in other domains. Once deeply inside game development, these characteristics are like the oxygen you breath, and become intrinsically ingrained in how you think. This is the case with this book and author: I have the impression he thought gamedev-focused examples were general and not specific.As I said, I was really positively influenced by this book, and I tend to go back to it from time to time. Always worth it. Just be aware of what it is focused on.replykaycebasques 11 hours ago | prev | next [\u2013]Pretty great intro paragraph. The eloquent writing and interesting ideas motivate me to keep reading:> Data is all we have. Data is what we need to transform in order to create a user experience. Data is what we load when we open a document. Data is the graphics on the screen, the pulses from the buttons on your gamepad, the cause of your speakers producing waves in the air, the method by which you level up and how the bad guy knew where you were so as to shoot at you. Data is how long the dynamite took to explode and how many rings you dropped when you fell on the spikes. It is the current position and velocity of every particle in the beautiful scene that ended the game which was loaded off the disc and into your life via transformations by machinery driven by decoded instructions themselves ordered by assemblers instructed by compilers fed with source-code.replycrabmusket 2 hours ago | parent | next [\u2013]I see these ideas in a lot of data-oriented-design literature and it always struck me as needlessly reductive. Maybe it's useful to set the scene and provide a \"cold shower\" to get you out of an OOP-abstraction state of mind. But aside from that it seems about as useful as an engineer saying \"look around! everything is made of atoms! engineering is fundamentally about moving atoms!\" Which is not wrong, just not really going to help actually do any engineering.replyCiantic 22 hours ago | prev | next [\u2013]Everytime I hear about Data-Driven/Oriented Design I remember a paper from OOP course I had to read in University, it used Data-Driven Design as an example how to not to do things.The paper in question is this from 1989: https://dl.acm.org/doi/pdf/10.1145/74877.74885It highlights that:\"Even though the goal of data-driven design is to encapsulate data and algorithms, it inherently violates that encapsulation by making the structure of an object part of the definition of the object. This in turn leads to the definition of operations that reflect that structure (because they were designed with the structure in mind). Attempts to change the structure of an object transparently are destined to fail because other classes rely on that structure. This is the antithesis of encapsulation.\"Then goes to show that Responsibility-Driven Design has better approach.What we mean with Data-Driven Design have come a long way though, and isn't comparable to those days.I find it a bit amusing that Data-Driven Design used to be an insult of sorts, like you didn't know how to do things OOP the right way.replydosshell 22 hours ago | parent | next [\u2013]You do realize that DoD has nothing to do with data-driven, right?Data oriented design is about structure the data in regard of how it is processed and the hardware. This is opposite of object oriented design which models the data around your mental model.For example in DoD you could design a map with keys in one array together and one array of values. This would make it much faster to iterate over keys while searching because of cache memory.While in object oriented you would store an array of pairs.Both approaches can use data-driven.Edit: would -> couldreplydrainyard 21 hours ago | root | parent | next [\u2013]While I agree with your point, the map example is not entirely well represented in your comment. The idea is not to just store keys and values in separate arrays, the idea is to look at your use case and model your data after the transformation you need. So if you have a case where storing keys and values as pairs because of your access pattern, then do that, if you have a case where you do a lot of searches through keys, then store them in separate arrays.The point of DoD is to look at the data you have and the data you need it transformed into and then structure your data after.replymorelisp 22 hours ago | root | parent | prev | next [\u2013]There are also multiple definitions of \"data-driven\" floating around depending on qualifier/context. An OLAP RDBMS, for example, will certainly be written in a data-oriented way, and also have almost fully data-driven behavior, but will certainly not have data-driven (in the sense of that paper) design.replydgb23 17 hours ago | parent | prev | next [\u2013]The example given as \"data driven design\" in this paper neither resembles modern data driven programming nor data oriented design.I don't quite understand why the authors thought of this as a good example of \"data driven design\".I know that functions in a data driven program tend to be very generic so the conclusion you cite is very much a mismatch from my experience.replycamjw 21 hours ago | prev | next [\u2013]Best SWE book I've ever read and I don't even work on something video game adjacent any morereplydeneas 20 hours ago | prev | next [\u2013]I like Data-Oriented Design, but beware of one thing: You organise your data like a database? You'll eventually be writing a database management system, unless you can use a framework like one of the many Entity-Component-System ones.replysynergy20 20 hours ago | prev | next [\u2013]How useful and practical for DoD in ML and AI field to do parallel (matrix) computing at scale? where performance is crucial as well, but not as crucial as gaming(milliseconds matter). DoD is a new paradigm for me.replyeska 22 hours ago | prev | next [\u2013]Looked at it for 10 seconds and immediately found random 0s at the end of the document or unreplaced text like \u201cNoel Llopis in his September 2009 article[#!NoelDOD!#]\u201dreplyjerrygenser 22 hours ago | parent | next [\u2013]The intro describes that it's a free version of a book that contains most of the content except for a few sections.The book was converted using an automated format from it's native design into html so a large portion could be made available for free and this automated conversion process might have some minor issues -- like the one you described.replyhtk 20 hours ago | prev | next [\u2013]Did anyone here by any chance create an epub out of this?If not, any recommendations on utilities to convert several linked html files into a single epub?replyasimpletune 20 hours ago | parent | next [\u2013]I think standardebook's has a command line utility that they use for producing their ebooks, which you might be able to use to produce an ebook from a bunch of html files. Ultimately an epub is a zip of html anyways, I think.replyhtk 17 hours ago | root | parent | next [\u2013]I'll give this a try, thank you.replydkarl 17 hours ago | prev [\u2013]> [Abstraction heavy paradigms] structure the code around the description of the problem domainMy experience of Domain-Driven Design has been that it is extremely effective for driving conversations about the domain throughout the product life-cycle, but it produces frustrating codebases that are poorly attuned to the world outside the running process. Domain-Driven Design codebases want to be self-contained universes and treat external systems as details. OO design paradigms in general seem to have little respect for messages exchanged with external systems.This wasn't true back when JavaBeans and object databases other distributed object systems were expected to take over the world, but the failure of those technologies shrank the OO world from distributed systems to isolated programs. These days, the messages exchanged with external systems are just data, without behavior. The marriage of data and behavior can only exist inside a single process. So object-oriented design turned inward and concentrated on the creation of rich inner worlds.I think this is backwards, or at least incomplete. As Rich Hickey says, effective programs need to be situated in the real world. They are not ethereal abstract models. They have concrete functions, inputs and outputs. Having rich internal abstractions that mimic some aspects of reality is a means to an end, entirely subordinate to the purpose of executing interactions with other computing systems. Data-driven design embraces this reality. By treating data as essential, and behavior as something to be added when it is needed, it allows inputs and outputs to be first-class citizens.> The data-oriented design approach doesn't build the real-world problem into the code. This could be seen as a failing of the data-oriented approach by veteran object-oriented developers, as examples of the success of object-oriented design come from being able to bring the human concepts to the machineI think this perfectly sums up the confusion that OO modeling creates. You use code to write programs, or services, or cloud functions, things like that. The real-world problem that a program, service, or cloud function solves is interacting in a certain way with other programs, services, cloud functions.The real-world problems that OO modeling paradigms want you to focus on are the domain problems. This is vitally important for design products, systems of programs that solve real human problems. If you are designing a system for managing medical records in a hospital, you need to model doctors, nurses, patients, labs, radiology images, patient stays, all those real-world things. However, when you are designing a piece of software to do one thing within that medical records system, the \"real-world problem\" your code is solving is limited to its role within the larger system.Data-oriented design is a natural mental fit for writing programs or services that play a limited part in larger systems, which is always what you are doing when you are writing code. Object-oriented design wants to take on the complexity of the whole real world, which is the right perspective for product design and architecture, not for writing code.replyrapnie 15 hours ago | parent [\u2013]> My experience of Domain-Driven Design has been that it is extremely effective for driving conversations about the domain throughout the product life-cycle, but it produces frustrating codebases that are poorly attuned to the world outside the running process.The DDD blue book by Eric Evans consists of two parts: Strategic Design and Tactical Patters. Is it accurate to summarize your comment as that the strategic design works very well, but that most of the information out there leads one to then learn about the tactical patterns in a particular OOP contexts, which isn't a universal approach, and in many cases shouldn't be the way to elaborate findings from Strategic Design?Note that searching the web for \"DDD\" yields mostly OOP-related tactical patterns guidance, and this is why I think many people are so sceptical about DDD. It is the strategic parts where most of the (low-hanging fruit) value is.Other non-OOP 'tactical' guidance, such as functional / functional-reactive / actor-driven, etc. DDD is harder to come by.replydkarl 14 hours ago | root | parent [\u2013]In my copy of the blue book, \"Strategic Design\" is Part IV, and there is no \"Tactical Patterns\" part or chapter. To be honest, I've only quickly read through Part IV, cherry-picking a couple of concepts, because it concentrates on techniques for dealing with very large domain models and/or very large organizations.I think the good and bad are interleaved together throughout the book. Part I Chapter 2, \"Communication and the Use of Language,\" is the one chapter I wish everybody I work with would read and digest. I think establishing a consistent language about the domain that is shared across functional groups is critical, so the domain terminology used in source code and comments is consistent with the terminology engineers use when talking with product and customer support. Part I Chapter 3 contains the clearest statement of (what I think is) the core error: \"Tightly relating the code to an underlying model [which context makes clear is the domain model] gives the code meaning and makes the model relevant.\" The rest of the book is like that for me, alternating between vigorously nodding my head and yelling \"WHY WOULD YOU TELL PEOPLE THAT,\" sometimes within the same chapter.I suspect overall there's a communication issue with the book, where he focuses entirely on the themes of DDD and only occasionally gives lip service to other aspects of design. For example, when he says that module names and structures should reflect domain concepts, I think, \"Yeah, that's really nice to the extent you can accomplish that, but you also want your module names and structures to reflect the logical structure of your program, so somebody can look at it and see how it works.\" You have these two aspects that should ideally both be legible in the code, but the DDD book doesn't show much respect for competing design priorities. In fact, it often warns against the dangers of being led astray by other design perspectives, such as architectural ones. Like so many other OO methodologies, the overwhelming thrust of the book is that if you attend to what it's teaching you, everything else will take care of itself.A priest once told me, a good priest will tell you when you need a lawyer, a good lawyer will tell you when you need a therapist, a good therapist will tell you when you need a doctor, and a good doctor will tell you when you need a priest. Be careful with an expert who always tells you that their expert perspective is what you need. The DDD book is definitely that kind of expert you have to be careful with.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- Data-Oriented Design (DOD) is a paradigm that focuses on efficiently organizing and manipulating data for high-performance code.\n- DOD emphasizes working with structs of arrays rather than arrays of structs, which can improve performance and enable cross-pollination of component data.\n- DOD is not incompatible with object-oriented (OO) programming, but it requires a shift in mindset and approach to problem-solving."
  },
  {
    "id": 36574645,
    "timestamp": 1688399421,
    "title": "An interactive guide to SVG paths",
    "url": "https://www.nan.fyi/svg-paths",
    "hn_url": "http://news.ycombinator.com/item?id=36574645",
    "content": "NaNUnderstanding SVG PathsIf you've ever looked at the SVG code for an icon before, you might have noticed that they're usually made up of a bunch of path elements, each with a cryptic d attribute.If you're anything like me, you might've glossed over this attribute in the past, assuming they're nothing more than the output of your designer's favorite vector graphics editor.While, er, that may be true, it's also a bit of an oversimplification.You see, understanding this attribute's inner workings ended up being a huge boon to my front-end skills; it allowed me to do things I never thought possible, like making this bendy square animation:Bend!This guide is an interactive deep dive into the d attribute, otherwise known as the path data. It's the post I wish I had when I first learned about SVG paths! Along the way, we'll learn about the different types of path commands and how to use them to draw various icons.Let's get started!A Path is a Series of CommandsWhile the d attribute may look like some magical incantation, it's actually a series of commands that tell the browser how the path should be drawn. This is a bit more obvious if we clean up the d attribute a little bit:M12.07.2C10.55.68.15.26.36.7C4.58.14.210.65.712.4L12.018.3L18.312.4C19.710.619.58.117.76.7C15.85.213.45.612.07.2ZTo draw the path, the browser executes these commands sequentially, each command drawing a little \"subsection\" of the path.PlayAll path commands follow the same basic syntax \u2014 a single-letter code followed by a series of numbers. The letter code identifies the command type, while the numbers act as the command's parameters.In some ways, you can think of the commands as function calls, where the letter code is the function name and the numbers are the function's arguments:M(12, 7.2);Absolute and Relative CommandsThe command code can either be uppercase or lowercase. Uppercase commands are absolute, meaning their parameters are relative to the origin point (0, 0), while lowercase commands are relative, meaning their parameters are relative to the previous command's endpoint.Consider the following commands:M10.010.0L5.05.0M10.010.0l5.05.0Here, we have two lines that start at the same point, (10, 10), with the same arguments, 5 5.Notice how the line made with the uppercase L command ended up at (5, 5) while the line made with the lowercase l command ended up at (15, 15).To summarize:The d attribute in a <path> element is a series of commands;Paths are drawn by executing the commands sequentially;Commands start with a single-letter code followed by one or more numbers;Uppercase commands are absolute, while lowercase commands are relative.I think that's enough talk about commands as a whole for now. Let's dive into the different types of commands!Cursors",
    "summary": "- The d attribute in SVG code is a series of commands that tell the browser how to draw a path.\n- The commands in the d attribute consist of a single-letter code followed by one or more numbers.\n- Uppercase commands are absolute, while lowercase commands are relative to the previous command's endpoint.",
    "hn_title": "An interactive guide to SVG paths",
    "original_title": "An interactive guide to SVG paths",
    "score": 293,
    "hn_content": "Hacker News new | past | comments | ask | show | jobs | submit loginAn interactive guide to SVG paths (nan.fyi)293 points by nansdotio 18 hours ago | hide | past | favorite | 34 commentsiza 17 hours ago | next [\u2013]This SVG path editor is incredibly useful if you're building or editing SVGs by hand: https://yqnn.github.io/svg-path-editor/replygunapologist99 14 hours ago | parent | next [\u2013]You can also do this in inkscape (although you have to hand-edit if you want to really optimize it)replyGualdrapo 11 hours ago | root | parent | next [\u2013]Yes but what I really like of that tool is that it \"dissects\" the entire d parameter into its bits and it's much easier to understand what's going on, and draw \"parametrically\" stuff - whereas with Inkscape the process is way more organic and you just can pretty much forget about the innards of it.replysamstave 14 hours ago | root | parent | prev | next [\u2013]I've built so many custom SVGs in Inkscape - I used to be an expert at it, but that information evaporates QUICKLY - and I open inkscape, and I cant even recall the mappings any longer...But inkscape had a lot of really fast keybindings that helped me on many a huge project...replyOwenFM 33 minutes ago | prev | next [\u2013]I like the explanations, but it's jarring to see B\u00e9zier treated with such little respect that he doesn't even get his name spelled correctly.replybobthepanda 17 hours ago | prev | next [\u2013]SVG is quite nice.It would be a lot nicer if the proposal for SVG 2 was ever adopted, but that has been stuck in draft for years. https://svgwg.org/svg2-draft/replydanielvaughn 13 hours ago | parent | next [\u2013]It's a shame because SVG is amazing. I came to web development from the art/design world, and I'd been using Adobe Illustrator for years. I was really excited to learn that I could program vectors, but was shocked to learn how little the average developer knew about them or SVG (at the time).replyNoujin 12 hours ago | parent | prev | next [\u2013]A few months ago I looked into how to do an angled gradient in SVG and was baffled that is isn't supported. Now looking at this spec, it's also not in there. Why the hell is this something nobody talks about. Photoshop has it since ages...replySyntheticate 10 hours ago | root | parent | next [\u2013]It's definitely supported via the `gradientTransform` attribute on e.g. linear gradients, as in this example: https://developer.mozilla.org/en-US/docs/Web/SVG/Element/lin...replyjahewson 6 hours ago | root | parent | next [\u2013]I think they\u2019re referring to conic gradients, also known as angular gradients which SVG doesn\u2019t support.replyappleflaxen 16 hours ago | parent | prev | next [\u2013]What are the key differences?Why hasn't it been?replybobthepanda 16 hours ago | root | parent | next [\u2013]SVG 1.1, the last widely supported standard, came out in 2003, and the web has changed a lot in 20 years.There are a lot of changes (probably a factor in lack of support) but broadly it is to bring CSS support to SVG, that the browsers have been doing for years.I worked on a side project to learn SVG and the two biggest features that SVG 2 has is* support for z-index. SVG elements are currently rendered in the order they are written in markup. This is very annoying if you have common groups of things that interweave with other things visually; but you can\u2019t put them under a single element to select.* HTML style text rendering. Today in HTML, you can have text respect a container\u2019s width and break to newline when needed, and you can align any corner of a text element to the general CSS box model. SVG text does not have a way to specify a width, and you can only align to the baseline of the first line of multiline text. This is very infuriating.\u2014-The major reason for lack of progress is that updating the w3c standards is like herding cats; and SVG needs to include the professional authoring tools that produce it, which is the 800-pound gorilla that is Adobe Creative, and everyone else trying to eat Adobe\u2019s lunch, so it\u2019s hardly a cooperative bunch.replybryanrasmussen 4 hours ago | root | parent | next [\u2013]>This is very annoying if you have common groups of things that interweave with other things visually; but you can\u2019t put them under a single element to select.I'm not sure I really understand what you're saying, you can put a group of paths etc. under a group element and then manipulate the display of that group. You can't control their z-indexing it's true but I've achieved relatively complex and pleasing interactions with this method.I guess I see the point that it would be nice to control it, as I normally want to be able to control everything it would be a bit hypocritical to suddenly say no you don't need to, but I would note one benefit of current SVG's approach is you don't end up with the fighting about z-index you get every now and then in HTML where suddenly things overlap and it is hard to figure out in code where or why, in SVG things overlap because one element is above another and are occupying the same x,y.replybobthepanda 2 hours ago | root | parent | next [\u2013]Say you have a transit map with lines, stops and labels. You always want lines below stops below labels, but that also makes it impossible to group a specific line\u2019s stops and labels together without just totally going on top of a different line\u2019s group. Which then makes interactivity annoying.replybryanrasmussen 2 hours ago | root | parent | next [\u2013]thanks, good use case, I tend to use it for aesthetic effects rather than data visualization.replysamstave 14 hours ago | root | parent | prev | next [\u2013]I may be talking out my butt ; but, wouldnt it be cool - if you had a slicer that made individual layers for visible elements on base elements, where, when stacked, you can hit a button and make all the sub layers for the components from such that were visible in the final image?Does exist?--Yes!What I was poorly articulating was the need for you to have that final stacked image with Z positioning, then be able to still have the visibly layered elements turned into their own layer for their own cutting (assume you have a partion of the model which is in background, and you want to chop that shape to its own element, such that you can cut/print on different colors/mediums, but have it still hit the image...It should take the VISIBLE parts of an element in the final satack, then create layers for each of the components from each layer, then provide a template for printing out mass numers of each component in a zero waste model to allow for morereplybobthepanda 13 hours ago | root | parent | next [\u2013]I'm not quite sure what you're getting at, but that's effectively what the current model forces you to do, to specify the elements in order that they go on top of each other. This is very bad.I would rather have z-index so I can group elements how I want and use z-index to dictate the final display order.replyttfkam 16 hours ago | root | parent | prev | next [\u2013]Here's a feature I desperately wanted a dozen years ago. Browser vendors kind of lost interest in improving their SVG support around that time.https://dev.w3.org/SVG/modules/vectoreffects/master/SVGVecto...replyspookie 16 hours ago | root | parent | next [\u2013]It's quite perplexing when SVGs have a lot of potential in responsive UX. I don't get it.replyygra 13 hours ago | root | parent | next [\u2013]Browser vendors seem to prefer SVG to be a somewhat dumb vector format and already hate many of the more complex parts. SVG 2 originally intended to have a bunch of more interesting features, but eventually resistance from implementors (mostly browsers) whittled it down to some clarifications of the spec along with a bit of CSS/HTML 5 harmonization.replydenvaar 14 hours ago | prev | next [\u2013]Love the detail and thought that has been put into this post, along with the other blog posts as well. Visualizations and animations go along way. A very ascetically pleasing site that is informative as well.reply082349872349872 11 hours ago | parent | next [\u2013]I agree on the aesthetic quality of the entire site.Given the amount of detail included in the post, I was surprised that the author didn't mention that the primitives used in the d attribute for the <path> element allow one to synthesise most of the other SVG elements ... or did I just miss that somewhere?replyWillAdams 12 hours ago | prev | next [\u2013]Back when I helped out on the assembly instructions for the Shapeoko 2:http://shapeoko.github.io/Docs/index.htmlone of the things which was worked up was a way to make an SVG interactive:http://shapeoko.github.io/Docs/content/tPictures/PS20028-100...(click on part #11 in the list --- in retrospect the parts of the drawing itself should have highlighted the matching part entry in the list)replynologic01 17 hours ago | prev | next [\u2013]Beatutifully made guide.The numerical represention of Bezier curves is quite a challenge to internalize and the clean interactive canvas helps build some intuition.Vector graphics and SVG deserve far more love.replyzoogeny 8 hours ago | parent | next [\u2013]If you are looking for solid content on Bezier curves then Freya Holm\u00e9r's video on The Beauty of B\u00e9zier Curves [1] is well worth the time it takes to watch.1. https://www.youtube.com/watch?v=aVwxzDHniEw&ab_channel=Freya...replygifkfkg7gfj 13 hours ago | prev | next [\u2013]Also good: https://svg-path-visualizer.netlify.app/That one might even be better for some people.replyalbert_e 6 hours ago | prev | next [\u2013]Excellent tutorial!The hands-on practice built into each page is brilliant. I wish more tutorials were like this!replyklysm 7 hours ago | prev | next [\u2013]Excellent use of scroll jacking herereplydvt 16 hours ago | prev | next [\u2013]Amazing guide, love how \"tactile\" the animations feel!replypolitelemon 16 hours ago | prev [\u2013]Not sure if it's me, it says NaN above the title, and the button that says \"Bend!\" or \"Straighten\" doesn't actually do anything. I've tried multiple hard refreshes.replywbobeirne 16 hours ago | parent | next [\u2013]> it says NaN above the titleThat is the name of the blog, the domain is nan.fyi.replycsallen 13 hours ago | parent | prev | next [\u2013]This happened to me, too. You have to scroll down to get the Bend/Straighten button to work. Basically, the visuals on the right side of the page correspond to what part of the instructions you have scrolled top the top.replyjer0me 16 hours ago | parent | prev | next [\u2013]NaN is the name of the blog. The animation is working for me on iOS Safari, but the text isn\u2019t aligned perfectly.replyjoemi 11 hours ago | parent | prev [\u2013]The NaN fooled me too, before I realized it was in the domain name too and must just be the name.replyGuidelines | FAQ | Lists | API | Security | Legal | Apply to YC | ContactSearch:",
    "hn_summary": "- The SVG path editor mentioned in the article is a useful tool for building or editing SVGs by hand.\n- Inkscape is another tool that can be used for editing SVGs, although it may require hand-editing for optimal optimization.\n- The SVG path editor \"dissects\" the d parameter of SVG paths, making it easier to understand and draw parametrically.\n- SVG is a powerful tool for web development, but many developers have limited knowledge about it.\n- There are features missing from SVG, such as angled gradients, which exist in programs like Photoshop.\n- SVG 2, a proposed update to the SVG standard, has been stuck in draft for years, limiting the advancements in SVG support.\n- SVG 2 aims to bring CSS support to SVG and address limitations of the current standard, such as z-index and text rendering.\n- The lack of progress in updating SVG standards is attributed to the complexity of the process, resistance from implementors (mostly browsers), and the need to consider professional authoring tools like Adobe Creative.\n- Interactive and visually appealing guides like the one mentioned in the article are valuable resources for learning SVG and building intuition for concepts like Bezier curves.\n- Other SVG path editors and tutorials, like the ones mentioned in the comments, are worth exploring for different perspectives and approaches to working with SVG."
  }
]
