[
  {
    "title": "New Deep Learning Model Identifies Unknown Phases in Crystalline Samples",
    "originLink": "https://www.eurekalert.org/news-releases/1008323",
    "originBody": "News Release 17-Nov-2023 Accelerating the phase identification of multiphase mixtures with deep learning Researchers develop a deep learning model that can detect a previously unknown quasicrystalline phase present in multiphase crystalline samples Peer-Reviewed Publication Tokyo University of Science image: Researchers propose a machine learning model that identifies a new Al–Si–Ru i-QC phase. With the potential for extended applications for identification of other structures like decagonal and dodecagonal QCs, the proposed model demonstrates immense potential for accelerating the phase identification process of multiphase samples. view more Credit: Tsunetomo Yamada from TUS Crystalline materials are made up of atoms, ions, or molecules arranged in an ordered, three-dimensional structure. They are widely used for the development of semiconductors, pharmaceuticals, photovoltaics, and catalysts. The type of structures that fall into the category of crystalline materials continues to expand as scientists design novel materials to address emerging challenges pertaining to energy storage, carbon capture, and advanced electronics. However, the development of such materials necessitates precise ways of identifying them. Currently, powder X-ray diffraction is widely used for this purpose. It identifies the structure of crystalline materials by examining scattered X-rays from a powdered sample. However, the task of identification becomes quite complex when dealing with multiphase samples containing different types of crystals with distinct structures, orientations, or compositions. In such cases, the accurate identification of the various phases present in the sample relies on the expertise of scientists, making the process time-consuming. To expedite this process, innovative data-driven methods, such as machine learning, have been used for distinguishing individual phases within multiphase samples. While substantial progress has been made in utilizing them for collecting information about known phases, the identification of unknown phases in multiphase samples still remains a challenge. Now, however, researchers have proposed a new machine learning “binary classifier” model that can identify the presence of icosahedral quasicrystal (i-QC) phases—a kind of long-range ordered solids that have self-similarity in their diffraction patterns—from multiphase powder X-ray diffraction patterns. This study involved collaboration among Tokyo University of Science (TUS), National Defense Academy, National Institute for Materials Science, Tohoku University, and The Institute of Statistical Mathematics. It was led by Junior Associate Professor Tsunetomo Yamada from TUS, Japan, and was published in the Advanced Science journal on 14 November 2023. “Across the world, researchers have made attempts to predict new substances using artificial intelligence and machine learning. However, identifying whether a desired substance is produced takes up substantial time and effort on the part of human experts. Therefore, we came up with the idea of using deep learning to identify new phases,” explains Dr. Yamada. For developing the said model, the researchers first created a “binary classifier” using 80 types of convolutional neural networks. They next trained the classifier model using synthetic multiphase X-ray diffraction patterns, which were designed as representations of the expected patterns associated with i-QC phases. Following the training phase, the model’s performance was assessed using both synthetic patterns and a database of actual patterns. Quite interestingly, the model achieved a prediction accuracy of over 92%. It also successfully identified an unknown i-QC phase within multiphase Al–Si–Ru alloys when used for screening 440 measured diffraction patterns from unknown materials in six different alloy systems. The presence of the unknown i-QC phase was further confirmed on analyzing the microstructure and composition of the material using transmission electron microscopy. Notably, the proposed deep learning method has the ability to identify the i-QC phase even when it is not the most prominent component in the mixture. Moreover, this model can be used for the identification of new decagonal and dodecagonal QCs and can be extended to various types of other crystalline materials as well. “With the proposed model, we were able to detect unknown quasicrystalline phases present in multiphase samples with high accuracy. The accuracy of this deep learning model thus points to the possibility of accelerating the process of phase identification of multiphase samples,” concludes Dr. Yamada on an optimistic note. Moreover, Dr. Yamada and his team are confident that this model will lead to a breakthrough in the field of materials science. In summary, this study is a significant step forward in the identification of entirely new phases in quasicrystals commonly found in materials such as mesoporous silica, minerals, alloys, and liquid crystals. We surely wish that this model opens up avenues for the discovery of interesting new materials in the future! *** Reference DOI: https://doi.org/10.1002/advs.202304546 Authors: Hirotaka Uryu1, Tsunetomo Yamada1, Koichi Kitahara2,3, Alok Singh4, Yutaka Iwasaki5, Kaoru Kimura5, Kanta Hiroki1, Naoki Miyao6, Asuka Ishikawa7, Ryuji Tamura6, Satoshi Ohhashi8, Chang Liu9, and Ryo Yoshida9,10 Affiliations: 1Department of Applied Physics, Tokyo University of Science 2Department of Materials Science and Engineering, National Defense Academy 3Department of Advanced Materials Science, The University of Tokyo 4Electron Microscopy Analysis Station, Structural Materials Microstructure Evaluation Group, National Institute for Materials Science 5Thermal Energy Materials Group, Research Center for Materials Nanoarchitectonics, National Institute for Materials Science 6Department of Materials Science and Technology, Tokyo University of Science 7Research Institute of Science and Technology, Tokyo University of Science, Tokyo 125-8585, Japan 8Institute of Multidisciplinary Research for Advanced Materials, Tohoku University 9The Institute of Statistical Mathematics, Research Organization of Information and Systems 10Department of Statistical Science, The Graduate University for Advanced Studies Funding information This work was supported in part by an MEXT KAKENHI Grant-in-Aid for Scientific Research on Innovative Areas (Grant Numbers 19H05818 and 19H05820), JST CREST Grant (Grant Number JPMJCR22O3), Grant-in-Aid for Scientific Research (A) (Grant Number 19H01132) from the Japan Society for the Promotion of Science (JSPS) and JST CREST Grant (Grant Number JPMJCR19I3). Journal Advanced Science DOI 10.1002/advs.202304546 Method of Research Computational simulation/modeling Subject of Research Not applicable Article Title Deep learning enables rapid identification of a new quasicrystal from multiphase powder diffraction patterns Article Publication Date 14-Nov-2023 COI Statement The authors declare no conflict of interest. Disclaimer: AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system.",
    "originSummary": [
      "Researchers from Tokyo University of Science have created a deep learning model that can identify previously unknown quasicrystalline phases in multiphase crystalline samples.",
      "The model achieved an impressive prediction accuracy of over 92% and successfully detected an unknown i-QC phase in a screening of diffraction patterns from unidentified materials.",
      "This model has the potential to speed up the phase identification process for multiphase samples and can also be expanded to identify novel types of crystalline materials, leading to advancements in the materials science field."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "Scientists Develop Noise-Canceling Headphones with Deep Learning AI for Customized Sound Filtering",
    "originLink": "https://www.livescience.com/technology/artificial-intelligence/these-noise-canceling-headphones-can-filter-specific-sounds-on-command-thanks-to-deep-learning",
    "originBody": "Artificial Intelligence These noise-canceling headphones can filter specific sounds on command, thanks to deep learning News By Keumars Afifi-Sabet published 17 November 2023 Scientists have created headphones that let users pick the sounds they want to listen to or block out, thanks to a new AI algorithm that could lead to \"superhuman hearing\". Comments (0) Scientists have designed the AI-embedded software so it can work with different kinds of noise-caneling headphones. (Image credit: Witthaya Prasongsin via Getty Images) Scientists have built noise-canceling headphones that filter out specific types of sound in real-time — such as birds chirping or car horns blaring — thanks to a deep learning artificial intelligence (AI) algorithm. The system, which researchers at the University of Washington dub \"semantic hearing,\" streams all sounds captured by headphones to a smartphone, which cancels everything before letting wearers pick the specific types of audio they'd like to hear. They described the protoype in a paper published Oct. 29 in the journa IACM Digital Library. Once sounds are streamed to the app, the deep learning algorithm embedded in the software means they can use voice commands, or the app itself, to choose between 20 categories of sound to allow. These include sirens, baby cries, vacuum cleaners, and bird chips among others. They chose these 20 categories because they felt humans could distinguish between them with reasonable accuracy, according to the paper. The time delay for this entire process is under one-hundredth of a second. \"Imagine being able to listen to the birds chirping in a park without hearing the chatter from other hikers, or being able to block out traffic noise on a busy street while still being able to hear emergency sirens and car honks or being able to hear the alarm in the bedroom but not the traffic noise,\" Shyam Gollakota, assistant professor in the Department of Computer Science and Engineering at the University of Washington, told Live Science in an email. Related link: Best running headphones 2023: Step up your workout Deep learning is a form of machine learning in which a system is trained with data in a way that mimics how the human brain learns. The deep learning algorithm was challenging to design, Gollakota said, because it needed to understand the different sounds in an environment, separate the target sounds from the interfering sounds, and preserve the directional cues for the target sound. The algorithm also needed all of this to happen within just a few milliseconds, so as not to cause lags for the wearer. His team first used recordings from AudioSet, a widely used database of sound recordings, and combined this with additional data from four separate audio databases. The team labeled these entries manually then combined them to train the first neural network. But this neural network was only trained on sample recordings — not real-world sound, which is messier and more difficult to process. So the team created a second neural network to generalize the algorithm it'd eventually deploy. This included more than 40 hours of ambient background noise, general noises you'd encounter in indoor and outdoor spaces, and recordings captured from more than 45 people wearing a variety of microphones. They used a combination of the two datasets to train the second neural network, so it can distinguish between the target categories of sound in the real world, regardless of which headphones the user is wearing, or the shape of their head. Differences, even small ones, may affect the way the headphones receive sound. The researchers plan to commercialize this technology in the future and find a way to build headphones fitted with the software and hardware to perform the AI processing on the device. RELATED STORIES — Hearing aids: How they work and which type is best for you — AI Can Now Decode Words Directly from Brain Waves — Listen to the sounds of Pando, the largest living tree in the world \"Semantic hearing is the first step towards creating intelligent hearables that can augment humans with capabilities that can achieve enhanced or even superhuman hearing,\" Gollakota continued, which likely means amplifying quiet noises or allowing wearers to hear previously inaudible frequencies. \"In the industry we are seeing custom chips that are designed for deep learning integrated into wearable devices. So it is very likely that technology like this will be integrated into headsets and earbuds that we are using.\" Live Science newsletter Stay up to date on the latest science news by signing up for our Essentials newsletter. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Keumars Afifi-Sabet Channel Editor, Technology Keumars is the technology editor at Live Science. He has written for a variety of publications including ITPro, The Week Digital, ComputerActive and TechRadar Pro. He holds a BSc in Biomedical Sciences, and has worked as a technology journliast for more than five years. MORE ABOUT ARTIFICIAL INTELLIGENCE 'Student of Games' is the 1st AI that can master different types of games, like chess and poker In a 1st, AI neural network captures 'critical aspect of human intelligence' LATEST The Peloton Guide is less than $100 right now with this massive 51% discount SEE MORE LATEST ► SEE ALL COMMENTS (0) No comments yet Comment from the forums MOST POPULAR What will happen if the Iceland volcano finally erupts? By Harry BakerNovember 16, 2023 Astronauts accidentally dropped a tool bag on a spacewalk, and you can see it with binoculars By Robert LeaNovember 16, 2023 Apple's best ever iPad mini is $30 off at Amazon By Lloyd CoombesNovember 16, 2023 CRISPR therapy for high cholesterol shows promise in early trial By Nicoletta LaneseNovember 16, 2023 Stone Age Europeans mastered spear-throwers 10,000 years earlier than we thought, study suggests By Tom MetcalfeNovember 16, 2023 'Student of Games' is the 1st AI that can master different types of games, like chess and poker By Keumars Afifi-SabetNovember 16, 2023 Why is Iceland so volcanically active? A geologist explains By Jaime ToroNovember 16, 2023 Brightest gamma-ray explosion of all time scrambled Earth's upper atmosphere By Robert LeaNovember 16, 2023 Cult temples and sacrificial pit unearthed at ancient Roman camp in Germany By Jennifer NalewickiNovember 16, 2023 Building blocks of life may have formed on dust in the cold vacuum of space By Paul SutterNovember 15, 2023 This Surface Pro 9 2-in-1 has a massive $540 price cut at Best Buy By Lloyd CoombesNovember 15, 2023 MOST READ MOST SHARED 1 Gulf Stream current could collapse in 2025, plunging Earth into climate chaos: 'We were actually bewildered' 2 Oops! US Space Force may have accidentally punched a hole in the upper atmosphere 3 'Time's finally up': Impending Iceland eruption is part of centuries-long volcanic pulse 4 Supervolcano 'megabeds' discovered at bottom of sea point to catastrophic events in Europe every 10,000 to 15,000 years 5 2 Salmonella outbreaks linked to pet foods hit US and Canada",
    "originSummary": [
      "Scientists at the University of Washington have created noise-canceling headphones with an AI algorithm that can filter out specific sounds in real-time.",
      "The algorithm allows users to stream all sounds captured by the headphones to a smartphone app, where they can select the specific types of audio they want to hear.",
      "The AI-embedded software is compatible with various noise-canceling headphones and can differentiate between 20 categories of sound.",
      "The researchers aim to commercialize the technology and integrate it into wearable devices like headsets and earbuds.",
      "This breakthrough could potentially enable wearers to have \"superhuman hearing\" and enhance their auditory experience."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "New deep learning model detects previously unknown quasicrystalline phase",
    "originLink": "https://phys.org/news/2023-11-deep-previously-unknown-quasicrystalline-phase.html",
    "originBody": "November 17, 2023 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked peer-reviewed publication trusted source proofread Deep learning model can detect a previously unknown quasicrystalline phase by Tokyo University of Science Researchers propose a machine learning model that identifies a new Al–Si–Ru i-QC phase. With the potential for extended applications for identification of other structures like decagonal and dodecagonal QCs, the proposed model demonstrates immense potential for accelerating the phase identification process of multiphase samples. Credit: Tsunetomo Yamada / TUS Crystalline materials are made up of atoms, ions, or molecules arranged in an ordered, three-dimensional structure. They are widely used for the development of semiconductors, pharmaceuticals, photovoltaics, and catalysts. The type of structures that fall into the category of crystalline materials continues to expand as scientists design novel materials to address emerging challenges pertaining to energy storage, carbon capture, and advanced electronics. However, the development of such materials necessitates precise ways of identifying them. Currently, powder X-ray diffraction is widely used for this purpose. It identifies the structure of crystalline materials by examining scattered X-rays from a powdered sample. However, the task of identification becomes quite complex when dealing with multiphase samples containing different types of crystals with distinct structures, orientations, or compositions. In such cases, the accurate identification of the various phases present in the sample relies on the expertise of scientists, making the process time-consuming. To expedite this process, innovative data-driven methods, such as machine learning, have been used for distinguishing individual phases within multiphase samples. While substantial progress has been made in utilizing them for collecting information about known phases, the identification of unknown phases in multiphase samples still remains a challenge. Now, however, researchers have proposed a new machine learning \"binary classifier\" model that can identify the presence of icosahedral quasicrystal (i-QC) phases—a kind of long-range ordered solids that have self-similarity in their diffraction patterns—from multiphase powder X-ray diffraction patterns. This study involved collaboration among Tokyo University of Science (TUS), National Defense Academy, National Institute for Materials Science, Tohoku University, and The Institute of Statistical Mathematics. It was led by Junior Associate Professor Tsunetomo Yamada from TUS, Japan, and was published in the Advanced Science journal on 14 November 2023. \"Across the world, researchers have made attempts to predict new substances using artificial intelligence and machine learning. However, identifying whether a desired substance is produced takes up substantial time and effort on the part of human experts. Therefore, we came up with the idea of using deep learning to identify new phases,\" explains Dr. Yamada. For developing the said model, the researchers first created a \"binary classifier\" using 80 types of convolutional neural networks. They next trained the classifier model using synthetic multiphase X-ray diffraction patterns, which were designed as representations of the expected patterns associated with i-QC phases. Following the training phase, the model's performance was assessed using both synthetic patterns and a database of actual patterns. Quite interestingly, the model achieved a prediction accuracy of over 92%. It also successfully identified an unknown i-QC phase within multiphase Al–Si–Ru alloys when used for screening 440 measured diffraction patterns from unknown materials in six different alloy systems. The presence of the unknown i-QC phase was further confirmed on analyzing the microstructure and composition of the material using transmission electron microscopy. Notably, the proposed deep learning method has the ability to identify the i-QC phase even when it is not the most prominent component in the mixture. Moreover, this model can be used for the identification of new decagonal and dodecagonal QCs and can be extended to various types of other crystalline materials as well. \"With the proposed model, we were able to detect unknown quasicrystalline phases present in multiphase samples with high accuracy. The accuracy of this deep learning model thus points to the possibility of accelerating the process of phase identification of multiphase samples,\" concludes Dr. Yamada. Moreover, Dr. Yamada and his team are confident that this model will lead to a breakthrough in the field of materials science. In summary, this study is a significant step forward in the identification of entirely new phases in quasicrystals commonly found in materials such as mesoporous silica, minerals, alloys, and liquid crystals. More information: Hirotaka Uryu et al, Deep Learning Enables Rapid Identification of a New Quasicrystal from Multiphase Powder Diffraction Patterns, Advanced Science (2023). DOI: 10.1002/advs.202304546 Journal information: Advanced Science Provided by Tokyo University of Science Citation: Deep learning model can detect a previously unknown quasicrystalline phase (2023, November 17) retrieved 18 November 2023 from https://phys.org/news/2023-11-deep-previously-unknown-quasicrystalline-phase.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "originSummary": [
      "Researchers from Tokyo University of Science have created a machine learning model that can identify a new quasicrystalline phase called Al-Si-Ru i-QC.",
      "The model achieved a prediction accuracy of over 92% and successfully detected the unknown phase within multiphase materials.",
      "This deep learning method has the potential to speed up the phase identification process for multiphase samples and could be expanded to identify other crystalline materials, which could have significant implications for materials science."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "New Deep Learning Model Enhances Earthquake Forecasting",
    "originLink": "https://www.engineering.com/story/deep-learning-model-improves-earthquake-forecasting",
    "originBody": "(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl +'&gtm_auth=6lOp15Y1OMdfoGNSH8uOrA&gtm_preview=env-2&gtm_cookies_win=x'; f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-PKQQG4C');document.cookie = `gtm_tracking_id=UA-346666-1; expires=${new Date().getTime() + 1000*36000}; path=/`!function (f, b, e, v, n, t, s) { if (f.fbq) return; n = f.fbq = function () { n.callMethod ? n.callMethod.apply(n, arguments) : n.queue.push(arguments) }; if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0'; n.queue = []; t = b.createElement(e); t.async = !0; t.src = v; s = b.getElementsByTagName(e)[0]; s.parentNode.insertBefore(t, s) }(window, document, 'script', 'https://connect.facebook.net/en_US/fbevents.js'); fbq('init', '565737710278505'); fbq('track', 'PageView');window.googletag = window.googletag || {}; window.googletag.cmd = window.googletag.cmd || []; (function () { var gads = document.createElement('script'); gads.async = true; gads.type = 'text/javascript'; var useSSL = 'https:' == document.location.protocol; gads.src = (useSSL ? 'https:' : 'http:') + '//www.googletagservices.com/tag/js/gpt.js'; var node = document.getElementsByTagName('script')[0]; node.parentNode.insertBefore(gads, node); })();(function (e, t, o, n, p, r, i) { e.visitorGlobalObjectAlias = n; e[e.visitorGlobalObjectAlias] = e[e.visitorGlobalObjectAlias] || function () { (e[e.visitorGlobalObjectAlias].q = e[e.visitorGlobalObjectAlias].q || []).push(arguments) }; e[e.visitorGlobalObjectAlias].l = (new Date).getTime(); r = t.createElement(\"script\"); r.src = o; r.async = true; i = t.getElementsByTagName(\"script\")[0]; i.parentNode.insertBefore(r, i) })(window, document, \"https://diffuser-cdn.app-us1.com/diffuser/diffuser.js\", \"vgo\"); vgo('setAccount', '1000801328'); vgo('setTrackByDefault', true); vgo('process');function reloadOnError() { var url = window.location.toString(); var key = 'la'; var re = new RegExp('([?&])' + key + '=([^&$]*)(&|$)', 'i'); var separator = url.indexOf('?') !== -1 ? '&' : '?'; var count = 0 if (matches = url.match(re)) { count = parseInt(matches[2]) url = url.replace(re, '$1' + key + \"=\" + (count + 1) + '$3'); } else { url = url + separator + key + '=' + (count + 1) } if (count Deep Learning Model Improves Earthquake ForecastingEngineering.com",
    "originSummary": [
      "A deep learning model has been built using Diffuser, a machine learning platform, to enhance earthquake forecasting.",
      "The model has displayed promising outcomes in predicting earthquakes before they happen.",
      "This advancement could have substantial implications for earthquake preparedness and prevention."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "Python code library surpasses 100k downloads, offers perspective on future of brain-inspired AI",
    "originLink": "https://techxplore.com/news/2023-11-paper-perspective-future-brain-inspired-ai.html",
    "originBody": "November 17, 2023 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked peer-reviewed publication trusted source proofread Paper offers perspective on future of brain-inspired AI as Python code library passes major milestone by University of California - Santa Cruz The brain is the perfect place to look for inspiration to develop more efficient neural networks. Spiking neural networks are pervading many streams of deep learning which are in need of low-power, resource-constrained, and often portable operation. Credit: Jason Eshraghian Four years ago, UC Santa Cruz's Jason Eshraghian developed a Python library that combines neuroscience with artificial intelligence to create spiking neural networks, a machine learning method that takes inspiration from the brain's ability to efficiently process data. Now, his open source code library, called \"snnTorch,\" has surpassed 100,000 downloads and is used in a wide variety of projects, from NASA satellite tracking efforts to semiconductor companies optimizing chips for AI. A paper published in the journal Proceedings of the IEEE documents the coding library but also is intended to be a candid educational resource for students and any other programmers interested in learning about brain-inspired AI. \"It's exciting because it shows people are interested in the brain, and that people have identified that neural networks are really inefficient compared to the brain,\" said Eshraghian, an assistant professor of electrical and computer engineering. \"People are concerned about the environmental impact [of the costly power demands] of neural networks and large language models, and so this is a very plausible direction forward.\" Building snnTorch Spiking neural networks emulate the brain and biological systems to process information more efficiently. The brain's neurons are at rest until there is a piece of information for them to process, which causes their activity to spike. Similarly, a spiking neural network only begins processing data when there is an input into the system, rather than constantly processing data like traditional neural networks. \"We want to take all the benefits of the brain and its power efficiency and smush them into the functionality of artificial intelligence—so taking the best of both worlds,\" Eshraghian said. Eshraghian began building the code for a spiking neural network in Python as a passion project during the pandemic, somewhat as a method to teach himself the coding language Python. A chip designer by training, he became interested in learning to code when considering that computing chips could be optimized for power efficiency by co-designing the software and the hardware to ensure they best complement each other. Now, snnTorch is being used by thousands of programmers around the world on a variety of projects, supporting everything from NASA's satellite tracking projects to major chip designers such as Graphcore. While building the Python library, Eshraghian created code documentation and educational materials, which came naturally to him in the process of teaching himself the coding language. The documents, tutorials, and interactive coding notebooks he made later exploded in the community and became the first point of entry for many people learning about the topics of neuromorphic engineering and spiking neural networks, which he sees as one of the major reasons that his library became so popular. An honest resource Knowing that these educational materials could be very valuable to the growing community of computer scientists and beyond who were interested in the field, Eshraghian began compiling his extensive documentation into a paper. The paper acts as a companion to the snnTorch code library and is structured like a tutorial, and an opinionated one at that, discussing uncertainty among brain-inspired deep learning researchers and offering a perspective on the future of the field. . Eshraghian said that the paper is intentionally upfront to its readers that the field of neuromorphic computing is evolving and unsettled in an effort to save students the frustration of trying to find the theoretical basis for code decision-making that the research community doesn't even understand. \"This paper is painfully honest, because students deserve that,\" Eshraghian said. \"There's a lot of things that we do in deep learning, and we just don't know why they work. A lot of times we want to claim that we did something intentionally, and we published because we went through a series of rigorous experiments, but here we say just: this is what works best and we have no idea why.\" The paper contains blocks of code, a format unusual to typical research papers. These code blocks are sometimes accompanied by explanations that certain areas may be vastly unsettled, but provide insight into why researchers think certain approaches may be successful. Eshraghian said he has seen a positive reception to this honest approach in the community, and has even been told that the paper is being used in onboarding materials at neuromorphic hardware startups. \"I don't want my research to put people through the same pain I went through,\" he said. Learning from and about the brain The paper offers a perspective on how researchers in the field might navigate some of the limitations of brain-inspired deep learning that stem from the fact that overall, our understanding of how the brain functions and processes information is quite limited. For AI researchers to move toward more brain-like learning mechanisms for their deep learning models, they need to identify the correlations and discrepancies between deep learning and biology, Eshraghian said. One of these key differences is that brains can't survey all of the data they've ever inputted in the way that AI models can, and instead focus on the real-time data that comes their way, which could offer opportunities for enhanced energy efficiency. \"Brains aren't time machines, they can't go back—all your memories are pushed forward as you experience the world, so training and processing are coupled together,\" Eshraghian said. \"One of the things that I make a big deal of in the paper is how we can apply learning in real time.\" Another area of exploration in the paper is a fundamental concept in neuroscience that states that neurons that fire together are wired together—meaning when two neurons are triggered to send out a signal at the same time, the pathway between the two neurons is strengthened. However, the ways in which the brain learns on an organ-wide scale still remains mysterious. The \"fire together, wired together\" concept has been traditionally seen as in opposition to deep learning's model training method known as backpropagation, but Eshraghian suggests that these processes may be complementary, opening up new areas of exploration for the field. Eshraghian is also excited about working with cerebral organoids, which are models of brain tissue grown from stem cells, to learn more about how the brain processes information. He's currently collaborating with biomolecular engineering researchers in the UCSC Genomics Institute's Braingeneers group to explore these questions with organoid models. This is a unique opportunity for UC Santa Cruz engineers to incorporate \"wetware\"—a term referring to biological models for computing research—into the software/hardware co-design paradigm that is prevalent in the field. The snnTorch code could even provide a platform for simulating organoids, which can be difficult to maintain in the lab. \"[The Braingeneers] are building the biological instruments and tools that we can use to get a better feel for how learning can happen, and how that might translate in order to make deep learning more efficient,\" Eshraghian said. Brain-inspired learning at UCSC and beyond Eshraghian is now using the concepts developed in his library and the recent paper in his class on neuromorphic computing at UC Santa Cruz called \"Brain-Inspired Deep Learning.\" Undergraduate and graduate students across a range of academic disciplines are taking the class to learn the basics of deep learning and complete a project in which they write their own tutorial for, and potentially contributing to, snnTorch. \"It's not just kind of coming out of the class with an exam or getting an A plus, it's now making a contribution to something, and being able to say that you've done something tangible,\" Eshraghian said. Eshraghian is collaborating with people to push the field in a number of ways, from making biological discoveries about the brain, to pushing the limits of neuromorphic chips to handle low-power AI workloads, to facilitating collaboration to bring the spiking neural network-style of computing to other domains such as natural physics. Discord and Slack channels dedicated to discussing the spiking neural network code support a thriving environment of collaboration across industry and academia. Eshraghian even recently came across a job posting that listed proficiency in snnTorch as a desired quality. More information: Jason K. Eshraghian et al, Training Spiking Neural Networks Using Lessons From Deep Learning, Proceedings of the IEEE (2023). DOI: 10.1109/JPROC.2023.3308088 Journal information: Proceedings of the IEEE Provided by University of California - Santa Cruz Citation: Paper offers perspective on future of brain-inspired AI as Python code library passes major milestone (2023, November 17) retrieved 18 November 2023 from https://techxplore.com/news/2023-11-paper-perspective-future-brain-inspired-ai.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "originSummary": [
      "The University of California - Santa Cruz has developed a Python code library called \"snnTorch\" that combines neuroscience with AI to create spiking neural networks.",
      "The library has been downloaded over 100,000 times and is used in various projects, including NASA satellite tracking and AI chip optimization.",
      "Its creator, Jason Eshraghian, has published a paper emphasizing the importance of understanding deep learning's limitations and the need for more research in neural networks to achieve greater efficiency. The library and research concepts are also being incorporated into Eshraghian's class on neuromorphic computing at UC Santa Cruz."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "New AI Technology Enhances Colorectal Cancer Detection",
    "originLink": "https://www.azorobotics.com/News.aspx?newsID=14479",
    "originBody": "1 Groundbreaking Study Aims to Improve Detection of Colorectal Cancer Using AI Technology",
    "originSummary": [
      "Researchers have conducted a groundbreaking study to improve the detection of colorectal cancer using artificial intelligence (AI) technology.",
      "The study aims to leverage AI to enhance the accuracy and efficiency of colorectal cancer diagnosis.",
      "The utilization of AI technology has the potential to revolutionize the early detection and treatment of colorectal cancer."
    ],
    "commentBody": "",
    "commentSummary": ["There is no summary provided for the text."],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "Development of Machine Learning Models for Methane Plume Segmentation Using Hyperspectral Data",
    "originLink": "https://www.nature.com/articles/s41598-023-44918-6",
    "originBody": "Article Open access Published: 17 November 2023 Semantic segmentation of methane plumes with hyperspectral machine learning models Vít Růžička, Gonzalo Mateo-Garcia, Luis Gómez-Chova, Anna Vaughan, Luis Guanter & Andrew Markham Scientific Reports volume 13, Article number: 19999 (2023) Cite this article Metrics details Abstract Methane is the second most important greenhouse gas contributor to climate change; at the same time its reduction has been denoted as one of the fastest pathways to preventing temperature growth due to its short atmospheric lifetime. In particular, the mitigation of active point-sources associated with the fossil fuel industry has a strong and cost-effective mitigation potential. Detection of methane plumes in remote sensing data is possible, but the existing approaches exhibit high false positive rates and need manual intervention. Machine learning research in this area is limited due to the lack of large real-world annotated datasets. In this work, we are publicly releasing a machine learning ready dataset with manually refined annotation of methane plumes. We present labelled hyperspectral data from the AVIRIS-NG sensor and provide simulated multispectral WorldView-3 views of the same data to allow for model benchmarking across hyperspectral and multispectral sensors. We propose sensor agnostic machine learning architectures, using classical methane enhancement products as input features. Our HyperSTARCOP model outperforms strong matched filter baseline by over 25% in F1 score, while reducing its false positive rate per classified tile by over 41.83%. Additionally, we demonstrate zero-shot generalisation of our trained model on data from the EMIT hyperspectral instrument, despite the differences in the spectral and spatial resolution between the two sensors: in an annotated subset of EMIT images HyperSTARCOP achieves a 40% gain in F1 score over the baseline. Introduction Methane leak detection from anthropogenic sources has seen increasing attention, as it is regarded as one of the most viable targets for preventing catastrophic scenarios in temperature increase due to climate change related effects1. Given methane’s short atmospheric lifetime, its removal from the atmosphere would have a very rapid effect in reducing global warming over the next decades. Large leaks, the so-called super-emitters, have been shown to contribute disproportionately to the concentration of methane in the atmosphere: Lavaux et al.2 recently showed that 12% of all oil and gas (O &G) methane emissions are episodic ultra-emission events that in many cases are caused by equipment failures in oil rigs, pipelines or well pads. Additionally, those emissions are highly underestimated: Alvarez et al.3 reported that O &G supply chain emissions in 2015 were 60% higher than bottom up estimates from the United States Environmental Protection Agency, and Zhang et al.4 reported that observed emissions using satellite data are two times higher than bottom-up inventories in the Permian basin. This is due to the fact that bottom-up inventories often underestimate emissions, which can be improved with the use of satellite-based information. Using different multispectral and hyperspectral satellite instruments several works5,6,7,8,9 have proposed methods for detection and identification of point sources of medium to large methane emissions (>100kg/h). However these methods still require a significant amount of manual intervention: for hyperspectral instruments, methods based on a matched filter, such as mag1c10, produce reliable enhancements, however, they are still prone to high false detection rates. Meanwhile, methods for multispectral data have not been automated and existing approaches6,8 require manual inspection by human experts looking at pre-computed spectral ratio products. Furthermore, there is no standard dataset for the task of methane plume detection; existing works5,6,7,8,9 report detection limits anecdotally and do not allow for an easy comparison between methods and sensors. Additionally, research in machine learning models aimed at processing hyperspectral data is limited, uses very small datasets, and usually focuses on the task of land cover classification. With the arrival of recent satellite missions, such as PRISMA7,11, EnMAP12 and the NASA’s Earth Surface Mineral Dust Source Investigation (EMIT)13 there is a need for a reliable automated method with low false detection rate capable of automatically detecting methane plume leaks that would also enable methane plume attribution. In this context, research on transferability of the knowledge from data collected from one sensor, ideally with reliable annotations, to other novel sensors would be highly useful14. The aim of this paper therefore is to address these problems and to foster artificial intelligence (AI) research in this area. For this, we improve and extend the annotation of the dataset of AVIRIS hyperspectral images from the Permian basin aerial campaign15. We release the extended annotation with this dataset in a machine learning ready format, that can serve as a testbed for further research in methane plume detection and in general for processing hyperspectral data with machine learning models. For a more in-depth analysis of the methane events present in the dataset, we refer the reader to15. In our dataset, we include 1878 images of high quality with verified plume events, which are matched with an equal number of background class samples with no observed emissions. Furthermore, we propose small and efficient machine learning models based on the U-Net architecture16 that use the established representations for both hyperspectral and multispectral data. Our model is also lightweight, with only 6.6M parameters in total. For hyperspectral data, we show that, using our HyperSTARCOP model, we can significantly reduce the false detection rate of mag1c10, while maintaining high semantic segmentation performance. For simulated WorldView-3 multispectral data, we propose a MultiSTARCOP model that enables automatic methane detection using existing pre-computed band ratios6,8. Figure 1 shows an example prediction of the proposed models in comparison with the existing baselines. Figure 1 Predictions on an example plume event from the AVIRIS data. On the left, we show some of the inputs used by our models and by the baselines and, on the right, we show the comparison of the predictions with the ground truth labels. It can be seen, that the major issue with the existing baselines is the high false positive ratio. Full size image Finally, we demonstrate sensor agnosticism of our proposed method, by showing that the models trained on the AVIRIS dataset work as zero-shot detectors on images from a different hyperspectral sensor such as EMIT. These two sensors have vastly different properties, resolutions, deployment and also scope: the AVIRIS training data is from the Permian Basin region in the US and collected aerially, while space based sensor EMIT acquires imagery on all arid and semi-arid regions where most oil and gas fields are located. We note, that our approach would likely work with data from other hyperspectral sensors as well. To summarise, the benefit of having an automated methane detection system is clear in the context of the ever-increasing size of Remote Sensing data and with the expected increased data collection cadence of the upcoming hyperspectral satellite missions. Such an automated system would ease the work of experts in this field, as it could sift through the vast amounts of data and propose locations of interest for manual confirmation and release through relevant agencies. Another interesting direction would be the deployment of our system for fully autonomous detection of methane plumes on-board of satellites to allow for increased autonomy of satellite constellations. Suspected detections could trigger automated scheduling of follow-up observations of the same area, potentially increasing the capture of scientifically interesting data. Finally, using the methods presented by17,18, it is possible to quantify methane emissions using the methane enhancement products and publicly available wind information. Using our system, one can effectively clean up typical confounders from these products, and arrive at better estimates of plume quantities. Background Methane signature and enhancement methods Figure 2 Illustration of the presence of the methane signal (shown through the methane transmittance) in comparison with the bands available in commonly used satellites. For clarity the hyperspectral sensors of AVIRIS-NG and EMIT, show the central wavelengths, while for the multispectral instruments of Sentinel-2 and the WorldView-3, we show the whole band ranges in the short-wave infrared (SWIR) region. We also highlight the region that corresponds to water vapour absorption, which is typically excluded from the data analysis. Full size image The methane signal in the near-infrared part of the electromagnetic spectrum is visible mainly in two spectral ranges: between the wavelengths of 1600 and 1850 nm, and between 2100 and 2500 nm. This is relevant for the choice of the instrument, as we likely want to capture at least one of these regions, and usually also a region outside of these ranges, to get information about the background signal. There are several satellites capable of observing methane concentrations with spectral bands on these ranges with various spatial resolutions9. On the one hand, hyperspectral imagers such as AVIRIS or EMIT cover both methane ranges at narrow spectral resolution (5–8 nm). On the other hand, multispectral satellites such as WorldView-3 or Sentinel-2 have broad bands overlapping those spectral ranges. Controlled methane release experiments have validated the retrieval methods on both airborne and satellite instruments19,20,21. Figure 2 shows the intersection of the expected signature of the methane signal with different types of satellites referenced in this paper. Uniquely, as compared with other semantic segmentation tasks conducted with Remote Sensing data, the methane plumes are not visible in any one isolated band. A vast array of different enhancement methods are therefore used to highlight the plume inside the data. In this work, we explore these products as inputs of the proposed models. For multispectral instruments (MSI) we explore the ratio products6,8, while for hyperspectral instruments (HSI) we use the improved matched filter approach10. We note that, in practice, most of the mentioned approaches remain manual. A visual inspection by human experts is needed, often with a requirement of parameter tweaking given different locations. Most of these methods produce many false positive detections. Furthermore, it is not obvious how to use these methods in an automated manner out of the box. In this paper, we use these classical approaches to extract relevant features of high dimensional input data, and we use thresholding methods to explore the feasibility of their direct implementation as baselines. Machine Learning for hyperspectral data processing Research in machine learning for hyperspectral data processing is limited mainly because there is a lack of relevant, large and annotated datasets with hyperspectral data, that have high spatial resolution and diverse geographical distribution across the world22. Existing benchmarks such as Indian Pines23 or University of Pavia are based on one or very few small image acquisitions. As noted in several overview papers24,25,26, this works particularly badly for the hyperspectral scenario, as the high dimensionality of the data, in combination with the low number of samples, introduces severe problems connected with the curse of dimensionality for the trained models. This is sometimes addressed by using simulated data, or private datasets26. To make matters worse, the typically taken approach is to divide this already small data into training and test sets, and the reported scores tend to be heavily overfitted, giving very large accuracy numbers almost regardless of the used method25. Finally, most of the existing hyperspectral datasets focus on per pixel classification of land cover classes while other problems where the hyperspectral signature would be more interesting are not available. Machine learning for methane detection Research in machine learning models to be used with remote sensing data for methane detection has been limited. A large amount of the work also remains manual. For example, the work of27 has used machine learning models to detect methane plumes in the extremely coarse and low resolution data of the TROPOMI sensor, and has used these automated detections for a manual search in higher resolution data. Similarly, there have been works on researching methane enhancement products for multispectral data – using Sentinel-2 and Landsat data5,6,28 and using the WorldView-3 data8. However, these works still require follow-up manual intervention, and it is difficult to estimate the performance of each of these enhancement methods in novel untested locations and when compared against each other. There is no benchmark dataset available in this domain that would permit fair comparison across different modalities of the data. There have been very recent works29,30,31 applying deep learning to hyperspectral data with simulated methane plumes. The workshop paper of30 frames the detection of methane plumes as semantic segmentation and uses the matched filter product generated using data from the on-demand satellite PRISMA. They create a synthetic dataset by combining the plume maps from 1000 Sentinel-2 images28 with real, matched filter backgrounds from 150 plume-free PRISMA images. The work of29 instead focuses on the regression task of estimating the emission rate from methane enhancement products. They generate artificial plume shapes using the Large Eddy Simulation (LES)32 and mix these with the background noise of matched filter outputs from the AVIRIS data. Finally, the preliminary work of31 combines the tasks of semantic segmentation with regression, by sequentially training several models to first segment and later quantify the methane emissions from the PRISMA satellite images. Similarly, as in the other instances, the annotation is made by methane plume simulations using the LES and mixing the generated signal back into the hyperspectral data. Unfortunately none of these works provide the datasets and data products and code and the trained models are also not open-sourced. In our view, the contributions of our work compared to these recent works are that (i) our paper considers a dataset made of real-world methane plumes instead of synthetic simulated plumes, (ii) our curated dataset is larger (from 300 different AVIRIS acquisitions) although it is limited to the Permian basin area (iii) we train and compare models from multispectral and hyperspectral views of the data iv) we show zero-shot transferability of the proposed hyperspectral model to other sensor and v) we open-source the dataset with the curated labels, the codebase and the trained models. Data Figure 3 Showing the limitations of the available annotation in the original labels released by15, with a circular mask cut out around the center of each plume. We manually refine and extend these plumes using the mag1c product as guidance. Full size image One of the purposes of our study is to compare the capacity to segment methane plumes of different models on different data modalities but under the same conditions (i.e. using the same dataset). For this, we constructed a balanced ML-ready dataset with different image instances for each element in the dataset. Those instances are manually annotated plume masks, \\(\\text {CH}_4\\) retrievals using the standard enhancement products, hyperspectral images and multispectral images simulated using the AVIRIS-NG hyperspectral data. In order to test the generalization capabilities of our hyperspectral model, we additionally collected a set of images from the EMIT hyperspectral sensor with verified emissions that we also manually labeled. AVIRIS machine learning ready dataset The data of the Permian basin airborne campaign conducted for the study of Cusworth et al.15 was selected to create our dataset. This campaign was conducted from September to November of 2019. 3068 individual methane plumes were found in 564 images retrieved on 31 different days using two hyperspectral airborne instruments: the Next-Generation Airborne Visible/Infrared Imaging Spectrometer (AVIRIS-NG) and the Global Airborne Observatory (GAO) instrument33. Images retrieved by those instruments have an spectral resolution of 5–10 nm and a spatial resolution of 3–10 m. The location, time of acquisition and rough plume segmentation mask of those plumes is available at Zenodo34. Since data from the GAO sensor is not public, we used only the plumes retrieved by AVIRIS-NG. The AVIRIS-NG flight lines of this campaign correspond to 300 level 1B hyperspectral images with an approximate size of 6TB. For each of these images, we derive: the \\(\\text {CH}_4\\) enhancement using the mag1c matched filter model proposed in10 and the simulation of the bands of WorldView-3 using the corresponding spectral response function. We describe each of these in detail in the following subsections. The final dataset size, which provides the relevant and used bands and products, is 60 GB. We sampled chips of size 512\\(\\times\\)512 px from the available AVIRIS-NG images to create the STARCOP dataset. In order to obtain a balanced dataset we selected all tiles containing plumes as positive examples (plume) and sample the same amount of tiles from the pool of locations without plumes to be negatives samples (no-plume). For the negative samples, half of them are randomly selected whereas the other half is chosen using the mag1c10 product as locations with high amount of confounders, i.e. locations where mag1c output is high but there are not plumes. Plume masks provided in original data15 are in a colour mapped RGB PNG format covering a 151\\(\\times\\)151 pixel area. Our first tests with these labels found several inconsistencies such as build up areas near the plume labelled as plume or labels covering only a circular area around the plume source – these are shown in Fig. 3. Also, our chips are larger (512\\(\\times\\)512) which was a problem since the original labels only covered 151\\(\\times\\)151 pixels. Since data quality is of paramount importance for ML models, we manually extended and curated the labels using the IRIS tool (Intelligently Reinforced Image Segmentation) graphical user interface35, which was previously used for similar tasks in36. We mainly used the brush tool to remove label errors and extend the plume to capture the tails. Additionally, we manually inspected all no-plume locations to make sure no plume is present in these chips since we found a couple of large plumes not reported in the original dataset34. In order to split the chips for training and testing, we manually selected chips coming from acquisitions of three days (18th, 21st and 25th of October) to avoid temporal overlap. We chose these days as they are clustered towards the end of the campaign and they have a balanced amount of plumes of different strengths. Image acquisitions from other days are used for training the models. Figure 4a shows the spatial location of AVIRIS training and testing tiles, and the statistics of training and testing chips stratified by the emission rate. For evaluation, we label the data in the test dataset with broad categorical labels: plumes with emission rate lower than 1000 kg/h are labelled as “strong”, while the rest is labelled as “weak”. In order to simulate retrievals of multispectral sensors from AVIRIS-NG images, we convolve the hyperspectral bands with the spectral response function (SRF) of the sensors we seek to simulate. We further converted the radiance values to top of atmosphere (TOA) reflectance using the date of acquisition of the image, the center location and the solar irradiance. For WorldView-3 the solar irradiance on each of the bands is obtained by convolving the SRF of the sensor with the Thuillier solar spectrum37. We have also experimented with the Sentinel-2 satellite as the target for the simulated data, however, even the largest of methane plumes in the Permian Basin area were not visible in the Sentinel-2 band ratio products6,18. We note that with the released dataset we provide the community a tool to simulate synthetic data for any target sensor working in this spectral range, given the knowledge of the properties of the captured bands such as the spectral response function. Similar ratio products would be used. We warn that in some cases this likely wouldn’t work with this particular set of plumes due to the low methane concentrations (as we saw in the case of Sentinel-2). We also note that our dataset can serve as a source of information about the shapes and intensities of methane plumes, which could be used for simulation of new labelled datasets29,30,31. Figure 4 ML-ready dataset produced in this study. (a) Location of the chips of size 512\\(\\times\\)512 pixels used for training and testing the proposed system in the Permian basin. (b) Location of the EMIT granules used for evaluation of zero-shot detection. In the bottom table: Statistics of the number of flight lines, days and amount of chips stratified by emission rate. Full size image We use several data augmentation techniques on the samples in our training dataset. We apply random rotations and extract 128\\(\\times\\)128 px tiles with an overlap of 64 px from the original 512\\(\\times\\)512 px scenes. With this approach our final training dataset holds 167,825 images, each with all the required intermediate feature products. Our test dataset is kept as 512\\(\\times\\)512 px scenes. In summary, we release the final dataset with labelled methane plumes from the AVIRIS-NG sensor alongside the refined labels, simulated multispectral products and added enhancement products (these are described in more details in the Methodology section). The dataset contains in total 1878 plume events, which are split between train and test datasets and samples with no plumes (from random locations and from areas with known confounders). Our dataset is available at: https://doi.org/10.5281/zenodo.786334338. EMIT dataset for estimation of generalisation ability across hyperspectral sensors In order to explore the generalisation ability of our proposed models, we have tested the hyperspectral models trained on the AVIRIS-NG data on a small dataset from another hyperspectral sensor, EMIT. Given the inputs used in our proposed model, we are able to extract similar image patches from another sensor, despite it having different spatial and spectral resolutions. Most importantly, the geographic locations where the data was collected differ – AVIRIS-NG was collected locally in the Permian Basin area of USA, while EMIT is global, covering arid regions around the world. Additionally, the AVIRIS-NG sensor is an aerial based mission, while the EMIT sensor is deployed on board of the International Space Station (ISS). This does have an influence on the amount of atmospheric disturbances affecting the data. Both the ground and the spectral resolution between the two sensors also differs as can be seen on Fig. 2. We are able to circumvent the dependency on one particular sensor, by using a matched filter product as one of the inputs to our models. We have annotated a small subset of the data from the EMIT sensor, which serves as an evaluation dataset. The selection was based on the initially released labels through the EMIT Open Data Portal. In total, we have selected and manually annotated 11 locations with known plumes and 9 locations without any reported plumes – as shown on Fig. 4b. We use the released L1 level of processing of the EMIT data without applying orthorectification, this is in a conscious attempt to simulate near raw data, which would be available on-board of the sensor. Methodology As has been discussed in the previous section, we have compiled a dataset of hyperspectral images, which allows us to simulate multispectral views of the same data. This gives us an option to design machine learning models operating on both types of data and to compare their performance. In this section we describe these models, the proposed feature extraction of inputs depending on the data modality (multi or hyperspectral) as well as the baselines that are compared against. Feature extraction from multispectral data Multispectral data is expected to give lower detection capabilities of methane plume detection, namely due to the lower intersection between available bands and the methane spectral absorption signature as seen on Fig. 2. This is typically addressed by comparing a single band that falls within the methane absorption with other bands that serve as a background reference. From recent literature in the field, we will be using two methods that create these methane enhancement products – we will use these both as baseline methods, and also as inputs to the later proposed machine learning models. These can be seen as classically extracted features. We have included two methods into our analysis, namely the band ratio method proposed by6, which we will denote as “Varon ratio”, and the multi-linear regression (MLR) method proposed by8, which we will denote as the “Sanchez ratio”. In practice, while these methods were tested with different multispectral satellite data, either using the two SWIR bands of Sentinel-2 or the eight SWIR bands of WorldView-3, they remain sensor agnostic. The work of6 proposed several methane enhancement methods, we use the mono-temporal variant that looks at the ratio between a signal band S and a background band B: \\(VaronRatio(S, B) = (c*S - B) / B\\). The parameter c is used to scale one of the bands into the range of the other band, and can be obtained as a least square fit, or as a simplified formula \\(c = sum(B') / sum(S')\\), where \\(S'\\) and \\(B'\\) corresponds to the signal and background bands with removal of outliers. The method of8 instead uses multiple linear regression (MLR) to estimate the background information in the signal band from a combination of other bands. The estimated band \\(S_{MLR}\\) is then compared with the signal band S: \\(SanchezRatio (S) = VaronRatio(S, S_{MLR})\\). We note that the MLR estimation of each tile is not fitted on the whole training set, instead it uses only a single tile. The original paper uses the WorldView-3 bands, namely the B7 or the B8 as the signal bands and bands B1-B6 as background bands. We will use the \\(\\leftrightarrow\\) symbol to refer to the Varon ratio, with first parameter being the signal band and the second parameter the background band. We explore these three variants: (1) First variant, denoted as “Varon”, uses the following ratios: B7\\(\\leftrightarrow\\)B5, B8\\(\\leftrightarrow\\)B5, and finally B7\\(\\leftrightarrow\\)B6. (2) Second variant, denoted as “Sanchez”, uses the B1-B2 and B4-B6 as background bands to compute the MLR products: B7\\(\\leftrightarrow\\)B7\\(_{MLR}\\), B8\\(\\leftrightarrow\\)B8\\(_{MLR}\\), and the SWIR band B1. (3) Finally, the third variant, denoted as “Varon+Sanchez”, is a combination of the two previous methods – using first two Varon ratios with the first Sanchez ratio. As the baseline method we use the Sanchez ratio computed for B8\\(\\leftrightarrow\\)B8\\(_{MLR}\\) thresholded by the experimentally found value of 0.05 and post-processing the binary output with the opening morphological operation. We have tested other ratio products as the baseline method, but the results were almost the same for all variants – the thresholded detections are very noisy regardless of the used ratio. Feature extraction from hyperspectral data Hyperspectral data has very narrow wavelength windows at high spectral resolution, as can be seen on Fig. 2, which is crucial for methane detection. In such cases, it is easier to contrast bands inside and outside of the typical methane absorption to enhance the visibility of the plume inside the image. However, in practice, this approach would still result in a relatively large amount of noise in the extracted features, which is why the typical state of the art methods in this domain use matched filter approaches. We build on top of the matched filter approach of mag1c10. A vanilla matched filter method measures, for every hyperspectral pixel, the similarity between the pixel value minus the average surface reflectance against the methane absorption spectrum (black line in Fig. 2). The proposal of10 improves the method by adding sparsity regularization and an albedo correction to the target spectrum to match. Although this method significantly reduces the amount of false positives, the retrieved image has still a high amount of noise; we found that this happens especially in urban areas (rooftops), water bodies, and human made infrastructures (photovoltaic panels, roads, etc.). As a baseline method we use the mag1c filter with the threshold of 500 ppm\\(\\times\\)m and an opening morphological filter to remove the speckle noise. Machine learning models Figure 5 The proposed HyperSTARCOP and MultiSTARCOP machine learning models based on the U-Net architecture with MobileNetV2 as its encoder network. We note that this architecture is quite lightweight and it has only 6.6M parameters. Full size image In this work, we propose two machine learning model variants, working with the multispectral and hyperspectral data – these two models however share the same architecture design illustrated in Fig. 5, except for using a different number of input channels. We have chosen to use the U-Net architecture16 with MobileNet-v2 encoder39. Our HyperSTARCOP model is trained from scratch, while for the MultiSTARCOP variant, we use the encoder network pre-trained on the ImageNet dataset. This limits our choice of input bands to 3 to mimic the RGB bands commonly used in computer vision tasks, however, experimentally this led to better results with multispectral data and the MultiSTARCOP model. We use min-max normalisation for the ratio products and selected bands, using the statistics from the training dataset. The MultiSTARCOP model uses ratio products computed from the WorldView-3 data. The challenge for this model remains in learning which part of the image contains a plume, and which contains the background information. We note that the ratio methods often highlight other structures present in the image with even stronger signal than that of the methane plume – as can be seen on Fig. 1 with highlighted building outline. In these cases, the signal of the methane is similar to other signatures present in the data. Our model has to learn to differentiate between the shapes and the strength of the signal corresponding to methane plumes and the other background classes. We also note that the strengths of different plumes vary quite significantly and, as such, the model needs to learn how to detect both weak and strong methane plume signatures. In initial exploratory experiments, we tried to train models on separate subsets of the data (such as data containing only strong plumes), but we saw a decrease in performance - we hypothesise that having a dataset of diverse plume shapes and sizes is beneficial. The HyperSTARCOP model instead aims to improve upon the limitation of the current state of the art method of mag1c, namely in reducing its false positive rate. Our model uses the mag1c product with a selection of other bands from the hyperspectral sensor as input features. The underlying assumption is that a machine learning model should be able to learn which of the methane plumes outlined by the mag1c method are true plumes and which are just false detections. This information can be obtained either from the shape of the plume data, where the spatial information seen by 2D convolutional layers should outperform the per pixel baseline. We have tested two versions of this model, one relying only on the mag1c data as the input, and another using the mag1c product with addition of the RGB bands from the AVIRIS data (bands with central wavelengths 640 nm, 550 nm and 460 nm). The assumption is that if a human expert can distinguish between a falsely detected signal from a roof of a house and a real plume, then our model can learn the same. Experimental setup The dataset we use for training the multispectral and the hyperspectral models contains the same samples, with just different views of the data – the original hyperspectral bands, or the simulated multispectral data corresponding to the bands of the WorldView-3 satellite. Correspondingly, we have taken similar approaches when training these two models, but in some instances, we used different hyperparameters. For all training runs we use the Adam optimiser with learning rate of 0.001, keeping other parameters to their default values. In addition, we use a scheduler that reduces the learning rate on plateau by multiplying it with a factor of 0.5, with the patience parameter set to 4. Therefore, the training rate reduces only after 4 epochs without improvement. In total, we train for 15 epochs. The exact values for these hyper-parameters have been established experimentally. For development (training and validation), we use a n1-highmem-8 instance on Google Cloud Platform with one NVIDIA Tesla V100 GPU, one full training and validation run takes between 6 to 8 hours (depending on the used configuration and the number of input products). The training dataset is heavily unbalanced in terms of the number of pixels corresponding to the plumes in contrast to the pixels corresponding to the background class. As such, we need to employ rebalancing measures. For both models, we oversample the instances from the minor class with the function provided by the PyTorch library, the WeightedRandomSampler. We take additional measures, but the approach differs for the multispectral and the hyperspectral scenario. For training the MultiSTARCOP model we use weighted binary cross-entropy loss with the plume pixels weighted by the value of 15. For the HyperSTARCOP model we instead introduce a novel training loss for the context of the task of methane plume detection with hyperspectral data. We weight the loss by the mag1c product – this means that pixels with larger concentration values in the mag1c product will contribute more to the computed loss. We multiply the non-weighted binary cross-entropy loss computed over the whole tile with the mag1c product. This approach is similar to the one used in the original U-Net paper16, where a loss weight mask was used to prioritize pixels between individual segmented detections. Metrics To evaluate our models, we first explore the raw outputs of our segmentation models. Secondly, we use a simple rule to convert these segmentation maps into classification decisions per each tile, to label them with the binary class of either having or not having any methane plume. The segmentation results are described with the area under the precision-recall curve (AUPRC) score, which is independent to the used threshold, and works well in unbalanced scenarios. Each pixel of the segmentation map is then thresholded with the value of 0.5 to produce a binary map – which is then used to compute the F1, precision and recall statistics. For better insight into the performance of the model, we report these scores separately for strong plumes (with emission rate larger than 1000 kg/h) and for weak plume events. Each tile in the evaluation dataset is finally marked as containing a plume if the prediction has more than 10 active pixels. We study the false positive rate (FPR) on the subset of the evaluation dataset that does not contain any plumes. Additionally, we report the percentage of captured plumes stratified into several plume size categories. For a tile in the evaluation dataset which was predicted as containing a plume, we consider this plume to be captured if the thresholded prediction has at least 1 pixel overlap with the ground truth annotation. Experiments with generalization ability of our hyperspectral models In the final experiment, we measure the capabilities of our model to serve as a zero-shot detector of methane leaks on data from other hyperspectral sensor. More concretely, we use the trained HyperSTARCOP models with inputs from the EMIT sensor. Given the knowledge of the wavelength ranges of each band in this new sensor, we can compute the required mag1c product. Furthermore, we re-normalise the RGB bands using the statistics from the AVIRIS training dataset, moving the data into the ranges expected by the models. While the deployment of machine learning models trained on standard computer vision datasets has been tested in-the-wild with other camera instruments (typically also RGB), the scenario with hyperspectral sensors is more complex as the exact number and location of bands, their specific noise profile and the ground spatial resolution differ. Despite these differences, we are able to compute similar input products, and reuse the pre-trained model in a zero-shot manner. We note that we do not alter the ground resolution of the data from EMIT (60 m). Given the diversity of the plumes present in our dataset, and namely their distribution across different sizes, we expect the model had to learn to be scale agnostic. Results We present the results from: (1) training the MultiSTARCOP model on the simulated WorldView-3 data, (2) training the HyperSTARCOP model with the hyperspectral data from the AVIRIS sensor, and finally (3) evaluating the HyperSTARCOP model for zero-shot detection of methane leaks in the hyperspectral data from the EMIT sensor. The simulation of the multispectral views of the data from the original hyperspectral data is described in more detail in Section “AVIRIS machine learning ready dataset”. The metrics we use to analyze the performance of our models are described in Section “Metrics”. We reiterate that we use annotation from real world plume leak events, and that a comparison between the different models is possible given the shared origin of the data. To illustrate this we show a single plume event on Fig. 1 evaluated with our proposed models in comparison with the appropriate baselines for the multispectral and the hyperspectral scenario. We note that the predictions of the baseline methods produce more false positive detections (shown as blue pixels), than any of our proposed models. When comparing the prediction between the two modalities of our proposed models, the MultiSTARCOP model is generally able to only detect the area of the plume with the higher gas concentration, while it misses the extended plume tail. On the other hand, the HyperSTARCOP model is capable of detecting the entire plume, including the areas of lower concentration in the plume tail. Figure 6 Showing the qualitative results of our models evaluated on strong plume samples (in the first two rows), on weak plume samples (in the second two rows) and finally on samples that are known as confounders (in the last two rows). Left to right columns in each sample shows: the RGB bands of the AVIRIS-NG data, the normalised mag1c product computed from the hyperspectral data, the normalised Varon ratios between bands B7 and B5 from the multispectral view of the data, prediction of the MultiSTARCOP model and the HyperSTARCOP model in comparison with the ground truth label. In the first three columns we show the areas outside of the sensor swath (no-data areas) with black background. Full size image In Fig. 6, we show qualitative results of our models on diverse tiles from the evaluation dataset. Performance on multispectral data We have trained the MultiSTARCOP model with the simulated WorldView-3 view of the data with three different enhancement product combinations. In Table 1, we show the segmentation and the classification scores of these three model variants. All of the proposed variants outperform the baseline approach in any of the used metrics. From the explored three variants, the model that uses the “Varon+Sanchez ratios” achieves the best performance in terms of the AUPRC and the F1 metrics. However, we note, that these results are within the range of standard deviation with the model that used the “Varon ratios”, which on the other hand receives the best (lowest) false positive rate by tile. We also see that the performance of all models drops rapidly with smaller plume events, which confirms the assumption that the detection of small methane plumes is a challenging task. Table 1 Results of the multispectral models on the test set, we show results of our proposed model in comparison with the existing baseline. We show the average results of 5 training runs of our models. Full size table Performance on hyperspectral data In Table 2 we report the results of training our HyperSTARCOP model on the AVIRIS data using two different input configurations. We see that the proposed HyperSTARCOP model variants both outperform the baseline approach. We see an increase in the F1 score across both strong (emission larger than 1000 kg/h) and weak plume events, while at the same time achieving a decrease in the false positive rate. This means that the proposed method produces better semantic segmentation of the methane plumes, while also being less sensitive to noise. Table 2 Results of the hyperspectral models on the entire test set, we show results of our proposed model in comparison with the existing baseline. We show the average of training 5 runs of our models. Full size table Figure 7 Results on the test set using fine grade distinction between plume sizes (average of 5 runs of our models). Full size image Furthermore, we explore a more fine-grained evaluation of the proposed models using the per tile classification scores. In Fig. 7a, we explore the percentage of captured plumes stratified by different plume emission sizes, showing the natural trend that stronger plumes are easier to detect. Both of the proposed model variants achieve similar performance. In comparison the mag1c baseline captures more plume tiles – as it predicts more tiles as containing plumes in general. In Fig. 7b, we show that this leads to larger false positive rate on no-plume tiles – the baseline method gets the FPR of 75.29. Both of the proposed models are able to significantly reduce the FPR, with the “mag1c+rgb” model outperforming the “mag1c only” variant. We also note that the FPR on no-plume tiles reported in Fig. 7b is similar to the score of FPR in Table 2, which is evaluated on all tiles (including the ones with plumes). In summary, on no-plume tiles, the HyperSTARCOP “mag1c+rgb” variant achieves the FPR score of 43.79, reducing the FPR by over 41.83% in contrast to the baseline. Furthermore, we see better performance in the segmentation statistics, namely increase of the F1 score for strong (by 21.51%) and weak (by 8.68%) plume events in comparison with the baseline. When using a simple rule to convert these segmentation predictions into per-tile classification, we see a mostly maintained performance, with a small drop in the detection capabilities of the weak methane plume events. We note that this is consistent with the fact that the model was trained on the task of semantic segmentation, and that there are likely more complex methods available for generating per tile classifications. Zero-shot generalisation on EMIT Figure 8 Results on example plume and confounder samples from the EMIT dataset, showing the baseline method and our HyperSTARCOP model using mag1c+rgb as inputs. Full size image We use the HyperSTARCOP models trained on our dataset from the AVIRIS sensor in a zero-shot manner with new data from the EMIT sensor. Table 3 shows that the performance of our proposed model outperforms the baseline approach in F1 score by up to 40.28%. Furthermore the “mag1c+rgb” variant gets better performance in all metrics in comparison with the “mag1c only” version. We also note that the standard deviation of these results is quite high, which is consistent with the fact that the models have to generalise on unseen data and cope with the associated spectral and spatial biases. Table 3 Results of the hyperspectral models on EMIT. We show the average of training 5 runs of our models. Full size table On Fig. 8 we selected few plume events and areas with typical confounders for the mag1c product (urban areas). We see that our HyperSTARCOP model is capable of detecting methane plume events, while also being able to reject the detection falsely highlighted in the mag1c product. We note that these are highly desired properties that have been transferred from the AVIRIS dataset onto evaluated data from another sensor. A typical next step would be to further use our trained models and to finetune them on labelled EMIT datasets. However, the number of labelled examples present in the EMIT dataset so far is much lower than the number of events in the AVIRIS dataset. To summarize, we see that our model has learned useful representations that allow for zero-shot generalisation on data from other sensors. Our models achieve better qualitative and quantitative results, namely they improve the F1 score of the baseline approach by 40.28%. Conclusion In this work, we explore semantic segmentation of methane plumes in the hyperspectral and multispectral data with machine learning models. We publicly release a large scale and high resolution dataset of hyperspectral images from the AVIRIS-NG sensor. We have refined the existing labels15, improving the annotation which is required for training machine learning models. We provide the raw hyperspectral data alongside with simulated multispectral views of the same data, allowing for direct comparison between the two modalities of data. We hope that this dataset will promote research in the areas of methane detection and processing hyperspectral data with machine learning models. We propose and evaluate models based on the small and efficient U-Net architecture with MobileNetV2 encoder with several different configurations. The resulting model architectures are lightweight, with only 6.6M parameters. Our experiments with simulated WorldView-3 data, showcases the difficulties of detecting methane plumes in data from multispectral instruments. On strong plume events, our MultiSTARCOP models get the average F1 score of 31.89 outperforming the multispectral baseline which has F1 score of only 7.44. Our proposed HyperSTARCOP model outperforms the state-of-the-art baseline approach of mag1c10 obtaining better performance in methane plume segmentation, namely increasing the F1 score for strong events by 21.51% and weak events by 8.68%. Importantly, our model also addresses the known limitation of matched filter methods, which produces many false positive detections. We reduce the false positive rate per tile by over 41.83% in contrast to the baseline, at the cost of small drop in the number of captured plumes. Finally, we show that our HyperSTARCOP model can be used for zero-shot generalisation on data from another hyperspectral sensor. Without fine-tuning the model, we obtain a superior score to the mag1c approach on data from the new EMIT sensor, on previously unseen locations. On a small, annotated evaluation set, we improve the F1 score on average by 40.28% over the baseline method. The initial results with the EMIT dataset provide interesting avenues for follow-up research in zero or few-shot learning with hyperspectral data. Furthermore, with the publicly released data, we provide a benchmark dataset to compare machine learning models for hyperspectral data processing, which has been highlighted as crucial by numerous recent overview studies and works22,24,25,26. As potential future research directions we see developing specific architectures for processing hyperspectral data, for which the very recent pre-print of40 is a promising direction. Alternatively, we would like to point towards exploration of general, sensor-agnostic systems, that would be able to detect signals of arbitrary gas signatures from hyperspectral data. Another avenue would be in pursuing development of lightweight models for deployment on-board a satellite. This will allow intelligent decision making in Space for near-real time alerting. This will require evaluating the speed of the trained models in a constrained environment with data available directly on the device, similarly as was done in41,42 in the cases of disaster event and flood detection models. Data availibility We are releasing the full annotated training and test STARCOP datasets on Zenodo https://doi.org/10.5281/zenodo.786334338, the code and the pre-trained models alongside this paper at https://github.com/spaceml-org/STARCOP. We further note that all figures of this paper have been produced with open source python libraries matplotlib, rasterio and folium. Original AVIRIS-NG imagery was gathered from the AVIRIS-NG data portal. EMIT imagery43 was downloaded from the NASA Earth data portal. References Kuylenstierna, J. C., Michalopoulou, E., & Malley, C. (Benefits and costs of mitigating methane emissions, Global methane assessment (2021). Lauvaux, T. et al. Global assessment of oil and gas methane ultra-emitters. Science 375, 557–561, https://doi.org/10.1126/science.abj4351 (2022). Publisher: American Association for the Advancement of Science. Alvarez, R. A. et al. Assessment of methane emissions from the U.S. oil and gas supply chain. Science 361, 186–188, https://doi.org/10.1126/science.aar7204 (2018). Publisher: American Association for the Advancement of Science. Zhang, Y. et al. Quantifying methane emissions from the largest oil-producing basin in the United States from space. Sci. Adv. 6, eaaz5120. https://doi.org/10.1126/sciadv.aaz5120 (2020). Irakulis-Loitxate, I., Guanter, L., Maasakkers, J. D., Zavala-Araiza, D. & Aben, I. Satellites detect abatable super-emissions in one of the world’s largest methane hotspot regions. Environ. Sci. Technol. 56, 2143–2152, https://doi.org/10.1021/acs.est.1c04873 (2022). Publisher: American Chemical Society. Varon, D. J. et al. High-frequency monitoring of anomalous methane point sources with multispectral Sentinel-2 satellite observations. Atmos. Meas. Tech. 14, 2771–2785. https://doi.org/10.5194/amt-14-2771-2021 (2021). Publisher: Copernicus GmbH. Guanter, L. et al. Mapping methane point emissions with the PRISMA spaceborne imaging spectrometer. Remote Sens. Environ. 265, 112671. https://doi.org/10.1016/j.rse.2021.112671 (2021). Article Google Scholar Sánchez-García, E., Gorroño, J., Irakulis-Loitxate, I., Varon, D. J. & Guanter, L. Mapping methane plumes at very high spatial resolution with the WorldView-3 satellite. Atmos. Meas. Tech. 15, 1657–1674. https://doi.org/10.5194/amt-15-1657-2022 (2022). Publisher: Copernicus GmbH. Jacob, D. J. et al. Quantifying methane emissions from the global scale down to point sources using satellite observations of atmospheric methane. Atmos. Chem. Phys. 22, 9617–9646, https://doi.org/10.5194/acp-22-9617-2022 (2022). Publisher: Copernicus GmbH. Foote, M. D. et al. Fast and accurate retrieval of methane concentration from imaging spectrometer data using sparsity prior. IEEE Trans. Geosci. Remote Sens. 58, 6480–6492. https://doi.org/10.1109/TGRS.2020.2976888 (2020). Conference Name: IEEE Transactions on Geoscience and Remote Sensing. Cogliati, S. et al. The PRISMA imaging spectroscopy mission: overview and first performance analysis. Remote Sens. Environ. 262, 112499. https://doi.org/10.1016/j.rse.2021.112499 (2021). Article Google Scholar Guanter, L. et al. The enmap spaceborne imaging spectroscopy mission for earth observation. Remote Sensing 7, 8830–8857. https://doi.org/10.3390/rs70708830 (2015). Article ADS Google Scholar Green, R. O. et al. The earth surface mineral dust source investigation: An earth science imaging spectroscopy mission. In 2020 IEEE Aerospace Conference, 1–15. https://doi.org/10.1109/AERO47225.2020.9172731 (2020). Mateo-García, G., Laparra, V., López-Puigdollers, D. & Gómez-Chova, L. Transferring deep learning models for cloud detection between Landsat-8 and Proba-V. ISPRS J. Photogramm. Remote. Sens. 160, 1–17. https://doi.org/10.1016/j.isprsjprs.2019.11.024 (2020). Article ADS Google Scholar Cusworth, D. H. et al. Intermittency of large methane emitters in the permian basin. Environ. Sci. Technol. Lett. 8, 567–573. https://doi.org/10.1021/acs.estlett.1c00173 (2021). Publisher: American Chemical Society. Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 234–241 (Springer, 2015). Duren, R. M. et al. California’s methane super-emitters. Nature575, 180–184, https://doi.org/10.1038/s41586-019-1720-3 (2019). Number: 7781 Publisher: Nature Publishing Group. Gorroño, J., Varon, D. J., Irakulis-Loitxate, I. & Guanter, L. Understanding the potential of Sentinel-2 for monitoring methane point emissions. Atmos. Meas. Techn. Discussions, 1–25, https://doi.org/10.5194/amt-2022-261 (2022). Publisher: Copernicus GmbH. Thorpe, A. et al. Mapping methane concentrations from a controlled release experiment using the next generation airborne visible/infrared imaging spectrometer (aviris-ng). Remote Sens. Environ. 179, 104–115. https://doi.org/10.1016/j.rse.2016.03.032 (2016). Article ADS Google Scholar Sherwin, E. D., Chen, Y., Ravikumar, A. P. & Brandt, A. R. Single-blind test of airplane-based hyperspectral methane detection via controlled releases. Elementa Sci. Anthropocene 9, 00063, https://doi.org/10.1525/elementa.2021.00063 (2021). Sherwin, E. D. et al. Single-blind validation of space-based point-source detection and quantification of onshore methane emissions. Sci. Rep. 13, 3836, https://doi.org/10.1038/s41598-023-30761-2 (2023). Number: 1 Publisher: Nature Publishing Group. Thompson, D. R. & Brodrick, P. G. Realizing machine learning’s promise in geoscience remote sensing. EOS 102, https://doi.org/10.1029/2021EO160605 (2021). Baumgardner, M. F., Biehl, L. L. & Landgrebe, D. A. 220 band aviris hyperspectral image data set: June 12, 1992 indian pine test site 3, https://doi.org/10.4231/R7RX991C (2015). Paoletti, M., Haut, J., Plaza, J. & Plaza, A. Deep learning classifiers for hyperspectral imaging: A review. ISPRS J. Photogramm. Remote. Sens. 158, 279–317 (2019). Article ADS Google Scholar Signoroni, A., Savardi, M., Baronio, A. & Benini, S. Deep learning meets hyperspectral image analysis: A multidisciplinary review. J. Imaging 5, 52 (2019). Article PubMed PubMed Central Google Scholar Gewali, U. B., Monteiro, S. T. & Saber, E. Machine learning based hyperspectral image analysis: a survey. arXiv preprint arXiv:1802.08701 (2018). Schuit, B. J. et al. Automated detection and monitoring of methane super-emitters using satellite data. Atmos. Chem. Phys. Discussions 1–47 (2023). Ehret, T. et al. Global tracking and quantification of oil and gas methane emissions from recurrent sentinel-2 imagery. Environ. Sci. Technol. 56, 10517–10529. https://doi.org/10.1021/acs.est.1c08575 (2022). Publisher: American Chemical Society. Jongaramrungruang, S., Thorpe, A. K., Matheou, G. & Frankenberg, C. MethaNet - An AI-driven approach to quantifying methane point-source emission from high-resolution 2-D plume imagery. Remote Sens. Environ. 269, 112809. https://doi.org/10.1016/j.rse.2021.112809 (2022). Article Google Scholar Groshenry, A., Giron, C., Lauvaux, T., d’Aspremont, A. & Ehret, T. Detecting methane plumes using prisma: Deep learning model and data augmentation. arXiv preprint arXiv:2211.15429 (2022). Joyce, P. et al. Using a deep neural network to detect methane point sources and quantify emissions from PRISMA hyperspectral satellite images. EGUsphere 1–22. https://doi.org/10.5194/egusphere-2022-924 (2022). Publisher: Copernicus GmbH. Matheou, G. & Chung, D. Large-eddy simulation of stratified turbulence. part ii: Application of the stretched-vortex model to the atmospheric boundary layer. J. Atmos. Sci. 71, 4439–4460 (2014). Asner, G. P. et al. Carnegie airborne observatory-2: Increasing science data dimensionality via high-fidelity multi-sensor fusion. Remote Sens. Environ. 124, 454–465. https://doi.org/10.1016/j.rse.2012.06.012 (2012). Article ADS Google Scholar Cusworth, D. Methane plumes for NASA/JPL/UArizona/ASU Sep-Nov 2019 Permian campaign, https://doi.org/10.5281/zenodo.5610307 (2021). Mrziglod, J., & Francis, A. (Intelligently Reinforced Image Segmentation graphical user interface (IRIS, 2019). Francis, A., Mrziglod, J., Sidiropoulos, P. & Muller, J.-P. Sensei: A deep learning module for creating sensor independent cloud masks. IEEE Trans. Geosci. Remote Sens. 60, 1–21. https://doi.org/10.1109/TGRS.2021.3128280 (2022). Article Google Scholar Thuillier, G. et al. The Solar Spectral Irradiance from 200 to 2400 nm as Measured by the SOLSPEC Spectrometer from the Atlas and Eureca Missions. Sol. Phys. 214, 1–22. https://doi.org/10.1023/A:1024048429145 (2003). Article ADS Google Scholar Růžička, V. et al. STARCOP dataset: Semantic Segmentation of Methane Plumes with Hyperspectral Machine Learning Modelshttps://doi.org/10.5281/zenodo.7863343 (2023). Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. & Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4510–4520 (2018). Kumar, S., Arevalo, I., Iftekhar, A. & Manjunath, B. Methanemapper: Spectral absorption aware hyperspectral transformer for methane detection. arXiv preprint arXiv:2304.02767 (2023). Růžička, V. et al. RaVÆn: unsupervised change detection of extreme events using ML on-board satellites. Sci. Rep. 12, 16939. https://doi.org/10.1038/s41598-022-19437-5 (2022). Article ADS CAS PubMed PubMed Central Google Scholar Mateo-Garcia, G. et al. Towards global flood mapping onboard low cost satellites with machine learning. Sci. Rep. 11, 1–12 (2021). Article Google Scholar Green, R. EMIT L1B At-Sensor Calibrated Radiance and Geolocation Data 60 m V001. Distributed by NASA EOSDIS Land Processes Distributed Active Archive Center (2022). Accessed 2023-09-19. Download references Acknowledgements This work has been enabled by Trillium Technologies and has been funded by ESA Cognitive Cloud Computing in Space initiative project number STARCOP I-2022-00380. G.M.-G. and L.G.-C. have been partially supported by the Spanish Ministry of Science and Innovation (project PID2019-109026RB-I00, MCIEI/10.13039/501100011033) and the European Social Fund. L.G.-C. and L.G. acknowledge support from the GVA PROMETEO programme (project CIPROM/2021/056). The authors would like to thank Rochelle Schneider, Nicolas Longépé and Gabriele Meoni (ESA) for discussions and comments throughout the development of this work, to James Parr and Jodie Hughes from Trillium Technologies for their support of the project and to Cesar Luis Aybar for his help with the IRIS annotation tool. Author information Authors and Affiliations University of Oxford, Oxford, UK Vít Růžička & Andrew Markham Trillium Technologies, London, UK Vít Růžička & Gonzalo Mateo-Garcia University of Valencia, Valencia, Spain Gonzalo Mateo-Garcia & Luis Gómez-Chova University of Cambridge, Cambridge, UK Anna Vaughan Polytechnic University of Valencia, Valencia, Spain Luis Guanter Environmental Defense Fund, Amsterdam, Netherlands Luis Guanter Contributions V.R. is responsible as the first and corresponding author, he was responsible for proposing the machine learning models, conducting and evaluating the experiments and publishing the code repository, results and writing the paper. V.R. and G.M.-G., share the conceivement of the experiments and coding, they were also responsible for collection and processing of the dataset. L.G.-C. and L.G. were the main domain supervisors for the project. A.M. provided supervision in the field of machine learning, aiding in scoping several future directions for research. All authors reviewed and contributed to the manuscript. Corresponding author Correspondence to Vít Růžička. Ethics declarations Competing interests The authors declare no competing interests. Duplicate publication statement An initial and shorter version of this work was previously presented at the ESA-ECMWF workshop of 2022 as an oral presentation (publicly available here). This paper has been significantly updated since the presentation, with the evaluation on the EMIT sensor. Additional information Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and Permissions About this article Cite this article Růžička, V., Mateo-Garcia, G., Gómez-Chova, L. et al. Semantic segmentation of methane plumes with hyperspectral machine learning models. Sci Rep 13, 19999 (2023). https://doi.org/10.1038/s41598-023-44918-6 Download citation Received05 May 2023 Accepted13 October 2023 Published17 November 2023 DOIhttps://doi.org/10.1038/s41598-023-44918-6 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Subjects Atmospheric science Climate change Computer science Environmental sciences Natural hazards Scientific data Software Comments By submitting a comment you agree to abide by our Terms and Community Guidelines. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.",
    "originSummary": [
      "The article focuses on the development of machine learning models to detect and segment methane plumes using hyperspectral data.",
      "The authors released a curated dataset of methane plume annotations and proposed sensor agnostic models that outperformed baseline approaches in terms of accuracy and false positive rates.",
      "The research aims to contribute to effective methods for methane leak detection and mitigation to combat climate change and reduce greenhouse gas emissions."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  },
  {
    "title": "Deep Learning Model Classifies Stages of Macular Degeneration from OCT Scans",
    "originLink": "https://www.marktechpost.com/2023/11/16/this-ai-paper-introduces-a-deep-learning-model-for-classifying-stages-of-age-related-macular-degeneration-using-real-world-retinal-oct-scans/",
    "originBody": "This AI Paper Introduces a Deep Learning Model for Classifying Stages of Age-Related Macular Degeneration Using Real-World Retinal OCT Scans By Adnan Hassan - November 16, 2023 Reddit Vote Flip Share Tweet 0 Shares A new research paper presents a deep learning-based classifier for age-related macular degeneration (AMD) stages using retinal optical coherence tomography (OCT) scans. Utilizing a two-stage convolutional neural network, the model classifies macula-centered 3D volumes from Topcon OCT images into Normal, early/intermediate AMD (iAMD), atrophic (GA), and neovascular (nAMD) stages. The first stage employs a 2D ResNet50 for B-scan classification, and the second stage uses smaller models (ResNets) for volume classification. The model, trained on a substantial dataset, performs strongly in categorizing macula-centered 3D volumes into Normal, iAMD, GA, and nAMD stages. The study emphasizes the significance of accurate AMD staging for timely treatment initiation. Performance metrics include ROC-AUC, balanced accuracy, accuracy, F1-Score, sensitivity, specificity, and Matthews correlation coefficient. The research details creating a deep learning-based system for automated AMD detection and staging using retinal OCT scans. OCT, a non-invasive imaging technique, is crucial in providing detailed insights into AMD staging compared to traditional methods. The study emphasizes the significance of accurate AMD staging for effective treatment and vision preservation. The research highlights the importance of high-quality datasets for robust analysis. ↗ Recommended Read: LLMWare Launches RAG-Specialized 7B Parameter LLMs: Production-Grade Fine-Tuned Models for Enterprise Workflows Involving Complex Business Documents The study implemented a two-stage deep learning model utilizing ImageNet-pretrained ResNet50 and four separate ResNets for binary classification of AMD biomarkers on OCT scans. The first stage localized disease categories within the volume, while the second stage performed volume-level classification. The model, trained on a real-world OCT dataset, demonstrated promising performance metrics, including ROC-AUC, balanced accuracy, accuracy, F1-Score, sensitivity, specificity, and Matthews correlation coefficient. The study acknowledged challenges in using diverse OCT datasets from different devices and discussed potential generalization issues. The deep learning-based AMD detection and staging system demonstrated promising performance with an average ROC-AUC of 0.94 in a real-world test set. Incorporating Monte-Carlo dropout at inference time enhanced the reliability of classification uncertainty estimates. The study utilized a curated dataset of 3995 OCT volumes from 2079 eyes, evaluating performance with various metrics, including AUC, BACC, ACC, F1-Score, sensitivity, specificity, and MCC. The results highlight the model’s potential for accurate AMD classification and staging, aiding in timely treatment and visual function preservation. 🔥Join the Fastest Growing ML Research SubReddit Now The study successfully developed an automated deep learning-based AMD detection and staging system using OCT scans. The two-stage convolutional neural network accurately classified macula-centered 3D volumes into four classes: Normal, iAMD, GA, and nAMD. The deep learning model showed comparable or better performance than baseline approaches, with the additional benefit of B-scan-level disease localization. Further research can enhance the deep learning model’s generalizability to various OCT devices, considering adaptations for scanners like Cirrus and Spectralis. Domain shift adaptation methods should be explored to address limitations related to dataset-specific training, ensuring robust performance across diverse signal-to-noise ratios. The model’s potential for retrospective AMD onset detection could be extended, allowing automatic labeling of longitudinal datasets. Application of uncertainty estimates in real-world screening settings and exploring the model for detecting other disease biomarkers beyond AMD are promising avenues for future investigation, aiding disease screening in a broader population. Check out the Paper. All credit for this research goes to the researchers of this project. Also, don’t forget to join our 33k+ ML SubReddit, 41k+ Facebook Community, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more. If you like our work, you will love our newsletter.. Adnan Hassan + posts Hello, My name is Adnan Hassan. I am a consulting intern at Marktechpost and soon to be a management trainee at American Express. I am currently pursuing a dual degree at the Indian Institute of Technology, Kharagpur. I am passionate about technology and want to create new products that make a difference. Reddit Vote Flip Share Tweet 0 Shares 🔥 Join The AI Startup Newsletter To Learn About Latest AI Startups Previous articleThis AI Paper from MIT Explores the Scaling of Deep Learning Models for Chemistry Research Next articleNetEase Youdao Open-Sources EmotiVoice: A Powerful and Modern Text-to-Speech Engine",
    "originSummary": [
      "A deep learning model has been developed to classify different stages of age-related macular degeneration (AMD) using retinal optical coherence tomography (OCT) scans.",
      "The model accurately classifies 3D volumes of the macula into Normal, early/intermediate AMD, atrophic AMD, and neovascular AMD stages, highlighting the importance of accurate AMD staging for timely treatment.",
      "The model shows promising results and suggests the potential for automated AMD detection and staging using OCT scans, but challenges remain in dealing with diverse datasets and generalization issues. Future research can focus on improving generalizability, uncertainty estimation, and detecting other disease biomarkers."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700284563328
  }
]
