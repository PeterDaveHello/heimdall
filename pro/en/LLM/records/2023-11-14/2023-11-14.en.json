[
  {
    "title": "Security Journey Introduces AI/LLM and API Learning Paths to Enhance Development Team's Secure Software Skills",
    "originLink": "https://www.globenewswire.com/news-release/2023/11/14/2780190/0/en/Security-Journey-Announces-New-AI-LLM-and-API-Learning-Paths-to-Teach-Development-Teams-How-to-Build-Software-Securely.html",
    "originBody": "Accessibility: Skip TopNav Security Journey Announces New AI/LLM and API Learning Paths to Teach Development Teams How to Build Software Securely dehaze search close rightElements.signin.Title RIGHTELEMENTS.REGISTER.TITLE search Security Journey Announces New AI/LLM and API Learning Paths to Teach Development Teams How to Build Software Securely New training efficiently addresses the industry’s identified risks in API and AI/LLM software. November 14, 2023 09:05 ETSource: Security Journey FOLLOW Share Pittsbugh, PA, Nov. 14, 2023 (GLOBE NEWSWIRE) -- Security Journey, a leading secure coding training provider, today launched two new Topic-Based learning paths supporting the recently published OWASP Top 10 2023 recommendations for AI applications built on Large Language Models (LLM) and for securing Application Programming Interfaces (APIs). Just a short time after the OWASP vulnerability lists were published, Security Journey responded with training that enterprises must adopt to build and integrate these technologies securely. The OWASP Top 10 AI/LLM learning path offers an in-depth training experience designed to equip development teams with expertise not only in secure AI system design, especially those built on LLMs (Large Language Models), but also in the secure integration and utilization of these systems. The training curriculum covers essential topics, enabling development teams to hone their engineering skills to secure data, AI models, and software applications, resulting in the design of robust systems. By completing this path, learners will gain actionable insights for the secure integration and leveraging of AI/LLM systems. The AI/LLM learning path includes the following lessons: Introduction to AI/LLM Security Data Science Engineering for AI/LLM Model Engineering for AI/LLM Application and Plugin Security for AI/LLM AI/LLM Security Toolchain Joe Ferrara, CEO of Security Journey, emphasized the importance of these new learning paths, saying, “With recent CISA guidance calling for AI software to be secure by design with little to no software configuration changes or additional cost, these lessons and learning paths are paramount to meeting this need. As we equip development teams with the necessary skills to guarantee the security of API and AI systems, we are assisting them in confronting the ever-evolving threat landscape.” The OWASP API Security Top 10 learning Path, to be released in December, is a progressive Topic-Based learning path with foundational, intermediate, and advanced lessons in a variety of learning formats from podcast-style videos to hands-on coding lessons. The new learning path will equip developers of all experience levels to combat the significant risks associated with insecure APIs. The OWASP API Top 10 path includes the following lessons: OWASP API Top 10Part 1 Broken Object Level Authorization (Hands-on Coding Lesson) Broken Authentication (Hands-on Coding Lesson) Broken Object Property Level Authorization (Hands-on Coding Lesson) OWASP API Top 10Part 2 Unrestricted Resource Consumption (Hands-on Coding Lesson) Broken Function Level Authorization (Hands-on Coding Lesson) Unrestricted Access to Sensitive Business Flows (Hands-On Coding Lesson) OWASP API Top 10Part 3 Server-Side Request Forgery (SSRF) (Hands-On Coding Lesson) Security Misconfigurations (Hands-On Coding Lesson) Improper Inventory Management (Hands-On Coding Lesson) Unsafe Consumption of APIs (Hands-On Coding Lesson) Fundamentals of gRPC Security Fundamentals of GraphQL Security The need to secure APIs has become abundantly clear. Recent research from TechTarget’s Enterprise Strategy Group on Securing the API Attack Surface found that 92% of organizations have experienced at least one security incident related to insecure APIs in the last 12 months, including 57% who have experienced multiple security incidents related to insecure APIs during the past year. \"While APIs are powerful tools for developers, it is important for them to understand the security implications and risks as they develop feature-rich applications using APIs. Our research shows there is a direct correlation with developers having a higher level of API risk understanding when their organizations have formal training programs in place,” said Melinda Marks, Practice Director, Cybersecurity, at Enterprise Strategy Group. “Continuous education and collaboration with developers can mitigate risk and reduce API security incidents to protect an organization’s digital assets.” In continuation of Security Journey's ongoing commitment to addressing the latest security challenges, these new Topic-Based Learning Paths build upon its recent announcement of Role-Based and Compliance-Based Recommended Learning Paths offerings. They are designed to empower learners to focus their efforts on enhancing expertise and skills in specific, high-priority areas. By doing so, development teams can effectively mitigate and avert prominent software risks, aligning with industry demands and standards such as the OWASP Top 10 2023 recommendations for AI and API security. These learning paths represent our dedication to staying at the forefront of security education, ensuring organizations are equipped to tackle the ongoing security challenges. To learn more about these essential learning paths and enhance your organization's security posture, please visit securityjourney.com. About Security Journey Security Journey helps enterprises reduce vulnerabilities with application security education for developers and all individuals involved in creating software. Development teams are empowered through practical, skill-oriented secure coding training that easily satisfies compliance needs and goes beyond that to build a security-first development culture. Over 450 companies around the world are teaching their teams how to build safer software using Security Journey. //ENDS// Media Contact: Katie Fegan, Account Manager kfegan@saycomms.co.uk Attachment Security Journey Announces New AI/LLM and API Topic-Based Learning Paths Security Journey Announces New AI/LLM and API Topic-Based Learning Paths Paths Support OWASP API Top 10 and OWASP AI/LLM Top 10 Recommendations Tags appsec secure development training' secure coding training secure software AI/LLM API Security Related Links TechTarget’s Enterprise Strategy Group on Securing the API Attack Surface CISA guidance Contact Data Amy Baker Security Journey 7246125137 amy_baker@securityjourney.com Contact Company ProfileSecurity JourneyWebsite: https://www.securityjourney.com Press Release Actions Print Download PDF Subscribe via RSS Subscribe via ATOM Javascript We use cookies to improve your GlobeNewswire experience. By continuing, you agree to our use of cookies. For more information, please see our Privacy Policy. ACCEPT",
    "originSummary": [
      "Security Journey has introduced two new learning paths to help development teams create secure software.",
      "The first learning path concentrates on developing secure AI applications using Large Language Models (LLM), covering system design, integration, and usage.",
      "The second learning path centers on securing Application Programming Interfaces (APIs) and mitigating the considerable risks associated with insecure APIs."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "Dell and Hugging Face Partner to Simplify Deployment of Large language Models (LLMs) for Enterprises",
    "originLink": "https://venturebeat.com/ai/dell-and-hugging-face-partner-to-simplify-llm-deployment/",
    "originBody": "VentureBeat presents: AI Unleashed - An exclusive executive event for enterprise data leaders. Hear from top industry leaders on Nov 15. Reserve your free pass Almost every enterprise today is at least exploring what large language models (LLMs) and generative AI can do for their business. Still, just as with the dawn of cloud computing and big data and analytics, many concerns remain: Where do they start in deploying the complex technology? How can they ensure the security and privacy of their sensitive, proprietary data? And what about time- and resource-intensive fine-tuning? Today, Dell and Hugging Face are announcing a new partnership to help address these hurdles, simplify on-premises deployment of customized LLMs and enable enterprises to get the most out of the powerful, evolving technology. “The impact of gen AI and AI in general will be “significant, in fact, it will be transformative,” Matt Baker, SVP for Dell AI strategy, said in a press pre-briefing. VB Event AI Unleashed Don’t miss out on AI Unleashed on November 15! This virtual event will showcase exclusive insights and best practices from data leaders including Albertsons, Intuit, and more. Register for free here “This is the topic du jour, you can’t go anywhere without talking about generative AI or AI,” he added. “But it is advanced technology and it can be pretty daunting and complex.” Dell and Hugging Face ‘embracing’ to support LLM adoption With the partnership, the two companies will create a new Dell portal on the Hugging Face platform. This will include custom, dedicated containers, scripts and technical documents for deploying open-source models on Hugging Face with Dell servers and data storage systems. The service will first be offered to Dell PowerEdge servers and will be available through the APEX console. Baker explained that it will eventually extend to Precision and other Dell workstation tools. Over time, the portal will also release updated containers with optimized models for Dell infrastructure to support new-gen AI use cases and models. “The only way you can take control of your AI destiny is by building your own AI, not being a user, but being a builder,” Jeff Boudier, head of product at Hugging Face, said during the pre-briefing. “You can only do that with open-source.” The new partnership is the latest in a series of announcements from Dell as it seeks to be a leader in generative AI. The company recently added ObjectScale XF960 to its ObjectScale tools line. The S3-compatible, all-flash appliance is geared towards AI and analytics workflows. Dell also recently expanded its gen AI portfolio from initial-stage inferencing to model customization, tuning and deployment. Of the latest news, Baker noted with a laugh: “I’m trying to avoid the puns of Dell and Hugging Face ‘embracing’ on behalf of practitioners, but that’s in fact what we are doing.” Challenges in adopting generative AI There are undoubtedly many challenges in enterprise adoption of gen AI. “Customers report a plethora of issues,” said Baker. To name a few: complexity and closed ecosystems; time-to-value; vendor reliability and support; ROI and cost management. Just as in the early days of big data, there’s also an overall challenge in progressing gen AI projects from proof of concept to production, he said. And, organizations are concerned about exposing their data as they seek to leverage it to gain insights and automate processes. “Today a lot of companies are stuck because they’re being asked to deliver on this new generative AI trend,” said Boudier, “while at the same time they cannot compromise their IP.” Just look at popular code assistants such as GitHub Copilot, he said: “Isn’t it crazy that every time a developer at an organization types a keystroke on a keyboard, your company source code goes up on the internet?” This underscores the value in — and need for — internalizing gen AI and ML apps. Dell research has found that enterprises overwhelmingly (83%) prefer on-prem or hybrid implementations. “There’s a significant advantage to deploying on-prem, particularly when you’re dealing with your most precious IP assets, your most precious artifacts,” said Baker. Curated models for performance, accuracy, use case The new Dell Hugging Face portal will include curated sets of models selected for performance, accuracy, use cases and licenses, Baker explained. Organizations will be able to select their preferred model and Dell configuration, then deploy within their infrastructure. “Imagine a LLama 2 model specifically configured and fine-tuned for your platform, ready to go,” Baker said. He pointed to use cases including marketing and sales content generation, chatbots and virtual assistants and software development. “We’re going to take the guesswork out of being a builder,” said Baker. “It’s the easy button to go to Hugging Face and deploy the capabilities you want and need in a way that takes away a lot of the minutiae and complexity.” What makes this new offering different from the spate of others emerging almost daily is Dell’s ability to tune “top to bottom,” Baker contended. This allows enterprises to quickly deploy the best configuration of a given model or framework. He emphasized that enterprises won’t be exchanging any data with public models. “It’s your data and nobody else is touching your data except you,” he said, adding that, “once that model has been fine-tuned, it’s your model.” Every company a vertical Ultimately, tuning models for maximum output can be a time-consuming process, and many enterprises currently experimenting with gen AI are using retrieval augmented generation (RAG) alongside off-the-shelf LLM tools. RAG incorporates external knowledge sources to supplement internal information. The method allows users to find relevant data to create stepwise instructions for many generative tasks, Baker explained, and the pattern can be instantiated in pre-built containers. “Techniques like RAG are a way of in essence not having to build a model, but instead providing context to the model to achieve the right generative answer,” he said. Dell aims to further simplify the fine-tuning process by providing a containerized tool based on the popular parameter efficient techniques LoRA and QLoRA, he said. This is an important step when it comes to customizing models to specific business use cases. Going forward, all enterprises will have their own vertical, in fact, “they themselves are vertical — they’re using their specific data,” Baker said. There’s much talk of verticalization in AI, but that doesn’t necessarily mean domain-specific models. “Instead, it’s taking your specific data, combining that with a model to provide a generative outcome,” he said. VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings.",
    "originSummary": [
      "Dell and Hugging Face have joined forces to simplify the deployment of large language models (LLMs) and generative AI for enterprises.",
      "A new Dell portal on the Hugging Face platform will be created, providing custom containers, scripts, and technical documents for deploying open-source models on Dell servers.",
      "This partnership aims to address the challenges enterprises face in terms of complexity, security, and customization when adopting generative AI technologies. Enterprises will be able to choose curated models, fine-tune them for their specific needs, and securely deploy them within their infrastructure."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "Dell Partners with Meta to Bring On-Premise LLM Compute for Telco AI",
    "originLink": "https://www.rcrwireless.com/20231114/deep-dive/telco-ai/telco-ai-deep-dive-dell-makes-the-case-for-on-prem-llm-compute",
    "originBody": "Home Deep Dive Series Telco AI Deep Dive: Dell makes the case for bringing LLM compute... Deep Dive Series Fundamentals Telco AI Deep Dive Telco AI Deep Dive: Dell makes the case for bringing LLM compute on prem By Sean Kinney, Editor in Chief - November 14, 2023 212 Dell SVP of AI Strategy talks through LLM curation, optimization for telco AI use cases Earlier this year at Dell Tech World, company CEO, Chairman and Founder Michael Dell identified generative AI as among the five big challenges enterprises are facing. The others were the future of work, multicloud, edge computing and security. A key piece here, whether its related to telco AI use cases, or healthcare AI use cases, or manufacturing AI use cases, etc…is taking a large language model (LLM) that was built to a general purpose tool, and layering in proprietary or industry-specific data to optimize the LLM for particular tasks in particular sectors. In an effort to simplify deployment of LLMs, Dell recently announced a partnership with Meta that will see the former combine its IT infrastructure, software and services with the latter’s Llama 2 family of LLMs. In an interview with RCR Wireless News, Dell SVP of AI Strategy Matt Baker talked through the process of curating an LLM for an individual business, and discussed the benefits of running generative AI on premise as opposed to in someone else’s centralized cloud. Another key aspect of Dell’s approach to gen AI is the development, with Meta, of pre-tested validated designs. According to the company, “With fully documented deployment and configuration guidance, organizations can get their gen AI infrastructure up and running quickly and operate Llama 2 with more predictability…Our goal is to be the preferred on-premises infrastructure provider for customers deploying Llama 2 and bring the best-of-breed generative AI solutions to our customers.” Looking into telco AI opportunities, Baker noted ongoing trends around operators pushing compute deeper into the networks to radio sites, customer premises and other locations, alongside the move toward hardware/software disaggregation which is currently a hot topic as it relates to the radio access network. “A lot of AI inferencing, we believe, will occur at the edge,” Baker said. “The broader world around us has a connectivity problem. And the way to deploy some of these edge inferencing elements may in fact be to deploy them at the network edge. Edge inferencing could be the killer app for these advanced networks.” Baker also called out overlap between AI and private 5G, which he said “go together very well…A lot of AI inferencing and automation requires a degree of connectivity, and what better way to do that than with telco infrastructure. We think that it’s a big boon to this sort of movement towards more openness in the network so you can deploy more open applications in the network.” Click here for more interviews in Telco AI Deep Dive series. d TAGS Dell Technologies edge computing enterprise IT generative AI large language models meta private 5G Previous articleEight notable 5G RedCap tests (Part 1) Sean Kinney, Editor in Chief Sean focuses on multiple subject areas including 5G, Open RAN, hybrid cloud, edge computing, and Industry 4.0. He also hosts Arden Media's podcast Will 5G Change the World? Prior to his work at RCR, Sean studied journalism and literature at the University of Mississippi then spent six years based in Key West, Florida, working as a reporter for the Miami Herald Media Company. He currently lives in Fayetteville, Arkansas. RELATED ARTICLESMORE FROM AUTHOR Eight notable 5G RedCap tests (Part 1) The kid is alright (it’s the folks you should worry about) – why we should lay off 5G Is 5G even relevant to process manufacturers? Featured Videos Editorial Reports 5G-Advanced deep dive: State of standards, products, and use cases October 23, 2023 In Review… Google Cloud Next 2023 October 18, 2023 Critical-edge 5G workloads for industrial IoT — what stays and what... October 17, 2023 White Papers Cambridge Consultants White Paper: Developing private network technologies for demanding use... November 7, 2023 Quectel White Paper: How to accelerate, simplify and optimize IoT device... November 6, 2023 Rohde & Schwarz White Paper: 5G Evolution – On the Path... August 21, 2023 Qualcomm White Paper: Growing 5G+Wi-Fi RF Complexity Demands Innovative, Advanced & Tightly... July 28, 2023 Webinars Editorial Webinar: Wi-Fi deep dive: State of standards, products and use... Qualcomm Webinar: 5G Advanced: The path toward a more sustainable future Volt Active Data Webinar: AI-powered CX Value Assurance for 5G and... Editorial Webinar: Smart metering – how IoT has changed-up a gear... Editorial Webinar: 5G-enabled IoT: Will RedCap help deliver on the promise...",
    "originSummary": [
      "Dell is advocating for on-premises deployment of large language model (LLM) compute for telco AI use cases.",
      "They have partnered with Meta and their Llama 2 family of LLMs to simplify the deployment process.",
      "Dell's SVP of AI Strategy, Matt Baker, emphasizes the benefits of running generative AI on-premises and the synergy between AI and private 5G, predicting that edge inferencing could be a game-changer in advanced networks."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "Orkes Launches AI Orchestration Platform for Large Language Models and Vector Database Tasks",
    "originLink": "https://thenewstack.io/with-conductor-orkes-tackles-llm-orchestration-workflows/",
    "originBody": "ARCHITECTURE Cloud Native Ecosystem Containers Edge Computing Microservices Networking Serverless Storage ENGINEERING AI Frontend Development Software Development TypeScript WebAssembly Cloud Services Data Security OPERATIONS Platform Engineering Operations CI/CD Tech Life DevOps Kubernetes Observability Service Mesh CHANNELS Podcasts Ebooks Events Newsletter TNS RSS Feeds THE NEW STACK About / Contact Sponsors Sponsorship Contributions PODCASTS EBOOKS EVENTS NEWSLETTER ARCHITECTURE ENGINEERING OPERATIONS AI / LARGE LANGUAGE MODELS / SOFTWARE DEVELOPMENT With Conductor, Orkes Tackles LLM Orchestration Workflows Orkes AI Orchestration provides a way for enterprises to weave model inferences and vector database tasks into their business logic and keep humans in the loop. Nov 13th, 2023 10:09am by Susan Hall Image from Kitreel on Shutterstock. ANNUAL READER SURVEY We are planning our coverage for 2024 and we need your input! Take 5 minutes to chime in and let us know what you're interested in. BEGIN THE SURVEY We'd love to hear how we can make TNS better for you. Amid all the hullabaloo about generative AI and large language models (LLMs), developers still have practical questions about how, exactly, to add the benefits of LLMs to their applications. Questions like: Do I need to maintain external stacks to manage the AI side of my application outside of where the rest of the components are? Do I need to write drivers to interact with vector databases for uses such as retrieval augmented generation (RAG)? How do I bring the human users working with AI into the picture? How do I ensure that enterprise applications are using LLMs in a safe, governed and observable manner while ensuring my organization’s proprietary data is not unnecessarily exposed? Those are issues that Orkes, which offers a cloud-hosted version of the Conductor open source platform, has been tackling. Its recently released Orkes AI Orchestration provides a way for enterprises to weave model inferences and vector database tasks into their business logic. A companion feature called Human Tasks enables users to designate points in a process where humans need to direct input or provide oversight. Workflows Across Microservices Netflix built out Conductor as a way to manage workflows that span across microservices during its explosive growth. It was open sourced with the Apache 2.0 license in 2016. Its creators, including Jeu George, Viren Baraiya and Boney Sekh, along with Dilip Lukose, created Cupertino, California-based Orkes, which offers a managed version of Conductor. Viren Baraiya TRENDING STORIES Jeu George Billed as an orchestration platform, Conductor sounds somewhat like Kubernetes. But there’s a difference. While Kubernetes automates the deployment, scaling and management of containerized applications, Conductor focuses on managing the execution, routing and coordination of tasks in a workflow. ChatGPT explained it this way: “While both manage different aspects of application orchestration, they can be complementary. Kubernetes can manage the deployment and scaling of applications, including microservices, while Conductor can handle the workflow and task coordination within those microservices.” Explained Baraiya, CTO at Orkes: “Essentially, if you are building applications in the cloud, you are using some sort of eventing system or microservices-based architecture, and one problem that you have to solve is how do you coordinate your work across all different systems, maintain the state and build your application? So Conductor is designed to solve that problem.” Workflows for Incorporating LLMs Orkes AI Orchestration creates workflows for incorporating LLMs and vector embeddings into application development. “When you are writing an application, and you have to wire up the calls to AI and your other services, this is where Conductor helps,” said Baraiya. “Let’s say, before you make a call to an LLM, you [call] your database to get some data, and then augment the prompt with the data and then make a call to an LLM. So how do you sequence these things out together? Based on the output of the LLM, you want to make some determination whether it needs human intervention or not. If that’s the case, then [it’s] routed to humans, so the routing piece is another thing. “So routing, wiring things up together. As a developer, you don’t have to worry about [writing these individual tasks],” he said. And you don’t have to build a custom UI to visualize these things, because the UI comes out of the box with Conductor. “And let’s say for whatever reasons the LLM fails — you get a rate limit error or something like that — it does automatic retries for you, you don’t have to worry about it. …In reality, there are going to be errors, there are going to be issues. This is where the system takes care of all of this for you automatically.” With the new Orkes AI Orchestration product, enterprises can: Easily add language models or ML inferences from model providers including Open AI, Hugging Face, Azure Open AI, Google Vertex AI, Amazon Bedrock. Integrate with vector database providers such as Pinecone and Weaviate to not only store embeddings but to provide vector search as well. Create granular role-based access to models and vector databases so that if sensitive information is stored as embeddings, you can designate only members of the finance team or certain members of that team, for instance, to have access to that information. It also offers: Prompt templates to experiment and test prompts visually. The ability to weave in model interactions and vector database system tasks, including LLM Text Complete to send a prompt to a text completion model and LLM Search Index, which uses natural language to find similar embeddings. Comprehensive and auditable governance with data about every interaction between Orkes Conductor and a model published to a queue with change data capture. With Human Tasks, the workflow pauses until the human interaction is completed. The developer can specify who is responsible for a certain task and the time period in which it is to be completed. If it is not, it reassigns the task to the next person in the chain and notifies them, and this keeps going until the task is either completed or times out. Language Agnostic, across Clouds “Especially with large language models, LLMs, is that when you want to incorporate elements in your application flow, you have to orchestrate across those elements. And it’s not just that you use one LLM from OpenAI or Google’s Bard or Lama 2. And when you build an application, it typically becomes part of your application stack, where you have microservices calls to your database, and then you have calls to your LLM. So they are just another API call in the end. But it requires some more effort and consideration,” Baraiya said. It’s similar to Langchain, a popular framework for orchestrating language models originally using Python, though it has added TypeScript and JavaScript as well. But unlike Langchain, Conductor is a fully distributed system that can scale to millions or even billions of calls a day, he said. And it’s not limited to Python or JavaScript — you could be using JavaScript, Python, Java, C#, Golang, C++, or any of the languages to work with language models. “So it decouples you from having to work with a very specific language and meet you where your application stack is designed,” he said. The company also is heavily invested in visibility. “When you are incorporating LLMs, you have to understand what inputs are you giving, what outputs are producing, what’s your entire application flow. So one advantage of using Conductor is that it is a fully full visual system. It can take your code and visualize the entire code graph into a diagram. So exactly what path was taken, why an LLM took a certain action, and so forth,” he said. It’s also agnostic to your cloud deployment. And using techniques like caching can help keep costs down. “Let’s say if you want to leverage Google’s Bard or Vertex AI, for example. It only runs in GCP (Google Cloud Platform). But the rest of the application runs in AWS. How do you connect them together? [Conductor] is a completely cross-cloud system. So part of your system could be running in AWS, when it comes to making calls to Vertex AI in GCP. That part alone runs in GCP. And then everything gets connected very seamlessly,” he said. Orkes also has built a security and governance model to allay enterprise concerns. “Who in the company will have access to and who can see that, who can delete that, who can create that, and in both your development test and production environments, right?” said George, the Orkes CEO. “That creates a comfort for enterprises saying that, ‘OK, this now restricts usage, and you can get the best out of it there.’” “[And] you have maybe three or five or 10 different models that you expose. It gives engineers the ability to see which model is actually enough for their usage. So you might be a very cost-efficient model that might work for use case one, but may not work for use case two. And there you can use an expensive model. So you can actually manage costs that way as well. And with the visibility piece, on how many executions are run and stuff like that can help you manage costs also.” Susan Hall is the Sponsor Editor for The New Stack. Her job is to help sponsors attain the widest readership possible for their contributed content. She has written for The New Stack since its early days, as well as sites... Read more from Susan Hall SHARE THIS STORY TRENDING STORIES THE NEW STACK UPDATE A newsletter digest of the week’s most important stories & analyses. SUBSCRIBE The New stack does not sell your information or share it with unaffiliated third parties. By continuing, you agree to our Terms of Use and Privacy Policy. ARCHITECTURE Cloud Native Ecosystem Containers Edge Computing Microservices Networking Serverless Storage ENGINEERING AI Frontend Development Software Development TypeScript WebAssembly Cloud Services Data Security OPERATIONS Platform Engineering Operations CI/CD Tech Life DevOps Kubernetes Observability Service Mesh CHANNELS Podcasts Ebooks Events Newsletter TNS RSS Feeds THE NEW STACK About / Contact Sponsors Sponsorship Contributions FOLLOW TNS roadmap.sh Community created roadmaps, articles, resources and journeys for developers to help you choose your path and grow in your career. Frontend Developer Roadmap Backend Developer Roadmap Devops Roadmap © The New Stack 2023 Disclosures Terms of Use Privacy Policy Cookie Policy FOLLOW TNS TNS DAILY SUBSCRIBE",
    "originSummary": [
      "Orkes AI Orchestration is a platform offered by Orkes, a cloud-hosted version of Conductor open source.",
      "It allows businesses to integrate large language models (LLMs) and vector database tasks into their processes.",
      "The platform includes a feature called Human Tasks, which enables human involvement in decision-making.",
      "Orkes AI Orchestration is language-agnostic and can connect with different model providers and vector database providers.",
      "The platform provides robust governance and visibility features, making it suitable for enterprise use."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "CLINGEN: Advancing Clinical NLP Tasks through Synthetic Clinical Text Generation and Knowledge Extraction",
    "originLink": "https://www.marktechpost.com/2023/11/14/can-synthetic-clinical-text-generation-revolutionize-clinical-nlp-tasks-meet-clingen-an-ai-model-that-involves-clinical-knowledge-extraction-and-context-informed-llm-prompting/",
    "originBody": "Can Synthetic Clinical Text Generation Revolutionize Clinical NLP Tasks? Meet ClinGen: An AI Model that Involves Clinical Knowledge Extraction and Context-Informed LLM Prompting By Aneesh Tickoo - November 14, 2023 Reddit Vote Flip Share Tweet 0 Shares Medical data extraction, analysis, and interpretation from unstructured clinical literature are included in the emerging discipline of clinical natural language processing (NLP). Even with its importance, particular difficulties arise while developing methodologies for clinical NLP. For instance, clinical texts might confuse ordinary NLP models since they are frequently filled with acronyms and specialized medical terminology. Fortunately, recent developments in large language models provide a promising solution to these problems since they are pre-trained on large corpora and include billions of parameters, naturally capturing substantial clinical information. These developments highlight the necessity for developing specific methods for modifying LLMs for use in clinical settings that both deal with the complexity of terminology and enhance models via fine-tuning clinical data. Even though generic LLMs have a lot of potential, using them directly to make inferences about clinical text data is only sometimes desirable in real-world settings. First, these LLMs frequently have billions of parameters, requiring substantial processing power even during conception. This results in high infrastructure costs and lengthy inference times. The clinical text’s sensitive patient information also raises concerns about privacy and regulatory compliance. Creating synthetic training data with LLMs is a potential technique to address these issues as it uses LLMs’ capabilities in a resource- and privacy-conscious way. Models can operate at high-performance levels while adhering to data privacy laws when trained on these artificial datasets, replicating clinical data from the real world. In general machine learning, one of the most common study areas is synthetic data creation using foundation models. However, using LLMs trained on available texts to create clinical data has special hurdles when providing high-quality data that follows the original dataset’s distribution. To evaluate the quality of the data produced by the existing techniques, they conduct a thorough analysis focused on variety and distribution. The Central Moment Discrepancy (CMD) score and the t-SNE embedding visualization reveal a notable shift in the data distribution. 🔥Join the Fastest Growing ML Research SubReddit Now They also look at the amounts and frequencies of clinically related entities in the synthetic data; a significant decrease is seen when comparing the synthetic data to the ground truth data. Although several studies have explored creating clinical data using language models, many of these initiatives are task-specific. Electronic health records, clinical notes, medical text mining, and medical conversations are a few examples. These studies can use excessive training data and frequently use language models directly for text production. There are only so many cohesive ideas for improving how LLMs are modified to produce synthetic text that will help with clinical downstream applications. Inspired by the above research, researchers from Emory University and Georgia Institute of Technology put forth CLINGEN, a generic framework imbued with clinical expertise for producing high-quality clinical texts in few-shot situations. Their ultimate objectives are to promote subject variety in the produced text and close the gap between synthetic and ground-truth data. They provide a method to use clinical knowledge extraction to contextualize the prompts to achieve this goal. This involves getting ideas for clinical themes from KGs and LLMs and advice for writing styles from LLMs. In this way, CLINGEN combines the internal parametric information embodied in big language models with non-parametric insights from external clinical knowledge graphs. ↗ Free Webinar:'How to Build GenAI Text-to-Speech Apps with LangChain' [ Nov 16 10 am PDT] It is important to note that CLINGEN may be easily used for various fundamental clinical NLP tasks and requires very little extra human work. The following is a summary of their contributions: • For creating clinical text data in few-shot circumstances, they suggest CLINGEN, a generic framework filled with clinical information. • They offer a straightforward yet efficient method to use clinical knowledge extraction to tailor the prompts toward the intended clinical NLP tasks, which may be easily applied to various activities in clinical NLP. This involves getting ideas for clinical themes from KGs and LLMs and advice for writing styles from LLMs. • They carry out a thorough analysis of the creation of synthetic clinical data using 16 datasets and 7 clinical NLP tasks. Experimental results show that CLINGEN increases the variety of the produced training samples while aligning more closely with the original data distribution. The empirical performance increases (8.98% for PubMedBERTBase and 7.27% for PubMedBERTLarge) are consistent across multiple tasks with different LLMs and classifiers. Check out the Paper and Github. All credit for this research goes to the researchers of this project. Also, don’t forget to join our 32k+ ML SubReddit, 41k+ Facebook Community, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more. If you like our work, you will love our newsletter.. We are also on Telegram and WhatsApp. Aneesh Tickoo + posts Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects. Reddit Vote Flip Share Tweet 0 Shares 🔥 Join The AI Startup Newsletter To Learn About Latest AI Startups Previous articleCan Transformer Blocks Be Simplified Without Compromising Efficiency? This AI Paper from ETH Zurich Explores the Balance Between Design Complexity and Performance Next articleA New Research Paper Introduces a Machine-Learning Tool that can Easily Spot when Chemistry Papers are Written Using the Chatbot ChatGPT",
    "originSummary": [
      "Researchers have created a framework called CLINGEN that uses clinical knowledge extraction to produce high-quality clinical texts in few-shot scenarios.",
      "CLINGEN combines parametric information from large language models with non-parametric insights from clinical knowledge graphs.",
      "Experimental results demonstrate that CLINGEN increases the diversity of training samples and improves performance in various clinical natural language processing tasks."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "Can GROK Token Mirror Musk's Dogecoin Success as xAI's Grok LLM Gains Popularity?",
    "originLink": "https://wccftech.com/shiba-inu-redux-we-now-have-a-grok-token-to-cash-in-on-the-popularity-of-xai-grok-llm/",
    "originBody": "Finance Dogecoin Redux: We Now Have the GROK Token to Cash In on the Growing Popularity of xAI’s Grok LLM Rohail Saleem • Nov 13, 2023 10:42 AM EST • Copy Shortlink Image Source: TS2 This is not investment advice. The author has no position in any of the stocks mentioned. Wccftech.com has a disclosure and ethics policy. Apart from having a reputation for possessing Midas' legendary touch of gold for having transformed almost all of his previous initiatives into profit-making enterprises, Elon Musk also excels in providing inspiration for a host of meme coins, most of which dissipate within a few weeks after capitalizing on the initial frenzy. Dogecoin (DOGE) and STOPELON tokens serve as an apt emblem of this phenomenon. Now, just a few days after the launch of xAI's Grok LLM, we have another meme token, quite imaginatively (sarc) named the GROK coin. Ethereum allows virtually everyone to issue an ERC-20 token that takes advantage of the blockchain's ability to read and enforce smart contracts. While this feature has played an important role in popularizing Decentralized Finance (DeFi), it has also enabled a plethora of rug pulls where new tokens are launched to capitalize on an emerging theme only to see the founders withdraw the underlying pool of capital, leaving bagholders with worthless coins. Related Story “A Bitter Pill to Swallow” – Elon Musk Teases as Lucid Group Adopts Tesla’s Charging Standard Do note that xAI and Elon Musk have no connection with the recently launched GROK token. Although Musk did bestow a sheen of legitimacy on Dogecoin after repeatedly endorsing it, going so far as to allow merchandise purchase on Tesla's official store via the meme coin, the GROK token enjoys no comforts. Source: https://www.dextools.io/app/en/ether/pair-explorer/0x69c66beafb06674db41b22cfc50c34a93b8d82a2 At the time of writing, the GROK token is up a whopping 22,000 percent since inception, though considerably weaker from its all-time highs. But with a tiny market cap of just $131 million and around 11,000 holders, we are talking about cents on the proverbial gambling table right now. Last week, Elon Musk's xAI launched its first Large Language Model (LLM) called Grok to take on the might of OpenAI's ChatGPT. During its current teething phase, Grok can interpret a prompt of around 25,000 characters in one go. For comparison, the GPT-4 Turbo can remember and process around 96,000 words before giving an answer. As its star attraction, however, Grok retains access to real-time information via the X social media platform. INSIGHT: Grok is able to summarize news trends on X! https://t.co/igdGVR1hf6 pic.twitter.com/RUICJ29sqc — X News Daily (@xDaily) November 8, 2023 The new LLM can accurately summarize news trends on X, which is a huge plus point for those frustrated by OpenAI's tiered internet access for its GPTs. Jokes aside, here's an example of an actual difference between ChatGPT and Grok. I think there's room for an LLM that doesn't treat users like children. Don't get me wrong. ChatGPT is a great tool and I use it a lot, but - by design - it does have its limitations. https://t.co/l1Y0MunI9e pic.twitter.com/IZAg7kPTTQ — Eric Farraro (@EFarraro) November 10, 2023 Some people also find Grok's unfiltered responses and a distinct lack of \"woke\" bias a plus point. Elon Musk relished his ability to move Dogecoin's price with a single tweet/post. Do you think a similar dispensation for the GROK token could ever materialize? Let us know your thoughts in the comments section below. Share this story Facebook Twitter Deal of the Day Further Reading Meet Grok – Elon Musk’s Answer to ChatGPT As X and Meta’s Threads Continue to Struggle, Elon Musk Takes a “Faceboob” Jibe at Mark Zuckerberg Twitter Is Working On Even More Subscription Tiers With Added Benefits Such As The Ability To Remove Ads Twitter Or X Could Soon Become Paid For Everyone, Musk Says This Is The Only Viable Way To Deal With Bots Comments Trending Stories Despite Subscribing To YouTube Premium, You Can’t Escape The Ads 106 Active Readers AMD Readies More Ryzen 5000 3D V-Cache CPUs For AM4: Ryzen 7 5700X3D & Ryzen 5 5500X3D 92 Active Readers The Elder Scrolls V: Skyrim Looks Glorious Remastered With Ray-Traced Global Illumination and Over 1000 Mods 62 Active Readers Netflix Only Had To Pay Google 10 Percent Of In-App Purchases On Android, But The Video Streaming Service Declined The Offer 26 Active Readers Jim Cramer Has Been Utterly Crushed on His Bitcoin Call 19 Active Readers Popular Discussions Hundreds of NVIDIA GeForce RTX 4090 GPUs Still Prone To “12VHPWR” Connector Issues, Says Repair Shop Owner 1848 Comments NVIDIA GeForce RTX 40 SUPER GPU Specs & Performance Leak: 4080 SUPER With Bigger AD103, 4070 Ti SUPER With AD103 & 16 GB, 4070 SUPER With AD104 & Gen5 Connector 1430 Comments AMD Instinct MI300X & MI300A AI Accelerators Detailed: CDNA 3 & Zen 4 Come Together In An Advanced Packaging Marvel 1353 Comments Starfield Beta Update Restores the Balance of Power Between GeForce and Radeon GPUs 1062 Comments Steam Deck OLED’s 6nm Sephiroth SOC Smiles For The Camera, Efficient AMD Zen 2 & RDNA 2 For Handheld Gaming 665 Comments",
    "originSummary": [
      "The GROK token has been recently launched and has seen a significant increase of 22,000% since its inception.",
      "The token currently has a small market cap of $131 million.",
      "The token is associated with Elon Musk's xAI's Grok LLM, which is a language model that provides news trend summaries on the X social media platform, offering an alternative to OpenAI's ChatGPT.",
      "Some users appreciate the unfiltered responses and lack of bias from Grok LLM.",
      "The article speculates on whether Elon Musk's influence could potentially have a similar impact on the GROK token as it did on Dogecoin."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "Iterate.ai Launches Interplay-AppCoder LLM, the Superior Generative AI Coding Model, Boosting Development Efficiency and Cost Savings",
    "originLink": "https://martechseries.com/predictive-ai/ai-platforms-machine-learning/iterate-ai-launches-new-interplay-appcoder-the-most-powerful-generative-ai-coding-model-available-to-boost-enterprise-productivity/",
    "originBody": "Iterate.ai Launches New Interplay-AppCoder, the Most Powerful Generative AI Coding Model Available, to Boost Enterprise Productivity AI/MLIn App Marketing By prweb On Nov 13, 2023 Share Interplay-AppCoder LLM outperforms leading coding LLM WizardCoder, earning a 440% higher functionality & 52% higher usefulness score, making it the top choice for developers & businesses Iterate.ai, whose AI innovation ecosystem enables enterprises to build production-ready applications, announced the availability of Interplay-AppCoder LLM, the company’s large language model for automated code generation. Code-generation LLMs have become an essential part of the software developers’ toolbox and a growing necessity for businesses looking to incorporate AI applications, improve their operations and stay ahead of their competitors. However, Iterate recognized that existing LLMs including ChatGPT do not support the most modern code-sets like LangChain, nor do they include the latest thinking around coding methodologies. The arrival of Interplay-AppCoder LLM changes this situation. “Our venture into training a code generation LLM on state-of-the-art generative AI frameworks and libraries like YOLO V8, LangChain, and VertexAI, has culminated in Interplay-AppCoder LLM,” states Brian Sathianathan, Iterate.ai’s CTO and Co-Founder. “The creation of Interplay-AppCoder LLM was made possible by the meticulous fine-tuning of CodeLlama-7B, 34B and Wizard Coder-15B, 34B on a bespoke dataset. Essentially, we are creating our own, updated version of ChatGPT specifically for developers to generate code at an enterprise level at levels that exceed what is currently available.” Other leading coding models include Meta’s Code Llama, the AI model built on top of Meta’s Llama 2 LLM and fine-tuned for code generation and discussion, and WizardCoder, an open-source coding assistant. As of mid-October 2023, WizardCoder stood as the dominant coding LLM, soundly beating Code Llama in testing. Now, the newly launched Interplay-AppCoder LLM vastly out-codes WizardCoder. Marketing Technology News: Autoflow Launches its New Email Builder to Boost Business and Prompt Customer Engagement Interplay-AppCoder LLM was created through meticulous fine-tuning of CodeLlama-7B, 34B and Wizard Coder-15B, 34B on a bespoke dataset of newer generative AI libraries such as LangChain, YOLO V8, and Vertex AI. The first release of the Interplay-AppCoder model’s performance scored high on the ICE Benchmark, a methodology that focuses on usefulness and functionality: Usefulness – 2.968 of 4.0 (52% higher than WizardCoder which scored 1.825) Functionality – 2.476 of 4.0 (440% higher than WizardCoder which scored 0.603) The Value of Interplay-AppCoder LLM Businesses and software teams can use Iterate’s Interplay-AppCoder to write software faster and to check code effectiveness as if another software developer is sitting over the coder’s shoulder. AppCoder LLM can also be used on private servers, meaning private work has no chance of leaking into the Internet. This means that it is a critical advantage across all of an enterprise’s departments, not just IT development teams. Jon Nordmark, CEO of Iterate.ai says, “This is not just a CIO issue for building Generative AI applications. Having a robust code generation LLM is important for digital teams and C-Suites to bring engaging generative AI digital applications to market faster.” Deploying Iterate’s new LLM for code generation underscores a significant reduction in development time and resources, thus translating into substantial cost savings and enhanced productivity. “Businesses can harness Interplay-AppCoder to stay ahead of the competition. Companies need the best tools available to them to work with generative AI, and the current leader is Iterate’s Interplay-AppCoder, which reduces time-to-market by automating code generation and extending support for the latest models and libraries. By automating the coding process, businesses can spend their time and resources focusing on strategic initiatives, fostering innovation and growth,” Sathianathan states. Iterate.ai is constantly beating benchmarks and will always continue to innovate, which is necessary for success in this AI space. Currently, Iterate.ai is building several private LLMs for large enterprises in the United States and Asia. The generative AI market worldwide is projected to reach $191.8 billion by 2032, growing at a CAGR of 34.1% from 2023 to 2032. Generative AI is expected to represent about 15% of the total AI market. Marketing Technology News: MarTech Interview with Kyle Mitnick, President at Mosaic Digital Systems AI innovation ecosystementerprise productivityGenerative AI Coding ModelInterplay-AppCoderIterate.aiNewsproduction-ready applications Share",
    "originSummary": [
      "Iterate.ai has launched Interplay-AppCoder LLM, a highly advanced generative AI coding model that surpasses other leading models like WizardCoder.",
      "Interplay-AppCoder LLM supports modern code-sets and coding methodologies, making it the preferred choice for developers and businesses.",
      "This coding model is created through fine-tuning on state-of-the-art generative AI frameworks and libraries, resulting in enhanced usefulness and functionality.",
      "Its deployment leads to reduced development time, cost savings, and improved productivity for businesses.",
      "Iterate.ai is actively developing private LLMs for large enterprises, aiming to tap into the growing generative AI market."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "AILET Exam 2024: New Exam Pattern for BA LLB, LLM, and PhD Courses Revealed",
    "originLink": "https://www.indiatimes.com/news/education/ailet-exam-2024-check-detailed-new-exam-pattern-for-ba-llb-llm-and-phd-courses-620568.html",
    "originBody": "News Education AILET Exam 2024: Check Detailed New Exam Pattern For BA LLB, LLM And PhD Courses AILET Exam 2024: Check Detailed New Exam Pattern For BA LLB, LLM And PhD Courses Srishti B Dutta Updated on Nov 14, 2023, 14:53 IST - 2 min read The AILET Exam 2024 is set to be held on December 10, from 11 am to 1 pm. The All India Law Entrance Test (AILET) is a national-level law exam conducted by National Law University Delhi (NLUD) every year to initiate the admission process for candidates who are interested in joining NLUD to study law in the various programmes it offers, such as B.A.LL.B.(Hons.), LL.M. and Ph.D. Programmes. National Law University (NLU), Delhi Read below to find out the detailed syllabus and exam pattern. Find the AILET 2024 Eligibility Criteria and Exam Details here. Jump To AILET 2024 Exam Pattern for BA LLB AILET 2024 Exam Pattern for LLM Course AILET 2024 Exam Pattern for PhD Course AILET 2024 Exam Pattern for BA LLB The BA LLB paper for AILET Exam contains a total of 150 multiple-choice questions, each question carrying 1 mark. There are 3 sections in total, and the exam duration is 90 minutes. AILET 2024 Exam Pattern for BA LLB Subject Number of questionsMaximum marks Topics English Language50 questions 50 marksVocabulary Grammar Reading Comprehension Inference-based questions Synonyms Antonyms Word usage Foreign words/ phrase Para Jumble Current Affairs & General Knowledge30 questions 30 marksInternational events UN bodies Major developments in the areas of sports Geopolitics Important environmental agreements Current affairs Eminent personalities Sports Awards and honours Logical Reasoning70 questions 70 marksSeries Blood Relations Directions Critical Reasoning Syllogism Analogy Total 150 questions 150 marks - The Mathematics and Legal Aptitude sections have been dropped from AILET 2022 onwards. AILET 2024 Exam Pattern for LLM Course The exam for LLM 2024 will be MCQ-based only, according to the new exam pattern. The AILET Exam 2024 LLM Paper will consist of 100 MCQs, with each question carrying 1 mark so that the maximum marks would be 100. The MCQs will be from different branches of law, as given below: Constitutional Law Jurisprudence Administrative Law Law of Contract Law of Torts Family Law Criminal Law Property Law Company Law Public International Law Tax Law Environmental Law Labour & Industrial Law AILET 2024 Exam Pattern for PhD Course AILET 2024 Exam Pattern for PhD exam details Particulars AILET 2024 PhD Exam modeOffline, i.e. Center-based Pen-and-paper exam Test duration120 minutes Type of questionsMultiple-choice questions: 50 questions on subject of Research Methodology 50 questions from different branches of Law, Social Sciences Total questions 100 questions Total marks 100 marks Marking scheme+1 mark for each correct answer -0.25 marks for each incorrect answer For more on news and current affairs from around the world, please visit Indiatimes News.",
    "originSummary": [
      "The AILET Exam 2024, organized by National Law University Delhi, is scheduled to take place on December 10.",
      "The exam is for admission into BA LLB, LLM, and PhD programs.",
      "The BA LLB paper has 150 multiple-choice questions and lasts for 90 minutes, while the LLM and PhD exams have 100 MCQs each on different branches of law and research methodology."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  },
  {
    "title": "Transgender Advocate from West Bengal Completes LLM with Impressive Score, Aims to Become a Judge",
    "originLink": "https://yespunjab.com/completing-llm-with-87-score-bengals-first-trans-woman-advocate-now-wants-to-be-a-judge/",
    "originBody": "Completing LLM with 87% score Bengal’s first trans-woman advocate now wants to be a judge Share - Advertisement - Kolkata, Nov 13, 2023 After completing LLM., Master’s degree in law, with 87 per cent score this year, West Bengal’s first trans-woman advocate Megh Sayantan Basu wishes to become a judge in the long run. “Initially, I would like to give up my career as an advocate with a lower court in Kolkata and appear for an entrance test for getting enrolled as a teacher of law in any reputed law college in the state. After pursuing my teaching career for some time I finally wish to become a judge,” Basu said in an interview with IANS. She narrated how the date of her final examination for LLM coincided with the sudden demise of her mother, who had always been her pillar of strength and had supported Basu in her journey of transformation from a male to female and then in her path of getting established as a lawyer. “I appeared for the examination with my mother’s body at home. After completing the examination I came back home and then completed her rituals as her daughter. My mother always insisted that I perform my duties towards my parents not as their biological son but as their daughter,” Basu said, recalling the tragic day she lost her mother. After finishing her schooling from Jadavpur Vidyapith School in South Kolkata, Basu completed her LLB from South Kolkata Law College. Successfully completing her LLB degree she got enrolled as an advocate with a lower court in Kolkata and specialised in matrimonial suits. Then she appeared for her LLM degree examination from Adamas University and finally came out with flying colours. Asked why she is keen to take up a teaching job now and leave her career as a practicing advocate, Basu said that she struggled throughout life during her journey of transformation and she felt that the bias and humiliation heaped upon transgender persons were because of lack of awareness on related laws. “So as a teacher I would like to focus on these aspects, especially the existing laws that call for protection of the rights of trans-women or trans-men. At the same time, teaching will also help me in consolidating my knowledge on the subject, since learning is a never-ending and life-long process. This consolidation of knowledge will ultimately help me in achieving my final goal to be a judge,” Basu said.(Agency) - Advertisement - Share Share Share Share YES PUNJAB Congress will win 145-150 seats in Madhya Pradesh: Rahul Gandhi Israeli–Canadian peace activist Vivian Silver confirmed dead in Oct 7 Hamas attack Amid rumoured feud with Salman Khan, Arjun Kapoor watches ‘Tiger 3’ Mumbai Police’s Mahadev App scam probe: Dabur Group denies involvement Trump’s micro-blogging platform Truth Social loses $73 mn since launch Cops arrest wanted sharpshooter of Kapil Sangwan a.k.a Nandu gang in Delhi Due to constant efforts of Mann Govt, Punjab’s AQI improved 22.8% on Diwali: AAP L-G approves creation of 10 more Family Courts in Delhi, to now have 31 in total GNDU VC Prof. Jaspal Singh bereaved, Renowned Gynaecologist Dr. Surinder Kaur Sandhu passes away India out to end World Cup knockout jinx against New Zealand We have failed children of Palestine: Priyanka Gandhi on Children’s Day ‘Al-Shifa Hospital in Gaza forced to bury dead bodies inside premises’ Rahul accuses Modi, Shah and Shivraj of toppling Cong govt in MP 4 in 10 Indian smartphone users regularly try photo-editing apps: Report One in 20 international students exposed to sex-for-rent proposals in Ireland: Report The man who defined luxury and set the gold standards for Indian hotels – Obituary After Revanth Reddy’s ‘khaki knicker’ jibe, Owaisi calls him ‘RSS puppet’ Overcrowded migrant boat overturns near Yemen, dozens missing Anurag Dobhal says he will be ‘voluntary exiting’ from ‘Bigg Boss 17’ 1 in 3 children worldwide exposed to life-threatening water scarcity: Unicef Armed robbers raid Indian restaurant in New Zealand, flee with cash ‘Congress sensing defeat in MP’, says PM Modi Delhi HC to consider policy framing for dealing with property of orphan kids Uttam Singh to lead India at FIH Hockey Men’s Junior World Cup 2023 Police SI dies, home guard injured as tractor mows them down in Bihar’s Jamui Badshah hilariously breaks silence on dating rumours with Mrunal Thakur India ranks second in global estimates of diabetes Secret Service, protecting Biden’s granddaughter, open fire to prevent car break-in WhatsApp rolling out new voice chat feature with large groups ‘Mockery of law’: AAP on CBI seeking L-G’s sanction to file case against Satyendar Jain 2nd video clip of Narendra Singh Tomar’s son discussing ‘crores of rupees’ surfaces Traffic advisory issued for India International Trade Fair in Delhi Doctors race against time to save newborns in Gaza 31 killed in Israeli bombardment of Gaza refugee camp 4 killed in confrontations between Hezbollah, Israel Ex-Hamas intel chief among senior operatives killed in Gaza: IDF Last 50 years, I have been fighting with whoever has been in power: Filmmaker Anand Patwardhan Ram Temple head priest condemns Swami Prasad Maurya’s remark IDF captures Hamas parliament building in Gaza King Charles III celebrates 75th birthday today TRANSFERS, POSTINGS, PROMOTIONS Haryana Transfers – 43 HCS officers transferred Punjab Police Promotions: 12 IPS officers promoted Load more STAY CONNECTED 217,673 Fans LIKE 113,236 Followers FOLLOW ENTERTAINMENT Amid rumoured feud with Salman Khan, Arjun Kapoor watches ‘Tiger 3’ Chandni Sharma was last person to be casted for ‘Jhanak’: ‘Instantly gave it a nod’ ‘Pippa’ makers apologies over rendition of ‘Karar Oi Louho Kopat’ ‘Barse Re’ from ‘Manush’ makes you want to dance to rain droplets Sonu Nigam gives incredibly somber, passionate performance in ‘Papa Meri Jaan’ from ‘Animal’ Anurag Dobhal says he will be ‘voluntary exiting’ from ‘Bigg Boss 17’ Badshah hilariously breaks silence on dating rumours with Mrunal Thakur Last 50 years, I have been fighting with whoever has been in power: Filmmaker Anand Patwardhan Load more Punjab News Prof Gurbhajan Gill gifts 22 of his poetry books to his Alma Mater Diwali Bonaza: CM Mann gives nod to start process for recruitment of 1450 cops in Punjab Kuldeep Dhaliwal urges people to celebrate Diwali in environment-friendly way Punjab Speaker Sandhwan greets people on Diwali, Bandi Chhor Diws and Vishwakarma Day Dr. Baljit Kaur greets people on Diwali and Bandi Chhor Diwas, urges people to celebrate Green Diwali Load more NRI - OCI 4 in 10 Indian smartphone users regularly try photo-editing apps: Report Armed robbers raid Indian restaurant in New Zealand, flee with cash Jaishankar visits BAPS temple, interacts with Indian community in UK Load more SPORTS India out to end World Cup knockout jinx against New Zealand Sports Uttam Singh to lead India at FIH Hockey Men’s Junior World Cup 2023 Sports SWOT Analysis of India and New Zealand ahead of their semifinal clash Sports Load more Health & Fitness India ranks second in global estimates of diabetes India ranks second in global estimates of diabetes Most Americans not concerned about respiratory illnesses despite severe risks Current vitamin D doses may not help patients achieve optimal levels Load more Gadgets & Tech 4 in 10 Indian smartphone users regularly try photo-editing apps: Report WhatsApp rolling out new voice chat feature with large groups Deepfakes reveal dark side of AI, call for stringent laws Tecno, Infinix, Apple fastest-growing smartphone brands in Southeast Asia Load more A solo traveller’s guide to festive travel Junk food, lack of exercise spiking diabetes in young Indians: Docs 5 engaging games to enjoy during festive season Deepfake Dilemma Destinations to visit this Diwali",
    "originSummary": [
      "Megh Sayantan Basu, the first trans-woman advocate in West Bengal, has achieved an impressive 87% score in her LLM degree.",
      "Basu intends to transition from her role as an advocate to a teaching position at a law college to raise awareness and understanding of transgender rights.",
      "Her ultimate goal is to leverage her teaching experience to become a judge in order to make a significant impact on the legal system."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699971090872
  }
]
