[
  {
    "title": "Cardano Founder Invites Ousted OpenAI CEO to Decentralized LLM Project",
    "originLink": "https://thecryptobasic.com/2023/11/18/cardano-founder-invites-ousted-openai-ceo-sam-altman-to-collaborate-on-decentralized-llm-on-cardano/",
    "originBody": "HomeCrypto NewsMarketCardano Founder Invites Ousted OpenAI CEO Sam Altman to Collaborate on Decentralized LLM on Cardano Cardano Founder Invites Ousted OpenAI CEO Sam Altman to Collaborate on Decentralized LLM on Cardano DATE: NOVEMBER 18, 2023 WRITTEN BY: ABDULKARIM ABDULWAHAB Analysis Altcoins Rally: Weekly Top 5 Cryptos To Watch – XRP, DOGE, SOL, YFI, AVAX Mark Brennan - November 12, 2023 Shiba Inu Records Uptrend Above $0.000008: Here are Next Key Levels to Watch Sam Wisdom Raphael - November 10, 2023 Analyst van de Poppe Identifies Entry Price For XRP for Next Rally Sam Wisdom Raphael - November 8, 2023 Load more cardano founder The fired OpenAI CEO received a proposal from Charles Hoskinson to collaborate on a decentralized LLM project on Cardano. - Advertisement - The founder of the Cardano blockchain, Charles Hoskinson, has extended an invitation to Sam Altman, the recently ousted CEO of OpenAI – the team behind ChatGPT. Hoskinson proposed a partnership on a decentralized Large Language Model (LLM) project. He reached out to Altman for the collaboration through a lighthearted post on the X platform. “Sam Altman, since you have some free time now, if you are interested in doing a decentralized LLM, then hit me up,” Hoskinson remarked. Sam @sama since you have some free time now. If you are interested in doing a decentralized LLM, then hit me up. Would be a fun Cardano Partnerchain — Charles Hoskinson (@IOHK_Charles) November 18, 2023 - Advertisement - Notably, the invitation from the Cardano founder comes barely 24 hours after the board of directors at OpenAI fired Altman. The directors sent Altman packing for allegations of inconsistencies and intransparent communications. They claim to have lost confidence in his ability to lead OpenAI. As a result, the board has decided to part ways with Altman. On the other hand, Hoskinson has seen Altman’s expertise in artificial intelligence as a valuable asset in advancing the development of decentralized LLMs. - Advertisement - Hoskinson and Altman Decentralized LLM Collab In the invitation note to Altman, Hoskinson expressed a conviction that Cardano is an ideal platform for developing decentralized LLMs. Cardano is a blockchain-based platform designed to be secure, scalable, and sustainable. Nevertheless, the nature of Hoskinson’s offer remains ambiguous, given the teasing tone of the tweet. It is unclear whether he was making a serious proposition or simply engaging in humor. Decentralized LLMs The unveiling of ChatGPT has marked the significant strides made in the capabilities of large language models in comprehending and processing human thoughts. However, experts have argued against the broad implementation of LLMs. They often cite concerns about data privacy, security, and computational resources. Gareth Hinde, Solutions Architect at Swipe iX, recently raised that communications with centralized LLMs are prone to exposure, misuse, and breaches. Hinde noted the risks abound because models hosted centrally typically involve sending data to a central server for processing. Meanwhile, the notion of decentralized LLMs has surfaced, potentially presenting keys for ensuring privacy, security, and optimal utilization. Specifically, the control of decentralized LLMs would be distributed across a network of computers, eliminating the reliance on a singular company or organization. Emphatically, experts believe decentralized LLMs guarantee that sensitive data remains stored on users’ local devices. Follow Us on Twitter and Facebook. Disclaimer: This content is informational and should not be considered financial advice. The views expressed in this article may include the author's personal opinions and do not reflect The Crypto Basic’s opinion. Readers are encouraged to do thorough research before making any investment decisions. The Crypto Basic is not responsible for any financial losses. -Advertisement- TAGSCARDANOCARDANO FOUNDERCHARLES HOSKINSONCHATGPTLLM Author Abdulkarim Abdulwahab http://thecryptobasic.com Abdulkarim Abdulwahab is a blockchain writer with a specific interest in journalistic writing. He covers breaking events in the crypto community and blockchain industry. Over the past year, he has published over 1,500 short-form and long-form content for Web3 publishing firms. More from Author Market XRPL One Step Closer to Welcoming EVM Sidechain as Security Firm Completes Audit Market Ripple VP Outlines How CBDCs Can Improve Financial Inclusion Market Floki and TokenFi Target Over 700 Million Cricket Fans with Indian Legends League Partnership Latest Stories Market XRPL One Step Closer to Welcoming EVM Sidechain as Security Firm Completes Audit Abdulkarim Abdulwahab - November 18, 2023 Market Fidelity Cites Grayscale Ruling In Its Spot Ethereum ETF Application Lele Jima - November 18, 2023 XRP Needed to Make $1M, $3M, $5M or $10M if XRP Recovers ATH at $3.3 November 18, 2023 Ripple VP Outlines How CBDCs Can Improve Financial Inclusion November 18, 2023 Expert Criticizes Fox Business Journalist For Attacking XRP Current Performance As Price Drops 3% November 18, 2023 Here Are 3 Great Outcomes From Ripple Case Since July November 18, 2023 Poloniex Confirms ID of Attacker Behind $120M Hack, Involves Police from China, Russia and the US November 18, 2023 Market Commentator Outlines Reasons He is Confident XRP Will Hit $100, Cites BTC History November 18, 2023 Deaton Gives Financial Advice After Bogus BlackRock XRP ETF Report November 18, 2023 XRP to $11: Here’s the Projected Potential Timeline November 18, 2023 Load more Guides Guides Vertcoin Guide (VTC)-Vertcoin 1 Click Miner-Vertcoin Price Prediction And More Guides What is Ethereum-How Does it Work-Complete Discussion in Simple Terms Post Ethereum 2.0 Merge ETH Could Qualify as a Security Or Not? How To Establish Business With Cryptocurrency: Exchange & Wallet Beginner’s Guide To Testnets: Blockchains On The Testnets & Mainnets What To Know About Crypto Demo Accounts What is Uptrennd Coin (1UP)-What Makes Uptrennd Special? Dogecoin Cloud Mining And Dogecoin Mining-How To Do Them Properly Is Crypto Investing legal in India? Here’s everything you need to know Understanding the Pros and Cons of Bitcoin Investment Load more",
    "originSummary": [
      "Cardano founder, Charles Hoskinson, has invited Sam Altman, former CEO of OpenAI, to collaborate on a decentralized Large Language Model (LLM) project on the Cardano blockchain.",
      "Hoskinson sees Altman's expertise in artificial intelligence as valuable for advancing the development of decentralized LLMs.",
      "Decentralized LLMs distribute control across a network of computers, ensuring privacy and optimal utilization, addressing concerns about data privacy and security with centralized LLMs."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "Google Delays Launch of Gemini, Potential Rival to OpenAI's GPT-4",
    "originLink": "https://www.uctoday.com/?p=59894",
    "originBody": "Google Delays Launch of GPT-4 Rival Gemini Google's landmark LLM has been delayed 4 CollaborationLatest News Published: November 17, 2023 Kieran Devlin Google is delaying the launch of its flagship large language model (LLM) and GPT-4 competitor Gemini. As first reported by The Information, Google representatives had told some of the tech giant’s cloud customers and partners that the AI model shouldn’t be expected until Q1 of 2024. The Information’s report noted that a factor in the delay was the uncertainty of whether Gemini could equal or surpass OpenAI’s most advanced LLM in GPT-4. Given much of the publicity around Gemini were the claims that it would be superior to the Microsoft-funded GPT-4, that standard possibly not being met suggested Google wanted more time to refine the product. The Information’s sources stated that Gemini’s delay was also predicated on wanting to strengthen its consumer offerings with the new AI-powered technology before granting external software developers access to it. Google had initially planned to release Gemini by December. Only two months ago, the business had reportedly provided a small group of companies access to Gemini, suggesting it was on schedule to meet its release date with reported plans to make Gemini available to organisations through its Google Cloud Vertex AI platform. According to the report, Google is approaching Gemini’s release with caution, including around using Gemini in Bard, its answer to ChatGPT and a less sophisticated LLM than Gemini. Bard made a factual error in a demo earlier this year, and the report suggests that error still concerns workers involved with the project. What is Gemini? Billed as Google’s flagship AI, the company has previously claimed that Gemini has five times greater computational power than GPT-4. Google is explicitly presenting Gemini as a direct competitor to OpenAI’s ChatGPT-4 and is trained on Google’s advanced TPUv5 chips, which can work with 16,384 chips simultaneously. Gemini can work chatbots, summarise text or create original text based on what users like to read, including email drafts or news stories. It also aims to help software developers write code. While Google’s claim that Gemini’s computational power dwarfs GPT-4’s remains unsubstantiated for now, and this delay brings its veracity further into the spotlight, the product has several confirmed differentiators. Gemini was designed with multimodal processing in mind. This means it can process both images and text, and it has been put forward that it will be able to produce context-sensitive images and texts in answer to prompts. Another key differentiator is Google’s availability of proprietary training data. Gemini can be trained across Google and Alphabet’s expansive portfolio of services and products, including YouTube, Google Search, Google Books and Google Scholar. This volume of proprietary data could hand it an edge over ChatGPT-4 as it will make its answers more accurate and better-informed. That wealth of training data, compounded by the (claimed) computational power that would dramatically speed up response times and introduce the benefit of visualised answers, could position Gemini as a market leader upon release — if it lives up to its hype. CEO Sundar Pichai was asked about Gemini during Alphabet’s recent third-quarter earnings call, in which Google Cloud fell behind in revenue estimates despite its extensive AI investments, and said: On Gemini, obviously, it’s effort from our combined Google DeepMind team. I’m very excited at the progress there and as we’re working through getting the model ready. To me, more importantly, we are just really laying the foundation of what I think of as the next generation series of models we’ll be launching throughout 2024.” Google and its Other AIs Since Microsoft backed OpenAI’s launch of ChatGPT last year, Google became aware of the market demand to invest further in generative AI to keep up with its rival’s pace. August saw Duet AI for Google Workspace launch in general availability, the company’s new generative AI-powered productivity tool. Duet AI aims to streamline workflows by providing meeting assistance, document and conversation summaries, a chatbot for Google Chat, and customised suggestions for Gmail responses. “With the introduction of Duet AI, we added AI as a real-time collaborator,” said Aparna Pappu, GM and Vice President at Google Workspace. “Since its launch, thousands of companies and more than a million trusted testers have used Duet AI as a powerful collaboration partner that can act as a coach, source of inspiration, and productivity booster — all while ensuring every user and organisation has control over their data.” Duet AI for Google Workspace is priced at $30 per month per person. Meanwhile, Google Bard opened access in March. Google’s announcement described Bard’s LLM as a predictive engine that creates responses to prompts by choosing the words most likely to be used in conversation, effectively a more advanced version of Gmail suggesting email replies or Google Docs suggesting ways to end a sentence, a feature set comparable to Duet AI’s offering in Google Workspace. Last month, Google invested $2 billion in generative AI startup Anthropic to maintain the pace of AI innovation with its competitor tech giants. Artificial IntelligenceCorporate FinanceGenerative AIUCaaS",
    "originSummary": [
      "Google has postponed the launch of its language model Gemini, which aims to compete with OpenAI's advanced model, to Q1 2024.",
      "The delay is due to concerns about whether Gemini can surpass OpenAI's model and Google's focus on improving consumer offerings first.",
      "Gemini boasts five times more computational power, processes images and text, and has access to Google's proprietary training data, potentially positioning Google as a market leader if successful."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "LLMWare Releases DRAGON: 7B Parameter LLMs for Complex Business Documents",
    "originLink": "https://www.marktechpost.com/2023/11/17/llmware-launches-rag-specialized-7b-parameter-llms-production-grade-fine-tuned-models-for-enterprise-workflows-involving-complex-business-documents/",
    "originBody": "LLMWare Launches RAG-Specialized 7B Parameter LLMs: Production-Grade Fine-Tuned Models for Enterprise Workflows Involving Complex Business Documents By Asif Razzaq - November 17, 2023 Reddit16 Vote Flip Share Tweet 16 Shares Last month, Ai Bloks announced the open-source launch of its development framework, llmware, for building enterprise-grade LLM-based workflow applications. Today, Ai Bloks takes another big step on the journey of delivering a next-generation RAG framework with the release of the DRAGON (Delivering RAG on …) series of 7B parameter LLMs, designed for business workflows and fine-tuned with the specific objective of fact-based question-answering for complex business and legal documents. As more enterprises look to deploy scalable RAG systems using their own private information, there is a growing recognition of several needs: Unified framework that integrates LLM models with a set of surrounding workflow capabilities (e.g., document parsing, embedding, prompt management, source verification, audit tracking); High-quality, smaller, specialized LLMs that have been optimized for fact-based question-answering and enterprise workflows and Open Source, Cost-effective, Private deployment with flexibility and options for customization. To meet these needs, LLMWare is launching seven DRAGON models available in open source in its Hugging Face repository, all of which have been extensively fine-tuned for RAG and built on top of leading foundation models with strong production-grade readiness for enterprise RAG workflows. 🔥 Free AI Webinar: How to Chat with Images Data Using New GPT-4 Vision API [Nov 20, 10 am PST] All of the DRAGON models have been evaluated using the llmware rag-instruct-benchmark with the full test results and methodology provided with the models in the repository. Each of the DRAGON models achieve accuracy in the mid-to-high 90s on a diverse set of 100 core test questions, with strong grounding to avoid hallucinations and to identify when a question cannot be answered from a passage (e.g., ‘not found’ classification). The DRAGON model family joins two other LLMWare RAG model collections: BLING and Industry-BERT. The BLING models are no-GPU required RAG-specialized smaller LLM models (1B – 3B) that can run on a developer’s laptop. Since the training methodology is very similar, the intent is that a developer can start with a local BLING model, running on their laptop, and then seamlessly drop-in a DRAGON model for higher performance in production. DRAGON models have all been designed for private deployment on a single enterprise-grade GPU server, so that enterprises can deploy an end-to-end RAG system, securely and privately in their own security zone. ↗ Recommended Read: LLMWare Launches RAG-Specialized 7B Parameter LLMs: Production-Grade Fine-Tuned Models for Enterprise Workflows Involving Complex Business Documents This suite of open-source RAG-specialized models, combined with the core LLMWare development framework and out-of-the-box integration with open-source private-cloud instances of Milvus and Mongo DB, provide an end-to-end solution for RAG. With a few lines of code, a developer can automate the ingestion and parsing of thousands of documents, attach embedding vectors, execute state-of-the-art LLM-based generative inferences, and run evidence and source verification, all in a private cloud, and in some cases, even from a single developer’s laptop. According to Ai Bloks CEO Darren Oberst, “Our belief is that LLMs enable a new automation workflow in the enterprise, and our vision for LLMWare is to bring together the specialized models, the data pipeline, and all of the enabling components in a unified framework in open source to enable enterprises to rapidly customize and deploy LLM-based automation at scale.” For more information, please see the llmware github repository at www.github.com/llmware-ai/llmware.git. For direct access to the models, please see the llmware Huggingface organization page at www.huggingface.co/llmware. Thanks to AI Bloks for the thought leadership/ Educational article. AI Bloks has supported us in this content/article. Asif Razzaq Website+ posts Asif Razzaq is the CEO of Marktechpost Media Inc.. As a visionary entrepreneur and engineer, Asif is committed to harnessing the potential of Artificial Intelligence for social good. His most recent endeavor is the launch of an Artificial Intelligence Media Platform, Marktechpost, which stands out for its in-depth coverage of machine learning and deep learning news that is both technically sound and easily understandable by a wide audience. The platform boasts of over 2 million monthly views, illustrating its popularity among audiences. Reddit16 Vote Flip Share Tweet 16 Shares 🔥 Join The AI Startup Newsletter To Learn About Latest AI Startups Previous articleMeet mPLUG-Owl2: A Multi-Modal Foundation Model that Transforms Multi-modal Large Language Models (MLLMs) with Modality Collaboration Next articleThis AI Paper Introduces LLaVA-Plus: A General-Purpose Multimodal Assistant that Expands the Capabilities of Large Multimodal Models",
    "originSummary": [
      "LLMWare has released DRAGON, a series of 7B parameter LLMs for fact-based question-answering in complex business and legal documents.",
      "The models have been extensively fine-tuned and achieve high accuracy in answering core test questions.",
      "LLMWare aims to provide a unified framework that integrates LLM models with various workflow capabilities and offers customizable deployment options for enterprise-grade automation."
    ],
    "commentBody": "",
    "commentSummary": ["The summary for the article is missing."],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "New Relic Expands Partnership with AWS for AI Monitoring",
    "originLink": "https://www.cxotoday.com/press-release/new-relic-deepens-relationship-with-aws-to-provide-ai-monitoring/",
    "originBody": "HomePress ReleaseNew Relic Deepens Relationship with AWS to Provide AI Monitoring Nov. 17, 2023 at 3:01 pm PRESS RELEASE New Relic Deepens Relationship with AWS to Provide AI Monitoring CXOtoday News Desk1 day ago New Relic AI Monitoring integrates with Amazon Bedrock to help engineers build and run AI applications with in-depth insights to optimize performance, quality, and cost New Relic, the all-in-one observability platform for every engineer, announced that New Relic AI Monitoring (AIM), one of the industry’s first APM solution for AI-powered applications, is now integrated with Amazon Bedrock, a fully managed service by Amazon Web Services, Inc. (AWS)‘s that makes foundation models (FMs) from leading AI companies accessible via an API to build and scale generative AI applications. AWS customers can now use New Relic to gain greater visibility and insights across the AI stack, making it easier to troubleshoot and optimize their applications for performance, quality, and cost. While AI is revolutionizing modern applications, it introduces new challenges and complexity to organization’s tech stacks. AI tech stacks include new components like large language models (LLMs) and vector data stores, and generate additional telemetry to track such as quality and cost. AIM solves these new challenges by bringing APM to the AI stack. Similar to how engineers monitor their application stack with New Relic APM, AIM provides engineers with full visibility into all components of the AI stack. AIM provides a single easy view to troubleshoot, compare, and optimize different LLM prompts and responses for performance, cost and tokens, and quality issues including hallucinations, bias, toxicity, and fairness across all models supported by Amazon Bedrock. AIM integrates with Amazon Bedrock to provide in-depth end-to-end observability. With AIM’s built-in integrations such as Langchain, Amazon Bedrock customers can get metrics and tracing throughout the life-cycle of LLM prompt and response, ranging from raw prompts to repaired and business-compliant responses. Key features and use cases include: ● 50+ (and growing) AI stack integrations: Instantly monitor your entire AI stack with quickstart integrations for popular LLMs, vector databases, and orchestration frameworks such as: ○ Orchestration framework: Langchain ○ Vector databases: Pinecone, Weaviate, Milvus, FAISS, Zilliz ○ LLM: Amazon Bedrock (models from AI21 Labs, Amazon, Anthropic, and Cohere) ○ AI infrastructure: Amazon SageMaker ● Visibility across the entire AI app stack: Holistic view across the application, infrastructure, and the AI layer, including AI metrics like response quality and tokens alongside your APM golden signals, all with no additional instrumentation required. ● Deep trace insights for every response: Trace the lifecycle of complex LLM responses built with tools such as Langchain including end-user feedback, to fix performance issues and quality problems such as bias, toxicity, and hallucination. ● Compare performance and costs across models: Track usage, performance, quality, and cost across all models in a single view to choose the right model for your needs and optimize costs. “Observability is essential for any company building AI applications,” said New Relic Chief Product Officer Manav Khurana. “Today’s news builds upon our deep work with AWS to bring the power of observability to engineers and developers who are modernizing their tech stacks. And by putting our AI monitoring solution front and center with AWS customers, we are multiplying our ability to reach every engineer using leading LLMs like Anthropic.” New Relic AIM builds upon New Relic’s deep relationship with AWS and adds to its more than 95 existing AWS integrations. AIM is now available in early access to New Relic and AWS users across the globe. About New Relic As a leader in observability, New Relic empowers engineers with a data-driven approach to planning, building, deploying, and running great software. New Relic delivers the only unified data platform that empowers engineers to get all telemetry—metrics, events, logs, and traces—paired with powerful full stack analysis tools to help engineers do their best work with data, not opinions. Delivered through the industry’s first usage-based consumption pricing that’s intuitive and predictable, New Relic gives engineers more value for the money by helping improve planning cycle times, change failure rates, release frequency, and mean time to resolution. This helps the world’s leading brands including adidas Runtastic, American Red Cross, Australia Post, Banco Inter, Chegg, GoTo Group, Ryanair, Sainsbury’s, Signify Health, TopGolf, and World Fuel Services (WFS) improve uptime, reliability, and operational efficiency to deliver exceptional customer experiences that fuel innovation and growth. www.newrelic.com. Tags :New Relic add a comment SonicWall Acquires Managed Detection and Response Services Tailor-Made for MSPs/MSSPs Klub backed Daalchini aims to cross ₹100 crores ARR in the next 12 months You Might Also Like PRESS RELEASE MediaTek Unveils RedCap Solutions to Deliver 5G Data Rates and Impressive Power Efficiency to a Broad Range of IoT Devices CXOtoday News Desk12 hours ago MediaTek debuts RedCap modem technology and chipset family to expedite the transition to 5G-NR for consumer, enterprise, and industrial IoT... PRESS RELEASE MediaTek Expands Wi-Fi 7 Portfolio with New Chipsets for Mainstream Devices CXOtoday News Desk12 hours ago Second-generation Filogic chipsets deliver Wi-Fi 7 speeds, peak performance, and always-on reliability MediaTek, one of the first adopters of Wi-Fi... PRESS RELEASE Securonix Appoints Scott Sampson as Chief Revenue Officer CXOtoday News Desk22 hours agoNovember 17, 2023 Unified Defense SIEM Leader Taps Proven Enterprise Software Executive to Guide Global Go-to-Market Organization Securonix, Inc., a leader in Unified... PRESS RELEASE BuyUcoin™ becomes an FIU Registered Reporting Entity, Demonstrating its Commitment to Financial Compliance for Virtual Digital Assets CXOtoday News Desk23 hours ago BuyUcoin, India’s second-oldest digital asset exchange, today announced that it has become Financial Intelligence Unit - India (FIU-IND) registered reporting... Subscribe Newsletter Get all latest content delivered straight to your inbox. Join us on socials News Corner 91% of banks and insurers have initiated their cloud journey, yet many are unable to realize full business value 23 hours ago Redis Cloud Powers LangChain OpenGPTs Project 24 hours ago SAP India recognises Organizations that Embraced Transformation and Prioritized Ease of Doing Business in 2023 24 hours ago Tsuyo Manufacturing Pvt. Ltd. (India) signs an MoU with Suzhou LVKON Transmission Technology Co., Ltd. (China) for high-power solutions for various BEVs and HEVs 1 day ago NTT DATA Transforming the Future of the Digital Workplace with Generative AI and Copilot for Microsoft 365 1 day ago Fireflies.ai Launches Mobile App, Bringing Its AI Notetaker to In-Person Meetings 1 day ago",
    "originSummary": [
      "New Relic has partnered with Amazon Web Services (AWS) to offer AI monitoring for engineers working on AI applications.",
      "The integration of New Relic AI Monitoring with Amazon Bedrock enables AWS customers to gain visibility and insights throughout their AI stack, helping them troubleshoot and optimize performance, quality, and cost.",
      "This partnership expands upon New Relic's existing collaboration with AWS and adds to its portfolio of AWS integrations."
    ],
    "commentBody": "",
    "commentSummary": ["The requested information is not available or missing from the source."],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "OpenAI Turmoil: Uncertain Future for Startups and the AI Industry",
    "originLink": "https://nextbigwhat.com/openai-what-lies-ahead/",
    "originBody": "Image Credit: Futurism OpenAI chaos and what lies ahead for startups OpenAI has just landed in deep chaos with founder and CEO, Sam Altman being fired from the company. While there is no clarity on what the actual reasons were (and we won’t get into it), here is what will happen in the next few days/weeks. First of all, the generative AI revolution which OpenAI started will continue, albeit lesser momentum. Here is what I believe will happen: A lot of Generative AI startups will lose momentum This is not your standard startup leadership shakeup. 10,000’s of startups are building on OpenAI, and have assumed a certain degree of technical velocity and commercial stability. This instantly changes the structure of the industry. — Aaron Levie (@levie) November 18, 2023 As somebody building on OpenAI, all I can tell you is that the next few days are going to be dark and dusty for most startups that were dependent on OpenAI LLM (which is still the best in the industry with no near competitor / Anthropic is very limited). Apart from the fact that there are talks of a recession in 2024, this is the last thing startups wanted to deal with – but then, that’s life! Developers will start moving out of OpenAI Really. It’s tough to trust a company which is in the middle of so much of chaos. Given that most generative AI startups are heavily dependent on AI LLM providers (what else is powering their magic?), it will be a sane call to explore OpenAI alternatives. GPT4 onwards will deteriorate. We even felt that GPT4-turbo wasn’t that great, but with so much of chaos – expect a lot of engineers and researchers leaving the company. Clearly, there were different factions in OpenAI and each had their own version of AGI. Leaving those differences aside, the only force that took everyone along was Sam Altman – and with his departure, do not expect massive tech moonshots from OpenAI. There won’t be any GPT-5 and I wouldn’t even touch the newly announced assistant APIs or vision APIs now. Open source LLMs will win As a serious business which aims to solve a problem statement using AI, you are better off trusting open source LLMs, training them on your data and host the infra. And..Meta will now have a strong chance. With Llama 2, Meta is slowly catching up on – expect a full throttle attack and promotions soon. Google will again back to…sleep Where is Google Gemini? Google has not just been slow to respond, but the quality of its AI products is also questionable – with very little to talk about. With OpenAI now under the bus, do not expect anything big from Google – now that there is no serious fight for its bread and butter, i.e. search engine /advertising business. Back to OKRs for Google! What do you think is going to happen post this OpenAI chaos? If you are a developer/maker, would love to know your early thoughts? What is your team talking about? 0 0 Share 0 Tweet 0 Share 0 Total 0 Shares # trending Previous Post Ververica Cloud’s Latest Innovations Set New Standards in Real-Time Data Processing Next Post Top 10 open source OpenAI alternatives",
    "originSummary": [
      "OpenAI is facing turmoil as its founder and CEO, Sam Altman, is fired, leading to uncertainty about the future of the company.",
      "Startups relying on OpenAI may experience setbacks as the generative AI revolution loses momentum.",
      "Developers might start seeking alternatives to OpenAI due to the company's chaotic situation, potentially leading to a rise in popularity for open-source language models and benefiting competitors like Meta."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "NLU Delhi to Release AILET 2024-25 Admit Card on Nov 20",
    "originLink": "https://www.ndtv.com/education/national-law-university-delhi-to-release-admit-card-for-law-exam-on-nov-20-4584873",
    "originBody": "New Delhi: The National Law University Delhi, will release the admit card for All India Law Entrance Test (AILET) 2024-25 on November 20, 2023. The exam is conducted for admission to BA LLB (Hons), LLM and PhD programmes for the academic year 2024-25. The exam has been scheduled for December 10, 2023. Candidates who qualified for class 12 with 45 per cent marks are eligible to apply for BA LLB. Candidates appearing for the class 12 exam in 2024 are also eligible to register for the exam. To apply for LLM one year programme, candidates having LLB or an equivalent law degree with 50 per cent marks are eligible to apply for the LLM exam. Candidates appearing for the final year LLB annual examination in 2024 are eligible to apply. To apply for PhD in Law, candidates will be required to have a degree in LLM or equivalent law degree with 55 per cent. Listen to the latest songs, only on JioSaavn.com The AILET is conducted only once for admission to BA LLB, LLM, PhD programmes each academic year. The exam is held for admission to law programmes at NLU Delhi. The National Law University Delhi was established in 2008 by Act No 1 of 2008 of Delhi. It is a premier Law University in the capital city of India. Track Latest News Live on NDTV.com and get news updates from India and around the world. WATCH LIVE NEWS: FOLLOW US: AILET Law Entrance Exam National Law University Delhi",
    "originSummary": [
      "The National Law University Delhi will release the admit card for the All India Law Entrance Test (AILET) 2024-25 on November 20, 2023.",
      "The AILET is an annual exam conducted by NLU Delhi for admission to BA LLB (Hons), LLM, and PhD programs for the academic year 2024-25.",
      "Each program has specific eligibility criteria, such as minimum marks in class 12 for BA LLB and a law degree with minimum marks for LLM."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "Amazon Invests in Large Language Model \"Olympus\" to Compete with OpenAI and Alphabet",
    "originLink": "https://www.opp.today/uncategorized-en/amazon-sets-new-team-to-trains-ambitious-ai-model-codenamed-olympus-sources/197063/",
    "originBody": "Uncategorized xDE xES xFR xIT xPT Amazon Invests Millions in Training Large Language Model to Challenge OpenAI and Alphabet 2 min read 1 week ago Allan Ellis Amazon is making a substantial investment in training a groundbreaking large language model (LLM), according to anonymous sources familiar with the matter. The model, known internally as “Olympus,” is expected to have a staggering 2 trillion parameters, potentially making it one of the largest models ever trained. This moveAmazon is viewed as a direct challenge to the top models developedOpenAI and Alphabet. Although Amazon has declined to comment on the project, it appears to be part of a broader initiative to boost the company’s presence in the artificial intelligence (AI) field, particularly within the realm of LLMs. SpearheadedRohit Prasad, the former head of Alexa, the team of researchers working on Olympus has expertise in Alexa AI and Amazon’s science team. This alignment of AI efforts across the company reflects Amazon’s commitment to developing AGI (Artificial General Intelligence) capabilities. Amazon’s endeavor to develop homegrown models, such as Titan, and partnerships with AI model startups like Anthropic and AI21 Labs, are in line with the company’s strategy to enhance the attractiveness of its offerings on Amazon Web Services (AWS). By providing access to top-performing models, Amazon aims to meet the demand of enterprise clients who require sophisticated AI tools. Training larger AI models like Olympus incurs significant costs due to the immense amount of computational power required. However, Amazon’s dedication to investing in LLMs and generative AI was made evident during an earnings call in April. Executives noted that the company would prioritize these areas while reducing investments in fulfillment and transportation within its retail business. While a specific timeline for the release of Olympus has not been disclosed, it is clear that Amazon is placing great emphasis on pushing the boundaries of LLM capabilities. By doing so, the company aims to solidify its position as a leading player in the AI landscape and drive innovation in language generation tools. FAQs What is a large language model (LLM)? A large language model (LLM) is an AI model trained to understand and generate human-like responses based on vast amounts of data. Why is Amazon investing in training its own LLM? Amazon believes that developing its own LLMs will make its offerings on Amazon Web Services (AWS) more appealing to enterprise clients who require access to high-performing AI models. How does Amazon’s large language model compare to other models? Amazon’s Olympus model, with an estimated 2 trillion parameters, is expected to be one of the largest models ever trained. It will directly challenge top models developedOpenAI and Alphabet. What are the challenges in training large AI models? Training larger AI models requires significant computational power, making it a resource-intensive and costly endeavor. However, the potential benefits in terms of enhanced language generation capabilities make it worthwhile for companies like Amazon to pursue such projects. (Source: Reuters) Continue Reading Previous New App Store Policy Changes Expected in Response to EU’s Digital Markets Act Next Microsoft to Retire Tips App: Making Way for New Generative AI Feature",
    "originSummary": [
      "Amazon is investing heavily in training a large language model (LLM) called \"Olympus\" with an estimated 2 trillion parameters to challenge OpenAI and Alphabet's top models in language processing.",
      "The goal is to strengthen Amazon's presence in the AI field and attract enterprise clients to its offerings on Amazon Web Services (AWS).",
      "While the release timeline for Olympus is unknown, Amazon is committed to advancing the capabilities of LLMs to establish itself as a key player in AI and drive innovation in language generation tools."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "Assessing the Efficacy of AI Vision Tools: A Peek into the Future of Visual Data Interpretation",
    "originLink": "https://techxplore.com/news/2023-11-peek-future-visual-framework-generative.html",
    "originBody": "November 17, 2023 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked preprint trusted source proofread A peek into the future of visual data interpretation: A framework for assessing generative AI's efficacy by Nathi Magubane, University of Pennsylvania Caption: GPT-Vision sometimes appeared to use context clues to describe some elements of the images, such as the Amazon Alexa Echo Dot circled in the right. Credit: Alyssa Hwang In the last year, large language models (LLMs) have come into prominence for boasting a suite of ever-expanding capabilities including text generation, image production, and, more recently, highly descriptive image analysis. The integration of artificial intelligence (AI) into image analysis represents a significant shift in how people understand and interact with visual data, a task that historically has been reliant on vision to see and knowledge to contextualize. Now, new AI tools present a paradigm that allows more and more people to interact with images by generating descriptions that could not only assist the visually impaired but could also inform lay audiences about the contents of a scientific figure. Associate professor Chris Callison-Burch, assistant professor Andrew Head and Ph.D. candidate Alyssa Hwang of the Department of Computer and Information Science in the School of Engineering and Applied Science at the University of Pennsylvania have developed a framework for gauging the efficacy of vision-based AI features by conducting a battery of tests on OpenAI's ChatGPT-Vision ahead of its release earlier this month. The team primarily assessed the LLM's competency at identifying scientific images and documented their findings in a research paper, which appears on the pre-print server arXiv. Hwang shares some of her observations with Penn Today, offering a glimpse into the future of AI-powered technologies and the promise they hold for interpreting complex images. What the AI does and how the team tested it Hwang says that vision-based LLMs like GPT-Vision are able to analyze images and can receive images and text as input to answer a wide range of requests using this data. The team's set of test photos included diagrams, graphs, tables, and screenshots of code, math equations, and full pages of text with the intent to gauge how well the LLM could describe them. Scientific images contain complex information, Hwang says, so the team selected 21 images from a diverse set of scientific papers. \"We prioritized breadth in our qualitative analysis, which we based on existing methods in the social sciences, and we discovered many interesting patterns,\" she says. Examples tested Credit: Alyssa Hwang The researchers analyzed a photo collage of 12 dishes labeled with their recipe names. When they noticed that GPT-Vision seamlessly incorporated these labels into its descriptions, they tried changing them to something completely different to see how the LLM would respond. A few of Hwang’s favorite GPT improvisations: [C1 steaks with bleu cheese butter] Chicken noodle soup as a bowl presented with a dark broth and a dollop of cream. [C2 eggless red velvet cake] Fish sticks arranged on a tray with tomato sauce and cheese. And [C12 ground beef bulgogi], an ice cream sundae as a plate with ground meat topped with chopped green onions. Credit: Courtesy of Alyssa Hwang \"Surprisingly and amusingly,\" Hwang says, \"GPT-Vision still tried to incorporate these false new labels.\" Hwang says, however, that the LLM did much better when told to determine whether the label was accurate before continuing, which shows that it has sufficient knowledge to make an inference based on its vision capabilities, factors she believes are a promising direction for major research work. She also notes that, when describing a full page, the LLM appears to summarize the paragraphs within but that these \"summaries,\" were usually incomplete and out of order and might misquote the author or lift large amounts of text directly from the source, which might lead to trouble when redistributing anything it writes. \"With the proper adjustments, however, I am confident that GPT-Vision can be taught to summarize properly, quote fully, and avoid overusing source text,\" Hwang says. The team's framework Researchers in the natural language processing community have relied on automatic metrics to evaluate large swathes of the data landscape, but that task is now more challenging, Hwang says. \"In what we call 'human evaluation,\" we would ask real people for their input as well, which was possible at a small scale because our tasks and data were smaller and simpler,\" she says. \"Now that generative AI has become so adept at producing long-form sophisticated text, automatic metrics are becoming much more challenging to incorporate. We have gone from asking, 'Is this sentence grammatically correct?' to asking, 'Is this story interesting?' This is difficult to define and measure.\" Hwang's previous work on Amazon's Alexa familiarized her with techniques from the social sciences and human-computer interaction research, including grounded theory, a method for qualitative analysis that helps researchers identify patterns from large amounts of text. Traditionally used to analyze documents like interview transcripts, Hwang and other researchers can apply the same principles to machine-generated text. \"Our process feels very familiar to what people were naturally doing already: gathering GPT-Vision's responses to a set of images, reading deeply for patterns, incrementally generating more responses as we learned more about the data, and using the patterns we found to form our final conclusions,\" Hwang says. \"We sought to formalize trial and error processing with research-based methods, which can help both researchers and a general audience become more familiar with new generative AI models as they come out,\" she says. Applications and risks AI's ability to describe images could be a great accessibility tool for blind or visually impaired readers, Hwang says, automatically generating alt text for existing images or helping authors write their own text before publishing work. \"Describing images can also help sighted readers with information processing disorders, like issues with long- or short-term memory, visual sequencing, or visual-spatial understanding,\" she says. \"Beyond accessibility, image descriptions can be a source of convenience or enrichment. An e-reader could describe the photographs in a news article while the listener takes a walk, for example. We could ask an image description model for more details or clarification while reading a textbook. Tools like this can help us all access more information.\" Hwang says that, heeding some degree of caution in embracing these technologies without testing their limitations, the researchers discussed risk in terms of high- or low-stakes scenarios. She says that in the context of medicine and cooking she believes inaccuracies present the most risk when the user cannot double-check what the model is saying. The GPT-Vision whitepaper, published by OpenAI, advises against using the tool to read the dosage for a medical treatment, for example, but Hwang says that such a risk is greater for those with vision loss, information processing disorders, or language difficulties, those who stand to benefit the most from these technical advances. \"We may also initially assume that some aspects of cooking are low-risk because we can often improvise according to our preferences, but what if GPT-Vision mistakenly tells me that the spice jar in my hand is cinnamon instead of paprika? Even if it does not necessarily hurt me, my oatmeal will be pretty strange,\" Hwang says. Overall impressions and next steps Hwang is generally impressed with the state of generative AI and thinks there are opportunities for future work, including strengthening inconsistencies and using these tools in creative and inclusive ways. \"Researchers need answers to subjective questions,\" she says. \"What makes a description good? What makes it useful? Is it annoying? So, I hope generative AI researchers keep looking to users' feedback as they continuously iterate.\" Hwang's work with GPT-Vision was inspired by the idea of reading the contents of a scientific paper aloud wherein the figures and formulas would be intuitively explained. For her next project, she says she plans on using AI models to improve how audiobooks deliver information to listeners. \"Instead of skipping around in 15-second increments,\" she says, \"maybe we could go sentence by sentence or paragraph by paragraph. Maybe we could 'fast forward' through an audiobook by summarizing in real time. Using AI, maybe there are ways to 'translate' math equations into natural language to help people listen to textbooks and research papers. These are all exciting applications that seem within reach and I'm happy to be a part of the process.\" More information: Alyssa Hwang et al, Grounded Intuition of GPT-Vision's Abilities with Scientific Images, arXiv (2023). DOI: 10.48550/arxiv.2311.02069 Journal information: arXiv Provided by University of Pennsylvania Citation: A peek into the future of visual data interpretation: A framework for assessing generative AI's efficacy (2023, November 17) retrieved 18 November 2023 from https://techxplore.com/news/2023-11-peek-future-visual-framework-generative.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "originSummary": [
      "Researchers from the University of Pennsylvania have developed a framework to evaluate the performance of vision-based AI tools, specifically focusing on OpenAI's ChatGPT-Vision.",
      "The study found that while the AI was able to accurately describe images in some cases, it also had limitations such as generating incomplete or incorrect summaries.",
      "The researchers believe that AI-powered image description tools have the potential to enhance accessibility for visually impaired individuals and aid in information processing disorders, but caution about the risks of inaccuracies, especially in critical areas like medicine and cooking."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  },
  {
    "title": "Google Delays AI Gemini Model, Aims for GPT-4 Parity",
    "originLink": "https://www.digitalinformationworld.com/2023/11/disappointment-at-peak-for-google-as.html",
    "originBody": "Disappointment At Peak For Google As Company Forced To Delay Its AI Gemini Model To Next Year Dr. Hura Anwar 11/18/2023 12:41:00 PM It seems like tech giant Google’s AI plans are going through a rough patch as the company announced how it would now be forced to delay its LLM Gemini to next year. While development plans were set for early delivery in 2023, that’s not happening as mentioned today. The news arises from media outlet The Information which published a new report speaking about how the project won’t be seeing this month as their launch date as was planned at the start. Moreover, we’re also hearing about how it could arrive during Q1 of next year, making it a huge delay and not one that the tech giant was looking forward to. But for now, no details have been provided in terms of why Google has opted to push back the launch. However, the head of Google did give hints about how this might be the only decision that they’re being left with as the firm focuses more on getting it at par with other competitors and ensuring its design is truly state of the art. Meanwhile, the report added how one other major factor regarding this decision could be linked to Google facing great strength from its opposing AI rival which is OpenAI’s ChatGPT. Ever since the breakthrough rollout of ChatGPT which has really emerged as the next best thing in terms of popularity, we’re becoming more and more aware of the fact that its dominating power isn’t slowing down anything soon. Throughout the massive generative AI wave that we witnessed this year, we’ve seen it transform into a huge force that people simply couldn’t get enough of. Not only did it work as a leading generator for online content for regular citizens, but we also saw firms make use of it for processes like fast summaries linked to long-form reports. In the same way, it was used for the likes of designing new apps that would help with internal management as well as projections. So far, it’s been truly successful and we’ve even heard about how the parent firm OpenAI was left with no decision but to halt sign-ups for its ChatGPT Plus endeavor because all of its respective servers could no longer facilitate any more users. Now, the question is what does Google have in mind in terms of its next course of action while it moves ahead in the race? Well, as per reports from The Information, the team at Google wishes to ensure this primary model is just as great or even better as the GPT-4 variant which happens to be OpenAI’s latest offering. As one can expect, that’s a major order. Remember, the fact that GPT-4 is dubbed as being multimodal means it can take on videos, text, and even speech as prompts from users who wish to make new content or whether they are in the mood of putting out a query. Similarly, we are well aware of the latest model offering the best performance when compared to other variants of the past. Hence, it’s not capable of doing more than a single task at any given moment in time. As far as Google’s Gemini is concerned, the firm shed light on how it has a few things in mind. It hopes to use AI technology to better power creator tools across YouTube. Similarly, it wants to enable upgrades for Bard and even enhance the famous Google Assistant. At the moment, it has succeeded in making mini Gemini variants that it claims are designed to take on various types of tasks. For now, the main focus is linked to making the main model the best product out there and also to ensuring it’s functional to the masses. And that’s exactly where the delay seems to be arising from. Lastly, Google wishes it could get advertisers on its side with this AI launch. Remember, this is where the majority of Google’s revenue and profits come from so that’s why executives in the firm are wishing for advertising campaigns that can entail pictures and text while video content may be in the pipeline later on as well. Photo: DIW/ AI-gen Read next: Sundar Pichai's Email Reveals Concerns Over Google's Safari Browser Dominance and Antitrust Implications Facebook Twitter",
    "originSummary": [
      "Google has postponed the launch of its AI model, LLM Gemini, to next year to develop a competitive product to OpenAI's ChatGPT.",
      "The delay is to ensure that the main model is functioning properly and to rival OpenAI's GPT-4.",
      "Google plans to leverage AI technology to improve creator tools, Bard, and Google Assistant, and aims to attract advertisers with this AI launch."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700314186521
  }
]
