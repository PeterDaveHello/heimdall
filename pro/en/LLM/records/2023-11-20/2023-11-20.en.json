[
  {
    "title": "Lasso Security Raises $6M for LLM Cybersecurity",
    "originLink": "https://www.calcalistech.com/ctechnews/article/hjzn4yovt",
    "originBody": "24/7 Buzz Startups VC AI Innovation Opinions Events Promising Startups 2022 BiblioTech Boarding Pass Startup: Confidential Appointments CTalk Tech Gateways 2022 VC Survey @Finance Ctech Testimonials DataTech Projects AboutNewsletterContact usRSSFacebookTwitter ACCESSIBILITY by Homepage Startups Startups HOME 24/7 buzz STARTUPS VC AI Innovation OPINIONS EVENTS ABOUT NEWSLETTERSEARCHCONTACT US ACCESSIBILITY Recently Read From the U.S. Army to Marriage in Israel: the Legal Status of DNA Testing Kits The top travel-tech trends set to revolutionize tourism in 2023 The shameful antisemitism of “Elite Institutions” Recommended videos The 50 most promising Israeli startups - 2023 Palo Alto Networks announces the acquisition of Israeli startup Talon Cyber Security Sharon Gadasi Lasso Security nets $6 million Seed funding for Gen AI and LLM cybersecurity The Israeli startup provides cybersecurity for Large Language Models (LLMs), safeguarding against threats across all LLM touchpoints CTech 14:00, 20.11.23 TAGS: Lasso Security AI Cybersecurity Funding Lasso Security, which has developed a cybersecurity platform for Large Language Models (LLMs), announced on Monday a $6 million Seed round led by Entrée Capital with the participation of Samsung Next. Full list of Israeli high-tech funding rounds in 2023 This swift adoption of Gen AI and LLMs has created significant cybersecurity challenges, including data exposure, security and compliance risks. Lasso’s platform aims to safeguard every LLM touchpoint, providing protection for businesses leveraging Gen AI and other large language model technologies. 1 View gallery Lasso founders (from left) Elad Schulman, Lior Ziv, Ophir Dror, and Yuval Abadi. (Sharon Gadasi) Lasso, which was founded in 2023 by Elad Schulman, Lior Ziv, Ophir Dror, and Yuval Abadi, helps organizations safely integrate and deploy LLMs in production environments, preserving both data and user privacy while safeguarding against malicious threats. Related articles: Vulcan Cyber raises $34 million Refine Intelligence raises $13 million in Seed funding for anti-money laundering Lunar.dev raises $6 million in Seed for its API Consumption Management solution “The capabilities of LLMs are extraordinary, revolutionizing information processing but also introducing openings for cyber criminals in enterprise settings,” said Elad Schulman, CEO of Lasso Security. “As early as next year, organizations without a dedicated Gen AI risk management solution will experience negative outcomes. At Lasso, we’ve analyzed numerous LLM security challenges and developed a comprehensive solution encompassing preventative and remediation measures. Our mission is to equip businesses with robust defenses to safeguard their systems, including those using generative AI technologies that leverage LLMs. Every data point, command, and prompt generated by LLMs will be under the vigilant oversight of our sophisticated security platform, enabling teams to embrace generative AI without jeopardizing their data’s safety and security.” TAGS Lasso Security AI Cybersecurity Funding Rss Contact Us Newsletter Facebook Twitter About CTechTerms of UsePrivacy Policy Developed by UI & UX by Basch_Interactive",
    "originSummary": [
      "Lasso Security, an Israeli cybersecurity startup, has raised $6 million in a seed funding round led by Entrée Capital and participated by Samsung Next.",
      "The startup has developed a cybersecurity platform specifically designed for Large Language Models (LLMs) and aims to protect businesses utilizing LLM technology.",
      "Lasso Security's platform assists organizations in integrating and deploying LLMs in production environments, ensuring data and user privacy, and defending against malicious threats."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "DeepMind's OPRO technique revolutionizes ChatGPT prompts optimization",
    "originLink": "https://bdtechtalks.com/2023/11/20/deepmind-opro-llm-optimization/",
    "originBody": "Blog Optimize your ChatGPT prompts with DeepMind’s OPRO technique By Ben Dickson - November 20, 2023 Facebook Twitter ReddIt Linkedin Image generated with Bing Image Creator This article is part of our coverage of the latest in AI research. Large language models (LLM), while powerful, exhibit peculiar sensitivities to the way their prompts are formulated. In fact, providing the same command with different prompt formulations can yield completely different results. For instance, informing the model that your career depends on its response may increase the likelihood of it adhering more closely to your instructions. Additionally, using phrases like “let’s go through it step by step” can elicit reasoning in the models and improve their accuracy. Prompt engineering techniques, such as Chain of Thought (CoT) and emotional prompts, have gained popularity in recent years. However, there is still much to uncover when it comes to optimizing LLM prompts. Since LLMs do not understand language in the same way humans do, attempting to optimize their prompts through our own perspective on language might not yield the best results. An alternative approach is to allow LLMs to optimize their own prompts and discover the most effective instructions to enhance their accuracy. This concept forms the basis of Optimization by PROmpting (OPRO), a simple yet powerful method developed by Google DeepMind to use LLMs as optimizers. How OPRO works With OPRO, you can use LLMs as an optimizer. But instead of using mathematical formulas, you can describe the optimization problem using natural language, instructing the LLM to iteratively generate and refine its solutions. This is especially useful for problems such as prompt optimization, where gradients are not readily available. OPRO starts with a “meta-prompt” as input, composed of a natural language description of the task and a few examples of problems and solutions. Throughout the optimization process, the LLM generates candidate solutions based on the problem description and previous solutions in the meta-prompt. OPRO then evaluates the results of these candidate solutions and adds them to the meta-prompt, along with their quality score. This process is repeated until the model no longer proposes new solutions with improved scores. One of the key advantages of LLMs for optimization is their ability to process natural language instructions. This allows users to describe optimization tasks without formal specifications. For instance, users can specify metrics such as “accuracy” while simultaneously providing other instructions, like requiring the model to deliver concise and generally applicable solutions. OPRO also capitalizes on LLMs’ capacity to detect in-context patterns, enabling the identification of an optimization trajectory based on the exemplars included in the meta-prompt. This aspect, in my opinion, is the primary magic of OPRO. Since it sees language as numerical tokens, it can find patterns that are not visible to human observers. “Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of solutions with high scores, encouraging the LLM to build upon existing good solutions to construct potentially better ones without the need of explicitly defining how the solution should be updated,” DeepMind explains in its paper. DeepMind OPRO algorithm (source: arxiv) Optimizing prompts with OPRO DeepMind tested OPRO on linear regression and the “traveling salesman problem,” two well-known mathematical optimization problems. The technique demonstrated promising results in both cases. However, the true potential of OPRO lies in optimizing the use of LLMs like ChatGPT and PaLM. The DeepMind paper shows that OPRO can guide LLMs toward optimizing their own prompts, meaning it can help find the prompt that maximizes the accuracy of responses for a specific task. For instance, to discover the optimal prompt for solving word-math problems, an “optimizer LLM” is given a meta-prompt containing instructions and examples with placeholders for the optimization prompt. The model generates a set of different optimization prompts and passes them to a “scorer LLM,” which tests them on problem examples and evaluates the results. The best prompts, along with their scores, are added to the beginning of the meta-prompt, and the process is repeated. The researchers evaluated the technique using several LLMs from the PaLM and GPT families. According to their experiments, all of the models were able to improve the performance of the generated prompts through iterative optimization. For example, when the researchers tested OPRO with PaLM-2 models on GSM8K, a benchmark of grade school math word problems, the model began with a prompt that finished with “Let’s solve the problem.” It then generated other prompt additions, such as “Let’s think carefully about the problem and solve it together,” “Let’s break it down,” “Let’s calculate our way to the solution,” and ultimately “Let’s do the math,” which yielded the highest accuracy. In another experiment, adding the string “Take a deep breath and work on this problem step-by-step” before the LLM’s answer produced the most accurate result. These examples demonstrate how OPRO can help explore the space of possible LLM prompts and identify the one that works best for a specific type of problem. But they are also a reminder of the differences between how LLMs and humans understand language. How to use OPRO Although DeepMind has not released the code for OPRO, the concept behind the technique is intuitive and straightforward, making it possible to create a custom implementation in just a few hours. Alternatively, you can follow this step-by-step guide by LlamaIndex, which demonstrates how to use OPRO to enhance an LLM’s performance on retrieval augmented generation (RAG) tasks using external documents. The code sample in the guide employs GPT-3.5 Turbo as both the optimizer and evaluator LLM. However, you can easily adapt it to your preferred model. OPRO is just one of several techniques that leverage LLMs to optimize their own performance. This area is actively being explored across various topics, including jailbreaking and red-teaming, as researchers continue to unlock the full potential of large language models. Like this: Like Loading... TAGS AI research papers Artificial intelligence (AI) large language models Facebook Twitter ReddIt Linkedin Previous articleThe Apple Watch SE: The affordable smartwatch for uncertain times Ben Dickson Ben is a software engineer and the founder of TechTalks. He writes about technology, business and politics.",
    "originSummary": [
      "DeepMind has developed a technique called Optimization by PROmpting (OPRO) to improve the accuracy of large language models (LLMs) like ChatGPT.",
      "OPRO instructs the LLM to generate and refine solutions based on a problem description and previous solutions in a meta-prompt.",
      "DeepMind has not released the code for OPRO, but a step-by-step guide allows users to create a custom implementation."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "Elon Musk's xAI Shareholding Boosts Value of AI Tokens",
    "originLink": "https://www.coinspeaker.com/cardano-hoskinson-sam-altman/",
    "originBody": "By Tolu Ajiboye November 20th, 2023 Artificial Intelligence, Blockchain News, Cryptocurrency News, News, Technology News AI Tokens Pump on Positive Industry News as X Corp Announces xAI Shareholding By Tolu Ajiboye November 20th, 2023 A number of AI tokens have seen recent significant increases, following Musk’s xAI shareholder announcement, along with other news updates.",
    "originSummary": [
      "Several AI tokens have seen significant value increases following Elon Musk's announcement of his xAI shareholding and positive industry news.",
      "Elon Musk's involvement and endorsement in the AI sector have influenced market sentiment and investor confidence.",
      "The market response highlights the impact of influential figures and positive developments on the value of AI tokens."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "AI Chatbots: Revolutionizing the Banking Sector with Personalized Service",
    "originLink": "https://medium.com/@fluidai/in-this-digital-age-the-banking-sector-is-undergoing-a-significant-transformation-and-one-of-the-18027590f3cc",
    "originBody": "AI Banking: A New Perspective to Legacy Systems Fluid AI · Follow 4 min read · 4 hours ago -- In this digital age, the banking sector is undergoing a significant transformation and one of the key drivers of this change is the adoption of artificial intelligence (AI), particularly Language Learning Models (LLMs), in the form of chatbots. These AI-powered chatbots are revolutionizing the way banks interact with their customers, providing a more efficient, personalized, and user-friendly service. What are LLM-based AI Chatbots? LLM AI-based chatbots are advanced conversational agents that use machine learning and natural language processing to understand and respond to human language. They can handle complex conversations, understand context, and provide accurate responses, making them ideal for customer service applications in the banking sector. LLM-Based Chatbots vs Traditional Chatbots Chatbots are already a common feature in many banks, providing 24/7 customer service, answering queries, and even performing transactions. They offer numerous benefits, including reduced operational costs, increased customer satisfaction, and improved efficiency. Now the important question is do LLM base chatbots really have a difference? Let’s find out Traditional chatbots typically rely on pre-programmed responses and lack the ability to understand context, which can lead to inaccurate responses and a poor user experience. On the other hand, LLM-based chatbots use machine learning and natural language processing to understand and respond to human language. For example, a traditional chatbot only answers correctly if the customer says “What’s my account balance” and that’s a major problem being solved by LLM-based chatbots. Unlike them, they understand human language instead of just phrases. So when a customer says “Tell my balance” or “Please tell my balance”, all of the queries work and give the same answer. The Impact of Chatbots in Banking: By the Numbers Here are some statistics that highlight the impact of AI chatbots in banking: Increased Lead Generation: Banks have reported six times more leads collected using chatbots compared to traditional lead generation. Cost Savings: By 2023, banks are expected to save $7.3 billion in operational costs due to the use of chatbots. Improved Loan Accessibility: Artificial intelligence expands loan accessibility, approving 27% more loan applicants and yielding 16% lower interest rates. Rapid Adoption: In 2021, The Fintech Times reported that the percentage of midsize banks and credit unions using chatbots tripled in a single year, leaping from only 4% to 13%. Customer Satisfaction: HDFC’s Electronic Virtual Assistant (EVA) has responded to more than 5 million inquiries with at least 85% accuracy. These figures demonstrate the significant benefits that AI chatbots bring to the banking sector, including cost savings, increased efficiency, and improved customer service. As AI technology continues to evolve, we can expect these benefits to grow even further. Case Studies: LLM based Conversational Agents in Action Several banks have successfully implemented chatbots and are reaping the benefits. For example: Bank of America’s chatbot, Erica, has over 10 million users and has handled more than 100 million client requests. Erica helps customers with tasks like checking balances, scheduling payments, and providing credit report updates. Wells Fargo’s chatbot, which is integrated with Facebook Messenger, allows customers to check their balance, request their transaction history, and locate ATMs. The chatbot uses AI to understand natural language, making it easy for customers to interact with it. The Future Outlook The future of LLM AI chatbots in banking looks promising, with continuous advancements in AI technologies. As these chatbots become more sophisticated, they will likely take on more complex roles in banking, further enhancing operational efficiency, customer service, and risk management. Conclusion LLM AI-based chatbots are transforming the banking industry by enhancing customer service, reducing operational costs, personalizing services, and improving risk management. While challenges exist, the potential benefits make these chatbots an invaluable asset in the banking sector’s ongoing digital transformation. At Fluid AI, we stand at the forefront of this AI revolution, helping organizations kickstart their AI journey giving utmost importance to security. If you’re seeking a solution for your organization, look no further. We’re committed to making your organization future-ready, just like we’ve done for many others. Take the first step towards this exciting journey by booking a free demo call with us today. Let’s explore the possibilities together and unlock the full potential of AI for your organization. Remember, the future belongs to those who prepare for it today.",
    "originSummary": [
      "AI chatbots, particularly Language Learning Models (LLMs), are revolutionizing the banking sector by offering efficient and personalized customer service.",
      "LLM-based chatbots have an edge over traditional chatbots as they can understand context and provide accurate responses, resulting in increased lead generation, cost savings, improved loan accessibility, and customer satisfaction.",
      "Successful case studies demonstrate the numerous benefits of implementing LLM AI chatbots in banks, and with continuous advancements in AI technologies, the future of LLM AI-based chatbots in banking looks promising in terms of operational efficiency, customer service, and risk management."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "Dataiku and Databricks Collaborate to Empower Generative AI Adoption",
    "originLink": "https://www.tahawultech.com/home-slide/dataiku-welcomes-databricks-to-its-llm-mesh-partner-program/",
    "originBody": "Channel, Home-Slide, News, Vendor Dataiku welcomes Databricks to its LLM Mesh Partner Program Daniel Shepherd7 hours agoNovember 20, 2023 Abhijit Madhugiri, Vice President, Global Technology Alliances at Dataiku. Dataiku, the platform for Everyday AI, has announced that Databricks is the latest addition to its LLM Mesh Partner Program. Through this integration and partnership, the two companies are paving a clearer and more vibrant path for Generative AI-driven business transformations while allowing the enterprise to capitalise on the immense potential of LLMs. LLMs offer ground-breaking capabilities but create challenges related to cost control, security, privacy, and trust. The LLM Mesh is the solution — a common backbone for securely building and scaling Generative AI applications in the enterprise context. It simplifies the complexities of integration, boosts collaboration, and optimises resources at a time when over 60% of senior AI professionals are setting their sights on Generative AI, including LLMs, in the coming year. Together, Dataiku and Databricks democratise access to data, analytics, machine learning, and AI, enabling a collaborative, visual experience that scales programs and accelerates the delivery of Generative AI projects. “Databricks recognises the immense opportunities and challenges organisations face with the intricacies of Generative AI applications and the strain it can place on both technology and talent resources. We’re excited to partner with Dataiku and look forward to enabling every enterprise to build, scale, and realise the benefits of Generative AI”, said Roger Murff, VP of Technology Partners at Databricks. Emphasising the unique features of the LLM Mesh, the partnership aims to provide: Simplified Adoption: Businesses can effortlessly delve into Generative AI, benefiting from the user-centric Dataiku platform and Databricks’ advanced Lakehouse infrastructure. Comprehensive Data Access: The real value of Generative AI applications emerges when they are tightly integrated with enterprise data. With Dataiku, you can connect directly into your Delta Tables from Databricks. Connectivity to Databricks endpoints: Administrators can define a secure connection to Databricks hosted LLM models, including MosaicML, for business users to use in Dataiku’s Prompt Studios, visual LLM recipes, and Retrieval Augmented Generation. “Databricks’ addition to the LLM Mesh Partner Program amplifies our combined mission to democratise AI”, said Abhijit Madhugiri, Vice President, Global Technology Alliances at Dataiku. “With Dataiku’s user-friendly visual platform, individuals across organisations, irrespective of their tech proficiency, can now seamlessly interact with Databricks. Our combined efforts mean businesses can expect enhanced productivity, swifter model deployments, and absolute trust in their AI endeavours. And this is just the beginning”. The inclusion of Databricks furthers the reach and capability of the LLM Mesh. As enterprises globally look to harness the power of Generative AI, partnerships like these underscore the industry’s commitment to simplifying AI integration while upholding standards of safety, efficiency, and innovation. Related Articles Dataiku enhances its partner network to accelerate everyday AI Databricks Dataiku LLM Mesh Partner Program",
    "originSummary": [
      "Dataiku has announced a partnership with Databricks, joining their LLM Mesh Partner Program to simplify the adoption of Generative AI and enhance data access.",
      "The LLM Mesh provides a secure foundation for building and scaling Generative AI applications, and this partnership will allow businesses to seamlessly interact with Databricks.",
      "The collaboration aims to boost productivity, accelerate model deployments, and instill confidence in AI initiatives, expanding the reach and capabilities of the LLM Mesh for global enterprises."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "NVIDIA's New \"ChipNeMo\" AI Model Boosts Semiconductor Design Efficiency",
    "originLink": "https://wccftech.com/nvidias-nemo-model-now-tuned-for-chip-development-showing-exceptional-results/",
    "originBody": "Hardware NVIDIA’s “NeMo” Model Now Tuned For Chip Development, Showing Exceptional Results Muhammad Zuhair • Nov 20, 2023 02:10 AM EST • Copy Shortlink The future of chip development apparently lies within the hands of generative AI, as NVIDIA unveils a new LLM called \"ChipNeMo,\" designed to aid engineers with designing semiconductors, bringing a new save of advantages to the table. Chip Development Would Now Become Much Faster, Courtesy Of The Immense Capabilities Brought By Generative AI Figuring out the foundational elements of a chip is one of the most difficult tasks in the development phase, especially when it comes to sophisticated semiconductors, \"built with tens of billions of transistors, connected on streets 10,000 times thinner than a human hair\". Chip designing not only requires great human brains, but it is a process that often takes years, if not decades, to reach its optimal position. However, with the immediate rise in generative AI capabilities, NVIDIA has managed to design a \"custom\" LLM that acts as an aid with engineers to provide an end product that can prove to be \"revolutionary\" in terms of the semiconductor industry. Related Story Remember NVIDIA ACE? That AI-Powered NPC Engine Is Now Even Better With NeMo SteerLM With Customizable Attributes In a blog post, NVIDIA has revealed that with their ChipNeMo model, the firm plans to implement its capabilities at individual stages of chip designing, which would not only result in significantly improved productivity but will help them save costs in terms of human manpower and the time taken for designing a chip. NVIDIA has also disclosed the initial use case of their LLM, revealing an instance where a chip designer responds to questions about GPU architecture. You can look at it below: Talking about the creation process, NVIDIA revealed that developers formulated a foundational model and then combined it with NeMO LLM for building, customizing, and deploying generative AI models. The result was that the company was able to build a model that supported up to 43 billion parameters, which is essential in this particular segment, given how \"complex\" chip designing actually is. The model was trained using over a trillion tokens, words, and symbols in text and software. Now, the big question is, how would NeMo actually contribute towards the progression of the semiconductor industry in the future? Well, think of it this way; NVIDIA's NeMo model is similar to what ChatGPT is for students, aiding them in the final process, but it doesn't benefit them in a way that will put them \"ahead of the curve\". Similarly, NeMo is designed to help chip designers with critical questions, technical facts, or even problem-solving to an extent. However, it is safe to say that generative AI is now shaping up to be a dominant part of every industry, and is that something to joy upon? Well, time will tell. News Source: NVIDIA Blog Share this story Facebook Twitter Further Reading Superpowered Dreame H12 Pro Wet-n-Dry Cordless Vacuum with Hot Air Mop Drying Gets a 35% Discount Call of Duty Developer Infinity Ward Opens New Studio in Austin to Create ‘New and Innovative Experiences’ Intel’s Lunar Lake-MX Lineup Surfaces Online, Featuring Xe2 GPU Cores, DisplayPort 2.1 Support & Much More Mira Murati, CTO At OpenAI, Says The Company Is ‘Nothing Without Its People,’ Hinting At One More Imminent Resignation Comments Trending Stories Apple To Remain Tethered To Qualcomm Even After It Successfully Launches Its Custom 5G Modem, Here’s How 34 Active Readers Snapdragon 8 Gen 3 Sees Double-Digit Performance Gains Against Overclocked Snapdragon 8 Gen 2, But At A 28 Percent Power Consumption Increase 32 Active Readers Forza Motorsport – High Performance (No-Longer a Review in Progress) 27 Active Readers Mira Murati, CTO At OpenAI, Says The Company Is ‘Nothing Without Its People,’ Hinting At One More Imminent Resignation 26 Active Readers Intel’s Lunar Lake-MX Lineup Surfaces Online, Featuring Xe2 GPU Cores, DisplayPort 2.1 Support & Much More 15 Active Readers Popular Discussions NVIDIA GeForce RTX 4090 GPU Prices Hit Close To $2000 In The US, Cheapest Model Now At $1900 US 2361 Comments AMD Radeon RX 7900M “RDNA 3” Beats The NVIDIA RTX 4090 Laptop GPU In Vulkan Benchmark 862 Comments Elon Musk Calls Advertisers “Oppressors” After Comcast, IBM, and NBC Universal Abandon X 805 Comments NVIDIA GeForce RTX 50 “Blackwell” GPUs Utilize TSMC 3nm Process, DisplayPort 2.1 Support 762 Comments NVIDIA GeForce RTX 50 Flagship Gaming GPU Rumored To Feature GDDR7 Memory & 384-bit Bus 757 Comments",
    "originSummary": [
      "NVIDIA has created a new generative AI model called \"ChipNeMo\" to assist engineers in the design of semiconductors.",
      "The model is expected to improve the efficiency and cost-effectiveness of chip development by providing technical support and problem-solving capabilities to chip designers.",
      "Generative AI is gaining importance in various industries, including chip development."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "Using Customer Embeddings to Transform Banks: Decoding Risk",
    "originLink": "https://medium.datadriveninvestor.com/decoding-risk-transforming-banks-with-customer-embeddings-47eaf83a36c4",
    "originBody": "Decoding Risk: Transforming Banks with Customer Embeddings. In this article, we explore the transformative power of embeddings and large language models (LLMs) in customer risk assessment and product recommendation in the financial industry. Pere Martra · Follow Published in DataDrivenInvestor · 8 min read · 13 hours ago -- This article is part of a free course about Large Language Models available on GitHub. Image generated by Author with Dall·E2 If we were tasked with creating a system using LLM to enhance a bank’s decision-making regarding risk assessment or product approval for its clients. The initial thought might be to provide the LLM with client and product information, enabling it to make decisions on whether to grant the product to the client or not. But let me tell you, this wouldn’t work. We can’t solve everything by providing information to an LLM and expecting it to generate an output based on the input data. What we’re going to set up is a project that will integrate with the bank’s other systems, aiming not only to enhance decision-making regarding risk but also to improve maintenance and streamline the overall development of the entity’s information systems. We’ll be altering the format in which we store customer information, and consequently, we’ll also be changing how this information travels within the systems. Before we proceed, a disclaimer: All diagrams and descriptions of the solutions presented here are simplified to the maximum extent. This is a project that will likely span years and requires an immense development team. Here, I’m only presenting the idea, with a brief outline of how it should be and the advantages it can bring. Actual Client Risk System. Let’s start by looking at an example of what a current risk system for any institution might look like. Image by Author I’m going to detail the flow of a request: A loan request is initiated from one of the channels. The operation data + customer identifier is passed. The risk application collects customer data. The risk application collects product data. With customer, product, and operation data, a decision is made. The decision is returned to the channel that initiated the request. The process doesn’t seem overly complicated, but let’s delve a bit deeper. What are we talking about when we refer to user data? Is it a snapshot of the moment? In other words: age, gender, account balances, committed balances, available credit cards, relationships with other entities, investments, funds, pension plans, salary, and so forth? Well, now we have a better idea about the amount of data that must be taken into account, and we are talking about just a snapshot, the current moment. But wouldn’t it be better if we made decisions based on how all these variables have been changing over time? This implies considering even more data. As you can imagine, obtaining all this data involves a very high number of calls to different systems within the bank. I’ve simplified it by referring to “user data,” but that’s not a singular entity. Behind that label, there’s a myriad of applications, each with its own set of data that we need to request. In summary, calculating the customer’s position is typically done in a batch process that updates every X days, depending on the bank. The decision of whether to grant a loan is made by an algorithm or a traditional machine learning model that takes specific product data and precalculated risk positions as input. How can a Large Language Model (LLM) help us improve this process and, above all, simplify it? In this project, the key lies not in the deployment of LLMs but rather in crafting embeddings to encapsulate the entirety of customer information. This is a important decision that requires a meticulous justification. Let’s delve into some advantages of employing embeddings. Improved Feature Representation: Can effectively represent complex and unstructured data sources, such as text data from customer interactions, financial reports, and social media profiles. By transforming these texts into vectors that capture their meaning, embeddings can enable credit risk models to incorporate richer and more informative features, leading to more accurate risk assessments. Similarity Analysis: Enable measuring similarity between different entities. This can be useful in identifying similar customer profiles helping to identify potential risks based on historical patterns. Enhanced Risk Identification: Can identify hidden patterns and relationships within vast amounts of data, allowing credit risk models to uncover subtle signals that may not be apparent from traditional numerical data. This ability to identify and understand risk factors will lead to more precise risk assessments and better-informed lending decisions. Handling Missing Data: Can handle missing data more gracefully than traditional methods. Reduced Dimensionality: Exhibit reduced dimensionality compared to the original data representations. This facilitates storage, transmission, and processing of information. Transferability Convenience: Given that embeddings are numerical representations, they can be easily transferred across various systems and components within the framework. This enables consistency in the utilization of data representations across different segments of the infrastructure. These are just a few of the advantages that storing customer information in embeddings can provide. Let’s say I am fully convinced, and possibly, even if we haven’t entirely convinced the leadership, we have certainly piqued their curiosity to inquire about how the project should be approached. In other words, a preliminary outline of the solution. First picture of the solution. The first step is to select a model and train it with a portion of the data we want to store as embeddings. The entity has sufficient data to train a model from scratch using only its own data, without having to resort to a pre-trained model with financial data, which are not particularly abundant. The challenge we face is not the quantity of data, but rather the selection of which data to use and determining the cost we are willing to assume for training our model. From now on, let’s refer to this kind of model as FMFD (Foundational Model on Financial Data). And we need to train at least three of them, one to create the embeddings of the client, the other with the data of the product, and one trained to return the credit default rate. The third model introduces an interesting feature to the project. Classical models are designed to solve a problem by returning a single output. That is, a traditional machine learning model might provide a binary output indicating whether to approve or deny credit, while another could indicate the likelihood of credit default. In contrast, when using an LLM (Multi-Output Language Model), the output can be multiple, and from a single call, we could obtain various pieces of information. We might even obtain a list of recommended products, the maximum allowable credit, or an acceptable risk-associated interest rate. It all depends on the data used to train this model. Image by Author Starting from the point where we have already trained the three necessary models, we store the customer embeddings in an embeddings database. This could be a standard database or a vector database. For now, we won’t delve into the advantages and disadvantages of using one type of database over another. The crucial point is that these embeddings can be continuously calculated as the customer’s situation evolves, as it is a lightweight process that can be performed online. Although we also have the option of employing a batch process at regular intervals to handle multiple embeddings simultaneously and optimize resource utilization. Let’s explore what happens when we receive a transaction that needs to be accepted or rejected using our system. With the transaction data, we generate embeddings for the product to be granted, employing the pre-trained model with product data. This yields specific embeddings for the transaction. To make the decision, we’ll need the third model, to which we’ll feed both embeddings — one for the customer and one for the transaction. There are several options for combining these embeddings: Concatenate the two vectors. Add them to a single vector, using, for example, vector summation. Pass the data as two independent embeddings. Pass the embeddings in a list. My preference is to pass them using a single concatenated vector. With this vector as input, the third model is capable of generating a response that, depending on the training performed, can provide more or less information. Conclusion. With this solution, we have a system that utilizes nearly real-time information about the customer’s position, along with all the historical data, to analyze whether they can obtain one of our financial products. We’ve increased the quality and quantity of information used for making decisions regarding our customers’ risk operations. But that’s not all; the use of embeddings also simplifies system maintenance and opens up a world of possibilities that are challenging to detail at this moment. We have a system that adapts better to changes in information because the entire system deals with embeddings of fixed length, as opposed to a multitude of fields that can vary. The compact size of the embeddings even allows them to reside on the client’s device, enabling it to make decisions independently without the need to make a call to the bank’s systems. Preparatory Steps when initiating the project. As one can imagine, we are not discussing a two-month project here. This initiative would redefine how a banking entity stores and processes information about its clients and products. It’s a transformation that could span years, impacting a significant portion of its information systems department to varying degrees. A project of this magnitude necessitates a phased approach, allowing for pivots throughout its lifespan and a gradual introduction. My recommendation is start with a proof of concept, utilizing a single model but trained using QLoRA for the three identified tasks: Client Embeddings, Product Embeddings, and Decision Making. By adopting this approach, we can have three models operational swiftly, with minimal resource requirements for both training and inference. For each of these three models, decisions should be made regarding the data we want/need to use for their training, or in this case, fine-tuning. Resources. The full course about Large Language Models is available at Github. To stay updated on new articles, please consider following the repository or starring it. This way, you’ll receive notifications whenever new content is added. GitHub - peremartra/Large-Language-Model-Notebooks-Course: Practical course about Large Language… Practical course about Large Language Models. . Contribute to peremartra/Large-Language-Model-Notebooks-Course… github.com This solution has been inspired by this article The Shaky Foundations of Foundation Models in Healthcare at Stanford University This article is part of a series where we explore the practical applications of Large Language Models. You can find the rest of the articles in the following list: Pere Martra Large Language Models Practical Course View list 14 stories I write about Deep Learning and AI regularly. Consider following me on Medium to get updates about new articles. And, of course, You are welcome to connect with me on LinkedIn, and twitter. Subscribe to DDIntel Here. Have a unique story to share? Submit to DDIntel here. Join our creator ecosystem here. DDIntel captures the more notable pieces from our main site and our popular DDI Medium publication. Check us out for more insightful work from our community. DDI Official Telegram Channel: https://t.me/+tafUp6ecEys4YjQ1 Follow us on LinkedIn, Twitter, YouTube, and Facebook.",
    "originSummary": [
      "Embeddings and large language models (LLMs) are being used in customer risk assessment and product recommendation in the financial industry.",
      "Simply providing information to an LLM is not sufficient for accurate decision-making, so the article proposes integrating with existing systems to improve decision-making, maintenance, and overall development of information systems.",
      "Advantages of using embeddings include improved feature representation, similarity analysis, enhanced risk identification, handling missing data, reduced dimensionality, and transferability convenience.",
      "A multi-output LLM can be used to generate customer embeddings and make more informed lending decisions.",
      "To initiate such a project, a phased approach should be taken, starting with a proof of concept."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "Dstl and Google Cloud Hold Hackathon for Defense AI Innovation",
    "originLink": "https://www.miragenews.com/dstl-google-cloud-launch-defence-ai-innovation-1127245/",
    "originBody": "Mirage News Mirage News Mirage News World 20 Nov 2023 8:04 pm AEDT Date Time Share Dstl, Google Cloud Launch Defence AI Innovation Hackathon In June 2023, Dstl and Google Cloud signed a memorandum of understanding (MOU) to accelerate the safe and responsible adoption of artificial intelligence (AI) across the UK defence sector. As part of bringing this agreement to life, Dstl and Google Cloud delivered a Large Language Model (LLM) hackathon on behalf of the Ministry of Defence's (MOD) Defence AI Centre (DAIC). Taking place at Google Next, this world-leading event helped better understand the opportunities and risks of LLMs for UK defence and security. The hackathon brought together over 200 people to apply Google Cloud's cutting-edge generative AI tools to help strengthen UK defence, security and prosperity. 20 teams, stewarded by Google Cloud AI engineers, included Dstl scientists and representatives from the following: Royal Navy British Army Royal Air Force Space Command MOD policy-makers, ethicists and user researchers Participants worked for 2 days with Google's latest AI technologies to develop innovative, ethical and user-centred prototypes to add to the growing pipeline of AI innovation ideas created via the MOU. Senior HM government leaders and Google's technology leadership formed the judging panel. This included GCHQ's Chief Data Scientist, Head of the DAIC, and Google Cloud's Vice President of Engineering. They assessed the prototypes for 6 awards, including 'best technical achievement' and 'most illuminating failure'. A core criteria of every award was alignment to the MOD's ethical principles for AI and Google AI Principles. Winning prototypes ranged from LLM-scanning of cyber security threats, to LLM-enabled image analysis, to LLMs for data-driven predictive maintenance. The ideas explored during the hackathon are now being considered for further development and testing in order to bring the prototypes to level of readiness where UK Armed Forces could safely adopt them. Andy Bell, Dstl CTO said: \"Dstl and Google Cloud brought together a hugely diverse range of participants on behalf of the MOD's newly established DAIC, to learn, experiment and solve real-world defence problems with innovative AI technologies. This hackathon has demonstrated how creative problem-solving can be harnessed to address pressing defence challenges, paving the way for breakthroughs in generative AI for defence applications.\" John Abel, Technical Director for Google Cloud said: \"This hackathon is the first step in delivering on the recent MOU signed between Dstl and Google Cloud, building on clear synergies between the 2 organisations. \"The hackathon served as a powerful example of how collaboration can drive innovation and ultimately benefit the wider AI ecosystem. It also provided valuable educational and training opportunities, helping to foster the next generation of AI leaders in defence.\" /Public Release. This material from the originating organization/author(s) might be of the point-in-time nature, and edited for clarity, style and length. Mirage.News does not take institutional positions or sides, and all views, positions, and conclusions expressed herein are solely those of the author(s).View in full here. Why? Well, unlike many news organisations, we have no sponsors, no corporate or ideological interests. We don't put up a paywall – we believe in free access to information of public interest. Media ownership in Australia is one of the most concentrated in the world (Learn more). Since the trend of consolidation is and has historically been upward, fewer and fewer individuals or organizations control increasing shares of the mass media in our country. According to independent assessment, about 98% of the media sector is held by three conglomerates. This tendency is not only totally unacceptable, but also to a degree frightening). Learn more here We endeavour to provide the community with real-time access to true unfiltered news firsthand from primary sources. It is a bumpy road with all sorties of difficulties. We can only achieve this goal together. Our website is open to any citizen journalists and organizations who want to contribute, publish high-quality insights or send media releases to improve public access to impartial information. You and we have the right to know, learn, read, hear what and how we deem appropriate. Your support is greatly appreciated. All donations are kept completely private and confidential. Thank you in advance! Tags: UK , Google , British , leadership , Government , Defence , AI , Bell , intelligence , collaboration , artificial intelligence , Engineering , air force , UK Government , innovation , cyber security , problem-solving",
    "originSummary": [
      "Dstl and Google Cloud hosted a hackathon to explore the use of Large Language Models (LLMs) for defense and security purposes.",
      "Over 200 participants, including scientists from Dstl and representatives from various military branches, attended the event.",
      "Winning prototypes included LLM-scanning for cyber security threats and LLM-enabled image analysis, which are being considered for further development and potential adoption by the UK Armed Forces."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "Oblix at The Shard Unveils Sky High Weekend Brunch",
    "originLink": "https://www.luxurylifestylemag.co.uk/food-and-drink/oblix-at-the-shard-launches-new-sky-high-weekend-brunch/",
    "originBody": "Oblix at The Shard launches new sky high weekend brunch The Shard’s iconic restaurant, Oblix, has launched a new brunch menu. LLM writer Georgie Bentley-Buckle visited the 32nd floor to sample the new opulent daytime offering. By Georgie Bentley-BuckleNovember 20, 2023 Facebook Twitter Email LinkedIn Share Since opening a decade ago, The Shard has become an instrumental landmark of the Big Smoke’s skyline, entering with it a gastronomic lineup. Since then, the lofty skyscraper (that when it opened was the tallest in Europe) raised the game of London’s world-class dining scene, offering guests a new perspective of the city’s iconic streets. One of The Shard’s most popular restaurants on the 32nd floor that has established itself as a firm destination on the competitive restaurant map is Oblix. A place I first experienced as the city sparkled at night for long, sophisticated dinners – I recently took the lift on a Saturday lunchtime to experience the daytime offering. From the standard classic brunch, you can supplement with bottomless prosecco, or limitless Champagne Jumping on the brunch brigade, an endless offering that seems to uniquely hit the spot with both locals and tourists alike – you can now book yourself in for brunch on a Saturday or Sunday, for a boozy bottomless experience. Whereas Oblix at night is an intimate, refined experience, the brunch opens a new level of energy into the dining space as the city goes about its daily life below. Filled with friends toasting and families with children filling the tables, it’s certainly a different experience to what I remember, and to be enjoyed in a whole new light. Opulence is staggered depending on what you’re looking for. From the standard classic brunch, you can supplement with bottomless prosecco, or limitless Champagne which we selected without hesitation. This option also comes with wines, cocktails and soft drinks. As is the case with most menus, this one too blends a mix of contemporary classics (to keep the majority of punters happy) alongside some innovative, seasonal dishes the team have had some fun creating. Set to the sweeping views of London’s skyline and postcard-worthy landmarks, dishes are as indulgent as the setting Set to the sweeping views of London’s skyline and postcard-worthy landmarks, dishes are as indulgent as the setting. The signature starters to share come as one and offer a little taster of Oblix’s culinary capabilities. This includes a market salad bowl, the never without burrata with olives and datterini tomatoes, a gorgeous red tuna tartare with citrus and chilli, a prawn cocktail and lastly, smoked salmon with rice crackers, seaweed and horseradish. Main dishes branch from brunch favourites to firm lunch options. My guest and I decided to try something from each end of the spectrum. Firstly, the truffle soft poached eggs served on top of orzo, wild mushrooms and watercress which washed down nicely with the bubbles and the Oblix ‘signature’ pepper steak creamy peppercorn sauce. We paired these with some starters which were as large as the main dishes themselves. These included truffle and parmesan fries with béarnaise sauce and tender stem broccoli with almonds, chilli, and lemon. Main dishes branch from brunch favourites to firm lunch options Other highlights from this brunch menu include a lobster and crab benedict, again with the soft poached eggs, watercress and citrus hollandaise, a 200g dry aged sirloin served with a duck egg, fries and truffle mayonnaise, or the signature BBQ black cod with pickled shallots and citrus. Those looking to end the meal on a sweet note can tuck into a sharing-style dessert platter. Vegetarians and vegans are also well looked after with a selection of plant-based dishes. Back in spring 2013, Oblix was the first restaurant to open in the now legendary Shard. Today, the restaurant endeavours to offer ‘sky-high’ standards, offering guests sophisticated, seasonal dishes with views from within the tallest building in Western Europe. Oblix is split into two spaces, turn right at the entrance for Oblix West where we tried this new brunch menu in a contemporary yet relaxed restaurant with views from St. Pauls to the West End. You can also turn left for Oblix East to visit the bar and lounge which takes inspiration from its view. Here you will find a buzzy bar designed for a creative, cosmopolitan crowd with an imaginative cocktail menu. Factbox Address: 32 London Bridge St, London SE1 9SG Phone: 020 7268 6700 Website: oblixrestaurant.com Tags: brunch fine dining London Oblix The Shard Restaurant the shard You Might Also Like How to make Hawksmoor’s decadent sticky toffee pudding at home By Felicity Williams Cleveland, Ohio: A culinary gem fit for a president By Baldwin Ho Meet Salvatore Broccu: Operations director at Geode, Knightsbridge in London By Anuja Gaur Brewery tours in style: Luxury travel experiences in Ontario’s Toronto By LLM Reporters Talking tequila: An interview with Daniele Umotte, UK brand ambassador for Tequila Herradura and El Jimador By LLM Reporters Experience Argentinian fun and food at London’s swanky Gaucho Covent Garden By Georgie Bentley-Buckle Meet the chef: Kin Min How, dim sum head chef at Park Chinois in London By LLM Reporters Restaurant Review: Angel Above Portobello, Notting Hill in London By Cristian Burbano Blended beverages: The luxury drinks craze taking the world by storm By LLM Reporters Discover Volonte Piccadilly: London’s new wellness café By LLM Reporters Latest on LLM 5 timekeepers that define timeless luxury Oblix at The Shard launches new sky high weekend brunch Prestigious honour for The Ritz-Carlton, DIFC: Named UAE’s top luxury business hotel by Luxury Lifestyle Awards A guide to vitamin deficiency: What it is, the common symptoms and how to combat correctly 5 of the best beaches to visit in Vietnam WIN Win a luxury 2-night spa break amongst the stunning landscape of the Lake District with Armathwaite Hall Win a 4-night luxury stay at Amari Raaya Maldives for an adventure of a lifetime Win a luxury 5-night stay for a family of up to 4 at Alma Resort in Vietnam",
    "originSummary": [
      "Oblix at The Shard in London has introduced a new brunch menu on its 32nd floor.",
      "The menu features a variety of dishes, ranging from classic brunch items to more decadent choices.",
      "Vegetarian and vegan options are available, and the restaurant also has a separate bar and lounge area."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  },
  {
    "title": "PM Modi Assures Safe Evacuation of Workers Trapped in Uttarkashi Tunnel Collapse",
    "originLink": "https://www.india.com/education/ailet-2024-exam-on-dec-10-admit-card-to-release-at-nationallawuniversitydelhi-in-heres-direct-link-6517208/",
    "originBody": "Uttarkashi Tunnel Collapse: PM Modi Reviews Rescue Operation, Says ‘Workers Will Be Evacuated Safely’",
    "originSummary": [
      "Prime Minister Narendra Modi personally reviewed the ongoing rescue operation in Uttarkashi tunnel collapse.",
      "He assured that the safety and evacuation of the trapped workers are of utmost importance.",
      "The government is committed to ensuring a successful rescue operation in Uttarkashi."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700489629308
  }
]
