[
  {
    "title": "NVIDIA and Microsoft Collaborate to Enhance AI Experiences on Windows 11 PCs with New TensorRT-LLM Release",
    "originLink": "https://blogs.nvidia.com/blog/ignite-rtx-ai-tensorrt-llm-chat-api/",
    "originBody": "Igniting the Future: TensorRT-LLM Release Accelerates AI Inference Performance, Adds Support for New Models Running on RTX-Powered Windows 11 PCs New tools and resources announced at Microsoft Ignite include TensorRT-LLM wrapper for OpenAI Chat API, RTX-powered performance improvements to DirectML for Llama 2, other popular LLMs. November 15, 2023 by Jesse Clayton Share Email0 Artificial intelligence on Windows 11 PCs marks a pivotal moment in tech history, revolutionizing experiences for gamers, creators, streamers, office workers, students and even casual PC users. It offers unprecedented opportunities to enhance productivity for users of the more than 100 million Windows PCs and workstations that are powered by RTX GPUs. And NVIDIA RTX technology is making it even easier for developers to create AI applications to change the way people use computers. New optimizations, models and resources announced at Microsoft Ignite will help developers deliver new end-user experiences, quicker. An upcoming update to TensorRT-LLM — open-source software that increases AI inference performance — will add support for new large language models and make demanding AI workloads more accessible on desktops and laptops with RTX GPUs starting at 8GB of VRAM. TensorRT-LLM for Windows will soon be compatible with OpenAI’s popular Chat API through a new wrapper. This will enable hundreds of developer projects and applications to run locally on a PC with RTX, instead of in the cloud — so users can keep private and proprietary data on Windows 11 PCs. Custom generative AI requires time and energy to maintain projects. The process can become incredibly complex and time-consuming, especially when trying to collaborate and deploy across multiple environments and platforms. AI Workbench is a unified, easy-to-use toolkit that allows developers to quickly create, test and customize pretrained generative AI models and LLMs on a PC or workstation. It provides developers a single platform to organize their AI projects and tune models to specific use cases. This enables seamless collaboration and deployment for developers to create cost-effective, scalable generative AI models quickly. Join the early access list to be among the first to gain access to this growing initiative and to receive future updates. To support AI developers, NVIDIA and Microsoft will release DirectML enhancements to accelerate one of the most popular foundational AI models, Llama 2. Developers now have more options for cross-vendor deployment, in addition to setting a new standard for performance. Portable AI Last month, NVIDIA announced TensorRT-LLM for Windows, a library for accelerating LLM inference. The next TensorRT-LLM release, v0.6.0 coming later this month, will bring improved inference performance — up to 5x faster — and enable support for additional popular LLMs, including the new Mistral 7B and Nemotron-3 8B. Versions of these LLMs will run on any GeForce RTX 30 Series and 40 Series GPU with 8GB of RAM or more, making fast, accurate, local LLM capabilities accessible even in some of the most portable Windows devices. Up to 5X performance with the new TensorRT-LLM v0.6.0. The new release of TensorRT-LLM will be available for install on the /NVIDIA/TensorRT-LLM GitHub repo. New optimized models will be available on ngc.nvidia.com. Conversing With Confidence Developers and enthusiasts worldwide use OpenAI’s Chat API for a wide range of applications — from summarizing web content and drafting documents and emails to analyzing and visualizing data and creating presentations. One challenge with such cloud-based AIs is that they require users to upload their input data, making them impractical for private or proprietary data or for working with large datasets. To address this challenge, NVIDIA is soon enabling TensorRT-LLM for Windows to offer a similar API interface to OpenAI’s widely popular ChatAPI, through a new wrapper, offering a similar workflow to developers whether they are designing models and applications to run locally on a PC with RTX or in the cloud. By changing just one or two lines of code, hundreds of AI-powered developer projects and applications can now benefit from fast, local AI. Users can keep their data on their PCs and not worry about uploading datasets to the cloud. Perhaps the best part is that many of these projects and applications are open source, making it easy for developers to leverage and extend their capabilities to fuel the adoption of generative AI on Windows, powered by RTX. The wrapper will work with any LLM that’s been optimized for TensorRT-LLM (for example, Llama 2, Mistral and NV LLM) and is being released as a reference project on GitHub, alongside other developer resources for working with LLMs on RTX. Model Acceleration Developers can now leverage cutting-edge AI models and deploy with a cross-vendor API. As part of an ongoing commitment to empower developers, NVIDIA and Microsoft have been working together to accelerate Llama on RTX via the DirectML API. Building on the announcements for the fastest inference performance for these models announced last month, this new option for cross-vendor deployment makes it easier than ever to bring AI capabilities to PC. Developers and enthusiasts can experience the latest optimizations by downloading the latest ONNX runtime and following the installation instructions from Microsoft, and installing the latest driver from NVIDIA, which will be available on Nov. 21. These new optimizations, models and resources will accelerate the development and deployment of AI features and applications to the 100 million RTX PCs worldwide, joining the more than 400 partners shipping AI-powered apps and games already accelerated by RTX GPUs. As models become even more accessible and developers bring more generative AI-powered functionality to RTX-powered Windows PCs, RTX GPUs will be critical for enabling users to take advantage of this powerful technology. Categories: Deep Learning Tags: Artificial IntelligenceGeForceGenerative AINVIDIA RTXTensorRT All nvidia news Ringing in the Future: NVIDIA and Amdocs Bring Custom Generative AI to Global Telco Industry In the Fast Lane: NVIDIA Announces Omniverse Cloud Services on Microsoft Azure to Accelerate Automotive Digitalization New NVIDIA H100, H200 Tensor Core GPU Instances Coming to Microsoft Azure to Accelerate AI Workloads NVIDIA Fast-Tracks Custom Generative AI Model Development for Enterprises What Is Retrieval-Augmented Generation?",
    "originSummary": [
      "NVIDIA has unveiled new tools and resources at Microsoft Ignite to enhance AI experiences on Windows 11 PCs.",
      "Updates to TensorRT-LLM improve AI inference performance and add compatibility with OpenAI's Chat API.",
      "The introduction of AI Workbench and DirectML enhancements aim to make AI development and deployment more accessible on RTX-powered Windows PCs."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "Galileo's Hallucination Index Identifies OpenAI's GPT-4 as Best-Performing LLM for Multiple Use Cases",
    "originLink": "https://venturebeat.com/ai/galileo-hallucination-index-identifies-gpt-4-as-best-performing-llm-for-different-use-cases/",
    "originBody": "Are you ready to bring more awareness to your brand? Consider becoming a sponsor for The AI Impact Tour. Learn more about the opportunities here. A new hallucination index developed by the research arm of San Francisco-based Galileo, which helps enterprises build, fine-tune and monitor production-grade large language model (LLM) apps, shows that OpenAI’s GPT-4 model works best and hallucinates the least when challenged with multiple tasks. Published today, the index looked at nearly a dozen open and closed-source LLMs, including Meta’s Llama series, and assessed each of their performance at different tasks to see which LLM experiences the least hallucinations when performing different tasks. In the results, all LLMs behaved differently with different tasks, but OpenAI’s offerings remained on top with largely consistent performance across all scenarios. The findings of the index come as the latest way to help enterprises navigate the challenge of hallucinations — which has kept many teams from deploying large language models across critical sectors like healthcare, at scale. VB Event The AI Impact Tour Connect with the enterprise AI community at VentureBeat’s AI Impact Tour coming to a city near you! Learn More Tracking LLM hallucination is not easy Though surveys indicate massive interest from the enterprise in using generative AI and LLMs in particular to drive business outcomes, when it comes to actually deploying them as inferences in production, companies can witness performance gaps, where LLM responses are not 100% factually correct, due to the fact that the LLM generates text or performs tasks according to its vector database of which terms and concepts are related — regardless of truth. “There are a lot of variables that go into deploying generative AI products. For example: is your product a general-purpose tool that generates stories based on a simple prompt? Or is it an enterprise chatbot that helps customers answer common questions based on thousands of proprietary product documentation?” Atindriyo Sanyal, co-founder and CTO of Galileo, explained to VentureBeat. Today, enterprise teams use benchmarks to study model performance, but there’s no comprehensive measurement of how they hallucinate — until now. To address this challenge, Sanyal and team chose eleven popular open-source and closed-source LLMs of varying sizes (after surveying multiple LLM repos, leaderboards, and industry surveys) and evaluated each model’s likelihood to hallucinate against three common tasks: question and answer without retrieval augmented generation (RAG), question and answer with RAG and long-form text generation. “To test the LLMs across these task types, we found seven of the most popular datasets available today. These datasets are widely considered to be thorough and rigorous benchmarks and effectively challenge each LLM’s capabilities relevant to the task at hand. For instance, for Q&A without RAG, we utilized broad-based knowledge datasets like TruthfulQA and TriviaQA to evaluate how well these models handle general inquiries,” Sanyal explained. The Galileo team sub-sampled the datasets to reduce their size and annotated them to establish ground truth to check for the accuracy and reliability of outputs. Next, using the appropriate datasets, they tested each model at each task. The results were evaluated using the company’s proprietary Correctness and Context Adherence metrics. “These metrics make it easy for engineers and data scientists to reliably pinpoint when a hallucination has likely taken place. Correctness is focused on capturing general logical and reasoning-based mistakes and was used to evaluate Q&A without RAG and long-form text generation task types. Meanwhile, Context Adherence measures an LLM’s reasoning abilities within provided documents and context and was used to evaluate Q&A with RAG,” the CTO noted. How did the models do? When handling questions and answers without retrieval, where the model relies on its internal knowledge and learnings to provide responses, OpenAI’s GPT family stood out from the crowd. The GPT-4-0613 model received a correctness score of 0.77 and was followed by GPT-3.5 Turbo-1106, GPT-3.5-Turbo-Instruct and GPT-3.5-Turbo-0613 with scores of 0.74, 0.70 and 0.70, respectively. In this category, only Meta’s Llama-2-70b came close to the GPT family with a score of 0.65. All other models lagged behind, especially Llama-2-7b-chat and Mosaic’s ML’s MPT-7b-instruct with scores of 0.52 and 0.40, respectively. For tasks related to retrieval, where the model pulls relevant information from a given dataset or document, GPT-4-0613 again came out as the top performer with a context adherence score of 0.76. But what’s more interesting is that GPT-3.5-turbo-0613 and -1106 also came very close and matched its performance with scores of 0.75 and 0.74, respectively. Hugging Face’s open-source model Zephyr-7b even performed well with a score of 0.71 and surpassed Meta’s much larger Llama-2-70b (score = 0.68). Notably, the biggest room for improvement was found in UAE’s Falcon-40b and Mosaic ML’s MPT-7b, which obtained scores of 0.60 and 0.58, respectively. Finally, for generating long-form texts, such as reports, essays and articles, GPT-4-0613 and Llama-2-70b obtained correctness scores of 0.83 and 0.82, respectively, showing the least tendency to hallucinate. GPT-3.5-Turbo-1106 matched Llama while the 0613 variant followed with a score of 0.81. In this case, MPT-7b trailed behind with a score of 0.53. Opportunity balance performance with cost While OpenAI’s GPT-4 stays on top for all tasks, it is important to note that OpenAI’s API-based pricing for this model can easily drive up costs. As such, Galileo recommends, teams can go for closely following GPT-3.5-Turbo models to get nearly as good performance without spending too much. In some cases, like text generation, open-source models like Llama-2-70b can also help balance performance and cost. That said, it is important to note that this is an evolving index. New models are cropping on a weekly basis and existing ones are improving over time. Galileo intends to update this index on a quarterly basis to give teams an accurate analysis ranking the least to most hallucinating models for different tasks. “We wanted to give teams a starting point for addressing hallucinations. While we don’t expect teams to treat the results of the Hallucination Index as gospel, we do hope the Index serves as an extremely thorough starting point to kick-start their Generative AI efforts. We hope the metrics and evaluation methods covered in the Hallucination Index arm teams with tools to more quickly and effectively evaluate LLM models to find the perfect LLM for their initiative,” Sanyal added. VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings.",
    "originSummary": [
      "Galileo, a research arm in San Francisco, has created a hallucination index to evaluate the performance of large language models (LLMs) in various tasks.",
      "OpenAI's GPT-4 model is ranked as the best performer, showcasing the least amount of hallucination when faced with multiple tasks.",
      "The index aims to assist enterprises in addressing the challenge of hallucination, allowing for the deployment of LLMs in critical sectors like healthcare. Galileo plans to update the index quarterly to monitor different models' performance."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "Martian secures $9 million in seed funding for innovative AI model-routing platform",
    "originLink": "https://itbrief.co.nz/story/seed-funding-secured-for-martian-ai-venture-which-routes-llm-requests",
    "originBody": "Seed funding secured for Martian AI venture which routes LLM requests Yesterday # AI # Investment # LLMs # Operations # Savings # Transformers By Sean Mitchell, Publisher Martian, a pioneering AI model-routing platform, has emerged from stealth with its groundbreaking Model Router, promising to revolutionize the way individual queries are handled by routing them to the most efficient Large Language Models (LLMs) in real time. This innovation, underpinned by Martian's unique \"Model Mapping\" interpretability technique, challenges existing AI models, including the highly regarded GPT-4. Martian's Model Router operates as an orchestration layer solution, routing each query to the best LLM, thereby achieving superior performance and lower costs. The underlying technology, Model Mapping, transforms LLMs from complex black boxes into a more interpretable architecture. This marks the first commercial application of what the company calls \"mechanistic interpretability\". The company's success has attracted significant investment, with a $9 million seed round of funding coming from prominent venture capital firms such as NEA, Prosus Ventures, Carya Venture Partners, and General Catalyst. Aaron Jacobson, a partner at NEA, highlighted the importance of Martian's work, stating, “All the effort being put into AI development is wasted if it’s unwieldy, cost-prohibitive and uncharted for enterprise and everyday users. We believe Martian will unlock the power of AI for companies and people en masse.\" Shriyash Upadhyay, co-founder of Martian, expressed his vision for the future of AI, saying, “Our goal is to consistently deliver such breakthroughs until AI is fully understood and we have a theory of machine intelligence as robust as our theories of logic or calculus.” Martian's unique approach involves converting AI models from numbers in matrices to human-readable programs, allowing for a deeper understanding of these models. This process enables Martian to build tools based on this newfound understanding. Martian's Model Router is the first tool built using Model Mapping. With over 380,000 open-source AI models available, the Router offers a unique opportunity for companies to optimize their AI systems. Etan Ginsberg, Martian's co-founder, explained, “Routing between models is fundamentally about understanding how they work: what makes them succeed or fail.” The Model Router offers several benefits to enterprises, including increased performance, reduced total cost of ownership, and futureproofing through continual optimization and dynamic updates of models. This enables Martian to outperform any single LLM on key benchmarks. Javier Valverde, investor at Prosus Ventures, praised the founders, saying, “Shriyash and Etan are among the best founders we’ve met globally in AI. They are offering a scalable solution to this question.” Quentin Clark, Managing Director of General Catalyst, emphasized the need for platforms that enable effective leverage of AI models in enterprises. Follow us on: Related stories Predicted growth in cloud-based Electronic Laboratory Notebook market ESET consolidates consumer cybersecurity offerings with new HOME subscriptions Storyblok & Bynder partnership to enhance content management precision Rapid AI adoption prompts urgent reassessment of data infrastructure Cybersecurity firm Cequence joins Microsoft's Security Copilot Private Preview Top stories Microsoft Fabric: To revolutionise the data landscape The future of omnichannel: Your success blueprint in a digital world Technical debt noted as major hurdle for CTOs by 2024, STX Next reports ControlUp enhances digital employee experience with new Microsoft features The future of finance in an uncertain world",
    "originSummary": [
      "AI model-routing platform, Martian, has raised $9 million in seed funding from NEA, Prosus Ventures, Carya Venture Partners, and General Catalyst.",
      "Martian's Model Router utilizes \"Model Mapping\" to route queries to the most efficient Large Language Models (LLMs) in real-time, resulting in improved performance and cost savings.",
      "The Model Router is the first tool developed using Model Mapping, providing increased performance, reduced costs, and continual optimization and dynamic updates for AI models. The scalable solution is seen as enabling effective utilization of AI models in enterprises."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "NVIDIA Enhances Local AI Computing with TensorRT-LLM Update, Adding Support for OpenAI's Chat API.",
    "originLink": "https://www.windowscentral.com/software-apps/nvidia-adds-support-for-openais-chat-api-to-its-latest-gpus-heres-why-its-its-a-big-deal",
    "originBody": "Software Apps NVIDIA adds support for OpenAI's Chat API to its latest GPUs. Here's why it's it's a big deal. By Colton Stradling published 15 November 2023 During Microsoft's Ignite conference, NVIDIA announced updates to TensorRT-LLM. Comments (0) Nvidia's TensorRT-LLM plans to optimize LLM models for deployment. (Image credit: NVIDIA) What you need to know TensorRT-LLM is adding OpenAI's Chat API support for desktops and laptops with RTX GPUs starting at 8GB of VRAM. Users can process LLM queries faster and locally without uploading datasets to the cloud. NVIDIA pairs this with \"Retrieval-Augmented Generation\" (RAG), allowing more bespoke LLM use cases. During Microsoft's Ignite conference today, NVIDIA announced an update to their TensorRT-LLM, which launched in October. The main announcements today are that the TensorRT-LLM feature is now gaining support for LLM APIs, specifically OpenAI Chat API, which is the most well-known at this point, and also that they have worked to improve performance with TensorRT-LLM to get better performance per token on their GPUs. There is a tertiary announcement that is quite interesting also. NVIDIA is going to include Retrieval-Augmented Generation with the TensorRT-LLM. This allows an LLM to use an external data source for its knowledge base rather than relying on anything online—a highly demanded feature for AI. What is TensorRT-LLM? READ MORE FROM IGNITE 2023 - Microsoft is making its own Arm chips - Copilot comes to all of Microsoft 365 - Bing Chat rebranded to Copilot - Microsoft Loop now generally available - Microsoft Mesh and Immersive Spaces - Microsoft Planner merges To Do and Project - Microsoft launches Copilot Studio - Microsoft Security Copilot - Copilot web app goes live NVIDIA recently rolled out NVIDIA TensorRT-LLM, an open-source library that allows for local computing of LLMs on NVIDIA hardware. NVIDIA touts this to gain privacy and efficiency when dealing with large datasets or private information. Whether that information is sent through an API like OpenAI's Chat API is secure. You can learn more about NVIDIA TensorRT-LLM at NVIDIA's developer site. The changes announced today to NVIDIA TensorRT-LLM are the addition of OpenAI's Chat API and performance improvements for previously supported LLMs and AI models like Llama 2 and Stable Diffusion through DirectML enhancements. This technology and computing can be done locally through NVIDIA's AI Workbench. This \"unified, easy-to-use toolkit allows developers to quickly create, test, and customize pre-trained generative AI models and LLMs on a PC or workstation.\" NVIDIA has an early access sign-up page for those interested in using it. NVIDIA TensorRT-LLM is an open-source library that accelerates and optimizes inference performance of the latest large language models (LLMs) on the NVIDIA AI platform NVIDIA Nvidia is also showing an improvement to per token performance for LLMs as we can see in these in-house NVIDIA benchmarks. As always, be wary of manufacturer benchmarks and testing for accurate reporting of performance gain. (Image credit: NVIDIA) Now that we know NVIDIA's TensorRT-LLM, why is this special or useful? For the most part, running locally on an NVIDIA-powered workstation or PC will likely result in the same answers to queries, albeit likely at a slower pace due to the lack of cloud computing power. NVIDIA's picture for this use case comes together when discussing the other announcement today from NVIDIA, namely the integration with a new technology or feature called Retrieval-Augmented Generation. What is Retrieval-Augmented Generation The term retrieval-augmented generation was coined in a paper by a series of authors, with the lead author being Patrick Lewis. It is the name being adopted by the industry for a solution to a problem everybody who has used an LLM has encountered. Out-of-date or information that is correct but erroneous in the context of the discussion. The in-depth details of how RAG works can be found in one of NVIDIA's Technical Briefs. Retrieval-augmented generation is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources. Rick Merritt By pairing retrieval-augmented generation with NVIDIA's TensorRT-LLM, end users can customize what information the LLM has access to when it runs its queries. ChatGPT recently announced custom GPTs that could offer similar results. As was discussed in our article around custom GPTs, the ability to make bespoke, single-purpose LLM instances with either a custom GPT or, in this case, an LLM instance that, using retrieval-augmented generation, only has access to all of the published works of Charles Dickens and nothing else, could assist in creating purpose-built, meaningful and accurate LLMs for different use cases. Will TensorRT-LLM be useful? What does this all mean together? There are some real opportunities for this to be used meaningfully. How easy will it be to implement, or how safe will the data be? Only time will tell. There is potential here for AI improvements, especially at an enterprise level, to improve workflows, offer more convenient access to complicated information, and assist employees with challenging tasks. Even though these tasks will be run locally, they will still go through the normal LLM APIs, which will face the same content restrictions and limitations they do now. However, as technologies like NVIDIA's TensorRT-LLM make it faster to use an LLM offline, somebody could integrate it with something like EvilGPT, which has no limitations on its conduct and is currently being used to craft malware and assist in cyber attacks, the potential for an AI performing some real damage only amplifies. What do you think about NVIDIA's updates to TensorRT-LLM? Can you think of any uses for it that I missed? Let us know in the comments. Get the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Colton Stradling Contributor Colton is a seasoned cybersecurity professional that wants to share his love of technology with the Windows Central audience. When he isn’t assisting in defending companies from the newest zero-days or sharing his thoughts through his articles, he loves to spend time with his family and play video games on PC and Xbox. Colton focuses on buying guides, PCs, and devices and is always happy to have a conversation about emerging tech and gaming news. TOPICS SURGE-WC MICROSOFT COPILOT CATEGORIES Software Apps SEE ALL COMMENTS (0) No comments yet Comment from the forums MOST READ 1 This new Windows 11 setting will 'prevent Phone Link from communicating with your mobile devices' entirely — and you can test it right now 2 ChatGPT's new code interpreting tool could become a hacker's paradise. Here's how. 3 NVIDIA adds support for OpenAI's Chat API to its latest GPUs. Here's why it's it's a big deal. 4 \"We'll see you at The Game Awards for the World Premiere of the exact release date,\" Larian Studios says about Baldur's Gate 3 on Xbox 5 Fences 5 update will turn your Windows 11 desktop into a chameleon",
    "originSummary": [
      "NVIDIA has released updates to its TensorRT-LLM at Microsoft's Ignite conference, featuring support for OpenAI's Chat API.",
      "Users can now process LLM queries locally, eliminating the need to upload datasets to the cloud.",
      "The update also introduces Retrieval-Augmented Generation (RAG), enabling LLMs to utilize external data sources as knowledge bases, enhancing privacy, efficiency, and customization options for LLM users."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "Microsoft and Nvidia team up to simplify AI model deployment on Windows",
    "originLink": "https://www.theverge.com/2023/11/15/23960471/microsoft-windows-ai-studio-nvidia-developers",
    "originBody": "Microsoft/ Tech/ Windows Microsoft and Nvidia are making it easier to run AI models on Windows Microsoft and Nvidia are making it easier to run AI models on Windows / Microsoft’s new Windows AI Studio lets developers access and configure AI models, such as Microsoft’s Phi, Meta’s Llama 2, and Mistral. By Emma Roth, a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Nov 15, 2023, 4:00 PM UTC| Share this story Illustration by Alex Castro / The Verge Microsoft and Nvidia want to help developers run and configure AI models on their Windows PCs. During the Microsoft Ignite event on Wednesday, Microsoft announced Windows AI Studio: a new hub where developers can access AI models and tweak them to suit their needs. Windows AI Studio allows developers to access development tools and models from the existing Azure AI Studio and other services like Hugging Face. It also offers an end-to-end “guided workspace setup” with model configuration UI and walkthroughs to fine-tune various small language models (SLMs), such as Microsoft’s Phi, Meta’s Llama 2, and Mistral. Image: Microsoft Windows AI Studio lets developers test the performance of their models using Prompt Flow and Gradio templates as well. Microsoft says it’s going to roll out Windows AI Studio as a Visual Studio Code extension in the “coming weeks.” Nvidia, similarly, revealed updates to TensorRT-LLM, which the company initially launched for Windows as a way to run large language models (LLMs) more efficiently on H100 GPUs. However, this latest update brings TensorRT-LLM to PCs powered by GeForce RTX 30 and 40 Series GPUs with 8GB of RAM or more. Additionally, Nvidia will soon make its TensorRT-LLM compatible with OpenAI’s Chat API through a new wrapper. This will allow developers to run LLMs locally on their PCs, which is ideal for those who are concerned about storing private data in the cloud. Nvidia says its next TensorRT-LLM 6.0 release will add up to five times faster inference, as well as support for the new Mistral 7B and Nemotron-3 8B models. This is all a part of Microsoft’s goal to create a “hybrid loop” development pattern, which is supposed to enable AI development across the cloud and locally on devices. With this concept, developers don’t have to rely solely on their own systems to power AI development, as they can access Microsoft’s cloud servers to take the brunt of the load off their devices. Most Popular The incredible shrinking heat pump Microsoft is finally making custom chips — and they’re all about AI Google’s new Titan security keys are ready for a world without passwords Microsoft officially launches Loop, its Notion competitor Sonos teases a major new product coming next year Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "originSummary": [
      "Microsoft has launched Windows AI Studio, a platform that allows developers to access and customize AI models on Windows.",
      "Nvidia has updated TensorRT-LLM to improve the efficiency of running large language models on PCs using GeForce RTX GPUs.",
      "These initiatives aim to promote AI development across cloud and local devices, reducing dependency on individual systems."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "OPPO Launches AndesGPT AI Language Model and ColorOS 14",
    "originLink": "https://www.shine.cn/biz/company/2311166993/",
    "originBody": "Biz / Company OPPO debuts its own 'GPT' model in Shanghai Zhu Shenshen Zhu Shenshen 20:37 UTC+8, 2023-11-16 0 OPPO, with over 600 million daily active users globally, released its own AI large language model, or LLM, and the next-generation operating system in Shanghai on Thursday, Zhu Shenshen Zhu Shenshen 20:37 UTC+8, 2023-11-16 0 Zhu Shenshen / SHINE OPPO launched its \"GPT\" model in Shanghai on Thursday. Chinese electronics giant OPPO released its own artificial intelligence large language model, or LLM, and the next-generation operating system in Shanghai on Thursday, making generative AI services accessible to more people, including millions of mobile phone users. The AI LLM, called AndesGPT, is integrated in OPPO's new ColorOS 14 system, which also made its debut at the OPPO Developer Conference 2023 in Shanghai on Thursday. OPPO's LLM has technical characteristics such as dialogue enhancement, personality assistance, and on-device and cloud collaboration, with core capabilities in four directions: knowledge, memory, tools and creation, the company said. The name AndesGPT reminds people of ChatGPT, or chat generative pre-trained transformer, a buzzy and popular LLM-based chatbot developed by OpenAI. OPPO, along with Chinese smartphone brand Vivo, aims to expand ChatGPT-like services in the cellphone sector, which covers more consumers and brings higher frequency usage. Vivo launched its X100 smartphone with on-device generative AI capabilities and features on Monday in Beijing. It further \"breaks down the barriers between data and services, and brings a new human-centric, intelligent and smooth new experience,\" OPPO said. Meanwhile, OPPO also announced that it will set up a joint laboratory with the University of Science and Technology of China for AI research and cooperation programs with 45 universities globally. Currently, OPPO has 600 million daily active users, with 320,000 developers globally. Ti Gong OPPO now has 600 million daily active users globally. Source: SHINE Editor: Yang Meiping Special Reports Building an Innovation Hub China International Import Expo Around The Delta Trash Talk Local Lingo Livable, Lovable Shanghai EXPLORE SHINE News In Focus Metro Nation World Sport Biz Economy Tech Auto Company Property Finance Event Video Feature Art & Culture Travel Lifestyle Taste Entertainment Wellness MENTAL Book Education iDEALShanghai Opinion Regions Special Projects Follow Us About UsContact UsFeedbackPrivacy PolicyTerms of Use 沪ICP证：沪ICP备05050403号-5互联网新闻信息服务许可证: 31120180004网络视听许可证：0909346广播电视节目制作许可证：沪字第354号增值电信业务经营许可证：沪B2-20120012 Copyright 2023 © Shanghai Daily. All Rights Reserved. Hotline: 8621-52920043 沪公网安备 31010602001940号 Copyright 2023 © Shanghai Daily All Rights Reserved. 沪ICP证：沪ICP备05050403号-5互联网新闻信息服务许可证: 31120180004网络视听许可证：0909346广播电视节目制作许可证：沪字第354号增值电信业务经营许可证：沪B2-20120012 沪公网安备 31010602001940号 × Scan to follow SHINE's official Wechat account. CLOSE Search Subscribe Thank you for subscription (5s) News In Focus Metro Nation World Sport Biz Economy Tech Auto Company Property Finance Event Video Opinion Feature Art & Culture Travel Lifestyle Taste Entertainment Wellness MENTAL Book Education iDEALShanghai Regions Baoshan Changning Chongming Huangpu Hongkou Jiading Jing'an Minhang Pudong Putuo Songjiang Xuhui Yangpu Qingpu Chengdu Hangzhou Ningbo Suzhou News In Focus Metro Nation World Sport Biz Economy Tech Auto Company Property Finance Event Opinion Feature Art & Culture Travel Lifestyle Taste Entertainment Wellness MENTAL Book Education iDEALShanghai Regions Baoshan Changning Chongming Huangpu Hongkou Jiading Jing'an Minhang Pudong Putuo Songjiang Xuhui Yangpu Qingpu Chengdu Hangzhou Ningbo Suzhou Video Special Projects Shanghai Daily PDF / Subscribe Shanghai Daily Archive APP Download About Us Contact Us Feedback Privacy Policy Terms of Use Top Open in App",
    "originSummary": [
      "OPPO has launched its own artificial intelligence large language model called AndesGPT, along with the latest operating system ColorOS 14.",
      "AndesGPT offers features like dialogue enhancement, personality assistance, and on-device and cloud collaboration.",
      "OPPO plans to expand its AI services in the cellphone sector to reach more users and increase usage frequency, and will collaborate with the University of Science and Technology of China as well as 45 universities worldwide."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "Streaming-LLM: Enhancing Language Models for Infinite-Length Inputs",
    "originLink": "https://www.kdnuggets.com/introduction-to-streaming-llm-llms-for-infinite-length-inputs",
    "originBody": "Blog Top Posts Submissions About Topics Artificial Intelligence Career Advice Computer Vision Data Engineering Data Science Language Models Machine Learning MLOps NLP News Programming Python SQL Datasets Education Certificates Courses Online Masters Resources Cheat Sheets Events Jobs Projects Publications Webinars Join Newsletter Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs A new approach for LLM in the real-time production. By Cornellius Yudha Wijaya, KDnuggets on November 15, 2023 in Language Models The large Language Model (LLM) has changed the way people work. With a model such as the GPT family that is used widely, everyone has gotten used to these models. Leveraging the LLM power, we can quickly get our questions answered, debugging code, and others. This makes the model useful in many applications. One of the LLM challenges is that the model is unsuitable for streaming applications because of the model's inability to handle long-conversation chat exceeding the predefined training sequence length. Additionally, there is a problem with the higher memory consumption. That is why these problems above spawn research to solve them. What is this research? Let’s get into it. StreamingLLM StreamingLLM is a framework established by Xiao et al. (2023) research to tackle the streaming application issues. The existing methods are challenged because the attention window constrains the LLMs during pre-training. The attention window technique might be efficient but suffers when handling texts longer than its cache size. That’s why the researcher tried to use the Key and Value states of several initial tokens (attention sink) with the recent tokens. The comparison of StreamingLLM and the other techniques can be seen in the image below. StreamingLLM vs Existing Method (Xiao et al. (2023)) We can see how StreamingLLM tackles the challenge using the attention sink method. This attention sink (initial tokens) is used for stable attention computation and combines it with recent tokens for efficiency and maintains stable performance on longer texts. Additionally, the existing methods suffer from memory optimization. However, LLM avoids these issues by maintaining a fixed-size window on the Key and Value states of the most recent tokens. The author also mentions the benefit of StreamingLLM as the sliding window recomputation baseline by up to 22.2× speedup. Performance-wise, StreamingLLM provides excellent accuracy compared to the existing method, as seen in the table below. StreamingLLM accuracy (Xiao et al. (2023)) The table above shows that StreamingLLM accuracy can outperform the other methods in the benchmark datasets. That’s why StreamingLLM could have potential for many streaming applications. To try out the StreamingLLM, you could visit their GitHub page. Clone the repository on your intended directory and use the following code in your CLI to set the environment. conda create -yn streaming python=3.8 conda activate streaming pip install torch torchvision torchaudio pip install transformers==4.33.0 accelerate datasets evaluate wandb scikit-learn scipy sentencepiece python setup.py develop Then, you can use the following code to run the Llama chatbot with LLMstreaming. CUDA_VISIBLE_DEVICES=0 python examples/run_streaming_llama.py --enable_streaming The overall sample comparison with StreamingLLM can be shown in the image below. StreamingLLM showed outstanding performance in more extended conversations (Streaming-llm) That’s all for the introduction of StreamingLLM. Overall, I believe StreamingLLM can have a place in streaming applications and help change how the application works in the future. Conclusion Having an LLM in streaming applications would help the business in the long run; however, there are challenges to implement. Most LLMs can’t exceed the predefined training sequence length and have higher memory consumption. Xiao et al. (2023) developed a new framework called StreamingLLM to handle these issues. Using the StreamingLLM, it is now possible to have working LLM in the streaming application. Cornellius Yudha Wijaya is a data science assistant manager and data writer. While working full-time at Allianz Indonesia, he loves to share Python and Data tips via social media and writing media. More On This Topic Machine Learning Is Not Like Your Brain Part 5: Biological Neurons… Falcon LLM: The New King of Open-Source LLMs How to Apply Transformers to Any Length of Text Web LLM: Bring LLM Chatbots to the Browser AI Infinite Training & Maintaining Loop Exploring Infinite Iterators in Python's itertools Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy <= Previous post Latest Posts Introduction to Streaming-LLM: LLMs for Infinite-Length Inputs A Better Way To Evaluate LLMs Make Your Own GPTs with ChatGPT’s GPTs! The Rise and Fall of Prompt Engineering: Fad or Future? KDnuggets News, November 15: 10 Essential Pandas Functions • 5 Free Courses to Master Data Science Everything you need to become a SAS Certified Machine Learning Engineer Top Posts 5 Free Courses to Master Data Science 7 Steps to Running a Small Language Model on a Local CPU Getting Started with Claude 2 API Back to Basics Week 2: Database, SQL, Data Management and Statistical Concepts Everything you need to become a SAS Certified Machine Learning Engineer The Rise and Fall of Prompt Engineering: Fad or Future? 10 Essential Pandas Functions Every Data Scientist Should Know 5 Free University Courses on Data Analytics 7 High Paying Side Hustles for Data Scientists Expert Insights on Developing Safe, Secure, and Trustworthy AI Frameworks Get the FREE ebook 'The Great Big Natural Language Processing Primer' and 'The Complete Collection of Data Science Cheat Sheets' along with the leading newsletter on Data Science, Machine Learning, AI & Analytics straight to your inbox. By subscribing you accept KDnuggets Privacy Policy © 2023 Guiding Tech MediaAboutContactPrivacy PolicyTerms of Service Subscribe To Our Newsletter (Get The Complete Collection of Data Science Cheat Sheets & Great Big NLP Primer ebook)",
    "originSummary": [
      "The article introduces a new framework called StreamingLLM that enables the use of large Language Models (LLMs) in streaming applications.",
      "LLMs have limitations in handling long conversations and memory consumption, which the StreamingLLM framework addresses through attention sink techniques and a fixed-size window for memory optimization.",
      "The framework showcases improved accuracy and performance compared to existing methods and offers instructions on how to implement StreamingLLM in streaming applications."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  },
  {
    "title": "Fireflies.ai Collaborates with OpenAI to Develop AI App Store",
    "originLink": "https://analyticsindiamag.com/andrew-ng-launches-a-new-course-on-llm-quality-and-security/",
    "originBody": "The OpenAI of Transcription Fireflies.ai, which works closely with OpenAI and is backed by Khosla Ventures, is planning to come up with its own AI App Store, similar to OpenAI’s GPT Store",
    "originSummary": [
      "Transcription startup Fireflies.ai plans to create an AI App Store similar to OpenAI's GPT Store.",
      "Fireflies.ai is collaborating with OpenAI and has received funding from Khosla Ventures.",
      "The AI App Store aims to provide a platform for developers to create and distribute AI-powered applications."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700143161999
  }
]
