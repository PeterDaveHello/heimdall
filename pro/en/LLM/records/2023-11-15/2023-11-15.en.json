[
  {
    "title": "Microsoft and Meta Dodging Questions on Paying Creators for AI's Use of Copyrighted Material",
    "originLink": "https://www.theregister.com/2023/11/15/house_of_lords_ai_copyright/",
    "originBody": "AI + ML 3 To pay or not to pay for AI's creative 'borrowing' – that is the question 3 One that Microsoft and Meta dodged during House of Lords committee Lindsay Clark Wed 15 Nov 2023 // 12:31 UTC In the UK's Parliament this week, Microsoft and Meta ducked the question of whether creators should be paid when their copyrighted material is used to train large language models. The tech titans, with combined revenues well in excess of $200 billion, were being grilled by the House of Lords Communications and Digital Committee when the copyright question came into focus. In September, the Authors' Guild, a trade association for published writers, and 17 authors filed a class-action lawsuit in the US over OpenAI's use of their material to create its LLM-based services. OpenAI CEO Sam Altman has since said the company would cover its clients' legal costs for copyright infringement suits rather than remove the material from its training sets. Microsoft has invested $13 billion in OpenAI. It has an extended partnership with the machine learning developer, powering its workloads on the Azure cloud platform and using its models to run the Copilot automated assistant. Speaking to the Lords yesterday, Owen Larter, director of public policy at Microsoft's Office of Responsible AI, said: \"It's important to appreciate what a large language model is. It's a large model trained on text data, learning the associations between different ideas. It's not necessarily sucking anything up from underneath.\" He said there should be a \"framework\" to provide some protection for copyrighted material and Microsoft would assume responsibility for any infringement by its LLM-based systems. But he also said Microsoft supports the recent Valance report into \"pro-innovation\" AI law in the UK which advocates for text and data exceptions in training models. But Donald Michael, Lord Foster of Bath, pressed Larter on whether he would accept that if a company uses copyrighted material to build an LLM for profit, the copyright owner should be reimbursed. The Microsoft director said: \"It's really important to understand that you need to train these large language models on large data sets if you're going to get them to perform effectively, if you're going to allow them to be safe and secure … There are also some competition issues [in making sure] that training of large models is available to everyone. If you go too far down a path where it's very hard to obtain data to train models, then all of a sudden, the ability to do so will only be the preserve of very large companies.\" Litigation is already under way to address how training data sets Books1, Books2, and Books3, which effectively pirate copyrighted material, have been used to help build popular LLMs. Google DeepMind's GraphCast AI weather predictor looks fascinating on paper but ... UnitedHealthcare's broken AI denied seniors' medical claims, lawsuit alleges YouTubers kindly asked to mark their deepfake vids as Fake Fakey McFake Fakes AI chemist creates catalysts to make oxygen using Martian meteorites Meta is behind the Llama 2 LLM, which scales up to 70 billion parameters. The social media giant has promoted the model as open source, although FOSS purists point to some caveats in its approach. Speaking to the Lords, Rob Sherman, vice president and deputy chief privacy officer for policy at Meta, said the company would comply with the law. But he added that \"maintaining broad access to information on the internet and information including for the use in innovation like this is quite important. I do support giving rights holders the ability to manage how their information is used. \"I'm a little bit cautious about the idea of forcing companies that are building AI to enter into bespoke agreements with individual rights holders or an order to pay for content that doesn't have economic value for them.\" Last week, Dan Conway, CEO of the UK's Publishers Association, told the committee that large language models were infringing copyrighted content on an \"absolutely massive scale.\" \"We know this in the publishing industry because of the Books3 database which lists 120,000 pirated book titles, which we know have been ingested by large language models,\" he said. \"We know that the content is being ingested on an absolutely massive scale by large language models. LLMs do infringe copyright at multiple parts of the process in terms of when they collect this information, how they store this information, and how they how they handle it. The copyright law is being broken on a massive scale.\" At the same hearing, Dr Hayleigh Bosher, reader in intellectual property law at Brunel University London, said she did not represent tech firms or content creators and offered up a neutral's perspective. \"The principle of when you need a licence and when you don't is clear,\" she said, \"and to make a reproduction of a copyright-protected work without permission would require a licence or would otherwise be infringement. That's what AI does at different steps of the process: The ingestion, the running of the program, and potentially even the output. \"Some AI and tech developers are arguing a different interpretation of the law. I don't represent either of those sides. I'm a copyright expert, and from my position, understanding of what copyright is supposed to achieve and how it achieves it, you would require a licence for that activity.\" ® Sponsored: Decreasing data warehouse downtime Share More about AI Copyright Government of the United Kingdom More like these × More about AI Copyright Government of the United Kingdom Meta Microsoft Narrower topics Active Directory Azure Bing BSoD Cabinet Office Competition and Markets Authority Computer Misuse Act DCMS Excel Exchange Server Facebook GCHQ Google AI GPT-3 HMRC HoloLens Home Office ICO Internet Explorer Large Language Model LinkedIn Machine Learning MCubed Microsoft 365 Microsoft Build Microsoft Edge Microsoft Ignite Microsoft Office Microsoft Surface Microsoft Teams NCSC .NET Neural Networks NHS NLP Office 365 Open Compute Project OS/2 Outlook Patch Tuesday Pluton RPA SharePoint Skype Software License SQL Server Star Wars Tensor Processing Unit Visual Studio Visual Studio Code Windows Windows 10 Windows 11 Windows 7 Windows 8 Windows Server Windows Server 2003 Windows Server 2008 Windows Server 2012 Windows Server 2013 Windows Server 2016 Windows Subsystem for Linux Windows XP Xbox Xbox 360 Broader topics Andrew McCollum Bill Gates Chris Hughes Dustin Moskovitz Eduardo Saverin Government Mark Zuckerberg Self-driving Car United Kingdom More about Share 3 COMMENTS More about AI Copyright Government of the United Kingdom More like these × More about AI Copyright Government of the United Kingdom Meta Microsoft Narrower topics Active Directory Azure Bing BSoD Cabinet Office Competition and Markets Authority Computer Misuse Act DCMS Excel Exchange Server Facebook GCHQ Google AI GPT-3 HMRC HoloLens Home Office ICO Internet Explorer Large Language Model LinkedIn Machine Learning MCubed Microsoft 365 Microsoft Build Microsoft Edge Microsoft Ignite Microsoft Office Microsoft Surface Microsoft Teams NCSC .NET Neural Networks NHS NLP Office 365 Open Compute Project OS/2 Outlook Patch Tuesday Pluton RPA SharePoint Skype Software License SQL Server Star Wars Tensor Processing Unit Visual Studio Visual Studio Code Windows Windows 10 Windows 11 Windows 7 Windows 8 Windows Server Windows Server 2003 Windows Server 2008 Windows Server 2012 Windows Server 2013 Windows Server 2016 Windows Subsystem for Linux Windows XP Xbox Xbox 360 Broader topics Andrew McCollum Bill Gates Chris Hughes Dustin Moskovitz Eduardo Saverin Government Mark Zuckerberg Self-driving Car United Kingdom TIP US OFF Send us news",
    "originSummary": [
      "Microsoft and Meta dodged questions regarding whether creators should be compensated for the use of their copyrighted material to train large language models during a House of Lords hearing.",
      "The issue was raised following a class-action lawsuit against OpenAI by the Authors' Guild and authors for unauthorized use of their material.",
      "Microsoft, an investor in OpenAI, supports text and data exceptions in model training, while Meta is cautious about forcing companies to enter agreements or pay for content that has no economic value to them."
    ],
    "commentBody": "",
    "commentSummary": ["-"],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "Upstage and Qanda Collaborate to Develop Math-Specific LLM, Challenging Global Math Solver AIs",
    "originLink": "https://www.kedglobal.com/artificial-intelligence/newsView/ked202311150013",
    "originBody": "Skip to content KOSPI 2486.67 +53.42 +2.20% KOSDAQ 809.36 +15.17 +1.91% KOSPI200 333.96 +7.39 +2.27% USD/KRW 1302 -6 -0.46% JPY100/KRW 865.84 -4.91 -0.56% EUR/KRW 1414.88 -8.94 -0.63% CNH/KRW 179.7 -0.72 -0.4% STOP View Market Snapshot KED Global - The Korea Economic Daily Global Edition Open Menu Search SIGN IN JOIN Companies Companies Home Latest on Korean Startups Future Unicorns Hidden Champions Branded Content Get It Made In Korea Companies A to Z Korean Investors Korean Investors Home Who's Who in Korean Investors Asset Owners Report Asset Managers' Perspectives Best Asset Managers by Korean Investors RFP Alert Deals M&A ECM/DCM Markets Culture & Trends Perspectives Hidden Champions Future Unicorns Open Menu Artificial intelligence Upstage to develop math-specific LLM with Qanda This would be the S.Korean AI startup’s first math-specific LLM, which is also the country’s first of its kind By See-Eun Lee 6 HOURS AGO 2 Min read see@hankyung.com Most Read Saudi Aramco’s $7 bn Korea project mired by labor shortage S.Korea's LS Materials set to boost earnings ahead of IPO process Samsung to unveil 3D AI chip packaging tech SAINT to rival TSMC Samsung to continue DRAM output cut until year-end; NAND recovery slow SK Hynix bets on DRAM upturn with $7.6 bn spending; HBM in focus Related Articles Upstage to develop shopping-specific AI with ConnectWave S.Korea's KT invests in domestic startups Upstage, Qanda S.Korean LLM by Upstage beats global benchmark ChatGPT Math app Qanda enjoys surge in popularity with 74 mn subscribers (Courtesy　of　Upstage) South Korean artificial intelligence technology startup Upstage will challenge global math solver AIs such as OpenAI’s ChatGPT and Google’s Photomath with its own math-specific large language model (LLM), which would be the first of its kind in the country. Upstage announced on Wednesday that it has agreed with Masspresso, the operator of Korean AI-powered learning platform Qanda, to develop a mathematics domain-specific private LLM. This is part of the two AI startups’ partnership with Korea’s telecom major KT Corp., which in September invested 10 billion won ($7.7 million) each in the AI startups to enhance its hyperscale AI capability. KOREA’S FIRST MATH-SPECIFIC LLM Domain-specific LLMs are designed to perform well-defined tasks by conveying the most significant features of a certain industry with an understanding of its unique jargon, context and intricacies. Compared with a generic language model weak at a clear understanding of specific terminology, these specialized models can enhance user experiences and unlock unprecedented value for enterprises across sectors, and more AI companies are rushing to develop domain-specific LLMs on growing demand. Qanda　is　popular　among　students　in　Vietnam　(Courtesy　of　Mathpresso) Under the agreement, Upstage will develop and fine-tune the math-specific LLM, while Qanda will provide its mathematics data, on which Upstage’s AI will train. Upstage said it will develop a math-specific LLM that will outperform not only generic LLMs with less hallucination but also other math solver AIs such as OpenAI’s ChatGPT, Google’s Photomath and Israeli startup’s Symbolab in solving math problems. The Korean startup said the country’s first math solver LLM trained on Qanda’s large-scale math data will be able to understand complex mathematical formulas and concepts that are difficult to explain in natural language. In July, Upstage’s language model trained with Meta’s LLaMA 2 topped the HuggingFace Open LLM Leaderboard, a standard for open-source LLM evaluation of over 500 LLMs, after beating OpenAI’s ChatGPT powered with GPT-3.5. AI-powered Qanda allows its users to get answers and solutions from the photos of math or science problems uploaded by the users, and the app is popular not only in Korea but also in other countries such as Vietnam. (Courtesy　of　Upstage) PRIVATE LLM EXPANSION With a math-specific LLM, which would be also the Korean startup’s first domain-specific private LLM in the education sector, Upstage expects it will accelerate the expansion of its private LLM applications across industries. A private LLM boasts a strong data protection and security system as it is specifically trained on a private internal dataset of a company. It is actively sought after because it is said to have less AI hallucination risk of generating misleading information presented as accurate. With its own LLM Solace, Upstage has joined hands with online e-commerce ConnectWave, Lotte Shopping Co. and Metaverse Entertainment to develop private LLMs that can be applied to their respective businesses. The Korean AI startup also plans to develop domain-specific private LLMs for the financial and manufacturing sectors. Write to See-Eun Lee at see@hankyung.com Sookyung Seo edited this article. #Upstage #Qanda #Masspresso #KT #domain specific LLM #large language model #private LLM #ChatGPT #Photomath #LLaMA 2 #Solace More to Read Artificial intelligence Upstage to develop shopping-specific AI with ConnectWave Sep 12, 2023 (Gmt+09:00) 1 Min read Artificial intelligence S.Korea's KT invests in domestic startups Upstage, Qanda Sep 11, 2023 (Gmt+09:00) 1 Min read Artificial intelligence S.Korean LLM by Upstage beats global benchmark ChatGPT Aug 01, 2023 (Gmt+09:00) 3 Min read Korean startups Math app Qanda enjoys surge in popularity with 74 mn subscribers Oct 11, 2022 (Gmt+09:00) 1 Min read Comment 0 LOG IN 0/300 THE KOREA ECONOMIC DAILY GLOBAL EDITION BRANDED CONTENT Companies Companies Home Latest on Korean Startups Future Unicorns Hidden Champions Branded Content Get It Made In Korea Companies A to Z Korean Investors Korean Investors Home Who's Who in Korean Investors Asset Owners Report Asset Managers' Perspectives Best Asset Managers by Korean Investors RFP Alert Deals M&A ECM/DCM Markets Culture & Trends Perspectives Hidden Champions Future Unicorns Follow us Facebook Twitter Linkedin Family Site The Korea Economic Daily The Korea Economic TV The Korea Economic Daily Magazine Hankyung BP Ten Asia About us Help Center Contact us Advertise with KED Global Terms of Service Privacy Policy Copyright Policy Editorial Policy & Code of Practice Online newspaper registration No: 서울, 아53319 Date of registration: Oct 7 2020 Publisher: Park HaeyoungAddress: 13F The Korea Economic Daily Bldg., 463, Cheongpa-ro, Jung-gu, Seoul, Korea 04505Business registration No: 796-87-01625Contact: kedglobal@hankyung.com, +82-2-360-4319KED Global ENGKED Global 中文(Chinese)KED Global 日本語(Japanese)SitemapCompany listNews topic Copyright© 2023 KED Global News Network . All Rights reserved Cookies on KED Global We use cookies to provide the best user experience. By continuing to browse this website, you will be considered to accept cookies. Please review our Privacy Policy to learn our cookie policy. KED Global Privacy Policy Accept & continue KONOMY by the korea economic daily Close Menu SIGN IN JOIN Companies Companies Home Latest on Korean Startups Future Unicorns Hidden Champions Branded Content Get It Made In Korea Companies A to Z Korean Investors Deals Markets Culture & Trends Perspectives Hidden Champions Future Unicorns Follow us ×",
    "originSummary": [
      "South Korean AI startup Upstage is collaborating with Masspresso to develop a math-specific large language model (LLM), aiming to compete with global math solver AIs like OpenAI's ChatGPT and Google's Photomath.",
      "Upstage plans to prioritize data protection and security for its private LLM and explore diverse industry applications.",
      "This collaboration is part of Upstage's partnership with Korean telecom major KT Corp. to strengthen its hyperscale AI capability."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "Former Twitter Executives Launch Freeplay: AI Development Platform for Incorporating Language Models in Products",
    "originLink": "https://thenewstack.io/freeplay-new-llm-dev-tool-for-java-developers-and-others/",
    "originBody": "ARCHITECTURE Cloud Native Ecosystem Containers Edge Computing Microservices Networking Serverless Storage ENGINEERING AI Frontend Development Software Development TypeScript WebAssembly Cloud Services Data Security OPERATIONS Platform Engineering Operations CI/CD Tech Life DevOps Kubernetes Observability Service Mesh CHANNELS Podcasts Ebooks Events Newsletter TNS RSS Feeds THE NEW STACK About / Contact Sponsors Sponsorship Contributions PODCASTS EBOOKS EVENTS NEWSLETTER ARCHITECTURE ENGINEERING OPERATIONS AI / DEVOPS / LARGE LANGUAGE MODELS Freeplay: New LLM Dev Tool for Java Developers (and Others) Two former Twitter Developer Platform execs have launched an AI development platform called Freeplay. It's more than observability, they say. Nov 14th, 2023 9:31am by Richard MacManus ANNUAL READER SURVEY We are planning our coverage for 2024 and we need your input! Take 5 minutes to chime in and let us know what you're interested in. BEGIN THE SURVEY We'd love to hear how we can make TNS better for you. Freeplay is a new LLM development platform (a sentence I have repeated multiple times this year). It’s also been given the “Figma of…” label by one of its investors, who called it “the Figma of LLM development.” So we’re two for two so far on the dev tools hype index. But let’s step back: what is Freeplay exactly? To find out, I spoke to co-founder and CEO Ian Cairns, who used to work on Twitter’s developer platform before that company was acquired by Elon Musk. Cairns told me that he and his co-founder Eric Ryan (who also came from Twitter) built Freeplay “to help product development teams make use of LLMs in their products.” Currently Freeplay helps with testing, experimentation, monitoring, and prompt management. “That cocktail works together to help people through the software development lifecycle,” said Cairns. “From when you’re at a prototyping stage, to when you’re testing, to make sure it’s ready to go to production, and then eventually — when you’re live — helping you know what’s happening at scale in your system, and then find ways to improve it.” Similar to Gradient, the most recent LLM development platform I’ve profiled, Freeplay is targeting enterprise software developers. “Most of our customers have been incumbent software companies,” said Cairns. “You know, they’re not AI-first startups. They’re companies that are trying to adopt LLMs, where they have an established business, an established customer base. They are great software developers, but they probably haven’t been working with LLMs before — and maybe not even ML [machine learning].” Bringing Java to AI World Freeplay offers developer SDKs for Python, Node and Java. Cairns claimed that the Java SDK is unique in the AI engineering space currently. TRENDING STORIES “A lot of the AI startup community are either gathering around Python, because there’s traditionally a lot of ML and AI tooling in Python world, or Node — or JavaScript generally — because they’re building React apps and you can do server-side and frontend work with JavaScript. The interesting thing about us: I haven’t seen someone in this space who’s built an SDK for the JVM.” He said that many established software companies are, for example, a “Kotlin shop” — and Kotlin is compatible with the JVM. He added that when he worked at Twitter, they were a “Scala shop” (Scala being another JVM-compatible language). “You might not want to adopt a new programming language just to make an API call to a language model,” he said. “And that’s what we found, is a lot of established companies are just integrating with the software that they’ve been writing for years — they’re not going out and pulling some new AI framework off the shelf.” Data Flywheel: More than Observability I mentioned that, at first glance, Freeplay reminded me of Humanloop, which describes itself as a “collaborative playground” where developers can test out and deploy prompts. I asked Cairns if that’s a fair comparison? He replied that it’s “a solid comp,” but that Freeplay tends to be used by their customers “after they’re already live in production” — the implication being that it’s less of a “playground” than perhaps Humanloop is. Freeplay in action I observed that both Freeplay and Humanloop include testing and monitoring capabilities, so they both seem similar to a traditional DevOps observability platform. Cairns somewhat agreed, but he noted that the needs of an LLM application are different to that of a traditional application. “In traditional observability, a lot of the goal is just — hey, what’s happening,” he explained. “But with ML systems, and I think this is true with LLMs, that observability needs — what’s happening — is still there, but it’s actually part of this data flywheel, that helps you start to make the product better in a different way when you’re doing ML.” The point is that because the quality of an LLM app relies on the underlying data (or how well that data is queried), anything from the software development cycle that helps optimize the application is fed back into the LLMs (or into the prompts/queries). As Cairns put it, Freeplay is “not just an observability platform — observability is a feature that helps with that bigger optimization loop.” The Figma Comparison Similar to a low-code platform, Freeplay is designed to be used by both developers and product or business people. This is where the Figma comparison comes in. It’s a tool that enables professional developers to collaborate with stakeholders in the business, using a web frontend. I asked Cairns who is typically driving this process — the developers or product managers. “The people that are doing the initial setup have been […] tech leadership,” he replied. “So whether it’s a CTO or an engineering director, you know, they are people who are saying, hey, we want to help give our teams better tools.” Where developers come in is with the implementation of a project. “Developers certainly start the work with Freeplay, to do an integration and continue to use it,” Cairns said. “But the way it works is you drop our SDK into an application that you’re building. And we start to manage prompts, like a server-side experiment.” He compared it to tools like Amplitude or LaunchDarkly, which let you do A/B testing — “where a product manager can enable an experiment.” So once a developer has set up the system, product managers can do that kind of experimentation or testing. Comparing OpenAI’s Developer Platform to Twitter’s Lastly, I asked Cairns if he and his co-founder Eric Ryan had any learnings from their time working on the Twitter Developer Platform. Both arrived at Twitter in 2014 due to its acquisition that year of Gnip, a social media API aggregation company. Twitter, as some of you will recall, had the potential to be a massive app development platform — but they screwed it up in the early 2010s by kneecapping third-party developers. Cairns was careful not to talk too much about that, but he did draw an interesting analogy with the position OpenAI is in now. “I think OpenAI actually did something really well that Twitter missed an opportunity [to do] back in 2010 before we were there. Which was, they did make a big change yesterday — where it seems like they’re competing with people who have been building other separate chatbots or agents, but they’re also giving them this app store and this opportunity to come [and] monetize on their platform. I thought they did a good job there.” Richard MacManus is a Senior Editor at The New Stack and writes about web and application development trends. Previously he founded ReadWriteWeb in 2003 and built it into one of the world’s most influential technology news sites. From the early... Read more from Richard MacManus SHARE THIS STORY TRENDING STORIES TNS owner Insight Partners is an investor in: LaunchDarkly. THE NEW STACK UPDATE A newsletter digest of the week’s most important stories & analyses. SUBSCRIBE The New stack does not sell your information or share it with unaffiliated third parties. By continuing, you agree to our Terms of Use and Privacy Policy. ARCHITECTURE Cloud Native Ecosystem Containers Edge Computing Microservices Networking Serverless Storage ENGINEERING AI Frontend Development Software Development TypeScript WebAssembly Cloud Services Data Security OPERATIONS Platform Engineering Operations CI/CD Tech Life DevOps Kubernetes Observability Service Mesh CHANNELS Podcasts Ebooks Events Newsletter TNS RSS Feeds THE NEW STACK About / Contact Sponsors Sponsorship Contributions FOLLOW TNS roadmap.sh Community created roadmaps, articles, resources and journeys for developers to help you choose your path and grow in your career. Frontend Developer Roadmap Backend Developer Roadmap Devops Roadmap © The New Stack 2023 Disclosures Terms of Use Privacy Policy Cookie Policy FOLLOW TNS TNS DAILY SUBSCRIBE",
    "originSummary": [
      "Freeplay is an AI development platform created by former Twitter Developer Platform executives, aimed at helping product development teams incorporate large language models (LLMs) into their products.",
      "The platform offers testing, experimentation, monitoring, and prompt management tools, catering to enterprise software developers.",
      "Freeplay provides developer SDKs for Python, Node, and Java, with the Java SDK being unique in the AI engineering space. It is designed to optimize LLM applications and includes features beyond observability to improve LLM-based products."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "Dell and Hugging Face partner to simplify deployment of large language models for enterprises",
    "originLink": "https://venturebeat.com/ai/dell-and-hugging-face-partner-to-simplify-llm-deployment/",
    "originBody": "VentureBeat presents: AI Unleashed - An exclusive executive event for enterprise data leaders. Hear from top industry leaders on Nov 15. Reserve your free pass Almost every enterprise today is at least exploring what large language models (LLMs) and generative AI can do for their business. Still, just as with the dawn of cloud computing and big data and analytics, many concerns remain: Where do they start in deploying the complex technology? How can they ensure the security and privacy of their sensitive, proprietary data? And what about time- and resource-intensive fine-tuning? Today, Dell and Hugging Face are announcing a new partnership to help address these hurdles, simplify on-premises deployment of customized LLMs and enable enterprises to get the most out of the powerful, evolving technology. “The impact of gen AI and AI in general will be “significant, in fact, it will be transformative,” Matt Baker, SVP for Dell AI strategy, said in a press pre-briefing. VB Event AI Unleashed Don’t miss out on AI Unleashed on November 15! This virtual event will showcase exclusive insights and best practices from data leaders including Albertsons, Intuit, and more. Register for free here “This is the topic du jour, you can’t go anywhere without talking about generative AI or AI,” he added. “But it is advanced technology and it can be pretty daunting and complex.” Dell and Hugging Face ‘embracing’ to support LLM adoption With the partnership, the two companies will create a new Dell portal on the Hugging Face platform. This will include custom, dedicated containers, scripts and technical documents for deploying open-source models on Hugging Face with Dell servers and data storage systems. The service will first be offered to Dell PowerEdge servers and will be available through the APEX console. Baker explained that it will eventually extend to Precision and other Dell workstation tools. Over time, the portal will also release updated containers with optimized models for Dell infrastructure to support new-gen AI use cases and models. “The only way you can take control of your AI destiny is by building your own AI, not being a user, but being a builder,” Jeff Boudier, head of product at Hugging Face, said during the pre-briefing. “You can only do that with open-source.” The new partnership is the latest in a series of announcements from Dell as it seeks to be a leader in generative AI. The company recently added ObjectScale XF960 to its ObjectScale tools line. The S3-compatible, all-flash appliance is geared towards AI and analytics workflows. Dell also recently expanded its gen AI portfolio from initial-stage inferencing to model customization, tuning and deployment. Of the latest news, Baker noted with a laugh: “I’m trying to avoid the puns of Dell and Hugging Face ‘embracing’ on behalf of practitioners, but that’s in fact what we are doing.” Challenges in adopting generative AI There are undoubtedly many challenges in enterprise adoption of gen AI. “Customers report a plethora of issues,” said Baker. To name a few: complexity and closed ecosystems; time-to-value; vendor reliability and support; ROI and cost management. Just as in the early days of big data, there’s also an overall challenge in progressing gen AI projects from proof of concept to production, he said. And, organizations are concerned about exposing their data as they seek to leverage it to gain insights and automate processes. “Today a lot of companies are stuck because they’re being asked to deliver on this new generative AI trend,” said Boudier, “while at the same time they cannot compromise their IP.” Just look at popular code assistants such as GitHub Copilot, he said: “Isn’t it crazy that every time a developer at an organization types a keystroke on a keyboard, your company source code goes up on the internet?” This underscores the value in — and need for — internalizing gen AI and ML apps. Dell research has found that enterprises overwhelmingly (83%) prefer on-prem or hybrid implementations. “There’s a significant advantage to deploying on-prem, particularly when you’re dealing with your most precious IP assets, your most precious artifacts,” said Baker. Curated models for performance, accuracy, use case The new Dell Hugging Face portal will include curated sets of models selected for performance, accuracy, use cases and licenses, Baker explained. Organizations will be able to select their preferred model and Dell configuration, then deploy within their infrastructure. “Imagine a LLama 2 model specifically configured and fine-tuned for your platform, ready to go,” Baker said. He pointed to use cases including marketing and sales content generation, chatbots and virtual assistants and software development. “We’re going to take the guesswork out of being a builder,” said Baker. “It’s the easy button to go to Hugging Face and deploy the capabilities you want and need in a way that takes away a lot of the minutiae and complexity.” What makes this new offering different from the spate of others emerging almost daily is Dell’s ability to tune “top to bottom,” Baker contended. This allows enterprises to quickly deploy the best configuration of a given model or framework. He emphasized that enterprises won’t be exchanging any data with public models. “It’s your data and nobody else is touching your data except you,” he said, adding that, “once that model has been fine-tuned, it’s your model.” Every company a vertical Ultimately, tuning models for maximum output can be a time-consuming process, and many enterprises currently experimenting with gen AI are using retrieval augmented generation (RAG) alongside off-the-shelf LLM tools. RAG incorporates external knowledge sources to supplement internal information. The method allows users to find relevant data to create stepwise instructions for many generative tasks, Baker explained, and the pattern can be instantiated in pre-built containers. “Techniques like RAG are a way of in essence not having to build a model, but instead providing context to the model to achieve the right generative answer,” he said. Dell aims to further simplify the fine-tuning process by providing a containerized tool based on the popular parameter efficient techniques LoRA and QLoRA, he said. This is an important step when it comes to customizing models to specific business use cases. Going forward, all enterprises will have their own vertical, in fact, “they themselves are vertical — they’re using their specific data,” Baker said. There’s much talk of verticalization in AI, but that doesn’t necessarily mean domain-specific models. “Instead, it’s taking your specific data, combining that with a model to provide a generative outcome,” he said. VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings.",
    "originSummary": [
      "Dell and Hugging Face have partnered to streamline the deployment of large language models (LLMs) and generative AI for enterprises.",
      "They will create a Dell portal on the Hugging Face platform, offering custom containers, scripts, and technical documents for deploying open-source models on Dell servers.",
      "This partnership aims to address the challenges of complexity, time-to-value, data security, and cost management faced by enterprises when implementing generative AI. The Dell Hugging Face portal will provide curated models for different use cases and licenses, enabling organizations to choose and deploy the most suitable configuration within their infrastructure. Additionally, Dell plans to simplify the fine-tuning process by providing containerized tools based on parameter-efficient techniques."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "Security Journey Introduces AI/LLM and API Learning Paths to Teach Secure Software Development",
    "originLink": "https://www.globenewswire.com/news-release/2023/11/14/2780190/0/en/Security-Journey-Announces-New-AI-LLM-and-API-Learning-Paths-to-Teach-Development-Teams-How-to-Build-Software-Securely.html",
    "originBody": "Accessibility: Skip TopNav Security Journey Announces New AI/LLM and API Learning Paths to Teach Development Teams How to Build Software Securely dehaze search close Newsroom press release distributionglobal optionsregulatory filingsmedia partners services contact us Français sign in REGISTER search Security Journey Announces New AI/LLM and API Learning Paths to Teach Development Teams How to Build Software Securely New training efficiently addresses the industry’s identified risks in API and AI/LLM software. November 14, 2023 09:05 ETSource: Security Journey FOLLOW Share Pittsbugh, PA, Nov. 14, 2023 (GLOBE NEWSWIRE) -- Security Journey, a leading secure coding training provider, today launched two new Topic-Based learning paths supporting the recently published OWASP Top 10 2023 recommendations for AI applications built on Large Language Models (LLM) and for securing Application Programming Interfaces (APIs). Just a short time after the OWASP vulnerability lists were published, Security Journey responded with training that enterprises must adopt to build and integrate these technologies securely. The OWASP Top 10 AI/LLM learning path offers an in-depth training experience designed to equip development teams with expertise not only in secure AI system design, especially those built on LLMs (Large Language Models), but also in the secure integration and utilization of these systems. The training curriculum covers essential topics, enabling development teams to hone their engineering skills to secure data, AI models, and software applications, resulting in the design of robust systems. By completing this path, learners will gain actionable insights for the secure integration and leveraging of AI/LLM systems. The AI/LLM learning path includes the following lessons: Introduction to AI/LLM Security Data Science Engineering for AI/LLM Model Engineering for AI/LLM Application and Plugin Security for AI/LLM AI/LLM Security Toolchain Joe Ferrara, CEO of Security Journey, emphasized the importance of these new learning paths, saying, “With recent CISA guidance calling for AI software to be secure by design with little to no software configuration changes or additional cost, these lessons and learning paths are paramount to meeting this need. As we equip development teams with the necessary skills to guarantee the security of API and AI systems, we are assisting them in confronting the ever-evolving threat landscape.” The OWASP API Security Top 10 learning Path, to be released in December, is a progressive Topic-Based learning path with foundational, intermediate, and advanced lessons in a variety of learning formats from podcast-style videos to hands-on coding lessons. The new learning path will equip developers of all experience levels to combat the significant risks associated with insecure APIs. The OWASP API Top 10 path includes the following lessons: OWASP API Top 10Part 1 Broken Object Level Authorization (Hands-on Coding Lesson) Broken Authentication (Hands-on Coding Lesson) Broken Object Property Level Authorization (Hands-on Coding Lesson) OWASP API Top 10Part 2 Unrestricted Resource Consumption (Hands-on Coding Lesson) Broken Function Level Authorization (Hands-on Coding Lesson) Unrestricted Access to Sensitive Business Flows (Hands-On Coding Lesson) OWASP API Top 10Part 3 Server-Side Request Forgery (SSRF) (Hands-On Coding Lesson) Security Misconfigurations (Hands-On Coding Lesson) Improper Inventory Management (Hands-On Coding Lesson) Unsafe Consumption of APIs (Hands-On Coding Lesson) Fundamentals of gRPC Security Fundamentals of GraphQL Security The need to secure APIs has become abundantly clear. Recent research from TechTarget’s Enterprise Strategy Group on Securing the API Attack Surface found that 92% of organizations have experienced at least one security incident related to insecure APIs in the last 12 months, including 57% who have experienced multiple security incidents related to insecure APIs during the past year. \"While APIs are powerful tools for developers, it is important for them to understand the security implications and risks as they develop feature-rich applications using APIs. Our research shows there is a direct correlation with developers having a higher level of API risk understanding when their organizations have formal training programs in place,” said Melinda Marks, Practice Director, Cybersecurity, at Enterprise Strategy Group. “Continuous education and collaboration with developers can mitigate risk and reduce API security incidents to protect an organization’s digital assets.” In continuation of Security Journey's ongoing commitment to addressing the latest security challenges, these new Topic-Based Learning Paths build upon its recent announcement of Role-Based and Compliance-Based Recommended Learning Paths offerings. They are designed to empower learners to focus their efforts on enhancing expertise and skills in specific, high-priority areas. By doing so, development teams can effectively mitigate and avert prominent software risks, aligning with industry demands and standards such as the OWASP Top 10 2023 recommendations for AI and API security. These learning paths represent our dedication to staying at the forefront of security education, ensuring organizations are equipped to tackle the ongoing security challenges. To learn more about these essential learning paths and enhance your organization's security posture, please visit securityjourney.com. About Security Journey Security Journey helps enterprises reduce vulnerabilities with application security education for developers and all individuals involved in creating software. Development teams are empowered through practical, skill-oriented secure coding training that easily satisfies compliance needs and goes beyond that to build a security-first development culture. Over 450 companies around the world are teaching their teams how to build safer software using Security Journey. //ENDS// Media Contact: Katie Fegan, Account Manager kfegan@saycomms.co.uk Attachment Security Journey Announces New AI/LLM and API Topic-Based Learning Paths Security Journey Announces New AI/LLM and API Topic-Based Learning Paths Paths Support OWASP API Top 10 and OWASP AI/LLM Top 10 Recommendations Tags appsec secure development training' secure coding training secure software AI/LLM API Security Related Links TechTarget’s Enterprise Strategy Group on Securing the API Attack Surface CISA guidance Contact Data Amy Baker Security Journey 7246125137 amy_baker@securityjourney.com Contact Company ProfileSecurity JourneyWebsite: https://www.securityjourney.com Press Release Actions Print Download PDF Subscribe via RSS Subscribe via ATOM Javascript We use cookies to improve your GlobeNewswire experience. By continuing, you agree to our use of cookies. For more information, please see our Privacy Policy. ACCEPT",
    "originSummary": [
      "Security Journey, a secure coding training provider, has launched two new learning paths to educate development teams on building software securely.",
      "The first learning path focuses on AI applications built on Large Language Models (LLM) and covers topics like secure system design and integration.",
      "The second learning path concentrates on securing Application Programming Interfaces (APIs) and provides lessons on security vulnerabilities and best practices. These learning paths aim to address identified risks and help development teams combat the changing threat landscape."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "You.com Launches YOU API, Enabling AI Chatbots and Large Language Models with Real-Time Web Access and Enhanced Accuracy",
    "originLink": "https://finance.yahoo.com/news/com-launches-api-search-retrieval-184000030.html",
    "originBody": "Business Wire You.com Launches API for Web Search and Retrieval Augmented Generation, Giving AI Chatbots And Large Language Models Real-Time Web Access And Unparalleled Accuracy Read full article Business Wire Tue, November 14, 2023 at 12:40 PM CST·4 min read PALO ALTO, Calif., November 14, 2023--(BUSINESS WIRE)--You.com announced today the launch of a first-of-its-kind self-service API designed to integrate seamlessly with any Large Language Model (LLM) and AI chatbot to provide unprecedented real-time internet access. The YOU API is a suite of tools designed to help developers address some of the challenges LLMs and AI chatbots face, such as hallucinations, source citation issues, and outdated information, by providing the most relevant and up-to-date information via web search. This new index is built to be consumed by LLMs and not as a list of blue links like all prior web indices. This means the snippets are much longer and more likely to contain the relevant information to be summarized. In addition to the search API, You.com offers a full LLM+search endpoint for end-to-end retrieval-augmented generation (RAG). \"Since You.com introduced the first consumer-facing LLM with internet access, providing up-to-date answers complete with citations, we received many requests for an API with these capabilities,\" said Richard Socher, CEO of You.com. \"Our response is the YOU API, a comprehensive solution that equips every chatbot and LLM with the means to retrieve real-time information through web search.\" You.com is not only the first API to provide web search for LLMs and chatbots but also the most accurate. You.com put the YOU API through a series of tests using academic benchmarks, such as the Stanford Question Answering Dataset (SQuAD v1/v2), HotpotQA, FreshQA, and MS MARCO, and found that it stands apart from the field. \"We’re not resting on assumptions; we’ve rigorously benchmarked our technology,\" said Bryan McCann, You.com CTO. \"The results reveal that the YOU API outperforms the capabilities of search providers like Google and Bing when paired with a variety of LLMs. By using the YOU API, developers ensure that their LLMs leverage the most recent web information and receive data verified by testing protocols.\" Story continues Illustrating the real-world impact of the YOU API, Kevin Baragona, founder of DeepAI, which ranks among the top 50 most visited AI sites in the past 12 months, shared his experience with the YOU API: \"We run a popular AI chatbot, lovingly called 'AI Chat,' which is often used for general knowledge or research use cases. Our users had been asking us to have the chatbot give up-to-date information, more recent than when the language model was trained, or even real-time data. We were super glad when we encountered You.com’s API, particularly its RAG feature. Within a week of development, we launched a new feature to our users, which we shipped as a checkbox labeled 'Online Mode.' Thanks to the RAG API, we were able to get the feature done in record time. Users tell us that 'AI Chat' acts like a completely new product with access to real-time knowledge. We feel like we’ve only scratched the surface of what is possible with the YOU RAG and search API, and we’ll continue building more great features with it.\" You.com offers API pricing plans that cater to teams of all sizes, from a complimentary trial to customized enterprise solutions. Every plan includes real-time web search, multilingual responses, long snippets for AI, and rights to use data for AI inference and citations. Learn more about how to use the API at docs.you.com. Visit api.you.com to start a free trial of the web and news search API. For the RAG endpoint, please email api@you.com to get access. About You.com Founded by leading AI research scientists Richard Socher and Bryan McCann, You.com is an AI assistant that helps you search online, write an essay, code, create digital art, or solve a physics problem. You.com pioneered many solutions for Large Language Model (LLM) challenges, especially around trust and accuracy. You.com notably introduced the first consumer-facing LLM with access to the internet to provide up-to-date answers and include citations and recently debuted an AI agent that executes code to provide the most accurate responses to math and science questions. You.com's API further enables other LLM-based chatbots to improve their accuracy with real-time web access. You.com is available via Chrome web extensions, iOS and Android mobile apps, and WhatsApp. View source version on businesswire.com: https://www.businesswire.com/news/home/20231114112050/en/ Contacts Julia La Roche Head of Communications julia@you.com TRENDING 1. France's supreme court annuls penalties against UBS in tax evasion case - ruling 2. Man United CEO Richard Arnold steps down as club seeks new investment 3. European Gas Swings as Brimming Stockpiles Counter Winter Risks 4. UPDATE 1-Russia concedes Ukrainian troops have crossed Dnipro River in Kherson region 5. Edmunds: How to make the most of falling used car prices",
    "originSummary": [
      "You.com has introduced a new self-service API named YOU API, which facilitates real-time access to Large Language Models (LLMs) and AI chatbots.",
      "The API is designed to solve problems such as hallucinations, source citation issues, and obsolete information by providing accurate and current data through web search.",
      "Extensive testing has demonstrated that the API, in combination with LLMs, outperforms established search platforms like Google and Bing. You.com offers a range of API pricing plans geared towards teams of various sizes, and includes features like multilingual responses and long snippets for AI."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "GPT-4 Turbo outshines Claude 2 in latest language model comparison",
    "originLink": "https://www.pcguide.com/ai/gpt-4-turbo-vs-claude-2/",
    "originBody": "Home > AI > GPT-4 Turbo vs Claude 2 – LLM’s compared GPT-4 Turbo vs Claude 2 – LLM’s compared OpenAI vs Anthropic LLMs compared. Steve Hook PC Guide is reader-supported. When you buy through links on our site, we may earn an affiliate commission. Prices subject to change. Read More Last Updated on November 14, 2023 We compare the brand-new and cutting-edge GPT-4 Turbo LLM to the much-underestimated Claude 2 (Stylized Claude-2) LLM. To be clear, this is a comparison of large language models, not the AI chatbots they power (except where relevant). We analyze GPT-4 Turbo vs Claude 2 in terms of capabilities, context windows, pricing, accuracy and more! So, how does OpenAI’s ChatGPT AI model lead by CEO Sam Altman fare against the AI safety showpiece lead by CEO Dario Amodei? GPT-4 Turbo vs Claude 2 GPT-4 Turbo is the latest LLM (large language model) from OpenAI. It was announced by the AI R&D firm at OpenAI DevDay in San Francisco, on November 6th, 2023. This was an impressive reveal consider that the prior models GPT-4 and GPT-4V, both identical in their natural language processing (NLP) capabilities, were jointly 1st place in the AI race. Now, OpenAI CEO Sam Altman, standing on stage with his leading investor, Microsoft CEO Satya Nadella, one-upped ChatGPT with what seems to be the only thing that can – a better ChatGPT. Company CEO AI Chatbot LLM API Open-source xAI Elon Musk Grok Grok-1 No No OpenAI Sam Altman ChatGPT GPT-3.5, GPT-4, GPT-4V, or GPT-4 Turbo Yes No Google Sundar Pichai Bard PaLM 2 Yes No Microsoft Satay Nadella Bing Chat GPT-4 No No Meta Mark Zuckerberg Meta AI LLaMA 2 No Yes Anthropic Dario Amodei Claude Claude-2 Yes No Amazon Andy Jassy Olympus (rumored) Olympus (rumored) No No The AI chat bots of big tech. xAI, the artificial intelligence firm founded by CEO Elon Musk, recently conducted research into the rankings of every AI chatbot and their respective AI models. The verdict was concluded after all leading foundational large language models of big tech were tested across four benchmarks – namely GSM8k, MMLU, HumanEval, and MATH. Included in this comprehensive comparison were OpenAI’s GPT-4, Anthropic’s Claude-2, Google’s PaLM 2, xAI’s Grok-1, OpenAI’s GPT-3.5, Pi’s Inflection-1, Meta’s LLaMA 2, and xAI’s Grok-0 in descending order of power / accuracy. This puts Claude-2 in 2nd place! Benchmark Grok-0 LLaMa 2 Inflection-1 GPT-3.5 Grok-1 PaLM 2 Claude-2 GPT-4 GSM8k 56.8% 56.8% 62.9% 57.1% 62.9% 80.7% 88% 92% MMLU 65.7% 68.9% 72.7% 70.0% 73.0% 78% 75% 86.4% HumanEval 39.7% 29.9% 35.4% 48.1% 63.2% 70% 67% MATH 15.7% 13.5% 16.0% 23.5% 23.9% 34.6% 42.5% The large language models of big tech, as benchmarked by xAI. OpenAI vs Anthropic – AI chatbot features The new GPT-4 model has the same use cases as existing variants of the GPT-4 foundation model. Internet access to real-time information, plugin support, and Advanced Data Analysis for math and PDF / Excel insights or summarization. By comparison, Anthropic’s Claude 2 has none of these ‘prompt modifiers’, which each add complexity but result in more useful evaluations for complex tasks. Claude-2 also falls short for image output, where GPT-4 Turbo will feature integration with AI image generator DALL-E 3 (Stylized DALL·E 3). Anthropic, by contrast, has no proprietary AI art generator. However, Claude-2 has something that GPT-4 Turbo doesn’t. Claude is a constitutional AI (CAI) which “shapes the outputs of AI systems according to a set of principles, with the goal of making a helpful, harmless, and honest AI assistant.” The principal purpose of Claude (and the Claude-2 LLM) is as an ethics research tool, to fine-tune our understanding of machine learning, a guide it towards AI safety goals with human feedback and reinforcement learning. GPT-4 Turbo does have more parameters though. In terms of comprehension, coherence, and high-quality output, the OpenAI chat bot model wins, with Claude-2 coming in 2nd place. Follow us Steve Hook Steve is the AI Content Writer for PC Guide, writing about all things artificial intelligence. Table of Contents ChatGPT vs Amazon Olympus AI – Chatbot rumors compared By Steve Hook Nov 14, 2023 Amazon Olympus AI – World’s largest LLM rumors By Steve Hook Nov 9, 2023 ChatGPT 4 Turbo and GPT-4 Turbo explained By Steve Hook Nov 14, 2023 AI Stats 2023 — Key artificial intelligence statistics and trends By Steve Hook Nov 9, 2023",
    "originSummary": [
      "The article compares the new GPT-4 Turbo large language model (LLM) from OpenAI to the Claude 2 LLM from Anthropic.",
      "The comparison looks at capabilities, context windows, pricing, and accuracy.",
      "GPT-4 Turbo is the latest model from OpenAI, announced at OpenAI DevDay, and is considered to have better comprehension, coherence, and high-quality output compared to Claude-2."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "You.com Launches New APIs to Improve Language Models' Access to the Web",
    "originLink": "https://techcrunch.com/2023/11/14/you-com-launches-new-apis-to-connect-llms-to-the-web/",
    "originBody": "(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Link Copied AI You.com launches new APIs to connect LLMs to the web Kyle Wiggers@kyle_l_wiggers / 3:00 PM UTC•November 14, 2023 Comment Image Credits: Adobe Stock When OpenAI connected ChatGPT to the internet, it supercharged the AI chatbot’s capabilities. Now, the search engine You.com wants to do the same for every large language model (LLM) out there. You.com today announced the launch of a set of APIs aimed at giving LLMs like Meta’s Llama 2 real-time access to the open web — or narrower slices of it. Starting at $100 per month, You.com’s APIs augment LLMs’ answers to questions from users (e.g. “Which holidays are this week?”) with up-to-date context from the internet. Customers including LlamaIndex, Anthropic and Cohere have already integrated it with their models. “[We’ve] received many requests for an API with these capabilities,” You.com CEO and founder Richard Socher told TechCrunch in an email interview. “When you ask about a recent event, like a Super Bowl score on the day of the Super Bowl, our API will search for those scores on the web and then you can add that information, in that moment, to the … LLM and it can then use it to answer your question more accurately.” Most LLMs are trained on publicly available, static data scraped from public web pages, e-books and elsewhere. That’s sufficient to get them to perform tasks, from writing emails to drafting cover letters and essays. But it limits the LLMs’ knowledge to the data’s time range; an LLM trained on info prior to September 2021 wouldn’t be aware of events that happened yesterday, of course. You.com’s new APIs enable LLMs to overcome this limitation by creating an index of long snippets of websites — a key point of differentiation over the standard search APIs provided by Bing and Google, which Socher claims serve only very short snippets “designed to entice someone to click a link.” LLMs can leverage this custom-built index when answering questions, identifying the relevant snippet and summarizing it to provide an updated answer. “Every LLM gets a prompt — a description for how it should behave and answer questions,” Socher explained. “You can add your own question to the end of that prompt to have a conversation with an LLM. What this API enables is that you can add a lot of up-to-date context from the web into the prompt, after the question is asked.” You.com is providing three “flavors” of API at launch: Web search, news and RAG. Web search gives LLMs access to the aforementioned index of long snippets, while news — as the name implies — provides exclusively news results. As for RAG (which stands for “retrieval-augmented generation”), it pairs You.com’s web search results with an LLM to generate what Socher claims are “more factual” responses, although the jury’s obviously out on that. Now, an LLM with web access can be a risky prospect — no matter which APIs it’s tapping. The live web is less curated than a static training dataset and, by implication, less filtered. Search results can be gamed, and they also aren’t necessarily representative of the totality of the web. Because most algorithms prioritize websites that use modern web technologies like encryption, mobile support and schema markup, websites with otherwise quality content get lost in the shuffle. Socher admitted that You.com’s API has weaknesses particularly in the areas of localized “near me”-style questions (e.g. “Where’s good sushi near me?”), since the API doesn’t know LLM users’ locations. But improvements are already being made, including upgrades that’ll allow You.com’s APIs to code and “produce much more complex answers” with traceable citations, Socher says. “We’ll soon merge news and general web search to make it even easier for companies using our APIs,” he added. “By incorporating our API into whatever solution is built by creators, their answers will be more relevant and helpful for their end users … [The solution can turn] to the web to verify facts.” The new APIs have this writer wondering if search is the next battlefield on the generative AI front. As open source LLMs approach the level of some of their closed-source counterparts, the strength of the search engine backing those closed-source LLMs (Bing in ChatGPT’s case, Google in Bard’s) becomes a stronger selling point — unless APIs like You.com’s effectively level the playing field. That’s a big “if” — no API’s perfect, and You.com’s surely has flaws beyond those Socher mention. But I’d argue that competition is always a good thing. The new You.com APIs start at $100 per month for 14,200 API calls after a 60-day trial that comes with 1,000 free monthly calls. You.com also offers bespoke packages for larger enterprise deals that come with annual subscriptions and discounts. Please login to comment Login / Create Account TechCrunch Early Stage April 25, 2024 Boston, MA Register Interest Sign up for Newsletters See all newsletters(opens in a new window) Daily Week in Review Startups Weekly Event Updates Advertising Updates TechCrunch+ Announcements TechCrunch+ Events TechCrunch+ Roundup Email Subscribe (opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window)(opens in a new window) Copy Tags AI APIs Generative AI large language models LLMs search engine you.com",
    "originSummary": [
      "You.com has introduced APIs that enable large language models (LLMs) to access real-time information from the internet, enhancing their accuracy in answering user queries.",
      "Unlike conventional search APIs, You.com's APIs generate an index of lengthy website snippets, enabling LLMs to provide up-to-date responses.",
      "The APIs offer three alternatives: web search, news, and retrieval-augmented generation (RAG). However, there are concerns regarding unfiltered and potentially manipulated search results. The APIs are available starting at $100 per month, with a 60-day trial period."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "New Relic introduces AI observability tool to aid in monitoring large language models (LLMs)",
    "originLink": "https://www.information-age.com/ai-observability-launched-by-new-relic-to-aid-llm-monitoring-123507977/",
    "originBody": "The new end-to-end AI monitoring solution is set to help engineers build and run applications using large language models (LLMs), using in-depth insights to optimise observability processes. Users will be able to keep tabs on AI model performance, quality, cost, and compliance, allowing for more effective finance and risk management. Over 50 integrations across the AI data stack will be made available to New Relic customers, allowing for more room for service customisation. These include: Azure, AWS, GCP and Kubernetes infrastructure; LangChain orchestration framework; Pytorch, Keras and TensorFlow machine learning libraries; Compatibility with Amazon SageMaker and AzureML. Three ideal scenarios for anomaly detection with machine learning — Machine learning can prove ideal for anomaly detection throughout the company network. Here are three key scenarios where this can be put to good use. Volumes of telemetry data need to be tracked and analysed by software engineers managing an array of applications. This calls for single-view access for troubleshooting, comparing and optimising LLM prompts and responses for performance, cost, security, and quality issues. “Almost every company is deciding how they are going to integrate AI into their operations and product offerings,” said Manav Khurana, chief product officer at New Relic. “Observability is fundamental to the function and growth of AI. With AIM, we are giving engineers the necessary visibility and control needed to navigate the complexities of AI and build applications in a safe and cost-effective manner.” Mitigating the organisational risks of generative AI — Risks including bias and hallucinations abound across generative AI projects. Jeff Watkins explains how businesses can mitigate them long-term. Edo Liberty, founder and CEO of early product trialists Pinecone, commented: “By integrating the Pinecone vector database with New Relic AI Monitoring, we are helping organisations embrace next-generation search capabilities with generative AI. “We have seen incredible demand for vector databases as companies build and deploy AI applications since it is an essential part of the AI stack. “Now, customers can build better search and generative AI solutions by ensuring relevant, accurate, and fast responses alongside their AI observability practice.” LLM challenges to address LLMs remain susceptible to output risks including bias and misinformation, the latter of which can be caused by data hallucinations as well as insufficient information assets. Meanwhile, operational costs for maintaining AI models such as GPT-4 and PaLM2 can become unmanageable if data and finance teams aren’t properly aligned with possible changes. As well as helping to overcome these common challenges, insights into how AI systems are operating, to ensure AI application compliance across the stack, can be utilised by New Relic customers. AIM is now available in early access to New Relic users worldwide. More information can be found here. Related: Observability – everything you need to know — Morgan Mclean, director of product management at Splunk, speaks with Information Age about all things observability. TAGGED: A.I. Artificial Intelligence, Generative AI, IT monitoring",
    "originSummary": [
      "New Relic has introduced AIM, an AI monitoring solution that assists engineers in developing and operating applications with large language models (LLMs).",
      "AIM provides valuable information on AI model performance, quality, cost, and compliance, enabling effective finance and risk management.",
      "The solution offers more than 50 integrations with popular AI data stack tools and platforms, helping address issues like bias, misinformation, and operational expenses related to LLMs. AIM is currently in early access for New Relic customers globally."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  },
  {
    "title": "Los Alamos National Laboratory and DOE/NNSA Partner with SambaNova for AI Expansion",
    "originLink": "https://www.businesswire.com/news/home/20231114158716/en/Los-Alamos-National-Laboratory-and-the-U.S.-Department-of-Energy%E2%80%99s-National-Nuclear-Security-Administration-Select-SambaNova-Suite-to-Facilitate-Their-Generative-AI-and-LLM-Capabilities",
    "originBody": "Los Alamos National Laboratory and the U.S. Department of Energy’s National Nuclear Security Administration Select SambaNova Suite to Facilitate Their Generative AI and LLM Capabilities LANL adopts SambaNova Suite and expands existing SambaNova DataScale® deployment to run AI workloads for performing national security, science, technology, and engineering projects November 14, 2023 09:00 AM Eastern Standard Time DENVER--(BUSINESS WIRE)--SC23—SambaNova Systems, makers of the only purpose-built, full stack AI platform, the Department of Energy’s National Nuclear Security Administration (DOE/NNSA), and Los Alamos National Laboratory (LANL) announce a strategic partnership to facilitate the laboratory's simulation needs. “SambaNova Suite offers the quickest and safest way to deploy generative AI on-premises, solving organizations’ biggest innovation challenges.” Post this Los Alamos National Laboratory is a multi-program, federally funded research, and development center for the NNSA. The organization is expanding its partnership with SambaNova in two ways: Scaling up SambaNova DataScale® to accelerate AI workloads that advance the laboratory's generative AI and LLM capabilities. Deploying SambaNova Suite to advance its Generative AI and large language model (LLM) technologies. “Los Alamos is investing in Generative AI to advance a broad spectrum of national security missions,” said Jason Pruet, director of the National Security AI Office for Los Alamos National Laboratory. “At the dawn of the exascale supercomputing era, we are increasingly relying on AI to be part of the ASC computing ecosystem to support our mission objectives now and in the coming years,” said NNSA ASC program director Thuc Hoang. “We are pleased to be scaling up our existing deployments of SambaNova Systems to advance generative AI and large language model technologies to contribute to the ASC program.” “The multi-year deal being announced today is an expansion of our current partnership with LANL,” said Marshall Choy, SVP of Product at SambaNova Systems. “The partnership showcases SambaNova’s performance advantage over GPU based systems on the most challenging foundation model and deep learning workloads. This means more experiments and more discoveries that accelerate and impact national initiatives.” “Generative AI and LLMs are showing great potential to transform enterprises, government institutions, and society,” said Peter Rutten, Research Vice-President, Performance-Intensive Computing, at International Data Corporation. “Seeing deep tech innovators such as LANL and SambaNova collaborating is a clear sign that we have moved to impactful implementations.” “LANL is focused on scaling up generative AI and LLMs and because of that, they’ve deployed SambaNova’s technology across the lab,” said Rodrigo Liang, CEO of SambaNova Systems. “SambaNova Suite offers the quickest and safest way to deploy generative AI on-premises, solving organizations’ biggest innovation challenges.” For more information about SambaNova Suite or DataScale please visit the company at SC23 in Booth #681. About the SambaNova Suite SambaNova Suite is the first full stack, generative AI platform, from chip to model, optimized for enterprise and government organizations. Powered by the intelligent SN40L chip, the SambaNova Suite is a fully integrated platform, delivered on-premises or in the cloud, combined with state-of-the-art open-source models, which can be easily and securely fine-tuned using customer data for greater accuracy. Once adapted with customer data, customers retain model ownership in perpetuity, so they can turn generative AI into one of their most valuable assets. About Los Alamos National Laboratory Los Alamos National Laboratory, a multidisciplinary research institution engaged in strategic science on behalf of national security, is managed by Triad, a public service oriented, national security science organization equally owned by its three founding members: Battelle Memorial Institute (Battelle), the Texas A&M University System (TAMUS), and the Regents of the University of California (UC) for the Department of Energy's National Nuclear Security Administration. Los Alamos enhances national security by ensuring the safety and reliability of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns. About SambaNova Systems Customers turn to SambaNova to quickly deploy state-of-the-art generative AI capabilities within the enterprise. Our purpose-built enterprise-scale AI platform is the technology backbone for the next generation of AI computing. Headquartered in Palo Alto, California, SambaNova Systems was founded in 2017 by industry luminaries, and hardware and software design experts from Sun/Oracle and Stanford University. Investors include SoftBank Vision Fund 2, funds and accounts managed by BlackRock, Intel Capital, GV, Walden International, Temasek, GIC, Redline Capital, Atlantic Bridge Ventures, Celesta, and several others. Visit us at sambanova.ai or contact us at info@sambanova.ai. Follow SambaNova Systems on LinkedIn. Contacts Virginia Jamieson Head of External Communications, SambaNova Systems virginia.jamieson@sambanova.ai 650-279-8619",
    "originSummary": [
      "SambaNova Systems, the DOE/NNSA, and LANL have formed a partnership to enhance LANL's generative AI and large language model (LLM) capabilities for national security projects.",
      "The collaboration aims to accelerate AI workloads, advance generative AI and LLM technologies, and support the laboratory's simulation needs.",
      "SambaNova's technology, which offers a performance advantage over GPU-based systems, is being used by LANL to scale up the deployment of generative AI and LLMs across the lab."
    ],
    "commentBody": "",
    "commentSummary": [
      "No information or context is given in the text.",
      "Therefore, a concise summary cannot be provided."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700055320684
  }
]
