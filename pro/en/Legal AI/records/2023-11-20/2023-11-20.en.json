[
  {
    "title": "Lawyers Aligning AI with Client Values for Meaningful Progress",
    "originLink": "https://news.bloomberglaw.com/us-law-week/lawyers-who-align-ai-with-client-values-blaze-a-meaningful-path",
    "originBody": "Attorneys can help their clients use AI tools ethically with thoughtful collaboration, open communication, and a focus on real-world human impact, says attorney Colin Levy. Artificial intelligence has emerged as a transformative force, rapidly advancing its own capabilities and enabling industries to become more automated, data-driven, and technology-enabled. As AI remains in focus, so has the concept of centering client needs with AI solutions. Harnessing AI’s immense potential means aligning its development and application with the specific goals and values of clients who will be using it. This ensures AI solutions deliver targeted benefits in an ethical, sustainable, and client-centric manner. Beyond Efficiency Clients adopt AI primarily to enhance efficiency, reduce costs, and gain a competitive edge. Walmart, for instance, has leveraged AI to optimize its truck routes, saving millions of dollars in annual fuel costs. Similarly, JPMorgan Chase’s COIN platform has automated 360,000 hours of manual work annually through machine learning, while Netflix uses AI to offer ultra-precise movie recommendations to engage subscribers. While efficiency and cost reduction are compelling benefits, aligning AI with client values is equally important. Clients increasingly are prioritizing sustainability, social responsibility, and transparency in their operations. Unilever, a global consumer goods giant, has embedded AI into its factories to minimize waste generation, demonstrating its commitment to eco-friendly practices. The UK National Health Service, recognizing the potential for bias in AI algorithms, developed a diabetic eye disease screening tool that openly informs users of potential biases, promoting transparency and trust. Pitfalls of Misaligned AI When AI isn’t aligned with client goals and values, it can lead to problems. AI recruitment tools might incorporate biases against certain types of individuals, and concerns have been raised over Facebook’s AI moderation systems, causing an uproar among users and raising concerns about potential censorship and manipulation. AI developers can build client-centric solutions by fostering open dialogue and collaboration to fully understand client goals and values. Toyota, a pioneer in the automotive industry, took this approach to heart. Before developing AI robots for its assembly lines, Toyota employees spent more than a year observing and understanding the intricate processes and nuances of human workers. This deep understanding enabled them to create AI solutions that enhanced the skills and capabilities of their human counterparts, rather than replacing them. Collaborative, Adaptive Culture Integrating client perspectives into every stage of AI development is crucial. To ensure its anti-discrimination algorithm reflected community standards, Airbnb designed simulations based on real user data, allowing the company to identify and mitigate potential biases before deploying the algorithm. Multidisciplinary teams also play a vital role in considering problems holistically, and ensuring AI solutions align with a wider range of client needs. Microsoft’s Healthcare NExT initiative, which aims to develop AI tools for health-care applications such as cancer screening and triage coordination, exemplifies this approach. By working cross-functionally, the team could anticipate and address potential blind spots in the AI models, ensuring their relevance and effectiveness in real-world health-care settings. Rapid iteration and feedback channels are also essential for tuning AI solutions to specific client requirements. Uber uses explainable machine learning models to dynamically adjust ride pricing based on real-time supply-and-demand dynamics. This transparency allows both drivers and riders to understand the factors influencing pricing decisions, fostering trust and enhancing overall user experience. Open-source initiatives, such as OpenAI’s publication of the entirety of its code for GPT-3’s natural language capabilities, further promote transparency and accountability. By making AI models and algorithms accessible to the broader research community, companies demonstrate their commitment to ethical AI development and open collaboration. Balancing Constraints and Aspirations While aligning AI with client goals and values is paramount, it’s important to acknowledge inherent challenges of balancing technical constraints and client aspirations. AI development is often a complex and iterative process, and achieving desired outcomes may require compromises and adjustments. However, thoughtful collaboration, open communication, and a focus on real-world human impact can bridge the gap between technical feasibility and client expectations. By working together, AI developers and clients can navigate the complexities of AI development and unleash its immense potential to drive meaningful progress across industries. This article does not necessarily reflect the opinion of Bloomberg Industry Group, Inc., the publisher of Bloomberg Law and Bloomberg Tax, or its owners. Author Information Colin S. Levy is a lawyer and director of legal at Malbek. Write for Us: Author Guidelines Continue Reading Learn About Bloomberg Law AI-powered legal analytics, workflow tools and premium legal & business news. Learn more Already a subscriber? Log in to keep reading or access research tools. Log In",
    "originSummary": [
      "Attorney Colin Levy highlights the importance of ethical use of AI tools by attorneys and their clients.",
      "The alignment of AI development with the goals and values of clients is crucial for targeted benefits in an ethical and sustainable manner.",
      "Examples are given of companies using AI for efficiency, cost reduction, and alignment with client values like sustainability and transparency."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "Cup of Justice Golf Classic Raises $74,000 for Legal Aid Society",
    "originLink": "https://www.floridabar.org/the-florida-bar-news/annual-cup-of-justice-golf-classic-raises-over-74000-for-the-legal-aid-society-of-palm-beach-county/",
    "originBody": "The Florida Bar Rules, Ethics & Professionalism About The Bar + - About The Bar Home Frequently Asked Questions President’s Welcome The Florida Bar Board of Governors Committees Sections / Divisions Board Certification Florida Voluntary Bars Florida Registered Paralegals Diversity / Inclusion Strategic Plan / Research Leadership Academy Contact Us Bar Employment Rules, Ethics & Professionalism News & Events + - News & Events Home News Releases Florida Bar News Florida Bar Journal Social Media Meetings & Conventions Daily News Summary Forms & Publications Podcasts Media Resources Calendars Public + - Public Home Lawyer Complaints and Discipline Clients’ Security Fund Consumer Information Lawyer Referral Service Legal/Civics Education Pro Bono & Legal Aid Prepaid Legal Services Plans Pro Hac Vice/Limited Appearance Speakers Bureau Unlicensed Practice of Law Volunteer Opportunities Members + - Members Home Benefits / Discounts Continuing Legal Education Practice Resources: LegalFuel Health and Wellness Center Lawyers Advising Lawyers Legislative Activity Appointments/Applications Join Lawyer Referral Service Pro Bono Service and Reporting Florida Lawyers Assistance Login Find a Lawyer Florida Bar News Home Journal & News MENU Home News Top Stories On the Move News & Notes In Memoriam Disciplinary Actions Letters BarNews.TV Journal Features Columns President’s Page Executive Directions Book Reviews Letters Annual Reports Submissions Classifieds Attorneys Exchange Notices Archives Contact Subscribe Submit a Letter Submit an Article Submit News and Notes Submit On the Moves Staff Advertise Annual Cup of Justice Golf Classic raises Over $74,000 for the Legal Aid Society of Palm Beach County Nov 20, 2023 News in Photos The team of Paul Shalhoub, Rossa Flood, Steve Gawithe, Jeff Heun, Jim Neil, and Bob Shalhoub took first place in the 21st Annual Cup of Justice Golf Classic. Gawaithe aced the 7th hole at Mayacoo Lakes Country Club, which earned him a cash prize of $25,000. Photo by Tracey Benson Photography. The 21st Annual Cup of Justice Golf Classic, hosted by the Legal Aid Society of Palm Beach County, raised over $74,000 to support the organization’s mission to provide free legal assistance to community residents in need. One of the highlights of the event was Steve Gawaithe’s hole-in-one on the 7th hole at Mayacoo Lakes Country Club, which earned him a cash prize of $25,000. Abigail Beebe, Bob Bertisch, Amie Swan, and Whitney Stewart. Photo by Tracey Benson Photography. The team of Jeff Heun/Morgan Stanley was awarded the “Cup of Justice” trophy after an outstanding round of golf. Other winners included (Flight B – 1st Place) Murray Guari Trial Attorneys and (Flight C – 1st Place) Dex Imaging. The Longest Drive winner was Zach Burke. “We are so grateful for the support of our sponsors and participants at the 21st Annual Cup of Justice Golf Classic,” said Bob Bertisch, executive director of the Legal Aid Society of Palm Beach County. “The funds raised from this event will help us continue to provide essential legal services to those who need them most.” The event was chaired by Robert Shalhoub. The golf tournament committee members included Benjamin Hartman, Devin Krauss, Scott Perry, Grier Pressly, and Gary Woodfield. High school student golf team members Hampton Beebe (Oxbridge Academy) and Lawson Ford (Cardinal Newman) volunteered as amateurs for the day. Sponsors included First Horizon Foundation, Florida Crystals Corporation, Dex Imaging, Inc., Dailey Janssen Architects, FurrCohen, eebe Armstorng, Legal Graphicworks, Pietraggalo Gordon Alfano Bosick and Raspanti, Murray Guari, Trial Attorneys, Schwed, Adams & McGinley, Pursuit Wealth Management, Ellrich, Neil, Smith & Stohlman, and Goldlaw. Search News Archives Sections Disciplinary Actions News and Notes On the Move In Memoriam Letters Errata Announcements Attorneys Exchange Classified Ads News in Photos Columns Be a Thankful Lawyer ColumnsNov 16, 2023 Mindfulness, Emotions, and the Likelihood of Confusion ColumnsOct 18, 2023 Be a Choosy Lawyer ColumnsOct 10, 2023 Be an early lawyer ColumnsSep 17, 2023 About the Bar About The Bar Home Frequently Asked Questions President’s Welcome The Florida Bar Board of Governors Committees Sections / Divisions Board Certification Florida Voluntary Bars Florida Registered Paralegal Program Diversity / Inclusion Strategic Plan / Research Leadership Academy Contact The Florida Bar News & Events News & Events Home Florida Bar News Florida Bar Journal News Releases Social Media Daily News Summary Calendars Meetings & Conventions Media Resources Forms & Publications For the Public Public Home Lawyer Complaints and Discipline Clients’ Security Fund Consumer Information Lawyer Referral Service Legal/Civics Education Prepaid Legal Services Pro Bono & Legal Aid Pro Hac Vice/Limited Appearance Speakers Bureau Unlicensed Practice of Law For Our Members Members Home Benefits / Discounts Continuing Legal Education Fastcase Login LegalFuel Health and Wellness Center Lawyers Advising Lawyers Legislative Activity Appointments and Applications Join Lawyer Referral Service Pro Bono Service and Reporting Florida Lawyers Assistance Directories Directories Home Lawyer Directory Authorized House Counsels Certified Foreign Legal Consultants Law Faculty Affiliates Florida Registered Paralegals Courts Legal Groups Judicial Nominating Commissions State of Florida Federal Government Rules, Ethics and Professionalism Rules, Ethics and Professionalism Home Rules Regulating The Bar Ethics Henry Latimer Center for Professionalism ‘To inculcate in its members the principles of duty and service to the public, to improve the administration of justice, and to advance the science of jurisprudence.’ ~ From the Rules Regulating The Florida Bar © 2023 The Florida Bar. All rights reserved. Accessibility Privacy Policy Disclaimer & Terms of Use CONTACT Address: 651 E Jefferson St Tallahassee, FL 32399 Phone: 850-561-5600 ▲",
    "originSummary": [
      "The 21st Annual Cup of Justice Golf Classic raised over $74,000 to support free legal assistance in Palm Beach County.",
      "Steve Gawaithe won a hole-in-one at the event, earning a cash prize of $25,000.",
      "The tournament was chaired by Robert Shalhoub and sponsored by organizations such as First Horizon Foundation and Florida Crystals Corporation."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "AI Reasoning vs. Reality: Debunking the Wage Gap Between Lawyers and Architects",
    "originLink": "https://commonedge.org/an-ai-experiment-why-do-lawyers-make-more-than-architects/",
    "originBody": "Essays An AI Experiment: Why Do Lawyers Make More Than Architects? 11.20.2023 By Eric J. Cesal In one of my early experiments with ChatGPT, I asked it a question that I’m sure every architect has considered at some point: Why do lawyers make more than architects? I’ve been considering this haunting question since 2007, when I read Peter Eisenman’s lament that his son, who’d graduated from law school three years prior, made more than he did after 40 years of field-leading practice. I asked ChatGPT this question because I’ve never really found a satisfactory answer, either from architects, lawyers, professors, economists, or anyone else. It just seems to be one of those truths at which we collectively shrug and offer tepid rationalizations like Architects undersell the value of their work. Really? Why? Because they all have self-esteem problems? Or enjoy poverty? Do you know many architects who aren’t fans of their own designs? I thought maybe ChatGPT would know, or at least have some ideas. It seems to know a lot. Its training model was purportedly the entire internet, and if I met a human who had read the entire internet, the first thing I would do would be to ask them all the stubborn, unanswered questions in my life. Reasoning, Not Recall Sam Altman, cofounder of Open AI, has repeatedly stressed that ChatGPT is a “reasoning engine,” not a knowledge database, which becomes clear when you use it regularly. ChatGPT doesn’t have the answers—it’s generating novel ones based on some insanely complicated prediction mathematics. It’s trying to figure things out, which is probably why it sometimes returns incorrect or even nonsensical responses. Case in point: it gave me a whole host of reasons that lawyers make more than architects, none of which made any sense (a bit disappointing), but largely in keeping with every human response I’ve ever heard, which, in itself, makes a certain kind of sense. If, in reading the entire internet, ChatGPT found 1,000 instances of the question about lawyers and architects and in 900 cases the response was Because lawyers require more rigorous training, it’s trained to anticipate that response as the most probable correct response, even if it isn’t true. I’m dramatically oversimplifying how ChatGPT works, but you get the idea. In a chat that would eventually last almost four hours, I asked ChatGPT, Why do lawyers make more than architects? in a variety of ways and interrogated its responses. It initially gave me five reasons, paraphrased below: Education and Licensing: Lawyers require more training and more rigorous licensure. Supply and Demand: The demand for lawyers often exceeds supply. Demand for architects varies greatly, depending on the economy. Billing Structure: Lawyers get paid by the hour, and on retainer. Architects charge by the project, resulting in lower compensation. Risk and Liability: Lawyers take on higher risk and liability. Perceived Value: Society perceives legal work as being of high importance, warranting higher compensation. ChatGPT’s responses seemed plausible above all else, and roughly in line with basic macroeconomic principles. If any Profession A requires more training, takes on more risk, and is in more demand than any Profession B, we would expect Profession A to earn more. That Can’t Be Right … However, I knew immediately that some of these assumptions weren’t true: e.g., it takes far more training to become a licensed architect than a licensed lawyer. ChatGPT knows this too, and was separately able to accurately summarize all the training requirements for both lawyers and architects for me. So how could it then reason that lawyers’ high compensation relative to architects was partially explained by the fact that: Law is generally a postgraduate degree requiring a bachelor’s degree first, then typically three years of law school. In comparison, you can become an architect with a bachelor’s degree, though some may also pursue a master’s degree. Passing the Bar Exam to become a lawyer is also typically quite challenging and requires substantial preparation. While simultaneously “knowing” that entry requirements for architecture are longer and more rigorous? My intuition at the time was that it was just reasoning backward. Compensation generally correlates with higher levels of training and more rigorous exams, and lawyers make more than architects, so one might assume that the path to becoming a lawyer must be harder and more rigorous. It occurred to me that it wasn’t ChatGPT’s fault—this is also the way most humans think. It was merely imitating widely held human inferences. (So much for artificial “intelligence.”) Rather than immediately confront ChatGPT with its own logical contradiction, I did what a good lawyer does: I gradually built a case by picking apart all five components of its initial reasoning. If it could admit that all of its component logics were incorrect, perhaps it would be willing to revisit its own answer to my initial question. Reason 1: Education Education was the first and easiest. I merely asked it to enumerate the education and licensing requirements for each profession and compare them side by side. Then I asked it to assess which profession had the greater, longer, or more rigorous training requirements, like so: I then pressed the attack, pointing out that this wasn’t exactly an apples-to-apples comparison. While a bachelor’s-plus law degree would take 7 years, the first four years didn’t necessarily include the study of law. One could major in anything, really, and proceed on to law school. A better way to calculate the length of study would be through assessing the required credit hours necessary for each course of study. That becomes difficult, because there are several different acceptable degree courses in architecture. Moreover, each institution is different. But, as a ballpark figure, and excluding general electives, it takes about 115 dedicated credit hours of study to complete a B.Arch, and about 123 for an M.Arch (three-year program). It only takes 80 to 90 hours for a J.D. Somewhere between 30% and 50% more schooling is required to become an architect. It’s not even close. Case closed. We moved onto Reason 2. Reason 2: Supply and Demand GPT’s observation that the demand for legal services often exceeds the supply of lawyers might be true. Its observation that the demand and supply for architects can vary greatly based on the health of the construction and real estate industries is certainly true. But I couldn’t get it to explain why these two things being true would explain a wage discrepancy. One thing being consistently high and another thing varyingly greatly don’t mean that one would necessarily exceed the other, overall. Moreover, supply and demand issues tend to correct over time—as demand increases, wages increase, and more people flow into the profession. Between 2010 and 2020, lawyers and architects in the U.S. experienced nearly identical growth, relative to population, and fairly similar wage growth as well: Which suggests that the number of architects and lawyers, as well as their relative compensation, is probably right where it should be, at least according to the market. ChatGPT vaguely agreed. Reason 3: Billing Structure According to ChatGPT, one of the reasons that architects make less is because they work off of a fixed-fee project basis, while lawyers bill by the hour. Therefore, when a project encounters unforeseen challenges, the architect ends up eating his own fee, thus earning less. While that’s not uncommon in our profession, I pointed out that architects have all sorts of fee mechanisms that they deploy—like contingencies, cost-plus, additional services, etc.—that can take some of the risk out of fixed-fee contracts. It agreed. I also pointed out that in most contractual arrangements, the party taking on the highest risk is accorded the largest fee, so even if an architect is on a strictly fixed-fee contract, they’re certainly absorbing a lot of the client’s risk in the progression of the project, and would therefore be entitled to higher fees, not lower ones. ChatGPT vaguely conceded the point, but then began to waffle. It waffled through the whole experiment, actually. All four hours of it. When asked to combine several obvious truths like higher risk should mean more fees + fixed fee contracts means more risk for the architect = architects should earn higher fees, it seemed to get flummoxed. It seemed as if it could not square that conclusion with its prior one: architects earn less because they take on fixed-fee contracts and hack their own fees when the project goes sideways. As if asking itself Why don’t architects just price the risk and add that to their fee upfront? Why indeed, ChatGPT, why indeed. Reason 4: Risk and Liability ChatGPT had opined that one of the reasons that lawyers earn more is because they take on more risk. When asked for specifics, it enumerated five types of risk endured by lawyers: Financial risk (for lawyers who work on contingency) Reputational risk (the risk of losing a case) Ethical and legal risk (they can be sued for malpractice) Career progression risk (taking on a high-profile case and losing) “Emotional risk” (which it explained thusly: Law can be a highly stressful profession, with demanding clients, tight deadlines, high stakes outcomes, and other pressures. This emotional toll can affect lawyers’ mental health, and could be considered another form of risk they take on.) It was the first time I’d actually made a physical wanking motion at my computer during this conversation, but it would not be the last. I pointed out that architects—and indeed all professionals—face these risks as well, and that architects face myriad external risks over which they have no control (unforeseen site conditions, mercurial clients, etc.). The lawyer’s risks that ChatGPT had pointed out were mostly related to a lawyer’s own performance. If a lawyer was terrible at his job and lost a case, it wasn’t so much a risk as much as it was that lawyer’s just being a bad lawyer. ChatGPT seemed to concede the point. I argued that if an architect was taking on so much risk external to her own performance, she should be compensated more than the lawyer, who was only managing the risk of himself being a shitty lawyer. Again, it waffled: In essence, you are correct in observing that architects may assume more risks related to external, uncontrollable factors while lawyers contend more directly with risks associated with their personal performance. This difference in risk profiles is indeed one of the factors influencing the earning potential in the two professions. However, it’s also worth bearing in mind that these are just part of a wider array of factors influencing earnings, from market demand to years of training to societal perceptions and more. It repeated this parry multiple times. Whenever I would point out the flaws in its component logic, it would return to a general, abstract there’s many factors that affect compensation argument. Interestingly, arguing like a human once again. Reason 5: Perceived Value Finally, I interrogated ChatGPT about why it thought that society perceived legal services were of “high importance” relative to design services. To be fair to ChatGPT, it made no claim that legal services were of more actual value than design services; it acknowledged that both lawyers and architects perform vital roles in society. But it opined that legal services were perceived as being more valuable, mostly because of two things: Visibility: The first was visibility. ChatGPT hypothesized (correctly, I think) that one of the reasons lawyers are perceived as having a greater value is because we see a lot more of them. Even if we don’t use them, they’re portrayed in all media forms as doing critical, dramatic, often life-saving work, even when it’s the case that most lawyers spend most of their days reading. Direct vs. Indirect Impact: The second turned on direct vs indirect consequences. The successes or mistakes of an attorney are visceral and can be felt in real time. If you’re standing trial for murder and your lawyer does a bad job, you go straight to jail for the rest of your life. If you’re renovating your home and the architect does a bad job, those effects might not be felt for years or decades, depending on the error. Taken together, architects make less than lawyers because, well, people generally think that they should. Where would they get such an idea? Unfortunately, I think it starts at home and finds its roots in something familiar: architects often value their work product more than their compensation for it. Once they do, the following nonsense starts to seem logical, even necessary: Working for free. Asking other architects to work for free. Doing a schematic design for a client, for free, that they can use in their marketing materials while they raise money. Bidding a project below cost in order to “get the job.” Balancing the books by underpaying junior staff. Accepting a bunch of risk on behalf of the client, without being compensated for it, in order to get the project moving. Squaring the Circle Once I had sufficiently broken down all of ChatGPT’s component rationalizations, I asked it to reassemble them in the form of a table, comparing each of its rationalizations on a line-by-line basis, and evaluate which profession took on the most risk, which had the hardest training requirements, etc. The table eventually looked like this: By breaking down ChatGPT’s original answer to the Architects vs. Lawyers question, and getting it to analyze its own response at a more granular level, I was able to help it see that architects do indeed have longer and more rigorous training requirements, have analogous billing structures, take on more risk, and have similar levels of social value. Moreover, the categories where lawyers justify their inflated compensation seem exclusively perceptual. Across any measurable, objective criteria, architects and lawyers are either on par, or architects exceed their lawyer colleagues in the professional rigors that demand compensation. To conclude the exercise, I asked it to evaluate the table and make a judgment about why lawyers tend to earn more than architects. It subsequently reversed its original answer, concluding that there was no good reason for the discrepancy, and that architects should make at least as much, if not more, than lawyers. Just kidding. It immediately defaulted to its original conclusion. Much like a human, when ChatGPT is confronted with its own logical errors, it will sometimes self-correct. Also like a human, it will also stubbornly hold onto two contradictory truths and argue both of them at the same time. So ended my experiment. The Middle Wisdom of Humanity I came away feeling as if I’d had a glimpse into one of the shaky certainties of humanity. Our stubborn tendency to “know” things, even when the logics that underwrite them are exposed as incorrect—what Stephen Colbert called “truthiness,” back when he was funny. If AI is going to be the “Google” of the future (the reference set that we all consult when we want to know the “real” answer to a question), then it’s troubling that this machine can’t ultimately explain why architects make less than lawyers, but seems to think that they should. The ideas/biases/certainties that are being programmed into AI today may well become the default certainties of humanity tomorrow. And ChatGPT seems to have already assimilated the architects are underpaid trope, and is confident of its reasoning. It was tempting to come away from the exercise with a deep suspicion about ChatGPT’s ability to reason, but I don’t think that’s it. I think maybe I was just asking it to make sense of something that doesn’t make sense. All that it is programmed to do is to fashion responses that an informed, competent human would offer. And why would any rational human go through more training, more risk, and more work in order to earn less? Why would any rational human be so passionate about their work product and its value, and then devalue the labor that created the product? I had given it an impossible circle to square. Architecture’s approach to its own value proposition violates basic laws of risk and reward, supply anddemand, etc., so much so that even a genius-level A.I. can’t make heads or tails of it. So maybe we’re the ones who are reasoning backward. Featured image: Midjourney V5 responding to the prompt “a photograph of a full body shot of an average lawyer, white background” on the left, and the prompt “a photograph of a full body shot of an average architect, white background” on the right. There are many interpretations of the image, and much to be gleamed from what A.I. “thinks” a lawyer looks like, vs. what an architect looks like. (They’re all white and male, for one thing!) But I think the relevant part for this article is that lawyers are depicted as white collar professionals, wearing suits, etc. Architects are portrayed as something less professional, and in two cases look remarkably like contractors. —EJC Share this story Facebook Twitter Email Tags AI architect's compensation architecture ChatGBT Author Bio Eric J. Cesal is a designer, educator, and author who works at the intersection of natural disasters, climate change, and design futures. He is the author of Down Detour Road and former host of Social Design Insights. He has taught at UC Berkeley and Harvard on the topics of disaster reconstruction, resilience and sustainable design, and has an M.Arch, an MBA, and a Masters in Construction Management from Washington University.",
    "originSummary": [
      "The author examines the reasoning abilities of an AI language model, ChatGPT, in relation to the wage gap between lawyers and architects.",
      "Various explanations provided by ChatGPT are questioned and refuted by the author, highlighting the flawed assumptions and limitations of AI reasoning.",
      "The article emphasizes the importance of addressing and correcting faulty logic in AI systems, while also discussing the training, risk, and societal value of architects as arguments for equal compensation. The lack of diversity in AI portrayals of lawyers and architects is also mentioned."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "AI's Role in the Legal Industry: Challenges and BenefitsCorporate Counsel",
    "originLink": "https://www.law.com/corpcounsel/2023/11/19/ai-is-here-to-stay-what-will-it-take-to-use-it-wisely/",
    "originBody": "Law.com",
    "originSummary": [
      "Law.com is a website dedicated to providing news, analysis, and resources specifically for legal professionals and the legal industry.",
      "The website offers a wide range of content, including articles, expert analysis, case studies, and legal templates, to keep legal professionals informed and up-to-date.",
      "Law.com serves as a valuable resource for legal professionals looking to stay informed about the latest trends, developments, and insights in the legal field."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "UnitedHealth accused of AI denying claims, lawsuit alleges",
    "originLink": "https://www.usatoday.com/story/news/health/2023/11/19/unitedhealth-artificial-intelligence-denies-claims-in-error-lawsuit-says/71579822007/",
    "originBody": "HEALTH UnitedHealth Group Add Topic Is your health insurer using AI to deny you services? Lawsuit says errors harmed elders. Ken Alltucker USA TODAY For years, vital decisions about who got medical care coverage took place in back offices at health insurance companies. Now, some of those life-altering decisions are being made by artificial intelligence programs. At least that's the contention of the two families who sued UnitedHealth Group this week, saying the insurance giant used emerging technology to deny or shorten rehabilitation stays for two elderly men in the months before they died. They say that UnitedHealth's artificial intelligence, or AI, is making \"rigid and unrealistic\" determinations about what it takes for patients to recover from serious illnesses and denying them care in skilled nursing and rehab centers that should be covered under Medicare Advantage plans, according to a federal lawsuit filed in Minnesota by the estates of two elderly Wisconsin patients. The lawsuit, which seeks class-action status, says it is illegal to let AI override doctors' recommendations for these men and patients like them. The families say assessments like that should be done by medical professionals. The families note in the suit that they believe the insurance company is denying care to elderly patients who won't fight back even though evidence shows the AI is doing a lackluster job of assessing people's needs. The company used algorithms to determine coverage plans and override doctors' recommendations despite the AI program's astonishingly high error rate, they say. More than 90% of patient claim denials were overturned through internal appeals or a federal administrative law judge, according to court documents. But in reality, few patients challenged the algorithms' determinations. A tiny percentage of patients − .2% − choose to fight claim denials through the appeals process. The vast majority of people insured by UnitedHealth's Medicare Advantage plans \"will either pay out-of-pocket costs or forgo the remainder of their prescribed post-acute care,\" the lawsuit says. Attorneys representing the families suing the Minnesota-based insurance giant said the high rate of denials is part of the insurance company's strategy. \"They're placing their own profits over the people that they are contracted with and then legally bound to cover,\" said Ryan Clarkson, a California attorney whose law firm has filed several cases against companies using AI. \"It's that simple. It's just greed.\" UnitedHealth told USA TODAY in a statement naviHealth's AI program, which is cited in the lawsuit, isn't used to make coverage determinations. \"The tool is used as a guide to help us inform providers, families and other caregivers about what sort of assistance and care the patient may need both in the facility and after returning home,\" the company said. Coverage decisions are based on the Centers for Medicare & Medicaid Services' criteria and the consumer's insurance plan, the company said. \"This lawsuit has no merit, and we will defend ourselves vigorously,” the company said. Lawsuits of this type are not new. They are part of a growing body of litigation. In July, the Lawson law firm filed a case against CIGNA Healthcare alleging the insurer employed AI to automate claims rejections. The firm also has pursued cases against ChatGPT maker OpenAI and Google. Families pay for expensive care that the insurer denies The plaintiffs in the suit this week are the relatives of two deceased Wisconsin residents, Gene B. Lokken and Dale Henry Tetzloff, who were both insured by UnitedHealth's private Medicare plans. In May 2022, Lokken, 91, fell at home and broke his leg and ankle, requiring a brief hospital stay followed by a month in a rehab facility while he healed. Lokken's doctor then recommended physical therapy so he could regain strength and balance. The Wisconsin man spent less than three weeks in physical therapy before the insurer terminated his coverage and recommended he be discharged and sent to recover at home. A physical therapist described Lokken's condition as \"paralyzed\" and \"weak,\" however his family appeals for continued therapy coverage were rejected, according to the lawsuit. His family opted to continue with treatment despite the denial. Without coverage, the family had to pay $12,000 to $14,000 per month for about a year of therapy at the facility. Lokken died at the facility in July 2023. The other man's family also raised concerns that necessary rehabilitation services had been denied by the AI algorithm. Tetzloff was recovering from a stroke in October 2022, and his doctors recommended the 74-year-old be transferred from a hospital to a rehab facility for at least 100 days. The insurer initially sought to end his coverage after 20 days, but the family appealed. The insurer then extended Tetzloff's stay another 20 days. The man's doctor had recommended additional physical and occupational therapy, but his coverage ended after 40 days. The family spent more than $70,000 on his care during the next 10 months. Tetzloff spent his final months in an assisted living facility, where he died on Oct. 11. 10 appeals to rehab a broken hip The legal action comes after Medicare advocates began raising concerns about the routine use of AI technology to deny or reduce care for older adults on private Medicare plans. In 2022, the Center for Medicare Advocacy examined multiple insurers' use of artificial intelligence programs in rehabilitation and home health settings. The advocacy group's report concluded that AI programs often made coverage decisions that were more restrictive than what Medicare would have allowed and the decisions lacked the level of nuance necessary to evaluate the unique circumstances of each case. \"We saw more care that would have been covered under traditional Medicare denied outright or prematurely terminated,\" said David Lipschutz, associate director and senior policy attorney for the Center for Medicare Advocacy. Lipschutz said some older adults who appeal rejections might win a reprieve only to be shut down again. He cited the example of a Connecticut woman who sought a three-month stay at a rehab center as she recuperated from a hip replacement surgery. She filed and won 10 appeals after an insurer repeatedly attempted to terminate her coverage and limit her stay. Importance of having 'human in the loop' Legal experts who are not involved in these cases said artificial intelligence is becoming a fertile target for people and organizations seeking to rein in or shape the use of emerging technology. Gary Marchant, faculty director at the Center for Law, Science and Innovation at Arizona State University's Sandra Day O'Connor College of Law, said an important consideration for health insurers and others deploying AI programs is making sure that humans are part of the decision-making process. While AI systems can be efficient and complete rudimentary tasks quickly, programs on their own can also make mistakes, Marchant said. \"Sometimes AI systems aren't reasonable and they don't have common sense,\" said Marchant. \"You have to have a human in the loop.\" In cases involving insurance companies using AI to guide claim decisions, Marchant said a key legal factor might be how much a company defers to an algorithm. The UnitedHealth lawsuit states that the company limited workers' \"discretion to deviate\" from the algorithm. Employees who deviated from the AI program's projections faced discipline or termination, the lawsuit said. Marchant said one factor to track in the UnitedHealth cases and similar lawsuits is how closely employees are required to follow an AI model. \"There, clearly, has to be an opportunity for the human decider to override the algorithm,\" Marchant said. \"That's just a huge issue in AI and healthcare.\" He said it's important to consider the consequences of how companies set up their AI systems. Companies should think about how much deference they give to an algorithm, knowing that AI can digest huge amounts of data and be\"incredibly powerful\" as well as \"incredibly accurate,\" he said, and leaders should also keep in mind that AI \"sometimes can just be completely wrong.\" Ken Alltucker is on X, formerly Twitter, at @kalltucker, or can be emailed at alltuck@usatoday.com.",
    "originSummary": [
      "Two families have filed a lawsuit against UnitedHealth Group, accusing the insurance company of using AI programs to deny or limit rehabilitation stays for their elderly relatives.",
      "The families claim that the AI is making unrealistic and inflexible determinations about patient recovery, leading to the denial of necessary care covered under Medicare Advantage plans.",
      "This case highlights a growing trend of litigation surrounding the use of AI by insurers to reject claims, and raises concerns about the need for human involvement in the decision-making process to prevent overly rigid decisions."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "AI Innovation in Legal Profession: Automating Contracts and Streamlining Document Review Process",
    "originLink": "https://techwireasia.com/2023/11/dealing-with-contracts-without-humans-is-now-a-possibility-with-luminances-ai-innovation/",
    "originBody": "15 SOCIAL BUZZ AI innovation – helping lawyers out, or putting them out of business? Dealing with contracts without humans is now a possibility with Luminance’s AI innovation By Muhammad Zulhusni20 November, 2023 Luminance’s AI innovation revolutionizes the legal industry with automated contract negotiations. AI transforms legal tasks into efficient processes through Luminance’s successful automation of contract negotiations. What will AI in the legal profession mean for lawyers? We are witnessing a paradigm shift in how AI is being used across various industries. A recent breakthrough AI firm Luminance marks a significant milestone in integrating AI into the legal domain. Showcasing AI’s ability to autonomously negotiate contracts, Luminance’s demonstration challenges traditional perceptions and signals the emergence of a new era in legal processes. This pioneering effort illustrates the potential of AI to transform complex, labor-intensive tasks, paving the way for more efficient, automated solutions in the legal field. This transformative initiative by Luminance involves its large language model (LLM), which has been meticulously trained on 150 million legal documents. Successfully automating a contract negotiation between two virtual entities, this UK-based company claims to have delivered the first fully AI-driven contract negotiation. This breakthrough event serves as a compelling example of how industry-specific AI models, such as Luminance’s, are increasingly diverging from general-purpose models like ChatGPT to cater to specialized sector needs, thereby revolutionizing how industries and professions like the law use technology for efficiency and precision. HERE’S HOW ENTERPRISES COULD EMBRACE GENERATIVE AI AND CLOUD TECH IN 2024 Dashveenjit Kaur9 November, 2023 Custom AI in various industries OpenAI, inching toward its goal of achieving artificial general intelligence (AGI), recently introduced a platform that allows users to craft custom versions of ChatGPT for their own industry-specific uses. This development extends the earlier ChatGPT plugins but now boasts enhanced functionalities. It targets regular ChatGPT users and enterprise customers who utilize a private version of ChatGPT for internal purposes. The new platform let users engage with existing custom GPTs, like those integrated with Canva for design creation or Zapier for task automation. It also offers the opportunity to build personalized chatbots without coding expertise. The intuitive nature of GPTs, that understand natural language, means that anyone with a concept and some guidelines can develop their AI tool. More and more law firms are jumping on the Luminance bandwagon. OpenAI plans to launch a GPT store later this month, functioning like an app store. This marketplace will allow users to explore and create custom GPTs and potentially monetize their creations. Customized models like Luminance’s offer the dual benefits of data privacy and specialized expertise, particularly valuable for businesses looking to automate specific tasks. Luminance focuses on legal contracts, aiming to reduce the hours lawyers spend on negotiations. The future of legal work with AI innovation Legal document review is notoriously labor-intensive, but LLMs like Luminance’s Autopilot can rapidly process and analyze vast amounts of data. Automating routine contract reviews, such as non-disclosure agreements, could save legal professionals time. Jaeger Glucina, Luminance’s chief of staff and managing director, told CNBC that Autopilot manages everyday negotiations, allowing lawyers to focus their creativity on more critical tasks rather than getting mired in routine work. GITHUB’S VISION ON AI, OPEN SOURCE, AND THE NEW FRONTIER OF INNOVATION Muhammad Zulhusni17 November, 2023 She explained that the entire process, from opening a contract in Word to negotiating terms and sending it for electronic signature via DocuSign, can now be entirely managed by AI. This AI is not only legally trained but also attuned to the specific needs of the business. How does Autopilot work? So, how does Autopilot work? AI plays a central role in a demonstration involving Luminance’s general counsel and the general counsel of ProSapient, a research firm. The lawyers’ photos are displayed on two monitors, but the AI actively analyzes and makes recommendations on a non-disclosure agreement (NDA). NDAs, known for their strict confidentiality requirements, often require extensive legal scrutiny. According to Glucina, these documents can delay business processes, affecting revenues and partnerships. By streamlining the NDA process, AI can significantly impact various business operations. AI innovation in the law – are we evolving towards judgment algorithms? Legal teams typically spend about 80% of their time reviewing and negotiating routine documents. Luminance’s software tackles this by initially marking contentious clauses in red. The AI then modifies these clauses to more suitable terms, logging the changes as the negotiation progresses. It also considers the company’s standard contract negotiation preferences. In one instance during the demonstration, the AI revised the NDA’s initial proposal of a six-year term down to three years. This change was made under Luminance’s standard policy preferences, and demonstrates the AI’s capability to understand and apply specific organizational guidelines, showcasing its adaptability in real-world scenarios. Glucina highlights the advantage of using a specialized tool like Luminance Autopilot over more general-purpose platforms like OpenAI’s software. Tailored specifically for the legal industry, Luminance offers a more focused approach than general tools like ChatGPT, Dall-E, and Anthropic’s Claude. Luminance’s Autopilot feature represents a significant advancement over its chatbot, ‘Ask Lumi,’ which launched earlier this year. While ‘Ask Lumi’ is the company’s first chatbot supported by legal-grade AI, allowing users to interact with contracts in Microsoft Word and get immediate AI responses, Autopilot takes this further. Drawing on its experience with over 150 million legal documents, Luminance combines its proprietary AI technology and deep legal expertise. This extensive background positions it as a highly advanced legal language model from the company’s perspective. With so much work removed from their daily pile, will bored lawyers become the norm? Luminance’s legal pre-trained transformer (LPT) technology, which stands apart from broader GPT technologies, is specifically trained on legally verified documents. This specialized training enables the LPT to generate content and analyze and comprehend content that third parties have created or edited. By combining generative and analytical capabilities, this AI technology is meticulously designed to maintain the highest standards of legal rigor. With Luminance’s chatbot technology, users can inquire about specific contract details within Microsoft Word, such as the contracting parties, jurisdiction, or assignability, and get instant responses. Furthermore, ‘Ask Lumi’ enhances efficiency across business departments by enabling non-legal teams to obtain answers to common queries, relieving the legal team of repetitive tasks. However, the capabilities of ‘Ask Lumi’ are just the beginning. Autopilot can function independently without human intervention, though human oversight is still integral. The software logs all AI-driven changes meticulously, ensuring transparency and accountability. Legal AI has been tested for some time, but Luminance believes it’s now reached the point of professional roll-out. GENERATIVE AI LAW LEGAL 15 SOCIAL BUZZ By Muhammad Zulhusni Muhammad Zulhusni As a tech journalist, Zul focuses on topics including cloud computing, cybersecurity, and disruptive technology in the enterprise industry. He has expertise in moderating webinars and presenting content on video, in addition to having a background in networking technology. READ MORE GXBank: Everything you need to know about Malaysia’s first digital bank Google announces data residency locations and AI partnerships Why quantum computing is the future of finance GitHub’s vision on AI, open source, and the new frontier of innovation Microsoft finally builds its own chips TRENDING TOPICS ANDROID GENERATIVE AI GOOGLE CLOUD QUANTUM COMPUTING AI ARTIFICIAL INTELLIGENCE CLOUD PASSWORD FOREIGN EXCHANGE GREEN TECH",
    "originSummary": [
      "Luminance's AI innovation has the potential to automate contract negotiations and transform the legal industry.",
      "The AI model has been trained on 150 million legal documents and can independently negotiate contracts.",
      "OpenAI's platform allows users to create industry-specific versions of ChatGPT, including for legal contracts, further enhancing automation in the legal profession."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "Navigating the Copyright Conundrum of Generative AI for Advertisers",
    "originLink": "https://digiday.com/marketing/how-advertisers-can-deal-with-generative-ais-copyright-conundrum/",
    "originBody": "SubscribeLogin News Digiday + Podcasts Events Awards News Digiday + Podcasts Events Awards Hot Topics The Programmatic Publisher The Programmatic Marketer Marketing on Platforms Life Beyond the Cookie Brands in Culture Gaming & Esports Modern Newsroom All Topics Login Subscribe Special Projects Digiday+ Research WTF Series Newsletters Influencer or creator? Here’s how marketers can know who to hire Media Marketing Media Buying Future of TV Español Special Projects Digiday+ Research WTF Series Newsletters Generative AI How advertisers can deal with generative AI’s copyright conundrum By Tim Peterson • November 20, 2023 • 1 min read • Facebook Twitter LinkedIn Reddit Ivy Liu Brands and agencies know better than to use copyrighted work in their campaigns without permission (or they should). But what about content created using generative AI tools that may be trained on copyrighted work without the copyright owners’ consent? To what extent copyright law applies to generative AI tools is a legal gray area. Companies including OpenAI, Google and Microsoft assert it’s fair use, whereas others such as News Media Alliance, IAC and The New York Times argue it’s not. The U.S. Copyright Office is studying the matter, but in the meantime, this ambiguity is cause for concern among marketers who may be best off deploying tactics to insulate themselves against any potential copyright claims, as covered in the video below. Facebook Twitter LinkedIn Reddit https://digiday.com/?p=526195 Trending in Generative AI 01 Beyond Ads Why the NFL released an AI-powered game with Amazon 02 Generative AI IAC and News Corp call out generative AI companies for scraping their content without compensation Sponsored How AI-driven contextual targeting helps advertisers beyond cookie deprecation 03 Generative AI As AI regulations loom, tech companies add new ways to improve their standards Most Read 01 Navigating Economic Instability Media companies announce more layoffs to cut costs, blaming a relentlessly challenging ad market 02 Life Beyond the Cookie GroupM agrees to new program that lets clients experiment within Google’s Privacy Sandbox 03 Strategizing for the Future Publishers double down on events heading into 2024 04 Brands in Culture Here is the anatomy of the 2023 Christmas ad season 05 Marketing on Platforms Snap’s new president of Americas and global partnerships talks returning to revenue growth after a rocky 2023 More in Marketing View More Member Exclusive Digiday+ Research: Brands turn up their TikTok investments for the holidays November 20, 2023 • 5 min read Facebook Twitter LinkedIn Reddit Brands have been spending more on TikTok leading up to the holiday season this year, teeing up the platform to play an important role in their holiday marketing efforts. The Creator Economy Influencer or creator? Here’s how marketers can know who to hire November 20, 2023 • 4 min read Facebook Twitter LinkedIn Reddit Influencer and creator marketing is still on the rise, and the shifts that affect whether marketers partner with one versus the other are just part of the ongoing evolution. Gaming & Esports Why McDonald’s and Coca-Cola still see gaming as an experimental marketing channel November 17, 2023 • 4 min read Facebook Twitter LinkedIn Reddit Despite the rise of gaming and esports as an entertainment channel in recent years, McDonald’s and Coca-Cola are still figuring out exactly which key performance metrics signal success among the gaming audience. For now, they’re leaning on tried-and-true KPIs such as brand awareness and brand loyalty — when they’re scrutinizing the numbers at all, that is. DIGIDAY+ Get access to tools and analysis to stay ahead of the trends transforming media and marketing Subscribe Newsletter Get Digiday's top stories every morning in your email inbox. Connect Follow @Digiday for the latest news, insider access to events and more. FAQ Advertise Privacy Policy Masthead Digiday Media © 2023. All rights reserved",
    "originSummary": [
      "The legality of using generative AI tools to create content without the permission of copyright owners is uncertain and considered a gray area.",
      "Different companies have differing opinions on whether this falls under fair use or not.",
      "The U.S. Copyright Office is currently investigating the matter, and until it is resolved, marketers should be cautious and take measures to protect themselves from possible copyright claims."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "Germany, France, and Italy agree on AI regulation: mandatory self-regulation and model cards emphasized",
    "originLink": "https://www.reuters.com/technology/germany-france-italy-reach-agreement-future-ai-regulation-2023-11-18/",
    "originBody": "Disrupted Exclusive: Germany, France and Italy reach agreement on future AI regulation By Andreas Rinke November 19, 20239:56 PM UTCUpdated 16 hours ago AI (Artificial Intelligence) letters are placed on computer motherboard in this illustration taken June 23, 2023. REUTERS/Dado Ruvic/Illustration/File Photo Acquire Licensing Rights Summary No sanctions should be imposed, joint paper says Commitments would involve large and small providers Paper suggests \"mandatory self-regulation through codes of conduct\" BERLIN, Nov 18 (Reuters) - France, Germany and Italy have reached an agreement on how artificial intelligence should be regulated, according to a joint paper seen by Reuters, which is expected to accelerate negotiations at the European level. The three governments support \"mandatory self-regulation through codes of conduct\" for so-called foundation models of AI, which are designed to produce a broad range of outputs. But they oppose \"un-tested norms.\" Advertisement · Scroll to continue \"Together we underline that the AI Act regulates the application of AI and not the technology as such,\" the joint paper said. \"The inherent risks lie in the application of AI systems rather than in the technology itself.\" The European Commission, the European Parliament and the EU Council are negotiating how the bloc should position itself on this topic. The paper explains that developers of foundation models would have to define model cards, which are used to provide information about a machine learning model. Advertisement · Scroll to continue \"The model cards shall include the relevant information to understand the functioning of the model, its capabilities and its limits and will be based on best practices within the developers community,\" the paper said. \"An AI governance body could help to develop guidelines and could check the application of model cards,\" the joint paper said. Initially, no sanctions should be imposed, the paper said. Advertisement · Scroll to continue If violations of the code of conduct are identified after a certain period of time, however, a system of sanctions could be set up. Germany's Economy Ministry, which is in charge of the topic together with the Ministry of Digital Affairs, said laws and state control should not regulate AI itself, but rather its application. Digital Affairs Minister Volker Wissing told Reuters he was very pleased an agreement had been reached with France and Germany to limit only the use of AI. \"We need to regulate the applications and not the technology if we want to play in the top AI league worldwide,\" Wissing said. State Secretary for Economic Affairs Franziska Brantner told Reuters it was crucial to harness the opportunities and limit the risks. \"We have developed a proposal that can ensure a balance between both objectives in a technological and legal terrain that has not yet been defined,\" Brantner said. As governments around the world seek to capture the economic benefits of AI, Britain in November hosted its first AI safety summit. The German government is hosting a digital summit in Jena, in the state of Thuringia, on Monday and Tuesday that will bring together representatives from politics, business and science. Issues surrounding AI will also be on the agenda when the German and Italian governments hold talks in Berlin on Wednesday. Reporting by Andreas Rinke; Writing by Maria Martinez; Editing by Mike Harrison, Barbara Lewis and Diane Craft Our Standards: The Thomson Reuters Trust Principles. Acquire Licensing Rights , opens new tab Read Next Disrupted category China successfully launches a pilot reusable spacecraft, state media report China successfully launched a pilot reusable spacecraft with its Long March-2F carrier rocket on Friday, state media CCTV reported. Disrupted category Apple forecasts faster sales growth, strong iPhone demand despite glum economy Apple Inc on Thursday said parts shortages are easing and that demand for iPhones is unceasing despite consumers tightening other spending, helping it top Wall Street expectations and forecast faster sales growth ahead. Disrupted category Meta posts first-ever revenue drop as inflation throttles ad sales Meta Platforms Inc issued a gloomy forecast after recording its first ever quarterly drop in revenue on Wednesday, with recession fears and competitive pressures weighing on its digital ads sales. Disrupted category YouTube's quarter shows problems Meta may face: TikTok, weakening economy Mostly reassuring financial results from Alphabet Inc included one concerning sign for social media companies: Its YouTube service posted its second difficult quarter in a row. Disrupted category Why Spotify's 'Car Thing' was destined for the hardware graveyard Spotify Technology SA's Car Thing player is now a thing of the past. Disrupted category Microsoft soothes market fears with forecast for strong revenue growth Microsoft Corp on Tuesday forecast revenue this fiscal year would grow by double digits, driven by demand for cloud computing services and sending shares up 5%.",
    "originSummary": [
      "Germany, France, and Italy have reached an agreement on regulating AI, which is expected to aid in European-level negotiations.",
      "The joint paper supports \"mandatory self-regulation through codes of conduct\" for AI foundation models, but opposes \"un-tested norms.\"",
      "The paper proposes using model cards to provide information about machine learning models and suggests enforcing sanctions for code of conduct violations. The focus is on regulating AI applications rather than the technology itself."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "EU heavyweights differ on AI regulations, support UK-style 'wait-and-see' approach",
    "originLink": "https://www.computing.co.uk/news/4148348/eu-heavies-disagree-blocs-ai-plans-support-uk-style-wait-proposal",
    "originBody": "Sign in Join Search Computing Search Latest Topics Choose a topic from the list below Diversity in Tech Big Data & Analytics DevOps Security GDPR AI & ML Cloud & Infrastructure CIO Deskflix Sustainability in Tech Diversity in Tech Deskflix Events Whitepapers More Most Popular content Big Data & Analytics Cloud Leadership & Strategy Open Source Storage Telecoms Top 100 IT Leaders Rising Stars 30 Featured content On-demand webinars Identity Access Management Hub Research Tech Marketing Hub Entry-level tech roles Cloud Sustainability Authors Stuart Sumner John Leonard Tom Allen Penny Horwood Computing Contributors Computing Resources English Français Deutsch Espagnol Italien About About us Code of Ethics Search Search Computing Search Join Sign in EU heavies disagree with bloc's AI plans, support UK-style 'wait-and-see' proposal MEP labels leaked policy 'a declaration of war' Mark Ballard 20 November 2023 • 3 min read Share Europe's largest states have put Anglo-centric proposals for light-touch regulation of artificial intelligence on the table in negotiations over the EU's proposed AI Act, which intends to impose harsh rules over developers of the emerging technology. Germany, France and Italy have declared a new, softer position on AI since they and other EU member states entered negotiations with members of the European Parliament (MEPs), and the European Commission, over proposals to impose sweeping AI laws. That's according to Reuters and news agencies, to whom the trio leaked copies of their policy paper over the weekend. The \"inherent risk\" of AI is in its application, not \"the technology itself,\" the big EU countries state in the paper. They echoed the pro-business policy the UK published in June, and which Prime Minister Rishi Sunak pushed at the Global AI Safety Summit in London early this month. Laws should therefore not regulate AI, as the EU intends, but its applications, German officials said in statements to Reuters, confirming the proposals set out in the leaked position paper. That means shelving plans to sanction AI developers for wrong-doing until regulators are able to see there really is a need for it. \"Initially, no sanctions should be imposed,\" said reports about the paper. Instead, regulators would develop general principles already being agreed in negotiations at the G7 group of advanced industrial nations, and write those into guidance for AI developers to self-regulate. The states proposed a compromise that would let the EU impose but one legal demand on AI developers: that they produce \"model cards\", which describe what the developers are doing in a way that allows public guardians to assess its risk of harm, according to the Euractiv news service. However, the EU is not taking the leaked paper lightly. An anonymous MEP told Euractiv the proposal was \"a declaration of war\". The revision would avert a general AI rule that might hinder innovations in foundation models (general purpose AI technologies that can be turned to diverse applications). Regulators would be told to concern themselves only with those specific uses of AI where it might actually be misused, but has been celebrated for its potential to do things such as spot cancerous cells in medical images and make industrial processes more efficient. Regulators would keep watch on AI developments, to determine what if any legal mandates might be needed in future to govern its use. That mirrors the light-touch proposals the UK first set out last year, and wrote into a draft policy in March. UK AI minister Viscount Camrose restated the UK's wait-and-see policy on AI regulation at a conference held by the Financial Times in London last week. The UK policy of not rushing to regulate AI still stands, he said, a month after prime minister Rishi Sunak himself spelled out the UK position ahead of his meeting with AI experts and policy makers in London. \"The UK's answer is not to rush to regulate. This is a point of principle - we believe in innovation. It's a hallmark of the British economy. We will always have a presumption to encourage it, not stifle it. And in any case, how can we write laws that make sense for something we don't yet fully understand?\" Sunak said. Share Related Topics Legislation and Regulation Artificial Intelligence ai Europe eu italy France Germany Regulation Previous Article Is gender-biased AI sending us back in time? You may also like Leadership Is gender-biased AI sending us back in time? Bias 'alive and well' across gen-AI platforms 20 November 2023 • 4 min read Corporate Sam Altman leaves OpenAI: Everything you need to know How Twitch's co-founder ended up running one of the word's leading AI companies 20 November 2023 • 3 min read Chips and Components Microsoft launches AI chips to support OpenAI and Copilot New chips for OpenAI and Copilot coming in 2024 16 November 2023 • 2 min read Most read 01 Samsung data breach affects UK customers 17 November 2023 • 1 min read 02 Ransomware gang files SEC complaint over undisclosed breach 17 November 2023 • 2 min read 03 Royal Mail budgets £10 million to improve system resilience 17 November 2023 • 2 min read 04 It's time for the Tech Women Celebration 50 19 November 2023 • 1 min read 05 Interview: Kirsty Baxter-Smith, BT Group, Women in Tech Excellence Awards finalist 16 November 2023 • 5 min read Upcoming events 23 Nov 06:30PM Award Women In Tech Excellence Awards 2023 Register now 06 Dec 03:00PM Website Beyond the 9 to 5: How to protect yourself from “after hours” cyber attacks Register now 14 Mar 06:00PM Award DevOps Excellence Awards 2024 Register now Sign up to our newsletter The best news, stories, features and photos from the day in one perfectly formed email. Get the newsletter More on Legislation and Regulation Legislation and Regulation Google's witness accidentally reveals important detail in antitrust trial Google attorney 'visibly cringed' at revelation, according to reports Muskan Arora 15 November 2023 • 2 min read Legislation and Regulation Biden and Xi to sign deal limiting use of AI in nuclear weapons systems Hopes are high that the landmark agreement will be a step towards improving global superpower relations Muskan Arora 14 November 2023 • 2 min read Legislation and Regulation Online Safety Act receives royal assent Contentious rules are now law in the UK Vikki Davies 27 October 2023 • 3 min read Delta view all Cyber, cloud and automation: UK IT leaders reveal their spending priorities Microsoft Azure V Google Cloud Platform: Which is the most sustainable cloud platform? DevOps tool consolidation: Where rationalisation is happening AWS V Microsoft Azure: Which is the most sustainable cloud platform? Contact us Marketing Solutions About The Channel Company Privacy Settings Terms & Conditions Policies Careers FOLLOW US © The Channel Company EMEA 180 Borough High Street, London SE1 1LB. Registered in England and Wales with company registration number 14078896",
    "originSummary": [
      "Germany, France, and Italy are advocating for a more flexible approach to regulating AI in the EU's proposed AI Act.",
      "They propose focusing on regulating the application of AI rather than the technology itself.",
      "Their proposal includes self-regulation by AI developers and the requirement to produce \"model cards\" describing the technology's risk of harm. This differs from the EU's plan for stricter regulations."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  },
  {
    "title": "From Crowdsourcing to AI: Casetext Redefines Legal Research",
    "originLink": "https://nextbigwhat.com/ai-and-the-future-of-law-the-casetext-journey-and-the-power-of-ai-bigideas/",
    "originBody": "AI and the Future of Law: The Casetext Journey and the Power of AI #bigIdeas AI and the Future of Law: The Casetext Journey and the Power of AI AI and the Future of Law: The Casetext Journey and the Power of AI This episode delves into the journey of Casetext, an AI-powered legal research platform that has revolutionized the legal field. Co-founder Jake Heller discusses the evolution of Casetext from a crowdsourced law library to a disruptive AI tool that significantly reduces the time required for legal work. The conversation also explores the potential of AI technology in transforming traditional industries and the challenges and opportunities it presents for startups. Powered by BigIdeas app Get BigIdeas from the world's best books, podcasts, books in 5 mins From Law Library to AI Powerhouse Casetext began its journey as a crowdsourced law library in 2013, offering a platform where users could edit, annotate, and vote on legal cases. Within a decade, it evolved into an AI-powered legal tool capable of condensing weeks of legal work into minutes. This transformation led to Casetext’s acquisition for $650 million, highlighting the potential of AI to revolutionize traditional industries. Navigating Product-Market Fit Casetext’s journey saw periods of growth and stagnation as they navigated product-market fit. Initially targeting large law firms, they attracted substantial clients before exhausting this market segment. Powered by BigIdeas app Get BigIdeas from the world's best books, podcasts, books in 5 mins Shifting their focus to smaller law firms led to a customer surge, followed by another plateau. This journey underlines the importance of understanding and adapting to market needs in building a successful startup. The Power of AI Models Casetext’s breakthrough came when they gained early access to OpenAI’s GPT-3 and GPT-4 models. These advanced AI models enabled them to build a product that drastically improved the efficiency of legal work, leading to rapid growth and increased revenue. The impact of these AI models extends beyond the legal field, demonstrating the potential of AI to automate and streamline complex tasks. Demonstrating Immediate Value A key factor in Casetext’s success was their ability to demonstrate the value of their product immediately. They created a ‘golden demo’ that showed potential clients how their AI could perform days worth of work in minutes. This immediate demonstration of value played a significant role in their rapid growth and success. It was also pretty apparent early on that it was an area that could be improved with technology pretty substantially. – Jake Heller The Importance of Grit and Determination Despite several periods of stagnation, the Casetext team remained committed to their vision and continually adapted their product to meet customer needs. Their story underscores the importance of grit, determination, and a customer-focused approach in building a successful startup. It serves as a testament to the potential of AI to revolutionize traditional industries.",
    "originSummary": [
      "Casetext, an AI-powered legal research platform, has transitioned from a crowdsourced law library to an innovative AI tool.",
      "This transformation has significantly decreased the time needed for legal work, showcasing the potential of AI in revolutionizing conventional industries.",
      "Casetext's success can be attributed to access to advanced AI models and the ability to demonstrate the immediate value of their product, underscoring the importance of perseverance, adaptability, and customer-centricity in building a thriving startup."
    ],
    "commentBody": "",
    "commentSummary": [],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1700487654116
  }
]
