[
  {
    "title": "SAG-AFTRA Union Disputes Studios' Offer, Seeks Assurances on AI Use and Streaming Era Compensation",
    "originLink": "https://www.reuters.com/business/media-telecom/hollywood-actors-union-notes-disagreements-with-studios-offer-including-ai-2023-11-07/",
    "originBody": "Sustainable Finance & Reporting Employee Benefits & Executive Compensation Worker Rights Litigation Employment Hollywood actors' union notes disagreements with studios' offer, including AI By Dawn Chmielewski and Lisa Richwine November 7, 202310:25 AM UTCUpdated 3 hours ago Companies Netflix Inc Follow Paramount Global Follow Walt Disney Co Follow Show more companies LOS ANGELES, Nov 6 (Reuters) - The Hollywood actors' union on Monday responded to the latest offer from major studios and streaming services, saying the two sides had yet to reach agreement on several items including the use of artificial intelligence. The SAG-AFTRA union said its negotiating committee was determined to secure the best deal and bring a responsible end to a strike that has lasted four months. \"We're at a critical point in our industry,\" the union said in a note to members posted on X, the social media platform formerly known as Twitter. \"We need a fair contract to make sure this career is viable now AND in the future.\" Advertisement · Scroll to continue SAG-AFTRA members walked off the job in July to demand higher compensation in the streaming TV era plus protections around the use of artificial intelligence (AI) and other gains. Duncan Crabtree-Ireland, SAG-AFTRA National Executive Director and Chief Negotiator, speaks next to SAG-AFTRA union President Fran Drescher at SAG-AFTRA offices after negotiations ended with the Alliance of Motion Picture and Television Producers (AMPTP), the entity that represents major studios and streamers, including Amazon,... Acquire Licensing Rights Read more The Alliance of Motion Picture and Television Producers (AMPTP), which represents Walt Disney (DIS.N), Netflix (NFLX.O) and other companies, presented what the studios described as their \"last, best and final\" offer on Saturday. Advertisement · Scroll to continue Netflix Co-CEO Ted Sarandos, in an interview with Reuters on Monday, said negotiations to find a resolution were ongoing. \"We’re at the table all day, every day and we’re trying to get the strike resolved and get the town back to work,\" Sarandos said at an event at the Egyptian Theatre, a Hollywood landmark that Netflix recently restored. \"We're in the business of telling stories and that’s what we want to do every day,\" Sarandos added. \"We are going to try our best to get things up and running and get the output back up for our fans too.\" Advertisement · Scroll to continue Last week, union leaders expressed \"cautious optimism\" that a deal could be reached soon but also said there were gaps between the two sides on various issues. On AI, actors are seeking assurances that their digital likenesses will not be used without their permission. Reporting by Dawn Chmielewski, Lisa Richwine and Omar Younis in Los Angeles Editing by Mary Milliken, Matthew Lewis & Shri Navaratnam Our Standards: The Thomson Reuters Trust Principles. Acquire Licensing Rights , opens new tab Read Next Sustainable Markets category Namibia starts construction of Africa's first decarbonised iron plant Namibia began construction on Monday of Africa's first decarbonised iron plant, to be powered exclusively by green hydrogen, the country's investment promotion body said. Markets category IMF warns of inflation risks as central Europe hammers out 2024 wage deals Big hikes in the minimum wage planned in central Europe for next year raise the risk of more persistent inflation or job losses amid relatively weak productivity growth across the region, the International Monetary Fund (IMF) has warned. ESG Investors category British pensions could invest 1.2 trln pounds in climate projects, with policy help-report Britain's pensions industry could invest up to 1.2 trillion pounds ($1.5 trillion), half the capital needed by 2035 to put the UK on track to its net-zero goals, if there were more attractive projects and fewer regulatory barriers, a report on Tuesday showed. World at Work category Amazon workers at UK warehouse strike again More than 1,000 workers at an Amazon warehouse in central England were striking on Tuesday as part of a long-running dispute over pay, the GMB trade union said. Charged category The Hemi fades away as Stellantis electrifies its Ram trucks Stellantis will offer a previously undisclosed plug-in hybrid model of its Ram pickup truck, called Ramcharger, and phase out V-8 Hemi engines as part of its strategy to cut the CO2 emissions of its North American fleet.",
    "originSummary": [
      "SAG-AFTRA union, representing the interests of actors and artists, has rejected the newest proposition from significant studios including Walt Disney Co., Netflix, and Paramount Global.",
      "The main disagreements circle around future pay in the streaming age, and safeguards related to the utilization of Artificial Intelligence (AI). Specifically, union leaders are advocating for a system where actors' digital profiles can't be employed without their approval.",
      "This conflict occurs amidst an ongoing strike by union members, initiated in July, with Netflix’s Co-CEO Ted Sarandos confirming that the negotiation discussions are yet to reach a conclusion."
    ],
    "commentBody": "",
    "commentSummary": [
      "The Screen Actors Guild‐American Federation of Television and Radio Artists (SAG-AFTRA) union has rejected the latest offer from major studios like Walt Disney Co., Netflix, and Paramount Global.",
      "The central issues revolve around future compensation in relation to the streaming era and protections concerning the use of Artificial Intelligence. The union leaders demand assurances against using actors' digital likenesses without their consent.",
      "Amid an ongoing strike by union members that started in July, Netflix Co-CEO Ted Sarandos mentions that negotiations to resolve the issues are still in progress."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "Balancing AI Regulation: Avoiding Overreach, Addressing Real-World Harms",
    "originLink": "https://www.nytimes.com/2023/11/07/opinion/biden-ai-regulation.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT OPINION GUEST ESSAY In Regulating A.I., We May Be Doing Too Much. And Too Little. Nov. 7, 2023, 5:02 a.m. ET Credit... Illustration by Sam Whitney/The New York Times Share full article 12 By Tim Wu Mr. Wu is a law professor at Columbia and the author, most recently, of “The Curse of Bigness: Antitrust in the New Gilded Age.” He served on the National Economic Council as a special assistant to the president for competition and tech policy from 2021 to 2023. When President Biden signed his sweeping executive order on artificial intelligence last week, he joked about the strange experience of watching a “deep fake” of himself, saying, “When the hell did I say that?” The anecdote was significant, for it linked the executive order to an actual A.I. harm that everyone can understand — human impersonation. Another example is the recent boom in fake nude images that have been ruining the lives of high-school girls. These everyday episodes underscore an important truth: The success of the government’s efforts to regulate A.I. will turn on its ability to stay focused on concrete problems like deep fakes, as opposed to getting swept up in hypothetical risks like the arrival of our robot overlords. Mr. Biden’s executive order outdoes even the Europeans by considering just about every potential risk one could imagine, from everyday fraud to the development of weapons of mass destruction. The order develops standards for A.I. safety and trustworthiness, establishes a cybersecurity program to develop A.I. tools and requires companies developing A.I. systems that could pose a threat to national security to share their safety test results with the federal government. In devoting so much effort to the issue of A.I., the White House is rightly determined to avoid the disastrous failure to meaningfully regulate social media in the 2010s. With government sitting on the sidelines, social media technology evolved from a seemingly innocent tool for sharing personal updates among friends to a large-scale psychological manipulation, complete with a privacy-invasive business model and a disturbing record of harming teenagers, fostering misinformation and facilitating the spread of propaganda. But if social networking was a wolf in sheep’s clothing, artificial intelligence is more like a wolf clothed as a horseman of the apocalypse. In the public imagination A.I. is associated with the malfunctioning evil of HAL 9000 in Stanley Kubrick’s “2001: A Space Odyssey” and the self-aware villainy of Skynet in the “Terminator” films. But while A.I. certainly poses problems and challenges that call for government action, the apocalyptic concerns — be they mass unemployment from automation or a superintelligent A.I. that seeks to exterminate humanity — remain in the realm of speculation. If doing too little, too late with social media was a mistake, we now need to be wary of taking premature government action that fails to address concrete harms. The temptation to overreact is understandable. No one wants to be the clueless government official in the disaster movie who blithely waves off the early signs of pending cataclysm. The White House is not wrong to want standardized testing of A.I. and independent oversight of catastrophic risk. The executive order requires companies developing the most powerful A.I. systems to keep the government apprised of safety tests, and also to have the secretary of labor study the risks of and remedies for A.I. job displacement. But the truth is that no one knows if any of these world-shattering developments will come to pass. Technological predictions are not like those of climate science, with a relatively limited number of parameters. Tech history is full of confident projections and “inevitabilities” that never happened, from the 30-hour and 15-hour workweeks to the demise of television. Testifying in grave tones about terrifying possibilities makes for good television. But that’s also how the world ended up blowing hundreds of billions of dollars getting ready for Y2K. To regulate speculative risks, rather than actual harms, would be unwise, for two reasons. First, overeager regulators can fixate shortsightedly on the wrong target of regulation. For example, to address the dangers of digital piracy, Congress in 1992 extensively regulated digital audio tape, a recording format now remembered only by audio nerds, thanks to the subsequent rise of the internet and MP3s. Similarly, today’s policymakers are preoccupied with large language models like ChatGPT, which could be the future of everything — or, given their gross unreliability stemming from chronic falsification and fabrication, may end up remembered as the Hula Hoop of the A.I. age. Second, pre-emptive regulation can erect barriers to entry for companies interested in breaking into an industry. Established players, with millions of dollars to spend on lawyers and experts, can find ways of abiding by a complex set of new regulations, but smaller start-ups typically don’t have the same resources. This fosters monopolization and discourages innovation. The tech industry is already too much the dominion of a handful of huge companies. The strictest regulation of A.I. would result in having only companies like Google, Microsoft, Apple and their closest partners competing in this area. It may not be a coincidence that those companies and their partners have been the strongest advocates of A.I. regulation. Actual harm, not imagined risk, is a far better guide to how and when the state should intervene. A.I.’s clearest extant harms are those related to human impersonation (such as the fake nudes), discrimination and the addiction of young people. In 2020, thieves used an impersonated human voice to swindle a Japanese company in Hong Kong out of $35 million. Facial recognition technology has led to wrongful arrest and imprisonment, as in the case of Nijeer Parks, who spent 10 days in a New Jersey jail because he was misidentified. Fake consumer reviews have eroded consumer confidence, and the fake social media accounts drive propaganda. A.I.-powered algorithms are used to enhance the already habit-forming properties of social media. These examples aren’t quite as hair-raising as the warning issued this year by the Center for A.I. Safety, which insisted that “mitigating the risk of extinction from A.I. should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” But the less exciting examples happen to feature victims who are real. To its credit, Mr. Biden’s executive order is not overly caught up in the hypothetical: Most of what it suggests is a framework for future action. Some of its recommendations are urgent and important, such as creating standards for the watermarking of photos, videos, audio and text created with A.I. But the executive branch, of course, is limited in its power. Congress should follow the lead of the executive branch and keep an eye on hypothetical problems while moving decisively to protect us against human impersonation, algorithmic manipulation, misinformation and other pressing problems of A.I. — not to mention passing the online privacy and child-protection laws that despite repeated congressional hearings and popular support, it keeps failing to enact. Regulation, contrary to what you hear in stylized political debates, is not intrinsically aligned with one or another political party. It is simply the exercise of state power, which can be good or bad, used to protect the vulnerable or reinforce existing power. Applied to A.I., with an eye on the unknown future, regulation may be used to aid the powerful by helping preserve monopolies and burden those who strive to use computing technology to improve the human condition. Done correctly, with an eye toward the present, it might protect the vulnerable and promote broader and more salutary innovation. The existence of actual social harm has long been a touchstone of legitimate state action. But that point cuts both ways: The state should proceed cautiously in the absence of harm, but it also a duty, given evidence of harm, to take action. By that measure, with A.I. we are at risk of doing too much and too little at the same time. Tim Wu (@superwuster) is a law professor at Columbia and the author, most recently, of “The Curse of Bigness: Antitrust in the New Gilded Age.” Source photographs by plepann and bebecom98/Getty Images. The Times is committed to publishing a diversity of letters to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips. And here’s our email: letters@nytimes.com. Follow The New York Times Opinion section on Facebook, Twitter (@NYTopinion) and Instagram. 12 Share full article 12 ADVERTISEMENT SKIP ADVERTISEMENT",
    "originSummary": [
      "Columbia Law Professor, Tim Wu, expresses apprehension about President Biden's executive order on AI regulation. He fears it could fluctuate between excessive and inadequate regulation.",
      "Wu proposes the government should prioritize addressing present, demonstratable issues such as deep fakes, AI-facilitated fraud or privacy and security concerns, instead of hypothetical fatal risks such as mass job loss to automation or rogue AI threats.",
      "He further warns that an overabundance of regulation could act as a barrier for small companies entering the industry, resulting in a concentration of power within big tech corporations like Google, Microsoft, and Apple."
    ],
    "commentBody": "",
    "commentSummary": [
      "Columbia Law Professor, Tim Wu, critiques President Biden's executive order on AI regulation, raising concerns over potential over-regulation or under-regulation.",
      "Wu advises against reacting to speculative concerns, like mass unemployment from automation or rogue AI threats, recommending focus on real-time issues such as deep fakes, AI-facilitated fraud, and privacy/security concerns.",
      "Wu expresses concern that excessive regulation could bar entry for small firms, leading to further power consolidation by big tech companies like Google, Microsoft, and Apple."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "Learning from Evolutionary Biology to Navigate the Future of AI: Survival, Integration and Regulation",
    "originLink": "https://www.scientificamerican.com/article/our-evolutionary-past-can-teach-us-about-ais-future/",
    "originBody": "EvolutionOpinion Our Evolutionary Past Can Teach Us about AI’s Future Evolutionary biology offers warnings, and tips, for surviving the advent of artificial intelligence By Eliot Bush on November 7, 2023 Share on Facebook Share on Twitter Share on Reddit Share on LinkedIn Share via Email Print Credit: Pict Rider/Getty Images Advertisement As artificial intelligence advances, experts have warned about its potential to cause human extinction. Exactly how this might come about is a matter of speculation—but it’s not hard to see that intelligent robots could build more of themselves, improve on their own designs and pursue their own interests. And that could be a threat to humanity. Last week, an AI Safety Summit was held at Bletchley Park in the U.K. It sought to address some of the threats associated with the most advanced AI technologies, among them “loss of control” risks—the possibility that such systems might become independent. It's worth asking what we can predict about such scenarios based on things we already know. Machines able to act independently and upgrade their own designs would be subject to the same evolutionary laws as bacteria, animals and plants. Thus evolution has a lot to teach us about how AI might develop—and how to ensure humans survive its rise. A first lesson is that, in the long run, there are no free lunches. Unfortunately, that means we can’t expect AI to produce a hedonistic paradise where every human need is met by robot servants. Most organisms live close to the edge of survival, eking out an existence as best they can. Many humans today do live more comfortable and prosperous lives, but evolutionary history suggests that AI could disrupt this. The fundamental reason is competition. This is an argument that traces back to Darwin, and applies more widely than just to AI. However, it’s easily illustrated using an AI-based scenario. Imagine we have two future AI-run nation-states where humans no longer make significant economic contributions. One slavishly devotes itself to meeting every hedonistic need of its human population. The other puts less energy into its humans and focuses more on acquiring resources and improving its technology. The latter would become more powerful over time. It might take over the first one. And eventually, it might decide to dispense with its humans altogether. The example does not have to be a nation-state for this argument to work; the key thing is the competition. One takeaway from such scenarios is that humans should try to keep their economic relevance. In the long run, the only way to ensure our survival is to actively work toward it ourselves. Another insight is that evolution is incremental. We can see this in major past innovations such as the evolution of multicellularity. For most of Earth’s history, life consisted mainly of single-celled organisms. Environmental conditions were unsuitable for large multicellular organisms due to low oxygen levels. However, even when the environment became more friendly, the world was not suddenly filled with redwoods and whales and humans. Building a complex structure like a tree or a mammal requires many capabilities, including elaborate gene regulatory networks and cellular mechanisms for adhesion and communication. These arose bit by bit over time. AI is also likely to advance incrementally. Rather than a pure robot civilization springing up de novo, it’s more likely that AI will integrate itself into things that already exist in our world. The resulting hybrid entities could take many forms; imagine, for example, a company with a human owner but machine-based operations and research. Among other things, arrangements like this would lead to extreme inequality among humans, as owners would profit from their control of AI, while those without such control would become unemployed and impoverished. Such hybrids are also likely to be where the immediate threat to humanity lies. Some have argued that the “robots take over the world” scenario is overblown because AI will not intrinsically have a desire to dominate. That may be true. However, humans certainly do—and this could be a big part of what they would contribute to a collaboration with machines. With all this in mind, perhaps another principle for us to adopt is that AI should not be allowed to exacerbate inequality in our society. Contemplating all this may leave one wondering if humans have any long-term prospects at all. Another observation from the history of life on Earth is that major innovations allow life to occupy new niches. Multicellularity evolved in the oceans and enabled novel ways of making a living there. For animals, these included burrowing through sediments and new kinds of predation. This opened up new food options and allowed animals to diversify, eventually leading to the riot of shapes and lifestyles that exist today. Crucially, the creation of new niches does not mean all the old ones go away. After animals and plants evolved, bacteria and other single-celled organisms persisted. Today, some of them do similar things to what they did before (and indeed are central to the functioning of the biosphere). Others have profited from new opportunities such as living in the guts of animals. Hopefully some possible futures include an ecological niche for humans. After all, some things that humans need (such as oxygen and organic food), machines do not. Maybe we can convince them to go out into the solar system to mine the outer planets and harvest the sun’s energy. And leave the Earth to us. But we may need to act quickly. A final lesson from the history of biological innovations is that what happens in the beginning matters. The evolution of multicellularity led to the Cambrian explosion, a period more than 500 million years ago when large multicellular animals appeared in great diversity. Many of these early animals went extinct without descendants. Because the ones that survived went on to found major groupings of animals, what happened in this era determined much about the biological world of today. It has been argued that many paths were possible in the Cambrian, and that the world we ended up with was not foreordained. If the development of AI is like that, then now is the time when we have maximum leverage to steer events. Sign up for Scientific American’s free newsletters. Sign Up Steering events, however, requires specifics. It is well and good to have general principles like “humans should maintain an economic role,” and “AI should not exacerbate inequality.” The challenge is to turn those into specific regulations regarding the development and use of AI. We’ll need to do that despite the fact that computer scientists themselves don’t know how AI will progress over the next 10 years, much less over the long term. And we’ll also need to apply the regulations we come up with relatively consistently across the world. All of this will require us to act with more coherence and foresight than we’ve demonstrated when dealing with other existential problems such as climate change. It seems like a tall order. But then again, four or five million years ago, no one would have suspected that our small-brained, relatively apelike ancestors would evolve into something that can sequence genomes and send probes to the edge of the solar system. With luck, maybe we’ll rise to the occasion again. This is an opinion and analysis article, and the views expressed by the author or authors are not necessarily those of Scientific American. Rights & Permissions ABOUT THE AUTHOR(S) Eliot Bush is a professor of computational biology and evolution at Harvey Mudd College. Recent Articles by Eliot Bush The Meaning of Time in the Place where Humanity's Earliest Ancestors Arose Read This Next Evolution Our Evolutionary Past Can Teach Us about AI's Future Eliot BushOpinion Animals Birds Named after People Will Get New English Names Meghan Bartels Climate Change New Climate Compensation Agreement Raises International Tensions Sara Schonhardt and E&E News Medicine The Search for New Psychedelics Rachel Nuwer Psychology Training Bartenders, Barbers and Divorce Attorneys as Counselors Could Reduce Gun Suicides Sara Novak Climate Change Climate Benefits of Hydrogen Are at Risk as Fossil Fuel Industry Pressures Mount Julie McNamaraOpinion Advertisement",
    "originSummary": [
      "Advances in AI technology raise concerns about its potential to function independently and self-improve, which could result in humans losing control.",
      "The progress of AI's integration into systems is predicted to be incremental, possibly causing extreme wealth inequality and posing a threat to human economic relevance based on evolutionary biology principles.",
      "Experts emphasize the need for immediate regulations on AI development and use, despite uncertainties, to prevent worsening societal inequality and ensure human economic roles, likening the challenge to other global issues such as climate change."
    ],
    "commentBody": "",
    "commentSummary": [
      "As AI advances, concerns arise about its potential to self-improve and act independently, leading to a possible loss of control and threat as AI competes for resources and renders humans economically irrelevant.",
      "The progress of AI is anticipated to be incremental, integrating into existing systems gradually, which could lead to extreme wealth disparity, stressing the need to not let AI exacerbate societal inequality while preserving the economic role of humans.",
      "Drawing from evolutionary biology, humans could maintain survival by finding new niches, and immediate regulations are essential for AI's development, even amidst uncertain progression. Addressing AI's existential threats necessitates foresight and decisive actions, similar to confronting global issues like climate change."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "AI Strategies in Job Listings Boost Applications by 17%, LinkedIn Research Reveals",
    "originLink": "https://fortune.com/2023/11/07/linkedin-research-recruitment-ai-job-ads-artificial-intelligence/",
    "originBody": "TECH ·RECRUITMENT Job listing are getting a boost by adding AI: ‘I’d consider it a requirement,’ says LinkedIn hiring boss BYORIANNA ROSA ROYLE LinkedIn’s research shows that vacancies that mention AI are getting the most engagement from job seekers. D3SIGN—GETTY IMAGES As the threat of artificial intelligence stealing jobs looms, employees are future-proofing their careers by specifically applying for job ads with AI mentioned in their listings—because if you can’t beat AI, you might as well join it. That’s according to LinkedIn’s research, which shows that during the past two years job posts on the networking platform that mention AI or Generative AI received 17% higher application growth than job posts that do not mention AI. “Candidates are savvy,” said Erin Scruggs, vice president of global talent acquisition at LinkedIn. “They’re showing they want to go where opportunities are.” It’s why she recommends companies detail their AI plans in their job ads—even if the role advertised isn’t involved in the plans—or risk losing top talent. “I would consider it a requirement for most companies to share at least a basic roadmap of their AI strategy in job posts to keep up with the market,” Scruggs added. What’s more, companies around the world should take note: LinkedIn’s conclusion that jop postings mentioning AI are hot on the market was based on data drawn from English, Spanish, French, Japanese, Dutch, Italian, German, Portuguese, Turkish, and Chinese-written ads. Join the AI bandwagon—or risk being replaced The rush to jump on the AI bandwagon comes as fears mount that automation will wipe out millions of jobs. Just last week, Tesla and X owner, Elon Musk told the U.K. AI Safety Summit that AI will one day eradicate employment. “You can have a job if you want to have it for personal pleasure. But AI could do everything,” Musk told Britain’s prime minister Rishi Sunak. “I don’t know if people are comfortable or uncomfortable with that.” At the same time, investment bank Goldman Sachs has estimated that AI could replace the equivalent of 300 million full-time jobs globally in the coming years. Meanwhile, IBM’s CEO Arvind Krishna predicted “repetitive, white-collar jobs” will be automated first. But, he added, that doesn’t mean humans will be out of jobs. “People mistake productivity with job displacement,” he said at Fortune’s CEO Initiative conference. As an example, he points to jobs created by the invention of the internet. “In 1995 no one thought there would be five million web designers—there are,” Krishna said. It’s why Reddit’s former CEO, Yishan Wong advised workers concerned about being replaced by AI to futureproof their roles by side-stepping into the industry because it doesn’t require “an enormous amount of technical skill.” “Nontechnical people can build pretty valuable and novel applications in AI,” he told Fortune. “There’s this enormous amount of leverage that an individual can have.” Similarly, Nvidia’s CEO Jensen Huang recently suggested that AI will “generate jobs”—with the caveat that while people might not lose their jobs to AI, they’ll likely lose it to another human using AI. It’s the one thing leaders can seemingly agree on—and judging by LinkedIn’s research, workers know it too. “AI may not replace managers, but the managers that use AI will replace the managers that do not,” IBM’s chief commercial officer Rob Thomas said during a press conference. “It really does change how people work.” Likewise, the economist Richard Baldwin echoed, “AI won’t take your job” during a panel at the 2023 World Economic Forum’s Growth Summit. “It’s somebody using AI that will take your job.” Subscribe to the Eye on AI newsletter to stay abreast of how AI is shaping the future of business. Sign up for free. Most popular Tech articles TECH Robot startups see huge market in replacing human workers: ‘We can sell millions of humanoids, billions maybe’ BYMATT O'BRIEN AND THE ASSOCIATED PRESS TECH Elon Musk says AI will create a world ‘where no job is needed,’ but Nvidia billionaire Jensen Huang couldn’t disagree... BYSTEVE MOLLMAN TECH An AI bot performed insider trading and deceived its users after deciding helping a company was worth the risk BYRYAN HOGG",
    "originSummary": [
      "LinkedIn's research found that job listings mentioning artificial intelligence (AI) or Generative AI attract 17% more applications than those that don't, a trend seen globally and across several languages.",
      "Observing an increasing interest in roles involving AI, LinkedIn's VP of global talent acquisition advises companies to include their AI strategies in job ads to attract top-tier talent.",
      "Despite fears that AI may replace human jobs, predictions on its impact on employment differ, with some predicting severe job losses and others foreseeing the creation of new jobs due to AI."
    ],
    "commentBody": "",
    "commentSummary": [
      "LinkedIn research indicates job listings mentioning artificial intelligence (AI) or Generative AI attract 17% more applicants than those that don't; the trend is consistent across multiple languages.",
      "Despite fears of AI phasing out human jobs, workers are actively searching for roles involving AI, implying a shift in job market trends.",
      "LinkedIn's global talent acquisition VP advises companies to highlight their AI strategy in job ads to attract high-level candidates; opinions on AI's impact on employment vary – some predict AI will replace millions of jobs, others argue it will generate new ones."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "Utilizing AI and Machine Learning for Enhanced Understanding of Brain Complexities",
    "originLink": "https://www.nature.com/articles/d41586-023-03426-3",
    "originBody": "COMMENT 07 November 2023 How AI could lead to a better understanding of the brain Early machine-learning systems were inspired by neural networks — now AI might allow neuroscientists to get to grips with the brain’s unique complexities. Viren Jain Twitter Facebook Email A reconstruction of incoming connections to a single human neuron, called a pyramidal cell. Credit: A. Shapson-Coe, M. Januszewski, D. Berger, A. Pope, V. Jain & J. Lichtman Can a computer be programmed to simulate a brain? It’s a question mathematicians, theoreticians and experimentalists have long been asking — whether spurred by a desire to create artificial intelligence (AI) or by the idea that a complex system such as the brain can be understood only when mathematics or a computer can reproduce its behaviour. To try to answer it, investigators have been developing simplified models of brain neural networks since the 1940s1. In fact, today’s explosion in machine learning can be traced back to early work inspired by biological systems. However, the fruits of these efforts are now enabling investigators to ask a slightly different question: could machine learning be used to build computational models that simulate the activity of brains? At the heart of these developments is a growing body of data on brains. Starting in the 1970s, but more intensively since the mid-2000s, neuroscientists have been producing connectomes — maps of the connectivity and morphology of neurons that capture a static representation of a brain at a particular moment. Alongside such advances have been improvements in researchers’ abilities to make functional recordings, which measure neural activity over time at the resolution of a single cell. Meanwhile the field of transcriptomics is enabling investigators to measure the gene activity in a tissue sample, and even to map when and where that activity is occurring. So far, few efforts have been made to connect these different data sources or collect them simultaneously from the whole brain of the same specimen. But as the level of detail, size and number of data sets increases, particularly for the brains of relatively simple model organisms, machine-learning systems are making a new approach to brain modelling feasible. This involves training AI programs on connectomes and other data to reproduce the neural activity you would expect to find in biological systems. Several challenges will need to be addressed for computational neuroscientists and others to start using machine learning to build simulations of entire brains. But a hybrid approach that combines information from conventional brain-modelling techniques with machine-learning systems that are trained on diverse data sets could make the whole endeavour both more rigorous and more informative. Brain mapping The quest to map a brain began nearly half a century ago, with a painstaking 15-year effort in the roundworm Caenorhabditis elegans2. Over the past two decades, developments in automated tissue sectioning and imaging have made it much easier for researchers to obtain anatomical data — while advances in computing and automated-image analysis have transformed the analysis of these data sets2. Connectomes have now been produced for the entire brain of C. elegans3, larval4 and adult5 Drosophila melanogaster flies, and for tiny portions of the mouse and human brain (one thousandth and one millionth respectively)2. This is the largest map of the human brain ever made The anatomical maps produced so far have major holes. Imaging methods are not yet able to map electrical connections at scale alongside the chemical synaptic ones. Researchers have focused mainly on neurons even though non-neuronal glial cells, which provide support to neurons, seem to play a crucial part in the flow of information through nervous systems6. And much remains unknown about what genes are expressed and what proteins are present in the neurons and other cells being mapped. Still, such maps are already yielding insights. In D. melanogaster, for example, connectomics has enabled investigators to identify the mechanisms behind the neural circuits responsible for behaviours such as aggression7. Brain mapping has also revealed how information is computed within the circuits responsible for the flies knowing where they are and how they can get from one place to another8. In zebrafish (Danio rerio) larvae, connectomics has helped to uncover the workings of the synaptic circuitry underlying the classification of odours9, the control of the position and movement of the eyeball10 and navigation11. Efforts that might ultimately lead to a whole mouse brain connectome are under way — although using current approaches, this would probably take a decade or more. A mouse brain is almost 1,000 times bigger than the brain of D. melanogaster, which consists of roughly 150,000 neurons. Alongside all this progress in connectomics, investigators have been capturing patterns of gene expression with increasing levels of accuracy and specificity using single-cell and spatial transcriptomics. Various technologies are also allowing researchers to make recordings of neural activity across entire brains in vertebrates for hours at a time. In the case of the larval zebrafish brain, that means making recordings across nearly 100,000 neurons12. These technologies include proteins with fluorescent properties that change in response to shifts in voltage or calcium levels, and microscopy techniques that can image living brains in 3D at the resolution of a single cell. (Recordings of neural activity made in this way provide a less accurate picture than electrophysiology recordings, but a much better one than non-invasive methods such as functional magnetic resonance imaging.) Maths and physics When trying to model patterns of brain activity, scientists have mainly used a physics-based approach. This entails generating simulations of nervous systems or portions of nervous systems using mathematical descriptions of the behaviour of real neurons, or of parts of real nervous systems. It also entails making informed guesses about aspects of the circuit, such as the network connectivity, that have not yet been verified by observations. In some cases, the guesswork has been extensive (see ‘Mystery models’). But in others, anatomical maps at the resolution of single cells and individual synapses have helped researchers to refute and generate hypotheses4. Mystery models A lack of data makes it difficult to evaluate whether some neural-network models capture what happens in real systems. The original aim of the controversial European Human Brain Project, which wrapped up in September, was to computationally simulate the entire human brain. Although that goal was abandoned, the project did produce simulations of portions of rodent and human brains (including tens of thousands of neurons in a model of a rodent hippocampus), on the basis of limited biological measures and various synthetic data-generation procedures. A major problem with such approaches is that in the absence of detailed anatomical or functional maps, it is hard to assess to what degree the resulting simulations accurately capture what is happening in biological systems20. Neuroscientists have been refining theoretical descriptions of the circuit that enables D. melanogaster to compute motion for around seven decades. Since it was completed in 201313, the motion-detection-circuit connectome, along with subsequent larger fly connectomes, has provided a detailed circuit diagram that has favoured some hypotheses about how the circuit works over others. Yet data collected from real neural networks have also highlighted the limits of an anatomy-driven approach. Gigantic map of fly brain is a first for a complex animal A neural-circuit model completed in the 1990s, for example, contained a detailed analysis of the connectivity and physiology of the roughly 30 neurons comprising the crab (Cancer borealis) stomatogastric ganglion — a structure that controls the animal’s stomach movements14. By measuring the activity of the neurons in various situations, researchers discovered that even for a relatively small collection of neurons, seemingly subtle changes, such as the introduction of a neuromodulator, a substance that alters properties of neurons and synapses, completely changes the circuit’s behaviour. This suggests that even when connectomes and other rich data sets are used to guide and constrain hypotheses about neural circuits, today’s data might be insufficiently detailed for modellers to be able to capture what is going on in biological systems15. This is an area in which machine learning could provide a way forward. Guided by connectomic and other data to optimize thousands or even billions of parameters, machine-learning models could be trained to produce neural-network behaviour that is consistent with the behaviour of real neural networks — measured using cellular-resolution functional recordings. Such machine-learning models could combine information from conventional brain-modelling techniques, such as the Hodgkin–Huxley model, which describes how action potentials (a change in voltage across a membrane) in neurons are initiated and propagated, with parameters that are optimized using connectivity maps, functional-activity recordings or other data sets obtained for entire brains. Or machine-learning models could comprise ‘black box’ architectures that contain little explicitly specified biological knowledge but billions or hundreds of billions of parameters, all empirically optimized. Researchers could evaluate such models, for instance, by comparing their predictions about the neural activity of a system with recordings from the actual biological system. Crucially, they would assess how the model’s predictions compare when the machine-learning program is given data that it wasn’t trained on — as standard practice in the evaluation of machine-learning systems. Axonal projections of neurons in a mouse brain.Credit: Adam Glaser, Jayaram Chandrashekar, Karel Svoboda, Allen Institute for Neural Dynamics This approach would make brain modelling that encompasses thousands or more neurons more rigorous. Investigators would be able to assess, for instance, whether simpler models that are easier to compute do a better job of simulating neural networks than do more complex ones that are fed more detailed biophysical information, or vice versa. Machine learning is already being harnessed in this way to improve understanding of other hugely complex systems. Since the 1950s, for example, weather-prediction systems have generally relied on carefully constructed mathematical models of meteorological phenomena, with modern systems resulting from iterative refinements of such models by hundreds of researchers. Yet, over the past five years or so, researchers have developed several weather-prediction systems using machine learning. These contain fewer assumptions in relation to how pressure gradients drive changes in wind velocity, for example, and how that in turn moves moisture through the atmosphere. Instead, millions of parameters are optimized by machine learning to produce simulated weather behaviour that is consistent with databases of past weather patterns16. This way of doing things does present some challenges. Even if a model makes accurate predictions, it can be difficult to explain how it does so. Also, models are often unable to make predictions about scenarios that were not included in the data they were trained on. A weather model trained to make predictions for the days ahead has trouble extrapolating that forecast weeks or months into the future. But in some cases — for predictions of rainfall over the next several hours — machine-learning approaches are already outperforming classical ones17. Machine-learning models offer practical advantages, too; they use simpler underlying code and scientists with less specialist meteorological knowledge can use them. On the one hand, for brain modelling, this kind of approach could help to fill in some of the gaps in current data sets and reduce the need for ever-more detailed measurements of individual biological components, such as single neurons. On the other hand, as more comprehensive data sets become available, it would be straightforward to incorporate the data into the models. Think bigger To pursue this idea, several challenges will need to be addressed. Machine-learning programs will only ever be as good as the data used to train and evaluate them. Neuroscientists should therefore aim to acquire data sets from the whole brain of specimens — even from the entire body, should that become more feasible. Although it is easier to collect data from portions of brains, modelling a highly interconnected system such as a neural network using machine learning is much less likely to generate useful information if many parts of the system are absent from the underlying data. Researchers should also strive to obtain anatomical maps of neural connections and functional recordings (and perhaps, in the future, maps of gene expression) from whole brains of the same specimen. Currently, any one group tends to focus on obtaining only one of these — not on acquiring both simultaneously. How the world’s biggest brain maps could transform neuroscience With only 302 neurons, the C. elegans nervous system might be sufficiently hard-wired for researchers to be able to assume that a connectivity map obtained from one specimen would be the same for any other — although some studies suggest otherwise18. But for larger nervous systems, such as those of D. melanogaster and zebrafish larvae, connectome variability between specimens is significant enough that brain models should be trained on structure and function data acquired from the same specimen. Currently, this can be achieved only in two common model organisms. The bodies of C. elegans and larval zebrafish are transparent, which means researchers can make functional recordings across the organisms’ entire brains and pinpoint activity to individual neurons. Immediately after such recordings are made, the animal can be killed, embedded in resin and sectioned, and anatomical measurements of the neural connections mapped. In the future, however, researchers could expand the set of organisms for which such combined data acquisitions are possible — for instance, by developing new non-invasive ways to record neural activity at high resolution, perhaps using ultrasound. Obtaining such multimodal data sets in the same specimen will require extensive collaboration between researchers, investment in big-team science and increased funding-agency support for more holistic endeavours19. But there are precedents for this type of approach, such as the US Intelligence Advanced Research Projects Activity’s MICrONS project, which between 2016 and 2021 obtained functional and anatomical data for one cubic millimetre of mouse brain. Besides acquiring these data, neuroscientists would need to agree on the key modelling targets and the quantitative metrics by which to measure progress. Should a model aim to predict the behaviour of a single neuron on the basis of a past state or of an entire brain? Should the activity of an individual neuron be the key metric, or should it be the percentage of hundreds of thousands of neurons that are active? Likewise, what constitutes an accurate reproduction of the neural activity seen in a biological system? Formal, agreed benchmarks will be crucial to comparing modelling approaches and tracking progress over time. Lastly, to open up brain-modelling challenges to diverse communities, including computational neuroscientists and specialists in machine learning, investigators would need to articulate to the broader scientific community what modelling tasks are the highest priority and which metrics should be used to evaluate a model’s performance. WeatherBench, an online platform that provides a framework for evaluating and comparing weather forecasting models, provides a useful template16. Recapitulating complexity Some will question — and rightly so — whether a machine-learning approach to brain modelling will be scientifically useful. Could the problem of trying to understand how brains work simply be traded for the problem of trying to understand how a large artificial network works? Yet, the use of a similar approach in a branch of neuroscience concerned with establishing how sensory stimuli (for example, sights and smells) are processed and encoded by the brain is encouraging. Researchers are increasingly using classically modelled neural networks, in which some of the biological details are specified, in combination with machine-learning systems. The latter are trained on massive visual or audio data sets to reproduce the visual or auditory capabilities of nervous systems, such as image recognition. The resulting networks demonstrate surprising similarities to their biological counterparts, but are easier to analyse and interrogate than the real neural networks. For now, perhaps it’s enough to ask whether the data from current brain mapping and other efforts can train machine-learning models to reproduce neural activity that corresponds to what would be seen in biological systems. Here, even failure would be interesting — a signal that mapping efforts must go even deeper. Nature 623, 247-250 (2023) doi: https://doi.org/10.1038/d41586-023-03426-3 References McCulloch, W. S. & Pitts, W. Bull. Math. Biophys. 5, 115–133 (1943). Article Google Scholar Jefferis, G., Collinson, L., Bosch, C., Costa, M. & Schlegel, P. Scaling up Connectomics (Wellcome, 2023). Google Scholar Cook, S. J. et al. Nature 571, 63–71 (2019). Article PubMed Google Scholar Winding, M. et al. Science 379, eadd9330 (2023). Article PubMed Google Scholar Dorkenwald, S. et al. Preprint at bioRxiv https://doi.org/10.1101/2023.06.27.546656 (2023). De Ceglia, R. et al. Nature 622, 120–129 (2023). Article PubMed Google Scholar Schretter, C. E. et al. eLife 9, e58942 (2020). Article PubMed Google Scholar Wilson, R. I. Annu. Rev. Neurosci. 46, 403–423 (2023). Article PubMed Google Scholar Wanner, A. A. & Friedrich, R. W. Nature Neurosci. 23, 433–442 (2020). Article PubMed Google Scholar Vishwanathan, A. et al. Preprint at bioRxiv https://doi.org/10.1101/2020.10.28.359620 (2020). Petrucco, L. et al. Nature Neurosci. 26, 765–773 (2023). Article PubMed Google Scholar Chen, X. et al. Neuron 100, 876–890 (2018). Article PubMed Google Scholar Takemura, S.-Y. et al. Nature 500, 175–181 (2013). Article PubMed Google Scholar Kilman, V. L. & Marder, E. J. Comp. Neurol. 374, 362–375 (1996). Article PubMed Google Scholar Bargmann, C. I. & Marder, E. Nature Methods 10, 483–490 (2013). Article PubMed Google Scholar Lam, R. et al. Preprint at https://arxiv.org/abs/2212.12794 (2023). Espeholt, L. et al. Nature Commun. 13, 5145 (2022). Article PubMed Google Scholar Witvliet, D. et al. Nature 596, 257–261 (2021). Article PubMed Google Scholar Haspel, G. et al. Preprint at https://arxiv.org/abs/2308.06578 (2023). Frégnac, Y. & Laurent, G. Nature 513, 27–29 (2014). Article PubMed Google Scholar Download references Reprints and Permissions Competing Interests The author declares no competing interests. Latest on: Brain Machine learning Neuroscience Coffee in stereo: how your brain records an odour’s spatial information NEWS 03 NOV 23 Psychedelic treatments are speeding towards approval — but no one knows how they work NEWS FEATURE 01 NOV 23 Dopamine determines how reward overrides risk NEWS & VIEWS 25 OCT 23 Jobs Post POSTDOCTORAL POSITION -- DEPARTMENT OF UROLOGY – BOSTON CHILDREN'S HOSPITAL AND HARVARD Fellow A postdoctoral position is open in the Urology Research Program at Boston Children's Hospital and Harvard Medical School working with the Lee an... Boston, Massachusetts Boston Children's Hospital (BCH) Welcome to join the Institute of Mathematics, Henan Academy of Sciences It recruits top talent in mathematics and aims to establish a high-level international center for mathematical exchange. Zhengzhou, Henan, China Institute of Mathematics, Henan Academy of Sciences Faculty Positions at Great Bay University, China We are now seeking outstanding candidates in Physics, Chemistry and Physical Sciences. Dongguan, Guangdong, China Great Bay University, China (GBU) Join China Pharmaceutical University Seeking Talents Worldwide for Exciting Opportunities Situated in the historical and cultural city of Nanjing, CPU seeks talented scientists from the globe. Nanjing, Jiangsu, China China Pharmaceutical University Faculty Positions at SUSTech Department of Biomedical Engineering We seek outstanding applicants for full-time tenure-track/tenured faculty positions. Positions are available for both junior and senior-level. Shenzhen, Guangdong, China Southern University of Science and Technology (Biomedical Engineering)",
    "originSummary": [
      "Artificial Intelligence (AI) and Machine Learning (ML) are progressively employed in neuroscience to comprehend the intricacies of the brain, with AI building computational models simulating brain activity, and ML enhancing understanding of complex neuronal networks.",
      "The performance of these models depends heavily on the quality of the data used for training.",
      "Despite the challenges, these technologies have helped researchers discover mechanisms behind certain animal behaviours, leading the scientific community to embrace these methodologies for brain mapping."
    ],
    "commentBody": "",
    "commentSummary": [
      "AI and ML are extensively used in neuroscience to simulate brain activity and comprehend complex neuronal networks respectively.",
      "The performance of these computational models heavily relies on the quality of the data used to train them.",
      "Despite being intricate, these methodologies are being increasingly adopted in the scientific community for brain mapping due to their contribution in understanding animal behaviours."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "AI Integration in Workplace to Transform Job Skill Requirements by 2030, Indicates LinkedIn's Global Talent Trends Report",
    "originLink": "https://www.zdnet.com/article/finding-the-path-toward-success-as-organizations-bring-ai-into-the-workplace/",
    "originBody": "innovation Home Innovation Artificial Intelligence Finding the path toward success as organizations bring AI into the workplace Companies that want to make the most of AI in the future must make the right preperations now. Written by Eileen Yu, Senior Contributing Editor Nov. 7, 2023 at 2:39 a.m. PT Eoneren/Getty Images Organizations that want to bring artificial intelligence (AI) into their workplaces are unlikely to gain the rewards for doing so if they overlook the need to take the necessary steps. What's more, businesses will have to start thinking about what these steps entail, so their workforce is prepared to face the oncoming wave. The skills that are needed for roles across the globe are estimated to change by at least 65% by 2030, driven by rapid developments in AI that will accelerate workplace change, according to LinkedIn's Global Talent Trends report. Also: The ethics of generative AI: How we can harness this powerful technology Shifts are already noticeable on the executive networking platform, where job posts containing references to AI or generative AI have more than doubled worldwide during the past two years. LinkedIn's research further reveals the majority of professionals in Asia-Pacific are excited to use AI at work, including 99% of employees in Indonesia, 98% in India, 97% in Singapore, and 84% in Australia. More employers are seeking talented inivdiduals who know how to use new AI technologies to boost productivity in their organization, said Chua Pei Ying, LinkedIn's Asia-Pacific head economist. The number of posts mentioning topics such as generative AI and GPT on the networking platform has grown 33 times during the past year, while job postings that mention GPT or ChatGPT have also increased by 21 times since November 2022. Also: How to use ChatGPT The spikes are hardly surprising because AI can be a powerful tool and it has the potential to revolutionize different aspects of work, improve productivity, and differentiate services, Chua told ZDNET. In India, for example, almost 70% of professionals think AI can help drive productivity, and another 60% believe it can push growth and revenue opportunities during the next year. Generative AI will change the way many people carry out their work tasks, such as assisting employees in drafting emails or running initial rounds of checks for errors in a budget report. Citing Microsoft's annual 2023 Work Trend Index, she said 70% of employees would delegate as much work as possible to AI to lessen their workload. Some 76% were comfortable tapping AI for administrative tasks, while 73% would do the same for creative work. In addition, 33% of pofessionals would use AI to produce quality work in half the time and 30% valued the ability to learn a new skill twice as fast. As it is, LinkedIn members worldwide are adding AI skills to their profiles more quickly than before, with the number of AI-skilled professionals being nine times bigger in June this year than in January 2016. Growth was the largest in Singapore, at 20 times larger over the same time period, followed by Finland at 16 times, India at 14 times, Ireland at 15 times, and Canada at 13 times. Also: 4 ways generative AI can stimulate the creator economy With AI already reshaping the way employees approach work, Chua said there will likely be greater emphasis on skills and employee training to use the technology moving forward. The global workplace has been in flux even before the emergence of ChatGPT, where skills for jobs have evolved by 25% since 2015, she noted. In addition, the pace of change is higher in Asia-Pacific, at 36% in Singapore, 30% in India, and 27% in Australia. \"There might be some trepidation among companies and professionals around the changing world of work and the impact of AI. The truth is, we've seen change like this before,\" she said. \"When the internet became more mainstream in the 1990s, it was viewed as a threat to many jobs and companies. But while some jobs were lost, new jobs were also created...[and] the internet also created many opportunities for companies. I believe the same will be true for AI.\" Figure out employee journey to reap AI rewards But before deciding whether to integrate AI into the workplace, organizations first need to understand what elements the technology comprises and the limitations of each type of AI, said Voo Poh Jee, partner of audit innovation at professional services firm KPMG in Singapore. Executives will then be able to determine whether implementing AI as a tool can meet their business' needs and evaluate the potential benefits and challenges associated with implementing it, Voo said in an email interview. Also: How does ChatGPT actually work? The implementation of new digital tools, such as AI, is also only part of the overall process, she said. Further iterations and refinements need to be considered along the way to ensure AI is properly integrated, adding that organizations should have the relevant resources to follow through on their strategies. AI will inform a lot of enterprise workflows, so organizations will need to understand the employee journey of different teams, in order to better determine where it should be applied to create the most value, said J. P. Gownder, Forrester's vice president and principal analyst. For instance, applying AI on data from the CRM system can help a sales executive seeking new leads to identify prospects that are most likely to convert to actual customers, which in turn helps the sales employee spend their time more productively. Gownder added that work tasks that are predictable, routine, or scalable are most likely to benefit from AI. AI can speed up research and development work, for example. He noted that Dow was able to reduce its product development process for polyurethane formulations by 200,000 times, cutting the discovery phase to just 30 seconds. Also: ChatGPT vs. Bing Chat vs. Google Bard: Which is the best AI chatbot? Voo also suggested that companies should start with processes that involve large volumes of data, varied datasets from multiple sources, or repetitive and manual tasks. In addition, AI could be used to facilitate data-monitoring processes and analysis. \"Implementing new technologies such as AI will require a substantial amount of time and investment to review the process, as tech implementation is also about process optimization, and ensure it runs smoothly,\" she said. \"This includes time spent on maintenance of the system and reviewing of work processes to ensure the integration is seamless.\" The management team also needs to be onboard, which helps set the path for the rest of the workforce to follow and creates an open mindset to embrace potential changes in workplace processes, she added. Despite the improvements in productivity that AI can bring into the workplace, companies' lack of trust in their workforce is a key challenge, according to Christina Janzer, Slack's senior vice president for research and analytics. Echoing Voo's views on the need for constant refinements, Janzer said the industry is still figuring out what the future of work means due to the emergence of AI. This effort requires continuous investment in trying new things and investing in new tools, she said. Also: The impact of artificial intelligence on software development? Still unclear Unless organizations have a foundation of trust, it will be difficult for employees to feel comfortable enough to try new things at work, she noted. Workers also function better if they are trusted, but one in four employees currently do not feel trusted, she said. AI has a lot of potential, but companies will need to be thoughtful about how it should be introduced into the workplace and how its impact is measured, Janzer said. This process will help ensure AI is bringing improved efficiencies and positive changes to the organization. Putting in place a proper change management plan is essential, she said, and this approach requires a two-way communications channel between employee and employer. Bringing AI into the workplace involves a big change and companies that have an open feedback loop can better understand, and resolve, potential employee concerns. This loop helps to create transparency, which will lead to employee trust, Janzer said. Also: Implementing AI into software engineering? Here's everything you need to know Companies can improve their chances of success with AI by looking at their employees and leaders, said Gownder. \"It all starts with people, specifically, the interactions between people and AI. Keeping humans in the loop -- retaining a role for human talent, guidance, oversight, and collaboration -- is crucial to success with any AI system today,\" he said. \"All employees, not just technical employees, need to have the skills, inclinations, and beliefs that will allow them to work successfully with AI.\" Employees will need to cultivate a growth mindset to adapt and pick up new, in-demand skills, Chua urged. Companies should also adopt a skills-first model in hiring and developing talent, she added. \"In this environment, employees are concerned about staying relevant and they want to expand their skillset,\" she said, noting that LinkedIn has seen a 65% increase in learning hours for the top 100 AI and generative AI courses from 2022 to 2023. Also: Six skills you need to become an AI prompt engineer Four in 10 executives in Australia and five in 10 in India are planning to upskill or hire for AI skills in the coming year, she said. The increase in hiring for AI talent in Asia-Pacific has also outpaced overall hiring growth, at 24% in Japan, 20% in Indonesia, 14% in Singapore, and 12% in Australia. \"AI is ushering in a new world of work, and the technology is already reshaping jobs, businesses, and industries,\" said Feon Ang, LinkedIn's vice president for talent solution and Asia-Pacific managing director. \"With so much change underway, now is the time for business leaders to assess the skills their organizations need now and in the years ahead, so they can set their teams up for success.\" artificial intelligence Can ChatGPT predict the future? Training AI to figure out what happens next Can AI curb loneliness in older adults? This robot companion is proving it's possible How AI in smart home tech can automate your life Robots plus generative AI: Everything you need to know when they work as one Editorial standards show comments related Few APAC firms will benefit from AI due to doubt and data management Companies aren't spending big on AI. Here's why that cautious approach makes sense How AI reshapes the IT industry will be 'fast and dramatic'",
    "originSummary": [
      "The skills required for jobs are predicted to shift by at least 65% by 2030 due to advancements in Artificial Intelligence (AI), as per LinkedIn's Global Talent Trends report.",
      "Globally, job postings citing AI or generative AI have more than doubled in the previous two years, revealing increased interest in AI technologies in the workforce.",
      "While AI implementation could enhance workplace productivity and create growth opportunities, the key challenge lies in the lack of trust companies have in their workforce to handle these advancements. Hence, emphasizing skills development and employee training could be pivotal."
    ],
    "commentBody": "",
    "commentSummary": [
      "The Global Talent Trends report by LinkedIn indicates a likely shift of at least 65% in required job skills by 2030 due to rapid advancements in artificial intelligence (AI).",
      "Worldwide job postings that reference AI or generative AI have more than doubled in the past two years, especially noticeable in the Asia-Pacific region where most professionals are eager to utilize AI in their work.",
      "Despite AI's potential to revolutionize work dynamics and boost productivity, growth, and revenue, its implementation faces hurdles, like a lack of trust in the workforce's capability to handle the technology; hence the imperative for skill training and development."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "Stanford Professor's Call for a Human-Centered, Academia-Industry Balanced Approach to AI Development",
    "originLink": "https://www.theatlantic.com/technology/archive/2023/11/tktk/675913/",
    "originBody": "Technology My North Star for the Future of AI The most powerful companies in the world are shaping what artificial intelligence will become—but they’ll never get it right without the ethos and values of university scientists. By Fei-Fei Li Illustration by Ben Kothe / The Atlantic. Source: Getty November 7, 2023, 7:06 AM ET Share Saved StoriesSave Whatever academics like me thought artificial intelligence was, or what it might become, one thing is now undeniable: It is no longer ours to control. As a computer science professor at Stanford, it had been a private obsession of mine—a layer of thoughts that superimposed itself quietly over my view of the world. By the mid-2010s, however, the cultural preoccupation with AI had become deafeningly public. Billboards along Highway 101 on the California coast heralded the hiring sprees of AI start-ups. Cover stories about AI fronted the magazines in my dentist’s waiting room. I’d hear fragments of conversation about AI on my car radio as I changed stations. The little red couch in my office, where so many of the projects that had defined our lab’s reputation had been conceived, was becoming the place where I’d regularly plead with younger researchers to keep some room in their studies for the foundational texts upon which our science was built. I’d noticed, first to my annoyance and then to my concern, how consistently those texts were being neglected as the ever-accelerating advances of the moment drew everyone’s attention to more topical sources of information. “Guys, I’m begging you—please don’t just download the latest preprints off arXiv every day,” I’d say. “Read Russell and Norvig’s book. Read Minsky and McCarthy and Winograd. Read Hartley and Zisserman. Read Palmer. Read them because of their age, not in spite of it. This is timeless stuff. It’s important.” arXiv (pronounced “archive”) is an online repository of academic articles in fields such as physics and engineering that have yet to be published but are made available to the curious in an early, unedited form known as a “preprint.” The repository has been a fixture of university culture for decades, but in the 2010s, it became an essential resource for staying current in a field that was progressing so rapidly that everything seemed to change from one week to the next, and sometimes overnight. If waiting months for the peer-review process to run its course was asking too much, was it any surprise that textbooks written years, if not entire generations, before were falling by the wayside? At the time, arXiv was just the start of the distractions competing for my students’ mindshare. More overtly, the hunt was already on as tech giants scrambled to develop in-house AI teams, promising starting salaries in the six-figure range, and sometimes higher, alongside generous equity packages. One machine-learning pioneer after another had departed Stanford, and even postdocs were on the menu by the middle of the decade. In one especially audacious episode, in early 2015, Uber poached some 40 roboticists from Carnegie Mellon University—all but decimating the department in the process—in the hopes of launching a self-driving car of its own. That was a hard-enough thing for my colleagues and me to witness. But for my students, young, eager, and still developing their own sense of identity, it seemed to fundamentally warp their sense of what an education was for. The trend reached its peak—for me, anyway—with an especially personal surprise. One of the computer scientists with whom I’d worked closest, Andrej Karpathy, told me he had decided to turn down an offer from Princeton to leave academia altogether. “You’re really turning them down? Andrej, it’s one of the best schools in the world!” “I know,” I remember him telling me. “But I can’t pass this up. There’s something really special about it.” Andrej had completed his Ph.D. and was heading into what must have been the most fertile job market in the history of AI, even for an aspiring professor. Despite a faculty offer from Princeton straight out of the gate—a career fast track that any one of our peers would have killed for—he was choosing to join a private research lab that no one had ever heard of. OpenAI was the brainchild of the Silicon Valley tycoons Sam Altman and Elon Musk, along with the LinkedIn co-founder Reid Hoffman and others, built with an astonishing initial investment of $1 billion. It was a testament to how seriously Silicon Valley took the sudden rise of AI, and how eager its luminaries were to establish a foothold within it. Andrej would be joining OpenAI’s core team of engineers. From the September 2023 issue: Does Sam Altman know what he’s creating? Shortly after OpenAI’s launch, I ran into a few of its founding members at a local get-together, one of whom raised a glass and delivered a toast that straddled the line between a welcome and a warning: “Everyone doing research in AI should seriously question their role in academia going forward.” The sentiment, delivered without even a hint of mirth, was icy in its clarity: The future of AI would be written by those with corporate resources. I was tempted to scoff, the way my years in academia had trained me to. But I didn’t. To be honest, I wasn’t sure I even disagreed. Where all of this would lead was anyone’s guess. Our field has been through dramatic ups and downs; the term AI winter—which refers to the several-years-long plateaus in artificial-intelligence capabilities, and the drying up of funding for AI research that came with it—was born from a history of great expectations and false starts. But in the 2010s, things felt different. One term in particular was gaining acceptance in tech, finance, and beyond: the Fourth Industrial Revolution. Even accounting for the usual hyperbole behind such buzz phrases, it rang true enough, and decision makers were taking it to heart. Whether driven by genuine enthusiasm, pressure from the outside, or some combination of the two, Silicon Valley’s executive class began making faster, bolder, and, in some cases, more reckless moves than ever. “So far the results have been encouraging. In our tests, neural architecture search has designed classifiers trained on ImageNet that outperform their human-made counterparts—all on its own.” The year was 2018, and I was seated at the far end of a long conference table at Google Brain, one of the company’s most celebrated AI-research orgs, in the heart of its headquarters—the Googleplex—in Mountain View, California. The topic was an especially exciting development that had been inspiring buzz across the campus for months: “neural architecture search,” an attempt to automate the optimization of a neural network’s architecture. A wide range of parameters defines how such models behave, governing trade-offs between speed and accuracy, memory and efficiency, and other concerns. Fine-tuning one or two of these parameters in isolation is easy enough, but finding a way to balance the push and pull between all of them is a task that often taxes human capabilities; even experts struggle to dial everything in just right. The convenience that automation would provide was an obviously worthy goal, and, beyond that, it could make AI more accessible for its growing community of nontechnical users, who could use it to build models of their own without expert guidance. Besides, there was just something poetic about machine learning models designing machine learning models—and quickly getting better at it than us. But all that power came with a price. Training even a single model was still cost-prohibitive for all but the best-funded labs and companies—and neural architecture search entailed training thousands. It was an impressive innovation, but a profoundly expensive one in computational terms. This issue was among the main points of discussion in the meeting. “What kind of hardware is this running on?” one researcher asked. The answer: “At any given point in the process, we’re testing a hundred different configurations, each training eight models with slightly different characteristics. That’s a combined total of 800 models being trained at once, each of which is allocated its own GPU.” Eight hundred graphics processing units. It was a dizzying increase. The pioneering neural network known as AlexNet had required just two GPUs to stop Silicon Valley in its tracks in 2012. The numbers grew only more imposing from there. Recalling from my own lab’s budget that the computing company Nvidia’s most capable GPUs cost something like $1,000 (which explained why we had barely more than a dozen of them ourselves), the bare-minimum expense to contribute to this kind of research now sat at nearly $1 million. Of course, that didn’t account for the time and personnel required to network so many high-performance processors together in the first place, and to keep everything running within an acceptable temperature range as all that silicon simmered around the clock. It doesn’t include the location either. In terms of both physical space and its astronomical power consumption, such a network wasn’t exactly fit for the average garage or bedroom. Even university labs like mine, at a prestigious and well-funded university with a direct pipeline to Silicon Valley, would struggle to build something of such magnitude. I sat back in my chair and looked around the room, wondering if anyone else found this as distressing as I did. I had decided to take a job as chief scientist of AI at Google Cloud in 2017. Nothing I’d seen in all my years at universities prepared me for what was waiting for me behind the scenes at Google. The tech industry didn’t just live up to its reputation of wealth, power, and ambition; it massively exceeded it. Everything I saw was bigger, faster, sleeker, and more sophisticated than what I was used to. The abundance of food alone was staggering. The breakrooms were stocked with more snacks, beverages, and professional-grade espresso hardware than anything I’d ever seen at Stanford or Princeton, and virtually every Google building had such a room on every floor. And all this before I even made my way into the cafeterias. Next came the technology. After so many years spent fuming over the temperamental projectors and failure-prone videoconferencing products of the 2000s, meetings at Google were like something out of science fiction. Cutting-edge telepresence was built into every room, whether executive boardrooms designed to seat 50 or closet-size booths for one, and everything was activated with a single tap on a touchscreen. Then there was the talent—the sheer, awe-inspiring depth of it. I couldn’t help but blush remembering the two grueling years it took to attract three collaborators to help build ambient intelligence for hospitals. Here, a 15-person team, ready to work, was waiting for me on my first day. And that was just the start—within only 18 months, we’d grow to 20 times that size. Ph.D.s with sterling credentials seemed to be everywhere, and reinforced the feeling that anything was possible. Whatever the future of AI might be, Google Cloud was my window into a world that was racing toward it as fast as it could. I still spent Fridays at Stanford, which only underscored the different level Google was at, as word of my new position spread and requests for internships became a daily occurrence. This was understandable to a point, as my students (and the occasional professor) were simply doing their best to network. What worried me, though, was that every conversation I had on the matter, without a single exception, ended with the same plea: that the research they found most interesting wouldn’t be possible outside a privately run lab. Even at a place like Stanford, the budgets just weren’t big enough. Often, in fact, they weren’t even close. Corporate research wasn’t just the more lucrative option; it was, more and more, the only option. Finally, there were the data—the commodity on which Google’s entire brand was based. I was surrounded by them—and not just by an indescribable abundance but data in categories I hadn’t even imagined before: from agriculture businesses seeking to better understand plants and soil, from media-industry customers eager to organize their content libraries, from manufacturers working to reduce product defects, and so much more. Back and forth I went, as the months stretched on, balancing a life between the two institutions best positioned to contribute to the future of AI. Both were brimming with talent, creativity, and vision. Both had deep roots in the history of science and technology. But only one seemed to have the resources to adapt as the barrier to entry rose like a mountain towering over the horizon, its peak well above the clouds. Read: The future of AI is GOMA My mind kept returning to those 800 GPUs gnawing their way through a computational burden that a professor and her students couldn’t even imagine overcoming. So many transistors. So much heat. So much money. A word like puzzle didn’t capture the dread I was beginning to feel. AI was becoming a privilege. An exceptionally exclusive one. Since the days of ImageNet, the database I’d created that helped advance computer vision and AI in the 2010s, it had been clear that scale was important—but the notion that bigger models were better had taken on nearly religious significance in recent years. The media was saturated with stock photos of server facilities the size of city blocks and endless talk about “big data,” reinforcing the idea of scale as a kind of magical catalyst, the ghost in the machine that separated the old era of AI from a breathless, fantastical future. And although the analysis could get a bit reductive, it wasn’t wrong. No one could deny that neural networks were, indeed, thriving in this era of abundance: staggering quantities of data, massively layered architectures, and acres of interconnected silicon really had made a historic difference. What did it mean for the science? What did it say about our efforts as thinkers if the secret to our work could be reduced to something so nakedly quantitative? To what felt, in the end, like brute force? If ideas that appeared to fail given too few layers, or too few training examples, or too few GPUs suddenly sprang to life when the numbers were simply increased sufficiently, what lessons were we to draw about the inner workings of our algorithms? More and more, we found ourselves observing AI empirically, as if it were emerging on its own. As if AI were something to be identified first and understood later rather than engineered from first principles. The nature of our relationship with AI was transforming, and that was an intriguing prospect as a scientist. But from my new perch at Google Cloud, with its bird’s-eye view of a world evermore reliant on technology at every level, sitting back and marveling at the wonder of it all was a luxury we couldn’t afford. Everything that this new generation of AI was able to do—whether good or bad, expected or otherwise—was complicated by the lack of transparency intrinsic to its design. Mystery was woven into the very structure of the neural network—some colossal manifold of tiny, delicately weighted decision-making units, meaningless when taken in isolation, staggeringly powerful when organized at the largest scales, and thus virtually immune to human understanding. Although we could talk about them in a kind of theoretical, detached sense—what they could do, the data they would need to get there, the general range of their performance characteristics once trained—what exactly they did on the inside, from one invocation to the next, was utterly opaque. An especially troubling consequence of this fact was an emerging threat known as “adversarial attacks,” in which input is prepared for the sole purpose of confusing a machine learning algorithm to counterintuitive and even destructive ends. For instance, a photo that appears to depict something unambiguous—say, a giraffe against a blue sky—could be modified with subtle fluctuations in the colors of individual pixels that, although imperceptible to humans, would trigger a cascade of failures within the neural network. When engineered just right, the result could degrade a correct classification like “giraffe” into something wildly incorrect, like “bookshelf” or “pocket watch,” while the original image would appear to be unchanged. But though the spectacle of advanced technology stumbling over wildlife photos might be something to giggle at, an adversarial attack designed to fool a self-driving car into misclassifying a stop sign—let alone a child in a crosswalk—hardly seemed funny. Granted, more engineering might have helped. A new, encouraging avenue of research known as “explainable AI,” or simply “explainability,” sought to reduce neural networks’ almost magical deliberations into a form humans could scrutinize and understand. But it was in its infancy, and there was no assurance it would ever reach the heights its proponents hoped for. In the meantime, the very models it was intended to illuminate were proliferating around the world. Even fully explainable AI would be only a first step; shoehorning safety and transparency into the equation after the fact, no matter how sophisticated, wouldn’t be enough. The next generation of AI had to be developed with a fundamentally different attitude from the start. Enthusiasm was a good first step, but true progress in addressing such complex, unglamorous challenges demanded a kind of reverence that Silicon Valley just didn’t seem to have. Read: We don’t actually know if AI is taking over everything Academics had long been aware of AI’s negative potential when it came to issues like these—the lack of transparency, the susceptibility to bias and adversarial influence—but given the limited scale of our research, the risks had always been theoretical. Even ambient intelligence, the most consequential work my lab had ever done, would offer ample opportunities to confront these pitfalls, as our excitement was always tempered by clinical regulations. But now that companies with market capitalizations approaching a trillion dollars were in the driver’s seat, the pace had accelerated radically. Ready or not, these were problems that needed to be addressed at the speed of business. As scary as each of these issues was in isolation, they pointed toward a future that would be characterized by less oversight, more inequality, and, in the wrong hands, possibly even a kind of looming, digital authoritarianism. It was an awkward thought to process while walking the halls of one of the world’s largest companies, especially when I considered my colleagues’ sincerity and good intentions. These were institutional issues, not personal ones, and the lack of obvious, mustache-twirling villains only made the challenge more confounding. As I began to recognize this new landscape—unaccountable algorithms, entire communities denied fair treatment—I concluded that simple labels no longer fit. Even phrases such as out of control felt euphemistic. AI wasn’t a phenomenon, or a disruption, or a puzzle, or a privilege. We were in the presence of a force of nature. What makes the companies of Silicon Valley so powerful? It’s not simply their billions of dollars, or their billions of users, or even the incomprehensible computational might and stores of data that dwarf the resources of academic labs. They’re powerful because of the many uniquely talented minds working together under their roof. But they can only harness those minds—they don’t shape them. I’d seen the consequences of that over and over: brilliant technologists who could build just about anything but who stared blankly when the question of the ethics of their work was broached. The time has come to reevaluate the way AI is taught at every level. The practitioners of the coming years will need much more than technological expertise; they’ll have to understand philosophy, and ethics, and even law. Research will have to evolve too. The vision I have for the future of AI is still tied together by something important: the university. AI began there, long before anyone was making money with it. Universities are where the spark of some utterly unexpected research breakthrough is still most likely to be felt. Perceptrons, neural networks, ImageNet, and so much since have come out of universities. Everything I want to build already has a foothold there. We just need to put them to use. This, collectively, is the next North Star: reimagining AI from the ground up as a human-centered practice. I don’t see it as a change in the journey’s direction so much as a broadening of its scope. AI must become as committed to humanity as it’s always been to science. It should remain collaborative and deferential in the best academic tradition, but unafraid to confront the real world. Starlight, after all, is manifold. Its white glow, once unraveled, reveals every color that can be seen. This article has been adapted from Fei-Fei Li’s new book, The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI. The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn Of AI By Fei-Fei Li Buy Book When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic.",
    "originSummary": [
      "Stanford professor, Fei-Fei Li, notes a shift in AI development from academic to industry-led, critiquing the trend of bypassing foundational principles and focusing excessively on new information.",
      "Fei-Fei Li raises concerns about increasing costs of AI training, and growing exclusivity in AI research owing to steep resource requirements, which could potentiate risks like adversarial attacks.",
      "She advocates for a re-evaluation of AI education, integrating fields like philosophy, ethics, and law for a more balanced, human-centric approach towards AI development."
    ],
    "commentBody": "",
    "commentSummary": [
      "Fei-Fei Li, a Stanford computer science professor, highlights the shift of artificial intelligence (AI) development from academia to the industry, criticizing the overreliance on new information and undermining of foundational ideologies.",
      "The article addresses the growing exclusivity of AI research due to the mounting need for resources, along with the potential risks of opaque AI algorithms, such as adversarial attacks leading to faults.",
      "The professor advocates for a reformed AI education, incorporating philosophy, ethics, and law, to achieve a more human-centered AI."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "Start-Up Captions Launches LipDub App, Enhancing Social Media Content Creation with AI Voice and Lip-Syncing Technology",
    "originLink": "https://www.forbes.com/sites/charliefink/2023/11/07/ai-lip-dubbing-puts-words-in-your-mouth/",
    "originBody": "FORBESINNOVATIONCONSUMER TECH AI Lip Dubbing Puts Words In Your Mouth Charlie Fink Contributor A former tech executive covering AI, XR and The Metaverse for Forbes. Nov 7, 2023,06:00am EST Lipdub is a new free app from New York City based Captions, a two year old video startup serving social media content creators with text captioning bundled with other services, like editing and special effects. Founder Gaurav Misra is the former head of design engineering at Snap and a former software developer at Microsoft. \"Captions entered the translation space in the fall of 2022 with the release of translated captioning. We then launched our voice translation feature, AI Dubbing, in early 2023, and were quickly met with enthusiastic adoption from users around the world,\" said Misra in an interview last week. Captions says it has more than 100,000 daily users and upwards of five million creators have tried its products, including its iOS video creation and editing app, and its website where creators can upload and compress video, use its AI Eye Contact feature to automatically correct videos where the speaker wasn’t looking at the camera, and automatically add AI-generated subtitles and captions to videos. Misra says in spite of the incredible costs of training new AI models, Captions is already profitable. In June, the company raised a $25 M Series B round led by Kleiner Perkins with participation from Sequoia Capital, Andreessen Horowitz (a16z) and SV Angel. To date, Captions has raised $40 M in funding. Gaurav Misra, co-founder and CEO of Captions, maker of the new LipDub app. CAPTIONS \"AI Dubbing's success inspired us to push this technology to the next level and introduce synced lip movement to the mix. Since then we’ve been focused on making the technology widely available, leading us to create and launch Lipdub as its own, separate app.” Misra said. Among those using the app are Disney-owned sports network ESPN and its commentator Omar Raja, “Mr. Wonderful” of Shark Tank fame, Twitch’s founder Justin Kan and the influencer Unnecessary Inventions. ElevenLabs, also based in New York City, has recently released its own AI Dubbing feature with support for 20 languages. It's a similarly well-funded startup, having raised a $20 million series A over the summer. It shares a notable investor with Captions, Andressen Horowitz. Victoria Weller, Chief of Staff to ElevenLabs’ CEO Mati Staniszewski, has nothing but praise for Captions. “We're close with their team and know about their lip sync release. To some extent it is similar, but in many ways it's also very different from what ElevenLabs does. The translation of videos is one aspect of the broader mission of ElevenLabs, where we're tackling to make any audio content accessible in any language, and that being possible in any voice.” MORE FOR YOU Ukraine’s American-Made M-1 Tanks Have Reached The Front Line Under Fire Over Robotaxi Safety, GM Halts Production Of Cruise Driverless Van WWE Raw Results: Winners And Grades On November 6, 2023 Meta Platforms has introduced SeamlessM4T, a versatile open-source platform capable of comprehending close to a hundred languages, whether spoken or written, and can convert these into translations simultaneously. Meanwhile, startups like MURF.AI, Play.ht, and WellSaid Labs are venturing into the realm of voice cloning by producing entirely artificial AI-generated voices that can be synchronized with video content. However, the focus of these companies is predominantly on the audio aspect, and unlike Captions, they do not provide as extensive a range of video editing capabilities. On one hand. I really love this tech. I’m already addicted. On the other hand, if this is what the good guys can do, what might bad actors might do, not with the apps these companies offer, but with their own, similar AI apps, purpose made for the spread of disinformation? I know, this takes some of the fun out of it for me, too. While we marvel at all these amazing new applications, I am fearful of their price. Follow me on Twitter or LinkedIn. Check out my website or some of my other work here. Charlie Fink Charlie Fink is a consultant, columnist, author, and adjunct, covering AI, XR, and the Metaverse for Forbes. He is the author of the critically acclaimed AR-enabled books Charlie Fink’s ... Read More Editorial Standards Print Reprints & Permissions",
    "originSummary": [
      "The start-up Captions has launched LipDub, a free app offering voice and lip-syncing technology to social media creators, including editing, special effects, translation, and AI-generated subtitles.",
      "Despite high training costs for new AI models, Captions is profitable and has already secured $40 million in funding.",
      "Companies such as ElevenLabs, Meta Platforms, MURF.AI, Play.ht, and WellSaid Labs are also making progress in the AI voice and translation technology sector."
    ],
    "commentBody": "",
    "commentSummary": [
      "Captions, a video start-up, has introduced LipDub, a free application offering voice and lip-syncing technology targeting social media creators and providing editing, special effects, translation, and AI-based subtitles.",
      "Despite the high expense involved in training new AI models, Captions has managed to attain profitability and has raised $40 million in funding.",
      "Notable users of LipDub include ESPN and influencer Unnecessary Inventions, highlighting the competitive landscape of AI voice and translation tech where companies like ElevenLabs, Meta Platforms, MURF.AI, Play.ht, and WellSaid Labs are similarly advancing."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "Crypto Startup Filta Pivots to AI-powered Digital Pets Amidst Declining Interest",
    "originLink": "https://www.wsj.com/articles/crypto-entrepreneurs-are-pivoting-to-ai-heres-one-founders-experience-34be4c51",
    "originBody": "By Marc Vartabedian Nov. 7, 2023 6:00 am ET|WSJ PRO Share Resize Listen (2 min) ‘The core of what we want to do is make something that people want,’ said Chris Horne. PHOTO: ANDY WOOD In 2021, at the height of the investor frenzy for crypto startups, entrepreneur Chris Horne raised $2 million in seed funding for Filta, a marketplace on which customers could buy and sell custom nonfungible token face filters that could digitally augment their face, say, by adding cat whiskers or a block head. But by the time the company launched in late summer of 2022, enthusiasm for crypto had waned and Filta was faltering. So Horne pivoted to the new hottest sector: artificial intelligence. He ditched the NFT idea, and this year relaunched Filta as a generative AI-powered digital pet, one that talks and can offer its owner emotional support. The technology behind his new company is OpenAI’s large language model, ChatGPT. And Horne is running his new Filta venture off the capital he raised for his original concept. Copyright ©2023 Dow Jones & Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8 Exclusive venture capital news and commentary. Stay on top of deals, funds, people and the broader influences on the market. SIGN IN SUBSCRIBE",
    "originSummary": [
      "Chris Horne, the founder of Filta, originally raised $2 million to establish a platform for trading custom NFT face filters in 2021.",
      "After experiencing declining interest in crypto and poor performance, Horne shifted the company's focus to leverage artificial intelligence (AI) in 2022.",
      "Filta was transformed into a platform offering AI-powered digital pets, featuring the use of OpenAI’s language model, ChatGPT. The financing for the pivot was sourced from the initial capital raised."
    ],
    "commentBody": "",
    "commentSummary": [
      "Chris Horne raised $2 million for Filta, a platform for trading custom NFT face filters, in 2021.",
      "Due to diminishing interest in crypto and subpar performance, Filta pivoted its business model to emphasize AI, particularly by creating an AI-driven digital pet using OpenAI's ChatGPT model.",
      "The new initiative is financed by the capital initially accumulated for the NFT platform."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  },
  {
    "title": "AI in Filmmaking: A Threat or Opportunity? Insights from Adobe-backed Film Festival",
    "originLink": "https://www.latimes.com/business/story/2023-11-07/ai-film-fest-movie-makings-contentious-future",
    "originBody": "BY BRIAN CONTRERASSTAFF WRITER NOV. 7, 2023 3 AM PT Facebook Twitter Show more sharing options Blazers brushed up against streetwear. Miniature cameras dangled from a woman’s earrings. One man’s hoodie read: “Rendered With Love.” Envoys from two parallel planets, software and showbiz, mingled in the Cary Grant Theatre on Thursday evening as they waited for the show to begin. Some recalled tales from the Cannes Film Festival; others debated the merits of different artificial intelligence platforms and pontificated on the future of “wearable AI.” They’d all gathered, several hundred of them, on the Sony Pictures lot for a film festival aimed at highlighting the nascent world of AI-assisted filmmaking. And though the mood in the venue was one of enthusiasm and curiosity, it came at a uniquely fraught moment for the two intertwined industries. After all, it was only a handful of weeks ago that Hollywood screenwriters wrapped up a protracted strike that found them picketing outside Sony and other major studios in protest of, among other things, the threat AI posed to their livelihoods. The writers’ union ultimately secured a contract that included substantial regulations on the use of the tech to script shows and films, but their on-screen counterparts in the Screen Actors Guild remain on strike over automation anxieties of their own. ADVERTISEMENT COMPANY TOWN Actors and writers aren’t the only ones worried about AI, new polling shows Aug. 6, 2023 Yet at Emergent Properties, the Adobe-backed festival featuring six short films made with a grab-bag of AI modules and techniques, that discord was largely background noise. Instead, the focus was on the doors that AI can open for independent filmmakers and hobbyists. “Tonight you’re gonna hear a lot about AI,” said Mike Gioia — one of the event’s organizers and a co-founder of the AI workflow startup Pickaxe — during his introductory remarks. “But really, tonight is about people. It’s about the filmmakers. And for anybody who’s a filmmaker in L.A., the reality that you deal with is there are just so many hoops you have to jump through to get an idea out of your head, onto a screen.” He continued: “In the best-case scenario, what AI does is it just makes [that] a lot simpler.” Many of the participating filmmakers emphasized what artificial intelligence software means for smaller-time creatives — people whose passion projects generally exist outside the Hollywood ecosystem subject to the recent strikes. A scene from French filmmaker Anna Apter’s “/Imagine,” screened during the Emergent Properties festival. (Jay L. Clendenin / Los Angeles Times) “I wanted to make something in my room and not have to wait two or three weeks for someone to say, ‘OK, let’s do this or that,’ ” said Anna Apter, a director who set AI-generated images of children’s birthday parties to a monologue about loneliness in her short film “/Imagine.” Speaking from Paris before the event, which she wasn’t able to attend in person, she added: “I know how all these jobs can be threatened by AI. But I feel like it gives people who don’t have big budgets — we don’t have excuses anymore, you know? We can do anything.” ENTERTAINMENT & ARTS FOR SUBSCRIBERS What’s really inside the Hollywood writers’ deal? Here’s the juicy stuff Oct. 7, 2023 “The whole idea is … how can we take this traditional model and not be afraid of these AI tools, but instead figure out a strategic way to let them work with every artist involved,” said Quinn Halleck, who used AI throughout the development of “Sigma_001,” a short film that drew inspiration from the real-life story of a Google engineer who thought the company’s AI chatbot may have become sentient. But not everyone is so optimistic about how these two sectors will butt up against each other as AI continues to develop. The last few months have seen Hollywood’s production pipeline grind to a halt amid dual strikes by the WGA screenwriters’ union and SAG-AFTRA, the actors’ union, both of which have expressed concerns that AI may put people out of work or neuter their creativity. The WGA ultimately secured a contract that didn’t close the door on AI screenwriting but did say writers can’t be compelled to use the software and blocked studios from cutting union members out of the loop entirely. Actors — who remain on strike — have focused their concerns on the digital simulation of performances. The Alliance of Motion Picture and Television Producers, which represents the major Hollywood studios in labor negotiations, has maintained that actors will retain control over their likenesses. Even the “Emergent Properties” film fest prompted some push-back. In the lead-up to the event, Gioia, one of the organizers, posted an invitation to the showcase on a Reddit forum for Los Angeles filmmakers. The response, at least publicly, was overwhelmingly negative. ADVERTISEMENT “I’m not trying to be a Luddite and realize AI is coming whether the industry as a whole wants it or not, but poor taste and poor timing,” reads the top comment. COMPANY TOWN On the brink of a possible SAG-AFTRA strike, some actors are wary of AI. Here’s why. July 7, 2023 Another highly ranked critic added: “Especially with the strikes going on you’re pretty out of line.” In private messages, people were more supportive of the event, Gioia said, and many ended up coming to the festival. Nevertheless, he said, he gets why a lot of commenters were critical. “For people who work in film for a paycheck doing skilled but non-creative labor (like rigging lights), it’s quite frightening and doesn’t have any upside,” he told The Times via text. (Sony, a struck company, did not sponsor the event, Gioia said; the festival organizers merely rented the venue from the studio.) The AI on display at Thursday’s event was largely used for special effects purposes, rather than to replace actors with digital doubles as SAG-AFTRA’s concerns have emphasized. Some of the filmmakers did use AI to write or develop their scripts, according to an event brochure, and a few films featured AI-generated faces or voices. ADVERTISEMENT Some event attendees admitted to feeling some hesitancy about the AI boom. A scene from filmmakers Caleb Ward and Aminah Folli’s “Zebulon Five.” Ward runs the AI storytelling community Curious Refuge with his wife, Shelby. (Jay L. Clendenin / Los Angeles Times) Shelby Ward, the co-founder of Curious Refuge — an online community for AI storytellers that helped make one of the evening’s entrants, a nature “documentary” about an alien planet — asked the audience during a post-screening question-and-answer session how many people had messed around with AI. Many raised their hands. She followed up: “I’m curious as well: who is kind of nervous about these tools? Anyone kind of anxious, a little overwhelmed?” A smattering of hands went up — fewer than before, but not none. “I would say I fall into that — I did,” Ward said. “I kind of went through a few months of, like, my paradigm was shifting.” But putting in the time to explore it, she continued, will ultimately make people more comfortable with the software. COMPANY TOWN Tom Hanks disavows AI clone amid Hollywood’s robot reckoning Oct. 2, 2023 The participating filmmakers cautioned that this technology still has its limits. Keeping characters’ appearances consistent between shots is tough, they said; eyeballs continue to flummox the software. “Right now, it’s impossible for it to tell a good story,” said Paul Trillo, another filmmaker. He continued, to applause: “I think that’s up to people to do.” (Trillo’s screening for the night was a music video filmed in the Louvre that used AI effects to distort and disfigure classic works of art. The conceit, he told The Times beforehand, is “a little tongue in cheek.”) Nevertheless, the filmmakers emphasized, AI technology is rapidly improving. Several talked about having to go back and redo parts of their films during production because a newer, better tool had come out midway through the process. In the world of AI filmmaking, Trillo said, “‘impossible’ is a very temporary term.”",
    "originSummary": [
      "Emergent Properties, an Adobe-sponsored film festival, featured six short films made using artificial intelligence (AI) techniques, amidst the Hollywood's Writers Guild of America (WGA) and Screen Actors Guild (SAG-AFTRA) strikes over AI-induced job threats.",
      "Some individuals criticize AI's potential to replace humans in certain creative tasks, while others view it as a tool to streamline and democratize filmmaking, particularly for independent producers.",
      "WGA has brokered a contract governing AI usage in scriptwriting, while the on-strike actors' union SAG-AFTRA has expressed concerns about digital simulations of performances."
    ],
    "commentBody": "",
    "commentSummary": [
      "Adobe-supported film festival, Emergent Properties, featured six short films made using artificial intelligence (AI) methods, amid two strikes from Hollywood's Writers Guild of America (WGA) and Screen Actors Guild (SAG-AFTRA) over AI job concerns.",
      "There are differing views on AI's role in the film industry; some worry about human job replacement, while others see AI as a chance to simplify and democratize the filmmaking process, notably for independent filmmakers.",
      "The WGA has negotiated a contract governing AI's use in scriptwriting, highlighting the ongoing negotiations and concerns in creative industries around AI utilization and its implications."
    ],
    "downloadMethod": "",
    "retryCount": 0,
    "time": 1699362340954
  }
]
