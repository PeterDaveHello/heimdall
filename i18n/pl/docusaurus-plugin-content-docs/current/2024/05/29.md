---
slug: '/2024/05/29'
---

# 2024-05-29

## [Słuchawki AI izolują pojedynczego mówcę w tłumie dzięki wykrywaniu wzroku](https://www.washington.edu/news/2024/05/23/ai-headphones-noise-cancelling-target-speech-hearing/)

- Uniwersytet Waszyngtoński (UW) opracował system sztucznej inteligencji o nazwie "Target Speech Hearing", który pomaga użytkownikom skupić się na jednym mówcy w hałaśliwym otoczeniu, patrząc na niego przez trzy do pięciu sekund.
- System ten, zaprezentowany na konferencji ACM CHI, wykorzystuje uczenie maszynowe do izolowania i wzmacniania głosu pożądanego mówcy w czasie rzeczywistym, nawet gdy użytkownik się porusza.
- Znajdująca się obecnie na etapie weryfikacji koncepcji technologia została przetestowana na 21 osobach, które zgłosiły znaczną poprawę czystości dźwięku, a w przyszłości planowane jest rozszerzenie jej na słuchawki douszne i aparaty słuchowe.

### [Reakcje](https://news.ycombinator.com/item?id=40508278)

- Tekst bada strategie i technologie mające na celu poprawę wrażeń słuchowych w hałaśliwym otoczeniu, koncentrując się na słuchawkach AI, zaawansowanym projektowaniu dźwięku i technologiach redukcji szumów.
- Podkreśla wyzwania związane z nowoczesnymi materiałami restauracyjnymi przyczyniającymi się do hałasu i wykorzystaniem technik tłumienia dźwięku pomimo kwestii konserwacyjnych i estetycznych.
- Omówiono postęp technologiczny, taki jak mikrofony kierunkowe, rozpoznawanie mowy w czasie rzeczywistym i selektywne filtrowanie dźwięku, a także obawy dotyczące prywatności i potencjalnych nadużyć.

## [Były członek zarządu OpenAI ujawnia kłamstwa i wykroczenia stojące za krótkim obaleniem Sama Altmana](https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5)

- Była członkini zarządu OpenAI Helen Toner ujawniła, że Sam Altman został na krótko usunięty ze stanowiska dyrektora generalnego z powodu wielu przypadków nieuczciwości i zatajania informacji przed zarządem.
- Przykłady obejmowały zarząd dowiadujący się o wydaniu ChatGPT za pośrednictwem Twittera i Altmana, który nie ujawnił swoich udziałów finansowych w firmie, wraz z oskarżeniami o dostarczanie niedokładnych informacji o bezpieczeństwie i "znęcanie się psychiczne" przez dwóch dyrektorów.
- Altman został przywrócony na stanowisko CEO niecały tydzień później po tym, jak pracownicy zagrozili odejściem, a Microsoft wyraził zainteresowanie zatrudnieniem jego zespołu; Toner zrezygnował wkrótce po powrocie.

### [Reakcje](https://news.ycombinator.com/item?id=40506582)

- CEO OpenAI, Sam Altman, został na krótko usunięty, a następnie ponownie zatrudniony, co ujawniło napięcia między autorytetem zarządu a wpływami kluczowych inwestorów i założycieli.
- Niewłaściwe postępowanie zarządu w sprawie zwolnienia Altmana doprowadziło do znacznego sprzeciwu pracowników i gróźb masowej rezygnacji, podkreślając złożoną dynamikę ładu korporacyjnego, wpływów pracowników i interesów finansowych.
- Incydent ten wywołał szerszą dyskusję na temat przywództwa w branży technologicznej, etycznych implikacji bezwzględnego zachowania oraz roli komunikacji i etyki w zarządzaniu przedsiębiorstwem.

## [Ponowne rozważenie przekierowania HTTP na HTTPS dla interfejsów API w celu zwiększenia bezpieczeństwa](https://jviide.iki.fi/http-redirects)

- Przekierowanie HTTP na HTTPS może ujawnić poufne dane lub umożliwić ataki typu Man-In-The-Middle (MITM), szczególnie w przypadku interfejsów API, do których dostęp uzyskuje oprogramowanie, które może nie obsługiwać nagłówków zabezpieczeń.
- Techniki takie jak HSTS (HTTP Strict Transport Security) i tryby HTTPS-Only poprawiają bezpieczeństwo, ale mogą nie być wystarczające dla interfejsów API, podkreślając potrzebę szybkiego podejścia do wczesnego wykrywania błędów.
- Najlepsze praktyki powinny zostać zaktualizowane w celu zalecenia, aby interfejsy API całkowicie odrzucały niezaszyfrowane żądania i odwoływały poświadczenia API wysyłane przez niezaszyfrowane połączenia, aby zapobiec zagrożeniom bezpieczeństwa.

### [Reakcje](https://news.ycombinator.com/item?id=40504756)

- Dyskusja kładzie nacisk na zwiększenie bezpieczeństwa API poprzez przekierowanie HTTP na HTTPS i unieważnienie kluczy API wysyłanych przez HTTP, aby zapobiec atakom typu Man-in-the-Middle (MITM).
- Podkreślono znaczenie właściwego zarządzania kluczami API, używania podpisanych hashy, nonces i znaczników czasu do uwierzytelniania, a także konieczność HTTPS dla integralności danych i prywatności.
- Rozmowa krytykuje poleganie na urzędach certyfikacji i sugeruje praktyczne rozwiązania, takie jak unikalne adresy URL lub klucze API do bezpiecznej kontroli dostępu w określonych kontekstach.

## [Llama3-V: Multimodalny model za 500 dolarów rywalizuje z GPT-4V pod względem wydajności](https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee)

- Llama3-V to nowy multimodalny model oparty na Llama3, zaprojektowany tak, aby konkurować z większymi modelami, takimi jak GPT-4V, ale w znacznie niższej cenie (poniżej 500 USD).
- Przewyższa on obecny najnowocześniejszy model, Llava, o 10-20% w multimodalnych testach porównawczych, wykorzystując SigLIP do osadzania obrazów i wyrównywania tokenów wizualnych i tekstowych za pomocą bloku projekcji z warstwami samoobserwacji.
- Kluczowe optymalizacje obejmują wstępne obliczanie osadzania obrazów i wykorzystanie MPS/MLX do wydajnego szkolenia, z procesem szkolenia obejmującym wstępne szkolenie na 600 000 przykładów i nadzorowane dostrajanie na 1 milionie przykładów.

### [Reakcje](https://news.ycombinator.com/item?id=40505099)

- Artykuł porównuje różne multimodalne modele sztucznej inteligencji, koncentrując się na Llama 3-V, która ma na celu dorównanie wydajności GPT-4V, ale jest mniejsza i tańsza.
- Podkreślono, że modele takie jak InternVL-1.5 i CogVLM przewyższają Llavę, a konkretne modele wyróżniają się w zadaniach takich jak OCR (optyczne rozpoznawanie znaków) i rozumienie GUI (graficzny interfejs użytkownika).
- Użytkownicy omawiają praktyczne zastosowania, ograniczenia i opłacalność tych modeli, w tym wykorzystanie GPT-4V w produkcji do zadań wizualnych oraz skuteczność nowoczesnych narzędzi OCR, takich jak PaddleOCR i TrOCR.

## [Mistral AI przedstawia Codestral: Potężna generatywna sztuczna inteligencja do generowania kodu](https://mistral.ai/news/codestral/)

- 29 maja 2024 r. firma Mistral AI uruchomiła Codestral, generatywny model sztucznej inteligencji o otwartej wadze do generowania kodu, przeszkolony w ponad 80 językach programowania.
- Codestral oferuje model o rozmiarze 22B i okno kontekstowe 32k, przewyższając konkurentów w testach porównawczych, takich jak RepoBench i HumanEval.
- Dostępny w ramach licencji nieprodukcyjnej Mistral AI, Codestral może być dostępny za pośrednictwem dedykowanego punktu końcowego lub zintegrowany z narzędziami takimi jak VSCode i JetBrains, a programiści chwalą jego szybkość, dokładność i wpływ na produktywność.

### [Reakcje](https://news.ycombinator.com/item?id=40512250)

- Mistral's Code Model, wydany przez mistral.ai, ma restrykcyjną licencję zabraniającą użytku komercyjnego, warunków na żywo i wewnętrznego użytku firmy, ograniczając jego praktyczne zastosowania i przyciągając krytykę.
- Debata wokół licencji Mistral podkreśla szersze kwestie praw autorskich i licencjonowania treści generowanych przez sztuczną inteligencję oraz nadużywanie terminu "open source" w sztucznej inteligencji.
- Użytkownicy wyrażają frustrację z powodu niespójnego generowania kodu przez sztuczną inteligencję, szczególnie w złożonych zadaniach, i omawiają ograniczenia i możliwości różnych modeli sztucznej inteligencji, w tym modeli Llama firmy Meta i GPT firmy OpenAI.

## [Kluczowe wnioski z rocznej pracy z dużymi modelami językowymi (część I)](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/)

- W artykule "What We Learned from a Year of Building with LLMs (Part I)" Eugene Yan i współpracownicy badają szybki postęp i praktyczne zastosowania dużych modeli językowych (LLM), jednocześnie podejmując wyzwania związane z opracowywaniem skutecznych produktów AI.
- Kluczowe lekcje obejmują najlepsze praktyki w zakresie podpowiedzi, generowania wspomaganego odzyskiwaniem (RAG), inżynierii przepływu i oceny, z podkreśleniem technik takich jak podpowiedzi n-shot i podpowiedzi łańcucha myśli.
- Artykuł zawiera również porady operacyjne dotyczące zarządzania agentami AI, udoskonalania podpowiedzi, dostrajania modeli oraz zmniejszania kosztów i opóźnień poprzez buforowanie, kładąc nacisk na praktyczne oceny i podejście skoncentrowane na człowieku.

### [Reakcje](https://news.ycombinator.com/item?id=40508390)

- Spostrzeżenia z rocznej pracy z dużymi modelami językowymi (LLM) podkreślają znaczenie wielokrotnego próbkowania w celu zmniejszenia liczby halucynacji i generowania uzasadnień przed podjęciem decyzji w celu uzyskania dokładniejszych wyników.
- W artykule omówiono wyzwania związane z oceną wyników LLM, wpływ temperatury na losowość wyników i błędne przekonania na temat próbkowania, a także doświadczenia z wykorzystaniem narzędzi takich jak patchboty i wyszukiwanie wiązką.
- Odnosi się do obaw branży, takich jak wysokie wskaźniki błędów, inwestycje oparte na FOMO oraz agresywne dążenie firm takich jak Google do integracji sztucznej inteligencji pomimo potencjalnych problemów z jakością usług.

## [Ekspert ostrzega przed ryzykiem utraty największych talentów w związku z obowiązkiem powrotu do pracy](https://www.rte.ie/brainstorm/2024/0521/1450272-return-to-office-mandates-employees-work-from-home/)

- Profesor Kevin Murphy z Uniwersytetu w Limerick twierdzi, że pracownicy zdalni są bardziej produktywni i zadowoleni w porównaniu do tych pracujących w biurach.
- Dążenie do powrotu do biura (RTO) po pandemii grozi utratą największych talentów, ponieważ wielu pracowników odrzuca obecnie tradycyjne normy biurowe.
- Kierownictwo powinno przedstawić przekonujące powody i zachęty do powrotu do biura, uznając zmianę dynamiki władzy faworyzującą pracowników lub ryzykując utratę cennych talentów na rzecz bardziej elastycznych konkurentów.

### [Reakcje](https://news.ycombinator.com/item?id=40509409)

- Debata między pracą zdalną a nakazem powrotu do biura (RTO) koncentruje się na elastyczności, komforcie i potencjalnej utracie pracowników, którzy preferują pracę zdalną.
- Dojazdy do pracy zapewniają niektórym odpoczynek psychiczny, ale dla innych stanowią wyzwanie, takie jak zanieczyszczenie środowiska, wysokie koszty i zatarte granice, wpływając na równowagę między życiem zawodowym a prywatnym i rozwój kariery.
- Praca zdalna jest postrzegana jako bardziej wydajna i zrównoważona, oferując korzyści, takie jak więcej czasu dla rodziny i zmniejszenie emisji dwutlenku węgla, ale może zaniedbywać młodszych pracowników i wymagać jasnej komunikacji na temat korzyści RTO.

## [Kanadyjska ustawa C-26: Kontrowersyjne uprawnienia do instalowania backdoorów w sieciach w celu inwigilacji](https://www.theglobeandmail.com/opinion/article-ottawa-wants-the-power-to-create-secret-backdoors-in-our-networks-to/)

- Bill C-26, federalna ustawa o cyberbezpieczeństwie w Kanadzie, przyznaje rządowi uprawnienia do zmuszania firm telekomunikacyjnych do instalowania backdoorów w zaszyfrowanych sieciach, potencjalnie zagrażając bezpieczeństwu.
- Krytycy, w tym Citizen Lab z Uniwersytetu w Toronto, twierdzą, że środki te osłabiłyby szyfrowanie 5G i inne funkcje bezpieczeństwa, zwiększając podatność na zagrożenia cybernetyczne.
- Pomimo ostrzeżeń ekspertów, ustawa została przyjęta bez poprawek, zaprzeczając pro-szyfrowemu stanowisku Kanady i potencjalnie ustanawiając niebezpieczny precedens dla innych krajów.

### [Reakcje](https://news.ycombinator.com/item?id=40512509)

- Kanadyjski rząd stara się uzyskać uprawnienia do tworzenia tajnych backdoorów w sieciach telekomunikacyjnych w celu inwigilacji, omijając tradycyjny nadzór prawny, co budzi poważne obawy o prywatność i możliwość nadużyć ze strony organów ścigania.
- Krytycy twierdzą, że może to prowadzić do inwazyjnego monitorowania podobnego do praktyk NSA, co wiąże się z debatami na temat kanadyjskiej konstytucji, "klauzuli niezależności" i możliwości legalnego przechwytywania.
- Dyskusja obejmuje historyczne przykłady inwigilacji, takie jak podczas protestów kierowców ciężarówek, a także szersze tematy związane z nadmiernym zasięgiem rządu, prywatnością i reakcjami społecznymi na władzę.

## [Trzy podstawowe prawa rządzące nieuniknioną złożonością systemów oprogramowania](https://maheshba.bitbucket.io/blog/2024/05/08/2024-ThreeLaws.html)

- Artykuł omawia trzy podstawowe prawa przyczyniające się do niepotrzebnej złożoności w inżynierii oprogramowania, w szczególności w systemach infrastrukturalnych.
- **Pierwsze prawo**: Dobrze zaprojektowane systemy z czasem ulegają degradacji do źle zaprojektowanych z powodu ciągłych modyfikacji.
- **Drugie prawo**: Złożoność wzrasta, gdy udane systemy przedkładają udział w rynku nad dobry projekt abstrakcji, co prowadzi do trudnych do modyfikacji systemów.
- **Trzecia zasada**: Nie ma górnej granicy złożoności oprogramowania, napędzanej przez różne umiejętności i filozofie programistów, co skutkuje skomplikowanymi projektami.

### [Reakcje](https://news.ycombinator.com/item?id=40509572)

- Dyskusja dotyczy wyzwań związanych z zarządzaniem złożonością oprogramowania, zwłaszcza w starszych systemach, oraz kompromisów między kosztami a jakością, co często prowadzi do długu technicznego.
- Podkreśla znaczenie przyrostowej refaktoryzacji, utrzymywania silnej kultury inżynieryjnej i rozróżniania między złożonością istotną a przypadkową w celu skutecznego zarządzania oprogramowaniem.
- Uczestnicy podkreślają konieczność ciągłej konserwacji, wpływ złych wyborów programistycznych oraz rolę wsparcia kierownictwa w uzasadnianiu wysiłków związanych z refaktoryzacją.

## [Od startupu do sprzedaży: Podróż Michaela Lyncha z TinyPilot](https://mtlynch.io/i-sold-tinypilot/)

- Michael Lynch stworzył TinyPilot w połowie 2020 roku, urządzenie do zdalnego sterowania serwerem, które szybko zyskało popularność i rozrosło się w firmę z rocznym przychodem w wysokości 1 miliona dolarów i siedmioosobowym zespołem.
- Lynch sprzedał TinyPilot za 600 tys. dolarów, uzyskując netto 490 803 dolarów po odliczeniu kosztów, z powodu stresu związanego z zarządzaniem biznesem sprzętowym i chęcią powrotu do kodowania i założenia rodziny.
- Sprzedaż, w której pośredniczyło Quiet Light Brokerage, wiązała się z wyzwaniami, takimi jak zrównoważenie stresu założyciela, znalezienie nabywcy i zarządzanie due diligence; nabywcą był Scott, specjalista ds. mediów korporacyjnych.

### [Reakcje](https://news.ycombinator.com/item?id=40512500)

- Michael Lynch sprzedał swoją firmę, TinyPilot, i omówił znaczące koszty związane ze sprzedażą, w tym prowizje maklerskie i opłaty prawne, które wyniosły około 18% ceny sprzedaży.
- Przedsiębiorcza podróż Lyncha obejmowała przejście od wysoko płatnej pracy w Google do doceniania autonomii i kreatywności, podkreślania wartości edukacyjnej przedsiębiorczości i krytykowania skupienia się branży technologicznej na całkowitym wynagrodzeniu.
- Lynch planuje bootstrapować przyszłe przedsięwzięcia, koncentrując się na produktach edukacyjnych i oprogramowaniu jako usłudze (SaaS), unikając sprzętu ze względu na jego złożoność i wyzwania.

## [Były członek zarządu OpenAI ujawnia powody zwolnienia i przywrócenia do pracy Sama Altmana](https://www.theverge.com/2024/5/28/24166713/openai-helen-toner-explains-why-sam-altman-was-fired)

- W listopadzie 2023 r. zarząd OpenAI niespodziewanie zwolnił dyrektora generalnego Sama Altmana, powołując się na "jawne kłamstwa" i manipulacyjne zachowanie, które podważyło zaufanie.
- Konkretne kwestie obejmowały nieujawnioną własność Altmana w OpenAI Startup Fund, dostarczanie niedokładnych informacji na temat bezpieczeństwa i tworzenie toksycznego środowiska pracy.
- Pomimo tych zarzutów, wewnętrzne i zewnętrzne naciski, w tym wsparcie ze strony pracowników i Microsoftu, doprowadziły do przywrócenia Altmana do pracy, a niezależny przegląd nie wykazał żadnych problemów z bezpieczeństwem produktów lub działalnością firmy.

### [Reakcje](https://news.ycombinator.com/item?id=40509399)

- Były członek zarządu OpenAI ujawnił, że Sam Altman został zwolniony z powodu nieuczciwości, co rodzi pytania o świadomość zarządu dotyczącą uruchomienia ChatGPT.
- Sytuacja ta wywołała dyskusje na temat przejrzystości organizacyjnej, nadzoru zarządu i etycznego zarządzania, z porównaniami do upadłości korporacyjnych, takich jak Enron.
- Istnieje sceptycyzm co do zaufania i praktyk bezpieczeństwa OpenAI, z odejściami pracowników i krytyką przywództwa Altmana, wraz z debatami na temat biegłości technicznej i roli zarządu.

## [Wyciek wyszukiwarki Google ujawnia sekrety algorytmu rankingowego i 2596 modułów](https://searchengineland.com/google-search-document-leak-ranking-442617)

- Poważny wyciek wewnętrznych dokumentów Google Search ujawnił krytyczne aspekty algorytmu rankingowego Google, w tym wykorzystanie kliknięć, linków, treści, podmiotów i danych Chrome.
- Eksperci branżowi Rand Fishkin i Michael King przeanalizowali dokumenty, ujawniając 2596 modułów rankingowych, znaczenie różnorodności linków, trafności, udanych kliknięć i rozpoznawalności marki.
- Dokumenty ujawniają również wykorzystanie przez Google informacji o autorach, autorytecie witryny i "twiddlerach" do dostosowywania rankingów, oferując cenne informacje dla SEO, pomimo nieznanej dokładnej wagi czynników rankingowych.

### [Reakcje](https://news.ycombinator.com/item?id=40510125)

- Dokument Google Search, który wyciekł, wywołał debatę na temat algorytmu rankingowego i wpływu programu reklamowego Google na wyniki wyszukiwania.
- Użytkownicy omawiają alternatywy, takie jak Kagi i search.marginalia.nu, z mieszanymi opiniami na temat dostosowywania Kagi, niekomercyjnego ukierunkowania oraz kwestii związanych ze spamem i treściami generowanymi przez sztuczną inteligencję.
- Rozmowa podkreśla pragnienie wyszukiwarek, które przedkładają preferencje użytkowników nad przychody z reklam, dotykając manipulacji SEO, potencjału dużych modeli językowych (LLM) oraz obaw dotyczących autentyczności recenzji online i kryteriów rankingowych Google.

## [ChatTTS: Zaawansowany model TTS o otwartym kodzie źródłowym do naturalnego dialogu w języku angielskim i chińskim](https://github.com/2noise/ChatTTS)

- ChatTTS to model zamiany tekstu na mowę (TTS) zoptymalizowany pod kątem dialogów, obsługujący zarówno język angielski, jak i chiński, i przeszkolony na podstawie ponad 100 000 godzin danych.
- Wersja open-source na HuggingFace zawiera 40 000 godzin wstępnie wytrenowanego modelu, wyróżniającego się naturalną i ekspresyjną syntezą mowy z precyzyjną kontrolą prozodyczną.
- Model jest przeznaczony wyłącznie do użytku akademickiego, a w przyszłości planowane jest udostępnienie dodatkowych funkcji i poprawa stabilności.

### [Reakcje](https://news.ycombinator.com/item?id=40507039)

- W dyskusji podkreślono rozwój i wydajność modeli TTS, takich jak ChatTTS i Piper TTS, zwracając uwagę na takie kwestie, jak powolne przetwarzanie i wyzwania związane z jakością głosu.
- Użytkownicy podkreślają potrzebę wysokiej jakości TTS w wielu językach i debatują nad skutecznością ludzkich i automatycznych głosów w audiobookach.
- Istnieje krytyka wprowadzających w błąd twierdzeń o "otwartym kodzie źródłowym" w projektach TTS i wezwanie do stworzenia kompleksowej listy prawdziwie otwartych modeli i danych TTS.

## [Google milczy w sprawie rzekomego wycieku 2500 stron zawierających szczegóły algorytmu wyszukiwania](https://www.theverge.com/2024/5/28/24166177/google-search-ranking-algorithm-leak-documents-link-seo)

- Wyciek 2500 stron wewnętrznych dokumentów Google, udostępnionych przez eksperta SEO Randa Fishkina, może ujawnić rozbieżności między publicznymi oświadczeniami Google a jego rzeczywistymi praktykami dotyczącymi algorytmów wyszukiwania.
- Dokumenty sugerują wykorzystanie danych Chrome w rankingach i śledzeniu informacji o autorach, podważając wcześniejsze zapewnienia Google i wywołując debatę na temat przejrzystości firmy.
- Google nie skomentował legalności dokumentów, a incydent ten podkreśla ciągłe obawy o nieprzejrzysty charakter operacji wyszukiwania Google w kontekście kontroli antymonopolowej.

### [Reakcje](https://news.ycombinator.com/item?id=40505310)

- Wyciek dokumentacji algorytmu wyszukiwania Google ujawnił potencjalne rozbieżności między publicznymi oświadczeniami Google a ich rzeczywistymi praktykami.
- Wyciek sugeruje, że przedstawiciele Google mogli zdyskredytować dokładne ustalenia społeczności marketingowych, technicznych i dziennikarskich, budząc obawy etyczne dotyczące manipulacji SEO.
- Dyskusje prawne na GitHubie debatują nad znaczeniem i legalnością wycieku, z różnymi opiniami na temat jego wpływu na status tajemnicy handlowej i ochronę praw autorskich.

<head>
  <meta property="og:title" content="Słuchawki AI izolują pojedynczego mówcę w tłumie dzięki wykrywaniu wzroku" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://og.cho.sh/api/og/?title=S%C5%82uchawki%20AI%20izoluj%C4%85%20pojedynczego%20m%C3%B3wc%C4%99%20w%20t%C5%82umie%20dzi%C4%99ki%20wykrywaniu%20wzroku&subheading=%C5%9Broda%2C%2029%20maja%202024%3A%20Podsumowanie%20Hacker%20News" />
</head>
